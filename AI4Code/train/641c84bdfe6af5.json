{"cell_type":{"ed8da002":"code","ba64ca62":"code","693a9def":"code","cb0f4fef":"code","6cfd2bfc":"code","645bfe21":"code","69161f2b":"code","0a65b1c9":"code","4c8dc756":"code","a923e7b2":"code","a89c4d3f":"code","9290d6d3":"code","9800aceb":"code","88f91baf":"code","106fb25f":"code","1c628df7":"code","851021de":"code","afd40d9c":"code","7f10e2a5":"markdown","9972a9e0":"markdown","87b87676":"markdown","7be5ddfc":"markdown","40eea82c":"markdown","066288a6":"markdown","e01087b5":"markdown","88fe741c":"markdown","af5feee7":"markdown","30dacfa5":"markdown"},"source":{"ed8da002":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport time, random, math, string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.datasets import Multi30k\nfrom torchtext.data import Field, BucketIterator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ba64ca62":"tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split() \nreverse_tokenizer = lambda x: tokenizer(x)[::-1]\n\nSRC = Field(tokenize=reverse_tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\nTRG = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n\ntrain_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'),\n                                                   fields=(SRC, TRG))","693a9def":"print(f\"Number of training examples: {len(train_data.examples)}\")\nprint(f\"Number of validation examples: {len(valid_data.examples)}\")\nprint(f\"Number of test examples: {len(test_data.examples)}\")","cb0f4fef":"print(vars(train_data.examples[0]))","6cfd2bfc":"SRC.build_vocab(train_data, min_freq=2)\nTRG.build_vocab(train_data, min_freq=2)\n\nprint(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\nprint(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")","645bfe21":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nBATHC_SIZE = 128\n# We use a BucketIterator instead of the standard Iterator as it create batches in such a way that it minimizes the amount \n# of padding in both the source and target sentences.\ntrain_iter, valid_iter, test_iter = BucketIterator.splits((train_data, valid_data, test_data),\n                                                          batch_size=BATHC_SIZE, device=device)","69161f2b":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        \n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n        \n        self.dropout = nn.Dropout(dropout)\n    def forward(self, src):\n        # src : [sen_len, batch_size]\n        embedded = self.dropout(self.embedding(src))\n        \n        # embedded : [sen_len, batch_size, emb_dim]\n        outputs, (hidden, cell) = self.rnn(embedded)\n        # outputs = [sen_len, batch_size, hid_dim * n_directions]\n        # hidden = [n_layers * n_direction, batch_size, hid_dim]\n        # cell = [n_layers * n_direction, batch_size, hid_dim]\n        return hidden, cell","0a65b1c9":"class Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.output_dim = output_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        \n        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n        \n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, cell):\n        \n        # input = [batch_size]\n        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n        # cell = [n_layers * n_dir, batch_size, hid_dim]\n        \n        input = input.unsqueeze(0)\n        # input : [1, ,batch_size]\n        \n        embedded = self.dropout(self.embedding(input))\n        # embedded = [1, batch_size, emb_dim]\n        \n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        # output = [seq_len, batch_size, hid_dim * n_dir]\n        # hidden = [n_layers * n_dir, batch_size, hid_dim]\n        # cell = [n_layers * n_dir, batch_size, hid_dim]\n        \n        # seq_len and n_dir will always be 1 in the decoder\n        prediction = self.fc_out(output.squeeze(0))\n        # prediction = [batch_size, output_dim]\n        return prediction, hidden, cell","4c8dc756":"class Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            'hidden dimensions of encoder and decoder must be equal.'\n        assert encoder.n_layers == decoder.n_layers, \\\n            'n_layers of encoder and decoder must be equal.'\n        \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src = [sen_len, batch_size]\n        # trg = [sen_len, batch_size]\n        # teacher_forcing_ratio : the probability to use the teacher forcing.\n        batch_size = trg.shape[1]\n        trg_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        # tensor to store decoder outputs\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        # last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(src)\n        \n        # first input to the decoder is the <sos> token.\n        input = trg[0, :]\n        for t in range(1, trg_len):\n            # insert input token embedding, previous hidden and previous cell states \n            # receive output tensor (predictions) and new hidden and cell states.\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            \n            # replace predictions in a tensor holding predictions for each token\n            outputs[t] = output\n            \n            # decide if we are going to use teacher forcing or not.\n            teacher_force = random.random() < teacher_forcing_ratio\n            \n            # get the highest predicted token from our predictions.\n            top1 = output.argmax(1)\n            # update input : use ground_truth when teacher_force \n            input = trg[t] if teacher_force else top1\n            \n        return outputs","a923e7b2":"# First initialize our model.\nINPUT_DIM = len(SRC.vocab)\nOUTPUT_DIM = len(TRG.vocab)\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM = 512\nN_LAYERS = 2\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nencoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\ndecoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n\nmodel = Seq2Seq(encoder, decoder, device).to(device)","a89c4d3f":"def init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","9290d6d3":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","9800aceb":"optimizer = optim.Adam(model.parameters())\n\nTRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n\ncriterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)","88f91baf":"def train(model, iterator, optimizer, criterion, clip):\n    \n    model.train()\n    \n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        src = batch.src\n        trg = batch.trg\n        \n        optimizer.zero_grad()\n        # trg = [sen_len, batch_size]\n        # output = [trg_len, batch_size, output_dim]\n        output = model(src, trg)\n        output_dim = output.shape[-1]\n        \n        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n        output = output[1:].view(-1, output_dim) \n        trg = trg[1:].view(-1)\n        # trg = [(trg_len-1) * batch_size]\n        # output = [(trg_len-1) * batch_size, output_dim]\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","106fb25f":"def evaluate(model, iterator, criterion):\n    \n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n        \n        for i, batch in enumerate(iterator):\n            \n            src = batch.src\n            trg = batch.trg\n            \n            output = model(src, trg, 0) # turn off teacher forcing.\n            \n            # trg = [sen_len, batch_size]\n            # output = [sen_len, batch_size, output_dim]\n            output_dim = output.shape[-1]\n            \n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n            \n            loss = criterion(output, trg)\n            \n            epoch_loss += loss.item()\n            \n    return epoch_loss \/ len(iterator)","1c628df7":"# a function that used to tell us how long an epoch takes.\ndef epoch_time(start_time, end_time):\n    \n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  \/ 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","851021de":"N_EPOCHS = 10\n\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iter, criterion)\n    \n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'Seq2SeqModel.pt')\n    print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")","afd40d9c":"def test():\n    best_model = Seq2Seq(encoder, decoder, device).to(device)\n    best_model.load_state_dict(torch.load('Seq2SeqModel.pt'))\n    \n    test_loss = evaluate(model, test_iter, criterion)\n    \n    print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n    \ntest()","7f10e2a5":"### Decoder\n\nNext, we will build our decoder. which also be a 2-layer LSTM.\n\n![](https:\/\/github.com\/bentrevett\/pytorch-seq2seq\/raw\/3a8dc5515ff28cb059532439c5687126dd30015f\/assets\/seq2seq3.png)\n\nWe can use the following equations to explain the decoder model.\n$$\n(s_t^1, c_t^1) = DecoderLSTM^1(y_t, (s_{t-1}^1, c_{t-1}^1))\n$$\n$$\n(s_t^2, c_t^2) = DecoderLSTM^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n$$\nRemember that the initial hidden and cell states to our decoder are our context vectors, which are the final hidden and cell of our encoder from the same layer. i.e. $(s_0^l, c_0^l) = z^l = (h_T^l, c_T^l)$.\n\nWe then pass the hidden state from the top layer of the RNN, $s_t^2$ through a linear layer $f$, to make a prediction of what the next token in the target (output) sequence should be $\\hat{y}_{t+1}$\n$$\n\\hat{y}_{t+1} = f(s_t^2) \n$$","9972a9e0":"Next, we'll define our training loop.\n\nFirst. we'll set the model into \"training mode\" (turn on the dropout & batch normalization), and then iterate through our data iterator.\n\nAs stated before, our decoder loop starts at 1, not 0. This means the 0th element of our outputs tensor remains all zeros. So our trg & outputs look something like:\n$$\ntrg = [<sos>, y_1, y_2, y_3, <eos>]\n$$\n$$\noutput = [0, \\hat{y}_1,\\hat{y}_2,\\hat{y}_3, <eos>]\n$$\nHere, when we calculate the loss, we cut off the first element of each tensor to get:\n$$\ntrg = [y_1, y_2, y_3, <eos>]\n$$\n$$\noutput = [\\hat{y}_1,\\hat{y}_2,\\hat{y}_3, <eos>]\n$$\nAt each iterator:\n\n- get the source and target sentences from the batch, X and Y\n\n- zero the gradients calculated from the last batch\n \n- feed the source and target into the model to get the output $\\hat{y}$\n\n- as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n    - we slice off the first column of the output and target tensors as mentioned above.\n    \n- calculate the gradients with loss.backward()\n\n- clip the gradients to prevent them from exploding (a common issue in RNNs)\n\n- update the parameters of our model by doing an optimizer.step()\n\n- sum the loss value to a running total.\n\nFinally, we return the loss that is average over all batches.","87b87676":"# Sequence to Sequence Learning with Neural Network\n\nAcknowledgement : this notebook origins from https:\/\/github.com\/bentrevett\/pytorch-seq2seq \n\nNote : This notebook is just for learning Seq2seq model.  ","7be5ddfc":"### Encoder \nFirst, the encoder, a 2 layer LSTM. The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut down to 2-layers. The concept of multi-layer RNN is easy to expand from 2 to 4 layers.\n\nFor a multi-layer RNN, the input sentence, $X$, goes into the first (bottom) layer of the RNN and hiddne states, $H=\\{h_1, h_2, \\cdots,h_T\\}$ output by this layer are used as inputs to the RNN in the layer above. Thus representing each layer with a superscript, the hidden states in the first layer are given by :\n$$\nh_t^1 = EncoderRNN^1(x_t, h_{t-1}^1) \\tag{4}\n$$\nThe hidden states in the second layer are given by:\n$$\nh_t^2 = EncoderRNN^2(h_t^1, h_{t-1}^2) \\tag{5} \n$$\nUsing a multi-layer RNN also means we'll also need an initial hidden state as input per layer, $h_0^l$, and we will also output a context vector per layer $z^l$","40eea82c":"## Summary \n\nThrough the model is tranditional Seq2Seq model as the author mentioned before. As a beginner of pytorch and deeplearning, there is also many useful tricks worth learning. As a notebook I list them below:\n\n### Data Preparing Part\n\n- In the original paper, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". \n\nit means when we want to get the context vector, it is beneficial to reverse the order of the input.\n\n- When build the vocabulary we can use min_freq parameter to remove the rare words in the corpus.\n\ne.g. SRC.build_vocab(train_data, min_freq=2)\n\n### Seq2Seq Model Part\n\n- In this notebook, we can learn how to build a deeplearning pipeline (seperate our model into different parts.) and combine them together.\n\n- It is a traditional Seq2Seq model, encoder is used to get the context vectors represented by the hidden and cell state generated by the last layer of LSTM. While decoder initialize its $h_0, c_0$ according to the encoder output. And each time-step $t$, generate the $t+1$ word in the target sentence. update the input according to the previous word and the teacher_force rate.\n\n- This is the first time I have ever seen the teacher_force rate. It is just a simple but useful way to restinct the changing of our model.\n\n### Train Part\n\n- Clip : as mentioned before, before we use optimizer.step(), we should use  torch.nn.utils.clip_grad_norm_(model.parameters(), clip) to avoiding gradients exploding ! Here we set clip=1. ","066288a6":"## Building the Seq2Seq Model\n\nWe will build our model in three parts: The encoder, the decoder, and a seq2seq model that encapsulates the encoder and decoder. ","e01087b5":"## Introduction\n\nIn this notebook, we will start simple model to understand the general concepts by implementing the model from the [Sequence to Sequence Learning with Neural Networks](https:\/\/arxiv.org\/abs\/1409.3215) paper.\n\nThe most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which (commonly) use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector (as an abstract representation of the entrie input sentence).\n\nThis vector is then decoded by a second RNN which learns to output the target(output) sentence by generating it one word at a time.\n\n![](https:\/\/github.com\/bentrevett\/pytorch-seq2seq\/raw\/3a8dc5515ff28cb059532439c5687126dd30015f\/assets\/seq2seq1.png)\nAbove image shows an example translation. The input sentence \"guten morgen\", is input into the encoder (green) one word at a time. We also append a start of sequence(<sos\\>) and end of sequence(<eos\\>) token to the start and end of sentence, respectively. At each time-step, the input to the encoder RNN is both the current word $x_t$ as well as the hidden state from the previous time-step $h_{t-1}$ You can think of the hidden state as a vector representation of the sentence so far. The RNN can be represented as a function of both $x_t$ and $h_{t-1}$ :\n$$\nh_t = EncoderRNN(x_t, h_{t-1}) \\tag{1}\n$$\nHere, we have $X={x_1, x_2, \\cdots, x_T}$ where $x_1$ = <sos\\> $x_2$ = guten, etc. The initial hidden states $h_0$ is usually either initialized to zeros or a learned parameter.\n\nOnce the final word $x_T$ has been passed into RNN, we use the final hidden state $h_T$ as the context vector i.e. $h_T = z$.\n \nWith our context vector $z$, we can start decoding it to get the target sentence, \"good morning\". Again we append start and end of sequence tokens to the target sentence. At each time-step, the input to the decoder RNN (blue) is the current word, $y_t$, as well as the hidden state from the previous time-step $s_{t-1}$, where the initail decoder hidden state $s_0 = z = h_T$ i.e. the initial hidden state is the final encoder hidden state. similar to the encoder, we can represent the decoder as:\n$$\ns_t = DecoderRNN(y_t, s_{t-1}) \\tag{2}\n$$\nIn the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a Linear layer, shown in purple) what we think is the next word in the sequence $\\hat{y}_t$.\n$$\n\\hat{y}_t = f(s_t) \\tag{3}\n$$\nThe word in the encoder are always generated one after another, with one per time-step. We always use the <sos\\> for the first input to the decoder $y_1$, but for subsequent inputs $y_{t > 1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder $\\hat{y}_{t-1}$. This is called teacher forcing.\n\nWhen training\/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference (i.e. real world usage) it is common to keep generating words until the model outputs an <eos\\> token or after a certain amount of words have been generated.\n\nOnce we get our prediction $\\hat{Y} = {\\hat{y_1},\\hat{y_2},\\cdots, \\hat{y_T}}$, we compare it against our actual target sentence $Y = {y_1, y_2, \\cdots y_T}$, to calculate our loss. We then use this loss to update all of the parameters in our model.","88fe741c":"## Preparing Data","af5feee7":"### Seq2Seq\nFor the final part of the implementation, we will implement the seq2seq model.\n\n- receive the input\/source sentence\n\n- using the encoder to produce the context vectors\n\n- using the decoder to produce the predicted output \/ target sentence.\n\nOur full model will look like this:\n\n![](https:\/\/github.com\/bentrevett\/pytorch-seq2seq\/raw\/3a8dc5515ff28cb059532439c5687126dd30015f\/assets\/seq2seq4.png)","30dacfa5":"## Training the Seq2Seq model\n"}}