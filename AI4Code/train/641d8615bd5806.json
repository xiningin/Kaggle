{"cell_type":{"9b779ff0":"code","7dba86fa":"code","8ffe931c":"code","7a4a394c":"code","7cb51e73":"code","6be8d2e6":"code","18ab3773":"code","e7c8b080":"code","c0aa3199":"code","834308ae":"code","75936103":"code","4dd95fe2":"code","c629d501":"markdown","9c4f1089":"markdown","3273b4a1":"markdown","b1c49148":"markdown","26a061a9":"markdown","7ed52c26":"markdown","27675185":"markdown","88b5c5f1":"markdown","bd90e408":"markdown","504aed52":"markdown","e32d287e":"markdown","36e89314":"markdown","975f2187":"markdown","1bbbe6a6":"markdown","1b303542":"markdown","3e02d2df":"markdown"},"source":{"9b779ff0":"import pandas as pd # data processing, uploading csv and working with dataframe\n\n#graph plotting library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#splitting the dataset\nfrom sklearn.model_selection import StratifiedKFold\n\n#Over Sampling and under sampling libraries\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n#Machine learning Pipeline libraries\nfrom imblearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\");","7dba86fa":"#This dataset is a pre-processed dataset of the original creditcard fraud detection.\n#In this dataset the amount and time column have already been scaled using sklearn.preprocessing.RobustScaler() method.\n#The scaled_amount and scaled_time columns are scaled version of Amount and Time columns of the original dataset respectively.\n\ndf = pd.read_csv(\"..\/input\/scaled-credit-card-fraud-detection\/scaled_creditcard.csv\")\ndf.head()","8ffe931c":"#printing the percentage of samples of the majority and minority classes.\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100 , 2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100 , 2), '% of the dataset')","7a4a394c":"#Graphical distribution of the classes\ndf['Class'].value_counts().plot(kind='bar');","7cb51e73":"#First Method of solving this problem which comes in mind will be by \n#taking same number of records of the majority class as the minority class\n\n# Taking all the records i.e. 492 of fraud classes(minority class).\nfraud_df = df.loc[df['Class'] == 1]\n\n# Taking the same number of records of the majority class(No Frauds)\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\n#concating the above dataframes to get a single dataframe\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)","6be8d2e6":"#Now we have exactly 50-50 percent rows of both majority and minority classes\n\nprint('No Frauds', round(new_df['Class'].value_counts()[0]\/len(new_df) * 100 , 2), '% of the dataset')\nprint('Frauds', round(new_df['Class'].value_counts()[1]\/len(new_df) * 100 , 2), '% of the dataset')","18ab3773":"# Make sure we use the 50-50 percent sample rows in our correlation, as it is balanced dataset\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(20,20))\n\n# correlation using Original imbalanced dataframe\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r',annot=True, annot_kws={'size':5}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n# correlation using new balanced dataframe\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","e7c8b080":"# Preparing data for ML algo by seperating target and the features.\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# Our data is already scaled and balanced\n# Splitting the data into training and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Let's implement simple classifiers\n\n# creating a dictionary of ML objects\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}\n\n# appying cross validation\nfrom sklearn.model_selection import cross_val_score\n\n#iterating thru each of the algos in the \"classifiers\" dictionary\nfor key, classifier in classifiers.items():\n    \n    #Splitting the data into 5 parts using cv=5 parameter of cross_val_score method\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    \n    #The above line of code will return 5 accuracy scores\n    #printing the mean of the accuracy metrics\n    print(\"Classifiers: \", classifier.__class__.__name__, \n          \"Has a training score of\", round(training_score.mean() * 100, 2) , \"% accuracy score\")","c0aa3199":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n\n# logistic regression best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\n#--------------------------\n\n# KNeighborsClassifier\nknears_params = {\"n_neighbors\": list(range(2,5,1))}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n#--------------------------\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n#--------------------------\n\n# DecisionTree Classifier\ntree_params = {\"max_depth\": list(range(2,4,1)), \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n\nprint(\"Classifers with their tuned parameters we got via GridSearchCV\")\nprint(log_reg)\nprint(knears_neighbors)\nprint(svc)\nprint(tree_clf)\n","834308ae":"#dictionary of classifiers objects which we got in the above cell via GridSearchCV\n#with the tuned parameters\ntuned_classifiers = {\n    \"LogisiticRegression\": log_reg,\n    \"Knears Neighbors\": knears_neighbors,\n    \"Support Vector Classifier\": svc,\n    \"DecisionTreeClassifier\": tree_clf\n}\n\n#Making our Classifiers train with the tuned parameters\nfor key, classifier in tuned_classifiers.items():\n    score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(f'{key} Cross Validation Score: {round(score.mean() * 100, 2)}%')","75936103":"# here we are using the imbalanced dataset \nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# StratifiedKFold is used for cross validation\n# This cross-validation object is a variation of KFold that returns stratified folds.\n# The folds are made by preserving the percentage of samples for each class.\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n# creating the object for Over Sampling the minority class\nover = SMOTE(sampling_strategy=0.01,k_neighbors=5)\n\n# creating the object for Under Sampling the majority class\nunder = RandomUnderSampler(sampling_strategy=0.5)\n\n# iterating thru the dictionary of tuned classifiers\nfor key, classifier in tuned_classifiers.items():\n    \n    # list of steps to be provided to the ML Pipeline\n    steps=[('o',over),('u',under),('model',classifier)]\n    \n    # Creating a ML Pipeline\n    FiPipeline=Pipeline(steps=steps)\n    \n    # Cross validating the classifiers\n    scores=cross_val_score(FiPipeline,X,y,cv=sss)\n    \n    # Printing the mean accuracy score\n    print(f\"Classifiers: {key} Has a training score of, {round(scores.mean() * 100, 2)} % accuracy score\")\n","4dd95fe2":"# taking a fold of a data to do GridSearchCV for best parameters for SMOTE\nfor train_index, test_index in sss.split(X, y):\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# creating a ML pipeline\nmodel = Pipeline([\n        ('sampling', SMOTE(sampling_strategy=0.01)),\n        ('Random',RandomUnderSampler(sampling_strategy=0.5)),\n        ('classification', log_reg)\n    ])\n\n# Running GridSearchCV on our ML pipeline by varying the k_neighbors from 1 to 10\n# to find the best k_neighbors parameter value\n# NOTE :- in paramgrid use above key + 2 _ followed by parameter\nSMOTE_KN = GridSearchCV(model,{'sampling__k_neighbors':list(range(1,10))})\nSMOTE_KN.fit(original_Xtrain,original_ytrain)\n\nprint(SMOTE_KN.best_estimator_)\nprint(SMOTE_KN.best_score_)\nprint(SMOTE_KN.best_params_)\n\n# Training logistic regression classifier using SMOTE best parameter value for k_neighbors parameter\nlog_reg_score = cross_val_score(SMOTE_KN.best_estimator_, original_Xtrain, original_ytrain, cv=5)\n\n# printing the mean accuracy score\nprint(f'Logistic Regression Cross Validation Score: {round(log_reg_score.mean() * 100, 2)}%')","c629d501":"From the above heatmap graphs we can clearly see that the balance dataset has high correlation than the unbalanced dataset.","9c4f1089":"# Synthetic Minority Oversampling Technique (SMOTE)\n![SMOTE1.png](attachment:SMOTE1.png)\nIn the below cell we oversampling our \"Fraud\" sample using SMOTE and we are under sampling the \"No Fraud\" samples.\nThe library imblearn has a over_sampling class of which we will using SMOTE() object to do over sampling.\n\nSMOTE() has two hyperparameters:\n\n* *k_neighbors=5* : k_neighbors determine what nearest vectors to use to generate new data. in this example we are considering 5 neighbors.\n* *sampling_strategy=0.01* : determines how many new data points to generate on those vectors to achieve a certain ratio. Here 0.01 means 1 percent of the majority class.\n\nWe are using RandomUnderSampler() object of imblearn.under_sampling class to under sample our majority class such that the manority class sample will be 50% of the majority class.","3273b4a1":"# Tuning SMOTE","b1c49148":"After doing SMOTE and undersampling the accuracy of our models has gone up drastically by 4-5%.","26a061a9":"*Support Vector Classifier* and *Logistic Regression* are giving better results compare to *K-Nearest Neighbour Classifier*.\n*Decision Tree Classifier* has been ineffiecient as this problem case is for binary classification.","7ed52c26":"# Summary\nIn this notebook, you learned how to handel imbalanced dataset in machine learning. We can improve our models on imbalanced dataset by over sampling the minority class using SMOTE.\n\nI am thankful to Rocky Jagtiani in mentoring me to complete this assignment. He is more of a friend to me then a mentor and have been influential in getting deep into machine learning.\n\nRecommended https:\/\/datascience.suvenconsultants.com\/","27675185":"# Simple Approcah of under sampling","88b5c5f1":"From the above output we can clearly see that the classes are heavily skewed. The data consists of 99.83% of records from the majority class i.e. No Frauds. We need to solve this only then would we use our data for Machine learning modeling.\n","bd90e408":"# Introduction\nThe challenge of working with imbalanced datasets is that the performance of the machine learning on the minority class is most important. But most of the machine learning techniques will ignore, and in turn have poor performance on the minority class. As in case of our sample dataset of credit card fraud, 99.83% of the samples are of majority class. After we train our model on this data, and while predicting if the model classifies all our test data as a majority class it will end up having an accuracy of more then 90%. But in real world this will be disastrous.\n\n\n> ### How to address imbalanced dataset problem\n> 1. The simple approach that we can think of under sampling the majority class i.e. just take random samples of the majority class equivalent to the number of samples of the minority class. This approach will definitely give better results then the imbalanced dataset, but there will be lot of information lost and the majority class will not look majority for our machine learning algorithm.\n> \n> 2. In this second approach we can oversample the minority class by simply duplicating the minority class samples to make the dataset balanced. Doing this will not add any new information to our model and hence we can conclude that its not the best model.\n> \n> 3. The best approach will be to synthetically create minority samples from the existing samples. This approach is known as Synthetic Minority Oversampling Technique, or SMOTE for short. In brief this technique considers the k_neighbors samples and creates a synthetic samples.\n\nIn this notebook, Myself [Parvez Ahmed Shaikh](http:\/\/) and my team member [Amar Sharma](https:\/\/www.kaggle.com\/amarsharma768), under mentor-ship of [Rocky Jagtiani](https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/) at [Suven](https:\/\/datascience.suvenconsultants.com\/) have tried to address the imbalanced classification datasets problem using SMOTE for oversampling.\n\nWe have completed many such assignment under \"Master in Data Science Programme\" at [Suven](https:\/\/datascience.suvenconsultants.com\/)","504aed52":"![](http:\/\/)![SMOTE.jpg](attachment:SMOTE.jpg)","e32d287e":"# Loding data","36e89314":"Applying tuned parameters has slightly increased the accuracy score of our models.","975f2187":"# Importing libraries","1bbbe6a6":"In the cell below we are using the classifiers we got via GridSearchCV to get the accuracy score.","1b303542":"Let's do some Machine learning on our 50%-50% balanced dataset.\nIn the below section we will train our data on\n* *Logistic Regression*\n* *K-Nearest Neighbour Classifier*\n* *Support Vector Classifier*\n* *Decision Tree Classifier*","3e02d2df":"# Tuning Model Parameters\n\nIn the section below we will be using GridSearchCV to find the best parameter for our classifiers. The GridSearchCV iterates thru each combination of parameters and returns best parameters for the model in the best_estimator_ object"}}