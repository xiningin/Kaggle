{"cell_type":{"d66188cd":"code","e0b34c46":"code","458967fd":"code","bf12a985":"code","01ca749b":"code","f6b9903e":"code","d1678d33":"code","a7ab51da":"code","bd3dac83":"code","b892bbaa":"code","24e97c47":"code","5b1aac77":"code","06a78e50":"code","d1ce94a3":"code","a2bd8b20":"code","f16d35a6":"code","6c899835":"code","4a1d70b2":"code","8a0f8ff6":"code","03db6f86":"code","c8ec1442":"code","b747c963":"code","3b140b6d":"code","04c1d0bf":"code","a9343dcf":"code","04f2cad8":"code","e847384f":"code","ee4ce8e1":"code","2e9e4aa5":"code","d23b6680":"code","761348e1":"code","6beb0bf4":"code","08d1af30":"code","05191c57":"code","e29bc00a":"code","92d44026":"code","eadb237f":"code","77971955":"code","f159c882":"code","fa63ae2f":"code","7de894b5":"code","9a42ed01":"code","c7ce0a17":"code","098ce985":"code","444d17fe":"code","670936b2":"code","d46c48ff":"code","ec01edb5":"markdown","acaf2c5e":"markdown","1ffe5be3":"markdown","9efbed46":"markdown","c22a8548":"markdown","74f74f56":"markdown"},"source":{"d66188cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e0b34c46":"#Importing Libraries\n#Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","458967fd":"#import train and test CSV files\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nprint(train.columns.values)","bf12a985":"train.head()","01ca749b":"test.head()","f6b9903e":"#see a summary of the training dataset\ntrain.describe(include = \"all\")","d1678d33":"#check for any null values in train data\nprint(pd.isnull(train).sum())","a7ab51da":"#check for any null values in test data\nprint(pd.isnull(test).sum())","bd3dac83":"#Finding data type of each column\ntrain.info()","b892bbaa":"#Draw a bar plot of survival by Pclass\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#Print percentage of people by Pclass that survived\nfor i in range(0,3):\n    print(\"Percentage of Pclass = \",i+1,\" who survived:\", train[\"Survived\"][train[\"Pclass\"] == i+1].value_counts(normalize = True)[1]*100)","24e97c47":"#Draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#Print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","5b1aac77":"#Draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\ntrain[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","06a78e50":"#Draw a bar plot for Parch vs. survival\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train)\n\ntrain[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d1ce94a3":"sns.set_style(\"whitegrid\")\ngrid = sns.FacetGrid(train, col='Survived',size=2.8, aspect=1.5)\ngrid.map(plt.hist, 'Age', bins=20)\ngrid.add_legend()\nplt.show()","a2bd8b20":"grid = sns.FacetGrid(train, row='Embarked', size=2.5, aspect=1.5)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()\nplt.show()","f16d35a6":"freq_port = train.Embarked.dropna().mode()[0]\nfreq_port","6c899835":"full_data = [train, test]\n\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4a1d70b2":"#Extract a title for each Name in the train and test datasets\nfor dataset in full_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","8a0f8ff6":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","03db6f86":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","c8ec1442":"#Drop the features which are not going to be used.\ntrain = train.drop(['Ticket', 'Cabin', 'Name'], axis = 1)\ntest = test.drop(['Ticket', 'Cabin', 'Name'], axis = 1)\nfull_data = [train, test]","b747c963":"#Map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in full_data:\n    \n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n\ntrain.head()","3b140b6d":"for dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain['AgeGroup'] = pd.cut(train['Age'], 5)\nprint(train[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean().sort_values(by='AgeGroup', ascending=True))\n\ntrain.head()","04c1d0bf":"for dataset in full_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain.head()","a9343dcf":"train = train.drop(['AgeGroup'], axis=1)\nfull_data = [train, test]\ntrain.head()","04f2cad8":"#Fill in missing Fare value in test since only test has a missing value\ntest['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)\n#Create FareBand\ntrain['FareBand'] = pd.qcut(train['Fare'], 4)\ntrain[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","e847384f":"for dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n#drop the FareBand\ntrain = train.drop(['FareBand'], axis=1)\nfull_data = [train, test]\n\ntrain.head()","ee4ce8e1":"#Map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\n\ntrain.head()","2e9e4aa5":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False))","d23b6680":"for dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean().sort_values(by='Survived', ascending=False))","761348e1":"train = train.drop(['Parch', 'SibSp', 'FamilySize',], axis=1)\ntest = test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\nfull_data = [train, test]\n\ntrain.head()","6beb0bf4":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, random_state = 101)","08d1af30":"# KNN or K Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_knn)","05191c57":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_logreg)","e29bc00a":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gaussian)","92d44026":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_svc)","eadb237f":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_test)\nacc_linear_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_linear_svc)","77971955":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_perceptron)","f159c882":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_test)\nacc_decisiontree = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_decisiontree)","fa63ae2f":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","7de894b5":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_test)\nacc_sgd = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_sgd)","9a42ed01":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","c7ce0a17":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","098ce985":"test_pred = svc.predict(test.drop(['PassengerId'], axis=1))","444d17fe":"test['Survived'] = test_pred","670936b2":"test.head()","d46c48ff":"submission = test[['PassengerId','Survived']]\nsubmission.to_csv(\"submission.csv\", index=False)","ec01edb5":"*Females have a much higher chance of survival than males. The Sex feature is essential in our predictions.*","acaf2c5e":"Based on data and data type, categorizing the columns.\n\n**Categorical** - Survived, Sex and Embarked\n\n**Ordinal** - Pclass\n\n**Numerical** - PassengerId, Age, SibSp, ParCh, Fare\n\n**Alphanumeric** - Ticket, Cabin\n\n**String** - Name\n","1ffe5be3":"Since SVM has highest accuracy (80.72%), lets use SVM to predict target for actual test data.","9efbed46":"I will be testing the following models with training data:\n\nKNN or k-Nearest Neighbors, Logistic Regression, Gaussian Naive Bayes, Support Vector Machines, Perceptron, \nDecision Tree Classifier, Random Forest Classifier, Stochastic Gradient Descent, Gradient Boosting Classifier\n\nEach model will be fit it with 75% of training data, predict for 25% of the training data and check the accuracy.","c22a8548":"*People with higher socioeconomic class had a higher rate of survival. (62.96% vs. 47.28% vs. 24.24%)*","74f74f56":"*In general, it is clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.54% vs 53.59% vs. 46.43%)*"}}