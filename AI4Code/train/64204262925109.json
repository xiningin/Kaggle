{"cell_type":{"2732fe8b":"code","47d290bc":"code","b80cf925":"code","a71a223a":"code","8d85d9a2":"code","c84f85af":"code","6248d25c":"code","63820ba5":"code","6252f590":"code","9c7745a0":"code","c443d1f3":"code","e1668066":"code","b53d6fb0":"code","8b6d9b0a":"code","85b471d5":"code","570e3d39":"code","3c119583":"code","05e84a69":"code","b1177a58":"code","0d0117f6":"code","7cf162e4":"markdown","142ab9e2":"markdown","5d86bbbf":"markdown","9f25d457":"markdown","358509f8":"markdown"},"source":{"2732fe8b":"import io\nimport json\nimport requests\nimport functools\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\n\npd.options.mode.chained_assignment = None\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.utils import data\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import functional as F\nfrom torchvision import datasets, models, transforms\nfrom queue import Queue","47d290bc":"EPOCHS        = 5\nDROPOUT       = 0.2\nDIRECTIONS    = 1\nNUM_LAYERS    = 2\nBATCH_SIZE    = 5\nOUTPUT_SIZE   = 1\nSEQ_LENGTH    = 30\n\nHIDDEN_SIZE   = 100 \nLEARNING_RATE = 0.001\nSHIFT_K       = 5  # feaute_1 ~ feaute_k\nSTATE_DIM     = NUM_LAYERS * DIRECTIONS, BATCH_SIZE, HIDDEN_SIZE\nTARGET        = \"Target\"\nFEATURES      = ['Close','High', 'Low', 'Open', 'VWAP', 'Volume']\n# add feaute_1 ~ feaute_k\nfor i in range(1, SHIFT_K+1):\n    FEATURES.append(f\"feature_{i}\")\nNUM_FEATURES  = len(FEATURES)","b80cf925":"def upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']","a71a223a":"# Start with 10k rows for testing\ndf_train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv', nrows=10000000) #### train data\n# df_train = pd.read_csv('..\/input\/my_data\/my_data.csv')\ndetails = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv') # asset_details\ndf_train.dropna(axis = 0, inplace = True) # dropna\ndf_train['Upper_Shadow'] = upper_shadow(df_train) # Upper_Shadow\ndf_train['Lower_Shadow'] = lower_shadow(df_train) # Lower_Shadow","8d85d9a2":"asset_df_dict = {}\n\n# split train data to 14 asset dataframe \nfor asset_id in range(14):\n    asset_df = df_train[df_train[\"Asset_ID\"]==asset_id][:10000].reset_index(drop=True) #### debug just 10000 row\n    # asset_df = df_train[df_train[\"Asset_ID\"]==asset_id].reset_index(drop=True)\n    asset_df_dict[asset_id] = asset_df\nlen(asset_df_dict.keys())","c84f85af":"# get feaute_1 ~ feaute_k\ndef feature_k(df, k=SHIFT_K, target_variable=\"Target\"):\n    for i in range(1, k+1):\n        df[f\"feature_{i}\"] = df[target_variable].shift(i)\n    return df\n\ntarget_dict = {}\nfor asset_id in range(14):\n    # save target of last row \n    asset_df = feature_k(asset_df_dict[asset_id]).fillna(0)\n    asset_df_dict[asset_id] = asset_df\n    target_dict[asset_id] = Queue(maxsize=SHIFT_K+1) # queue\n    last_row = asset_df.iloc[-1]\n    for k in range(SHIFT_K-1, 0, -1):\n        target_dict[asset_id].put(last_row[f\"feature_{k}\"])\n    target_dict[asset_id].put(last_row[\"Target\"])    ","6248d25c":"training_data_list = []\nvalidation_data_list = []\n\n# train_test_split for 14 asset\nfor i in range(14):\n    training_data, validation_data = train_test_split(asset_df_dict[i], test_size=0.2, shuffle=False)\n    training_data_list.append(training_data)\n    validation_data_list.append(validation_data)","63820ba5":"class CryptoDataset(Dataset):\n    \"\"\"Onchain dataset.\"\"\"\n\n    def __init__(self, csv_file, seq_length, features, target):\n        \"\"\"\n        Args:\n        \"\"\"\n        self.csv_file = csv_file\n        self.target = target\n        self.features = features\n        self.seq_length = seq_length\n        self.data_length = len(csv_file)\n\n        self.metrics = self.create_xy_pairs()\n\n    def create_xy_pairs(self):\n        pairs = []\n        for idx in range(self.data_length - self.seq_length):\n            x = self.csv_file[idx:idx + self.seq_length][self.features].values\n            y = self.csv_file[idx + self.seq_length:idx + self.seq_length + 1][self.target].values\n            pairs.append((x, y))\n        return pairs\n\n    def __len__(self):\n        return len(self.metrics)\n\n    def __getitem__(self, idx):\n        return self.metrics[idx]","6252f590":"params = {'batch_size': BATCH_SIZE,\n          'shuffle': False,\n          'drop_last': True, # Disregard last incomplete batch\n          'num_workers': 2}\n\nparams_test = {'batch_size': 1,\n          'shuffle': False,\n          'drop_last': False, # Disregard last incomplete batch\n          'num_workers': 2}\n\n#  datasets and dataloader for 14 asset\ntraining_ds_list = [CryptoDataset(training_data, SEQ_LENGTH, FEATURES, TARGET) for training_data in training_data_list]\ntraining_dl_list = [DataLoader(training_ds, **params) for training_ds in training_ds_list]\n\nvalidation_ds_list = [CryptoDataset(validation_data, SEQ_LENGTH, FEATURES, TARGET) for validation_data in validation_data_list]\nvalidation_dl_list = [DataLoader(validation_ds, **params_test) for validation_ds in validation_ds_list]","9c7745a0":"# Transfer to accelerator\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nnp.random.seed(0)\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob, directions=1):\n        super(LSTM, self).__init__()\n\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.directions = directions\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def init_hidden_states(self, batch_size):\n        state_dim = (self.num_layers * self.directions, batch_size, self.hidden_size)\n        return (torch.zeros(state_dim).to(device), torch.zeros(state_dim).to(device))\n\n    def forward(self, x, states):\n        x, (h, c) = self.lstm(x, states)\n        out = self.linear(x)\n        return out, (h, c)","c443d1f3":"# 14 models\nmodels_list = [LSTM(NUM_FEATURES,HIDDEN_SIZE,NUM_LAYERS,OUTPUT_SIZE,DROPOUT).to(device) for _ in range(14)]\n# 14 criterion\ncriterion_list = [nn.MSELoss() for _ in range(14)]\n# 14 optimizer\noptimizer_list = [optim.AdamW(model.linear.parameters(), lr=LEARNING_RATE, weight_decay=0.01) for model in models_list]","e1668066":"def save_checkpoint(epoch, min_val_loss, model_state, opt_state, asset_id):\n    print(f\"New minimum reached at epoch #{epoch + 1}, saving model state...\")\n    checkpoint = {\n    'epoch': epoch + 1,\n    'min_val_loss': min_val_loss,\n    'model_state': model_state,\n    'opt_state': opt_state,\n    }\n    torch.save(checkpoint, f\".\/model_state_{asset_id}.pt\")\n\n\ndef load_checkpoint(path, model, optimizer):\n    # load check point\n    checkpoint = torch.load(path)\n    min_val_loss = checkpoint[\"min_val_loss\"]\n    model.load_state_dict(checkpoint[\"model_state\"])\n    optimizer.load_state_dict(checkpoint[\"opt_state\"])\n    return model, optimizer, checkpoint[\"epoch\"], min_val_loss\n\n\ndef training(asset_id, model, criterion, optimizer, epochs, validate_every=2):\n\n    training_losses = []\n    validation_losses = []\n    min_validation_loss = np.Inf\n\n    # Set to train mode\n    model.train()\n\n    for epoch in tqdm(range(epochs)):\n\n        # Initialize hidden and cell states with dimension:\n        # (num_layers * num_directions, batch, hidden_size)\n        states = model.init_hidden_states(BATCH_SIZE)\n        running_training_loss = 0.0\n\n        # Begin training\n        for idx, (x_batch, y_batch) in enumerate(training_dl_list[asset_id]):\n            # Convert to Tensors\n            x_batch = x_batch.float().to(device)\n            y_batch = y_batch.float().to(device)\n\n            # Truncated Backpropagation\n            states = [state.detach() for state in states]          \n\n            optimizer.zero_grad()\n\n            # Make prediction\n            output, states = model(x_batch, states)\n\n            # Calculate loss\n            loss = criterion(output[:, -1, :], y_batch)\n            loss.backward()\n            running_training_loss += loss.item()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n\n        # Average loss across timesteps\n        training_losses.append(running_training_loss \/ len(training_dl_list[asset_id]))\n\n        if epoch % validate_every == 0:\n\n            # Set to eval mode\n            model.eval()\n\n            validation_states = model.init_hidden_states(BATCH_SIZE)\n            running_validation_loss = 0.0\n\n            for idx, (x_batch, y_batch) in enumerate(validation_dl_list[asset_id]):\n\n                # Convert to Tensors\n                x_batch = x_batch.float().to(device)\n                y_batch = y_batch.float().to(device)\n\n                validation_states = [state.detach() for state in validation_states]\n                output, validation_states = model(x_batch, validation_states)\n                validation_loss = criterion(output[:, -1, :], y_batch)\n                running_validation_loss += validation_loss.item()\n\n        validation_losses.append(running_validation_loss \/ len(validation_dl_list[asset_id]))\n        # Reset to training mode\n        model.train()\n\n        is_best = running_validation_loss \/ len(validation_dl_list[asset_id]) < min_validation_loss\n\n        if is_best:\n            min_validation_loss = running_validation_loss \/ len(validation_dl_list[asset_id])\n            save_checkpoint(epoch + 1, min_validation_loss, model.state_dict(), optimizer.state_dict(), asset_id)\n\n\n    # Visualize loss\n    epoch_count = range(1, len(training_losses) + 1)\n    plt.plot(epoch_count, training_losses, 'r--')\n    plt.legend(['Training Loss'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()\n\n    val_epoch_count = range(1, len(validation_losses) + 1)\n    plt.plot(val_epoch_count, validation_losses, 'b--')\n    plt.legend(['Validation loss'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()","b53d6fb0":"# Train\nfor asset_id, model in enumerate(models_list):\n    print(f\"Training asset {asset_id}\")\n    training(asset_id, model, criterion_list[asset_id], optimizer_list[asset_id], EPOCHS)","8b6d9b0a":"# load checkpoint\nsaved_model_list = []\nfor asset_id, model in enumerate(models_list):\n    print(f\"load_checkpoint asset {asset_id}\")\n    model, optimizer, start_epoch, valid_loss_min = load_checkpoint(f\".\/model_state_{asset_id}.pt\", model, optimizer_list[asset_id])\n    saved_model_list.append(model)\n    # print(\"model = \", model)\n    # print(\"optimizer = \", optimizer)\n    print(\"valid_loss_min = \", valid_loss_min)\n    print(\"valid_loss_min = {:.6f}\".format(valid_loss_min))","85b471d5":"import gresearch_crypto\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()","570e3d39":"# get test data feature_1 ~ feature_k\ndef test_feature_k(df, k=SHIFT_K, target_variable=\"Target\"):\n    new_df = []\n    for row in df.iterrows():\n        row = row[1]\n        row_asset_id = int(row.Asset_ID)\n        asset_df = asset_df_dict[row_asset_id]\n        target_q = target_dict[row_asset_id]\n        for i in range(k):\n            row[f\"feature_{k-i}\"] = target_q.queue[i]\n        new_df.append(row)\n    return pd.DataFrame(new_df).astype({'timestamp': 'int64', 'Asset_ID':'int8', 'Count':'int32', 'row_id':'int32'})","3c119583":"# (test_df, sample_prediction_df) = next(iter_test)\n# test_df['Upper_Shadow'] = upper_shadow(test_df)\n# test_df['Lower_Shadow'] = lower_shadow(test_df)\n# test_df\n# test_df = test_feature_k(test_df)\n# test_df","05e84a69":"# test_asset_id_list = test_df.Asset_ID.to_list()\n# selected_features = test_df[FEATURES]\n# x = torch.Tensor(selected_features.values)","b1177a58":"# idx = 1\n# asset_id = 3\n\n# x = x[idx].unsqueeze(0)\n# x = x.float().to(device)\n# x = x.view(1, -1, NUM_FEATURES)\n# model = saved_model_list[asset_id]\n# model.eval()\n# validation_states = model.init_hidden_states(1)\n# validation_states = [state.detach() for state in validation_states]\n# output, _ = model(x, validation_states)\n# pred = output[:, -1, :].item()","0d0117f6":"for (test_df, sample_prediction_df) in iter_test:\n    pred_list = []\n    test_df['Upper_Shadow'] = upper_shadow(test_df) # test Upper_Shadow\n    test_df['Lower_Shadow'] = lower_shadow(test_df) # test Lower_Shadow\n    \n    test_df = test_feature_k(test_df) # get test data feature_1 ~ feature_k\n    test_asset_id_list = test_df.Asset_ID.to_list() # get asset_id_list\n\n    selected_features = test_df[FEATURES]\n    x_values = torch.Tensor(selected_features.values)\n    for idx, asset_id in enumerate(test_asset_id_list):\n        x = x_values[idx].unsqueeze(0)\n        x = x.float().to(device)\n        x = x.view(1, -1, NUM_FEATURES)\n        model = saved_model_list[asset_id]\n        model.eval()\n        validation_states = model.init_hidden_states(1)\n        validation_states = [state.detach() for state in validation_states]\n        output, _ = model(x, validation_states)\n        pred = output[:, -1, :].item()\n        target_dict[asset_id].get()\n        target_dict[asset_id].put(pred)\n        pred_list.append(pred)\n    sample_prediction_df['Target'] = pred_list\n    env.predict(sample_prediction_df)","7cf162e4":"### Submission","142ab9e2":"## Training","5d86bbbf":"## Model Settings","9f25d457":"## Load Data","358509f8":"## Dataset"}}