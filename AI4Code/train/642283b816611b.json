{"cell_type":{"96135a64":"code","c0793dc3":"code","24b495b2":"code","cd5258b3":"code","4bb21372":"code","6453024c":"code","4612a35e":"code","a678d114":"code","ceb464ab":"code","c1859375":"code","8913f107":"code","945d731f":"code","e7e00578":"code","ccf8534a":"code","88086c4a":"code","040bc4f7":"code","845ab9b5":"code","2842dedb":"code","e2cef0fc":"code","09ebe67e":"markdown"},"source":{"96135a64":"!pip install pyvi\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom tqdm import tqdm\ntqdm.pandas(desc='Progress')\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport random\nimport torch\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom torch.optim.optimizer import Optimizer\nimport time","c0793dc3":"SEED = 2019\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","24b495b2":"class DataSource(object):\n    def _load_raw_data(self,filename, is_train=True):\n        a = []\n        b = []\n        regex = 'train_'\n        if not is_train:\n            regex = 'test_'\n        with open(filename, 'r') as file:\n            for line in file :\n                if regex in line:\n                    b.append(a)\n                    a = [line]\n                elif line!='\\n':\n                    a.append(line)\n        b.append(a)      \n        return b[1:]\n    \n    \n    def _create_row(self, sample, is_train=True):\n        d = {}\n        d['id'] = sample[0].replace('\\n','')\n        review = \"\"\n        if is_train:\n            for clause in sample[1:-1]:\n                review+= clause.replace('\\n','').strip()\n            d['label'] = int(sample[-1].replace('\\n',''))          \n        else:         \n            for clause in sample[1:]:\n                review+= clause.replace('\\n','').strip()\n        d['review'] = review\n        return d\n    \n    def load_data(self, filename, is_train=True):\n        raw_data = self._load_raw_data(filename, is_train)\n        lst = []\n        for row in raw_data:\n            lst.append(self._create_row(row, is_train))\n        return lst\n\nds = DataSource()\ntrain_df = pd.DataFrame(ds.load_data('..\/input\/aivivn-demo\/sa_demo\/train.crash')).set_index(\"id\")\ntest_df = pd.DataFrame(ds.load_data('..\/input\/aivivn-demo\/sa_demo\/test.crash', is_train=False)).set_index(\"id\")","cd5258b3":"train_df['length'] = train_df.review.apply(lambda x: len(x.split()))","4bb21372":"_,_,_ = plt.hist(train_df.length, bins=50)","6453024c":"def load_fasttext(word_index, max_features):    \n    EMBEDDING_FILE = '..\/input\/compressed-vi\/elmo_aivivn_train_test_wikn_sen.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    \n    cover = 0\n    oov = []\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            cover += 1\n            embedding_matrix[i] = embedding_vector\n        else:\n            oov.append(word)\n    print(\"Coverage = {:.4f}%\".format(cover\/len(word_index)*100))\n    return embedding_matrix, oov","4612a35e":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\ndef clean_text(x):\n    x = str(x)\n    x = x.replace(\"\\xa0\",\" \")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x","a678d114":"from pyvi import ViTokenizer, ViPosTagger\n","ceb464ab":"def load_and_prec(maxlen):\n    train_df[\"review\"] = train_df[\"review\"].apply(lambda x: ViTokenizer.tokenize(x))\n    test_df[\"review\"] = test_df[\"review\"].apply(lambda x: ViTokenizer.tokenize(x))\n    # lower\n    train_df[\"review\"] = train_df[\"review\"].apply(lambda x: x.lower())\n    test_df[\"review\"] = test_df[\"review\"].apply(lambda x: x.lower())\n\n    # Clean the text\n    train_df[\"review\"] = train_df[\"review\"].progress_apply(lambda x: clean_text(x))\n    test_df[\"review\"] = test_df[\"review\"].apply(lambda x: clean_text(x))\n    \n    \n    ## fill up the missing values\n    train_X = train_df[\"review\"].fillna(\"_##_\").values\n    test_X = test_df[\"review\"].fillna(\"_##_\").values\n\n    tokenizer = Tokenizer(filters='\\t\\n',lower=True)\n    tokenizer.fit_on_texts(list(train_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    ## Pad the sentences \n    train_X = pad_sequences(train_X, maxlen=maxlen)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    ## Get the target values\n    train_y = train_df['label'].values\n    \n    \n    #shuffling the data\n    np.random.seed(SEED)\n    trn_idx = np.random.permutation(len(train_X))\n\n    train_X = train_X[trn_idx]\n    train_y = train_y[trn_idx]\n    \n    return train_X, test_X, train_y, tokenizer.word_index","c1859375":"maxlen = 100\nx_train, x_test, y_train, word_index = load_and_prec(maxlen=maxlen) ","8913f107":"max_features = len(word_index)\nembed_size = 1024\nn_splits = 5\nbatch_size = 512\nn_epochs = 4\n\nembedding_matrix,oov = load_fasttext(word_index=word_index,max_features=max_features)","945d731f":"oov","e7e00578":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","ccf8534a":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","88086c4a":"def model_lstm_atten(embedding_matrix):\n    \n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.1)(x)\n    x = Bidirectional(CuDNNLSTM(40, return_sequences=True))(x)\n    y = Bidirectional(CuDNNGRU(40, return_sequences=True))(x)\n    \n    atten_1 = Attention(maxlen)(x) # skip connect\n    atten_2 = Attention(maxlen)(y)\n    avg_pool = GlobalAveragePooling1D()(y)\n    max_pool = GlobalMaxPooling1D()(y)\n    \n    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n    conc = Dense(16, activation=\"relu\")(conc)\n    conc = Dropout(0.1)(conc)\n    outp = Dense(1, activation=\"sigmoid\")(conc)    \n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[\"accuracy\"])\n    \n    return model\n\ndef train_pred(model,train_X, train_y, val_X, val_y, test_X, epochs=3):\n    model.fit(train_X, train_y, batch_size=512, epochs=epochs, validation_data=(val_X, val_y))\n    pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n    model.save(\"model\")\n    return pred_val_y, pred_test_y","040bc4f7":"splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED).split(x_train, y_train))\n\nvals = []\npreds_val = []\npreds_test = []\nfor i, (train_idx, val_idx) in enumerate(splits):    \n    train_X = x_train[train_idx]\n    train_y = y_train[train_idx]\n    val_X = x_train[val_idx]\n    val_y = y_train[val_idx]\n    test_X = x_test\n    pred_val_y, pred_test_y = train_pred(model_lstm_atten(embedding_matrix),train_X, train_y, val_X, val_y, test_X, epochs =20)\n    \n    vals.append(val_y)\n    preds_val.append(pred_val_y)\n    preds_test.append(pred_test_y)","845ab9b5":"def best_threshold(y_true,y_preds):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n        tmp[1] = f1_score(y_true, np.array(y_preds)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n    return delta\n\ny_true = np.concatenate(vals)\ny_preds = np.concatenate(preds_val)\ndelta = best_threshold(y_true,y_preds)","2842dedb":"preds_test = np.mean(preds_test,axis=0)\npreds_test = (preds_test > delta).astype(np.int)\nsub_df = pd.read_csv(\"..\/input\/aivivn-demo\/sa_demo\/sample_submission.csv\").set_index(\"id\")\nsub_df.label = preds_test\nsub_df.to_csv(\"submission.csv\")\nsub_df.head()","e2cef0fc":"!ls -halt","09ebe67e":"# Fasttext + LSTM for AIviVN comments classification\n\nCh\u00e0o m\u1ecdi ng\u01b0\u1eddi, m\u00ecnh gi\u1edbi thi\u1ec7u m\u1ed9t h\u01b0\u1edbng ti\u1ebfp c\u1eadn n\u1eefa cho b\u00e0i to\u00e1n text classification c\u1ee7a AIviVN. C\u1ee5 th\u1ec3:\n\n- S\u1eed d\u1ee5ng LSTM + Fasttext embeddings cho ti\u1ebfng vi\u1ec7t. Fasttext th\u1ef1c s\u1ef1 l\u00e0 kh\u00f4ng t\u1ed1t l\u1eafm nh\u01b0ng m\u00ecnh kh\u00f4ng t\u00ecm \u0111\u01b0\u1ee3c b\u1ed9 n\u00e0o m\u1ef3 \u0103n li\u1ec1n m\u00e0 \u1ed5n h\u01a1n n\u00f3  c\u1ea3.\n\n- \u0110\u00e1nh gi\u00e1 k\u1ebft qu\u1ea3 b\u1eb1ng 5 fold CV, testing l\u1ea5y trung b\u00ecnh c\u1ee7a 5 - out of fold preditctions.\n\nNgo\u00e0i ra:\n\n- D\u1eef li\u1ec7u clean ch\u01b0a t\u1ed1t, kh\u00f4ng r\u00f5 v\u00f4 t\u00ecnh hay c\u1ed1 \u00fd nh\u01b0ng c\u00f3 r\u1ea5t nhi\u1ec1u \u00e2m ti\u1ebft d\u00ednh v\u1edbi nhau g\u00e2y ra oov.\n\n- D\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i c\u0169ng kh\u00e1 nh\u1ecf n\u00ean d\u00f9ng c\u00e1ch n\u00e0y ch\u01b0a ph\u1ea3i l\u00e0 t\u1ed1i \u01b0u, nh\u01b0ng n\u1ebfu v\u1edbi data th\u1ef1c l\u1edbn h\u01a1n 10-15 l\u1ea7n m\u00ecnh ngh\u0129 n\u00f3 s\u1ebd scale t\u1ed1t h\u01a1n."}}