{"cell_type":{"821db53e":"code","1f570425":"code","7602ea14":"code","e346c280":"code","2c4e561e":"code","4480f247":"code","11ba9f0d":"code","51d24094":"code","90fdf949":"code","53bba5bf":"code","41fad9b6":"code","8bfe91a0":"code","9a6365b1":"code","17e9d208":"code","6fb93bf9":"code","ec997999":"code","6661c27c":"code","04a26553":"code","0d218fd3":"code","d1894dee":"code","11600ff1":"code","55775f0a":"code","52ecfdbc":"code","a3325f09":"code","75e52500":"code","d3634b1e":"code","c65e751c":"code","bb768226":"code","e56c33f4":"code","34852d92":"markdown","70e49e5e":"markdown","e4e1c7c8":"markdown"},"source":{"821db53e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n#load data\ntitanic_df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_df_sm = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","1f570425":"titanic_df_train.head(3)","7602ea14":"print('\\n ## train data info ### \\n')\nprint(titanic_df_train.info())","e346c280":"titanic_df_train.isnull().sum()","2c4e561e":"titanic_df_train['Age'].fillna(titanic_df_train['Age'].mean(), inplace =True)\ntitanic_df_train['Cabin'].fillna('N', inplace =True)\ntitanic_df_train['Embarked'].fillna('N', inplace =True)\ntitanic_df_train['Fare'].fillna(titanic_df_train['Fare'].mean(), inplace=True)\nprint('data NUll counts', titanic_df_train.isnull().sum().sum())","4480f247":"print('distribution of Sex :\\n', titanic_df_train['Sex'].value_counts())\nprint('\\n distribution of Cabin :\\n', titanic_df_train['Cabin'].value_counts())\nprint('\\n distribution of Embarked:\\n', titanic_df_train['Embarked'].value_counts())","11ba9f0d":"# we only need first char of Cabin column's values\n\ntitanic_df_train['Cabin'] = titanic_df_train['Cabin'].str[:1]\nprint(titanic_df_train['Cabin'].head(3))\ntitanic_df_train['Cabin'].value_counts()\n","51d24094":"# I want to see Number of Survivors by sex\ntitanic_df_train.groupby(['Sex', 'Survived'])['Survived'].count()","90fdf949":"sns.barplot(x = 'Sex', y='Survived', data =  titanic_df_train)","53bba5bf":"sns.barplot(x='Pclass', y='Survived', hue='Sex', data = titanic_df_train)","41fad9b6":"# Declares a function that returns a delimiter according to the input age. Used in DataFrame application labda.\n\ndef get_category(age):\n    cat = ''\n    if age <= -1: cat = 'Unknown'\n    elif age <= 5: cat = 'Baby'\n    elif age <= 12: cat = 'Child'\n    elif age <= 18: cat = 'Teenager'\n    elif age <= 25: cat = 'Student'\n    elif age <= 35: cat = 'Young Adult'\n    elif age <= 60: cat = 'Adult'\n    else : cat = 'Elderly'\n    \n    return cat\n\n\nplt.figure(figsize=(10,6))\n\ngroup_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']\n\ntitanic_df_train['Age_cat'] = titanic_df_train['Age'].apply(lambda x: get_category(x))\nsns.barplot(x = 'Age_cat', y = 'Survived', hue='Sex', data = titanic_df_train, order = group_names)\n","8bfe91a0":"titanic_df_train.drop('Age_cat', axis =1, inplace = True)","9a6365b1":"from sklearn import preprocessing\n\ndef encode_features(datadf):\n    features =['Cabin', 'Sex','Embarked']\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(datadf[feature])\n        datadf[feature] = le.transform(datadf[feature])\n        \n    return datadf\n\ntitanic_df_train = encode_features(titanic_df_train)\ntitanic_df_train.head()","17e9d208":"titanic_df_train.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)","6fb93bf9":"from sklearn.model_selection  import train_test_split\nX_train_data = titanic_df_train.drop('Survived', axis=1)\ny_train_data = titanic_df_train['Survived']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_data, y_train_data, test_size=0.2, random_state=11\\\n                                                   ,stratify=y_train_data)","ec997999":"def get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    \n    roc_auc = roc_auc_score(y_test, pred_proba)\n    \n    print('confusion matrix')\n    print(confusion)\n    \n    print('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f}, F1: {3:.4f},\\\n    AUC: {4:.4f}\\n'.format(accuracy, precision, recall, f1, roc_auc))","6661c27c":"def precision_recall_curve_plot(y_test = None, pred_proba_c1 = None):\n    # threshold ndarray\uc640 \uc774threshold\uc5d0 \ub530\ub978 \uc815\ubc00\ub3c4, \uc7ac\ud604\uc728 ndarray\ucd94\ucd9c\n    precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1)\n    \n    plt.figure(figsize=(8, 6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label = 'precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary], label='recall')\n    \n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1), 2))\n    \n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend(); plt.grid()\n    plt.show()","04a26553":"\n# DecisionTree, RandomForest, LogisticRegression\ndt_clf = DecisionTreeClassifier(random_state =11)\nrf_clf = RandomForestClassifier(random_state =11)\nlr_clf = LogisticRegression()\n\n# DecisionTreeClassifier fit\/precict\/eval\nprint('DecisionTreeClassifier fit\/precict\/eval')\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\nget_clf_eval(y_test, dt_pred, dt_clf.predict_proba(X_test)[:,1])\nprecision_recall_curve_plot(y_test, dt_clf.predict_proba(X_test)[:,1])\n\n# RandomForest fit\/precict\/eval\nprint('RandomForest fit\/precict\/eval')\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nget_clf_eval(y_test, dt_pred, rf_clf.predict_proba(X_test)[:,1])\nprecision_recall_curve_plot(y_test, rf_clf.predict_proba(X_test)[:,1])\n\n# LogisticRegression fit\/precict\/eval\nprint('LogisticRegression fit\/precict\/eval')\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nget_clf_eval(y_test, lr_pred, lr_clf.predict_proba(X_test)[:, 1])\nprecision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1])","0d218fd3":"from sklearn.model_selection import KFold\n\ndef exec_kfold(clf, folds = 5):\n    kfold = KFold(n_splits = folds)\n    scores =[]\n    \n    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_train_data)):\n        \n        X_train, X_test = X_train_data.values[train_index], X_train_data.values[test_index]\n        y_train, y_test = y_train_data.values[train_index], y_train_data.values[test_index]\n        \n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        scores.append(accuracy)\n        print('cross validation {0} '.format(iter_count), end='')\n        get_clf_eval(y_test, predictions, clf.predict_proba(X_test)[:,1])\n        \n    \n    mean_score = np.mean(scores)\n    print('mean accuracy: {0:.4f}'.format(mean_score))\n    \nexec_kfold(dt_clf, folds = 5)","d1894dee":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(dt_clf, X_train_data, y_train_data, cv=5)\nfor iter_count, accuracy in enumerate(scores):\n    print('cross validation {0} accuracy: {1:.4f}'.format(iter_count, accuracy))\n    \nprint('DecisionTreeClassifier mean accuracy: {0:.4f}'.format(np.mean(scores)))","11600ff1":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf_clf, X_train_data, y_train_data, cv=5)\nfor iter_count, accuracy in enumerate(scores):\n    print('cross validation {0} accuracy: {1:.4f}'.format(iter_count, accuracy))\n    \nprint('RandomForestClassifier mean accuracy: {0:.4f}'.format(np.mean(scores)))","55775f0a":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(lr_clf, X_train_data, y_train_data, cv=5)\nfor iter_count, accuracy in enumerate(scores):\n    print('cross validation {0} accuracy: {1:.4f}'.format(iter_count, accuracy))\n    \nprint('LogisticRegression mean accuracy: {0:.4f}'.format(np.mean(scores)))","52ecfdbc":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'max_depth':[2,5,8,10],\n             'min_samples_split':[2,4,6],\n             'min_samples_leaf':[1,5,7],\n             'n_estimators': list(range(10,100,20))}\n\ngrid_rfclf = GridSearchCV(rf_clf, param_grid=parameters, scoring='accuracy',\n                         cv =5)\ngrid_rfclf.fit(X_train, y_train)\n\nprint('GridSearchCV optimal hypter parameters: ', grid_rfclf.best_params_)\nprint('GridSearchCV best accuracy: {0:.4f}'.format(grid_rfclf.best_score_))\n\nbest_rfclf = grid_rfclf.best_estimator_\n\nrfpredictions = best_rfclf.predict(X_test)\naccuracy = accuracy_score(y_test, rfpredictions)\nprint('test set RandomForestClassifier accuracy : {0:.4f}'.format(accuracy))","a3325f09":"# Null\ndef fillna(df):\n    df['Age'].fillna(df['Age'].mean(),inplace=True)\n    df['Cabin'].fillna('N',inplace=True)\n    df['Embarked'].fillna('N',inplace=True)\n    df['Fare'].fillna(0,inplace=True)\n    return df\n\n\ndef drop_features(df):\n    df.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n    return df\n\n\ndef format_features(df):\n    df['Cabin'] = df['Cabin'].str[:1]\n    features = ['Cabin','Sex','Embarked']\n    for feature in features:\n        le = LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\n\ndef transform_features(df):\n    df = fillna(df)\n    df = drop_features(df)\n    df = format_features(df)\n    return df\n","75e52500":"titanic_df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_df_sm = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","d3634b1e":"titanic_df_test = transform_features(titanic_df_test)\ntitanic_df_train = transform_features(titanic_df_train)","c65e751c":"rf_clf = RandomForestClassifier(\n    n_estimators=50,\n    max_depth=10,\n    min_samples_split=4,\n    min_samples_leaf=1\n)\n\nX_train = titanic_df_train.drop('Survived', axis=1)\ny_train = titanic_df_train['Survived']\n\nrf_clf.fit(X_train, y_train)\npredcitions_sub = rf_clf.predict(titanic_df_test)","bb768226":"titanic_df_sm.Survived = predcitions_sub","e56c33f4":"titanic_df_sm.to_csv('submission.csv', index=False)","34852d92":"This notebook is not yet complete\n\nI will make this notebook a textbook for the binary classification process. \n\nThank you, Mr. Kwon Chul-min, for your inspiration.\n","70e49e5e":"* Passengerid: Passenger data serial number\n* Survived: Survival, 0 = death, 1 = survival\n* Pclass: Class of cabin on ticket, 1 = first class, 2 = second class, 3 = third class\n* sex: occupant sex\n* name: passenger name\n* Age of passengers\n* sibsp: Number of siblings or spouses on board together\n* Parch: Number of parents or children on board together\n* ticket: ticket number\n* fare: Charges\n* cabin: cabin number\n* Embarked: Intermediate port C = Cherbourg, Q = Queenstown, S = Southampton","e4e1c7c8":"**Processing for NULL columns**"}}