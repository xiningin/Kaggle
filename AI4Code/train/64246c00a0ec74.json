{"cell_type":{"a462a2b6":"code","9dc39faf":"code","da6a53de":"code","c113f66c":"code","ea4767db":"code","fc4746a5":"code","f2561a0e":"code","0c68c554":"code","6fb33655":"code","d4de6d0a":"code","26189d46":"code","65474aa6":"code","9efddd98":"code","540465f5":"code","71b60421":"code","11ee35ce":"code","b1ff5840":"code","9692726a":"code","a546a343":"code","370ec726":"code","ec0de474":"code","e2feb043":"code","e263f346":"code","ea300910":"code","aac7b83a":"markdown","bb6fcbe4":"markdown","82a3d2ad":"markdown","d7cb9a5b":"markdown","1248c52f":"markdown","9ffa873d":"markdown","3add6be6":"markdown","93d64f47":"markdown","2ced8c0b":"markdown","f45351ef":"markdown","d4c2615d":"markdown","2679c049":"markdown","e866a5ba":"markdown","626983ff":"markdown","134b0e13":"markdown"},"source":{"a462a2b6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9dc39faf":"df = pd.read_csv(\"..\/input\/multilabel-solar-flare-dataset\/C_training.csv\")\ndf_test = pd.read_csv(\"..\/input\/multilabel-solar-flare-dataset\/C_testing.csv\")\n# M5_testing = pd.read_csv(\"..\/input\/multilabel-solar-flare-dataset\/M5_testing.csv\")\n# M5_training = pd.read_csv(\"..\/input\/multilabel-solar-flare-dataset\/M5_training.csv\")\n# M_testing = pd.read_csv(\"..\/input\/multilabel-solar-flare-dataset\/M_testing.csv\")\n# M_training = pd.read_csv(\"..\/input\/multilabel-solar-flare-dataset\/M_training.csv\")","da6a53de":"print(df.shape)\nprint(df_test.shape)\nprint(df['label'].describe())\nprint(df_test['label'].describe())","c113f66c":"display(df.head())\ndisplay(df_test.head())","ea4767db":"def encode(string):\n    if string==\"Negative\":\n        return -1\n    elif string=='Positive':\n        return +1","fc4746a5":"df = df.rename(columns = {'label' : 'target'})\ndf_test = df_test.rename(columns = {'label' : 'target'})\ntarget = df['target']\ntarget_test = df_test['target']\ndf.drop(['target','flare','timestamp','NOAA','HARP'], inplace = True, axis =1)\ndf_test.drop(['target','flare','timestamp','NOAA','HARP'], inplace = True, axis =1)\ntarget = target.apply(encode)\ntarget_test = target_test.apply(encode)\ntrain = df\ntest = df_test","f2561a0e":"sns.countplot(target).set_title('Target distribution')\nplt.show()\nsns.countplot(target_test).set_title('Target distribution')","0c68c554":"sns.heatmap(df.corr()).set_title('Correlation heatmap between features')","6fb33655":"sns.distplot(df['USFLUX']).set_title(\"Distribution of the USFLUX feature\")","d4de6d0a":"sns.distplot(df['TOTBSQ']).set_title(\"Distribution of TOTBSQ feature\")","26189d46":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostClassifier \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, StratifiedKFold, cross_validate\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","65474aa6":"n_folds = 5\ndef auc_score(model):\n    kf = KFold( n_folds, shuffle= True).get_n_splits(train.values)\n    score = cross_val_score(model, train.values, target, scoring = \"roc_auc\", cv = kf)\n    return score","9efddd98":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","540465f5":"glm = LogisticRegression( random_state=1, solver='lbfgs', max_iter=2020, fit_intercept=True, penalty='none', verbose=0)","71b60421":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","11ee35ce":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","b1ff5840":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","9692726a":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(train.values, target.values)\nprint(model_lgb.feature_importances_)\nplt.plot(model_lgb.feature_importances_)\nplt.title(\"Feature importances according to LGBM\")\nplt.show()","a546a343":"score = auc_score(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = auc_score(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = auc_score(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nscore = auc_score(glm)\nprint(\"Logistic Regression: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n# score = auc_score(KRR)\n# print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n# score = auc_score(GBoost)\n# print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = auc_score(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","370ec726":"class average_stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,models):\n        self.models = models\n    def fit(self, x,y):\n        self.model_clones = [clone(x) for x in self.models]\n        \n        for model in self.model_clones:\n            model.fit(x,y)\n        return self\n    def predict(self, x):\n        preds = np.column_stack([\n            model.predict(x) for model in self.model_clones\n        ])\n        return np.mean(preds, axis = 1)","ec0de474":"averaged_models = average_stacking(models = (ENet, glm, model_lgb, lasso, model_xgb))\n\nscore = auc_score(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e2feb043":"# Dividing the trainset using a stratified kfold split, we are generating a train set. Next, we generate a test set which consists of the predictions of the pre-existing models\n\ndef generate_oof_trainset( train, target, test, test_target, strat_kfold, models,):\n\n    oof_train = pd.DataFrame() # Initializing empty data frame\n    \n    count = 0\n    print(train.shape, target.shape)\n\n    for train_id, test_id in strat_kfold.split(train, target):\n        count += 1\n        print(\"Current fold number is :\", count)\n        xtrain, xtest = train.iloc[train_id], train.iloc[test_id]\n        ytrain, ytest = target.iloc[train_id], target.iloc[test_id]\n        \n        curr_split = [None]*(len(models)+1) # Initializing list of lists to save all predictions for a split from all models for the current split\n        \n        for i in tqdm(range(len(models))):\n            \n            model = models[i]\n            model.fit(xtrain, ytrain)\n            \n            curr_split[i] = model.predict_proba(xtest)[:,1]      \n            \n        curr_split[-1] = ytest\n        oof_train = pd.concat([oof_train,pd.DataFrame(curr_split).T], ignore_index= True)\n    \n    oof_test = [None]*len(models)\n    for i, model in enumerate(models):\n        model.fit( train, target)\n        oof_test[i] = model.predict_proba(test)[:,1]\n    oof_test = pd.DataFrame(oof_test)\n    return oof_train, oof_test","e263f346":"# we fit the generated trainset and perform cross validation.\nfrom tqdm import tqdm\nstrat_kfold = StratifiedKFold( n_splits = 10, shuffle = True)\n\nlog_reg = LogisticRegression(max_iter= 1000, random_state = 0)\ngbr = GradientBoostingClassifier(\n        max_depth=6,\n        n_estimators=35,\n        warm_start=False,\n        random_state=42)\nadar = AdaBoostClassifier(n_estimators=100, random_state=0)\n\nmodels = [ log_reg, gbr, adar, glm ]\ntrain_generated, test_generated = generate_oof_trainset( train, target, test, target_test, strat_kfold, models)","ea300910":"lr_clf = LogisticRegression()\ntarget = train_generated[train_generated.columns[-1]]\ntrain_generated.drop([train_generated.columns[-1]], axis = 1 , inplace = True)\n\ncv_results = cross_validate(lr_clf,\n                            train_generated.values,\n                            target.values,\n                            cv = 3,\n                            scoring = 'roc_auc',\n                            verbose = 1,\n                            return_train_score = True,\n                            return_estimator = True)\n\nprint(\"Fit time :\", cv_results['fit_time'].sum(),\"secs\")\nprint(\"Score time :\", cv_results['score_time'].sum(),\"secs\")\nprint(\"Test score :\", cv_results['test_score'].mean())  ","aac7b83a":"Now that we have generated our out of fold train and test sets, lets train them further using the meta model. We are going to use a simple Logistic Regressor because we do not expect much non-linearity in the data.\n\nOf course you are welcome to try out different models to see which performs the best.","bb6fcbe4":"# Modelling","82a3d2ad":"Let's begin by importing the preprocessed dataset.","d7cb9a5b":"Wow, that is actually a big jump in the score. Next we can implement hyperparameter optimization to squeeze every bit of information from our features into our models.\n\nDo upvote the kernel if you enjoyed reading my work.","1248c52f":"### Averaged Stacking\nHere we will be using one of the techniques that I have implemented in one of my [other notebooks](https:\/\/www.kaggle.com\/amoghjrules\/intro-to-stacking-averaging-base-models) \n\nAn averaged stacking model takes the mean of predictions of multiple models and generates a result. We will be using the same KFold split for validation as we did before","9ffa873d":"The biased nature of the train data towards the 0 class is evident in the above plot. We should implement validation and encoding techniques such that this does not affect the training of the models.","3add6be6":"Although there is extensive space-weather data available, it is often very convoluted and messy. Here, we understand how important data collection and preprocessing techniques are. To ease our task I have extracted the data preprocessed by @mbobra [here](https:\/\/github.com\/mbobra\/machine-learning-with-solar-data).\n\nEDIT : I was able to find an larger, more comprehensive dataset for different classes of flares and have now modified the code to able to train on that data","93d64f47":"We can see that this technique has given us better accuracy than any of our models taken individually. But the increase in score is not that great. I'm sure we can do better.","2ced8c0b":"### Methods used:\nWe will be using multiple methods to find the optimal approach for this problem statement\n\n-> General linear and non-linear model fitting\n\n-> Averaged stacking\n\n-> Out of Fold meta-modelling","f45351ef":"# Applying various machine learning methods to predict Solar activity\n\n","d4c2615d":"I am using KFold split to improve validation accuracy rather than using a static train\/test split. I have limited the number of folds because of the relatively limited dataset. If the dataset is large, we can apply sophisticated encoding techniques.\n\nFor validation and scoring, I have written a small function and used the roc-auc metric. The function evalutes a given classifier and returns the score as a float value.","2679c049":"### Out of Fold Meta-Modelling\nThis is a slightly complicated way of training data, wherein the predictions by different models are feed into another model (meta-model) and the output of this meta-model is considered as a valid prediction. More over, we will be using a Stratified KFold Split for validation to overcome the apparent bias in the target classes as visualized before. \n\nStratified KFold essentially means the distribution of both the target variables will be more or less identical in every fold\/split that is made. This helps the model to train in a more balanced and uniform manner.\n\nTo understand more on how this technique works, check out my previous notebook [here](https:\/\/www.kaggle.com\/amoghjrules\/and-the-stacking-continues-oof-meta-modelling)","e866a5ba":"To build upon the the detailed work done in predicting solar weather, lets start by analysing the data that is presented to us","626983ff":"The above map illustrates the correlation between various features. We prefer the correlation between any 2 features to be minimal which implies that these features do not contain information redundant with respect to each other. On the other hand a high correlation value implies that both features contain a greater degree of overlapping (and hence redundant) information.","134b0e13":"### Function for scoring"}}