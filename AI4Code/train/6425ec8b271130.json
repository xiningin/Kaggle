{"cell_type":{"f9f4e43d":"code","fb55ddda":"code","4530059f":"code","c680cbf0":"code","07bd3e16":"code","dae86e9a":"code","8a3aa34e":"code","26d7d7c9":"code","ef253327":"code","ec29bddd":"code","e8d6c678":"code","8b30da6b":"code","4b18ee48":"code","7c49751d":"code","a85ff971":"markdown","07c1de64":"markdown","9067fb02":"markdown","9850c46b":"markdown","b2ae5cda":"markdown","99594d60":"markdown"},"source":{"f9f4e43d":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold","fb55ddda":"df_train = pd.read_csv('..\/input\/bengaliai-cv19\/train.csv')","4530059f":"df_train.head(2)","c680cbf0":"grapheme2idx = {grapheme: idx for idx, grapheme in enumerate(df_train.grapheme.unique())}\ndf_train['grapheme_id'] = df_train['grapheme'].map(grapheme2idx)","07bd3e16":"df_train.head(2)","dae86e9a":"n_fold = 5\nskf = StratifiedKFold(n_fold, random_state=42)\nfor i_fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train.grapheme)):\n    df_train.loc[val_idx, 'fold'] = i_fold\ndf_train['fold'] = df_train['fold'].astype(int)","8a3aa34e":"df_train.head(2)","26d7d7c9":"df_train['unseen'] = 0\ndf_train.loc[df_train.grapheme_id >= 1245, 'unseen'] = 1","ef253327":"df_train.unseen.value_counts()","ec29bddd":"df_train.to_csv('train_v2.csv', index=False)","e8d6c678":"n_fold = 5\nfor fold in range(n_fold):\n    train_idx = np.where((df_train['fold'] != fold) & (df_train['unseen'] == 0))[0]\n    valid_idx = np.where((df_train['fold'] == fold) | (df_train['unseen'] != 0))[0]\n\n    df_this_train = df_train.loc[train_idx].reset_index(drop=True)\n    df_this_valid = df_train.loc[valid_idx].reset_index(drop=True)\n    \n    #################################\n    # Do training and validating here\n    #################################\n    \n    break","8b30da6b":"n_uniq_grapheme = df_this_train.grapheme_id.nunique()\nn_uniq_root = df_this_train.grapheme_root.nunique()\nn_uniq_vowel = df_this_train.vowel_diacritic.nunique()\nn_uniq_diacritic = df_this_train.consonant_diacritic.nunique()\n\nprint(f'We have only {n_uniq_grapheme} grapheme in training data, but all {n_uniq_root} roots, {n_uniq_vowel} vowels, {n_uniq_diacritic} diacritics are remains')","4b18ee48":"n_uniq_grapheme = df_this_valid.grapheme_id.nunique()\nn_uniq_root = df_this_valid.grapheme_root.nunique()\nn_uniq_vowel = df_this_valid.vowel_diacritic.nunique()\nn_uniq_diacritic = df_this_valid.consonant_diacritic.nunique()\n\nprint(f'While we have all {n_uniq_grapheme} grapheme in validation, and all {n_uniq_root} roots, {n_uniq_vowel} vowels, {n_uniq_diacritic} diacritics as well')","7c49751d":"# We have 7578 unseen samples in validation set, which is approximately 16.4%\ndf_this_valid['unseen'].value_counts()","a85ff971":"# Usage Example","07c1de64":"Hi every one. I've made you guys a validation split, which considered unseen graphemes.\n\nIt have only 1245 graphemes in training set, while all components are remains. All unseen graphemes are used in every fold while training.\n\nUsing my split, we will have approximately **38k SEEN** samples, and exacly **7578 UNSEEN** samples for validation in each fold.\n\nDownload the file `train_v2.csv` generated by this script and have a try on yourself!\n","9067fb02":"# Analysis","9850c46b":"# StratifiedKFold On Grapheme","b2ae5cda":"# Add Unseen Flag","99594d60":"# Map grapheme to id"}}