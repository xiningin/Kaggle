{"cell_type":{"205b10c3":"code","d26a4be9":"code","b8d028dc":"code","ff32be38":"code","76bc1e87":"code","9878662b":"code","3e36293d":"code","3701f87a":"code","b9cd57a4":"code","f07e2f20":"code","19bc30dc":"code","e64eb86d":"code","bb1fc3a9":"code","6ab3174c":"code","0ea33d2e":"code","c443156b":"code","73c0f16e":"code","bc87fb33":"code","3474b348":"code","1374abda":"code","908030e6":"code","60648c5e":"code","1a49bab4":"code","2b9f16a5":"code","1c3b392f":"code","e51a35e5":"code","5343c6b9":"code","cb75eb75":"code","30c08d86":"code","86a9abef":"code","fdf1eb7d":"code","6254552e":"code","4f899d12":"code","68fe5a48":"code","7210c4b6":"code","48269334":"markdown","e830bcbe":"markdown","b23b67cb":"markdown","5db3967b":"markdown","0339799d":"markdown","1ba4992b":"markdown","1e5822e6":"markdown","7f4f3474":"markdown","c1fdd150":"markdown","a44c6f34":"markdown","60a65c84":"markdown","325cf1c9":"markdown","390b611b":"markdown","20a1df98":"markdown","59a7235c":"markdown","e2a7cbf7":"markdown","54fc44f4":"markdown","eafe5e62":"markdown","01656cc7":"markdown","f31ec275":"markdown","5c2c0be4":"markdown","4902e7af":"markdown","5f2e0d9d":"markdown","f11d0fa3":"markdown","c3577387":"markdown","aa0d10ec":"markdown","d019b546":"markdown","a6da27c4":"markdown","ed16828e":"markdown","fad2071e":"markdown","372864ef":"markdown","8badb8d9":"markdown","293a6a06":"markdown","04b48b80":"markdown","75c5d509":"markdown","c5b8ed02":"markdown","c4c18cd5":"markdown"},"source":{"205b10c3":"import os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\n\nimport missingno as msno","d26a4be9":"os.listdir('..\/input\/titanic')","b8d028dc":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n\n# First few rows of the training dataset\ntrain.head()\n","ff32be38":" \ntrain['Survived'].value_counts()","76bc1e87":"s = sns.countplot(x = 'Survived',data = train)\nsizes=[]\nfor p in s.patches:\n    height = p.get_height()\n    sizes.append(height)\n    s.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/len(train)*100),\n            ha=\"center\", fontsize=14) ","9878662b":"# credit: https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction. \n# One of the best notebooks on getting started with a ML problem.\n\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\n","3e36293d":"train_missing= missing_values_table(train)\ntrain_missing","3701f87a":"test_missing= missing_values_table(test)\ntest_missing","b9cd57a4":"msno.bar(train)","f07e2f20":"msno.matrix(train)","19bc30dc":"msno.matrix(train.sample(100))","e64eb86d":"msno.matrix(train)","bb1fc3a9":"#sorted by Age\nsorted = train.sort_values('Age')\nmsno.matrix(sorted)","6ab3174c":"msno.heatmap(train)","0ea33d2e":"msno.dendrogram(train)","c443156b":"train.isnull().sum()","73c0f16e":"train_1 = train.copy()\ntrain_1['Age'].mean() #pandas skips the missing values and calculates mean of the remaining values.","bc87fb33":"#Drop rows which contains any NaN or missing value for Age column\ntrain_1.dropna(subset=['Age'],how='any',inplace=True)\ntrain_1['Age'].isnull().sum()","3474b348":"# imputing with a constant\n\nfrom sklearn.impute import SimpleImputer\ntrain_constant = train.copy()\n#setting strategy to 'constant' \nmean_imputer = SimpleImputer(strategy='constant')#\ntrain_constant.iloc[:,:] = mean_imputer.fit_transform(train_constant)\ntrain_constant.isnull().sum()","1374abda":"from sklearn.impute import SimpleImputer\ntrain_most_frequent = train.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ntrain_most_frequent.iloc[:,:] = mean_imputer.fit_transform(train_most_frequent)","908030e6":"train_most_frequent.isnull().sum()","60648c5e":"os.listdir('..\/input\/air-quality-data-in-india')","1a49bab4":"city_day = pd.read_csv('..\/input\/air-quality-data-in-india\/city_day.csv',parse_dates=True,index_col='Date')\ncity_day1=city_day.copy(deep=True)\ncity_day.head()","2b9f16a5":"#Missing Values\ncity_day_missing= missing_values_table(city_day)\ncity_day_missing","1c3b392f":"# Imputation using ffill\/pad\n# Imputing Xylene value\n\ncity_day['Xylene'][50:64]\n","e51a35e5":"city_day.fillna(method='ffill',inplace=True)\ncity_day['Xylene'][50:65]","5343c6b9":"# Imputing AQI value\n\ncity_day['AQI'][20:30]","cb75eb75":"city_day.fillna(method='bfill',inplace=True)\ncity_day['AQI'][20:30]","30c08d86":"city_day1['Xylene'][50:65]","86a9abef":"# Interpolate using the linear method\ncity_day1.interpolate(limit_direction=\"both\",inplace=True)\ncity_day1['Xylene'][50:65]","fdf1eb7d":"train_knn = train.copy(deep=True)","6254552e":"from sklearn.impute import KNNImputer\ntrain_knn = train.copy(deep=True)\n\nknn_imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\ntrain_knn['Age'] = knn_imputer.fit_transform(train_knn[['Age']])","4f899d12":"train_knn['Age'].isnull().sum()\n","68fe5a48":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\ntrain_mice = train.copy(deep=True)\n\nmice_imputer = IterativeImputer()\ntrain_mice['Age'] = mice_imputer.fit_transform(train_mice[['Age']])","7210c4b6":"train_mice['Age'].isnull().sum()","48269334":"## Finding reason for missing data using Dendrogram \nA dendogram is a tree diagram of missingness. It groups the highly correlated variables together.","e830bcbe":"\n### Dropping complete columns \n\nIf a column contains a lot of missing values, say more than 80%, and the feature is not significant, you might want to delete that feature. However, again, it is not a good methodology to delete data.\n","b23b67cb":"# References and good resources \n\n* [Dealing with Missing Data in Python](https:\/\/campus.datacamp.com\/courses\/dealing-with-missing-data-in-python\/the-problem-with-missing-data?ex=1)\n* [How to Handle Missing Data](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4)","5db3967b":"### Imputation using Linear Interpolation method\n\nTime series data has a lot of variations against time. Hence, imputing using backfill and forward fill isn't the ebst possible solution to address the missing value problem. A more apt alternative would be to use interpolation methods, where the values are filled with incrementing or decrementing values.\n\n[Linear interpolation](https:\/\/www.lexjansen.com\/nesug\/nesug01\/ps\/ps8026.pdf) is an imputation technique that assumes a linear relationship between data points and utilises non-missing values from adjacent data points to compute a value for a missing data point. \n\nRefer to the official documentation for a complete list of interpolation strategies [here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.interpolate.html)","0339799d":"## Imputations Techniques for non Time Series Problems \n\n![](https:\/\/imgur.com\/bL0iHde.png)\n\n>Imputation refers to replacing missing data with substituted values.There are a lot of ways in which the missing values can be imputed depending upon the nature of the problem and data. Dependng upon the nature of the problem, imputation techniques can be broadly they can be classified as follows:\n\n\n### Basic Imputation Techniques\n  \n  - Imputating with a constant value\n  - Imputation using the statistics (mean, median or most frequent) of each column in which the missing values are located\n\nFor this we shall use the `The SimpleImputer` class from sklearn.","1ba4992b":"# Treating Missing values  \n\nAfter having identified the patterns in missing values, it is important to treat them too. Here is a flowchart which could prove handy:\n\n\n\n## Deletions \n\n![](https:\/\/imgur.com\/tBvdfyX.png)\n\n>Deletion means to delete the missing values from a dataset. This is however not recommended as it might result in loss of information from the dataset. We should only delete the missing values from a dataset if their proportion is very small. Deletions are further of three types: \n\n### Pairwise Deletion \n\nParwise Deletion is used when values are missing completely at random i.e MCAR. During Pairwise deletion, only the missing values are deleted. All operations in pandas like mean,sum etc intrinsically skip missing values.\n\n\n","1e5822e6":"## Advanced Imputation Techniques \n\nAdvanced imputation techniques uses machine learning algorithms to impute the missing values in a dataset unlike the previous techniques where we used other column values to predict the missing values. We shall look at the following two techniques in this notebook:\n\n* [Nearest neighbors imputation](https:\/\/scikit-learn.org\/stable\/modules\/impute.html#nearest-neighbors-imputation)\n* [Multivariate feature imputation](https:\/\/scikit-learn.org\/stable\/modules\/impute.html#multivariate-feature-imputation)\n\n### K-Nearest Neighbor Imputation\n\nThe [KNNImputer class](https:\/\/scikit-learn.org\/stable\/modules\/impute.html#multivariate-feature-imputation) provides imputation for filling in missing values using the k-Nearest Neighbors approach.Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. ","7f4f3474":"## Examining the Target column \n\n>For each passenger in the test set, we need to create a model that predicts whether or not the passengers survived the sinking of the Titanic. Hence Survived is that target column in the dataset. Let's examine the Distribution of the target column","c1fdd150":"This shows that the Embarked column has no nullity in the first 100 cases.","a44c6f34":"# Handling Missing Values in Python\n\n![](https:\/\/imgur.com\/68u0dD2.png)\n\n>Real world data is messy and often contains a lot of missing values. There could be multiple  reasons for the missing values but primarily the reason for missingness can be attributed to \n\n| Reason for missing Data |  \n|--|\n|Data doesn't exist  |  \n|Data not collected due to human error.  |  \n|Data deleted accidently  |  \n|                         |\n\nEither way we need to address this issue before we  proceed with the modeling stuff. Some algorithms like XGBoost or LightGBM can treat missing data without any preprocessing. However, it is a good idea to implement one's own missing value imputation to get better performance.\n<hr>\n","60a65c84":">Since the focus of the notebook is to detect and handle missing values, we'll jump directly into it. Let's now look at a step by step process to manage the missing values in a dataset.\n<hr>","325cf1c9":"## Finding reason for missing data using a Heatmap ","390b611b":"Both the train and test set have the same proportion of the missing values. ","20a1df98":"# Objective\nThe objective of this notebook is to detect missing values and then go over some of the methods used for imputing them.\n<hr>\n\n\n# Data\n\nThere are two publically available datasets which will be used to explain the concepts:\n\n* 1. [Titanic Dataset](https:\/\/www.kaggle.com\/c\/titanic) for Non Time Series problem\n* 2. [Air Quality Data in India (2015 - 2020)](https:\/\/www.kaggle.com\/rohanrao\/air-quality-data-in-india) for Time Series problem\n\n\n# Loading necessary libraries and datasets","59a7235c":"Here:\n* 0: Did not Survive while \n* 1: Survived. \n\nClearly, less people survived the accident.","e2a7cbf7":">* The `Embarked` Column has very few missing values and donot seem to be correlated with any other column, Hence, the missingness in Embarked column can be attributed as Missing Completely at Random.\n* Both the `Age` and the `Cabin` columns have a lot of missing values.This could be a case of MAR as we cannot directly observe the reason for missingness of data in these columns.\n\nThe missingno package also let's us sort the graph by a particluar column. Let's sort the values by `Age` and `Cabin` column to see if there is a pattern in the missing values","54fc44f4":"# Reasons for Missing Values \n\n>Before we start treating the missing values ,it is important to understand the various reasons for the missingness in data. Broadly speaking, there can be three possible reasons:\n\n![](https:\/\/cjasn.asnjournals.org\/content\/clinjasn\/early\/2014\/02\/06\/CJN.10141013\/F2.large.jpg?width=800&height=600&carousel=1)\n\nsource:https:\/\/cjasn.asnjournals.org\/content\/early\/2014\/02\/06\/CJN.10141013\/tab-figures-data?versioned=true\n\n\n**1. Missing Completely at Random (MCAR) **\n\n>The missing values on a given variable (Y) are not associated with other variables in a given data set or with the variable (Y) itself. In other words, there is no particular reason for the missing values.\n\n**2. Missing at Random (MAR) **\n\n>MAR occurs when the missingness is not random, but where missingness can be fully accounted for by variables where there is complete information.\n\n**3. Missing Not at Random (MNAR) **\n>Missingness depends on unobserved data or the value of the missing data itself. \n\n*All definitions taken from Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Missing_data*\n\n\n>Now let us look at nullity matrix again to see if can find what type of missingness is present in the dataset.\n","eafe5e62":"## Imputations Techniques for Time Series Problems\n\nNow let's look at ways to impute data in a typical time series problem. Tackling missing values in time Series problem is a bit different. The `fillna()` method is used for imputing missing values in such problems.\n\n* Basic Imputation Techniques\n  - 'ffill' or 'pad' - Replace NaN s with last observed value\n  - 'bfill' or 'backfill' - Replace NaN s with next observed value\n  -  Linear interpolation method\n\n### Time Series dataset\n\nThe dataset is called [Air Quality Data in India (2015 - 2020)](https:\/\/www.kaggle.com\/rohanrao\/air-quality-data-in-india) Tand it contains air quality data and AQI (Air Quality Index) at hourly and daily level of various stations across multiple cities in India.The dataset has a lot of missing values and and is a classic Time series problem.","01656cc7":"We can see that all missing values have been filled with the last observed values.\n\n### Imputation using bfill","f31ec275":"## Detecting missing data visually using Missingno library\n\n>To graphically analyse the missingness of the data, let's use a library called [Missingno](https:\/\/github.com\/ResidentMario\/missingno) It is a package for graphical analysis of missing values. To use this library, we need to import it as follows: `import missingno as msno`","5c2c0be4":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Objective<\/a><\/span><\/li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data<\/a><\/span><\/li><li><span><a href=\"#Loading-necessary-libraries-and-datasets\" data-toc-modified-id=\"Loading-necessary-libraries-and-datasets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Loading necessary libraries and datasets<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Reading-in-the-dataset\" data-toc-modified-id=\"Reading-in-the-dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Reading in the dataset<\/a><\/span><\/li><li><span><a href=\"#Examining-the-Target-column\" data-toc-modified-id=\"Examining-the-Target-column-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Examining the Target column<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Detecting-Missing-values\" data-toc-modified-id=\"Detecting-Missing-values-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Detecting Missing values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Detecting-missing-values-numerically\" data-toc-modified-id=\"Detecting-missing-values-numerically-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Detecting missing values numerically<\/a><\/span><\/li><li><span><a href=\"#Detecting-missing-data-visually-using-Missingno-library\" data-toc-modified-id=\"Detecting-missing-data-visually-using-Missingno-library-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Detecting missing data visually using Missingno library<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Visualizing-the-locations-of-the-missing-data\" data-toc-modified-id=\"Visualizing-the-locations-of-the-missing-data-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;<\/span>Visualizing the locations of the missing data<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Reasons-for-Missing-Values\" data-toc-modified-id=\"Reasons-for-Missing-Values-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Reasons for Missing Values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Finding-reason-for-missing-data-using-matrix-plot\" data-toc-modified-id=\"Finding-reason-for-missing-data-using-matrix-plot-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Finding reason for missing data using matrix plot<\/a><\/span><\/li><li><span><a href=\"#Finding-reason-for-missing-data-using-a-Heatmap\" data-toc-modified-id=\"Finding-reason-for-missing-data-using-a-Heatmap-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Finding reason for missing data using a Heatmap<\/a><\/span><\/li><li><span><a href=\"#Finding-reason-for-missing-data-using-Dendrogram\" data-toc-modified-id=\"Finding-reason-for-missing-data-using-Dendrogram-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Finding reason for missing data using Dendrogram<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Treating-Missing-values\" data-toc-modified-id=\"Treating-Missing-values-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Treating Missing values<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Deletions\" data-toc-modified-id=\"Deletions-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Deletions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Pairwise-Deletion\" data-toc-modified-id=\"Pairwise-Deletion-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;<\/span>Pairwise Deletion<\/a><\/span><\/li><li><span><a href=\"#Listwise-Deletion\/-Dropping-rows\" data-toc-modified-id=\"Listwise-Deletion\/-Dropping-rows-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;<\/span>Listwise Deletion\/ Dropping rows<\/a><\/span><\/li><li><span><a href=\"#Dropping-complete-columns\" data-toc-modified-id=\"Dropping-complete-columns-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;<\/span>Dropping complete columns<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Imputations-Techniques-for-non-Time-Series-Problems\" data-toc-modified-id=\"Imputations-Techniques-for-non-Time-Series-Problems-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Imputations Techniques for non Time Series Problems<\/a><\/span><\/li><li><span><a href=\"#Imputations-Techniques-for-Time-Series-Problems\" data-toc-modified-id=\"Imputations-Techniques-for-Time-Series-Problems-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Imputations Techniques for Time Series Problems<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Time-Series-dataset\" data-toc-modified-id=\"Time-Series-dataset-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;<\/span>Time Series dataset<\/a><\/span><\/li><li><span><a href=\"#Imputing-using-ffill\" data-toc-modified-id=\"Imputing-using-ffill-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;<\/span>Imputing using ffill<\/a><\/span><\/li><li><span><a href=\"#Imputation-using-bfill\" data-toc-modified-id=\"Imputation-using-bfill-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;<\/span>Imputation using bfill<\/a><\/span><\/li><li><span><a href=\"#Imputation-using-Linear-Interpolation-method\" data-toc-modified-id=\"Imputation-using-Linear-Interpolation-method-6.3.4\"><span class=\"toc-item-num\">6.3.4&nbsp;&nbsp;<\/span>Imputation using Linear Interpolation method<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Advanced-Imputation-Techniques\" data-toc-modified-id=\"Advanced-Imputation-Techniques-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>Advanced Imputation Techniques<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#K-Nearest-Neighbor-Imputation\" data-toc-modified-id=\"K-Nearest-Neighbor-Imputation-6.4.1\"><span class=\"toc-item-num\">6.4.1&nbsp;&nbsp;<\/span>K-Nearest Neighbor Imputation<\/a><\/span><\/li><li><span><a href=\"#Multivariate-feature-imputation---Multivariate-imputation-by-chained-equations-(MICE)\" data-toc-modified-id=\"Multivariate-feature-imputation---Multivariate-imputation-by-chained-equations-(MICE)-6.4.2\"><span class=\"toc-item-num\">6.4.2&nbsp;&nbsp;<\/span>Multivariate feature imputation - Multivariate imputation by chained equations (MICE)<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#References-and-good-resources\" data-toc-modified-id=\"References-and-good-resources-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>References and good resources<\/a><\/span><\/li><\/ul><\/div>","4902e7af":">Let's read the above dendrogram from a top-down perspective: \n* Cluster leaves which linked together at a distance of zero fully predict one another's presence\u2014one variable might always be empty when another is filled, or they might always both be filled or both empty, and so on(missingno documentation)\n>\n>![Screenshot%202020-04-25%20at%208.19.56%20AM.png](attachment:Screenshot%202020-04-25%20at%208.19.56%20AM.png)\n>\n>* the missingness of Embarked tends to be more similar to Age than to Cabin and so on.However, in this particluar case, the correlation is high since Embarked column has a very few missing values.\n\nThis dataset doesn't have much missing values but if you use the same methodology on datasets having a lot of missing values, some interesting pattern will definitely emerge.","5f2e0d9d":"# Detecting Missing values\n\n## Detecting missing values numerically \n\n>The first step is to detect the count\/percentage of missing values in every column of the dataset. This will give an idea about the distribution of missing values.","f11d0fa3":"Above we see, there are 3 missing values in the Xylene column. \n\n### Imputing using ffill","c3577387":"We can see that all missing values have been filled with the next observed values.","aa0d10ec":"There are a lot of missing values and some of the columns like Xylene and PM10 have more than 50% of the values missing. Let's now see how we can impute these missing values.\n\n","d019b546":">The bar chart above gives a quick graphical overview of the completeness of the dataset. We can see that Age, Cabin and embarked columns have missing values. Next,it would make sense to find out the locations of the missing data.","a6da27c4":"### Multivariate feature imputation - Multivariate imputation by chained equations (MICE)\nA strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. It performns multiple regressions over random sample ofthe data, then takes the average ofthe multiple regression values and uses that value to impute the missing value. In sklearn, it is implemented as follows:\n\n","ed16828e":"## Reading in the dataset\n* Reading in the Titanic Dataset.","fad2071e":"I won't go much into explaining the data since I have done a lot of relatedw work in my kernel titled [\ud83d\ude37 Breathe India: COVID-19 effect on Pollution](https:\/\/www.kaggle.com\/parulpandey\/breathe-india-covid-19-effect-on-pollution).In this notebook, let's keep our focus on the missing values only. As id evident, city_day dataframe consists of daily pollution level data of some of the prominent cities in India.\n\n","372864ef":"\n### Listwise Deletion\/ Dropping rows\n\n>During Listwise deletion, complete rows(which contain the missing values) are deleted. As a result, it is also called Complete Case deletion. Like Pairwise deletion, listwise deletions are also only used for MCAR values.\n","8badb8d9":">The Age column doesn't have any missing values.A major diadvantage of Listwise deletion is that a major chunk of data and hence a lot of information is lost. Hence, it is advisable to use it only when the number of missing values is very small.","293a6a06":"## Finding reason for missing data using matrix plot ","04b48b80":"Hence it is clear that here is no relation between the missingness in Age and Cabin column.To cement this conclusion further we can also draw a Heatmap among the different variables in the dataset.","75c5d509":"### Visualizing the locations of the missing data \n\n>The [msno.matrix](https:\/\/github.com\/ResidentMario\/missingno#matrix) nullity matrix is a data-dense display which lets you quickly visually pick out patterns in data completion.\n","c5b8ed02":"The heatmap function shows that there are no strong correlations between missing values of different features. This is good; low correlations further indicate that the data are MAR.","c4c18cd5":">* The plot appears blank(white) wherever there are missing values. For instance, in Embarked column there are only two instances of missing data, hence the two white lines.\n>\n>* The sparkline on the right gives an idea of the general shape of the completeness of the data and points out the row with the minimum nullities and the total number of columns in a given dataset, at the bottom.\n>\n>It is also possible to sample the dataset to pinpoint the exact location of the missing values. For instance let's check the first 100 rows."}}