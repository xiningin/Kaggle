{"cell_type":{"1a0d5f6e":"code","b2ccf772":"code","66f97ed0":"code","cc1dfbc4":"code","29374454":"code","88fc3fc4":"code","fc719335":"code","fdda35ce":"code","a00ca75b":"code","115a449e":"code","7fae27c5":"code","333ebaf8":"code","54f8cb28":"code","90eeb8c2":"code","b407924f":"code","49ac0c1b":"code","f1da26a1":"code","6f3dc888":"code","09681328":"code","83cd13b9":"code","85259a00":"code","87fac338":"code","03f749f1":"code","2d6e9d4f":"code","d18b9fb8":"code","9b0393aa":"code","adc7b89f":"code","63e14a81":"code","00e05f03":"code","9dfe7d0d":"code","de3bfdd7":"code","a0b71e13":"code","978ec839":"code","8ebe20c3":"code","440757cf":"code","2a0d78c5":"code","f98e50be":"code","fe33083d":"code","d1835eec":"code","4db35278":"code","26f3c850":"code","66f8f89e":"code","d5f36d55":"code","613ebf75":"code","59086bc7":"code","da7bbb23":"code","dc01d39f":"code","7682543a":"code","89a1c999":"code","3ed57338":"code","6f3239de":"code","e195ec37":"code","29eadd77":"code","da537947":"code","6d4720d1":"code","7383ce9d":"code","501a1bac":"code","825b1980":"code","3003c289":"code","5cb2ec1d":"code","0d559fbb":"code","c47c12d7":"code","68454813":"code","c46e002e":"code","5e79ac26":"code","c2ad374b":"code","03ee1818":"code","151deeea":"code","bdca2453":"code","5133883c":"code","5ebcefa6":"code","ae21a76f":"code","7779fb74":"code","b1606996":"code","6418550c":"markdown","d0427444":"markdown","052cbc14":"markdown","d36c2afd":"markdown","abc4bbb6":"markdown","7b86fc97":"markdown","c1923c99":"markdown","94f6f687":"markdown","38e718b5":"markdown","5758f617":"markdown","7e7863cf":"markdown","f723d373":"markdown","a397d44a":"markdown","885c89a9":"markdown","904ead04":"markdown","8d7c9966":"markdown","7018e74a":"markdown","bbc79c63":"markdown","0e209a34":"markdown","57867523":"markdown","6f8ed761":"markdown","46709677":"markdown","5c22ff83":"markdown","63dd6d42":"markdown","1049ccb4":"markdown","073972c0":"markdown","10ec47a6":"markdown","557d87e0":"markdown","a227ccd5":"markdown","346f0aac":"markdown","c52746a2":"markdown","1ac8da3c":"markdown","dd7a510d":"markdown","be927c3c":"markdown","dfd66b7e":"markdown","228b59ef":"markdown","fb119e3f":"markdown","76ab9905":"markdown","0f9f0b6a":"markdown","6fea5e37":"markdown","bb829c22":"markdown","0ca014c2":"markdown","d1e5d805":"markdown","aa8f1a3c":"markdown","b8aaabef":"markdown","53920b1a":"markdown","4aaf8f80":"markdown","0c7a546c":"markdown","280bc8d7":"markdown","2e98de7c":"markdown","09c44316":"markdown","8ddbb77c":"markdown","fee68b38":"markdown"},"source":{"1a0d5f6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # regular expressions\nimport nltk\nimport random # for random sampling 300 abstracts for annotation. Can be removed later\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nimport matplotlib.pyplot as plt\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b2ccf772":"!ls \/kaggle\/input\/CORD-19-research-challenge\/","66f97ed0":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, parse_dates=['publish_time'], dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","cc1dfbc4":"meta_df.info()","29374454":"meta_df_new = meta_df[meta_df['publish_time'] >= '2019-01-01']\nmeta_df_new.info()","88fc3fc4":"abstracts_new = meta_df_new[['cord_uid', 'abstract']].dropna() # create new df with only id and abstract\nabout_coronavirus = abstracts_new['abstract'].apply(lambda x: ('coronavirus' in x.lower() or 'covid' in x.lower())) # create condition that abstract contains 'coronavirus' or 'COVID'\nabstracts_new = abstracts_new[about_coronavirus] # filter abstracts based on about_coronavirus condition\nabstracts_new.info()","fc719335":"abstracts_new.head()","fdda35ce":"abstracts_new['tokens'] = abstracts_new['abstract'].apply(lambda x: re.sub('[^a-zA-z\\s]',' ',x)) # remove punctuation and numbers from abstract text\nabstracts_new.head()","a00ca75b":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: word_tokenize(x.lower())) #tokenize lowercase words\nabstracts_new.head()","115a449e":"# define a function to remove stop words from a list of words\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","7fae27c5":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: remove_stopwords(x)) #remove stopwords\nabstracts_new.head()","333ebaf8":"# define a function to remove the word 'abstract' from a list of words\ndef remove_abstract(text):\n    words = [w for w in text if w != 'abstract']\n    return words","54f8cb28":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: remove_abstract(x)) #remove the word 'abstract' from token list\nabstracts_new.head()","90eeb8c2":"# define a function to lematize a list of tokens\nlemmatizer = WordNetLemmatizer() # instantiate a lemmatizer\ndef lemmatize_tokens(tokens):\n    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    return lemmatized_tokens","b407924f":"abstracts_new['tokens'] = abstracts_new['tokens'].apply(lambda x: lemmatize_tokens(x)) #lemmatize words\nabstracts_new.head()","49ac0c1b":"abstracts_new['bow'] = abstracts_new['tokens'].apply(lambda x: Counter(x)) # create new column called 'bow' that translates lemmatized tokens into bag of words \nabstracts_new.head()","f1da26a1":"# define a function that converts a list of tokens into n-grams\ndef getNGrams(tokens, n):\n    return [tokens[i : i + n] for i in range(len(tokens) - (n - 1))]","6f3dc888":"abstracts_new['2-grams'] = abstracts_new['tokens'].apply(lambda x: getNGrams(x, 2))\nabstracts_new.head()","09681328":"abstracts_new.info()","83cd13b9":"# read in data\nmodel_data_df = pd.read_csv('..\/input\/covid19-data-for-modeling\/Metadata_for_modeling.csv',encoding = \"ISO-8859-1\")\n# get training and test samples\ntrain_test = model_data_df[model_data_df['split_label'].notna()]\ntrain_test.info()","85259a00":"# Training set\ntrain = train_test[train_test['split_label']==\"Training\"]\ntrain.info()","87fac338":"# Test set\ntest = train_test[train_test['split_label']==\"Test\"]\ntest.info()","03f749f1":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 2000) \n                             \nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\n# Top words in the trianing set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')\n\n","2d6e9d4f":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","d18b9fb8":"# Train a simple logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nbow_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(bow_lr_train)","9b0393aa":"# Test performance\nbow_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(bow_lr_test)","adc7b89f":"# Train a random forest model\n\n# Select hyperparameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nforestCV = RandomForestClassifier(n_estimators = 100, max_depth=17, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nbow_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(bow_rf_train)\n\n","63e14a81":"# Test performance\nbow_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(bow_rf_test)","00e05f03":"# Train an MLP model\n\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n# param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n# clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(4, 2), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nbow_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(bow_mlp_train)","9dfe7d0d":"# Test performance\nbow_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(bow_mlp_test)","de3bfdd7":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             ngram_range = (1,2),    # <- include 1 and 2-grams\n                             max_features = 2000) \n                             \nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\n# Top words in the trianing set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","a0b71e13":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","978ec839":"# Train a logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nngram_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(ngram_lr_train)","8ebe20c3":"# Test performance\nngram_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(ngram_lr_test)","440757cf":"# Train a random forest model\n\n# Select hyperparameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nforestCV = RandomForestClassifier(n_estimators = 100,max_depth=13, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nngram_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(ngram_rf_train)","2a0d78c5":"# Test performance\nngram_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(ngram_rf_test)","f98e50be":"# Train an MLP model\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n#param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n#clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n#grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n#grid_clf.fit(x_train, train['risk_factor'])\n\n#grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nngram_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(ngram_mlp_train)","fe33083d":"# Test performance\nngram_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(ngram_mlp_test)","d1835eec":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             ngram_range = (1,1),\n                             max_features = 2000)\n\nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","4db35278":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","26f3c850":"# Train a simple logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nTfidf_bow_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(Tfidf_bow_lr_train)","66f8f89e":"# Test performance\nTfidf_bow_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(Tfidf_bow_lr_test)","d5f36d55":"# Train a random forest model\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\nforestCV = RandomForestClassifier(n_estimators = 50,max_depth=9, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_bow_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(Tfidf_bow_rf_train)","613ebf75":"# Test performance\nTfidf_bow_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(Tfidf_bow_rf_test)","59086bc7":"# Train an MLP model\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n#param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n#clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n#grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n#grid_clf.fit(x_train, train['risk_factor'])\n\n#grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_bow_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(Tfidf_bow_mlp_train)","da7bbb23":"# Test performance\nTfidf_bow_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(Tfidf_bow_mlp_test)","dc01d39f":"train_test_tmp = train_test\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = TfidfVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,   \n                             ngram_range = (1,2),  \n                             max_features = 2000)\n\nvectorizer.fit(train_test_tmp['tokens'])\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['tokens'])\nx_train = x_train.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","7682543a":"#vectorizing test set\nx_test = vectorizer.transform(test['tokens'])\nx_test = x_test.toarray()\n\n# Top words in the test set\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","89a1c999":"# Train a simple logistic regression\nlogisticCV = LogisticRegressionCV(cv=5, random_state=19, max_iter = 10000).fit(x_train, train['risk_factor'])\n# Training performance\nTfidf_ngram_lr_train = logisticCV.score(x_train, train['risk_factor']) \nprint(Tfidf_ngram_lr_train)","3ed57338":"# Test performance\nTfidf_ngram_lr_test = logisticCV.score(x_test, test['risk_factor'])  \nprint(Tfidf_ngram_lr_test)","6f3239de":"# Train a random forest model\n\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n# param_grid = {\n#                 'n_estimators': [50,100,150,200,500],\n#                 'max_depth': list(range(1,20, 2))\n#             }\n\n# clf = RandomForestClassifier(random_state=19)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nforestCV = RandomForestClassifier(n_estimators = 50,max_depth=11, random_state=19) \nforestCV = forestCV.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_ngram_rf_train  = forestCV.score(x_train, train['risk_factor']) \nprint(Tfidf_ngram_rf_train)\n","e195ec37":"# Test performance\nTfidf_ngram_rf_test = forestCV.score(x_test, test['risk_factor']) \nprint(Tfidf_ngram_rf_test)","29eadd77":"# Train an MLP model\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n#param_grid = {'hidden_layer_sizes': [(20,10),(10,5),(4,2)]}\n\n#clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n#grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n#grid_clf.fit(x_train, train['risk_factor'])\n\n#grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='lbfgs', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nNN = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_ngram_mlp_train = NN.score(x_train, train['risk_factor'])\nprint(Tfidf_ngram_mlp_train)","da537947":"# Test performance\nTfidf_ngram_mlp_test = NN.score(x_test, test['risk_factor'])  \nprint(Tfidf_ngram_mlp_test)","6d4720d1":"# summary table for training performance:\ntrain_res = [['Bow',bow_lr_train, bow_rf_train, bow_mlp_train],\n             ['1,2-gram', ngram_lr_train, ngram_rf_train, ngram_mlp_train],\n             ['Tfidf-Bow', Tfidf_bow_lr_train, Tfidf_bow_rf_train, Tfidf_bow_mlp_train], \n             ['Tfidf-1,2-gram', Tfidf_ngram_lr_train, Tfidf_ngram_rf_train, Tfidf_ngram_mlp_train]]\ntrain_res = pd.DataFrame(train_res, columns = ['Features','Logistic','Random Forest','MLP'])","7383ce9d":"train_res","501a1bac":"# summary table for testing performance:\ntest_res = [['Bow',bow_lr_test, bow_rf_test, bow_mlp_test],\n             ['1,2-gram', ngram_lr_test, ngram_rf_test, ngram_mlp_test],\n             ['Tfidf-Bow', Tfidf_bow_lr_test, Tfidf_bow_rf_test, Tfidf_bow_mlp_test], \n             ['Tfidf-1,2-gram', Tfidf_ngram_lr_test, Tfidf_ngram_rf_test, Tfidf_ngram_mlp_test]]\ntest_res = pd.DataFrame(test_res, columns = ['Features','Logistic','Random Forest','MLP'])","825b1980":"test_res","3003c289":"# Merge previously annotated labels back to the collection of filtered abstracts: \n\nlabels = train_test[[\"cord_uid\",\"risk_factor\"]]\n\nabstracts_new_merged = pd.merge(abstracts_new,labels, how = 'left', on = ['cord_uid'])\nabstracts_new_merged.info()\n","5cb2ec1d":"abstracts_new_merged.head()","0d559fbb":"# Initialize vectorizer on the entire dataset\n\ntrain_test_tmp = abstracts_new_merged\n\n# Note\uff1a The direct use of abstracts_new_merged['token'] dosen't work so I repeated the preoprocessing here. \n\ntrain_test_tmp['newtokens'] = train_test_tmp['abstract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n# Removing punctuation\ntrain_test_tmp['newtokens'] = train_test_tmp['newtokens'].apply(lambda x: re.sub(r'[^a-zA-z\\w\\s]','',x))\n\n# Stop word removal\nstop = stopwords.words('english')\nstop.append('abstract')\ntrain_test_tmp['newtokens'] = train_test_tmp['newtokens'].apply(lambda x: \" \".join(w for w in x.split() if not w in stop))\n\n#Stemming\nlemmatizer = WordNetLemmatizer()\ntrain_test_tmp['newtokens'] = train_test_tmp['newtokens'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n\n\n#train Vectorizer the entire training+test set\nwords = set(nltk.corpus.words.words())\n\nvectorizer = TfidfVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             ngram_range = (1,2),    # <- indicate 1 and 2-grams\n                             max_features = 2000)\n\nvectorizer.fit(train_test_tmp['newtokens'])\n\n# This is our new training set\ntrain = train_test_tmp[train_test_tmp['risk_factor'].notna()]\ntrain.info()\n\n\n","c47c12d7":"# This is our new test set that includes all the unlabeled abstracts\ntest = train_test_tmp[train_test_tmp['risk_factor'].isna()]\ntest.info()","68454813":"\n\n#Vectorizing training set\nx_train = vectorizer.transform(train['newtokens'])\nx_train = x_train.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_train.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')\n\n\n","c46e002e":"#vectorizing testing set\nx_test = vectorizer.transform(test['newtokens'])\nx_test = x_test.toarray()\n\nword_count = pd.DataFrame({'word': vectorizer.get_feature_names(), 'count': np.asarray(x_test.sum(axis=0))})\nword_count.sort_values('count', ascending=False).set_index('word')[:30].sort_values('count', ascending=True).plot(kind='barh')","5e79ac26":"# Train the final MLP model\n# Select hyper parameter through cross-validation. To shorten the process time of this notebook, this step is only run once and the results will be used directly.  \n\n# param_grid = {'hidden_layer_sizes': [(50,20),(10,5),(10,2)]}\n# clf = MLPClassifier(random_state=19, max_iter = 10000)\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=5)\n\n# grid_clf.fit(x_train, train['risk_factor'])\n\n# grid_clf.best_estimator_\n\nNN = MLPClassifier(solver='adam', alpha=0.0001,activation='relu',\n                    hidden_layer_sizes=(10, 5), random_state=19, max_iter = 1000)\n\nfinal_model = NN.fit(x_train, train['risk_factor'])\n\n# Training performance\nTfidf_ngram_mlp_final = final_model.score(x_train, train['risk_factor'])\nprint(Tfidf_ngram_mlp_final)","c2ad374b":"# Merge results and obtain all risk-factor related articles\nresults = final_model.predict(x_test)\n\noutput = pd.DataFrame( data={\"cord_uid\":test[\"cord_uid\"], \"Predicted_label\":results})\n\noutput.info()","03ee1818":"tmp = pd.merge(abstracts_new_merged,output,how = 'left', on = ['cord_uid'])\n\nRiskFactor_df = tmp[(tmp['Predicted_label'] == \"Yes\" ) | (tmp['risk_factor']== \"Yes\")]\n\nRiskFactor_df.info()","151deeea":"RiskFactor_df.head()","bdca2453":"LDA_data = RiskFactor_df['tokens']\n\n# Reformatting tokens for LDA\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ntexts = list(sent_to_words(LDA_data))\n\ndictionary = corpora.Dictionary(texts)\ndict(list(dictionary.token2id.items())[0:10])","5133883c":"corpus = [dictionary.doc2bow(item) for item in texts]","5ebcefa6":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                            id2word=dictionary,\n                                            num_topics=10,\n                                            random_state=100,\n                                            update_every=1,\n                                            chunksize=100,\n                                            passes=10,\n                                            alpha='auto',\n                                            per_word_topics=True)\n","ae21a76f":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.","7779fb74":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","b1606996":"import pyLDAvis\nimport pyLDAvis.gensim  \n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, R = 50)\nvis","6418550c":"**Remove punctuation and numbers from abstracts:**","d0427444":"**Run LDA model:**","052cbc14":"**Test performance:**","d36c2afd":"**Unigram and bigram:**","abc4bbb6":"**Bag of words adjusted by Tf-idf weighting:**","7b86fc97":"## c Manual annotation\nWe randomly sampled 352 abstracts from the collection of filtered abstracts. The manual annotation was done by reading the 352 abstracts and label them as either related to this risk-factor task or not. The criteria we used in the manual process for classifying an abstract as risk-factor related are based on the Task Details, i.e.: \n* Data on potential risks factors\n  * Smoking, pre-existing pulmonary disease\n  * Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n  * Neonates and pregnant women\n  * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n* Susceptibility of populations\n* Public health mitigation measures that could be effective for control\n\nAfter manual annotation, we have also split the 352 abstracts into training (N = 281) and test (N = 71) sets stratified by the annotation label. The annotation labels and and training\/test identifiers are stored in [this dataset](https:\/\/www.kaggle.com\/qingxiangyan\/covid19-data-for-modeling). This dataset is used for model building.  ","c1923c99":"**Bag of word**","94f6f687":"## b Classifying Remaining Articles with Tf-idf Adjusted MLP Model\n\nAfter identify the best model, i.e., the MLP model on Tf-idf adjusted uni- and bi-gram, we are now ready to make predictions. Before that, we will first re-train our model on the training and test combined dataset (n = 352). ","38e718b5":"### I Vectorization","5758f617":"## b Tokenization, Lemmatization, and 2-grams:","7e7863cf":"**Create a column for 2-grams:**\n","f723d373":"# 5 Visualizing LDA Clustering","a397d44a":"**LDA clustering:**","885c89a9":"# 4 Clustering Articles by Risk Factors using LDA","904ead04":"**Prepare dataset:**","8d7c9966":"**Import nucessary packages:**","7018e74a":"## Authors:\n\n* **Anthony Mazzulli** (4278070) - anthony.mazzulli@roche.com\n* **Ju Zhang** (177660) - ju.zhang.jz1@roche.com\n* **Mark Yan** (4742456) - mark.yan@roche.com\n* **Yanling Jin** (1908340) - yanling.jin@roche.com","bbc79c63":"**Building dictionary and corpus:**","0e209a34":"**Lemmatize tokens:**","57867523":"**Model Building:**","6f8ed761":"# 1 Loading the Data:","46709677":"**Filter on articles that talk about coronavirus or COVID only:**","5c22ff83":"**Build Final Model:**","63dd6d42":"**Calculate perplexity and coherence score:**","1049ccb4":"During the model building phase, we have tried different ways of engineering features from text:\n1. Bag of words (unigram)\n2. A mix of unigram and bigram (unigram + bigram) \n3. Tf-idf weighted unigram and unigram + bigram \n\nDifferent classification methods we have tried are: \n1. Logistic regression\n2. Random forrest\n3. Multi-layer Perceptron classifier \n\nModels will be trained in the training set and their performances will be compared in the test set. ","073972c0":"**Training performance:**","10ec47a6":"## a Filtering and Cleaning:","557d87e0":"## a Testing Models","a227ccd5":"**Read in data and get training and test samples:**  \n\nThe variable \"risk-factor\" contains the annotation label ('Yes' = risk-factor related, and 'No' = otherwise). And the variable \"split_label\" contains the training\/test identifier. ","346f0aac":"### II Logistic Regression","c52746a2":"# COVID-19 Risk Factors Literature Clustering","1ac8da3c":"## Summary:\n\n**MLP with Tf-idf adjusted uni- and bi-gram model performance the best. Its accuracy on the test dataset reached 87.3%. LDA clustering results, based on clustering targeted risk-factor groups, showed promising results.** Some clusters we identified are directly associated with the key questions of interests: \n\n* Data on potential risks factors\n  * Smoking, pre-existing pulmonary disease  ---- ***Cluster 8***\n  * Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n  * Neonates and pregnant women ---- ***Cluster 9***\n  * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n* Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors  ---- ***Cluster 4***\n* Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups    ---- ***Cluster 2 and 5***\n* Susceptibility of populations  ---- ***Cluster 3***\n* Public health mitigation measures that could be effective for control   ---- ***Cluster 3 and 8***","dd7a510d":"**Filter on Years 2019, 2020:**","be927c3c":"**Model Summary:**\n\nThe best model is MLP on Tf-idf adjusted uni- and bi-gram. Its accuracy on the test dataset is 87.3%. ","dfd66b7e":"**Model building:**","228b59ef":"**Re-train MLP model on the training\/test combined dataset:**","fb119e3f":"## Approach:\n\n* Build a supervised machine learning model to predict whether a given article discusses COVID-19 risk factors\n    * Data processing\n        * Selected articles published within 2019-2020\n        * Selected articles with keywords \u201ccoronavirus\u201d or \u201cCOVID\u201d\n        * Tokenized abstracts\n        * Removed punctuation and numbers\n        * Transformed words to lowercase\n        * Removed stop words\n        * Created bag of words and 2-grams\n    * Manually label a subset of 352 articles based on whether they discuss risk factors in their abstracts\n        * Separated the articles into training (N=281) and testing (N=71)\n    * Train different models using the 352 labelled articles to predict whether the rest of the papers discuss risk factors and label those articles accordingly, select the best performed model to conduct clustering, and re-train the model with labelled data\n        * Selected models include\n            * Logistic regression\n            * Random forest\n            * Multi-layer perceptron classifier\n            * Tf-idf adjusted model\n\n\n* Use LDA clustering, an unsupervised machine learning model, to cluster risk-factor related articles based on discussed risk factor categories and visualize the top salient terms\n    * Targeted risk-factor categories include\n        * Data on potential risks factors\n            * Smoking, pre-existing pulmonary disease\n            * Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities\n            * Neonates and pregnant women\n            * Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\n        * Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\n        * Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\n        * Susceptibility of populations\n        * Public health mitigation measures that could be effective for control","76ab9905":"### III Random Forest","0f9f0b6a":"**Create a bag of words for each abstract:**","6fea5e37":"### IV MLP","bb829c22":"**Classify the remaining abstracts:**","0ca014c2":"**Model Building:**","d1e5d805":"**View file directory:**","aa8f1a3c":"## Future steps for model performance improvement:\n\n* Remove common words, but not related to the risk-factors of COVID-19, and re-train the model \n    * E.g., name of months, 'study' , 'year' from the datasets\n* Deliberately include rare articles of interest into the model training process to help program pick up the rare cases more accurately, instead of random sampling\n    * E.g., economic impact of COVID-19","b8aaabef":"## Table of Contents:\n\n<p> 1 Loading Data <\/p>\n<p> 2 Data pre-processing <\/p>\n<p style=\"text-indent: 40px\"> a Filtering and cleaning <\/p>\n<p style=\"text-indent: 40px\"> b Tokenization, Lemmatization, and 2-grams  <\/p>\n<p style=\"text-indent: 40px\"> c Manual Annotation  <\/p>\n3 Classifying Articles - Risk or No Risk <\/p>\n<p style=\"text-indent: 40px\"> a Testing Models <\/p>\n<p style=\"text-indent: 80px\"> i Vectorization <\/p>\n<p style=\"text-indent: 80px\"> ii Logistic Regression <\/p>\n<p style=\"text-indent: 80px\"> iii Random Forest <\/p>\n<p style=\"text-indent: 80px\"> iv MLP <\/p>\n<p style=\"text-indent: 80px\"> v Tf-idf Adjusted Models <\/p>\n<p style=\"text-indent: 40px\"> b Classifying Remaining Articles with Tf-idf Adjusted MLP Model <\/p>\n<p>4 Clustering Articles by Risk Factors using LDA <\/p>\n<p>5 Visualizing LDA Clustering <\/p>","53920b1a":"**Uni- and bi-gram adjusted by Tf-idf weighting:**","4aaf8f80":"**Tokenize abstracts into list of lowercase words:**","0c7a546c":"## Objective:\n\n**Identify available literature on risk factors and cluster risk papers based on risk factors discussed.**","280bc8d7":"# 2 Data Pre-Processing:","2e98de7c":"# 3 Classifying Articles - Risk or No Risk ","09c44316":"**Import Metadata:**","8ddbb77c":"**Remove stop words:**","fee68b38":"**Remove word 'abstract' from token list (many abstracts begin with this word)**"}}