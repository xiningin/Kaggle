{"cell_type":{"88802df8":"code","3f9ef428":"code","96696c02":"code","4fbbaada":"code","5d9d8268":"code","1ee9883a":"code","2b111ca0":"code","a2429362":"code","bc2f758d":"code","7c72841e":"code","d7b7603c":"code","30eb5f2d":"code","2778b85a":"code","d2b3720d":"code","3eb1cb1d":"code","b0319a7c":"code","b8c90062":"code","7dc0976f":"code","336a0fb4":"code","f8fc2747":"code","c0f74b8b":"code","8f2cf1fa":"code","74d50902":"markdown","07419549":"markdown","6e64d0c7":"markdown","ac8b0537":"markdown","cb14f832":"markdown","913c2b2b":"markdown","bd1c74dc":"markdown","a39905fe":"markdown"},"source":{"88802df8":"import os \nprint(os.listdir(\"..\/input\"))","3f9ef428":"import torch\nimport torch.nn.functional as F\nfrom torchvision import datasets,transforms\nfrom torch import nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.utils as vutils","96696c02":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","4fbbaada":"# Dowload the dataset\n# from torchvision.datasets.utils import download_url \n# dataset_url = \"http:\/\/files.fast.ai\/data\/cifar10.tgz\" \n# download_url(dataset_url, '.') \n# import tarfile \n# Extract from archive \n# with tarfile.open('.\/cifar10.tgz', 'r:gz') as tar: \n#     tar.extractall(path='.\/data')","5d9d8268":"transform_train = transforms.Compose([transforms.Resize((32,32)),  #resises the image so it can be perfect for our model.\n                                      #transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n                                      #transforms.RandomRotation(10),     #Rotates the image to a specified angel\n                                      #transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n                                      #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n                                      transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n                                      #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) #Normalize all the images\n                                      ])\ntransform = transforms.Compose([transforms.Resize((32,32)),\n                               transforms.ToTensor(),\n                               #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                               ])\ntraining_dataset = datasets.CIFAR10(root='.\/data', train=True, download=True,transform=transform_train) # Data augmentation is only done on training images\nvalidation_dataset = datasets.CIFAR10(root='.\/data', train=False, download=True,transform=transform)\nprint(training_dataset,validation_dataset)\n","1ee9883a":"# !pip install d2lzh\n# import d2lzh as d2l \n# import os \n# import pandas as pd \n# import shutil \n# import time","2b111ca0":"# demo = True \n# if demo:     \n#     import zipfile     \n#     for f in ['train_tiny.zip', 'test_tiny.zip', 'trainLabels.csv.zip']:         \n#         with zipfile.ZipFile('..\/data\/kaggle_cifar10\/' + f, 'r') as z:             \n#             z.extractall('..\/data\/kaggle_cifar10\/')","a2429362":"training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=100, shuffle=True) # Batch size of 100 i.e to work with 100 images at a time \nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 100, shuffle=False)","bc2f758d":"training_loader.__iter__().__next__()[0].shape","7c72841e":"# We need to convert the images to numpy arrays as tensors are not compatible with matplotlib.\ndef im_convert(tensor):     \n    image = tensor.cpu().clone().detach().numpy() # This process will happen in normal cpu.\n    image = image.transpose(1, 2, 0)   \n#     image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))   \n#     image = image.clip(0, 1)   \n    return image\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n","d7b7603c":"# We iter the batch of images to display \ndataiter = iter(training_loader) # converting our train_dataloader to iterable so that we can iter through it.\nimages, labels = dataiter.next() #going from 1st batch of 100 images to the next batch \nprint(images.shape)\nfig = plt.figure(figsize=(25, 4))   # We plot 20 images from our train_dataset \n# plt.imshow(np.transpose(vutils.make_grid(images, padding=2, normalize=True),(1,2,0)))\n# plt.show()\nfor idx in np.arange(20):   \n    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])    \n#     plt.imshow(np.transpose(vutils.make_grid(images, padding=2, normalize=True),(1,2,0)))\n    plt.imshow(images[idx].permute(1,2,0)) #converting to numpy array as plt needs it.\n    ax.set_title(classes[labels[idx].item()]) ","30eb5f2d":"\nclass SKConv(nn.Module):\n    def __init__(self, features, WH, M, G, r, stride=1 ,L=32):\n            \"\"\" Constructor\n            Args:\n                features: input channel dimensionality.\n                WH: input spatial dimensionality, used for GAP kernel size.\n                M: the number of branchs.\n                G: num of convolution groups.\n                r: the radio for compute d, the length of z.\n                stride: stride, default 1.\n                L: the minimum dim of the vector z in paper, default 32.\n            \"\"\"\n            super(SKConv, self).__init__()\n            d = max(int(features\/r), L)\n            self.M = M\n            self.features = features\n            self.convs = nn.ModuleList([])\n            for i in range(M):\n                self.convs.append(nn.Sequential(\n                    nn.Conv2d(features, features, kernel_size=3+i*2, stride=stride, padding=1+i, groups=G),#\u4f7f\u7528\u7ec4\u5377\u79ef\u4ee5\u51cf\u5c11\u53c2\u6570\n                    nn.BatchNorm2d(features),\n                    nn.ReLU(inplace=False)\n                ))\n            # self.gap = nn.AvgPool2d(int(WH\/stride))\n            self.fc = nn.Linear(features, d)\n            self.fcs = nn.ModuleList([])\n            for i in range(M):\n                self.fcs.append(\n                    nn.Linear(d, features)\n                )\n            self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        for i, conv in enumerate(self.convs):\n            fea = conv(x).unsqueeze_(dim=1)\n            if i == 0:\n                feas = fea\n            else:\n                feas = torch.cat([feas, fea], dim=1)\n        fea_U = torch.sum(feas, dim=1)\n        # fea_s = self.gap(fea_U).squeeze_()\n        fea_s = fea_U.mean(-1).mean(-1)\n        fea_z = self.fc(fea_s)\n        for i, fc in enumerate(self.fcs):\n            vector = fc(fea_z).unsqueeze_(dim=1)\n            if i == 0:\n                attention_vectors = vector\n            else:\n                attention_vectors = torch.cat([attention_vectors, vector], dim=1)\n        attention_vectors = self.softmax(attention_vectors)\n        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)\n        fea_v = (feas * attention_vectors).sum(dim=1)\n        return fea_v\n\n\nclass SKUnit(nn.Module):\n    def __init__(self, in_features, out_features, WH, M, G, r, mid_features=None, stride=1, L=32):\n        \"\"\" Constructor\n        Args:\n            in_features: input channel dimensionality.\n            out_features: output channel dimensionality.\n            WH: input spatial dimensionality, used for GAP kernel size.global adaptive pool\n            M: the number of branchs.\n            G: num of convolution groups.\n            r: the radio for compute d, the length of z.\n            mid_features: the channle dim of the middle conv with stride not 1, default out_features\/2.\n            stride: stride.\n            L: the minimum dim of the vector z in paper.\n        \"\"\"\n        super(SKUnit, self).__init__()\n        if mid_features is None:\n            mid_features = int(out_features\/2)\n        self.feas = nn.Sequential(\n            nn.Conv2d(in_features, mid_features, 1, stride=1), #1*1\u5377\u79ef\n            nn.BatchNorm2d(mid_features),\n            SKConv(mid_features, WH, M, G, r, stride=stride, L=L),\n            nn.BatchNorm2d(mid_features),\n            nn.Conv2d(mid_features, out_features, 1, stride=1),\n            nn.BatchNorm2d(out_features)\n        )\n        if in_features == out_features: # when dim not change, in could be added diectly to out\n            self.shortcut = nn.Sequential()\n        else: # when dim not change, in should also change dim to be added to out\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_features, out_features, 1, stride=stride),\n                nn.BatchNorm2d(out_features)\n            )\n    \n    def forward(self, x):\n        fea = self.feas(x)\n        return fea + self.shortcut(x)\n\n\nclass SKNet(nn.Module):\n    def __init__(self, class_num):\n        super(SKNet, self).__init__()\n        self.basic_conv = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.BatchNorm2d(64)\n        ) # 32x32\n        self.stage_1 = nn.Sequential(\n            SKUnit(64, 256, 32, 2, 8, 2, stride=1),\n            nn.ReLU(),\n            SKUnit(256, 256, 32, 2, 8, 2),\n            nn.ReLU(),\n            SKUnit(256, 256, 32, 2, 8, 2),\n            nn.ReLU()\n        ) # 32x32\n        self.stage_2 = nn.Sequential(\n            SKUnit(256, 512, 32, 2, 8, 2, stride=2),\n            nn.ReLU(),\n            SKUnit(512, 512, 32, 2, 8, 2),\n            nn.ReLU(),\n            SKUnit(512, 512, 32, 2, 8, 2),\n            nn.ReLU()\n        ) # 16x16\n        self.stage_3 = nn.Sequential(\n            SKUnit(512, 1024, 32, 2, 8, 2, stride=2),\n            nn.ReLU(),\n            SKUnit(1024, 1024, 32, 2, 8, 2),\n            nn.ReLU(),\n            SKUnit(1024, 1024, 32, 2, 8, 2),\n            nn.ReLU()\n        ) # 8x8\n        self.pool = nn.AvgPool2d(8)\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, class_num),\n            # nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        fea = self.basic_conv(x)\n        fea = self.stage_1(fea)\n        fea = self.stage_2(fea)\n        fea = self.stage_3(fea)\n        fea = self.pool(fea)\n        fea = torch.squeeze(fea)\n        fea = self.classifier(fea)\n        return fea\n\n","2778b85a":"model = SKNet(10).to(device) # run our model on cuda GPU for faster results\nmodel","d2b3720d":"criterion = nn.CrossEntropyLoss().cuda()# same as categorical_crossentropy loss used in Keras models which runs on Tensorflow\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.001) # fine tuned the lr","3eb1cb1d":"epochs = 15\nrunning_loss_history = []\nrunning_corrects_history = []\nval_running_loss_history = []\nval_running_corrects_history = []\n\nfor e in range(epochs): # training our model, put input according to every batch.\n    running_loss = 0.0\n    running_corrects = 0.0\n    val_running_loss = 0.0\n    val_running_corrects = 0.0\n  \n    for inputs, labels in training_loader:\n        inputs = inputs.to(device) # input to device as our model is running in mentioned device.\n        labels = labels.to(device)\n        outputs = model(inputs) # every batch of 100 images are put as an input.\n        loss = criterion(outputs, labels) # Calc loss after each batch i\/p by comparing it to actual labels. \n    \n        optimizer.zero_grad() #setting the initial gradient to 0\n        loss.backward() # backpropagating the loss\n        optimizer.step() # updating the weights and bias values for every single step.\n    \n        _, preds = torch.max(outputs, 1) # taking the highest value of prediction.\n        running_loss += loss.item()\n        running_corrects += torch.sum(preds == labels.data) # calculating te accuracy by taking the sum of all the correct predictions in a batch.\n\n\n    with torch.no_grad(): # we do not need gradient for validation.\n        for val_inputs, val_labels in validation_loader:\n            val_inputs = val_inputs.to(device)\n            val_labels = val_labels.to(device)\n            val_outputs = model(val_inputs)\n            val_loss = criterion(val_outputs, val_labels)\n\n            _, val_preds = torch.max(val_outputs, 1)\n            val_running_loss += val_loss.item()\n            val_running_corrects += torch.sum(val_preds == val_labels.data)\n      \n    epoch_loss = running_loss\/len(training_loader) # loss per epoch\n    epoch_acc = running_corrects.float()\/ len(training_loader) # accuracy per epoch\n    running_loss_history.append(epoch_loss) # appending for displaying \n    running_corrects_history.append(epoch_acc)\n    \n    val_epoch_loss = val_running_loss\/len(validation_loader)\n    val_epoch_acc = val_running_corrects.float()\/ len(validation_loader)\n    val_running_loss_history.append(val_epoch_loss)\n    val_running_corrects_history.append(val_epoch_acc)\n    print('epoch :', (e+1))\n    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))\n\n","b0319a7c":"plt.style.use('ggplot')\nplt.plot(running_loss_history, label='training loss')\nplt.plot(val_running_loss_history, label='validation loss')\nplt.legend()","b8c90062":"plt.style.use('ggplot')\nplt.plot(running_corrects_history, label='training accuracy')\nplt.plot(val_running_corrects_history, label='validation accuracy')\nplt.legend()","7dc0976f":"import PIL.ImageOps","336a0fb4":"import requests\nfrom PIL import Image\n\nurl = 'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcT76mSMtKQWGstcqGi-0kPWJyVBqz8RCp8SuplMipkidRY0z9Mc&usqp=CAU'\nresponse = requests.get(url, stream = True)\nimg = Image.open(response.raw)\nplt.imshow(img)","f8fc2747":"img = transform(img)  # applying the transformations on new image as our model has been trained on these transformations\nplt.imshow(im_convert(img)) # convert to numpy array for plt","c0f74b8b":"image = img.to(device).unsqueeze(0) # put inputs in device as our model is running there\noutput = model(image)\n_, pred = torch.max(output, 1)\nprint(classes[pred.item()])","8f2cf1fa":"\ndataiter = iter(validation_loader)\nimages, labels = dataiter.next()\nimages = images.to(device)\nlabels = labels.to(device)\noutput = model(images)\n_, preds = torch.max(output, 1)\n\nfig = plt.figure(figsize=(25, 4))\n\nfor idx in np.arange(20):\n  ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n  plt.imshow(im_convert(images[idx]))\n  ax.set_title(\"{} ({})\".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))","74d50902":"Transformations and Data Augmentation","07419549":"Input new image from web to check our model's accuracy","6e64d0c7":"Defining our Model","ac8b0537":"Use the images from our validation dataset to check the precdictions","cb14f832":"Fitting our model with the inputs to generate output. \nDisplaying the progress. ","913c2b2b":"Converting the Input images to plot using plt","bd1c74dc":"Initializing GPU Usage","a39905fe":"**As seen the predictions on new images are really good. Maybe by running more epochs I can get better accuracy or some more hyper parameter tuning will help. Additionally I am sure that Transfer learning will yield much better accuracy, if I use Vgg16 for suppose. But for this case I wanted to code my own model. Even though LeNet is a old it is still a very good model to start with.**\n> I have build this on PyTorch but personally I like Tensorflow Keras more. Thats my opinion.\nLet me know the suggestions for hyper parameter tunning for the same model."}}