{"cell_type":{"20ab97aa":"code","e9fca577":"code","97d746ef":"code","7898579a":"code","c7662806":"code","ef4ee20e":"code","66befe01":"code","e4d7bbdf":"code","c8b7666f":"markdown","acfab9ef":"markdown","4d0f4d1c":"markdown","96a1c811":"markdown","ab3704a8":"markdown","7ce28c09":"markdown","bd3f7e9e":"markdown","43f47a83":"markdown","8c179fd1":"markdown","bc16a556":"markdown","0fb0c469":"markdown","99ab8cf2":"markdown","9742ba0d":"markdown","4d70404a":"markdown"},"source":{"20ab97aa":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import preprocessing \nfrom xgboost import XGBRegressor","e9fca577":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_pred = []\nscore = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = encoder.transform(xtest[object_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method=\"gpu_hist\", gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    pred_valid = model.predict(xvalid)\n    test_pred = model.predict(xtest)\n    final_pred.append(test_pred)\n    rmse = mean_squared_error(yvalid, pred_valid, squared=False)\n    print(fold, rmse)\n    score.append(rmse)\nprint(np.mean(score), np.std(score))","97d746ef":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\nfinal_pred = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_pred.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","7898579a":"\ndf = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnum_cols = [col for col in useful_features if 'cont' in col]\ndf_test = df_test[useful_features]\n\n#log transformation\nfor col in num_cols:\n    df[col] = np.log1p(df[col])\n    df_test[col] = np.log1p(df_test[col])\n\nfinal_pred = []\nscore = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = encoder.transform(xtest[object_cols])\n    \n    \n    model = XGBRegressor(random_state=fold, tree_method=\"gpu_hist\", gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    pred_valid = model.predict(xvalid)\n    test_pred = model.predict(xtest)\n    final_pred.append(test_pred)\n    rmse = mean_squared_error(yvalid, pred_valid, squared=False)\n    print(fold, rmse)\n    score.append(rmse)\nprint(np.mean(score), np.std(score))","c7662806":"#normilization\ndf = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\nfinal_pred = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    #normilization\n    \n    normalizer = preprocessing.Normalizer()\n    xtrain[numerical_cols] = normalizer.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = normalizer.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = normalizer.transform(xtest[numerical_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_pred.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","ef4ee20e":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_preds = []\nscore = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[object_cols])\n    xvalid_ohe = ohe.transform(xvalid[object_cols])\n    xtest_ohe = ohe.transform(xtest[object_cols])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    xtest = pd.concat([xtest, xtest_ohe], axis=1)\n    \n    # this part is missing in the video:\n    xtrain = xtrain.drop(object_cols, axis=1)\n    xvalid = xvalid.drop(object_cols, axis=1)\n    xtest = xtest.drop(object_cols, axis=1)\n    # missing part ends\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_preds.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    score.append(rmse)\n\nprint(np.mean(score), np.std(score))","66befe01":"pred = np.mean(np.column_stack(final_preds), axis=1)\nsample_submission.target = pred\nsample_submission.to_csv(\"submission_ohe.csv\", index=False)","e4d7bbdf":"# polynomial features\ndf = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\npoly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df[numerical_cols])\ntest_poly = poly.fit_transform(df_test[numerical_cols])\n\ndf_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ndf = pd.concat([df, df_poly], axis=1)\ndf_test = pd.concat([df_test, df_test_poly], axis=1)\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_pred = []\nscore = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    model = XGBRegressor(random_state=fold, tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_pred.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    score.append(rmse)\n\nprint(np.mean(score), np.std(score))","c8b7666f":"# **OrdinalEncoder**","acfab9ef":"1. **OneHotEncoder there Mean Value is 0.725499**\n1. **OrdinalEncoder, StandardScaler there mean value is 0.725506**\n1. **Log1p Transformation, OrdinalEncoder there Mean Value is 0.725618**\n1. **OrdinalEncoder mean value is 0.725688**\n1. **Polynomial Feature, OrdinalEncoder there Mean Value is 0.728146**\n1. **OrdinalEncoder, Normilization there Mean Value is 0.739602**","4d0f4d1c":"# **OrdinalEncoder, StandardScaler**","96a1c811":"# **Polynomial Feature, OrdinalEncoder**","ab3704a8":"**OneHotEncoder there Mean Value is 0.725499**","7ce28c09":"**Log1p Transformation, OrdinalEncoder there Mean Value is 0.725618**","bd3f7e9e":"# **Log1p Transformation, OrdinalEncoder**","43f47a83":"**OrdinalEncoder, Normilization there Mean Value is 0.739602**","8c179fd1":"**OrdinalEncoder, StandardScaler there mean value is 0.725506**","bc16a556":"# **OrdinalEncoder, Normilization**","0fb0c469":"# **OneHotEncoder**","99ab8cf2":"**Polynomial Feature, OrdinalEncoder there Mean Value is 0.728146**","9742ba0d":"**OrdinalEncoder mean value is 0.725688**","4d70404a":"**Finally OneHotEncoder perform well Compare to other features**"}}