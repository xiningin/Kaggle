{"cell_type":{"5d27e0e3":"code","6a2d821a":"code","48212b44":"code","a81e4728":"code","e5dd62f2":"code","de4a172b":"code","0883d904":"code","10d4367e":"code","af929833":"code","0ced64ee":"code","f87c8964":"code","8b9fe733":"code","e7f01f0a":"code","dfa6d9d4":"code","e2cd71cf":"code","f7e4e192":"code","d2dc4562":"markdown","7c554ce1":"markdown","774ab653":"markdown","018f308d":"markdown","e7b943b1":"markdown","67d6c8c7":"markdown","f74213da":"markdown","9cc9743c":"markdown","30bcbf4d":"markdown","ec134417":"markdown","af4887bf":"markdown","ccb79d86":"markdown","8f6f337e":"markdown","1ae95933":"markdown","0c50f01a":"markdown","49ac513e":"markdown","1223c5bd":"markdown","b3a888d9":"markdown"},"source":{"5d27e0e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n\ntrain_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","6a2d821a":"null_columns = [c for c in train_data.columns if train_data[c].isnull().sum() > (train_data.shape[0]\/2)]\nprint(null_columns)\nX_train = train_data.drop(null_columns, axis = 1)\nX_test = test_data.drop(null_columns, axis = 1)","48212b44":"X_train = X_train.drop(['Id'], axis = 1)","a81e4728":"X_num_col = X_train.select_dtypes(include=['int64','float'])\nX_cat_col = X_train.select_dtypes(include=['object'])","e5dd62f2":"X_num_col.hist(figsize=(20, 20), bins=100, xlabelsize=8, ylabelsize=8);","de4a172b":"import seaborn as sns\nfor i in range(0, len(X_train.columns), 5):\n    sns.pairplot(data=X_train,\n                x_vars=X_train.columns[i:i+5],\n                y_vars='SalePrice')","0883d904":"y = X_num_col['SalePrice']\nX_num_col.drop(['SalePrice'],axis=1, inplace=True)","10d4367e":"X_num_col.fillna(X_num_col.mean(), inplace=True)\n\nX_test[X_num_col.columns].fillna(X_test[X_num_col.columns].mean(), inplace=True)","af929833":"X_cat_col.fillna('NA', inplace=True)\nX_test[X_cat_col.columns].fillna('NA', inplace=True)","0ced64ee":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (20,16))\nsns.heatmap(X_num_col.corr(), annot=True)","f87c8964":"X_num_col['GrLivArea'] = (X_num_col['GrLivArea'] \/ X_num_col['TotRmsAbvGrd'])\nX_num_col.drop(['TotRmsAbvGrd'], axis = 1, inplace=True)\n\nX_num_col['GarageArea'] = X_num_col['GarageArea'] + X_num_col['GarageCars']\nX_num_col.drop(['GarageCars'], axis = 1, inplace=True)\n\nX_test['GrLivArea'] = X_test['GrLivArea'] \/ X_test['TotRmsAbvGrd']\nX_test['GarageArea'] = X_test['GarageArea'] + X_test['GarageCars']\nX_test.drop(['TotRmsAbvGrd', 'GarageCars'], axis=1, inplace=True)\n","8b9fe733":"Xt = pd.get_dummies(pd.concat([X_num_col, X_cat_col],axis=1, sort=False))\nXtst = pd.get_dummies(X_test)\nX_tr, X_test = Xt.align(Xtst, join='left', axis=1)","e7f01f0a":"from sklearn.model_selection import train_test_split\nX_tr, X_val, y_tr, y_val = train_test_split(Xt, y, train_size=0.8, test_size=0.2, random_state=0)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\nlin_model = LinearRegression()\nlin_model.fit(X_tr, y_tr)\nlin_pred = lin_model.predict(X_val)\nlin_mean = mean_absolute_error(y_val, lin_pred)\nlin_mean","dfa6d9d4":"from xgboost import XGBRegressor\n\nxbr_model = XGBRegressor(n_estimators=500, learning_rate=0.06, random_state=5) \nxbr_model.fit(X_tr, y_tr)\nxbr_preds = xbr_model.predict(X_val)\nxbr_mea = mean_absolute_error(y_val, xbr_preds)\nxbr_mea","e2cd71cf":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor(n_estimators=1250, learning_rate=0.04) \ngbr_model.fit(X_tr, y_tr)\ngbr_pred = gbr_model.predict(X_val)\ngbr_mean = mean_absolute_error(y_val, gbr_pred)\ngbr_mean","f7e4e192":"test_preds= xbr_model.predict(X_test)\noutput = pd.DataFrame({'Id': test_data['Id'],\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)","d2dc4562":"![](http:\/\/)**8. Find correlation and combine the highly co-related columns**","7c554ce1":"**5. Numerical Data and its correlation with Sale Price**","774ab653":"* Hist Plot","018f308d":"* Combining the highly correlated columns based on the distribution (Let's see what happens ,,,)","e7b943b1":"**10. Measure the model fit against multiple models**","67d6c8c7":"**A naive approach to Advanced Housing Price Competition**","f74213da":"1.  **Import the module**","9cc9743c":"**3. Removing the not-needed ID column**","30bcbf4d":"**6. Fill the empty values (NaN) in the numerical columns**","ec134417":"Conclusion: We'll keep all the above columns. Eeeeeeee","af4887bf":"* Pair Plot for all cols","ccb79d86":"**I have been following few of the notebooks in the Housing Price competetion. And this notebook is a key-takeway (or you can call it ensemble lol!) of those approaches.**","8f6f337e":"**7. Let's try something different. We'll make a new category for every categorical column as 'NA'**","1ae95933":"**4. Separte the Categorical and numerical columns**","0c50f01a":"* Remove the SalePrice and assign it to Y","49ac513e":"**11. Predict the Sale Price using the best fit (XGBBRegressor) model**","1223c5bd":"**9. Encode the Categorical features**","b3a888d9":"![](http:\/\/)**2. Study the columns and remove the empty columns**"}}