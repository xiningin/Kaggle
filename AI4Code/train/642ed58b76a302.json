{"cell_type":{"2e7319b4":"code","eb996351":"code","17f44c69":"code","f880358e":"code","9e619be5":"code","e7f2d120":"code","df866065":"code","f6f17994":"code","d51dab32":"code","e9fddb59":"code","dbea945a":"code","e74d17b2":"code","6b168bed":"code","39973e35":"markdown","e89a58fd":"markdown","3c669c6b":"markdown","3f5789f5":"markdown","fee1a28c":"markdown"},"source":{"2e7319b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb996351":"train_set = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nsub_set = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","17f44c69":"train_set.columns, sub_set.columns","f880358e":"X = train_set.text.values\nXsub = sub_set.text.values\ny = train_set.target.values\nXall = np.concatenate([X, Xsub], axis=0)","9e619be5":"import nltk; from nltk.corpus import stopwords; from nltk.stem.porter import PorterStemmer\nnltk.download('stopwords'); all_stopwords = stopwords.words('english'); all_stopwords.remove('not')\nimport re; from sklearn.feature_extraction.text import CountVectorizer; corpus = []\nfor i in range(0, Xall.shape[0]):\n    stemmer = PorterStemmer()\n    tweet = re.sub('[^a-zA-Z]', ' ', Xall[i])\n    tweet = tweet.lower()\n    tweet = tweet.split()\n    tweet = [stemmer.stem(word) for word in tweet if not word in set(all_stopwords)]\n    tweet = ' '.join(tweet)\n    corpus.append(tweet)\nvectorizer = CountVectorizer(max_features = 3000)\ntextColumn = vectorizer.fit_transform(corpus).toarray()","e7f2d120":"X = textColumn[:y.shape[0], :]\nXsub = textColumn[y.shape[0]:, :]","df866065":"\"\"\"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 777)\n\nann = tf.keras.models.Sequential(layers=[\n    tf.keras.layers.Dense(100, 'relu'),\n    tf.keras.layers.Dropout(.86),\n    tf.keras.layers.Dense(135, 'linear'),\n    \n    tf.keras.layers.Dense(1, 'sigmoid')\n])\nann.compile(optimizer='nadam', loss='binary_crossentropy', metrics=['accuracy'])\"\"\"","f6f17994":"\"\"\"def lr_sch(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\nhistory = ann.fit(X_train, y_train,\n       epochs=100, batch_size=200,\n       validation_data=(X_test, y_test),\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8, restore_best_weights=True),\n            tf.keras.callbacks.LearningRateScheduler(lr_sch)\n        ]\n)\"\"\"","d51dab32":"\"\"\"loss = history.history['loss']\nacc = history.history['accuracy']\nvloss = history.history['val_loss']\nvacc = history.history['val_accuracy']\n\nfig, axs = plt.subplots(2, 2, figsize=(20, 20))\naxs[0, 0].plot(loss, color='red', label='loss')\naxs[1, 0].plot(vloss, color='orange', label='vloss')\naxs[0, 0].legend()\naxs[1, 0].legend()\n\naxs[0, 1].plot(acc, color='blue', label='acc')\naxs[1, 1].plot(vacc, color='cyan', label='vacc')\naxs[0, 1].legend()\naxs[1, 1].legend()\nplt.show()\"\"\"","e9fddb59":"\"\"\"from sklearn.metrics import accuracy_score, f1_score\ny_pred = ann.predict(X_test)>=.5\nprint(\"acuracy score:\", accuracy_score(y_test, y_pred))\nprint(\"f1 score:\", f1_score(y_test, y_pred))\"\"\"","dbea945a":"\"\"\"from sklearn.svm import SVC, NuSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB, ComplementNB, GaussianNB, MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 777)\n\nclassifiers = [('nusvc', NuSVC()), ('svc', SVC()), ('logistic', LogisticRegression()), ('benoulli', BernoulliNB()), ('complement', ComplementNB()), ('gaussian', GaussianNB()), ('m\u0131ltinomial', MultinomialNB())]\nfor name, classifier in classifiers:\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    print((name, accuracy_score(y_test, y_pred), '\\n', confusion_matrix(y_test, y_pred)))\"\"\"","e74d17b2":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(X, y)\nypred = model.predict(Xsub)","6b168bed":"submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsubmission.iloc[:, 1] = ypred\nsubmission.to_csv('submission.csv', index=False)","39973e35":"---","e89a58fd":"### these TF model and parameters give approximately the same results to SVC model, but overfits really fast;","3c669c6b":"---","3f5789f5":"---","fee1a28c":"---"}}