{"cell_type":{"aab0e250":"code","6ca67dd3":"code","07339c85":"code","e3a14037":"code","4f9d78d8":"code","1db6d717":"code","63f9e798":"code","10db5ef3":"code","17a4770f":"code","517cb9a9":"code","e8c25d63":"code","5a5961d0":"code","6473e02e":"code","a415b0fa":"code","69b1759d":"code","e300f7b7":"code","e54374ee":"code","6c251644":"markdown","54eb8885":"markdown","6c79bc2f":"markdown","23b96219":"markdown","4b6fee25":"markdown","21535566":"markdown","887a55ac":"markdown","597477d7":"markdown","a450d6fa":"markdown","d7358ede":"markdown","e4ebe7be":"markdown","1c964aad":"markdown","e3be666e":"markdown","e74ebc58":"markdown","e0407914":"markdown","85baf439":"markdown","882bd61c":"markdown","1bb466a8":"markdown","1eb2d634":"markdown","fa8975a5":"markdown"},"source":{"aab0e250":"import catboost\nimport optuna\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\nfrom sklearn.metrics import balanced_accuracy_score, make_scorer\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\n\nfrom catboost import CatBoostClassifier, Pool, EShapCalcType, EFeaturesSelectionAlgorithm","6ca67dd3":"import warnings\nwarnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\npd.options.mode.chained_assignment = None\n\nnp.random.seed(0)\n\nsns.set_style(\"dark\")","07339c85":"df = pd.read_csv('..\/input\/housepricingtg2021\/train.csv')\n\ndf","e3a14037":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.barplot(x=\"TARGET\", y=\"bedrooms\", data=df, ax=ax[0]);\nsns.barplot(x=\"TARGET\", y=\"bedrooms\", data=df, ax=ax[1]);","4f9d78d8":"fig, ax = plt.subplots(figsize=(15, 5))\n\nsns.barplot(x=\"TARGET\", y=\"price\", data=df, ax=ax);","1db6d717":"fig, ax = plt.subplots(1, 1, figsize=(15, 10))\nN = 10000\nax.scatter(df['longitude'].values[:N], df['latitude'].values[:N],\n           color='blue', s=1, label='train', alpha=0.1)\n\nfig.suptitle('Houses locations')\nax.legend(loc=0)\nax.set_ylabel('latitude')\nax.set_xlabel('longitude')\nplt.ylim((40.65, 40.9))\nplt.xlim((-74.05, -73.8))\nplt.show()","63f9e798":"class BaseExtracter(BaseEstimator, TransformerMixin):\n    def __init__(self, column, extraction_function):\n        self.extraction_function = extraction_function\n        self.column = column\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X_ = X.copy()\n        X_[self.column] = X_[self.column].apply(self.extraction_function) \n        return X_\n    \nclass BaseMultiLabelBinarizer(BaseEstimator, TransformerMixin):\n    def __init__(self, column):\n        self.mlb = MultiLabelBinarizer()\n        self.column = column\n    \n    def fit(self, X, y=None):\n        self.mlb.fit(X[self.column])\n        return self\n    \n    def transform(self, X, y=None):\n        X_ = X.copy()\n        encoded = pd.DataFrame(self.mlb.transform(X_[self.column]), columns=self.mlb.classes_)\n        X_ = pd.concat([X_.reset_index(), encoded], axis=1)  \n\n        X_.drop(columns=['index', self.column], inplace=True)\n\n        return X_","10db5ef3":"class PandasSimpleImputer(SimpleImputer):\n    def fit(self, X, y=None):\n        self.columns = X.columns\n        return super().fit(X, y)\n\n    def transform(self, X):\n        return pd.DataFrame(super().transform(X), columns=self.columns)","17a4770f":"class GenerateMoreFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        coords = np.vstack((X[['latitude', 'longitude']].values))\n        self.pca = PCA().fit(coords)\n        return self\n    \n    def transform(self, X, y=None):\n        X_ = X.copy()\n\n        X_[f'pca{0}'] = self.pca.transform(X_[['latitude', 'longitude']])[:, 0]\n        \n        \"\"\"Price per bedroom and bathroom\"\"\"\n        \n        X_['price_per_bedroom']  = X_[\"price\"] \/ X_[\"bedrooms\"]\n        X_[\"price_per_bathroom\"] = X_[\"price\"] \/ X_[\"bathrooms\"]\n        X_['price_per_bedroom'][X_['price_per_bedroom'] == np.Inf] = 0\n        X_['price_per_bathroom'][X_['price_per_bathroom'] == np.Inf] = 0\n        X_.drop(columns=['bedrooms','bathrooms'], inplace=True)\n        \n        \"\"\"Seller and Building popularity\"\"\"\n        \n        building_ids = X_['building_id'].value_counts()\n        manager_ids = X_['manager_id'].value_counts()\n        X_['manager_ids_count'] = X_['manager_id'].apply(lambda x: manager_ids[x] if x in manager_ids else 0)\n        X_['building_ids_count'] = X_['building_id'].apply(lambda x: building_ids[x] if x in building_ids else 0)\n        X_.drop(columns=['manager_id','building_id'], inplace=True)\n        \n        \"\"\"Date and time\"\"\"\n        \n        X_[\"created\"] = X_[\"created\"].astype(\"datetime64\")\n        X_['Weekday'] = X_.created.dt.weekday\n        X_['day_of_month'] = X_.created.dt.day\n        X_['hour'] = X_.created.dt.hour\n        X_['is_weekend'] = X_.created.apply(lambda x: 1 if x.date().weekday() in (5, 6) else 0)\n        X_['month'] = X_.created.dt.month\n        X_['week'] = X_.created.dt.isocalendar().week\n        X_['minute'] = X_['created'].dt.minute\n        X_['pickup_week_hour'] = X_['Weekday'] * 24 + X_['hour']\n        \n        timestamp = pd.Timestamp('2016-06-29 18:30:41')\n        \n        X_['days_since_last'] = X_.created.apply(lambda x: (timestamp - x).days)\n\n        X_.drop(columns=['created'], inplace=True)\n        \n        return X_","517cb9a9":"features = make_pipeline(PandasSimpleImputer(strategy='constant', fill_value='xxx'),\n                         BaseExtracter('features', lambda x: x[1:-1].replace(\"'\", \"\").replace(\" \", \"\").split(',')),\n                         BaseMultiLabelBinarizer('features'),\n                         VarianceThreshold(threshold=0.01))\n\ndisplay_address = make_pipeline(PandasSimpleImputer(strategy='constant', fill_value='xxx'),\n                                BaseExtracter('display_address', lambda x: x.split(' ')),\n                                BaseMultiLabelBinarizer('display_address'),\n                                VarianceThreshold(threshold=0.01))\n\nstreet_address= make_pipeline(PandasSimpleImputer(strategy='constant', fill_value='xxx'),\n                              BaseExtracter('street_address', lambda x: x.split(' ')),\n                              BaseMultiLabelBinarizer('street_address'),\n                              VarianceThreshold(threshold=0.01))\n\nspecial_features = make_pipeline(GenerateMoreFeatures(), StandardScaler())\n\npolynomial_features = make_pipeline(PolynomialFeatures(2), StandardScaler())\n\ncolumns = ColumnTransformer(transformers=[\n        ('Special Features', special_features, ['price', 'bedrooms', 'bathrooms', \n                                                'building_id', 'manager_id', 'created',\n                                                'latitude', 'longitude', 'listing_id']),\n        ('Polynomial Features', polynomial_features, ['price', 'bedrooms', 'bathrooms',\n                                                      'latitude', 'longitude', 'listing_id']),\n        ('Features', features, ['features']),\n        ('Display Address', display_address, ['display_address']),\n        ('Street Address', street_address, ['street_address']),\n])","e8c25d63":"X, y = df.loc[:, df.columns != 'TARGET'], df.loc[:, 'TARGET']\n\nle = LabelEncoder()\n\ny = le.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)","5a5961d0":"X_train = columns.fit_transform(X_train);\nX_test  = columns.transform(X_test)\n\nfeatures_num = X_train.shape[1]\n\nprint(f'Generated {features_num} features')","6473e02e":"classes = np.unique(y_train)\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\nclass_weights = dict(zip(classes, weights))\n\nclass_weights","a415b0fa":"features_select = 90\n\nclf = CatBoostClassifier(class_weights=class_weights, verbose=0,\n                                    task_type=\"GPU\", devices='0', random_seed=0,\n                                    iterations=2000, depth=5, l2_leaf_reg=4,\n                                    leaf_estimation_method='Newton')\n            \ntrain_pool = catboost.Pool(data=X_train, label=y_train)\ntest_pool = catboost.Pool(data=X_test, label=y_test)\n\nsummary = clf.select_features(train_pool, eval_set=test_pool,\n                              features_for_select=f'0-{features_num - 1}',\n                              num_features_to_select=features_select,steps=5,\n                              logging_level='Silent',\n                              algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n                              shap_calc_type=EShapCalcType.Regular, train_final_model=False);\n\nselected_features = summary['selected_features']\n\nX_train_subset = X_train[:, selected_features]\nX_test_subset  = X_test[:, selected_features]","69b1759d":"def objective(trial):\n    depth = trial.suggest_int(\"rf_max_depth\", 3, 6, log=True)\n    l2_leaf_reg = trial.suggest_int(\"l2_leaf_reg\", 2, 30, log=True)\n    iterations = trial.suggest_discrete_uniform(\"iterations\", 2000, 3000, q=500)\n\n    all_scores = []\n    for seed in np.random.randint(0, 100, size=(5)):\n        clf = CatBoostClassifier(class_weights=class_weights, verbose=0, \n                                 task_type=\"GPU\", devices='0', random_seed=seed,\n                                 iterations=iterations, depth=depth,\n                                 l2_leaf_reg=l2_leaf_reg)\n                    \n        clf.fit(X_train_subset, y_train)\n        score = balanced_accuracy_score(y_test, clf.predict(X_test_subset))\n        all_scores.append(score)\n\n    average_score = sum(all_scores) \/ len(all_scores)\n    return average_score\n                \n\nsampler = optuna.samplers.TPESampler(seed=0)\nstudy = optuna.create_study(sampler=sampler, direction='maximize')\nstudy.optimize(objective, n_trials=5)\nstudy.best_params","e300f7b7":"clf = CatBoostClassifier(class_weights=class_weights, verbose=0, \n                                 task_type=\"GPU\", devices='0', random_seed=42,\n                                 iterations=2500, depth=4,\n                                 l2_leaf_reg=5)\n\nclf.fit(X_train_subset, y_train);","e54374ee":"df_test = pd.read_csv('..\/input\/housepricingtg2021\/test.csv')\nID = df_test['Id']\ndf_test = columns.transform(df_test)\nnew_df_test = df_test[:, selected_features]\npreds = clf.predict(new_df_test, prediction_type='Class')\npreds = le.inverse_transform(preds)\noutput = pd.DataFrame({'Id': ID,'TARGET': preds})\noutput.to_csv('submission.csv', index=False)","6c251644":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Feature Selection<\/h1>","54eb8885":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Turn Off Warnings<\/h1>","6c79bc2f":"Now we are ready to split our data and start training","23b96219":"<h1 style='text-align: center'>Predict House Popularity In New York \ud83e\udd47<\/h1>\n\n<p  style='text-align: center'>\nThis notebook is in <span style='color: green; font-weight: 700'>Active<\/span> state of development!\n<a style='font-weight:700' href='https:\/\/github.com\/LilDataScientist'> Code on GitHub! <\/a><\/p>","4b6fee25":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Extra classes for feature engineering<\/h1>","21535566":"Since such columns as **description, display_address, features and street_address** looks like a bunch of strings we would like to gain some information form them, probably one hot them, and for doing it I will introduce **BaseExtracter** class that will extract words from string of strings and **BaseMultiLabelBinarizer** that will one hot all these features!","887a55ac":"Since it is much easier to deal with pandas, we will implement our **PandasSimpleImputer**. The point of doing this was that **sklearn.impute.SimpleImputer** returns numpy array while for this particular task we want our data to be in pandas format","597477d7":"Let's add some Nonlinearity to our model by adding more features.","a450d6fa":"<div style='text-align: center'>\n    <img src='https:\/\/media.istockphoto.com\/photos\/winter-in-manhattan-picture-id1292824324?b=1&k=20&m=1292824324&s=170667a&w=0&h=GpVutoJrAAYP_h_ddXm_hIR1_22WebYn3ym6jz6hRNQ=' width='1000' \/>\n<\/div>","d7358ede":"I came across such common warnings as setting to the copy of pandas dataframe","e4ebe7be":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Train final model and submit to competition<\/h1>","1c964aad":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Pipeline<\/h1>","e3be666e":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Feature Engineering<\/h1>","e74ebc58":"Now we are ready to build a pipeline for our particular problem. For such encoded features as **display_address, features and street_address** we will use **VarianceThreshold** which basically removes elements by variance threshold> If you would skip this turn you would end up having million of features!","e0407914":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Hyperparameters Tuning<\/h1>","85baf439":"Since while some features are good other may not be like that and that's why we need to select the ones which perfoms the best for our model","882bd61c":"Dealing with class imbalance problem","1bb466a8":"Since we are using **Gradient Boosting** we have to choose between different depth for trees, number of iterations and other important hyperparameters","1eb2d634":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Load Dataset<\/h1>","fa8975a5":"<h1 style='background-color: #dae8fc; border: 1px solid #94add0; padding: 10px; font-weight: 400; text-align:center'>Import Dependencies<\/h1>"}}