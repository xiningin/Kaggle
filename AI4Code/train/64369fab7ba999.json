{"cell_type":{"5b8ec9e8":"code","5bd4c036":"code","a98eb1c7":"code","6a00e812":"code","7bbdc7a2":"code","e4d77981":"code","0c846d6b":"code","84833899":"code","c290e00f":"code","27c3b058":"code","a2650d84":"code","f32598d9":"code","9eb2bb22":"code","9a148331":"code","8fd024fd":"code","958f953b":"code","6d448bdf":"code","8b48b97a":"code","e27ec027":"code","ce2af8a4":"code","09f806c5":"markdown","8009ff96":"markdown","bd706a57":"markdown","c1b911d8":"markdown","ea78a93d":"markdown","e1029266":"markdown","86187062":"markdown","c082cd98":"markdown","a77f34c9":"markdown"},"source":{"5b8ec9e8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# For the dataset\n# main module, I will use it to cerate the tensors\nimport torch\n# base class to create a dataset class\nfrom torch.utils.data import Dataset\n# create the datasets for train and test\nfrom torch.utils.data import DataLoader\n\n# for the model\n# the trainable layers (dense layers, convolutional layers ...)\nimport torch.nn as nn # idk why dropout is in nn x_x\n# not trainable layers (dropout, batchnormalization, pooling, ...)\nimport torch.nn.functional as F\n# for the model otimizers\nimport torch.optim as optim\n# \nfrom sklearn.metrics import confusion_matrix","5bd4c036":"# First read the dataset and transform it\nds = pd.read_csv('..\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv')\nprint(ds.shape)\nds.head()","a98eb1c7":"# select the features and the labels from the dataset, ds\n\nfeature_cols = ds.columns[:-2] # all except the last 2\nfeatures = ds[feature_cols].values.astype('float32') # select the values in a float32 type\n\n# I chose to use as target the character column\n\nlabel_cols = ds.columns[-1:] # the last one\nlabels = ds[label_cols].values # these vales are going to be string values\n# the shape of labels is (15000, 1), lets 'delete' this extra dimention\nlabels = np.squeeze(labels)\n\nfeatures.shape, labels.shape","6a00e812":"labels, features","7bbdc7a2":"# test set contains 3000 images\nx_train, x_test, y_train, y_test = train_test_split(\n    features, labels,\n    test_size=0.2,\n    random_state=314,\n    shuffle=True\n)\n# and val set will contain 1000 images from tarin set\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train, y_train,\n    test_size=1\/12,\n    random_state=314,\n    shuffle=True\n)\n\n# yes, the target and data are not processed\ny_test, x_test.shape","e4d77981":"class CustomDataset(Dataset):\n    # data will be a numpy array, in this case: features\n    # labels will be tha characters, in this case: labels\n\n    ######################### THE \"BASIC\" NEEDED FUNCTIONS\n    \n    def __init__(self, data, labels, labels_ids=[]):\n        # process the labels and the data\n        self.process_data(data)\n        self.process_labels(labels, labels_ids) # this labels ids is too important, so\n                                                # read bellow why and check the function\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels_n[idx]\n    \n    ######################### THE PROCESSING FUNCTIONS\n    \n    # function inside the class for process the data\n    def process_data(self, data):\n        # convert data to a torch tensor\n        self.data = torch.from_numpy(data)\n        # normalize the data\n        self.data = self.data\/255\n        # reshape the tensor, keep the batch and chanel dim and reshape the images\n        self.data = self.data.view(self.data.shape[0], 1, 64, 64)\n        \n    # function inside the class for process the target\n    def process_labels(self, labels, labels_ids):\n        # count all the different values in labels\n        self.distrib, self.num_labels = self.__count_values(labels)\n        \n        # verify if there are existant ids !important\n        if labels_ids == []:\n            # set an id for each label if there's no labels\n            self.id_labels = {}\n            for label, i in zip(self.distrib.keys(), range(self.num_labels)):\n                self.id_labels[label] = i\n\n        # if there are existant labels use them\n        else:\n            self.id_labels = labels_ids   \n            \n        # then create an array with the ids\n        ids = [] # will be converted in an array\n        for label in labels:\n            # append the id of the label\n            ids.append(self.id_labels[label])\n        # use one hot encoding for the labels\n        self.labels = []\n        self.labels_n = [] # labels without encoding\n        for i in ids:\n            # append the normal label\n            self.labels_n.append(i)\n            # append the one hot encoded label\n            self.labels.append(self.__one_hot(i))\n        # convert to a numpy array, this is because the final dtype\n        # was torch.float64 and the data is torch.float32\n        self.labels = np.array(self.labels).astype('float32')\n        # and convert to torch tensor\n        self.labels = torch.from_numpy(self.labels)\n        self.labels_n = torch.tensor(self.labels_n, dtype=torch.int64)\n        \n    \n    ######################### EXTRA FUNCTIONS\n    \n    # extra function to count the different items from an array\n    def __count_values(self, arr):\n        dic = {}\n        for val in arr:\n            if val not in dic.keys():\n                dic[val] = 1\n            else:\n                dic[val] += 1\n        return dic, len(dic.keys())\n    \n    # extra function to make one hot encoding for the labels\n    def __one_hot(self, label):\n        res = np.zeros((self.num_labels))\n        res[label]+=1\n        return res\n    \n    # extra function to decode from one hot to numbers\n    def decode(self, labels): # recieve a torch tensor, a prediction\n        # select the indexs of the max elements in each label\n        decoded = torch.argmax(labels, dim=1).numpy()\n        return decoded\n        \n        \n\n#### DATASET CLASS ATRIBUTES: (the ones I use)\n# data: all the tensors (images) with shape (bacth,1,64,64), pytorch tensor\n# labels: all the labels encoded with one hot encoding, pytorch tensor\n# distrib: a dict that contains how many data there are from each label {character: number}\n# id_labels: a dict with and id for each label or character {character:id}\n## as this has to be the same for every dataset it will be passed as a param\n## for the test and val datasets. The data was shuffled, not being passing the dict\n## as a param for the others will cause diferent ids and LOW val and test accuracy.\n# labels_n: a list the labels but without one hot encoding, are the ids from id_labels\n# num_labels: how many different labels or classes there are\n\n\n# use the class for create dataset objects\ntrain_set = CustomDataset(x_train, y_train)\n# the next ones will contain the same ids of train_set\ntest_set = CustomDataset(x_test, y_test, train_set.id_labels)\nval_set = CustomDataset(x_val, y_val, train_set.id_labels)\n\n\n# see some data of the datasets\n# the lengths of each dataset, the dtypes of data and labels, and an example of the labels\nlen(train_set), len(test_set), test_set[0][0].shape, test_set[0][0].dtype, test_set[0][1].dtype, test_set[1000][1]","0c846d6b":"# Lets plot some images using the function of my other notebook\n\n# plot multiple images, preds is for the titles\n# preds must be like [[real, pred]]\ndef plot_images(imgs, dims, figsize, title_size, preds=[]):\n    plt.figure(figsize=figsize)\n    for img, i, in zip(imgs, np.arange(imgs.shape[0])):\n        plt.subplot(dims[0], dims[1], i+1)\n        plt.imshow(np.squeeze(img), cmap='gray')\n        plt.axis('off')\n        title = f'Image {i+1}'\n        if preds != []:\n            title = f'Real: {preds[i][0]}, Pred: {preds[i][1]}'\n        plt.title(title, fontsize=title_size)\n    plt.show()\n\n\n# select the first 10 images of test set\nsample_images = []\nfor i in range(10):\n    img = test_set[i][0] # select the image\n    sample_images.append(img.numpy())\n\n# plot the images\nplot_images(np.array(sample_images), dims=(2,5), figsize=(16,8), title_size=22)","84833899":"# then use the module DataLoader to 'adapt' the datasets for the model\nbatch_size = 16\n\ntrain_loader = DataLoader(\n    dataset=train_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)\n\ntest_loader = DataLoader(\n    dataset=test_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)\n\nval_loader = DataLoader(\n    dataset=val_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)","c290e00f":"num_labels = test_set.num_labels\n\nclass Network(nn.Module):\n    # in the init are going to be defined the layers\n    def __init__(self):\n        super(Network, self).__init__()\n        # define the layers, here the order is not important\n        # CONVOLUTIONAL LAYERS\n        # here there's no input shape, only the number of channels and the\n        # output number of chanels\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n        self.pool1 = nn.MaxPool2d(2)\n        # DENSE LAYERS\n        # the first para is like the input_shape, the seccond the number\n        # of outputs, like the number of neurons\n        self.dense1 = nn.Linear(32*30*30, 256)\n        self.dense2 = nn.Linear(256, 256)\n        self.dense3 = nn.Linear(256, 256)\n        self.dense4 = nn.Linear(256, 15)\n        # DROPOUT LAYER\n        self.dropout = nn.Dropout(0.3)\n        \n    # here we define the forward propagation of the net\n    def forward(self, x): # and use the not trainable layers\n        # convolutional process\n        x = F.relu(self.pool1(self.conv1(x)))\n        # apply the flatten process, conserving the batch dim\n        x = torch.flatten(x,1)\n        # dense process with droput layers\n        x = self.dropout(F.relu(self.dense1(x)))\n        x = self.dropout(F.relu(self.dense2(x)))\n        x = self.dropout(F.relu(self.dense3(x)))\n        x = self.dense4(x)\n        return x\n        \n        \nmodel = Network()\n\n# this is a prediction from the model, if we play and modificate the\n# forward function we will see how the x.shape is changing\nmodel(torch.randn((32,1,64,64)))\n\n# SET THE DEVICE\n# if we hace a gpu (with cuda) set the device as the gpu, else in cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device) # move the model to the selected device\ndevice","27c3b058":"lr = 0.001\n\n# this is like instance the loss function\ncriterion = nn.CrossEntropyLoss()\n# the optimizer receives the model weights and a learning rate\noptimizer = optim.Adam(model.parameters(), lr=lr)","a2650d84":"# define a metric\ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval() # is like swich the model mode, this changes the\n    # behave of layers like Dropouts Layers, BatchNorm Layers\n\n    with torch.no_grad(): # deactivcate the back propagation,\n    # it will reduce memory and speed up computations\n\n        for x,y in loader:\n            # move the data and targets to the device\n            x = x.to(device)\n            y = y.to(device)\n            # obtain the scores\n            scores = model(x)\n            # we ned the max from the second dim\n            _, preds = scores.max(1)\n            # select the correct preds and sum them\n            num_correct += (preds == y).sum()\n            # count the num of samples\n            num_samples += preds.shape[0]\n    \n    # calculate the accuracy, float since numbers are tensors\n    acc = float(num_correct) \/ float(num_samples)\n    print(f'Got {num_correct} \/ {num_samples} with accuracy {acc*100}%')\n\n    # switch the model to train mode\n    model.train()\n    \n    return acc","f32598d9":"# train variables\nepochs = 24\nhist = {\n    'accuracy': [],\n    'val accuracy': []\n}\n\n# early stopping variables\npatience = 8\nwait=0\nmax_acc = 0\n\n# train the network\nfor epoch in range(epochs):\n    # this iters the the data and targets, and with an id\n    # data and targets are the batch for each train step\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # move the data and taregets to the model's device\n        data = data.to(device)\n        targets = targets.to(device)\n\n        # forward propagation, predict and measure the error\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        # backward propagation, use the error to fit the weights\n        optimizer.zero_grad() # clean the gradient, it's needed\n        # contains the directions to redice the error value of\n        # each prediction made, else is going to be adding value and fail\n        ## back propagation of the gradient and fit the weights\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n        \n    print(f'==> Epoch{epoch}')\n    # check the accuracy from train and val in each epoch\n    acc = check_accuracy(train_loader, model)\n    val_acc = check_accuracy(val_loader, model)\n    \n    # regist the accuacy values\n    hist['accuracy'].append(acc)\n    hist['val accuracy'].append(val_acc)\n    \n    # add a simple early stopping manually\n    if acc > max_acc:\n        acc = max_acc\n        wait=0\n    else:\n        wait += 1\n    if wait == patience:\n        break","9eb2bb22":"# plot the accuracy increase\nx = np.arange(len(hist['accuracy']))\nplt.figure(figsize=(10,6))\nplt.plot(x, hist['accuracy'], label='train')\nplt.plot(x, hist['val accuracy'], label='val')\nplt.title('Accuracy')\nplt.grid(True)\nplt.legend()\nplt.xlabel('Epoch')\nplt.show()","9a148331":"# evaluate the model with the test set\n_ = check_accuracy(test_loader, model)","8fd024fd":"# for flatten a list of 2 dims\ndef flatten2d(x):\n    l = []\n    for row in x:\n        for i in row:\n            l.append(i)\n    return l\n        \n\n# define a function to get y_real, y_pred\ndef pred_and_real(loader, model):\n    y_real = []\n    y_pred = []\n    model.eval() # is like swich the model mode, this changes the\n    # behave of layers like Dropouts Layers, BatchNorm Layers\n\n    with torch.no_grad(): # deactivcate the back propagation,\n    # it will reduce memory and speed up computations\n\n        for x,y in loader:\n            # move the data and targets to the device\n            x = x.to(device)\n            y = y.to(device)\n    \n            # obtain the predictions\n            # and move them to cpu\n            scores = model(x).to('cpu')\n            # decode the predictions\n            scores = test_set.decode(scores)\n            # move the answers to cpu\n            y = y.to('cpu').numpy()\n            \n            # append the results\n            y_real.append(list(y))\n            y_pred.append(list(scores))\n    \n    # flatten the arrays\n    y_real = flatten2d(y_real)\n    y_pred = flatten2d(y_pred)\n\n    return y_real, y_pred\n            \n# use the functions\nanswers, predicts = pred_and_real(test_loader, model)\nlen(answers) == len(predicts)","958f953b":"# define the matrix with the real classes and the predicted\nm = confusion_matrix(answers, predicts)\n# the labels for the plot\nlabels = list(test_set.id_labels.values()) # the characters throw warnings\nplt.figure(figsize=(20, 8))\n# create the plot\nheatmap = sns.heatmap(m, xticklabels=labels, yticklabels=labels, annot=True, fmt='d', color='blue')\n# labels for the axes\nplt.xlabel('Predicted Class')\nplt.ylabel('True Class')\nplt.title('Confusion Matrix')\nplt.show()\n# print the ids and the labels\nprint('Labels and ids')\nprint(test_set.id_labels.keys())\nprint(test_set.id_labels.values())","6d448bdf":"# select n images from the test_set\nnum_samples = 15\nsample_images = []\nsample_labels = []\nsample_preds = []\n\nfor x,y in test_loader: # x and y will be 32 examples, batch size\n    sample_images = x[:num_samples].to(device) # move the images to the model's device\n    # make the predictions and move them to the cpu\n    sample_preds = model(sample_images).to('cpu')\n    # and decode the predictions\n    sample_preds = test_set.decode(sample_preds)\n    # get the labels with the same format of sample_preds\n    sample_labels = y[:num_samples].numpy()\n    # move the images to cpu and convert to numpy\n    sample_images = sample_images.to('cpu').numpy()\n    break # only one iteration","8b48b97a":"# format the preds for the plot\npreds = []\nfor real,pred in zip(sample_labels, sample_preds):\n    # first the real and later the label\n    preds.append((real,pred))\n\n# plot the images with the labels and the predictions\nplot_images(sample_images, (3,5), figsize=(16,10), title_size=18, preds=preds)","e27ec027":"# save the model\ntorch.save(model.state_dict(), '.\/model.pth')","ce2af8a4":"# load the model, we need to have the model class defined\nm = Network() # define a new model\nm.load_state_dict(torch.load('.\/model.pth')) # load the weights\nm.eval() # set the model on 'evaluation mode', this just change\n# the behavior of some layers like batchnorm and dropout\n\n# evaluate the loaded model\nm = m.to(device) # first move to the device the model\n_ = check_accuracy(test_loader, m) # and evaluate the model","09f806c5":"# Evaluate the model and Confusion Matrix","8009ff96":"# Save the model\n[Here](https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html) there are more information about saving and loading **PyTorch** models","bd706a57":"# Chinese Digit Recognizer, This time with PyTorch and GPU\n\nIt's my very first time in **PyTorch** and I will try to do my best with this dataset :3","c1b911d8":"# Create the Model and set the device\nWith **PyTorch** we can locate easily the model and tensors in a specific device (cpu, gpu, tpu)","ea78a93d":"# Define the loss functionn optimizer and train the model","e1029266":"# See some examples","86187062":"# Split the Data\nI consider that is some early to apply this process, but in this case its okay since `I will include the data and target processing inside CustomDataset class`. Also inside this process the data will be normalized.","c082cd98":"# Creating the Dataset Class\nThis is like part of the data processing, but creating a dataset class is important on PyTorch as I see","a77f34c9":"# Reading the data\nIn this dataset we have `15000 rows with a 64x64 image per row`. Each image is in each row of the .csv file and the last two columns are the labels (both can be used as target with a categorical encoder). "}}