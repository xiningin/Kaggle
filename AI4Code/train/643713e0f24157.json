{"cell_type":{"8a9693f2":"code","32a9c3a0":"code","49f12d6f":"code","2c9c29d9":"code","dc2b5281":"code","63366755":"code","8441eb54":"code","762a99ff":"code","ebf93419":"code","899d9982":"code","ede397b4":"code","61c62233":"code","5a849e9e":"code","82d762e4":"code","47fddf26":"code","5fea00a2":"code","11321114":"code","bbb7900a":"code","af0849bf":"code","cefb5838":"code","369bb617":"code","0cee58e0":"code","eaf2a95c":"code","3bbe4326":"code","78a14145":"code","29ba4658":"code","a5ab1af3":"code","52513d7f":"code","3d555a29":"code","266c0234":"code","fc1caf1f":"code","fac4540d":"code","05bffc8c":"code","e14d61dd":"code","0379f2a1":"code","ec8e844a":"code","061e8003":"code","dabafcc0":"code","e29714a8":"code","017181c7":"code","045b24aa":"code","2a9c75f2":"code","9b3ebe2c":"code","4aab23c1":"code","39494813":"code","19d6e671":"code","36ada14d":"code","073647c2":"code","b6998697":"code","a4911f20":"code","2544a4ad":"code","a04038a3":"code","235e4318":"code","b878893e":"code","222c401b":"code","38241a62":"code","e1eca0b7":"code","ac17d798":"code","1def72f0":"code","866ee554":"code","db6df700":"code","c1e01444":"code","8a5405e5":"code","55609179":"code","66cf9d0a":"code","a0325ca0":"markdown","62304740":"markdown","15407065":"markdown","c9f4c72a":"markdown","8e868b19":"markdown","ea32a466":"markdown","bb4dd626":"markdown","be47debe":"markdown","a9be3c6a":"markdown","380d0aa4":"markdown","c4f6285c":"markdown","4c1aa694":"markdown","f24b1f2f":"markdown","4d97d25e":"markdown","6b5e22d6":"markdown","bbeb1a2d":"markdown","26bd1428":"markdown","0b4dc245":"markdown","e54e6533":"markdown","dc8c3f38":"markdown","813a8c24":"markdown","adcf490e":"markdown","cc5bf4bc":"markdown","d13e17fd":"markdown"},"source":{"8a9693f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32a9c3a0":"data  = pd.read_csv('\/kaggle\/input\/lung-cancer\/survey lung cancer.csv')\ndata.head()","49f12d6f":"#Shape of data \nprint(data.shape)\n#dtypes of data \nprint(data.dtypes)","2c9c29d9":"# Info of data\ndata.info()","dc2b5281":"# Checking for null values\ndata.isnull().sum()","63366755":"# label encoding\ndata.replace({\"LUNG_CANCER\":{'YES':0,'NO':1}},inplace=True)\n# printing the first 5 rows of the dataframe\ndata.head(5)","8441eb54":"# Value_counts of loan_status\ndata['LUNG_CANCER'].value_counts()","762a99ff":"# label encoding\ndata.replace({\"GENDER\":{'M':0,'F':1}},inplace=True)\n# printing the first 5 rows of the dataframe\ndata.head(5)","ebf93419":"# education & Loan Status\nimport seaborn as sns\nsns.countplot(x='LUNG_CANCER',hue='LUNG_CANCER',data=data)","899d9982":"# education & Loan Status\nimport seaborn as sns\nsns.countplot(x='GENDER',hue='LUNG_CANCER',data=data)","ede397b4":"# let's see how data is distributed for every column\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (20, 25))\nplotnumber = 1\nfor column in data:\n    if plotnumber <= 9:\n        ax = plt.subplot(3, 3, plotnumber)\n        sns.distplot(data[column])\n        plt.xlabel(column, fontsize = 15)\n        \n    plotnumber += 1\nplt.show()","61c62233":"# separating the data and target\nX = data.drop(columns=['LUNG_CANCER'],axis=1)\ny = data['LUNG_CANCER']","5a849e9e":"print(\"The shape of X is \" , X.shape)\nprint(\"The shape of Y is \" , y.shape)","82d762e4":"from sklearn.model_selection import train_test_split\n# separating into train and testing\nX_train, X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)\nprint(\"Shape of X_train is \" ,X_train.shape)\nprint(\"Shape of X_test  is \" ,X_test.shape)\nprint(\"Shape of Y_train is \" ,Y_train.shape)\nprint(\"Shape of Y_test  is \" ,Y_test.shape)","47fddf26":"# After statify Y train & test values\nprint(Y_train.value_counts())\nprint(Y_test.value_counts())","5fea00a2":"# scaling the data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","11321114":"X_train","bbb7900a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlr = LogisticRegression()\nlr.fit(X_train, Y_train)\ny_pred = lr.predict(X_test)\n\nlr_train_acc = accuracy_score(Y_train, lr.predict(X_train))\nlr_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Logistic Regression Model is {lr_train_acc}\")\nprint(f\"Test Accuracy of Logistic Regression Model is {lr_test_acc}\")","af0849bf":"# confusion matrix \nconfusion_matrix(Y_test, y_pred)","cefb5838":"# classification report\nprint(classification_report(Y_test, y_pred))","369bb617":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\n\ny_pred = knn.predict(X_test)\n\nknn_train_acc = accuracy_score(Y_train, knn.predict(X_train))\nknn_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of KNN Model is {knn_train_acc}\")\nprint(f\"Test Accuracy of KNN Model is {knn_test_acc}\")","0cee58e0":"# confusion matrix \nconfusion_matrix(Y_test, y_pred)","eaf2a95c":"# classification report\nprint(classification_report(Y_test, y_pred))","3bbe4326":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\n\ny_pred = svc.predict(X_test)\n\nsvc_train_acc = accuracy_score(Y_train, svc.predict(X_train))\nsvc_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of SVC Model is {svc_train_acc}\")\nprint(f\"Test Accuracy of SVC Model is {svc_test_acc}\")","78a14145":"# confusion matrix\nconfusion_matrix(Y_test, y_pred)","29ba4658":"# classification report\nprint(classification_report(Y_test, y_pred))","a5ab1af3":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, Y_train)\n\ny_pred = dtc.predict(X_test)\n\ndtc_train_acc = accuracy_score(Y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Decision Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decision Tree Model is {dtc_test_acc}\")","52513d7f":"# confusion matrix\nconfusion_matrix(Y_test, y_pred)","3d555a29":"# classification report\nprint(classification_report(Y_test, y_pred))","266c0234":"# hyper parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(dtc, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, Y_train)","fc1caf1f":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","fac4540d":"dtc = grid_search.best_estimator_\ny_pred = dtc.predict(X_test)\ndtc_train_acc = accuracy_score(Y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Decesion Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decesion Tree Model is {dtc_test_acc}\")","05bffc8c":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(dtc,filled=True)","e14d61dd":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'gini', max_depth = 3, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 4, n_estimators = 180)\nrand_clf.fit(X_train, Y_train)\n\ny_pred = rand_clf.predict(X_test)\n\nrand_clf_train_acc = accuracy_score(Y_train, rand_clf.predict(X_train))\nrand_clf_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Random Forest Model is {rand_clf_train_acc}\")\nprint(f\"Test Accuracy of Random Forest Model is {rand_clf_test_acc}\")","0379f2a1":"from sklearn.neighbors import KNeighborsClassifier\nk_model = KNeighborsClassifier(n_neighbors=16)\nkfitModel = k_model.fit(X_train, Y_train)\n\n# accuracy score on training data\n\nkX_train_prediction = kfitModel.predict(X_train)\ntraining_data_accuray = accuracy_score(kX_train_prediction,Y_train)\nprint('Accuracy on training data  : ', training_data_accuray)\n\n# accuracy score on testing data\nkX_test_prediction = kfitModel.predict(X_test)\nkx_lgr_test_data_accuray = accuracy_score(kX_test_prediction,Y_test)\nprint('Accuracy on test data      : ', kx_lgr_test_data_accuray)","ec8e844a":"#Ada Boost Classifie\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 5, verbose = 1)\ngrid_search.fit(X_train, Y_train)","061e8003":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","dabafcc0":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME.R', learning_rate = 0.1, n_estimators = 180)\nada.fit(X_train, Y_train)\n\nada_train_acc = accuracy_score(Y_train, ada.predict(X_train))\nada_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Ada Boost Model is {ada_train_acc}\")\nprint(f\"Test Accuracy of Ada Boost Model is {ada_test_acc}\")","e29714a8":"# confusion matrix\nconfusion_matrix(Y_test, y_pred)","017181c7":"# classification report\nprint(classification_report(Y_test, y_pred))","045b24aa":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search = GridSearchCV(gb, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, Y_train)","2a9c75f2":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n","9b3ebe2c":"gb = GradientBoostingClassifier(learning_rate = 1, loss = 'exponential', n_estimators = 180)\ngb.fit(X_train, Y_train)\n\ny_pred = gb.predict(X_test)\n\ngb_train_acc = accuracy_score(Y_train, gb.predict(X_train))\ngb_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Gradient Boosting Classifier Model is {gb_train_acc}\")\nprint(f\"Test Accuracy of Gradient Boosting Classifier Model is {gb_test_acc}\")","4aab23c1":"# confusion matrix\n\nconfusion_matrix(Y_test, y_pred)","39494813":"# classification report\n\nprint(classification_report(Y_test, y_pred))","19d6e671":"sgbc = GradientBoostingClassifier(learning_rate = 0.1, subsample = 0.9, max_features = 0.75, loss = 'deviance',\n                                  n_estimators = 100)\n\nsgbc.fit(X_train, Y_train)\n\ny_pred = sgbc.predict(X_test)\n\nsgbc_train_acc = accuracy_score(Y_train, sgbc.predict(X_train))\nsgbc_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of SGB Model is {sgbc_train_acc}\")\nprint(f\"Test Accuracy of SGB Model is {sgbc_test_acc}\")","36ada14d":"# confusion matrix\n\nconfusion_matrix(Y_test, y_pred)","073647c2":"# classification report\n\nprint(classification_report(Y_test, y_pred))","b6998697":"#Cat boost\nfrom catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(iterations = 30, learning_rate = 0.1)\ncat.fit(X_train, Y_train)\n\ny_pred = cat.predict(X_test)","a4911f20":"cat_train_acc = accuracy_score(Y_train, cat.predict(X_train))\ncat_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of Cat Boost Classifier Model is {cat_train_acc}\")\nprint(f\"Test Accuracy of Cat Boost Classifier Model is {cat_test_acc}\")","2544a4ad":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, n_estimators = 10)\nxgb.fit(X_train, Y_train)\n\ny_pred = xgb.predict(X_test)\n\nxgb_train_acc = accuracy_score(Y_train, xgb.predict(X_train))\nxgb_test_acc = accuracy_score(Y_test, y_pred)\n\nprint(f\"Training Accuracy of XGB Model is {xgb_train_acc}\")\nprint(f\"Test Accuracy of XGB Model is {xgb_test_acc}\")","a04038a3":"# let's divide our dataset into training set and holdout set by 50% \n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val_train, test, val_test = train_test_split(X, y, test_size = 0.5, random_state = 355)","235e4318":"# let's split the training set again into training and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(train, test, test_size = 0.2, random_state = 355)","b878893e":"svm = SVC()\nsvm.fit(X_train, y_train)\n","222c401b":"# using Logistic Regression and SVM algorithm as base models.\n# Let's fit both of the models first on the X_train and y_train data.\n\nlr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)\n","38241a62":"predict_val1 = lr.predict(val_train)\npredict_val2 = svm.predict(val_train)","e1eca0b7":"predict_val = np.column_stack((predict_val1, predict_val2))\n","ac17d798":"#Let's get the prediction of all the base models on test set X_set\npredict_test1 = lr.predict(X_test)\npredict_test2 = svm.predict(X_test)","1def72f0":"# Let's stack the prediction values for validation set together as 'predict_set'\npredict_test = np.column_stack((predict_test1, predict_test2))","866ee554":"rand_clf = RandomForestClassifier()\nrand_clf.fit(predict_val, val_test)","db6df700":"stacking_acc = accuracy_score(y_test, rand_clf.predict(predict_test))\nprint(stacking_acc)","c1e01444":"# confusion matrix\nconfusion_matrix(y_test, rand_clf.predict(predict_test))","8a5405e5":"# classification report\n\nprint(classification_report(y_test, rand_clf.predict(predict_test)))","55609179":"models = ['Logistic Regression', 'KNN', 'SVC', 'Decision Tree', 'Random Forest','Ada Boost', 'Gradient Boosting', 'SGB', 'XgBoost', 'Stacking', 'Cat Boost']\nscores = [lr_test_acc, knn_test_acc, svc_test_acc, dtc_test_acc, rand_clf_test_acc, ada_test_acc, gb_test_acc, sgbc_test_acc, xgb_test_acc, stacking_acc, cat_test_acc]\n\nmodels = pd.DataFrame({'Model' : models, 'Score' : scores})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","66cf9d0a":"plt.figure(figsize = (18, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.show()","a0325ca0":"# **Feature Scalilng**","62304740":"## **XGB Classifier**","15407065":"## **Exploratory data analysis**","c9f4c72a":"## **Logistic Regression**","8e868b19":"## **KNN**","ea32a466":"# **Stacking**","bb4dd626":"## **Cat Boost**","be47debe":"**We can see that the distribution of data is normal ! lets move for the Model preparation.** \ud83d\ude80","a9be3c6a":"### **DataFraming**\n\n**Read .csv file into pandas**","380d0aa4":"# **Boosting**","c4f6285c":"## **Stochastic Gradient Boosting (SGB)**","4c1aa694":"# **Model Preparation**\n\nSpilt into X & Y","f24b1f2f":"## **DecisionTreeClassifier**","4d97d25e":"**Visualization the DTC tree.**","6b5e22d6":"# Data Visualization","bbeb1a2d":"# **Getting Started** \n\n**Title : Lung Cancer Prediction**\n  \n  **Lung Cancer Status :**\n\n  0 -- > Yes \n\n  1 -- > NO\n","26bd1428":"### ***Logistic Regression gives us the best result so we will save this model for production.***","0b4dc245":"## **Random Forest Classifier**","e54e6533":"# **Model Training**\n\n**We will train different model after the evaluation of model we will select out best model for production.**\n\n1.   Logistic Regression\n2.   KNN\n3.   SVC\n3.   Decision Tree\n4.   Random Forest Regressor\n5.   XgBoost\n6.   Ada Boost\n7.   Gradient Boosting\n8.   Stochascated Gradient Boosting\n9.   Stacking","dc8c3f38":"# **Transformation**","813a8c24":"## **Hyper parameter tuning**","adcf490e":"## **SVC**\n","cc5bf4bc":"## **KNeighborsClassifier**","d13e17fd":"## **GradientBoostingClassifier**"}}