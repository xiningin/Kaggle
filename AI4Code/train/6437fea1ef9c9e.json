{"cell_type":{"2a37c59a":"code","4667622b":"code","f3da0d82":"code","b00b6567":"code","33b96490":"code","f8d98f4d":"code","4a0818b2":"code","9e30e02e":"code","19236758":"code","b618723e":"code","8113f64d":"code","03624597":"code","01c19d47":"code","29ab8f66":"code","94ad7eac":"code","29fcb9db":"code","948fc323":"code","329fabc8":"code","f6f6e663":"code","399e0231":"code","d6f7d925":"code","349bfa9f":"code","e28374ec":"code","a023c165":"code","bc22ad90":"code","32fbe9e9":"code","4644a0be":"code","40717546":"code","40ef446b":"code","e1d0bf49":"code","2777fc7c":"code","3e7bd671":"code","d22e847f":"code","292454b9":"code","4c1de3f6":"code","beb873db":"code","7e74d9fe":"markdown","be16e654":"markdown","57069270":"markdown","18975128":"markdown","387af8b0":"markdown","f2ea6f5d":"markdown","d6c8e189":"markdown","38061b84":"markdown","9c21c24d":"markdown"},"source":{"2a37c59a":"!pip install torchinfo","4667622b":"debug_mode = False\n\n# Switch to training mode from here\ntraining_mode = False\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport math\nimport pickle\nfrom pickle import dump, load\nimport glob\nimport re\nimport string\nimport collections\nimport json\nimport gc\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim import Optimizer, Adam, lr_scheduler\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule, Trainer\n\nfrom torchinfo import summary\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ngc.enable()\n\nrand_seed = 1120\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"PyTorch Lightning Version: {pl.__version__}\")","f3da0d82":"experiment_name = f\"lightning-unsupervised-tst\"\n\ndataset_folder = f\"..\/input\/ventilator-pressure-prediction\"\n\nmodel_output_folder = f\".\/{experiment_name}\"\nos.makedirs(model_output_folder, exist_ok=True)\n\n# pretrained_model_path = glob.glob(f'{model_output_folder}\/epoch*.ckpt')[0]\npretrained_model_path = \"..\/input\/lightning-unsupervised-tst\/epoch904-train_loss_epoch0.011572.ckpt\"","b00b6567":"num_workers = 4\ngpus = [0]\n\nbreath_steps = 80\n\nepochs = 1000 if not debug_mode else 1\n\ntrain_batch_size = 128\ninfer_batch_size = 2048\n\nmixed_precision = False\n\nlearning_rate = 1e-3\nlearning_rate *= len(gpus)\n\nweight_decay = 0\n\n# Model hyperparameters #\n# Explanation reference: https:\/\/timeseriesai.github.io\/tsai\/models.TST.html\n\nd_model = 128  # total dimension of the model (number of features created by the model) Usual values: 128-1024.\nn_heads = 8  # parallel attention heads. Usual values: 8-16.\nnum_layers = 3  # the number of sub-encoder-layers in the encoder. Usual values: 2-8.\ndim_feedforward = 256  # the dimension of the feedforward network model. Usual values: 256-4096.\ndropout = 0.1  # amount of residual dropout applied in the encoder. Usual values: 0.-0.3.\npos_encoding = 'learnable'  # fixed, learnable\nactivation = 'gelu'  # # activation function of intermediate layer, relu or gelu.\nnorm = 'BatchNorm'  # BatchNorm, LayerNorm\n\nlr_decay_steps = 983 * 300  # every K epochs\nlr_decay_rate = 0.1  # every K epochs\n\nmean_mask_length = 3  # Imputation: the desired mean length of masked segments. Used only when `mask_distribution` is 'geometric'.\nmasking_ratio = 0.15  # Imputation: mask this proportion of each variable\nmask_mode = 'separate'  # Imputation: whether each variable should be masked separately\nmask_distribution = 'geometric'  # Imputation: whether each mask sequence element is sampled independently at random\n\naccumulate_grad_batches = 1\ngradient_clip_val = 4.0","33b96490":"train_df = pd.read_csv(f\"{dataset_folder}\/train.csv\")\ntest_df = pd.read_csv(f\"{dataset_folder}\/test.csv\")\nsubmit_df = pd.read_csv(f\"{dataset_folder}\/sample_submission.csv\")\ntrain_df.shape, test_df.shape, submit_df.shape","f8d98f4d":"target_column = \"pressure\"\nmeta_columns = [\"breath_id\", \"time_step\"]\nraw_features = [\n    c for c in train_df.columns\n    if c not in [\"id\", target_column, \"R\", \"C\"] + meta_columns\n]\nraw_features","4a0818b2":"# Reference: https:\/\/www.kaggle.com\/hijest\/gaps-features-tf-lstm-resnet-like-ff\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","9e30e02e":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","19236758":"def save_pickle(obj, folder_path):\n    dump(obj, open(folder_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n\n\ndef load_pickle(folder_path):\n    return load(open(folder_path, 'rb'))","b618723e":"# Reference: https:\/\/github.com\/gzerveas\/mvts_transformer\/blob\/master\/src\/optimizers.py\n# From https:\/\/github.com\/LiyuanLucasLiu\/RAdam\/blob\/master\/radam\/radam.py\nclass RAdam(Optimizer):\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 weight_decay=0,\n                 degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(\n                betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(\n                betas[1]))\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params,\n                      (list, tuple)) and len(params) > 0 and isinstance(\n                          params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0]\n                                         or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n                        p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2**state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 -\n                                                                       beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt(\n                            (1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) *\n                            (N_sma - 2) \/ N_sma * N_sma_max \/\n                            (N_sma_max - 2)) \/ (1 - beta1**state['step'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 \/ (1 - beta1**state['step'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'],\n                                         p_data_fp32)\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg,\n                                         denom)\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'],\n                                         p_data_fp32)\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss","8113f64d":"def add_features(df):\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df = pd.get_dummies(df)\n    return df","03624597":"train_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","01c19d47":"train_features = add_features(train_df[['time_step'] + raw_features +\n                                       ['R', 'C']].copy())\ntest_features = add_features(test_df[['time_step'] + raw_features +\n                                     ['R', 'C']].copy())\ntrain_features.shape, test_features.shape","29ab8f66":"train_features.columns","94ad7eac":"train_indices = train_df.index.to_numpy().reshape(-1, breath_steps)\noof_df = train_df[[\"id\", \"pressure\"]].copy()","29fcb9db":"scaler = StandardScaler(with_mean=True, with_std=True)\ntrain_features = scaler.fit_transform(train_features)\ntest_features = scaler.transform(test_features)","948fc323":"# (batch, steps, features)\ntrain_features = train_features.reshape(-1, breath_steps,\n                                        train_features.shape[-1])\ntest_features = test_features.reshape(-1, breath_steps,\n                                      test_features.shape[-1])\n\ntrain_u_out = train_df[['u_out']].to_numpy().reshape(-1, breath_steps)\ntest_u_out = test_df[['u_out']].to_numpy().reshape(-1, breath_steps)\ntargets = train_df[['pressure']].to_numpy().reshape(-1, breath_steps)\n\ntrain_breath_ids = train_df[\"breath_id\"].unique()","329fabc8":"train_features.shape, test_features.shape, train_u_out.shape, test_u_out.shape, targets.shape","f6f6e663":"if training_mode:\n    all_features = np.concatenate([train_features, test_features], axis=0)\n    all_u_out = np.concatenate([train_u_out, test_u_out], axis=0)\n    print(all_features.shape, all_u_out.shape)","399e0231":"del train_df, test_df\ngc.collect()","d6f7d925":"# Reference: https:\/\/github.com\/gzerveas\/mvts_transformer\n\ndef noise_mask(X,\n               masking_ratio,\n               lm=3,\n               mode='separate',\n               distribution='geometric',\n               exclude_feats=None):\n    \"\"\"\n    Creates a random boolean mask of the same shape as X, with 0s at places where a feature should be masked.\n    Args:\n        X: (seq_length, feat_dim) numpy array of features corresponding to a single sample\n        masking_ratio: proportion of seq_length to be masked. At each time step, will also be the proportion of\n            feat_dim that will be masked on average\n        lm: average length of masking subsequences (streaks of 0s). Used only when `distribution` is 'geometric'.\n        mode: whether each variable should be masked separately ('separate'), or all variables at a certain positions\n            should be masked concurrently ('concurrent')\n        distribution: whether each mask sequence element is sampled independently at random, or whether\n            sampling follows a markov chain (and thus is stateful), resulting in geometric distributions of\n            masked squences of a desired mean length `lm`\n        exclude_feats: iterable of indices corresponding to features to be excluded from masking (i.e. to remain all 1s)\n\n    Returns:\n        boolean numpy array with the same shape as X, with 0s at places where a feature should be masked\n    \"\"\"\n    if exclude_feats is not None:\n        exclude_feats = set(exclude_feats)\n\n    if distribution == 'geometric':  # stateful (Markov chain)\n        if mode == 'separate':  # each variable (feature) is independent\n            mask = np.ones(X.shape, dtype=bool)\n            for m in range(X.shape[1]):  # feature dimension\n                if exclude_feats is None or m not in exclude_feats:\n                    mask[:, m] = geom_noise_mask_single(\n                        X.shape[0], lm, masking_ratio)  # time dimension\n        else:  # replicate across feature dimension (mask all variables at the same positions concurrently)\n            mask = np.tile(\n                np.expand_dims(\n                    geom_noise_mask_single(X.shape[0], lm, masking_ratio), 1),\n                X.shape[1])\n    else:  # each position is independent Bernoulli with p = 1 - masking_ratio\n        if mode == 'separate':\n            mask = np.random.choice(np.array([True, False]),\n                                    size=X.shape,\n                                    replace=True,\n                                    p=(1 - masking_ratio, masking_ratio))\n        else:\n            mask = np.tile(\n                np.random.choice(np.array([True, False]),\n                                 size=(X.shape[0], 1),\n                                 replace=True,\n                                 p=(1 - masking_ratio, masking_ratio)),\n                X.shape[1])\n\n    return mask\n\n\ndef geom_noise_mask_single(L, lm, masking_ratio):\n    \"\"\"\n    Randomly create a boolean mask of length `L`, consisting of subsequences of average length lm, masking with 0s a `masking_ratio`\n    proportion of the sequence L. The length of masking subsequences and intervals follow a geometric distribution.\n    Args:\n        L: length of mask and sequence to be masked\n        lm: average length of masking subsequences (streaks of 0s)\n        masking_ratio: proportion of L to be masked\n\n    Returns:\n        (L,) boolean numpy array intended to mask ('drop') with 0s a sequence of length L\n    \"\"\"\n    keep_mask = np.ones(L, dtype=bool)\n    p_m = 1 \/ lm  # probability of each masking sequence stopping. parameter of geometric distribution.\n    p_u = p_m * masking_ratio \/ (\n        1 - masking_ratio\n    )  # probability of each unmasked sequence stopping. parameter of geometric distribution.\n    p = [p_m, p_u]\n\n    # Start in state 0 with masking_ratio probability\n    state = int(np.random.rand() >\n                masking_ratio)  # state 0 means masking, 1 means not masking\n    for i in range(L):\n        keep_mask[\n            i] = state  # here it happens that state and masking value corresponding to state are identical\n        if np.random.rand() < p[state]:\n            state = 1 - state\n\n    return keep_mask\n\n\ndef padding_mask(lengths, max_len=None):\n    \"\"\"\n    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n    where 1 means keep element at this position (time step)\n    \"\"\"\n    batch_size = lengths.numel()\n    max_len = max_len or lengths.max_val(\n    )  # trick works because of overloading of 'or' operator for non-boolean types\n    return (torch.arange(0, max_len,\n                         device=lengths.device).type_as(lengths).repeat(\n                             batch_size, 1).lt(lengths.unsqueeze(1)))","349bfa9f":"# Modified from: https:\/\/github.com\/gzerveas\/mvts_transformer\/blob\/master\/src\/datasets\/dataset.py\n\ndef collate_unsuperv(data, max_len=None, mask_compensation=False):\n    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n    Args:\n        data: len(batch_size) list of tuples (X, mask).\n            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n            - mask: boolean torch tensor of shape (seq_length, feat_dim); variable seq_length.\n        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n    Returns:\n        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n            0 indicates masked values to be predicted, 1 indicates unaffected\/\"active\" feature values\n        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 ignore (padding)\n    \"\"\"\n\n    batch_size = len(data)\n    features, masks, u_out = zip(*data)\n\n    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n    lengths = [X.shape[0] for X in features\n               ]  # original sequence length for each time series\n    if max_len is None:\n        max_len = max(lengths)\n    X = torch.zeros(\n        batch_size, max_len,\n        features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n    target_masks = torch.zeros_like(\n        X, dtype=torch.bool\n    )  # (batch_size, padded_length, feat_dim) masks related to objective\n    for i in range(batch_size):\n        end = min(lengths[i], max_len)\n        X[i, :end, :] = features[i][:end, :]\n        target_masks[i, :end, :] = masks[i][:end, :]\n\n    targets = X.clone()\n    X = X * target_masks  # mask input\n    if mask_compensation:\n        X = compensate_masking(X, target_masks)\n\n    padding_masks = torch.zeros(\n        batch_size, max_len, dtype=torch.bool)  # (batch_size, padded_length)\n    for i in range(batch_size):\n        padding_masks[i, :] = torch.where(u_out[i] == 0, 1, 0)\n\n    target_masks = ~target_masks  # inverse logic: 0 now means ignore, 1 means predict\n\n    return X, targets, target_masks, padding_masks\n\n\nclass VPPMaskedInputDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        features,\n        u_out,\n        mean_mask_length=3,\n        masking_ratio=0.15,\n        mode='separate',\n        distribution='geometric',\n    ):\n        self.features = features\n        self.u_out = u_out\n\n        self.masking_ratio = masking_ratio\n        self.mean_mask_length = mean_mask_length\n        self.mode = mode\n        self.distribution = distribution\n\n    def __getitem__(self, index):\n        \"\"\"\n        For a given integer index, returns the corresponding (seq_length, feat_dim) array and a noise mask of same shape\n        Args:\n            index: integer index of sample in dataset\n        Returns:\n            X: (seq_length, feat_dim) tensor of the multivariate time series corresponding to a sample\n            mask: (seq_length, feat_dim) boolean tensor: 0s mask and predict, 1s: unaffected input\n        \"\"\"\n\n        X = self.features[index, :, :]  # (seq_length, feat_dim) array\n\n        mask = noise_mask(X, self.masking_ratio, self.mean_mask_length,\n                          self.mode, self.distribution,\n                          None)  # (seq_length, feat_dim) boolean array\n\n        return torch.from_numpy(X), torch.from_numpy(mask), torch.from_numpy(\n            self.u_out[index, :])\n\n    def update(self):\n        self.mean_mask_length = min(20, self.mean_mask_length + 1)\n        self.masking_ratio = min(1, self.masking_ratio + 0.05)\n\n    def __len__(self):\n        return self.features.shape[0]","e28374ec":"class VPPTestDataset(torch.utils.data.Dataset):\n    def __init__(self, data, u_out):\n        self.X = data\n        self.u_out = u_out\n\n    def __len__(self):\n        return self.X.shape[0]\n\n    def __getitem__(self, index):\n        return self.X[index, :, :], self.u_out[index, :]","a023c165":"class MaskedMAELoss(nn.Module):\n    \"\"\" Masked MAE Loss\n    \"\"\"\n    def __init__(self, reduction: str = 'mean'):\n\n        super().__init__()\n\n        self.reduction = reduction\n        self.mae_loss = nn.L1Loss(reduction=self.reduction)\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor,\n                mask: torch.BoolTensor) -> torch.Tensor:\n        \"\"\"Compute the loss between a target value and a prediction.\n\n        Args:\n            y_pred: Estimated values\n            y_true: Target values\n            mask: boolean tensor with 0s at places where values should be ignored and 1s where they should be considered\n\n        Returns\n        -------\n        if reduction == 'none':\n            (num_active,) Loss for each active batch element as a tensor with gradient attached.\n        if reduction == 'mean':\n            scalar mean loss over batch as a tensor with gradient attached.\n        \"\"\"\n\n        # for this particular loss, one may also elementwise multiply y_pred and y_true with the inverted mask\n        masked_pred = torch.masked_select(y_pred, mask)\n        masked_true = torch.masked_select(y_true, mask)\n\n        return self.mae_loss(masked_pred, masked_true)\n\n\nclass MaskedMSELoss(nn.Module):\n    \"\"\" Masked MSE Loss\n    \"\"\"\n    def __init__(self, reduction: str = 'mean'):\n\n        super().__init__()\n\n        self.reduction = reduction\n        self.mse_loss = nn.MSELoss(reduction=self.reduction)\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor,\n                mask: torch.BoolTensor) -> torch.Tensor:\n        \"\"\"Compute the loss between a target value and a prediction.\n\n        Args:\n            y_pred: Estimated values\n            y_true: Target values\n            mask: boolean tensor with 0s at places where values should be ignored and 1s where they should be considered\n\n        Returns\n        -------\n        if reduction == 'none':\n            (num_active,) Loss for each active batch element as a tensor with gradient attached.\n        if reduction == 'mean':\n            scalar mean loss over batch as a tensor with gradient attached.\n        \"\"\"\n\n        # for this particular loss, one may also elementwise multiply y_pred and y_true with the inverted mask\n        masked_pred = torch.masked_select(y_pred, mask)\n        masked_true = torch.masked_select(y_true, mask)\n\n        return self.mse_loss(masked_pred, masked_true)","bc22ad90":"from typing import Optional, Any\nimport math\n\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import functional as F\nfrom torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n\n\ndef _get_activation_fn(activation):\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return F.gelu\n    raise ValueError(\n        \"activation should be relu\/gelu, not {}\".format(activation))\n\n\n# From https:\/\/github.com\/pytorch\/examples\/blob\/master\/word_language_model\/model.py\nclass FixedPositionalEncoding(nn.Module):\n    r\"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos\/10000^(2i\/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos\/10000^(2i\/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=1024).\n    \"\"\"\n    def __init__(self, d_model, dropout=0.1, max_len=1024, scale_factor=1.0):\n        super(FixedPositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)  # positional encoding\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() *\n            (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = scale_factor * pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer(\n            'pe', pe\n        )  # this stores the variable in the state_dict (used for non-trainable variables)\n\n    def forward(self, x):\n        r\"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        \"\"\"\n\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass LearnablePositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=1024):\n        super(LearnablePositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        # Each position gets its own embedding\n        # Since indices are always 0 ... max_len, we don't have to do a look-up\n        self.pe = nn.Parameter(torch.empty(\n            max_len, 1, d_model))  # requires_grad automatically set to True\n        nn.init.uniform_(self.pe, -0.02, 0.02)\n\n    def forward(self, x):\n        r\"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        \"\"\"\n\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\ndef get_pos_encoder(pos_encoding):\n    if pos_encoding == \"learnable\":\n        return LearnablePositionalEncoding\n    elif pos_encoding == \"fixed\":\n        return FixedPositionalEncoding\n\n    raise NotImplementedError(\n        \"pos_encoding should be 'learnable'\/'fixed', not '{}'\".format(\n            pos_encoding))\n\n\nclass TransformerBatchNormEncoderLayer(nn.modules.Module):\n    r\"\"\"This transformer encoder layer block is made up of self-attn and feedforward network.\n    It differs from TransformerEncoderLayer in torch\/nn\/modules\/transformer.py in that it replaces LayerNorm\n    with BatchNorm.\n\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n    \"\"\"\n    def __init__(self,\n                 d_model,\n                 nhead,\n                 dim_feedforward=2048,\n                 dropout=0.1,\n                 activation=\"relu\"):\n        super(TransformerBatchNormEncoderLayer, self).__init__()\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n        # Implementation of Feedforward model\n        self.linear1 = Linear(d_model, dim_feedforward)\n        self.dropout = Dropout(dropout)\n        self.linear2 = Linear(dim_feedforward, d_model)\n\n        self.norm1 = BatchNorm1d(\n            d_model, eps=1e-5\n        )  # normalizes each feature across batch samples and time steps\n        self.norm2 = BatchNorm1d(d_model, eps=1e-5)\n        self.dropout1 = Dropout(dropout)\n        self.dropout2 = Dropout(dropout)\n\n        self.activation = _get_activation_fn(activation)\n\n    def __setstate__(self, state):\n        if 'activation' not in state:\n            state['activation'] = F.relu\n        super(TransformerBatchNormEncoderLayer, self).__setstate__(state)\n\n    def forward(self,\n                src: Tensor,\n                src_mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n        r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        src2 = self.self_attn(src,\n                              src,\n                              src,\n                              attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[0]\n        src = src + self.dropout1(src2)  # (seq_len, batch_size, d_model)\n        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n        # src = src.reshape([src.shape[0], -1])  # (batch_size, seq_length * d_model)\n        src = self.norm1(src)\n        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)  # (seq_len, batch_size, d_model)\n        src = src.permute(1, 2, 0)  # (batch_size, d_model, seq_len)\n        src = self.norm2(src)\n        src = src.permute(2, 0, 1)  # restore (seq_len, batch_size, d_model)\n        return src\n\n\nclass TSTransformerEncoder(nn.Module):\n    def __init__(self,\n                 feat_dim,\n                 max_len,\n                 d_model,\n                 n_heads,\n                 num_layers,\n                 dim_feedforward,\n                 dropout=0.1,\n                 pos_encoding='fixed',\n                 activation='gelu',\n                 norm='BatchNorm',\n                 freeze=False):\n        super(TSTransformerEncoder, self).__init__()\n\n        self.max_len = max_len\n        self.d_model = d_model\n        self.n_heads = n_heads\n\n        self.project_inp = nn.Linear(feat_dim, d_model)\n        self.pos_enc = get_pos_encoder(pos_encoding)(d_model,\n                                                     dropout=dropout *\n                                                     (1.0 - freeze),\n                                                     max_len=max_len)\n\n        if norm == 'LayerNorm':\n            encoder_layer = TransformerEncoderLayer(d_model,\n                                                    self.n_heads,\n                                                    dim_feedforward,\n                                                    dropout * (1.0 - freeze),\n                                                    activation=activation)\n        else:\n            encoder_layer = TransformerBatchNormEncoderLayer(\n                d_model,\n                self.n_heads,\n                dim_feedforward,\n                dropout * (1.0 - freeze),\n                activation=activation)\n\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer, num_layers)\n\n        self.output_layer = nn.Linear(d_model, feat_dim)\n\n        self.act = _get_activation_fn(activation)\n\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.feat_dim = feat_dim\n\n    def forward(self, X, padding_masks):\n        \"\"\"\n        Args:\n            X: (batch_size, seq_length, feat_dim) torch tensor of masked features (input)\n            padding_masks: (batch_size, seq_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n        Returns:\n            output: (batch_size, seq_length, feat_dim)\n        \"\"\"\n\n        # permute because pytorch convention for transformers is [seq_length, batch_size, feat_dim]. padding_masks [batch_size, feat_dim]\n        inp = X.permute(1, 0, 2)\n        inp = self.project_inp(inp) * math.sqrt(\n            self.d_model\n        )  # [seq_length, batch_size, d_model] project input vectors to d_model dimensional space\n        inp = self.pos_enc(inp)  # add positional encoding\n        # NOTE: logic for padding masks is reversed to comply with definition in MultiHeadAttention, TransformerEncoderLayer\n        output = self.transformer_encoder(\n            inp, src_key_padding_mask=~padding_masks\n        )  # (seq_length, batch_size, d_model)\n        output = self.act(\n            output\n        )  # the output transformer encoder\/decoder embeddings don't include non-linearity\n        output = output.permute(1, 0, 2)  # (batch_size, seq_length, d_model)\n        output = self.dropout1(output)\n        # Most probably defining a Linear(d_model,feat_dim) vectorizes the operation over (seq_length, batch_size).\n        output = self.output_layer(\n            output)  # (batch_size, seq_length, feat_dim)\n\n        return output","32fbe9e9":"class TSTLightning(pl.LightningModule):\n    def __init__(\n            self,\n            fold=None,\n            training_set=None,\n            in_features=None,\n            out_features=breath_steps,\n            d_model=128,  # total dimension of the model (number of features created by the model) Usual values: 128-1024.\n            n_heads=8,  # parallel attention heads. Usual values: 8-16.\n            num_layers=3,  # the number of sub-encoder-layers in the encoder. Usual values: 2-8.\n            dim_feedforward=256,  # the dimension of the feedforward network model. Usual values: 256-4096.\n            dropout=0.1,  # amount of residual dropout applied in the encoder. Usual values: 0.-0.3.\n            pos_encoding='fixed',  # fixed, learnable\n            activation='gelu',  # # activation function of intermediate layer, relu or gelu.\n            norm='BatchNorm',  # BatchNorm, LayerNorm\n            learning_rate=1e-3):\n        super(TSTLightning, self).__init__()\n\n        self.fold = fold\n        self.training_set = training_set\n\n        self.learning_rate = learning_rate\n\n        self.model = TSTransformerEncoder(\n            feat_dim=in_features,\n            max_len=breath_steps,\n            d_model=d_model,\n            n_heads=n_heads,\n            num_layers=num_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            pos_encoding=pos_encoding,  # fixed, learnable\n            activation=activation,  # relu, gelu\n            norm=norm,  # BatchNorm, LayerNorm\n            freeze=False)\n\n        self.num_parameters = count_parameters(self.model)\n        print(f\"Trainable params: {self.num_parameters:,}\")\n\n        self.loss_fn = MaskedMSELoss(reduction=\"mean\")\n\n        # Save passed hyperparameters\n        self.save_hyperparameters(\"in_features\", \"d_model\", \"n_heads\",\n                                  \"num_layers\", \"dim_feedforward\", \"dropout\",\n                                  \"pos_encoding\", \"activation\", \"norm\",\n                                  \"learning_rate\")\n\n        # Important: Activates manual optimization\n        # https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/common\/optimizers.html#manual-optimization\n        self.automatic_optimization = False\n\n    def forward(self, x, masks):\n        # print(x.shape, masks.shape)\n        return self.model(x, masks)\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n\n        opt.zero_grad(set_to_none=True)\n\n        X, targets, target_masks, padding_masks = batch\n\n        logits = self(X, padding_masks)  # (batch_size, breath_steps)\n\n        # Cascade noise masks (batch_size, padded_length, feat_dim) and padding masks (batch_size, padded_length)\n        target_masks = target_masks * padding_masks.unsqueeze(-1)\n\n        loss = self.loss_fn(\n            logits, targets, target_masks\n        )  # (num_active,) individual loss (square error per element) for each active value in batch\n\n        current_lr = self.lr_schedulers().get_last_lr()[0]\n\n        self.manual_backward(loss)\n\n        grad_norm = torch.nn.utils.clip_grad_norm_(self.parameters(),\n                                                   gradient_clip_val)\n\n        opt.step()\n\n        scheduler = self.lr_schedulers()\n        scheduler.step()\n\n        self.log_dict({\n            'train_loss': loss,\n            'learning_rate': current_lr\n        },\n                      on_step=True,\n                      on_epoch=True,\n                      prog_bar=True,\n                      logger=True)\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        pass\n\n    def validation_epoch_end(self, val_step_outputs):\n        pass\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n        # Extract z representation features\n        with torch.no_grad():\n            X, u_out = batch\n\n            batch_size = X.shape[0]\n            padding_masks = torch.zeros(\n                batch_size, breath_steps, dtype=torch.bool,\n                device=self.device)  # (batch_size, padded_length)\n            for i in range(batch_size):\n                padding_masks[i, :] = torch.where(u_out[i] == 0, 1, 0)\n\n            logits = self(X.float(),\n                          padding_masks)  # (batch_size, breath_steps)\n            return logits.detach().cpu()\n\n    def setup(self, stage=None):\n        if self.training:\n            self.train_dataset = VPPMaskedInputDataset(self.training_set[0],\n                                                       self.training_set[1])\n\n    def train_dataloader(self):\n        train_dataloader = DataLoader(\n            self.train_dataset,\n            batch_size=train_batch_size,\n            shuffle=True,\n            num_workers=num_workers,\n            pin_memory=True,\n            collate_fn=lambda x: collate_unsuperv(x, max_len=breath_steps),\n            drop_last=False)\n        print(f\"Train iterations: {len(train_dataloader)}\")\n        return train_dataloader\n\n    def val_dataloader(self):\n        pass\n\n    def test_dataloader(self):\n        pass\n\n    def predict_dataloader(self):\n        pass\n\n    def configure_optimizers(self):\n        print(f\"Initial Learning Rate: {self.hparams.learning_rate:.6f}\")\n\n        adam_beta1 = 0.9\n        adam_beta2 = 0.999\n        adam_epsilon = 1e-8\n        optimizer = RAdam(self.parameters(),\n                          lr=self.hparams.learning_rate,\n                          betas=(adam_beta1, adam_beta2),\n                          eps=adam_epsilon,\n                          weight_decay=weight_decay,\n                          degenerated_to_sgd=True)\n\n        train_steps = epochs * (len(self.train_dataloader()) \/\/\n                                accumulate_grad_batches)\n        print(f\"Total number of training steps: {train_steps}\")\n\n        scheduler = lr_scheduler.MultiStepLR(\n            optimizer,\n            milestones=list(range(lr_decay_steps, train_steps,\n                                  lr_decay_steps)),\n            gamma=lr_decay_rate)\n\n        return [optimizer], [scheduler]","4644a0be":"def get_model(fold_i,\n              training_set,\n              in_features=None,\n              model_path=None,\n              print_model=False):\n\n    if training_mode:\n        model = TSTLightning(\n            fold=fold_i,\n            training_set=training_set,\n            in_features=in_features,\n            out_features=breath_steps,\n            d_model=d_model,\n            n_heads=n_heads,\n            num_layers=num_layers,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            pos_encoding=pos_encoding,  # fixed, learnable\n            activation=activation,  # relu, gelu\n            norm=norm,  # BatchNorm, LayerNorm\n            learning_rate=learning_rate)\n        if print_model:\n            print(model)\n    else:\n        model = TSTLightning.load_from_checkpoint(\n            model_path,\n            fold=fold_i,\n            training_set=training_set,\n            in_features=in_features,\n        )\n\n        model.freeze()\n        model.eval()\n    return model","40717546":"# Ensure Reproducibility\nseed_everything(rand_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","40ef446b":"if training_mode:\n    print(f\"Unsupervised Train Shape: {all_features.shape}, {all_u_out.shape}\")\n\n    logger = TensorBoardLogger(model_output_folder,\n                               name=f\"logs\",\n                               default_hp_metric=True)\n\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=f\"{model_output_folder}\",\n        filename=\"{epoch}-{train_loss_epoch:.6f}\",\n        save_top_k=1,\n        save_weights_only=True,\n        save_last=False,\n        verbose=True,\n        monitor='train_loss_epoch',\n        mode='min')\n\n    callbacks = [checkpoint_callback]\n\n    model = get_model(fold_i=None,\n                      training_set=(all_features, all_u_out),\n                      in_features=all_features.shape[-1])\n\n    trainer = Trainer(\n        gpus=gpus if torch.cuda.is_available() else None,\n        distributed_backend=\"dp\"\n        if torch.cuda.is_available() else None,  # multiple-gpus, 1 machine\n        max_epochs=epochs,\n        benchmark=False,\n        deterministic=True,\n        log_gpu_memory=False,\n        checkpoint_callback=True,\n        callbacks=callbacks,\n        accumulate_grad_batches=accumulate_grad_batches,\n        precision=16 if mixed_precision and torch.cuda.is_available() else 32,\n        logger=logger)\n\n    trainer.fit(model)\n    \n    del model, trainer\n\n    torch.cuda.empty_cache()\n    gc.collect()","e1d0bf49":"training_mode = False\nmodel = get_model(model_path=pretrained_model_path,\n                  fold_i=None,\n                  training_set=(None, None),\n                  in_features=train_features.shape[-1])\n# Drop output layer\nmodel.model.output_layer = nn.Identity()","2777fc7c":"summary(model.cuda(),\n        input_size=[(2, breath_steps, train_features.shape[-1]),\n                    (2, breath_steps)],\n        dtypes=[torch.float, torch.bool])","3e7bd671":"# Ensure Reproducibility\nseed_everything(rand_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ntrainer = Trainer(\n    logger=False,\n    gpus=gpus if torch.cuda.is_available() else None,\n    distributed_backend=\"dp\" if torch.cuda.is_available() else None,\n    precision=16 if mixed_precision and torch.cuda.is_available() else 32,\n    benchmark=False,\n    deterministic=True)","d22e847f":"pred_dataset = VPPTestDataset(train_features, train_u_out)\npred_dataloader = torch.utils.data.DataLoader(pred_dataset,\n                                              batch_size=infer_batch_size,\n                                              shuffle=False,\n                                              num_workers=num_workers,\n                                              pin_memory=True,\n                                              drop_last=False)\n\npred_logits = trainer.predict(model,\n                              dataloaders=pred_dataloader,\n                              return_predictions=True)\npred_logits = torch.cat(pred_logits, dim=0)\ntrain_encoder_features = pred_logits.numpy()\nprint(pred_logits.shape)\nprint(pred_logits[0, :, :])\n\nfilename = f\"train_encoder_{d_model}features.pkl\"\nsave_pickle(train_encoder_features, filename)","292454b9":"del pred_logits, train_encoder_features\ngc.collect()","4c1de3f6":"pred_dataset = VPPTestDataset(test_features, test_u_out)\npred_dataloader = torch.utils.data.DataLoader(pred_dataset,\n                                              batch_size=infer_batch_size,\n                                              shuffle=False,\n                                              num_workers=num_workers,\n                                              pin_memory=True,\n                                              drop_last=False)\n\npred_logits = trainer.predict(model,\n                              dataloaders=pred_dataloader,\n                              return_predictions=True)\npred_logits = torch.cat(pred_logits, dim=0)\ntest_encoder_features = pred_logits.numpy()\nprint(pred_logits.shape)\nprint(pred_logits[0, :, :])\n\nfilename = f\"test_encoder_{d_model}features.pkl\"\nsave_pickle(test_encoder_features, filename)","beb873db":"del pred_logits, test_encoder_features\ngc.collect()","7e74d9fe":"# Multivariate Time Series Transformer\nReference: https:\/\/github.com\/gzerveas\/mvts_transformer","be16e654":"# Feature Engineering\nOnly raw features are used here. You could consider to add more useful engineering featues.","57069270":"# Lightning Module","18975128":"# Extract Transformer Encoder Features","387af8b0":"# Training","f2ea6f5d":"# EOF\nThanks for reading through this notebook!","d6c8e189":"# Utility Functions","38061b84":"# Dataset","9c21c24d":"<center>\n    <h1>Transformer-based Multivariate Time Series Representation Learning<\/h1>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-markpeng\/MoA\/pytorch_lightning.png\" alt=\"Pytorch Ligntning\" width=\"300\"\/>\n<\/center>\n<br\/>\n\nThis notebook implements an unsupervised Transformer Encoder model for learning the representation of time series dataset in Google Brain Ventilator Pressure Prediction competition. A **masking probability of 15%** is applied to sampled breath sequences of `time_step`, `u_in`, `u_out`, `R` and `C` features independently at random in geometric distribution. It learns to attend both to preceding and succeeding segments in individual features, as well as the inter-dependencies between features.\n\nThe model was trained under PyTorch Lightning architecture for about 1,000 epochs using combined training and test sets. The loss function is **masked MSE loss** for `u_out == 0` inhale sequences. Mean length of masked segments is set to 3 as suggested by the paper. The **128-dim feature vectors** are extracted and saved as pickle files in numpy format. You could play around the hyperparameters for a larger model or using more hand-crafted features as the input.\n\nUsing those generated feature vectors, we could create diverse downstream regression models or do nearest neighborhood search in the feature space for closest breath examples for the ensembles.\n\n\n<!-- ![Multivariate Time Series Transformer Framework](https:\/\/storage.googleapis.com\/kaggle-markpeng\/GoogleBrainVentilator\/mvts_transformer_architecture.png) -->\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-markpeng\/GoogleBrainVentilator\/mvts_transformer_architecture.png\" alt=\"Multivariate Time Series Transformer Framework\" width=\"800\"\/>\n\n<div align=\"center\">\n    Source: <a href=\"https:\/\/arxiv.org\/abs\/2010.02803\" target=\"_blank\">https:\/\/arxiv.org\/abs\/2010.02803<\/a>\n<\/div>\n\n<br\/>\n\n\n***\n\nPlease upvote or cite this notebook if you like it, thanks!\n\n***\n\n**Reference:**\n\nGeorge Zerveas _et al._ (2021). \"_A Transformer-based Framework for Multivariate Time Series Representation Learning_,\" Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '21).\n\n**ArXiV paper:** https:\/\/arxiv.org\/abs\/2010.02803\n\n**Github Repository:** https:\/\/github.com\/gzerveas\/mvts_transformer\n"}}