{"cell_type":{"c2a25c0d":"code","e1a10fb7":"code","3f04b8fb":"code","5fed58d5":"code","5c739c9e":"code","687ef2d0":"code","e6479425":"code","ec79d448":"code","21da9808":"code","5d8a1bf2":"code","c648f301":"code","66934912":"code","08af7167":"code","36a923a0":"code","851a3d37":"code","af596758":"code","3987b82a":"code","1db36d00":"code","e80cf36e":"code","190914f6":"code","ec86492d":"code","e0249f71":"code","3146c36c":"code","a60a0524":"code","73660940":"code","1484a054":"code","c9e019ff":"code","0d29b268":"code","a69fb347":"markdown","842b5cdf":"markdown","18529346":"markdown","5b5a8088":"markdown","4f90aa1f":"markdown","de1d99da":"markdown","7f04578c":"markdown","60865c46":"markdown","0241fa60":"markdown","b4ab44c5":"markdown","87eaaa43":"markdown","bcd30586":"markdown","043013b5":"markdown","5f2671d2":"markdown","89a6e537":"markdown","06939929":"markdown","267dfd1a":"markdown","236258f4":"markdown","73318dfd":"markdown","821cbec1":"markdown","00e731fd":"markdown","31bef02a":"markdown","637d8928":"markdown","87741fc0":"markdown","fbc43f1b":"markdown","41f1c436":"markdown","35d37c6d":"markdown","05fdb399":"markdown","90dc40dc":"markdown","0da5b08c":"markdown","8d1e8d21":"markdown","5c6a181d":"markdown"},"source":{"c2a25c0d":"#import the required modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e1a10fb7":"#import our data file \"glass.csv\" into a pandas dataframe\nglass_df = pd.read_csv(\"..\/input\/glass\/glass.csv\")","3f04b8fb":"#get information about the general values of each feature\nglass_df.describe()","5fed58d5":"#see the first 10 values of the dataset\nglass_df.head(10)","5c739c9e":"#get info about the datatypes of each feature\nglass_df.info()","687ef2d0":"#get counts of null values for each feature\nglass_df.isnull().sum()","e6479425":"#plot a histogram of each feature\nglass_df.hist(figsize=(20,20))","ec79d448":"# Checking to see how spread out our features values are\nax = sns.boxplot(data=glass_df)","21da9808":"#Get the distribution of the different classifications\nglass_df.Type.value_counts().plot(kind=\"bar\")","5d8a1bf2":"# This will be the dataframe containing the features used to train our model\nX = pd.DataFrame(glass_df.drop([\"Type\"], axis = 1),\n            columns=['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe'])\n\n# This will be the dataframe containing the labels of each data point\ny=glass_df.Type","c648f301":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=1\/5, random_state=5, stratify = y)","66934912":"#importing the k classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\n\n# we will be testing 14 different values for k, starting with 1 and ending before 15\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","08af7167":"# This score comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","36a923a0":"# This score comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","851a3d37":"#plotting the train and test score for each value of k we tested\nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","af596758":"from sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_minmax = min_max_scaler.fit_transform(X)\nX_norm = pd.DataFrame(X_minmax)","3987b82a":"# a peek at the new normalized features\nX_norm.head()","1db36d00":"#now all of our values fall between 0 and 1\nax = sns.boxplot(data=X_norm)","e80cf36e":"#now lets run the model on the normalized data and see if it has any effect on the accuracy\n#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_norm,y, test_size=1\/5, random_state=5, stratify = y)\n\ntest_scores = []\ntrain_scores = []\n\n# we will be testing 14 different values for k, starting with 1 and ending before 15\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","190914f6":"# This score comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","ec86492d":"# This score comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","e0249f71":"from sklearn.decomposition import PCA\n\nscores = []\n\nfor i in range(9):\n    pca = PCA(n_components=(i+1))\n    principalComponents = pca.fit_transform(X_norm)\n    principalDf = pd.DataFrame(data = principalComponents)\n    \n    X_train, X_test, y_train, y_test = train_test_split(principalDf,y, test_size=1\/5, random_state=5, stratify = y)\n\n    test_scores = []\n    train_scores = []\n\n    # we will be testing 14 different values for k, starting with 1 and ending before 15\n    for i in range(1,15):\n\n        knn = KNeighborsClassifier(i)\n        knn.fit(X_train,y_train)\n    \n        train_scores.append(knn.score(X_train,y_train))\n        test_scores.append(knn.score(X_test,y_test))\n        \n    # This score comes from testing on the datapoints that were split in the beginning to be used for testing solely\n    max_test_score = max(test_scores)\n    scores.append(max_test_score)\n    \nfor i in range(len(scores)):\n    print(\"With {} components, our accuracy was {}.\".format(i+1,scores[i]))","3146c36c":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,10),scores)\np.set(xlabel=\"Number of Principal Components\", ylabel=\"Model Accuracy\", title = \"Model Accuracy per number of Principal Components\")","a60a0524":"print(\"Our model had a maximum accuracy score of {:.2f}% with {} principal components.\".format(max(scores)*100,scores.index(max(scores))+1))","73660940":"ax = y.value_counts().plot(kind=\"bar\")\nax.set(xlabel=\"Type of Glass\", ylabel=\"Count\", title = \"Before SMOTE\")","1484a054":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=42)\nx_res, y_res = sm.fit_resample(X_norm,y)\ny_res_df = pd.DataFrame(y_res)\nax = y_res_df.Type.value_counts().plot(kind=\"bar\")\nax.set(xlabel=\"Type of Glass\", ylabel=\"Count\", title = \"After SMOTE\")","c9e019ff":"scores = []\n\nfor i in range(1,3):\n    pca = PCA(n_components=(i))\n    principalComponents = pca.fit_transform(x_res)\n    principalDf = pd.DataFrame(data = principalComponents)\n    \n    X_train, X_test, y_train, y_test = train_test_split(x_res,y_res, test_size=1\/5, random_state=5, stratify = y_res)\n\n    test_scores = []\n    train_scores = []\n\n    # we will be testing 14 different values for k, starting with 1 and ending before 15\n    for i in range(1,15):\n\n        knn = KNeighborsClassifier(i)\n        knn.fit(X_train,y_train)\n    \n        test_scores.append(knn.score(X_test,y_test))\n        \n    # This score comes from testing on the datapoints that were split in the beginning to be used for testing solely\n    max_test_score = max(test_scores)\n    scores.append(max_test_score)\n    \nfor i in range(len(scores)):\n    print(\"With {} components, our accuracy was {:.2f}%.\".format(i+1,scores[i]*100))","0d29b268":"finalDf = pd.concat([principalDf, y_res], axis = 1)\ncolor_dict = dict({1:'brown',\n                  2:'green',\n                  3: 'orange',\n                  5: 'red',\n                   6: 'dodgerblue',\n                  7: 'purple'})\nplt.figure(figsize=(10,10))\nax = sns.scatterplot(x=0,y=1,hue=\"Type\",data=finalDf, palette=color_dict)\nax.set(xlabel=\"PC1\",ylabel=\"PC2\", title = \"Our Final Dataset with 2 Principal Components\")","a69fb347":"The dataset I will be using is the \"Glass Classification\" dataset found on Kaggle at: https:\/\/www.kaggle.com\/uciml\/glass. The study of classification of types of glass was motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidence if it is correctly identified.\n\nWith the techniques I use below, I was able to take the KNN algorithm from 79% accuracy to 88% accuracy. With different random states, the model would score over 93%.","842b5cdf":"With this visualization, you can see where the various types of glass tend to group together.","18529346":"## 6. Improve the model","5b5a8088":"If our model's dependent variables, in our case Type of Glass, have an unequal distribution, our model might benefit from an oversampling technique called SMOTE. SMOTE will even out the difference by inserting syntheticly produced rows of data which fall in between the already existing minority dependent variables.","4f90aa1f":"The features don't seem to have a very large difference in range. Normalization could still help our model though.","de1d99da":"## 4. Run the KNN algorithm over a range of different K values","7f04578c":"Now our data is equally distributed accross each of our different dependent variables. Lets run our KNN algorithm over the new data.","60865c46":"We have an unequal distribution of dependent variables. We may need to use oversampling to even that out.","0241fa60":"PCA is a dimensionality reduction technique that allows us to keep most of the impoortant information from our data, while reducing the total number of features our model is using to predict the Type of glass. KNN models suffer from the curse of dimensionality, which basically means that the more features you are using to predict your outcome, the less accurate the model will be because of the distance between each data point. If we reduce the dimensionality, our model might become more accurate.","b4ab44c5":"<u><b>The Features<\/b><\/u>:\n\n<b>RI<\/b>: refractive index\n\n<b>Na<\/b>: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n\n<b>Mg<\/b>: Magnesium\n\n<b>Al<\/b>: Aluminum\n\n<b>Si<\/b>: Silicon\n\n<b>K<\/b>: Potassium\n\n<b>Ca<\/b>: Calcium\n\n<b>Ba<\/b>: Barium\n\n<b>Fe<\/b>: Iron","87eaaa43":"I took inspiration from the kaggle user \"Shruti_lyyer\" and her KNN guide: https:\/\/www.kaggle.com\/shrutimechlearn\/step-by-step-diabetes-classification-knn-detailed\n\nThe kaggle user \"KenTu\" has also put together a good notebook on KNN, PCA, and Oversampling: https:\/\/www.kaggle.com\/yundoou\/knn-precision-rate-grows-from-72-to-92","bcd30586":"We don't have any missing values. However, we will do some manipulation to the data after we run our first KNN model.","043013b5":"### Principal Component Analysis (PCA)","5f2671d2":"# KNN for Classification","89a6e537":"With only 7 components, our model performed just as well as the normalized model with 9 components. However the overall accuracy was not increased.","06939929":"## 3. Load the data","267dfd1a":"## 1. Get to know your dataset","236258f4":"### Normalizing the data","73318dfd":"After Normalizing, running SMOTE, and running PCA our KNN model is 88% accurate with only 1 principal component. With SMOTE you run the risk of overfitting. Our model is just as accurate with 2 components. Lets see what a 2D plot of our datapoints looks like.","821cbec1":"Noticed inside the train_test_split function, we used \"stratify = y.\" The stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to the \"stratify\" parameter. In our case, we are using it to ensure there is an equal proportion of labels in our training and testing sets.","00e731fd":"As you can see, a couple of our types of glass have many more values than the rest. SMOTE will fill in the difference.","31bef02a":"We can see that normalizing our data increased our model's accuracy by 2%. Now let's try Principal Component Analysis and see if that further improves our results.","637d8928":"## 2. Clean the data","87741fc0":"## Conclusion","fbc43f1b":"Normalizing our data will ensure that all of our features are being treated equally by our KNN model. Features that range greatly in value compared to other features can hurt the effectiveness of our KNN model. Normalizing\/Standardizing our data will put each value between 0 and 1.","41f1c436":"A few ways that we can improve the model is through normalization, which will reduce the distance between our various features, Principal Component Analysis, which will reduce our model's dimensions down to a smaller number without losing very much information, and oversampling with SMOTE which will even out the distribution of samples.","35d37c6d":"## Credits","05fdb399":"<u><b>The Dependent Variable<\/b><\/u>\n\n<b>Type of Glass<\/b>: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps","90dc40dc":"The k with the highest accuracy for both the training and testing data was 1.","0da5b08c":"We were able to solve our model's problem of unequal range with normalization. We used SMOTE to solve the dependent variables unequal distribution problem. We used PCA to reduce the dimensionality of our dataset and visualize our clusters.\n\nI hope you enjoyed this notebook. Thank you for reading.","8d1e8d21":"## 5. Assess the model","5c6a181d":"### Oversampling"}}