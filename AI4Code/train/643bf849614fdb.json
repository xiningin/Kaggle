{"cell_type":{"f25c6c82":"code","945b1d28":"code","d352a4a6":"code","3633938a":"code","da915f12":"code","3d2901aa":"code","6962a02f":"code","4ad768b6":"code","09f48838":"code","7fc9c8b2":"code","443cff63":"code","6685fb0d":"code","8708a4d9":"code","3ceff3a1":"code","ed9467c3":"code","593a891d":"code","00d9bcf1":"code","1c12594d":"code","037ca385":"code","88d99fa1":"code","899547f4":"code","6c0b745d":"code","1b4d4ddd":"code","c313de56":"code","2d782d3b":"code","5ffbcb4c":"code","c5510bf7":"code","24e6ff11":"code","f2208388":"code","b842e1a6":"code","eaee11b4":"code","dfe52832":"code","126a357e":"code","32133a19":"markdown","a103c7a9":"markdown","1e92b40d":"markdown","7a40c8ee":"markdown","9949bafc":"markdown","3fc170f2":"markdown","b3888a8c":"markdown","0abdf89d":"markdown","9ff089bc":"markdown","a98feaa6":"markdown","10bf211a":"markdown","3cbcd55b":"markdown","d16a84cd":"markdown","b77dba4d":"markdown","5ebc0888":"markdown","4cb42108":"markdown","9b885bc8":"markdown","3eb32544":"markdown"},"source":{"f25c6c82":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nimport math\nimport timeit\nimport pickle\nprint('TF version',tf.__version__)","945b1d28":"MAX_LEN = 100\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 7 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n","d352a4a6":"sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","3633938a":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\ntest.head()","da915f12":"empties=[]\nfor x in range(0,train.shape[0]):\n    text1=train['text'][x]\n    if text1==\"\":\n        empties.append(x)\n        train=train.drop([x])\ntrain=train.reset_index(drop=True)","3d2901aa":"empties2=[]\nurl_parts=['http:\/\/','https:\/\/','www.']\nfor x in range(0,train.shape[0]):\n    text1=train['text'][x]\n    for y in range(0,len(url_parts)):\n        url_part=url_parts[y]\n        idx = text1.find(url_part)\n        if idx>=0:\n            if idx<3:\n                len_url=text1[idx:].find(' ')\n                if len_url==-1:\n                    empties2.append(x)\n                    train=train.drop([x])\ntrain=train.reset_index(drop=True)","6962a02f":"def lower_text(text):\n    return text.lower()\ntrain['text_cleaned']=train['text'].apply(lambda x : lower_text(x))\ntrain['selected_text_cleaned0']=train['selected_text'].apply(lambda x : lower_text(x))\ntest['text_cleaned']=test['text'].apply(lambda x : lower_text(x))","4ad768b6":"def Fix_Spaces(text):\n    return \" \".join(text.split())\ntrain['text_cleaned']=train['text_cleaned'].apply(lambda x : Fix_Spaces(x))\ntrain['selected_text_cleaned0']=train['selected_text_cleaned0'].apply(lambda x : Fix_Spaces(x))\ntest['text_cleaned']=test['text_cleaned'].apply(lambda x : Fix_Spaces(x))","09f48838":"test['selected_text']=-1\ntest.head()\ndf_data=pd.concat([train,test],ignore_index=True, sort=True)","7fc9c8b2":"df_data['text_cleaned_0']=df_data['text_cleaned']","443cff63":"tic = timeit.default_timer()\n\nreplace_text='[url]'\nenc_url_replace=tokenizer.encode(replace_text).ids\n\nMAX_LEN=200\nTokenizer_indices_orig = np.zeros((df_data.shape[0],MAX_LEN))\nTokenizer_indices_cleaned = np.zeros((df_data.shape[0],MAX_LEN))\nTokenizer_encoding_orig = np.zeros((df_data.shape[0],MAX_LEN))\nTokenizer_encoding_cleaned = np.zeros((df_data.shape[0],MAX_LEN))\n\nurl_parts=['http:\/\/','https:\/\/',' www.']\nfor x in range(0,df_data.shape[0]):\n    text1=df_data['text_cleaned'][x]\n    \n    enc1 = tokenizer.encode(text1)\n    Tokenizer_encoding_1=np.zeros((MAX_LEN))\n    Tokenizer_encoding_1[0:len(np.array(enc1.ids))]=np.array(enc1.ids)\n    Tokenizer_indices_1=np.zeros((MAX_LEN))\n    Tokenizer_indices_1[0:len(np.array(enc1.ids))]=np.array((list(range(0, len(enc1.ids)))))+1\n    \n    Tokenizer_encoding_orig[x,0:len(np.array(enc1.ids))]=np.array(enc1.ids)\n    Tokenizer_indices_orig[x,0:len(np.array(enc1.ids))]=np.array((list(range(0, len(enc1.ids)))))+1\n    \n    for y in range(0,len(url_parts)):\n        F=True\n        while F:\n            url_part=url_parts[y]\n            text1=' '+text1\n            idx = text1.find(url_part)\n            \n            if idx != -1:\n                if y==2:\n                    idx=idx+1\n                \n                len_url=text1[idx:].find(' ')\n                if len_url==-1 or len(text1)==idx+len_url+1:\n                    text2=text1[:idx]\n                    if text1[idx-1]==' ':\n                        text2=text1[:idx-1]\n                    text2=text2+' '+replace_text\n                else:\n                    part1=text1[:idx]\n                    part2=text1[idx+len_url:]\n                    text2=part1+replace_text+part2\n                \n                text1=text1[1:]\n                text2=text2[1:]\n                enc1 = tokenizer.encode(text1)\n                enc2 = tokenizer.encode(text2)\n                \n                Tokenizer_encoding_1=np.zeros((MAX_LEN))\n                Tokenizer_encoding_2=np.zeros((MAX_LEN))\n                Tokenizer_encoding_1[0:len(np.array(enc1.ids))]=np.array(enc1.ids)\n                Tokenizer_encoding_2[0:len(np.array(enc2.ids))]=np.array(enc2.ids)\n                \n                a=0\n                part1_tokenized_indices=[]\n                while Tokenizer_encoding_2[a]==Tokenizer_encoding_1[a]:\n                    part1_tokenized_indices.append(Tokenizer_indices_1[a])\n                    a=a+1\n                part1_tokenized_indices=np.array(part1_tokenized_indices)\n                \n                a1=np.where(Tokenizer_encoding_1 == 0)[0].item(0)\n                a2=np.where(Tokenizer_encoding_2 == 0)[0].item(0)\n                part2_tokenized_indices=[]\n                while Tokenizer_encoding_2[a2]==Tokenizer_encoding_1[a1]:\n                    part2_tokenized_indices.append(Tokenizer_indices_1[a1])\n                    a1=a1-1\n                    a2=a2-1\n                part2_tokenized_indices=np.flip(np.array(part2_tokenized_indices))\n                \n                part3_length=a2-a+1\n                Available_space=a1-a+1\n                \n                if part3_length<0:\n                    part2_tokenized_indices=part2_tokenized_indices[abs(part3_length):]     # Happens once for index 28430, the end of part1 is the same as the start of part2. a comma is counted double.\n                \n                if len(part1_tokenized_indices)!=0:\n                    part3_tokenized_indices=np.array([part2_tokenized_indices[0]-1 , part2_tokenized_indices[0]-1 , part2_tokenized_indices[0]-1])\n                else:\n                    part3_tokenized_indices=np.array([1 , 1 , 1])\n                \n                new_indices=np.append(part1_tokenized_indices , part3_tokenized_indices)\n                new_indices=np.append(new_indices , part2_tokenized_indices)\n                Tokenizer_indices_2=new_indices\n                \n                text1=text2\n                Tokenizer_encoding_1=Tokenizer_encoding_2\n                Tokenizer_indices_1=Tokenizer_indices_2\n            else:\n                F=False\n                text1=text1[1:]\n        \n    Tokenizer_indices_cleaned[x,0:len(Tokenizer_indices_1)]=Tokenizer_indices_1\n    Tokenizer_encoding_cleaned[x,0:len(Tokenizer_encoding_1)]=Tokenizer_encoding_1\n    df_data['text_cleaned'][x]=text1\n\ntoc = timeit.default_timer()\nprint(toc-tic)","6685fb0d":"difference_URL_library=[]\ndifference_URL_locs=[]\nfor x in range(0,df_data.shape[0]):\n    if df_data['text_cleaned'][x]!=df_data['text_cleaned_0'][x]:\n        difference_URL_locs.append([x])\n        difference_URL_library.append(df_data['text_cleaned_0'][x])\nlen(difference_URL_library)\n","8708a4d9":"length1=[]\nindex=[]\nfor x in range(0,df_data.shape[0]):\n    text=df_data['text_cleaned'][x]\n    enc1 = tokenizer.encode(text)\n    length1.append(len(enc1.ids))\n    if length1[x] >= max(length1):\n        index.append(x)\n\nMAX_LEN=max(length1)\nMAX_LEN=MAX_LEN+5\nTokenizer_encoding_cleaned_MaxLen=Tokenizer_encoding_cleaned[:,:MAX_LEN]\nTokenizer_indices_cleaned_MaxLen=Tokenizer_indices_cleaned[:,:MAX_LEN]\n","3ceff3a1":"del train, test\ntrain=df_data[df_data.selected_text!=-1]\ntrain.shape","ed9467c3":"test=df_data[df_data.selected_text==-1]\ntest.drop('selected_text',axis=1,inplace=True)\ntest=test.reset_index(drop=True)\ntest.shape","593a891d":"train_mislabeled2 = train.groupby(['text_cleaned']).nunique().sort_values(by='sentiment', ascending=False)\ntrain_mislabeled2 = train_mislabeled2[train_mislabeled2 ['sentiment'] > 1]['sentiment']\ntrain_mislabeled2.index.tolist()\nprint(train_mislabeled2)\n\ntrain_mislabeled3 = train.groupby(['text_cleaned']).nunique().sort_values(by='selected_text_cleaned0', ascending=False)\ntrain_mislabeled3 = train_mislabeled3[train_mislabeled3 ['selected_text_cleaned0'] > 1]['selected_text_cleaned0']\ntrain_mislabeled3.index.tolist()\nprint(train_mislabeled3)\n\n# train.index[train['text_cleaned'] == \"lost luggage? sorry to hear. you should check out our selection of travel luggage here: http:\/\/budurl.com\/9mua\"].tolist()\n\n# print(train['text'][9727])\n# print(train['text_cleaned'][9727])\n# print(train['selected_text_cleaned0'][9727])\n# print(train['selected_text'][9727])\n# print(train['sentiment'][9727])\n# \n# print([])\n# print(train['text'][13528])\n# print(train['text_cleaned'][13528])\n# print(train['selected_text_cleaned0'][13528])\n# print(train['selected_text'][13528])\n# print(train['sentiment'][13528])","00d9bcf1":"train['sentiment'][16438]='positive'\ntrain['selected_text_cleaned0'][11431]=\"holy **** it`s super sunny, friday and whitsun, my tube is deeeesearted. wish i was in the park\"\ntrain['selected_text'][11431]=\"Holy **** it's super sunny, Friday and Whitsun, my tube is deeeesearted. Wish I was in the park\"\ntrain['sentiment'][11431]='neutral'\ntrain['selected_text_cleaned0'][13897]=\"holy **** it`s super sunny, friday and whitsun, my tube is deeeesearted. wish i was in the park\"\ntrain['selected_text'][13897]=\"Holy **** it`s super sunny, Friday and Whitsun, my tube is deeeesearted. Wish I was in the park\"\ntrain['selected_text_cleaned0'][95]='happy'\ntrain['selected_text'][95]='Happy'\ntrain['sentiment'][8291]='positive'\ntrain['sentiment'][21234]='positive'\ntrain['selected_text_cleaned0'][5303]=\"lol\"\ntrain['selected_text'][5303]=\"lol\"\ntrain['sentiment'][5303]='positive'\ntrain['sentiment'][4830]='positive'\ntrain['sentiment'][22933]='positive'\ntrain['selected_text_cleaned0'][13528]=\"sorry\"\ntrain['selected_text'][13528]=\"Sorry\"\ntrain['sentiment'][13528]='negative'\ntrain['selected_text_cleaned0'][6403]=\"well us brits have to wait a few more days for it! i thought it was all gonna released at once! i guess it`s worth the wait!\"\ntrain['selected_text'][6403]=\"well us Brits have to wait a few more days for it!  I thought it was all gonna released at once! I guess it`s worth the wait!\"\ntrain['sentiment'][6403]='neutral'\ntrain['selected_text_cleaned0'][14964]=\"happy\"\ntrain['selected_text'][14964]=\"Happy\"\ntrain['selected_text_cleaned0'][18573]=\"happy\"\ntrain['selected_text'][18573]=\"happy\"\ntrain['selected_text_cleaned0'][14199]=\"sad\"\ntrain['selected_text'][14199]=\"sad\"\ntrain['selected_text_cleaned0'][14947]=\"sad\"\ntrain['selected_text'][14947]=\"sad\"\ntrain['selected_text_cleaned0'][9305]=\"happy\"\ntrain['selected_text'][9305]=\"HAPPY\"\ntrain['selected_text_cleaned0'][15817]=\"happy\"\ntrain['selected_text'][15817]=\"Happy\"\ntrain['selected_text_cleaned0'][10620]=\"instant internet marketing empire! + *bonus* recoup your investment in 24 hours or less\"\ntrain['selected_text'][10620]=\"Instant Internet Marketing EMPIRE! + *BONUS* recoup your investment in 24 hours or less\"\ntrain['selected_text_cleaned0'][20251]=\"happy\"\ntrain['selected_text'][20251]=\"happy\"\ntrain['selected_text_cleaned0'][5547]=\"happy\"\ntrain['selected_text'][5547]=\"happy\"\ntrain['selected_text_cleaned0'][11961]=\"thank you. we had a blast\"\ntrain['selected_text'][11961]=\"thank you.  we had a blast\"\ntrain['selected_text_cleaned0'][24046]=\"joy!!\"\ntrain['selected_text'][24046]=\"joy!!\"\ntrain['selected_text_cleaned0'][5105]=\"happy\"\ntrain['selected_text'][5105]=\"happy\"\ntrain['selected_text_cleaned0'][3029]=\"happy\"\ntrain['selected_text'][3029]=\"Happy\"\ntrain['selected_text_cleaned0'][22683]=\"thanks!\"\ntrain['selected_text'][22683]=\"thanks!\"\ntrain['selected_text_cleaned0'][3259]=\"happy\"\ntrain['selected_text'][3259]=\"Happy\"\ntrain['selected_text_cleaned0'][15265]=\"happy\"\ntrain['selected_text'][15265]=\"happy\"\ntrain['selected_text_cleaned0'][19000]=\"happy\"\ntrain['selected_text'][19000]=\"HAPPY\"\ntrain['selected_text_cleaned0'][7121]=\"happy\"\ntrain['selected_text'][7121]=\"happy\"\ntrain['selected_text_cleaned0'][8487]=\"happy\"\ntrain['selected_text'][8487]=\"Happy\"\ntrain['selected_text_cleaned0'][13594]=\"happy\"\ntrain['selected_text'][13594]=\"Happy\"\ntrain['selected_text_cleaned0'][14355]=\"happy\"\ntrain['selected_text'][14355]=\"Happy\"\ntrain['selected_text_cleaned0'][1787]=\"good morning\"\ntrain['selected_text'][1787]=\"Good morning\"\ntrain['selected_text_cleaned0'][16641]=\"goodnight!\"\ntrain['selected_text'][16641]=\"goodnight!\"\ntrain['selected_text_cleaned0'][19045]=\"i cant afford\"\ntrain['selected_text'][19045]=\"i cant afford\"\ntrain['selected_text_cleaned0'][16767]=\"happy mothers day!!!\"\ntrain['selected_text'][16767]=\"Happy Mothers Day!!!\"\ntrain['selected_text_cleaned0'][8369]=\"good morning!\"\ntrain['selected_text'][8369]=\"Good Morning!\"\ntrain['selected_text_cleaned0'][10938]=\"nice one\"\ntrain['selected_text'][10938]=\"nice one\"\ntrain['selected_text_cleaned0'][24068]=\"g`night!\"\ntrain['selected_text'][24068]=\"G`night!\"\ntrain['selected_text_cleaned0'][7086]=\"thank you\"\ntrain['selected_text'][7086]=\"Thank you\"\ntrain['selected_text_cleaned0'][21612]=\"awesome to meet you all lol,\"\ntrain['selected_text'][21612]=\"awesome to meet you all lol,\"\ntrain['selected_text_cleaned0'][19490]=\"but **** it..\"\ntrain['selected_text'][19490]=\"but **** it..\"\ntrain['selected_text_cleaned0'][7546]=\"amazing\"\ntrain['selected_text'][7546]=\"amazing\"","1c12594d":"train['selected_text_cleaned0'][176]=\"can`t wait to see her bad n grown ****! lol\"\ntrain['selected_text'][176]=\"can`t wait to see her bad n grown ****! Lol\"\ntrain['sentiment'][176]='positive'\ntrain['selected_text_cleaned0'][254]=\"lol :p\"\ntrain['selected_text'][254]=\"lol :p\"\ntrain['sentiment'][254]='positive'\ntrain['selected_text_cleaned0'][851]=\"lol, i`ve done that one b4 i`m a victim 2 that! lol\"\ntrain['selected_text'][851]=\"lol, i`ve done that one b4  i`m a victim 2 that! lol\"\ntrain['sentiment'][851]='positive'\ntrain['selected_text_cleaned0'][1234]=\"lol missed you ha bye hun ****\"\ntrain['selected_text'][1234]=\"lol  missed you ha bye hun ****\"\ntrain['sentiment'][1234]='positive'\ntrain['selected_text_cleaned0'][1570]=\"because i had a blast!!\"\ntrain['selected_text'][1570]=\"because i had a blast!!\"\ntrain['sentiment'][1570]='positive'\ntrain['selected_text_cleaned0'][2124]=\"lol - that`s what hubby`s are there for\"\ntrain['selected_text'][2124]=\"lol - that`s what hubby`s are there for\"\ntrain['sentiment'][2124]='positive'\ntrain['selected_text_cleaned0'][2854]=\"but a good time was had by all\"\ntrain['selected_text'][2854]=\"but a good time was had by all\"\ntrain['sentiment'][2854]='positive'\ntrain['selected_text_cleaned0'][2908]=\"lol hi boys!\"\ntrain['selected_text'][2908]=\"LOL  Hi boys!\"\ntrain['sentiment'][2908]='positive'\ntrain['selected_text_cleaned0'][3503]=\";) xoxo\"\ntrain['selected_text'][3503]=\";) xoxo\"\ntrain['sentiment'][3503]='positive'\ntrain['selected_text_cleaned0'][3847]=\"totally worth it!! great movie cool 3d glasses!\"\ntrain['selected_text'][3847]=\"totally worth it!! great movie cool 3D glasses!\"\ntrain['sentiment'][3847]='positive'\ntrain['selected_text_cleaned0'][9895]=\"how much fun kyle is!\"\ntrain['selected_text'][9895]=\"how much fun kyle is!\"\ntrain['sentiment'][9895]='positive'\ntrain['selected_text_cleaned0'][10347]=\"i miss you too!! and don`t say 'damn'!!! lol\"\ntrain['selected_text'][10347]=\"I miss you too!!   And don`t say 'damn'!!!  lol\"\ntrain['sentiment'][10347]='positive'\ntrain['selected_text_cleaned0'][10827]=\"lol only if you make me that cookie\"\ntrain['selected_text'][10827]=\"Lol Only if you make me that cookie\"\ntrain['sentiment'][10827]='positive'\ntrain['selected_text_cleaned0'][11423]=\"lol xoxo\"\ntrain['selected_text'][11423]=\"lol xoxo\"\ntrain['sentiment'][11423]='positive'\ntrain['selected_text_cleaned0'][15558]=\"lol i feel like drunk right now...\"\ntrain['selected_text'][15558]=\"LOL I feel like drunk right now\"\ntrain['sentiment'][15558]='positive'\ntrain['selected_text_cleaned0'][18980]=\"no u miss me !!!!! lol\"\ntrain['selected_text'][18980]=\"No u miss me !!!!! LOL\"\ntrain['sentiment'][18980]='positive'\ntrain['selected_text_cleaned0'][19647]=\"lol\"\ntrain['selected_text'][19647]=\"lol\"\ntrain['sentiment'][19647]='positive'\ntrain['selected_text_cleaned0'][20706]=\"yen lol but i can only get the vid on my phone and ipod cant find the song lol\"\ntrain['selected_text'][20706]=\"yen lol but i can only get the vid on my phone and ipod cant find the song  lol\"\ntrain['sentiment'][20706]='positive'\ntrain['sentiment'][26899]='positive'\ntrain['selected_text_cleaned0'][27468]=\"lol i know and haha\"\ntrain['selected_text'][27468]=\"lol i know  and haha\"\ntrain['sentiment'][27468]='positive'\ntrain['selected_text_cleaned0'][17539]=\"happy mothers day\"\ntrain['selected_text'][17539]=\"HAPPY MOTHERS DAY\"\ntrain['sentiment'][17539]='positive'\ntrain['selected_text_cleaned0'][24850]=\"good work\"\ntrain['selected_text'][24850]=\"Good work\"\ntrain['sentiment'][24850]='positive'","037ca385":"train['selected_text_cleaned0'][870]=\"oh no!!\"\ntrain['selected_text'][870]=\"Oh no!!\"\ntrain['sentiment'][870]='negative'\ntrain['sentiment'][982]='negative'\ntrain['sentiment'][1871]='negative'\ntrain['sentiment'][2665]='negative'\ntrain['sentiment'][2976]='negative'\ntrain['selected_text_cleaned0'][7457]=\"sorry your still not well\"\ntrain['selected_text'][7457]=\"sorry your still not well\"\ntrain['sentiment'][7457]='negative'\ntrain['selected_text_cleaned0'][9475]=\"sorry for your loss dear\"\ntrain['selected_text'][9475]=\"sorry for your loss dear\"\ntrain['sentiment'][9475]='negative'\ntrain['sentiment'][11891]='negative'\ntrain['sentiment'][14558]='negative'\ntrain['sentiment'][21604]='negative'\ntrain['sentiment'][26664]='negative'","88d99fa1":"# In[ ]:\nct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nstart_tokens_values=[]\nend_tokens_values=[]\nstart_tokens_values_cleaned=[]\nend_tokens_values_cleaned=[]\nstart_tokens_cleaned=np.zeros((train.shape[0],MAX_LEN),dtype='int32')\nend_tokens_cleaned=np.zeros((train.shape[0],MAX_LEN),dtype='int32')\n\nerror_list=[]\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text_cleaned_0'].split())\n    text2 = \" \".join(train.loc[k,'selected_text_cleaned0'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n    \n    start_tokens[k,toks[0]+1] = 1\n    end_tokens[k,toks[-1]+1] = 1\n    start_tokens_values.append(toks[0]+1)\n    end_tokens_values.append(toks[-1]+1)\n    \n    # Below convert from original start and end toks to new start and end toks\n    Tokenizer_indices=Tokenizer_indices_cleaned_MaxLen[k,:]\n    start_token_2=[]\n    for x in range(MAX_LEN):\n        if Tokenizer_indices[x]==start_tokens_values[k]:\n            start_token_2.append(x+1)\n    if len(start_token_2)>1:\n        start_token_2=list([start_token_2[0]])  # If more than 1 index found, pick the first one.\n    if len(start_token_2)==0:\n        F=True\n        x=0\n        while F:\n            if Tokenizer_indices[x]<=start_tokens_values[k]:\n                x=x+1\n            else:\n                F=False\n        if x==0:\n            x=1\n        start_token_2=list([x])\n    \n    end_token_2=[]\n    for x in range(MAX_LEN):\n        if Tokenizer_indices[x]==end_tokens_values[k]:\n            end_token_2.append(x+1)\n    if len(end_token_2)>1:\n        end_token_2=list([end_token_2[-1]])  # If more than 1 index found, pick the last one.\n    if len(end_token_2)==0:\n        for x in range(MAX_LEN):\n            if Tokenizer_indices[x]>=end_tokens_values[k]:\n                end_token_2.append(x+1)\n        if len(end_token_2)>=1:\n            end_token_2=list([end_token_2[0]])\n        else:\n            F=True\n            x=0\n            while F:\n                if Tokenizer_indices[x]!=0:\n                    x=x+1\n                else:\n                    F=False\n            end_token_2=list([x])\n    if end_token_2<start_token_2:\n        start_token_2=1\n        end_token_2=end_tokens_values[k]\n        error_list.append(k)\n    \n    start_tokens_values_cleaned.append(np.int(start_token_2[0]))\n    end_tokens_values_cleaned.append(np.int(end_token_2[0]))\n    start_tokens_cleaned[k,np.int(start_token_2[0])] = 1\n    end_tokens_cleaned[k,np.int(end_token_2[0])] = 1\n    \n    text1_cleaned = \" \"+train.loc[k,'text_cleaned']\n    enc_cleaned = tokenizer.encode(text1_cleaned) \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc_cleaned.ids)+5] = [0] + enc_cleaned.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc_cleaned.ids)+5] = 1","899547f4":"ct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n    # INPUT_IDS\n    text1_cleaned = \" \"+\" \".join(test.loc[k,'text_cleaned'].split())\n    enc_cleaned = tokenizer.encode(text1_cleaned)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc_cleaned.ids)+5] = [0] + enc_cleaned.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc_cleaned.ids)+5] = 1\n","6c0b745d":"def save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model","1b4d4ddd":"def loss_fn(y_true, y_pred):\n    weight_ratio=0.05\n    weight_offset=1\n    \n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    \n    true_index=tf.argmax(y_true, axis=1)\n    pred_index=tf.argmax(y_pred, axis=1)\n    true_index=tf.cast(true_index, tf.float32)\n    pred_index=tf.cast(pred_index, tf.float32)\n    \n    weight=abs(true_index-pred_index)*weight_ratio+weight_offset\n    weight=tf.cast(weight, tf.float32)\n    \n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = loss*weight\n    loss = tf.reduce_mean(loss)\n    return loss","c313de56":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(0.2)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.2)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","2d782d3b":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","5ffbcb4c":"# In[ ]:\ndef scheduler(epoch):\n    return 3e-5 * 0.5**epoch","c5510bf7":"# In[ ]:\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\nval_epoch_start = np.zeros((input_ids.shape[0],MAX_LEN))\nval_epoch_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED) #originally 5 splits\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n    \n    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    \n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens_cleaned[idxT,], end_tokens_cleaned[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens_cleaned[idxV,], end_tokens_cleaned[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    \n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[reduce_lr],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        \n        # Save model only when jaccard score on validation improves for the current epoch\n        val_epoch_start[idxV,],val_epoch_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n        all = []\n        for idxV_val in idxV:\n            a = np.argmax(val_epoch_start[idxV_val,])\n            b = np.argmax(val_epoch_end[idxV_val,])\n            \n            a2=Tokenizer_indices_cleaned_MaxLen[idxV_val,a-1]\n            b2=Tokenizer_indices_cleaned_MaxLen[idxV_val,b-1]\n            if a>b: \n                st = train.loc[idxV_val,'text']\n            else:\n                text1 = \" \"+\" \".join(train.loc[idxV_val,'text'].split())\n                enc = tokenizer.encode(text1)\n                st = tokenizer.decode(enc.ids[int(a2-1):int(b2)])\n            all.append(jaccard(st,train.loc[idxV_val,'selected_text']))\n        score_value=np.mean(all)\n        print('>>>> Jaccard =',score_value)\n        \n        if epoch==1:\n            score_value0=score_value\n            print('save weights')\n            save_weights(model, weight_fn)\n            print()\n        else:\n            if score_value>score_value0:\n                score_value0=score_value\n                print('save weights')\n                save_weights(model, weight_fn)\n            print()\n    \n    print('Loading model...')\n    load_weights(model, weight_fn)\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n            \n        a2=Tokenizer_indices_cleaned_MaxLen[k,a-1]\n        b2=Tokenizer_indices_cleaned_MaxLen[k,b-1]\n        if a>b: \n            st = train.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[int(a2-1):int(b2)])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()\n    \n    with open('Predictions_test_%i'%(fold)+'.pickle', 'wb') as f:\n        pickle.dump([preds_start, preds_end], f)","24e6ff11":"# In[ ]:\nwith open('Predictions_Validation.pickle', 'wb') as f:\n    pickle.dump([oof_start, oof_end], f)\n","f2208388":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","b842e1a6":"all = []\nlog_indices=[]\nfor k in range(oof_start.shape[0]):\n    a = np.argmax(oof_start[k,])\n    b = np.argmax(oof_end[k,])\n    \n    a2=Tokenizer_indices_cleaned_MaxLen[k,a-1]\n    b2=Tokenizer_indices_cleaned_MaxLen[k,b-1]\n    if a>b: \n        log_indices.append(k)\n        st = train.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[int(a2-1):int(b2)])\n    all.append(jaccard(st,train.loc[k,'selected_text']))\nprint('>>>> Jaccard =',np.mean(all))\nprint()","eaee11b4":"all = []\nlog_indices=[]\nfor k in range(oof_start.shape[0]):\n    a = np.argmax(oof_start[k,])\n    b = np.argmax(oof_end[k,])\n    \n    a2=Tokenizer_indices_cleaned_MaxLen[k,a-1]\n    b2=Tokenizer_indices_cleaned_MaxLen[k,b-1]\n    if train.loc[k,'sentiment']=='neutral':\n        st = train.loc[k,'text']\n    else:\n        if a>b: \n            log_indices.append(k)\n            st = train.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[int(a2-1):int(b2)])\n    all.append(jaccard(st,train.loc[k,'selected_text']))\nprint('>>>> Jaccard =',np.mean(all))\nprint()","dfe52832":"all = []\noffset_train=train.shape[0]\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    \n    k_shift=k+offset_train\n    a2=Tokenizer_indices_cleaned_MaxLen[k_shift,a-1]\n    b2=Tokenizer_indices_cleaned_MaxLen[k_shift,b-1]\n    if test.loc[k,'sentiment']=='neutral':\n        st = test.loc[k,'text']\n    else:\n        if a>b: \n            st = test.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[int(a2-1):int(b2)])\n    all.append(st)","126a357e":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","32133a19":"I calculate what the maximum tweet length in the train and test set is after the cleaning.\nWe will use MAX_LEN to ensure we don't use too long data inputs for the model.","a103c7a9":"The model is as described by Wei Hao Khoong in 'https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution'.\n\nI did try to do some tricks in the loss function. I read the basis of this idea in another notebook, but I can't it back.\nThe idea is that there should be a larger penalty if a start\/end token is found that is far away from the actual start\/end token.\nIf the y_true=[0,1,0,0,0] there is no difference in loss function between a y_pred of [0.1,0.1,0.6,0.1,0.1] or [0.1,0.1,0.1,0.1,0.6]. While for the final jaccard score this makes a difference.\nI had some doubts about the code below. But the idea is that the length between true and prediction is calculated, and this is multiplied with a small weight (0.05).\nNext we increase the loss with a factor of (1+0.05*length) to increase the penalty of a large position difference. I tried to tune the parameter of 0.05, but the final result did not really improve.\n","1e92b40d":"Lets see how many tweets contained a url.","7a40c8ee":"Additionally I did a quick scan through the tweets and there were a few that had a clearly wrong label. Some tweets that should have been labelled positive had a negative label, and vica versa. So here I correct a few of them. I have not done a really thorough analysis so there could definitely be more wrong labels.","9949bafc":"# Trying various data treatment ideas and tuning of the BERT model hyperparameters","3fc170f2":"In the final jaccard calculation all capital letters and double spaces are removed. Therefore I already perform this operation on the datasets.","b3888a8c":"This is my first time participating in a kaggle competition, so I figured to share the model and hope you guys can give helpfull feedback and exchange ideas.\nStarting out the main BERT model is forked from: 'https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution'.\nI implemented some ideas for pre-treatment of the data. A lot of the tweets contain urls that I think don't really contribute to sentiment identification, often they are also not part of the selected text for positive and negative sentiment tweets.\nAdditionally I noticed there were multiple identical tweets with different labels 'positive\/neutral\/negative', and identical tweets with different selected-texts. Altough it did not seem to improve the score much I did some data treatment on the train set to remove these inconsistensies.\nI also tried to perform some tuning in the loss function, but am not 100% sure there are no errors in here.","0abdf89d":"I noticed that increasing the dropout and training a little longer reduced overfitting","9ff089bc":"Create the submission file.","a98feaa6":"after pretreatment (url filtering) on the train and test set concatenated in df_data, here we split df_data back into train and test.","10bf211a":"First loading libraries, huggingface transformers, and datasets","3cbcd55b":"Here we start the actual work.\nWe loop through the model and find urls using url_parts=['http:\/\/','https:\/\/',' www.']. If a url is found it is replaced by the text '[url]'. My thinking here is that this might help the model to recognize the tweet is making reference to something (the url) while still removing all the large uninformative urls\nThe problem is ofcourse that later the model output selected text does need refer back to the original tweet.\nTo do this I create Tokenizer_indices_cleaned where I record the indice numbers of the original text. If a tweet consists of 10 tokens, the indices are [1,2,3,4,5,6,7,8,9,10]. When a url of for example 4 tokens long is removed theTokenizer_indices_cleaned=[1,2,3,4,5,10]. As a result if the model output predicts that the selected text should start with token number 3 and end with number 6, the theTokenizer_indices_cleaned will be used to convert this back to values 3 and 10.","d16a84cd":"As described by Chris Deotte in https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705 we will need to tokenize the data, locate start and end tokens, and create the attention mask.\n\nA thing to consider here is that the start and end tokens have to be obtained for the tweets after pretreatment. For this I use Tokenizer_indices_cleaned to convert the start\/end positions from the original tweet to the start\/end position of the tweet after treatment. The code looks a bit like a mess but it works.\n","b77dba4d":"Next I noticed there were a few identical tweets that have been labelled differently. This is not so strange as some tweets might be interpreted by one person as positive while another thinks it is neutral. What happened more often is that identical tweets have different selected texts. Which is also not so strange as it is quite arbitrary to differentiate between 'good' and 'good morning' when determining what identifies the tweet sentiment. I guess that it is not optimal to train a model where there are multiple variations of sentiment and selected-text possible. In the below part I find the tweets that are double-labelled in train_mislabeled2 and train_mislabeled3, and change them.\n","5ebc0888":"There is a single tweet that in the trainset that contains no text, this one is removed from the dataset\nAdditionally since we are going to perform url filtering it makes no sense to keep tweets that contain only a url. These tweets are also dropped from the dataset.","4cb42108":"perform the same tokenization on the test data.","9b885bc8":"The Jaccard score is calculated on the validation data. In general I think it helps to set the selected-text for tweets with neutral sentiment to the full tweet text.\nIn this part we use Tokenizer_indices_cleaned_MaxLen to convert the start\/end positions.\n","3eb32544":"In the model training I included a section where for each epoch the jaccard score on the validation data is calculated. The model weights are only saved for epochs where the validation jaccard score improves."}}