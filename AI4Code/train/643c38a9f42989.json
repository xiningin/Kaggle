{"cell_type":{"e93f2d40":"code","2870eb44":"code","0f014b2d":"code","c817c007":"code","b2b2dac8":"code","4f3f6a1a":"code","246d8345":"code","8e0e4dd0":"code","367f1d0e":"code","c37f5e1a":"code","dccf665f":"code","9f71a038":"code","592b72ba":"code","b93cf2b5":"code","1f559ead":"code","b61fc3d4":"code","9ba66214":"code","656cac78":"code","c32333be":"code","7d001ebb":"code","6b612953":"code","69d71663":"code","e30a0567":"code","839762b5":"code","0558434a":"code","240940e6":"code","ff12b42d":"code","54304c0c":"code","6e6b4ccb":"code","18707f80":"code","8d348595":"code","0af57157":"code","0210dcaf":"markdown","117c8e38":"markdown","64794db3":"markdown","63f17dca":"markdown","dda9c050":"markdown","665a0339":"markdown","c2e52c01":"markdown","af7fd9c4":"markdown","3a0a3e08":"markdown","e37df9a1":"markdown"},"source":{"e93f2d40":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report","2870eb44":"input_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(input_)\ndf = data.copy()\n\ndata.head(10)","0f014b2d":"df.describe()","c817c007":"fig = px.histogram(data, \"age\", title=\"Age Distribution\", width=750)\nfig.show()","b2b2dac8":"fig = px.histogram(data, \"time\", title=\"Time Distribution\", width=750)\nfig.show()","4f3f6a1a":"fig = px.histogram(data, \"creatinine_phosphokinase\", title=\"Creatinine Phosphokinase Distribution\", width=750)\nfig.show()","246d8345":"fig = px.histogram(data, \"ejection_fraction\", title=\"Ejection Fraction Distribution\", width=750)\nfig.show()","8e0e4dd0":"fig = px.histogram(data, \"platelets\", title=\"Platelets Distribution\", width=750)\nfig.show()","367f1d0e":"fig = px.histogram(data, \"serum_creatinine\", title=\"Serum Creatinine Distribution\", width=750)\nfig.show()","c37f5e1a":"fig = px.histogram(data, \"serum_sodium\", title=\"Serum Sodium Distribution\", width=750)\nfig.show()","dccf665f":"anaemia_dis = data[\"anaemia\"].value_counts().reset_index()\nfig = px.bar(anaemia_dis, x=\"index\", y=\"anaemia\", title=\"Anaemia Distribution\",\n             width=750, labels={\"index\": \"Anaemia\", \"anaemia\": \"Count\"})\nfig.show()","9f71a038":"diabetes_dis = data[\"diabetes\"].value_counts().reset_index()\nfig = px.bar(diabetes_dis, x=\"index\", y=\"diabetes\", title=\"Diabetes Distribution\", \n             width=750, labels={\"index\": \"Diabetes\", \"diabetes\": \"Count\"})\nfig.show()","592b72ba":"hbp_dis = data[\"high_blood_pressure\"].value_counts().reset_index()\nfig = px.bar(hbp_dis, x=\"index\", y=\"high_blood_pressure\", title=\"High Blood Pressure Distribution\",\n             width=750, labels={\"index\": \"High Blood Pressure\", \"high_blood_pressure\": \"Count\"})\nfig.show()","b93cf2b5":"sex_dis = data[\"sex\"].value_counts().reset_index()\nfig = px.bar(sex_dis, x=\"index\", y=\"sex\", title=\"Sex Distribution\",\n             width=750, labels={\"index\": \"Sec\", \"sex\": \"Count\"})\nfig.show()","1f559ead":"smooking_dis = data[\"smoking\"].value_counts().reset_index()\nfig = px.bar(smooking_dis, x=\"index\", y=\"smoking\", title=\"Sex Distribution\",\n             width=750, labels={\"index\": \"Smooking\", \"smoking\": \"Count\"})\nfig.show()","b61fc3d4":"death_dis = data[\"DEATH_EVENT\"].value_counts().reset_index()\nfig = px.bar(death_dis, x=\"index\", y=\"DEATH_EVENT\", title=\"DEATH EVENT Distribution\",\n             width=750, labels={\"index\": \"DEATH_EVENT\", \"DEATH_EVENT\": \"Count\"})\nfig.show()","9ba66214":"fig = px.pie(data, values='DEATH_EVENT',names='sex', title='GENDER',\n      width=680, height=480)\nfig.show()","656cac78":"f, ax = plt.subplots(figsize=(14,14))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt=\".1f\", ax=ax)\nplt.show()","c32333be":"sns.pairplot(data[['age', 'creatinine_phosphokinase',\n       'ejection_fraction', 'platelets',\n       'serum_creatinine', 'serum_sodium','time',\n       'DEATH_EVENT']], hue=\"DEATH_EVENT\")","7d001ebb":"inp_data = data.drop(data[['DEATH_EVENT']], axis=1)\nout_data = data[['DEATH_EVENT']]\n\nscaler = StandardScaler()\ninp_data = scaler.fit_transform(inp_data)\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=42)","6b612953":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","69d71663":"def weightInitialization(n_features):\n    w = np.zeros((1, n_features))\n    b = 0\n    return w,b","e30a0567":"def sigmoid_activation(result):\n    final_result = 1\/(1 + np.exp(-result))\n    return final_result","839762b5":"def model_optimize(w, b, X, Y):\n    m = X.shape[0]\n    \n    # Prediction\n    final_result = sigmoid_activation(np.dot(w,X.T) + b)\n    cost = (-1\/m)*(np.sum(Y.T * np.log(final_result)) + ((1-Y.T) * (np.log(1-final_result))))\n    \n    # Gradient Calculation\n    dw = (1\/m)*(np.dot(X.T, (final_result-Y.T).T)) # look down (photo)\n    db = (1\/m)*(np.sum(final_result-Y.T))\n    \n    grads = {\n        \"dw\": dw,\n        \"db\": db\n    }\n    \n    return grads, cost","0558434a":"def model_predict(w, b, X, Y, learning_rate, no_iterations):\n    costs = []\n    for i in range(no_iterations):\n        grads, cost = model_optimize(w, b, X, Y)\n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - (learning_rate * dw.T) # look up (photo)\n        b = b - (learning_rate * db)\n        \n        if (i % 100 == 0):\n            costs.append(cost)\n            \n    # final parameters\n    coeff = {\"w\":w, \"b\":b}\n    gradient = {\"dw\":dw, \"db\":db}\n    \n    return coeff, gradient, costs","240940e6":"def predict(final_pred, m):\n    y_pred = np.zeros((1,m))\n    for i in range(final_pred.shape[1]):\n        if final_pred[0][i] > 0.5:\n            y_pred[0][i] = 1\n    return y_pred","ff12b42d":"# Get number of features\nn_features = X_train.shape[1]\nprint('Number of Features: {}'.format(n_features))\n\nw, b = weightInitialization(n_features)\n# Gradient Descent\ncoeff, gradient, costs = model_predict(w, b, X_train, y_train.values.reshape(-1,1), learning_rate=0.0001,no_iterations=4500)\n# Final Prediction\nw = coeff['w']\nb = coeff['b']\nprint('Optimized weights: {}'.format(w))\nprint('Optimized intercept: {}'.format(b))\n\nfinal_train_pred = sigmoid_activation(np.dot(w,X_train.T)+b)\nfinal_test_pred = sigmoid_activation(np.dot(w,X_test.T)+b)\n\nprint(\"=\"*60)\n\ny_train_pred = predict(final_train_pred, X_train.shape[0])\nprint('Training Accuracy             : {:.4f}'.format(accuracy_score(y_train_pred.T, y_train)))\n\ny_test_pred = predict(final_test_pred, X_test.shape[0])\nprint('Test Accuracy                 : {:.4f}'.format(accuracy_score(y_test_pred.T, y_test)))\n\nprint('Logistic Regression f1-score  : {:.4f}'.format(f1_score(y_test_pred.T, y_test)))\nprint('Logistic Regression precision : {:.4f}'.format(precision_score(y_test_pred.T, y_test)))\nprint('Logistic Regression recall    : {:.4f}'.format(recall_score(y_test_pred.T, y_test)))\nprint(\"\\n\",classification_report(y_test_pred.T, y_test))","54304c0c":"cf_matrix = confusion_matrix(y_test_pred.T, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","6e6b4ccb":"from imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\n\nsms = SMOTE(random_state=12345)\nX_res, y_res = sms.fit_sample(inp_data, out_data)\n\nprint(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","18707f80":"X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)\n\nprint('Accuracy of logistic regression classifier on test set: {}'.format(logreg.score(X_test, y_test)))\nprint('Logistic Regression f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('Logistic Regression precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('Logistic Regression recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","8d348595":"cf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","0af57157":"logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure(figsize=(10,6))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Reporting')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","0210dcaf":"## Model Construction (with our own talent)","117c8e38":"## After the SMOTE process (Shortcut for logistic regression)","64794db3":"### Import the necessary packages","63f17dca":"## Reporting\n\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n\n**Correctly predicted -> %81.67 (244 of 299 predict are correct)**\n- True Negative -> %56.67 -> Those who were predicted not to die and who did not die\n- True Positive -> %25.00 -> Those who were predicted to die and who did die\n\n**Wrong predicted-> %18.33 (50 of 299 predict are wrong)**\n- False Positive -> %16.67 -> Those who were predicted to die but who did not die\n- False Negative -> %01.67 -> Those who were predicted to not die but who did die\n\n**Not dead**\n- 203 -> Those who haven't died in the real data set\n- 219 -> Predicted for test data set\n\n**The dead**\n- 96 -> Those who have died in the real data set\n- 80 -> Predicted for test data set","dda9c050":"Cost Formula\n![Cost Formula](https:\/\/miro.medium.com\/max\/2908\/1*dEZxrHeNGlhfNt-JyRLpig.png)","665a0339":"### Data Visualization","c2e52c01":"## Coding Time\n![](https:\/\/ac-cdn.azureedge.net\/infusionnewssiteimages\/agingcare\/21e637ea-aa74-4ae2-b278-181d2cded7a3.jpg)","af7fd9c4":"![](https:\/\/i.ibb.co\/ZV334Mn\/20.png)","3a0a3e08":"![Logistic Regression](https:\/\/i.ibb.co\/Ptg3Czv\/kaggle-ml-part1.png)\n\n- **ML Part 1 - Logistic Regression**\n- **ML Part 2** - K-Nearest Neighbors (KNN)\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN) \n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost\n\n\nLike linear regression, Logistic regression is the right algorithm to start with classification algorithms. Although it has the name 'regression', this is a classification model, not a regression model. It uses a logistical function to frame the binary output model. The output of the logistic regression will be a probability (0\u2264x\u22641) and can be used to predict binary 0 or 1 as output (x <0.5, output = 0, otherwise output = 1).\n\n## Basic Theory\nLogistic Regression behaves quite similar to linear regression. It also calculates the linear output, then follows a storage function through the regression output. The sigmoid function is the logistic function that is used frequently. Below you can clearly see that the z value is the same as the linear regression output in Equation (1).\n\n![](https:\/\/i.ibb.co\/X7NGG5W\/Ek-A-klama-2020-08-27-113356.jpg)\n\nThe value of h (\u03b8) here corresponds to P (y = 1 | x), that is, the probability that the output is binary 1 when input x is given. P (y = 0 | x) will be equal to 1-h ().\n\nWhen the value of z is 0, g (z) will be 0.5. When Z is positive, h () will be greater than 0.5 and the output will be binary 1. Similarly, when z is negative, the value of y will be 0. When we use a linear equation to find the classifier, the output model will also be a linear dimension, i.e. divide the input size into two spaces so that all points in a field correspond to the same label.\n\nThe figure below shows the distribution of a sigmoid function.\n\n![](https:\/\/i.ibb.co\/mtbyBzZ\/Ek-A-klama-2020-08-27-113557.jpg)\n\n## Loss Function\n\nWe cannot use Mean Squared Error as a loss function (like linear regression) because we are using a nonlinear sigmoid function at the end. The MSE function can fetch local minimums and will affect the Gradient Descent algorithm.\n\nSo here we are using Cross Entropy as the missing function. y = 1 and two equations corresponding to y = 0 will be used. The basic logic here is that when my guess is too wrong (for example: y '= 1 & y = 0), the cost will be -log (0), which is infinite.\n\n![](https:\/\/i.ibb.co\/MDRJZKw\/Ek-A-klama-2020-08-27-113827.jpg)\n\nIn the given equation, m represents the training data size, y 'represents the estimated output and y represents the actual output.\n\n\n## Advantages\n- Easy, fast and simple method of classification.\n- \u03b8 parameters describe the direction and density of the importance of independent variables on the dependent variable.\n- It can also be used for multi-class classifications.\n- Its lost function is always convex.\n\n\n## Disadvantages\n- It cannot be applied to nonlinear classification problems.\n- Appropriate feature selection is required.\n- Good signal-to-noise ratio is expected.\n- Collinearity and outliers deteriorate the accuracy of the LR model.\n\n\n## Hyperparameters\nLogistic regression hyperparameters are similar to those of linear regression. The learning speed (\u03b1) and the smoothing parameter (\u03bb) must be set correctly to achieve high accuracy.\n\n\n## Comparison with Other Models\n\n\n\n![Logistic Regression vs SVM](https:\/\/i.ibb.co\/MDTY8GG\/lg-vs-svm.png)\n- While SVM can handle nonlinear solutions, logistic regression can only process linear solutions.\n- Linear SVM manages outliers better as it achieves maximum margin solution.\n- The hinge loss in SVM outperforms its daily loss in LR.\n\n![Logistic Regression vs Decision Trees](https:\/\/i.ibb.co\/3czHSLR\/lg-vs-Decision-Trees.png)\n- Decision tree handles collinearity better than LR.\n- Decision trees cannot deduce the importance of features, but LR can.\n- Decision trees are better for categorical values than LR.\n\n![Logistic Regression vs Neural Network(NN)](https:\/\/i.ibb.co\/PTCJCx8\/lr-vs-Neural-Network.png)\n- NN can support nonlinear solutions that LR cannot.\n- LR has a convex loss function so it won't hang at a local minimum whereas NN can hang.\n- While LR performs better than NN when training data is less and features are large, NN needs large training data.\n\n![Logistic Regression vs Decision Trees](https:\/\/i.ibb.co\/Z8cgqwd\/lg-vs-naive-bayes.png)\n- Naive Bayes is a productive model, while LR is a distinctive model.\n- Naive Bayes works well with small data sets, whereas LR regulation can provide similar performance.\n- Since Naive Bayes expects all features to be independent, LR outperforms Naive Bayes on linearity.\n\n![Logistic Regression vs KNN](https:\/\/i.ibb.co\/tXjJMmM\/lr-vs-knn.png)\n- KNN is a nonparametric model in which LR is a parametric model.\n- KNN is relatively slower than Logistic Regression.\n- KNN supports nonlinear solutions where LR only supports linear solutions.\n- LR can derive (about the estimate) confidence level while KNN can only omit tags.","e37df9a1":"### Import and read dataset"}}