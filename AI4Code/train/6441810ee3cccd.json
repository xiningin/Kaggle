{"cell_type":{"c59d55e2":"code","a19dd3e9":"code","d70b7c12":"code","bd155278":"code","e3da234a":"code","f30c0fc1":"code","90e1b1ca":"code","27f71dc9":"code","a4efd203":"code","c41b0f6d":"code","5ed00355":"code","82013ee2":"code","71e2ef3a":"code","990f2de2":"code","1b2fa410":"code","5bed0e5b":"code","f5d64f05":"code","07b0e041":"code","ad1139e4":"code","79b86567":"code","d462defe":"code","432cc3de":"code","6cdaebe6":"code","e9c80644":"code","00986a98":"markdown","d52664e8":"markdown","01ee6573":"markdown","773428d4":"markdown","0c112106":"markdown","11779a4f":"markdown","9fb8a5cd":"markdown","879e5939":"markdown","f2c532a2":"markdown","fda8133f":"markdown","3eec4c16":"markdown","e53f261b":"markdown","163e6ec4":"markdown","158b5a76":"markdown","7051ca25":"markdown","7f3ed5d6":"markdown","eae261f3":"markdown","d4f18bf9":"markdown","2cd6fd97":"markdown","bfeff785":"markdown"},"source":{"c59d55e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split \n\n# KERAS \nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\n\n\nimport warnings \n# Filter Warnings \nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a19dd3e9":"IMG_HEIGHT = 64 \nIMG_WIDTH = 64 \nPIX = IMG_HEIGHT * IMG_WIDTH ","d70b7c12":"# Load data set \nx_l = np.load('..\/input\/sign-language-digits-dataset\/X.npy')\nY_l = np.load('..\/input\/sign-language-digits-dataset\/Y.npy')\n\nplt.subplot(1,2,1)\nplt.imshow(x_l[260].reshape(IMG_WIDTH, IMG_HEIGHT))\nplt.axis('off')\n\nplt.subplot(1,2,2)\nplt.imshow(x_l[900].reshape(IMG_WIDTH, IMG_HEIGHT))\nplt.axis('off')\n\nplt.show()","bd155278":"# Prepare our X and Y \n\nX = np.concatenate((x_l[204:409], x_l[822:1027]), axis=0)\nY = np.concatenate((np.zeros(205), np.ones(205)), axis=0).reshape(X.shape[0], 1)\n\nprint(f\"X's shape is {X.shape} and Y's shape is {Y.shape}\")","e3da234a":"# Then let's create our train and test sets \n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size= 0.15, random_state=42)\n\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","f30c0fc1":"X_train_flatten = X_train.reshape(number_of_train, PIX) \nX_test_flatten = X_test.reshape(number_of_test, PIX)\nprint(f\"X_train_flatten's shape is {X_train_flatten.shape} and X_test_flatten's shape is {X_test_flatten.shape}\")","90e1b1ca":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T \ny_train = Y_train.T \ny_test = Y_test.T \nprint('X train shape', x_train.shape)\nprint('X test shape', x_test.shape)\nprint('Y train shape', y_train.shape)\nprint('X test shape', y_test.shape)","27f71dc9":"def initialize_parameters(dimension): \n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    \n    return w, b","a4efd203":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","c41b0f6d":"# Example \nz = 15\ny_head = sigmoid(z) \ny_head","5ed00355":"def forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T, x_train) + b \n    y_head = sigmoid(z)\n    loss = -y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    \n    return cost","82013ee2":"def forward_backward_propagation(w,b,x_train,y_train):\n    # Forward pro\n    z = np.dot(w.T, x_train) + b \n    y_head = sigmoid(z)\n   \n    \n    loss = -y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    \n    # Backward pro. \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost, gradients","71e2ef3a":"def update(w,b,x_train,y_train, learning_rate, number_of_iteration): \n    cost_list = [] \n    cost_list2 = [] \n    index = [] \n    \n    for i in range(number_of_iteration): \n        cost, gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients['derivative_weight']\n        b = b - learning_rate * gradients['derivative_bias']\n        \n        if i % 10 == 0: \n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n            \n    parameters = {'weight' : w , 'bias': b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","990f2de2":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_predict = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0 \n        else : \n            y_predict[0,i] = 1 \n        \n    return y_predict\n","1b2fa410":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations): \n    dimension = x_train.shape[0]\n    w,b = initialize_parameters(dimension)\n    \n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate, num_iterations)\n    \n    y_prediction_test = predict(parameters['weight'], parameters['bias'], x_test)    \n    y_prediction_train = predict(parameters['weight'], parameters['bias'], x_train)\n    \n     # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)\n","5bed0e5b":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state=45, max_iter=150)\nprint('Test accuracy:' , logreg.fit(x_train.T, y_train.T).score(x_test.T,y_test.T))\n#print('Train accuracy:' , logreg.fit(x_train.T, y_train.T).score(x_test.T,y_test.T))","f5d64f05":"def initialize_parameters_and_layer_sizes_NN(x_train,y_train):\n    parameters = {\n        'weight1':np.random.randn(3, x_train.shape[0]) * 0.1 ,\n        'bias1': np.zeros((3,1)),\n        'weight2':np.random.randn(y_train.shape[0],3) * 0.1, \n        'bias2':np.zeros((y_train.shape[0],1))\n                 } \n    return parameters","07b0e041":"def forward_propagation_NN(x_train,parameters):\n    Z1 = np.dot(parameters['weight1'],x_train ) + parameters['bias1']\n    A1 = np.tanh(Z1) \n    Z2 = np.dot(parameters['weight2'],A1 ) + parameters['bias2'] \n    A2 = np.tanh(Z2)\n    \n    cache = {\n        'Z1': Z1, 'A1':A1, 'Z2':Z2, 'A2':A2\n    } \n    \n    return A2, cache","ad1139e4":"def compute_cost_NN(A2,Y,parameters): \n    logprobs = np.multiply(np.log(A2), Y)\n    cost = -np.sum(logprobs) \/ Y.shape[1]\n    return cost ","79b86567":"def backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","d462defe":"def update_parameters(parameters,grads,learning_rate=0.01):\n    parameters = {\n        'weight1' : parameters['weight1'] - learning_rate* grads['dweight1'],\n        \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n        \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n        \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]\n    }\n    \n    return parameters","432cc3de":"def predict_NN(parameters,x_test):\n    A2, cache = forward_propagation_NN(x_test, parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            Y_prediction[0,i] = 0 \n        else: \n            Y_prediction[0,i] = 1\n            \n    return Y_prediction","6cdaebe6":"def two_layer_neural_network(x_train,y_train,x_test,y_test,num_iterations):\n    cost_list = [] \n    index_list = [] \n    \n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n    \n    for i in range(0,num_iterations):\n        A2,cache  = forward_propagation_NN(x_train, parameters)\n        \n        cost = compute_cost_NN(A2, y_train, parameters)\n        \n        grads = backward_propagation_NN(parameters,cache, x_train, y_train)\n        \n        parameters = update_parameters(parameters,grads, learning_rate=0.01) \n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n    \n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=2500)","e9c80644":"def build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[0]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train.T, y = y_train.T, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","00986a98":"## Prediction ","d52664e8":"Now, we can update our weights and biasses.[](http:\/\/)","01ee6573":"## Loss and Cost Function \n* We will use the Cross Entropy Function as a loss function ","773428d4":"### Backward Propagation and Gradient Descent Algorithm \n* We've covered initialization processes and forward propagation. \n* Now, We know what our cost is, so we can start to train our model. \n* Actually, our main goal is decreasing our cost, so if cost is high, our model don't work well. \n    * Let's think first step, every thing starts with initializing weights and bias. Therefore cost is dependen on them. \n    * In order to decrease cost, we need to update weights and bias.\n    * To do this, we have to use Gradient Descent Alg. and we will update our biasses and weights \n    ","0c112106":"## Forward Propagation \n* FP is almost the same with logistic regression.\n* The only difference is we use tanh funct instead of sigmoid func and we make the all proccess twice. \n* Also, Numpy has the tanh function, we can easily implement it.","11779a4f":"### Forward Propagation \n* The whole step from inputs to cost is called forward propagation\n    * z = (w.T).x + b => in this equation, we know that x is our pixel array,w(weights), b(bias) \n    * Now, our result is z. After this, we're gonna put our equation into the sigmoid function, and it gives an output for it between 0 and 1. \n    * We use sigmoid because of their y interval. its output has to be between 0 and 1. And we can use our output as a probability. \n    * Then we calculate loss(error) function. Cost function is summation of all loss(error).\n    ","9fb8a5cd":"# Logistic Regression \n* When we talk about binary classification, you can firstly think of Logistic Regression Algorithm. \n* Actually, Logistic Regression is not a deep learning algorithm, but we can easily accept it is a very simple neural network. \n* In order to understand what ANN is, we have to understand the Logistic Regr. ","879e5939":"## At first \n* At first, we will use just 0 and 1\n* After this, we improve our model for all classes \n* In data, sign zero is between indexes 204 and 408. Number of zero sign is 205.\n* Also sign one is between indexes 822 and 1027. Number of one sign is 206. Therefore, we will use 205 samples from each classes(labels).","f2c532a2":"* Now, we have 3d array for our X's and have 2d array our test. \n* We need 2d array for training proccess, in this case we have already 2d array for test datasets \n* So we need to reshape our train sets ","fda8133f":"* Like you see, we have 348 images and each image has 4096 pixels for train set \n* we have 62 images and each image has 4096 pixels for test set \n* so now we need to transpose our matrix because of multiplication process. We do this because we're gonna multiply our weights matrix with our features. ","3eec4c16":"# Constants","e53f261b":"# Neural Net with KERAS\n* We did it with 2 hidden layer because of simplicity and we did all of things by ourself because we had to understand what's going on under the hood. \n* Now, we can use libraries to implement our neural network like Keras. \n* If we use these libraries, we don't have to determine lots of things, we don't have to calculate anything, because Keras does it for us. ","163e6ec4":"# Artifical Neural Network \n* In logistic regression, there are just input and output layer. However, there is least one hidden layer between input and output layer in ANN. \n","158b5a76":"### Initializing parameters\n* Our input that is our images are 4096, and every input has own weights. \n* The first step is multiplying each pixels with their own weights. \n* All rigth, What are these weights initial values ? \n* First, we have to give them randomly.","7051ca25":"## Update Parameters ","7f3ed5d6":"## Backward Propagation ","eae261f3":"# Imports","d4f18bf9":"## Size of Layers and Initializing Parameters weights and bias  ","2cd6fd97":"# Create Model \n* Let's put them all together ","bfeff785":"*  Up to this point we learn our parameters. It means we fit the data. \n*  In order to predict we should have parameters. so, let's predict."}}