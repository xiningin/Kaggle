{"cell_type":{"61f317bf":"code","d9a9df0f":"code","d7f037a8":"code","5a3e052c":"code","4efbcae4":"code","6f0243de":"code","3bb33596":"code","2fd63f69":"code","ea58b17d":"code","44b58043":"code","8e613ee1":"code","065dd921":"code","5dddcfc5":"code","48ff89c1":"markdown","1a21143f":"markdown","54bd5d7a":"markdown","5c3ca879":"markdown","0e9f241e":"markdown","51dcbfc1":"markdown","8e191493":"markdown","3e313e81":"markdown","9f5631bc":"markdown","1a1a7938":"markdown","19cc0bdc":"markdown","66bd9c5b":"markdown","77c40e45":"markdown","9960b938":"markdown","4e3ca310":"markdown","3017abb8":"markdown","fbe9db8e":"markdown","6eaa20a0":"markdown"},"source":{"61f317bf":"import os\nfrom glob import glob\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Binarizer\nimport tensorflow as tf\nfrom tensorflow.data import Dataset, AUTOTUNE\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPool2D, SpatialDropout2D, Concatenate, LeakyReLU","d9a9df0f":"RANDOM_STATE = 7\nIMAGE_SIZE = (384, 512)\nVAL_SPLIT = 0.1\nBATCH_SIZE = 64\nSHUFFLE_BUFFER = 400\nLR_ALPHA = 0.3\nLEARNING_RATE = 1e-4\nEPOCHS = 25\nPLOTS_DPI = 150\nPATIENCE = 3","d7f037a8":"all_train_files = glob(\"..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset\/*\/*\/*.png\")\ntrain_gt_files = glob(\"..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset\/*\/* GT\/*.png\")\ntrain_image_files = list(set(all_train_files) - set(train_gt_files))\n\ntrain_image_files = np.array(sorted(train_image_files))\ntrain_gt_files = np.array(sorted(train_gt_files))\n\nlen(train_image_files), len(train_gt_files)","5a3e052c":"X_train, X_val, y_train, y_val = train_test_split(train_image_files, train_gt_files, test_size = VAL_SPLIT)\n\ntrain_ds = Dataset.from_tensor_slices((X_train, y_train))\nval_ds = Dataset.from_tensor_slices((X_val, y_val))\n\ntrain_ds, val_ds","4efbcae4":"def load_images(original_file, gt_file):\n\n    original_img = tf.io.read_file(original_file)\n    original_img = tf.io.decode_png(original_img, channels = 3)\n    original_img = tf.image.resize(original_img, IMAGE_SIZE)\n    \n    gt_img = tf.io.read_file(gt_file)\n    gt_img = tf.io.decode_png(gt_img, channels = 1)\n    gt_img = tf.image.resize(gt_img, IMAGE_SIZE)\n    \n    return original_img\/255.0, gt_img\/255.0\n\ntrain_dataset = train_ds.shuffle(SHUFFLE_BUFFER, seed = RANDOM_STATE) \\\n                        .map(load_images, num_parallel_calls = AUTOTUNE) \\\n                        .batch(BATCH_SIZE, drop_remainder = True) \\\n                        .prefetch(buffer_size = AUTOTUNE)\n    \nval_dataset = val_ds.map(load_images, num_parallel_calls = AUTOTUNE) \\\n                    .batch(BATCH_SIZE, drop_remainder = True) \\\n                    .prefetch(buffer_size = AUTOTUNE)\n\ntrain_dataset, val_dataset","6f0243de":"sample_images = [i for i  in train_dataset.take(1)][0]\n\nfig, axes = plt.subplots(4, 4,figsize = (16, 12))\naxes = axes.flatten()\n\nfor i in range(8):    \n    axes[i * 2].imshow(sample_images[0][i].numpy())\n    axes[i * 2].grid(False)\n    axes[i * 2].axis(False)\n    \n    axes[(i * 2) + 1].imshow(sample_images[1][i].numpy(), cmap = 'gray')\n    axes[(i * 2) + 1].grid(False)\n    axes[(i * 2) + 1].axis(False)\n    \nplt.suptitle(\"Training data\", fontsize = 24)\nplt.tight_layout()\nplt.show()","3bb33596":"sample_images = [i for i  in val_dataset.take(1)][0]\n\nfig, axes = plt.subplots(4, 4,figsize = (16, 12))\naxes = axes.flatten()\n\nfor i in range(8):    \n    axes[i * 2].imshow(sample_images[0][i].numpy())\n    axes[i * 2].grid(False)\n    axes[i * 2].axis(False)\n    \n    axes[(i * 2) + 1].imshow(sample_images[1][i].numpy(), cmap = 'gray')\n    axes[(i * 2) + 1].grid(False)\n    axes[(i * 2) + 1].axis(False)\n    \nplt.suptitle(\"Validation data\", fontsize = 24)\nplt.tight_layout()\nplt.show()","2fd63f69":"def unet_model():\n    input_layer = Input(shape = (*IMAGE_SIZE, 3), name = 'Input_Layer')\n    \n    conv_1 = Conv2D(8, 5, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_1')(input_layer)\n    pool_1 = MaxPool2D(name = 'Max_Pool_1')(conv_1)\n    conv_2 = Conv2D(16, 5, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_2')(pool_1)\n    pool_2 = MaxPool2D(name = 'Max_Pool_2')(conv_2)\n    spd_1 = SpatialDropout2D(0.1, name = 'SPD_1')(pool_2)\n    \n    conv_3 = Conv2D(32, 4, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_3')(spd_1)\n    pool_3 = MaxPool2D(name = 'Max_Pool_3')(conv_3)\n    conv_4 = Conv2D(64, 4, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_4')(pool_3)\n    pool_4 = MaxPool2D(name = 'Max_Pool_4')(conv_4)\n    spd_2 = SpatialDropout2D(0.1, name = 'SPD_2')(pool_4)\n    \n    conv_5 = Conv2D(128, 3, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_5')(spd_2)\n    pool_5 = MaxPool2D(name = 'Max_Pool_5')(conv_5)\n    conv_6 = Conv2D(256, 3, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_6')(pool_5)\n    pool_7 = MaxPool2D(name = 'Max_Pool_6')(conv_6)\n    spd_3 = SpatialDropout2D(0.1, name = 'SPD_3')(pool_7)\n    \n    conv_7 = Conv2D(512, 2, padding = 'same', activation = LeakyReLU(LR_ALPHA), name = 'Conv_7')(spd_3)\n    pool_7 = MaxPool2D(name = 'Max_Pool_7')(conv_7)\n    \n    conv_t_1 = Conv2DTranspose(256, 2, padding = 'same', strides = 2, activation = LeakyReLU(LR_ALPHA), name = 'Conv_T_1')(pool_7)\n    concat_1 = Concatenate(name = 'Concat_1')([conv_t_1, spd_3])\n    spd_4 = SpatialDropout2D(0.1, name = 'SPD_4')(concat_1)\n    \n    conv_t_2 = Conv2DTranspose(128, 3, padding = 'same', strides = 2, activation = LeakyReLU(LR_ALPHA), name = 'Conv_T_2')(spd_4)\n    conv_t_3 = Conv2DTranspose(64, 3, padding = 'same', strides = 2, activation = LeakyReLU(LR_ALPHA), name = 'Conv_T_3')(conv_t_2)\n    concat_2 = Concatenate(name = 'Concat_2')([conv_t_3, spd_2])\n    spd_5 = SpatialDropout2D(0.1, name = 'SPD_5')(concat_2)\n    \n    conv_t_4 = Conv2DTranspose(32, 4, padding = 'same', strides = 2, activation = LeakyReLU(LR_ALPHA), name = 'Conv_T_4')(spd_5)\n    conv_t_5 = Conv2DTranspose(16, 4, padding = 'same', strides = 2, activation = LeakyReLU(LR_ALPHA), name = 'Conv_T_5')(conv_t_4)\n    concat_3 = Concatenate(name = 'Concat_3')([conv_t_5, spd_1])\n    spd_6 = SpatialDropout2D(0.1, name = 'SPD_6')(concat_3)\n    \n    conv_t_6 = Conv2DTranspose(8, 5, padding = 'same', strides = 2, activation = LeakyReLU(LR_ALPHA), name = 'Conv_T_6')(spd_6)\n    conv_t_7 = Conv2DTranspose(1, 5, padding = 'same', strides = 2, activation = 'sigmoid', name = 'Conv_T_7')(conv_t_6)    \n    \n    return Model(inputs = input_layer, outputs = conv_t_7, name = 'Fish_Segmentation')\n                 \nmodel = unet_model()\nmodel.compile(optimizer = Adam(LEARNING_RATE), loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel.summary()","ea58b17d":"plot_model(model, to_file = 'model.jpg', show_shapes = True, dpi = PLOTS_DPI)","44b58043":"%%time\n\nearly_stop = EarlyStopping(monitor = 'val_loss', patience = PATIENCE, restore_best_weights = True)\n\nhistory = model.fit(\n    train_dataset,\n    epochs = EPOCHS,\n    validation_data = val_dataset,\n    callbacks = [early_stop]\n)","8e613ee1":"loss = history.history['loss']\nval_loss = history.history['val_loss']\n\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\nepochs_range = history.epoch\n\nplt.figure(figsize = (18, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\nplt.legend(loc = 'upper right', fontsize = 14)\nplt.ylim(0, None)\nplt.title('Loss', fontsize = 20)\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, accuracy, label = 'Training Accuracy')\nplt.plot(epochs_range, val_accuracy, label = 'Validation Accuracy')\nplt.legend(loc = 'lower right', fontsize = 14)\nplt.title('Accuracy', fontsize = 20)\n\nplt.suptitle(\"Evaluation Metrics\", fontsize = 24)\nplt.savefig('loss_and_accuracy.jpg', dpi = PLOTS_DPI, bbox_inches = 'tight')\nplt.show()","065dd921":"all_test_files = np.array(glob(\"..\/input\/a-large-scale-fish-dataset\/NA_Fish_Dataset\/*\/*\"))\n\nlen(all_test_files)","5dddcfc5":"def load_test_image(filepath):\n    img = tf.io.read_file(filepath)\n    img = tf.io.decode_png(img, channels = 3)\n    img = tf.image.resize(img, IMAGE_SIZE)\n    return img\/255.0\n\nrnd_index = np.random.randint(len(all_test_files), size = 6)\npred_imgs = []\n\nfor i in rnd_index:\n    pred_imgs.append(load_test_image(all_test_files[i]))\n    \npred = model.predict(np.array(pred_imgs))\npred_mask = Binarizer(threshold = 0.5).transform(pred.reshape(-1, 1)).reshape(pred.shape)\n\nfig, axes = plt.subplots(6, 4,figsize = (15, 18))\naxes = axes.flatten()\n\nfor i in range(6):    \n    axes[i * 4].imshow(pred_imgs[i])\n    axes[i * 4].grid(False)\n    axes[i * 4].axis(False)\n    axes[i * 4].set_title(f\"Test sample #{i + 1}\", fontsize = 16)\n    \n    axes[(i * 4) + 1].imshow(pred[i], cmap = 'gray')\n    axes[(i * 4) + 1].grid(False)\n    axes[(i * 4) + 1].axis(False)\n    axes[(i * 4) + 1].set_title(f\"Soft mask #{i + 1}\", fontsize = 16)\n    \n    axes[(i * 4) + 2].imshow(pred_mask[i], cmap = 'gray')\n    axes[(i * 4) + 2].grid(False)\n    axes[(i * 4) + 2].axis(False)\n    axes[(i * 4) + 2].set_title(f\"Binary mask #{i + 1}\", fontsize = 16)\n    \n    axes[(i * 4) + 3].imshow(pred_imgs[i] * pred_mask[i])\n    axes[(i * 4) + 3].grid(False)\n    axes[(i * 4) + 3].axis(False)\n    axes[(i * 4) + 3].set_title(f\"Masked image #{i + 1}\", fontsize = 16)\n    \nplt.suptitle(\"Segmentation predictions\", fontsize = 24)\nplt.tight_layout(rect = [0, 0, 1, 0.98])\nplt.savefig('predictions.jpg', dpi = PLOTS_DPI, bbox_inches = 'tight')\nplt.show()","48ff89c1":"## Constants","1a21143f":"### Loading file paths for test images","54bd5d7a":"### Schematic Diagram","5c3ca879":"### Splitting the data into train and validation datasets","0e9f241e":"#### Validation dataset","51dcbfc1":"### Loading all image file paths","8e191493":"### Sample data points","3e313e81":"## Model Evaluation","9f5631bc":"## Model training","1a1a7938":"## Data Loading","19cc0bdc":"### Model design and summary","66bd9c5b":"### Sample test data points and get predictions and binary mask","77c40e45":"## Model Building","9960b938":"## Imports","4e3ca310":"### Create `tf.data.Dataset` object for loading images for training and validation","3017abb8":"## Overview\nThis model implements a U-Net style architecture to obtain a binary segmentation mask to identify the portion of the image that is a fish and the portion that is not.","fbe9db8e":"## Predictions","6eaa20a0":"#### Training dataset"}}