{"cell_type":{"8ad041d3":"code","a07d2b73":"code","bca9eb43":"code","f6a0aabe":"code","e26b2844":"code","3368eac6":"code","28c7401b":"code","6a00771c":"code","1b3d9cca":"code","39d4d0d0":"code","06c6998b":"code","f250aa9c":"code","8bb0f451":"code","5834e77d":"code","c40c8a60":"code","8c817d3a":"code","f4f9945f":"code","66baba35":"code","e90694df":"code","d8f3f2bb":"code","b88bba37":"code","9ca61eed":"code","a25555bf":"code","eac24994":"code","07fb5423":"code","44eaa3cf":"markdown","eaa541d6":"markdown","7c9a1b87":"markdown","fe813506":"markdown","a9efd0a3":"markdown","bc9c0263":"markdown","f5390135":"markdown","eada684c":"markdown","931e031a":"markdown","e03647a0":"markdown","6da36376":"markdown","d515f3ff":"markdown","10038077":"markdown","33244f05":"markdown","29e94602":"markdown","a688ec4b":"markdown","86f0f060":"markdown","44247cc4":"markdown","161b71a0":"markdown","b6a3aac9":"markdown","00e473e9":"markdown","9616b06f":"markdown","fb787e15":"markdown"},"source":{"8ad041d3":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data.sampler import SubsetRandomSampler \nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nimport torch.functional as F\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport seaborn as sns\nplt.ion()","a07d2b73":"os.listdir('..\/input\/back-ground-for-xnature\/gdxray')","bca9eb43":"os.listdir('..\/input\/xnaturev2\/xnature\/XNature')","f6a0aabe":"# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms =transforms.Compose([\n            transforms.Resize((224,224)),\n            transforms.ToTensor()\n            ])\n\n\n\ndata_dir = '..\/input\/xnaturev2\/xnature\/XNature'\n# loading datasets with PyTorch ImageFolder\nimage_dataset = datasets.ImageFolder(data_dir,\n                                          data_transforms)\n# defining data loaders to load data using image_datasets and transforms,\n# here we also specify batch size for the mini batch\ndataloader =  torch.utils.data.DataLoader(image_dataset, batch_size=4,\n                                             shuffle=True, num_workers=4)\n\ndataset_size = len(image_dataset)\nclass_names = image_dataset.classes\n\nuse_gpu = torch.cuda.is_available()","e26b2844":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n    \ncount = 0\nstart = time.time()\n\nfor batch_id, (images,labels) in enumerate(dataloader): \n    count += 1\n    imshow(torchvision.utils.make_grid(images, nrow = 4))\n    \n    if count == 5:\n          break","3368eac6":"# Creating data indices for training and validation splits:\nvalidation_split = 0.2\nbatch_size = 16\n\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\n\n# Shuffle the dataset\nnp.random.shuffle(indices)\n\ntrain_indices, val_indices = indices[split:], indices[:split]\n\n# Creating PT data samplers and loaders:\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(image_dataset, batch_size=batch_size, \n                                           sampler=train_sampler)\nvalidation_loader = torch.utils.data.DataLoader(image_dataset, batch_size=batch_size,\n                                                sampler=valid_sampler)\n\ndataloaders = {'train': train_loader, 'test': validation_loader}\ndataset_sizes = {'train': len(train_indices), 'test':len(val_indices)}\n","28c7401b":"def train_model(model, criterion, optimizer, num_epochs=10):\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                #scheduler.step()\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda())\n                    labels = Variable(labels.cuda())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                outputs = model(inputs)\n                \n                _, preds = torch.max(outputs.data, 1)\n                loss = criterion(outputs, labels)\n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n\n                # statistics\n                running_loss += loss.data.to('cpu')\n                running_corrects += torch.sum(preds == labels.data).to('cpu').numpy()\n            \n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects \/ dataset_sizes[phase]\n            \n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n                state = {'model':model_ft.state_dict(),'optim':optimizer_ft.state_dict()}\n                torch.save(state,'point_resnet_best.pth')\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best test Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","6a00771c":"model_ft = models.resnet18(pretrained=True) # loading a pre-trained(trained on image net) resnet18 model from torchvision models\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names))      # changing the last layer for this dataset by setting last layer neurons to 200 as this dataset has 200 categories\n \nif use_gpu:                                 # if gpu is available then use it\n    model_ft = model_ft.cuda()       \ncriterion = nn.CrossEntropyLoss()           # defining loss function\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)","1b3d9cca":"model_ft = train_model(model_ft, criterion, optimizer_ft,num_epochs=1)","39d4d0d0":"for batch_id, (images,labels) in enumerate(validation_loader): \n    correct = True\n    outputs = model_ft(images.cuda())\n    _, predicted = torch.max(outputs, 1)\n    \n    preddictions = predicted.cpu().numpy()\n    \n\n    for i in range(len(preddictions)):\n        if preddictions[i] != labels[i].cpu().numpy():\n            correct = False\n    \n    if not correct:\n        imshow(torchvision.utils.make_grid(images, nrow = 8))     \n        for i in range(len(preddictions)):\n            print(class_names[preddictions[i]], end='\\t')\n","06c6998b":"!wget -c -O anaconda.sh 'https:\/\/repo.continuum.io\/archive\/Anaconda3-5.1.0-Linux-x86_64.sh'\n!bash anaconda.sh -b\n!cd \/usr\/bin\/ && ln -sf \/content\/anaconda3\/bin\/conda conda\n!yes y | conda install faiss-gpu -c pytorch","f250aa9c":"import time\nimport os \nimport annoy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom PIL import Image, ImageOps\n\n\nimport torch\nimport torchvision","8bb0f451":"from sklearn.neighbors import NearestNeighbors\n\ndef retrieve_images(index_embeddings, querry_embeddings, index_labels, querry_labels, k=5):\n    '''\n    Retriev k images form the index for each querry image and compute mean values of p@k (precision at rank k)\n    \n    args:\n        index_embedding (dict): keys are image names, and values are the associated embeddings\n        querry_embedding (dict): keys are image names, and values are the associated embeddings\n        index_labels (dict): keys are image names, and values are a list of labels present in the image\n        querry_labels (dict): keys are image names, and values a list of labels present in the image\n        k (int): The rank to consider when computing precision@k\n        \n    outputs:\n        retrieval_results (dict): key is a querry image name, and value is a k-size list of index retrieved images\n        precision_at_k (float): precision at rank k\n    '''\n    d = 512\n    nb = len(index_embeddings)\n    nq = len(querry_embeddings)\n\n    xb = np.zeros((nb,d),dtype=np.float32)\n    yb = nb*[None]\n    index_names = nb*[None]\n\n    xq = np.zeros((nq,d),dtype=np.float32)\n    yq = nq*[None]\n    querry_names = nq*[None]\n\n    for ii, image in enumerate(index_embeddings.keys()):\n        xb[ii,:] = index_embeddings[image]\n        yb[ii] = index_labels[image]\n        index_names[ii] = image\n    \n    for ii, image in enumerate(querry_embeddings.keys()):\n        xq[ii,:] = querry_embeddings[image]\n        yq[ii] = querry_labels[image]\n        querry_names[ii] = image\n\n    '''\n    # Building Index  \n    index = faiss.IndexFlatL2(d)\n    index.add(xb)\n    # Searching\n    D, I = index.search(xq, k) \n    '''\n\n    \n    # Building Index  \n    nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(xb)\n    D, I = nbrs.kneighbors(xq)\n\n    retrieval_results = {}\n\n    # Compute precision\n    precision_at_k = 0\n    for querry in range(nq):\n        neighbours = I[querry]\n        retrievad_images = []\n        are_correct_retrievals = []\n    \n        relevant_retrievals = 0\n        for neighbour in neighbours:\n            relevant_retrievals += (len(set(yq[querry]) & set(yb[neighbour]))!=0)\n            retrievad_images.append(index_names[neighbour])\n            are_correct_retrievals.append((len(set(yq[querry]) & set(yb[neighbour]))!=0))\n    \n        retrieval_results[querry_names[querry]] = (retrievad_images,are_correct_retrievals)\n    \n        precision_at_k += relevant_retrievals\/(k*nq)\n    \n    print('mean p@k: ', precision_at_k)\n    \n    return retrieval_results, precision_at_k\n\n\ndef visualize_retrieval(retrieval_results, k=5, nbr_querries=5):\n    '''\n    Produces a plot of retrieved images for a fixed number of querry images,\n    coorect retrievals are bounded in green and uncorrect ones are bounded in red.\n    args:\n        retrieval_results (dict): key is paths to querry image, and value is a tuple of two lists\n                                the first list contain path to retrieved images and the second \n                                store a boolean that represnets if the retrieval is correct\n        k (int): Number of image to retreival for each querry image\n        nbr_querries (int): The number of querry images to show\n    '''\n    \n    count = 0\n    for querry_image_name,(retrieved_images_names, are_correct_retrievals) in retrieval_results.items():\n        w=10\n        h=10\n        fig=plt.figure(figsize=(16, 16))\n        columns = k+1\n        rows = 1\n        querry_image = Image.open(querry_image_name)\n        fig.add_subplot(rows, columns, 1)\n        plt.imshow(querry_image,cmap='gray')\n        for ii, retrieved_image_name in enumerate(retrieved_images_names):\n            retrieved_image = Image.open(retrieved_image_name)\n            retrieved_image = retrieved_image.convert('RGB')\n            if are_correct_retrievals[ii]:\n                retrieved_image = ImageOps.expand(retrieved_image,\n                                                  border=int(retrieved_image.size[0]*0.1),\n                                                  fill='rgb(0,255,0)')\n            else:\n                retrieved_image = ImageOps.expand(retrieved_image,\n                                                  border=int(retrieved_image.size[0]*0.1),\n                                                  fill='rgb(255,0,0)')\n                \n            fig.add_subplot(rows, columns, ii+2)\n            plt.imshow(retrieved_image, cmap='gray')\n   \n        count+=1\n        if count==nbr_querries:\n            break\n        \n        plt.show()\n        \n\ndef get_embedding_of_image(image_name, model):\n    '''\n    args:\n        image_name (str): Path to the image of interest\n        model (torch.nn.Module): THe model to use for embedding computing\n    '''\n    model.eval()\n    image_pil = Image.open(image_name).resize([256,256])\n\n    image_array = np.array(image_pil)\/(255)\n\n    image_rgb = np.stack((image_array,)*3, axis=-1)\n\n    # swap color axis because\n    # numpy image: H x W x C\n    # torch image: C X H X W \n    image_tensor = torch.from_numpy(image_rgb.transpose((2, 0, 1)))\n\n    image_tensor = image_tensor.expand(1,-1,-1,-1)\n\n    embeddings = model(image_tensor.float().cuda())\n    \n    return embeddings.cpu().detach().numpy()","5834e77d":"data_dir = '..\/input\/xnaturev2\/xnature\/XNature'\nimages = {}\n\nfor dir_ in os.listdir(data_dir):\n    images_path = os.path.join(data_dir,dir_)\n    for file in os.listdir(images_path):\n        images[os.path.join(images_path, file)] = dir_\n        \nimages = {'path': list(images.keys()),\n           'labels': list(images.values())}\n\ndf = pd.DataFrame.from_dict(images)\ndf_train, df_test =train_test_split(df, test_size = 0.2)","c40c8a60":"from torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\n\n\n## Transforms\nclass Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, int)\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        sample['querry_image'] = sample['querry_image'].resize([self.output_size,self.output_size])\n        sample['image1'] = sample['image1'].resize([self.output_size,self.output_size])\n        sample['image2'] = sample['image2'].resize([self.output_size,self.output_size])\n\n        return sample\n    \nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n    def __call__(self, sample):   \n        sample['querry_image'] = np.array(sample['querry_image'])\/(255)\n        sample['image1'] = np.array(sample['image1'])\/(255)\n        sample['image2'] = np.array(sample['image2'])\/(255)\n        \n        ## Check if images are grayscale or RGBA\n        if len(sample['querry_image'].shape)==3:\n            sample['querry_image'] = sample['querry_image'][:,:,0]\n            \n        if len(sample['image1'].shape)==3:\n            sample['image1'] = sample['image1'][:,:,0]\n            \n        if len(sample['image2'].shape)==3:\n            sample['image2'] = sample['image2'][:,:,0]\n        \n  \n        \n        sample['querry_image'] = np.stack((sample['querry_image'],)*3, axis=-1)\n        sample['image1'] = np.stack((sample['image1'],)*3, axis=-1)\n        sample['image2'] = np.stack((sample['image2'],)*3, axis=-1)\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W \n        sample['querry_image'] = torch.from_numpy(sample['querry_image'].transpose((2, 0, 1)))\n        sample['image1'] = torch.from_numpy(sample['image1'].transpose((2, 0, 1)))\n        sample['image2'] = torch.from_numpy(sample['image2'].transpose((2, 0, 1)))\n        \n        return sample\n      \n\nclass XrayDataSet(Dataset):\n    def __init__(self, dataframe, transform = None):\n        '''\n        args: \n        dataframe (pd.DataFrame): dataframe of index images, paths and labels\n        transform (callable, optional): Optional transform to be applied\n                on a sample.\n        '''\n        self.df = dataframe\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def get_image_from_idx(self, idx):\n        '''\n        Output:\n            image (PIL Image): The image in self.df[idx]\n            image_labels (list): List of the labels present in image\n        '''\n        image_path, image_labels = self.df.iloc[idx].values\n        image = Image.open(image_path)\n        return image, image_labels, image_path\n        \n    \n    def __getitem__(self, idx):\n        '''\n        return a sample of NIH chest X-rays dataset\n        A sample is compsed of tree images, and the labels associated ot each of them\n        '''\n        # Get the querry image\n        querry_path, querry_labels = self.df.iloc[idx].values\n        querry_image  = Image.open(querry_path)\n              \n            \n        # Get random similar image\n        similar_images = self.df[self.df['labels'] == querry_labels]\n        idx_sim = np.random.randint(0,len(similar_images), 1)[0]\n        path1, labels1 = similar_images.iloc[idx_sim].values\n        image1  = Image.open(path1)\n        \n        # Get random similar image\n        different_images = self.df[self.df['labels'] != querry_labels]\n        idx_diff = np.random.randint(0,len(different_images), 1)[0]\n        path2, labels2 = different_images.iloc[idx_diff].values\n        image2  = Image.open(path2)\n        \n        sample = {'querry_image': querry_image,\n                  'querry_labels': querry_labels,\n                  'image1': image1,\n                  'labels1': labels1,\n                  'image2': image2,\n                  'labels2': labels2,\n                  'querry_image_name': querry_path}\n        \n        \n        if self.transform:\n            sample = self.transform(sample)\n            \n        return sample","8c817d3a":"batch_size = 32\n\ntrain_dataset = XrayDataSet(df_train, transform = transforms.Compose([Rescale(256), ToTensor()]))\n\n\ntest_dataset = XrayDataSet(df_test, transform = transforms.Compose([Rescale(256), ToTensor()]))\n\n\nTrainLoader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nTestLoader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)","f4f9945f":"def imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n    \ncount = 0\nstart = time.time()\nLoader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nfor sample in Loader:\n    count += 1\n\n    imshow(torchvision.utils.make_grid(sample['querry_image'], nrow = 4))\n  \n    \n    imshow(torchvision.utils.make_grid(sample['image1'], nrow = 4))\n\n    \n    imshow(torchvision.utils.make_grid(sample['image2'], nrow = 4))\n\n    \n    if count%5 == 0:\n        print(count)\n        break\n        \n    print('-------------------------------------------')\n \n    \nend = time.time()\nprint(end - start) ","66baba35":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\ndef l2n(x, eps=1e-6):\n    return x \/ (torch.norm(x, p=2, dim=1, keepdim=True) + eps).expand_as(x)\n\nclass GEM_Net(nn.Module):\n    #TO DO: Add a validation metric: (mAP, r@k) \n    def __init__(self, p=2):\n        # TODO: Define the netwrok with other feature extractors and choose, how many layers to train, chosse the optimizerr\n        super(GEM_Net, self).__init__()\n        self.encoder = models.resnet18(pretrained=True)\n        modules = list(self.encoder.children())[:-2]\n        self.encoder = nn.Sequential(*modules)\n    \n        ct = 0\n        for child in self.encoder.children():\n            ct += 1\n            if ct < 7:\n                for param in child.parameters():\n                    param.requires_grad = True\n        \n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    \n        self.encoder.to(self.device)\n        self.p = p\n        self.optimizer = optim.SGD(self.encoder.parameters(), lr=0.001, momentum=0.9)\n        \n    def forward(self, x, eps=1e-6):\n        features = F.relu(self.encoder(x))\n        gem = F.avg_pool2d(features.clamp(min=eps).pow(self.p), (features.size(-2), features.size(-1))).pow(1.\/self.p)\n        gem_normal = l2n(gem).squeeze(-1).squeeze(-1)\n    \n        return gem_normal\n    \n    def get_embedding(self, loader):\n        self.eval()\n        \"Get a dictionnary that stores embeddings for each image\"\n        embeddings_dict = {}\n        labels_dict = {}\n        loss = 0.\n\n        for sample in loader:\n            querry_image = sample['querry_image'].float().to(self.device)\n            image1 = sample['image1'].float().to(self.device)\n            image2 = sample['image2'].float().to(self.device)\n                \n            querry_embedding = self(querry_image)\n            embedding1 = self(image1)\n            embedding2 = self(image2)\n                \n            loss_sim = torch.norm(querry_embedding-embedding1, dim=1)\n            loss_diff = torch.norm(querry_embedding-embedding2, dim=1)\n                \n                \n            querry_labels = sample['querry_labels']\n            labels1 = sample['labels1']\n            labels2 = sample['labels2']\n                \n            querry_labels = list(map(lambda string: string.split('|'), querry_labels))\n            labels1 = list(map(lambda string: string.split('|'), labels1))\n            labels2 = list(map(lambda string: string.split('|'), labels2))\n   \n                \n            dynamic_triplet_loss = F.relu(loss_sim - loss_diff + 1).mean()\n            \n            loss += dynamic_triplet_loss.cpu().data.numpy()\n            \n            names = sample['querry_image_name']\n            embeddings = querry_embedding.detach().cpu().numpy()\n            for ii,name in enumerate(names):\n                embeddings_dict[name] = embeddings[ii,:]\n                labels_dict[name] = querry_labels[ii]\n            \n        \n        return(embeddings_dict, labels_dict, loss)\n                \n        \n    \n    def test_on_loader(self, querry_loader, index_loader, rank=5):\n        \" Test the model on a dataloader\"\n        \"\"\"\n        args: \n            index_loader(torch.utils.data.DataLoder): index images loader generated from ChestDataSet object\n            querry_loader(torch.utils.data.DataLoder): querry images loader generated from ChestDataSet object\n            rank (int): The number of images retrived for each query by our retrieval system\n        \n        outputs: \n            Loss (float): Value oof the loss function on the data provided by the loader\n            p@k (float): precision at rank k \n        \"\"\"\n        self.eval()\n        print(\"compute the index set embedding\")\n        index_embedding, index_labels, index_loss = self.get_embedding(index_loader)\n        print(\"compute the querry set embedding\")\n        querry_embedding, querry_labels, querry_loss = self.get_embedding(querry_loader)\n        \n        print('querry_loss: ',querry_loss,'. index_loss: ', index_loss)\n        retrieval_results, precision_at_k = retrieve_images(index_embedding, querry_embedding, index_labels, querry_labels)\n         \n        return(retrieval_results, precision_at_k)\n        \n    def train_on_loader(self, train_loader, val_loader,  epochs=1, validations_per_epoch=1, hist_verbosity=1, verbosity=1):\n        \" Train the model with dynamic triplet loss\"\n        \"\"\"\n        args: \n            train_loader(torch.utils.data.DataLoder): train loader generated from ChestDataSet object\n            val_loader(torch.utils.data.DataLoder): validation loader generated from ChestDataSet object\n            epochs (int): Number of epochs\n            validations_per_epoch (int): Number of validation to perform at each epoch\n            hist_verbosity (int): if 1 compute history by epoch, if 2 compute history by batch\n            verbosity (int): Controls the logs of the training. The number of epochs before performing a test\n        \n        outputs: \n            hist (list): history of train_loss and validation loss (To Do)\n        \"\"\"\n        assert hist_verbosity in [1,2] , \"hist verbosity should be 1 or 2\"\n        \n        for epoch in range(epochs):\n            print('epoch', epoch ,'\\\\', epochs,':')\n            for batch_id, sample in enumerate(train_loader):\n                self.optimizer.zero_grad()\n                \n                querry_image = sample['querry_image'].float().to(self.device)\n                image1 = sample['image1'].float().to(self.device)\n                image2 = sample['image2'].float().to(self.device)\n                \n                querry_embedding = self(querry_image)\n                embedding1 = self(image1)\n                embedding2 = self(image2)\n                \n                loss_sim = torch.norm(querry_embedding-embedding1, dim=1)\n                loss_diff = torch.norm(querry_embedding-embedding2, dim=1)\n                \n                \n                querry_labels = sample['querry_labels']\n                labels1 = sample['labels1']\n                labels2 = sample['labels2']\n            \n                dynamic_triplet_loss = F.relu(loss_sim - loss_diff + 1).mean()\n                \n                dynamic_triplet_loss.backward()  \n                self.optimizer.step()\n                \n            if epoch%verbosity==0:\n                pass\n                #retrieval_results, precision_at_k= self.test(val_loader, train_loader)","e90694df":"gem_net = GEM_Net()\n\nstart = time.time()\ngem_net.train_on_loader(TrainLoader, TestLoader, epochs=3,verbosity=5)\nend = time.time()\nprint(\"Train time is: \",end-start)\n\n'''\nstart = time.time()\nretrieval_results, precision_at_k = gem_net.test_on_loader(TestLoader, TrainLoader)\nend = time.time()\nprint(\"Test time is: \",end-start)\n'''","d8f3f2bb":"start = time.time()\nindex_embedding, index_labels, index_loss = gem_net.get_embedding(loader=TrainLoader)\nend = time.time()\n\nprint('Building Index Embeddings in:', end-start)","b88bba37":"start = time.time()\nquerries_embedding, querries_labels, _ = gem_net.get_embedding(loader=TestLoader)\nend = time.time()\n\nprint('Building querries Embeddings in:', end-start)","9ca61eed":"retrieval_results, precision_at_k = retrieve_images(index_embedding, querries_embedding, index_labels, querries_labels)\nvisualize_retrieval(retrieval_results,nbr_querries=10)","a25555bf":"image_name = '..\/input\/xnaturev2\/xnature\/XNature\/Fruits\/N0001_0001.png'\nembedding = get_embedding_of_image(image_name, gem_net)\n\nquerry_embedding = {image_name: embedding}\nquerry_labels = {image_name: ['Fruits']}\n\nretrieval_results, precision_at_k = retrieve_images(index_embedding, querry_embedding,\n                                                   index_labels, querry_labels)\nvisualize_retrieval(retrieval_results,nbr_querries=100)\n","eac24994":"import cv2\n\ndef image_to_tensor(image_name):\n    \"\"\"\n    args:\n        image_name(str): path to a .png file \n    outputs:\n        tensor of shape (1,3,256,256)\n    \"\"\"\n    image_pil = Image.open(image_name).resize([256,256])\n\n    image_array = np.array(image_pil)\/(255)\n\n    image_rgb = np.stack((image_array,)*3, axis=-1)\n\n    # swap color axis because\n    # numpy image: H x W x C\n    # torch image: C X H X W \n    image_tensor = torch.from_numpy(image_rgb.transpose((2, 0, 1)))\n\n    image_tensor = image_tensor.expand(1,-1,-1,-1)\n\n    return image_tensor.float().cuda()","07fb5423":"image_name = '..\/input\/xnaturev2\/xnature\/XNature\/Gun\/B0049_0001.png'\nimage = image_to_tensor(image_name)\n\nembedding = get_embedding_of_image(image_name, gem_net)\n\nquerry_embedding = {image_name: embedding}\nquerry_labels = {image_name: ['Gun']}\n\nretrieval_results, precision_at_k = retrieve_images(index_embedding, querry_embedding,\n                                                   index_labels, querry_labels)\n\nvisualize_retrieval(retrieval_results,nbr_querries=100)\n\nfeatures_blobs = []\ndef hook_feature(module, input, output):\n    features_blobs.append(output.data.cpu().numpy())\n      \ngem_net._modules.get('encoder').register_forward_hook(hook_feature)\n\nembeddings = []\nembedding = gem_net(image).detach()\nembeddings.append(embedding)\n\nfor ii, result in enumerate(retrieval_results[image_name][0]):\n    image_ = image_to_tensor(result)\n    embedding = gem_net(image_).detach()\n    embeddings.append(embedding)\n\n#residuals = torch.abs(torch.cat(embeddings[2:3]).mean(dim=0) - embeddings[0].squeeze())\nresiduals = torch.abs(embeddings[2] - embeddings[0])\n#residuals = F.softmax(residuals,dim=0)\nresiduals \/= residuals.sum()\nresiduals = residuals.reshape((1,512,1,1))\nfeature_map = torch.Tensor(features_blobs[0]).cuda()\nheat_map = ((1-residuals)*feature_map).sum(dim=1).squeeze().cpu().numpy()\nheat_map = np.uint8(255*(heat_map))\nheat_map = cv2.resize(heat_map,(256,256))\nheat_map = cv2.applyColorMap(heat_map, cv2.COLORMAP_JET)\/255\n\nimg = Image.open(image_name).resize([256,256])\nimg = np.array(img)\nimg = np.stack((img,)*3, axis=-1)\/255\n\nresult = heat_map * 0.3 + img * 0.7\n\nred = result[:,:,0].copy()\nblue = result[:,:,2].copy()\n\nresult[:,:,0] = blue\nresult[:,:,2] = red\n\nplt.imshow(result)","44eaa3cf":"### Train and test model","eaa541d6":"#### Result on a different querry image","7c9a1b87":"#### DataSet class:","fe813506":"We donwload a new querry image of a gun with a background","a9efd0a3":"#### Build Index\/querries Images Embedding:","bc9c0263":"### Train\/Val split ","f5390135":"### Install FAISS for efficient gpu vector retrieval","eada684c":"We create a function that train the model, evaluate it and save the best model after training (this is equivalent ot keras .fit)","931e031a":"## CAM for CBIR interpretation and weakly supervised detection ","e03647a0":"## Training The model","6da36376":"### Visualization of the model's predection","d515f3ff":"Let's visualize now some images from the train set and some images from the test set","10038077":"In this section we build a weakly supervised detector suitable for the CBIR system that we have defined using. The method we propose is highky inspired by Class Activation Maps [5] (CAM)","33244f05":"### GeM poolin network","29e94602":"## Visualize and load the dataset ","a688ec4b":"### Load and Vizualise the dataset \n\nWe start by separating our data set into a test part (querry images) and a train part (index)","86f0f060":"## References \n\n[[1]](https:\/\/arxiv.org\/abs\/1512.03385) Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual Learning for Image Recognition. In arXiv 2015\n\n[[2]](http:\/\/cnnlocalization.csail.mit.edu\/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf) Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba. Learning Deep Features for Discriminative Localization. In CVPR 2016.\n\n[[3]](https:\/\/arxiv.org\/pdf\/1711.02512.pdf) Filip Radenovic, Giorgos Tolias, Ondrej Chum. Fine-tuning CNN Image Retrieval with No Human Annotation. arXiv: 1711.02512\n\n[[4]](https:\/\/arxiv.org\/abs\/1702.08734) Johnson, Jeff and Douze, Matthijs and J\u00e9gou, Herv\u00e9. Billion-scale similarity search with GPUs. In arXiv preprint arXiv:1702.08734","44247cc4":"### Utils\n\n","161b71a0":"## Looking at the problem as a retrieval problem","b6a3aac9":"This notebook contain two main parts:\n\n* XNatureV2 Classification:\n\n   We propose to build a simple classifier, to classify the X-ray scans using XNature dataset. Our data set is small, thus we use \"transfer learning\" to build the classifer. In this notebook we well:\n    * build a baseline classifer, by training a classic model (ResNet[[1]](https:\/\/arxiv.org\/abs\/1512.03385)) pretrained on [ImageNet](http:\/\/www.image-net.org\/).\n    * We will analyse qualitativly our classifer using Class Activation Maps [[2]](http:\/\/cnnlocalization.csail.mit.edu\/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf)\n    \n* XNatureV2 CBIR system:\nWe propose to build a content-based image retrieval system for the XNatureV2 dataset. \n\n    * We well use Aggregated GeM pooling, wich is a new layer that we propose for the feature extraction. This laye is highly inspired by the work[[3]](https:\/\/arxiv.org\/pdf\/1711.02512.pdf) of **Filip Radenovic et al.** \n    * We use [FAISS[4]](https:\/\/github.com\/facebookresearch\/faiss) for efficent gpu information retrieval\n","00e473e9":"### Results \n\nLet's visualize now the retrieved images for each querry image:","9616b06f":"In this notebook we propose a Content-Based Medical X-ray Image Retrieval (CBMXIR) system. We propose Aggregated Generalized Mean Pooling with an attention mehcanism o build an efficient embedding of the images. Our model is highly inspired by the work of Filip Radenovic et al. [1]. We will use the XNature dataset to train this system","fb787e15":"We train now our model, using resnet50 architecture"}}