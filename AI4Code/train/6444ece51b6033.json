{"cell_type":{"d7bd4620":"code","603ea9ef":"code","555a78d2":"code","5e88b812":"code","571ef620":"code","44ac0421":"code","1dead0c1":"code","2a815413":"code","5bf69e34":"code","4c1e85f5":"code","2c4a35f5":"code","3f9d6be4":"code","a1193ba4":"code","4333d5af":"code","421372bf":"code","fc3e1047":"code","e125ae90":"code","2985d5ac":"code","3be6b221":"code","d28367a3":"code","c09e0144":"code","7dcdc66e":"code","bce6de85":"code","46897aaa":"code","fcc72d8f":"code","1a167d5b":"code","20a55d06":"code","0d58fd6c":"code","cba72a62":"code","b699d663":"code","fec49f10":"markdown","c2391d8b":"markdown","167c6573":"markdown","816d384d":"markdown","8c18748f":"markdown","d1528a36":"markdown","5e94ba53":"markdown","e3859241":"markdown","caf32d6b":"markdown","61557d2c":"markdown","c0d0e91e":"markdown","74a95a5d":"markdown","b67c1ac9":"markdown","98f6523a":"markdown","cdd5f653":"markdown","bcb20d40":"markdown","f9378cf8":"markdown","23a2c2d1":"markdown","e2820d68":"markdown","a0802c6e":"markdown","34ef9745":"markdown","2407f353":"markdown","d65efa9e":"markdown","2572b8f7":"markdown","896ebab0":"markdown","beab6fea":"markdown","affe1b36":"markdown","c23d3de6":"markdown","a48de451":"markdown","1f708fe4":"markdown","0c3609f3":"markdown","59779bfc":"markdown","c4108876":"markdown"},"source":{"d7bd4620":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom scipy import stats\nimport statsmodels.formula.api as smf \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import neighbors\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","603ea9ef":"students = pd.read_csv(\"\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv\")","555a78d2":"students.head()","5e88b812":"students.columns = \"gender\",\"race\",\"parental_edu\",\"lunch\",\"test_prep\",\"math\",\"reading\",\"writing\"","571ef620":"students.isna().sum()\n# No missing data in this dataset","44ac0421":"f, axs = plt.subplots(3,3,figsize=(15,15))\nstudents['gender'].value_counts().plot(kind='bar', ax=axs[0,0])\naxs[0,0].title.set_text('Gender')\nstudents['race'].value_counts().plot(kind='bar', ax=axs[0,1])\naxs[0,1].title.set_text('Race')\nstudents['parental_edu'].value_counts().plot(kind='bar', ax=axs[0,2])\naxs[0,2].title.set_text('Parental Education')\nstudents['lunch'].value_counts().plot(kind='bar', ax=axs[1,0])\naxs[1,0].title.set_text('Lunch')\nstudents['test_prep'].value_counts().plot(kind='bar', ax=axs[1,1])\naxs[1,1].title.set_text('Test Prep')\naxs[1,2].hist(students['math'])\naxs[1,2].title.set_text('Math')\naxs[2,0].hist(students['reading'])\naxs[2,0].title.set_text('Readiing')\naxs[2,1].hist(students['writing'])\naxs[2,1].title.set_text('Writing')\n\nf.delaxes(axs[2][2])\nf.tight_layout()\nplt.show()","1dead0c1":"sns.pairplot(students.iloc[:,:])\nstudents.corr()","2a815413":"scores = students.loc[:,['math','reading','writing']].transpose()\n\nChisquares_results=scipy.stats.chi2_contingency(scores)\nprint(\"Chi-Squared test returns P-value of\", Chisquares_results[1])","5bf69e34":"students[\"average_score\"] = students.loc[:,['math','reading','writing']].mean(axis=1).round(1)\nstudents","4c1e85f5":"plt.hist(students['average_score'])\nplt.title('Distribution of Student Average Scores')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.show()","2c4a35f5":"students = pd.get_dummies(students, columns=['gender', 'race', 'parental_edu', 'lunch', 'test_prep'],\n               drop_first=True, prefix=['gender', 'race', 'parental_edu', 'lunch', 'test_prep'], prefix_sep='_')\n\nstudents.rename(columns={'race_group B': 'race_B', 'race_group C': 'race_C', 'race_group D': 'race_D', \n                             'race_group E': 'race_E', 'parental_edu_bachelor\\'s degree': 'parental_bachelor', \n                             'parental_edu_high school': 'parental_hs', 'parental_edu_master\\'s degree': 'parental_masters',\n                             'parental_edu_some college': 'parental_somecol', 'parental_edu_some high school': 'parental_somehs'}, inplace=True)","3f9d6be4":"train_data,test_data = train_test_split(students, test_size = 0.3)","a1193ba4":"ml1 = smf.ols('average_score ~ gender_male+race_B+race_C+race_D+race_E+parental_bachelor+parental_hs+parental_masters+parental_somecol+parental_somehs+lunch_standard+test_prep_none', data=train_data).fit()\n\nml1.summary()","4333d5af":"test_pred = ml1.predict(test_data)\n\nplt.hist(test_pred)\nplt.title('Distribution of Predicted Scores using Logistic Regression')\nplt.xlabel('Scores')\nplt.ylabel('Frequency')\nplt.show()","421372bf":"test_resid  = test_pred - test_data.average_score\n\n# RMSE value for test data \nlr_test_rmse = np.sqrt(np.mean(test_resid*test_resid))\n\nprint(\"Test RMSE using logistic regression is:\",lr_test_rmse)","fc3e1047":"train_X = train_data.drop(['average_score','math','writing','reading'], axis=1)\ntrain_Y = train_data.loc[:,'average_score']\ntest_X = test_data.drop(['average_score','math','writing','reading'], axis=1)\ntest_Y = test_data.loc[:,'average_score']","e125ae90":"rmse_val = [] #to store rmse values for different k\nfor K in range(150):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(train_X, train_Y)  #fit the model\n    pred=model.predict(test_X) #make prediction on test set\n    error = sqrt(mean_squared_error(test_Y,pred)) #calculate rmse\n    rmse_val.append(error) #store rmse values","2985d5ac":"data = {\"k\": range(1,151),\"RMSE\": rmse_val}\nfinal_rmse=pd.DataFrame(data)\nmin_index = final_rmse.iloc[:,1].idxmin()","3be6b221":"plt.plot(final_rmse.k, final_rmse.RMSE)\nplt.plot(final_rmse.k[min_index],final_rmse.RMSE[min_index],'ro')\nplt.title('Plot of Test RMSE vs K number of neighbours')\nplt.xlabel('K')\nplt.ylabel('Test RMSE')\nplt.show()\n","d28367a3":"model = neighbors.KNeighborsRegressor(n_neighbors = 35)\nmodel.fit(train_X, train_Y) \npred=model.predict(test_X)\n\nplt.hist(pred)\nplt.title('Distribution of Predicted Scores for KNN Regression')\nplt.xlabel('Scores')\nplt.ylabel('Frequency')\nplt.show()","c09e0144":"knn_test_rmse = sqrt(mean_squared_error(test_Y,pred))\nprint(\"Test RMSE using KNN regression is:\",knn_test_rmse)","7dcdc66e":"model_linear = SVR(kernel=\"linear\")\nmodel_linear.fit(train_X, train_Y)\npred_linear = model_linear.predict(test_X)\nlinear_rmse = sqrt(mean_squared_error(test_Y,pred_linear))\n\n# kernel = poly\nmodel_poly = SVR(kernel=\"poly\")\nmodel_poly.fit(train_X, train_Y)\npred_poly = model_poly.predict(test_X)\npoly_rmse = sqrt(mean_squared_error(test_Y,pred_poly))\n\n# kernel = sigmoid\nmodel_sigmoid = SVR(kernel=\"sigmoid\")\nmodel_sigmoid.fit(train_X, train_Y)\npred_sigmoid = model_sigmoid.predict(test_X)\nsigmoid_rmse = sqrt(mean_squared_error(test_Y,pred_sigmoid))\n\n# kernel = rbf\nmodel_rbf = SVR(kernel=\"rbf\")\nmodel_rbf.fit(train_X, train_Y)\npred_rbf = model_rbf.predict(test_X)\nrbf_rmse = sqrt(mean_squared_error(test_Y,pred_rbf))\n\ndata = {\"kernel\":pd.Series([\"linear\",\"polynomial\",\"sigmoid\",\"rbf\"]),\n            \"Test RMSE\":pd.Series([linear_rmse,poly_rmse,sigmoid_rmse,rbf_rmse]),\n            \"Pred\":pd.Series([pred_linear,pred_poly,pred_sigmoid,pred_rbf])}\ntable_rmse=pd.DataFrame(data)\ntable_rmse\n","bce6de85":"K = 15\nparameters = [{'kernel': ['linear','sigmoid','rbf'], 'gamma': [2e-3,2e-2, 2e-1, 1, 2, 4, 8, 16],'C': [2e-5,2e-4,2e-3,2e-2, 2e-1, 1, 2, 4, 8, 16]}]\nscorer = make_scorer(mean_squared_error, greater_is_better=False)\nsvr_gs = GridSearchCV(SVR(epsilon = 0.01), parameters, cv = K, scoring=scorer)\n\nsvr_gs.fit(train_X, train_Y)\nprint(svr_gs.best_params_)","46897aaa":"regressor = SVR(**svr_gs.best_params_)\nregressor.fit(train_X,train_Y)\npred=regressor.predict(test_X)\n\nerror = sqrt(mean_squared_error(test_Y,pred))\ndata = {\"kernel\":pd.Series([\"GS Output\"]),\"Test RMSE\":pd.Series([error]),\"Pred\":pd.Series([pred])}\ntable_rmse = table_rmse.append(pd.DataFrame(data))\nprint(table_rmse)","fcc72d8f":"plt.hist(table_rmse.Pred.iloc[table_rmse[\"Test RMSE\"].idxmin()])\nplt.title('Distribution of Predicted Scores for SVR')\nplt.xlabel('Scores')\nplt.ylabel('Frequency')\nplt.show()","1a167d5b":"svr_test_rmse = table_rmse[\"Test RMSE\"].min()\nprint(\"Test RMSE using SVR is:\",svr_test_rmse)","20a55d06":"import tensorflow as tf\n\n# Importing necessary models for implementation of ANN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ncont_model = Sequential()\ncont_model.add(Dense(100, input_dim=train_X.columns.value_counts().sum(), activation=\"softmax\"))\ncont_model.add(Dense(60, activation=\"relu\"))\ncont_model.add(Dense(1, kernel_initializer=\"normal\"))\ncont_model.compile(loss=\"mean_squared_error\", optimizer = \"adam\", metrics = [\"mse\"])\n\nmodel = cont_model\nmodel.fit(np.array(train_X), np.array(train_Y), epochs=300)\n\n# On Test dataset\npred = model.predict(np.array(test_X))\npred = pd.Series([i[0] for i in pred])","0d58fd6c":"plt.hist(pred)\nplt.title('Distribution of Predicted Scores for Neural Network')\nplt.xlabel('Scores')\nplt.ylabel('Frequency')\nplt.show()","cba72a62":"nn_test_rmse = sqrt(mean_squared_error(test_Y,pred))\nprint(\"Test RMSE using ANN is:\",nn_test_rmse)","b699d663":"data = {\"Model\":pd.Series([\"Linear Regression\",\"KNN Regression\",\"SVR\",\"Neural Network\"]),\n            \"Test RMSE\":pd.Series([lr_test_rmse,knn_test_rmse,svr_test_rmse,nn_test_rmse])}\ntable_final=pd.DataFrame(data)\ntable_final","fec49f10":"We then plot bar plots and histograms to visualize the distribution of the data for each variable","c2391d8b":"We also check if there are any missing data in the dataset","167c6573":"From the histogram, it can be seen that all the predicted scores fall on a short range, from 55 to 85. Note that this is a similar theme across all models built\n\nWe store the lowest RMSE value for final tabulation and comparison","816d384d":"For the model that returns the lowest RMSE value, we plot a histogram to look at the distribution of the prediction values","8c18748f":"# Data & Feature Engineering\n**Target Variable**\n\nBased on the high correlation between the 3 scores above, we hypothesize that the proportion of the scores are equal across the samples (students). We will perform the hypothesis test to verify this:\n\n* H0: Proportion of math scores = Proportion of writing scores = Proportion of reading scores across all students\n* H1: Proportions are not equal\n\nWe will perform the Pearson Chi-Squared test for proportion similarity","d1528a36":"Based on the P-value, we can deduce that the proportion of scores are similar across all students. Thus, we will calculate the average score, to be used as the prediction target","5e94ba53":"# Linear Regression Model\nFor model validation, we will use validation set approach. For this, we first perform a train-test split on the data in the ratio of 70:30","e3859241":"# Exploratory Data Analytics\nWe first take a look at the data","caf32d6b":"From the histogram, it can be seen that all the predicted scores fall on a short range, from 55 to 85. Similar to the previous models, it is also unable to predict scores on the upper and lower ranges\n\nWe then store the lowest RMSE value for final tabulation and comparison","61557d2c":"# Comparison & Conclusion\nWe will tabulate the test & train accuracies for the 4 algorithms","c0d0e91e":"# Neural Networks\n\nWe will fit the same train & test split data onto an artificial neural network\n\nNote that multiple activation functions and neural network structures are tested, but only the final network is shown here","74a95a5d":"We will store the rmse values for final tabulation and comparison","b67c1ac9":"**Independent Variables**\n\nAs the columns \"gender\", \"race\", \"parental_edu\", \"lunch\" and \"test_prep\" are qualitative variables, we will create dummy variables for them, removing 1 dummy variable for each variable to prevent dummy trap (multi-collinearity problems)\n\nWe then concatenate the dummy variables with the original dataset, and remove the original variable, we will store this as a new dataframe students_d\n\nThe column names containing spaces are then renamed to omit the spaces","98f6523a":"From this, we observe the following regarding the data:\n\n* Qualitative variables are distributed rather evenly between the classes, with no sparse classes.\n* Quantitative variables 'Math', 'Reading', and 'Writing' have a relatively normal distribution. Besides that, they have also taken on acceptable values within the range of 0 to 100 (i.e no outliers due to typos\/data entry)","cdd5f653":"We plot a histogram to look at the distribution of the prediction values","bcb20d40":"So far, it seems like SVR is giving the best (lowest) RMSE out of the models. We will try to further improve the model by doing a grid search to find the best parameters","f9378cf8":"The dataframe columns are renamed for easier accessibility","23a2c2d1":"We perform prediction for test data using the best parameters from grid search, and append this test RMSE to the table of results for SVR","e2820d68":"Saving the data as a dataframe","a0802c6e":"Plotting the rmse values against the number of neighbours, k, as well as the value which k returns min RMSE","34ef9745":"From this, it can be observed that the model is predicting all score values between 50 and 90 only, and not able to handle extreme cases. \n\nHowever, the distribution of the data is rather similar to the average score, showing that the model is not blindly predicting certain scores all the time\n\nWe will calculate and store the RMSE values for final tabulation and comparison","2407f353":"From the model summary, the R-squared value is extremely low. This is likely due to the low number of predictors that are all categorical variables.\n\nI have tried dropping variables, with no improvement on the R-squared value. Thus, we will leave the model as is. Besides that, transformations on the predictors will not work either as they are all coded as binary values only.\n\nWe are also interested to have a visualization of the predicted values. We will plot a histogram for this","d65efa9e":"From the final results, it is observed that linear regression and SVR provide the best models to predict student score. \n\nHowever, in all 4 models, the ranges of scores predicted are quite small, and the models are unable to predict scores on the higher and lower ranges of the spectrum. This is because the dataset is relatively small, and the input variables are all qualitative variables. A more accurate model can be produced to predict student scores, given more data & quantitative variables, for example:\n* hours spent studying a week\n* average score on previous exams\n* attendance in school\n* enrollment in extra classes","2572b8f7":"Fitting the model for values of k from 1 to 150 and calculating the resulting RMSE\n\nNote that this is only done for learning purpose as it is a small dataset, not recommended to run so many k values sequentially on large datasets","896ebab0":"# Support Vector Regression (SVR)\nWe will utilize the same train & test split data to model for SVR\n\nWe will model the data for the following kernels to select the best one:\n* Linear\n* Polynomial\n* Sigmoid\n* Gaussian (rbf)","beab6fea":"We are also interested in the shape of the data for average score, thus we will plot a histogram","affe1b36":"\n# Scores Prediction\n\nFor score prediction, we will try and compare several models, namely:\n* Linear Regression\n* K Nearest Neighbour Regression\n* SVR\n* Neural Networks","c23d3de6":"Besides that, we also study the relationship between the quantitative variables","a48de451":"From this, it can be observed that the 3 score variables are quite highly correlated, with highest correlation between reading & writing. As this may affect the model output, we may remove certain variables during modelling or combine them during the data modelling stage","1f708fe4":"From the elbow plot, we will select the smallest value of K that provides largest reduction in RMSE. From the plot, we will select the value K = 35 \n\nFor this value of K, we will plot a histogram to look at the distribution of predictions","0c3609f3":"# Introduction\n\nIn this notebook, we will try to predict the average score of a student based on their socio economic status, family background, as well as their gender. Below are the steps that we will be performing:\n\n1. Exploratory Data Analytics\n    * Checking for missing values\n    * Visualization of variables & correlations\n2. Data Engineering\n    * Hypothesis testing on the proportion of the scores, to further validate the use of students' average score\n    * Converting qualitative variables to dummy variables\n3. Data Modelling with:\n    * Linear Regression\n    * K Nearest Neighbour Regression\n    * Support Vector Regression (SVR)\n    * Neural Networks\n4. Comparison and Conclusion","59779bfc":"From the histogram, it can be seen that all the predicted scores fall on a very short range, from 55 to 80 only. \n\nThis is because the KNN algorithm averages the scores of the nearest neighbours, thus the predictions tend to fall closer to the average value of the data. \n\nIn other words, KNN regression is not able to give very accurate predictions of student scores (especially those on the lower ranges)","c4108876":"# K Nearest Neighbours Regression\nWe will fit the same test & train data onto the KNN regression algorithm"}}