{"cell_type":{"0eb437ba":"code","f5a5330c":"code","531bca96":"code","9ed5ad82":"code","eb34b37f":"code","8cec6f41":"code","d711649e":"markdown","d948417b":"markdown","914c105b":"markdown","7d3c3048":"markdown","8257406e":"markdown","5ba61f79":"markdown","4532e66a":"markdown","9ad966f6":"markdown","5b2a0c80":"markdown"},"source":{"0eb437ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5a5330c":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","531bca96":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","9ed5ad82":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","eb34b37f":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","8cec6f41":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","d711649e":"Similarly we inspect the TEST.CSV dataset. It has all the columns as in the TRAIN.CSV dataset, except for the feature column SURVIVED (Y). The outcome of the passengers survival is already known, but we wish to test the models capabilities and hence we do not give it the outcome. We only give it features (X) and test if it can predict Y. It is typical to split the data into TRAINING and TEST in the ratio 70:30 or 60:40. There is no fixed rule but generally you split the dataset you already have in such ratios.","d948417b":"To train a machine to perform predictions, we need to first provide it input data (X). This input data has several columns. Some are FEATURES, in this case Sex, Age, Fare etc. and one of the columns in this dataset is the TARGET, in this case its the column SURVIVED. So the task of the machine is to find the pattern among the features (X) to be able to predict the outcome (Y). So given X -> Y relationship, can the machine (called model) train itself to predict future outcomes. The test dataset (test.csv) does not have the Y value column, it just has features (X). When you feed data to the model which has trained itself on the TRAIN.CSV dataset, it should be able to predict the values in the SURVIVED column.","914c105b":"Here we use the read_csv method to read the comma seperated values in our input file train.csv. \nUsing the head method we display only the first 5 rows from top. This is just to get a feel for the data inside the command seperated file (CSV). The above output is only for the TRAIN.CSV file.","7d3c3048":"Above code prepares the python environment by importing necessary python libraries (numpy, pandas, os).\nLibraries help you easily reference functions inside your code.\nFor example **os.path.join(dirname, filename)** is a method derived from the os.path module. All you have to do is to pass the directory name and filename and it lists all the file names in the input directory.","8257406e":"After doing some basic exploratory analysis of the data, we come to the important step of training the algorithm with the TRAINING data. In this example we train the Random Forest Classifier on 4 features [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"] and ask it to find patterns which lead to the outcome (Y) of SURVIVED = 1 or DIED = 0. So we hope the random forest classifier is able to create trees or forests with branches which decide which features or combination of features made it more likely for the passenger to surive or die. So perhaps being a man on the titanic meant certain death or perhaps most men were travelling first class and hence they faced certain death.","5ba61f79":"EDA tells us that 74% of the women survived the tragedy of the ship sinking. Were there many Kate Winslets on board?","4532e66a":"A very tiny fraction (approx 20%) of men surived. Does this mean men on boat were bad swimmers or did the captain send the women and children to safety using the rescue boats.","9ad966f6":"Ultimately you save the prediction as yet another CSV named my_submission.csv. If you open this file, you will see two columns, PassengerId and Survived.","5b2a0c80":"Next up we do some Exploratory Data Analysis (EDA). EDA is essential for us to understand the data and spot patterns ourselves before we ask the machine to do it. EDA can also help you decide which algorithm to use for this problem and sometimes also help you understand if the data is valid. For example if you found out that 80% of the columns were missing the gender information, then you would not train the machine on this column."}}