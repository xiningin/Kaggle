{"cell_type":{"18429c26":"code","f6883cbb":"code","fdf9062b":"code","bc89d474":"code","84ff90bf":"code","92b0e550":"code","e83bc70d":"code","0d8cb287":"code","7da27c0e":"code","79022ac0":"code","48c7a8a3":"code","706210e2":"code","e82d9cf1":"code","2aa84f8b":"code","1744a90f":"code","16030dd2":"code","d6c2c507":"code","51d9db30":"code","a9a2b519":"code","ba18327e":"code","76e1f1a6":"code","79133473":"code","59678c21":"code","626a2c40":"code","74229ad1":"code","9f126138":"code","b86db3d2":"code","8b01f31b":"code","71d04867":"code","618115f5":"markdown","fc24b370":"markdown","9d81a4e2":"markdown","8facd271":"markdown","3700100c":"markdown","5fb1c352":"markdown","f29e4fbf":"markdown","fa154536":"markdown","7adf5c02":"markdown","de1a525d":"markdown","228b13d4":"markdown","df9b7685":"markdown","af21bed2":"markdown","f8f2f9ea":"markdown","d5ce7666":"markdown","2ccfbbc3":"markdown","b1c41fba":"markdown","5f3568d9":"markdown","14422c08":"markdown"},"source":{"18429c26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6883cbb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport pandas_profiling","fdf9062b":"pd.set_option('max_columns', None)\nbeer_recipe = pd.read_csv('..\/input\/beer-recipes\/recipeData.csv', index_col='BeerID', encoding='latin1')\nbeer_recipe= beer_recipe.drop(columns=['UserId'])\nbeer_recipe.head()","bc89d474":"print(beer_recipe.dtypes)\nprint('Data set shape:{}'.format(beer_recipe.shape))\nprint('Unique styles: {}'.format(beer_recipe['Style'].nunique()))","84ff90bf":"report = pandas_profiling.ProfileReport(beer_recipe)","92b0e550":"display(report)","e83bc70d":"print( list( beer_recipe.select_dtypes(exclude=object)))\nprint(list(beer_recipe.select_dtypes(include=object)))","0d8cb287":"plt.figure(figsize=(12,12))\ncount=0\nfor col, color in zip(['OG', 'FG', 'ABV', 'IBU','Color','Efficiency'],['r','y','c','m','g','r']):\n    count+=1\n    if(count==6):\n        plt.subplot(3,4,(5,6))\n    else:\n        plt.subplot(3,2,count)\n    sns.distplot(beer_recipe[col], bins=100, label=col, color=color)\n    plt.title('{} Distribution'.format(col), fontsize=15)\n    plt.legend()\n    plt.ylabel('Normed Frequency', fontsize=15)\n    plt.xlabel(col, fontsize=15)\n\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\nplt.show()","7da27c0e":"gb_style = beer_recipe.groupby(['Style']).count().sort_values(['Name'], ascending=False).reset_index()[:20]\ngb_style['Name'] = (gb_style['Name'] \/ len(beer_recipe)) * 100\n#print(gb_style['Name'])\nplt.figure(figsize=(12,8))\ng = sns.barplot(x=gb_style['Name'], y=gb_style['Style'], orient='h',color ='c')\nplt.title('20 most popular Styles', fontsize=22)\nplt.ylabel('Style Name', fontsize=20)\nplt.xlabel('Style Popularity (%)', fontsize=20)\n\nplt.xlim(0,18)\n\nfor index, row in gb_style.iterrows():\n    g.text(y=index+0.2,x=row['Name']+1,s='{:.2f}%'.format(row['Name']),\n           color='black', ha=\"center\", fontsize=16)\n\nplt.show()","79022ac0":"gb_style = beer_recipe.groupby(['Style']).count().sort_values(['Name'], ascending=True).reset_index()[:20]\ngb_style['Name'] = (gb_style['Name'] \/ len(beer_recipe)) * 100\n#print(gb_style['Name'])\nplt.figure(figsize=(12,8))\ng = sns.barplot(x=gb_style['Name'], y=gb_style['Style'], orient='h',color ='c')\nplt.title('20 least popular Styles', fontsize=22)\nplt.ylabel('Style Name', fontsize=20)\nplt.xlabel('Style Popularity (%)', fontsize=20)\n\nplt.xlim(0,18)\n\nfor index, row in gb_style.iterrows():\n    g.text(y=index+0.2,x=row['Name']+1,s='{:.2f}%'.format(row['Name']),\n           color='black', ha=\"center\", fontsize=16)\n\nplt.show()","48c7a8a3":"beer_recipe1 = beer_recipe.drop(['PrimingMethod', 'PrimingAmount'], axis='columns')\n#Checking existing columns with null values\nbeer_recipe1.isnull().sum()\n","706210e2":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\nX= ['BoilGravity', 'MashThickness', 'PitchRate', 'PrimaryTemp']\nimputer.fit(beer_recipe1[X])\nXtrans = imputer.transform(beer_recipe1[X])\n\n","e82d9cf1":"beer_recipe1[X] = Xtrans\nbeer_recipe2 = beer_recipe1# taking backup of the dataframe\nprint(beer_recipe2.shape)\nbeer_recipe1.isnull().sum()\n","2aa84f8b":"#beer_recipe1 = beer_recipe2","1744a90f":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\ncat_feats = ['BrewMethod','SugarScale']\n#print(cat_feats)\nfor feat in cat_feats:\n    encoder = LabelEncoder()\n    beer_recipe1[feat] = encoder.fit_transform(beer_recipe1[feat])\nprint(beer_recipe1.dtypes)\nprint(beer_recipe1.shape)","16030dd2":"from sklearn.utils import resample\nnumbers = [141,76,133,3,62,79,95,46,117,96,123,104,126,154,101,110,99,16,127,164,73,131,32,44,149,88,57,18,77,84,98,100,153,176,112,42,91,83,119,33,69,52,158,146,70,17,142,156,128,55,60,140,41,172,173,139,74,2,161,121,89,48,78,64,47,130,122,166,125,97]\ndf_upsampled =pd.DataFrame()\ndf_majority = beer_recipe1\nfor j in numbers:\n    \n    df_minority_j = beer_recipe1[beer_recipe1.StyleID==j]\n    df_minority_upsampled = resample(df_minority_j, \n                                 replace=True,     # sample with replacement\n                                 n_samples=400,    # to match majority class\n                                 stratify= df_minority_j,\n                                 random_state=123)\n    df_upsampled = pd.concat([df_upsampled, df_minority_upsampled])\n    count=count+1\n#df_upsampled.StyleID.value_counts(ascending=True)\ndf_upsampled.StyleID.value_counts(ascending=True)\n\n","d6c2c507":"print(df_upsampled.shape)\nbeer_recipe1= pd.concat([beer_recipe1, df_upsampled])\nbeer_recipe1.StyleID.value_counts()\nprint(beer_recipe1.shape)","51d9db30":"from sklearn.utils import resample\nnumbers = [163,143,138,51,124,137,165,120,1,103,43,107,71,168,82,174,38,106,49,136,29,36,59,80,61,171,40,135,8,147,66,13,160,54,50,53,63,108,93,35,118,105,113,152,116,90,151,157,85,102,109,5,19,31,23,144,72,115,34,28,27]\ndf_upsampled =pd.DataFrame()\ndf_majority = beer_recipe1\nfor j in numbers:\n    \n    df_minority_j = beer_recipe1[beer_recipe1.StyleID==j]\n    df_minority_upsampled = resample(df_minority_j, \n                                 replace=True,     # sample with replacement\n                                 n_samples=500,    # to match majority class\n                                 stratify= df_minority_j,\n                                 random_state=123)\n    df_upsampled = pd.concat([df_upsampled, df_minority_upsampled])\n    count=count+1\n#df_upsampled.StyleID.value_counts(ascending=True)\ndf_upsampled.StyleID.value_counts(ascending=True)","a9a2b519":"print(df_upsampled.shape)\nbeer_recipe1= pd.concat([beer_recipe1, df_upsampled])\nbeer_recipe1.StyleID.value_counts()\nprint(beer_recipe1.shape)","ba18327e":"gb_style = beer_recipe1.groupby(['Style']).count().sort_values(['Name'], ascending=True).reset_index()[:21]\ngb_style['Name'] = (gb_style['Name'] \/ len(beer_recipe1)) * 100\n#print(gb_style['Name'])\nplt.figure(figsize=(12,8))\ng = sns.barplot(x=gb_style['Name'], y=gb_style['Style'], orient='h',color ='c')\nplt.title('20 least popular Styles', fontsize=22)\nplt.ylabel('Style Name', fontsize=20)\nplt.xlabel('Style Popularity (%)', fontsize=20)\n\nplt.xlim(0,18)\n\nfor index, row in gb_style.iterrows():\n    g.text(y=index+0.2,x=row['Name']+1,s='{:.2f}%'.format(row['Name']),\n           color='black', ha=\"center\", fontsize=16)\n\nplt.show()","76e1f1a6":"def get_sg_from_plato(plato):\n    sg = 1 + (plato \/ (258.6 - ( (plato\/258.2) *227.1) ) )\n    return sg\n\nbeer_recipe1['OG_sg'] = beer_recipe1.apply(lambda row: get_sg_from_plato(row['OG']) if row['SugarScale'] == 'Plato' else row['OG'], axis=1)\nbeer_recipe1['FG_sg'] = beer_recipe1.apply(lambda row: get_sg_from_plato(row['FG']) if row['SugarScale'] == 'Plato' else row['FG'], axis=1)\nbeer_recipe1['BoilGravity_sg'] = beer_recipe1.apply(lambda row: get_sg_from_plato(row['BoilGravity']) if row['SugarScale'] == 'Plato' else row['BoilGravity'], axis=1)\nmed_BoilGravity_sg = round(beer_recipe1['BoilGravity_sg'].median())\nbeer_recipe1['BoilGravity_sg'].fillna(med_BoilGravity_sg, inplace =True)","79133473":"x_df= beer_recipe1[list(beer_recipe1.select_dtypes(exclude=object).columns)].drop(columns=['StyleID'])\ny=beer_recipe1['StyleID']\nX_train, X_valid, Y_train, Y_valid = train_test_split(x_df, y, test_size=0.2, stratify =y, random_state=0)\n","59678c21":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_valid)","626a2c40":"from sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier as xgb\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score","74229ad1":"'''model_params = {\n    'svm': {\n        'model': svm.SVC(gamma='auto'),\n        'params' : {\n            'C': [1,10,20],\n            'kernel': ['rbf','linear']\n        }  \n    },\n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params' : {\n            'n_estimators': [1,50,100]\n        }\n    },\n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear',multi_class='auto'),\n        'params': {\n            'C': [1,5,10]\n        }\n    },\n    \n    'Xgboost_classifier' : {\n        'model' : XGBClassifier(gamma =2),\n        'params' : {\n            'lambda' : [2,3,4],\n            'eta' : [0.3, 0.5,1]\n        }\n    }\n}\n\n'''","9f126138":"'''scores = []\n\nfor model_name, mp in model_params.items():\n    clf1 =  GridSearchCV(mp['model'], mp['params'], cv=2, return_train_score=False)\n    clf1.fit(X_train, Y_train)\n    scores.append({\n        'model': model_name,\n        'best_score': clf1.best_score_,\n        'best_params': clf1.best_params_\n        \n    })\n    \ndf1 = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf1\n'''","b86db3d2":"#imports\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\n#clf = XGBClassifier()\nclf.fit(X_train, Y_train)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\ny_pred = clf.predict(X_test)\nscore = accuracy_score(Y_valid, y_pred)\nprint('Accuracy: {}'.format(score))\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\nprint('Rand accuracy score is {:4f}'.format(accuracy_score(Y_valid,y_pred)))\nprint('Rand Precision score: ', precision_score(Y_valid, y_pred,average='micro'))\nprint('Rand Recall score: ', recall_score(Y_valid, y_pred,average='micro'))","8b01f31b":"feats_imp = pd.DataFrame(clf.feature_importances_, index=x_df.columns, columns=['FeatureImportance'])\nfeats_imp = feats_imp.sort_values('FeatureImportance', ascending=False)\n\nfeats_imp.plot(kind='barh', figsize=(12,6), legend=False)\nplt.title('Feature Importance from RandomForest Classifier')\nsns.despine(left=True, bottom=True)\nplt.gca().invert_yaxis()","71d04867":"confusion_matrix(Y_valid, y_pred)","618115f5":"# EDA - Imbalanced Class label visualization\n","fc24b370":"# Model Selection ","9d81a4e2":"# Model Buliding and prediction","8facd271":"# FE-Scaling numeric data columns","3700100c":"# Resampling minority class\n\nThis would help us to handle imbalance class problem","5fb1c352":"# Regularization","f29e4fbf":"* Checking data types of each of the columns.\n* Shape of the data frame.\n* unique values in class label Style","fa154536":"Resampling all the class labels having less than 100 occurrence to sample size =200","7adf5c02":"# Exploratory data analysis","de1a525d":"# EDA - Pandas Profiling Details\n\nData profiling gives us following information:\n\n* Number of numerical columns are 15, number of categorical columns are 6\n* There are missing values in Style(0.8%), BoilGravity(4%), MashingThickness(40%), PitchRate(53%), PrimaryTemp(30%),PrimimgMethod(90.8%) and PrimingAmount(93.5%).\n* High correlation between BoilSize and Size, BoilTime and Size, OG and FG, BoilGravity and OG, BoilGravity and FG.\n* Style has 175 unique values whereas Style id has 176 unique values, which means there is a wrong value for one row in any of the columns.\n","228b13d4":"# Feature Engineering\n\n* Encoding categorical features\n* Resampling minority classes\n* Adding new features into the data set\n* Scaling numerical features\n\n","df9b7685":"Encoding categorical features","af21bed2":"# Test Train Split","f8f2f9ea":"We can see percentage of least occurring classes have been increased.","d5ce7666":"# Data Pre-Processong","2ccfbbc3":"* Most of the distributions are rightly skewed, signifies there are outliers in the data","b1c41fba":"Resampling all the class labels having original occurence <500  to sample size =500","5f3568d9":"***DPP* - Missing Value Handling**\n* Droping features with high null values\n* Applying KNN Imputer for other fields","14422c08":"# EDA - Distribution plots"}}