{"cell_type":{"312dd568":"code","74cf6706":"code","4b04ce5c":"code","48a6f227":"code","ed56ff88":"code","619c7cb1":"code","5de176ef":"code","f179a782":"code","1182f3f6":"code","8b0b4e25":"code","9155a29e":"code","34c89df1":"code","24311905":"code","03507d45":"code","1b91d2be":"code","bee99662":"code","11c73275":"code","8a15d85f":"code","c578d56f":"code","6ab23408":"code","ed072df5":"code","4197ebe4":"code","d751b040":"code","51309634":"code","5039e9e1":"code","f6f73349":"code","8d5441df":"code","f37e76c6":"code","4c91d452":"code","c0f67595":"code","dfa25816":"code","094ca62e":"code","6a075388":"code","c8f696a0":"code","c4bf7ad6":"code","125a5227":"code","2e8d0a4a":"code","eedd1b23":"code","2cc8bb85":"code","04df6a9e":"code","9b6b22cc":"code","41caae79":"code","058e71ff":"code","738635bc":"code","90422200":"code","7bbeea19":"code","690de920":"code","7f1d885a":"code","7b7245b3":"code","11d7f4fb":"code","1737ca35":"code","65f46976":"code","314891bb":"code","de4591cd":"code","5f527ac3":"code","b6c8100b":"code","f65c2c96":"code","76008946":"code","86770193":"code","fae9097a":"code","9feebf12":"code","576e229e":"markdown","6c69d70d":"markdown","662a5761":"markdown","c02d7f73":"markdown","8335e2a8":"markdown","93f0e55d":"markdown","5bbd4fcd":"markdown","f7bffeac":"markdown","a1bcbca9":"markdown","1cb4b128":"markdown","449dd33c":"markdown","0bb26de9":"markdown","19000647":"markdown","bdda7859":"markdown","0932e082":"markdown","2d24b837":"markdown","fe2339a9":"markdown","6d47fb43":"markdown","0778776b":"markdown","2cb2d28b":"markdown","86e75772":"markdown","db5428a7":"markdown","32e4551d":"markdown","ffc7c9e4":"markdown","94f3c736":"markdown","ec2697c3":"markdown","f2c75ae7":"markdown","3fbf96a7":"markdown"},"source":{"312dd568":"import pandas as pd\nimport numpy as np\nimport os\nfrom os import chdir\nimport sys\nimport re \n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns ","74cf6706":"answers = pd.read_csv('..\/input\/answers.csv')\ncomments = pd.read_csv('..\/input\/comments.csv')\nemails = pd.read_csv(\"..\/input\/emails.csv\")\ngroup_memberships = pd.read_csv('..\/input\/group_memberships.csv')\ngroups = pd.read_csv('..\/input\/groups.csv')\nmatches = pd.read_csv('..\/input\/matches.csv')\nprofessionals = pd.read_csv(\"..\/input\/professionals.csv\")\nquestions = pd.read_csv('..\/input\/questions.csv')\nschool_memberships = pd.read_csv('..\/input\/school_memberships.csv')\nstudents = pd.read_csv('..\/input\/students.csv')\ntag_questions = pd.read_csv(\"..\/input\/tag_questions.csv\")\ntag_users = pd.read_csv('..\/input\/tag_users.csv')\ntags = pd.read_csv('..\/input\/tags.csv')","4b04ce5c":"answers.head()","48a6f227":"answers.answers_body[1] ","ed56ff88":"answers.describe()","619c7cb1":"comments.head()","5de176ef":"comments.comments_body[1]","f179a782":"comments.describe()","1182f3f6":"emails.head()","8b0b4e25":"emails.describe() ","9155a29e":"group_memberships.head()","34c89df1":"group_memberships.describe()","24311905":"groups.head()","03507d45":"groups.groups_group_type.unique() ","1b91d2be":"groups.describe()","bee99662":"sorted_groups = groups['groups_group_type'].value_counts()\nplt.figure(figsize=(20,10))\nsns.barplot(sorted_groups.values,sorted_groups.index)\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Group Type\", fontsize=20)\nplt.show()","11c73275":"matches.head()","8a15d85f":"matches.describe()","c578d56f":"professionals.head()","6ab23408":"print('location:', professionals.professionals_location.unique())\n\nprint('Industry:', professionals.professionals_industry.unique())","ed072df5":"professionals.describe()","4197ebe4":"professionals_locations = professionals['professionals_location'].value_counts().head(30)\nplt.figure(figsize=(20,10))\nsns.barplot(professionals_locations.values, professionals_locations.index)\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Location\", fontsize=20)\nplt.show()","d751b040":"professionals_industries = professionals['professionals_industry'].value_counts().head(30)\nplt.figure(figsize=(20,10))\nsns.barplot(professionals_industries.values, professionals_industries.index)\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Industry\", fontsize=20)\nplt.show()","51309634":"professionals_headlines = professionals['professionals_headline'].value_counts().head(30)\nplt.figure(figsize=(20,10))\nsns.barplot(professionals_headlines.values, professionals_headlines.index)\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Headlines\", fontsize=20)\nplt.show()","5039e9e1":"questions.head()","f6f73349":"questions.describe()","8d5441df":"school_memberships.head()","f37e76c6":"school_memberships.describe()","4c91d452":"students.head()","c0f67595":"students.describe()","dfa25816":"students_locations = students['students_location'].value_counts().head(30)\nplt.figure(figsize=(20,10))\nsns.barplot(students_locations.values, students_locations.index)\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Location\", fontsize=20)\nplt.show()","094ca62e":"tag_questions.head()","6a075388":"tag_questions.describe()","c8f696a0":"tag_users.head()","c4bf7ad6":"tag_users.describe()","125a5227":"tags.head()","2e8d0a4a":"tags.describe()","eedd1b23":"tag_n_user = pd.merge(tags, tag_users, left_on='tags_tag_id', right_on='tag_users_tag_id', how='outer')\ntag_n_user.sort_values('tag_users_tag_id')[:10]","2cc8bb85":"tag_n_user.count()","04df6a9e":"tag_n_questions = pd.merge(tags, tag_questions, left_on='tags_tag_id', right_on='tag_questions_tag_id', how='outer')\ntag_n_questions.sort_values('tags_tag_id')[:10]","9b6b22cc":"tag_n_questions.count()","41caae79":"tag_n_questions[['tags_tag_name','tag_questions_question_id']]","058e71ff":"tags_top20 = tag_n_questions['tags_tag_name'].value_counts().head(20)\nplt.figure(figsize=(20,10))\nsns.barplot(tags_top20.values, tags_top20.index)\nplt.xlabel(\"Count\", fontsize=20)\nplt.ylabel(\"Tags_top20\", fontsize=20)\nplt.show()","738635bc":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom nltk.corpus import words\n\nvectorizer = CountVectorizer(analyzer = 'word', \n                             lowercase = True,\n                             tokenizer = None,\n                             preprocessor = None,\n                             stop_words = 'english',\n                             min_df = 2, # \ud1a0\ud070\uc774 \ub098\ud0c0\ub0a0 \ucd5c\uc18c \ubb38\uc11c \uac1c\uc218\ub85c \uc624\ud0c0\ub098 \uc790\uc8fc \ub098\uc624\uc9c0 \uc54a\ub294 \ud2b9\uc218\ud55c \uc804\ubb38\uc6a9\uc5b4 \uc81c\uac70\uc5d0 \uc88b\ub2e4. \n                             ngram_range=(1, 3),\n                             vocabulary = set(words.words()), # nltk\uc758 words\ub97c \uc0ac\uc6a9\ud558\uac70\ub098 \ubb38\uc11c \uc790\uccb4\uc758 \uc0ac\uc804\uc744 \ub9cc\ub4e4\uac70\ub098 \uc120\ud0dd\ud55c\ub2e4. \n                             max_features = 90000\n                            )\nvectorizer","90422200":"pipeline = Pipeline([\n    ('vect', vectorizer),\n    ('tfidf', TfidfTransformer(smooth_idf = False)),\n])  \npipeline","7bbeea19":"# See answers\n%time answer_train_tfidf_vector = pipeline.fit_transform(answers['answers_body'].values.astype('U'))  ","690de920":"vocab = vectorizer.get_feature_names()\nprint(len(vocab))\nvocab[:10]","7f1d885a":"import numpy as np\ndist = np.sum(answer_train_tfidf_vector, axis=0)\n\nfor tag, count in zip(vocab, dist):\n    print(count, tag)\n\npd.DataFrame(dist, columns=vocab)","7b7245b3":"# questions.questions_id = answers.answers_question_id\n\nQ_n_A = pd.merge(questions[['questions_id','questions_title','questions_body']], answers[['answers_question_id','answers_id','answers_body']], left_on='questions_id', right_on='answers_question_id', how='outer')\nQ_n_A.sort_values('questions_id')\n\n# inner(Except NaN) : questions_id = (51123) 23931, answers_id = 51123\n# outer(Include NaN) : questions_id = (51944) 23931, answers_id = 51123","11d7f4fb":"Q_n_A.describe()","1737ca35":"# professionals.professionals_id = answers.answers_author_id\n\nProfessionals_ID = pd.merge(professionals[['professionals_id','professionals_industry','professionals_headline']], answers[['answers_author_id','answers_id','answers_body']], left_on='professionals_id', right_on='answers_author_id', how='inner')\nProfessionals_ID.sort_values('professionals_id')\n\nProfessionals_ID\n\n# inner(Except NaN) : professionals_id = (50106) 10067, answers_author_id = (50106) 10067\n# outer(Include NaN) : professionals_id = (68191) 28152, answers_author_id = (51123) 10169\n\n### No answers professionals delete SO inner join is our choice","65f46976":"Professionals_ID.describe()","314891bb":"Professionals_ID.answers_body[:]","de4591cd":"Answers_Text = re.sub('<.+?>', '', Professionals_ID.answers_body[0], 0, re.I|re.S)\nAnswers_Text = re.sub('\\n','. ', Answers_Text)\nAnswers_Text","5f527ac3":"Answers_Text = []\nAnswers_Text = re.sub('<.+?>', '', Professionals_ID.answers_body[0], 0, re.I|re.S)\nA_list = []\nA_list.append(Answers_Text)\nA_list\n#Answers_Text.append(re.sub('<.+?>', '', Professionals_ID.answers_body[1], 0, re.I|re.S))\n","b6c8100b":"Professionals_ID.answers_body[0]","f65c2c96":"str(Answers_Text)","76008946":"from gensim.summarization import summarize \nfrom gensim.summarization import keywords \n\nprint(\"1. Summarizing :\", '\\n', summarize(str(Answers_Text)), '\\n')\nprint(\"######################################################################################################\", '\\n')\nprint(\"2. Keywords :\", '\\n', keywords(str(Answers_Text)))","86770193":"Professionals_ID[\"answers_keywords\"] = \"\"\n\nProfessionals_ID.head()","fae9097a":"Professionals_ID['answers_body'].fillna(\"No Answer\", inplace = True)\nfor num in range(len(Professionals_ID.answers_body)):\n    Answers_Text = Professionals_ID['answers_body'][num]\n    Answers_Text = re.sub('<.+?>', '', Answers_Text, 0, re.I|re.S)\n    Answers_Text = re.split('\\n', Answers_Text)\n    for n in Answers_Text:\n        if (n.startswith('http')):\n            del Answers_Text[Answers_Text.index(n)]\n    \n    result = \"\"\n    for n in Answers_Text:\n        result += n + \". \"\n    \n    if (num%500 == 0 ):\n        try:\n            print(\"iteration : \" + str(num))\n            print(\"1. Summarizing :\", '\\n', summarize(str(result)), '\\n')\n            print(\"######################################################################################################\", '\\n')\n            print(\"2. Keywords :\", '\\n', keywords(str(result)))\n        except ValueError:\n            pass  # do nothing!\n    \n    keywords1 = re.split('\\n', keywords(str(result)))\n    \n    Professionals_ID[\"answers_keywords\"][num] = keywords1","9feebf12":"Professionals_ID[['professionals_id','answers_body','answers_keywords']].head(10)","576e229e":"##### It seems that group_type is determined through a specific program. See if there is a connection between groups","6c69d70d":"### 2-5. Groups","662a5761":"### 2-6. Matches","c02d7f73":"- 51122 answers\n\n- The unique value of answerers_author_id has been reduced by one-fifth, and experts have responded several times.\n\n- We should hash out the author's keywords in the text of the people who answered.\n\n##### If the hashtag enters a new question, it will be necessary to connect it to an expert who answered a lot of the hashtags.\n","8335e2a8":"3. **Making Model 1 : Answers' Keyword** <br\/>\n1) Mapping Answer's author_id and professinals_id <br\/>\n2) target columns: the categories of Professionals' (1) <br\/>\n3) input data: Answers' text. (2) <br\/>","93f0e55d":"### 3.  <a id=\"analyze\"> Analyzing <\/a>","5bbd4fcd":"## 2. <a id=\"eda\"> EDA <\/a>","f7bffeac":"### 2-12. Tag_users","a1bcbca9":"### 2-1. Answers","1cb4b128":"# Summary For CareerVilage \n\n# Hypothesis\n1. With the \"answers of professional data\", we can predict the \"industry and headline\" of professional\n2. This problem is a Multi-Labeled Classification\"\n\n# Work Flow\n1. Preprocess Professionals' data into categories. \n\n\n2. Extract only nouns from Answers' text.\n\n\n3. **Making Model 1 : Answers' Keyword** <br\/>\n1) Mapping Answer's author_id and professinals_id <br\/>\n2) target columns: the categories of Professionals' (1) <br\/>\n3) input data: Answers' text. (2) <br\/>\n\n\n4. Estimate the NA values of professinals' industry and headline with a predictive model. <br\/>\n1) (NA Values Handling \u2192 Softmax (Classifier)\n\n\n5. Extract keywords by combining Quests + tags & Extract only nouns from Questions' text.\n\n\n6. **Making Model 2 : Classify Professionals' Keyword** <br\/>\n1) target columns: the industry and headline of Professionals' (4) <br\/>\n2) input data: Questions' text (5) <br\/>\n\n\n# Expected Result\n- Data \u2192 Model 1 \u2192 Model 2 \u2192 Output(Result)","449dd33c":"### 2-11. Tag_questions","0bb26de9":"### 4.  <a id=\"analyze2\"> Analyze <\/a>","19000647":"### 2-10. Students","bdda7859":"1. Preprocess Professionals' data into categories. ","0932e082":"# Proposal\n\n1. Add entries so that people can select the Professionals' Industry and Headline within the pre-treated category.\n\n\n2. Floating tags to top10 so that people can be automatically viewed when you click them on a webpage\n\n\n3. Checking the relationship between Professional and Student Offline Matching (Direction of Development)<br\/>\n1) In the future, allowing students in the same position as professional to match offline will help them more in their careers and career choices.","2d24b837":"4. Extract keywords by combining Quests + tags & Extract only nou****ns from Questions' text.<br\/>\n1) Place net sentence except html code of Answers_body into a corpus <br\/>\n2) Run Summarization & Keywords pull with gensim.summarization package (Check parameter to see if Top 3 can be pulled)","fe2339a9":"### 1.  <a id=\"dataload\"> Data Load <\/a>","6d47fb43":"- Idea: Why don't you float the tags to top10 so that people can be automatically answered by clicking them on a webpage?","0778776b":"### 2-9. School_memberships","2cb2d28b":"### 2-13. Tags","86e75772":"### 2-3. Emails","db5428a7":"### 2-7. Professionals","32e4551d":"2. Extract only nouns from Answers' text","ffc7c9e4":"### 2-4. Group_memberships","94f3c736":"### Contents\n1. [Data Load](#dataload)\n2. [EDA](#eda)\n3. [Analyze](#analyze)\n4. [Analyze2](#analyze2)","ec2697c3":"### 2-8. Questions","f2c75ae7":"### 2-2. Comments","3fbf96a7":"- Primary classification can be performed with hashtags of questions according to the location\/industry of experts\n- Location: It's the United States, it's divided into 52 states.\n- Industry: Therefore it seems necessary to classify into section categories (manufacturing\/communication\/services etc.)"}}