{"cell_type":{"3d02c916":"code","2d36a882":"code","9c525acd":"code","e1e671ec":"code","fbefcd4c":"code","6c9074ad":"code","74a9bf90":"code","7cb754d7":"code","bed19f20":"code","09adca38":"code","9050957b":"code","ee21c8fd":"code","de232968":"code","bf4ba3bd":"code","e555eb28":"code","89a43514":"code","8e7f7aa7":"code","2f820d0d":"code","945468fb":"code","a3dfe1ce":"code","0dcc531f":"code","6f25f8a3":"code","2b1c9533":"code","adfc05a6":"code","d22aff40":"code","39d4016b":"code","7f7726ae":"code","f01f060d":"code","34c7470f":"code","e8cbb13f":"code","2302648c":"code","5d884e83":"code","057ac12e":"code","66d0ba7e":"code","95b87c51":"code","ba4bd2b3":"code","d8eaabb5":"code","de4d2b8d":"code","3b7baa94":"code","cb835b0d":"code","a534d009":"markdown","154b9493":"markdown"},"source":{"3d02c916":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import stats\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,MultiLabelBinarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom lightgbm import LGBMClassifier","2d36a882":"column = ['Class', 'Message']\n#df = pd.read_csv('SMSSpamCollection')\ndf = pd.read_csv('..\/input\/spam.csv',delimiter=',',encoding='latin-1')\ndf = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\ndf.rename(columns={'v1':'Class', 'v2':'Message'}, inplace= True)\ndf.head()","9c525acd":"df.info()","e1e671ec":"df.describe()","fbefcd4c":"df[df['Class'] == 'spam'].describe()","6c9074ad":"df[df['Class'] == 'ham'].describe()","74a9bf90":"df.groupby('Class').describe().T","7cb754d7":"df.head()","bed19f20":"df['Mes_len'] = df['Message'].apply(len)","09adca38":"df.head()","9050957b":"df.groupby('Class').describe().T","ee21c8fd":"df['Mes_len'].plot(kind='hist',bins=50)\nplt.show()","de232968":"sns.barplot(df['Mes_len'],df['Class'])\nplt.show()","bf4ba3bd":"plt.bar(df['Class'], df['Mes_len'])\n#plt.xlabel()\n#plt.legend()\nlabel = ['spam', 'ham']\nplt.show()","e555eb28":"df.groupby('Class')['Mes_len'].max()","89a43514":"# lets check the message with the longest length for both classes\nprint(df.loc[df['Mes_len']==910,'Message'][1084])\n","8e7f7aa7":"print(df.loc[df['Mes_len']==224,'Message'][1020])","2f820d0d":"df.hist(column='Mes_len', by='Class', bins=50, figsize=(10,4))\nplt.show()","945468fb":"#Removing Stopwords & Punctuations and applying Lemmatising\/Stemming\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import stem\nimport string\n\ndef token(example_sent):\n    \n    stemmer = stem.SnowballStemmer('english')\n    \n    stop_words = set(stopwords.words('english') + list(string.punctuation))\n\n    word_tokens = word_tokenize(example_sent) \n\n    filtered_sentence = [w for w in word_tokens if w.lower() not in stop_words] \n    \n    msg = [stemmer.stem(word) for word in filtered_sentence]\n    return(msg)\n","a3dfe1ce":"df['Message'].apply(token)","0dcc531f":"df.head()","6f25f8a3":"bag_of_words = CountVectorizer(token).fit(df['Message'])","2b1c9533":"message_bag_of_words = bag_of_words.transform(df['Message'])","adfc05a6":"message_bag_of_words.nnz\n","d22aff40":"tf_idf_trans = TfidfTransformer().fit(message_bag_of_words)","39d4016b":"messages_tfidf_trans = tf_idf_trans.transform(message_bag_of_words)","7f7726ae":"spam_detector_model = MultinomialNB().fit(messages_tfidf_trans,df['Message'])","f01f060d":"msg_Train,msg_Test,Classification_train, Classification_test=train_test_split(df['Message'],df['Class'],test_size=0.3, random_state=123)\n\n","34c7470f":"pipeline=Pipeline([\n    ('bow',CountVectorizer(token)),\n    ('Tfidf',TfidfTransformer()),\n    ('NBMultinoial',MultinomialNB())\n])\n#Using Pipleline to perform differnt stepe in one go","e8cbb13f":"pipeline.fit(msg_Train,Classification_train)","2302648c":"prediction=pipeline.predict(msg_Test)","5d884e83":"print(classification_report(Classification_test,prediction))","057ac12e":"#Using Random forest for the prediction\npipeline1=Pipeline([\n    ('bow',CountVectorizer(token)),\n    ('Tfidf',TfidfTransformer()),\n    ('Randomforest',RandomForestClassifier(random_state=123))\n])","66d0ba7e":"pipeline1.fit(msg_Train,Classification_train)","95b87c51":"prediction1=pipeline1.predict(msg_Test)","ba4bd2b3":"print(classification_report(Classification_test,prediction1))","d8eaabb5":"#Using Light GBM for the prediction\npipeline2=Pipeline([\n    ('bow',CountVectorizer(token)),\n    ('Tfidf',TfidfTransformer()),\n    ('LightGBM',LGBMClassifier(random_state=123))\n])","de4d2b8d":"pipeline2.fit(msg_Train,Classification_train)","3b7baa94":"prediction2=pipeline2.predict(msg_Test)","cb835b0d":"print(classification_report(Classification_test,prediction2))","a534d009":"It means usually shorter messages are hams and longer messages are spams. Hence, classifiers such as Naive Bayes might turnout to be a success over here","154b9493":"### This means that the classification seems to be apt enough to be taken as correct"}}