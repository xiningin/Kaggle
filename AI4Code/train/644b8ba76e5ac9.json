{"cell_type":{"740a8dfc":"code","7b47122f":"code","7797727f":"code","e57bf5d4":"code","c25395ce":"code","71f2f9fe":"code","af5ba255":"code","a91e4ce9":"code","330c1c3e":"code","ee3459d6":"code","0f683db1":"code","5b117482":"code","ba8e938e":"code","c2345338":"code","2e54d426":"code","3129f16c":"code","88069355":"code","ef6a5f82":"code","8e04f545":"code","3d508bfe":"code","b4d4de2d":"code","e2c1f827":"code","813f2b6e":"code","77f364b3":"code","eb92c1b4":"code","2700fcb6":"markdown","03a5828f":"markdown","8e1745c7":"markdown","b89b7d74":"markdown","f9bd1f55":"markdown","74ab0067":"markdown","1cc9e4a2":"markdown","0c879e9a":"markdown","8a00180a":"markdown","7247bc6a":"markdown","64024f2c":"markdown","bbea7015":"markdown","b3e91164":"markdown","69acb14a":"markdown","d209f887":"markdown","853f33dc":"markdown","b1cc3c82":"markdown","9917c11b":"markdown","1d591cfa":"markdown","8d004c7c":"markdown"},"source":{"740a8dfc":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","7b47122f":"!ls ..\/input\/","7797727f":"train_df = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ntest_df = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"Number of rows and columns in train set : \",train_df.shape)\nprint(\"Number of rows and columns in test set : \",test_df.shape)","e57bf5d4":"train_df.head()","c25395ce":"target_col = \"target\"\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df[target_col].values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('Loyalty Score', fontsize=12)\nplt.show()","71f2f9fe":"plt.figure(figsize=(12,8))\nsns.distplot(train_df[target_col].values, bins=50, kde=False, color=\"red\")\nplt.title(\"Histogram of Loyalty score\")\nplt.xlabel('Loyalty score', fontsize=12)\nplt.show()","af5ba255":"(train_df[target_col]<-30).sum()","a91e4ce9":"cnt_srs = train_df['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in train set\")\nplt.show()\n\ncnt_srs = test_df['first_active_month'].dt.date.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('First active month', fontsize=12)\nplt.ylabel('Number of cards', fontsize=12)\nplt.title(\"First active month count in test set\")\nplt.show()","330c1c3e":"# feature 1\nplt.figure(figsize=(8,4))\nsns.violinplot(x=\"feature_1\", y=target_col, data=train_df)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 1', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 1 distribution\")\nplt.show()\n\n# feature 2\nplt.figure(figsize=(8,4))\nsns.violinplot(x=\"feature_2\", y=target_col, data=train_df)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 2', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 2 distribution\")\nplt.show()\n\n# feature 3\nplt.figure(figsize=(8,4))\nsns.violinplot(x=\"feature_3\", y=target_col, data=train_df)\nplt.xticks(rotation='vertical')\nplt.xlabel('Feature 3', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Feature 3 distribution\")\nplt.show()","ee3459d6":"hist_df = pd.read_csv(\"..\/input\/historical_transactions.csv\")\nhist_df.head()","0f683db1":"gdf = hist_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_hist_transactions\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","5b117482":"cnt_srs = train_df.groupby(\"num_hist_transactions\")[target_col].mean()\ncnt_srs = cnt_srs.sort_index()\ncnt_srs = cnt_srs[:-50]\n\ndef scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x=cnt_srs.index[::-1],\n        y=cnt_srs.values[::-1],\n        showlegend=False,\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\ntrace = scatter_plot(cnt_srs, \"orange\")\nlayout = dict(\n    title='Loyalty score by Number of historical transactions',\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Histtranscnt\")","ba8e938e":"bins = [0, 10, 20, 30, 40, 50, 75, 100, 150, 200, 500, 10000]\ntrain_df['binned_num_hist_transactions'] = pd.cut(train_df['num_hist_transactions'], bins)\ncnt_srs = train_df.groupby(\"binned_num_hist_transactions\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_num_hist_transactions\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_num_hist_transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"binned_num_hist_transactions distribution\")\nplt.show()","c2345338":"gdf = hist_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \"min_hist_trans\", \"max_hist_trans\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","2e54d426":"bins = np.percentile(train_df[\"sum_hist_trans\"], range(0,101,10))\ntrain_df['binned_sum_hist_trans'] = pd.cut(train_df['sum_hist_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_hist_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_sum_hist_trans', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of historical transaction value (Binned) distribution\")\nplt.show()","3129f16c":"bins = np.percentile(train_df[\"mean_hist_trans\"], range(0,101,10))\ntrain_df['binned_mean_hist_trans'] = pd.cut(train_df['mean_hist_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_mean_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_mean_hist_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('Binned Mean Historical Transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Mean of historical transaction value (Binned) distribution\")\nplt.show()","88069355":"new_trans_df = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\nnew_trans_df.head()","ef6a5f82":"gdf = new_trans_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].size().reset_index()\ngdf.columns = [\"card_id\", \"num_merch_transactions\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","8e04f545":"bins = [0, 10, 20, 30, 40, 50, 75, 10000]\ntrain_df['binned_num_merch_transactions'] = pd.cut(train_df['num_merch_transactions'], bins)\ncnt_srs = train_df.groupby(\"binned_num_merch_transactions\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_num_merch_transactions\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned_num_merch_transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Number of new merchants transaction (Binned) distribution\")\nplt.show()","3d508bfe":"gdf = new_trans_df.groupby(\"card_id\")\ngdf = gdf[\"purchase_amount\"].agg(['sum', 'mean', 'std', 'min', 'max']).reset_index()\ngdf.columns = [\"card_id\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\", \"min_merch_trans\", \"max_merch_trans\"]\ntrain_df = pd.merge(train_df, gdf, on=\"card_id\", how=\"left\")\ntest_df = pd.merge(test_df, gdf, on=\"card_id\", how=\"left\")","b4d4de2d":"bins = np.nanpercentile(train_df[\"sum_merch_trans\"], range(0,101,10))\ntrain_df['binned_sum_merch_trans'] = pd.cut(train_df['sum_merch_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_sum_merch_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned sum of new merchant transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Sum of New merchants transaction value (Binned) distribution\")\nplt.show()","e2c1f827":"bins = np.nanpercentile(train_df[\"mean_merch_trans\"], range(0,101,10))\ntrain_df['binned_mean_merch_trans'] = pd.cut(train_df['mean_merch_trans'], bins)\n#cnt_srs = train_df.groupby(\"binned_sum_hist_trans\")[target_col].mean()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x=\"binned_mean_merch_trans\", y=target_col, data=train_df, showfliers=False)\nplt.xticks(rotation='vertical')\nplt.xlabel('binned mean of new merchant transactions', fontsize=12)\nplt.ylabel('Loyalty score', fontsize=12)\nplt.title(\"Mean of New merchants transaction value (Binned) distribution\")\nplt.show()","813f2b6e":"train_df[\"year\"] = train_df[\"first_active_month\"].dt.year\ntest_df[\"year\"] = test_df[\"first_active_month\"].dt.year\ntrain_df[\"month\"] = train_df[\"first_active_month\"].dt.month\ntest_df[\"month\"] = test_df[\"first_active_month\"].dt.month\n\ncols_to_use = [\"feature_1\", \"feature_2\", \"feature_3\", \"year\", \"month\", \n               \"num_hist_transactions\", \"sum_hist_trans\", \"mean_hist_trans\", \"std_hist_trans\", \n               \"min_hist_trans\", \"max_hist_trans\",\n               \"num_merch_transactions\", \"sum_merch_trans\", \"mean_merch_trans\", \"std_merch_trans\",\n               \"min_merch_trans\", \"max_merch_trans\",\n              ]\n\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"min_child_weight\" : 50,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 2018,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\ntrain_X = train_df[cols_to_use]\ntest_X = test_df[cols_to_use]\ntrain_y = train_df[target_col].values\n\npred_test = 0\nkf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test += pred_test_tmp\npred_test \/= 5.\n    ","77f364b3":"fig, ax = plt.subplots(figsize=(12,10))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","eb92c1b4":"sub_df = pd.DataFrame({\"card_id\":test_df[\"card_id\"].values})\nsub_df[\"target\"] = pred_test\nsub_df.to_csv(\"baseline_lgb.csv\", index=False)","2700fcb6":"Looks like the distribution is kind of similar between train and test set. So we need not really have to do time based split I think.\n\n### Feature 1,2 & 3:","03a5828f":"In this section, let us see if the other variables in the train dataset has good predictive power in finding the loyalty score.","8e1745c7":"Loyalty scores seem to increase with the increase in the sum of new merchant transaction values but for the last bin.","b89b7d74":"### New Merchant Transactions\n\nIn this section, let us look at the new merchant transactions data and do some analysis","f9bd1f55":"Now let us bin the count of historical transactions and then do some box plots to see the plots better.","74ab0067":"### Target Column Exploration:\n\nIn this section, let us explore the target column","1cc9e4a2":"### First Active Month\n\nIn this section, let us see if there are any distribution change between train and test sets with respect to first active month of the card.","0c879e9a":"We have about 2207 rows (almost 1% of the data), which has values different from the rest. Since the metric RMSE these rows might play an important role. So beware of them.","8a00180a":"The field descriptions are as follows:\n* card_id\t- Card identifier\n* month_lag\t- month lag to reference date\n* purchase_date\t- Purchase date\n* authorized_flag -\t'Y' if approved, 'N' if denied\n* category_3 - anonymized category\n* installments -\tnumber of installments of purchase\n* category_1 -\tanonymized category\n* merchant_category_id -\tMerchant category identifier (anonymized )\n* subsector_id -\tMerchant category group identifier (anonymized )\n* merchant_id -\tMerchant identifier (anonymized)\n* purchase_amount -\tNormalized purchase amount\n* city_id -\tCity identifier (anonymized )\n* state_id -\tState identifier (anonymized )\n* category_2 -\tanonymized category\n\nNow let us make some features based on the historical transactions and merge them with train and test set.\n\n#### Number of Historical Transactions for the card","7247bc6a":"### Dataset Exploration\n\nIn this part, let us look at the given files before we get into data exploration.","64024f2c":"So we are given the above files. The description of the files are\n\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\n* historical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\n* merchants.csv - additional information about all merchants \/ merchant_ids in the dataset.\n* new_merchant_transactions.csv - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.\n\nFirst let us explore the train and test set.","bbea7015":"As we could see, the loyalty score seem to increase with the \"sum of historical transaction value\". This is expected. Now we can do the same plot with \"Mean value of historical transaction\".","b3e91164":"Loyalty score seem to decrease as the number of new merchant transactions increases except for the last bin. ","69acb14a":"To the naked eyes, the distribution of the different categories in all three features look kind of similar. May be the models are able to find something here. ","d209f887":"### Competition Objective\n\n![Elo](https:\/\/www.cloudera.com\/content\/dam\/www\/marketing\/images\/logos\/customers\/cartao-elo.png)\n\n[Elo](https:\/\/www.cartaoelo.com.br\/), one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders.  The objective of the competition is to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty.\n\n### Objective of the Notebook\n\nObjevtive of the notebook is to explore the given data and get some interesting insights which will help in our model building process.","853f33dc":"### Historical Transactions:\n\nNow let us look at the historical transactions data for the cards.","b1cc3c82":"Let us add more features in the upcoming versions and check the results.\n\n**More to come. Stay Tuned.!**","9917c11b":"We can see that some of the loyalty values are far apart (less than -30) compared to others. Let us just get their count.","1d591cfa":"#### Value of Historical Transactions\n\nNow let us check the value of the historical transactions for the cards and check the loyalty score distribution based on that.","8d004c7c":"### Baseline Model\n\nLet us build a baseline model using the features created so far."}}