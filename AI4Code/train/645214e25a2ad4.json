{"cell_type":{"7dbb5d77":"code","0ca24f43":"code","bc532ab7":"code","0f9fd46f":"code","14dfc0f8":"code","0d79025e":"code","cd2cc541":"code","16b961c9":"code","d57100fc":"code","d33f65f7":"code","539cc414":"code","40f005a3":"code","d87c7696":"code","cab94c8f":"code","095e4172":"code","4cfd9b41":"code","6888ddd4":"code","f020337b":"code","465e0e11":"markdown","07489005":"markdown","06245833":"markdown","6bbb26fb":"markdown","6cd71279":"markdown","ec7417a3":"markdown","f00101ab":"markdown","c4d8541b":"markdown","9a6171f2":"markdown","b9bcddbf":"markdown","e67a78a5":"markdown","465f8392":"markdown","c8f0a343":"markdown"},"source":{"7dbb5d77":"#Files\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport glob\n\n#DATA\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import one_hot\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n#CNN\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\nfrom keras.optimizers import Adam\nfrom keras.losses import CategoricalCrossentropy\n\n#VIS\nfrom keras.utils.vis_utils import plot_model","0ca24f43":"def _prepareData(path): \n    '''\n    parameters: path(STR) of the directory and flag(INT) to know if we prepare data of training or testing\n    return: (LIST) of images of the dataset and the (LIST) of labels\n    \n    For training:\n    -Read images of every directory and extract all images\n    -Resize to (128,128,3)\n    -Read the directory name and asign as a class\n    '''\n    imgsList = []\n    labels = []\n    for directory in sorted(glob.glob(os.path.join(path, '*')), key = lambda k: k.split(\"\/\")[-1]):\n            for imgs in glob.glob(os.path.join(directory,'*.jpg')):\n                img_cv = cv2.imread(imgs)\n                img_cv_r = cv2.resize(img_cv,(128,128))\n                imgsList.append(img_cv_r)\n                labels.append(int(directory.split(\"\/\")[-1].replace('c','')))\n    \n    X_Train, X_Test, Y_Train, Y_Test =  train_test_split(imgsList,labels, test_size = 0.2)\n    Y_Train = tf.keras.utils.to_categorical(Y_Train, num_classes=10)\n    Y_Test = tf.keras.utils.to_categorical(Y_Test, num_classes=10)\n\n    return np.array(X_Train), np.array(X_Test), Y_Train, Y_Test","bc532ab7":"#Paths\npathTrain_Images = \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/train\/\"\npathPropagate_Images =  \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/test\/\"\n\n#List of Images for Train and Test\nX_Train, X_Test, Y_Train, Y_Test = _prepareData(pathTrain_Images)\n\nprint(\"Size X_Train: {}, Size Y_Train: {}\".format(len(X_Train),len(Y_Train)))\nprint(\"Size X_Test: {}, Size Y_Test: {}\".format(len(X_Test),len(Y_Test)))","0f9fd46f":"print(len(X_Train))\nprint(X_Train[202].shape)\nim = X_Train[202]\nRGB_im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\nplt.imshow(RGB_im)\nplt.show()\nprint(\"Class: {}\".format(Y_Train[202]))","14dfc0f8":"data_file = pd.read_csv(\"\/kaggle\/input\/state-farm-distracted-driver-detection\/driver_imgs_list.csv\")\ndata_classes = data_file.loc[:,['classname','img']].groupby(by='classname').count().reset_index()\n\ndata_x = list(pd.unique(data_file['classname']))\ndata_y =list(data_classes['img'])\n\n# Par\u00e1metros de ploteo (Se va a generar un plot diferente para cada Clase)\nplt.rcParams.update({'font.size': 22})\nplt.figure(figsize=(30,10))\nplt.bar(data_x, data_y, color=['cornflowerblue', 'lightblue', 'steelblue'])  \nplt.ylabel('Count classes')\nplt.title('Classes')\nplt.xticks(rotation=45)","0d79025e":"\nmodel = keras.models.Sequential()\n\nmodel.add(keras.layers.InputLayer(\n    input_shape=(128, 128, 3)\n))\n\nmodel.add(\n    keras.layers.Conv2D(\n        filters=32,\n        kernel_size=(5,5),\n        strides = (1,1),\n        padding='same',\n        activation='relu',\n        name='Conv_1'))\n\nmodel.add(\n    keras.layers.MaxPool2D(\n        pool_size = (2,2),\n        name = 'Pool_1'))#Image_size: 32*64*64(32 filters,image_size 64*64)\n\nmodel.add(\n    keras.layers.Conv2D(\n        filters = 64,\n        kernel_size = (5,5),\n        strides = (1,1),\n        padding = 'same',\n        activation = 'relu',\n        name = 'Conv_2'))\n\nmodel.add(\n    keras.layers.MaxPool2D(\n        pool_size = (2,2),\n        name = 'Pool_2'))#Image_size: 64*32*32(64 filters,image_size 32*32)\n\nmodel.add(\n    keras.layers.Conv2D(\n        filters = 128,\n        kernel_size = (5,5),\n        strides = (1,1),\n        padding = 'same',\n        activation = 'relu',\n        name = 'Conv_3'))\n\nmodel.add(\n    keras.layers.MaxPool2D(\n        pool_size = (2,2),\n        name = 'Pool_3'))#Image_size: 128*16*16(128 filters,image_size 16*16)\n\nmodel.add(\n    keras.layers.Conv2D(\n        filters = 256,\n        kernel_size = (5,5),\n        strides = (1,1),\n        padding = 'same',\n        activation = 'relu',\n        name = 'Conv_4'))\n\nmodel.add(\n    keras.layers.MaxPool2D(\n        pool_size = (2,2),\n        name = 'Pool_4'))#Image_size: 256*8*8(256 filters,image_size 8*8)\n\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(units=1024, activation='relu',name = 'fc_1'))\nmodel.add(keras.layers.Dropout(rate=0.2))\nmodel.add(keras.layers.Dense(units=512, activation='relu',name = 'fc_2'))\nmodel.add(keras.layers.Dense(units=10,activation='softmax',name = 'fc_3'))\nmodel.save('\/tmp\/model')\n#model.compute_output_shape(input_shape=(256,8,8,1))","cd2cc541":"tf.random.set_seed(1)\n#model.build(input_shape=(None,128,128,3))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False), metrics = ['accuracy'])\nprint(model.summary())","16b961c9":"history = model.fit(x = X_Train, y=Y_Train,epochs = 10, batch_size = 500, verbose = 1,validation_split=0.2)","d57100fc":"test_loss, test_acc = model.evaluate(X_Test, Y_Test, verbose = 1)","d33f65f7":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n#plt.ylim([0.9,1])\nplt.legend(['train','test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n#plt.ylim([0,.4])\nplt.legend(['train','test'], loc='upper left')\nplt.show()","539cc414":"model_json = model.to_json()\nmodel.save_weights('Train_weights_1.h5',overwrite = True)","40f005a3":"model.load_weights('Train_weights_1.h5')","d87c7696":"keras.utils.plot_model(model,\"model.png\",show_shapes = True)","cab94c8f":"df = pd.DataFrame({'img':[],'c0':[], 'c1':[],'c2':[], 'c3':[], 'c4':[],'c5':[], 'c6':[], 'c7':[], 'c8':[], 'c9':[]})\ndef _submission(pathPropagate_Images,df):\n    for imgs in glob.glob(os.path.join(pathPropagate_Images,'*.jpg')):\n        img_cv = cv2.imread(imgs)\n        img_cv_r = cv2.resize(img_cv,(128,128))\n        img_cv_predict = np.reshape(img_cv_r,[1,128,128,3])\n        arr_predict = model.predict(img_cv_predict,batch_size = 1)\n        #print(imgs.split('\/')[-1])\n        df = df.append(\n            {\n                'img':imgs.split('\/')[-1],\n                'c0':round(arr_predict[0][0],2), \n                'c1':round(arr_predict[0][1],2),\n                'c2':round(arr_predict[0][2],2),\n                'c3':round(arr_predict[0][3],2),\n                'c4':round(arr_predict[0][4],2),\n                'c5':round(arr_predict[0][5],2),\n                'c6':round(arr_predict[0][6],2),\n                'c7':round(arr_predict[0][7],2),\n                'c8':round(arr_predict[0][8],2),\n                'c9':round(arr_predict[0][9],2)\n            },\n            ignore_index=True\n        )\n    return df","095e4172":"img_cv = cv2.imread(\"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/test\/img_41.jpg\")\nimg_cv_r = cv2.resize(img_cv,(128,128))\nimg_cv_predict = np.reshape(img_cv_r,[1,128,128,3])\narr_predict = model.predict(img_cv_predict,batch_size = 1)\n\nprint(arr_predict)\nprint(round(arr_predict[0][9],2))","4cfd9b41":"pathPropagate_Images =  \"\/kaggle\/input\/state-farm-distracted-driver-detection\/imgs\/test\/\"\ndf = _submission(pathPropagate_Images,df)","6888ddd4":"print(df.shape)\ndf.head(50)","f020337b":"df.to_csv('submission_file.csv',index = False)","465e0e11":"## Train model","07489005":"## Show architecture distribution","06245833":"## Check data integrity\n\n### Classes:\n* c0: safe driving\n* c1: texting - right\n* c2: talking on the phone - right\n* c3: texting - left\n* c4: talking on the phone - left\n* c5: operating the radio\n* c6: drinking\n* c7: reaching behind\n* c8: hair and makeup\n* c9: talking to passenger","6bbb26fb":"### Save weights","6cd71279":"## Preprocessing data","ec7417a3":"## Get Data","f00101ab":"## Check data distribution","c4d8541b":"# Create architecture","9a6171f2":"## Save submission file","b9bcddbf":"## Libraries","e67a78a5":"<div class=\"alert alert-block alert-info\">\n    <h2><center>Distracted Driver Detection Using CNN<\/center><\/h2>\n    <center><p>Languaje used: Python<\/p>\n        <p>Framework used: tf.keras<\/p>\n    <\/center>\n<\/div>","465f8392":"## Predict Test data and create a submission file","c8f0a343":"## Evaluate model with test data"}}