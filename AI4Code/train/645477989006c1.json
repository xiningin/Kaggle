{"cell_type":{"faf19d42":"code","c064284e":"code","0cfe4f5b":"code","7370b01a":"code","5b4f898f":"code","a38840fe":"code","3f21ece8":"code","d2d02f4b":"code","d4f74586":"code","1631e0d3":"code","4b7f8f7e":"code","cd2ec3ed":"code","aeb6315e":"code","b41ace0f":"code","3c933520":"code","34807bd7":"code","ee9d7800":"code","e0aab954":"code","f3ec3dd8":"markdown","4c05d703":"markdown","88573fe3":"markdown","9e8290fb":"markdown","ea2b6574":"markdown","b30bb7d3":"markdown","4cbafbfb":"markdown","e29bd22d":"markdown","87d6ab8f":"markdown"},"source":{"faf19d42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c064284e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pandas as pd\nimport itertools","0cfe4f5b":"data=pd.read_excel('..\/input\/Superstore.xls')\ndata.head()","7370b01a":"\"\"\"\nThere are several categories in the Superstore sales data, \nwe start from time series analysis and forecasting for furniture sales.\n\"\"\"\nfurniture = data.loc[data['Category'] == 'Furniture']\nfurniture.head()","5b4f898f":"cols_to_be_dropped = ['Row ID', 'Order ID', 'Ship Date', 'Ship Mode', \n        'Customer ID', 'Customer Name', 'Segment', 'Country', \n        'City', 'State', 'Postal Code', 'Region', 'Product ID', 'Category', \n        'Sub-Category', 'Product Name', 'Quantity', 'Discount', 'Profit']","a38840fe":"furniture.drop(cols_to_be_dropped, axis=1, inplace=True)\nfurniture.head()","3f21ece8":"furniture = furniture.sort_values('Order Date')\nfurniture.isnull().sum()\n# we need to group the sales by date\nfurniture_modified=furniture.groupby('Order Date')['Sales'].sum().reset_index()\nfurniture_modified.head()","d2d02f4b":"furniture_modified=furniture_modified.set_index('Order Date')\ny = furniture_modified['Sales'].resample('MS').mean() #reducing Sales to the average sales per month\ny.head()","d4f74586":"y.plot(figsize=(15, 6))\nplt.show()","1631e0d3":"# below code is to decompose the data series\nimport statsmodels.api as sm\nfrom pylab import rcParams\nplt.style.use('fivethirtyeight')\nrcParams['figure.figsize'] = 15, 10\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\nfig = decomposition.plot()\nplt.show()","4b7f8f7e":"# Define the p, d and q parameters to take any value between 0 and 2\np = d = q = range(0, 2)\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","cd2ec3ed":"import warnings\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages\nresults={}\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n\n            result = mod.fit()\n            if result.aic not in results:\n                results[result.aic]=[param,param_seasonal]\n            #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, result.aic))\n        except:\n            continue","aeb6315e":"min_aic=min(results)\nprint(min_aic)\nresults[min_aic]","b41ace0f":"dataset=y.reset_index()\n\ntrain_dataset=dataset.iloc[:36,:]\ntest_dataset=dataset.iloc[36:,:]","3c933520":"import numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nfrom statistics import mean\n\ntscv = TimeSeriesSplit()\nprint(tscv)\nlist_MSE=[] # A list to store means square error for each fold of validation\n\n# using TimeSeriesSplit for kfold validation\nfor train_index, validation_index in tscv.split(train_dataset):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    y_train, y_validate = train_dataset.iloc[train_index], train_dataset.iloc[validation_index]\n    mod = sm.tsa.statespace.SARIMAX(y_train.set_index(\"Order Date\"),\n                                order=(0, 1, 1),\n                                seasonal_order=(0, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\n    model=mod.fit()\n    y_validate.set_index(\"Order Date\")\n    y_pred=model.forecast(6, alpha=0.05) # 6 samples to be forecast with 95% conf\n\n    mse = mean_squared_error(y_validate.values[:,1], y_pred.values) # y_test is the true values against which \"predicted values\" are evaluated.\n    list_MSE.append(mse)\n    print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n\nprint(\"The average of MSE is {}\".format(mean(list_MSE)))\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mean(list_MSE)), 2)))","34807bd7":"# Now it is time to build the final model of SARIMA \n\nFinal_Model=sm.tsa.statespace.SARIMAX(train_dataset.set_index(\"Order Date\"),\n                                order=(0, 1, 1),\n                                seasonal_order=(0, 1, 1, 12),\n                                enforce_stationarity=False,\n                                enforce_invertibility=False)\nFinal_Model=Final_Model.fit()\ntest_dataset.set_index(\"Order Date\")\npredictions=Final_Model.forecast(12, alpha=0.05) # 12 samples of year 2017 to be forecast with 95% conf\n\nmse = mean_squared_error(test_dataset.values[:,1], predictions.values) # y_test is the true values against which \"predicted values\" are evaluated.\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","ee9d7800":"print(Final_Model.summary().tables[1])\n","e0aab954":"#testing the model\n# pred = fitted_model.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False)\npred_ci = Final_Model.conf_int()\npred_ci\n\nax = y.plot(label='observed')\npredictions.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n\n# ax.fill_between(pred_ci.index,\n#                 pred_ci.iloc[:, 0],\n#                 pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Furniture Sales')\nplt.legend()\n\nplt.show()","f3ec3dd8":"## Testing the fitted model with test dataset (i.e. sales of 2017)","4c05d703":"We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise.\n\nSome distinguishable patterns appear when we plot the data. The time-series has seasonality pattern , such as sales are always low at the beginning of the year and high at the end of the year. There is always an upward trend within any single year with a couple of low months in the mid of the year","88573fe3":"# - Implementing Seasonal ARIMA Model#\n\n","9e8290fb":"### Train a model (SARIMAX) and validate using Cross validation","ea2b6574":"## Below code is cross-validate the ARIMA model ","b30bb7d3":"**Visualizing Furniture Sales Time Series Data**","4cbafbfb":"## The above figures clearly shows that the sales of furniture is unstable, along with its obvious seasonality.","e29bd22d":"### Tuning the hyper-parameters using Grid Search\n\nWe will use a \u201cgrid search\u201d to iteratively explore different combinations of parameters. For each combination of parameters, we fit a new seasonal ARIMA model with the SARIMAX() function from the statsmodels module and assess its overall quality. Once we have explored the entire landscape of parameters, our optimal set of parameters will be the one that yields the best performance for our criteria of interest\n\n","87d6ab8f":"# **Data Preprocessing**\nThis step includes removing columns we do not need, check missing values, aggregate sales by date and so on."}}