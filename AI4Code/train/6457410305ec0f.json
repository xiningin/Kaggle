{"cell_type":{"53fc7245":"code","fde8c04a":"code","9b0d01ec":"code","4b6b709a":"code","77075f63":"code","76438d49":"code","c888ffe0":"code","758f810e":"code","d5fa4a6e":"code","9f54d7b6":"code","3d17b98f":"code","2b336f67":"code","f6bf7c2a":"code","833f442f":"code","048e9b08":"code","9db3ae86":"code","4b1a4d8c":"code","50c65cd4":"code","92e316c9":"code","03faf830":"code","0c1e45cc":"code","595e3a53":"code","bae10928":"code","c70f78aa":"code","b040008a":"code","5da3319f":"code","bfa150d6":"code","b4cf6d98":"code","37d19f02":"code","99165bb8":"code","6d56e680":"code","14ce2cbe":"code","a7afd641":"code","6666b9d5":"code","6bc7d91b":"code","378304fb":"code","c8666024":"code","e93b8677":"code","6405e62c":"code","09b4f7b5":"code","f8c53cb9":"code","87511160":"code","9b894430":"code","bbe1c3b1":"code","d6c55307":"markdown","3d2d0c29":"markdown","1b39fb62":"markdown","0e4dde76":"markdown","8f128d20":"markdown","690e81bf":"markdown","e4a56eae":"markdown","337b7b00":"markdown","33f37b35":"markdown","2d16acf0":"markdown","2cb0af38":"markdown","a7a7df0f":"markdown","9fd8cd0f":"markdown","fdb40589":"markdown","31eb2085":"markdown","bb5de7fc":"markdown","5955e416":"markdown","f5b5aa74":"markdown","4f69c902":"markdown","6628a79a":"markdown"},"source":{"53fc7245":"!pip install IQA_pytorch #For SSIM Score","fde8c04a":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision\nfrom torch.optim import *\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom IQA_pytorch import DISTS, utils\n\nimport numpy as np\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport albumentations\nimport albumentations.pytorch\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib import font_manager, rc\nfrom IPython import display\nimport random\nimport glob\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\nimport warnings\nimport sys\nfrom tqdm import tqdm\nimport pickle\nimport gc\nimport random\nimport urllib.request\n\nwarnings.filterwarnings(\"ignore\")","9b0d01ec":"gc.collect()\ntorch.cuda.empty_cache()","4b6b709a":"%matplotlib inline\n\nplt.rcParams['axes.unicode_minus'] = False\nfontpath = \"..\/input\/koreanfont\/NanumBrush.ttf\"\nfontprop = font_manager.FontProperties(fname=fontpath)\n\nplt.rcParams[\"animation.html\"] = \"jshtml\"\nplt.rcParams['figure.dpi'] = 150  \nplt.ioff()","77075f63":"# Device\nUSE_CUDA = torch.cuda.is_available()\n\nprint(\"Device : {0}\".format(\"GPU\" if USE_CUDA else \"CPU\"))\ndevice = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\ncpu_device = torch.device(\"cpu\")\n\nDEBUG = False\n\nRANDOM_SEED = 2004\n\n# Train\nstart_epoch = 0\nall_epochs = 1\nbatch_size = 14\n\nlrG = 0.0002\nlrD = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\nL1lambda = 100\nGAMMA = 0.59\n\nTIME_STEP = 4\nTEST_TIME_STEP = 6\n\npatch = (1,256\/\/2**4,256\/\/2**4)\n\n# Path\nDATASET1_PATH = '..\/input\/the-cloudcast-dataset'\n\n# Checkpoint\nUSE_CHECKPOINT = True\n\nOLD_PATH = '..\/input\/checkpoint'\nOLD_GENERATOR_MODEL = os.path.join(OLD_PATH, 'Generator.pth')\nOLD_DISCRIMINATOR_MODEL = os.path.join(OLD_PATH, 'Discriminator.pth')\nOLD_G_LOSS = os.path.join(OLD_PATH, 'gloss.txt')\nOLD_D_LOSS = os.path.join(OLD_PATH, 'dloss.txt')","76438d49":"torch.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed(RANDOM_SEED)\ntorch.cuda.manual_seed_all(RANDOM_SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\nprint('Random Seed : {0}'.format(RANDOM_SEED))","c888ffe0":"def log(text):\n    global DEBUG\n    if DEBUG:\n        print(text)","758f810e":"def torch_tensor_to_plt(img):\n    img = img.detach().numpy()[0]\n    img = np.transpose(img, (1, 2, 0))\n    return img ","d5fa4a6e":"def show_video_in_jupyter_nb(width, height, video_url):\n    from IPython.display import HTML\n    return HTML(\"\"\"<video width=\"{}\" height=\"{}\" controls>\n    <source src={} type=\"video\/mp4\">\n    <\/video>\"\"\".format(width, height, video_url))","9f54d7b6":"def plt_image_animation(frames, update_func):\n    fig, ax = plt.subplots(figsize=(4,4))\n    plt.axis('off')\n    anim = animation.FuncAnimation(fig, update_func, frames=frames)\n    video = anim.to_html5_video()\n    html = display.HTML(video)\n    display.display(html)\n    plt.close()","3d17b98f":"plt_image_animation(15, lambda t : plt.imshow(np.load(join(DATASET1_PATH, '2017M01', '{0}.npy'.format(t))), cmap='gray'))","2b336f67":"transformer = transforms.Compose([transforms.ToTensor(),\n                                  torchvision.transforms.Resize(128),\n                                  transforms.Normalize((0.5), (0.5)), #GrayScale\n                                 ])\n","f6bf7c2a":"nowpath = \"\"\n\nclass TimeStepImageDataset(Dataset):\n    def __init__(self, date, time_step, transform=None):\n        self.date = date\n        self.time_step = time_step\n        self.transformer = transform\n        self.file = []\n        \n        file_list = glob.glob(join(self.date, '*'))\n        self.file = [file for file in file_list if (file.endswith(\".npy\") and not file.endswith('TIMESTAMPS.npy'))]\n        \n    def __len__(self):\n        return len(self.file)-self.time_step     \n    \n    def transform(self, image):\n        if self.transformer:\n            return self.transformer(image)\n        else :\n            return image\n\n    def __getitem__(self, idx):\n        global nowpath\n        log(join(self.date, str(idx)+'.npy'))\n        X = self.transform(np.load(join(self.date, str(idx)+'.npy')))\n        nowpath = join(self.date, str(idx)+'.npy')\n        Y_list = []\n        for i in range(1, self.time_step+1):\n            Y_list.append(self.transform(np.load(join(self.date, str(idx+i)+'.npy'))).unsqueeze(0))\n        Y = torch.cat(Y_list)       \n        return X, Y","833f442f":"DATASET1_DIRS = glob.glob(join(DATASET1_PATH, '*'))\n\nrandom.shuffle(DATASET1_DIRS)\n\ntraindatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[:20]):\n    traindatasetlist.append(TimeStepImageDataset(name, TIME_STEP, transform=transformer))\ntrain_dataset = torch.utils.data.ConcatDataset(traindatasetlist)\n\ntestdatasetlist = []\nfor ind, name in enumerate(DATASET1_DIRS[20:]):\n    testdatasetlist.append(TimeStepImageDataset(name, TEST_TIME_STEP, transform=transformer))\ntest_dataset = torch.utils.data.ConcatDataset(testdatasetlist)","048e9b08":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataloader_bs1_shuffle = DataLoader(test_dataset, batch_size=1, shuffle=True) \ntest_dataloader_bs1_noshuffle = DataLoader(test_dataset, batch_size=1, shuffle=False) ","9db3ae86":"def ShowDatasetImage(x, y):\n    grid = torchvision.utils.make_grid(y)\n\n    fig = plt.figure(figsize=(2, 2))\n    plt.imshow(torch_tensor_to_plt(x.unsqueeze(0)), cmap='gray')\n    plt.axis('off')\n    plt.title('Input (Now)', fontproperties=fontprop)\n    plt.show()   \n    \n    fig = plt.figure(figsize=(8, 2.5))\n    plt.title('Real Weather Image', fontproperties=fontprop)\n    plt.axis('off')\n    for i in range(1, TIME_STEP+1):\n        ax = fig.add_subplot(1, TIME_STEP, i)\n        ax.axis('off')\n        ax.imshow(torch_tensor_to_plt(y[i-1].unsqueeze(0)), cmap='gray')\n        ax.set_title('after {0} minutes'.format(15*i), fontproperties=fontprop)\n    plt.show()\n\n    del x, y","4b1a4d8c":"for ind, (x, y) in enumerate(train_dataset):\n    if ind != 0:\n        continue\n    ShowDatasetImage(x, y)\n    break","50c65cd4":"class UNetDown(nn.Module):\n    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n        super().__init__()\n\n        layers = [nn.Conv2d(in_channels, out_channels, 4, stride=2, padding=1, bias=False)]\n\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_channels)),\n\n        layers.append(nn.LeakyReLU(0.2))\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.down = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.down(x)\n        return x","92e316c9":"class UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0):\n        super().__init__()\n\n        layers = [\n            nn.ConvTranspose2d(in_channels, out_channels,4,2,1,bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU()\n        ]\n\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.up = nn.Sequential(*layers)\n\n    def forward(self,x,skip):\n        x = self.up(x)\n        x = torch.cat((x,skip),1)\n        return x","03faf830":"class GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=1, out_channels=1):\n        super().__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64,128)                 \n        self.down3 = UNetDown(128,256)               \n        self.down4 = UNetDown(256,512,dropout=0.5) \n        self.down5 = UNetDown(512,512,dropout=0.5)      \n        self.down6 = UNetDown(512,512,dropout=0.5)             \n        self.down7 = UNetDown(512,512,dropout=0.5)              \n        self.down8 = UNetDown(512,512,normalize=False,dropout=0.5)\n\n        self.up1 = UNetUp(512,512,dropout=0.5)\n        self.up2 = UNetUp(1024,512,dropout=0.5)\n        self.up3 = UNetUp(1024\/\/2,512,dropout=0.5)\n        self.up4 = UNetUp(1024,512,dropout=0.5)\n        self.up5 = UNetUp(1024,256)\n        self.up6 = UNetUp(512,128)\n        self.up7 = UNetUp(256,64)\n        self.up8 = nn.Sequential(\n            nn.ConvTranspose2d(128,out_channels,4,stride=2,padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = d6\n        u2 = self.up3(u1,d5)\n        u3 = self.up4(u2,d4)\n        u4 = self.up5(u3,d3)\n        u5 = self.up6(u4,d2)\n        u6 = self.up7(u5,d1)\n        u7 = self.up8(u6)\n        \n        return u7","0c1e45cc":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.relu = nn.ReLU(False)\n\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = self.relu(out)\n        return out","595e3a53":"class Discriminator(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=1):\n        super(Discriminator, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear1 = nn.Linear(1*1*512*block.expansion, 1024)\n        self.linear2 = nn.Linear(1024, num_classes)\n\n        self.relu = nn.ReLU(False)\n        self.sigmoid = nn.Sigmoid()\n        self.avg_pool2d = nn.AvgPool2d(16, 16)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool2d(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear1(out)\n        out = self.linear2(out)\n        out = self.sigmoid(out)\n        return out","bae10928":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) == nn.Conv2d:\n        m.weight.data.normal_(0.0, 0.02)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","c70f78aa":"Generator = GeneratorUNet().to(device)\nDiscriminator = Discriminator(BasicBlock, [3, 4, 6, 3]).to(device)\n\nsummary_g = Generator.apply(weights_init)\nsummary_d = Discriminator.apply(weights_init)","b040008a":"optimizerG = Adam(Generator.parameters(), lr=lrG, betas=(beta1, beta2))\noptimizerD = Adam(Discriminator.parameters(), lr=lrD, betas=(beta1, beta2))","5da3319f":"img_list = []\nG_loss = []\nD_loss = []\n\nFAKE_LABEL = 0.0\nREAL_LABEL = 1.0","bfa150d6":"l1loss = nn.L1Loss()\nbceloss = nn.BCELoss()","b4cf6d98":"def generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=0.0):\n    def G_error(G_output, real, D_output):\n        return l1loss(G_output, real)*L1lambda + bceloss(D_output, real_label)\n    \n    next_input = sketch\n    error = None\n    \n    real_list = []\n    for i in range(TIME_STEP):\n        real_list.append(real[:,i,:,:,:])\n    \n    for ind, y in enumerate(real_list):\n        G_output = netG(next_input)\n        next_input = G_output.clone().detach()\n        D_output = netD(G_output).view(-1)     \n        \n        if ind==0:\n            error = G_error(G_output, y, D_output)\n        else :\n            error += (gamma ** ind) * G_error(G_output, y, D_output)\n            \n        del G_output, D_output\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    return error","37d19f02":"def discriminator_error(netG, netD, sketch, real, real_label, fake_label, avg=True):\n    output_g = netG(sketch)\n    outputs_fake = netD(output_g.detach()).view(-1)\n    errD = bceloss(outputs_fake, fake_label)\n    \n    del output_g, outputs_fake\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    for i in range(0, TIME_STEP):\n        outputs_real = netD(real[:,i,:,:]).view(-1)\n        if avg:\n            errD += bceloss(outputs_real, real_label)\/TIME_STEP\n        else:\n            errD += bceloss(outputs_real, real_label)\n        del outputs_real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    return errD","99165bb8":"def apply_checkpoint(use_checkpoint=True):\n    global Generator, Discriminator, optimizerG, optimizerD, G_Loss, D_Loss, start_epoch\n    \n    if os.path.isdir(OLD_PATH) and use_checkpoint:        \n        checkpoint = torch.load(OLD_GENERATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Generator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerG.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        checkpoint = torch.load(OLD_DISCRIMINATOR_MODEL)\n        start_epoch = checkpoint['epoch']\n        Discriminator.load_state_dict(checkpoint['model_state_dict'])\n        optimizerD.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        with open(OLD_G_LOSS, 'rb') as f:\n            G_loss = pickle.load(f)\n            \n        with open(OLD_D_LOSS, 'rb') as f:\n            D_loss = pickle.load(f)\n        \n        print('Continue training. (Epoch : {0})'.format(start_epoch))\n    else :\n        print('Begin training newly.')","6d56e680":"nowepoch = 0\nstrange_error_limit = 10\nstrange_error_num = 0\n\ndef fit(device, num_epochs=1000):\n    global nowepoch\n    iters = 0\n    for epoch in range(start_epoch+1, num_epochs+start_epoch+1):\n        nowepoch = epoch\n        print(\"< EPOCH{0} >\".format(epoch))\n        result = train_one_epoch(device, train_dataloader, Generator, Discriminator, optimizerG, optimizerD, epoch, num_epochs)\n        if not result:\n            return\n    \n\ndef train_one_epoch(device, dataloader, netG, netD, optimizerG, optimizerD, epoch, num_epochs, iters=0):\n    global nowpath, strange_error_num, strange_error_limit\n    with torch.autograd.set_detect_anomaly(True):\n        for i, data in enumerate(dataloader):   \n            sketch, real = data\n            sketch, real = sketch.to(device), real.to(device)\n            \n            b_size = sketch.size(0)\n            real_label = torch.full((b_size,), REAL_LABEL, dtype=torch.float, device=device)\n            fake_label = torch.full((b_size,), FAKE_LABEL, dtype=torch.float, device=device)\n            \n            #Train Discriminator\n            netG.eval()\n            netD.train()\n            netD.zero_grad()\n            \n            errD = discriminator_error(netG, netD, sketch, real, real_label, fake_label)\n            \n            log('Complete calcuating of Discriminator')\n            errD.backward()\n            log('Complete backprogration of Discriminator')\n            optimizerD.step()\n            log('Complete stepping OptimizerD')\n        \n            #Train Generator\n            netG.train()\n            netD.eval()\n            netG.zero_grad()\n            \n            \n            errG = generator_error(netG, netD, sketch, real, real_label, fake_label, gamma=GAMMA)\n            \n            log('Complete calcuating of Generator')\n            errG.backward()\n            log('Complete backprogration of Genereator')\n            optimizerG.step()\n            log('Complete stepping OptimizerG')\n            \n            del b_size, real_label, fake_label, sketch, real\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            #Log\n            if i % 1 == 0:\n                print('[%d\/%d][%d\/%d]\\tLoss_G: %.4f\\tLoss_D: %.4f'\n                      % (epoch, num_epochs, i, len(dataloader),\n                         errG.item(), errD.item()))\n                \n            \n\n            G_loss.append(errG.item())\n            D_loss.append(errD.item())\n            \n            del errG, errD\n            gc.collect()\n            torch.cuda.empty_cache()\n\n            iters += 1\n    return True","14ce2cbe":"apply_checkpoint(use_checkpoint=USE_CHECKPOINT)","a7afd641":"summary = Generator.train()\nsummary = Discriminator.train()\n\nif all_epochs>0:\n    fit(device, num_epochs=all_epochs)\n\nsummary = Generator.eval()\nsummary = Discriminator.eval()","6666b9d5":"plt.figure(figsize=(10,5))\nplt.title('Loss of Generator')\nplt.plot(G_loss,label=\"\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","6bc7d91b":"plt.figure(figsize=(10,5))\nplt.title('Loss of Discriminator')\nplt.plot(D_loss,label=\"train\")\nplt.xlabel(\"Iter\")\nplt.legend()\nplt.show()","378304fb":"def model_predict(model, time, input):\n    if time%15==0 and time!=0:\n        model.eval()\n        num = time\/\/15\n        \n        next_input = input\n        for i in range(num):\n            next_input = model(next_input).clone().detach()\n        return next_input\n    else:\n        raise ValueError('Please set the time to a multiple of 15.')","c8666024":"from IQA_pytorch import SSIM, utils\n\ntoPILImage = transforms.ToPILImage()\nssim_model = SSIM(channels=1)\n\ndef one_time_step_ssim_score(dataloader, model, time_step, num=-1):\n    model.eval()\n    score = 0\n    total = 0\n    for ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n        x, y = x.squeeze(0).to(device), y.squeeze(0).to(device)\n        outputG = model_predict(model, time_step*15, x.unsqueeze(0))\n\n        sketch = utils.prepare_image(toPILImage(outputG.squeeze(0))).to(device)\n        real = utils.prepare_image(toPILImage(y[time_step-1])).to(device)\n\n        score += ssim_model(sketch, real, as_loss=False).item()\n        total += 1\n\n        del x, y, outputG, sketch, real\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        if num != -1:\n            if ind+1 >= num:\n                break\n            \n    print(\"SSIM Score of the prediction {0} minutes later : {1}\".format(time_step*15, score\/total))\n    return score\/total\n\nfor ind in range(1, TEST_TIME_STEP+1):\n    one_time_step_ssim_score(test_dataloader_bs1_shuffle, Generator, ind, num=2000)","e93b8677":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_noseries_ls = []\nreal_noseries_ls = []\n\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_shuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    x, y = x.to(device), y[0].to(device)\n        \n    outputg = Generator(x).to(cpu_device)\n    \n    outputg = outputg*127.5+127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg)*30)\n    cv2.imwrite('.\/Real_NOSERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_noseries_ls.append('.\/AI_NOSERIES_Answer{0}.png'.format(ind+1))\n    real_noseries_ls.append('.\/Real_NOSERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_noseries.zip\", 'w') as my_zip:\n    for i in ai_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n\n\nwith zipfile.ZipFile(\"real_noseries.zip\", 'w') as my_zip:\n    for i in real_noseries_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \nfor file in (ai_noseries_ls + real_noseries_ls):\n    os.remove(file)\n    \nprint('NOSERIES Images are generated.')","6405e62c":"import zipfile\n\ny_nums = 40\niter = 0\n\nai_series_ls = []\nreal_series_ls = []\n\nnext_input = None\nstart_ind = 200\n\nfor ind, (x, y) in enumerate(test_dataloader_bs1_noshuffle):\n    if ind < start_ind:\n        continue\n        \n    iter += 1\n    \n    if ind == start_ind:\n        next_input = x.clone().detach().to(device)\n        cv2.imwrite('.\/Input_SERIES.png', torch_tensor_to_plt(next_input.to(cpu_device)*127.5+127.5)*30)\n    \n    x, y = x.to(device), y[0].to(device)\n    \n    outputg_series = Generator(next_input).to(cpu_device)\n    next_input = outputg_series.clone().detach().to(device)\n\n    outputg_series = outputg_series * 127.5 + 127.5\n    realimage = y*127.5+127.5\n\n    cv2.imwrite('.\/AI_SERIES_Answer{0}.png'.format(ind+1), torch_tensor_to_plt(outputg_series)*30)\n    cv2.imwrite('.\/Real_SERIES{0}.png'.format(ind+1), torch_tensor_to_plt(realimage.to(cpu_device))*30)\n    \n    ai_series_ls.append('.\/AI_SERIES_Answer{0}.png'.format(ind+1))\n    real_series_ls.append('.\/Real_SERIES{0}.png'.format(ind+1))\n    \n    if iter > y_nums:\n        break\n\nwith zipfile.ZipFile(\"ai_series.zip\", 'w') as my_zip:\n    for i in ai_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n\nwith zipfile.ZipFile(\"real_series.zip\", 'w') as my_zip:\n    for i in real_series_ls:\n        my_zip.write(i)\n    my_zip.close()\n    \n\n    \nprint('SERIES Images are generated')","09b4f7b5":"v1 = cv2.VideoWriter('oraclegan_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in ai_series_ls:\n    v1.write(cv2.imread(name))\nv1.release()\n\nv2 = cv2.VideoWriter('real_series.mp4',cv2.VideoWriter_fourcc(*'DIVX'), 3, (128, 128))\nfor name in real_series_ls:\n    v2.write(cv2.imread(name))\nv2.release()\n\nprint('Videos are generated')\nprint('video path : \".\/oraclegan_series.mp4\" and \".\/real_series.mp4\"')","f8c53cb9":"urllib.request.urlretrieve('https:\/\/storage.googleapis.com\/kaggle-script-versions\/80102153\/output\/pix2pix_series.mp4?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20211118%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211118T232151Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=8cb00c08f73986d99bac385f6910658fe1b34ee66e9e9127ba169245b0e063860cefefb9816971bf362075f1183c111df561b01a660841d6207a834dd645b21e501a7fabe15d5f82946bf07286da07bf97b7c3859f83e7c4c28cc5d8f224c353dad78e43c355845c0067fe4658d431940b320f828224c5a2a85542ece1fde70b3845468ab268a69794420f599be28d2ff09ad76142e90185567b00ad6f667bb4e751d4e871640131944954d2ac60349e0df27e0b3cdff9d318a31512538ae7024ae1b9098fa5ac6d358aeb501f533d0bbd94851b756fe34028fe88f2ef335d683d78d276d25c0f0490ed7508a6c9f4878f795ab1ae252d0419f176e1df455528', '.\/pix2pix_series.mp4') \nprint('Videos are saved')\nprint('video path : \".\/pix2pix_series.mp4\"')","87511160":"for file in (ai_series_ls + real_series_ls):\n    os.remove(file)","9b894430":"torch.save({\n            'epoch': nowepoch,\n            'model_state_dict': Generator.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            }, 'Generator.pth')\n\ntorch.save({\n            'epoch': nowepoch,\n            'model_state_dict': Discriminator.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            }, 'Discriminator.pth')","bbe1c3b1":"with open('.\/gloss.txt', 'wb') as f:\n    pickle.dump(G_loss, f)\nwith open('.\/dloss.txt', 'wb') as f:\n    pickle.dump(D_loss, f)","d6c55307":"# Save Checkpoint","3d2d0c29":"# Install additional libraries\n\nIQA_pytorch is a library which is used to calculate SSIM Score","1b39fb62":"# Define Cost Functions","0e4dde76":"# Visualize Data\n|Name of Function|Explanation|\n|-----|-----|\n|torch_tensor_to_plt|Convert torch image to matplotlib image|\n|plt_image_animation|show a video by update_function|","8f128d20":"# **Key Featues of OracleGAN**\n-------------------------------------\n- [Time Step Image Dataset](#time_step_image_dataset)\n- [Cost Function of Generator](#cost_of_generator)\n- [Cost Function of Discriminator](#cost_of_discriminator)","690e81bf":"> **< SSIM Score of normal Pix2Pix >**  *(check \"[Pix2Pix (Compared to OracleGAN)](https:\/\/www.kaggle.com\/lapl04\/pix2pix-compared-to-oraclegan)\")*\n>\n> |Prediction|SSIM Score|\n> |-------------------|----------------|\n> |prediction 15 minutes later|0.788236691981554|\n> |prediction 30 minutes later|0.6658438920378685|\n> |prediction 45 minutes later|0.5865897158235311|\n> |prediction 60 minutes later|0.5096726692765952|\n> |prediction 75 minutes later|0.42021833929419516|\n> |prediction 90 minutes later|0.3831377309216186|","e4a56eae":"# Apply Checkpoint","337b7b00":"# Preprocess Dataset","33f37b35":"# Define Hyperparameters\n|Name of Hyperparameter|Explanation|\n|-----|-----|\n|USE_CUDA|whether to use GPU|\n|DEBUG|whether to print specific logs|\n|RANDOM_SEED|random seed of pytorch, random, numpy|\n|start_epoch|this is used to continuing train from checkpoint|\n|all_epochs|Epochs|\n|batch_size|Batch Size|\n|lrG|the learning rate of Generator|\n|lrD|the learning rate of Discriminator|\n|beta1, beta2|the beta1 and beta2 of Generator and Discriminator|\n|**L1Lambda**|lambda of pix2pix objective function|\n|**GAMMA**|factor similar to discount factor of DQN. (0<$\\gamma$<1) (check cost function of OracleGAN Generator)|\n|**TIME_STEP**|the number of future images which is used to calculate loss (check cost function of OracleGAN Generator)|\n\n","2d16acf0":"# **Goal**\n- Predict future weather images using current weather images.","2cb0af38":"# **OracleGAN**\n--------------\nThis study explores how to effectively learn time series data using GAN-based neural networks.\n\nThe cost function is combination of Pix2Pix's cost function and future predictive loss function using the discount coefficient used in DQN.\nAlso, it is easy to apply future predictive loss function using a one-to-many dataset structure.","a7a7df0f":"# Define Train Function","9fd8cd0f":"<a id=\"cost_of_discriminator\"><\/a>\n## **Cost Function of Discriminator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_D(x, y) = E_x\\left [ log D(G(x)) \\right ] + \\frac{1}{t} \\sum_{i=1}^{t} E_{y_i}\\left [ log(1-D(G(y_i)))) \\right ]} $$**\n\n$t$ is Time Step.","fdb40589":"# Initiate Weights and Biases","31eb2085":"<a id=\"cost_of_generator\"><\/a>\n## **Cost Function of Generator**\n\n------------------------------------------------\n\n**$$ \\mathbf{Loss_G(x, y) = \\sum_{i=1}^{t}\\gamma ^ {i-1}\\times \\left \\{ \\lambda _1 \\times  E_{x,y_i}\\left [ \\left \\| y_i - G^i(x) \\right \\|_1 \\right ] + E_{x}\\left [ log(1-D(G^i(x))) \\right ] \\right \\} } $$**\n\n$t$ is Time Step. $\\gamma$ is discount factor(GAMMA). $\\lambda _1$ is L1Lambda.","bb5de7fc":"# Test\n\n1. Calculate SSIM Score each Time Steps\n2. Generate test predicted images.\n3. Generate video which consist of series predicted images.","5955e416":"# Define Neural Networks and Optimizers\n|Name|Sort|\n|----|----|\n|Generator|UNet|\n|Discriminator|ResNet|\n|Optimizer of Generator|Adam|\n|Optimizer of Disciminator|Adam|","f5b5aa74":"<a id=\"time_step_image_dataset\"><\/a>\n## **Time Step Image Dataset**\n\n------------------------------------------------\n\nOracleGAN calculates loss between predicted image and real image not only after 15 minutes but also **after 15\u00d7TimeStep minutes**.\n \nSo, dataset need to have **multiple output** images per **one input** image.","4f69c902":"# Train","6628a79a":"# Import Libraries"}}