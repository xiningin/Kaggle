{"cell_type":{"c5b23137":"code","1195f896":"code","ba1e542a":"code","5734a5b9":"code","eacbec98":"code","0db768fb":"code","92ae49da":"code","a1732617":"code","34c0874d":"code","a09db17a":"code","ad443516":"code","6eda255f":"code","408249e2":"code","2de9536d":"code","56cfad47":"code","f73150d9":"code","4bdca1c4":"code","ebafb165":"code","b5f3c3b3":"code","b3200b0a":"code","9d6e0702":"code","cec94b9d":"code","f2c6c5dd":"code","c51bd125":"code","b3bc8086":"code","87ce5407":"code","4ecb3e8a":"code","e8bffcf5":"code","83722947":"code","d14fcbc9":"code","2465e1e9":"code","d5f4d5b9":"code","e7ca127e":"code","897dbcfe":"code","5280766f":"code","8e512c01":"code","3b544838":"code","124be895":"code","a64c5a8e":"code","9d23f1ec":"code","7336393a":"code","0aedbc15":"code","606a7ed9":"code","cbc201ef":"code","cef8c656":"code","d8078c3a":"code","8f57a934":"code","64c60ae5":"code","6961ddaf":"code","ec3e6f96":"code","99fb0610":"markdown","eb8fd318":"markdown","8cac67cf":"markdown","3362d382":"markdown","e6963f1e":"markdown","91215071":"markdown","b1939a5b":"markdown","d6c06abe":"markdown","a48dcb4d":"markdown","fb7728b8":"markdown","84cfe501":"markdown","75be23d2":"markdown","e7f1f9a0":"markdown","4cbc4fba":"markdown","e7e66b6e":"markdown","bee26a98":"markdown","7d8238a2":"markdown","93df5b59":"markdown","895272a4":"markdown","b284997b":"markdown","d3cd8c94":"markdown","1fe1cf23":"markdown","23ed52fd":"markdown","f473dcfd":"markdown","0579dee3":"markdown","2077b2dc":"markdown","93a6ea67":"markdown","19d9f69f":"markdown","bc3cd759":"markdown","c92827d9":"markdown","19c7f658":"markdown","9013678f":"markdown","2bb03f0d":"markdown","982445c0":"markdown","bc0e4ae7":"markdown","0253b8b2":"markdown","816a64fa":"markdown","46053342":"markdown","a4a35e90":"markdown","c59446f0":"markdown","6cc35058":"markdown","ab6cd7a5":"markdown","12c47a06":"markdown","62101ce0":"markdown","4c883189":"markdown","b8d72a2c":"markdown","2d2599a8":"markdown","b363f295":"markdown","304abfa3":"markdown","064b7dc7":"markdown","f74a1d8f":"markdown","92209326":"markdown","afd034b6":"markdown","44ea6328":"markdown","6098bf6e":"markdown","9542e965":"markdown","acbe5111":"markdown","3c986c75":"markdown","cd8c0ccd":"markdown","1d9b5126":"markdown","fc7f4e23":"markdown","1d0d7cf8":"markdown","9a057164":"markdown","13e6693d":"markdown","957e5d26":"markdown","30e196bb":"markdown","bc4b668b":"markdown","19e21b48":"markdown","6b18c7b5":"markdown","f6d7ca67":"markdown","d92ea5b5":"markdown","bef01a51":"markdown"},"source":{"c5b23137":"### Libraries\n\nimport pandas as pd\nimport numpy as np\nimport time\nimport gc\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# https:\/\/matplotlib.org\/\nimport matplotlib.pyplot as plt \n\n# https:\/\/seaborn.pydata.org\/\nimport seaborn as sns \n\n# https:\/\/scikit-learn.org\/stable\/about.html#citing-scikit-learn\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score, mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.inspection import plot_partial_dependence, partial_dependence\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC # support vector classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\n# catboost.ai\nfrom catboost import CatBoostClassifier, Pool, cv\n\n#https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html\nimport xgboost","1195f896":"# Original\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv') # check your path\ndisplay(df.head(2), 'Dataset is small with only {} observation'.format(df.shape[0]))","ba1e542a":"assert df.dtypes.any() != object\ndf.isnull().values.any() # check if there are missing values ","5734a5b9":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm', low=0.15)","eacbec98":"fig = plt.figure()\nsns.pairplot(df, vars=['age', 'trestbps', 'chol', 'thalach', 'oldpeak'], hue='target')\\\n            .fig.suptitle('Distributions of higly correlated features over Target variable')\nplt.subplots_adjust(top=0.9)\nplt.show();","0db768fb":"# plot \nfig = plt.figure(figsize=(10,10))\ng = sns.catplot(x='cp', y='age', hue='sex', col='target', data=df)\n(g.set_axis_labels(\"Level of pain\", \"Age\")\n .set_xticklabels([\"no pain\", \"start to feel\", \"middle pain\", 'hight pain'])\n .set_titles(\"Person is ill : {col_name}\")\n .fig.suptitle('Distribution of ill and not ill person for Age, Sex and Level of Pain'))\nplt.subplots_adjust(top=0.8)\nplt.show()","92ae49da":"pd.crosstab(df.age, df.target).plot(kind='bar', figsize=(20, 5))\nplt.title('Age distribution over Target Variable')\nplt.show();","a1732617":"fig, axarr = plt.subplots(3, 2)\nfig.suptitle(\"Features that are not binary or normally distributed\", fontsize=12)\nplt.subplot(3, 2, 1, title='cp')\ndf.cp.hist()\nplt.subplot(3, 2, 2, title='restecg')\ndf.restecg.hist()\nplt.subplot(3, 2, 3, title='oldpeak')\ndf.oldpeak.hist()\nplt.subplot(3, 2, 4, title='slope')\ndf.slope.hist()\nplt.subplot(3, 2, 5, title='ca')\ndf.ca.hist()\nplt.subplot(3, 2, 6, title='thal')\ndf.thal.hist()\nplt.subplots_adjust(top=1.5)\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\nplt.show();","34c0874d":"# Function for One Hot for tranformation\ndef encode(data, col):\n    return pd.concat([data, pd.get_dummies(col, prefix=col.name)], axis=1)\ndf = encode(df, df.cp)\ndf = encode(df, df.thal)\ndf = encode(df, df.slope)\ndf = encode(df, df.restecg)\ndf = encode(df, df.ca)\ndf.drop(['cp', 'thal', 'slope', 'restecg', 'ca'], axis=1, inplace=True)\ndf.head(2)","a09db17a":"df[['age', 'trestbps', 'chol', 'thalach']].describe()","ad443516":"scale = StandardScaler()\nse = pd.DataFrame(scale.fit_transform(df[['age', 'trestbps', 'chol', 'thalach']]))","6eda255f":"df = pd.concat([df, se], axis=1)\ny = df.target \ndf.drop(['target', 'age', 'trestbps', 'chol', 'thalach'], axis=1, inplace=True)\ndf.rename(columns={0: 'target', 1: 'age', 2:'trestbps', 3:'chol', 4:'thalach'}, inplace=True)\ndf.head(2)","408249e2":"pca = PCA(n_components=10)\ndf_pca = pca.fit_transform(df)","2de9536d":"total = 0\nfor i, component in enumerate(pca.components_):\n    print(\"{} component: {}% of initial variance\".format(i + 1, \n          round(100 * pca.explained_variance_ratio_[i], 2)))\n    total += pca.explained_variance_ratio_[i]\nprint()\nprint('Total % of initial varianve {}'.format(total))","56cfad47":"# Check the RandomForestClassifier\ncross_val_score(RandomForestClassifier(n_estimators=50, random_state=11), df, y, scoring='roc_auc').mean()","f73150d9":"# Check the LogisticRegression\ncross_val_score(LogisticRegression(random_state=11), df, y, scoring='roc_auc').mean()","4bdca1c4":"cross_val_score(RandomForestClassifier(n_estimators=50, random_state=11), df_pca, y, scoring='roc_auc').mean()","ebafb165":"cross_val_score(LogisticRegression(random_state=11), df_pca, y, scoring='roc_auc').mean()","b5f3c3b3":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectKBest, f_classif","b3200b0a":"df_variance = VarianceThreshold(0.75).fit_transform(df)\ndisplay('from {} features we have {}'.format(df.shape[1], df_variance.shape[1]))","9d6e0702":"df_best_5 = SelectKBest(f_classif, k=5).fit_transform(df, y)","cec94b9d":"df_pca_variance = VarianceThreshold(.075).fit_transform(df_pca)","f2c6c5dd":"cv_df = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=11), df, y, scoring='roc_auc').mean()\ncv_var_5 = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=11), df_best_5, y, scoring='roc_auc').mean()\ncv_var_75 = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=11), df_variance, y, scoring='roc_auc').mean()\ncv_pca = cross_val_score(RandomForestClassifier(n_estimators=50, random_state=11), df_pca_variance, y, scoring='roc_auc').mean()","c51bd125":"print('------------------------')\ndisplay('Initial CV score {}'.format(cv_df))\ndisplay('CV score with 5 best features with SelectKBest sklearn class {}'.format(cv_var_5))\ndisplay('CV score with Threshold (75%) using VarianceThreshold sklearn class {}'.format(cv_var_75))\ndisplay('CV score on PCA tranformed data with Threshold (75%) using VarianceThreshold sklearn class {}'.format(cv_pca))\nprint('------------------------')","b3bc8086":"#http:\/\/rasbt.github.io\/mlxtend\/\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(df, y, train_size=0.2)\ndisplay('total {} features now'.format(X_train.shape[1]))","87ce5407":"selector = SequentialFeatureSelector(RandomForestClassifier(random_state=11), scoring='roc_auc', \n                                     verbose=0,  k_features=10, forward=True, n_jobs=-1)\nselector.fit(X_train, y_train) # train selector","4ecb3e8a":"display('features idx {}, features name {}, max score {}'.format(selector.k_feature_idx_, selector.k_feature_names_, selector.k_score_))","e8bffcf5":"df_less_features = df[list(selector.k_feature_names_)]\ndf_less_features.shape","83722947":"random_forest = RandomForestClassifier(n_estimators=50, max_features=2, random_state=11, verbose=False).fit(X_train, y_train)\nlogreg = LogisticRegression(random_state=11, verbose=False).fit(X_train, y_train)","d14fcbc9":"def plot_features_importance(model, X_train, X_test, y_test):\n    ddd = pd.DataFrame(model.feature_importances_, X_train.columns, columns=[\"feature\"])\n    ddd = ddd.sort_values(by='feature', ascending=False).reset_index()\n    plt.figure(figsize=[5,5])\n    sns.barplot(x='index', y='feature', data=ddd[:15], palette=\"Blues_d\")\n    plt.title('Feature inportance of Random Forest')\n    plt.xticks(rotation=60)\n    plt.show();\n    display('ROC_AUC is {}'.format(roc_auc_score(y_test, random_forest.predict_proba(X_test)[:,1])))\n    \n# run function\nplot_features_importance(random_forest, X_train, X_test, y_test)","2465e1e9":"# Plot \nplt.figure(figsize=(6,6))\nplt.bar(X_train.columns, logreg.coef_[0])\nplt.title('Feature inportance of LogisticRegression')\nplt.xticks(rotation=60)\nplt.show();","d5f4d5b9":"def graph(model, X_train, y_train):\n    obb = []\n    est = list(range(5, 200, 5))\n    for i in tqdm(est):\n        random_forest = model(n_estimators=i, random_state=11,max_features=2,\n                                           verbose=0, oob_score=True, n_jobs=-1)\n        random_forest.fit(X_train, y_train)\n        obb.append(random_forest.oob_score_)\n        \n    display('max oob {} and number of estimators {}'.format(max(obb), est[np.argmax(obb)]))\n    plt.plot(est, obb)\n    plt.title('model')\n    plt.xlabel('number of estimators')\n    plt.ylabel('oob score')\n    plt.show();\n    \ngraph(RandomForestClassifier, X_train, y_train)","e7ca127e":"dt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\nroc_auc_score(y_test, dt.predict_proba(X_test)[:,1])","897dbcfe":"f_dt, t_dt, tt_dt = roc_curve(y_test, dt.predict_proba(X_test)[:,1])\nf_rf, t_rf, tt_rf = roc_curve(y_test, random_forest.predict_proba(X_test)[:,1])","5280766f":"plt.plot(f_dt, t_dt, c='r', label='Decision Tree')\nplt.plot(f_rf, t_rf, label='Random Forest')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Roc curve of Decision Tree and Random Forest')\nplt.show();","8e512c01":"X_t, Xt, y_t, yt = train_test_split(df[['thal_0', 'thal_1', 'thal_2', 'thal_3', 'sex', 'cp_0', 'cp_1', 'cp_2', 'cp_3']], y)","3b544838":"adaboost = AdaBoostClassifier(n_estimators=100, random_state=11)\nadaboost.fit(X_t, y_t)\n\nroc_auc_score(yt, adaboost.predict_proba(Xt)[:,1])","124be895":"# Parameters \nparam_grid = [{'n_estimators' : list(range(25, 150, 25))\n              }]\ncv = StratifiedKFold(random_state=11)","a64c5a8e":"adaboost_1 = AdaBoostClassifier(random_state=11)\ngrid = GridSearchCV(adaboost_1, param_grid=param_grid, scoring='roc_auc', cv=cv)","9d23f1ec":"grid.fit(X_t, y_t)","7336393a":"grid.best_estimator_, grid.best_params_, grid.best_score_","0aedbc15":"cv = cross_val_score(grid.best_estimator_, X_t, y_t, scoring='roc_auc', cv=5)\ndisplay('CV score is {}'.format(np.mean(cv)))","606a7ed9":"# save features\nfeatures = list(X_t.columns)\n# plot \nplot_partial_dependence(grid.best_estimator_, X_t, features,\n                        n_jobs=3, grid_resolution=20)\nfig = plt.gcf()\nfig.suptitle('partial dependence of features ')\nfig.subplots_adjust(hspace=0.2)\nplt.show();","cbc201ef":"'''To get same results you need run it two times. \nFirst time with df X_train, X_test, y_train, y_test = train_test_split(df, y) \nget the results df dataset with 27 features.\n\nSecond time \nX_train, X_test, y_train, y_test = train_test_split(df_less_features, y)\nget result for df_less_features with 10 features'''\n\nX_train, X_test, y_train, y_test = train_test_split(df, y)\ndf_less_features.shape, df.shape","cef8c656":"n_jobs = -1\nrandom_state = 11 \n\n### Pipeline\n\npipe_rf = Pipeline([('rf', RandomForestClassifier(random_state=random_state, n_jobs=n_jobs))])\n\npipe_ada = Pipeline([('ada', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), random_state=random_state))])\n\npipe_gbc = Pipeline([('gbc', GradientBoostingClassifier(random_state=random_state))])\n\npipe_ext = Pipeline([('ext', ExtraTreesClassifier(random_state=random_state, n_jobs=n_jobs))])\n\npipe_cat = Pipeline([('cat', CatBoostClassifier(random_state=random_state, verbose=False))])\n\npipe_bag = Pipeline([('bag', BaggingClassifier(n_jobs=n_jobs, random_state=random_state))])\n\npipe_svc = Pipeline([('svc', SVC(random_state=random_state, probability=True))])\n\npipe_xgb = Pipeline([('xgb', xgboost.XGBClassifier(n_jobs=n_jobs, random_state=random_state))])\n\n### params\n\ncv = StratifiedKFold(shuffle=False, n_splits=5, random_state=11)\n\ngrid_params_rf = {\n                'rf__criterion': ['entropy', 'gini'],\n                'rf__min_samples_leaf': [10, 15, 20],\n                'rf__max_depth': [10, 15, 20],\n                'rf__n_estimators' : range(40, 120, 20)\n                }\n\n\ngrid_params_ada = {\n                'ada__learning_rate' : [0.5, 0.8, 1],\n                'ada__n_estimators': [25, 50]\n                }\n\ngrid_params_gbc = {\n                'gbc__learning_rate' : list(np.linspace(0.01, 0.5, 5))\n                }\n\ngrid_params_ext = {\n                'ext__min_samples_leaf': [15, 18, 20],\n                'ext__max_depth': [10, 15, 16],\n                'ext__n_estimators' : [50, 100, 150]\n                }\n\ngrid_params_cat = {\n                'cat__learning_rate' : [0.003, 0.004],\n                'cat__iterations': [1000]\n    \n                }\n\ngrid_params_bag = {'bag__base_estimator': [KNeighborsClassifier()],\n                'bag__n_estimators': [50, 70, 80] #list(range(10, 80, 10))\n                    }\n\ngrid_params_svc = {\n                    'svc__C': [1, 3, 4]\n                    }\n\ngrid_params_xgb = {\n                'xgb__booster': ['gbtree', 'gblinear'],\n                'xgb__max_depth': list(range(3,15,3)),\n                'xgb__learning_rate': list(np.linspace(0.1, 0.9, 3)),\n                'xgb__n_estimators': list(range(100, 300, 100)),\n                'xgb__min_child_weight': list(range(1,6,2)),\n                'xgb__eta': [0.2, 0.3]\n                }\n\n### RandomizedSearchCV\n\ngs_rf = RandomizedSearchCV(pipe_rf, param_distributions=grid_params_rf,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_ada = RandomizedSearchCV(pipe_ada, param_distributions=grid_params_ada,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_gbc = RandomizedSearchCV(pipe_gbc, param_distributions=grid_params_gbc,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_ext = RandomizedSearchCV(pipe_ext, param_distributions=grid_params_ext,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_cat = RandomizedSearchCV(pipe_cat, param_distributions=grid_params_cat,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_bag = RandomizedSearchCV(pipe_bag, param_distributions=grid_params_bag,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_svc = RandomizedSearchCV(pipe_svc, param_distributions=grid_params_svc,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\ngs_xgb = RandomizedSearchCV(pipe_xgb, param_distributions=grid_params_xgb,\n                     scoring='roc_auc', cv=cv, n_iter=15)\n\n### Dict of models\nlook_for = [gs_rf, gs_ada, gs_gbc, gs_ext, gs_cat, gs_bag, gs_svc, gs_xgb]\n\nmodel_dict = {0:'RandomForest', 1: 'Adaboost', 2: \"Gradient boost\", 3:'ExtraTreeBoost', 4: 'Catboost', 5:'Bagging', \\\n             6:'SupportVEctorClass', 7:'Xgb'}","d8078c3a":"''' Function to iterate over models and obtain results'''\n# set empty dicts and list\nresult_acc = {}\nresult_auc = {}\nmse = {}\nmodels = []\n# iterate over models \nfor index, model in enumerate(look_for):\n        start = time.time()\n        print()\n        print('+++++++ Start New Model ++++++++++++++++++++++')\n        print('Estimator is {}'.format(model_dict[index]))\n        model.fit(X_train, y_train)\n        print('---------------------------------------------')\n        print('best params {}'.format(model.best_params_))\n        print('best score is {}'.format(model.best_score_))\n        auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n        print('---------------------------------------------')\n        print('ROC_AUC is {} and accuracy rate is {}'.format(auc, model.score(X_test, y_test)))\n        end = time.time()\n        print('It lasted for {} sec'.format(round(end - start, 3)))\n        print('++++++++ End Model +++++++++++++++++++++++++++')\n        print()\n        print()\n        models.append(model.best_estimator_)\n        result_acc[index] = model.best_score_\n        result_auc[index] = auc\n        mse[index] = mean_squared_error(y_test, model.predict(X_test))     ","8f57a934":"# func to show results\n\ndef model_performance(df, result_acc, result_auc, mse, model_dict):\n    result = pd.DataFrame([result_acc, result_auc, mse])\n    result.columns = list(model_dict.values())\n    result.index = ['accouracy', 'roc_auc', 'MSE']\n    display(result.head(), 'shape of data is {}'.format(df.shape))\n    plt.figure(figsize=(8,8))\n    plt.plot(result.columns.values, result.iloc[0,:].values, label=['accouracy'])\n    plt.plot(result.columns.values, result.iloc[1,:].values, label=['roc_auc'])\n    plt.plot(result.columns.values, result.iloc[2,:].values, label=['mse'])\n\n    plt.xticks(rotation=60)\n    plt.title('Compare performance')\n    plt.legend()\n    plt.show()\n\ndef plot_ExtraTreesClassifier():\n    extra_tree = ExtraTreesClassifier(n_estimators=100, min_samples_leaf=15, max_depth=15).fit(df, y)\n\n    temp_df = pd.DataFrame(df.columns, extra_tree.feature_importances_, \\\n                           columns=['feature']).reset_index().sort_values(by='index', ascending=False)\n    temp_df.columns = ['Feature_importance', 'Feature']\n\n    plt.bar(temp_df[:7].Feature, temp_df[:7].Feature_importance)\n    plt.xticks(rotation=60)\n    plt.title('Feature_importance')\n    plt.show();","64c60ae5":"model_performance(df, result_acc, result_auc, mse, model_dict)","6961ddaf":"model_performance(df_less_features, result_acc, result_auc, mse, model_dict)","ec3e6f96":"plot_ExtraTreesClassifier()","99fb0610":"### 5 Use a random forest and LogisticRegression for Feature Importance\n\nWe would like to see first look on feature importance rank","eb8fd318":"#### Check data types and missing values","8cac67cf":"### Features correlation","3362d382":"Here is results. We can use just 9 features (out of 14) to get value of roc_auc - 0.93","e6963f1e":"### 6. Let us play with more advanced model - `AdaBoost`. We consider the variables `Thal`, `Sex`, `ChestPain` and Chol`\n\nWe want to compare how such algoritm perform with limited data comparing to more simple one with complite dataset\n","91215071":"[](http:\/\/)Wow! Just with 9 features (selected rather randomly) we have good score. ","b1939a5b":"#### Check features that are not binary and nor normaly distrubuted","d6c06abe":"![](https:\/\/raw.githubusercontent.com\/VolodymyrGavrysh\/My_RoadMap_Data_Science\/master\/pictures\/Screenshot%20from%202020-01-17%2022-06-37.png)","a48dcb4d":"The initial value of 1 is due to the small amount of data. \n\nAnd we can say that PSA and a feature reduction by the threshold of 0.75 does not increase or even worsens the results.\n\nSelect seems to do better. But a small amount of data allows us to use it fully.\n\n**So we are going to work with a full set of features.**\n\nAnd final test of our hypothesis we use one more solution.","fb7728b8":"Prepare data for modeling","84cfe501":"Partial dependence plots show the dependence between the target function 2 and a set of \u2018target\u2019 features, marginalizing over the values of all other features (the complement features). \nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py","75be23d2":"Scale these numeric values","e7f1f9a0":"### Compare decision tree and the random forest by checking performances `ROC` metrics","4cbc4fba":"Here we see two strong dependence (a linear relationship). People get ill with thal_2 feature (top left). cp_0 (no chest pain) says that they are not ill at all.\n\nthal_0', 'thal_1' - week dependences but with small positive rate. all others are not dependent.","e7e66b6e":"We can see that some data can be separated quite well (different colors for the target variable on the chart).\nHere you can also see what distribution the correlating variables have - we will be interested in those that have different (not similar) boundaries.","bee26a98":"## 2. Short Data visualization.","7d8238a2":"### Results WITHOUT feature reduction. Here we use 27 features.","93df5b59":"## Evaluation method","895272a4":"### 5.Feature engineering","b284997b":"## 8. Conclusion","d3cd8c94":"### Use Feature selection tool to check our hypothesis","1fe1cf23":"-----------------------------------","23ed52fd":"**Motivation**\n\nAfter struggling with the Data Science theory one need to practice it with real task and master his ability to apply variety of different tools and libraries\n\nI have choisen well studed dataset of heart disease for two reason:\n- small amount numerical data make it easy to condact experiments faster . \n- understandable features makes it easy to interpreted the results.\n- it is possible to compare your results with well studied data","f473dcfd":"There is only numerical features. No NaN. No missing values. Nice! )","0579dee3":"#### Can we do better? Let us use GridSearch\n","2077b2dc":"### 1. Load dataset:","93a6ea67":"RandomForestClassifier give us a bit worst result. LogisticRegression small improvement.\n\nSince there is not much data, it is likely that the transformation of the PCA will negatively affect the final results.\n\nLet's look at other ways to select (or delete) features. Especially collinear ones.","19d9f69f":"Start selector RandomForestClassifier. You can change verbose to 2, just to see how roc_auc changes","bc3cd759":"**See the results**","c92827d9":"Once again - 303 observation is not enought to use such approach to reduce noise (via feature reduction or transformation).\n\nSo I propose to **use whole data set.** Even feature engineering in such a case will affect the accuracy of classification on new data.\n\nBut for accurate reliability and applying a scientific approach to experiments - we will save the date set with a reduced number of features (10 out of 14)","19c7f658":"Check the numeric values ","9013678f":"Let chectk how PCA will change the performance","2bb03f0d":"### Results WITH feature reduction. Here we use 10 features. See above how df_less_features was generated.","982445c0":"### General overview:\n\n### 1 Here are most important factors that influence on heart disease in any age or affect any gender\n\n* Chest Pain\n* Fixed defect or heart without defect\n* Number of vessels\n* Exercise induced angina\n* Slopes of the peak exercise ST segment (more about https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/0002870386902656)\n","bc0e4ae7":"#### I used OOB score (out-of-bag) as more stable statistic for trees that better track Trees estimator performance. \n\nWe see that **80 estimators** is enought for best score.\n\nMore about OOB https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_ensemble_oob.html","0253b8b2":"Such studies can help with early diagnosis of diseases, helping to reduce the risk of complications.\n\nAlso modeling can be used by other specialists in the medical field without the high cost of IT infrastructure with acceptable accuracy and speed.\n\nNaturally, this modeling can be used by a wider circle: patients, insurance companies, researchers etc.","816a64fa":"#### Plot graph of features importance ","46053342":"#### What cross val score WITH PCA (10 componets)?","a4a35e90":"### Check the variance of features without PCA - with sklearn tools","c59446f0":"Let use Threshold (75%) to reduce number of feature variance","6cc35058":"### Plot distribution of correlated features","ab6cd7a5":"Almost all models showed quite good results.\n\n* As expected, models built on ensembles were able to accurately determine non-linear relationships.\n\n* Processing speed fast enough to use personal computers.\n\n* Reducing the variables from 27 to 10 did not significantly affect the result, which may reduce the cost (time) of diagnosing patients.\n\n* conducted EDA (see p 1) allow clearly articulate a risk group by age, gender and other characteristics","12c47a06":"## 3. Data cleaning, transforning","62101ce0":"> ### Let calculate the partial effects for the previous subset of variables (thal_0', 'thal_1', 'thal_2', 'thal_3', 'sex', 'cp_0', 'cp_1', 'cp_2', 'cp_3)","4c883189":"**The results are bit better!**\n\nBut complex algorithms on small data can behave unstably.\n","b8d72a2c":"In the task we will investigate **Heart Disease Data Set** applying different ensemble learning technics.\n\nThis dataset provides information on the risk factors for\nheart disease. The [original](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease) database contains 76 attributes, but all published experiments refer to using a subset of 14 of thema and is refferenced as Cleveland dataset ([link](https:\/\/raw.githubusercontent.com\/devrepublik\/data-science-course\/master\/data\/boosting\/heart.csv)).\n\nExperiments with the Cleveland database have concentrated on attempting to distinguish presence (value 1) or absence (value 0) of heart disease in the patient.\n\nThis is a typical **binary classification task.**","2d2599a8":"-------------------------------------","b363f295":"1. Short Data visualization. This task details the visualization aspect of the data analysis.\n2. Data cleaning. This task relates to the data cleaning aspect of the analysis.\n3. Insights generation. Analysis might leads to insights generation to generate unique and visually understandable insights.\n4. Feature engineering. This task relates to the feature engineering aspect of the analysis.\n5. Start analysis with Random Forest model. Compare than two simple model - decision tree and random forest.\n6. Continue with AdaBoost model with selected features. Use Cross validation and explore partial effects of features on model.\n7. Compare models results during Grid Search process. Find best performed model with best model parameters.\n8. Conclusion.","304abfa3":"### 4 How to use results and models?","064b7dc7":"Thank you for your time! I will be glad to hear your comments or questions.\nGavrysh Volodymyr https:\/\/www.linkedin.com\/in\/volodymyrgavrish\/\nemail: vladimir101175@gmail.com\n\nAttribution 4.0 International (CC BY 4.0)","f74a1d8f":"### 3 Metrics and evaluation models.\n\nThe presented metrics evaluate different approaches of models in the process of data evaluation.\n\nThe choice of them, accordingly, *** will depend on the priorities of management ***, which set priorities.\n\nFor example, if we assume that erros in classification for a healthy person is less significant (Type I error) than erros in classification of a patient, then the appropriate metric would be ***Precision.***\n\nAbout Precision https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall#Precision\n\nHere is additionally about metrics https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic","92209326":"### Attribute Information:\n\n> 1. **Age**: Age\n> 2. **Sex**: Sex (1 = male; 0 = female)\n> 3. **ChestPain**: Chest pain (typical, asymptotic, nonanginal, nontypical)\n> 4. **RestBP**: Resting blood pressure\n> 5. **Chol**: Serum cholestoral in mg\/dl\n> 6. **Fbs**: Fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n> 7. **RestECG**: Resting electrocardiographic results\n> 8. **MaxHR**: Maximum heart rate achieved\n> 9. **ExAng**: Exercise induced angina (1 = yes; 0 = no)\n> 10. **Oldpeak**: ST depression induced by exercise relative to rest\n> 11. **Slope**: Slope of the peak exercise ST segment\n> 12. **Ca**: Number of major vessels colored by flourosopy (0 - 3)\n> 13. **Thal**: (3 = normal; 6 = fixed defect; 7 = reversable defect)\n> 14. **target**: AHD - Diagnosis of heart disease (1 = yes; 0 = no)","afd034b6":"\nMeasurement of any model ability to generalize data requires a clear metric.\n\nHere will use well-studied several metrics at the same time. In conclusion we may select the one that might be more important to some  observer.\n\nSince we are limited with data (almost always), a part of it will be used to train the model (80%), and the rest of the data (20%) will be used solely to evaluate with mentioned metrics. \n\nMore details here - \"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\" (https:\/\/arxiv.org\/abs\/1811.12808)","44ea6328":"**We select 10 PCA components to explane data variance**","6098bf6e":"#### Let see how many estinators random_forest needs for best performance","9542e965":"### Explore distribution of target variable (ill and not ill) with Age, Sex and Level of Pain features.","acbe5111":"#### What cross val score WITHOUT PCA (10 componets)?","3c986c75":"So, let transfrom cp, restecg, oldpeak, slope, ca, thal features","cd8c0ccd":"#### Highly correlated features with target variable can increase error on new (unseen data), so we need to reduce their influence.","1d9b5126":"## Strategy","fc7f4e23":"### 7. Calculate and compare models (linear, boosting and others) with and without feature reduction\n\nMetrics are:\n\n- Mean squared error\n- Prediction accuracy\n- ROC_AUC","1d0d7cf8":"## Solution\n","9a057164":"Here you can clearly see how the random forest more accurately describes (line is more flexible with bigger total square) the data, with more **than 0.90 ROC_AUC score**","13e6693d":"<center><img src=\"https:\/\/raw.githubusercontent.com\/VolodymyrGavrysh\/My_RoadMap_Data_Science\/master\/pictures\/5F828D8129CBAFFB.jpg\" width=\"400\"><\/center>\n<p>\n<center><font size=\"6\"><b>Heart Disease<\/b><\/font><\/center>\n\n<center><font size=\"4\"><b>Data set exploration<\/b><\/font><\/center>\n\n***\n","957e5d26":"We can use other SelectKBest with AVONA stat classifier and top 5 features","30e196bb":"### Age distribution ","bc4b668b":"Two peaks of the disease:\n41-46 years old and 51-54 years old\nMaybe we can use it. For example, we can create a categorical variable.","19e21b48":"The graph gives us several ides about of the ***distribution of healthy and sick people, depending on age, gender and level of pain.***\n\nWe see that the ill group is more normal distributed by gender and density.\n\nMedium pain is the most clear features, which, oddly enough, works for all ages.\n\nWe observe some less dense zones, but not many outlayres. And it is obvious that some groups contain very little data - for example, healthy people with severe pain.","6b18c7b5":"We can use LogisticRegression coefs to see 'influence' of features.\n\nRandom_forest and LogisticRegression see data differently. LogisticRegression is more about linear relations, while random_forest catch non-linear as well.\n\nFor example, cp_0 feature at random_forest model is 2th, while for logreg it is negative! At the same time ca_0 in both models are number 1. Check age as well. \n\nSuch insights help to understand which features has more linear or non-linear processes. (in certain point it is mixture of them)\n\nAlthough, for me all possible ways to use these differences are not yet available - I'm sure they can be useful for analyzing more complex data. \n\nThis can give confidence, for example, in the **choice of transformation methods at the date cleaning stage.**\n","f6d7ca67":"### 2 With additional data record (person profile data) it is possible to predict illness with more than 90% accuracy","d92ea5b5":"Check the optimal number of componets for PCA","bef01a51":"Let us get rid of multicollinearity with target variable with PCA"}}