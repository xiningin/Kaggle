{"cell_type":{"d053a6c2":"code","ba94fb48":"code","d3895f0a":"code","95f8046b":"code","8df23bd7":"code","4cfbf86e":"code","0f5101d3":"code","20286456":"code","cbd28c06":"code","c378cb08":"code","3c88b22e":"code","278efc82":"code","4042cc4c":"code","fa1a8f17":"code","6e3dcf5f":"code","c9f34adf":"code","1a21a805":"code","135788e2":"code","36965732":"code","db35e238":"code","5fb96a3b":"code","50f41100":"code","8e65dc87":"code","a5fecf25":"code","ee2178c4":"code","360fe82a":"code","8f60269b":"code","dae8a91d":"code","5aae593b":"code","082dedb1":"code","b144e363":"code","bb767906":"code","ed2140d7":"code","65c432ff":"code","fc119229":"code","017895ce":"code","2cf4eb1a":"code","a9938d66":"code","15fc4a98":"code","431311c0":"code","0c80ba68":"code","ebc7b76f":"code","b3714c11":"code","674c3071":"code","9b9354ed":"code","8883d8c8":"code","cee1fd25":"code","8910bcd0":"code","9d2adbcf":"code","e7733b48":"code","5fa67b53":"code","e2185f0a":"code","ad4e9c41":"code","0f727782":"code","eb561b5e":"code","f4b996e8":"code","c5c8c199":"code","b5c37769":"code","14162213":"code","90061872":"code","215dfc8c":"code","3d3a4b42":"code","a182dc73":"code","c6ed7e62":"code","1647329c":"code","f8d62232":"code","1da06200":"markdown","485b6d5a":"markdown","29741fd1":"markdown"},"source":{"d053a6c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba94fb48":"pd.set_option('display.max_columns', None)\nfrom sklearn.preprocessing import MinMaxScaler\n","d3895f0a":"train_data = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-data-science-finhack-3\/Data\/Data\/train_Data.csv\")\ntrain_bureau = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-data-science-finhack-3\/Data\/Data\/train_bureau.csv\")\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-data-science-finhack-3\/Data\/Data\/test_Data.csv\")\ntest_bureau = pd.read_csv(\"\/kaggle\/input\/analytics-vidhya-ltfs-data-science-finhack-3\/Data\/Data\/test_bureau.csv\")","95f8046b":"train_bureau = train_bureau.drop_duplicates()\ntest_bureau = test_bureau.drop_duplicates()","8df23bd7":"train_data['source'] = 'train'\ntest_data['source'] = 'test'\n\ntrain_bureau['source'] = 'train'\ntest_bureau['source'] = 'test'","4cfbf86e":"bureau = pd.concat([train_bureau, test_bureau])\ndata = pd.concat([train_data, test_data])","0f5101d3":"for col in bureau.columns:\n    if bureau[col].dtype=='object':\n        print(bureau[col].value_counts())\n        print('')\n        \n","20286456":"bureau['ACCT-TYPE'] = bureau['ACCT-TYPE'].replace(bureau['ACCT-TYPE'].value_counts()[bureau['ACCT-TYPE'].value_counts()\/bureau.shape[0]*100<3].index, 'Rest')\nbureau['ACCT-TYPE'].value_counts()","cbd28c06":"bureau['CONTRIBUTOR-TYPE'] = bureau['CONTRIBUTOR-TYPE'].replace(['HFC', 'MFI', 'FRB',\n       'SFB', 'ARC', 'OFI' ,'CCC'], 'OTHERS')\nbureau['CONTRIBUTOR-TYPE'].value_counts()","c378cb08":"bureau['ACCOUNT-STATUS'] = bureau['ACCOUNT-STATUS'].replace(['SUIT FILED (WILFUL DEFAULT)',\n       'Written Off', 'Suit Filed', 'Restructured', 'Settled',\n       'WILFUL DEFAULT', 'Cancelled', 'Sold\/Purchased'], 'Written Off')\nbureau['ACCOUNT-STATUS'].value_counts()","3c88b22e":"bureau.isnull().sum()[bureau.isnull().sum()>0]\/bureau.shape[0]*100","278efc82":"#drop columns with greater than 20% null values\nbureau.drop(bureau.isnull().sum()[bureau.isnull().sum()\/bureau.shape[0]*100>20].index, axis=1, inplace=True)","4042cc4c":"bureau.isnull().sum()[bureau.isnull().sum()>0]\/bureau.shape[0]*100","fa1a8f17":"#can drop hist columns for now\nbureau = bureau.drop(['REPORTED DATE - HIST', 'DPD - HIST', 'CUR BAL - HIST',\n       'AMT OVERDUE - HIST', 'AMT PAID - HIST'], axis=1)","6e3dcf5f":"bureau['DISBURSED-DT'] = pd.to_datetime(bureau['DISBURSED-DT'])\nbureau['DATE-REPORTED'] = pd.to_datetime(bureau['DATE-REPORTED'])\nbureau['DATE-REPORTED'].max(), bureau['DISBURSED-DT'].max()","c9f34adf":"bureau['DATE-REPORTED'] = bureau['DATE-REPORTED'].fillna(bureau['DATE-REPORTED'].max())\nbureau['DISBURSED-DT'] = bureau['DISBURSED-DT'].fillna(bureau['DATE-REPORTED'])","1a21a805":"bureau.drop('MATCH-TYPE', axis=1, inplace=True)","135788e2":"bureau['CURRENT-BAL'] = bureau['CURRENT-BAL'].apply(lambda x: str(x).replace(',', ''))\nbureau['DISBURSED-AMT\/HIGH CREDIT'] = bureau['DISBURSED-AMT\/HIGH CREDIT'].apply(lambda x: str(x).replace(',', ''))\nbureau['WRITE-OFF-AMT'] = bureau['WRITE-OFF-AMT'].apply(lambda x: str(x).replace(',', ''))","36965732":"#bureau['CURRENT-BAL'] = bureau['CURRENT-BAL'].fillna('0')\nbureau.isnull().sum()","db35e238":"for col in bureau.columns[-4:-1]:\n    print(col)\n    bureau[col] = bureau[col].replace('nan', 0)\n    bureau[col] = bureau[col].apply(lambda x: float(x))","5fb96a3b":"bureau.head()","50f41100":"bureau = pd.get_dummies(data=bureau, columns=['ACCT-TYPE', 'OWNERSHIP-IND', 'CONTRIBUTOR-TYPE', \n                                    'ACCOUNT-STATUS'])","8e65dc87":"bureau.columns","a5fecf25":"bureau_data = bureau.groupby(['ID', 'source']).sum().reset_index()\nbureau_data.to_csv('bureau_data.csv', index=False)","ee2178c4":"bureau_data.head()","360fe82a":"df = pd.read_csv('bureau_data.csv')\ndf.head()","8f60269b":"df.tail()","dae8a91d":"num_cols = ['DISBURSED-AMT\/HIGH CREDIT', 'CURRENT-BAL']\nscaler = StandardScaler()\n\n\nfor col in num_cols:\n    df[col] = np.log(1+df[col])\n    print(col)","5aae593b":"#target variable is \"Top-up Month\"","082dedb1":"# can drop unwanted columns\ndata = data.drop(['Area', 'BranchID', 'City', 'ZiPCODE'], axis=1)","b144e363":"data['DisbursalDate'] = pd.to_datetime(data['DisbursalDate'])\ndata['MaturityDAte'] = pd.to_datetime(data['MaturityDAte'])\ndata['AuthDate'] = pd.to_datetime(data['AuthDate'])","bb767906":"for col in data.columns:\n    if data[col].dtype=='object':\n        print(data[col].value_counts())\n        print('')","ed2140d7":"data['State'] = data['State'].replace(data['State'].value_counts()[data['State'].value_counts()\/data.shape[0]*100<3].index, 'Rest')\ndata['State'].value_counts()","65c432ff":"data['PaymentMode'] = data['PaymentMode'].replace(data['PaymentMode'].value_counts()[data['PaymentMode'].value_counts()\/data.shape[0]*100<3].index, 'Rest')\ndata['PaymentMode'].value_counts()","fc119229":"data.isnull().sum()","017895ce":"cat_cols = ['ManufacturerID', 'SupplierID', 'AssetID']\nfor col in cat_cols:\n    print('value counts')\n    print(data[col].value_counts())\n    print('number of unique values', data[col].nunique())\n    print()","2cf4eb1a":"data.drop(['SupplierID', 'AssetID'], axis=1, inplace=True)","a9938d66":"data['ManufacturerID'] = data['ManufacturerID'].replace(data['ManufacturerID'].value_counts()[data['ManufacturerID'].value_counts()\/data.shape[0]*100<3].index, 1111)\ndata['ManufacturerID'].value_counts()","15fc4a98":"data.head()","431311c0":"data['MonthlyIncome'] = data['MonthlyIncome'].fillna(0)\ndata['SEX'] = data['SEX'].fillna(data['SEX'].mode()[0])\ndata['AGE'] = data['AGE'].fillna(data['AGE'].mode()[0])\ndata['ManufacturerID'] = data['ManufacturerID'].fillna(data['ManufacturerID'].mode()[0])\ndata['MaturityDAte'] = data['MaturityDAte'].fillna(data['MaturityDAte'].mode()[0])","0c80ba68":"from sklearn.preprocessing import LabelEncoder, StandardScaler","ebc7b76f":"data.columns","b3714c11":"le = LabelEncoder()\ncat_cols = ['Frequency', 'InstlmentMode', 'LoanStatus', 'PaymentMode', 'SEX', 'State']\n\nfor col in cat_cols:\n    print(col)\n    data[col] = le.fit_transform(data[col])","674c3071":"num_cols = ['AssetCost', 'AmountFinance', 'DisbursalAmount', 'EMI', 'MonthlyIncome','Tenure']\nscaler = StandardScaler()\n\n\nfor col in num_cols:\n    data[col] = np.log(1+data[col])\n    print(col)","9b9354ed":"data.drop(['DisbursalDate', 'MaturityDAte', 'AuthDate'], axis=1, inplace=True)\ndata.head()","8883d8c8":"data['EMItoIncome'] = data['EMI']\/data['MonthlyIncome']\ndata['AssettoIncome'] = data['AssetCost']\/(data['MonthlyIncome']*12)\ndata['DifferenceAmount'] = data['AmountFinance'] - data['DisbursalAmount']\ndata['YearsOfService'] = 60 - data['AGE']","cee1fd25":"num_cols = ['AssetCost', 'AmountFinance', 'DisbursalAmount', 'EMI', 'MonthlyIncome']\nscaler = StandardScaler()\nfor col in num_cols:\n    data[col] = np.log(1+data[col])\n    print(col)","8910bcd0":"data.head()","9d2adbcf":"data.drop('AGE', 1, inplace=True)","e7733b48":"data.to_csv('cleaned_data.csv', index=False)","5fa67b53":"final = pd.merge(data, df, on=['ID', 'source'], how='inner')\nfinal.head()","e2185f0a":"dict ={' > 48 Months':6,'12-18 Months':1,'18-24 Months':2,'24-30 Months':3,'30-36 Months':4,'36-48 Months':5,'No Top-up Service':0}\n","ad4e9c41":"train_df = final[final['source']=='train']\ntest_df = final[final['source']=='test']","0f727782":"train_df.drop('source',1, inplace=True)\ntest_df.drop(['source', 'Top-up Month'],1, inplace=True)","eb561b5e":"test_df.head()","f4b996e8":"test_df.corr()\nimport seaborn as sns\nsns.heatmap(train_df.corr())","c5c8c199":"test_df.shape, test_data.shape","b5c37769":"test_df['ManufacturerID'] = le.fit_transform(test_df['ManufacturerID'])\ntrain_df['ManufacturerID'] = le.fit_transform(train_df['ManufacturerID'])\ntrain_df=train_df.round(3)\ntest_df=test_df.round(3)","14162213":"from sklearn.model_selection import train_test_split\n\ntrain = train_df.drop('ID', 1)\nprint(train_df['Top-up Month'].value_counts())\n\ntrain_df['Top-up Month'] = train_df['Top-up Month'].map(dict)\nX = train_df.drop(['ID', 'Top-up Month'], 1)\nprint(train_df['Top-up Month'].value_counts())\n\ny = train_df['Top-up Month']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\ntest = test_df.drop(['ID'], 1)\nX_train.shape, y_train.shape, test.shape","90061872":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression, SGDClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\n","215dfc8c":"n_folds = 5\n\ndef model_acc(model):\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_val)\n    accuracy=round(accuracy_score(y_pred,y_val)*100, 2)\n    return(accuracy)\n    ","3d3a4b42":"\nmodel_xgb = XGBClassifier()\n","a182dc73":"a=model_acc(model_xgb)\nimport lightgbm as lgb\nclf = lgb.LGBMClassifier()\nb=model_acc(clf)\n\n","c6ed7e62":"ensemble = float(a*0.60) + float(b*0.40)\nensemble","1647329c":"pred = clf.predict(test)\n","f8d62232":"dict1 ={6:' > 48 Months',1:'12-18 Months',2:'18-24 Months',3:'24-30 Months',4:'30-36 Months',5:'36-48 Months',0:'No Top-up Service'}\n\nsubm = pd.DataFrame({'ID': test_df['ID'], 'Top-up Month': pred})\nsubm['Top-up Month'] = subm['Top-up Month'].map(dict1)\nsubm.to_csv('ss2.csv', index=False)\n","1da06200":"Can drop MATCH-TYPE, since very few are of secondary type\nfix account status\nfix contributor type","485b6d5a":"# Working on Bureau Data","29741fd1":"# Working on demographic data"}}