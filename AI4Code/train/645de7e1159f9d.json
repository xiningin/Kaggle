{"cell_type":{"d7d51c81":"code","28114b1d":"code","b2b52933":"code","a965a712":"code","4401d9aa":"code","002a6b74":"code","ffb8bc2e":"code","26442e16":"code","e6f7aae4":"code","0fa7a3f8":"code","f048ab4a":"code","d77ec0d8":"code","637f7e84":"code","c958d594":"code","c4c95c50":"code","59a9098a":"code","721ee7ef":"markdown","d300656c":"markdown"},"source":{"d7d51c81":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import StandardScaler","28114b1d":"torch.manual_seed(1)\n\nif torch.cuda.is_available():\n    device = 'cuda'\n    torch.cuda.manual_seed_all(1)\nelse:\n    device = 'cpu'","b2b52933":"train = pd.read_csv('..\/input\/nbaprediction\/train.csv')\ntest = pd.read_csv('..\/input\/nbaprediction\/test.csv')","a965a712":"train.shape, test.shape","4401d9aa":"train.head()","002a6b74":"# Null \uac12 \ud655\uc778\ntrain.isnull().sum()","ffb8bc2e":"# \ud574\ub2f9 \ud589 \uc81c\uac70\n#train.dropna(inplace=True)\ntrain.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)","26442e16":"# \ud559\uc2b5\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc900\ube44\nX_train = train.drop(['ID', 'Win'], axis=1)\nX_test = test.drop('ID', axis=1)\ny_train = train['Win']\n\n\n# Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Tensor\ub85c \ubcc0\ud658\nX_train = torch.FloatTensor(X_train).to(device)\nX_test = torch.FloatTensor(X_test).to(device)\n\ny_train = torch.LongTensor(y_train.values).to(device)","e6f7aae4":"# Hyperparameter \uc124\uc815\nlearning_rate = 0.1\nn_epochs = 500\ndrop_prob = 0.3\n\n\n# Model \uc124\uc815\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.fc1 = nn.Linear(X_train.shape[1], 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n        self.fc4 = nn.Linear(256, 256)\n        self.fc5 = nn.Linear(256, 256)\n        self.fc6 = nn.Linear(256, 2)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight.data)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc3(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc4(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc5(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc6(out)\n        return out","0fa7a3f8":"model = Net().to(device)\n\n\n# optimizer\uc640 Loss Function \uc124\uc815\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\nloss_fn = nn.CrossEntropyLoss()\n\n\n# \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 Learning rate Scheduler \uc0ac\uc6a9\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)","f048ab4a":"## \ud559\uc2b5 ##\n\nfor epoch in range(1, n_epochs+1):\n    model.train()\n    H = model(X_train)\n    loss = loss_fn(H, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    accuracy = (torch.argmax(H, dim=1) == y_train).float().mean()\n    \n    scheduler.step()\n    \n    if epoch % 20 == 0:\n        print('Epoch {:4d} \/ {}, Loss : {:.4f}, Accuracy : {:.2f} %'.format(\n            epoch, n_epochs, loss.item(), accuracy*100))","d77ec0d8":"with torch.no_grad():\n    model.eval()\n    pred = model(X_test)","637f7e84":"submit = pd.read_csv('..\/input\/nbaprediction\/sample_submit.csv')","c958d594":"submit['Win'] = torch.argmax(pred, dim=1).cpu()","c4c95c50":"submit.head()","59a9098a":"submit.to_csv('submission.csv', index=False)","721ee7ef":"## Data Preparation","d300656c":"## Training"}}