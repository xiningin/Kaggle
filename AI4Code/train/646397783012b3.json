{"cell_type":{"25e78f6b":"code","921daef4":"code","b265616a":"code","8dcf4d1e":"code","0b99705a":"code","540b8fea":"code","6256f2be":"code","3f8534c4":"code","5c6a1bec":"code","309e0f62":"code","eac98499":"code","c153f127":"code","7a0605c1":"code","2c969e83":"code","ae330daa":"code","0abe0331":"code","6a205ce7":"code","5ab7cf33":"code","547da36b":"code","9f6ef858":"code","81a777f7":"code","5c65afa0":"code","4e201f1c":"code","1eac1ddb":"markdown","51c78d75":"markdown","9963c98a":"markdown","79b9a367":"markdown","8de9a56c":"markdown","5cb6c60f":"markdown"},"source":{"25e78f6b":"# Let's install Feature-engine\n# an open-source Python library for feature engineering\n# it allows us to select which features we want to transform\n# straight-away from within each transformer\n\n# https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html\n\n!pip install feature-engine","921daef4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# for the model\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    StandardScaler, \n    MinMaxScaler,\n    RobustScaler,\n    )\n\n# for feature engineering\nfrom feature_engine import imputation as mdi\nfrom feature_engine import encoding as ce\nfrom feature_engine import discretisation as disc\nfrom feature_engine import transformation as t","b265616a":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ndata.head()","8dcf4d1e":"# the aim of this notebook is to show how to select the best data\n# transformations\n\n# So I will take a shortcut and remove some features to make things simpler\n\ncols = [\n    'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin',\n    'Embarked', 'Survived'\n]\n\ndata = data[cols]\n\ndata.head()","0b99705a":"# Cabin: extract numerical and categorical part and delete original variable\n\ndata['cabin_num'] = data['Cabin'].str.extract('(\\d+)') # captures numerical part\ndata['cabin_num'] = data['cabin_num'].astype('float')\ndata['cabin_cat'] = data['Cabin'].str[0] # captures the first letter\n\ndata.drop(['Cabin'], axis=1, inplace=True)\n\ndata.head()","540b8fea":"# make list of variables types\n# we need these lists to tell Feature-engine which variables it should modify\n\n# numerical: discrete\ndiscrete = [\n    var for var in data.columns if data[var].dtype != 'O' and var != 'Survived'\n    and data[var].nunique() < 10\n]\n\n# numerical: continuous\ncontinuous = [\n    var for var in data.columns\n    if data[var].dtype != 'O' and var != 'Survived' and var not in discrete\n]\n\n# categorical\ncategorical = [var for var in data.columns if data[var].dtype == 'O']\n\nprint('There are {} discrete variables'.format(len(discrete)))\nprint('There are {} continuous variables'.format(len(continuous)))\nprint('There are {} categorical variables'.format(len(categorical)))","6256f2be":"# discrete variables\n\ndiscrete","3f8534c4":"# continuous variables\n\ncontinuous","5c6a1bec":"# categorical variables\n\ncategorical","309e0f62":"# separate into training and testing set\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop('Survived', axis=1),  # predictors\n    data['Survived'],  # target\n    test_size=0.1,  # percentage of obs in test set\n    random_state=0)  # seed to ensure reproducibility\n\nX_train.shape, X_test.shape","eac98499":"# Numerical imputation:\n#----------------------\n\n# Should I do mean, median imputation or imputation with an arbitrary value?\n\nmean_imputer = mdi.MeanMedianImputer(imputation_method = 'mean', variables=['Age', 'Fare', 'cabin_num'])\n\nmedian_imputer = mdi.MeanMedianImputer(imputation_method = 'median', variables=['Age', 'Fare', 'cabin_num'])\n\narbitrary_imputer = mdi.EndTailImputer(variables=['Age', 'Fare', 'cabin_num'])\n\nnum_imputer = [mean_imputer, median_imputer, arbitrary_imputer]","c153f127":"# Categorical encoding\n\n# Should I do one hot? ordinal imputation or mean encoding?\n\nonehot_enc = ce.OneHotEncoder(variables=categorical)\nordinal_enc = ce.OrdinalEncoder(encoding_method='ordered', variables=categorical)\nmean_enc = ce.MeanEncoder(variables=categorical)\n\ncat_encoder = [onehot_enc, ordinal_enc, mean_enc]","7a0605c1":"# Continuous variables\n\n# should I discretise them or transform them?\n\nefd = disc.EqualFrequencyDiscretiser(q=5, variables=continuous)\ndtd = disc.DecisionTreeDiscretiser(variables=continuous)\n\nyj = t.YeoJohnsonTransformer(variables=continuous)\n\ntransformers = [efd, dtd, yj]","2c969e83":"# finally, I want to scale the variables before passing them\n# to the logit:\n\nscalers = [StandardScaler(), MinMaxScaler(), RobustScaler()]","ae330daa":"# Now I set up the pipeline with some parameters.\n# We will modify the steps later during the random search\n\n\ntitanic_pipe = Pipeline([\n\n    # missing data imputation - numerical\n    ('imputer_num', mean_imputer),\n    \n    # missind data imputation - categorical\n    ('imputer_cat', mdi.CategoricalImputer(variables=['Embarked', 'cabin_cat'])),\n\n    # categorical encoding - we will group rare categories into 1\n    ('encoder_rare_label', ce.RareLabelEncoder(\n        tol=0.01,\n        n_categories=2,\n        variables=['Embarked', 'cabin_cat'],\n    )),\n    \n    # categorical encoding (into numbers)\n    ('categorical_encoder', onehot_enc),\n    \n    # continuous variable transformation\n    ('transformation', efd),\n    \n    # variable scaling\n    ('scalers', StandardScaler),\n\n    # Logistic regression\n    ('logit', LogisticRegression(random_state=0))\n])","0abe0331":"# not we enter into the param_grid, the different options\n# that we want to test\n\nparam_grid = {\n    \n    # test different numerical variable imputation\n    'imputer_num': num_imputer,\n    \n    # test imputation with frequent category or string missing\n    # we modify the paramater of the feature-engine transformer directly\n    'imputer_cat__imputation_method': ['missing','frequent'],\n    \n    # test different thresholds to group rare labels\n    # we modify the paramater of the feature-engine transformer directly\n    'encoder_rare_label__tol': stats.uniform(0.1, 0.2),\n    \n    # test different encoding strategies\n    'categorical_encoder': cat_encoder,\n    \n    # test different variable transformation strategies\n    'transformation': transformers,\n    \n    # test different scalers\n    'scalers': scalers,\n    \n    # try different logistic regression hyperparamenters\n    'logit__C': stats.uniform(0, 1),\n}\n\n# (note how we call the step name in the pipeline followed by __\n# followed by the name of the hyperparameter that we want to modify\n# when we want to access directly the parameters inside the pipeline step)","6a205ce7":"# now we set up the randomized search with cross-validation\n\nsearch = RandomizedSearchCV(\n    titanic_pipe, # the pipeline\n    param_grid, # the hyperparameter space\n    cv=3, # the cross-validation\n    scoring='roc_auc', # the metric to optimize\n    n_iter = 20, # the number of combinations to sample, \n)\n\n# for more details in the randomized search visit:\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html","5ab7cf33":"# and now we train over all the possible combinations of the parameters\n# specified above\n\nsearch.fit(X_train, y_train)\n\n# and we print the best score over the train set\nprint((\"best roc-auc from search: %.3f\"\n       % search.score(X_train, y_train)))","547da36b":"# and finally let's check the performance over the test set\n\nprint((\"best linear regression from grid search: %.3f\"\n       %search.score(X_test, y_test)))","9f6ef858":"# we can find the best pipeline with its parameters like this\n\nsearch.best_estimator_","81a777f7":"# and find the best fit parameters like this\n\nsearch.best_params_","5c65afa0":"# we also find the data for all models evaluated\n\nresults = pd.DataFrame(search.cv_results_)\n\nprint(results.shape)\n\nresults.head()","4e201f1c":"# we can order the different models based on their performance\n\nresults.sort_values(by='mean_test_score', ascending=False, inplace=True)\n\nresults.reset_index(drop=True, inplace=True)\n\n\n# plot model performance and the generalization error\n\nresults['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n\nplt.ylabel('Mean test score - ROC-AUC')\nplt.xlabel('Hyperparameter combinations')","1eac1ddb":"### Set up the pipeline\n\nI want to assemble a pipeline that contains the following steps:\n\n- impute numerical variables\n- impute categorical variables\n- encode categorical variables\n- either discretise or transform continuous variables\n- scale all variables\n- train a logistic regression\n\nBut I am unsure of the way to select the best imputation methods, the best encoding method, or if I should transform or discretise the continuous variables. \n\nLet's take it one step at the time.","51c78d75":"## Load the data","9963c98a":"## Finding the best data transformation with Randomized Search\n\nIn a [previous notebook](https:\/\/www.kaggle.com\/solegalli\/feature-engineering-pipeline-and-hyperparam-tuning), I made a grid search to optimize the hyperparameters of various feature engineering transformers and a gradient boosting classifier.\n\nWhat if I am not sure which transformer to use to begin with? Can I also make a search to find the best transformation?\n\nYes, we can!\n\nIn this notebook, I will:\n\n- assemble a feature engineering pipeline\n- automatically find out the best data transformation\n- train a Logistic Regression \n\nUsing Randomized search.\n\nWe will:\n\n- set up a series of feature engineering steps using [Feature-engine](https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html)\n- train a Logistic Regression\n- train the pipeline with cross-validation, looking over different feature-engineering transformation and model hyperparameters\n\nFor more details on feature engineering and hyperparameter optimization feel free to check my [online courses](https:\/\/www.trainindata.com\/).","79b9a367":"If you liked this notebook and would like to know more about feature engineering and hyperparameter optimization feel free to check my [online courses](https:\/\/www.trainindata.com\/).","8de9a56c":"## Random Search with Cross-validation\n\nFor hyperparameter search we need:\n\n- the machine learning pipeline that we want to optimize (in the previous cell)\n- the hyperparamter space that we want to sample (next cell)\n- a metric to optimize\n- an algorithm to sample the hyperparameters (Random Search in this case)\n\nLet's do that.","5cb6c60f":"In the previous cell we can see which were the data transformations that worked best."}}