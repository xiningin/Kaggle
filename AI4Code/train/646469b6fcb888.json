{"cell_type":{"f9ef3fc2":"code","22a7c096":"code","a147ff0d":"code","6d6f19d1":"code","8da89de7":"code","1be5326b":"code","fd315a37":"code","ada7bab3":"code","e21265d9":"code","742e9fab":"code","ea4dfe61":"code","18210fd1":"code","fc9c0b20":"code","ef0ee658":"code","fe8343e8":"code","4bc12361":"code","417a3790":"code","6561e47e":"code","3684b5cd":"code","9393f356":"markdown","fda54193":"markdown","36c10a4e":"markdown"},"source":{"f9ef3fc2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Dense, Embedding, LSTM, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","22a7c096":"#Hyperparameters\nvocab_size = 30000\nembedding_size = 200\nhidden_units_size = 128\ndropout_lstm = 0.6\ndropout_regular = 0.6\nepochs = 50\nbatch_size = 32","a147ff0d":"#Load data\ntrain_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","6d6f19d1":"train_data.head(2)","8da89de7":"test_data.head(2)","1be5326b":"#Prepare variables\nx = np.array(train_data[\"excerpt\"])\ny = np.array(train_data[\"target\"])\nx_test = np.array(test_data[\"excerpt\"])","fd315a37":"#Create tokenizer for splitting words and number labeling\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(x)","ada7bab3":"#Tokenize\nx = tokenizer.texts_to_sequences(x)","e21265d9":"# First 15 words of second example\nx[1][0:15]","742e9fab":"#Get longest text\nmaximum_length = np.max([len(example) for example in x])\nmaximum_length","ea4dfe61":"#Pad text so each has same size\nx = pad_sequences(x, padding='post', maxlen=maximum_length)","18210fd1":"#Prepare test set\nx_test = tokenizer.texts_to_sequences(x_test)\nx_test = pad_sequences(x_test, padding='post', maxlen=maximum_length)","fc9c0b20":"#Create bidirectional lstm with 1 output layer\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=maximum_length, trainable=True))\nmodel.add(Dropout(dropout_regular))\nmodel.add(Bidirectional(LSTM(hidden_units_size, recurrent_dropout = dropout_lstm, dropout = dropout_regular,  return_sequences=True)))\nmodel.add(Bidirectional(LSTM(hidden_units_size, recurrent_dropout = dropout_lstm, dropout = dropout_regular)))\nmodel.add(Dense(1, activation='linear'))","ef0ee658":"#Create callbacks\ncheckpoint = ModelCheckpoint(\"\", monitor=\"val_loss\", verbose=1, save_best_only=True)\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience = 12)\nreduce_lr = ReduceLROnPlateau(patience=5)","fe8343e8":"model.compile(optimizer=\"Adam\", loss=\"mean_squared_error\", metrics=[\"MeanSquaredError\"])\nhistory = model.fit(x, y, batch_size=batch_size, validation_split=0.2, epochs=epochs, verbose=1,  callbacks = [early_stop, checkpoint, reduce_lr], use_multiprocessing = True)","4bc12361":"output = model.predict(x_test)","417a3790":"output_table = pd.DataFrame(test_data['id'])\noutput_table['target'] = output","6561e47e":"output_table.to_csv(\"submission.csv\", index=False)","3684b5cd":"output_table","9393f356":"# Create model and train","fda54193":"# Data loading and preprocessing","36c10a4e":"# Libraries"}}