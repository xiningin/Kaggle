{"cell_type":{"147580d3":"code","b8754c22":"code","f8681c4e":"code","9adcdf1b":"code","8cd6c4d5":"code","8eeebaac":"code","e222aa0f":"code","d2dda24d":"code","70a3b31f":"code","adb65d4f":"code","0d7f3e81":"code","7f5ddb2a":"code","e3a4e8f0":"markdown","52e8f301":"markdown","cc3b41ce":"markdown","2d7e0ba7":"markdown","60ba7074":"markdown","3730a8b8":"markdown","47792c47":"markdown","1f532967":"markdown","3db49e72":"markdown"},"source":{"147580d3":"import os, cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python import keras\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten,  AveragePooling2D\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\nfrom keras.layers import BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import img_to_array, array_to_img, load_img\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam, SGD\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom PIL import Image","b8754c22":"fpath = \"..\/input\/stanford-dogs-dataset\/images\/Images\/\"\nrandom_seed = 42\n\ncategories = os.listdir(fpath)\ncategories = categories[:20]\nprint(\"List of categories = \",categories,\"\\n\\nNo. of categories = \", len(categories)) ","f8681c4e":"def load_images_and_labels(categories):\n    img_lst=[]\n    labels=[]\n    for index, category in enumerate(categories):\n        for image_name in os.listdir(fpath+\"\/\"+category):\n            img = cv2.imread(fpath+\"\/\"+category+\"\/\"+image_name)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            img_array = Image.fromarray(img, 'RGB')\n            \n            #resize image to 227 x 227 because the input image resolution for AlexNet is 227 x 227\n            resized_img = img_array.resize((227, 227))\n            \n            img_lst.append(np.array(resized_img))\n            \n            labels.append(index)\n    return img_lst, labels\n\nimages, labels = load_images_and_labels(categories)\nprint(\"No. of images loaded = \",len(images),\"\\nNo. of labels loaded = \",len(labels))\nprint(type(images),type(labels))\nimages = np.array(images)\nlabels = np.array(labels)","9adcdf1b":"images = np.array(images)\nlabels = np.array(labels)\n\nprint(\"Images shape = \",images.shape,\"\\nLabels shape = \",labels.shape)\nprint(type(images),type(labels))","8cd6c4d5":"def display_rand_images(images, labels):\n    plt.figure(1 , figsize = (19 , 10))\n    n = 0 \n    for i in range(9):\n        n += 1 \n        r = np.random.randint(0 , images.shape[0] , 1)\n        \n        plt.subplot(3 , 3 , n)\n        plt.subplots_adjust(hspace = 0.3 , wspace = 0.3)\n        plt.imshow(images[r[0]])\n        \n        plt.title('Dog breed : {}'.format(labels[r[0]]))\n        plt.xticks([])\n        plt.yticks([])\n        \n    plt.show()\n    \ndisplay_rand_images(images, labels)\n","8eeebaac":"images, labels = shuffle(images, labels )\nimages.resize((images.shape[0],224,224,3))\n#images = images.reshape((images.shape[0],1))\n#types_2class = np.zeros((images.shape[0],2))\n\nfrom keras.utils import to_categorical\n#label_output_matrix = to_categorical(labels, num_classes = 20, dtype =\"int32\")\n\n# Split into training set and testing set\nX_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.4,\n                                                random_state=48)\nCat_test_y = np_utils.to_categorical(y_test)\ny_train=np_utils.to_categorical(y_train)\nprint(\"x_train shape = \",X_train.shape)\nprint(\"y_train shape = \",y_train.shape)\nprint(\"\\nx_test shape = \",x_test.shape)\nprint(\"y_test shape = \",y_test.shape)\n# set train Generator\ndatagen = ImageDataGenerator(rotation_range=30,width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\ndatagen.fit(X_train)","e222aa0f":"import keras\nfrom keras.models import Model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Input, Activation, Dropout, GlobalAveragePooling2D, \\\n    BatchNormalization, concatenate, AveragePooling2D\nfrom keras.optimizers import Adam\n\n\n\ndef conv_layer(conv_x, filters):\n    conv_x = BatchNormalization()(conv_x)\n    conv_x = Activation('relu')(conv_x)\n    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)\n    conv_x = Dropout(0.2)(conv_x)\n\n    return conv_x\n\n\ndef dense_block(block_x, filters, growth_rate, layers_in_block):\n    for i in range(layers_in_block):\n        each_layer = conv_layer(block_x, growth_rate)\n        block_x = concatenate([block_x, each_layer], axis=-1)\n        filters += growth_rate\n\n    return block_x, filters\n\n\ndef transition_block(trans_x, tran_filters):\n    trans_x = BatchNormalization()(trans_x)\n    trans_x = Activation('relu')(trans_x)\n    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)\n    trans_x = AveragePooling2D((2, 2), strides=(2, 2))(trans_x)\n\n    return trans_x, tran_filters\n\n\ndef dense_net(filters, growth_rate, classes, dense_block_size, layers_in_block):\n    input_img = Input(shape=(224,224,3))\n    x = Conv2D(24, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_img)\n\n    dense_x = BatchNormalization()(x)\n    dense_x = Activation('relu')(x)\n\n    dense_x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(dense_x)\n    for block in range(dense_block_size - 1):\n        dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n        dense_x, filters = transition_block(dense_x, filters)\n\n    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n    dense_x = BatchNormalization()(dense_x)\n    dense_x = Activation('relu')(dense_x)\n    dense_x = GlobalAveragePooling2D()(dense_x)\n\n    output = Dense(classes, activation='softmax')(dense_x)\n\n    return Model(input_img, output)\n","d2dda24d":"dense_block_size = 3\nlayers_in_block = 4\n\ngrowth_rate = 12\nclasses = 20\nmodel = dense_net(growth_rate * 2, growth_rate, classes, dense_block_size, layers_in_block)\nmodel.summary()\n\n\n# training\nbatch_size = 32\nepochs = 10\noptimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])\nhistory=model.fit(X_train,y_train, epochs=epochs, shuffle=True,validation_data=(x_test, Cat_test_y))\n","70a3b31f":"# set the matplotlib backend so figures can be saved in the background\n# plot the training loss and accuracy\nimport sys\nimport matplotlib\nprint(\"Generating plots...\")\nsys.stdout.flush()\nmatplotlib.use(\"Agg\")\nmatplotlib.pyplot.style.use(\"ggplot\")\nmatplotlib.pyplot.figure()\nN = epochs \nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\nmatplotlib.pyplot.title(\"Cactus Image Classification\")\nmatplotlib.pyplot.xlabel(\"Epoch #\")\nmatplotlib.pyplot.ylabel(\"Loss\/Accuracy\")\nmatplotlib.pyplot.legend(loc=\"lower left\")\nmatplotlib.pyplot.savefig(\"plot.png\")","adb65d4f":"# plot model performance\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.show()\n\nplt.figure()\nplt.gcf().clear()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.show()","0d7f3e81":"from sklearn import metrics\nlabel_pred = model.predict(x_test)\n\npred = []\nfor i in range(len(label_pred)):\n    pred.append(np.argmax(label_pred[i]))\n\nY_test = np.argmax(Cat_test_y, axis=1) # Convert one-hot to index\n\n\nprint(metrics.classification_report(Y_test, pred))","7f5ddb2a":"from sklearn import metrics\nlabel_pred = model.predict(x_test)\n\npred = []\nfor i in range(len(label_pred)):\n    pred.append(np.argmax(label_pred[i]))\n\nY_test = np.argmax(Cat_test_y, axis=1) # Convert one-hot to index\n\nprint(f\"Accuracy Score: %{metrics.accuracy_score(Y_test, pred,normalize=False)}\")","e3a4e8f0":"# Prediction with DenseNet","52e8f301":"**accuracy Score of each class**","cc3b41ce":"# Refrences\n\n* https:\/\/towardsdatascience.com\/densenet-2810936aeebb\n* https:\/\/theailearner.com\/2018\/12\/09\/densely-connected-convolutional-networks-densenet\/\n* https:\/\/towardsdatascience.com\/review-densenet-image-classification-b6631a8ef803","2d7e0ba7":"Overall accuracy Score","60ba7074":"# Notebook log\n* UnderStanding Data\n* Converting images into pixels\n* IntroDuction to Dense Net\n* Implementation of denseNet\n* training with DenseNet\n* Traning and Testing Visualisation\n* Testing Our Model\n* Advantages of Dense Net\n* Submission of Competetion\n* Exploring Others Notebook\n* Refrences","3730a8b8":"# IntroDuction to DenseNet (2018)\n\n![](https:\/\/arthurdouillard.com\/figures\/densenet.png)\n**Now we are going to use Dense net for classifying these Images**\n\nDenseNet Architecture\n\nThe best way to illustrate any architecture is done with the help of code. So, I have implemented DenseNet architecture in Keras using MNIST data set.\n\n*  **Dense Block: **\n A DenseNet consists of dense blocks. Each dense block consists of convolution layers. After a dense block a transition layer is added to proceed to next dense block\nEvery layer in a dense block is directly connected to all its subsequent layers. Consequently, each layer receives the feature-maps of all preceding layer.\n\n* **Covolutional layers :**\n Each convolution layer is consist of three consecutive operations: batch normalization (BN) , followed by a rectified linear unit (ReLU) and a 3 \u00d7 3 convolution (Conv). Also dropout can be added which depends on your architecture requirement.An essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in DenseNet architecture it divides the network into multiple densely connected dense blocks\n \n* **Transition Block : **\n DenseNets can scale naturally to hundreds of layers, while exhibiting no optimization difficulties. Because of their compact internal representations and reduced feature redundancy, DenseNets may be good feature extractors for various computer vision tasks that build on convolutional features\n \n ","47792c47":"# Testing","1f532967":"# Traning and Testing Visualisation","3db49e72":"# Implementation of denseNet"}}