{"cell_type":{"8be1efb8":"code","e98954fa":"code","3321d8a7":"code","594bb11f":"code","99a59211":"code","780a7dbb":"code","79c0f3e9":"code","bf361aa9":"code","fbd0ac67":"code","8996d22c":"code","8c812b11":"code","77fa2c88":"code","eeafb7b7":"code","e61048cf":"code","42fbdbb8":"code","396d7b63":"code","2baa81f2":"code","1353adf5":"code","04f3eb8d":"code","c5fbc986":"code","6e5aca18":"code","b25aff35":"code","56b5997c":"code","bfc3f1cd":"code","bfc92642":"code","d90912c2":"code","79653035":"code","cca153cb":"code","2ba5f47f":"code","97c63888":"markdown","1a308ebd":"markdown","0ab44257":"markdown","3a518f2d":"markdown","66faaf83":"markdown","d2d3b1c5":"markdown","a7e5fd5f":"markdown","10529d78":"markdown","926ceb38":"markdown","ab0b5efa":"markdown","8d40b855":"markdown","ea93e104":"markdown","22c3b678":"markdown","20780e2a":"markdown"},"source":{"8be1efb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e98954fa":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm","3321d8a7":"train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nn_train=train.shape[0]\ny=train['SalePrice']\ntrain=train.drop(['SalePrice'],axis=1)\n\ndata = pd.concat([train,test],ignore_index=True,sort=False)","594bb11f":"train.head()","99a59211":"train.info()","780a7dbb":"nans=pd.isnull(data).sum()\/data.shape[0]\nprint(nans[nans>0].sort_values(ascending=False)*100)","79c0f3e9":"data = data.drop(['Id','PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],axis=1)","bf361aa9":"data['LotFrontage'] = data[['LotFrontage','Neighborhood']].groupby('Neighborhood')['LotFrontage'].transform(\n    lambda x: x.fillna(x.median()))","fbd0ac67":"data['MSZoning'] = data[['MSZoning','MSSubClass']].groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()))","8996d22c":"data.loc[:,['GarageQual','GarageCond','GarageFinish','GarageType']] = data.loc[:,['GarageQual','GarageCond','GarageFinish','GarageType']].fillna('None')\ndata['GarageYrBlt'] = data['GarageYrBlt'].fillna(0)\ndata = data.fillna(data.median())","8c812b11":"print(\"Skewness of SalePrice: %f\" %y.skew())\nsns.distplot(y)","77fa2c88":"c_salePrice = pd.concat([data,y],axis=1).corr()['SalePrice']\nc_salePrice = abs(c_salePrice).sort_values(ascending = False)\n\nfig,ax=plt.subplots(figsize=(25,25))\nsns.heatmap(pd.DataFrame(c_salePrice),annot=True,square=True,ax=ax,cmap='Blues')\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)","eeafb7b7":"sns.regplot(train['OverallQual'],y)","e61048cf":"sns.regplot(train['GrLivArea'],y)","42fbdbb8":"print(data['Utilities'].value_counts())","396d7b63":"print(data['Street'].value_counts())","2baa81f2":"print(data['Condition1'].value_counts())","1353adf5":"print(data['Condition2'].value_counts())","04f3eb8d":"print(data['BldgType'].value_counts())","c5fbc986":"data = data.drop(['Utilities','Street','Condition1','Condition2','BldgType'],axis=1)","6e5aca18":"def create_dummy(data,feature):\n    dummy = pd.get_dummies(data[feature])\n    dummy.columns=[feature+s for s in dummy.columns]\n    data = pd.concat([data,dummy],axis=1)\n    data=data.drop(feature,axis=1)\n    return data","b25aff35":"categorical=[f for f in data.columns if data.dtypes[f] =='object']\nnumeric=[f for f in data.columns if data.dtypes[f]!='object']\n\ndata[numeric]=data[numeric].astype('float64')","56b5997c":"for f in categorical:\n    data=create_dummy(data,f)","bfc3f1cd":"y=np.log1p(y)\n\ndata[numeric]=np.log1p(data[numeric])","bfc92642":"scaler=RobustScaler()\ndata[numeric]=scaler.fit_transform(data[numeric])","d90912c2":"train = data.iloc[:n_train,:]\ntest = data.iloc[n_train:,:]\ntest = test.reset_index()\n\ndef forward_selection(X,y):\n    \n    final_features=[]\n    \n    not_reduced_features=list(X.columns)\n    MSE_hist=100000000000\n    \n    \n    while (len(not_reduced_features)>0):\n        MSE={}\n        pvalue={}\n        print('---------%d features ----------' %len(final_features))\n        for f in not_reduced_features:\n            features=final_features.copy()\n            features.append(f)\n            \n            intercept=pd.DataFrame(np.ones((train.shape[0],1),dtype='float64'))\n            intercept.columns=['Intercept']\n            \n            X_intercept = pd.concat([X[features],intercept],axis=1)\n            \n            reg = sm.OLS(endog=y,exog=X_intercept).fit()\n            \n            y_predict=reg.predict(X_intercept)\n            r2=mean_squared_error(y,y_predict)\n            \n            MSE[f]=r2\n            pvalue[f]=reg.pvalues[f]\n                        \n        mini,mini_f=min(zip(MSE.values(),MSE.keys()))\n        if (mini<MSE_hist and pvalue[mini_f]<0.05):\n            print('{} ---> MSE = {} ---- pvalue = {}\\n'.format(mini_f,mini,pvalue[mini_f]))\n            MSE_hist=mini\n            final_features.append(mini_f)\n            not_reduced_features.remove(mini_f)\n        else:\n            return final_features\n    \n    return final_features\n            \n            \nfeatures_selected = forward_selection(train,y)","79653035":"train = train[features_selected]\ntest = test[features_selected]","cca153cb":"intercept=pd.DataFrame(np.ones((train.shape[0],1),dtype='float64'))\nintercept.columns=['Intercept']\n\ntrain_intercept = pd.concat([train,intercept],axis=1)\n\n# Train our model\nreg=sm.OLS(y,train_intercept).fit()\n\n# Predict the results\nintercept=pd.DataFrame(np.ones((test.shape[0],1),dtype='float64'))\nintercept.columns=['Intercept']\n\ntest_intercept = pd.concat([test,intercept],axis=1)\n\nres=reg.predict(test_intercept)\n\n# we make exp(x)-1 transformation : the reverse of log(1+x)\nres=np.exp(res)-1\n\nprint(res[:9])\nprint(reg.summary())","2ba5f47f":"print(reg.pvalues.sort_values())","97c63888":"# Missing Data","1a308ebd":"# Data transformation","0ab44257":"# Features engineering","3a518f2d":"# Libraries needed\u00b6\n","66faaf83":"# Conclusion\n\n* After training our model we had a score: 0.929.\n* We deduce that Heating Type is the most significant feature in our model because it has the least p-value, followed by GrvLivArea, RoofMatlClyTile which is a type of roof, LotArea and OverallQual which is the most corrolated feature with SalePrice.","d2d3b1c5":"We'll use Forward selection algorithm to select best features that minimize the mean squarred error and which have p-value less than 0.05.\n\np-value indicates the level of significance of null hypothisis in our model.","a7e5fd5f":"# Load Data","10529d78":"* Scaling","926ceb38":"* log(1+x) transformation\n\nLinear Regression assumes normality of errors This transformation helps the reduction of the impact on residuals of large values in the response.\n\nNB: we add 1 for zeros values","ab0b5efa":"__Kaggle Competition__: House Prices Prediction using Linear Regeression","8d40b855":"# Features selection","ea93e104":"# Data visualisation","22c3b678":"* Dummies transformation","20780e2a":"# Model\n\nAs a model we'll use Linear regression model.\n\nWe'll use statsmodel.formula.api.OLS."}}