{"cell_type":{"1e0ffeb7":"code","cdf79fbb":"code","ec1164ec":"code","cdb9b618":"code","cf7aea16":"code","113d4863":"code","78513146":"code","eca25a11":"code","ca562f85":"code","14a0e173":"code","a1842afb":"code","4ba7240d":"code","78645f14":"code","edc3b5e6":"code","6f8b21a0":"code","b61630fc":"code","2614b135":"code","0b887e7b":"code","d0c36812":"code","3d080eaa":"code","5b8efdc6":"code","0933675d":"code","f8dd4131":"code","205c772b":"code","0f62b4d7":"code","b0dbc785":"code","4c844425":"code","f9058ea3":"code","84320ee5":"code","c540d274":"code","69d932cb":"code","d58c56d8":"code","5ce1e534":"code","b597eef8":"code","f8b18dd7":"code","cc6e5de7":"code","53b73f7d":"code","11ec7f4b":"code","879ca90c":"code","ac65dc95":"code","67d5647c":"code","f7377b23":"code","c29f3c24":"code","f82dfd42":"code","d7407970":"code","ed09bd65":"code","15f24f4d":"code","5a932324":"code","931828e0":"code","84ef45a8":"code","25ecd4ad":"code","a6cbb36b":"code","e2507b68":"code","1377a4e2":"code","1230338a":"code","f9889ffa":"code","23db4f3b":"code","c99652a3":"code","06ca32a0":"code","3d3e0435":"code","8c30825a":"code","ebe7f1c3":"code","20c50c4d":"code","b7784ecf":"code","d19a5793":"code","6d8d0b0e":"code","1144a31b":"code","6b2486fb":"code","bda0e7a6":"code","e2ef28e2":"code","bc7c949a":"markdown","1dfb9b00":"markdown","5642a590":"markdown","fb448627":"markdown","d34fe276":"markdown","f911bc16":"markdown","cd1e63db":"markdown","aa682ce4":"markdown","e9de2940":"markdown","602a12e3":"markdown","58eca4dc":"markdown","a5239737":"markdown","5ba96e0b":"markdown","93621a74":"markdown","a4c5ec6a":"markdown","1670a307":"markdown","fc3e66e9":"markdown","fc6fb621":"markdown","7140ae28":"markdown"},"source":{"1e0ffeb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cdf79fbb":"train  = pd.read_csv('..\/input\/train.csv',sep='~')\ntest = pd.read_csv('..\/input\/test.csv',sep='~')","ec1164ec":"train.head()","cdb9b618":"test.head()","cf7aea16":"#Keeping only the necessary features\ntrain_Desc_Only = train[['Description','Is_Response']]","113d4863":"train_Desc_Only.head()","78513146":"#Checking for possible Nuetral Values\ntrain['Is_Response'].unique()","eca25a11":"#Splitting into positive and Negative comments to check for possible Imbalance\ntrain_Desc_Only_Pos = train_Desc_Only[train_Desc_Only['Is_Response']=='Good']\ntrain_Desc_Only_Pos = train_Desc_Only_Pos['Description']\ntrain_Desc_Only_Neg = train_Desc_Only[train_Desc_Only['Is_Response']=='Bad']\ntrain_Desc_Only_Neg = train_Desc_Only_Neg['Description']","ca562f85":"train_Desc_Only_Pos.head()","14a0e173":"train_Desc_Only_Neg.head()","a1842afb":"#We have twice as many positive Responses than negative responses\ntrain_Desc_Only_Pos.shape[0],train_Desc_Only_Neg.shape[0]\n#We will deal with this class Imbalance after deploying all the cleaning functions","4ba7240d":"#Adding a Sentiment Column to the dataset\nx=[]\nfor i in train['Is_Response']:\n    if i=='Good':\n        x.append(1)\n    elif i == 'Bad':\n        x.append(0)","78645f14":"train['sentiment'] = x","edc3b5e6":"train.head()","6f8b21a0":"#Total number of words in Description.\nbase_list=[]\nfor i in train['Description']:\n    i=str(i)\n    for j in i.split():\n        base_list.append(j)\n#Set of Unique words\nbase_uniq = set(base_list)\nprint(\"Total number of words: {} and Total number of unique words {}\".format(len(base_list),len(base_uniq)))     ","b61630fc":"#checking for alphabets\nx_words=[]\nfor i in base_uniq:\n    if i.isalpha()==True:\n        x_words.append(i)\nlen(set(x_words))\n#Hence this makes us believe there are many special characters present","2614b135":"#Replacing special characters \ntrain['Description'] = train['Description'].str.replace(\"[^a-zA-Z#]\", \" \")","0b887e7b":"#Total number of words.\nbase_list=[]\nfor i in train['Description']:\n    i=str(i)\n    for j in i.split():\n        base_list.append(j)\n#Set of Unique words\nbase_uniq = set(base_list)\nprint(\"Total number of words: {} and Total number of unique words {}\".format(len(base_list),len(base_uniq))) ","d0c36812":"#checking for alphabets\nx_words=[]\nfor i in base_uniq:\n    if i.isalpha()==True:\n        x_words.append(i)\nlen(set(x_words))\n#Still few special characters exist we will now set a probe as to which all special characters still exist","3d080eaa":"#checking further for Special Charcters\nx_words_Special=[]\nfor i in base_uniq:\n    if i.isalpha()==False:\n        x_words_Special.append(i)\nprint(set(x_words_Special),len(set(x_words_Special)))\n#So we now need to check and replace for '#' in each word","5b8efdc6":"#Function to find #\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt  ","0933675d":"#remove # from Description\nimport re\ntrain['Description'] = np.vectorize(remove_pattern)(train['Description'],'#[\\w]*')","f8dd4131":"#Total number of words.\nbase_list=[]\nfor i in train['Description']:\n    i=str(i)\n    for j in i.split():\n        base_list.append(j)\n#Set of Unique words\nbase_uniq = set(base_list)\n#checking for Special Charcters\nx_words_Special=[]\nfor i in base_uniq:\n    if i.isalpha()==False:\n        x_words_Special.append(i)\nprint(set(x_words_Special),len(set(x_words_Special)))\n#So we now need to check and replace for '#' in each word","205c772b":"#Total number of unique words.\nbase_list=[]\nfor i in train['Description']:\n    i=str(i)\n    for j in i.split():\n        base_list.append(j)\n#Set of Unique words\nbase_uniq = set(base_list)\n\nx_words=[]\nfor i in base_uniq:\n    if i.isalpha()==True:\n        x_words.append(i)\nlen(set(x_words))\n\nprint(\"Total number of words: {} and Total number of unique words {}, out of which {} are valid\".format(len(base_list),len(base_uniq),len(set(x_words)))) ","0f62b4d7":"#Removing Punctuations, Numbers, and Special Characters\ntrain['Description'] = train['Description'].str.replace(\"[^a-zA-Z#]\", \" \")","b0dbc785":"train.head(2)","4c844425":"#Unique values for Browser_Used\ntrain['Browser_Used'].unique()","f9058ea3":"train['Browser_code'] = train['Browser_Used'].map({'Google Chrome':1,'Firefox':2,'Mozilla':3,'InternetExplorer':4,'Edge':5,'Mozilla Firefox':6,'Internet Explorer':7,\n                                                  'Chrome':8,'IE':9,'Opera':10,'Safari':11})","84320ee5":"#Unique values for Device_Used\ntrain['Device_Used'].unique()","c540d274":"train['Device_code'] = train['Device_Used'].map({'Desktop':1,'Tablet':2,'Mobile':3})","69d932cb":"train.head()","d58c56d8":"#Keeping only the necessary features\ntrain_Desc_Only = train[['Description','Is_Response','sentiment','Browser_code','Device_code']]\ntrain_Desc_Only.head()","5ce1e534":"#Removing Short Words(Words of the length 3 and less)\ntrain_Desc_Only['Description'] = train_Desc_Only['Description'].apply(lambda x: ' '.join([w for w in x.split() if len(x) > 3]))","b597eef8":"stopwords = set(stopwords.words('english'))\ntrain_Desc_Only['Description'] = train_Desc_Only['Description'].apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))","f8b18dd7":"#Adding a new feature 'text_length'\ntrain_Desc_Only['text_length'] = train_Desc_Only['Description'].apply(len)","cc6e5de7":"train_Desc_Only.head()","53b73f7d":"#Setting the style for seaborn\nsns.set_style('white')","11ec7f4b":"#Using FacetGrid to compare the variation of size of the text\ng = sns.FacetGrid(train_Desc_Only,col='Is_Response')\ng.map(plt.hist,'text_length',bins = 50)\n#Hence we can say that generally longer texts lead too 'Good' Review","879ca90c":"#BoxPlot of the text_length field and the Is_Response field\nsns.boxplot(x='Is_Response',y='text_length',data=train_Desc_Only)\n#this shows that text length cannot be used as a useful feature to predict the Resoponse since there are many values for text_length in the outliers","ac65dc95":"#CountPlot of number of occurences of Good and Bad\nsns.countplot(x='Is_Response',data = train_Desc_Only,palette='rainbow')\n#This depicts that the number of responses for Good far exceeds the number responses for Bad","67d5647c":"#Using FacetGrid to compare the variation of Device Code\ng = sns.FacetGrid(train_Desc_Only,col='Is_Response')\ng.map(plt.hist,'Device_code',bins = 50)\n#Here we can see that we cannot decide the Response value from the Device_code feature","f7377b23":"#BoxPlot of the text_length field and the Is_Response field\nsns.boxplot(x='Is_Response',y='Device_code',data=train_Desc_Only)","c29f3c24":"#Using FacetGrid to compare the variation of Browser code \ng = sns.FacetGrid(train_Desc_Only,col='Is_Response')\ng.map(plt.hist,'Browser_code',bins = 50)\n#Here we can see that we cannot decide the Response value from the Device_code feature","f82dfd42":"#We will now use Group By to get the mean values of text_length column\nResponse = train_Desc_Only.groupby('Is_Response').mean()\nResponse","d7407970":"#Correlation between the variables\nResponse.corr()","ed09bd65":"#Correlation Map\nsns.heatmap(Response.corr(),cmap='coolwarm',annot = True)","15f24f4d":"Ant = train_Desc_Only","5a932324":"Ant.info()","931828e0":"#Splitting into x and y\nx = Ant['Description']\ny = Ant['Is_Response']","84ef45a8":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()","25ecd4ad":"#Using CountVectorize object to transform x\nx = cv.fit_transform(x)","a6cbb36b":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .3,random_state = 101)","e2507b68":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","1377a4e2":"nb.fit(X_train,y_train)","1230338a":"predictions = nb.predict(X_test)","f9889ffa":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))","23db4f3b":"from sklearn.feature_extraction.text import TfidfTransformer","c99652a3":"from sklearn.pipeline import Pipeline","06ca32a0":"pipe = Pipeline([('bow',CountVectorizer()),\n                ('tfidf',TfidfTransformer()),('model',MultinomialNB())])\n#bow : bag of words","3d3e0435":"#Splitting into x and y\nx = Ant['Description']\ny = Ant['Is_Response']","8c30825a":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .3,random_state = 101)","ebe7f1c3":"pipe.fit(X_train,y_train)","20c50c4d":"predictions = pipe.predict(X_test)","b7784ecf":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))","d19a5793":"#Trying now with Bernoullis NB\n\nfrom sklearn.naive_bayes import BernoulliNB\n\npipe = Pipeline([('bow',CountVectorizer()),\n                ('tfidf',TfidfTransformer()),('model',BernoulliNB())])\n#bow : bag of words\n\n#Splitting into x and y\nx = Ant['Description']\ny = Ant['Is_Response']\n\n## Train Test Split\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,test_size = .3,random_state = 101)\n\n## Fitting the pipeline to the training data\n\npipe.fit(X_train,y_train)\n\n## Predictions and Evaluation\n\npredictions = pipe.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))","6d8d0b0e":"private = test[0:2628]\npublic = test[2628:]\n","1144a31b":"#Trying now with Bernoullis NB\n\nfrom sklearn.naive_bayes import BernoulliNB\n\npipe = Pipeline([('bow',CountVectorizer()),\n                ('tfidf',TfidfTransformer()),('model',BernoulliNB())])\n#bow : bag of words\n\n#Splitting into x and y\nX_train = Ant['Description']\ny_train = Ant['Is_Response']\n\nX_test = private['Description']\n\n## Fitting the pipeline to the training data\n\npipe.fit(X_train,y_train)\n\n## Predictions and Evaluation\n\nprivate['Is_Response'] = pipe.predict(X_test)\n\n","6b2486fb":"#Total number of words.\nbase_list=[]\nfor i in test['Description']:\n    i=str(i)\n    for j in i.split():\n        base_list.append(j)\n#Set of Unique words\nbase_uniq = set(base_list)\nprint(\"Total number of words: {} and Total number of unique words {}\".format(len(base_list),len(base_uniq))) \n\n#checking for alphabets\nx_words=[]\nfor i in base_uniq:\n    if i.isalpha()==True:\n        x_words.append(i)\nlen(set(x_words))\n#Still few special characters exist we will now set a probe as to which all special characters still exist\n\n#checking further for Special Charcters\nx_words_Special=[]\nfor i in base_uniq:\n    if i.isalpha()==False:\n        x_words_Special.append(i)\nprint(set(x_words_Special),len(set(x_words_Special)))\n#So we now need to check and replace for '#' in each word\n\n#Function to find #\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt  \n\n#remove # from Description\nimport re\ntest['Description'] = np.vectorize(remove_pattern)(test['Description'],'\"\"[\\w]*')","bda0e7a6":"test['Description'] = np.vectorize(remove_pattern)(test['Description'],'--[\\w]*')","e2ef28e2":"#Splitting into x and y\nX_train = Ant['Description']\ny_train = Ant['Is_Response']\nX_test = test['Description']\n### Employing Count Vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n#Using CountVectorize object to transform x\nX_train = cv.fit_transform(X_train)\n\n\n## Import Naive Bayes Classifier\n\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n## Fitting Naive Bayes Classifier\n\nnb.fit(X_train,y_train)\n\n## Predictions and Evaluations\nX_test = cv.fit_transform(X_test)\nprint(test.shape,X_test.shape)","bc7c949a":"## Train Test Split","1dfb9b00":"### **Length of text**","5642a590":"## NLP Classification","fb448627":"## Metric","d34fe276":"## **Browser_Code**","f911bc16":"## Train Test Split","cd1e63db":"## Predictions and Evaluation","aa682ce4":"## Predictions and Evaluations","e9de2940":"## Removing StopWords","602a12e3":"## **Device Used**","58eca4dc":"## Fitting the pipeline to the training data","a5239737":"### Employing Count Vectorizer","5ba96e0b":"## Trying to predict with only count vectorizer","93621a74":"## Fitting Naive Bayes Classifier","a4c5ec6a":"## Import Naive Bayes Classifier","1670a307":"## Import pipeline from sklearn","fc3e66e9":"## Now we will try employing TFIDF","fc6fb621":"## Confusion Matrix and Classification report","7140ae28":"## Now we create a pipeline :"}}