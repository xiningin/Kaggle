{"cell_type":{"606f20e7":"code","085ab00c":"code","b8487451":"code","ae6996b7":"code","2b8442ca":"code","951f95eb":"code","d3fbf271":"code","72fff9c8":"code","ae5b725e":"code","ce815746":"code","a52f8a46":"code","362af1df":"code","3d1f68bb":"code","a70436af":"code","38b0c592":"code","6d479e06":"code","1c71e3ac":"code","d5b2d299":"code","5846eb1f":"markdown","d8d668ed":"markdown","65f7cbf7":"markdown","34cb00b9":"markdown","b67e9841":"markdown","03b7f677":"markdown","c7788ee7":"markdown","25ed8f95":"markdown","a73f1610":"markdown","498b4386":"markdown","32f6bc2b":"markdown","2e95c0a9":"markdown","dd53f4fd":"markdown","4cedc81d":"markdown","62f021d2":"markdown","787974fe":"markdown","1c310d65":"markdown"},"source":{"606f20e7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors","085ab00c":"\ndata = pd.read_csv('..\/input\/data.csv')\n\nprint(data.head())\nprint(data['class'].unique())","b8487451":"\nX = data.loc[:, data.columns != 'class'].values\ny = data['class'].values\n###End code\n\nassert X.shape == (10000, 10)\nassert y.shape == (10000, )","ae6996b7":"colors=['green','blue']\ncmap = matplotlib.colors.ListedColormap(colors)\n#Plot the figure\nplt.figure()\nplt.title('Non-linearly separable classes')\nplt.scatter(X[:,0], X[:,3], c=y,\n           marker= 'o', s=50,cmap=cmap,alpha = 0.5 )\nplt.show()","2b8442ca":"from pandas.plotting import scatter_matrix\n%matplotlib inline\ncolor_wheel = {0: \"#0392cf\", \n               1: \"#7bc043\", \n            }\n\ncolors_mapped = data[\"class\"].map(lambda x: color_wheel.get(x))\n\naxes_matrix = scatter_matrix(data.loc[:, data.columns != 'class'], alpha = 0.2, figsize = (10, 10), color=colors_mapped )","951f95eb":"X_data = X.T\ny_data = y.reshape(1,len(y))\n\nassert X_data.shape == (10, 10000)\nassert y_data.shape == (1, 10000)","d3fbf271":"layer_dims = [10,9,9,1]","72fff9c8":"import tensorflow as tf","ae5b725e":"def placeholders(num_features):\n    A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n    Y = tf.placeholder(dtype = tf.float64, shape = ([1,None]))\n    return A_0,Y","ce815746":"def initialize_parameters_deep(layer_dims):\n    tf.set_random_seed(1)\n    L = len(layer_dims)\n    parameters = {}\n    for l in range(1,L):\n        parameters['W' + str(l)] = tf.get_variable(\"W\" + str(l), shape=[layer_dims[l], layer_dims[l-1]], dtype = tf.float64,\n                                   initializer=tf.random_normal_initializer())\n                                   \n        parameters['b' + str(l)] = tf.get_variable(\"b\"+ str(l), shape = [layer_dims[l], 1], dtype= tf.float64, initializer= tf.zeros_initializer() )\n        \n    return parameters ","a52f8a46":"def linear_forward_prop(A_prev,W,b, activation):\n    Z = tf.add(tf.matmul(W, A_prev), b)\n    if activation == \"sigmoid\":\n        A = Z\n    elif activation == \"relu\":\n        A = tf.nn.relu(Z)\n    return A","362af1df":"def l_layer_forwardProp(A_0, parameters):\n    A = A_0\n    L = len(parameters)\/\/2\n    for l in range(1,L):\n        A_prev = A\n        A = linear_forward_prop(A_prev,parameters['W' + str(l)],parameters['b' + str(l)], \"relu\")     \n        #call linear forward prop with relu activation\n    A = linear_forward_prop(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\" )                  \n    #call linear forward prop with sigmoid activation\n    \n    return A","3d1f68bb":"def final_cost(Z_final, Y , parameters, regularization = False, lambd = 0):\n    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=Z_final,labels=Y)\n    if regularization:\n        reg_term = 0\n        L = len(parameters)\/\/2\n        for l in range(1,L+1):\n            \n            reg_term +=  tf.nn.l2_loss(parameters['W'+str(l)])              #add L2 loss term\n            \n        cost = cost + (lambd\/2) * reg_term\n    return tf.reduce_mean(cost)","a70436af":"import numpy as np\ndef random_samples_minibatch(X, Y, batch_size, seed = 1):\n    np.random.seed(seed)\n    \n    m =  X.shape[1]                                          #number of samples\n    num_batches = int(m \/ batch_size )                               #number of batches derived from batch_size\n    \n    indices =  np.random.permutation(m)                                 # generate ramdom indicies\n    shuffle_X = X[:,indices]\n    shuffle_Y = Y[:,indices]\n    mini_batches = []\n    \n    #generate minibatch\n    for i in range(num_batches):\n        X_batch = shuffle_X[:,i * batch_size:(i+1) * batch_size]\n        Y_batch = shuffle_Y[:,i * batch_size:(i+1) * batch_size]\n        \n        assert X_batch.shape == (X.shape[0], batch_size)\n        assert Y_batch.shape == (Y.shape[0], batch_size)\n        \n        mini_batches.append((X_batch, Y_batch))\n    \n    #generate batch with remaining number of samples\n    if m % batch_size != 0:\n        X_batch = shuffle_X[:, (num_batches * batch_size):]\n        Y_batch = shuffle_Y[:, (num_batches * batch_size):]\n        mini_batches.append((X_batch, Y_batch))\n    return mini_batches","38b0c592":"def model(X_train,Y_train, layer_dims, learning_rate, optimizer ,num_iter, mini_batch_size):\n    tf.reset_default_graph()\n    num_features, num_samples = X_train.shape\n    \n    A_0, Y = placeholders(num_features)\n    #call placeholder function to initialize placeholders A_0 and Y\n    parameters =  initialize_parameters_deep(layer_dims)                   \n    #Initialse Weights and bias using initialize_parameters\n    Z_final = l_layer_forwardProp(A_0, parameters)                      \n    #call the function l_layer_forwardProp() to define the final output\n    \n    cost =  final_cost(Z_final, Y , parameters, regularization = True)\n    #call the final_cost function with regularization set TRUE\n    \n    \n    \n    if optimizer == \"momentum\":\n        train_net = tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(cost)                 \n        #call tensorflow's momentum optimizer with momentum = 0.9\n    elif optimizer == \"rmsProp\":\n        train_net = tf.train.RMSPropOptimizer(learning_rate, decay=0.999).minimize(cost)\n                   \n        #call tensorflow's RMS optimiser with decay = 0.999\n    elif optimizer == \"adam\":\n        train_net = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999).minimize(cost)                 \n        ##call tensorflow's adam optimizer with beta1 = 0.9, beta2 = 0.999\n    \n    seed = 1\n    num_minibatches = int(num_samples \/ mini_batch_size)\n    init = tf.global_variables_initializer()\n    costs = []\n    with tf.Session() as sess:\n        sess.run(init)\n        for epoch in range(num_iter):\n            epoch_cost = 0\n            \n            mini_batches = random_samples_minibatch(X_train, Y_train, mini_batch_size, seed)\n            #call random_sample_minibatch to return minibatches\n            \n            seed = seed + 1\n            \n            #perform gradient descent for each mini-batch\n            for mini_batch in mini_batches:\n                \n                X_batch, Y_batch = mini_batch            #assign minibatch\n                \n                _,mini_batch_cost = sess.run([train_net, cost], feed_dict={A_0: X_batch, Y: Y_batch})\n                epoch_cost += mini_batch_cost\/num_minibatches\n            \n            if epoch % 2 == 0:\n                costs.append(epoch_cost)\n            if epoch % 10 == 0:\n                print(epoch_cost)\n        with open(\"output.txt\", \"w+\") as file:\n            file.write(\"%f\" % epoch_cost)\n        plt.ylim(0 ,2, 0.0001)\n        plt.xlabel(\"epoches per 2\")\n        plt.ylabel(\"cost\")\n        plt.plot(costs)\n        plt.show()\n        params = sess.run(parameters)\n    return params","6d479e06":"params_momentum = model(X_data,y_data, layer_dims, learning_rate=0.001, optimizer='momentum' ,num_iter=100, mini_batch_size=256)","1c71e3ac":"params_momentum = model(X_data,y_data, layer_dims, learning_rate=0.001, optimizer='rmsProp' ,num_iter=100, mini_batch_size=256)","d5b2d299":"params_momentum = model(X_data,y_data, layer_dims, learning_rate=0.001, optimizer='adam' ,num_iter=100, mini_batch_size=256)","5846eb1f":"define function named initialize_parameters_deep() to initialize weights and bias for each layer\n- Use tf.random_normal() to initialise weights and tf.zeros() to initialise bias. Set datatype as float64\n- Parameters - layer_dims\n- Returns - dictionary of weights and bias","d8d668ed":"import tensorflow as tf","65f7cbf7":"Call the method model_with_minibatch() with learning rate 0.001, **optimizer = momentum** num_iter = 100 and minibatch 256","34cb00b9":"Define a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n- Set the datatype of placeholders as float64\n- parameters - num_features\n- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)","b67e9841":"Call the method model_with_minibatch() with learning rate 0.001, **optimizer = rmsProp** num_iter = 100 and minibatch 256","03b7f677":"Define the function to generate mini-batches.","c7788ee7":"- Run the below cell to visualize the data in x-y plane. \n- The green spots corresponds to target value 0 and green spots corresponds to target value 1\n- Though the data is more than 2 dimension only first two features are considered for visualization","25ed8f95":"Call the method model_with_minibatch() with learning rate 0.001, **optimizer = adam** num_iter = 100 and minibatch 256","a73f1610":"Define forward propagation for entire network as l_layer_forward()\n- Parameters: A_0(input data), parameters(dictionary of weights and bias)\n- returns: A(output from final layer)  ","498b4386":"- Define the cost function\n- parameters:\n  - Z_final: output fro final layer\n  - Y: actual output\n  - parameters: dictionary of weigths and bias\n  - regularization : boolean\n  - lambd: regularization parameter\n- First define the original cost using tensoflow's sigmoid_cross_entropy function\n- If **regularization == True** add regularization term to original cost function","32f6bc2b":"The data is provided as file named 'data.csv'.  \nUsing pandas read the csv file and assign the resulting dataframe to variable 'data'   \nfor example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** ","2e95c0a9":"### Gradient Descent with Momentum\n- Gradient descent with momentum is an improvised version of minibatch gradient descent.\n- This method accelerates the gradient descent steps much faster by eliminating unwanted oscillations.\n- In other words, it smoothens the path taken by gradient descent towards the global minimum.\n- This method makes use of the concept called exponentially weighted moving average when performing parameter update during gradient descent.\n\n### RMS Prop\n- Root Mean Square propagation is another technique to boost the gradient descent.\n- This technique aims to damp out oscillation perpendicular to the path of gradient descent and at the same time increase the step size in the direction of the global minimum.\n\n### Adam Optimizer\n- Adam optimization is nothing but the combination of gradient descent with momentum and RMS prop.\n- It incorporates the advantages of both of these algorithms by dividing the moving average by the root mean square of derivates\n\n## Intro\n- In this handson you will be using the concept of GD with momentum, RMS prop and Adam prop to build optimized deep neural network\n- You will also be implementing minibatch gradient and L2 regularization to train you network\n- Follow the instruction provided for cell to write the code in each cell.\n- Run the below cell for to import necessary packages to read and visualize data","dd53f4fd":"Define the network dimension to have **10** input features, **two** **hidden layers** with **9** nodes each, one output node at final layer. ","4cedc81d":" Extract all the feature values from dataframe 'data' and assign it to variable 'X'\n- Extract target variable 'class' and assign it to variable 'y'.  \nHint:\n - Use .values to exract values from dataframe","62f021d2":"- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n- Transpose X and assign it to variable 'X_data'\n- reshape y to have shape (1, number of samples) and assign to variable 'y_data'","787974fe":"The data is provided as file named 'blobs.csv'.  \nUsing pandas read the csv file and assign the resulting dataframe to variable 'data'   \nfor example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')** ","1c310d65":"Define functon named linear_forward_prop() to define forward propagation for a given layer.\n- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n- returns: A(output from the current layer)\n- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is sigmoid"}}