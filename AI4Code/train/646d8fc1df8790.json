{"cell_type":{"49ba4ab4":"code","24ac2dac":"code","1312bd85":"code","7fd0c90c":"code","8929c300":"code","1812fdb7":"code","f78d768a":"code","9bed24b2":"code","89ecbfee":"markdown","37ee8c73":"markdown","2e95ed73":"markdown","6775d724":"markdown","cf729d43":"markdown","a53f0a70":"markdown","2adb9bb2":"markdown"},"source":{"49ba4ab4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24ac2dac":"dataset = pd.read_csv('\/kaggle\/input\/play-tenniscsv\/play_tennis_dataset.csv', index_col=0)","1312bd85":"dataset","7fd0c90c":"class Node:\n    \n    # Attribute Name, Attribute Value, Next Node\n    # Attribute Name can be string or Node \n    def __init__(self, attribute_name):\n        \n        self.attribute_name = attribute_name\n        self.branch_val = []\n        self.next = []\n        \n    def set_branch_value(self, branch_val):\n        self.branch_val.append(branch_val)\n        \n    def set_child(self, child):\n        self.next.append(child)\n\n        ","8929c300":"pstpr = []\ndef id3(data, target_value, attributes):    \n    \n    \n    if data.shape[0] == 0:\n        return Node(attributes[0])\n    \n    if data[target_value].nunique == 1:        \n\n        return Node(data[target_value].value_counts().keys()[0])\n            \n    if len(attributes) == 0:        \n        return Node(data[target_value].value_counts().keys()[0])\n    \n    elif(type(attributes) == list) :        \n        \n        # Root should of node type        \n        gain, root_attr = getBestClassificationAttribute(data, target_value, attributes)        \n        root = Node(root_attr)\n        \n        for attribute in dataset[root_attr].unique(): \n            \n#             print('Attribute is : {} of column {}'.format(attribute, root_attr))\n            \n            root.set_branch_value(attribute)            \n            subset = data[data[root_attr] == attribute]                        \n            new_attributes = attributes.copy()            \n            new_attributes.remove(root_attr)\n            \n            if(subset[target_value].nunique() == 1):\n                root.set_child(subset[target_value].unique()[0]) \n            elif(subset.shape[0] == 0):\n                print(subset)\n                root.set_child(data[target_value].value_counts()[0])\n            else:                \n                root.set_child(id3(subset,target_value,new_attributes))                \n        return root\n\ndef getBestClassificationAttribute(data, target_attr, attributes, show_gain=False):\n        \n    \n    S_vi = data[target_attr].unique()        \n    \n    if (len(S_vi) == 1):\n        S_negative = 0\n        S_positive = data[data[target_attr] == data[target_attr].unique()[0]].shape[0]\n        total = data[data[target_attr] == data[target_attr].unique()[0]].shape[0]\n        Entropy_S = 1\n    else :\n        S_negative = data[data[target_attr] == data[target_attr].unique()[0]].shape[0]\n        S_positive = data[data[target_attr] == data[target_attr].unique()[1]].shape[0]\n        total  = S_negative + S_positive\n        Entropy_S =  -(S_negative\/ total)*math.log(S_negative\/ total, 2) - (S_positive\/total)*math.log(S_positive \/total,2)\n\n    MAX_GAIN = -1\n    MAX_GAIN_ATTRIBUTE = ''\n\n    for attribute in attributes:\n\n        A_gain = 0\n\n        A = data[attribute].unique()\n\n        for avi in A:\n\n            pi = data[data[attribute] == avi].shape[0]\n            \n            avi_negative = data[(data[attribute] == avi) & (data[target_attr] == S_vi[0])].shape[0]\n            \n            \n            if (len(S_vi) > 1):\n                avi_positive = data[(data[attribute] == avi) & (data[target_attr] == S_vi[1])].shape[0]\n            else:\n                avi_positive = 0\n            \n            avi_entropy = 0\n            \n            if(avi_negative != 0):\n                avi_entropy = avi_entropy + (avi_negative\/ pi)*math.log(avi_negative\/pi,2)\n            \n            if(avi_positive != 0):\n                avi_entropy = avi_entropy + (avi_positive\/ pi)*math.log(avi_positive\/pi,2)\n\n            A_gain = A_gain + (pi\/total)*(-avi_entropy)\n        \n        \n        A_gain = Entropy_S  - A_gain\n        \n        if (show_gain):\n            print(A_gain, attribute)\n        if A_gain > MAX_GAIN:\n            MAX_GAIN = A_gain\n            MAX_GAIN_ATTRIBUTE = attribute\n    \n    return MAX_GAIN, MAX_GAIN_ATTRIBUTE","1812fdb7":"def print_tree(root, offset=0):\n    \n    prefix = '\\t'*offset\n    if(offset == 0 ):\n        print(prefix+root.attribute_name)\n    \n    message = ''\n    for child, branch in zip(root.next, root.branch_val):        \n        if (type(child) == Node ):            \n              print(prefix+' |---'+child.attribute_name+'({} = {})'.format(root.attribute_name,branch))  \n        else:\n              print(prefix+' |---'+str(child)+'({} = {})'.format(root.attribute_name,branch))\n        \n        \n        if (type(child) != str):\n            print_tree(child,offset+1)\n    print(message)\n    \n","f78d768a":"root = id3(dataset,'PlayTennis', list(dataset.columns.drop('PlayTennis')))","9bed24b2":"print_tree(root)","89ecbfee":"# Psuedo Algorithm for constructing a Decision Tree (ID3)\n```\n\nDefine ID3(Examples, Target_Attribute, Attributes):\n      Create a Root node for the tree\n      If all Examples are positive, Return the single-node tree Root, with positive label\n      If all Examples are negative, Return the single-node tree Root, with negative label\n      If Attributes is empty, Return the single-node tree Root, with label = most common value of Target attribute\n      Else Begin\n          A := getBestClassficationAttribute(Examples)\n          Root := A\n          For each value V_i of A:\n             Add a new tree branch below Root, corresponding to the test A=V_i\n             Examples_vi := subset of Examples that have value A = V_i\n             If Examples_vi is empty:\n                 Add leaf node with label that is most common value of Target_attributes in Examples\n             Else:\n                 ID3(Examples_vi, Target_Attribute, Attributes - {A})\n\n\n\nDefine getBestClassificationAttribute(Examples, Target_Attribute, Attributes):\n    \n        S_negative := Examples where Target_Attribute is negative\n        S_positive := Examples where Target_Attrivute is positive\n        total  := S_negative + S_positive\n\n         Entropy_S :=  -(S_negative\/ total)log(S_negative\/ total) - (S_positive\/total)log(S_positive \/total)\n\n         MAX_GAIN := -1\n        \n         for attribute of Attributes:\n        \n          A_gain = 0\n     \n            A := possible values of attribute\n\n            for avi of A:\n\n                pi = Examples where A is avi\n\n                avi_negative = Examples where target values in pi is negative\n                avi_positive = Examples where target values in pi is positive\n\n                avi_entropy = -(avi_negative\/ pi)log(avi_negative\/pi) - (avi_positive\/pi)log(avi_positive\/pi)\n\n                A_gain = A_gain - (pi\/total)*(avi_entropy)\n        \n        \n```\n\nReference: Machine Learning Tom M. Mitchell (ISBN-13: 978-1-25-909695-2)","37ee8c73":"**Note: This is just for understanding purposes. This code has many limitations, only works with discrete varibles and \ncan only be used for binary classification.**","2e95ed73":"<img src=\"https:\/\/nullpointerexception1.files.wordpress.com\/2017\/11\/decision-tree-e1513448957591.jpg?w=310\">","6775d724":"## Helper function to display tree","cf729d43":"## A Little Information about Decision Trees\n\n**Decision trees classify an instance starting from root node to a leaf node, every pass through a node is like a if else\ncondition on a attribute of the instance. We can say that decision is represented as a set of disjunction of conjuction of\nof checks on a attribute. A typical decision tree contains the set of values that the target variable can take.**","a53f0a70":"# The Dataset\n\n**This is a small and simple dataset which contains forecast data under which we can play tennis.**","2adb9bb2":"## Todo\n\n* Add Limitations for this Implementation\n* Predict a instance using the decision tree"}}