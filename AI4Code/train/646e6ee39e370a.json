{"cell_type":{"1c2f42db":"code","bb31e5b3":"code","8f3a8be0":"code","07a01ce1":"code","0e41df45":"code","b199ab81":"code","330a8bda":"code","d61e8431":"code","9978530a":"code","b0cc34fa":"code","f6067073":"code","5b161423":"code","2e27056c":"code","5088e7a7":"code","48ff3238":"code","64a4a510":"code","a1ec4f54":"code","e75f7da4":"code","749656a0":"code","6d1bc117":"markdown","7da281da":"markdown","af8cdc5e":"markdown","9e187661":"markdown","f5d68ca0":"markdown"},"source":{"1c2f42db":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndata = '..\/input\/hcc.csv'\ndataHCC = pd.read_csv(data)\ndataHCC.head(5)","bb31e5b3":"dataHCC = dataHCC.apply(lambda x: x.replace('?',np.nan)) # replacing ? for Nan.\ndataHCC.head(5)","8f3a8be0":"print('(number of instances, number of attributes) ')\nprint(dataHCC.shape)\nprint('##########################')\nprint(dataHCC.columns) \nprint('##########################')\ndataHCC.info()\nprint('##########################')\nprint(dataHCC['Class'].value_counts()) #instances per class\nprint('##########################')\nprint('NaN per attribute')\nprint(dataHCC.isnull().sum())\ndataHCC.describe(include='all')","07a01ce1":"dataHCC.drop_duplicates() #removing duplicate instances\ndataHCC=dataHCC.dropna(thresh=40) #maintaining only the instances with at least 80 non-NA values, that is, 80%.  \ndataHCC=dataHCC.dropna(axis=1, thresh=125) # maintaining only attributes with at least 125 non-NA values, ie 80%.\nprint(dataHCC.shape)\nprint(dataHCC.columns)\ndataHCC['Class'].value_counts() \ndataHCC.isnull().sum() ","0e41df45":"dataHCC= dataHCC.convert_objects(convert_numeric=True) # converting the data type  \ndataHCC.describe(include='all')","b199ab81":"data1=dataHCC.iloc[:,0:19]  #categorical data 1\/0\ncat=dataHCC.iloc[:,39]\ndata1['Class'] = cat\ndata2=dataHCC.iloc[:,19:40]   #non categorical data","330a8bda":"data2.hist(figsize=(22,20))","d61e8431":"data2.iloc[:,0:20].plot(kind='box', subplots=True, layout=(4,5),figsize=(14,12))","9978530a":"corr= dataHCC.corr()\ncorr","b0cc34fa":"ax = sns.heatmap(corr, annot = True, cmap=\"YlGnBu\", cbar=False)\nplt.setp(ax.axes.get_xticklabels(), rotation=90)\nplt.rcParams['figure.figsize']=(24,20)","f6067073":"corr1= data1.corr()\nax1 = sns.heatmap(corr1, annot = True, cmap=\"YlGnBu\", cbar=False)\nplt.setp(ax1.axes.get_xticklabels(), rotation=90)\nplt.rcParams['figure.figsize']=(24,20)","5b161423":"corr2= data2.corr()\nax2 = sns.heatmap(corr2, annot = True, cmap=\"YlGnBu\", cbar=False)\nplt.setp(ax2.axes.get_xticklabels(), rotation=90)\nplt.rcParams['figure.figsize']=(24,20)","2e27056c":"dataHCC.iloc[:,0:19].mode()","5088e7a7":"x = range(19)\nfor n in x:\n dataHCC=dataHCC.replace({dataHCC.columns[n]: np.nan}, dataHCC.loc[:,dataHCC.columns[n]].mode().loc[0]) ","48ff3238":"dataHCC.iloc[:,19:39].mean()","64a4a510":"x = range(19,40)\nfor n in x:\n dataHCC=dataHCC.replace({dataHCC.columns[n]: np.nan}, dataHCC.loc[:,dataHCC.columns[n]].mean()) \n","a1ec4f54":"dataHCC.isnull().sum() ","e75f7da4":"dataHCC.info()\nprint('##########################')\nprint(dataHCC['Class'].value_counts()) #verificando quantidade de inst\u00e2ncia em cada classe\nprint('##########################')\ndataHCC.iloc[:,19:39].describe()","749656a0":"dataHCC.to_csv('EDAdataHCC.csv') #saving the new clean database","6d1bc117":"In the database, missing data are represented as **?** then the first step is to replace **?** for **NaN**.","7da281da":"the rest of the data is replaced by the mean\n","af8cdc5e":"Null categorical data are replaced by mode.","9e187661":"**Data cleaning.**","f5d68ca0":"**Exploratory Data Analysis (EDA)**\n\nExploratory data analysis consists of analyzing the data to clear the dataset and then apply machine learning techniques.\n\n**Dataset cleaning** \n\nDataset cleaning, in general, consists of removing or replacing null data from the dataset."}}