{"cell_type":{"5ef1e500":"code","704696f5":"code","b63e3986":"code","b9fa39d5":"code","e0fd988b":"code","82ccad52":"code","d206d265":"code","f21ccb4d":"code","2ae3fd23":"code","7a1fa3d4":"code","f6fa756f":"code","0e02f0f1":"code","337e7cae":"code","d90b35ca":"markdown","ef3d885e":"markdown","49a8a7be":"markdown","9c9003f5":"markdown","8ca0300b":"markdown"},"source":{"5ef1e500":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","704696f5":"train = pd.read_excel('\/kaggle\/input\/predict-news-category\/Data_Train.xlsx')\ntest = pd.read_excel('\/kaggle\/input\/predict-news-category\/Data_Test.xlsx')\nsubmission = pd.read_excel('\/kaggle\/input\/predict-news-category\/Sample_submission.xlsx')\n","b63e3986":"# Importing the Libraries\n\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\n\n# Download the Following Modules once\n\nnltk.download('stopwords')\nnltk.download('wordnet')","b9fa39d5":"print(train.shape)\ntrain.head()","e0fd988b":"print(test.shape)\ntest.head()","82ccad52":"#Printing the group by description of each category\n\ntrain.groupby('SECTION').describe()","d206d265":"# Removing Duplicates to avoid Overfitting\ntrain.drop_duplicates(inplace=True)\n\n#A punctuations string for reference (added other valid characters from the dataset)\n\nall_punctuations = string.punctuation + '\u2018\u2019,:\u201d][],'\n\n#Method to remove punctuation marks from the data\n\ndef punc_remove(raw_text):\n    no_punc = \"\".join([punc for punc in raw_text if punc not in all_punctuations])\n    return no_punc\n\ndef stopword_remover(raw_text):\n    words = raw_text.split()\n    raw_text = \" \".join([i for i in words if i not in stopwords.words('english')])\n    return raw_text\n\nlemmer = nltk.stem.WordNetLemmatizer()\n\ndef lem(words):\n    return \" \".join([lemmer.lemmatize(word,'v') for word in words.split()])\n\n\n# All together \n\ndef text_cleaner(raw):\n    cleaned_text = stopword_remover(punc_remove(raw))\n    return lem(cleaned_text)\n","f21ccb4d":"#Applying the cleaner method to the entire data\n\ntrain['CLEAN_STORY'] = train['STORY'].apply(text_cleaner)","2ae3fd23":"from sklearn.feature_extraction.text import CountVectorizer\n\n\n# Creating a bag of words Dictionery of words from the Data\n\nbow_dictionery = CountVectorizer().fit(train['CLEAN_STORY'])\n\nlen(bow_dictionery.vocabulary_)\n\nbow = bow_dictionery.transform(train['CLEAN_STORY'])\n\nprint(bow.shape)\n\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf = TfidfTransformer().fit(bow)\n\nstorytfidf = tfidf.transform(bow)\n\n","7a1fa3d4":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(storytfidf, train['SECTION'])","f6fa756f":"#cleaning the test data\n\ntest['CLEAN_STORY'] = test['STORY'].apply(text_cleaner)","0e02f0f1":"#Importing the Pipeline module from sklearn\nfrom sklearn.pipeline import Pipeline\n\n#Initializing the pipeline with necessary transformations and the required classifier\npipe = Pipeline([('Bow', CountVectorizer()),\n                ('TfIdf', TfidfTransformer()),\n                ('Classifier',MultinomialNB())])\n\n\n#Fitting the training data to the pipeline\npipe.fit(train['CLEAN_STORY'],train['SECTION'])\n\n#Predicting the SECTION \ntest_pred = pipe.predict(test['CLEAN_STORY'])\n\n#Writing the predictions to an excel sheet\npd.DataFrame(test_pred, columns = ['SECTION']).to_excel('predictions.xlsx')\n","337e7cae":"print(test['CLEAN_STORY'],test_pred)","d90b35ca":"## Data Cleaning","ef3d885e":"* Exploratory Data Analysis: A Simple analysis of Data\n* Data Cleaning\n* Data Preprocessing: Count Vectors and TF-IDF Vectors\n* Training the Classifier\n* Predicting For The Test set\n* Submitting your solution at MachineHack","49a8a7be":"### Training the Classifier","9c9003f5":"### Creating A Pipeline To Pre-Process The Data & Initialise The Classifier","8ca0300b":"### Predicting for the Test Data"}}