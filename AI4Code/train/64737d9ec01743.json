{"cell_type":{"3e87d406":"code","b5e331c5":"code","58646e3c":"code","5a57711c":"code","0d5910d8":"code","3517b614":"code","2007568d":"code","6ca09ad2":"code","9ae7e464":"code","084d1552":"code","c7307419":"code","ae1c56ba":"code","ebf3f931":"code","2e4300ac":"code","52467677":"code","93fb714a":"code","718b41bf":"code","182631a3":"code","eeb54b19":"code","acd27634":"code","81ea7fe6":"code","2c54854f":"code","b2cb7900":"code","6caa0c8f":"code","b6b15426":"code","2605c511":"code","f82b919b":"code","81e9ac68":"code","188f168d":"code","ed6fb858":"code","b90290ae":"code","1e87980e":"code","ca0c50b9":"markdown","acd6ec37":"markdown","12bf2fb5":"markdown","69aac631":"markdown","8e9a3172":"markdown","192ace2a":"markdown","1091a2dc":"markdown","1ba7339c":"markdown","0790eedc":"markdown","25b63309":"markdown","a2303c66":"markdown","b2444875":"markdown","121be785":"markdown","5a3ae43c":"markdown","35755b86":"markdown","0e0c38eb":"markdown","d05fd2ae":"markdown","553c85a3":"markdown","efb4af2a":"markdown","51fcf7c1":"markdown","9a010f4d":"markdown","85d424d5":"markdown","139f2061":"markdown","baae2474":"markdown","50e834de":"markdown","b4624d2b":"markdown","986f1487":"markdown","f1832ba2":"markdown"},"source":{"3e87d406":"import numpy as np\nimport pandas as pd\nimport os","b5e331c5":"\ndata_train = pd.read_csv('..\/input\/titanic\/train.csv',index_col=0)\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv',index_col=0)\n\ndf_train = pd.DataFrame(data = data_train)\ndf_test = pd.DataFrame(data = data_test)\n\ndataset = [df_train, df_test]\n\nprint(df_train.info())\nprint('='*40)\nprint(df_test.info())\nprint('='*40)","58646e3c":"N_JOBS_ = -1\nN_ITER_RAN_SEARCH = 500","5a57711c":"df_train.describe()","0d5910d8":"df_train[df_train['Embarked'].isna()]","3517b614":"# Replace the NaNs in the Embarked column with the mode of the Embarked column.\n\nfor df in dataset:\n    df.loc[df['Embarked'].isna(),'Embarked'] = df['Embarked'].mode()[0]","2007568d":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\n\nscaler = StandardScaler()\nimputer = KNNImputer(n_neighbors=5)\n\nfor df in dataset:\n\n    df_scaled = pd.DataFrame(data = scaler.fit_transform(df.loc(axis=1)['Age','SibSp','Parch','Fare']), \n                                    columns = ['Age','SibSp','Parch','Fare'],\n                                    index = df.index)\n\n    df_new = pd.DataFrame(imputer.fit_transform(df_scaled), \n                          columns = df_scaled.columns, \n                          index = df_scaled.index)\n    \n    # replace unscaled and missing values with the scaled and imputed values\n    df.loc(axis=1)['Age','SibSp','Parch','Fare'] = df_new.loc(axis=1)['Age','SibSp','Parch','Fare']","6ca09ad2":"df_train","9ae7e464":"for df in dataset:\n    # Split the names by commas then by full stop. Then remove spaces to homogenise the format of the titles.\n    df['title'] = df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n    df.drop('Name', axis=1, inplace = True)","084d1552":"df_test","c7307419":"for df in dataset:\n    df['alone'] = ~((df['Parch'] + df['SibSp']) > 0)*1\n","ae1c56ba":"df_test","ebf3f931":"for df in dataset:\n    # Create a list of all other titles besides Mr, Mrs, Master, Miss\n    misc = [x for x in np.unique(df['title']) if x not in (['Mr','Mrs','Miss','Master'])]\n\n    # Replace all of the titles in the misc list of with misc \n    df['title'].replace(misc , 'misc', inplace = True)\n    ","2e4300ac":"# Next we need to define our categorical variables and one-hot encode them\ndf_train = pd.get_dummies(df_train, columns=['Sex', 'Embarked','title'], drop_first=True)\ndf_test = pd.get_dummies(df_test, columns=['Sex', 'Embarked','title'], drop_first=True)","52467677":"df_test","93fb714a":"\n# Remove the original categorical columns that are now replaced by one-hot encoded columns\ntry:\n    df_train = df_train.drop(['Ticket','Cabin'],axis=1)\n    df_test = df_test.drop(['Ticket','Cabin'],axis=1)\nexcept:\n    print(\"Tried to drop something not found in the axis\")\n\n","718b41bf":"from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold, cross_val_score, cross_validate\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=2)\n\nX = df_train.drop(['Survived'],axis=1)\n\ny = df_train['Survived']\n\n# create list of models to evaluate\nmodels = [LogisticRegression(), \n          RandomForestClassifier(), \n          XGBClassifier(verbosity=0, use_label_encoder = False),\n          GradientBoostingClassifier()]\n\nmod_cols = ['Name', \n            'Parameters',\n            'Time',\n            'Train Accuracy',\n            'Test Accuracy']\n\ndf_mod = pd.DataFrame(columns=mod_cols)\n\nfor i,model in enumerate(models):\n    \n    # evaluate model\n    cv_results = cross_validate(model, X, y, cv=cv, return_train_score = True)\n    df_mod.loc[i, 'Parameters'] = str(model.get_params())\n    df_mod.loc[i, 'Name'] = model.__class__.__name__\n    df_mod.loc[i, 'Time'] = cv_results['fit_time'].mean()\n    df_mod.loc[i, 'Train Accuracy'] = cv_results['train_score'].mean()\n    df_mod.loc[i, 'Test Accuracy'] = cv_results['test_score'].mean()\n    \n#     report performance\n    print(f'\\033[1m{model.__class__.__name__}\\033[0m \\t\\t Train Accuracy: {cv_results[\"train_score\"].mean()*100:.3f}\\\n% +\/- {cv_results[\"train_score\"].std()*100:.3f} \\t | \\t Test Accuracy: {cv_results[\"test_score\"].mean()*100:.3f}\\\n% +\/- {cv_results[\"test_score\"].std()*100:.3f}')","182631a3":"subsample = np.linspace(0.1,1.0,10)\nfor i, val in enumerate(subsample):\n    model = GradientBoostingClassifier(subsample=val)\n    new_score = cross_validate(model,X, y, cv=cv, return_train_score = True)\n\n    print(f'\\033[1m subsample = {round(val,2)} \\033[0m \\t\\t Train Accuracy: {new_score[\"train_score\"].mean()*100:.3f}\\\n% +\/- {new_score[\"train_score\"].std()*100:.3f} \\t | \\t Test Accuracy: {new_score[\"test_score\"].mean()*100:.3f}\\\n% +\/- {new_score[\"test_score\"].std()*100:.3f}')","eeb54b19":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import loguniform, uniform, randint\n\n\nspace = dict()\n# Define the distribution over which the random sampling should occur for each of the hyperparameters being searched\nspace['learning_rate'] = uniform(0.001,1)\nspace['min_samples_split'] = randint(2,20)\nspace['n_estimators'] = randint(150,500)\nspace['max_depth'] = randint(2,10)\nspace['subsample'] = uniform(0.001,1-0.001)\nspace['max_features'] = randint(1,len(X.columns))\n\nsearch = RandomizedSearchCV(estimator=GradientBoostingClassifier(), \n                            param_distributions=space, \n                            n_jobs=N_JOBS_, \n                            cv=5, \n                            n_iter=N_ITER_RAN_SEARCH,\n                            scoring='accuracy',\n                            verbose=1)\nresult = search.fit(X,y)\n# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)\n\n","acd27634":"tuned_params = result.best_params_","81ea7fe6":"model = GradientBoostingClassifier(**tuned_params)\ncv_results = cross_validate(model, X, y, cv=cv, return_train_score = True)\n\nprint(f'Train accuracy with tuned hyperparameters: {cv_results[\"train_score\"].mean()}')\nprint(f'Test accuracy with tuned hyperparameters: {cv_results[\"test_score\"].mean()}')","2c54854f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nfont = {'family' : 'DejaVu Sans',\n        'weight' : 'normal',\n        'size'   : 16}\n\nmatplotlib.rc('font', **font)\n\nplt.figure(figsize=(12,8))\nsns.heatmap(X.corr(), annot=False)\nplt.show()","b2cb7900":"# Create a random feature column\nX['random'] = np.random.uniform(low=0.0, high=1.0, size=(X.shape[0]))","6caa0c8f":"# Establish the baseline accuracy\nacc_baseline = df_mod.loc[3,'Test Accuracy']\nparam_baseline = {\n    'learning_rate':0.1,\n    'n_estimators':100,\n    'subsample':1.0,    \n    'min_samples_split':2,        \n    'max_depth':3,\n    'max_features':None\n}","b6b15426":"from sklearn.inspection import permutation_importance\n\n# Initialize a DataFrame to contain the importances of each feature for each model\ndf_imp = pd.DataFrame(index=range(0,len(X.columns)*len(models)), columns=['feature','model','importance'])\n\n# len_feat will allow us to populate the features for each model\nlen_feat = int(len(X.columns))\n\nfor i in range(len(models)):\n    results = permutation_importance(models[i].fit(X, y), X, y, scoring=None, \n                                n_repeats=10, n_jobs=None, \n                                random_state=4)   \n    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'importance'] = (results['importances_mean'])\n    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'model'] = models[i].__class__.__name__\n    df_imp.loc[range(len_feat*i,len_feat*(i+1)),'feature'] = X.columns\n","2605c511":"import plotly.express as px\nfig = px.bar(data_frame=df_imp, \n             x=\"importance\", \n             y=\"feature\", \n             color=\"model\",\n            barmode=\"group\",\n            orientation=\"h\")\nfig.show()","f82b919b":"gbc_imp = df_imp[(df_imp['model']==\"GradientBoostingClassifier\")]\nrand_thresh = gbc_imp[gbc_imp['feature']=='random']['importance'].values[0]\n\nfeatures_to_drop = gbc_imp[gbc_imp['importance']<=rand_thresh].index\ngbc_imp_new = gbc_imp.drop(features_to_drop, axis=0)\n\n# Create a new feature set using the selected features\nX_new = X.loc(axis=1)[gbc_imp_new['feature'].values]\nmodel = GradientBoostingClassifier()\ncv_results = cross_validate(model, X_new, y, cv=cv, return_train_score = True)\n\nprint(f'Baseline test accuracy with all features: {acc_baseline}')\nprint(f'New test accuracy with selected features: {cv_results[\"test_score\"].mean()}')\nprint(f'New train accuracy with selected features: {cv_results[\"train_score\"].mean()}')","81e9ac68":"space = dict()\nspace['learning_rate'] = uniform(0.005,0.1-0.01)\nspace['min_samples_split'] = randint(2,7)\nspace['n_estimators'] = randint(200,400)\nspace['max_depth'] = randint(2,6)\nspace['subsample'] = uniform(0.01,1-0.01)\nspace['max_features'] = randint(1,len(X_new.columns))\n\nsearch = RandomizedSearchCV(estimator=GradientBoostingClassifier(), \n                            param_distributions=space, \n                            n_jobs=N_JOBS_, \n                            cv=5, \n                            n_iter=N_ITER_RAN_SEARCH,\n                            scoring='accuracy')\nresult_feat_select = search.fit(X_new, y)\n# summarize result\nprint('Best Score: %s' % result_feat_select.best_score_)\nprint('Best Hyperparameters: %s' % result_feat_select.best_params_)\n\ntuned_params_feat_select = result_feat_select.best_params_","188f168d":"model = GradientBoostingClassifier(**tuned_params_feat_select)\ncv_results = cross_validate(model, X_new, y, cv=cv, return_train_score = True)\n\nprint(f'New train accuracy with selected features: {cv_results[\"train_score\"].mean()}')\nprint(f'New test accuracy with selected features: {cv_results[\"test_score\"].mean()}')","ed6fb858":"model = GradientBoostingClassifier(**tuned_params).fit(X.drop('random',axis=1), y)\nX_test = df_test\ny_pred = model.predict(X_test)","b90290ae":"submission=pd.DataFrame({\"PassengerId\": df_test.index.values,\"Survived\":y_pred})","1e87980e":" submission.to_csv('submission.csv',index=False)","ca0c50b9":"# This notebook outlines some simple data cleaning, model comparison, and hyperparameter tuning to achieve a top 10% accuracy. <br>\n","acd6ec37":"# Modelling","12bf2fb5":"Looking at the performance of the 4 chosen models will help us decide how to move forward. <br>\n**Logistic Regression:** Has good training accuracy and good test accuracy indicating not much variance. The model has generalised well enough to achieve similar accuracy on unseen data. This model would likely be a fine choice. <br>\n**Random Forest Classifier:** The random forest classifier appears to have overfit the training data indicating we have a variance problem. This is shown by the very high training accuracy and the much lower test accuracy. To improve this feature selection and regularization should be used. <br>\n**XGBoost:** XGBoost has overfit to the training data aswell. Too much variance in the model, regularization and hyperparameter tuning is needed to fix this. <br>\n**Gradient Boosting Classifier:** This has a lesser degree of overfitting than the random forest and XGBoost and it has the advantage of consistently outperforming the other algorithms on the test set although by a marginal amount. The high variance is still an issue that will need to be solved. <br> <br>\n_I am choosing to move forward with the Gradient Boosting Classifier due to its marginally better accuracy and seeing as this notebook is a personal learning experience this is a model where I am not as familar with the hyperparameters so I would like to get to know them a bit better!_","69aac631":"With hyperparameter tuning an improvement in test accuracy of approximately 2% is achieved. Next, determine if selecting specific features will improve test-set accuracy and help the model generalise better to unseen data and reduce overfitting.\n\n## Feature Selection <br>\n\n\n### Begin with feature correlation <br>\nIf any two (or more) features are highly correlated it is more favourable to remove all but one of the correlated features. This avoids potentially harmful bias introduced by heavily correlated features and simplifies the model.\n","8e9a3172":"The test accuracy of the model using only the 4 most important features is negligibly lower than the model with all of the original features, this essentially means the discarded features contributed very little (if anything) to the test accuracy. The train accuracy has also slightly reduced meaning **overfitting has been reduced**. The extra features likely contributed to some overfitting of the training data. This feature selection is a good first step towards a more generalised model. <br>\nRandomized search will now be used to tune the hyperparameters with the selected features.","192ace2a":"#### (Partially) Exploring high variance \/ overfitting\nThe hyperparameter explored below demonstrates the effect it has on variance. By taking only a fraction of the samples the model becomes Stochastic Gradient Boosting. This reduces variance and increases bias. This is shown by the printed output below. ","1091a2dc":"### Set up some parameters to be used throughout","1ba7339c":"### Import libraries","0790eedc":"# Using Randomized search for hyperparameter tuning <br>\nHere to tune the hyperparameters we define the distribution of the hyperparameters and search over the space testing against the scoring metric. Here through hyperparameter tuning we are able to add approximately 2% to the test accuracy through random search of the hyperparameter space.","25b63309":"**No features have a pearson correlation coefficient above 0.9.** This is a reasonable threshold to use for feature selection, however, seeing as none of the features are this highly correlated none will be excluded on the basis of correlation. *Sex_male* and *title_Mr* are understandably correlated, however, both features should be retained given that the difference in the two features is summed up in this: there are male children in the dataset (the majority of whom survived) and there are male adults (the majority of whom died). <br>\n**Addiontally any features less important than a randomly generated feature can be removed.** <br>\nThe reasoning behind this is any feature less important than a random guess doesn't add predictive power to the model.\n","a2303c66":"**Remove features based on their low importance:** (_alone, Embarked_Q, title_Miss, Parch, title_Mrs, title_misc_) will all be removed due to their low importance to the model. <br> Additionally, we may consider removing any features that are less important than the _random_ feature.","b2444875":"### Load the dataset into a dataframe","121be785":"# Conclusion and parting thoughts <br>\n- Feature selection resulted in a slightly less accurate model and is unneccesary in this case. \n- The full feature set with the newly created features (_title, alone, etc._) is enough to score in approximately the top 10% of the competition. \n- A logistic regression model probably would do the job equally as well as the gradient boosting classifier or a random forest. __However, feature engineering seems to be where most accuracy is gained.__ \n- Although random searching of the hyperparameter space can be imprecise, this method is faster than an exhaustive grid search and can yield similar or better results.\n<br> <br>\n\n__Potential improvements:__\n- A potential avenue for improvement would be to use the whole dataset (test and train data) for the data imputation and completing during data cleaning.\n- Different strategies for data imputation should be tested and accuracy compared for each strategy.\n- Creating more new features based on the existing dataset may lead to a more accurate model. \n<br> <br>\n__If you have any feedback or tips for improvement I'd love to hear it \ud83d\ude0a__","5a3ae43c":"## 3. Creating: <br>\nHere we can create additional features from our existing features to see if they provide any additional signals to predict our outcome. Here we will examine the Name on the ticket and extract Title information. We will also examine the number of family members a person had on board. For now we will engineer these two features. This can be improved upon in the next iteration if necessary. <br>\n\n**Create a *title* column using the *name* column.** <br>\n\nWe will do this by taking the name component and separating on the basis of commas followed by full stops. After this we can discard the name column as the meaningful data has been extracted.","35755b86":"## 2: **Completing:**<br> \nHere we address missing or null values in the data. In this dataset there are null\/missing values in the *Age* and *Cabin* columns. There are a variety of strategies to deal with these missing values. One strategy is to delete the observation\/record, however, with a small dataset this option is not ideal, especially if missing values constitute a non-negligible percentage of the overall dataset. The other option then is to impute the data. For categorical data a common strategy is to use the mode. For quantitative data a common approach is to use the mean or median values to impute the data. Here for the age category we will use a kNN-Imputation algorithm. <br>\nIn order to use the KNNImputer function there are a few steps to take to prepare the data. We need to scale any numeric data seeing as kNNImputer uses the euclidian distance to calculate neighbors. \n<br> Before we set up one-hot labels we need to ensure there are no missing values in the Embarked column. We will impute the missing values based on the mode of the other passengers with that passenger class. This is a first approach which we can improve upon in the next iteration if required.","0e0c38eb":"**Create an *alone* column using the *SibSp* and *Parch* columns**","d05fd2ae":"Here I explore 4 different models. Logistic Regression, Random Forest classifier, XGBoost classifier, and Gradient Boosting Classifier. I examine and report the training and testing accuracy for each of these models. In order to fairly compare each model Repeated Stratified K-fold cross validation is performed to ensure that no model benefits from a favourable random shuffling of data or favourable distribution of classes. This is a good method to fairly compare model performance.","553c85a3":"Consolidate titles in the title column to fewer titles for one hot encoding.","efb4af2a":"It can sometimes be beneficial to break up continuous data into ordinal data such as converting the age column into age-groups. However this is mainly appropriate when there are discontinuities at the cut points e.g. {'ice':<0, 'water':0-100, 'steam':>100}. Given the small dataset it is more more beneficial to retain the information of the continuos variable on the first iteration instead of converting it to an arbitrary number of bins. This can be changed in later iterations if appropriate.","51fcf7c1":"### Use permutation_importance to determine the features that contribute to the predictive power of the GradientBoostingClassifier\n**Note: This section is built to examine the feature importances for each of the models, although, the information will only be applied to the GradientBoostingClassifier.** To read more about permutation feature importance see: https:\/\/explained.ai\/rf-importance\/index.html","9a010f4d":"## 4. Converting: <br>\n","85d424d5":"Feature selection ended up being unneccesary here and a slightly better score was achieved using the full feature set. The benefit of the model with fewer features is largely interperability and likely a more generalised model. ","139f2061":"**Convert categorical columns to one-hot encoded columns using dummy variables**","baae2474":"#### Perform feature scaling and impute missing data using the kNNImputer <br>\nHere its fine to use the Standard Scaler seeing as we don't have any major outliers.","50e834de":"**Replace the redundant categorical columns with their one-hot encoded counterparts**","b4624d2b":"### Display the importance of each feature (as determined by the permutation_importance algorithm). <br>\n**Note: These importance scores are relevant only for their respective models. For example, title_Mr is very important in the GradientBoostingClassifier but is only the fourth most important in the RandomForestClassifier.**","986f1487":"# The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting","f1832ba2":"## 1. **Correcting:**<br> \nHere is where we check the data for any obvious errors or incorrect values, e.g. age = 650. A quick and easy way to do this is using the pandas describe() method. "}}