{"cell_type":{"99f15160":"code","3c867d0e":"code","17fa9462":"code","767434b1":"code","e86c0c62":"code","b7c1eb15":"code","9a82824c":"code","c613f6e0":"code","5c456d78":"code","1964f683":"code","d2dcdfe0":"code","5265c2c8":"code","3be8ba45":"code","1cd3a1fe":"code","7a311ab5":"code","4e487e4b":"code","0409bc81":"code","9599e448":"code","387c5bee":"code","ad8bb0c0":"code","412c1257":"code","d64f6255":"code","681b736b":"code","ca7fe3cb":"code","26d1749d":"code","948a3f26":"code","2d294d7b":"code","714f7d25":"code","f73db819":"code","4c8f2ca0":"code","8d36c89b":"code","f66e6369":"markdown","c29e3c5e":"markdown","fa42d172":"markdown"},"source":{"99f15160":"'''Internet on required'''\n## Install and import GAM modelling from pyGAM \n!pip install pygam\nfrom pygam import LinearGAM, s, f\n## read Rdata files\n!pip install pyreadr\nimport pyreadr\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib.colors import ListedColormap\n##Import train test split for generation of X_train, y_train, X_test, y_test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n## Import Linear model from sklearn\nfrom sklearn.linear_model import LinearRegression\n##Import KNN model from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n## Import Random Forest from sklearn\nfrom sklearn.ensemble import RandomForestClassifier\n##Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\n","3c867d0e":"''' Data Download Functions importAURN R to python - conversion of rpy2 pyAURN to pyreadr'''\nfrom urllib.request import urlretrieve\nfrom urllib.error import HTTPError\nimport warnings\n\n\ndef importAURN(site, years):\n    site = site.upper()\n\n    # If a single year is passed then convert to a list with a single value\n    if type(years) is int:\n        years = [years]\n\n    downloaded_data = []\n    df = pd.DataFrame()\n    errors_raised = False\n\n    for year in years:\n        # Generate correct URL and download to a temporary file\n        url = f\"https:\/\/uk-air.defra.gov.uk\/openair\/R_data\/{site}_{year}.RData\"\n\n        try:\n            filename, headers = urlretrieve(url)\n\n            result = pyreadr.read_r(filename)\n\n            # done! let's see what we got\n#             print(result.keys()) # let's check what objects we got\n    \n            df1 = result[f\"{site}_{year}\"] # extract the pandas data frame for object df1\n        \n            df = df.append(df1)\n        \n        except HTTPError:\n            errors_raised = True\n            continue\n#     df.set_index('date')\n    return df\n\n\ndef importMeta():\n    url = \"http:\/\/uk-air.defra.gov.uk\/openair\/R_data\/AURN_metadata.RData\"\n    \n    filename, headers = urlretrieve(url)\n\n    result = pyreadr.read_r(filename)\n    \n    meta = result['AURN_metadata']\n    \n    meta = meta.drop_duplicates(subset=['site_id'])\n    \n    return meta","17fa9462":"# ''' For CSV based data file '''\n\n# # ''' Data Load - from CSV '''\n# # df = pd.read_csv(\"..\/input\/leamington-aurn-air-quality-data\/LEAR.csv\") \n# # df.head()\n# df.columns\n\n# '''Data Clean'''\n# ## drop unecessary variables\n# # df.drop(['Unnamed: 0'], inplace = True, axis = 1)\n# df.drop(['date'], inplace = True, axis = 1)\n# # df.drop(['latitude'], inplace = True, axis = 1)\n# # df.drop(['longitude'], inplace = True, axis = 1)\n# # df.drop(['site.type'], inplace = True, axis = 1)\n# df.drop(['site'], inplace = True, axis = 1)\n# df.drop(['code'], inplace = True, axis = 1)\n# df.head()\n\n# print(df.shape)\n# ## Calculate amount of NA data in each column\n# print((df.isna().sum())\/len(df) * 100)\n\n# ## clean data set dropping the >50 NA columns\n# df.drop(['NV10'], inplace = True, axis = 1)\n# df.drop(['V10'], inplace = True, axis = 1)\n# df.drop(['NV2.5'], inplace = True, axis = 1)\n# df.drop(['V2.5'], inplace = True, axis = 1)\n# df.drop(['AT10'], inplace = True, axis = 1)\n# df.drop(['AP10'], inplace = True, axis = 1)\n# df.drop(['AT25'], inplace = True, axis = 1)\n# df.drop(['AP25'], inplace = True, axis = 1)\n# df.drop(['RAWPM25'], inplace = True, axis = 1)\n\n# ## drop remaining NA rows\n# df_clean = df.dropna()\n\n# ## add classification label (0 = NO2 below 40ug\/m3, 1 = NO2 above 40ug\/m3)\n# df_clean['label'] = np.where(df_clean['NO2'] >= 40, 1, 0)","767434b1":"''' Data Load - from importAURN function (live AURN air quality data)'''\n##import AURN (site code, year (single value or list))\ndf = importAURN(\"LEAR\", 2019)\n## Import meta data about all AURN stations operating in the UK. \nmeta = importMeta()","e86c0c62":"yearsaurn = list(range(2015,2020))","b7c1eb15":"meta.columns","9a82824c":"# remove stations that have been retired. \nmeta_clean = meta[meta['date_ended'].isna()]\n\n#create empty site list\nsites = []\n# populate site list with currently active AURN sites\nfor i in meta_clean['site_id']:\n    sites.append(i)\n","c613f6e0":"#create empty dictionary   \naurndict = {}\n\n# for each station run importAURN and filled against dictionary key for station. - takes a while\nfor x in sites:\n    aurndict[\"{0}\".format(x)]= importAURN(x, yearsaurn)","5c456d78":"#example of the AURN station dataframe held in a dictionary value based on station cod\naurndict['ABD7']","1964f683":"# in each aurn dataframe set the date as the index\nfor i in aurndict.keys():\n    aurndict[i].set_index('date', inplace = True)","d2dcdfe0":"#create dictionary for column lengths\ncolumn_lengths = {}\nfor x in aurndict.keys():\n    column_lengths[\"{0}\".format(x)] = len(aurndict[x].columns)\n    \n# AURN station with minimum pollutant types\nmin(column_lengths, key=column_lengths.get)","5265c2c8":"# AURN stations without NO2 measurements\nno2 = []\nfor i in aurndict.keys():\n    if \"NO2\" not in aurndict[i].columns:\n        no2.append(i)\n        \nno2\n\n#remove stations without no2 measurements\nfor x in no2: \n    del aurndict[x]","3be8ba45":"# Merge all AURN station dataframes into single dataframe\nmerged_df = pd.concat(aurndict.values(), axis = 0, join='outer', ignore_index=False)","1cd3a1fe":"'''Data Clean'''\nmerged_df.drop(['site'], inplace = True, axis = 1)\nmerged_df.drop(['code'], inplace = True, axis = 1)","7a311ab5":"## print shape of dataframe\nprint(merged_df.shape)\n## Calculate amount of NA data in each column\nprint((merged_df.isna().sum())\/len(merged_df) * 100)","4e487e4b":"((merged_df.isna().sum())\/len(merged_df) * 100) > 70\n","0409bc81":"merged_df.columns","9599e448":"columns_low_na = ['temp','wd','ws','NO2','NO','NOXasNO2','NV10','NV2.5','O3','PM10','PM2.5','V10','V2.5']","387c5bee":"## select dataframe columns dropping the >70 NA columns\ndf_lowna = merged_df[columns_low_na]","ad8bb0c0":"print((df_lowna.isna().sum())\/len(df_lowna) * 100)","412c1257":"## drop remaining NA rows\ndf_clean = df_lowna.dropna()","d64f6255":"df_clean.shape","681b736b":"df_clean.head()","ca7fe3cb":"''' Linear Model '''\n\n## Dataset for linear regression\n## features read from columns\nfeatures_linear = df_clean.columns\n## read features\nfeatures_linear\n## drop the no2 feature to avoid overfit to target variable, drop nox feature as its also 1:1 relationship to no2.\nfeatures_linear = ['temp', 'wd', 'ws','NV10', 'NV2.5', 'O3','PM10', 'PM2.5', 'V10', 'V2.5','NO']\nfeatures = []\n\n## loop through adding each variable in order to the model\nfor i in features_linear:\n    features.append(i)\n    X_linear = df_clean[features]\n    y_linear = df_clean['NO2']\n\n    X_train_linear, X_test_linear, y_train_linear, y_test_linear = train_test_split(X_linear, y_linear, test_size=0.20, random_state=42)\n\n    lm_model = LinearRegression()\n\n    lm_model.fit(X_train_linear,y_train_linear)\n\n    y_pred_linear = lm_model.predict(X_test_linear)\n\n    print(\"Accuracy for\",features, len(features),\":\",lm_model.score(X_test_linear, y_test_linear))\n\nfor idx, col_name in enumerate(X_train_linear.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, lm_model.coef_[idx]))","26d1749d":"'''Classification Models Below'''\n## add classification label (0 = NO2 below 40ug\/m3, 1 = NO2 above 40ug\/m3)\ndf_clean['label'] = np.where(df_clean['NO2'] >= 40, 1, 0)","948a3f26":"''' Random Forest '''\n\nfeatures = df_clean.columns\nfeatures_rf = ['temp', 'wd', 'ws','NV10', 'NV2.5', 'O3','PM10', 'PM2.5', 'V10', 'V2.5']\n\n\ny_rf = df_clean['label']\nX_rf = df_clean[features_rf]\n\nnormalized_X_rf = preprocessing.normalize(X_rf)\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(normalized_X_rf, y_rf, test_size=0.20, random_state=42)\n\nrf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1, random_state=1)\n\nrf.fit(X_train_rf, y_train_rf)\n\npred_rf = rf.predict(X_test_rf)\n\nprint(' random forest Val accuracy : ', accuracy_score(pred_rf, y_test_rf))\nprint(classification_report(y_test_rf, pred_rf, target_names = ['Not Exceedance', 'Exceedance']))\n","2d294d7b":"## features name list for classification\nfeatures_knn = df_clean.columns\nprint(features_knn)","714f7d25":"'''KNN Model'''\n## features name list for classification\nfeatures_knn = df_clean.columns\n## Standard list of features from AURN output.\nfeatures_knn = ['temp', 'wd', 'ws','NV10', 'NV2.5', 'O3','PM10', 'PM2.5', 'V10', 'V2.5']\nfeatures = []\n## loop through variables running the model then adding the next variable and rerunning the model. \nfor i in features_knn:\n    features.append(i)\n    ## set X and y. X = data without label, y = label\n    X_knn = df_clean[features]\n    y_knn = df_clean['label']\n\n    ## normalise the features to scale  0 to 1\n    normalized_X = preprocessing.normalize(X_knn)\n    # split into train and test data\n    X_train, X_test, y_train, y_test = train_test_split(normalized_X, y_knn, test_size=0.20, random_state=42)\n    # KNN model creation and fit\n    knn = KNeighborsClassifier(n_neighbors=14)\n    knn.fit(X_train,y_train)\n    y_pred_knn = knn.predict(X_test)\n\n    # Model Accuracy, how often is the classifier correct?\n    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_knn))\n    print(classification_report(y_test, y_pred_knn, target_names = ['Not Exceedance', 'Exceedance']))","f73db819":"'''Optimising K: Long run time on CPU'''\n\n## creating list of K for KNN\nk_list = list(range(1,50,2))\n## creating list of cv scores\ncv_scores = []\n\n## perform 10-fold cross validation\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \nMSE = [1 - x for x in cv_scores]\n\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\nplt.xlabel('Number of Neighbors K', fontsize=15)\nplt.ylabel('Misclassification Error', fontsize=15)\nsns.set_style(\"whitegrid\")\nplt.plot(k_list, MSE)\n\nplt.show()\n\n## Print best k\nbest_k = k_list[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d.\" % best_k)","4c8f2ca0":"df_clean.columns","8d36c89b":"'''GAM Model: Long run time on CPU'''\n\n## features minus no2 target variable\nfeatures_gam = features_linear\nfeatures_two = []\n\nfor i in features_gam:\n    features_two.append(i)\n    X_linear_gam = df_clean[features].to_numpy()\n    y_linear_gam = df_clean['NO2'].to_numpy()\n\n    X_train_linear_gam, X_test_linear_gam, y_train_linear_gam, y_test_linear_gam = train_test_split(X_linear_gam, y_linear_gam, test_size=0.20, random_state=42)\n\n    gam = LinearGAM().gridsearch(X_train_linear_gam, y_train_linear_gam)\n\n    print(len(features),gam.statistics_['pseudo_r2'])\n    print(gam.statistics_['GCV'])\n   ","f66e6369":"References: \n\n*     KNN:\n\n        https:\/\/kevinzakka.github.io\/2016\/07\/13\/k-nearest-neighbor\/\n        https:\/\/www.datacamp.com\/community\/tutorials\/k-nearest-neighbor-classification-scikit-learn\n        https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html\n*     Linear Regression: \n\n        http:\/\/benalexkeen.com\/linear-regression-in-python-using-scikit-learn\/\n*     GAM:\n\n        https:\/\/pygam.readthedocs.io\/en\/latest\/\n*     Random Forest:\n\n        https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n*     importAURN function (merge of PyAURN and pyreadr): \n        https:\/\/github.com\/robintw\/PyAURN\/blob\/master\/importAURN.py\n        https:\/\/github.com\/ofajardo\/pyreadr","c29e3c5e":"To do: \n\n* Feature engineering - explore partial dependencies of features \n* Investigate alternative data sources for more metrology features - NOAA?\n* Investigate sources of Traffic Data feature to incorporate.\n* Investigate affect of seasonality of air quality data on the models. \n","fa42d172":"The purpose of this notebook is to assess different modelling methodologies in regards to air quality data - This notebook will be iterative, and will develop as my own personal understanding of ML techniques and implementation progresses. "}}