{"cell_type":{"7cce3788":"code","930fcd18":"code","70db42e6":"code","0d6f3650":"code","27e00a8b":"code","0963cb15":"code","03ce358b":"code","f77fcf9c":"code","8441b756":"code","23688eaf":"code","63275497":"code","31bd814d":"code","2a467532":"code","5df1db77":"code","25361e76":"code","f31b4820":"code","2834232a":"code","cae9934f":"code","b3619fdc":"code","a7f72b55":"code","27bf0462":"code","f87108f6":"code","ffa7507d":"code","89983a81":"code","4df902c5":"code","63442eb1":"code","3d13d0be":"code","50f5fba8":"code","18be4dcf":"code","1ddbd516":"code","6a1dcbf4":"code","26266e47":"code","89bb10ad":"code","c9fd7f40":"code","6ea7f0e3":"code","723275c8":"code","cf805925":"code","358a6bbf":"code","eff9b360":"code","8e7a2912":"code","1db3dbbc":"code","864f6877":"code","972c88b7":"code","6e4069a5":"code","e505fbe8":"code","1e42feb1":"code","35fb846f":"code","fa8253e9":"code","1100232a":"code","3915c9e0":"code","ba01b171":"code","9dbbf777":"code","c790c5c4":"markdown","4fbdc708":"markdown","2573a06c":"markdown","b66aebf8":"markdown","ceda0d05":"markdown","c72ab337":"markdown","a01f50df":"markdown","e759d117":"markdown","3a3ba679":"markdown","9df92cf5":"markdown","d1315314":"markdown","cbe7d385":"markdown","443e106c":"markdown","2735d071":"markdown","3dfa1ad5":"markdown","8f553c5a":"markdown","4543986d":"markdown","104f5a26":"markdown","353847ae":"markdown","0191bd79":"markdown","d81f0af0":"markdown","c2c87849":"markdown","0514e6e7":"markdown","dbfc3c18":"markdown","4f10ce59":"markdown","c419a3ae":"markdown","d889e01f":"markdown","5b2de61a":"markdown","a88ba0ba":"markdown","ba304f71":"markdown","f5bc2a1b":"markdown","7e7fbecd":"markdown","ecdee723":"markdown","ea95cfdc":"markdown","53224e1f":"markdown","25175d4e":"markdown","3e28d8b1":"markdown","ab07eb09":"markdown","0d7170b2":"markdown","0d7747b8":"markdown","a87a8707":"markdown","94ea7140":"markdown","87bca760":"markdown","2f724c5d":"markdown"},"source":{"7cce3788":"from __future__ import print_function, division\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torch.optim as optim\nimport seaborn as sns\nimport sklearn.metrics\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\nfrom torchvision import transforms, utils, models\nfrom tqdm import tqdm\nfrom IPython.display import Image as Img\nfrom IPython.display import display\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport time\nfrom time import sleep\nimport os\n\nfrom IPython.display import display","930fcd18":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70db42e6":"TRAIN_IMG_PATH = \"\/kaggle\/input\/polytech-ds-2020\/ip102_polytech_v2\/ip102_polytech\/training_images\/\"\nTEST_IMG_PATH = \"\/kaggle\/input\/polytech-ds-2020\/ip102_polytech_v2\/ip102_polytech\/test_images\/\"\nLABELS_TXT_PATH = \"\/kaggle\/input\/polytech-ds-2020\/classes.txt\"\nSAMPLE_SUB_PATH = \"\/kaggle\/input\/polytech-ds-2020\/ip102_polytech_v2\/ip102_polytech\/sample_submission.csv\"\nTRAINING_LABELS_PATH = \"\/kaggle\/input\/polytech-ds-2020\/ip102_polytech_v2\/ip102_polytech\/train.txt\"\nVAL_LABELS_PATH =  \"\/kaggle\/input\/polytech-ds-2020\/ip102_polytech_v2\/ip102_polytech\/val.txt\"","0d6f3650":"class InsectDataset(Dataset):\n\n    def __init__(self, img_dir, dataframe, transform=None):\n        self.labels_frame = dataframe\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, self.labels_frame.id[idx])\n        image = Image.open(img_name).convert('RGB')\n        label = self.labels_frame.target[idx]\n        \n        if self.transform:\n          image = self.transform(image)\n          \n        return [image, label] ","27e00a8b":"def plot_image_distribution(dataframe):\n  count_df = dataframe['target'].value_counts().to_frame()\n  count_df['id'] = count_df.index\n  count_df.rename(columns={'target':'count'}, inplace=True)\n\n  dims = (22, 10)\n  fig, ax = plt.subplots(figsize=dims)\n\n  # seaborn histogram\n  sns.barplot(data=count_df, x='id', y='count', ax=ax)\n  sns.color_palette(\"pastel\")\n  # Add labels\n  plt.title('Distribution of images per class',fontweight='bold', fontsize=16)\n  plt.xlabel('Class',fontweight='bold', fontsize=12)\n  plt.xticks(rotation=90)\n  plt.ylabel('Number of images', fontweight='bold', fontsize=12)","0963cb15":"def image_size_distribution(dataframe):\n  shapes = []\n  for index, row in dataframe.iterrows():\n    img_name = os.path.join(TRAIN_IMG_PATH, row['id'])\n    image = Image.open(img_name)\n    shapes.append(image.size)\n\n  df_shapes = pd.DataFrame({'Shapes': shapes})\n  counts = df_shapes['Shapes'].value_counts()\n\n  plt.figure(figsize=(14, 10))\n  sns.barplot(x=counts.head(15).index, y=counts.head(15).values)\n\n  plt.title('Distribution of images per size',fontweight='bold', fontsize=16)\n  plt.xlabel('Size',fontweight='bold', fontsize=12)\n  plt.xticks(rotation=90)\n  plt.ylabel('Number of images', fontweight='bold', fontsize=12)\n\n  return counts","03ce358b":"def plot_distrib_in_n_batches(batches=5):\n  df = pd.DataFrame()\n  labels = []\n\n  for i in range(batches):\n    img, label = next(iter(train_loader))\n    for i in range(31):\n      labels.append(label[i].item())\n\n  df[\"count\"] = labels\n\n  count_df = df[\"count\"].value_counts().to_frame()\n  count_df['id'] = count_df.index\n\n  dims = (22, 10)\n  fig, ax = plt.subplots(figsize=dims)\n\n  # seaborn histogram\n  sns.barplot(data=count_df, x='id', y='count', ax=ax)\n  sns.color_palette(\"pastel\")\n  # Add labels\n  plt.title('Distribution of images per class',fontweight='bold', fontsize=16)\n  plt.xlabel('Class',fontweight='bold', fontsize=12)\n  plt.xticks(rotation=90)\n  plt.ylabel('Number of images', fontweight='bold', fontsize=12)","f77fcf9c":"# a little helper function do directly display a Tensor\ndef display_tensor(t):\n  trans = transforms.ToPILImage()\n  display(trans(t))","8441b756":"def get_weights(dataframe, num_classes):\n    #Counting number of images per class                        \n    count = [0] * num_classes                                                      \n    for idx, row in dataframe.iterrows():     \n      count[row['target']] += 1\n\n    #Computing weight of each class\n    weight_per_class = [0.] * num_classes                                      \n    N = float(sum(count))                                                   \n    for i in range(num_classes):                                                   \n      weight_per_class[i] = N\/float(count[i])\n\n    #Assigning weight to each image                                     \n    weight = [0] * len(dataframe)                                              \n    for idx, row in dataframe.iterrows():     \n      weight[idx] = weight_per_class[row['target']]\n\n    return weight","23688eaf":"def train_model(model, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = model.state_dict()\n    best_acc = 0.0\n    all_epoch_loss, all_epoch_acc, all_epoch_val_loss, all_epoch_val_acc = [], [], [], []\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:     \n            since_epoch = time.time()\n            if phase == 'train':\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n    \n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for data in tqdm(dataloaders[phase]):\n                # get the inputs\n                inputs, labels = data\n\n                inputs = Variable(inputs.type(Tensor))\n                labels = Variable(labels.type(LongTensor))\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                outputs = model(inputs)\n                _, preds = torch.max(outputs.data, 1)\n                loss = criterion(outputs, labels)\n\n                # backward + optimize only if in training phase\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                    \n                # statistics\n                running_loss += loss.data\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = torch.true_divide(running_loss,len(datasets[phase]))\n            epoch_acc = torch.true_divide(running_corrects,len(datasets[phase]))\n\n            time_elapsed_epoch = time.time() - since_epoch\n            print('{} Loss: {:.4f} Acc: {:.4f} in {:.0f}m {:.0f}s'.format(\n                phase, epoch_loss, epoch_acc, time_elapsed_epoch \/\/ 60, time_elapsed_epoch % 60))\n            \n            if phase == 'train':\n                all_epoch_loss.append(epoch_loss)\n                all_epoch_acc.append(epoch_acc)\n            else:\n                all_epoch_val_loss.append(epoch_loss)\n                all_epoch_val_acc.append(epoch_acc)\n            \n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = model.state_dict()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    #Plotting accuracy\/loss curves\n    x = np.arange(num_epochs)\n    plt.figure()\n    plt.plot(x, all_epoch_acc, label=\"training\")\n    plt.plot(x, all_epoch_val_acc, label=\"validation\")\n    plt.title('Accuracy curve',fontweight='bold', fontsize=12)\n    plt.legend()\n\n    plt.figure()\n    plt.plot(x, all_epoch_loss, label=\"training\")\n    plt.plot(x, all_epoch_val_loss, label=\"validation\")\n    plt.title('Loss curve',fontweight='bold', fontsize=12)\n    plt.legend()\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","63275497":"def show_combined_val_accuracy(model,model2):\n  model.eval()\n  model2.eval()\n  running_val_accuracy = 0 \n  for i, batch in enumerate(tqdm(val_loader)):\n          with torch.no_grad():\n              x = batch[0]\n              labels = batch[1]\n              x = x.cuda()\n              labels = labels.cuda()\n              \n              y1 = model(x)\n              y2 = model2(x)\n              y = y1 + y2\n              if int(y.max(1)[1]) == int(labels):\n                running_val_accuracy += 1\n  print(running_val_accuracy \/ len(val_ds))","31bd814d":"def test_model(model,model2):\n    since = time.time()\n\n    if torch.cuda.is_available():\n        model.cuda()\n        model2.cuda()\n    model.eval()\n    model2.eval()\n    preds = []\n    \n    for i, (inputs,targets) in tqdm(enumerate(sub_loader)):\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            pred1 = model(inputs).data.cuda().cpu().numpy().copy()\n            pred2 = model2(inputs).data.cuda().cpu().numpy().copy()\n            pred = pred1+pred2\n        else:\n            pred1 = model(inputs).data.numpy().copy()\n            pred2 = model2(inputs).data.numpy().copy()\n            pred = pred1+pred2\n        preds.append(pred)\n            \n    time_elapsed = time.time() - since\n    print('Run complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed \/\/ 60, time_elapsed % 60))\n    \n    return np.concatenate(preds)","2a467532":"def get_confusion_matrix(model, dataloader):\n    since = time.time()\n\n    if torch.cuda.is_available():\n        model.cuda()\n    model.eval()\n    preds = []\n    trues = []\n\n    for i, (inputs,targets) in tqdm(enumerate(dataloader)):\n        if torch.cuda.is_available():\n            inputs = inputs.cuda()\n            pred = model(inputs).data.cuda().cpu().numpy().copy()\n        else:\n            pred = model(inputs).data.numpy().copy()\n            \n        preds.append(pred)\n        trues.append(targets)\n            \n    time_elapsed = time.time() - since\n    print('Run complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed \/\/ 60, time_elapsed % 60))\n    \n    y_pred = np.concatenate(preds).argmax(1)\n    y_true = np.concatenate(trues)\n    \n    cf_matrix = sklearn.metrics.confusion_matrix(y_true,y_pred)\n    \n    return cf_matrix","5df1db77":"training_df = pd.read_csv(TRAINING_LABELS_PATH, sep=\" \", header=None)\ntraining_df.columns = [\"id\", \"target\"]\ntraining_df","25361e76":"val_df = pd.read_csv(VAL_LABELS_PATH, sep=\" \", header=None)\nval_df.columns = [\"id\", \"target\"]\nval_df","f31b4820":"imgs=[Img(filename=TRAIN_IMG_PATH+'0000{}.jpg'.format((i+1)*3)) for i in range(3)]\ndisplay(*imgs)","2834232a":"img=[Image.open(TRAIN_IMG_PATH+\"00002.jpg\"), Image.open(TRAIN_IMG_PATH+\"00018.jpg\"), Image.open(TRAIN_IMG_PATH+\"00021.jpg\")]\ndisplay(*img)","cae9934f":"df_70 = training_df.loc[training_df['target'] == 70]\nfor index, row in df_70[:4].iterrows():\n    im = Image.open(TRAIN_IMG_PATH+row['id'])\n    plt.imshow(im)\n    plt.axis('off')\n    plt.show()\n    plt.clf() \n    im.close()\n    sleep(2)","b3619fdc":"img=[Image.open(TRAIN_IMG_PATH+\"00033.jpg\"), Image.open(TRAIN_IMG_PATH+\"00042.jpg\")]","a7f72b55":"plot_image_distribution(training_df)","27bf0462":"plot_image_distribution(val_df)","f87108f6":"count_df = training_df['target'].value_counts().to_frame()\ncount_df['id'] = count_df.index\ncount_df.rename(columns={'target':'count'}, inplace=True)\nmean = count_df['count'].mean()\nprint(\"Mean of number of images per class:\", mean)\n","ffa7507d":"# image_size_distribution(training_df)","89983a81":"#num_classes = 102\n#weights = get_weights(training_df,num_classes)\n#weights = torch.DoubleTensor(weights)\n# train_sampler = WeightedRandomSampler(weights, len(weights), replacement = True)                     ","4df902c5":"#num_classes = 102\n#weights = get_weights(val_df,num_classes)\n#weights = torch.DoubleTensor(weights)\n# val_sampler = WeightedRandomSampler(weights, len(weights), replacement = True)","63442eb1":"augm_transform = transforms.Compose([\n        transforms.Resize((224,224)),       \n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n\nsimple_transform = transforms.Compose([\n        transforms.Resize((224,224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n\ntransform = {\"augm\": augm_transform, \"simple\": simple_transform}\n","3d13d0be":" train_ds = InsectDataset(TRAIN_IMG_PATH, training_df, augm_transform)\n val_ds = InsectDataset(TRAIN_IMG_PATH, val_df, augm_transform)\n\n datasets = {\"train\": train_ds, \"val\": val_ds}","50f5fba8":"print(\"Number of images in training dataset:\", len(train_ds))\nprint(\"Number of images in validation dataset:\", len(val_ds))","18be4dcf":"print(\"Shape of image is: \", train_ds[6][0].shape)","1ddbd516":"img15 = Img(filename=TRAIN_IMG_PATH+'00015.jpg')\ndisplay(img15)","6a1dcbf4":"for i in range(5):\n  sample = train_ds[6][0]\n  display_tensor(sample)","26266e47":"#With Balancing\n#train_loader = DataLoader(train_ds, batch_size=32, sampler=train_sampler, num_workers=4)\n#val_loader = DataLoader(val_ds, batch_size=32, sampler=val_sampler, num_workers=4)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=True, num_workers=4)\n\ndataloaders = {\"train\": train_loader, \"val\": val_loader}\n","89bb10ad":"#Uncomment if balancing is applied\n#plot_distrib_in_n_batches(32)","c9fd7f40":"if torch.cuda.is_available():\n    use_gpu = True\n    print(\"Using GPU\")\nelse:\n    use_gpu = False\nFloatTensor = torch.cuda.FloatTensor if use_gpu else torch.FloatTensor\nLongTensor = torch.cuda.LongTensor if use_gpu else torch.LongTensor\nByteTensor = torch.cuda.ByteTensor if use_gpu else torch.ByteTensor\nTensor = FloatTensor","6ea7f0e3":"!nvidia-smi -L","723275c8":"model = models.resnet152(pretrained=True)","cf805925":"num_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 120)\n\nif torch.cuda.device_count() > 1 and multiGPU:\n  print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n  model = nn.DataParallel(model)\n\nif use_gpu:\n   model.cuda()\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nepochs=5","358a6bbf":"model = train_model(model, criterion, optimizer_ft, num_epochs=epochs)","eff9b360":"# torch.save(model,\"\/content\/saved_model\")","8e7a2912":"#Kaggle\n#model = torch.load('\/kaggle\/input\/trainedmodel\/model')","1db3dbbc":"# Colab\nmodel2 = torch.load('..\/input\/resnext-101\/model.pt')","864f6877":"val_loader = DataLoader(val_ds, batch_size=1, num_workers=2)","972c88b7":"show_combined_val_accuracy(model,model2)","6e4069a5":"sub_df = pd.read_csv(SAMPLE_SUB_PATH)\nsub_df.columns = [\"id\", \"target\"]\nsub_df","e505fbe8":"sub_ds = InsectDataset(TEST_IMG_PATH, sub_df, simple_transform)\n\nsub_loader = DataLoader(sub_ds, batch_size=32,\n                        shuffle=False, num_workers=4)","1e42feb1":"preds = test_model(model,model2)","35fb846f":"print(preds.argmax(1))","fa8253e9":"distrib_df = sub_df.copy()\ndistrib_df = distrib_df.drop(['target'], axis=1)\ndistrib_df['target'] = preds.argmax(1)\ndistrib_df.head()","1100232a":"plot_image_distribution(distrib_df)","3915c9e0":"val_ds = InsectDataset(TRAIN_IMG_PATH, val_df, simple_transform)\neval_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4)\ncf_matrix = get_confusion_matrix(model, eval_loader)\n\nf,ax = plt.subplots(figsize=(115,115))\nres = sns.heatmap(cf_matrix, annot=True, annot_kws={\"size\":30}, linewidths=0.8, ax=ax)\nres.set_xticklabels(res.get_xmajorticklabels(), fontsize=30)\nres.set_yticklabels(res.get_xmajorticklabels(), fontsize=30)","ba01b171":"output_df = sub_df.copy()\noutput_df['class'] = preds.argmax(1)\noutput_df = output_df.drop(['target'], axis=1)\noutput_df = output_df.rename({'id':'name'}, axis=1)\noutput_df.head()","9dbbf777":"output_df.to_csv(\"submission.csv\", index=False)","c790c5c4":"### 3.3.2 - Distribution of images per size","4fbdc708":"Notes:\n\n\n*   The colors of the image are distorted since we applied normalization on it in order to make training faster.\n*   The image is randomly rotated horizontally with a 50% probabilty of rotation for data augmentation purposes\n","2573a06c":"## 2.1 - Definition of own Dataset","b66aebf8":"3.   Some images are **repeated**","ceda0d05":"Let's check how the transformations apply on our data.","c72ab337":"As it can be observed, **the mean of the number of images per class is less than 450** since most of the classes contain less than 200 images. For a good training dataset we should have around 2000 of images per class.","a01f50df":"## 4.4 - Definition of Data Loaders","e759d117":"## 2.2 - Definition of functions","3a3ba679":"# 6 - Testing","9df92cf5":"\n\n1.   There are images which **does not contain insects**\n\n","d1315314":"## 6.1 - Test model","cbe7d385":"## 3.1 - Creation of training and validation dataframes","443e106c":"### 2.2.3 - Training function","2735d071":"# 1 - Import libraries and files path initialization\n","3dfa1ad5":"### 3.3.1 - Distribution of images per class","8f553c5a":"## 6.2 - Test set distribution","4543986d":"## 3.2 - Visualizing content of dataset","104f5a26":"This samplers were used for applying balancing techniques on the dataset, but they are not used in the final model.","353847ae":"2.   Some images between one class represent **different insects**. Below one can find the 1st 4 images of the class 70:","0191bd79":"### 4.3.1 - Checking transformed images","d81f0af0":"Original image","c2c87849":"## 4.1 - Creating sampler to balance dataset","0514e6e7":"### 2.2.4 - Testing function","dbfc3c18":"# 3 - Data exploration","4f10ce59":"# Project summary\n\n\nThe project summary gives on overall view of the methods and the development process for the finalised model. Additionally, the summary entails the intermediate steps of the progress toward final accuracy. \n\n## Data analysis\nThe dataset was explored to get a better overall undertanding of the data that was used for analysis.\n\nHence, multiple problems from the dataset were outlined accordingly, that could pose a negative effect for the future model performance:\n\n1. Some images that don't contain insects. (section 3.2)\n2. Some images between one class represent different insects. (section 3.2)\n3. Some images are repeated. (section 3.2)\n\nSubsequently, the distribution of images per class which showed a highly unbalanced dataset (section 3.3.1) and in the light of these results, different techniques were applied to solve the issue:\n\n*   Implementation of splitting the dataset (0.9 training, 0.1 validation). After testing the model, there weren't significant changes in final the performance.\n\n*   The application of different balancing techniques to solve the matter:\n\n  *   1) Attempt was to oversample the classes with less number of images and \nat the same time undersampling the few classes with a high number of images. Hence, in order to avoid overfitting different data augmentation techniques were applied to the oversampling images. Finally, this method was deemed implausible since it overfits the model.\n\n  *   2) Attempt of balancing the dataset was through using [WeightedRandomSampler](https:\/\/pytorch.org\/docs\/stable\/data.htmin the dataloader. In order to accomplish it, a weight was assigned for each class. The dataloader would use samples of the classes with higher weights (the classes with less images). After testing this method, it was discarded since it gave worse results in terms of validation accuracy.\n\n## Why was the balancing technique unsuccesful with the model?\n\nThe test set was also unbalanced. It followed the same distribution as the training and validation sets, meaning there is a large dispersion between the number of images of different classes. (Section 6.2)\n\nTherefore, the overall accuracy is greater when using an unbalanced dataset since the most tested classes are the ones that the network had trained the most.\n\nSubsequently, the unbalanced models that were tested presented a higher overall accuracy than the balanced models suggesting that the test set used in Kaggle follows the same distribution pattern as the dataset used during training.\n\n## Data pre-processing\n\nData pre-processing was applied to some data refining over the dataset images as it can be seen in the data transformers (Section 4.2). The changes over the images are the following:\n\n*   Resize of images to 224x224 to suit the Resnet\/Resnext model input\n*   Application of data augmentation techniques on the images to increase the variety of images. Many different  combinations of augmentations were tried and only \"RandomHorizontalFlip\" was kept, since it yielded the best results.\n* Image to tensor\n* Normalise the images such that all the features would be on the same scale which speeds up the training phase.\n\n\n## Model choice and hyperparameters\nTo reach a higher accuracy, the ensemble learning technique was applied with two models. The two models used for this technique were Resnet152 and Resnext101 architectures. \n\nResnet152 and Resnext101 architectures offer great performance on multiclass image classification with deep convulational layers, which would allow the training of the model in shorter time durations (approximately 1 hour). Both models have low top-5 errors in the learning rate, which make both architectures efficient.    \n\nIt's worth mentioning, by using more models with the ensemble learning technique the overall accuracy could be even higher. Also, two models were chosen in order to save GPU time. All in all, the sum of the two probability vectors have the higher value of the overall combined vector. Thus, the model improved by 2,7% with the ensemble learning technique by combining Resnet152 and Resnext101. \n\nBelow are the model hyperparameters that were refined after various iterations:\n\n\nHyperparameters for Resnet152:\n- Criterion: Cross Entropy Loss \n\n- Optimizer: Stochastic Gradient Descent (Learning rate: 0.001 & Momentum: 0.9)\n\n- Epochs: 5\n\n- Batch size: 32\n \nfor Resnext101:\n- Criterion: Cross Entropy Loss \n\n- Optimizer: Stochastic Gradient Descent (Learning rate: 0.001 & Momentum: 0.9)\n\n- Epochs: 10\n\n- Scheduler: step 4 \/ gamma 0.1\n\n- Batch size: 32","c419a3ae":"### 2.2.2 - Computing weights of each class","d889e01f":"We define the following 4 functions that we will later use to display de different distributions (Images per class, per size, per class and up to 5 batches, and to display the tensors)","5b2de61a":"\n\n## 3.3 - Exploring dataset distribution\n\n\n","a88ba0ba":"## 6.3 - Confusion matrix","ba304f71":"Furthermore, we can observe that the images sizes are very heterogeneus which might imply some problems when resizing and croping images","f5bc2a1b":"# 4 - Data pre-processing","7e7fbecd":"### 2.2.1 - Plotting\/Displaying functions","ecdee723":"## 5.1 - Training parameters","ea95cfdc":"## 1.1 - Kaggle","53224e1f":"# 7 - Submission","25175d4e":"After exploring the dataset we found various aspects that will negatively affect the performance:\n\n","3e28d8b1":"## 4.3 - Definition of datasets","ab07eb09":"## 4.2 - Definition of transformers","0d7170b2":"### 4.4.1 - Checking data distribution after balancing","0d7747b8":"As we can see in the previous figures the Dataset is unbalanced since in the training dataset we can find **classes with more than 3000 samples** (i.e. class 70) and **classes with less than 50 samples** (i.e. class 61)","a87a8707":"# 5 - Training","94ea7140":"## 5.2 - Train model","87bca760":"# 2 - Definition of Dataset and functions","2f724c5d":"### 2.2.5 - Confusion Matrix function"}}