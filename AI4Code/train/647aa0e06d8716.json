{"cell_type":{"580d516a":"code","e4bd4377":"code","42fc6d88":"code","eb9dc0da":"code","f63fdaa8":"code","11f786aa":"code","93f7556d":"code","9b6c6066":"code","b8daeba9":"code","7a6a5c33":"code","585259ad":"code","84f32001":"code","24a0168e":"code","46748c54":"code","9253733e":"code","0f2f1ad2":"code","64fa6ec9":"code","75cc6722":"code","0c64a64e":"code","4219a2b8":"code","ce08d5f5":"code","1d0973b2":"code","e335a6d7":"code","7a2203cc":"code","131a0399":"code","bb28d7f7":"code","45ccf340":"code","169ebad5":"code","da3c572e":"code","91fefc2c":"code","abe2e5e8":"code","4c4cbffa":"code","d53fbce3":"code","1d6d99e2":"code","042f07dd":"code","4c497d69":"code","4332ff8a":"code","5ca8707c":"code","cd8c8ff5":"code","4635696a":"code","9296a4b4":"code","22d2cf80":"markdown","bf68ded5":"markdown","7a46b51c":"markdown","6051b722":"markdown","aa67dc5c":"markdown","57034a35":"markdown","1898d8c8":"markdown","0a7900ee":"markdown","d4f724d3":"markdown","06723472":"markdown","e9816012":"markdown","664bfb44":"markdown","dc189f68":"markdown","5169d82c":"markdown","a38a4cbf":"markdown","f0e0f125":"markdown","a1ac8953":"markdown","8437fea8":"markdown"},"source":{"580d516a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\n%matplotlib inline\n\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.model_selection import  train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn.linear_model import  Ridge\nfrom sklearn.svm import SVR\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))","e4bd4377":"# Read the dataset \ndata = pd.read_csv('..\/input\/abalone.csv')","42fc6d88":" data['age'] = data['Rings']+1.5\n data.drop('Rings', axis = 1, inplace = True)","eb9dc0da":"print('This dataset has {} observations with {} features.'.format(data.shape[0], data.shape[1]))","f63fdaa8":"data.columns","11f786aa":"data.info()","93f7556d":"data.describe()","9b6c6066":"data.hist(figsize=(20,10), grid=False, layout=(2, 4), bins = 30)","b8daeba9":"numerical_features = data.select_dtypes(include=[np.number]).columns\ncategorical_features = data.select_dtypes(include=[np.object]).columns","7a6a5c33":"numerical_features","585259ad":"categorical_features","84f32001":"skew_values = skew(data[numerical_features], nan_policy = 'omit')\ndummy = pd.concat([pd.DataFrame(list(numerical_features), columns=['Features']), \n           pd.DataFrame(list(skew_values), columns=['Skewness degree'])], axis = 1)\ndummy.sort_values(by = 'Skewness degree' , ascending = False)","24a0168e":"# Missing values\nmissing_values = data.isnull().sum().sort_values(ascending = False)\npercentage_missing_values = (missing_values\/len(data))*100\npd.concat([missing_values, percentage_missing_values], axis = 1, keys= ['Missing values', '% Missing'])","46748c54":"sns.countplot(x = 'Sex', data = data, palette=\"Set3\")","9253733e":"plt.figure(figsize = (20,7))\nsns.swarmplot(x = 'Sex', y = 'age', data = data, hue = 'Sex')\nsns.violinplot(x = 'Sex', y = 'age', data = data)","0f2f1ad2":"data.groupby('Sex')[['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight',\n       'Viscera weight', 'Shell weight', 'age']].mean().sort_values('age')","64fa6ec9":"sns.pairplot(data[numerical_features])","75cc6722":"plt.figure(figsize=(20,7))\nsns.heatmap(data[numerical_features].corr(), annot=True)","0c64a64e":"data = pd.get_dummies(data)\ndummy_data = data.copy()","4219a2b8":"data.boxplot( rot = 90, figsize=(20,5))","ce08d5f5":"var = 'Viscera weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","1d0973b2":"# outliers removal\ndata.drop(data[(data['Viscera weight']> 0.5) & (data['age'] < 20)].index, inplace=True)\ndata.drop(data[(data['Viscera weight']<0.5) & (data['age'] > 25)].index, inplace=True)","e335a6d7":"var = 'Shell weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","7a2203cc":"data.drop(data[(data['Shell weight']> 0.6) & (data['age'] < 25)].index, inplace=True)\ndata.drop(data[(data['Shell weight']<0.8) & (data['age'] > 25)].index, inplace=True)","131a0399":"var = 'Shucked weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","bb28d7f7":"data.drop(data[(data['Shucked weight']>= 1) & (data['age'] < 20)].index, inplace=True)\ndata.drop(data[(data['Shucked weight']<1) & (data['age'] > 20)].index, inplace=True)","45ccf340":"var = 'Whole weight'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","169ebad5":"data.drop(data[(data['Whole weight']>= 2.5) & (data['age'] < 25)].index, inplace=True)\ndata.drop(data[(data['Whole weight']<2.5) & (data['age'] > 25)].index, inplace=True)","da3c572e":"var = 'Diameter'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","91fefc2c":"data.drop(data[(data['Diameter']<0.1) & (data['age'] < 5)].index, inplace=True)\ndata.drop(data[(data['Diameter']<0.6) & (data['age'] > 25)].index, inplace=True)\ndata.drop(data[(data['Diameter']>=0.6) & (data['age']< 25)].index, inplace=True)","abe2e5e8":"var = 'Height'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","4c4cbffa":"data.drop(data[(data['Height']>0.4) & (data['age'] < 15)].index, inplace=True)\ndata.drop(data[(data['Height']<0.4) & (data['age'] > 25)].index, inplace=True)","d53fbce3":"var = 'Length'\nplt.scatter(x = data[var], y = data['age'],)\nplt.grid(True)","1d6d99e2":"data.drop(data[(data['Length']<0.1) & (data['age'] < 5)].index, inplace=True)\ndata.drop(data[(data['Length']<0.8) & (data['age'] > 25)].index, inplace=True)\ndata.drop(data[(data['Length']>=0.8) & (data['age']< 25)].index, inplace=True)","042f07dd":"X = data.drop('age', axis = 1)\ny = data['age']","4c497d69":"standardScale = StandardScaler()\nstandardScale.fit_transform(X)\n\nselectkBest = SelectKBest()\nX_new = selectkBest.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.25)","4332ff8a":"np.random.seed(10)\ndef rmse_cv(model, X_train, y):\n    rmse =- (cross_val_score(model, X_train, y, scoring='neg_mean_squared_error', cv=5))\n    return(rmse*100)\n\nmodels = [LinearRegression(),\n             Ridge(),\n             SVR(),\n             RandomForestRegressor(),\n             GradientBoostingRegressor(),\n             KNeighborsRegressor(n_neighbors = 4),]\n\nnames = ['LR','Ridge','svm','GNB','RF','GB','KNN']\n\nfor model,name in zip(models,names):\n    score = rmse_cv(model,X_train,y_train)\n    print(\"{}    : {:.6f}, {:4f}\".format(name,score.mean(),score.std()))","5ca8707c":"\ndef modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['age'])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    #dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = -cross_val_score(alg, dtrain[predictors], dtrain['age'], cv=cv_folds, \n                                                    scoring='r2')\n    \n    #Print model report:\n    print (\"\\nModel Report\")\n    print( \"RMSE : %.4g\" % mean_squared_error(dtrain['age'].values, dtrain_predictions))\n    print( \"R2 Score (Train): %f\" % r2_score(dtrain['age'], dtrain_predictions))\n    \n    if performCV:\n        print( \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),\n                                                                                 np.min(cv_score),np.max(cv_score)))\n        \n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(alg.coef_, predictors).sort_values(ascending=False)\n        plt.figure(figsize=(20,4))\n        feat_imp.plot(kind='bar', title='Feature Importances')\n        plt.ylabel('Feature Importance Score')","cd8c8ff5":"# Base Model\npredictors = [x for x in data.columns if x not in ['age']]\nlrm0 = Ridge(random_state=10)\nmodelfit(lrm0, data, predictors)","4635696a":"# Let's do hyperparameter tunning using GrideSearchCV\nfrom sklearn.model_selection import  GridSearchCV\nparam  = {'alpha':[0.01, 0.1, 1,10,100],\n         'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\nglrm0 = GridSearchCV(estimator = Ridge(random_state=10,),\nparam_grid = param,scoring= 'r2' ,cv = 5,  n_jobs = -1)\nglrm0.fit(X_train, y_train)\nglrm0.best_params_, glrm0.best_score_","9296a4b4":"modelfit(Ridge(alpha = 0.1,random_state=10,), data, predictors)","22d2cf80":"## Bivariate Analysis\nBivariate analysis is vital part of data analysis process for, it gives clear picture on how each features are affected in presence of other features.  \nIt also helps us understand and identify significance features, overcome multi-collinearity effect, inter-dependency and thus, provides insights on hidden data noise pattern.","bf68ded5":"## Preprocessing, Modeling, Evaluation\nThe base steps followed in any data modeling pipelines are:\n               - pre-processing \n               - suitable model selection\n               - modeling\n               - hyperparamaters tunning using GridSearchCV\n               - evaluation","7a46b51c":"You have seen the perofrmance of each one of above models.\n\nSo, according to you which model should we start or choose?\nWell the answer lies in Occam's razor principle from philosophy https:\/\/simple.wikipedia.org\/wiki\/Occam%27s_razor.\" Suppose there exist two explanations for an occurrence. In this case the simpler one is usually better. Another way of saying it is that the more assumptions you have to make, the more unlikely an explanation.\"\nHence, starting with the simplest model Ridge, for various reasons:\n            - Feature Dimension is less\n            - No misisng values\n            - Few categorical features","6051b722":"## Univariate analysis\nUnderstanding feature wise statistics using various inbuilt tools ","aa67dc5c":"No missing values as said before","57034a35":"        Male : age majority lies in between 7.5 years to 19 years\n        Female: age majority lies in between 8 years to 19 years\n        Immature: age majority lies in between 6 years to < 10 years","1898d8c8":"##  Abalone Age Prediction\nDescription- Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem. ","0a7900ee":"CV score has improved slightly while, R2_score has decreased showing base model was overfitted.\nUsing above process multiple options can be tried to could up with much more robust model.\nThis process can also be tried on different models : RF, GB, etc.","d4f724d3":"For normally distributed data, the skewness should be about 0. For unimodal continuous distributions, a skewness value > 0 means that there is more weight in the right tail of the distribution. The function skewtest can be used to determine if the skewness value is close enough to 0, statistically speaking.\n        - Height has highest skewedness followed by age, Shucked weight (can be cross verified through histogram plot)","06723472":"key insights\n            length is linearly correlated with diameter while, non-linear relation with height, whole weight, shucked weight, viscera weight and shell weight\n        \n        ","e9816012":"Hyperparameter tunning is an iterative process and it can go on. As this kernal primary focuses on EDA of Abalone dataset, modeling building will be taken into another kernal [\"Modeling - Abalone Age Prediction\" ]. Hope I have helped you getting insights of Abalone dataset through this kernal.","664bfb44":"## Outliers handlings","dc189f68":"Key insights : \n            - No missing values in the dataset\n            - All numerical features but 'sex'\n            - Though features are not normaly distributed, are close to normality\n            - None of the features have minimum = 0 except Height (requires re-check)\n            - Each feature has difference scale range","5169d82c":"Motivate me so that I will come up soon with  \"Modeling - Abalone Age Prediction\" . :)","a38a4cbf":"        Whole Weight is almost linearly varying with all other features except age\n        Heigh has least linearity with remaining features\n        Age is most linearly proprtional with Shell Weight followed by Diameter and length\n        Age is least correlated with Shucked Weight\n        \n  Such high correlation coefficients among features can result into multi-collinearity. We need to check for that too, however, I have not done it here.","f0e0f125":"In this article I have focussed on exploratory data analysis on Abalone Dataset. ","a1ac8953":"## Hyperparameter tunning using GrideSearchCV","8437fea8":"From problem statement and feature discription, let's first compute the target varible of the problem ' Age' and assign it to the dataset. \nAge = 1.5+Rings"}}