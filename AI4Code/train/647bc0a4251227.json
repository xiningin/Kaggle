{"cell_type":{"74380209":"code","8b251879":"code","fc1619b7":"code","453747cd":"code","3d2e5f16":"code","a74be089":"code","f5c0aff0":"code","f2e1c132":"code","d67dbe74":"code","9f63662a":"code","71cec24a":"code","e0dcd03e":"code","1fc44d1a":"code","e8ac38db":"code","d8647d97":"code","56abf0ea":"code","504831fb":"code","bf3c7cc5":"code","6f19fbad":"code","a7991773":"code","d28c8b41":"code","20f811b2":"code","8daf1aac":"code","954a52a9":"code","4820e583":"code","3cf9b7a0":"code","735d6c18":"code","dee9d4c3":"code","78202178":"code","839e192d":"code","991cd817":"code","4c4e0ee3":"code","d796bb68":"code","586ad9c6":"code","4adfbd58":"code","6e0f3d9e":"code","06e33ebf":"code","0c7f6d25":"code","78f99a09":"code","a0c529a0":"code","fba53a3a":"code","9a2b3c63":"code","971a66b4":"code","16a5c746":"code","736af404":"code","d3f88b5a":"markdown","1f0ee800":"markdown","73a78916":"markdown","80b1c85e":"markdown","a3029137":"markdown","63ef0637":"markdown","6194689b":"markdown","be9c724a":"markdown","2008e55b":"markdown","06ff1f3e":"markdown","0ca2e01d":"markdown","c0bc98fc":"markdown","b1a1a4ec":"markdown","ed6208cf":"markdown","1f6156ee":"markdown","b607b680":"markdown","626ba836":"markdown","2db848a6":"markdown","2b501973":"markdown","27e9750c":"markdown","1f9252b6":"markdown","d974d88f":"markdown","abc626d2":"markdown","c2155b7f":"markdown","0d619405":"markdown","3adfb54f":"markdown","c62e00ee":"markdown","bde6d95e":"markdown","4dfe72c3":"markdown","0cc6a2a5":"markdown","46292d1c":"markdown","8eaa6ec0":"markdown","4e4c2e57":"markdown","c9fd0bde":"markdown","5e783ef0":"markdown","80590d24":"markdown","88b1e4d8":"markdown","24f0147d":"markdown","7984cb64":"markdown","0297dfc3":"markdown","a08ca9e1":"markdown","eabd128f":"markdown","778d63d2":"markdown","7719494f":"markdown","3a255fca":"markdown","99019c24":"markdown","dfbc710b":"markdown","fa4cd166":"markdown","6e36560f":"markdown","ba82de43":"markdown","62ff1ba9":"markdown"},"source":{"74380209":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV fIle I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom time import time\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nimport warnings; warnings.simplefilter('ignore')\n\nprint(os.listdir(\"..\/input\"))","8b251879":"df = pd.read_csv(\"..\/input\/heart.csv\")\ndf.head()","fc1619b7":"df.shape","453747cd":"categoricals = ['cp','restecg','slope','thal']\ndf[categoricals].head(2)","3d2e5f16":"numerics = np.setdiff1d(df.columns.tolist(),categoricals)\ndf[numerics].head(3)","a74be089":"# There aren't any null values\ndf.isnull().sum()","f5c0aff0":"df.describe()","f2e1c132":"plt.scatter(df['target'],df['chol']);","d67dbe74":"#We do see this individual as an extreme outlier, but I will leave this in the model.\n#I am curious to see their other attributes however next to the rest of the distributions shown in the \"describe\" method.\ndf[df['chol']==564]","9f63662a":"df.describe()","71cec24a":"df['thal'].max()","e0dcd03e":"pd.get_dummies(df[categoricals].astype(str)).head()","1fc44d1a":"skew_calcs = df[numerics].skew()\nskew_calcs[abs(skew_calcs)>0.7]","e8ac38db":"skewed_features = skew_calcs[skew_calcs.abs()>0.7].index.tolist()\nskewed_features","d8647d97":"df[skewed_features].hist();","56abf0ea":"np.log1p(df[skewed_features]).hist();","504831fb":"df.head()","bf3c7cc5":"modeldata = pd.merge(df[numerics],pd.get_dummies(df[categoricals].astype(str)),left_index = True, right_index = True,how = 'inner')\nmodeldata.head()","6f19fbad":"from sklearn.model_selection import train_test_split\nX, y = modeldata.drop(['target'],axis = 1), modeldata['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","a7991773":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import fbeta_score, accuracy_score","d28c8b41":"def train_predict_with_parameters(learner, X_train, y_train, X_test, y_test,parameters = {}): \n    results = {}\n    start = time() # Get start time\n    if parameters != {}:\n        clf = GridSearchCV(learner, parameters)\n        learner = clf.fit(X_train,y_train)\n        results['best_params'] = learner.best_params_\n    else:\n        learner = learner.fit(X_train,y_train)\n        results['best_params'] = ''\n    end = time() # Get end time\n    results['train_time'] = end - start\n    start = time() # Get start time\n    predictions_test = learner.predict(X_test)\n    predictions_train = learner.predict(X_train)\n    end = time() # Get end time\n    results['pred_time'] = end - start\n    results['acc_train'] = accuracy_score(y_train,predictions_train)\n    results['acc_test'] = accuracy_score(y_test,predictions_test)\n    results['f_train'] = fbeta_score(y_train,predictions_train,.8)\n    results['f_test'] = fbeta_score(y_test,predictions_test,.8)\n    #results['best_params'] = learner.best_params_\n    return results","20f811b2":"learners= []\n\nfrom sklearn.linear_model import LogisticRegression\nlearners.append([LogisticRegression(),{}])\n\nfrom sklearn.tree import DecisionTreeClassifier\nlearners.append([DecisionTreeClassifier(),{}])\n\nfrom sklearn.neighbors import KNeighborsClassifier\nlearners.append([KNeighborsClassifier(),{}])\n\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier \nlearners.append([GradientBoostingClassifier(),{ 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] }])\nlearners.append([AdaBoostClassifier(),{'n_estimators': [16, 32]}])\n\nlearners.append([BaggingClassifier(),{}])\nlearners.append([RandomForestClassifier(),\n                 {'bootstrap': [True, False],\n                 'max_depth': [3, 5, 10, 20, None],\n                 'max_features': ['auto', 'sqrt'],\n                 'min_samples_leaf': [1, 2, 4],\n                 'min_samples_split': [2, 5, 10]}\n                ])\n\nlearners.append([RandomForestClassifier(),{}])\n\nfrom sklearn.svm import LinearSVC, SVC\nlearners.append([SVC(),{'kernel': ['linear', 'rbf'],'C':[1, 10]}])\nlearners.append([LinearSVC(),{}])\n\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nlearners.append([BernoulliNB(),{}])\nlearners.append([GaussianNB(),{}])\nlearners.append([MultinomialNB(),{}])","8daf1aac":"results = {}\nfor clf in learners:\n    #No Params\n    clf_name = clf[0].__class__.__name__\n    results[clf_name] = train_predict_with_parameters(clf[0], X_train, y_train, X_test, y_test)\n    #GridSearchCV\n    if clf[1] != {}:\n        clf_name = clf[0].__class__.__name__\n        results[str(clf_name) + '_gridsearch'] = train_predict_with_parameters(clf[0], X_train, y_train, X_test, y_test, clf[1])","954a52a9":"pd.DataFrame.from_dict(results).transpose().sort_values(['f_test','acc_test'], ascending = [False,False])","4820e583":"from sklearn.ensemble import VotingClassifier","3cf9b7a0":"clf1 = BernoulliNB()\nclf2 = RandomForestClassifier()\nclf3 = AdaBoostClassifier()\neclf1 = VotingClassifier(estimators=[('bnb', clf1), ('rf', clf2), ('ab', clf3)], voting='hard')\n#If \u2018hard\u2019, uses predicted class labels for majority rule voting. Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n","735d6c18":"clf_name = eclf1.__class__.__name__\nresults[clf_name] = train_predict_with_parameters(eclf1, X_train, y_train, X_test, y_test)\npd.DataFrame.from_dict(results).transpose().sort_values(['f_test','acc_test'], ascending = [False,False])","dee9d4c3":"#We'll be using this function a few times to show feature importances\n\ndef show_importances(importances,vars = 8):\n    indices = np.argsort(importances)[::-1]\n    columns = X_train.columns.values[indices[:vars]]\n    values = importances[indices][:vars]\n    fig = plt.figure(figsize = (9,5))\n    plt.title(\"Normalized Weights for Most Predictive Features\", fontsize = 16)\n    plt.bar(np.arange(vars), values, width = 0.6, align=\"center\", color = '#00A000', \\\n          label = \"Feature Weight\")\n    plt.bar(np.arange(vars) - 0.3, np.cumsum(values), width = 0.2, align = \"center\", color = '#00A0A0', \\\n          label = \"Cumulative Feature Weight\")\n    plt.xticks(np.arange(vars), columns)\n    plt.ylabel(\"Weight\", fontsize = 12)\n    plt.xlabel(\"Feature\", fontsize = 12)\n    plt.legend(loc = 'upper center')\n    plt.tight_layout()\n    plt.show()","78202178":"clf = BaggingClassifier()\nclf.fit(X, y)\n\nimportances = np.mean([\n    tree.feature_importances_ for tree in clf.estimators_\n], axis=0)\n\nshow_importances(importances,8)","839e192d":"### Let's distinguish by target to see the distributions of the features by target\ntarget1 = modeldata[modeldata['target']==1]\ntarget0 = modeldata[modeldata['target']==0]","991cd817":"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (12,4))\n\nax1.hist(target1['oldpeak'],int(modeldata['oldpeak'].max()),alpha = 0.5, label = 'with heart disease')\nax1.hist(target0['oldpeak'],int(modeldata['oldpeak'].max()),alpha = 0.5, label = 'without heart disease')\nax1.title.set_text('oldpeak')\nax1.legend(loc=0)\n\nax2.hist(target1['ca'],int(modeldata['ca'].max()),alpha = 0.5, label = 'with heart disease')\nax2.hist(target0['ca'],int(modeldata['ca'].max()),alpha = 0.5, label = 'without heart disease')\nax2.title.set_text('ca')\nax2.legend(loc=0);","4c4e0ee3":"modeldata.corr()['target'].sort_values().head(5)","d796bb68":"cp0 = modeldata[['cp_0','target']]\ncp0['combined'] = modeldata['cp_0'] * modeldata['target']\n\nprint('{}% of all had heart disease%'.format(round(df['target'].sum()\/df.shape[0]*100,0)))\nprint('{}% of those with cp_0 had heart disease%'.format(round(39\/143*100,0)))\ncp0.sum()","586ad9c6":"pd.merge(pd.get_dummies(df['thal']), df[['target']], left_index = True, right_index = True, how = 'inner').corr()['target']","4adfbd58":"modeldata.corr()['target'].sort_values().head(5)","6e0f3d9e":"print('{}% of those with thal_2 had heart disease%'.format(round(130\/166*100,0)))\nthal2 = modeldata[['thal_2','target']]\nthal2['combined'] = modeldata['thal_2'] * modeldata['target']\nthal2.sum()","06e33ebf":"modeldata.corr()['target'].sort_values(ascending = False).head(10)","0c7f6d25":"pd.merge(pd.get_dummies(df['thal']), df[['target']], left_index = True, right_index = True, how = 'inner').corr()['target']","78f99a09":"clf = RandomForestClassifier()\nclf.fit(X,y)\n\nimportances = np.mean([\n    tree.feature_importances_ for tree in clf.estimators_\n], axis=0)\n\nshow_importances(importances,8)","a0c529a0":"sns.kdeplot(data = target1['thalach'],color = 'red',shade = True)#,bw=True)\nsns.kdeplot(data = target0['thalach'],color = 'blue',shade = True)#, bw = True)\nplt.show()    ","fba53a3a":"clf = AdaBoostClassifier()\nclf.fit(X,y)\n\nimportances = np.mean([\n    tree.feature_importances_ for tree in clf.estimators_\n], axis=0)\n    \nshow_importances(importances,6)    ","9a2b3c63":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (16,4))\n\nsns.kdeplot(data = target1['age'],color = 'red',shade = True, ax = ax1)#,bw=True)\nsns.kdeplot(data = target0['age'],color = 'blue',shade = True, ax = ax1)#, bw = True)\n\nsns.kdeplot(data = target1['chol'],color = 'red',shade = True, ax = ax2)#,bw=True)\nsns.kdeplot(data = target0['chol'],color = 'blue',shade = True, ax = ax2)#, bw = True)\n\nsns.kdeplot(data = target1['trestbps'],color = 'red',shade = True, ax = ax3)#,bw=True)\nsns.kdeplot(data = target0['trestbps'],color = 'blue',shade = True, ax = ax3)#, bw = True)\nplt.show()    ","971a66b4":"modeldata[['target','age','chol','trestbps']].corr()['target'].sort_values()","16a5c746":"eclf1.fit(X, y)\n\nimportances = np.mean([\n    tree.feature_importances_ for tree in clf.estimators_\n], axis=0)\n\nshow_importances(importances,8)","736af404":"modeldata[['target','age','chol','trestbps']].corr()['target'].sort_values()","d3f88b5a":"- **oldpeak**\n- **ca**\n<br>**Below we see that oldpeak and ca values of zero are positively correlated with the presence of heart disease.** \n<br>We will also see this in the correlation table below, where there are 4th and 5th most negatively correlated features.","1f0ee800":"### Voting Classifier","73a78916":"This compels a strong critique of the results of this study.  I think there are two places to point the finger.","80b1c85e":"Higher rates of **thalach**, the maximum heart rate achieved, are correlated with a greater incidence of heart disease.","a3029137":"### thalach is the new feature in the top four, so let's take a closer look","63ef0637":"There are only 303 records, and it's questionable as to whether this sample size will be enough to arrive at significant and reliable conclusions regarding the most important factors for heart disease.","6194689b":"https:\/\/www.urmc.rochester.edu\/encyclopedia\/content.aspx?ContentTypeID=92&ContentID=p07662\nHere we can see that the flouroscopy will show the flow of blood through coronary arteries to check for arterial blockages. From this, we can infer at a very simple level that have zero major vessels is a problem.  Having no major arteries will significantly limit blood flow throughout the body.","be9c724a":"## Summary:\n\nWe should be skeptical data scientists, and not blindly trust our models' results. We should be particularly careful when tempted to extrapolate meaning where we do not understand any logical rationale for the results. We can do this by researching the findings of others, testing significance, testing a variety of models, and not assuming our dataset is free of any bias.\nWe like to solve problems, but the problems we try to solve are often much more complex than applying your typical machine learning model and simply reading out results that revolutionize the world. We need to ask a lot of questions about each specific case.\n\n- Does machine learning work for this case? Or, should I just stick with descriptive statistics?\n- How many different models should I test?\n- What is the specialized level of knowledge required to understand the results?\n- Do the results corroborate the established understanding in this field and can we defend the results if they do not?\n\nFor my particular study, I have a few answers to these scrutinizing questions.\nMachine learning should work for this case, but a much larger dataset would be required. I would also need to have an in-depth understanding of how the dataset was created to make sure I understand any potential biases. Finally, this is a study where an engaged partnership with trained medical professionals would be essential.","2008e55b":"Before passing the numerical features to the model, it's important to see if there is skewness in the distributions.","06ff1f3e":"This is a great reminder that the setup of the study itself to be completely unbiased is incredibly important.  It appears that this study may have some kind of biases given the high significance of age being negatively correlated with heart disease, which is completely contrary to medical knowledge.","0ca2e01d":"- **thal_2**\n<br>With the statistics and correlation table below, we see that **thal_2** is the most highly correlated feature with heart disease.","c0bc98fc":"## Potential Improvements:\nI think the main area for improvement here is finding a larger dataset.  While the UCI dataset is the most commonly used dataset for this problem historically, there are other datasets.  There may be reasons they are not as commonly used, but they might clarify some issues, particularly the unresolved negative correlation with age.","b1a1a4ec":"My Final model will be a VotingClassifier based on the top-scoring models above.  I chose the top three models, but I only used one of the Naive Bayes models (I picked BernoulliNB), because the logic they use to train will be very similar.  So, I wouldn't expect any added value from an ensemble such similar models.","ed6208cf":"#### Let's look for any meaningful outliers","1f6156ee":"The only distribution of concern is **chol**, which is total cholesterol.  The maximum is more than twice the 75th percentile.","b607b680":"### We'll look more closely into the top four above, with the exception of oldpeak, which we've already looked into.\n- **age**\n- **chol**\n- **trestbps**","626ba836":"#### The distribution of log-transformed features below does not appear to improve the distribution to appear more \"normal\", so I will stick with the original features without any transformation.","2db848a6":"The Adaboost throws some curveballs for us, as you can see below.","2b501973":"- ca is the number of major vessels (0-3) colored by flouroscopy.","27e9750c":"As previously mentioned, the negative correlations of **age, chol, and trestbps** are discouraging regarding the reliability of our study.","1f9252b6":"#### Objectives:\nIn this notebook, I will attempt to find the main contributors to heart disease through the UCI Heart Disease dataset.<br>\nMy approach will include Exploratory Data Analysis, Feature Engineering (minimal required in this example), Testing Multiple Models, including several with GridSearch, and one-by-one analysis the models' top features as they relate current medical knowledge.","d974d88f":"External sources shows that age, total cholesterol (chol), and resting blood pressure (trestbps) should be positively correlated with the presence of heart disease.<br>\n- **age**: https:\/\/www.nia.nih.gov\/health\/heart-health-and-aging\n- **chol**: https:\/\/www.webmd.com\/heart-disease\/guide\/heart-disease-lower-cholesterol-risk#1\n- **trestbps**: https:\/\/www.webmd.com\/hypertension-high-blood-pressure\/guide\/hypertensive-heart-disease#1","abc626d2":"### We'll look more closely into the top four above:\n- **thal_2**\n- **cp_0**\n- **ca**\n- **oldpeak**","c2155b7f":"References:\n- https:\/\/www.kaggle.com\/rgoodman\/elo-merchant-with-no-peeking-at-other-kernels\n- My ML Donors Udacity Project\n- https:\/\/github.com\/dmitriyboyuk\/sl_classifier_framework-master\/blob\/master\/finding_opportunity_segment_v1.ipynb","0d619405":"**Created dummy variables for the categoricals and looks like everything merged\/joined correctly.**","3adfb54f":"**Reading into the descriptions of the features of the dataset, it looks like a few of them should be reclassified as categoricals instead of numerical features.**\n- cp\n- restecg\n- slope\n- thal","c62e00ee":"I'll have to trust the data on this one, since it looks like understanding electrocardiogram patterns is an entire field by itself. https:\/\/ecg.utah.edu\/","bde6d95e":"## Exploratory Data Analysis","4dfe72c3":"The **age** feature is still perplexing, but it's nice to see the top six features are among those that I've already researched.  This is encouraging and makes sense since the VotingClassifier is an amalgam of the top models.","0cc6a2a5":"We can see both visually and with the correlation table that these three features are negatively correlated with the presence of heart disease.  This will, however, pose a bit of a problem to our study.  We will find that as we seek out external research, these three figures should have a positive correlation with heart disease.","46292d1c":"We can also see how much more positively correlated thal_2 is with heart disease than the other thalassemia categories.","8eaa6ec0":"- **cp_0**\n<br>Above we also saw how **cp_0** (typical angina) is the most negatively correlated feature with heart disease.","4e4c2e57":"If we look at these correlations again, compared with tests for statistical significance of Pearson correlation with a sample size ~300, we see that at 99% confidence, only age is significant, and at 95%, only age and trestbps are significant.","c9fd0bde":"## So, let's summarize our findings and see if they make sense in the real world.","5e783ef0":"#### I will use accuracy score and f_score as my metrics for this binary classification problem.\nI will look at **f_score**, since this is interesting and relevant in practice when looking at the potential for a disease.  I will set beta at .8, weighting recall more heavily than precision with the idea that this could be used as an indicator to take preventative action.  While only theoretical, this higher recall sensitivity would potentially be used to encourage change in behavior like diet and exercise.  One would possibly argue that this sensitivity would be too high if the outcome were related to actual prescription of medications related to heart disease.<br>\n\n**Accuracy score** will be the most relevant measure for my purposes, since I am looking primarily for which features are most important in predicting the incidence of heart disease.  So, accuracy score will weight false positives and false negatives equally.","80590d24":"- age: The person's age in years\n- sex: The person's sex (1 = male, 0 = female)\n- cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n- trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n- chol: The person's cholesterol measurement in mg\/dl\n- fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n- restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n- thalach: The person's maximum heart rate achieved\n- exang: Exercise induced angina (1 = yes; 0 = no)\n- oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot)\n- slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n- ca: The number of major vessels (0-3)\n- thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n- target: Heart disease (0 = no, 1 = yes)","88b1e4d8":"### Metrics","24f0147d":"### It's nice to see 3 of our top four features are the same.","7984cb64":"Angina is a type of chest pain cause by reduced blood flow to the heart.  This one does seem to make sense, but would again be much more well understood by someone with specialized training in medicine.","0297dfc3":"### Bagging Classifier:\n#### Let's take a closer look","a08ca9e1":"We see that values of zero for both **oldpeak** and **ca** are positively correlated with heart disease.\n- oldpeak is the ST depression induced by exercise relative to rest.","eabd128f":"As we saw earlier, typical angina has the most negative correlation with the presence of heart disease. Only 27% of those with typical angina had heart disease.  ","778d63d2":"The high correlation between **thal_2**, thalassemia - reversible defect doesn't sound surprising.  It almost sounds as if anyone with a \"defect\" would by definition already have heart disease, but that doesn't appear to be the case, since 78% of those had heart disease in the study.  I wasn't able to find much more information online about this feature, which is just another example of how specialized a data scientist would need to be to understand this type of study.","7719494f":"# Heart Disease Analytics with Healthy Skepticism","3a255fca":"Here is the breakdown of the types of angina in the study:\n- typical angina\n- atypical angina\n- non-anginal pain\n- asymptomatic","99019c24":"The individual doesn't stand out too much in any other areas, so I don't want to take them from input to the model since they don't seem like they will distort training with only one feature as an extreme.","dfbc710b":"### AdaBoostClassifier:\n#### Let's take a closer look","fa4cd166":"### RandomForestClassifier:\n#### Let's take a closer look","6e36560f":"### Let's take a look at the feature importance of the ensemble VotingClassifier I created.","ba82de43":"## Prep Model for Training and Validation","62ff1ba9":"## Now to dig into some model results."}}