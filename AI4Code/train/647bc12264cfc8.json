{"cell_type":{"1d625dd3":"code","7789e728":"code","20a07239":"code","f9b93a9d":"code","54ffe0a5":"code","06840706":"code","86f4a0ae":"code","5e3e50a3":"code","210c04fd":"code","e0b771cd":"code","5bd8e567":"code","03c40c2a":"code","e692ec98":"code","85b8b1c1":"code","56022658":"code","07dac608":"code","1a648743":"code","eb8d21f0":"markdown","4bd1a029":"markdown","0c5ed320":"markdown","46a652b4":"markdown","e82e37d7":"markdown","b3160321":"markdown","2ff26dbf":"markdown","5fd11230":"markdown","9f4b776e":"markdown","187c1649":"markdown"},"source":{"1d625dd3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7789e728":"from collections import Counter\nimport math\n\ndef knn(data, query, k, distance_fn, choice_fn):\n    neighbor_distances_and_indices = []\n    \n    # 3. For each example in the data\n    for index, example in enumerate(data):\n        # 3.1 Calculate the distance between the query example and the current\n        # example from the data.\n        distance = distance_fn(example[:-1], query)\n        \n        # 3.2 Add the distance and the index of the example to an ordered collection\n        neighbor_distances_and_indices.append((distance, index))\n    \n    # 4. Sort the ordered collection of distances and indices from\n    # smallest to largest (in ascending order) by the distances\n    sorted_neighbor_distances_and_indices = sorted(neighbor_distances_and_indices)\n    \n    # 5. Pick the first K entries from the sorted collection\n    k_nearest_distances_and_indices = sorted_neighbor_distances_and_indices[:k]\n    \n    # 6. Get the labels of the selected K entries\n    k_nearest_labels = [data[i][1] for distance, i in k_nearest_distances_and_indices]\n\n    # 7. If regression (choice_fn = mean), return the average of the K labels\n    # 8. If classification (choice_fn = mode), return the mode of the K labels\n    return k_nearest_distances_and_indices , choice_fn(k_nearest_labels)\n\ndef mean(labels):\n    return sum(labels) \/ len(labels)\n\ndef mode(labels):\n    return Counter(labels).most_common(1)[0][0]\n\ndef euclidean_distance(point1, point2):\n    sum_squared_distance = 0\n    for i in range(len(point1)):\n        sum_squared_distance += math.pow(point1[i] - point2[i], 2)\n    return math.sqrt(sum_squared_distance)\n\n\n'''\n# Regression Data\n# \n# Column 0: height (inches)\n# Column 1: weight (pounds)\n'''\nreg_data = [\n   [65.75, 112.99],\n   [71.52, 136.49],\n   [69.40, 153.03],\n   [68.22, 142.34],\n   [67.79, 144.30],\n   [68.70, 123.30],\n   [69.80, 141.49],\n   [70.01, 136.46],\n   [67.90, 112.37],\n   [66.49, 127.45],\n]\n\n# Question:\n# Given the data we have, what's the best-guess at someone's weight if they are 60 inches tall?\nreg_query = [60]\nreg_k_nearest_neighbors, reg_prediction = knn(\n    reg_data, reg_query, k=3, distance_fn=euclidean_distance, choice_fn=mean\n)\n\n'''\n# Classification Data\n# \n# Column 0: age\n# Column 1: likes pineapple\n'''\nclf_data = [\n   [22, 1],\n   [23, 1],\n   [21, 1],\n   [18, 1],\n   [19, 1],\n   [25, 0],\n   [27, 0],\n   [29, 0],\n   [31, 0],\n   [45, 0],\n]\n# Question:\n# Given the data we have, does a 33 year old like pineapples on their pizza?\nclf_query = [33]\nclf_k_nearest_neighbors, clf_prediction = knn(\n    clf_data, clf_query, k=3, distance_fn=euclidean_distance, choice_fn=mode\n)\n","20a07239":"import seaborn as sns\nfrom sklearn import preprocessing\n\ndf = pd.read_csv('\/kaggle\/input\/teleCust1000t.csv')\ndf.head()","f9b93a9d":"df['custcat'].value_counts()","54ffe0a5":"sns.distplot(df[\"income\"], kde=False, bins=100)","06840706":"df.columns","86f4a0ae":"#Converting to numpy array\nX = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float)\nX[0:5]\n","5e3e50a3":"#Converting the labels also\ny = df['custcat'].values\ny[0:5]","210c04fd":"X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\nX[0:5]","e0b771cd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","5bd8e567":"from sklearn.neighbors import KNeighborsClassifier","03c40c2a":"k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","e692ec98":"yhat = neigh.predict(X_test)\nyhat[0:5]","85b8b1c1":"from sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","56022658":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","07dac608":"import matplotlib.pyplot as plt\nplt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()","1a648743":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","eb8d21f0":"The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n\n An understanding of how we calculate the distance between points on a graph is necessary before moving on.","4bd1a029":"K in KNN, is the number of nearest neighbors to examine. It is supposed to be specified by the User. So, how can we choose right value for K?\nThe general solution is to reserve a part of your data for testing the accuracy of the model. Then chose k =1, use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. Repeat this process, increasing the k, and see which k is the best for your model.\n\nWe can calculate the accuracy of KNN for different Ks.","0c5ed320":"**Choosing the right value for K**\n\nTo select the K that\u2019s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm\u2019s ability to accurately make predictions when it\u2019s given data it hasn\u2019t seen before.\nHere are some things to keep in mind:\n\nAs we decrease the value of K to 1, our predictions become less stable. Just think for a minute, imagine K=1 and we have a query point surrounded by several reds and one green (I\u2019m thinking about the top left corner of the colored plot above), but the green is the single nearest neighbor. Reasonably, we would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n\nInversely, as we increase the value of K, our predictions become more stable due to majority voting \/ averaging, and thus, more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\nIn cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make K an odd number to have a tiebreaker.\n\n**Advantages**\nThe algorithm is simple and easy to implement.\nThere\u2019s no need to build a model, tune several parameters, or make additional assumptions.\nThe algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section).\n**Disadvantages**\nThe algorithm gets significantly slower as the number of examples and\/or predictors\/independent variables increase.","46a652b4":"Imagine a telecommunications provider has segmented its customer base by service usage patterns, categorizing the customers into four groups. If demographic data can be used to predict group membership, the company can customize offers for individual prospective customers. It is a classification problem. That is, given the dataset,  with predefined labels, we need to build a model to be used to predict class of a new or unknown case. \n\nThe example focuses on using demographic data, such as region, age, and marital, to predict usage patterns. \n\nThe target field, called\u00a0__custcat__,\u00a0has four possible values that correspond to the four customer groups, as follows:\n  1- Basic Service\n  2- E-Service\n  3- Plus Service\n  4- Total Service\n\nOur objective is to build a classifier, to predict the class of unknown cases. We will use a specific type of classification called K nearest neighbour.\n","e82e37d7":"> KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).","b3160321":"Defining features","2ff26dbf":"There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.\nThe KNN Algorithm\n1. Load the data\n2. Initialize K to your chosen number of neighbors\n3. For each example in the data\n   1. Calculate the distance between the query example and the current example from the data.\n   2. Add the distance and the index of the example to an ordered collection\n4. Sort the ordered collection of distances and indices from smallest to largest (in ascending order) by the distances\n5. Pick the first K entries from the sorted collection\n6. Get the labels of the selected K entries\n7. If regression, return the **mean** of the K labels\n8. If classification, return the **mode** of the K labels","5fd11230":"### Accuracy evaluation\nIn multilabel classification, __accuracy classification score__ is a function that computes subset accuracy. This function is equal to the jaccard_similarity_score function. Essentially, it calculates how closely the actual labels and predicted labels are matched in the test set.","9f4b776e":"Data Standardization give data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on distance of cases:","187c1649":"More details [here](https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)"}}