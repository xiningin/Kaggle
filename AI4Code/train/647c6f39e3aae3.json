{"cell_type":{"d1ba8476":"code","8d3c776d":"code","5e34c45c":"code","939530ad":"code","d4602cb8":"code","0dff30a5":"code","53f66f53":"code","6908b844":"code","7d83af8f":"code","1e1a6f3b":"code","13ac6798":"code","8391b6d1":"code","2cff6156":"code","892e51f8":"code","9377fabb":"code","79598155":"code","790604e1":"code","404fa6d8":"code","484b51bf":"code","049a9ea2":"code","016938e9":"code","baeb0c6d":"code","7a642413":"code","018cfbc5":"code","a3b5433b":"code","f28fac66":"code","53552ee3":"code","e5b27384":"code","b9f9e422":"code","a662364d":"code","6a8a4f3e":"code","e152a26e":"code","d23fba9d":"code","695c06c9":"code","419aa98e":"code","01b82660":"code","46eff61e":"code","957c3188":"code","fae35273":"code","8d12bfe3":"code","4995c7a0":"code","b15a51b4":"code","d098062e":"code","2d07d400":"code","8f4efe5c":"code","0a701204":"code","39ceaf34":"code","dbff78fb":"code","0a485521":"code","1fd852b2":"code","c2374407":"code","5861220a":"code","9c7afe57":"code","43f6f7fe":"code","1d3cc85f":"code","e30e37d5":"code","e0e2278d":"code","e7b99f24":"code","d6470473":"code","74409a9f":"code","3420cf69":"code","0c2ddfed":"code","631c1bed":"code","122f076a":"code","4b35fb42":"code","281f5f39":"code","45c11ead":"code","7fb1d059":"code","fe396b6e":"code","3455e4c9":"code","bd99234c":"code","4520e656":"code","97425bc0":"code","a9912940":"code","2e45d656":"code","6c478d07":"code","63713fea":"code","6f24a554":"code","77de8a30":"code","49db09d2":"code","44f98f04":"code","f5ba6a3a":"code","00464d09":"markdown","3acf7977":"markdown","46c95d4c":"markdown","23e8f382":"markdown","b04c5650":"markdown","4488332f":"markdown","6e3b8817":"markdown","6b19c585":"markdown","9a141a09":"markdown","c177c65e":"markdown","2a181de1":"markdown","9121a8d1":"markdown","40125062":"markdown","855b0201":"markdown","59ebce10":"markdown","8f779955":"markdown","67bdeffb":"markdown","0bfa6180":"markdown","1dbe8daf":"markdown","a8ca71c1":"markdown","d13e13c2":"markdown","460955c5":"markdown","d730ec09":"markdown","7b87ca77":"markdown","662dd698":"markdown","bf7be9d1":"markdown","20a39579":"markdown","e1b622cf":"markdown","4064dfc6":"markdown","a4312594":"markdown","f3b02d4a":"markdown","8321e282":"markdown","baed3975":"markdown","5f769027":"markdown","77bf8319":"markdown","ee9c5180":"markdown","db2d934a":"markdown","ed539fed":"markdown","60875e5c":"markdown","d7ae83f0":"markdown","dcb0b5ec":"markdown","dc34638e":"markdown","7d963f88":"markdown","7e5c184e":"markdown","6adb0891":"markdown","301b6878":"markdown","acfcf884":"markdown","f47cd294":"markdown","16323507":"markdown","75751e3b":"markdown","d88884d4":"markdown","761e024f":"markdown","d7a0d7fb":"markdown","1cf2d688":"markdown","06271819":"markdown","947f1336":"markdown","8bb56861":"markdown","19d06cf0":"markdown","6246f0ed":"markdown","a6a808a8":"markdown"},"source":{"d1ba8476":"import pandas as pd \nimport numpy as np\nimport string\nimport nltk\nimport nltk.corpus\nimport sklearn\nimport csv\nimport re\nimport tensorflow as tf\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Modules for visualization \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Tools for preprocessing input data \nfrom matplotlib import rcParams\nimport matplotlib as mpl\n\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords \nfrom nltk import NaiveBayesClassifier\nfrom nltk.corpus import wordnet \nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk import pos_tag\nfrom wordcloud import WordCloud\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom keras.backend import clear_session\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.utils import class_weight\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing import text, sequence \nfrom keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import precision_score,accuracy_score, f1_score,recall_score,roc_auc_score,precision_recall_curve, average_precision_score,auc,roc_curve\nfrom keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n\n# tokenize the dataset corpus, delete uncommon words such as names.\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences       \nfrom IPython.display import display, HTML\n\nfrom keras.utils.vis_utils import plot_model\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score \nimport pickle\n","8d3c776d":"#load the data \ndata = pd.read_csv (\"..\/input\/amazon-alexa-reviews\/amazon_alexa.tsv\", sep = '\\t')\ndata.head(10)","5e34c45c":"#Describe Data\ndata.describe()","939530ad":"# How many reviews we have \nprint('There are', data.shape[0], 'reviews')\n# check for duplicates \nprint(\"There are\",len(data[data.duplicated()]),\"duplicates\")\n# check if we have missing values \nprint(\"There are\",data.isnull().sum().sum(),\"missing values\")","d4602cb8":"uniqueValues = data.nunique()\nprint('number of unique values in each column :')\nprint(uniqueValues)","0dff30a5":"#exploring of \"rating\" Variable\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8,5))\ntotal = float(len(data))\nax = sns.countplot(x=\"rating\", data=data)\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='center')\nplt.show()","53f66f53":"#  data's categories\ncategories= pd.DataFrame(data['rating'].apply(lambda x: 1 if (x > 3) else 0) ).to_numpy()\ncategories","6908b844":"#create new  column called sentiment and store the pos for rate over 3 and negaive for the rest \ndata['sentiment']= data['rating'].apply(lambda x: \"Positive\" if (x > 3) else \"Negative\") \ndata.head(40)","7d83af8f":"plt.figure(figsize=(6, 6))\n\nexplode = [0, 0.01]\nplt.pie(data['sentiment'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['Positive', 'Negative'])\nplt.title('Customer Recommendation Distribution', fontsize=15)\nplt.axis('off');","1e1a6f3b":"sns.set(style=\"whitegrid\")\nplt.figure(figsize=(32,10))\ntotal = float(len(data))\nax = sns.countplot(x=\"variation\", hue = \"sentiment\", data=data)\n\nplt.title('Device Distribution By sentiment', fontsize=24)\nplt.xlabel(\"Device Distribution By sentiment\", fontsize=20)\nplt.ylabel(\"The Number of Divices\", fontsize=20)\nplt.legend(title='Recommendation Indicator', loc='center', labels=['Positive', 'Negative'])\n\nfor p in ax.patches:        \n   # percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()* 1.025\n    ax.annotate(p.get_height(), (x, y),ha='center')\n\nplt.show()","13ac6798":"# count the number of different type of devices\ndata[\"variation\"].value_counts()\n","8391b6d1":"#exploring of \"variation\" Variable\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(30,10))\ntotal = float(len(data))\nax = sns.countplot(x=\"variation\", data=data)\nfor p in ax.patches:        \n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width()\n    y = p.get_height()* 1.025\n    ax.annotate(percentage, (x, y),ha='center')\nplt.show()","2cff6156":"data[\"feedback\"].value_counts()\n","892e51f8":"plt.figure(figsize=(6, 6))\nexplode = [0, 0.01]\nplt.pie(data['feedback'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['1', '0'])\nplt.title('Customer feedback Distribution', fontsize=15)\nplt.axis('off');","9377fabb":"data.drop([\"rating\",\t\"date\"\t,\"variation\"\t,\"feedback\"], axis=1, inplace=True)\ndata","79598155":"# create lemmatizer \nlemmatizer = WordNetLemmatizer()","790604e1":"# function to get the simpler virsion of pos tag  to use  in lemmitazation \ndef get_simple_pos(tag):\n    if tag.startswith('N') or tag.startswith('J'):\n        return wordnet.NOUN\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN #default case","404fa6d8":"\n# function to clear the reviews from email addresses,URLs,numbers,stopwords patterns and lemmitize the reviews \ndef clean_reviews(review):\n    clean_words=[]\n    # Remove email addresses patterns \n    raw_review = re.sub('\\b[\\w\\-.]+?@\\w+?\\w+?\\.\\w{2,4}\\b',\" \",review)\n    # Remove URLs  patterns\n    raw_review =re.sub(\"[^a-zA-Z]\",\" \",raw_review)\n    #Remove numbers\n    raw_review= re.sub(\"\\d+(\\.\\d+)?\",\" \",raw_review)\n\n    stop_words=set(stopwords.words(\"english\"))\n    words_tokens= nltk.word_tokenize(raw_review)  \n    for word in words_tokens :\n\n      if word.lower() not in stop_words:\n        pos = pos_tag([word]) # get the part of speech of each word \n    \n        clean_word=(lemmatizer.lemmatize(word.lower(), pos=get_simple_pos(tag)) for word, tag in pos)\n        clean_words.append(\"\".join(clean_word))\n\n    return (\" \".join(clean_words))","484b51bf":"\n# store the cleaned reviews inside clean_reviews_corpus array \nclean_reviews_corpus=[]\n\nfor index,review in enumerate(data[\"verified_reviews\"]):\n  clean_reviews_corpus.append(clean_reviews(data[\"verified_reviews\"][index]))","049a9ea2":"data[\"verified_reviews\"] = data[\"verified_reviews\"].apply(clean_reviews)\ndata[\"verified_reviews\"].head()","016938e9":"data.head()","baeb0c6d":"# review 5 cleaned  \nprint(\"review 5 cleaned {\",data[\"verified_reviews\"][5],\"}\")\n","7a642413":"words_values = pd.Series(\" \".join(data[\"verified_reviews\"]).split()).value_counts()\nprint(words_values)\nrare_words=words_values[words_values <= 3]\nprint(rare_words)\nprint(rare_words.value_counts())\n","018cfbc5":"data[\"verified_reviews\"] = data[\"verified_reviews\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\ndata[\"verified_reviews\"].head(10)","a3b5433b":"# Create function to display wordcload\ndef create_WordCloud(data, back_ground_color=\"black\",title= None):\n  wordcloud=WordCloud(background_color=back_ground_color, max_words=300, max_font_size=30 , scale=3,random_state=1).generate(str(data))\n  mpl.rcParams['figure.figsize']=(15,15) \n  mpl.rcParams['font.size']=20  \n  #plt.style.use('fast')\n  fig = plt.figure(1)\n  if title:\n    plt.title(title, fontsize= 30)\n\n  plt.imshow(wordcloud)\n  plt.axis('off')\n  plt.show()\n\n","f28fac66":"# seprate the pos and neg words \n#Negative_words = \" \".join(data[data[\"verified_reviews\"]  ==\"0\"]in categories).split()\nNegative_words =[]\nPositive_words=[]\n\n\n#clean_reviews(data[\"verified_reviews\"][index]\nfor index,num  in enumerate (categories):\n    if num == 1:\n        Positive_words.append(data[\"verified_reviews\"][index])\n    else :\n        Negative_words.append(data[\"verified_reviews\"][index])\n","53552ee3":"Negative_words[:20]","e5b27384":"Positive_words[:20]","b9f9e422":"# wordcload for positive words \ncreate_WordCloud(Positive_words,back_ground_color=\"white\",title=\"Most Common Positive Words\")","a662364d":"# Top 15 words are in positive reviews\nfreq_pos=nltk.FreqDist(Positive_words)\nfreq_pos.most_common(15)","6a8a4f3e":"# wordcload for negative words \ncreate_WordCloud(Negative_words,back_ground_color=\"red\",title=\"Most Common Negative Words\")","e152a26e":"# Top 15 words are in negative reviews\nfreq_neg=nltk.FreqDist(Negative_words)\nfreq_neg.most_common(10)","d23fba9d":"# wordcload for all words \ncreate_WordCloud(data[\"verified_reviews\"],title=\"Most Common Words\")","695c06c9":"# Top 15 words  in reviews\nfreq_allwords=nltk.FreqDist(data[\"verified_reviews\"])\nfreq_allwords.most_common(15)","419aa98e":"\nx_train , x_test , y_train , y_test = train_test_split(data[\"verified_reviews\"] ,categories,stratify=categories, test_size= 0.20,random_state=42)","01b82660":"# here we create new CountVectorizer, which will help us understand and count the words.and it has different ways to use it, but we will only use it with  2-3 gram \ncount_vec= CountVectorizer(ngram_range=(1,2))\n# vectorizer to read the train text for us\nx_train_features= count_vec.fit_transform(x_train)\nx_train_features = x_train_features.toarray() #to get the frequincy of the words ","46eff61e":"# print the features in count vectorizer with 2 -3 combination of words\nprint(count_vec.get_feature_names())","957c3188":"x_train_features","fae35273":"x_test_features=count_vec.transform(x_test)","8d12bfe3":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\nX_train_Tfid = tfidf_vectorizer.fit_transform(x_train)\nX_test_Tfid = tfidf_vectorizer.transform(x_test)\n","4995c7a0":"tfidf_vectorizer.get_feature_names()","b15a51b4":"\nX_train_Tfid.toarray()","d098062e":"!wget https:\/\/raw.githubusercontent.com\/Mohamed1-2\/TensorFlow-Helper-Functions\/master\/Helper_functions.py\n","2d07d400":"# import confusion_matrix from github \nfrom Helper_functions import make_confusion_matrix ","8f4efe5c":"# create MultinomialNB object \nmnb_count = MultinomialNB()\n# fit on x_train and y_train\nmnb_count.fit(x_train_features,y_train)","0a701204":"model_score = {}\n\n# get the score of the model \ny_pred_mnb_count = mnb_count.predict(x_test_features)\nscores1=round(accuracy_score(y_test,y_pred_mnb_count)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_test,y_pred_mnb_count, average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_pred_mnb_count, average='macro')*100,2)\nprint (\"Recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_pred_mnb_count, average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_pred_mnb_count, average='macro')*100,2)\nprint (\"roc_auc_score:{}\".format(scores5)) \nprint ('\\n clasification report:\\n', classification_report(y_test,y_pred_mnb_count))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_pred_mnb_count))\nmodel_score[\"Naive Bayes with count vectorizer\"] = [scores1, scores2,  scores3,scores4,scores5]","39ceaf34":"# custom reviews will be use later to compare the models \nreview_1 = 'Its not like Siri, Siri answers more accurately then Alexa.'\nreview_2=\"i think siri device is better . this device is useless \"\nreview_3=\" I\u2019ve have Echoes all over the house...I didn't have any problem\"\nreview_4=\"I like it very much! Thank you for great quality product with cheaper price!\"\nreview_5=\"Love it! Just takes some time to configure it but it is fabulous! Thank you!\"\nreview_6=\"when i give any command Alexa goes on search mode and minewhile it also gets the sorrounding sound and Alexa belives it as it's command.. So finally the search result is Nothing\"\nreviews=[review_1,review_2,review_3,review_4,review_5,review_6]","dbff78fb":"\nresults={}\npreds=[]\nfor ind ,review in enumerate(reviews) :\n    review = clean_reviews(review)\n    text_count=count_vec.transform([review]).toarray()\n    prediction= mnb_count.predict(text_count)\n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n\n    else:\n        pred = \"Negative\"\n    preds+=[pred]\nresults[\"Naive Bayes with count vectorizer\"] =preds\n     ","0a485521":"# create MultinomialNB object \nmnb_Tfid = MultinomialNB()\n# fit on x_train and y_train\nmnb_Tfid.fit(X_train_Tfid, y_train)","1fd852b2":"# Get the predicited probability of testing data\ny_pred_Tfid_mnb = mnb_Tfid.predict(X_test_Tfid)\nscores1=round(accuracy_score(y_test,y_pred_Tfid_mnb)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_test,y_pred_Tfid_mnb, average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_pred_Tfid_mnb, average='macro')*100,2)\nprint (\"Recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_pred_Tfid_mnb, average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_pred_Tfid_mnb, average='macro')*100,2)\nprint (\"roc_auc_score:{}\".format(scores5)) \nprint ('\\n clasification report:\\n', classification_report(y_test,y_pred_Tfid_mnb))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_pred_Tfid_mnb))\nmodel_score[\"Naive Bayes with TF-IDF vectorizer\"] = [scores1, scores2,  scores3,scores4,scores5]","c2374407":"preds=[]\nfor ind ,review in enumerate(reviews) :\n    review = clean_reviews(review)\n    text_tfidf=tfidf_vectorizer.transform([review]).toarray()\n    prediction= mnb_Tfid.predict(text_count)\n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n\n    else:\n        pred = \"Negative\"\n    preds+=[pred]\nresults[\"Naive Bayes with TF-IDF Vectorizer\"] =preds","5861220a":"svc_count = LinearSVC( class_weight=\"balanced\")\n\nsvc_count.fit(x_train_features,y_train)","9c7afe57":"y_pred_count_svc = svc_count.predict(x_test_features)\nscores1=round(accuracy_score(y_pred_count_svc,y_test)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_pred_count_svc,y_test, average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_pred_count_svc, average='macro')*100,2)\nprint (\"recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_pred_count_svc, average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_pred_count_svc, average='macro')*100,2)\nprint (\"roc_auc_score:{}\".format(scores5)) \nprint ('\\n clasification report:\\n', classification_report(y_test,y_pred_count_svc))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_pred_count_svc))\nmodel_score[\"Support Vector Machine with Count Vectorizor\"] = [scores1, scores2,  scores3,scores4,scores5]","43f6f7fe":"preds=[]\nfor ind ,review in enumerate(reviews) :\n    review = clean_reviews(review)\n    text_count=count_vec.transform([review]).toarray()\n    prediction= svc_count.predict(text_count)\n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n\n    else:\n        pred = \"Negative\"\n    preds+=[pred]\nresults[\"SVM with Count Vectorizor\"] =preds","1d3cc85f":"svc_Tfid = LinearSVC( class_weight=\"balanced\")\n\nsvc_Tfid.fit(X_train_Tfid, y_train)","e30e37d5":"y_pred_Tfid_svc = svc_Tfid.predict(X_test_Tfid)\nscores1=round(accuracy_score(y_test,y_pred_Tfid_svc)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_test,y_pred_Tfid_svc, average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_pred_Tfid_svc, average='macro')*100,2)\nprint (\"recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_pred_Tfid_svc, average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_pred_Tfid_svc, average='macro')*100,2)\nprint (\"roc_auc_score:{}\".format(scores5)) \nprint ('\\n clasification report:\\n', classification_report(y_test,y_pred_Tfid_svc))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_pred_Tfid_svc))\nmodel_score[\"Support Vector Machine with TF-IDF Vectorizer \"] = [scores1, scores2,  scores3,scores4,scores5]\n","e0e2278d":"preds=[]\nfor ind ,review in enumerate(reviews) :\n    review = clean_reviews(review)\n    text_count=tfidf_vectorizer.transform([review]).toarray()\n    prediction= svc_Tfid.predict(text_count)\n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n\n    else:\n        pred = \"Negative\"\n    preds+=[pred]\nresults[\"SVM with TF-IDF Vectorizer\"] =preds","e7b99f24":"# We'll use 100 weak learners to build a strong learner and using DecisionTreeClassifier as base estimtor\nadaboost_classifier_count = AdaBoostClassifier(n_estimators=100,base_estimator=DecisionTreeClassifier() )\n\nadaboost_classifier_count.fit(x_train_features,y_train)","d6470473":"y_pred_count_adaboost = adaboost_classifier_count.predict(x_test_features)\nscores1=round(accuracy_score(y_test,y_pred_count_adaboost)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_test,y_pred_count_adaboost, average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_pred_count_adaboost, average='macro')*100,2)\nprint (\"Recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_pred_count_adaboost, average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_pred_count_adaboost, average='macro')*100,2)\nprint (\"roc_auc_score:{}\".format(scores5))\n\nprint ('\\n clasification report:\\n', classification_report(y_test,y_pred_count_adaboost))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_pred_count_adaboost))\nmodel_score[\"AdaBoost with Count Vectorizor\"] = [scores1, scores2,  scores3,scores4,scores5]","74409a9f":"preds=[]\nfor ind ,review in enumerate(reviews) :\n    review = clean_reviews(review)\n    text_count=count_vec.transform([review]).toarray()\n    prediction= adaboost_classifier_count.predict(text_count)\n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n\n    else:\n        pred = \"Negative\"\n    preds+=[pred]\nresults[\"adaboost with count Vectorizer\"] =preds","3420cf69":"# We'll use 100 weak learners to build a strong learner\nadaboost_classifier_Tfid = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100)\n\nadaboost_classifier_Tfid.fit(X_train_Tfid,y_train)","0c2ddfed":"y_pred_Tfid_adaboost = adaboost_classifier_Tfid.predict(X_test_Tfid)\nscores1=round(accuracy_score(y_test,y_pred_Tfid_adaboost)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_test,y_pred_Tfid_adaboost ,average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_pred_Tfid_adaboost, average='macro')*100,2)\nprint (\"Recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_pred_Tfid_adaboost, average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_pred_Tfid_adaboost, average='macro')*100,2)\nprint (\"roc_auc_score:{}\".format(scores5))\n\nprint ('\\n clasification report:\\n', classification_report(y_test,y_pred_Tfid_adaboost))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_pred_Tfid_adaboost))\nmodel_score[\"AdaBoost with TF-IDF Vectorizer \"] = [scores1, scores2,  scores3,scores4,scores5]","631c1bed":"preds=[]\nfor ind ,review in enumerate(reviews) :\n    review = clean_reviews(review)\n    text_count=tfidf_vectorizer.transform([review]).toarray()\n    prediction= adaboost_classifier_Tfid.predict(text_count)\n    \n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n\n    else:\n        pred = \"Negative\"\n    preds+=[pred]\nresults[\"adaboost with TF-IDF Vectorizer\"] =preds","122f076a":"# get the class weight \nfig = plt.figure(figsize=(14,5),facecolor='#f7f6f6',)\naxes = plt.subplot2grid((1,1),(0,0))\n\nplt.pie(x = [len(y_train[y_train == 0]), len(y_train[y_train == 1])],\n        pctdistance=0.70,labels = ['Negative','Posetive'], autopct='%1.1f%%',\n        colors = ['red','green'], labeldistance= 1.1, radius = 0.9,)\n\ncentre_circle = plt.Circle((0,0),0.8,fc='white') \n\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# plt.tight_layout()\nplt.text(2.5,-0.2,f'The Dataset is imbalanced!\\n\\nWe have only 327 Negative reviews for the minority \\nclass and 4861 Posetive reviews for the majority class.', fontsize=15)\nplt.show()","4b35fb42":"# find the length of the largest sentence in training data\nmax_len = max(x_train, key = len)\nprint(f'Max number of words in a text in training data: {len(max_len)}')\n","281f5f39":"# Convert reviews to padded sequences\n\n#We choose 10000 repeated words in corpus for tokenizing.\nmax_words = 100000\ntokenizer = Tokenizer(num_words = max_words)\n# create the vocabulary by fitting on X_train text\ntokenizer.fit_on_texts(x_train)\n# get the sequence of tokens\nxtrain_seq = tokenizer.texts_to_sequences(x_train)\nxtest_seq = tokenizer.texts_to_sequences(x_test)\n# padding the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=len(max_len))\nxtest_pad = sequence.pad_sequences(xtest_seq, maxlen=len(max_len))\nword_index = tokenizer.word_index\n\n\n","45c11ead":"#get a random example\nprint('train review num 10:', x_train[9])\nprint('before padding:', xtrain_seq[9])\nprint('after padding:', xtrain_pad[9])","7fb1d059":"# data shape after padding\nprint(xtrain_pad.shape)\nprint (xtest_pad.shape)\nprint (y_train.shape)\nprint (y_test.shape)","fe396b6e":"#creating Word Index\ntokenizer.word_index","3455e4c9":"#check a random word in the word index\ntokenizer.word_index[\"alexa\"]","bd99234c":"# create the model \ndef create_model():\n    \n\n        model = tf.keras.models.Sequential([tf.keras.layers.Embedding(max_words, 100, input_length=max_words),\n                                        tf.keras.layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'),\n                                        tf.keras.layers.MaxPooling1D(pool_size=2),\n                                        tf.keras.layers.LSTM(256),\n                                        tf.keras.layers.Dense(1, activation='sigmoid')\n                                        \n                            \n                                       \n    \n           ])\n\n        model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(3e-4), metrics=['accuracy'])\n        return model\nmodel=create_model()\nmodel.summary()","4520e656":"clear_session()","97425bc0":"# print the block diagram of our model\nplot_model(model, to_file='modelplot.png', show_shapes=True, show_layer_names=True)","a9912940":"# create model checkpoint and callbacks to save the model weights when validation accuracy is maximum\ncallback=tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=2, verbose=2, mode='auto',\n    baseline=None, restore_best_weights=True)\n#fit the model\nhist = model.fit(xtrain_pad,y_train, validation_data=(xtest_pad, y_test),class_weight= {0:87, 1:13},epochs=10, batch_size=32)","2e45d656":"# predict probabilities for test set\ny_probs = model.predict(xtest_pad, verbose=0)\n# predict classes for test set\ny_classes = (model.predict(xtest_pad) > 0.5).astype(\"int32\")\ny_train_pred=(model.predict(xtrain_pad) >= 0.5).astype(\"int32\")\n# reduce the 2D prediction arrays to 1D arrays to work with scikit-learn metrics API\ny_probs = y_probs[:, 0]\ny_classes = y_classes[:, 0]\n\nscores1=round(accuracy_score(y_test,y_classes)*100,2)\nprint (\"Accuracy:{}\".format(scores1))\nscores2=round(f1_score(y_test,y_classes,  average='macro')*100,2)\nprint (\"F1 score:{}\".format(scores2))\nscores3=round(recall_score(y_test,y_classes, average='macro')*100,2)\nprint (\"Recall:{}\".format(scores3))\nscores4=round(precision_score(y_test,y_classes,  average='macro')*100,2)\nprint (\"Precision:{}\".format(scores4))\nscores5=round(roc_auc_score(y_test,y_classes,  average='macro')*100,2)\nprint (\"roc_score:{}\".format(scores5))\n\nprint ('\\n clasification report:\\n', classification_report(y_test,y_classes))\nprint ('\\n confussion matrix:\\n',make_confusion_matrix(y_test, y_classes))\nmodel_score[\"CNN_LSTM Model\"] = [scores1, scores2,  scores3,scores4,scores5]\n","6c478d07":"model.save('my_model.h5')","63713fea":"from Helper_functions import plot_loss_curves \n# Plot loss curves\nplot_loss_curves(hist)","6f24a554":"# Model evaluation \ntrain_nn_results = model.evaluate(xtrain_pad, y_train, verbose=0)\ntest_nn_results = model.evaluate(xtest_pad, y_test, verbose=0)\nprint(f'Train accuracy: {train_nn_results[1]*100:0.2f}')\nprint(f'Test accuracy: {test_nn_results[1]*100:0.2f}')","77de8a30":"preds=[]\ndef get_predictions(text):\n    sequence = tokenizer.texts_to_sequences([text])\n    # pad the sequences\n    model_=load_model(\".\/my_model.h5\")\n    sequence = pad_sequences(sequence, maxlen=max_words)\n    # get the prediction\n    prediction= model_.predict(sequence)[0]\n    if (np.round(prediction,2) >= 0.5):\n        pred = \"Positive\"\n    else:\n        pred = \"Negative\"\n    return pred\nfor ind ,review in enumerate(reviews) :\n    prediction =get_predictions(review)\n    preds+=[prediction]\n    results[\"CNN_LSTM_Model\"] =preds ","49db09d2":"score_df = pd.DataFrame(model_score).transpose()\nscore_df.rename(columns = {0 :'Accuracy', 1:'F1' , 2: 'Recall', 3:'precision_score' , 4:\"roc_auc_score\" }, inplace = True)\nscore_df.sort_values(by = 'roc_auc_score', ascending = False, inplace = True)","44f98f04":"fig = plt.figure(figsize=(20,5), facecolor = '#e6dfeb')\naxes = plt.subplot2grid((1,1),(0,0))\n\nsns.heatmap(data = score_df, cmap = 'coolwarm', annot = True,)\nplt.show()","f5ba6a3a":"df_reviews=pd.DataFrame(reviews,index=[\"Review 1\",\"Review 2\",\"Review 3\",\"Review 4\",\"Review 5\",\"review 6\"])\ndisplay(HTML(df_reviews.to_html()))\n#print(reviews.values)\ndf=pd.DataFrame.from_dict(results, orient ='index',columns=[\"Review 1\",\"Review 2\",\"Review 3\",\"Review 4\",\"Review 5\",\"review 6\"])\ndf.style.set_table_styles([\n                                {\n                                    \"selector\" :\".row2,.row3,.row6\",\n                                    \"props\": [(\"border\",\"2px solid black\"),\n                                              (\"color\", \"black\"),\n                                              (\"background-color\", \"lightgreen\")]\n                                },\n    {\n        \"selector\":\"thead\",\n                                        \"props\":\"background-color:aqua; color:black; border:3px solid black;\"\n\n    }\n   \n                               ]\n                             )","00464d09":"###### <span style=\"color:#5642C5;\"> **7.1.5 AdaBoost with Count Vectorizor** <\/span> <a class=\"anchor\"  id=\"AdaBoost_count\"><\/a>\n\n\n\n","3acf7977":"#### <span style=\"color:#5642C5;\"> 3.1 About the data <\/span> <a class=\"anchor\"  id=\"aboutdata_3_1\"><\/a>\n","46c95d4c":"The data is made up of 3150 rows and 5 column variables. Each row includes a written comment as well as extra information about the customer.\n\n**About the features :**\n\n1. **rating** : the customer's product score, which ranges from 1 to 5, is stored as a positive ordinal integer variable.\n2. **date** : integer contain the date of review\n3. **variation** :String variable for various amazon Alexa products\n4. **verified_reviews** : String variable for the reviews.\n5. **feedback** :Integer documenting the number of feedbacks of various amazon Alexa products","23e8f382":"![Alexa](https:\/\/www.rcrwireless.com\/wp-content\/uploads\/2017\/12\/Alexa_Home_Banner.jpg)\n> Image From *rcrwireless.com*","b04c5650":"> **The data appears to be ready for analysis**","4488332f":"\"feedback\" Integer documenting the number of feedbacks of various amazon Alexa products","6e3b8817":"#### <span style=\"color:#5642C5;\"> 8.2 Custom Reviews \ud83d\udc53 <\/span> <a class=\"anchor\"  id=\"CustomReviews\"><\/a>\n\n> Here we will see the results of the models with the unseen data which are random reviews taken from the Internet to see how models perfurm inreal data.","6b19c585":"**I tried a few different models in ML and a CNN with LSTM Model , but SVM with TF-IDF Vectorizer proved to be the most effective for this imbalanced dataset. Our model had a ROC AUC score of 0.82, Accuracy of 0.92, F1 of 0.82, Pecision of 0.82, and Recall of 0.82 .More feature engineering and model implementation can be performed to find a better performing model.**","9a141a09":"###### <span style=\"color:#5642C5;\"> **7.1.3 Support Vector Machine with Count Vectorizor** <\/span> <a class=\"anchor\"  id=\"svm_count\"><\/a>\n","c177c65e":"###### <span style=\"color:#5642C5;\"> 6.1.1 Tokenisation <\/span> <a class=\"anchor\"  id=\"nlp_6_1\"><\/a>\n\n> Tokenization is a critical stage in the natural language processing process. Tokenization is the process of breaking down a sentence, paragraph, or even an entire text document into smaller parts, such as individual words or phrases. Each of these smaller components is referred to as a token.\n\n> in this project we will be using word tokenize in clean reviews function ","2a181de1":"> **It seems from the graph that \"walnut finish\" which is the least used by the users in this data, while the \"black Dot\" is the highest used**\n","9121a8d1":"## <span style=\"color:#5642C5;\"> 4. EDA(Exploratory data analysis) <\/span> <a class=\"anchor\"  id=\"EDA\"><\/a>\n","40125062":"###### <span style=\"color:#5642C5;\"> **7.1.4.1 Test the model with custom reviews** <\/span> <a class=\"anchor\"  id=\"svm_tf.7.1.4.1\"><\/a>","855b0201":"* [*1.PREFACE*](#PREFACE)\n\n* [*2.LIBRARIES*](#LIBRARIES)\n\n* [*3.DATA*](#DATA)\n  * [About the data 3.1](#aboutdata_3_1)\n  \n\n* [*4.EDA(Exploratory data analysis)*](#EDA)\n  * [Exploring the Variables 3.1](#Exploringvar_4_1)\n\n* [*5.Feature Slelection*](#Feature_Slelection)\n* [*6.NLP(natural language processing)*](#nlp)\n  * [6.1.Pre-processing the raw Reviews 6.1](#preprocessing_reviews_6_1)\n    * [6.1.1.Tokenisation  ](#Tokenisation_6.1.1)\n    * [6.1.2.Stopwords Removal ](#StopwordsRemoval_6.1.2)\n    * [6.1.3.Lemmatization ](#Tokenisation_6.1.3)\n  * [*6.2.WordCloud*](#WordCloud)\n  * [*6.3.Split data to Train & Test*](#split)\n  * [*6.4.Vectorization*](#vector)\n      * [6.4.1.count vectorizer ](#count_vec.6.4.1)\n      * [6.4.2.TF-IDF vectorizer ](#TF-IDF)\n      \n\n* [*7.MODELLING (MACHINE LEARNING & DEEP LEARNING)*](#modelling)\n  * [7.1.Machine Learning Models](#ml.7.1)\n    * [7.1.1.Multinomial Naive Bayes with count vectorizer](#mnp_count.7.1.1)\n         * [7.1.1.1.Test the model ](#mnp_count.7.1.1.1)\n    * [7.1.2.Multinomial Naive Bayes with TF-IDF Vectorizer](#mnp_tf.7.1.2)\n         * [7.1.2.1.Test the model ](#mnp_tf.7.1.2.1)\n    * [7.1.3.SVM with with count vectorizer](#svm_count)\n         * [7.1.3.1.Test the model ](#svm_count.7.1.3.1)\n    * [7.1.4.SVM with with TF-IDF vectorizer](#svm_tf)\n         * [7.1.4.1.Test the model ](#svm_tf.7.1.4.1)\n    * [7.1.5.Adaboost  with count vectorizer](#AdaBoost_count)\n         * [7.1.5.1.Test the model ](#test_model_5.7.1.5.1)\n    * [7.1.6.Adaboost  with TF-IDF vectorizer](#AdaBoost_tf)\n         * [7.1.6.1.Test the model ](#test_model_6.7.1.6.1)\n  * [7.2.Deep Learning Models](#ml.7.2)\n    * [7.2.1.CNN and LSTM Model](#cnn_lstm_model)\n         * [7.2.1.1.Padding and truncating the input training sequences ](#padding)\n         * [7.2.1.2.Test the model ](#cnn_lstm_model_test)\n  * [8.Results](#results)\n    * [8.1 Model comparisons](#Modelcomparisons)\n    * [8.2 Custom Reviews](#CustomReviews)\n  * [9.Conclusion](#Conclusion)\n\n\n\n","59ebce10":"\n###### <span style=\"color:#5642C5;\"> **7.2.1.1 Padding and truncating the input training sequences** <\/span> <a class=\"anchor\"  id=\"padding\"><\/a>\n\nthe use of padding is to ensure that all sequences have the same length,and we will be assign the maximum length of each sequence (max_len) to the length of the largest sentence in the train data \n","8f779955":" **Note**: *The rows in green are the rows that predict the correct labels* ","67bdeffb":"  **Plz up vote if you like my work \ud83d\ude48\u2764**","0bfa6180":"#### <span style=\"color:#5642C5;\"> 3.2 Reading the data <\/span> <a class=\"anchor\"  id=\"readdata_4_1\"><\/a>\n","1dbe8daf":"## <span style=\"color:#5642C5;\"> 3. DATA <\/span> <a class=\"anchor\"  id=\"DATA\"><\/a>\n","a8ca71c1":"> **It seems that the majority of customers have given positive feedback about the device**\n","d13e13c2":"###### <span style=\"color:#5642C5;\"> 6.1.2 Stopwords Removal <\/span> <a class=\"anchor\"  id=\"StopwordsRemoval_6.1.2\"><\/a>\n\n> stopwords removal step is a way to remove the unnecessary words that will not add any important information into the reviews","460955c5":"###### <span style=\"color:#5642C5;\"> **7.1.3.1 Test the model with custom reviews** <\/span> <a class=\"anchor\"  id=\"svm_count.7.1.3.1\"><\/a>","d730ec09":"## <span style=\"color:#5642C5;\"> 5. Feature Slelection <\/span> <a class=\"anchor\"  id=\"Feature_Slelection\"><\/a>\n","7b87ca77":"###### <span style=\"color:#5642C5;\"> **7.1.5.1 Test the model with custom reviews** <\/span> <a class=\"anchor\"  id=\"test_model_5.7.1.5.1\"><\/a>","662dd698":"## <span style=\"color:#5642C5;\">1. PREFACE <\/span> <a class=\"anchor\"  id=\"PREFACE\"><\/a>\n\n","bf7be9d1":"**We will be using helper functions from my GitHub to use in modelling part**\n\nhttps:\/\/raw.githubusercontent.com\/Mohamed1-2\/TensorFlow-Helper-Functions\/master\/Helper_functions.py","20a39579":"###### <span style=\"color:#5642C5;\"> **7.1.4 Support Vector Machine with TF-IDF Vectorizer** <\/span> <a class=\"anchor\"  id=\"svm_tf\"><\/a>\n","e1b622cf":"<div style=\"color:white;\n           display:fill;\n           border-radius:8px;\n           background-color:#5642C5;\n           font-size:100%;\n           font-family:Verdana;\n           letter-spacing:0.9px;\">\n\n<p style=\"padding: 10px;      text-align: center;\n              color:white;\">\n Table of Contents\n<\/p>\n<\/div>","4064dfc6":"> In this section, I'll only look at the review text and sentiment columns to see whether the user's reviews on the device are positive or negative based on the text. This reduces the model's complexity and turns it into a classification binary model.","a4312594":"###### <span style=\"color:#5642C5;\"> 6.4.2 TF-IDF Vectorization <\/span> <a class=\"anchor\"  id=\"TF-IDF\"><\/a>\n\n> This is a frequently used approach for converting text to a meaningful representation of numbers.\n","f3b02d4a":"> For NLP we will keep only the necessary columns such as \"verified reviews\" and \"sentiment\" columns and we will drop the rest","8321e282":"###### <span style=\"color:#5642C5;\"> **7.1.2.1 Test the model with custom reviews** <\/span> <a class=\"anchor\"  id=\"test_model_2.7.1.2.1\"><\/a>","baed3975":"#### <span style=\"color:#5642C5;\"> **6.4 Vectorization** <\/span> <a class=\"anchor\"  id=\"vector\"><\/a>\n","5f769027":"\"Rating\" integer variable where Customers who give stars more than three are considered to recommend the product, while less than or equal to three will not recommend the product.","77bf8319":"###### <span style=\"color:#5642C5;\"> **7.1.2 Multinomial Naive Bayes with TF-IDF Vectorizer** <\/span> <a class=\"anchor\"  id=\"mnp_tf.7.1.2\"><\/a>\n","ee9c5180":"###### <span style=\"color:#5642C5;\"> **7.1.2 Testing the Model** <\/span> <a class=\"anchor\"  id=\"test_model\"><\/a>\n\n","db2d934a":"#### <span style=\"color:#5642C5;\"> **6.3 Split data to Train & Test** <\/span> <a class=\"anchor\"  id=\"split\"><\/a>\n\n we split the data into train and test sets:\n    \n*   80% for training\n\n*   20% for testing","ed539fed":"###### <span style=\"color:#5642C5;\"> **7.1.1 Multinomial Naive Bayes with count vectorizer** <\/span> <a class=\"anchor\"  id=\"mnp_count.7.1.1\"><\/a>\n","60875e5c":"###### <span style=\"color:#5642C5;\"> 6.1.3 Lemmatization <\/span> <a class=\"anchor\"  id=\"Tokenisation_6.1.3\"><\/a>\n\n> Another technique will be used called **Lemmatization** instead of using **stemming**. Lemmatization is a powerful technique that can be used to give the root word for a given word but it requires the part of speech of that word. \n\n> To know more about this technique and the differences between it and stemming i suggest you to have a look into this URL [https:\/\/medium.com\/geekculture\/introduction-to-stemming-and-lemmatization-nlp-3b7617d84e65](http:\/\/)\n","d7ae83f0":"###### <span style=\"color:#5642C5;\"> **7.1.6.1 Test the model with custom reviews** <\/span> <a class=\"anchor\"  id=\"test_model_6.7.1.6.1\"><\/a>","dcb0b5ec":"###### <span style=\"color:#5642C5;\"> **7.1.1.1 Test the model with custom reviews** <\/span> <a class=\"anchor\"  id=\"test_model_1.7.1.2\"><\/a>","dc34638e":"## <span style=\"color:#5642C5;\"> 8. RESULTS \u2728\ud83c\udf89 <\/span> <a class=\"anchor\"  id=\"results\"><\/a>\n\n> In this part, I  will test the result of the models with custom reviews and compare the accuracy of the models to see which model has highest accuracy\n> ","7d963f88":"#### <span style=\"color:#5642C5;\"> 8.1 Model comparisons <\/span> <a class=\"anchor\"  id=\"Modelcomparisons\"><\/a>\n\n> Let's compare the models'AUC Scores,Accuracy, Recall Scores,F1 Scores, and Precision Score","7e5c184e":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:180%;\n           font-family:Verdana;\n           letter-spacing:0.9px\n            ;\">\n\n<p style=\"padding: 17px;\n              color:white;\">\n SENTIMENT ANALYSIS OF ALEXA REVIEWS \ud83c\udfc6\n<\/p>\n<\/div>\n","6adb0891":"###### <span style=\"color:#5642C5;\"> 6.4.1 count vectorizer <\/span> <a class=\"anchor\"  id=\"count_vec.6.4.1\"><\/a>\n\n> count vectorizer is a great technique from scikit-learn library. It is used to convert a given text to a vector based on the number of words occurring in the text as a whole.\n","301b6878":"###### <span style=\"color:#5642C5;\"> **7.1.5 AdaBoost with TF-IDF Vectorizer** <\/span> <a class=\"anchor\"  id=\"AdaBoost_tf\"><\/a>\n\n","acfcf884":"\"variation\" Categorical variable of the device name","f47cd294":"> Awesome \ud83d\ude0d\ud83e\uddbe, it reached 90% val accuracy after 10 epochs of training.\n","16323507":"## <span style=\"color:#5642C5;\"> 6. NLP(natural language processing) <\/span> <a class=\"anchor\"  id=\"nlp\"><\/a>\n","75751e3b":"#### <span style=\"color:#5642C5;\"> **7.2 Deep learning** <\/span> <a class=\"anchor\"  id=\"machinelearnig\"><\/a>\n","d88884d4":"## <span style=\"color:#5642C5;\"> 9. Conclusion <\/span> <a class=\"anchor\"  id=\"Conclusion\"><\/a>\n","761e024f":"#### <span style=\"color:#5642C5;\"> 4.1 Exploring the Variables <\/span> <a class=\"anchor\"  id=\"Exploringvar_4_1\"><\/a>\n","d7a0d7fb":"## <span style=\"color:#5642C5;\"> 7. MODELLING (MACHINE LEARNING & DEEP LEARNING) <\/span> <a class=\"anchor\"  id=\"modelling\"><\/a>\n","1cf2d688":"#### <span style=\"color:#5642C5;\"> **7.1 Machine Learning** <\/span> <a class=\"anchor\"  id=\"ml.7.1\"><\/a>\n\nIn Machine learning modelling we will be useing Multinomial Naive Bayes Classifer from sklearn , and will be useing both Vectorization approches  ","06271819":"## <span style=\"color:#5642C5;\"> 2. LIBRARIES <\/span> <a class=\"anchor\"  id=\"LIBRARIES\"><\/a>\n\n","947f1336":"#### <span style=\"color:#5642C5;\"> **6.1 Pre-processing the raw Reviews** <\/span> <a class=\"anchor\"  id=\"preprocessing_reviews_6_1\"><\/a>\n\n> A few techniques will be applied to get our reviews ready for Modelling :","8bb56861":"#### <span style=\"color:#5642C5;\"> **6.2 WordCloud** <\/span> <a class=\"anchor\"  id=\"wordcload\"><\/a>\n\n> Visualize the review after cleaning With a Word Cloud which enables us to discover and understand the reviews. It is a word picture in which the size of each word hence, more frequent words appear larger","19d06cf0":"##### <span style=\"color:#5642C5;\"> **7.2.1 CNN and LSTM Model** <\/span> <a class=\"anchor\"  id=\"cnn_lstm_model\"><\/a>\n\nIn Deep learning modelling we will be using CNN (Convolutional neural networks) with LSTM (Long Short-Term Memory)model . CNN are particularly good at detecting spatial structure in data. The sequence of words in reviews has a one-dimensional spatial structure, and the CNN could be able to chose out invariant features for positive and negative sentiment. An LSTM layer can then learn sequences from the learned spatial features.\n\nThe Model Architecture :\n*  The 1st layer of the model is Embedding layer which uses the 100 length vector.the Embedding layer is initialized with random weights and will learn an embedding for all of the words in the train data\n*  The 2nd layer is Conv1D with 32 convolution kernels and 2 kernel size \n*  The 3rd layer is MaxPooling1D with 2 poolsize\n*  The 4nd layer is  LSTM layer with 256 neurons which will work as the memory unit of the model. \n*  ouput layer with 1 unit \"Sigmoid function \" which will helps in providing the labels .\n\nBy using LSTM we dont need to use preprocessing taskes such as stopwords elimination coz this network have its own special feature for elimination of unnecessary information.It also has another feature: LSTM has a capability that allows it to memorise the data sequence.this features makes LSTM a powerful tool for text classification . ","6246f0ed":"> **In order to run machine learning models we have to convert the text in review to numerical feature vectors**","a6a808a8":"> **In this project, natural language processing (NLP) techniques will be applied in order to detect large-scale patterns among written reviews provided by customers on alexa devices. The goal of this project is to predict whether customers liked an Alexa device they have purchased using the information in their reviews.**\n"}}