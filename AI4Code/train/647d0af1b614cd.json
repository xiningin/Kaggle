{"cell_type":{"4562172a":"code","c8cab595":"code","0affcb46":"code","9f256d3f":"code","69aa928a":"code","ae897898":"code","b3785bcd":"code","c6d6394a":"code","789ccac1":"code","ab6bfbba":"code","27e78b79":"code","da49728a":"code","6c38ea66":"code","e50b8ff7":"code","b4de7386":"code","35535108":"code","305e0b90":"code","24f7a4d4":"code","602a99db":"code","fb5dd9eb":"code","699df227":"code","e60d1e45":"code","dda8e9ba":"code","dfb4f783":"code","91004086":"code","e8dfab52":"code","18dbcb56":"code","77e420f8":"code","87a4e8ab":"code","88c86139":"code","9cadb116":"code","01302f14":"code","caef840f":"code","dd6acf4c":"markdown","b0298874":"markdown","cb80cdc8":"markdown","3a89d9d1":"markdown","1c7bd555":"markdown","5ba4d39f":"markdown","ed30a9d8":"markdown","878b1186":"markdown","9d171e96":"markdown","8b121e34":"markdown","5128caf7":"markdown","cf63f5bb":"markdown","846d1deb":"markdown","928fdfcf":"markdown","a6ac0f1d":"markdown","e29a5fb5":"markdown","e8a340c7":"markdown","2fb2daf8":"markdown","502f8882":"markdown","fe375e22":"markdown","01629f90":"markdown","f933a803":"markdown"},"source":{"4562172a":"!pip install -q efficientnet","c8cab595":"#libraries\n\nimport numpy as np\nimport pandas as pd \nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nimport matplotlib.pyplot as plt\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\nsns.set()\n\nimport re, math, cv2, PIL\nimport os\nos.listdir('..\/input\/chest-xray-pneumonia\/chest_xray\/')","0affcb46":"#getting paths and files\n\nhome_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\nnor_train = os.listdir(home_dir + 'train\/NORMAL')\npne_train = os.listdir(home_dir + 'train\/PNEUMONIA')\nnor_val = os.listdir(home_dir + 'val\/NORMAL')\npne_val = os.listdir(home_dir + 'val\/PNEUMONIA')","9f256d3f":"print(\"Normal pictures\")\nplt.figure(figsize=[20, 8])\nfor i in range(10):\n    img = cv2.imread(home_dir + 'train\/NORMAL\/' + nor_train[i])\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.subplot(2, 5, i+1)\n    plt.axis('off')\n    plt.imshow(img)\nplt.show()\n\nprint(\"Pneumonia pictures\")\nplt.figure(figsize=[20, 8])\nfor i in range(10):\n    img = cv2.imread(home_dir + 'train\/PNEUMONIA\/'+ pne_train[i])\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.subplot(2, 5, i+1)\n    plt.axis('off')\n    plt.imshow(img)\nplt.show()","69aa928a":"DEVICE = 'TPU'\n\n#IMAGE SIZES\nIMG_SIZE = 256\n\n#batch size and epochs\nBATCH_SIZE = 16\nEPOCHS = 8\n\n# Test Time Augmentation\nTTA = 5","ae897898":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","b3785bcd":"gcs_path = KaggleDatasets().get_gcs_path('chest-xray-pneumonia')","c6d6394a":"#method for getting files paths\ndef get_files_labels(folder):\n    normal = os.listdir(home_dir + f'{folder}\/NORMAL')\n    pneumonia = os.listdir(home_dir + f'{folder}\/PNEUMONIA')\n    normal_labels = np.int64(np.zeros(len(normal)))\n    pneumonia_labels = np.int64(np.ones(len(pneumonia)))\n    labels = np.concatenate([normal_labels, pneumonia_labels])\n    files_path = [gcs_path + f'\/chest_xray\/{folder}\/NORMAL\/{file_name}' for file_name in normal] + [gcs_path + f'\/chest_xray\/{folder}\/PNEUMONIA\/{file_name}' for file_name in pneumonia]\n    files_path = np.array(files_path)\n    return files_path, labels","789ccac1":"# get paths\ntrain_files, train_labels = get_files_labels('train')\nvalid_files, valid_labels = get_files_labels('val')\ntest_files, test_labels = get_files_labels('test')","ab6bfbba":"train_files = np.concatenate([train_files, valid_files])\ntrain_labels = np.concatenate([train_labels, valid_labels])\ntrain_files, valid_files, train_labels, valid_labels = train_test_split(train_files, train_labels, shuffle=True, test_size=0.2)","27e78b79":"# imbalance Valid Data\npd.Series(valid_labels).value_counts()","da49728a":"#ploting\n\nimbalance = pd.DataFrame([[len(nor_train), 'Normal'], [len(pne_train), 'Pneumonia']], columns=['Numbers', 'Case'])\nimbalance['Percentage'] = round(((imbalance['Numbers'] \/ sum(imbalance['Numbers'])) * 100),2)\nplt.figure(figsize=[6,6])\nsns.barplot(x=imbalance['Case'], y=imbalance['Percentage'])\nplt.title('Checking imbalancing', fontsize=16)\nplt.show()","6c38ea66":"# configuration for augmentation\n\nROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0","e50b8ff7":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))","b4de7386":"def transform(image, DIM=256):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM\/\/2, -DIM\/\/2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM\/\/2, DIM\/\/2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM\/\/2+XDIM+1, DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM, DIM,3])","35535108":"def prepare_img(img, augment, dim):\n    img = tf.io.read_file(img)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [dim, dim])\n    img = tf.cast(img, tf.float32) \/ 255.\n    \n    if augment:\n        img = transform(img, DIM=dim)\n        img = tf.image.random_brightness(img, 0.1)\n        img = tf.image.random_flip_left_right(img)\n    \n    img = tf.reshape(img, [dim,dim, 3])\n    return img","305e0b90":"def get_dataset(files, augment = False, shuffle = False, repeat = False, \n                label=None, return_image_names=True):\n    if label is not None:\n        data = tf.data.Dataset.from_tensor_slices((files, label))\n    else:\n        data = tf.data.Dataset.from_tensor_slices((files))\n    \n    data.cache()\n    \n    if repeat:\n        data = data.repeat()\n    \n    if shuffle:\n        data = data.shuffle(1024*3)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        data = data.with_options(opt)\n    \n    if label is not None:\n        data = data.map(lambda img, target: (prepare_img(img, augment, IMG_SIZE), target), num_parallel_calls=AUTO)\n    else:\n        data = data.map(lambda img: (prepare_img(img, augment, IMG_SIZE)), num_parallel_calls=AUTO)\n    \n    data = data.batch(BATCH_SIZE * REPLICAS)\n    data = data.prefetch(AUTO)\n    return data","24f7a4d4":"#get images\n\ntrain_dataset = get_dataset(train_files, label=train_labels, augment=True, repeat=True, shuffle=True)\nvalid_dataset = get_dataset(valid_files, label=valid_labels)\ntest_dataset = get_dataset(test_files, label=test_labels)","602a99db":"def build_model(dim, output_bias=None):\n    \n    #data is imbalance. For imbalance data accuracy metrics is a bad choice. So I'll choose other metrics.  The metrics \n    #Which are specifically for imbalance data.\n    \n    METRICS = [\n      tf.metrics.TruePositives(name='tp'),\n      tf.metrics.FalsePositives(name='fp'),\n      tf.metrics.TrueNegatives(name='tn'),\n      tf.metrics.FalseNegatives(name='fn'), \n      tf.metrics.BinaryAccuracy(name='accuracy'),\n      tf.metrics.Precision(name='precision'),\n      tf.metrics.Recall(name='recall'),\n     # tf.metrics.AUC(name='auc'),\n    ]\n    \n    if output_bias is not None:\n        output_bias = tf.initializers.Constant(output_bias)\n    \n    input_layer = tf.keras.Input((dim, dim, 3), name='ImgIn')\n    \n    base_model = efn.EfficientNetB6(input_shape=(dim, dim, 3), weights='imagenet', \n                          include_top=False)\n    \n    x = base_model(input_layer)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs= input_layer, outputs = x)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    \n    model.compile(optimizer=optimizer, loss= loss, metrics= METRICS)\n    return model","fb5dd9eb":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","699df227":"# model check point\nsv = tf.keras.callbacks.ModelCheckpoint( 'pneumonia_detection_model.h5', monitor= 'val_loss', verbose=0, \n                                            save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')","e60d1e45":"# Counts data\n#labels are in nd_array so first i'll convert them to pandas series than convert them.\n#than use value_counts() 1 indicates pneumonia cases and 0 for normal cases.\n\nCOUNT_NORMAL = pd.Series(train_labels).value_counts()[0]\nCOUNT_PNEUMONIA = pd.Series(train_labels).value_counts()[1]\nCOUNT_TRAIN = len(train_files)","dda8e9ba":"#initializing bias\ninitial_bias = np.log([COUNT_PNEUMONIA\/COUNT_NORMAL])\ninitial_bias","dfb4f783":"#initializing weights\n\nweight_for_0 = (1\/COUNT_NORMAL)*(COUNT_TRAIN)\/2.0\nweight_for_1 = (1\/COUNT_PNEUMONIA)*(COUNT_TRAIN)\/2.0\n\nclass_weight = {0:weight_for_0, 1:weight_for_1}\n\nprint(f'Weight for Normal Class: {weight_for_0}')\nprint(f'Weight for Pneumonia Class: {weight_for_1}')","91004086":"with strategy.scope():\n    model = build_model(IMG_SIZE)","e8dfab52":"model.summary()","18dbcb56":"history = model.fit(train_dataset, epochs=EPOCHS, callbacks=[sv, get_lr_callback(BATCH_SIZE)], \n                    steps_per_epoch=COUNT_TRAIN\/BATCH_SIZE\/\/REPLICAS, validation_data=valid_dataset, verbose=1, \n                    class_weight=class_weight)","77e420f8":"#evaluating test data without tta\nloss, tp, fp, tn, fn, acc, precision, recall = model.evaluate(test_dataset)","87a4e8ab":"print(f'Model accuracy without TTA: {round(acc,4)}')","88c86139":"TEST_COUNTS = len(test_labels)\nSTEPS = TTA * TEST_COUNTS \/ BATCH_SIZE \/ REPLICAS\n\npred = model.predict(get_dataset(test_files, label=None, repeat=True, augment=True), steps=STEPS, verbose=1)[:TTA * TEST_COUNTS]","9cadb116":"pred_tta = np.mean(pred.reshape((TEST_COUNTS, TTA), order='F'), axis=1)\naccuracy = accuracy_score(y_pred=pred_tta.round(), y_true=test_labels)","01302f14":"confusion_matrix(y_pred=pred_tta.round(), y_true=test_labels)","caef840f":"print(f'Model accuracy with TTA: {round(accuracy,4)}')","dd6acf4c":"### Notebook Versions History\n**Version 1:** <br>\nBase version <br>\naccuracy = 82.85% <br><br>\n**Version 2,3:** <br>\nTried Custom weights and bias. Only weights perform well<br>\nNumber of epochs decreased because model was converging fast.<br>\naccuracy = 84.46% <br>\n**Issue:** When i was training model **Test** accuracy was 90% but after commiting notebook test accuracy reduced to 84%.<br><br>\n**Version 4:**<br>\nTried Test Time Augmentation.","b0298874":"# Domain Knowledge","cb80cdc8":"# Initializing Environment","3a89d9d1":"**Upvote if you like it.**","1c7bd555":"Our data is imbalance so we will need to fix this.","5ba4d39f":"## Visualizing some pictures","ed30a9d8":"# Step 3: Build Model","878b1186":"# Step 2: Data Augmentation","9d171e96":"# Step 7: Predictions\/Evaluation","8b121e34":"### Prediction With TTA","5128caf7":"# Configuration","cf63f5bb":"# Step 6: Train Model","846d1deb":"Valid files have only 16 pics. I will concatenate with train files and split standard 80, 20.","928fdfcf":"![Pneumonia](https:\/\/www.drugs.com\/health-guide\/images\/022dc126-fc5d-4e54-9a78-75f2c9ea4bb6.jpg)\n### Overview\nPneumonia is an infection that inflames the air sacs in one or both lungs. The air sacs may fill with fluid or pus (purulent material), causing cough with phlegm or pus, fever, chills, and difficulty breathing. A variety of organisms, including bacteria, viruses and fungi, can cause pneumonia. <br> <br>\nPneumonia can range in seriousness from mild to life-threatening. It is most serious for infants and young children, people older than age 65, and people with health problems or weakened immune systems.\n### Causes\nMany germs can cause pneumonia. The most common are bacteria and viruses in the air we breathe. Your body usually prevents these germs from infecting your lungs. But sometimes these germs can overpower your immune system, even if your health is generally good. <br> <br>\nPneumonia is classified according to the types of germs that cause it and where you got the infection.\n* Community-acquired pneumonia\n* Hospital-acquired pneumonia\n* Health care-acquired pneumonia\n* Aspiration pneumonia <br>\n[Read More](https:\/\/www.mayoclinic.org\/diseases-conditions\/pneumonia\/symptoms-causes\/syc-20354204)","a6ac0f1d":"This notebook uses rotation, sheer, zoom, shift augmentation first shown in this notebook [here](https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96#Data-Augmentation-using-GPU\/TPU-for-Maximum-Speed!) and successfully used in Melanoma comp by AgentAuers [here](https:\/\/www.kaggle.com\/agentauers\/incredible-tpus-finetune-effnetb0-b6-at-once).","e29a5fb5":"# Step 1: Preprocess","e8a340c7":"# Step 4: Train Schedule","2fb2daf8":"## EfficientNet Model \nI will use EfficientNet (Rethinking Model) Model. EfficientNet propose a novel model scaling method that uses a simple yet highly effective compound coefficient to scale up CNNs in a more structured manner. Unlike conventional approaches that arbitrarily scale network dimensions, such as width, depth and resolution, our method uniformly scales each dimension with a fixed set of scaling coefficients.<br><br>\n**Compound Model Scaling:**\n![loading Error](https:\/\/1.bp.blogspot.com\/-Cdtb97FtgdA\/XO3BHsB7oEI\/AAAAAAAAEKE\/bmtkonwgs8cmWyI5esVo8wJPnhPLQ5bGQCLcBGAs\/s640\/image4.png)\n<br>\nComparison of different scaling methods. Unlike conventional scaling methods (b)-(d) that arbitrary scale a single dimension of the network, scaling method uniformly scales up all dimensions in a principled way.<br><br>\n**EfficientNet Architecture**\n![loading Error](https:\/\/1.bp.blogspot.com\/-DjZT_TLYZok\/XO3BYqpxCJI\/AAAAAAAAEKM\/BvV53klXaTUuQHCkOXZZGywRMdU9v9T_wCLcBGAs\/s640\/image2.png)<br>\nIf you want to know everything about this model click [here](https:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html#:~:text=EfficientNet%3A%20Improving%20Accuracy%20and%20Efficiency%20through%20AutoML%20and%20Model%20Scaling,-Wednesday%2C%20May%2029&text=Powered%20by%20this%20novel%20scaling,efficiency%20(smaller%20and%20faster).).","502f8882":"Weight of Normal class is greater than Pneumonia because Normal have less images. These weights will balance the CNN for training by increasing images weights.","fe375e22":"## Checking imbalancing of Data","01629f90":"# Step 5: Correct for imbalance Data\nWe saw earlier data is imbalance. Pneumonia has more images than normal. we will fix this here.","f933a803":"### Evaluation Without TTA"}}