{"cell_type":{"b7c827c9":"code","02642c62":"code","250a321a":"code","64d426cc":"code","fb6c5c56":"code","f9bca722":"code","a8078711":"code","c3a06248":"code","3084f24e":"code","dd875a1a":"code","0341224d":"code","4ba7b887":"code","e362d192":"code","a4dbaf41":"code","14e69e19":"code","b98b4bab":"code","6e097a5d":"code","6988d702":"code","6dcd6f46":"code","0472c5f0":"code","63ed69e2":"code","6eaeee54":"code","d1c67e90":"code","22931f72":"code","d5ee5e62":"code","e962ea3b":"code","4fa9212d":"markdown","83757e1d":"markdown","233fa2c5":"markdown","1d47f1df":"markdown","e9dd4501":"markdown","3e8d09b2":"markdown","7f7359ed":"markdown","9917df4a":"markdown","ad8421b4":"markdown","a10656c7":"markdown","79d77ac2":"markdown","e8e3b314":"markdown","7c5f85f8":"markdown","ac01f706":"markdown"},"source":{"b7c827c9":"import numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport json\nfrom shapely.geometry import LineString, Point\n\ndef load_data(fname, nrows):\n    df = pd.read_csv(fname, nrows=nrows)\n    df['traj'] = json.loads('[' + df.POLYLINE.str.cat(sep=',') + ']')\n    df = df[df.traj.str.len() > 1].copy()\n    df['lines'] = gpd.GeoSeries(df.traj.apply(LineString))\n    return gpd.GeoDataFrame(df, geometry='lines')\n\ndf = load_data('..\/input\/train.csv', nrows=1000)\ndf.head()","02642c62":"%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = [15,8]\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\ndf.lines.plot(figsize=[15,15]);\nplt.xlabel('Longitude')\nplt.ylabel('Latitude');","250a321a":"df.iloc[[2]].lines.plot(figsize=[15,15]);\nplt.xlabel('Longitude')\nplt.ylabel('Latitude');","64d426cc":"df.iloc[[3]].lines.plot(figsize=[15,15]);\nplt.xlabel('Longitude')\nplt.ylabel('Latitude');","fb6c5c56":"from sklearn.neighbors import DistanceMetric\nmetric = DistanceMetric.get_metric('haversine')\ndef haversine(x):\n    return metric.pairwise(np.radians(x)[:,::-1])\nR = 6371 # radius of earth in km\ndt = 15\/3600 # coordinates are reported in 15 second intervals\n\ndef velocities(coords):\n    return R\/dt*np.diag(haversine(coords)[1:])\n\ndef plot_cdf(ax, values, xlabel, logscale=False):\n    ax.set_ylabel('F(x)')\n    ax.set_xlabel(xlabel)\n    sns.lineplot(ax=ax, x=np.sort(values), y=np.arange(1,len(values)+1)\/len(values), palette='tab10')\n    if logscale:\n        plt.xscale('log')\n\nfig, ax = plt.subplots()\nplot_cdf(ax, velocities(df.traj[2]), 'Speed (km\/h)')\nplot_cdf(ax, velocities(df.traj[3]), 'Speed (km\/h)')","f9bca722":"def velocity_graph(ax, coords):\n    n = len(coords)\n    dist = haversine(coords)\n    interval = dt*np.abs(np.arange(n)[:,None] - np.arange(n)).clip(1)\n    vel = R*dist\/interval\n    sns.heatmap(vel, ax=ax, square=True, robust=True)\n    ax.set_title('Velocity (km\/h)')\n    ax.set_xlabel('GPS Sample Index')\n    ax.set_ylabel('GPS Sample Index')\n\nfig, axes = plt.subplots(1, 2, figsize=[25,10])\nfig.suptitle(\"Average Velocities Between GPS Samples\")\nvelocity_graph(axes[0], df.traj[2])\nvelocity_graph(axes[1], df.traj[3])","a8078711":"df.iloc[[2,3]].plot(figsize=[15,15]);\nplt.xlabel('Longitude')\nplt.ylabel('Latitude');","c3a06248":"fig, ax = plt.subplots(1, 2, figsize=[25,10])\nvelocity_graph(ax[0], df.traj[45])\nvelocity_graph(ax[1], df.traj[39])","3084f24e":"df.iloc[[45,39]].plot(figsize=[15,15]);\nplt.xlabel('Longitude')\nplt.ylabel('Latitude');","dd875a1a":"def offset_distances(coords, offset):\n    dist = R*haversine(coords)\n    dists = np.diag(dist[offset:])\n    return dists\n\ndef plot_dists(ax, di):\n    dists = np.hstack(df.traj.apply(lambda x: offset_distances(x, di)).values)\n    sns.distplot(dists, ax=ax, kde=False, bins=300)\n    ax.set_title('Time Offset: {} min'.format(di*dt*60))\n    ax.set_xlabel('Distance (km)')\n    ax.set_ylabel('Count')\n\nfig, axes = plt.subplots(2,2, figsize=[20,17])\nplot_dists(axes[0,0], 1)\nplot_dists(axes[0,1], 10)\nplot_dists(axes[1,0], 40)\nplot_dists(axes[1,1], 100)","0341224d":"def dist_sequence(coords):\n    n = len(coords)\n    dist = R*metric.pairwise(np.radians(coords)).ravel()\n    offsets = (np.arange(n)[:,None] - np.arange(n)).ravel()\n    return pd.DataFrame([offsets[offsets>0]*dt*60,dist[offsets>0]], index=['time_offset', 'distance']).T\n\ndist_ungrouped = pd.concat(df.traj.apply(dist_sequence).values).set_index('time_offset')","4ba7b887":"dists = np.sqrt((dist_ungrouped**2).groupby('time_offset').mean()\/2)\nsns.lineplot(data=dists)\nplt.xlabel('Time Offset (minutes)')\nplt.ylabel('Distance (km)');","e362d192":"def fit_rational(x,y,w=1):\n    ws = np.sqrt(w)\n    (a,b),_,_,_ = np.linalg.lstsq(np.column_stack([x,-y])*ws[:,None], x*y*ws, rcond=None)\n    return a*x\/(x+b), (a,b)\n\ndists['curve'], coeffs = fit_rational(dists.index.values, dists.distance.values, ((1+np.arange(len(dists)))\/(1+len(dists)))**-3)\nprint('a:', coeffs[0])\nprint('b:', coeffs[1])\nprint('a\/b:', coeffs[0]\/coeffs[1])\nsns.lineplot(data=dists);\nplt.xlabel('Time Offset (minutes)')\nplt.ylabel('Distance (km)');","a4dbaf41":"def likelihood(coords, ab):\n    n = len(coords)\n    a,b = coeffs\n    dist = R*metric.pairwise(np.radians(coords))\n    time = dt*60*np.abs(np.arange(n)[:,None] - np.arange(n))\n    sigma = a*time\/(time + b) + np.eye(n)\n    lr = -0.5*(dist**2\/sigma**2).sum(axis=1)\n    return lr\n\ndef norm_lr(lr):\n    return (lr-lr.max())\/len(lr)\n\ndef plot_likelihood(ax, coords):\n    lr = likelihood(coords,coeffs)\n    lr = norm_lr(lr)\n    sns.lineplot(x=np.arange(len(coords)),y=lr, ax=ax);\n    ax.set_xlabel('Sequence ID')\n    ax.set_ylabel('Normalized Likelihood')\n\nfig,ax = plt.subplots(2,2, figsize=[20,10])\nplot_likelihood(ax[0,0], df.traj[2])\nplot_likelihood(ax[0,1], df.traj[3])\nplot_likelihood(ax[1,0], df.traj[45])\nplot_likelihood(ax[1,1], df.traj[39])","14e69e19":"n = int(df.traj.str.len().quantile(0.9))\nthresh = -2\ninvalid = df.traj.apply(lambda t: (norm_lr(likelihood(t,coeffs)) < thresh)[:n].tolist() + [False]*(n-len(t))).values.tolist()\nplt.figure(figsize=[20,300])\nsns.heatmap(data=invalid, cbar=False);\nplt.xlabel('Sequence ID')\nplt.ylabel('Row ID');","b98b4bab":"bad_routes = df.traj.apply(lambda t: (norm_lr(likelihood(t,coeffs)) < thresh).any()).values\nprint(\"Routes with invalid points: {} \/ 1000\".format(bad_routes.sum()))","6e097a5d":"def spot_check(i):\n    coords = np.array(df.iloc[i].traj)\n    bad = norm_lr(likelihood(coords, coeffs)) < thresh\n    df.iloc[[i]].plot()\n    plt.scatter(x=coords[bad,0],y=coords[bad,1], color='red')\n    plt.show()\nspot_check(15)","6988d702":"spot_check(690)","6dcd6f46":"spot_check(941)","0472c5f0":"spot_check(848)","63ed69e2":"spot_check(924)","6eaeee54":"spot_check(603)","d1c67e90":"spot_check(730)","22931f72":"spot_check(629)","d5ee5e62":"spot_check(625)","e962ea3b":"df[~bad_routes].lines.plot(figsize=[15,15]);","4fa9212d":"Each row of the dataset contains taxi information and trajectories for that taxi. The coordinates in the trajectories are evenly spaced by 15 second intervals.","83757e1d":"About 10% of routes have been detected to have invalid GPS coordinates. That's a lot, in my opinion! Time for some spot checking.","233fa2c5":"These distribution plots show some interesting behavior. First, there are a lot of pairs of points with approximately 0 distance from each other. Second, once the time between GPS samples is about 10 minutes, it reaches a static equilibrium. Until that, it seems to match a Rayleigh distribution fairly well (i.e. the magnitude of a gaussian distribution). A simple estimate of the standard deviation of the gaussian process is given by $$\\hat{\\sigma}^2 = \\frac{1}{2N}\\sum_{i=1}^N x_i^2.$$ Let's look at this (or the square root of it, since it's more intuitive) as a function of time: ","1d47f1df":"The above plot approximates the standard deviation of an underlying 2-dimensional normal distribution. Intuitively, it represents the average distance between GPS points as the time offset increases. After about 10-20 minutes, the distance stabilizes. This makes sense: taxis generally only service a region of limited length; in this case, a region of about 5 km.\n\nFrom a modeling standpoint, we mostly care about the first 10 minutes. Let's zoom in on this section and fit a rational function to it, $$y=\\frac{ax}{x+b},$$ which is equivalent to minimizing $$C = \\sum_i (ax_i - x_iy_i - by_i)^2.$$\nThis is easy to do by using `lstsq` in `numpy`.","e9dd4501":"Ok, this visualization makes it extremely obvious which GPS coordinates are a problem. Just to double check let's try to find some routes that don't have any problems.","3e8d09b2":"As expected, the speed is *significantly* out of character with the rest of the speeds in the trip.\nA simple algorithm to detect invalid GPS data is would be to set a threshold, say at 200 km\/hr, and hope that it catches most of the bad routes in the dataset. Before we do that, however, let's look more closely at the pairwise distances between GPS coordinates. Specifically, let's plot the velocities between points as the distances increases.","7f7359ed":"Nice! In general, this seems to do a pretty good job of detecting outlier GPS segments. In some cases it may be a little too conservative (e.g. 629), but for the most part this does a good job. Of course, once we have a good estimate of the what points are invalid, there are plenty of options for further work:\n\n1. What causes invalid points to appear? Are there tall buildings nearby?\n2. How can we replace the invalid segments with good estimates of the actual location?\n3. Do some taxis have a higher likelihood of having invalid points?\n4. If the invalid route consists of multiple segments of similar duration, how do we pick the segment more likely to be the truth?\n5. Does it improve results if we use the road network to estimate the distance between two points?\n\nSome of these questions may require domain knowledge to answer (of which I have very little). The interesting thing about this approach is how little domain knowledge it requires. In other words, all we need is a way to estimate $\\sigma$ between two GPS points, which means it can be applied to pretty much anything that generates GPS coordinates (hikers\/bikers, regular cars, animal trackers, etc.).","9917df4a":"From what I know about taxis, these routes clearly have discontinuities. Let's first try to characterize outliers so that we can detect them automatically. To get a feel for how much the \"speed\" at the discontinuities is out of character for the trip, let's look at the empirical distribution function of speeds in the trip.","ad8421b4":"Ok, these seem like \"normal\" routes. So we seem to have a heuristic way to identify bad GPS points (i.e. look at pairwise velocities). How can we characterize them systematically? One thought is to use information about the road network. If a taxi appears to cross major highways, it is more likely an error in the GPS coordinates.\n\nBut is there a way to detect them without using this side information? Consider what the images represent: the average velocity between two points in time. In other words, it is the distance between points normalized by the time it required to reach that point. If the taxi were always traveling in a straight line, this would be equal to the average (instantaneous) speed of the taxi between those points. However, the taxi has rather erratic behavior, actually. In fact, what if we consider the taxi routes to be random walks? First, let's confirm this by considering the distribution of distances between points.","a10656c7":"Let's see if we can use this to detect routes with invalid GPS points.","79d77ac2":"That looks fairly good. I'll be honest, the weights $w_i = (n+1)^3\/(i+1)^3$ were arbitrarily chosen to indicate that we care more about the shape of the curve for small time offsets than later on. Furthermore, observe that the inital slope of the line is given by $a\/b$ and the limiting value as $t\\to\\infty$ is given by $a$. The value of $b$ gives a good idea of where the \"elbow\" of the function exists. For example,\n\\begin{aligned}\n\\alpha &= \\frac{x}{x+b} \\\\\n\\alpha x + \\alpha b &= x \\\\\n\\left(\\frac{\\alpha}{1-\\alpha}\\right)b &= x \\\\\n\\end{aligned}\nindicates the value of $x$ where the function has reached $\\alpha$ of it's maximum value. Let's define the elbow as the value where $\\alpha=0.8$; i.e. $x_0=4b$. In the above case, the elbow is at a time offset of approximately 20 minutes. This is a useful quantity because it indicates the amount of time that we can consider the GPS points in a particular taxi route to be correlated.\n\nSo how can we use this to detect invalid GPS coordinates? We can frame this using Bayesian probability as such: let $x_i$ be a location in a route, and $r_i$ be the rest of the locations in that route. Apply Bayes Rule: $$P(x_i | r_i) = \\frac{P(r_i | x_i) P(r_i)}{P(x_i)} .$$ Assume that $P(x_i)$ and $P(r_i)$ are constant for a particular route. Furthermore, assume $P(x_{j\\ne i} | x_i)$ are independent (a naive Bayes). Then:\n\\begin{aligned}\n\\log{P(x_i | r_i)} &= C + \\sum_{j \\ne i} \\log P(x_j | x_i) \\\\\n &= C + \\sum_{j \\ne i} \\frac{-d(x_i,x_j)^2}{2\\sigma_{ij}^2} - \\log \\sigma_{ij} , \\\\\n \\end{aligned}\n where $C$ absorbs constant offsets and $\\sigma_{ij}$ is the standard deviation of the random walk distances (estimated by the curve shown above)\n . Clearly, this is maximized when $\\forall i \\ne j, \\, d(x_i,x_j) =0$ (the taxi never moves). Define the log-likelihood ratio as $$LR(x_i | r_i) = -\\frac{1}{2}\\sum_{j \\ne i} \\frac{d(x_i,x_j)^2}{\\sigma_{ij}^2} .$$\n This quantity estimates the probability of the rest of the taxi's route given a single location, divided by the probability of a route where it never moves. Let's try this out for a couple routes.","e8e3b314":"And finally, let's plot only the routes not detected to have invalid point.","7c5f85f8":"Already we can see the existence of inconsistent data&mdash;there are huge discontinuities in the taxi routes, probably due to outliers or a misunderstanding of how the data was collected. Let's look at a particular instance of this:","ac01f706":"# Identifying Invalid GPS Points in Taxi Trips\n\nThis notebook explores automatically detecting invalid GPS points in taxi trip data from\nPorto, Portugal. My goal was to determine a somewhat systematic method of doing so; the \napproach that I converged on was to model the movement of a taxi as a random walk and\napproximate the likelihood of each coordinate pair given the other coordinate pairs in\na sequence.\n\nTo speed up the process of investigation, I only used the first 1000 rows of the data set\nand assumed that it was representative of the rest of the data. After doing the analysis,\nI discovered that this might not be true (in particular, there are a couple long trips in\nlater segments). However, I think 1000 examples is large enough that the analysis still\nholds enough to demonstrate the concept."}}