{"cell_type":{"3c585fd7":"code","1b6fa6a2":"code","5dfc621e":"code","50360d36":"code","8116e2b2":"code","fce0d6d4":"code","30c940bb":"code","635a1a48":"code","a5a87d46":"code","1912594c":"code","5cf6d8ac":"code","aeab40e4":"code","67f60afa":"code","70349356":"code","f587fbfe":"code","030260bd":"code","46e844b9":"code","c1aaad74":"code","baac9fe8":"code","638271b4":"code","26383951":"code","098c564b":"code","455b1001":"code","4ba56a28":"code","ad4dbc4c":"code","68283bd4":"code","4539caf4":"code","bbcd8172":"code","c5b95d80":"code","1f492c15":"code","e7bb9299":"markdown","1ec10d52":"markdown","542f68d0":"markdown","1cb1a6aa":"markdown","f9dda293":"markdown","39b9675f":"markdown","f9db3639":"markdown","8f92fdf1":"markdown","b65f8ac8":"markdown","93777ccc":"markdown","b92fe52f":"markdown","39d02172":"markdown","d70bb6c2":"markdown","f1606e35":"markdown","78ed95aa":"markdown","6192e442":"markdown","da7475c5":"markdown","d24ddb8c":"markdown"},"source":{"3c585fd7":"! lsb_release -a","1b6fa6a2":"! nvcc --version","5dfc621e":"! pip3 install tensorflow==1.14.0","50360d36":"! apt-get update && \\\n    apt-get install -y \\\n        libnvonnxparsers-dev=6.0.1-1+cuda10.2 \\\n        libnvparsers-dev=6.0.1-1+cuda10.2 \\\n        libnvinfer-plugin-dev=6.0.1-1+cuda10.2 \\\n        libnvinfer-dev=6.0.1-1+cuda10.2 \\\n        python3-libnvinfer=6.0.1-1+cuda10.2 \\\n        python3-libnvinfer-dev=6.0.1-1+cuda10.2","8116e2b2":"! dpkg -i \/kaggle\/input\/tensorrt-install-packages\/nv-tensorrt-repo-ubuntu1804-cuda10.2-trt6.0.1.8-ga-20191108_1-1_amd64.deb && \\\n    apt-key add \/var\/nv-tensorrt-repo-cuda10.2-trt6.0.1.8-ga-20191108\/7fa2af80.pub && \\\n    apt-get update && \\\n    apt-get install -y tensorrt # uff-converter-tf","fce0d6d4":"! python3 -m pip install --no-cache-dir \\\n    \/kaggle\/input\/tensorrt-install-packages\/TensorRT-6.0.1.8.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn7.6\/TensorRT-6.0.1.8\/python\/tensorrt-6.0.1.8-cp37-none-linux_x86_64.whl \\\n    \/kaggle\/input\/tensorrt-install-packages\/TensorRT-6.0.1.8.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn7.6\/TensorRT-6.0.1.8\/uff\/uff-0.6.5-py2.py3-none-any.whl \\\n    \/kaggle\/input\/tensorrt-install-packages\/TensorRT-6.0.1.8.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn7.6\/TensorRT-6.0.1.8\/graphsurgeon\/graphsurgeon-0.4.1-py2.py3-none-any.whl","30c940bb":"import tensorrt as trt\nimport ctypes\nimport uff\nimport tensorrt as trt\nimport graphsurgeon as gs\nimport argparse\nimport tensorflow as tf","635a1a48":"! \/usr\/src\/tensorrt\/bin\/trtexec","a5a87d46":"! \/usr\/src\/tensorrt\/bin\/trtexec \\\n    --model=..\/input\/pretrained-trt-engines-cocotacohardhatposenet\/base-models\/resnet10_fd_lpd.caffemodel \\\n    --deploy=..\/input\/pretrained-trt-engines-cocotacohardhatposenet\/base-models\/resnet10_fd_lpd.prototxt \\\n    --output=output_bbox \\\n    --saveEngine=resnet10_fd_lpd.engine","1912594c":"    ! \/usr\/src\/tensorrt\/bin\/trtexec \\\n    --onnx=..\/input\/pretrained-trt-engines-cocotacohardhatposenet\/tensorrt6\/resnet18_posenet.onnx \\\n    --saveEngine=resnet18_posenet.engine","5cf6d8ac":"# https:\/\/forums.developer.nvidia.com\/t\/tensorrt-error-could-not-register-plugin-creator-flattenconcat-trt-in-namespace\/83631\/3\n! sed -i 's\/\"FlattenConcat_TRT\"\/\"_FlattenConcat_TRT\"\/g' \/usr\/src\/tensorrt\/samples\/python\/uff_ssd\/plugin\/FlattenConcat.cpp","aeab40e4":"! cd \/usr\/src\/tensorrt\/samples\/python\/uff_ssd && \\\n    mkdir -p build && \\\n    cd build && \\\n    cmake .. && \\\n    make && \\\n    mkdir -p \/kaggle\/working\/lib && \\\n    cp libflattenconcat.so \/kaggle\/working\/lib\/","67f60afa":"ctypes.CDLL(\".\/lib\/libflattenconcat.so\")","70349356":"# https:\/\/devtalk.nvidia.com\/default\/topic\/1069148\/tensorrt\/how-to-modify-nms-inputorder-in-sampleuffssd-config-py\/\n# https:\/\/devtalk.nvidia.com\/default\/topic\/1061865\/tensorrt\/problem-converting-customised-trained-ssd-mobilenet-v2-uff-to-engine-format-tensorrt\/\n# https:\/\/forums.developer.nvidia.com\/t\/problems-with-ssd-mobilenet-v2-uff\/74326\n\n# Preprocess function to convert TF model to UFF\ndef ssd_mobilenet_v2_unsupported_nodes_to_plugin_nodes(ssd_graph, input_shape, num_classes):\n  # Makes ssd_graph TensorRT comparible using graphsurgeon.\n  channels, height, width = input_shape\n\n  Input = gs.create_plugin_node(\n    name=\"Input\",\n    op=\"Placeholder\",\n    dtype=tf.float32,\n    shape=[1, channels, height, width]\n  )\n  PriorBox = gs.create_plugin_node(\n    name=\"GridAnchor\",\n    op=\"GridAnchor_TRT\",\n    minSize=0.2,\n    maxSize=0.95,\n    aspectRatios=[1.0, 2.0, 0.5, 3.0, 0.33],\n    variance=[0.1, 0.1, 0.2, 0.2],\n    featureMapShapes=[19, 10, 5, 3, 2, 1],\n    numLayers=6\n  )\n  NMS = gs.create_plugin_node(\n    name=\"NMS\",\n    op=\"NMS_TRT\",\n    shareLocation=1,\n    varianceEncodedInTarget=0,\n    backgroundLabelId=0,\n    confidenceThreshold=1e-8,\n    nmsThreshold=0.6,\n    topK=100,\n    keepTopK=100,\n    numClasses=num_classes,\n    # see above for extra information\n    inputOrder=[1,0,2],\n    confSigmoid=1,\n    isNormalized=1\n  )\n  concat_priorbox = gs.create_node(\n    \"concat_priorbox\",\n    op=\"ConcatV2\",\n    dtype=tf.float32,\n    axis=2\n  )\n  concat_box_loc = gs.create_plugin_node(\n    \"concat_box_loc\",\n    op=\"_FlattenConcat_TRT\",\n    dtype=tf.float32,\n    axis=1,\n    ignoreBatch=0\n  )\n  concat_box_conf = gs.create_plugin_node(\n    \"concat_box_conf\",\n    op=\"_FlattenConcat_TRT\",\n    dtype=tf.float32,\n    axis=1,\n    ignoreBatch=0\n  )\n\n  # Create a mapping of namespace names -> plugin nodes.\n  namespace_plugin_map = {\n    \"MultipleGridAnchorGenerator\": PriorBox,\n    \"Postprocessor\": NMS,\n    \"Preprocessor\/map\": Input,\n    \"ToFloat\": Input,\n    # \"image_tensor\": Input,\n    # Future warning: Note for updated tf models api: https:\/\/devtalk.nvidia.com\/default\/topic\/1057901\/jetson-nano\/error-while-converting-ssd-mobilenet-v2-to-tensorrt-engine-in-nano\/\n    # \"MultipleGridAnchorGenerator\/Concatenate\": concat_priorbox,  # for 'coco'\n    \"Concatenate\": concat_priorbox,\n    \"concat\": concat_box_loc,\n    \"concat_1\": concat_box_conf\n  }\n  for node in ssd_graph.graph_inputs:\n    namespace_plugin_map[node.name] = Input\n\n  # Create a new graph by collapsing namespaces\n  ssd_graph.collapse_namespaces(namespace_plugin_map)\n  # Remove the outputs, so we just have a single output node (NMS).\n  # If remove_exclusive_dependencies is True, the whole graph will be removed!\n  ssd_graph.remove(ssd_graph.graph_outputs, remove_exclusive_dependencies=False)\n  # Disconnect the Input node from NMS, as it expects to have only 3 inputs.\n  ssd_graph.find_nodes_by_op(\"NMS_TRT\")[0].input.remove(\"Input\")\n\n  return ssd_graph\n\ndef build_uff_pb(output_graph, num_classes):\n  input_shape = (3, 300, 300)\n\n  dynamic_graph = gs.DynamicGraph(output_graph)\n  dynamic_graph = ssd_mobilenet_v2_unsupported_nodes_to_plugin_nodes(dynamic_graph, input_shape, num_classes)\n  return dynamic_graph\n\ndef save_uff(dynamic_graph, output_uff_filename):\n  uff.from_tensorflow(dynamic_graph.as_graph_def(), output_nodes=[\"NMS\"], output_filename=output_uff_filename)","f587fbfe":"pb_file = '\/kaggle\/input\/pretrained-trt-engines-cocotacohardhatposenet\/base-models\/ssd_mobilenet_v2_coco_2018_03_29.pb'\nuff_file = '.\/ssd_mobilenet_v2_coco_2018_03_29.uff'\nnum_classes = 91\nuff_model = build_uff_pb(pb_file, num_classes)\nsave_uff(uff_model, uff_file)","030260bd":"# https:\/\/devtalk.nvidia.com\/default\/topic\/1069148\/tensorrt\/how-to-modify-nms-inputorder-in-sampleuffssd-config-py\/\n# https:\/\/devtalk.nvidia.com\/default\/topic\/1061865\/tensorrt\/problem-converting-customised-trained-ssd-mobilenet-v2-uff-to-engine-format-tensorrt\/\nTRT_LOGGER = trt.Logger()\n\ndef build_engine_uff(uff_file_path):\n  trt.init_libnvinfer_plugins(TRT_LOGGER, '')\n  with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n    builder.max_workspace_size = 1 << 30\n    builder.fp16_mode = True\n    builder.max_batch_size = 1\n    parser.register_input(\"Input\", (3, 300, 300))\n    parser.register_output(\"MarkOutput_0\")\n\n    print(parser.parse(uff_file_path, network))\n\n    print('Building an engine from file {}; this may take a while'.format(uff_file_path))\n    return builder.build_cuda_engine(network)","46e844b9":"def save_engine(engine, engine_dest_path):\n    buf = engine.serialize()\n    with open(engine_dest_path, 'wb') as f:\n      f.write(buf)","c1aaad74":"uff_file = '.\/ssd_mobilenet_v2_coco_2018_03_29.uff'\ntrt_file = '.\/ssd_mobilenet_v2_coco_2018_03_29.engine'\nengine = build_engine_uff(uff_file)\nsave_engine(engine, trt_file)","baac9fe8":"uff_file = '\/kaggle\/input\/pretrained-trt-engines-cocotacohardhatposenet\/tensorrt6\/ssd_mobilenet_v2_hardhat_2018_03_29.uff'\ntrt_file = '.\/ssd_mobilenet_v2_hardhat_2018_03_29.engine'\nengine = build_engine_uff(uff_file)\nsave_engine(engine, trt_file)","638271b4":"uff_file = '\/kaggle\/input\/trained-models-taco-trash-annotations-in-context\/ssd_mobilenet_v2_taco_2018_03_29.uff'\ntrt_file = '.\/ssd_mobilenet_v2_taco_2018_03_29.engine'\nengine = build_engine_uff(uff_file)\nsave_engine(engine, trt_file)","26383951":"! ls *.engine","098c564b":"# copy helper python utils to current working directory\n! cp -r \/usr\/src\/tensorrt\/samples\/python\/uff_ssd\/utils .\/utils && \\\n    cp \/usr\/src\/tensorrt\/samples\/python\/common.py .\/","455b1001":"import time\nimport numpy as np\nimport pycuda.driver as cuda\nfrom PIL import Image\nimport matplotlib.pyplot as plt","4ba56a28":"import utils.engine as engine_utils # TRT Engine creation\/save\/load utils\n\ntrt_engine_path = '.\/ssd_mobilenet_v2_coco_2018_03_29.engine'\ntrt_runtime = trt.Runtime(TRT_LOGGER)\n\nprint(\"Loading cached TensorRT engine from {}\".format(trt_engine_path))\ntrt_engine = engine_utils.load_engine(trt_runtime, trt_engine_path)\ncontext = trt_engine.create_execution_context()\ninputs, outputs, bindings, stream = engine_utils.allocate_buffers(trt_engine)","ad4dbc4c":"import utils.inference as inference_utils\nimport utils.coco as coco_utils\nimport utils.boxes as boxes_utils\nimport common\n\n# specifics for SSD MobileNet v2\nmodel_input_width = 300\nmodel_input_height = 300\nmodel_input_channels = 3\n\n# Layout of TensorRT network output metadata\nTRT_PREDICTION_LAYOUT = {\n    \"image_id\": 0,\n    \"label\": 1,\n    \"confidence\": 2,\n    \"xmin\": 3,\n    \"ymin\": 4,\n    \"xmax\": 5,\n    \"ymax\": 6\n}\n\ndef fetch_prediction_field(field_name, detection_out, pred_start_idx):\n    # Fetches prediction field from prediction byte array.\n    return detection_out[pred_start_idx + TRT_PREDICTION_LAYOUT[field_name]]\n\ndef analyze_prediction(detection_out, pred_start_idx, img_pil):\n    # Confidence threshold for drawing bounding box\n    VISUALIZATION_THRESHOLD = 0.5\n    image_id = int(fetch_prediction_field(\"image_id\", detection_out, pred_start_idx))\n    label = int(fetch_prediction_field(\"label\", detection_out, pred_start_idx))\n    confidence = fetch_prediction_field(\"confidence\", detection_out, pred_start_idx)\n    xmin = fetch_prediction_field(\"xmin\", detection_out, pred_start_idx)\n    ymin = fetch_prediction_field(\"ymin\", detection_out, pred_start_idx)\n    xmax = fetch_prediction_field(\"xmax\", detection_out, pred_start_idx)\n    ymax = fetch_prediction_field(\"ymax\", detection_out, pred_start_idx)\n    if confidence > VISUALIZATION_THRESHOLD:\n        class_name = coco_utils.COCO_CLASSES_LIST[label]\n        confidence_percentage = \"{0:.0%}\".format(confidence)\n        print(\"Detected {} with confidence {}\".format(class_name, confidence_percentage))\n        boxes_utils.draw_bounding_boxes_on_image(img_pil, np.array([[ymin, xmin, ymax, xmax]]), display_str_list=[\"{}: {}\".format(class_name, confidence_percentage)],color=coco_utils.COCO_COLORS[label])\n\ndef load_image_into_numpy_array(image):\n    (im_width, im_height) = image.size\n    return np.array(image).reshape((im_height, im_width, model_input_channels)).astype(np.uint8)\n\ndef load_img(image_path):\n    image = Image.open(image_path)\n    # Note: Bilinear interpolation used by Pillow is a little bit\n    # different than the one used by Tensorflow, so if network receives\n    # an image that is not 300x300, the network output may differ\n    # from the one output by Tensorflow\n    image_resized = image.resize(\n        size=(model_input_width, model_input_height),\n        resample=Image.BILINEAR\n    )\n    img_np = load_image_into_numpy_array(image_resized)\n    # HWC -> CHW\n    img_np = img_np.transpose((2, 0, 1))\n    img_np = img_np.ravel()\n    return img_np\n\ndef infer(image_path):\n    img = load_img(image_path)\n    # Copy it into appropriate place into memory (inputs was returned earlier by allocate_buffers())\n    np.copyto(inputs[0].host, img.ravel())\n    inference_start_time = time.time()\n    [detection_out, keepCount_out] = common.do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n    print(\"TensorRT inference time: {} ms\".format(int(round((time.time() - inference_start_time) * 1000))))\n    return detection_out, keepCount_out\n\ndef detect_image(input_img_path):\n    inference_start_time = time.time()\n    detection_out, keep_count_out = infer(input_img_path)\n\n    img_pil = Image.open(input_img_path)\n    prediction_fields = len(TRT_PREDICTION_LAYOUT)\n    for det in range(int(keep_count_out[0])):\n        analyze_prediction(detection_out, det * prediction_fields, img_pil)\n\n    print(\"Total time taken for one image: {} ms\\n\".format(int(round((time.time() - inference_start_time) * 1000))))\n\n    %matplotlib inline\n    plt.figure(figsize = (15,10))\n    plt.imshow(np.asarray(img_pil))","68283bd4":"detect_image('\/kaggle\/input\/coco-2017-dataset\/coco2017\/test2017\/000000000080.jpg')","4539caf4":"detect_image('\/kaggle\/input\/coco-2017-dataset\/coco2017\/test2017\/000000001024.jpg')","bbcd8172":"detect_image('\/kaggle\/input\/coco-2017-dataset\/coco2017\/test2017\/000000002219.jpg')","c5b95d80":"detect_image('\/kaggle\/input\/coco-2017-dataset\/coco2017\/test2017\/000000002680.jpg')","1f492c15":"! rm -rf .\/utils && rm common.py","e7bb9299":"# Validate Python Import","1ec10d52":"## ONNX","542f68d0":"NVIDIA [TensorRT](https:\/\/developer.nvidia.com\/tensorrt) is already widely used to optimize inference times. I noticed that there's not a lot to find about it on Kaggle, so that's why I made this notebook so other people can run their TensorRT engines in their notebooks. TensorRT is also great to be used with [DeepStream](https:\/\/developer.nvidia.com\/deepstream-sdk) for realtime inference. I made a TensorRT engine from the [TACO](http:\/\/tacodataset.org\/) dataset and used DeepStream to do inference with in realtime https:\/\/www.linkedin.com\/feed\/update\/urn:li:activity:6695264486607130624\/.","1cb1a6aa":"First we need to verify Ubuntu and CUDA version (TensorRT deb package is for CUDA 10.2 \/ Ubuntu 18.04)","f9dda293":"It's possible to convert Caffe models to a TRT engine","39b9675f":"## UFF","f9db3639":"# Install TensorRT","8f92fdf1":"# NVIDIA TensorRT on Kaggle","b65f8ac8":"It's possible to convert ONNX models to a TRT engine","93777ccc":"It's also possible to convert UFF models to a TRT engine. But for an SSD MobileNet v2 we need to do some extra processing in python. This is due to plugin required by SSD MobileNet called FlattenConcat that isn't included in TensorRT6. So we made a custom `_FlattenConcat` operation in the corresponding uff file already. This custom operation has to be included again for a TRT engine to be built.","b92fe52f":"## Caffemodel","39d02172":"# Show built engines","d70bb6c2":"# Build TensorRT Engines","f1606e35":"I'm not sure if I can make the tensorrt install packages publicly available (https:\/\/developer.download.nvidia.com\/compute\/machine-learning\/tensorrt\/docs\/TensorRT-SLA-Sep-2019.pdf). If you know if it is possible to put it on Kaggle, please let me know, I'll make my dataset public then. I used my own private dataset and downloaded 2 files. You can download them by pasting it in your webbrowser after you've logged in https:\/\/developer.nvidia.com\/.\n\n- https:\/\/developer.nvidia.com\/compute\/machine-learning\/tensorrt\/secure\/6.0\/GA_6.0.1.8\/local_repos\/nv-tensorrt-repo-ubuntu1804-cuda10.2-trt6.0.1.8-ga-20191108_1-1_amd64.deb\n- https:\/\/developer.nvidia.com\/compute\/machine-learning\/tensorrt\/secure\/6.0\/GA_6.0.1.8\/tars\/TensorRT-6.0.1.8.Ubuntu-18.04.x86_64-gnu.cuda-10.2.cudnn7.6.tar.gz","78ed95aa":"# Cleanup","6192e442":"We can use the `trtexec` binary to convert certain file types to a TensorRT engine.","da7475c5":"Inference is done based on the `\/usr\/src\/tensorrt\/samples\/python\/uff_ssd\/detect_objects.py` file. I separated some core components to make inference more clear.","d24ddb8c":"We will first construct a UFF file from the TensorFlow frozen graph (.pb)"}}