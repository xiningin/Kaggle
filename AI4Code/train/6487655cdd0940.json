{"cell_type":{"e302b5dd":"code","63a1406c":"code","46c926b5":"code","35810012":"code","24ccb3d9":"code","65162465":"code","b6bc12c2":"code","d2dc7328":"code","cfbaaa07":"code","ebb80cfd":"code","2980bef4":"code","e14fce24":"code","33c90abb":"code","c8ba9ff7":"code","1b6107bf":"code","7a1de3a5":"code","4468e140":"code","3b71ce9b":"code","ac9d9965":"code","7f6c25ad":"code","8ef667b3":"code","930d813e":"code","e249ad4c":"code","1d60506d":"code","35e566d2":"code","3a5efc20":"code","f0f37915":"code","333a9947":"code","f419aa29":"code","14e0570f":"code","4c51e718":"code","aaf86ad0":"code","cb5cae22":"markdown","2ea7a8cb":"markdown","366b6939":"markdown","d928a040":"markdown","d8e03e36":"markdown","e86be66c":"markdown","b4ba1f82":"markdown","258c3033":"markdown","c5b247f2":"markdown","ae4adb6f":"markdown","ca0dd57a":"markdown","ecc4632e":"markdown","9d4de1ce":"markdown","5003e3b9":"markdown","7163ef70":"markdown","c5a28e9e":"markdown","bb30fd7e":"markdown","2924f1c0":"markdown","76667682":"markdown"},"source":{"e302b5dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import learning_curve\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer, QuantileTransformer\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom catboost import CatBoostRegressor\n!pip install catboost\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostRegressor,BaggingRegressor, GradientBoostingRegressor, VotingRegressor, HistGradientBoostingRegressor, StackingRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Bidirectional, Dropout, BatchNormalization\n#!pip install scikit-garden\n#from statsmodels.tsa.arima.model import ARIMA\n#from skgarden import MondrianForestRegressor\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63a1406c":"dataset_path = '\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/'\ndf = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\nedf = pd.read_csv(os.path.join(dataset_path, 'test.csv'))\nprint('Number of instances, features', df.shape)","46c926b5":"print(df.head())","35810012":"print(df.info())","24ccb3d9":"print(df.describe())","65162465":"dups = df.duplicated()\nprint(dups.any())","b6bc12c2":"df.hist(figsize = (30,20))\nplt.show()","d2dc7328":"scatter_matrix(df, figsize=(30,30))\nplt.show()","cfbaaa07":"# Plot correlation matrix\ncorrelations = df.corr()\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nplt.show()","ebb80cfd":"df.plot(kind='box', subplots=True, figsize=(15,15), layout=(5,5), sharex=False, sharey=False)\nplt.show()","2980bef4":"\"\"\"\ndf['label_day_night']=df['Hour'].apply(lambda x : 'Night' if (x >20 or x<5) else('Day'))\ndf['Mean_temp'] = df[['Temperature(\ufffdC)','Dew point temperature(\ufffdC)']].mean(axis=1)\ndf['label_hot_cold']=df['Mean_temp'].apply(lambda x : 'cold' if (x < 15) else('average' if (15 <=x<= 25) else('hot')))\ndf.Date = pd.to_datetime(df.Date)\ndf['Month'] = df['Date'].dt.month\ndf['Month'] = df['Month'].astype(str)\ndf['DayOfWeek'] = df['Date'].dt.weekday\ndf['DayOfWeek'] = df['DayOfWeek'].astype(str)\ndf['label_humidity'] = df['Humidity(%)'].apply(lambda x : 'high' if (x >= 50) else ('low'))\n\ndf.Date = pd.to_datetime(df.Date)\n#df['Day'] = df['Date'].dt.day\ndf['DayOfWeek'] = df['Date'].dt.weekday\ndf['label_weekend'] = df['Day'].apply(lambda x : 'yes' if (x == 5 or x == 6) else ('no'))\ndf.head()\n\ndf.Date = pd.to_datetime(df.Date)\ndf['Year'] = df['Date'].dt.year\n#df['Year'] = df['Year'].astype(str)\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\ndf['dow'] =  df['Date'].dt.dayofweek\ndf['doy'] =  df['Date'].dt.dayofyear\ndf['woy'] =  df['Date'].dt.weekofyear\n#df.describe()\ndf['Year_part'] = df['Month'].apply(lambda x : '1st' if (1<=x<=3) else('2nd' if (4 <=x<= 6) else('3rd' if (7 <=x<= 9) else('4th'))))\n#df['Month'] = df['Month'].astype(str)\n\"\"\"","e14fce24":"ContinuousCols=['Hour','Temperature(\ufffdC)','Humidity(%)','Wind speed (m\/s)','Visibility (10m)', 'Dew point temperature(\ufffdC)','Solar Radiation (MJ\/m2)','Rainfall(mm)','Snowfall (cm)']\n\nfor predictor in ContinuousCols:\n    df.plot.scatter(x=predictor, y='y', figsize=(10,5), title=predictor+\" VS \"+ 'Rented Bikes')","33c90abb":"CategoricalColsList=['Seasons', 'Holiday', 'Functioning Day']\n\nfig, PlotCanvas=plt.subplots(nrows=1, ncols=len(CategoricalColsList), figsize=(18,5))\n\nfor PredictorCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):\n    df.boxplot(column='y', by=PredictorCol, figsize=(5,5), vert=True, ax=PlotCanvas[i])","c8ba9ff7":"#np.sqrt(df['y']).hist()","1b6107bf":"Q1 = df[['y']].quantile(0.25)\nQ3 = df[['y']].quantile(0.75)\nIQR = Q3 - Q1\ndf = df[~((df[['y']] < (Q1 - 1.5 * IQR)) |(df[['y']] > (Q3 + 1.5 * IQR))).any(axis=1)]\n#df = df[df['Wind speed (m\/s)']!=0]\ndataset = df.drop(columns=['y'])","7a1de3a5":"def add_features(dataset, df1):\n    #dataset['label_day_night'] = dataset['Hour'].apply(lambda x : 'Night' if (x >20 or x<5) else('Day'))\n    dataset['label_day_night'] = dataset['Hour'].apply(lambda x : 1 if (x >=5 or x<=12) else(2 if (x >12 or x<=17) else(3 if (x >17 or x<=21) else(4))) )\n    dataset['Mean_temp'] = dataset[['Temperature(\ufffdC)','Dew point temperature(\ufffdC)']].mean(axis=1)\n    dataset['label_hot_cold'] = dataset['Mean_temp'].apply(lambda x : 0 if (x <= 10) else(2 if (10 <x<= 18) else(3 if (18 <x<= 25) else(4 if (25 <x<= 32) else(1)))))\n    #dataset[\"bad_humidity\"]=0.6215*(dataset[\"Mean_temp\"])+35.74-35.75*(dataset[\"Wind speed (m\/s)\"]**0.16)+0.4275*(dataset[\"Wind speed (m\/s)\"]**0.16)\n    #dataset[\"bad_humidity\"]=9\/5*dataset[\"Temperature(\ufffdC)\"]-0.55*(1-dataset[\"Humidity(%)\"]\/100)*(9\/5*dataset[\"Temperature(\ufffdC)\"]-26)+3\n    #dataset['label_hot_cold'] = dataset['Mean_temp'].apply(lambda x : 'cold' if (x < 15) else('average' if (15 <=x<= 25) else('hot')))\n    #dataset['label_hot_cold'] = dataset['Mean_temp'].apply(lambda x : 1 if (x < 15) else(2 if (15 <=x<= 23) else(3 if (23 <=x<= 30) else(4))))\n    #dataset['label_hot_cold'] = dataset['Mean_temp'].apply(lambda x : 1 if (x < 15) else(2 if (15 <=x<= 23) else(3 if (23 <=x<= 30) else(4))))\n    #dataset['label_humidity'] = dataset['Humidity(%)'].apply(lambda x : 1 if (x >= 50) else (0))\n    #dataset['label_humidity'] = dataset['Humidity(%)'].apply(lambda x : 'high' if (x >= 50) else ('low'))\n    #dataset['label_sr'] = dataset['Solar Radiation (MJ\/m2)'].apply(lambda x : 1 if (x >= 2.5) else (0))\n    #dataset['label_ws'] = dataset['Wind speed (m\/s)'].apply(lambda x : 1 if (x < 3) else(2 if (3 <=x<= 5) else(4)))\n    #dataset['Seasons'] = dataset['Seasons'].apply(lambda x : 4 if (x == 'Summer') else(3 if (x == 'Autumn') else(2 if(x=='Spring') else(1))))\n    #dataset[\"avg_temp_season\"] = (dataset.groupby(\"Month\")[\"Mean_temp\"].transform(\"mean\"))\n    #dataset['label_hot_month'] = dataset['avg_temp_season'].apply(lambda x : 1 if (x < 15) else(2 if (15 <=x<= 23) else(3 if (23 <=x<= 30) else(4))))\n\n    #dataset['label_Visibility (10m)'] = dataset['Visibility (10m)'].apply(lambda x : 1 if (x < 200) else (2 if (x <= 200 and x<=1000) else (3)))\n    #dataset['Wind speed (m\/s)'] = dataset['Wind speed (m\/s)'].apply (lambda x: np.log1p(x))\n    #dataset['Visibility (10m)'] = dataset['Visibility (10m)'].apply (lambda x: np.power(x,10))\n    #dataset['Snowfall (cm)']=dataset['Snowfall (cm)'].apply(lambda x : x\/1000)\n    #dataset['Rainfall(mm)']=dataset['Rainfall(mm)'].apply(lambda x : (x-5)*0.75)\n    #power = PowerTransformer(method='yeo-johnson', standardize=True)\n    #dataset[\"avg_temp\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])[\"Mean_temp\"].transform(\"mean\"))\n    #dataset[\"avg_hum\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])[\"Humidity(%)\"].transform(\"mean\"))\n    dataset[\"avg_rain\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])[\"Rainfall(mm)\"].transform(\"mean\"))\n    #dataset[\"avg_snow\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])[\"Snowfall (cm)\"].transform(\"mean\"))\n    #dataset[\"temp_season\"] = (dataset.groupby([\"Year\",\"Seasons\"])[\"Mean_temp\"].transform(\"mean\"))\n    #dataset[\"hum_season\"] = (dataset.groupby([\"Year\",\"Seasons\"])[\"Humidity(%)\"].transform(\"mean\"))\n    dataset[\"rain_season\"] = (dataset.groupby([\"Year\",\"Seasons\"])[\"Rainfall(mm)\"].transform(\"mean\"))\n    #dataset[\"snow_season\"] = (dataset.groupby([\"Year\",\"Seasons\"])[\"Snowfall (cm)\"].transform(\"mean\"))\n    #dataset[\"vis_hour\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])['Visibility (10m)'].transform(\"mean\"))\n    #dataset[\"solar_hour\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])['Solar Radiation (MJ\/m2)'].transform(\"mean\"))\n    #dataset[\"holiday_week\"] = (dataset.groupby([\"Year\",\"WeekOfYear\"])['Holiday'].transform(\"sum\"))\n    #dataset[\"wind_week\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])[\"Wind speed (m\/s)\"].transform(\"mean\"))\n    #dataset[\"avg_solar\"] = (dataset.groupby([\"Year\",\"Month\",'Day'])[\"Solar Radiation (MJ\/m2)\"].transform(\"mean\"))\n    power = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n    #dataset['Wind speed (m\/s)']= power.fit_transform(dataset['Wind speed (m\/s)'].values.reshape(-1,1))\n    #dataset['Visibility (10m)'] = power.fit_transform(dataset['Visibility (10m)'].values.reshape(-1,1))\n    #dataset['Snowfall (cm)'] = power.fit_transform(dataset['Snowfall (cm)'].values.reshape(-1,1))\n    #dataset['Rainfall(mm)'] = power.fit_transform(dataset['Rainfall(mm)'].values.reshape(-1,1))\n    #dataset['Solar Radiation (MJ\/m2)'] = power.fit_transform( dataset['Solar Radiation (MJ\/m2)'].values.reshape(-1,1))\n\n    #dataset['avg_temp']= power.fit_transform(dataset['avg_temp'].values.reshape(-1,1))\n    #dataset['avg_hum'] = power.fit_transform(dataset['avg_hum'].values.reshape(-1,1))\n    #dataset['avg_rain'] = power.fit_transform(dataset['avg_rain'].values.reshape(-1,1))\n    #dataset['avg_snow'] = power.fit_transform(dataset['avg_snow'].values.reshape(-1,1))\n    \n    #scaler = MinMaxScaler(feature_range=(1, 2))\n    #pipeline = Pipeline(steps=[('s', scaler),('p', power)])\n    #df['Wind speed (m\/s)']= pipeline.fit_transform(df['Wind speed (m\/s)'].values.reshape(-1,1))\n    #df['Visibility (10m)'] = pipeline.fit_transform(df['Visibility (10m)'].values.reshape(-1,1))\n    #df['Snowfall (cm)'] = pipeline.fit_transform(df['Snowfall (cm)'].values.reshape(-1,1))\n    #df['Rainfall(mm)'] = pipeline.fit_transform(df['Rainfall(mm)'].values.reshape(-1,1))\n    #df['Solar Radiation (MJ\/m2)'] = pipeline.fit_transform(df['Solar Radiation (MJ\/m2)'].values.reshape(-1,1))\n    return dataset","4468e140":"def decompose_date(df):\n    def year_part(row):\n        if row['Year']==2017 and row['Year_part']==1:\n            return 1\n        elif row['Year']==2017 and row['Year_part']==2:\n            return 2\n        elif row['Year']==2017 and row['Year_part']==3:\n            return 3\n        elif row['Year']==2017 and row['Year_part']==4:\n            return 4\n        elif row['Year']==2018 and row['Year_part']==1:\n            return 5\n        elif row['Year']==2018 and row['Year_part']==2:\n            return 6\n        elif row['Year']==2018 and row['Year_part']==3:\n            return 7\n        elif row['Year']==2018 and row['Year_part']==4:\n            return 8\n\n    def hour_part(row):\n      if row['Hour']<8:\n          return 1\n      elif row['Hour']>=22:\n          return 2\n      elif row['Hour']>9 and  row['Hour']<18:\n          return 3\n      elif row['Hour']==8:\n          return 4\n      elif row['Hour']==9:\n          return 5\n      elif row['Hour']==20 or row['Hour']==21:\n          return 6\n      elif row['Hour']==18 or row['Hour']==19:\n          return 7\n\n    def non_working(row):\n      if row['Holiday'] == 1 or row['label_weekend'] == 1:\n          return 0\n      else:\n          return 1\n\n    def climate(row):\n      if row['Visibility (10m)'] >=500:\n          return 1\n      if row['Visibility (10m)'] <500:\n          return 2\n      if row['Rainfall(mm)'] >=10 and row['Rainfall(mm)'] <=50:\n          return 3\n      if row['Snowfall (cm)'] >=2:\n          return 4\n\n    def daytype(row):\n      if row['Holiday']==0 and row['Functioning Day']=='No':\n          return 1\n      elif row['Holiday']==1:\n          return 2\n      elif row['Holiday'] ==0 and row['Functioning Day']=='Yes':\n          return 3\n    \n    #df.Date = pd.to_datetime(df['Date'])\n    df.Date = pd.DatetimeIndex(df.Date.values,dayfirst=True)\n    df['Year']=(df['Date']+pd.to_timedelta(6-df['Date'].dt.weekday, unit='d')).dt.year\n    #df.Date = pd.to_datetime(df.Date)\n    df['Month'] = df['Date'].dt.month\n    #df['Month'] = df['Month'].apply(lambda x : 3if (x==1) else (1if (x==2) else (2if (x==3) else (11if (x==4) else (6if (x==5) else (11 if (x==6) else (4if (x==7) else (9 if (x==8) else (7if (x==9) else (8if (x==10) else (5if (x==11) else (10))))))))))))\n    #df['year'] = df['Date'].dt.year\n    df['Year_part'] = df['Month'].apply(lambda x : 1 if (1<=x<=3) else(2 if (4 <=x<= 6) else(3 if (7 <=x<= 9) else(4))))\n    df['YearQuarter'] = df.apply (lambda row: year_part(row), axis=1)\n    #df['Year'] = df['Year'].apply(lambda x : 0 if (x==2017) else (1))\n    df['Day'] = df['Date'].dt.day\n    #df['Day'] = df['Day'].apply(lambda x : if (x==) else ())\n    #df['Year'] = df['Year'].astype(str)\n    df['DayOfWeek'] = df['Date'].dt.weekday\n    df['DayOfYear'] = df['Date'].dt.dayofyear\n    df['WeekOfYear'] = df['Date'].dt.weekofyear\n    #df['DayOfWeek'] = df['DayOfWeek'].astype(str)\n    df['label_weekend'] = df['DayOfWeek'].apply(lambda x : 1 if (x == 5 or x == 6) else (0))\n    #df['label_weekend'] = df['DayOfWeek'].apply(lambda x : 1 if (x == 5 or x == 6) else (0))\n    #df['label_rain'] = df['Rainfall(mm)'].apply(lambda x : 1 if (x < 10) else ( 2 if (x <= 10 and x <= 50) else(3)))\n    #df['label_snow'] = df['Snowfall (cm)'].apply(lambda x : 1 if (x < 1) else ( 2 if (x <= 1 and x <= 5) else(3)))\n    #df['label_Visibility (10m)'] = df['Visibility (10m)'].apply(lambda x : 1 if (x < 200) else (2 if (x <= 200 and x<=1000) else (3)))\n    df['Holiday'] = df['Holiday'].apply(lambda x : 1 if (x == 'Holiday') else (0))\n    #df['Seasons'] = df['Seasons'].apply(lambda x : 4 if (x == 'Summer') else (4 if (x == 'Summer') else (0)))\n    df['RushHour']= df['Hour'].isin([8,17,18,19,20,21])\n    df['lowHour']= df['Hour'].isin([0,1,2,3,4])\n    #df['Morning'] = np.where((5 <= df.Hour) & (df.Hour <= 12), 1, 0)\n    #df['Afternoon'] = np.where((12 < df.Hour) & (df.Hour <= 17), 1, 0)\n    #df['Evening'] = np.where((17 < df.Hour) & (df.Hour <= 21), 1, 0)\n    #df['Night'] = np.where((21 < df.Hour) | (df.Hour < 5 ), 1, 0)\n    #df['day_type'] = df.apply(lambda row: daytype(row), axis=1)\n    #df['Functioning Day'] = df['Functioning Day'].apply(lambda x : 1 if (x == 'Yes') else (0))\n    #df[\"avg_temp_season\"] = (df.groupby(\"Month\")[\"Mean_temp\"].transform(\"mean\"))\n    #df['label_hot_month'] = df['avg_temp_season'].apply(lambda x : 1 if (x < 15) else(2 if (15 <=x<= 23) else(3 if (23 <=x<= 30) else(4))))\n    #df['Is_non_workday'] = df.apply (lambda row: non_working(row), axis=1)\n    #df['Is_non_workday'] = df['Holiday'] + df['label_weekend']\n    #df['Is_non_workday'] = df['Is_non_workday'].replace(1,2)\n    #df = df.drop(['is_holiday','is_weekend'],axis=1)\n    #df['Year_part'] = df['Month'].apply(lambda x : 1 if (1<=x<=3) else(2 if (4 <=x<= 6) else(3 if (7 <=x<= 9) else(4))))\n    #df['YearQuarter'] = df.apply (lambda row: year_part(row), axis=1)\n    df['Hour_part'] = df.apply (lambda row: hour_part(row), axis=1)\n    #df['weather'] = df.apply(lambda row: climate(row), axis=1)\n    #df['work_day'] = df.apply (lambda row: non_working(row), axis=1)\n    #df['peak'] = df[['work_day', 'Hour']].apply(lambda x: (0, 1)[x['work_day']== 1 and ((x['Hour'] >=7 and x['Hour'] <=9) or (x['Hour'] >=17 and x['Hour'] <=19))], axis = 1)\n    #df['label_night'] = df['Hour'].apply(lambda x: 1 if (x < 8 or x > 20) else(0)) \n    #df['year_season'] = df.Year + df.Seasons \/ 10\n    #df['h_sin'] = np.sin(2 * np.pi * df['Hour'] \/ 23)\n    #df['h_cos'] = np.cos(2 * np.pi * df['Hour'] \/ 23)\n\n    #df['m_sin'] = np.sin(2 * np.pi * df['Month'] \/ 12)\n    #df['m_cos'] = np.cos(2 * np.pi * df['Month'] \/ 12)\n\n    \n    #df['dw_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] \/ 6)\n    #df['dw_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] \/ 6)\n\n    #df['dy_sin'] = np.sin(2 * np.pi * df['DayOfYear'] \/ 365)\n    #df['dy_cos'] = np.cos(2 * np.pi * df['DayOfYear'] \/ 365)\n\n    #df['wy_sin'] = np.sin(2 * np.pi * df['WeekOfYear'] \/ 47)\n    #df['wy_cos'] = np.cos(2 * np.pi * df['WeekOfYear'] \/ 47)\n    \n    return df","3b71ce9b":"\ndef encode_data(df):\n    #categoric = ['Seasons', 'label_day_night', 'Functioning Day', 'label_humidity']\n    categoric = ['Seasons', 'Functioning Day']\n    label_encoder = preprocessing.LabelEncoder()\n    for feature in categoric:\n        df[feature] = label_encoder.fit_transform(df[feature])\n    df = pd.get_dummies(df,columns=['Month','Year'])\n    return df","ac9d9965":"\ndef drop_columns(df):\n    df = df.drop(columns=['ID','Date','Temperature(\ufffdC)','Dew point temperature(\ufffdC)','Year_part','Day'])\n    #df = df.drop(columns=['ID','Date','Temperature(\ufffdC)','Dew point temperature(\ufffdC)'])\n    #df = df.drop(columns=['ID','Date'])\n    return df","7f6c25ad":"def rescale_data(df):\n    #scaler = MinMaxScaler(feature_range=(0, 1))\n    #scaler = StandardScaler()\n    scaler = RobustScaler()\n    return scaler.fit_transform(df)","8ef667b3":"def remove_low_varience(df):\n    selector = VarianceThreshold(threshold=0.1)\n    return selector.fit_transform(df)","930d813e":"def add_polynomial_features(df):\n    trans = PolynomialFeatures(degree=2)\n    return trans.fit_transform(df)","e249ad4c":"def perform_pca(df):\n    pca = PCA(n_components=8)\n    res = pca.fit_transform(df)\n    features = range(pca.n_components_)\n    plt.bar(features, pca.explained_variance_)\n    plt.xlabel('PCA feature')\n    plt.ylabel('variance')\n    plt.xticks(features)\n    plt.show()\n    return res","1d60506d":"def remove_outliers(df):\n    \n    numeric_var = ['Wind speed (m\/s)']\n    for i in numeric_var:\n        q75, q25 = np.percentile(df.loc[:,i], [75, 25])\n        iqr = q75 - q25\n        Innerfence = q25 - (iqr*1.5)\n        Upperfence = q75 + (iqr*1.5)\n    \n        # replace outliers with NA\n\n        df.loc[df[i]<Innerfence, i] = np.nan\n        df.loc[df[i]>Upperfence, i] = np.nan\n        #print(df.isnull().sum())\n        \n        df[i] = df[i].fillna(df[i].median())\n\n    return df\n    \"\"\"\n    from sklearn.impute import KNNImputer\n    df['Wind speed (m\/s)'] = df['Wind speed (m\/s)'].replace({0:np.nan})\n    df['Humidity(%)'] = df['Humidity(%)'].replace({0:np.nan})\n    imputer = KNNImputer(n_neighbors=10)\n    df = pd.DataFrame(imputer.fit_transform(df),columns=df.columns)\n    return df\n    \"\"\"","35e566d2":"def preprocess_data(df1):\n    global df\n    df1 = decompose_date(df1)\n    df1 = add_features(df1,df)\n    #df1 = lag(df1)\n    #df1 = encode_time(df1)\n    df1 = drop_columns(df1)\n    #scatter_matrix(df1, figsize=(30,30))\n    #plt.show()\n    df1 = remove_outliers(df1)\n    #df1['year_season'] = (df1.Year + df1.Seasons) \/ 10\n    df1 = encode_data(df1)\n    #df1['year_season'] = (df1.Year + df1.Seasons) \/ 10\n    #df1 = df1.drop(columns=['year','month'])\n    print(df1.info())\n    print(df1.head())\n    #df1 = lag(df1)\n    #print(df1)\n    #df1 = rescale_data(df1)\n    #df1 = add_polynomial_features(df1)\n    #df1 = perform_pca(df1)\n    return df1","3a5efc20":"train_df = preprocess_data(dataset)","f0f37915":"#y = df['y']\n#x = train_df\n#trainx, valx,trainy, valy = train_test_split(x, y, test_size=0.2)\n\"\"\"\nmodel =RandomForestRegressor()\n# Create 3 folds\nseed = 13\nkfold = KFold(n_splits=3, shuffle=True, random_state=seed)\n# Define our candidate hyperparameters\nhp_candidates = [{'n_estimators': [100,150,200,250,300,350], 'max_depth': [5,10,15,20,25], 'max_features':['auto', 'sqrt', 'log2'], 'max_leaf_nodes':[5,10,15]}]\n# Search for best hyperparameters\ngrid = GridSearchCV(estimator=model, param_grid=hp_candidates, cv=kfold, scoring='neg_mean_squared_log_error')\ngrid.fit(trainx,trainy)\n# Get the results\nprint(grid.best_score_)\nprint(grid.best_estimator_)\nprint(grid.best_params_)\n\"\"\"","333a9947":"\nfrom sklearn.linear_model import ElasticNetCV\n#from skgarden import MondrianForestRegressor\ny = df['y']\nx = train_df\n#4521..1130      4608..1152\n#trainx, valx,trainy, valy = train_test_split(x, y, test_size=0.2)\n#trainx = pd.DataFrame(x).head(4521)\n#trainy = pd.DataFrame(y).head(4521)\n#valx = pd.DataFrame(x).tail(1130)\n#valy = pd.DataFrame(y).tail(1130)\ntrainx = x\ntrainy = y\n\"\"\"\ndef trans(x,l1=0.3,l2=0):\n    if l1!=0:\n        return ((x+l2)**l1-1)\/l1\n    else:\n        return np.log(x+l2)\ndef rev_trans(x,l1=0.3,l2=0):\n    return (x*l1+1)**(1\/l1)-l2\n\"\"\"\n#estimators = [('cb', CatBoostRegressor(verbose = 0)),('hist', HistGradientBoostingRegressor(max_bins= 150, max_depth=11, max_iter = 115, max_leaf_nodes= 15)), ('xg', XGBRegressor( max_depth = 9)), ('svr', SVR()),('rf',RandomForestRegressor(n_estimators = 400, criterion='mse',random_state=1, n_jobs=-1))]\n#finalmodel = StackingRegressor(estimators=estimators)\n#finalmodel = XGBRegressor(colsample_bytree = 0.8, learning_rate = 0.2,min_child_weight =20, max_depth=20)\n#finalmodel=HistGradientBoostingRegressor(max_bins= 150, max_depth=11, max_iter = 115, max_leaf_nodes= 15)\nfinalmodel=XGBRegressor( colsample_bytree = 0.8, learning_rate = 0.149,min_child_weight =25, max_depth = 3,  n_estimators = 450)\n#finalmodel = MondrianForestRegressor(n_estimators = 400, random_state=1, n_jobs=-1)\nfinalmodel.fit(trainx,np.log1p(trainy))\n##finalmodel.fit(trainx,trainy)\n##results = finalmodel.predict(valx)\n#results = np.round(np.exp(finalmodel.predict(valx))-1)\ntraining_results = np.round(np.exp(finalmodel.predict(trainx))-1)\n##training_results = finalmodel.predict(trainx)\nprint('Training Score: ', np.sqrt(mean_squared_log_error(trainy,abs(training_results))))\n#msg = \"%s: %f \" % ('Validation Score: ', np.sqrt(mean_squared_log_error(valy,abs(results))))\n#print(msg)","f419aa29":"print(finalmodel.feature_importances_)\n# plot\nplt.bar(range(len(finalmodel.feature_importances_)), finalmodel.feature_importances_)\nplt.show()","14e0570f":"\"\"\"\nhistory = []\nfor t in range(len(trainx)):\n    model = ARIMA(history, order=(5,1,0))\n    model_fit = model.fit()\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\nprint(np.sqrt(mean_squared_log_error(trainy,np.abs(history))))\n\"\"\"","4c51e718":"\"\"\"\nfinalmodel = CatBoostRegressor()\nfinalmodel.fit(trainx,np.log1p(trainy))\nresults = np.exp(finalmodel.predict(valx))-1\ntraining_results = np.exp(finalmodel.predict(trainx))-1\nprint('Training Score: ', np.sqrt(mean_squared_log_error(trainy,abs(training_results))))\nmsg = \"%s: %f \" % ('Validation Score: ', np.sqrt(mean_squared_log_error(valy,abs(results))))\nprint(msg)\n\"\"\"","aaf86ad0":"#X_test['lag_1'] = edf['y'].shift(1)\n#X_test['lag_1'].fillna(X_test['lag_1'].median(), inplace=True)\nX_test = preprocess_data(edf)\ny_test_predicted = abs((np.exp(finalmodel.predict(X_test))-1))\nedf['y'] = y_test_predicted\nedf[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","cb5cae22":"### Hyper parameter Tuning","2ea7a8cb":"## Data Preparation","366b6939":"### Box Plot","d928a040":"## Test File Submission","d8e03e36":"## Spot Checking Models","e86be66c":"### Data Peek","b4ba1f82":"### Identify Duplicates","258c3033":"## EDA","c5b247f2":"### Random Forest","ae4adb6f":"## Fine Tuning Best Models","ca0dd57a":"### Data Shape","ecc4632e":"### Correalation Matrix","9d4de1ce":"### Data Information","5003e3b9":"### Target vs Categorical Features","7163ef70":"### Data Description","c5a28e9e":"### Features' Distribution","bb30fd7e":"### Scatter Plot Matrix","2924f1c0":"## Data Visualization","76667682":"### Target vs Numeric Features"}}