{"cell_type":{"7282fc64":"code","dc131717":"code","e88f4fdb":"code","98daa86f":"code","d8da9e1e":"code","9f96f56e":"code","2d5b0981":"code","3af6f7b2":"code","f7057169":"code","948ec955":"code","f775fd3e":"code","2f4dee04":"code","12e9fc49":"code","63a5d3ae":"code","29885919":"code","dce9bb22":"markdown","7c75db97":"markdown","05f99182":"markdown","bfc6e7ef":"markdown","a166c2dc":"markdown"},"source":{"7282fc64":"#Import all the needed libraries\nimport sqlite3\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import tree, preprocessing\nimport sklearn.ensemble as ske\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb","dc131717":"#Make a connection to the database\ncnx = sqlite3.connect('..\/input\/188-million-us-wildfires\/FPA_FOD_20170508.sqlite')\n","e88f4fdb":"df = pd.read_sql_query(\"SELECT FIRE_YEAR,STAT_CAUSE_DESCR,LATITUDE,LONGITUDE,STATE,DISCOVERY_DATE,FIRE_SIZE FROM 'Fires'\", cnx)","98daa86f":"df['DATE'] = pd.to_datetime(df['DISCOVERY_DATE'] - pd.Timestamp(0).to_julian_date(), unit='D')\ndf['MONTH'] = pd.DatetimeIndex(df['DATE']).month\ndf['DAY_OF_WEEK'] = df['DATE'].dt.day_name()\ndf_orig = df.copy()\nprint(df.head)","d8da9e1e":"#The preprocessing procedure: Turning all the needed values into a numerical form. The 'Date' and all of the na's have to be dropped\nle = preprocessing.LabelEncoder()\ndf['STAT_CAUSE_DESCR'] = le.fit_transform(df['STAT_CAUSE_DESCR'])\ndf['STATE'] = le.fit_transform(df['STATE'])\ndf['DAY_OF_WEEK'] = le.fit_transform(df['DAY_OF_WEEK'])\ndf = df.drop('DATE',axis=1)\ndf = df.dropna()","9f96f56e":"#The goal is to predict the cause of the fire = STAT_CAUSE_DESCR.\nX = df.drop(['STAT_CAUSE_DESCR'], axis=1).values\ny = df['STAT_CAUSE_DESCR'].values","2d5b0981":"#Creating the test and the training sets. 70% for training and 30% for testing.\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)","3af6f7b2":"#The Decision tree model\nclf_dt = tree.DecisionTreeClassifier(random_state=0, max_depth=20)\nclf_dt = clf_dt.fit(X_train, y_train)\nprint(clf_dt.score(X_test,y_test))","f7057169":"#The Random forest model\nclf_rf = ske.RandomForestClassifier(n_estimators=50)\nclf_rf = clf_rf.fit(X_train, y_train)\nprint(clf_rf.score(X_test,y_test))","948ec955":"#The Gradient boost model\nxg_clf = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0, objective ='binary:logistic', colsample_bytree = 0.5, learning_rate = 0.2, max_depth = 15, alpha = 20, n_estimators = 500, subsample=0.8)\nxg_clf = xg_clf.fit(X_train, y_train)\nprint(xg_clf.score(X_test,y_test))","f775fd3e":"#Putting all the causes into 4 different categories, since there are so many of them. \ndef set_label(cat):\n    cause = 0\n    natural = ['Lightning']\n    accidental = ['Structure','Fireworks','Powerline','Railroad','Smoking','Children','Campfire','Equipment Use','Debris Burning']\n    malicious = ['Arson']\n    other = ['Missing\/Undefined','Miscellaneous']\n    if cat in natural:\n        cause = 1\n    elif cat in accidental:\n        cause = 2\n    elif cat in malicious:\n        cause = 3\n    else:\n        cause = 4\n    return cause\n\ndf['LABEL'] = df_orig['STAT_CAUSE_DESCR'].apply(lambda x: set_label(x)) # I created a copy of the original df earlier in the kernel\ndf = df.drop('STAT_CAUSE_DESCR',axis=1)\nprint(df.head())\n#The new categories are: \"natural, accidental, malicious and other\"","2f4dee04":"#This time we want to predict 'Label'. Splitting the dataset into train and test sets once again.\nX = df.drop(['LABEL'], axis=1).values\ny = df['LABEL'].values\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)","12e9fc49":"#The Decision tree model\nclf_dt = tree.DecisionTreeClassifier(random_state=0, max_depth=30)\nclf_dt = clf_dt.fit(X_train, y_train)\nprint(clf_dt.score(X_test,y_test))","63a5d3ae":"#The Random forest model\nclf_rf = ske.RandomForestClassifier(n_estimators=50)\nclf_rf = clf_rf.fit(X_train, y_train)\nprint(clf_rf.score(X_test,y_test))","29885919":"#The Gradient boost model\nxg_clf = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0, objective ='binary:logistic', colsample_bytree = 0.5, learning_rate = 0.2, max_depth = 15, alpha = 20, n_estimators = 500, subsample=0.8)\nxg_clf = xg_clf.fit(X_train, y_train)\nprint(xg_clf.score(X_test,y_test))","dce9bb22":"*  I will be using the Decision tree, Random forest, and Gradient boost models to see if I can get better answers than O'Neill could.","7c75db97":"# Results:\nSuprsingly the Random forest (70,1%) model beat the Gradient boost model (70,6%) by 0,13%! Maybe it's because I didnt optimize the Gradient boost model properly, but the results are still pretty impressive. They both saw a growth of about 12% which is pretty good. The Decision tree model scored last place again, which is understandable since it is the weakest link. It received a score of 62,9%. The models growth was impressive, it grew by 8%! Not as good as the other 2 but still pretty impressive.\n\nOverall the Gradient boost and Random forest models are the strongest ones, and they're pretty equal in terms of accuracy in this task. Maybe they could be optimized even further for better results. \n\n# For the future:\nI had an idea that I could make the models guess the 4 different categories first, and then make it guess which one of the reasons it is from the said categories. It could improve the accuracy by a lot, or make it worse since it has to predict more things. This could be pretty interesting and I will be updating this notebook later if I manage to do it. ","05f99182":"# Predicting the causes of wildfires using python part 2\nA continuation of Paul O'Neill's notebook on the subject. Please check his notebook out at: https:\/\/www.kaggle.com\/edhirif\/predict-the-causes-of-wildfires-using-python \n\nI simply just add two more algorithms and compare them.\n\nNote: I am a beginner, so if you have any suggestions on improvements please let me know! Feedback is much appreciated.","bfc6e7ef":"# Results:\nThe Gradient boost model got the best result out of the ran models (58,2%), though it only beat the Random forest model (58%) by 0,2%. The Decision tree model is by far the weakest out of these and it got a score of 54%. It was 8% better than what O'Neill said he got from his Decision tree model. ","a166c2dc":"* I advise you to check the original notebook made by O'Neill, if you want to see data exploration. I'm only focusing on the results."}}