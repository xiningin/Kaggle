{"cell_type":{"b5e463b5":"code","bbc8a3fa":"code","7b107a12":"code","edea0a90":"code","2889745a":"code","1c3f4018":"code","a3e0e591":"code","e534cf93":"code","8c91306a":"code","fb1f45d4":"code","fdd10117":"code","f1f5774f":"code","44bb0033":"code","24cdf0a8":"code","8a9a416f":"code","dd4a50e0":"code","94699c6f":"code","4d185fbb":"code","a6c9e0b7":"code","36731e6e":"code","d1f04aaf":"code","74482bdd":"code","42a8a26e":"code","480ad2aa":"code","c233edaf":"code","836d185e":"code","cf78afaa":"code","1dec632b":"code","7dfbd58c":"code","e759f251":"code","53c17938":"code","23422414":"code","000dfafd":"code","1d7ce8d7":"code","b8df5299":"code","0b6ed1ee":"code","dcda8fc1":"code","709bf148":"code","e1d25fe3":"code","47c90da7":"code","bfd59a88":"code","4f4f246e":"code","0761f629":"code","26953843":"code","d678b4f1":"code","009177e9":"code","a24bba5e":"code","1030011b":"code","6eff9be6":"code","d8fc2de5":"code","dc9ded2f":"code","f67a0065":"code","0269abaf":"code","b3d663cc":"code","842d5b21":"code","d145d448":"code","7ebeebe3":"code","115276f1":"code","f7f2d358":"code","fb37f245":"code","d47b24af":"code","7d167af1":"code","e1cd06bc":"code","4f47d22c":"code","e81a407d":"code","5bbaade4":"code","4e2449c8":"code","627ae7fd":"code","11a3be8b":"code","9fd1bfcc":"code","4a93706e":"code","e972d9ae":"code","93e11426":"code","6299135d":"code","946d9cf7":"code","9e707a8b":"code","d1693173":"code","8afd1ddd":"code","55cf3208":"code","750b8887":"code","a5febd43":"code","29c940fe":"code","7d01241b":"code","c6d6ed90":"code","f089e705":"code","0f3cda64":"code","9e562a91":"code","a9b8fa1e":"code","b78d5792":"code","c0d4b470":"code","fb2831fe":"code","6123f9b1":"code","c7b37aa7":"code","70995deb":"code","546c9402":"code","a87c301e":"code","ea142bf5":"code","8281ce2b":"code","d485fee5":"code","7eeb8f89":"code","1e594ec8":"code","4cabdb47":"code","5bfb10ce":"code","b80128ef":"code","ee75b66d":"code","fb56bbbc":"code","bc0942ec":"code","37915f5a":"code","a0aa1e7b":"code","c277c0dc":"code","e5ecfed9":"code","55278118":"code","e6a10e26":"code","d4accef7":"code","f4aacfba":"code","5b041ae6":"code","551b9c5a":"code","0d8a8c76":"code","71955eda":"code","387cc262":"code","71a2cb4c":"code","fc6d2e71":"code","d221fcee":"code","e031ee0a":"code","d4e6d205":"code","05cf014d":"code","55b97306":"code","a9e55334":"code","88bb416f":"code","b8660610":"code","8e070e1f":"code","caef6154":"code","e277a396":"markdown","8c161cca":"markdown","a18597b1":"markdown","28a3925e":"markdown","00f0c95d":"markdown","725e7379":"markdown","4d9e36dd":"markdown","7f8b7c98":"markdown","dedd3315":"markdown","23b76507":"markdown","e2383b2c":"markdown","dfa28e38":"markdown","6ac55a85":"markdown","434925db":"markdown","2417e66d":"markdown","3a8eb8b4":"markdown","2b85231e":"markdown","20e2cbce":"markdown","c1564eb7":"markdown","b1a9f23b":"markdown","e9b64dfe":"markdown","fcbc48be":"markdown","771f43ff":"markdown","a38573d6":"markdown","194c2033":"markdown","7f762d17":"markdown","f2cff231":"markdown","b844963a":"markdown","ecc1c224":"markdown","c78555e5":"markdown","6205bf9d":"markdown","5bf75793":"markdown","d40838b5":"markdown","ef90e223":"markdown","d4f2425c":"markdown","dc22e241":"markdown","486b2217":"markdown","5ba0961c":"markdown","578602e6":"markdown","e53a471c":"markdown","a184aefb":"markdown","263843a9":"markdown","8e53d19b":"markdown","21a7d0ff":"markdown","02c92aff":"markdown","5a8b90ce":"markdown","483c235c":"markdown","c7eb7001":"markdown","3fe5701f":"markdown","80723196":"markdown","2c26d732":"markdown","a62726c0":"markdown","edaed179":"markdown","32eae24f":"markdown","2b2240ed":"markdown","cb8eb683":"markdown","49435ae9":"markdown","df1fab7c":"markdown","6d0b3e97":"markdown","16d3de9d":"markdown","fa8c3473":"markdown","5ce15176":"markdown","d78d01b4":"markdown","866525d3":"markdown","bb205688":"markdown","aa346333":"markdown","7aa51598":"markdown","d30af99f":"markdown","cb1bc465":"markdown","9fcba74d":"markdown","49beb43f":"markdown"},"source":{"b5e463b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#encoding\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\n#plots\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#for learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\n#metrics\nfrom sklearn import metrics\n\n\n\n\n#for data set object columns evaluation\nimport collections\n\npd.set_option('display.max_columns', 1000)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bbc8a3fa":"#Read the data\ndf_lead_score=pd.read_csv(\"..\/input\/Leads.csv\")\n#df_lead_score=pd.read_csv(\"Leads.csv\")","7b107a12":"#examine the data set\nprint('Samples in the data set:', df_lead_score.shape[0])\nprint('Columns in the data set:', df_lead_score.shape[1])\nprint('\\n\\n Columns Data Type:')\nprint(df_lead_score.info())\nprint('\\n\\n Nulls in Columns:',df_lead_score.isnull().sum().sum())","edea0a90":"#function to get missing values\ndef getMissingPercentageFeature(col,input_df):\n    null_counts = (input_df[col].isnull().sum()\/len(input_df[col]))\n    print(\"\\tMissing values for Feature:\",col,\"-\",round(null_counts*100,4))","2889745a":"#function to get missing values\ndef getMissingPercentage(input_df):\n    null_counts = (input_df.isnull().sum()\/len(input_df)).sort_values(ascending=False)\n    null_counts=null_counts[null_counts!=0]\n    null_counts=round(null_counts*100,4)\n    #print(null_counts)\n    plt.figure(figsize=(16,8))\n    plt.xticks(np.arange(len(null_counts))+0.5,null_counts.index,rotation='vertical')\n    plt.ylabel('Fraction of Rows with Missing Columns')\n    plt.bar(np.arange(len(null_counts)),null_counts)\n    return null_counts","1c3f4018":"#function to drop columns\ndef dropcolumns(df,percentage):\n    df_missing_percentage=getMissingPercentage(df)\n    columns_to_be_dropped=df_missing_percentage[df_missing_percentage>=percentage]\n    result_df=df.drop(columns=columns_to_be_dropped.index,axis=1)\n    return result_df,list(columns_to_be_dropped.index)","a3e0e591":"#view the null % for each column based on number of rows in the data set\ngetMissingPercentage(df_lead_score)","e534cf93":"#Create array(index set) for object features and non-object features\ndef classifyfeatures(df):\n    object_features = df.select_dtypes(include = [\"object\"]).columns\n    non_object_features = df.select_dtypes(exclude = [\"object\"]).columns\n    return(list(object_features),list(non_object_features))","8c91306a":"#function to display features in a numbered way\ndef printFeatures(featureslist):\n    print( \"Number of features:\",len(featureslist))\n    for i,col in enumerate(featureslist):\n        print(\"Feature:\",i+1,col)","fb1f45d4":"#function to display features in a numbered way\ndef printCountOfFeatures(featureslist,featurelistname):\n    print( \"Number of features for\",featurelistname,\":\",len(featureslist))","fdd10117":"#function to replace feature keys characters for python usage\ndef replacecolumnkeys(col,df):\n    df[col]= df[col].str.replace(' ', '_')\n    df[col]= df[col].str.replace(',', '_')\n    df[col]= df[col].str.replace('-', '_')\n    df[col]= df[col].str.replace('\/', '_')\n    df[col]= df[col].str.replace(':', '_')\n    return df","f1f5774f":"#assign list of object and non-object features \/ columns for the data set\nobj_features_lead_score, non_obj_features_lead_score=classifyfeatures(df_lead_score)\n\n#before cleaning\nprintCountOfFeatures(obj_features_lead_score,'object_features')\nprintCountOfFeatures(non_obj_features_lead_score,'non_object_features ')\n\n#print categorical features\nprint( \"\\n\\nCategorical features:\\n\")\nprintFeatures(obj_features_lead_score)\n#print Non Object features\nprint( \"\\n\\nNon Object features:\\n\")\nprintFeatures(non_obj_features_lead_score)","44bb0033":"#copy data set for cleaning\ndf_lead_score_cleaned=df_lead_score.copy()\n\n#Fixing column names\n# Replace space with underscore\ndf_lead_score_cleaned.columns = df_lead_score_cleaned.columns.str.replace(\" \", \"_\")\n\n# Reassign feature groups after fixing of Categorical features\nobj_features_lead_score, non_obj_features_lead_score=classifyfeatures(df_lead_score_cleaned)\n\n#lets remove Prospect_ID,Lead_Number,Converted from the object features list for handling perspective\nobj_features_lead_score.remove('Prospect_ID')\nnon_obj_features_lead_score.remove('Lead_Number')\nnon_obj_features_lead_score.remove('Converted')","24cdf0a8":"#function to print unique keys and values of a feature\ndef uniquecolumnkeys(columns,df):\n        for i,col in enumerate(columns):\n            #print count having less than 0\n            print(\"\\nFeature:\",i+1,col)\n            print(\".....................................\")\n            print(\"\\tCategories:\",df[col].unique())\n            freq = collections.Counter(df[col])\n            print(\"\\tMost Common Key:\",freq.most_common()[0][0])\n            for key, value in freq.items(): \n                print(\"\\t\",key,\" :\", value)","8a9a416f":"#lets view the frequency for each of the object\nuniquecolumnkeys(obj_features_lead_score,df_lead_score_cleaned)","dd4a50e0":"# lets replace value Select with nan, Select means that user hasn't given any value for a specific question \/ information parameter\ndf_lead_score_cleaned[obj_features_lead_score]=df_lead_score_cleaned[obj_features_lead_score].replace('Select',np.nan)","94699c6f":"#Features with Yes\/No values\n#Feature: 3 Do_Not_Email - we can keep\n#Feature: 4 Do_Not_Call, only 2 values are no, so we can drop it\n#Feature: 11 Search , only 14 values are no, so we can drop it\n#Feature: 12 Magazine all are no - we can drop it as well\n#Feature: 13 Newspaper_Article, only 2 values are no, so we can drop it\n#Feature: 14 X_Education_Forums , only 1 is yes, so we can drop it\n#Feature: 15 Newspaper , only 1 is yes, so we can drop it\n#Feature: 16 Digital_Advertisement all are no, so we can drop it\n#Feature: 17 Through_Recommendations , only 7 yes, so we can drop it\n#Feature: 18 Receive_More_Updates_About_Our_Courses, all are no, so we can drop it\n#Feature: 21 Update_me_on_Supply_Chain_Content , all are no, so we can drop it\n#Feature: 22 Get_updates_on_DM_Content , all are no, so we can drop it\n#Feature: 27 I_agree_to_pay_the_amount_through_cheque , all are no, so we can drop it\nfeatures_dropped=['Do_Not_Email','Do_Not_Call','Search','Magazine','Newspaper_Article','X_Education_Forums','Newspaper','Digital_Advertisement','Digital_Advertisement',\n                 'Through_Recommendations','Receive_More_Updates_About_Our_Courses','Update_me_on_Supply_Chain_Content','Get_updates_on_DM_Content',\n                 'I_agree_to_pay_the_amount_through_cheque']\ndf_lead_score_cleaned=df_lead_score_cleaned.drop(columns=features_dropped,axis=1)","4d185fbb":"#Feature: 1 Lead_Origin , no nans\n","a6c9e0b7":"# function for univariate \/ bivariate and ratio analysis\nfrom numpy import nan\ndef TargetvsColFrequency(col1,col2,df):\n    freq = collections.Counter(df[col1])\n    del freq[nan]\n    fig, ax =plt.subplots(figsize=(10,5))\n    sns.set_context({\"figure.figsize\": (10, 4)})\n    ax = sns.countplot(y=col1,data=df,hue=col2,order = df[col1].value_counts().index)\n    df_final1=pd.DataFrame()\n    df_final0=pd.DataFrame()\n    df_temp1=pd.DataFrame()\n    df_temp0=pd.DataFrame()\n    rowvalue1=0\n    rowvalue0=0\n    for key, value in freq.items(): \n        if key=='nan':\n            continue\n        df_plot=pd.DataFrame(df[df[col1]==key].groupby(col2)[col1].count())\n        df_plot['RATIO']=df_plot\/df_plot.sum()      \n        title=str(key)+' in '+str(col1)\n        print(\"\\n\",title,\"\\n............................\\n\",df_plot)\n        df_plot.reset_index(inplace=True)\n        fig, ax =plt.subplots(figsize=(5,5))\n        ax.set_title(title)\n        sns.barplot(y='RATIO',x=col2,data=df_plot)\n        plt.tight_layout()\n        try:\n            rowvalue1=df_plot['RATIO'][1] \n            df_temp1[key] =[rowvalue1]\n        except KeyError:\n            continue\n        try:\n            rowvalue0=df_plot['RATIO'][0] \n            df_temp0[key] =[rowvalue0]\n        except KeyError:\n            continue\n    df_final1= pd.concat([df_final1,df_temp1], axis=1, sort=False)\n    df_final1=df_final1.reset_index()\n    df_final1=pd.DataFrame(df_final1.T.iloc[:,-1].sort_values(ascending=False)).rename(columns={'0':'RATIO'})\n    df_final1.drop(df_final1.tail(1).index,inplace=True)\n    fig, ax =plt.subplots(1,2,figsize=(15,7))\n    title=str(col1)+ ' % Of Ratio for Converted Leads'\n    ax[0].set_title(title)\n    df_final1.iloc[:,-1].plot(kind='barh',ax=ax[0])\n    \n    df_final0= pd.concat([df_final0,df_temp0], axis=1, sort=False)\n    df_final0=df_final0.reset_index()\n    df_final0=pd.DataFrame(df_final0.T.iloc[:,-1].sort_values(ascending=False)).rename(columns={'0':'RATIO'})\n    df_final0.drop(df_final0.tail(1).index,inplace=True)\n    title1=str(col1)+ ' % Of Ratio for Not Converted Leads'\n    ax[1].set_title(title1)\n    df_final0.iloc[:,-1].plot(kind='barh',ax=ax[1])\n    plt.tight_layout()","36731e6e":"# Feature: 2 Lead_Source\n#view before cleaning\nuniquecolumnkeys(['Lead_Source'],df_lead_score_cleaned)\nTargetvsColFrequency('Lead_Source','Converted',df_lead_score_cleaned)","d1f04aaf":"# Feature: 2 Lead_Source\n## 1. replace google with Goolge in Lead Score\ndf_lead_score_cleaned['Lead_Source']=df_lead_score_cleaned['Lead_Source'].replace('google','Google')\n## 2. Replace nan  : 36 with Most Common Key: Google (mode) , as this wouldnt create major bias towards Google as a lead_source\ndf_lead_score_cleaned['Lead_Source']=df_lead_score_cleaned['Lead_Source'].replace(np.nan,'Google')\n\n#view after cleaning\nuniquecolumnkeys(['Lead_Source'],df_lead_score_cleaned)\nTargetvsColFrequency('Lead_Source','Converted',df_lead_score_cleaned)","74482bdd":"#Feature: 5 Last_Activity\n#view before cleaning\nuniquecolumnkeys(['Last_Activity'],df_lead_score_cleaned)\nTargetvsColFrequency('Last_Activity','Converted',df_lead_score_cleaned)","42a8a26e":"#Feature: 5 Last_Activity\n## 1. Replace nan  : 103 with Most Common Key: Email Opened (mode)\ndf_lead_score_cleaned['Last_Activity']=df_lead_score_cleaned['Last_Activity'].replace(np.nan,'Email Opened')\n\n#view after cleaning\nuniquecolumnkeys(['Last_Activity'],df_lead_score_cleaned)\nTargetvsColFrequency('Last_Activity','Converted',df_lead_score_cleaned)","480ad2aa":"#Feature: 6 Country\n#view before cleaning\nuniquecolumnkeys(['Country'],df_lead_score_cleaned)\nTargetvsColFrequency('Country','Converted',df_lead_score_cleaned)","c233edaf":"#Feature: 6 Country\n ## 1.  Replace nan  : 2461 with unknown as value. We think we shouldnt update it with Most Common Key: India, as this would create high bias over india\ndf_lead_score_cleaned['Country']=df_lead_score_cleaned['Country'].replace(np.nan,'unknown')\n\n## 2. lets group the countries as India, AMERICAS,EMEA, APAC,UAE \n\ndf_lead_score_cleaned['Country']=df_lead_score_cleaned['Country'].replace(['Russia', 'Kuwait', 'Oman' ,'United Kingdom' ,'Bahrain', 'Ghana','Qatar','Saudi Arabia',\n                                                                           'Belgium', 'France','Netherlands','Sweden', 'Nigeria','Germany',\n                                                                           'Uganda', 'Kenya', 'Italy' ,'South Africa', 'Tanzania'\n                                                                           ,'Liberia','Switzerland' ,'Denmark'],'EMEA')\n\ndf_lead_score_cleaned['Country']=df_lead_score_cleaned['Country'].replace(['Singapore','Sri Lanka','China','Hong Kong','Asia\/Pacific Region','Malaysia',\n                                                                          'Philippines','Bangladesh','Vietnam','Indonesia','Australia'],'APAC')\n\n\ndf_lead_score_cleaned['Country']=df_lead_score_cleaned['Country'].replace(['United States','Canada'],'AMERICAS')\n\n\n#view after cleaning\nuniquecolumnkeys(['Country'],df_lead_score_cleaned)\nTargetvsColFrequency('Country','Converted',df_lead_score_cleaned)","836d185e":"#Feature: 7 Specialization\n#view before cleaning\nuniquecolumnkeys(['Specialization'],df_lead_score_cleaned)\nTargetvsColFrequency('Specialization','Converted',df_lead_score_cleaned)\n","cf78afaa":"#Feature: 7 Specialization\n## 1. Most Common Key is nan. So lets replace it with a value of Other as a value\ndf_lead_score_cleaned['Specialization']=df_lead_score_cleaned['Specialization'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['Specialization'],df_lead_score_cleaned)\nTargetvsColFrequency('Specialization','Converted',df_lead_score_cleaned)\n","1dec632b":"#Feature: 8 How_did_you_hear_about_X_Education\n#view before cleaning\nuniquecolumnkeys(['How_did_you_hear_about_X_Education'],df_lead_score_cleaned)\nTargetvsColFrequency('How_did_you_hear_about_X_Education','Converted',df_lead_score_cleaned)\n","7dfbd58c":"#Feature: 8 How_did_you_hear_about_X_Education\n## 1. Most Common Key is nan. So lets replace it with a value of Other as a value\ndf_lead_score_cleaned['How_did_you_hear_about_X_Education']=df_lead_score_cleaned['How_did_you_hear_about_X_Education'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['How_did_you_hear_about_X_Education'],df_lead_score_cleaned)\nTargetvsColFrequency('How_did_you_hear_about_X_Education','Converted',df_lead_score_cleaned)\n\n","e759f251":"#Feature: 9 What_is_your_current_occupation\n\n#view before cleaning\nuniquecolumnkeys(['What_is_your_current_occupation'],df_lead_score_cleaned)\nTargetvsColFrequency('What_is_your_current_occupation','Converted',df_lead_score_cleaned)","53c17938":"#Feature: 9 What_is_your_current_occupation\n## 1. nan  : 2709  So lets replace it with a value of Other as a value\ndf_lead_score_cleaned['What_is_your_current_occupation']=df_lead_score_cleaned['What_is_your_current_occupation'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['What_is_your_current_occupation'],df_lead_score_cleaned)\nTargetvsColFrequency('What_is_your_current_occupation','Converted',df_lead_score_cleaned)","23422414":"# Feature: 10 What_matters_most_to_you_in_choosing_a_course\n\n#view before cleaning\nuniquecolumnkeys(['What_matters_most_to_you_in_choosing_a_course'],df_lead_score_cleaned)\nTargetvsColFrequency('What_matters_most_to_you_in_choosing_a_course','Converted',df_lead_score_cleaned)\n","000dfafd":"# Feature: 10 What_matters_most_to_you_in_choosing_a_course\n## 1. nan  : 2709. So lets replace it with a value of Other as a value\ndf_lead_score_cleaned['What_matters_most_to_you_in_choosing_a_course']=df_lead_score_cleaned['What_matters_most_to_you_in_choosing_a_course'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['What_matters_most_to_you_in_choosing_a_course'],df_lead_score_cleaned)\nTargetvsColFrequency('What_matters_most_to_you_in_choosing_a_course','Converted',df_lead_score_cleaned)\n","1d7ce8d7":"#Feature: 19 Tags\n#view before cleaning\nuniquecolumnkeys(['Tags'],df_lead_score_cleaned)\nTargetvsColFrequency('Tags','Converted',df_lead_score_cleaned)\n","b8df5299":"#Feature: 19 Tags\n## 1. Most Common Key is nan. So lets replace it with a value of Other as a value\ndf_lead_score_cleaned['Tags']=df_lead_score_cleaned['Tags'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['Tags'],df_lead_score_cleaned)\nTargetvsColFrequency('Tags','Converted',df_lead_score_cleaned)","0b6ed1ee":"#Feature: 20 Lead_Quality\n#view before cleaning\nuniquecolumnkeys(['Lead_Quality'],df_lead_score_cleaned)\nTargetvsColFrequency('Lead_Quality','Converted',df_lead_score_cleaned)","dcda8fc1":"#Feature: 20 Lead_Quality\n## 1. Most Common Key: nan so lets move all nan into a bucket with value as 'unknown'\ndf_lead_score_cleaned['Lead_Quality']=df_lead_score_cleaned['Lead_Quality'].replace(np.nan,'Other')\n#view after cleaning\nuniquecolumnkeys(['Lead_Quality'],df_lead_score_cleaned)\nTargetvsColFrequency('Lead_Quality','Converted',df_lead_score_cleaned)","709bf148":"#Feature: 23 Lead_Profile\n#view before cleaning\nuniquecolumnkeys(['Lead_Profile'],df_lead_score_cleaned)\nTargetvsColFrequency('Lead_Profile','Converted',df_lead_score_cleaned)\n","e1d25fe3":"#Feature: 23 Lead_Profile\n## 1. Most Common Key: nan \n## 2. Other Leads  : 487, lets move all of them into Other as value\ndf_lead_score_cleaned['Lead_Profile']=df_lead_score_cleaned['Lead_Profile'].replace(np.nan,'Other')\ndf_lead_score_cleaned['Lead_Profile']=df_lead_score_cleaned['Lead_Profile'].replace('Other Leads','Other')\n\n#view after cleaning\nuniquecolumnkeys(['Lead_Profile'],df_lead_score_cleaned)\nTargetvsColFrequency('Lead_Profile','Converted',df_lead_score_cleaned)","47c90da7":"#Feature: 24 City\n#view before cleaning\nuniquecolumnkeys(['City'],df_lead_score_cleaned)\nTargetvsColFrequency('City','Converted',df_lead_score_cleaned)","bfd59a88":"#Feature: 24 City\n## 1. Most Common Key: nan \n## 2. there are many 'other' type city values like - Other Metro Cities  : 380,Other Cities  : 686,Other Cities of Maharashtra  : 457, Tier II Cities  : 74\n## so lets move nan to Other Cities  : 686, which is the max value in other cities\ndf_lead_score_cleaned['City']=df_lead_score_cleaned['City'].replace(np.nan,'Other Cities')\ndf_lead_score_cleaned['City']=df_lead_score_cleaned['City'].replace('Other Cities','Other')\n\n#view after cleaning\nuniquecolumnkeys(['City'],df_lead_score_cleaned)\nTargetvsColFrequency('City','Converted',df_lead_score_cleaned)","4f4f246e":"#Feature: 25 Asymmetrique_Activity_Index and Feature: 26 Asymmetrique_Profile_Index\n# 1. Most Common Key: nan , lets treat them later based on respective score values. no changes to be done now\n#Feature: 29 Last_Notable_Activity - no nans\n","0761f629":"#features to be not considered initially as they have significant number of No values\n#featueres 'Asymmetrique_Activity_Index' and 'Asymmetrique_Profile_Index' are redundant as we have the corresponding score features.\nfeatures_excluded=['Asymmetrique_Activity_Index','Asymmetrique_Profile_Index']\ndf_lead_score_cleaned.drop(features_excluded,axis=1,inplace=True)\n","26953843":"\n#The below features are categorical than non-object type, so we have moved them to object dtype\ndf_lead_score_cleaned['Asymmetrique_Profile_Score']=df_lead_score_cleaned['Asymmetrique_Profile_Score'].astype(object)\ndf_lead_score_cleaned['Asymmetrique_Activity_Score']=df_lead_score_cleaned['Asymmetrique_Activity_Score'].astype(object)\n","d678b4f1":"# Asymmetrique_Activity_Score data cleaning\ndf_lead_score_cleaned['Asymmetrique_Activity_Score']=df_lead_score_cleaned['Asymmetrique_Activity_Score'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['Asymmetrique_Activity_Score'],df_lead_score_cleaned)\nTargetvsColFrequency('Asymmetrique_Activity_Score','Converted',df_lead_score_cleaned)","009177e9":"# Asymmetrique_Profile_Score data cleaning\ndf_lead_score_cleaned['Asymmetrique_Profile_Score']=df_lead_score_cleaned['Asymmetrique_Profile_Score'].replace(np.nan,'Other')\n\n#view after cleaning\nuniquecolumnkeys(['Asymmetrique_Profile_Score'],df_lead_score_cleaned)\nTargetvsColFrequency('Asymmetrique_Profile_Score','Converted',df_lead_score_cleaned)","a24bba5e":"# Reassign feature groups after fixing of Categorical features\nobj_features_lead_score, non_obj_features_lead_score=classifyfeatures(df_lead_score_cleaned)\n#lets remove Prospect_ID,Lead_Number,Converted from the object features list for handling perspective\nobj_features_lead_score.remove('Prospect_ID')\nnon_obj_features_lead_score.remove('Lead_Number')\nnon_obj_features_lead_score.remove('Converted')","1030011b":"#lets view the missing dat and then drop columns having >70% of missing values\ndf_lead_score_cleaned,dropped_columns1=dropcolumns(df_lead_score_cleaned,70)","6eff9be6":"# print dropped Columns\nprint('Dropped Columns Set 1 having >70% missing values:')\nprint('\\t number of columns:',len(dropped_columns1))\nprint('\\t columns:',dropped_columns1)","d8fc2de5":"#print per feature value <0, >0 and equal to 0\ndef lessgreaterequaltoZero(columns,df):\n    for i,col in enumerate(columns):\n        #print count having less than 0\n        print(\"Feature:\",i+1,col)\n        print(\"\\tless than 0:\",len(df[df[col]<0][col]))\n        #print count having greater than 0\n        print(\"\\tgreater than 0:\",len(df[df[col]>0][col]))\n        #print count having 0\n        print(\"\\tequal to 0:\",len(df[df[col]==0][col]))\n        getMissingPercentageFeature(col,df)","dc9ded2f":"#print for amount columns\nlessgreaterequaltoZero(non_obj_features_lead_score,df_lead_score_cleaned)\n","f67a0065":"#lets visualize the numeric features\n\ndf_lead_score_cleaned_not_converted=df_lead_score_cleaned[df_lead_score_cleaned.Converted==0]\ndf_lead_score_cleaned_converted=df_lead_score_cleaned[df_lead_score_cleaned.Converted==1]\n\nfig=plt.figure(figsize=(10,5)) ## setting over-all figure size (optional)\n\nplt.subplot(2,3,1) \n\nplt.subplot(2,3,1) \nax1=sns.boxplot(df_lead_score_cleaned_not_converted.TotalVisits)\nax1.set_title('Not Converted Leads',color='red',size=15)\n\nplt.subplot(2,3,2) \nsns.boxplot(df_lead_score_cleaned_not_converted.Total_Time_Spent_on_Website)\n\nplt.subplot(2,3,3) \nsns.boxplot(df_lead_score_cleaned_not_converted.Page_Views_Per_Visit)\n\n\n\nplt.subplot(2,3,4) \nax2=sns.boxplot(df_lead_score_cleaned_converted.TotalVisits)\nax2.set_title('Converted Leads',color='green',size=15)\n\nplt.subplot(2,3,5) \nsns.boxplot(df_lead_score_cleaned_converted.Total_Time_Spent_on_Website)\n\nplt.subplot(2,3,6) \nsns.boxplot(df_lead_score_cleaned_converted.Page_Views_Per_Visit)\n\nplt.tight_layout()","0269abaf":"#calculate imbalance\nimbalance_lead_score=df_lead_score_cleaned['Converted'].sum()\/(df_lead_score_cleaned['Converted'].count()+df_lead_score_cleaned['Converted'].sum())\nprint(\"Imbalance for Converted Lead Score %\",round(imbalance_lead_score*100,2))","b3d663cc":"#print categorical features\nprint( \"Categorical features:\\n\")\nprintFeatures(obj_features_lead_score)\n#print Non Object features\nprint( \"\\n\\nNon Object features:\\n\")\nprintFeatures(non_obj_features_lead_score)","842d5b21":"#view the null % for each column based on number of rows in the data set\ngetMissingPercentage(df_lead_score_cleaned[non_obj_features_lead_score])","d145d448":"#lets view the stats for numerical features BEFORE treating for missing values\nprint('stats for Page_Views_Per_Visit:\\n',df_lead_score_cleaned.Page_Views_Per_Visit.describe())\nprint('\\nstats for TotalVisits:\\n',df_lead_score_cleaned.TotalVisits.describe())","7ebeebe3":"#lets replace missing values with median value 50% value for each\n\n# lets replace nan with median of 2 for Page_Views_Per_Visit\ndf_lead_score_cleaned.Page_Views_Per_Visit=df_lead_score_cleaned.Page_Views_Per_Visit.replace(np.nan,2)\n\n# lets replace nan with median of 3 for TotalVisits\ndf_lead_score_cleaned.TotalVisits=df_lead_score_cleaned.TotalVisits.replace(np.nan,3)\n","115276f1":"#lets view the stats for numerical features AFTER treating for missing values\nprint('stats for Page_Views_Per_Visit:\\n',df_lead_score_cleaned.Page_Views_Per_Visit.describe())\nprint('\\nstats for TotalVisits:\\n',df_lead_score_cleaned.TotalVisits.describe())","f7f2d358":"# evaluate data set for nulls\ndf_lead_score_cleaned.isnull().sum().sum()","fb37f245":"dropped_columns=['Prospect_ID','Lead_Number']\ndf_lead_score_cleaned.drop(dropped_columns,axis=1,inplace=True)","d47b24af":"#function to create dummies\ndef CreateDummies(df,feature):\n    # create dummy data frame with new name with original column name as prefix  \n    feature_dummy = pd.get_dummies(df[feature], drop_first = True,prefix=feature,prefix_sep='_')\n    \n    #concat feature_dummy with df    \n    df=pd.concat([df,feature_dummy],axis=1)\n    \n    #drop original column\n    df=df.drop(feature,axis=1)\n    \n    # return original df\n    return df","7d167af1":"# create dummy for A_free_copy_of_Mastering_The_Interview\ndf_lead_score_cleaned_encoded=CreateDummies(df_lead_score_cleaned,obj_features_lead_score)","e1cd06bc":"df_lead_score_cleaned_encoded.T.head(162)","4f47d22c":"#function for frequency based encoding for unordered categorical variables\ndef convertColsToFreqEncoding(df,col):\n    for x in col:\n        tempDict = df[x].value_counts().to_dict()\n        df[x] = df[x].map(tempDict)\n    return df","e81a407d":"#create frequency based encoding data set for model building at a later time\ndf_lead_score_FequEncoding_cleaned = df_lead_score_cleaned.copy()\ndf_lead_score_FequEncoding_cleaned = convertColsToFreqEncoding(df_lead_score_FequEncoding_cleaned,obj_features_lead_score)","5bbaade4":"#prepare X adn y\nX = df_lead_score_cleaned_encoded.drop(['Converted'], axis=1)\ny=df_lead_score_cleaned_encoded['Converted']","4e2449c8":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","627ae7fd":"def FeatureScalingdf(df_train,features,scaler):\n    #1.instantiate an object\n    if scaler=='minmax':\n        scaler=MinMaxScaler()\n    if scaler=='std':\n        scaler=StandardScaler()\n\n    #2. create a list of numeric variables\n    #already done numeric_features_car\n\n    #3.fit the object on data set\n    df_train[features]=scaler.fit_transform(df_train[features])\n\n    # 4. Asssess Numerical features\n    print( \"Numerical features of Training Data set after Scaling:\")\n    return df_train,scaler","11a3be8b":"#1. create a list of numeric variables\nfeatures_for_scaling=['Total_Time_Spent_on_Website','Page_Views_Per_Visit','TotalVisits']\n\n#2. Apply scalling\nX_train,X_train_scaler=FeatureScalingdf(X_train,features_for_scaling,'std')\nprint(X_train.head())\n\n#3. lets scale test data set as well \nX_test[features_for_scaling]=X_train_scaler.transform(X_test[features_for_scaling])\nprint(X_test.head())\n\n#4.Create logistic regression object\nlogreg = LogisticRegression()\n\n#5. run RFE\nrfe = RFE(logreg, 30)             # running RFE with 30 variables as output\nrfe = rfe.fit(X_train, y_train)\nprint(rfe.support_)","9fd1bfcc":"#create RFE Rank data frame for exploration\nfeature_list=[]\nrank_list=[]\nfor i in range (0, 158):\n    df_temp=pd.DataFrame()\n    feature_list.append(list(zip(X_train.columns, rfe.support_, rfe.ranking_))[i][0])\n    rank_list.append(list(zip(X_train.columns, rfe.support_, rfe.ranking_))[i][2])\ndf_rfe=pd.DataFrame({'feature':feature_list,'rank':rank_list})\n\n#view RFE features data set\ndf_rfe.sort_values(by='rank',ascending=True)[:50]\n","4a93706e":"#model 1\nimport statsmodels.api as sm\ncol = X_train.columns[rfe.support_]\nX_train_sm = sm.add_constant(X_train[col])\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","e972d9ae":"#model 2\ncol=list(col)\ncol.remove('Tags_Lateral student')\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","93e11426":"#model 3\ncol.remove('Tags_number not provided')\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","6299135d":"# Tags_wrong number given\n#model 4\ncol.remove('Tags_wrong number given')\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","946d9cf7":"#Tags_Interested in full time MBA\n#model 5.1\ncol.remove('Tags_Interested  in full time MBA')\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","9e707a8b":"#Tags_Interested in full time MBA\n#model 5.2\ncol.remove('Tags_invalid number')\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","d1693173":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8afd1ddd":"#Tags_Other\n#model 6\n# high VIF\ncol.remove('Tags_Other')\n\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","55cf3208":"#model 6.1\n# high p-value\ncol.remove('Lead_Quality_Other')\n\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","750b8887":"#model 6.2\n# high p-value\ncol.remove('Tags_in touch with EINS')\n\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","a5febd43":"#model 6.3\n# high p-value\ncol.remove('Last_Activity_Email Bounced')\n\nX_train_sm = sm.add_constant(X_train[col])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","29c940fe":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7d01241b":"def predict_probability(X_sm,y,cutoff):\n    \n    # PREDICT VALUES IN TRAINING DATA SET\n\n    #1.  Getting the predicted values on the train set\n    y_pred = res.predict(X_sm)\n    print(y[:10])\n\n    #2. Create data set with Converted values from original data set and the predicted probability\n    y_pred_final = pd.DataFrame({'Converted':y.values, 'Conversion_Probability':y_pred,'Lead_Score':round(y_pred*100,0)})\n    y_pred_final['LeadId'] = y.index\n    print(y_pred_final.head())\n\n    #3. Take a random cut off and review the metrics\n    y_pred_final['Predicted'] = y_pred_final.Conversion_Probability.map( lambda x: 1 if x > cutoff else 0)\n\n    #4. View Lead_score distribution ( top 20)\n    plt.figure(figsize=(10,10))\n    sns.countplot(y = 'Lead_Score',data = y_pred_final,order = y_pred_final['Lead_Score'].value_counts().index[:20],hue='Converted')\n    plt.title('Count of Leads per Lead Score')\n    plt.show()\n   \n    return y_pred_final","c6d6ed90":"#predict y_train values\ny_pred_train_final_dummy_encoded=predict_probability(X_train_sm,y_train,0.32)","f089e705":"# Metrics and Analysis\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import brier_score_loss\n\ndef confusion_metric_analysis(df,target,predicted):\n    #1. Create confusion matrix\n    confusion = metrics.confusion_matrix(df[target], df[predicted] )\n    print('Confusion Matrix:\\n',confusion)\n\n    #2.# Let's check the overall accuracy.\n    print('Metrics Accuracy Score:', metrics.accuracy_score(df[target], df[predicted]))\n    print('Metrics Balanced Accurancy:',metrics.balanced_accuracy_score(df[target],df[predicted]))\n\n    #3. Assess TP,TN,FP,FN\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    print('Converted predicted as Converted:',TP)\n    print('Not converted predicted as Not converted:',TN)\n    print('Converted predicted as Not Converted:',FP)\n    print('Not Converted predicted as Converted:',FN)\n    # Let's see the sensitivity of our logistic regression model\n    print ('Sensitivity :',TP \/ float(TP+FN))\n    # Let us calculate specificity\n    print('Specificity : ',TN \/ float(TN+FP))\n    # Calculate false postive rate - predicting Conversion when customer does not have converted\n    print('False Positve Rate : ',FP\/ float(TN+FP))\n    # positive predictive value \n    print ('Positive Predictive Value : ',TP \/ float(TP+FP))\n    # Negative predictive value\n    print ('Negative Predictve Value : ',TN \/ float(TN+ FN))\n    \n    print('\\nMathew \/ Phi co-efficient:',matthews_corrcoef(df[target],df[predicted]))\n    \n    print('\\nBrier Score for Probabilistic Prediction:',brier_score_loss(df[target],df[predicted]))\n\n    ","0f3cda64":"# metrics based on confusion matrix\nconfusion_metric_analysis(y_pred_train_final_dummy_encoded,'Converted','Predicted')","9e562a91":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","a9b8fa1e":"# Metrics ROC,\nfpr, tpr, thresholds = metrics.roc_curve( y_pred_train_final_dummy_encoded.Converted, y_pred_train_final_dummy_encoded.Conversion_Probability, drop_intermediate = False )\n\n#Draw ROC\ndraw_roc(y_pred_train_final_dummy_encoded.Converted, y_pred_train_final_dummy_encoded.Conversion_Probability)","b78d5792":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_pred_train_final_dummy_encoded[i]= y_pred_train_final_dummy_encoded.Conversion_Probability.map(lambda x: 1 if x > i else 0)\ny_pred_train_final_dummy_encoded.head()","c0d4b470":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_pred_train_final_dummy_encoded.Converted, y_pred_train_final_dummy_encoded[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","fb2831fe":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","6123f9b1":"#set cut off and redo labeling\ncutoff=0.32\ny_pred_train_final_dummy_encoded['final_predicted'] = y_pred_train_final_dummy_encoded.Conversion_Probability.map( lambda x: 1 if x > cutoff else 0)\n","c7b37aa7":"#analyse the metrics\nconfusion_metric_analysis(y_pred_train_final_dummy_encoded,'Converted','final_predicted')","70995deb":"from sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import classification_report\n\ndef precision_recall_metrics(df,target,predicted):\n        #confusion = metrics.confusion_matrix(df[target], y_train_pred_final[predicted])\n        #Precision TP \/ TP + FP\n        #print('Precision:',confusion[1,1]\/(confusion[0,1]+confusion[1,1])\n        #Recall TP \/ TP + FN\n        #print('Recall:',confusion[1,1]\/(confusion[1,0]+confusion[1,1])\n        #classification report\n        target_names = ['0', '1']\n        print('\\nClassification Report:\\n',classification_report(df[target],df[predicted],target_names=target_names))\n    ","546c9402":"from sklearn.metrics import hamming_loss\nfrom sklearn.metrics import log_loss\ndef metrics_loss(df,target,predicted,probability):\n    print('Hamming Loss:\\n',hamming_loss(df[target], df[predicted]))\n    print('\\nLogistic Regression Loss:\\n',log_loss(df[target], df[probability]))","a87c301e":"#analyse the metrics\nprecision_recall_metrics(y_pred_train_final_dummy_encoded,'Converted','final_predicted')\nmetrics_loss(y_pred_train_final_dummy_encoded,'Converted','final_predicted','Conversion_Probability')","ea142bf5":"#trade off\n#1. get the values\nfrom sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_pred_train_final_dummy_encoded.Converted, y_pred_train_final_dummy_encoded.Conversion_Probability)\n\n#2.visualize\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","8281ce2b":"# 1. Prediction for X_Test\nprint('Features shortlisted:',col)\n\n# 2. Prepare X_test\nX_test = X_test[col]\n\n#3. Add constant\nX_test_sm = sm.add_constant(X_test)\n\n#4 Predictions on test data set\ny_test_pred = res.predict(X_test_sm)\nprint('Predictions',y_test_pred[:10])\n\n#5.Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n\n##6.Putting LeadId to index\n#y_test_pd = pd.DataFrame(y_test)\n#y_test['LeadId'] = y_test.index\n#y_test_df=pd.DataFrame(y_test)\n\n#7.Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\n\n#8. Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test, y_pred_1],axis=1)\n\n#9.Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Probability'})\n\n#10. cut off probability\ny_pred_final['final_predicted'] = y_pred_final.Converted_Probability.map(lambda x: 1 if x > cutoff else 0)\n#y_pred_final['Converted'] = y_pred_final['Converted'].astype(int)\n\n#11. Sensitivity, Specificity Metrics\nconfusion_metric_analysis(y_pred_final,'Converted','final_predicted')\n\n#12. Precision , Recall Metrics\nprecision_recall_metrics(y_pred_final,'Converted','final_predicted')\n\n#13.Loss metrics,\nmetrics_loss(y_pred_final,'Converted','final_predicted','Converted_Probability')\n\n#14. Lead_Score Assignment\ny_pred_final['Lead_Score']=round(y_pred_final.Converted_Probability*100,0)\n\n#15. View Lead_score distribution ( top 30)\nplt.figure(figsize=(10,7))\nsns.countplot(y = 'Lead_Score',data = y_pred_final,order = y_pred_final['Lead_Score'].value_counts().index[:30],hue='Converted')\nplt.title('Count of Leads per Lead Score')\nplt.show()\n","d485fee5":"#plot correlation between amount features for target=1\ncorr_features=['Converted']\ncorr_features.extend(col)\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(40,15))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df_lead_score_cleaned_encoded[corr_features].corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n","7eeb8f89":"#view the correlation of features with Converted Leads\ndf_lead_score_cleaned_encoded[corr_features].corr()['Converted'].sort_values(ascending=False)[1:]","1e594ec8":"#create not converted data frame\ndf_not_converted=df_lead_score_cleaned_encoded[df_lead_score_cleaned_encoded.Converted==0][col]\ndf_not_coverted_numeric=df_lead_score_cleaned_encoded['Total_Time_Spent_on_Website']\ndf_not_converted_other=df_not_converted.drop('Total_Time_Spent_on_Website',axis=1)\n\n#create converted data frame\ndf_converted=df_lead_score_cleaned_encoded[df_lead_score_cleaned_encoded.Converted==1][col]\ndf_coverted_numeric=df_converted['Total_Time_Spent_on_Website']\ndf_converted_other=df_converted.drop('Total_Time_Spent_on_Website',axis=1)","4cabdb47":"#analyse numeric variable\ndf_not_coverted_numeric.describe()","5bfb10ce":"#combine metrics for further analysis\ndf_temp1=pd.DataFrame(df_not_coverted_numeric.describe()).reset_index()\ndf_temp2=pd.DataFrame(df_coverted_numeric.describe()).reset_index()\ndf_numeric=pd.merge(df_temp1,df_temp2,on='index')\ndf_numeric.columns=['Metric','Total_Time_Spent_on_Website_Not_converted','Total_Time_Spent_on_Website_converted']\ndf_numeric","b80128ef":"#visualize numeric variable through box plot\nsns.boxplot(df_numeric.Total_Time_Spent_on_Website_Not_converted)\n","ee75b66d":"#visualize numeric variable through box plot\nsns.boxplot(df_numeric.Total_Time_Spent_on_Website_converted)\n\n","fb56bbbc":"# create count of each dummy feature with value=1\ndf_not_converted_summary=pd.DataFrame(df_not_converted_other.sum()).reset_index()\ndf_converted_summary=pd.DataFrame(df_converted_other.sum()).reset_index()\ndf_not_converted_summary.columns=['feature','not_converted_count']\ndf_converted_summary.columns=['feature','converted_count']\n\n#create combined data set for comparision and analysis\ndf_compare=pd.merge(df_not_converted_summary,df_converted_summary,on='feature')\n\n#find the ratio of not_converted count out of total count\ndf_compare['percent_not_converted']=round(df_compare.not_converted_count\/(df_compare.not_converted_count+df_compare.converted_count)*100,2)\n\n#find the ratio of converted coutn out of total count\ndf_compare['percent_converted']=round(df_compare.converted_count\/(df_compare.not_converted_count+df_compare.converted_count)*100,2)\n\n#find the diff btw converted and not converted ratios to see the largest differences\ndf_compare['percent_diff']=df_compare.percent_converted-df_compare.percent_not_converted\ndf_compare.sort_values(by='percent_diff',inplace=True,ascending=False)\ndf_compare","bc0942ec":"#visualise the outcome of ratio differences btw converted and not-converted\nplt.figure(figsize=(15,8))\ndf=df_compare[['feature','percent_diff']].sort_values(by='percent_diff')\nsns.barplot(x=df.percent_diff,y=df.feature)","37915f5a":"#prepare df and view coff of the model\ndf = pd.DataFrame(res.params).reset_index()\ndf.columns =['Feature','Coeff']\ndf.sort_values(by='Coeff',ascending=False,inplace=True)\ndf","a0aa1e7b":"#view the frequency encoded data set\ndf_lead_score_FequEncoding_cleaned.head()","c277c0dc":"# create X , y data frames\nX_freq = df_lead_score_FequEncoding_cleaned.drop(['Converted'], axis=1)\ny_freq=df_lead_score_FequEncoding_cleaned['Converted']","e5ecfed9":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X_freq, y_freq, train_size=0.7, test_size=0.3, random_state=100)","55278118":"#1. create a list of numeric variables\n#features_for_scaling=['Total_Time_Spent_on_Website','Page_Views_Per_Visit','TotalVisits']\nfeatures_for_scaling=list(X_train.columns)\n\n#2. Apply scalling\nX_train,X_train_scaler=FeatureScalingdf(X_train,features_for_scaling,'std')\nprint(X_train.head())\n\n#3. lets scale test data set as well \nX_test[features_for_scaling]=X_train_scaler.transform(X_test[features_for_scaling])\nprint(X_test.head())\n\n#4.Create logistic regression object\nlogreg = LogisticRegression()\n\n#5. run RFE\nrfe_freq = RFE(logreg, 19)             # running RFE with 30 variables as output\nrfe_freq = rfe_freq.fit(X_train, y_train)\nprint(rfe_freq.support_)","e6a10e26":"X_train_sm = sm.add_constant(X_train[features_for_scaling])\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","d4accef7":"features_for_scaling.remove('A_free_copy_of_Mastering_The_Interview')\nX_train_sm = sm.add_constant(X_train[features_for_scaling])\nlogm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm8.fit()\nres.summary()","f4aacfba":"vif = pd.DataFrame()\nvif['Features'] = X_train[features_for_scaling].columns\nvif['VIF'] = [variance_inflation_factor(X_train[features_for_scaling].values, i) for i in range(X_train[features_for_scaling].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5b041ae6":"features_for_scaling.remove('Asymmetrique_Profile_Score')\nX_train_sm = sm.add_constant(X_train[features_for_scaling])\nlogm9 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm9.fit()\nres.summary()","551b9c5a":"vif = pd.DataFrame()\nvif['Features'] = X_train[features_for_scaling].columns\nvif['VIF'] = [variance_inflation_factor(X_train[features_for_scaling].values, i) for i in range(X_train[features_for_scaling].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0d8a8c76":"features_for_scaling.remove('City')\nX_train_sm = sm.add_constant(X_train[features_for_scaling])\nlogm10 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm10.fit()\nres.summary()","71955eda":"vif = pd.DataFrame()\nvif['Features'] = X_train[features_for_scaling].columns\nvif['VIF'] = [variance_inflation_factor(X_train[features_for_scaling].values, i) for i in range(X_train[features_for_scaling].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","387cc262":"# PREDICT VALUES IN TRAINING DATA SET\ny_train_pred_final=predict_probability(X_train_sm,y_train,0.5)","71a2cb4c":"#analysis of metrics based on confusion matrix\nconfusion_metric_analysis(y_train_pred_final,'Converted','Predicted')","fc6d2e71":"# Metrics ROC,\nfpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Conversion_Probability, drop_intermediate = False )\n\n#Draw ROC\ndraw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Probability)\n","d221fcee":"\n# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Probability.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()\n","e031ee0a":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)\n\n","d4e6d205":"\n# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","05cf014d":"#reassing cutoff\ncutoff=0.35\ny_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Probability.map( lambda x: 1 if x > cutoff else 0)\n","55b97306":"\n#analyse the metrics\nconfusion_metric_analysis(y_train_pred_final,'Converted','final_predicted')\n\n\n#analyse the metrics\nprecision_recall_metrics(y_train_pred_final,'Converted','final_predicted')\nmetrics_loss(y_train_pred_final,'Converted','final_predicted','Conversion_Probability')","a9e55334":"#trade off\n#1. get the values\nfrom sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Probability)\n\n#2.visualize\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","88bb416f":"# 1. Prediction for X_Test\nprint('Features shortlisted:',features_for_scaling)\n\n# 2. Prepare X_test\nX_test = X_test[features_for_scaling]\n\n#3. Add constant\nX_test_sm = sm.add_constant(X_test)\n\n#4 Predictions on test data set\ny_test_pred = res.predict(X_test_sm)\nprint('Predictions',y_test_pred[:10])\n\n#5.Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n\n##6.Putting LeadId to index\n#y_test_pd = pd.DataFrame(y_test)\n#y_test['LeadId'] = y_test.index\n#y_test_df=pd.DataFrame(y_test)\n\n#7.Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\n\n#8. Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test, y_pred_1],axis=1)\n\n#9.Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Probability'})\n\n#10. cut off probability\ny_pred_final['final_predicted'] = y_pred_final.Converted_Probability.map(lambda x: 1 if x > cutoff else 0)\n#y_pred_final['Converted'] = y_pred_final['Converted'].astype(int)\n\n#11. Sensitivity, Specificity Metrics\nconfusion_metric_analysis(y_pred_final,'Converted','final_predicted')\n\n#12. Precision , Recall Metrics\nprecision_recall_metrics(y_pred_final,'Converted','final_predicted')\n\n#13.Loss metrics,\nmetrics_loss(y_pred_final,'Converted','final_predicted','Converted_Probability')\n\n#14. Lead_Score Assignment\ny_pred_final['Lead_Score']=round(y_pred_final.Converted_Probability*100,0)\n\n#15. View Lead_score distribution ( top 50)\nplt.figure(figsize=(10,10))\nsns.countplot(y = 'Lead_Score',data = y_pred_final,order = y_pred_final['Lead_Score'].value_counts().index[:50],hue='Converted')\nplt.title('Count of Leads per Lead Score')\nplt.show()\n","b8660610":"#plot correlation between amount features for target=1\ncorr_features=['Converted']\ncorr_features.extend(features_for_scaling)\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(40,15))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(df_lead_score_FequEncoding_cleaned[corr_features].corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","8e070e1f":"#view the correlation of features with Converted Leads\ndf_lead_score_FequEncoding_cleaned[corr_features].corr()['Converted'].sort_values(ascending=False)[1:]","caef6154":"#prepare df and view coff of the model\ndf = pd.DataFrame(res.params).reset_index()\ndf.columns =['Feature','Coeff']\ndf.sort_values(by='Coeff',ascending=False,inplace=True)\ndf","e277a396":"#### Prediction and Lead Score assignment","8c161cca":"#### Checking VIFs","a18597b1":"#### Metrics based on Confusion Matrix","28a3925e":"### Feature Selection Using RFE\n","00f0c95d":"#### Redo Metrics based on Confusion Matrix, Precision Recall","725e7379":"#### Metrics based on Confusion Matrix","4d9e36dd":"### Test-Train Split","7f8b7c98":"### Numerical Features - Data Preparation","dedd3315":"#### Lets manage all individual changes that we require based on the feature keys\n","23b76507":"#### Model 6.3 Outcome\n1. Model 6.3 has better p-values and VIFs<5","e2383b2c":"# Data Preparation","dfa28e38":"#### View and Handle Missing Data in Numerical Features","6ac55a85":"### View the Co-efficient of the Selected Model","434925db":"#### Precision and recall tradeoff\n","2417e66d":"## Read Data Set","3a8eb8b4":"## Model based on Frequency Encoded Data set \n","2b85231e":"#### Model 2 Outcome\n1. Feature 'Tags_number not provided' has high P value, lets drop it","20e2cbce":"#### Feature : Last Activity analysis\n1. Phone conversation and SMS sent are leading to more conversions\n2. Email bounced ( wrong emails may be) and Olark Chat conversation and Converted to Lead are not leading to enough conversion","c1564eb7":"### Analysis of Selected Features","b1a9f23b":"### Feature Selection Using RFE","e9b64dfe":"## Examine Data Set properties","fcbc48be":"#### Drop less frequently present features(0.1% only)\n1. Analyse the frequency of categorical columns, and drop the very less frequently used features (0.1% only)\n1. i.e., 14\/9240, which is the maximum frequency that we are dropping from the below list","771f43ff":"#### Redo Prediction based on Selected Cut off","a38573d6":"## Model based on Dummy Encoded Data set \n","194c2033":"#### Model 1 Outcome\n1. Feature 'Tags_Lateral student' has high P value, lets drop it","7f762d17":"### Dummy Encoding","f2cff231":"An education company named X Education sells online courses to industry professionals. The typical lead conversion rate at X education is around 30%. \n1. Improve Target Lead Conversion using past data of conversion\n1. Implement Logistic Regression model and assigned Predicted Lead Score (0..100)\n1. Find Hot Leads(Higher Score) and the features influencing Hot Leads","b844963a":"#### Top Positively influencing Features","ecc1c224":"# Business Problem\n","c78555e5":"#### Redo Prediction based on Selected Cut off","6205bf9d":"#### Top Negatively influencing Features","5bf75793":"1. Remove 'Asymmetrique_Profile_Score' due to VIF>5","d40838b5":"### Lead_source analysis\n1. Welingak Website has most number of conversion followed by reference category\n2. Bing and then followed by Facebook  \/ referal sites\/Olark Chat are not leading to enough conversion","ef90e223":"#### Numerical Feature Analysis\n1. Converted leads have spent considerable amount on time on the website, as their median is around 750, when compared to not converted leads having a median of 250\n2. Total Visits, Time spent and Page views per view have significant outliers. Lets treat them later if required.","d4f2425c":"#### Model 3 Outcome\n1. Feature 'Tags_wrong number given' has high P value, lets drop it","dc22e241":"#### Replace Select wtih np.nan \n1. Select means that user hasn't given any value for a specific question \/ information parameter, so we can consider it as not keyed in value(nan)","486b2217":"#### Model 5.1 Outcome\n1. P-values of Tags_invalid number","5ba0961c":"#### ROC and AUC Metrics and Cut off selection","578602e6":"#### Analysis of Metrics","e53a471c":"## Model Evaluation","a184aefb":"#### Redo Metrics based on Confusion Matrix","263843a9":"### Feature Scaling and Initial RFE","8e53d19b":"#### Precision and recall tradeoff","21a7d0ff":"#### Correlation Analysis","02c92aff":"### Rename Columns","5a8b90ce":"### Model Evaluation","483c235c":"#### Imbalance Score","c7eb7001":"### Test-Train Split","3fe5701f":"### Making predictions on the test set and Evaluation","80723196":"#### Metrics based on Precision and Recall","2c26d732":"## Encoding","a62726c0":"## Making predictions on the test set and Evaluation","edaed179":"# Solution Steps\n1. Read Data set\n1. Examine Data set Properties\n1. Inspecting the Dataframe\n1. Data Preparation\n    1. Rename Columns\n\t1. CATEGORICAL Features\n\t\t1. Fixing Features(Columns) Data Types\n\t\t1. Replace Select wtih np.nan.\n\t\t1. Drop less frequently present features(0.1% only)\n\t\t1. Lets manage all individual changes that we require based on the feature keys\n\t\t1. Feature : Last Activity analysis\n\t\t1. Feature: Specialization Analysis\n\t\t1. Feature: How_did_you_hear_about_X_Education analysis\n\t\t1. Drop Features with >70% Null\n\t1. Numerical Features - Data Preparation\n\t\t1. Numerical Feature Analysis\n\t\t1. Imbalance Score\n\t\t1. View and Handle Missing Data in Numerical Features\n1. Encoding\n    1. Dummy Encoding\n    1. Frequency Encoding\n1. Model based on Dummy Encoded Data set \n    1. Test-Train Split\n    1. Feature Scaling and Initial RFE\n    1. Feature Selection Using RFE\n        1. Model 1 Outcome\n\t\t1. Model 2 Outcome\n\t\t1. Model 3 Outcome\n\t\t1. Model 4 Outcome\n\t\t1. Model 5 Outcome\n\t\t1. Checking VIFs\n\t\t1. Model 6 Outcome\n    1. Model Evaluation\n\t\t1. Prediction and Lead Score assignment\n\t\t1. Metrics based on Confusion Matrix\n\t\t1. ROC and AUC Metrics and Cut off selection\n\t\t1. Redo Prediction based on Selected Cut off\n\t\t1. Redo Metrics based on Confusion Matrix\n\t\t1. Metrics based on Precision and Recall\n\t\t1. Precision and recall tradeoff\n\t\t1. Analysis of Metrics\n    1. Making predictions on the test set and Evaluation\n    1. Analysis of Metrics\n    1. Analysis of Selected Features\n        1. Correlation Analysis\n        1. Numeric Features\n        1. Categorical Dummy Features\n        1. View the Co-efficient of the Selected Model\n        1. Top Positively influencing Features\n        1. Top Negatively influencing Features\n1. Model based on Frequency Encoded Data set \n    1. Test-Train Split\n    1. Feature Scaling and Initial RFE\n\t\t1. Feature Selection Using RFE\n        1. Model 7 Outcome\n\t\t1. Model 8 Outcome\n\t\t1. Checking VIFs\n\t\t1. Model 9 Outcome\n\t\t1. Model 10 Outcome\n    1. Model Evaluation\n\t\t1. Prediction and Lead Score assignment\n\t\t1. Metrics based on Confusion Matrix\n\t\t1. ROC and AUC Metrics and Cut off selection\n\t\t1. Redo Prediction based on Selected Cut off\n\t\t1. Redo Metrics based on Confusion Matrix, Precision Recall\n\t\t1. Precision and recall tradeoff\n\t\t1. Analysis of Metrics\n    1. Making predictions on the test set and Evaluation\n    1. Analysis of Metrics\n    1. Analysis of Selected Features\n        1. Correlation Analysis\n        1. View the Co-efficient of the Selected Model\n        1. Top Positively influencing Features\n        1. Top Negatively influencing Features","32eae24f":"### Categorical Dummy Features","2b2240ed":"#### Model 4 Outcome\n1. Feature 'Tags_Interested  in full time MBA' has high P value, lets drop it","cb8eb683":"#### Feature: How_did_you_hear_about_X_Education analysis\n1. SMS sent are leading to more conversions if it is a follow up action as per feature 'Last Activity', but SMS as promotion is not converting well\n2. Email as promotion leads to higher conversion of leads\n","49435ae9":"### Model 7 Outcome\n1. Lets remove feature 'A_free_copy_of_Mastering_The_Interview')' which has got a  high p value","df1fab7c":"#### Checking VIFs","6d0b3e97":"#### Drop Features with >70% Null","16d3de9d":"#### Feature: Specialization Analysis\n1. 'Other' is the most commonly used. We recommend to capture the industry if user is entering other to specifically know the type of industry. Since this category of leads are not converting well, it is essential to understand the group with additional data in future\n2. Health care and Banking\/investment\/insurance leads are well converted","fa8c3473":"### Feature Scaling and Initial RFE","5ce15176":"### Analysis of Metrics","d78d01b4":"### Frequency Encoding","866525d3":"#### Numeric Features","bb205688":"#### Model 10 Outcome\n1. Looks good interms of p-value and VIFs","aa346333":"#### ROC and AUC Metrics and Cut off selection","7aa51598":"#### Analysis of Metrics","d30af99f":"#### Prediction and Lead Score assignment","cb1bc465":"### Model 9 Outcome\n1. Lets remove 'City' which has high p-value","9fcba74d":"#### Model 8 Outcome\n1. City is having higher p-values, but lets check VIF before removing City","49beb43f":"### CATEGORICAL Features\n#### Fixing Features(Columns) Data Types"}}