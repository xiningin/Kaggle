{"cell_type":{"0bb1a86b":"code","ef32dc1a":"code","fd8ed18c":"code","ac9a28d9":"code","e3cfc84f":"code","2259f193":"code","94971eed":"code","9f145145":"code","498aea66":"code","05d3d9fd":"code","88e4bb32":"code","fb191b82":"code","4cc71780":"code","8a0087f6":"code","17234359":"code","07b2a61d":"code","e7e18b16":"code","b2ffa954":"code","4110fa2f":"code","7f1580ad":"code","f076e30f":"code","622413f4":"code","33378e62":"code","8e667c29":"code","0d5d258c":"code","f1ed61bf":"code","0b1a090e":"code","6deaad54":"code","ea94f81f":"code","95ed30f7":"code","082f9a8d":"code","70f9f961":"code","7d34847a":"code","86d9af8b":"code","bd096001":"code","7362e52f":"code","96b67d09":"code","cdecd830":"code","6937a8ef":"code","81a0f211":"code","7c96927a":"code","8d1c520b":"code","309388a2":"code","49636b52":"code","c23ce503":"code","c1777489":"code","651a3182":"code","bb110b1f":"code","601431a2":"code","3439336e":"code","9be764c7":"code","f5939c36":"code","5ed12661":"code","42f39b2f":"code","9a6c7417":"code","43167d28":"code","ff5ab523":"code","38bc390b":"code","7c568eda":"code","4e65535c":"code","7026dc6f":"code","0744a899":"code","73a8ba49":"code","3e99fe1a":"code","0d5a757a":"code","be3c4260":"code","27735238":"code","f5578575":"code","3bda4ce0":"code","288ae7b1":"code","2ef4fe47":"code","6d55b7e7":"code","17b69104":"code","e3ebcd5a":"code","059d3d57":"code","94dce0b3":"code","8f539df7":"code","59531124":"code","f2c5e092":"code","85559f1c":"code","745186ff":"code","7c43772a":"code","1969f433":"code","be1efae3":"code","4b6876e9":"code","45f748a0":"code","67623f54":"code","0b25a356":"code","076f8af2":"code","1810cbe8":"code","a3299de2":"code","c5ef21c9":"code","320abd3d":"code","9776e1e0":"code","0955eef7":"code","655a8bee":"code","ffde8373":"code","0eb50100":"code","679ad229":"code","c6cc2095":"code","1a31ba14":"code","a671f4e6":"code","ec854ee6":"code","3022081a":"code","17aaa5f4":"code","6e6f263f":"code","31981035":"code","385f012d":"code","8700e7dd":"code","a205dbc7":"code","97adaeea":"code","ce064dc7":"code","2cf586ed":"code","11a0a3af":"code","d2d05732":"code","1c42fb2f":"code","12a043db":"code","5bbb0510":"code","d811f530":"code","02521025":"code","e2bd9865":"code","134ff79a":"markdown","56168c50":"markdown","45ce4a2c":"markdown","98c1053c":"markdown","256b6e37":"markdown","2550beb9":"markdown","a71b6859":"markdown","9350102e":"markdown","4e9b7aa1":"markdown","823c2b12":"markdown","b778e452":"markdown","50fc2bf7":"markdown","dd839a30":"markdown","f9068551":"markdown","1b765306":"markdown","87b8970b":"markdown","e9e96e7a":"markdown","7bc407df":"markdown","658da871":"markdown","029ec7f2":"markdown","fa68d670":"markdown","abff0e72":"markdown","5badb71c":"markdown","759f0d3d":"markdown","c61e6091":"markdown","f7596ecd":"markdown","a31749c3":"markdown","2af70f9e":"markdown","363be336":"markdown","b8efaf1c":"markdown","ced57f65":"markdown","79cf2e87":"markdown","10bbd058":"markdown","6a55b52b":"markdown","08c8e2ba":"markdown","ec0a2794":"markdown","a00e9ad9":"markdown","03772e9d":"markdown","a7c83e4b":"markdown","e3f3bf55":"markdown","1688d12d":"markdown","6105cb45":"markdown","54042957":"markdown","d9054dc3":"markdown","94849780":"markdown","9bd1e3f4":"markdown","f247a8dc":"markdown","af0bb839":"markdown","012840ae":"markdown","58bd8866":"markdown","28d5ba48":"markdown","625e90e7":"markdown"},"source":{"0bb1a86b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef32dc1a":"import fuzzywuzzy\nfrom fuzzywuzzy import process\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom pylab import rcParams\nimport nltk\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC","fd8ed18c":"df=pd.read_csv('\/kaggle\/input\/gufhtugu-publications-dataset-challenge\/GP Orders - 5.csv')\ndf.dropna(axis=0,inplace=True)\n","ac9a28d9":"# df.isnull().sum()","e3cfc84f":"df = df.rename(columns={'Order Number': 'Order_Number',\"Order Status\":\"Order_Status\", \"Book Name\":\"Book_Name\",\"Order Date & Time\":\"Date_Time\",\"Payment Method\":\"Payment_Method\",\"Total items\":\"Total_Items\",\"Total weight (grams)\":\"Weight\"})\nprint(\"After rename, columns names are: \", \"\\n\", \"\\n\" , df.columns)","2259f193":"df['Payment_Method'] = df['Payment_Method'].map({'Cash on Delivery (COD)': 'cash_on_delivery','Cash on delivery': 'cash_on_delivery','EasyPaisa':'easypaisa','BankTransfer':'banktransfer'}) ","94971eed":"df.Payment_Method = df.Payment_Method.fillna('not_mentioned')\n","9f145145":"df.isnull().sum()","498aea66":"# djdd","05d3d9fd":"df=df[df.Total_Items !=80022]","88e4bb32":"print(df.info())","fb191b82":"df['Date_Time'] = pd.to_datetime(df['Date_Time'])\nprint(df.info())","4cc71780":"df[\"Month_Number\"] = df[\"Date_Time\"].dt.month\ndf[\"Week_Day\"] = df[\"Date_Time\"].dt.dayofweek\ndf['year'] = df[\"Date_Time\"].dt.year\ndf['day'] = df[\"Date_Time\"].dt.day\ndf['dayofyear'] = df[\"Date_Time\"].dt.dayofyear\ndf['year'] = df[\"Date_Time\"].dt.year\ndf['hour'] = df[\"Date_Time\"].dt.hour\ndf['quarter'] = df[\"Date_Time\"].dt.quarter\ndf['dayofyear'] = df[\"Date_Time\"].dt.dayofyear\ndf['weekend'] = df['Date_Time'].dt.dayofweek >=5\ndf['Month_name'] = pd.to_datetime(df['Date_Time'], format='%m').dt.month_name().str.slice(stop=3)\n","8a0087f6":"df.tail(5)","17234359":"df['Book_Name'].nunique()","07b2a61d":"df[\"Book_Name\"] = df[\"Book_Name\"].str.replace(\"(\" , \"\").str.replace(\")\" , \"\")\ndf[\"Book_Name\"] = df[\"Book_Name\"].str.replace(\"[\" , \"\").str.replace(\"]\" , \"\")","e7e18b16":"df['Book_Name'].nunique()","b2ffa954":"# df['Book_Name'].unique().tolist()","4110fa2f":"df[\"Book_Name\"] = df[\"Book_Name\"].str.split(\"\/\").str[0]","7f1580ad":"# df['Book_Name'].unique()","f076e30f":"df['Book_Name'] = df['Book_Name'].replace('\u0627\u0646\u0679\u0631\u0646\u06cc\u0679 \u0633\u06d2 \u067e\u06cc\u0633\u06c1 \u06a9\u0645\u0627\u0626\u06cc\u06ba','Internet Sy Paisa Kamayen')\ndf['Book_Name'] = df['Book_Name'].replace('\/\u0627\u0646\u0679\u0631\u0646\u06cc\u0679 \u0633\u06d2 \u067e\u06cc\u0633\u06c1 \u06a9\u0645\u0627\u0626\u06cc\u06ba','Internet Sy Paisa Kamayen')\ndf['Book_Name'] = df['Book_Name'].replace('\u0627\u0646\u0679\u0631\u0646\u06cc\u0679 \u0633\u06d2 \u067e\u06cc\u0633\u06c1 \u06a9\u0645\u0627\u0626\u06cc\u06ba\u061f- \u0645\u0633\u062a\u062d\u0642\u06cc\u0646 \u0632\u06a9\u0648\u0627\u0629','Internet Sy Paisa Kamayen')\ndf['Book_Name'] = df['Book_Name'].replace('\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633','Data Science')\ndf['Book_Name'] = df['Book_Name'].replace('\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633 \u06d4 \u0627\u06cc\u06a9 \u062a\u0639\u0627\u0631\u0641','Data Science')\ndf['Book_Name'] = df['Book_Name'].replace('\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af','Machine Learning')\ndf['Book_Name'] = df['Book_Name'].replace('(C++) ++\u0633\u06cc','c++')\ndf['Book_Name'] = df['Book_Name'].replace('Python Programming- Release Date: August 14, 2020','Python Programming')\ndf['Book_Name'] = df['Book_Name'].replace('\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633 \u06d4 \u0627\u06cc\u06a9 \u062a\u0639\u0627\u0631\u0641','Data Science')\ndf['Book_Name'] = df['Book_Name'].replace('\u0628\u0644\u0627\u06a9 \u0686\u06cc\u0646 \u0627\u0648\u0631 \u06a9\u0631\u067e\u0679\u0648 \u06a9\u0631\u0646\u0633\u06cc','Blockchain, Cryptocurrency And Bitcoin')\ndf['Book_Name'] = df['Book_Name'].replace('\u0622\u0631 \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641','R ka Taaruf')\ndf['Book_Name'] = df['Book_Name'].replace('R ka Taaruf  \u0622\u0631 \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641','R ka Taaruf')\ndf['Book_Name'] = df['Book_Name'].replace('Bit Coin Block Chain aur Crypto Currency \u0628\u0679 \u06a9\u0648\u0627\u0626\u0646\u060c \u0628\u0644\u0627\u06a9 \u0686\u06cc\u0646 \u0627\u0648\u0631 \u06a9\u0631\u067e\u0679\u0648 \u06a9\u0631\u0646\u0633\u06cc' ,'Blockchain, Cryptocurrency And Bitcoin')\ndf['Book_Name'] = df['Book_Name'].replace('Earn from Internet' ,'Internet Sy Paisa Kamayen')\ndf['Book_Name'] = df['Book_Name'].replace('Bit Coin Block Chain aur Crypto Currency','Blockchain, Cryptocurrency And Bitcoin')\ndf['Book_Name'] = df['Book_Name'].replace('Masnoi Zahanat','Artificial Intelligence')\ndf['Book_Name'] = df['Book_Name'].replace('Ikeesvin Saddi k Ikees Sabaq','21 Lessons for the 21st century')\ndf['Book_Name'] = df['Book_Name'].replace('\u0627\u0631\u0637\u063a\u0631\u0644 \u063a\u0627\u0632\u06cc','Ertugrul Ghazi')\ndf['Book_Name'] = df['Book_Name'].replace(['animal farm-urdu','animal form','animal farm - english'],'animal farm')\ndf['Book_Name'] = df['Book_Name'].replace(['Molo Masali - \u0645\u0648\u0644\u0648 \u0645\u0635\u0644\u06cc'],'molo masali')\ndf['Book_Name'] = df['Book_Name'].replace(['alchemist | graphic edition'],'alchemist')","622413f4":"books_to_convert=['C++','Linux - An Introduction']","33378e62":"def clean_book(book):\n    for i in books_to_convert:\n        if i in str(book):\n            return i\n    return book\ndf[\"Book_Name\"] = df[\"Book_Name\"].apply(clean_book)","8e667c29":"df=df.applymap(lambda s: s.lower() if type(s)==str else s)","0d5d258c":"df['Book_Name'].nunique()","f1ed61bf":"df['City'].nunique()","0b1a090e":"pak_cities = ['islamabad', 'ahmed nager chatha', 'ahmadpur east', 'ali khan abad', 'alipur', 'arifwala', 'attock', 'bhera',\n              'bhalwal', 'bahawalnagar','bahawalpur', 'bhakkar', 'burewala', 'chillianwala', 'chakwal', 'chichawatni',\n              'chiniot', 'chishtian',\n              'daska', 'darya khan', 'dera ghazi khan', 'dhaular', 'dina', 'dinga', 'dipalpur', 'faisalabad', 'ferozewala',\n              'fateh jhang','ghakhar mandi', 'gojra', 'gujranwala', 'gujrat', 'gujar khan', 'hafizabad', 'haroonabad', 'hasilpur',\n              'haveli lakha', 'jatoi',\n              'jalalpur', 'jattan', 'jampur', 'jaranwala', 'jhang', 'jhelum', 'kalabagh', 'karor lal esan', 'kasur', 'kamalia', 'kamoke',\n              'khanewal',\n              'khanpur', 'kharian', 'khushab', 'kot addu', 'jauharabad', 'lahore', 'lalamusa', 'layyah', 'liaquat pur',\n              'lodhran', 'malakwal', 'mamoori', 'mailsi', 'mandi bahauddin', 'mian channu', 'mianwali', 'multan', 'murree', \n              'muridke', 'mianwali bangla', 'muzaffargarh', 'narowal', 'nankana sahib', 'okara', 'renala khurd', 'pakpattan', \n              'pattoki', 'pir mahal', 'qaimpur', 'qila didar singh', 'rabwah', 'raiwind', 'rajanpur', 'rahim yar khan',\n              'rawalpindi',\n              'sadiqabad', 'safdarabad', 'sahiwal', 'sangla hill', 'sarai alamgir', 'sargodha', 'shakargarh', 'sheikhupura',\n              'sialkot',\n              'sohawa', 'soianwala', 'siranwali', 'talagang', 'taxila', 'toba tek singh', 'vehari', 'wah cantonment', \n              'wazirabad', \n              'badin', 'bhirkan', 'rajo khanani', 'chak', 'dadu', 'digri', 'diplo', 'dokri', 'ghotki', 'haala', 'hyderabad',\n              'islamkot', 'jacobabad', 'jamshoro', 'jungshahi', 'kandhkot', 'kandiaro', 'karachi', 'kashmore', 'keti bandar',\n              'khairpur', 'kotri', 'larkana', 'matiari', 'mehar', 'mirpur khas', 'mithani', 'mithi', 'mehrabpur', 'moro',\n              'nagarparkar', 'naudero', 'naushahro feroze', 'naushara', 'nawabshah', 'nazimabad', 'qambar', 'qasimabad', \n              'ranipur', 'ratodero', 'rohri', 'sakrand', 'sanghar', 'shahbandar', 'shahdadkot', 'shahdadpur',\n              'shahpur chakar', 'shikarpaur', 'sukkur', 'tangwani', 'tando adam khan', 'tando allahyar',\n              'tando muhammad khan', 'thatta', 'umerkot', 'warah', 'abbottabad', 'adezai', 'alpuri', 'akora khattak',\n              'ayubia', 'banda daud shah', 'bannu', 'batkhela', 'battagram', 'birote', 'chakdara', 'charsadda', 'chitral',\n              'daggar', 'dargai', 'darya khan', 'dera ismail khan', 'doaba', 'dir', 'drosh', 'hangu', 'haripur', 'karak',\n              'kohat', 'kulachi', 'lakki marwat', 'latamber', 'madyan', 'mansehra', 'mardan', 'mastuj', 'mingora', 'nowshera',\n              'paharpur', 'pabbi', 'peshawar', 'saidu sharif', 'shorkot', 'shewa adda', 'swabi', 'swat', 'tangi', 'tank',\n              'thall', 'timergara', 'tordher', 'awaran', 'barkhan', 'chagai', 'dera bugti', 'gwadar', 'harnai', 'jafarabad',\n              'jhal magsi', 'kacchi', 'kalat', 'kech', 'kharan', 'khuzdar', 'killa abdullah', 'killa saifullah', 'kohlu',\n              'lasbela', 'lehri', 'loralai', 'mastung', 'musakhel', 'nasirabad', 'nushki', 'panjgur', 'pishin valley', \n              'quetta', 'sherani', 'sibi', 'sohbatpur', 'washuk', 'zhob', 'ziarat']","6deaad54":"def clean_city(city):\n    for i in pak_cities:\n        if i in str(city):\n            return i\n    return city\ndf[\"City\"] = df[\"City\"].apply(clean_city)","ea94f81f":"df[\"City\"] = df[\"City\"].apply(clean_city)","95ed30f7":"from textblob import TextBlob\nfrom time import sleep","082f9a8d":"urdu_city = df['City'][~(df['City'].str.contains(\"[a-zA-Z]\"))]\nurdu_city.unique()","70f9f961":"# to_en = {}\n# for x in urdu_city:\n#     # sleep to not exceed the limit of requests\n#     sleep(0.5)\n#     try:\n#         tr = TextBlob(x).translate().string\n#         to_en[x] = tr\n# #         print(x,\" - \", tr)\n#     except:\n#         pass","7d34847a":"# len(to_en)","86d9af8b":"# to_en","bd096001":"changename={\n'\u0628\u06be\u0645\u0628\u0631 \u0622\u0632\u0627\u062f\u06a9\u0634\u0645\u06cc\u0631': 'Bhimber',\n'\u062a\u062d\u0635\u06cc\u0644 \u0648\u0632\u06cc\u0631\u0622\u0628\u0627\u062f \u0636\u0644\u0639 \u06af\u0648\u062c\u0631\u0627\u0646\u0648\u0644\u06c1': 'Wazirabad',\n'\u0679\u0648\u0628\u06c1 \u0679\u06cc\u06a9 \u0633\u0646\u06af\u06be': 'Toba Tek Singh',\n'\u06af\u0648\u062c\u0631\u0627\u0646\u0648\u0627\u0644\u06c1': 'Gujranwala',\n'\u0627\u0633\u0644\u0627\u0645 \u0622\u0628\u0627\u062f': 'Islam Abad',\n'\u0636\u0644\u0639 \u0631\u062d\u06cc\u0645 \u06cc\u0627\u0631 \u062e\u0627\u0646 \u062a\u062d\u0635\u06cc\u0644 \u0644\u06cc\u0627\u0642\u062a \u067e\u0648\u0631': 'Liaquatpur',\n'\u0688\u06cc\u0631\u06c1 \u063a\u0627\u0632\u06cc \u062e\u0627\u0646': 'Dera Ghazi Khan',\n'\u06a9\u0644\u0631 \u0633\u06cc\u062f\u0627\u06ba \u0636\u0644\u0639 \u0631\u0627\u0648\u0644\u067e\u0646\u0688\u06cc': 'Rawalpindi',\n'\u0645\u0646\u0688\u06cc \u0628\u06c1\u0627\u0624 \u0627\u0644\u062f\u06cc\u0646': 'Mandi Bahauddin',\n'\u062f\u06cc\u0631 \u067e\u0627\u06cc\u0646': \"dir\",\n'\u0636\u0644\u0639 \u0627\u0679\u06a9': 'Attock',\n'\u0648\u0633\u0637\u06cc': 'Wasti',\n'\u0642\u0635\u0648\u0631': 'kasur',\n'\u0635\u0627\u062f\u0642 \u0622\u0628\u0627\u062f \u0636\u0644\u0639 \u0631\u062d\u06cc\u0645 \u06cc\u0627\u0631 \u062e\u0627\u0646': 'sadiqabad',\n'\u0628\u0646\u0648\u06ba': 'Bannu',\n'\u0631\u0627\u0648\u0644\u0627\u06a9\u0648\u0679': 'rawalakot',\n'\u062a\u0648\u0646\u0633\u06c1 \u0634\u0631\u06cc\u0641 \u0636\u0644\u0639 \u0688\u06cc\u0631\u06c1 \u063a\u0627\u0632\u06cc\u062e\u0627\u0646': 'Taunsa Sharif',\n'\u0645\u0644\u062a\u0627\u0646': 'Multan',\n'\u06a9\u0631\u0627\u0686\u06cc \u0648\u06cc\u0633\u0679': 'Karachi',\n'\u0635\u0648\u0627\u0628\u06cc': 'Swabi',\n'\u0645\u06cc\u0631\u067e\u0648\u0631\u0622\u0632\u0627\u062f\u06a9\u0634\u0645\u06cc\u0631': 'Mirpur',\n'\u062a\u062d\u0635\u06cc\u0644 \u06a9\u0631\u0648\u0691 \u0636\u0644\u0639 \u0644\u06cc\u06c1': 'Layyah',\n'\u0645\u0638\u0641\u0631\u06af\u0691\u06be': 'Muzaffargarh',\n'\u0639\u0644\u06cc \u067e\u0648\u0631 \u0686\u0679\u06be\u06c1 \u0636\u0644\u0639 \u06af\u0648\u062c\u0631\u0627\u0646\u0648\u0627\u0644\u06c1': 'Gujranwala',\n'\u0645\u0644\u06cc\u0631 \u06a9\u06cc\u0646\u0679': 'Karachi',\n'\u062c\u06be\u0646\u06af \u0633\u0679\u06cc': 'Jhang',\n'\u0644\u0627\u06be\u0648\u0631': 'Lahore',\n'\u0627\u0633\u0644\u0627\u0645 \u0627\u0628\u0627\u062f': 'IslamAbad',\n'\u063a\u0644\u0627\u0645 \u0645\u062d\u0645\u062f \u0622\u0628\u0627\u062f \u0646\u0645\u0628\u0631 1': 'Faisalabad',\n'\u0645\u0646\u0688\u06cc \u06cc\u0632\u0645\u0627\u0646': 'Mandi Yazman',\n'\u0631\u0627\u0648\u0644\u067e\u0646\u0688\u06cc': 'Rawalpindi',\n'\u0641\u062a\u062d \u062c\u0646\u06af': 'Fateh jang',\n'\u062d\u0627\u0635\u0644\u067e\u0648\u0631': 'Produced',\n'\u0688\u06be\u0688\u06cc\u0627\u0644 \u062a\u062d\u0635\u06cc\u0644 \u0648 \u0636\u0644\u0639 \u0686\u06a9\u0648\u0627\u0644': 'Chakwal',\n'\u0628\u0644\u06cc \u0679\u0646\u06af \u06a9\u0648\u06be\u0627\u0679': 'Kohat',\n'\u0644\u0627\u06c1\u0648\u0631': 'Lahore',\n'\u0688\u06cc\u0631\u06c1 \u0627\u0633\u0645\u0627\u0639\u06cc\u0644 \u062e\u0627\u0646': 'Dera Ismail Khan',\n'\u06a9\u0631\u0627\u0686\u06cc': 'Karachi',\n'\u0639\u06cc\u0633\u06cc\u0670 \u062e\u06cc\u0644': 'Isa Khel',\n'\u0642\u0644\u0639\u06c1 \u062f\u06cc\u062f\u0627\u0631 \u0633\u0646\u06af\u06be': 'gujranwala',\n'\u062e\u0627\u0646 \u067e\u0648\u0631': 'Khanpur',\n'\u0642\u0644\u0639\u06c1 \u0633\u06cc\u0641 \u0627\u0644\u0644\u06c1': 'Fort Saifullah',\n'\u0644\u06cc\u0627\u0642\u062a \u067e\u0648\u0631 \u0636\u0644\u0639 \u0631\u062d\u06cc\u0645 \u06cc\u0627\u0631\u062e\u0627\u0646': 'Liaquatpur',\n'\u0636\u0644\u0639 \u0645\u0679\u06cc\u0627\u0631\u06cc \u062a\u062d\u0635\u06cc\u0644 \u0633\u0639\u06cc\u062f\u0622\u0628\u0627\u062f': 'Matiari',\n'\u0633\u0631\u06af\u0648\u062f\u06c1\u0627': 'Sargodha',\n'\u062c\u06cc\u06a9\u0628 \u0622\u0628\u0627\u062f': 'Jacobabad',\n'\u067e\u0634\u062a\u0648\u0646 \u0622\u0628\u0627\u062f \u06a9\u0648\u0626\u0679\u06c1': 'Quetta',\n'\u062d\u0627\u0641\u0638 \u0648\u0627\u0644\u0627 \u0688\u0633\u0679\u0631\u06a9\u0679 \u0645\u06cc\u0627\u0646\u0648\u0627\u0644\u06cc': 'Mianwali',\n'\u0644\u0627\u0691\u06a9\u0627\u0646\u06c1': 'Larkana',\n'\u0627\u0633\u0644\u0627\u0645 \u0628\u0627\u062f': 'Islamabad',\n'\u0628\u06be\u0645\u0628\u0631': 'Bhimber',\n'\u0628\u0635\u06cc\u0631\u06c1 \u062a\u062d\u0635\u06cc\u0644 \u0648 \u0636\u0644\u0639 \u0645\u0638\u0641\u0631\u06af\u0691\u06be': 'Muzaffargarh ',\n'\u0648\u0632\u06cc\u0631\u0622\u0628\u0627\u062f': 'Wazirabad',\n'\u0686\u0646\u06cc\u0648\u0679': 'Chiniot',\n'\u06be\u0631\u06cc \u067e\u0648\u0631 \u06be\u0632\u0627\u0631\u06c1': 'Haripur',\n'\u0633\u0631\u06af\u0648\u062f\u06be\u0627': 'Sargodha',\n'\u0645\u0638\u0641\u0631\u0622\u0628\u0627\u062f': 'Muzaffarabad',\n'\u062c\u06c1\u0644\u0645': 'Jhelum',\n'\u0648\u06c1\u0627\u0691\u06cc': 'Vehari',\n'\u0645\u06cc\u0646\u06af\u0648\u0631\u06c1 \u0633\u0648\u0627\u062a': 'Swat',\n'\u0636\u0644\u0639 \u0688\u06cc\u0631\u06c1 \u0627\u0633\u0645\u0627\u0639\u06cc\u0644 \u062e\u0627\u0646 \u062a\u062d\u0635\u06cc\u0644 \u067e\u06c1\u0627\u0691\u067e\u0648\u0631\u060c \u0688\u0627\u06a9\u062e\u0627\u0646\u06c1 \u0631\u0646\u06af\u067e\u0648\u0631\u060c\u0645\u0679\u06be\u0627\u067e\u0648\u0631': 'Dera Ismail Khan',\n'\u06a9\u0648\u06c1\u06cc\u0679\u06c1': '\u06a9\u0648\u06c1\u06cc\u0679\u0627',\n'\u0645\u0638\u0641\u0631\u06af\u0691\u06c1': 'Muzaffargarh',\n'\u06af\u0648\u062c\u0631\u062e\u0627\u0646': 'Gujjar Khan',\n'\u0686\u062a\u0631\u0627\u0644': 'Chitral',\n'\u0628\u0679\u06af\u0631\u0627\u0645': 'Battagram',\n'\u0633\u06cc\u0627\u0644\u06a9\u0648\u0679': 'Sialkot',\n'\u0646\u0648\u0634\u06c1\u0631\u06c1': 'Nowshera',\n'\u0631\u062d\u06cc\u0645 \u06cc\u0627\u0631 \u062e\u0627\u0646': 'Rahim Yar Khan',\n'\u0628\u06c1\u0627\u0648\u0644 \u067e\u0648\u0631': 'Bahawalpur',\n'\u0644\u0627\u0644 \u0633\u0648\u06c1\u0627\u0646\u0631\u0627 \u060c \u0628\u06c1\u0627\u0648\u0644\u067e\u0648\u0631': 'Bahawalpur',\n'\u0644\u0627\u0644\u0627 \u0645\u0648\u0633\u06cc': 'Lala Musa',\n'\u062e\u062f\u06cc\u062c\u06c1 \u0633\u0679\u0631\u06cc\u0679 \u0633\u06cc\u0627\u0644\u06a9\u0648\u0679': 'Sialkot',\n'\u06a9\u0631\u0627\u0686\u06cc\u060c \u06a9\u0648\u0631\u0646\u06af\u06cc \u0646\u0645\u0628\u0631 1': 'Karachi',\n'\u0628\u0648\u0646\u06cc\u0631': 'Buner',\n'\u0645\u0646\u0688\u06cc \u0628\u06c1\u0627\u0648\u0627\u0644\u062f\u06cc\u0646': 'Mandi Bahauddin',\n'\u0636\u0644\u0639 \u0645\u0646\u0688\u06cc \u0628\u06c1\u0627\u0648\u0627\u0644\u062f\u06cc\u0646': 'Mandi Bahauddin',\n'\u0645\u0646\u0688\u06cc \u0628\u06c1\u0627\u0624\u0627\u0644\u062f\u06cc\u0646': 'Mandi Bahauddin',\n'\u062e\u0627\u0646\u06cc\u0648\u0627\u0644 \u0627\u0628\u0631\u0627\u06c1\u06cc\u0645 \u0679\u0627\u0624\u0646': 'Khanewal',\n'\u0644\u0627\u0648\u06c1 \u062a\u062d\u0635\u06cc\u0644 \u0644\u0627\u0648\u06c1 \u0636\u0644\u0639 \u0686\u06a9\u0648\u0627\u0644': 'Chakwal',\n'\u062a\u062d\u0635\u06cc\u0644 \u0648 \u0636\u0644\u0639 \u0644\u06cc\u06c1': 'Layyah',\n'\u062f\u06cc\u0631 \u0644\u0648\u06cc\u0631': 'Dir',\n'\u0686\u0648\u06a9 \u0622\u0639\u0638\u0645': 'Chowk Azam',\n'\u0636\u0644\u0639 \u0627\u0679\u06a9 \u062a\u062d\u0635\u06cc\u0644 \u067e\u0646\u0688\u06cc \u06af\u06be\u06cc\u0628': 'Attock',\n'\u0644\u0627\u0644 \u0633\u0648\u06c1\u0627\u0646\u0631\u0627 \u0628\u06c1\u0627\u0648\u0644\u067e\u0648\u0631': 'Bahawalpur',\n'\u062f\u06cc\u067e\u0627\u0644\u067e\u0648\u0631': 'Depalpur',\n'\u062d\u0628 \u0686\u0648\u06a9\u06cc \u0628\u0644\u0648\u0686\u0633\u062a\u0627\u0646': 'hub',\n'\u06a9\u0648\u0626\u0679\u06c1': 'Quetta',\n'\u0686\u0648\u0679\u06cc \u0632\u06cc\u0631\u06cc\u06ba \u062a\u062d\u0635\u06cc\u0644 \u06a9\u0648\u0679 \u0686\u06be\u0679\u06c1 \u0636\u0644\u0639 \u0688\u06cc\u0631\u06c1 \u063a\u0627\u0632\u06cc \u062e\u0627\u0646': 'Dera Ghazi Khan',\n'\u06af\u0627\u0624\u06ba \u0628\u0627\u0646\u0688\u06c1 \u0646\u0628\u06cc\u060c\u0688\u0627\u06a9\u062e\u0627\u0646\u06c1 \u0688\u0627\u06af\u0626\u06cc \u0628\u0627\u0646\u0688\u06c1\u060c\u062a\u062d\u0635\u0644 \u067e\u0628\u06cc \u0636\u0644\u0639 \u0646\u0648\u0634\u06c1\u0631\u06c1': 'Nowshera',\n'\u0633\u0648\u06a9\u0646 \u0648\u0646\u0688 \u062a\u062d\u0635\u06cc\u0644 \u067e\u0633\u0631\u0648\u0631 \u0636\u0644\u0639 \u0633\u06cc\u0627\u0644\u06a9\u0648\u0679': 'Sialkot',\n'\u0698\u0648\u0628': 'Zhob',\n'\u0686\u06a9\u0648\u0627\u0644\/\u0628\u0644\u06a9\u0633\u0631': 'Chakwal',\n'\u0635\u0627\u062f\u0642 \u0622\u0628\u0627\u062f': 'Sadiqabad',\n'\u0642\u0627\u062f\u0631 \u067e\u0648\u0631 \u0631\u0627\u06ba \u0645\u0644\u062a\u0627\u0646': 'Multan',\n'\u062f\u0631\u06af\u0626\u06cc \u0645\u0627\u0644\u0627\u06a9\u0646\u0688': 'Malakand',\n'\u06a9\u0648\u06cc\u0679\u06c1': 'Quetta',\n'\u062a\u062d\u0635\u06cc\u0644 \u0635\u0627\u062f\u0642 \u0622\u0628\u0627\u062f': 'Sadiqabad',\n'\u0634\u06cc\u062e\u0648\u067e\u0648\u0631\u06c1': 'Sheikhupura',\n'\u06c1\u0631\u06cc\u067e\u0648\u0631': 'Haripur',\n'\u0641\u0627\u0631\u0648\u0642 \u0622\u0628\u0627\u062f': 'Farooq Abad',\n'\u0631\u062d\u06cc\u0645 \u06cc\u0627\u0631\u062e\u0627\u0646': 'Rahim Yar Khan',\n'\u06af\u062c\u0631\u0627\u062a': 'Gujarat',\n'\u0631\u0627\u0648\u06cc \u0631\u06cc\u0627\u0646 \u0688\u0627\u06a9 \u062e\u0627\u0646\u06c1 \u062a\u062d\u0635\u06cc\u0644 \u0641\u06cc\u0631\u0648\u0632 \u0648\u0627\u0644\u06c1 \u0636\u0644\u0639 \u0634\u06cc\u062e\u0648\u067e\u0648\u0631\u06c1 \u06d4': 'Sheikhupura',\n'\u0628\u06c1\u0627\u0648\u0644\u0646\u06af\u0631': 'Bahawalnagar',\n'\u062a\u062d\u0635\u06cc\u0644 \u0631\u06cc\u0646\u0627\u0644\u06c1 \u0636\u0644\u0639 \u0627\u0648\u06a9\u0627\u0691\u06c1': 'Okara',\n'\u0644\u0627\u0646\u0688\u06be\u06cc \u0679\u0627\u0624\u0646 \u06a9\u0631\u0627\u0686\u06cc': 'Karachi',\n'\u0688\u0627\u06a9\u062e\u0627\u0646\u06c1 \u0686\u0648\u0679\u06cc \u0632\u06cc\u0631\u06cc\u06ba \u06af\u0648\u0631\u0646\u0645\u0646\u0679 \u067e\u0631\u0627\u0626\u0645\u0631\u06cc \u0633\u06a9\u0648\u0644 \u0633\u0645\u06cc \u0648\u0627\u0644\u0627 \u0645\u0648\u0636\u0639 \u0646\u0648\u0627\u06ba \u062c\u0646\u0648\u0628\u06cc \u062a\u062d\u0635\u06cc\u0644 \u06a9\u0648\u0679 \u0686\u06be\u0679\u06c1 \u0636\u0644\u0639 \u0688\u06cc\u0631\u06c1 \u063a\u0627\u0632\u06cc \u062e\u0627\u0646': 'Dera Ghazi Khan',\n'\u0688\u06be\u0648\u06a9 \u0645\u0635\u0627\u062d\u0628': '\u062f\u06be\u0648\u06a9 \u0645\u0635\u0627\u062d\u0628',\n'\u062c\u06be\u0646\u06af': 'Jhang',\n'\u0686\u0648\u0646\u06cc\u0627\u06ba \u0636\u0644\u0639 \u0642\u0635\u0648\u0631': 'Chunian',\n'\u0633\u06a9\u06be\u0631 \u062f\u06c1\u0644\u06cc \u0645\u0633\u0644\u0645 \u0633\u0648\u0633\u0627\u0626\u0679\u06cc \u0646\u0632\u062f \u0622\u0626 \u0628\u06cc \u0627\u06d2 \u06a9\u0627\u0644\u062c': 'Sukkur',\n'\u0628\u0627\u063a \u0622\u0632\u0627\u062f \u06a9\u0634\u0645\u06cc\u0631': 'bagh',\n'\u0686\u06a9\u062f\u0631\u06c1 \u062a\u062d\u0635\u06cc\u0644 \u0627\u062f\u0646\u0632\u0627\u0626\u06cc \u0636\u0644\u0639 \u0644\u0648\u0626\u06cc\u0631 \u062f\u06cc\u0631': 'Dir',\n'\u0688\u0627\u06a9\u062e\u0627\u0646\u06c1 \u062e\u0627\u0635 \u0688\u06be\u0648\u06a9 \u0645\u0635\u0627\u062d\u0628 \u062a\u062d\u0635\u06cc\u0644 \u062a\u0644\u06c1 \u06af\u0646\u06af \u0636\u0644\u0639 \u0686\u06a9\u0648\u0627\u0644\u06d4': 'Chakwal',\n'\u0633\u06cc\u062f\u0648\u0627\u0644\u06c1 \u062a\u062d\u0635\u06cc\u0644 \u0648 \u0636\u0644\u0639 \u0646\u0646\u06a9\u0627\u0646\u06c1 \u0635\u0627\u062d\u0628': 'Syedwala Tehsil and District Nankana Sahib',\n'\u0688\u06cc\u0631\u06c1 \u063a\u0627\u0632\u06cc\u062e\u0627\u0646': 'Dera Ghazi Khan',\n'\u0631\u062d\u06cc\u0645 \u06cc\u0627\u0631 \u062e\u0627\u0646\/\u0641\u0636\u06cc\u0644\u062a \u0679\u0627\u0624\u0646': 'Rahim Yar Khan \/ Fazilat Town',\n'\u0634\u06c1\u0631 \u062e\u06cc\u0631\u067e\u0648\u0631 \u0679\u0627\u0645\u06cc\u0648\u0627\u0644\u06cc': 'City of Khairpur Tamiwali',\n'\u0679\u0646\u0688\u0648\u0622\u062f\u0645': 'Tando Adam',\n'\u0645\u0627\u0646\u0633\u06c1\u0631\u06c1': 'Mansehra',\n'\u062e\u0627\u0646\u06cc\u0648\u0627\u0644':'khanewal',\n'\u06c1\u06cc\u0644\u0627\u06ba':'helan',\n'\u0627\u0648\u06a9\u0627\u0691\u06c1':'okara',\n'\u067e\u06be\u0627\u0644\u06cc\u06c1':'phalia',\n'\u067e\u06cc\u0631\u0645\u062d\u0644':'pirmahal',\n'\u0644\u06cc\u06c1':'layyah',\n'\u0633\u0646\u0627\u0646\u0648\u0627\u06ba':'kotaddu',\n'\u06af\u06cc\u0644\u06d2\u0648\u0627\u0644':'lodhran',\n'\u062c\u0639\u0641\u0631\u0627\u0628\u0627\u062f':'jaffarabad',\n'\u06a9\u0648\u06c1\u06cc\u0679\u0627':'quetta',\n'\u0631\u0648\u06c1\u06cc\u0644\u0627\u0646\u0648\u0627\u0644\u06cc':'Muzaffargarh',\n'\u06c1\u0631\u0646\u0627\u0626\u06cc':'Harnai',\n'\u06c1\u0646\u06af\u0648':'hangu', \n'\u0645\u062d\u0644\u06c1\u06af\u0648\u0691\u06be\u0627':'Mandi Bahauddin', \n'\u0679\u0645\u0646':'Talagang',\n'\u06c1\u0631\u0646\u0648\u0644\u06cc':'Mianwali',\n'\u0633\u06a9\u06c1\u0631':'sukkur',\n'\u062c\u0627\u0645\u067e\u0648\u0631':'jampur', \n'\u0645\u064a\u0631\u067e\u0648\u0631\u062e\u0627\u0635':'mirpurkhas',\n'\u0644\u0627\u0644\u06cc\u0627\u06ba':'lalian',\n'\u0628\u06be\u06a9\u0631':'bhakkar',\n'\u067e\u0644\u0646\u062f\u0631\u06cc':'Sudhanoti ',\n'\u06a9\u0646\u0688\u06cc\u0627\u0631\u0648':'Kandiaro',\n'\u062f\u06be\u0648\u06a9\u0645\u0635\u0627\u062d\u0628':'chakwal',\n'\u0635\u0627\u0644\u062d\u067e\u0679':'sukkur',\n'\u0644\u0648\u062f\u06be\u0631\u0627\u06ba':'lodhran',\n'\u067e\u062a\u0648\u06a9\u06cc':'pattoki'    \n}\ndf['City']=df['City'].replace(changename)","7362e52f":"df.City = df.City.str.replace(' ', '')","96b67d09":"df['City'].nunique()","cdecd830":"df['City']=df['City'].replace(['detaismailkhan','dikhan','d.i.khan','d.i.khan','dikhan','dearismailkhan'],'deraismailkhan')\ndf['City']=df['City'].replace(['dg khan\/ dg khan','d g khan','d. g. khan','d.g.khan','dg khan','dgkhan','dear ghazi khan'],'deraghazikhan')\ndf['City']=df['City'].replace(['malir','korangi','orangitown','nazimabad','sadertown','khi','karachi','karachai','karach','karahi','karchi','krachi','kararachi','katlrachi','kaeachi','karacho','karachu','karacji','karavhi','karqchi','karwchi'],'karachi')\ndf['City']=df['City'].replace(['lhr','lahorre','lakhore','lahor','labore','lahire','lahorr','lahote','lhore3'],'lahore')\ndf['City']=df['City'].replace(['faisalababd','fsd','faisalabd','faisalbad','faisalabaf','faidalabad','faiaslabad','faisalababad','fisalabad','faislabad','faisalbabd','faslabad','fesalabad','faisdadab'],'faisalabad')\ndf['City']=df['City'].replace(['alkhidmatraazihospitalcbrtown','isb','uslamabaf','islsmabad','islmbad','i\u0307slmabad','islamanad','islamanad','islamaabd','islmabad','islambad','islamabd','islamababd'],'islamabad')\ndf['City']=df['City'].replace(['rwp','raawalpindi','ameentown','rawalphindi','rawalpind','rawapindi','rawapindi','rawlapindi','rawlpandi'],'rawalpindi')\ndf['City']=df['City'].replace(['mirpurazadkashmir','mirpurajk'],'mirpur')\ndf['City']=df['City'].replace(['muzaffrabad','muzaffarabadazadkashmir','muzaffarbad,gharidupata','muzaffarabad,azadkashmir','muzaffarabadcentralbarmuzaffarabad','muzzaffarabadazadkashmir','muzffarabad'],'muzaffarabad')\ndf['City']=df['City'].replace(['kotliajk'],'kotli')\ndf['City']=df['City'].replace(['bhimberazadkashmir'],'bhimber')\ndf['City']=df['City'].replace(['mandibahuddin','mandibahaudin','mandi-bahul-din','\u0636\u0644\u0639\u0645\u0646\u0688\u06cc\u0628\u06c1\u0627\u0648\u0627\u0644\u062f\u06cc\u0646'],'mandibahauddin')\ndf['City']=df['City'].replace(['liaqatpur'],'liaquatpur')\ndf['City']=df['City'].replace(['kotliazadkashmir'],'kotli')\ndf['City']=df['City'].replace(['taunsa'],'taunsasharif')\ndf['City']=df['City'].replace(['kharain'],'kharian')\ndf['City']=df['City'].replace(['wazirbad'],'wazirabad')\ndf['City']=df['City'].replace(['texila'],'taxila')\ndf['City']=df['City'].replace(['baghazadkashmir'],'bagh')\ndf['City']=df['City'].replace(['shikarpursindh'],'shikarpur')\ndf['City']=df['City'].replace(['ahmadpureast'],'ahmedpureast')\ndf['City']=df['City'].replace(['rawlakot'],'rawalakot')\ndf['City']=df['City'].replace(['srainaurangdistrictlakkimarwat','sarainaurangdistrictlakkimarwat'],'sarainaurang')\ndf['City']=df['City'].replace(['chynian','chunain'],'chunian')\ndf['City']=df['City'].replace(['umarkot'],'umerkot')\ndf['City']=df['City'].replace(['lakki','lakkimerwat'],'lakkimarwat')\ndf['City']=df['City'].replace(['sehwanshareef'],'sehwan')\ndf['City']=df['City'].replace(['kamoki'],'kamoke')\ndf['City']=df['City'].replace(['khirpur'],'khairpur')\ndf['City']=df['City'].replace(['skardubaltistan'],'skardu')\ndf['City']=df['City'].replace(['issakhel'],'isakhel')\ndf['City']=df['City'].replace(['kotadu'],'kotaddu')\ndf['City']=df['City'].replace(['kamracantt'],'kamra')\ndf['City']=df['City'].replace(['gwader','gawadar'],'gwadar')\ndf['City']=df['City'].replace(['dipalpur'],'depalpur')\ndf['City']=df['City'].replace(['hubchowki'],'hub')\n","6937a8ef":"books = df.Book_Name.value_counts()\nplt.figure(figsize=(13,8))\nsns.barplot(x=books[:10].index,y=books[:10].values)\nplt.ylabel('Sold Quantity')\nplt.xlabel('Book Name')\nplt.title('Top 10 books',color = 'blue',fontsize=15)\nplt.xticks(rotation=90)\nplt.show()","81a0f211":"cities_ = df.City.value_counts()\nplt.figure(figsize=(13,8))\nsns.barplot(x=cities_[:10].index,y=cities_[:10].values)\nplt.ylabel('Sold Quantity')\nplt.xlabel('Cities')\nplt.title('Top 10 cities',color = 'blue',fontsize=15)\nplt.xticks(rotation=90)\nplt.show()","7c96927a":"counts = df.groupby('day')['Total_Items'].sum()\nplt.bar(counts.index, counts)\nplt.show()","8d1c520b":"counts = df.groupby('Week_Day')['Total_Items'].sum()\nplt.bar(counts.index, counts, color='orange')\nplt.show()","309388a2":"ff = df[(df['Date_Time'] > '2020-1-1') & (df['Date_Time'] <= '2020-12-31')]","49636b52":"ff=ff.groupby('day').sum()","c23ce503":"counts = ff.groupby('day')['Total_Items'].sum()\nplt.bar(counts.index, counts)\nplt.show()","c1777489":"ff1 = df[(df['Date_Time'] > '2020-1-1') & (df['Date_Time'] <= '2020-12-31')]\nff1=ff1.groupby('Week_Day').sum()\ncounts = ff1.groupby('Week_Day')['Total_Items'].sum()\nplt.bar(counts.index, counts, color='green')\nplt.show()","651a3182":"ff2 = df[(df['Date_Time'] > '2020-1-1') & (df['Date_Time'] <= '2020-12-31')]\nff2=ff2.groupby('dayofyear').sum()\ncounts = ff2.groupby('dayofyear').sum().reset_index()\nsns.lineplot(counts['dayofyear'],counts['Total_Items'])\nplt.show()","bb110b1f":"ff3 = df[(df['Date_Time'] > '2020-1-1') & (df['Date_Time'] <= '2020-12-31')]\nff3=ff3.groupby('Month_Number').sum()\ncounts = ff3.groupby('Month_Number')['Total_Items'].sum()\nplt.bar(counts.index, counts, color='orange')\nplt.show()","601431a2":"ff4 = df[(df['Date_Time'] > '2020-1-1') & (df['Date_Time'] <= '2020-12-31')]\nff4=ff4.groupby('hour').sum()\ncounts = ff4.groupby('hour')['Total_Items'].sum()\nplt.bar(counts.index, counts, color='blue')\nplt.show()","3439336e":"col='City'\ndf=df[df.groupby(col)[col].transform('count').ge(3)]","9be764c7":"filt = df[df['City'] == 'city'].index\ndf=df.drop(index=filt)\n\nfilt1 = df[df['City'] == 'pakistan'].index\ndf=df.drop(index=filt1)\n\nfilt2 = df[df['City'] == 'kpk'].index\ndf=df.drop(index=filt2)\n\nfilt3 = df[df['City'] == 'sindh'].index\ndf=df.drop(index=filt3)\n\nfilt4 = df[df['City'] == 'ict'].index\ndf=df.drop(index=filt4)\n\nfilt5 = df[df['City'] == 'bahriatown'].index\ndf=df.drop(index=filt5)","f5939c36":"# df['City'].unique().tolist()","5ed12661":"# get all the unique values in the 'City' column\ncities = df['City'].unique()\n\n# sort them alphabetically and then take a closer look\ncities.sort()\n# cities","42f39b2f":"top_cities=df['City'].value_counts().head(40).reset_index()\ntop_cities.columns=['City','Sold Quantity']","9a6c7417":"for city in top_cities['City']:\n    matches = process.extract(city, cities , limit = len(cities))\n    for potential_match in matches:\n        if potential_match[1] > 80:\n                df.loc[df['City'] == potential_match[0],\"City\"] = city","43167d28":"df['City'].nunique()","ff5ab523":"# df['City'].unique().tolist()","38bc390b":"df.head()","7c568eda":"df[[\"Payment_Method\",\"Total_Items\"]].groupby('Payment_Method').mean()","4e65535c":"df['Payment_Method'].value_counts()","7026dc6f":"ordinal_map = {'cash_on_delivery':1,\n               'easypaisa':2,\n#                'jazzcash':3,\n               'not_mentioned':4,\n               'banktransfer':5\n\n}\n\ndf['Payment_Method_new'] = df.Payment_Method.map(ordinal_map)\ndf=df.drop(['Payment_Method'],axis=1)\ndf.head()\n# encoder=OneHotEncoder(sparse=False)\n\n# df2 = pd.DataFrame (encoder.fit_transform(df[['Payment_Method']]))\n\n# df2.columns = encoder.get_feature_names(['Payment_Method'])\n\n# df.drop(['Payment_Method'] ,axis=1, inplace=True)\n\n# df= pd.concat([df, df2 ], axis=1)\n\n\n# df[\"Payment_Method\"] = df[\"Payment_Method\"].astype('category')\n# dummy_gender = pd.get_dummies(df['Payment_Method'],drop_first=True)\n# df = pd.merge(\n#     left=df,\n#     right=dummy_gender,\n#     left_index=True,\n#     right_index=True,\n# )\n# df=df.drop(['Payment_Method'],axis=1)","0744a899":"# df_=df[df.Payment_Method_new !=1]\n# df_=df_[df.Payment_Method_new !=2]\n# df_=df_[df.Payment_Method_new !=3]\n# df_=df_[df.Payment_Method_new !=4]","73a8ba49":"# df_.tail(50)","3e99fe1a":"df=df.applymap(lambda s: s.lower() if type(s)==str else s)","0d5a757a":"for col in df.columns[0:]:\n    print(col, ': ', len(df[col].unique()), ' labels')","be3c4260":"# df.Book_Name.value_counts().to_dict()","27735238":"df_frequency_map = df.Book_Name.value_counts().to_dict()","f5578575":"df.Book_Name = df.Book_Name.map(df_frequency_map)\n\ndf.head()","3bda4ce0":"# df.City.value_counts().to_dict()","288ae7b1":"# urdu_city = df['City'][~(df['City'].str.contains(\"[a-zA-Z]\"))]\n# urdu_city.unique()","2ef4fe47":"df_frequency_map2=df.City.value_counts().to_dict()\ndf.City = df.City.map(df_frequency_map2)\n\ndf.head()","6d55b7e7":"# df.City = df.City.map(df_frequency_map2)\ndf['Bg_city'] = np.where(df['City'] > 400, 1,0)\n","17b69104":"df['Order_Status'].value_counts()","e3ebcd5a":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['Order_Status']= label_encoder.fit_transform(df['Order_Status'])","059d3d57":"ff3 = df[(df['Date_Time'] > '2020-1-1') & (df['Date_Time'] <= '2020-12-31')]\nff3=ff3.groupby('Month_Number').sum()\ncounts = ff3.groupby('Month_Number')['Total_Items'].sum()\ncounts","94dce0b3":"ordinal_map = {'jan':858,\n               'feb':93,\n               'mar':135,\n               'apr':214,\n               'may':1729,\n               'jun':2036,\n               'jul':1971,\n               'aug':3554,\n               'sep':2297,\n               'oct':2935,\n               'nov':4049,\n               'dec':4580\n\n\n}\n\ndf['Month_name_new'] = df.Month_name.map(ordinal_map)\ndf=df.drop(['Month_name'],axis=1)\ndf=df.drop(['Month_Number'],axis=1)\ndf=df.drop(['Order_Number'],axis=1)\n\n\ndf.head()","8f539df7":"plt.figure(figsize=(20,20))\ncor=df.corr()\nsns.heatmap(cor,annot=True)\nplt.show()","59531124":"print('Completed', round(df['Order_Status'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\nprint('Returned', round(df['Order_Status'].value_counts()[2]\/len(df) * 100,2), '% of the dataset')\nprint('Canceled', round(df['Order_Status'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\n","f2c5e092":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Order_Status', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: Canceled || 2: Returned || 1: Completed)', fontsize=14)","85559f1c":"df['Order_Status'].replace(to_replace= 0, value = 1, inplace=True )\ndf['Order_Status'].replace(to_replace= 2, value = 0, inplace=True )","745186ff":"df=df.drop(['Date_Time'],axis=1)","7c43772a":"df=df.dropna()","1969f433":"df['Order_Status'].value_counts()","be1efae3":"data1= df.sample(frac = 0.4,random_state=1)\n\ndata1.shape","4b6876e9":"data1['Order_Status'].value_counts()","45f748a0":"data1 = data1.dropna()","67623f54":"# data1.replace(to_replace= 0, value = 1, inplace=True )","0b25a356":"Completed = data1[data1['Order_Status']==1]\nReturned = data1[data1['Order_Status']==0]\noutlier_fraction = (len(Returned))\/(len(Completed))","076f8af2":"inliers = data1[data1.Order_Status==1]\ninliers = inliers.drop(['Order_Status'], axis=1)\noutliers = data1[data1.Order_Status==0]\noutliers = outliers.drop(['Order_Status'], axis=1)\ninliers_train, inliers_test = train_test_split(inliers, test_size=0.30, random_state=42)","1810cbe8":"inliers_train.head()","a3299de2":"inliers_test.head()","c5ef21c9":"model = IsolationForest()\nmodel.fit(inliers_train)\ninlier_pred_test = model.predict(inliers_test)\noutlier_pred = model.predict(outliers)","320abd3d":"print(\"Accuracy in Detecting Completed Cases:\", list(inlier_pred_test).count(1)\/inlier_pred_test.shape[0])\nprint(\"Accuracy in Detecting Canceled\/Returned Cases:\", list(outlier_pred).count(-1)\/outlier_pred.shape[0])","9776e1e0":"# new_df2=df","0955eef7":"df['Order_Status'].value_counts()","655a8bee":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of returned\/canceled classes 728 rows.\nfraud_df = df.loc[df['Order_Status'] == 0]\n# unknown_payment_df = df.loc[df['Payment_Method_new'] == 4]\nnon_fraud_df = df.loc[df['Order_Status'] == 1][:1500]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","ffde8373":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Order_Status'].value_counts()\/len(new_df))\n\n\n\nsns.countplot('Order_Status', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","0eb50100":"# new_vis['Payment_Method_new'].value_counts()","679ad229":"# sns.distplot(new_df[new_df['not_mentioned']==0]['Order_Status'],hist=False)\n# sns.distplot(new_df[new_df['not_mentioned']==1]['Order_Status'],hist=False)\n# sns.boxplot(new_df['not_mentioned'],new_df['Total_Items'],hue=new_df['Order_Status'])\n# counts.index\n# sns.boxplot(data = new_df, x='cash_on_delivery', y='Order_Status')\nvis_df = df.loc[df['Payment_Method_new'] == 2][:478]\nvis_df0 = df.loc[df['Payment_Method_new'] == 4][:478]\nvis_df1 = df.loc[df['Payment_Method_new'] == 5][:478]\nvis_df2 = df.loc[df['Payment_Method_new'] == 1][:478]\nnormal_distributed_df = pd.concat([vis_df, vis_df0,vis_df1,vis_df2])\n# Shuffle dataframe rows\nnew_vis = normal_distributed_df.sample(frac=1, random_state=42)\ng = sns.factorplot(x = \"Payment_Method_new\", y = \"Order_Status\", data = new_vis, kind = \"bar\", size = 6)\ng.set_ylabels(\"Order Completion Probability\")\nplt.show()\n","c6cc2095":"# g = sns.factorplot(x = \"Payment_Method_new\", y = \"Total_Items\", data = new_vis, kind = \"bar\", size = 6)\n# g.set_ylabels(\"Order Completion Probability\")\n# plt.show()","1a31ba14":"# g = sns.factorplot(x = \"Total_Items\", y = \"Order_Status\", data = new_vis, kind = \"bar\", size = 6)\n# g.set_ylabels(\"Order Completion Probability\")\n# plt.show()\nnew_vis\ng = sns.FacetGrid(new_vis, col = \"Order_Status\")\ng.map(sns.distplot, \"Total_Items\", bins = 25)\nplt.show()","a671f4e6":"# g = sns.FacetGrid(new_vis, col = \"Order_Status\", row = \"Payment_Method_new\", size = 2)\n# g.map(plt.hist, \"Total_Items\", bins = 25)\n# g.add_legend()\n# plt.show()\n# g = sns.FacetGrid(new_vis, row = \"Payment_Method_new\", size = 2)\n# g.map(sns.pointplot, \"Payment_Method_new\",\"Order_Status\",'Bg_city')\n# g.add_legend()\n# plt.show()\ng = sns.FacetGrid(new_vis, row = \"Payment_Method_new\", col = \"Order_Status\", size = 2.3)\ng.map(sns.barplot, \"Payment_Method_new\", \"Total_Items\")\ng.add_legend()\nplt.show()","ec854ee6":"plt.figure(figsize=(20,20))\ncorrr=new_df.corr()\nsns.heatmap(corrr,annot=True)\nplt.savefig('tinVSts',dp=300)\nplt.show()\n","3022081a":"df","17aaa5f4":"%%capture\n!pip install pycaret[full]","6e6f263f":"from pycaret.classification import *","31981035":"\n    #setting up the pyCaret regression algorithm\nreg = setup(data = new_df,\n            target = 'Order_Status',\n            train_size = 0.7, #70:30 train\/validation split\n            normalize = True, #normalisation helps some algorithms\n            normalize_method = 'robust', #resilient to outliers\n            create_clusters = True, #adds additional feature by assigning clusters\n            feature_interaction = True, #new features are created by interacting (a * b) all the numeric variables in the dataset\n#             feature_selection=True,\n            polynomial_features=True,\n            use_gpu = True, #use GPU acceleration to train models\n            silent = True, #removes need for confirmation step\n            fold = 10, #number of cross-fold validation folds\n            n_jobs = -1) #use all processor threads","385f012d":"compare_models()","8700e7dd":"#create models for blending\nlr = create_model('lr')\ncb = create_model('catboost')\nrf = create_model('rf')\ngbc=create_model('gbc')\n#blend trained models\nblend_specific = blend_models(estimator_list = [lr,rf,cb,gbc])","a205dbc7":"# plot_model(blend_specific, plot = 'ks')","97adaeea":"plot_model(blend_specific,plot = 'confusion_matrix')","ce064dc7":"# plot_model(blend_specific,plot = 'learning')\n","2cf586ed":"plot_model(blend_specific,plot = 'error')\n","11a0a3af":"plot_model(blend_specific,plot = 'class_report')\nplt.savefig('tinVSts',dp=300)\nplt.show()","d2d05732":"plot_model(blend_specific,plot = 'boundary')\n\n","1c42fb2f":"# plot_model(lr,plot = 'rfe')\n","12a043db":"# plot_model(blend_specific,plot = 'learning')\n","5bbb0510":"plot_model(blend_specific,plot = 'calibration')\n","d811f530":"# plot_model(lr,plot = 'vc')\n","02521025":"plot_model(blend_specific,plot = 'dimension')\n\n","e2bd9865":"plot_model(lr,plot = 'feature')\n\n","134ff79a":"# **Modeling**\n","56168c50":"# **Data Cleaning**","45ce4a2c":"##  Seems like using Isolation_Forest algorithms give us reasonable results, let's try undersampling, to see which works better.","98c1053c":"## City Name Cleaning","256b6e37":"# **Correlation**\n## Now that we have converted all strings to numbers using different encoding techniques, let's see their correlation.","2550beb9":"# Remove Outliers\n##  On one day 80022 books were bought, it must be an outlier","a71b6859":"## **Step.1**\n##  Let's contain list of all Pakistani cities so that if City_Name is mentioned as 'East Karachi' it will be converted to 'Karachi'","9350102e":"# Renaming the columns\n##  Pandas doesn't recognize 'Order Status' format so we need to convert it into \"Order_Status\"","4e9b7aa1":"## Major sales between 10-15 and 25-30 (It is to note that 5 out 12 months doesn't have 31, that could be the reason of down trend)","823c2b12":"## Let's see how many orders were completed, canceled and returned.","b778e452":"## **Step.3**\n## Let's delete space between city names so that 'sadiq abad' and sadiqabad' becomce one string","50fc2bf7":"##  Seems like brackets was not used in the book's name. Let's try something else","dd839a30":"##  Currently the data type of 'date_time' is object, let's convert it into datetime data type.","f9068551":"## **Step.5**\n##  Let's use the lowercase in the dataframe to avoid any repitition.","1b765306":"# **Data Pre-processing**","87b8970b":"### 4pm to 11pm are peak hours.","e9e96e7a":"## **Step.5**","7bc407df":"##  **Books name cleaning**","658da871":"## Let's see how many books we have","029ec7f2":"## The reason of sudden spike at the end of year is  because Sir Zeeshan Usmani was uploading introductory videos of his courses and books on Al-Nafi.","fa68d670":"## Since our EDA is completed let's drop the cities that occurred less than 3 times and Cities with names like 'pakistan','kpk','sindh' because they are not cities and add no value to our dataset.","abff0e72":"## Let's see the book sale across the month","5badb71c":"## **Step.3**\n##  Good to go. Still there are name of same books having different spellings, let's correct the spellings.","759f0d3d":"###  We see a big spike at the beginning of the month indicating more people are buying at the start of the month,then second spike at 9th and 10th after which the sale decreases and we see good recovery during the last three days of the month. ","c61e6091":"## **Step.1**\n## Let's delete brackets and check if it reduces the number of unique books.","f7596ecd":"## **Step.4**\n## Let's convert the remaining spell confusion to their respective cities.","a31749c3":"###   So, after cities name cleaning we have 317 distinct cities which were 3599 in the beginning.","2af70f9e":"# Calling all the dependencies","363be336":"### Notice how the correlation has changed as compared to last heatmap.","b8efaf1c":"## Apply frequency encoding on City column as well","ced57f65":"## Before beginning I want to thank:\n## @muhammaddaniyal214(https:\/\/www.kaggle.com\/muhammaddaniyal214\/returns-probability-of-an-order-return)\n## @hammad40241(https:\/\/www.kaggle.com\/hammad40241\/gufhtugu-publications-analysis)\n## @sohailds (https:\/\/www.kaggle.com\/sohailds\/gp-challenge-top-10-books)\n## @janiobachmann (https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)\n## @mfaisalqureshi (https:\/\/www.kaggle.com\/mfaisalqureshi\/gufhtugu-publications-eda)\n## @nilanml(https:\/\/www.kaggle.com\/nilanml\/fraud-detection-using-isolation-forest)\n## Their notebooks helped greatly in the making of this notebook","79cf2e87":"# Frequency Encoding\n### Replace each book by number of time it was purchased","10bbd058":"## **Step.6**\n##  Spelling Correction using FuzzyWuzzy\n###  If a city is spelled like 'sheikhupure' it will convert it into its correct spelling 'sheikhupura'","6a55b52b":"## Almost 94% orders were successfully executed. It is important to recognize that orders that were either canceled or returned are only 6%. If we use traditional algorithms like KNN it would give high accuracy and it would be misleading because our model will be inclined to consider most of the orders as completed. \n###   You can read more about that:\n###   1) https:\/\/github.com\/krishnaik06\/Credit-Card-Fraudlent\/blob\/master\/Anamoly%20Detection.ipynb\n###   2) https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\n##  Isolation Forest Algorithm works best for such problems","08c8e2ba":"# **Modeling using Pycaret**","ec0a2794":"## Add month, day of week, day of year which will help us in Exploratory Data Analysis.","a00e9ad9":"# But But But\n##  We want to delete city names that contain 'Pakistan', 'Sindh', 'kpk' because they are not cities and secondly  deleting cities whose name occurs less than three times, but that can effect our EDA. So, we have to perform EDA before deleting the mentioned rows.","03772e9d":"## **Step.2**\n##  Let's convert all books name mentioned in Urdu to English","a7c83e4b":"### Major Sales on weekend.","e3f3bf55":"## **Step.2**\n##  If you run the above code you will see that alot of books 'unique'books are separated by '\/'. For example, \n##  'Kaggle for Begginers\/Bitcoin Blockchain aur Crypto Currency - FREE E-Book'\n##  is currently treated as one distinct books which infact is not the case they are three  different and distinct books. So, let's just split books by '\/'","1688d12d":"##  So, we obtained '647'unique books that were '3518' in the beginning.","6105cb45":"## **Step.4**\n## Two books that contained the strings 'c++' and 'Linux - An Introduction' were still not converted. Let's replace the name of these books if they contain these strings","54042957":"# Top ten cities that purchased most books","d9054dc3":"# **Let's analyze the Covid-19 Year 2020 separately.**","94849780":"###   Monday=0\n###   Tuesday=1\n###   Wednesday=2\n###   Thursday=3\n###   Friday=4\n###   Saturday=5\n###   Sunday=6","9bd1e3f4":"## Gentle Feature Engineering\n### Replacing month name by number of books sold that montH","f247a8dc":"# Ordinal Encoding\n## Transaction  of most items happened through bank transfer and least items were paid through COD.","af0bb839":"##  Now check unique books.","012840ae":"## Label Encoding the output Order_Status","58bd8866":"# Import Dataset","28d5ba48":"##  Book Sale across the week","625e90e7":"###   No increase in sale during the peak of COVID March, April, and May. Quite surprising."}}