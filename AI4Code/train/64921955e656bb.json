{"cell_type":{"1d1f6a88":"code","2f3eb2a5":"code","293a9f47":"code","d3cf4fb9":"code","c912c544":"code","f16a9c08":"code","a74a1741":"code","979e958a":"code","d3494ff5":"code","68e8e137":"code","adafb7ed":"code","185da511":"code","3fab3300":"code","d6dcadcb":"code","4b41e93f":"markdown"},"source":{"1d1f6a88":"!pip install kaggle-environments --upgrade -q","2f3eb2a5":"%%writefile agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","293a9f47":"%%writefile always_first_agent.py\n\ndef always_first(observation, configuration):\n    return 0","d3cf4fb9":"%%writefile sample_agent.py\n\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] \/ numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) \/ numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","c912c544":"%%writefile bay_sub.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        post_a[bandit] += r\n        post_b[bandit] += (1 - r)\n\n    \n    bound = post_a \/ (post_a + post_b).astype(float) + beta.std(post_a, post_b) * 4\n    bandit = int(np.argmax(bound))\n    \n    return bandit","f16a9c08":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.run([\"sample_agent.py\", \"bay_sub.py\"])\nenv.render(mode=\"ipython\", width=800, height=800)","a74a1741":"print('Sant 2020')\nenv.run([\"..\/input\/santa-2020\/submission.py\", \"bay_sub.py\"])\nenv.render(mode=\"ipython\", width=800, height=500)","979e958a":"%%writefile ucb_decay.py\n\nimport numpy as np\n\ndecay = 0.97\ntotal_reward = 0\nbandit = None\n\ndef agent(observation, configuration):\n    global reward_sums, n_selections, total_reward, bandit\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        n_selections, reward_sums = np.full((2, n_bandits), 1e-32)\n    else:\n        reward_sums[bandit] += decay * (observation.reward - total_reward)\n        total_reward = observation.reward\n\n    avg_reward = reward_sums \/ n_selections    \n    delta_i = np.sqrt(2 * np.log(observation.step + 1) \/ n_selections)\n    bandit = int(np.argmax(avg_reward + delta_i))\n\n    n_selections[bandit] += 1\n\n    return bandit","d3494ff5":"%%writefile bayesian_ucb.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\npost_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a \/ (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    return bandit","68e8e137":"env.reset()\nenv.run([\"..\/input\/santa-2020\/submission.py\", \"ucb_decay.py\"])","adafb7ed":"env.reset()\nenv.run([\"..\/input\/santa-2020\/submission.py\", \"bayesian_ucb.py\"])\nenv.render(mode=\"ipython\", width=800, height=500)","185da511":"def print_rounds(file1, file2, N=5):\n    env = make(\"mab\", debug=True)\n\n    for i in range(N):\n        env.run([file1, file2])\n        p1_score = env.steps[-1][0]['reward']\n        p2_score = env.steps[-1][1]['reward']\n        env.reset()\n        print(f\"Round {i+1}: {p1_score} - {p2_score}\")","3fab3300":"print('Default vs UCB+decay')\nprint_rounds(\"..\/input\/santa-2020\/submission.py\", \"ucb_decay.py\")","d6dcadcb":"print('Default vs BayesianUCB')\nprint_rounds(\"..\/input\/santa-2020\/submission.py\", \"bayesian_ucb.py\")","4b41e93f":"References:\n\n* Santa 2020 starter: Re-used writefile magic command and make_env function for creating a simulation."}}