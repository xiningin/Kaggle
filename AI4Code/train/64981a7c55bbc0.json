{"cell_type":{"637b19ba":"code","5de1df2f":"code","0090b841":"code","d072c06f":"code","d224754d":"code","2cf72af8":"code","a66bf01b":"code","b054f0a9":"code","70f6712d":"code","b3d6c739":"code","bc39e4d9":"code","658e3078":"code","52b54837":"code","7930ed04":"code","7b408f26":"code","afd398b0":"code","dc0f88d7":"code","86e520fb":"code","c587c922":"code","8162ab36":"code","455e4ef9":"code","f7770118":"code","94c7828f":"code","2df8c6d1":"code","3eb9a112":"code","4b79b34d":"code","cb2e6910":"code","6e496f56":"code","cfb64d01":"code","016032e9":"code","26c3e3f7":"code","753b13be":"code","454d6008":"code","be6b4db5":"code","da052800":"code","d959d1c5":"code","91b14741":"code","3f533b28":"code","353251e2":"code","6879feb5":"code","aba92917":"code","1b21150f":"code","23a7de05":"code","133c585f":"code","79a34bd5":"code","90953769":"code","0b44c55b":"code","ea379d30":"code","14f5a9eb":"code","a3c89b3b":"code","96424319":"code","e5475a04":"code","e080bc2c":"code","190a46d0":"code","05faac60":"code","e0a1ea94":"code","a5aa1af0":"code","1ac94538":"code","0aa6277b":"code","a351dfa5":"code","ec055285":"code","fd6ef25a":"code","0c6720a5":"code","69534bd7":"code","9393c644":"code","0e5cc13d":"code","aab9db25":"code","87e15eae":"code","8e12097f":"code","b5266b41":"code","6fba79fa":"code","c13cc674":"code","aa3dc0ec":"code","b66b6b58":"code","64447af8":"code","623ba3a1":"code","e8429764":"code","29be0221":"code","adb13128":"code","8f536a41":"code","2a7834b0":"code","86cc39d5":"code","e5cfc56d":"code","de12904d":"code","48a9b997":"code","4cece475":"code","b5531fc0":"code","f74dcf75":"code","4d3d5711":"code","138aad4d":"code","268a5ee3":"code","9fc25def":"code","f9503199":"code","86aa27d4":"code","9aaa695a":"code","090ae6aa":"code","f8642289":"code","32b1a91a":"code","9ca07cb4":"code","3f70b33b":"code","16163dfe":"code","ed17882a":"code","9eb9bc57":"code","95d9eb76":"code","261852ba":"code","69b260c5":"code","93f9b8ce":"code","74c05194":"markdown","aefb7e54":"markdown","b7bb2366":"markdown","a6697c4d":"markdown","307b5b1a":"markdown","ba608006":"markdown","52b94302":"markdown","b850a0c7":"markdown","d7f90df5":"markdown","c581d8c0":"markdown","36d2428f":"markdown","89d43624":"markdown","7a76ea6f":"markdown","0e18d6f5":"markdown","88337109":"markdown","328d283a":"markdown"},"source":{"637b19ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5de1df2f":"import pandas as pd","0090b841":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","d072c06f":"train_df.head()","d224754d":"print(train_df.shape)\n","2cf72af8":"real_len = train_df[train_df.target == 1].shape[0]\nnot_len = train_df[train_df.target == 0].shape[0]","a66bf01b":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(7,5))\nplt.bar(10, real_len, 3, label=\"Real\", color=\"blue\")\nplt.bar(15, not_len, 3, label=\"Not\", color=\"red\")\nplt.legend()\nplt.ylabel(\"Number of examples\")\nplt.title(\"Propertion of example\")\nplt.show()","b054f0a9":"def length(string):\n    return len(string)\n\ntrain_df[\"length\"] = train_df.text.apply(length)","70f6712d":"plt.figure(figsize=(18,6))\nbins = 150\n\nplt.hist(train_df[train_df.target == 1][\"length\"], alpha=0.8, bins=bins, label=\"Real\")\nplt.hist(train_df[train_df.target == 0][\"length\"], alpha=0.6, bins=bins, label=\"Not\")\nplt.xlabel(\"Length\")\nplt.ylabel(\"Numbers\")\nplt.legend(loc=\"upper right\")\nplt.xlim(0, 150)\nplt.grid()\nplt.show()","b3d6c739":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 5))\ntweet_len = train_df[train_df.target == 1][\"text\"].str.len()\nax1.hist(tweet_len, color=\"blue\")\nax1.set_title(\"Disaster tweets\")\ntweet_len = train_df[train_df.target == 0][\"text\"].str.len()\nax2.hist(tweet_len, color=\"red\")\nax2.set_title(\"Not disaster tweets\")\nfig.suptitle(\"Characters in tweets\")\nplt.show()","bc39e4d9":"import re","658e3078":"def remove_url(string):\n    URL_PATTERN = r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%|\\-)*\\b'\n    return re.sub(URL_PATTERN,  '', string)","52b54837":"print(\"Example of text with URL: \\n\", train_df[\"text\"][3912], end=\"\\n\\n\")\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_url)\n\nprint(\"Example of text with URL: \\n\", train_df[\"text\"][3912], end=\"\\n\\n\")\n","7930ed04":"def handle_tags(string):\n    pattern = re.compile(r'[@|#][^\\s]+')\n    matches = pattern.findall(string)\n    tags = [match[1:] for match in matches]\n    string = re.sub(pattern, '', string)\n    return string + ' ' + ' '.join(tags) + ' '+ ' '.join(tags) + ' ' + ' '.join(tags)","7b408f26":"print(\"Example of text without Handling Tags: \\n\", train_df['text'][3914],end = \"\\n\\n\")\ntrain_df['text'] = train_df['text'].apply(handle_tags)\nprint(\"Example of text with Handling Tags: \\n\", train_df['text'][3914])","afd398b0":"!pip install demoji","dc0f88d7":"import demoji","86e520fb":"demoji.download_codes()","c587c922":"def handle_emoji(string):\n    return demoji.replace_with_desc(string)","8162ab36":"print(\"Example of text without Handled Emojis: \\n\", train_df[\"text\"][17], end=\"\\n\\n\")\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(handle_emoji)\n\nprint(\"Example of text with Handled Emoji: \\n\", train_df['text'][17])","455e4ef9":"def remove_html(string):\n    HTML_TAGS_PATTERN = r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});'\n    return re.sub(HTML_TAGS_PATTERN, '', string)","f7770118":"train_df[\"text\"] = train_df[\"text\"].apply(remove_html)","94c7828f":"import nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\nstemmer = SnowballStemmer(\"english\")","2df8c6d1":"stopword = stopwords.words(\"english\")\n\ndef remove_stop_and_stem(string):\n    string_list = string.split()\n    return ' '.join([stemmer.stem(item) for item in string_list if item not in stopword])","3eb9a112":"print(\"Example of text before Removing Stopwords: \\n\", train_df['text'][17], end = \"\\n\\n\")\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_stop_and_stem)\n\nprint(\"Example of text after Removing Stopwords and Stemming: \\n\", train_df['text'][17])","4b79b34d":"def remove_uc(string):\n    res_string = re.sub(r'[^a-zA-Z\\s]', '', string)\n    res_string = re.sub(r'\\b\\w{1,2}\\b', '', res_string)\n    \n    return re.sub(' +', ' ', res_string)","cb2e6910":"print(\"Example of text before Removing Useless Character: \\n\", train_df['text'][17],end = \"\\n\\n\")\ntrain_df['text'] = train_df['text'].apply(remove_uc)\nprint(\"Example of text after Removing Useless Character: \\n\", train_df['text'][17])","6e496f56":"def merging_details(data):\n    df_list = []\n    \n    for row in data.itertuples():        \n        keyword = re.sub(r'[^a-zA-Z\\s]', '', str(row[2]))\n        location = re.sub(r'[^a-zA-Z\\s]', '', str(row[3]))\n        \n        keyword = re.sub(r'\\b\\w{1,2}\\b', '', keyword)\n        location = re.sub(r'\\b\\w{1,2}\\b', '', location)\n        \n        text = str(row[4])\n        \n        if keyword == 'nan':\n            if location == 'nan':\n                prs_data = text\n                \n            else:\n                prs_data = location + ' ' + text\n        else:\n            if location == 'nan':\n                prs_data = keyword + ' ' + text\n            else:\n                prs_data = keyword + ' ' + location + ' ' + text\n                \n        prs_data = re.sub(' +', ' ', prs_data)    \n        \n        df_list.append(prs_data)\n        \n    return df_list","cfb64d01":"train_df[\"cleaned_data\"] = merging_details(train_df)\n\ntrain_df","016032e9":"from wordcloud import WordCloud\n\n%time\n\ndict_of_words = {}\n\nfor row in train_df.itertuples():\n    for i in row[4].split():\n        try:\n            dict_of_word[i] += 1\n        except:\n            dict_of_words[i] = 1\n            \n            \nword_cloud = WordCloud(background_color=\"black\", width=1000, height=500).generate_from_frequencies(dict_of_words)\nfig = plt.figure(figsize=(10,5))\nplt.imshow(word_cloud)\nplt.tight_layout(pad=1)\nplt.show()","26c3e3f7":"test_df['text'] = test_df['text'].apply(remove_url)\n\ntest_df['text'] = test_df['text'].apply(handle_tags)\n\ntest_df['text'] = test_df['text'].apply(handle_emoji)\n\ntest_df['text'] = test_df['text'].apply(remove_html)\n\ntest_df['text'] = test_df['text'].apply(remove_stop_and_stem)\n\ntest_df['text'] = test_df['text'].apply(remove_uc)\n\ntest_df[\"cleaned_data\"] = merging_details(test_df)","753b13be":"test_df","454d6008":"x_test = test_df[\"cleaned_data\"]","be6b4db5":"from sklearn.model_selection import train_test_split\n\nx_train, x_validation, y_train, y_validation = train_test_split(train_df[\"cleaned_data\"], train_df[\"target\"], test_size=0.1, random_state=1)","da052800":"from sklearn.feature_extraction.text import TfidfVectorizer","d959d1c5":"%time\n\nvectorizer = TfidfVectorizer(min_df = 0.0005,\n                             max_features=100000,\n                             tokenizer= lambda x:x.split(),\n                             ngram_range=(1,4))\n\nx_train = vectorizer.fit_transform(x_train)\nx_validation = vectorizer.transform(x_validation)","91b14741":"x_test = vectorizer.transform(x_test)","3f533b28":"print(\"Training Points: \", len(x_train.toarray()), \"| training Features: \", len(x_train.toarray()[0]))\nprint(\"Validation Points: \", len(x_validation.toarray()), \"| Validation Features: \", len(x_validation.toarray()[0]))","353251e2":"print(\"Test Points: \", len(x_test.toarray()), \"| Test Features: \", len(x_test.toarray()[0]))","6879feb5":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support","aba92917":"from sklearn.linear_model import LogisticRegression\n\n%time\n\nLogisticRegression_model = LogisticRegression(C=1, penalty='l2')\n\nLogisticRegression_model.fit(x_train, y_train)","1b21150f":"def print_score(y_pred, y_real):\n    print(\"Accuracy: \", accuracy_score(y_real, y_pred))\n\n    print()\n    print(\"Macro precision_recall_fscore_support (macro) average\")\n    print(precision_recall_fscore_support(y_real, y_pred, average=\"macro\"))\n\n    print()\n    print(\"Macro precision_recall_fscore_support (micro) average\")\n    print(precision_recall_fscore_support(y_real, y_pred, average=\"micro\"))\n\n    print()\n    print(\"Macro precision_recall_fscore_support (weighted) average\")\n    print(precision_recall_fscore_support(y_real, y_pred, average=\"weighted\"))","23a7de05":"y_pred = LogisticRegression_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","133c585f":"from sklearn.naive_bayes import GaussianNB\n\n%time\n\nGaussianNB_model = GaussianNB()\n\nGaussianNB_model.fit(x_train.toarray(), y_train)","79a34bd5":"y_pred = GaussianNB_model.predict(x_validation.toarray())\n\nprint_score(y_validation, y_pred)","90953769":"from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n\n%time\n\nBernoulliNB_model = BernoulliNB()\n\nBernoulliNB_model.fit(x_train, y_train)","0b44c55b":"y_pred = BernoulliNB_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","ea379d30":"from sklearn.naive_bayes import ComplementNB\n\n\n%time\n\nComplementNB_model = ComplementNB()\n\nComplementNB_model.fit(x_train, y_train)","14f5a9eb":"y_pred = ComplementNB_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","a3c89b3b":"from sklearn.naive_bayes import MultinomialNB\n\n%time\n\nMultinomialNB_model = MultinomialNB()\n\nMultinomialNB_model.fit(x_train, y_train)\n","96424319":"y_pred = MultinomialNB_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","e5475a04":"from sklearn import svm\n\n%time\n\nSVM_RBF_model = svm.SVC(kernel=\"rbf\")\n\nSVM_RBF_model.fit(x_train, y_train)","e080bc2c":"y_pred = SVM_RBF_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","190a46d0":"from sklearn import svm\n\n%time\n\nSVM_linear_model = svm.SVC(kernel=\"linear\")\n\nSVM_linear_model.fit(x_train, y_train)","05faac60":"y_pred = SVM_linear_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","e0a1ea94":"from sklearn.ensemble import RandomForestClassifier\n\n%time\n\nRandomForest_model = RandomForestClassifier(random_state=1)\n\nRandomForest_model.fit(x_train, y_train)","a5aa1af0":"y_pred = SVM_linear_model.predict(x_validation)\n\nprint_score(y_validation, y_pred)","1ac94538":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\nfrom tensorflow.python.keras.initializers import RandomNormal","0aa6277b":"x_train = x_train.toarray()\nx_validation = x_validation.toarray()\nx_test = x_test.toarray()","a351dfa5":"from keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)","ec055285":"output_dim = 1\ninput_dim = x_train.shape[1]\nbatch_size = 200\nnum_epoch = 100","fd6ef25a":"def plot_training_history(history, epochs):\n    plt.figure(figsize=(20,5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history.history[\"loss\"], color=\"blue\", label=\"training_loss\")\n    plt.plot(epochs, history.history[\"val_loss\"], color=\"red\", label=\"validation_loss\")\n    plt.legend(loc=\"best\")\n    plt.title(\"Training\")\n    plt.xlabel(\"Epoch\")\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history.history[\"accuracy\"], color=\"blue\", label=\"training_accuracy\")\n    plt.plot(epochs, history.history[\"val_accuracy\"], color=\"red\", label=\"validation_accuracy\")\n    plt.legend(loc=\"best\")\n    plt.title(\"validation\")\n    plt.xlabel(\"Epoch\")","0c6720a5":"%time\n\nSLP_model = Sequential()\n\nSLP_model.add(Dense(output_dim, \n                    input_dim=input_dim, \n                    activation='softmax'))\n\nSLP_model.summary()","69534bd7":"SLP_model.compile(optimizer=\"sgd\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = SLP_model.fit(x_train, \n                        y_train, \n                        batch_size=batch_size,\n                        epochs=num_epoch,\n                        verbose=1,\n                        validation_data=(x_validation, y_validation),\n                        callbacks=[early_stopping])","9393c644":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","0e5cc13d":"y_pred = SLP_model.predict(x_validation)\nprint_score(y_validation, y_pred)","aab9db25":"input_dim = x_train.shape[1]\nhidden_layer_1 = 2048\nhidden_layer_2 = 1024\nhidden_layer_3 = 512\nout_dim = 1\n\nbatch_size = 200\nnum_epoch = 100","87e15eae":"%time\n\nMLP_model = Sequential()\n\nMLP_model.add(Dense(hidden_layer_1, \n                    activation=\"sigmoid\",\n                    input_shape=(input_dim,)))\n\nMLP_model.add(Dense(hidden_layer_2,\n                    activation=\"sigmoid\"))\n\nMLP_model.add(Dense(hidden_layer_3,\n                    activation=\"sigmoid\"))\n\nMLP_model.add(Dense(output_dim,\n                    activation=\"softmax\"))\n\nMLP_model.summary()","8e12097f":"MLP_model.compile(optimizer=\"adam\", \n                  loss=\"binary_crossentropy\",\n                  metrics=[\"accuracy\"])\n\nhistory = MLP_model.fit(x_train, \n                        y_train, \n                        batch_size=batch_size,\n                        epochs=num_epoch,\n                        verbose=1,\n                        validation_data=(x_validation, y_validation),\n                        callbacks=[early_stopping])","b5266b41":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","6fba79fa":"y_pred = MLP_model.predict(x_validation)\nprint_score(y_validation, y_pred)","c13cc674":"MLP_RELU_model = Sequential()\n\nMLP_RELU_model.add(Dense(hidden_layer_1,\n                         activation=\"relu\",\n                         input_shape=(input_dim,)))\n\nMLP_RELU_model.add(Dense(hidden_layer_2,\n                         activation=\"relu\"))\n\nMLP_RELU_model.add(Dense(hidden_layer_3,\n                         activation=\"relu\"))\n\nMLP_RELU_model.add(Dense(output_dim,\n                         activation=\"relu\"))\n\nMLP_RELU_model.summary()","aa3dc0ec":"MLP_RELU_model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = MLP_RELU_model.fit(x_train, \n                             y_train,\n                             batch_size=batch_size,\n                             epochs=num_epoch,\n                             verbose=1,\n                             validation_data=(x_validation,y_validation),\n                             callbacks=[early_stopping])","b66b6b58":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","64447af8":"y_pred = MLP_RELU_model.predict(x_validation)\ny_pred = [[1] if val>0 else [0] for sublist in y_pred for val in sublist]\nprint_score(y_validation, y_pred)","623ba3a1":"MLP_SGD_model = Sequential()\n\nMLP_SGD_model.add(Dense(hidden_layer_1,\n                        activation=\"relu\",\n                        input_shape=(input_dim,)))\n\nMLP_SGD_model.add(Dense(hidden_layer_2,\n                        activation=\"relu\"))\n\nMLP_SGD_model.add(Dense(hidden_layer_3,\n                        activation=\"relu\"))\n\nMLP_SGD_model.add(Dense(1,\n                        activation=\"relu\"))\n\nMLP_SGD_model.summary()","e8429764":"MLP_SGD_model.compile(optimizer=\"sgd\", \n                      loss=\"binary_crossentropy\",\n                      metrics=[\"accuracy\"])\n\nhistory = MLP_SGD_model.fit(x_train, \n                            y_train, \n                            batch_size=batch_size,\n                            epochs=num_epoch,\n                            verbose=1,\n                            validation_data=(x_validation, y_validation),\n                            callbacks=[early_stopping])","29be0221":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","adb13128":"y_pred = MLP_SGD_model.predict(x_validation)\ny_pred = [[1] if val>0 else [0] for sublist in y_pred for val in sublist]\nprint_score(y_validation, y_pred)","8f536a41":"MLP_BatchNorm_ADAM_model = Sequential()\n\nMLP_BatchNorm_ADAM_model.add(Dense(hidden_layer_1,\n                                   activation=\"sigmoid\",\n                                   input_shape=(input_dim,)))\nMLP_BatchNorm_ADAM_model.add(BatchNormalization())\n\nMLP_BatchNorm_ADAM_model.add(Dense(hidden_layer_2,\n                                   activation=\"sigmoid\"))\nMLP_BatchNorm_ADAM_model.add(BatchNormalization())\n\nMLP_BatchNorm_ADAM_model.add(Dense(hidden_layer_3,\n                                   activation=\"sigmoid\"))\nMLP_BatchNorm_ADAM_model.add(BatchNormalization())\n\nMLP_BatchNorm_ADAM_model.add(Dense(output_dim,\n                                   activation=\"softmax\"))\n\nMLP_BatchNorm_ADAM_model.summary()","2a7834b0":"MLP_BatchNorm_ADAM_model.compile(optimizer=\"adam\",\n                                 loss=\"binary_crossentropy\",\n                                 metrics=[\"accuracy\"])\n\nhistory = MLP_BatchNorm_ADAM_model.fit(x_train, \n                                       y_train,\n                                       batch_size=batch_size,\n                                       epochs=num_epoch,\n                                       validation_data=(x_validation, y_validation),\n                                       callbacks=[early_stopping])","86cc39d5":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","e5cfc56d":"y_pred = MLP_BatchNorm_ADAM_model.predict(x_validation)\nprint_score(y_validation, y_pred)","de12904d":"MLP_SIGMOID_BatchNorm_SGD_model = Sequential()\n\nMLP_SIGMOID_BatchNorm_SGD_model.add(Dense(hidden_layer_1,\n                                           activation=\"sigmoid\",\n                                           input_shape=(input_dim,)))\nMLP_SIGMOID_BatchNorm_SGD_model.add(BatchNormalization())\n\nMLP_SIGMOID_BatchNorm_SGD_model.add(Dense(hidden_layer_2,\n                                           activation=\"sigmoid\"))\nMLP_SIGMOID_BatchNorm_SGD_model.add(BatchNormalization())\n\nMLP_SIGMOID_BatchNorm_SGD_model.add(Dense(hidden_layer_3,\n                                           activation=\"sigmoid\"))\nMLP_SIGMOID_BatchNorm_SGD_model.add(BatchNormalization())\n\nMLP_SIGMOID_BatchNorm_SGD_model.add(Dense(out_dim,\n                                           activation=\"softmax\"))\n\nMLP_SIGMOID_BatchNorm_SGD_model.summary()","48a9b997":"MLP_SIGMOID_BatchNorm_SGD_model.compile(optimizer=\"sgd\", \n                                        loss=\"binary_crossentropy\",\n                                        metrics=[\"accuracy\"])\n\nhistory = MLP_SIGMOID_BatchNorm_SGD_model.fit(x_train,\n                                              y_train,\n                                              batch_size=batch_size,\n                                              epochs=num_epoch,\n                                              verbose=1,\n                                              validation_data=(x_validation, y_validation),\n                                              callbacks=[early_stopping])","4cece475":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","b5531fc0":"y_pred = MLP_SIGMOID_BatchNorm_SGD_model.predict(x_validation)\nprint_score(y_validation, y_pred)","f74dcf75":"MLP_RELU_DROPOUT_ADAM_model = Sequential()\n\nMLP_RELU_DROPOUT_ADAM_model.add(Dense(hidden_layer_1, \n                                      activation='relu', \n                                      input_shape=(input_dim,)))\nMLP_RELU_DROPOUT_ADAM_model.add(Dropout(0.5))\n\nMLP_RELU_DROPOUT_ADAM_model.add(Dense(hidden_layer_2,\n                                      activation='relu'))\nMLP_RELU_DROPOUT_ADAM_model.add(Dropout(0.5))\n                                \nMLP_RELU_DROPOUT_ADAM_model.add(Dense(hidden_layer_3, \n                                      activation='relu'))\nMLP_RELU_DROPOUT_ADAM_model.add(Dropout(0.5))\n                                \nMLP_RELU_DROPOUT_ADAM_model.add(Dense(out_dim, activation='relu'))\n\nMLP_RELU_DROPOUT_ADAM_model.summary()","4d3d5711":"MLP_RELU_DROPOUT_ADAM_model.compile(optimizer='adam', \n                                    loss='binary_crossentropy', \n                                    metrics=['accuracy'])\n\nhistory = MLP_RELU_DROPOUT_ADAM_model.fit(x_train, \n                                          y_train, \n                                          batch_size=batch_size, \n                                          epochs=num_epoch, \n                                          verbose=1, \n                                          validation_data=(x_validation, y_validation),\n                                          callbacks=[early_stopping])\n","138aad4d":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","268a5ee3":"y_pred = MLP_RELU_DROPOUT_ADAM_model.predict(x_validation)\ny_pred = [[1] if val>0 else [0] for sublist in y_pred for val in sublist]\nprint_score(y_validation, y_pred)","9fc25def":"### MLP_RELU_DROPOUT_SGD\n\nMLP_RELU_DROPOUT_SGD_model = Sequential()\n\nMLP_RELU_DROPOUT_SGD_model.add(Dense(hidden_layer_1, \n                                      activation='relu', \n                                      input_shape=(input_dim,)))\nMLP_RELU_DROPOUT_SGD_model.add(Dropout(0.5))\n\nMLP_RELU_DROPOUT_SGD_model.add(Dense(hidden_layer_2,\n                                      activation='relu'))\nMLP_RELU_DROPOUT_SGD_model.add(Dropout(0.5))\n                                \nMLP_RELU_DROPOUT_SGD_model.add(Dense(hidden_layer_3, \n                                      activation='relu'))\nMLP_RELU_DROPOUT_SGD_model.add(Dropout(0.5))\n                                \nMLP_RELU_DROPOUT_SGD_model.add(Dense(out_dim, activation='relu'))\n\nMLP_RELU_DROPOUT_SGD_model.summary()","f9503199":"MLP_RELU_DROPOUT_SGD_model.compile(optimizer='sgd', \n                                   loss='binary_crossentropy', \n                                   metrics=['accuracy'])\n\nhistory = MLP_RELU_DROPOUT_SGD_model.fit(x_train, \n                                         y_train, \n                                         batch_size=batch_size, \n                                         epochs=num_epoch, \n                                         verbose=1, \n                                         validation_data=(x_validation, y_validation),\n                                         callbacks=[early_stopping])","86aa27d4":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","9aaa695a":"y_pred = MLP_RELU_DROPOUT_SGD_model.predict(x_validation)\ny_pred = [[1] if val>0 else [0] for sublist in y_pred for val in sublist]\nprint_score(y_validation, y_pred)","090ae6aa":"from sklearn.preprocessing import LabelEncoder\n\nx_train = train_df[\"cleaned_data\"]\ny_train = train_df[\"target\"]\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(y_train)\ny_train = y_train.reshape(-1,1)\n\nx_test = test_df[\"cleaned_data\"]","f8642289":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\nmax_words = 1000\nmax_len = 150\ntoken = Tokenizer(num_words=max_words)\ntoken.fit_on_texts(x_train)\nsequences = token.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","32b1a91a":"x_train, x_validation, y_train, y_validation = train_test_split(sequences_matrix, y_train, test_size=0.1)","9ca07cb4":"from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\ndef create_LSTM_model():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='hidden_layer')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(out_dim,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","3f70b33b":"LSTM_model = create_LSTM_model()\nLSTM_model.summary()\nLSTM_model.compile(loss='binary_crossentropy',\n                   optimizer=Adam(),\n                   metrics=['accuracy'])","16163dfe":"history = LSTM_model.fit(x_train,\n                         y_train,\n                         batch_size=batch_size,\n                         epochs=num_epoch,\n                         validation_split=0.1,\n                         callbacks=[early_stopping])","ed17882a":"epochs = [i for i in range(1, len(history.history[\"loss\"])+1)]\nplot_training_history(history, epochs)","9eb9bc57":"y_pred = LSTM_model.predict(x_validation)\ny_pred = [[1] if val>0 else [0] for sublist in y_pred for val in sublist]\nprint_score(y_validation, y_pred)","95d9eb76":"test_sequences = token.texts_to_sequences(x_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","261852ba":"accr = LSTM_model.evaluate(x_validation, y_validation)","69b260c5":"x_test = test_df[\"cleaned_data\"]\nx_test = vectorizer.transform(x_test)\ny_pred = LogisticRegression_model.predict(x_test)\ny_pred","93f9b8ce":"KAGGLE_SUBMISSION_FILE=\"sample_submission.csv\"\n\nsample_submission = pd.DataFrame({'id': test_df.id, 'target': y_pred})\n\nsample_submission.to_csv(KAGGLE_SUBMISSION_FILE,\n                         index=False,\n                         header=True)","74c05194":"### Complement Naive Bayes","aefb7e54":"### Logistic Regression","b7bb2366":"### MLP_SGD","a6697c4d":"### Random Forest","307b5b1a":"y_pred = MLP_BatchNorm_ADAM_model.predict(x_validation)\nprint_score(y_validation, y_pred)### MLP_SIGMOID_Batch_Norm_SGD","ba608006":"### MLP_BatchNorm_ADAM","52b94302":"\n### Bernoulli Naive Bayes","b850a0c7":"### MLP_RELU_DROPOUT_ADAM","d7f90df5":"### Single Layer Perceptron","c581d8c0":"### Multilayer perceptron","36d2428f":"### Support Vector Machine (SVM) (RBF Kernel)","89d43624":"### Support Vector Machine (SVM) (Linear Kernel)","7a76ea6f":"### Naive Bayes","0e18d6f5":"### Multinomial Naive Bayes","88337109":"### Deep Learning","328d283a":"### Multilayer perceptron RELU"}}