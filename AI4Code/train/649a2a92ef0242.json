{"cell_type":{"bdf42cd7":"code","59bbec84":"code","ef2bd13f":"code","d8775766":"code","0f44e539":"code","269274fc":"code","e7bf2dfa":"code","b7dff7c3":"code","5c7b746c":"code","1e70589e":"code","2011b86f":"code","1a683098":"markdown","17acaf94":"markdown","ad24b915":"markdown","c2cf64e7":"markdown","b7d4b428":"markdown"},"source":{"bdf42cd7":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\nimport gc\nimport time\nfrom contextlib import contextmanager","59bbec84":"@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))","ef2bd13f":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train,test,verbose=False):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https:\/\/www.kaggle.com\/kyakovlev\/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations, train_df, test_df, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,train,test):\n    nm = col1+'_'+col2\n    train[nm] = train[col1].astype(str)+'_'+train[col2].astype(str)\n    test[nm] = test[col1].astype(str)+'_'+test[col2].astype(str) \n    encode_LE(nm,train,test)\n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df, test_df):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","d8775766":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","0f44e539":"def comb_mails(emails,us_emails,train,test):\n    for c in ['P_emaildomain', 'R_emaildomain']:\n                train[c + '_bin'] = train[c].map(emails)\n                test[c + '_bin'] = test[c].map(emails)\n\n                train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n                test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n\n                train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n                test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","269274fc":"def love():\n    print('\\n'.join([''.join([(' I_Love_Data_Science_'[(x-y) % len('I_Love_Data_Science_')] if ((x*0.05)**2+(y*0.1)**2-1)**3-(x*0.05)**2*(y*0.1)**3 <= 0 else ' ') for x in range(-30, 30)]) for y in range(15, -15, -1)]))","e7bf2dfa":"def combine():\n     with timer('Combining :'):   \n        print('Combining Start...')\n        # Read train and test data with pd.read_csv():\n        train_id= pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n        test_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\n        train_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\n        test_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n        train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\n        test=pd.merge(test_tr, test_id, on = \"TransactionID\",how=\"left\",left_index=True, right_index=True)\n        del train_id, train_tr, test_id, test_tr\n        test.columns=train.columns.drop(\"isFraud\")\n    \n        \n        \n\n        return train,test","b7dff7c3":"def pre_processing_and_feature_engineering():\n    train,test=combine()   \n    \n    with timer('Preprocessing and Feature Engineering'):\n        print('-' * 30)\n        print('Preprocessing and Feature Engineering start...')\n        print('-' * 10)\n        print(\"After corelation test we can drop this columns\")\n        drop_col=[]\n        train_colmns=train.loc[:,\"V1\":\"V339\"].columns\n        test_columns=test.loc[:,\"V2\":\"V339\"].columns\n        for col1,col2 in zip(train_colmns,test_columns):            \n                if ((train.loc[:,col1:col2].corr().loc[col2].sum()-1)>0.75) & (train[col1].isna().sum()== train[col2].isna().sum()):\n                    print(\"'\"+col2+\"'\",', ',end='')\n                    drop_col.append(col2)\n        train=train.drop(drop_col,axis=1)\n        test=test.drop(drop_col,axis=1)\n        del drop_col\n        print(\"-\")\n        print(\"We drop these columns as well because more than 90 percent of these columns are nan\")\n        for col in train.columns: \n                   if sum(train[col].isnull())\/float(len(train.index)) > 0.90:\n                    print(\"'\"+col+\"'\",', ',end='')\n                    train=train.drop(col,axis=1)\n                    test=test.drop(col,axis=1)\n                            \n        emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n              'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n              'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',     \n              'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n              'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n              'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n              'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n              'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n              'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n              'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n              'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n              'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n              'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n              'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n              'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n              'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n              'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n              'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n              'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\n        us_emails = ['gmail', 'net', 'edu']\n        comb_mails(emails,us_emails,train,test)\n        print(\"-\")\n        print(\"Creating New Features...\")\n        na_low=['TransactionAmt' , 'ProductCD' , 'card1' , 'card2' , 'card3' , 'card4' , 'card5' , 'card6' , 'addr1' ,\n        'addr2' , 'P_emaildomain' , 'C1' , 'C2' , 'C3' , 'C4' , 'C5' , 'C6' , 'C7' , 'C8' , 'C9' , 'C10' , \n        'C11' , 'C12' , 'C13' , 'C14'  , 'M6' ]\n        numeric=train[na_low]._get_numeric_data().columns\n        \n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        encode_CB('card1','addr1',train,test)\n        train['day'] = train.TransactionDT \/ (24*60*60)\n        train['uid'] = train.card1_addr1.astype(str)+'_'+np.floor(train.day-train.D1).astype(str)\n\n        test['day'] = test.TransactionDT \/ (24*60*60)\n        test['uid'] = test.card1_addr1.astype(str)+'_'+np.floor(test.day-test.D1).astype(str)\n        encode_FE(train,test,['uid'])\n        encode_AG(numeric, ['uid'],['mean',\"std\"], train, test)\n        categorical_columns=test.columns.drop(test._get_numeric_data().columns)\n        categorical_columns=categorical_columns.drop('uid')\n        encode_AG2(categorical_columns, ['uid'], train, test)\n        # FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n        encode_FE(train,test,['addr1','card1','card2','card3','P_emaildomain'])\n        # COMBINE COLUMNS CARD1+ADDR1+P_EMAILDOMAIN\n        encode_CB('card1_addr1','P_emaildomain',train, test)\n        # FREQUENCY ENOCDE\n        encode_FE(train,test,['card1_addr1','card1_addr1_P_emaildomain'])\n        # GROUP AGGREGATE\n        encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],train,test,usena=True)\n        del train['uid'], test['uid']\n        print(\"Creating New Features Finished\")\n        print('-' * 10)\n        print('Label Coding and One Hot Encoding Start...')\n        categorical_columns=test.columns.drop(test._get_numeric_data().columns)\n        from sklearn import preprocessing\n        for i in categorical_columns: \n            lbe=preprocessing.LabelEncoder()\n            train[i]=lbe.fit_transform(train[i].astype(str))\n        for i in categorical_columns:    \n            test[i]=lbe.fit_transform(test[i].astype(str))\n        for i in categorical_columns:\n            if (test[i].max()== train[i].max())&(train[i].max()<8):\n                    test = pd.get_dummies(test, columns = [i])\n                    train=pd.get_dummies(train, columns = [i])\n          \n        \n                \n        print('-' * 10)\n        return train,test","5c7b746c":"def modeling():\n    train,test=pre_processing_and_feature_engineering()\n    with timer('Machine Learning '):\n        #feature_drop=[]\n        train_TransactionID= train[\"TransactionID\"]\n        test_TransactionID=test[\"TransactionID\"]\n        X= train.sort_values('TransactionDT').drop([ 'TransactionDT', 'TransactionID'], axis=1)\n        y = train.sort_values('TransactionDT')['isFraud']\n        test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n        del train\n        X=X.drop(\"isFraud\", axis=1)\n        print('-' * 30)\n        print(\"Droping unusefull Features which were determined end of the  first training ...\")\n        #for i in feature_drop:\n               #X=X.drop([i],axis=1)\n                #test=test.drop([i],axis=1)\n               #print(i,', ',end='')\n        print('-')\n        print('-' * 20)\n        print('Machine Learning start ... ')\n     \n        params = {'num_leaves': 491,\n              'min_child_weight': 0.03454472573214212,\n              'feature_fraction': 0.3797454081646243,\n              'bagging_fraction': 0.4181193142567742,\n              'min_data_in_leaf': 106,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': 0.006883242363721497,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": 11,\n              \"metric\": 'auc',\n              \"verbosity\": -1,\n              'reg_alpha': 0.3899927210061127,\n              'reg_lambda': 0.6485237330340494,\n              'random_state': 47}\n        folds = TimeSeriesSplit(n_splits=5)\n\n        aucs = list()\n        feature_importances = pd.DataFrame()\n        feature_importances['feature'] = X.columns\n\n        \n        for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n            \n            print('Training on fold {}'.format(fold + 1))\n\n            trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n            val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n            clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n\n            feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n            aucs.append(clf.best_score['valid_1']['auc'])\n\n            print('Fold {} finished '.format(fold + 1))\n        print('-' * 10)\n        print('Training has finished.')\n        \n        print('Mean AUC:', np.mean(aucs))\n        print('-' * 10)\n        print(\"final model, set the output as a dataframe and convert to csv file named submission.csv\")\n        # clf right now is the last model\n        best_iter = clf.best_iteration\n        clf = lgb.LGBMClassifier(**params, num_boost_round=best_iter)\n        clf.fit(X, y)\n        #set the output as a dataframe and convert to csv file named submission.csv\n        predictions = clf.predict_proba(test)[:, 1]\n        output = pd.DataFrame({ \"TransactionID\" : test_TransactionID, \"isFraud\": predictions })\n        output.to_csv('submission_lgbm.csv', index=False)\n        print('Feature importances...')\n                \n        feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n        feature_importances.to_csv('feature_importances.csv')\n        love()\n             \n        plt.figure(figsize=(16, 16))\n        sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n        plt.title('50 TOP feature importance ');\n        \n       \n     \n        return ","1e70589e":"def main():\n    with timer('Full Model Run '):\n        print(\"Full Model Run Start...\")\n        print('-' * 50)\n        modeling() \n        print('-' * 50)","2011b86f":"if __name__ == \"__main__\":\n     main()","1a683098":"# Combine","17acaf94":"> # Preprocessing and Feature Engineering","ad24b915":"# Helper Functions","c2cf64e7":"# main","b7d4b428":"# Libraries"}}