{"cell_type":{"355f99d8":"code","acda51fa":"code","ca3efcd7":"code","b52f6e55":"code","45ff1a66":"code","f5c8d097":"code","74968b15":"code","c5cf243f":"code","69535446":"code","2eb4d7da":"code","cc46c019":"code","89a91311":"code","6ba030b9":"code","db5987d7":"code","7b11be7c":"code","ea7471ec":"code","3d74a517":"code","96b4286d":"code","c4d3d25a":"code","22f9a0c5":"markdown","3fa03ed6":"markdown","145f9b55":"markdown","c78a06b6":"markdown","45849816":"markdown","2f2c1d26":"markdown","28680523":"markdown","6568f01a":"markdown","08dc2c9b":"markdown","9b76ef61":"markdown","637aff73":"markdown","b84dc9fe":"markdown","1abae88d":"markdown"},"source":{"355f99d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nfrom numpy import random\nimport seaborn as sns\n%matplotlib inline","acda51fa":"dataset = pd.read_csv(\"..\/input\/iris\/Iris.csv\").drop(\"Id\", axis = 1 ,)\n\n\n#independent feature\nindFeat = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]","ca3efcd7":"dataset.head()","b52f6e55":"dataset.info()","45ff1a66":"dataset.describe()","f5c8d097":"dataset[\"Species\"].value_counts()","74968b15":"sns.pairplot(dataset, hue = \"Species\")\nplt.show()","c5cf243f":"plt.figure(figsize = (15,10))\nsns.scatterplot(dataset[\"SepalLengthCm\"], dataset[\"SepalWidthCm\"], hue = dataset[\"Species\"], s = 100)\nplt.show()","69535446":"plt.figure(figsize = (15,10))\nsns.scatterplot(dataset[\"PetalLengthCm\"], dataset[\"PetalWidthCm\"], hue = dataset[\"Species\"], s = 100)\nplt.show()","2eb4d7da":"#Dimensionality reduction\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 1)\ndataDecompose = pd.DataFrame(pca.fit_transform(dataset[[\"PetalLengthCm\", \"PetalWidthCm\"]]))\ndataDecompose[\"Species\"] = dataset[\"Species\"]","cc46c019":"plt.figure(figsize = (20,10))\nsns.pairplot(dataDecompose, hue = \"Species\", height = 5)","89a91311":"def NaiveClassifier(data, input = 0):\n    \n    if input <= data[data[\"Species\"] == \"Iris-setosa\"].max()[0] and input >= data[data[\"Species\"] == \"Iris-setosa\"].min()[0]:\n        return \"Iris-setosa\"\n    \n    elif input < data[data[\"Species\"] == \"Iris-virginica\"].min()[0]:\n        return \"Iris-versicolor\"\n    \n    elif input > data[data[\"Species\"] == \"Iris-versicolor\"].max()[0]:\n        return \"Iris-virginica\"\n    \n    else:\n        return random.choice([\"Iris-versicolor\", \"Iris-virginica\"])\n    ","6ba030b9":"sumasi = 0\nfor x in range(5):\n    test = []\n    for data in dataDecompose[0]:\n        test.append(NaiveClassifier(dataDecompose, data))\n    \n    sumasi +=((pd.Series(test) == dataDecompose[\"Species\"]).astype(int)).sum()","db5987d7":"akurasi = sumasi \/ (150*5)\n\nprint(\"Accuration on Simple Tresholding and Random :\", akurasi)","7b11be7c":"# data partitioning for k=5 fold\n\ndef partitioning(dataset, k = 5):\n    grouped = list(dataset.groupby(\"Species\"))\n    test = []\n    train = []\n    for i in range (k):\n        temp_concat = \"\"\n        temp_concat = pd.concat([grouped[0][1][i*10:(i+1)*10],grouped[1][1][i*10:(i+1)*10],grouped[2][1][i*10:(i+1)*10]])\n        test.append(temp_concat)\n        train.append(pd.DataFrame(dataset[~(dataset.isin(test[i]))].dropna(how = \"all\")))\n    return test, train\n\ntest,train = partitioning(dataset)","ea7471ec":"from sklearn.naive_bayes import GaussianNB\n\ngaussNB = GaussianNB()\n\nacc = 0\nfor i in range(5):\n    NB = gaussNB.fit( X = train[i][indFeat], y = train[i][\"Species\"])\n    pred = NB.predict(test[i][indFeat])\n    acc += (pred == test[i][\"Species\"]).sum()\n\nacc = acc\/150\n\nprint(\"Naive Bayes Accuracy =\",acc)","3d74a517":"from sklearn.neighbors import KNeighborsClassifier\n\nkNN = KNeighborsClassifier(n_neighbors = 7, metric = \"euclidean\")\n\nfor i in range(5):\n    neigh = kNN.fit(X = train[i][indFeat], y = train[i][\"Species\"])\n    pred = neigh.predict(test[i][indFeat])\n    acc += (pred == test[i][\"Species\"]).sum()\n\nacc = acc\/150\nprint(\"kNN Accuracy=\",acc)","96b4286d":"from sklearn import svm\nVecMachine = svm.SVC(gamma = \"auto\")\nfor i in range(5):\n    neigh = VecMachine.fit(X = train[i][indFeat], y = train[i][\"Species\"])\n    pred = VecMachine.predict(test[i][indFeat])\n    acc += (pred == test[i][\"Species\"]).sum()\n\nacc = acc\/150\nprint(\"SVM Accuracy=\",acc)","c4d3d25a":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\n\n\ntree_model = DecisionTreeClassifier()\ntree_model.fit(X = dataset[indFeat], y=dataset['Species'])\n\nprint(\"Decision Tree Accuracy =\",(tree_model.predict(dataset[indFeat])\n       ==dataset[\"Species\"]).sum()\/len(dataset))\n\ndec = export_graphviz(tree_model)\n\nfrom IPython.display import display\n\ndisplay(graphviz.Source(dec))","22f9a0c5":"To prove that a model is not overfitted we can use the k-fold validation as an evaluation method and take the model with highest accuracy, let's use k = 5 in this instance.\n\nThe Classifier used in this datasets are as follows and the hypothesis of choosing it: (even though Iris Datasets are a very beautiful dataset which almost every alghoritm will give a great result)\n\n- Using Gaussian naive Bayes classifier because the data distribution is gaussian distribution\n- kNN becausee of it's very simple alghoritm and having a very balanced class ratio also helps this algorithm a lot","3fa03ed6":"The cells above show summary of the data set, we can conclude that the data is made up of 4 independent features that that are continous those features are :\n\n1. Sepal Length\n2. Sepal Width\n3. Petal Length\n4. Petal Width\n\nand one categorical dependent feature\n\nThe distribution of data are also pretty close where mostly the STD Deviations are under 2, with ranges of each independent in the single digit,this means we might be able to process data without much hassle such as normalization.\n\nThe class distribution also pretty much equals for each class, a very beatiful data","145f9b55":"Using this knowledge we could try to implement Principal Component Analysis to reduce the dimension of Petal into one dimension only feature and use threshold to classify iris, compromised accuracy but it would be the simplest way to model a classifier","c78a06b6":"* It seems that SVM is accurate enough on classifying the Iris Dataset","45849816":"# Creating Simple Classification Model (Treshold + Probability Based Classifier)","2f2c1d26":"One of the most human friendly alghoritm is no other than decision tree, so let's try to create a decision tree classifier and print out the tree so we can gain knowledge on how to differentiate iris species in the wild, how Exciting!","28680523":"# A Report on Iris Classification\n\nThe purpose of this notebook is to create a simple classifier on Iris Dataset, to make things more fun here are some challenges to play with Iris Dataset including:\n\n1. Create the simplest classification model from scratch (with acceptable accuracy, more or less 80%)\n2. Find the most accurate classification model with as little of overfitting possible\n3. Create a human friendly knowledge mined from classification model (minimal accuracy of 90%)","6568f01a":"# Understanding the Data","08dc2c9b":"The model could work even better with weighted probability based on input, as we can see, there are certain value where the probability of it being Virginica is higher than versicolor, but hey it works like a charm, and we are looking for simple!\n\nso simple you got!","9b76ef61":"# Modelling an accurate classifier","637aff73":"From the pairplot above we can observe that petals feature have a better separation than sepal features to better see the comparison between Sepal's and Petal's data distribution we will observe the following graph","b84dc9fe":"# On Simple Classifier:\n\nto make the simples classifier we can 100% predict Iris-setosa using simple thresholding which will give 30% accuracy already, and we can also use thresholding on both versicolor and virginica for certain value range with guaranteed success, the rest of them will do with simple naive bayes and or probability theorem\n\nlet's combine tresholding and simple probability","1abae88d":"# Modelling a human friendly classifier"}}