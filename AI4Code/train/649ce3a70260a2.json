{"cell_type":{"4c650372":"code","4b49ef8a":"code","aaa33087":"code","b6a519bf":"code","5e111669":"code","ff340a9c":"code","8dc9a8e6":"code","570fffa9":"code","1a1f846c":"code","70820a65":"code","ce330611":"code","246eaef7":"code","a6b36340":"code","f4a243b8":"code","301b08f4":"code","5268801f":"code","bfd6de63":"code","acc58d0e":"code","88b12b39":"code","54c6d156":"code","4266630d":"code","b58a144e":"code","7d57f890":"code","ffc5d6e9":"code","302b6f40":"code","fb3933f4":"code","5ac931e5":"code","06f82e72":"code","06384f4b":"code","470db661":"code","14cdf4cc":"code","486a173e":"code","4f5fe42a":"code","7bb48b97":"code","956e5be3":"code","d052ddeb":"code","1b75b508":"code","9f8c072d":"code","199949bb":"code","fff3a1fe":"markdown","35c23dc3":"markdown","81c652c5":"markdown","38f76177":"markdown","c5ce1c6d":"markdown","2d48ca33":"markdown","8eb0d18d":"markdown","2c58877a":"markdown","1891086b":"markdown","fa29db19":"markdown","98a0f0f3":"markdown","c2fd3abb":"markdown","429027c0":"markdown","9ba9242d":"markdown","dfaf0683":"markdown","e783caac":"markdown","5443a4f9":"markdown","7db4f752":"markdown","7151f197":"markdown","2713228b":"markdown","10f05f51":"markdown","3580d2f9":"markdown","9f879555":"markdown","4cdf71d4":"markdown","f3c72f8e":"markdown","8685755d":"markdown","39f59730":"markdown","65f4710f":"markdown","5a00698d":"markdown","5b0e474f":"markdown","a3674232":"markdown","ad3e758c":"markdown","422c84f3":"markdown","e074b823":"markdown","66d8405e":"markdown","c759fcbe":"markdown","77d68c17":"markdown","f9a4792e":"markdown","676b9a9d":"markdown","336fb328":"markdown","73a4d519":"markdown","848dc330":"markdown"},"source":{"4c650372":"#importing libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nfrom os import listdir\nimport seaborn as sns\nfrom operator import itemgetter \nimport matplotlib.image as mpimg\nimport random\nfrom PIL import Image\nimport collections as co\nimport cv2\nimport scipy as sp\nimport copy\nimport plotly.graph_objs as go\nimport plotly.offline as py","4b49ef8a":"trainDir = \"..\/input\/whale-categorization-playground\/train\/train\"\ntestDir=\"..\/input\/whale-categorization-playground\/test\/test\/\"\nvaluesFile= \"..\/input\/whale-categorization-playground\/train.csv\"","aaa33087":"\nlntrd=len(listdir(trainDir))\nlntsd=len(listdir(testDir))\n\nprint(\"number of train files: \"+ str(lntrd))\nprint(\"number of test files: \" + str(lntsd))\ntrainPD=pd.read_csv(valuesFile)\nif lntrd>0:\n    print(\"lengths of test set to train set: %6.2f\" % (lntsd\/lntrd))\n    if trainPD.shape[0]==lntrd:\n        print(\"number of values and length of train set are consistent\")\n    else:\n        print(\"number of values and length of train set are inconsistent\")\nelse:\n    print(\"train set is empty\")","b6a519bf":"FrameID = trainPD.groupby(\"Id\",as_index = False)[\"Image\"].count()\nsortedID_train = FrameID.sort_values(\"Image\",ascending = False)\nidnum=sortedID_train.shape[0]\nprint(idnum)","5e111669":"sortedID_train.head()","ff340a9c":"plt.plot(range(idnum),sortedID_train[\"Image\"])\n\nplt.xlabel(\"sorted index\")\nplt.ylabel(\"frequency of occurence\")\nplt.title(\"frequency of occurence of labels\")","8dc9a8e6":"plt.plot(range(1,idnum),sortedID_train[\"Image\"][1:idnum])\n\nplt.xlabel(\"sorted index\")\nplt.ylabel(\"density\")\nplt.title(\"Density Plot for Labels\")","570fffa9":"plt.plot(range(1,idnum+1),sortedID_train[\"Image\"])\nplt.yscale(\"log\")\nplt.xlabel(\"ID\")\nplt.ylabel(\"Frequency of occurence\")\nplt.title(\"Frequency of occurence: log scale\")","1a1f846c":"plt.plot(range(1,idnum),sortedID_train[\"Image\"][1:idnum])\nplt.yscale(\"log\")\n#plt.yscale(\"log\")\nplt.xlabel(\"ID\")\nplt.ylabel(\"Frequency of occurence\")\nplt.title(\"Frequency of occurence: log scale\")","70820a65":"imnum=25\nplt.rcParams[\"figure.figsize\"] = (70,70)\nfig, subplots = plt.subplots(5,5)\n\nfor i in range(imnum):\n    readImg=mpimg.imread(trainDir+\"\/\"+(listdir(trainDir))[i])\n    subplots[i \/\/ 5,i % 5].imshow(readImg)","ce330611":"readImg=mpimg.imread(trainDir+\"\/\"+(listdir(trainDir))[10])\nplt.rcParams[\"figure.figsize\"] = (10,10)\nplt.imshow(readImg)\nprint(listdir(trainDir)[10])","246eaef7":"trainPD[trainPD[\"Image\"] == \"47841f63.jpg\"]","a6b36340":"imnum=25\nplt.rcParams[\"figure.figsize\"] = (70,70)\nfig, subplots = plt.subplots(5,5)\n\nfor i in range(imnum):\n    readImg=mpimg.imread(testDir+\"\/\"+(listdir(testDir))[i])\n    subplots[i \/\/ 5,i % 5].imshow(readImg)","f4a243b8":"sizedict_train=dict()\nfilelist=listdir(trainDir)\nfor filename in filelist:\n    size=(Image.open(trainDir+\"\/\"+filename)).size\n    if size in sizedict_train:\n        sizedict_train[size]+=1\n    else:\n        sizedict_train[size]=1","301b08f4":"sortpairs_train= sorted(sizedict_train.items(), key = itemgetter(1), reverse = True)","5268801f":"sortsized_train = [sortpairs_train[i][1] for i in range(len(sortpairs_train))]\nsortsized_train = sortsized_train\/ np.sum(sortsized_train)","bfd6de63":"numsizes=len(sizedict_train)\nprint(numsizes)\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.plot(sortsized_train)\n\nplt.xlabel(\"index\")\nplt.ylabel(\"probability\")\nplt.title(\"probability of size\")","acc58d0e":"sortsized_train[0:10]","88b12b39":"numsizes=len(sizedict_train)\nprint(numsizes)\n\nplt.plot(sortsized_train)\nplt.yscale(\"log\")\nplt.xlabel(\"index\")\nplt.ylabel(\"probability\")\nplt.title(\"probability of size\")","54c6d156":"sizedict_test=dict()\nfilelist=listdir(testDir)\nfor filename in filelist:\n    size=(Image.open(testDir+\"\/\"+filename)).size\n    if size in sizedict_test:\n        sizedict_test[size]+=1\n    else:\n        sizedict_test[size]=1","4266630d":"sortpairs_test= sorted(sizedict_test.items(), key = itemgetter(1), reverse = True)","b58a144e":"sortpairs_test[0:10]","7d57f890":"sortsized_test = [sortpairs_test[i][1] for i in range(len(sortpairs_test))]\nsortsized_test = sortsized_test\/ np.sum(sortsized_test)","ffc5d6e9":"numsizes=len(sizedict_test)\nprint(numsizes)\n\nplt.plot(sortsized_test)\n\nplt.xlabel(\"index\")\nplt.ylabel(\"probability\")\nplt.title(\"probability of size\")","302b6f40":"numsizes=len(sizedict_test)\nprint(numsizes)\n\nplt.plot(sortsized_test)\nplt.yscale(\"log\")\nplt.xlabel(\"sorted index\")\nplt.ylabel(\"density\")\nplt.title(\"Density Plot for Labels\")","fb3933f4":"sortsized_test[0:10]","5ac931e5":"def checkrgb(rgb):\n    \n    if len(rgb.shape)==3:\n        return 0\n    else:\n        return 1","06f82e72":"lntd=len(listdir(trainDir))\ngrayscale=[checkrgb(mpimg.imread(trainDir+\"\/\"+(listdir(trainDir))[i])) for i in range(lntd)]","06384f4b":"share_grey_train=np.sum(grayscale)\/len(grayscale)\nshare_grey_train","470db661":"lntd=len(listdir(testDir))\ngrayscale=[checkrgb(mpimg.imread(testDir+\"\/\"+(listdir(testDir))[i])) for i in range(lntd)]","14cdf4cc":"share_grey_test=np.sum(grayscale)\/len(grayscale)\nshare_grey_test","486a173e":"def rgb2grey(rgb): \n    if len(rgb.shape)==3:\n        return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140]) \n    else:\n        return rgb\n\n\ndef transform_image(img, rsc_dim):\n    resized = cv2.resize(img, (rsc_dim, rsc_dim), cv2.INTER_LINEAR)\n    \n    normalized = cv2.normalize(resized, None, 0.0, 1.0, cv2.NORM_MINMAX)\n                         \n    trans = normalized.reshape(1, np.prod(normalized.shape))\n\n    return trans\/np.linalg.norm(trans)","4f5fe42a":"trainImg=[rgb2grey(mpimg.imread(trainDir+\"\/\"+(listdir(trainDir))[i])) for i in range(400)]","7bb48b97":"testImg=[rgb2grey(mpimg.imread(testDir+\"\/\"+(listdir(testDir))[i])) for i in range(400)]","956e5be3":"rsc_dim=100\ngray_all_images_train = [transform_image(img, rsc_dim) for img in trainImg]\ngray_all_images_test  = [transform_image(img, rsc_dim) for img in testImg]","d052ddeb":"gray_imgs_mat_train = np.array(gray_all_images_train).squeeze()\ngray_imgs_mat_test= np.array(gray_all_images_test).squeeze()","1b75b508":"inputtsne=np.concatenate([gray_imgs_mat_train, gray_imgs_mat_test])","9f8c072d":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(inputtsne)","199949bb":"\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (20,20)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nx =tsne[0:400,0]\ny =tsne[0:400,1]\nz =tsne[0:400,2]\n\nax.scatter3D(x, y, z, c='r', marker='o')\n\nx =tsne[400:800,0]\ny =tsne[400:800,1]\nz =tsne[400:800,2]\n\nax.scatter3D(x, y, z, c='b', marker='o')\n\n\nax.set_xlabel('X Label')\nax.set_ylabel('Y Label')\nax.set_zlabel('Z Label')\n\nplt.show()","fff3a1fe":"Let's plot the statistics:","35c23dc3":"There are 4251 unique labels. ","81c652c5":"Around 15 % of images in train set are in greyscale format. ","38f76177":"## Statistics of Labels","c5ce1c6d":"## Clustering of images","2d48ca33":"The test set has 22% of greyscale images. ","8eb0d18d":"Let's take a look at first 25 images: ","2c58877a":"First of all we calculate the frequency of occurence of ID in the files with values:","1891086b":"Imbalance is still evident. More than half of labels have frequency of only 1:","fa29db19":"In such case of extreme distribution we would definetely require augmentation of train dataset.","98a0f0f3":"We set the path to the folders of train and test images together with the path to the file with labels.","c2fd3abb":"## Image Visualization","429027c0":"Let's take a look at statistics of labels.","9ba9242d":"We have prepared the array of transformed images for t-SNE procedure.","dfaf0683":"Without the first element: ","e783caac":"The test has 3527 different sizes of images. The probability of the first three: 13%, 8.3%, 4.34%. ","5443a4f9":"Lets take a look at test set:","7db4f752":"Image transformation functions: ","7151f197":"Again we see that two most common sizes are (1050,600),(1050,700) and (1050,450) ","2713228b":"We can see the images of test set also differ in size a lot and are not consistent in terms of colour spectrum. In the next section we study how significant is this issue. ","10f05f51":"On the next step we estimate the number of files in each dataset, and compare the number of images in training set with number of values in the file with labels","3580d2f9":"We make several conclusions on the basis of exploration:\n\n1) We have to augment our data because there are many classes that are underrepresented. \n\n2) There are thousand of different image sizes in the datasets. We need to recise the picture, and probably try different sizes for transformation.  \n\n3) We also see that there are different color schemes in datasets. We need to greyscale (or red scale) the images before applying the model.\n\n4) We have selected the subsets of training and test images and used projection to low dimensional space and found that projections from both datasets form large claster \nwith few outliers. That means that training and test are quit similar in terms of low diminsional patterns. ","9f879555":"We have 2587 different sizes of images. The first three most frequent sizes occur with frequency: 11.2%, 9.6%, 4.1 %. The plot in logarithm scale:","4cdf71d4":"In this section we estimate how many images are in grayscale format. ","f3c72f8e":"We see that information on the label on yellow space is not consistent with ID. Most likely, this information is useless for identification.  ","8685755d":"The same graph in logarithmic scale:","39f59730":"First of all we will check how many image files we have for training and testing. ","65f4710f":"We can see that three most common sizes are (1050,600), (1050,700) and (1050,450)","5a00698d":"Normalization of arrays:","5b0e474f":"We will now examine how similar are train set images to the images of test set. \n\nIn order to do this we will tranform all images to grayscale and resize them. We choose the size (100,100), which will result in loss of finer details on images.\n\nAfter transformation we select the subset of train and test images and use t-SNE - the machine learning algorithm for dimensionality reduction. The algorithm t-SNE maps high dimensional objects to two- or three-dimensional dots in the way that similar objects are modelled by nearby dots and dissimilar ones by distant dots. ","a3674232":"# Whales identification challenge\n\nWhales are group of aquatic marine mammals, whose closest relatives among land animals are hippopotamuses. After hundreds of years of relentless hunting, whales are now protected by international law. The North Atlantic right whales were close to extinction in the twentieth century, with a population of 450, and the North Pacific grey whale population is ranked Critically Endangered by the IUCN (according to Wiki). For the purpose of preservation of their population, it's important to count the population of whales and monitor their activity. The goal of this notebook is to propose the computer vision approach to identification of whale based on the photo of its humpback. \n\n\n## Files Statistics","ad3e758c":"Let's sort the dictionary by values in descending order","422c84f3":"We are dealing with extemely imbalance dataset. Let's plot without the first class: ","e074b823":"We have larger test set than train set, but this is not an issue if the images of train set are representative set of population.","66d8405e":"\n### Distribution of sizes of images","c759fcbe":"We can see there is a big heterogeneous cluster for both training and test sets and a few quite distant outliers. This means that training and test sets are quite similar after projection to low dimension. ","77d68c17":"# Conclusion of exploration","f9a4792e":"The situation with test set is similar: ","676b9a9d":"The frequency of the first class is over 20 times larger than the one of the second most frequent one. Let's plot the statistics:","336fb328":"### Color scheme","73a4d519":"Let's count the frequency of occurence of different sizes using dictionary: ","848dc330":"We can notice the following issues:\n1. They are not consistent in terms of color spectrum: we can notice several images in black-and-white and most others in full color. \n2. They vary in size a lot. The model's pipeline would require substantial resizing.  \n3. Some of them also have the fields with labels unrelated to the ID. "}}