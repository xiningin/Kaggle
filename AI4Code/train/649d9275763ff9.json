{"cell_type":{"b6f063d0":"code","828333bd":"code","4821e9bf":"code","4ee94149":"code","8d82e765":"code","ae1a9b90":"code","33686c5b":"code","74d254e7":"code","15533888":"code","c701fce0":"code","886a124b":"code","80d7a116":"code","8484cedb":"code","0137c603":"code","c573a823":"markdown","e086eeb4":"markdown","da4da9c0":"markdown","56d01c2e":"markdown","da88d9a5":"markdown","743a9c65":"markdown"},"source":{"b6f063d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport time\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchtext.vocab import Vectors\nfrom torchtext import data\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","828333bd":"def load_file(file_path, device):\n    tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n\n    TEXT = data.Field(sequential=True, lower=True, tokenize=tokenizer, include_lengths=True)\n    LABEL = data.Field(sequential=False, use_vocab=False)\n    \n    datafields = [('text', TEXT), ('label', LABEL)]\n    # Step two construction our dataset.\n    train, valid, test = data.TabularDataset.splits(path=file_path,\n                                                    train=\"Train.csv\", validation=\"Valid.csv\",\n                                                    test=\"Test.csv\", format=\"csv\",\n                                                    skip_header=True, fields=datafields)\n    # because of input dir is read-only we must change the cache path.\n    cache = ('\/kaggle\/working\/.vector_cache')\n    if not os.path.exists(cache):\n        os.mkdir(cache)\n    # using the pretrained word embedding.\n    vector = Vectors(name='\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt', cache=cache)\n    TEXT.build_vocab(train, vectors=vector, max_size=25000, unk_init=torch.Tensor.normal_)\n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), device=device, batch_size=64, \n                                                             sort_key=lambda x:len(x.text), sort_within_batch=True)\n    \n    return TEXT, LABEL, train_iter, valid_iter, test_iter\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nTEXT, LABEL, train_iter, valid_iter, test_iter = load_file('\/kaggle\/input\/imdb-dataset-sentiment-analysis-in-csv-format', \n                                                          device)","4821e9bf":"class SentimentModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n        \n        if bidirectional:\n            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        else:\n            self.fc = nn.Linear(hidden_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        \n        # text : [sen_len, batch_size]\n        embedded = self.dropout(self.embedding(text))\n        \n        # embedded : [sen_len, batch_size, emb_dim]\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        \n        # packed_output : [num_word, emb_dim]     hidden : [num_layers * num_direction, batch_size, hid_dim]    \n        # cell : [num_layers * num_direction, batch_size, hid_dim]\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        #unpacked sequence\n        # output : [sen_len, batch_size, hid_dim * num_directions]\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n        \n        hidden = self.dropout(torch.cat([hidden[-2,:,:], hidden[-1,:,:]], dim=1)).squeeze()    \n        # hidden : [batch_size, hid_dim * num_dir]\n        return self.fc(hidden)\n    \nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\nmodel = SentimentModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)","4ee94149":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","8d82e765":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)\n\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","ae1a9b90":"optimizer = optim.Adam(model.parameters())\n\ncriterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","33686c5b":"def binary_accuracy(preds, y):\n    '''\n    Return accuracy per batch ..\n    '''\n    \n    # round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() \/ len(correct)\n    \n    return acc\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  \/ 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","74d254e7":"def train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for i, batch in enumerate(iterator):\n        \n        text, text_lengths = batch.text\n        \n        predictions = model(text, text_lengths).squeeze(1)\n        \n        loss = criterion(predictions, batch.label.float())\n        \n        acc = binary_accuracy(predictions, batch.label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n        if i % 100 == 99:\n            print(f\"[{i}\/{len(iterator)}] : epoch_acc: {epoch_acc \/ len(iterator):.2f}\")\n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","15533888":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            \n            text, text_lengths = batch.text\n            \n            predictions = model(text, text_lengths).squeeze(1)\n            \n            loss = criterion(predictions, batch.label.float())\n        \n            acc = binary_accuracy(predictions, batch.label)\n            \n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            \n    return epoch_loss \/ len(iterator),  epoch_acc \/ len(iterator)","c701fce0":"N_epoches = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_epoches):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'Sentiment-model.pt')\n        \n    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain  Loss: {train_loss: .3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tValid  Loss: {valid_loss: .3f} | Valid Acc: {valid_acc*100:.2f}%')","886a124b":"model.load_state_dict(torch.load('Sentiment-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iter, criterion)\n\nprint(f\"Test Loss: {test_loss:.3f} | Test Acc : {test_acc*100:.3f}%\")","80d7a116":"def predict_sentiment(model, sentence):\n    model.eval()\n    tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n    tokenized = [tok for tok in tokenizer(sentence)]\n    print(tokenized)\n    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n    length = [len(indexed)]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(1)\n    length_tensor = torch.LongTensor(length).to(device)\n    prediction = torch.sigmoid(model(tensor, length_tensor))\n    return prediction.item()","8484cedb":"predict_sentiment(model, \"This movie is terrible\")","0137c603":"predict_sentiment(model, \"This movie is great\")","c573a823":"## Next Steps\n\nWe finally built a decent sentiment analysis model for movie reviews! In the next we will implement a model that gets comparable accuracy with far fewer parameters and trains much faster.","e086eeb4":"## Preparing Data\n\npacked padded sequences, which will make our RNN only process the non-padded elements of our sequence\n\nTo use the packed padded sequences, we have to tell the RNN how long the actual sequences are.\n\nsetting **include_lengths=True** for our TEXT field. \n\nThis will cause batch.text to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences.","da4da9c0":"**Thanks to pytorch-sentiment-analysis tutorial on Github.**\n\nhttps:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/\n\nAnd Because of the constraint of the kaggle kernels, some modification was made.","56d01c2e":"## Train the model\n\nThe only change here is changing the optimizer from SGD to Adam. \n\nSGD updates all parameters with the same learning rate and choosing this learning rate can be tricky. \n\nAdam adapts the learning rate for each parameter, giving parameters that are update more frequently lower learning rate and parameters that are update infrequently higher learning rates. \n","da88d9a5":"## User Input \n\nWe can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provide should also be movie reviews.\n\nWhen using a model for interface it should always be in evaluation mode. We explicitly set it to avoid tasks.\n\nOur predict_sentiment function does a few things:\n\n- sets the model to eval\n- tokenizes the sentences i.e. splits it from a raw string into a list of tokens.\n- indexes the token by converting them into their integer representation from our vocabulary\n- get length of our sequence.\n- converts the indexes, list->tensor\n- add a batch dimension by unsqueezeing\n- converts the length into a tensor\n- squashes the output prediction from a real number to 0 ~ 1 with the sigmoid function \n- convert the tensor holding a single value into an integer with the item() method ","743a9c65":"## Build the Model\n\nUsing LSTM instead of RNN. (RNN suffers from vanishing gradient problem.) "}}