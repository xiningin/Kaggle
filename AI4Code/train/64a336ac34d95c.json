{"cell_type":{"1d0f88f6":"code","c088f95e":"code","b013a508":"code","dd451f7e":"code","20be8c09":"code","35669497":"code","6aa72368":"code","aac57be4":"code","053f8d58":"code","618ba496":"code","ef4ff28f":"code","fffd754c":"code","ed95585d":"code","aab85ed6":"code","d36a6182":"code","73e81956":"code","716117c1":"code","bc6558fc":"code","8e73a41d":"code","a4c1aa21":"code","e94f044b":"code","159d9149":"code","a2c5398f":"code","e9c51e55":"code","16813b46":"code","9e146516":"code","46655bd4":"code","8be9247e":"code","91747f3e":"code","3f492ccd":"code","62d6fa77":"code","2ebbc890":"code","04819bce":"code","626a873b":"markdown","c33adf29":"markdown","3d8f04b9":"markdown","f8f23f0c":"markdown","9481f2fc":"markdown","05ca61a3":"markdown","95d24082":"markdown","5bbf5db0":"markdown","99e8786d":"markdown","cb5781f8":"markdown","911e9969":"markdown","a02285ac":"markdown","5059752e":"markdown","6cda5acd":"markdown","b6966499":"markdown"},"source":{"1d0f88f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#Importing libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\nimport matplotlib.pyplot as plt#visualization\nfrom matplotlib import cm as cm\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer,auc,f1_score,precision_score,recall_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c088f95e":"#Importing datasets from the defined path\ntelecom_data = pd.read_excel('..\/input\/Customer_Churn.xlsx')\n\ntelecom_data.head()","b013a508":"telecom_data.describe()","dd451f7e":"#Display the shape of the dataframe to figure out the no. of rows and columns.\nprint (\"Dimension   : \" ,telecom_data.shape)\nprint (\"\\nFeatures : \\n\" ,telecom_data.columns.tolist())\nprint (\"\\nMissing values :  \",telecom_data.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",telecom_data.nunique())","20be8c09":"sns.heatmap(telecom_data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","35669497":"#Dropping null values from total charges\ntelecom_data['TotalCharges'] = telecom_data[\"TotalCharges\"].replace(\" \",np.nan)\ntelecom_data = telecom_data[telecom_data[\"TotalCharges\"].notnull()]\ntelecom_data = telecom_data.reset_index()[telecom_data.columns]\n\n#Data manipulation for columns\n# 1.Changing the value \"No internet service\" to \"No\" for columns in cols \n# 2.Changing Boolean value for \"SeniorCitizen\" \ncols = ['OnlineSecurity','OnlineBackup', 'DeviceProtection',\n                'TechSupport','StreamingTV', 'StreamingMovies']\n\nfor i in cols : \n    telecom_data[i]  = telecom_data[i].replace({'No internet service' : 'No'})\n    \ntelecom_data[\"MultipleLines\"] = telecom_data[\"MultipleLines\"].replace({'No phone service' : 'No'})  \ntelecom_data[\"SeniorCitizen\"] = telecom_data[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})\n\nchurn     = telecom_data[telecom_data[\"Churn\"] == \"Yes\"]\nnot_churn = telecom_data[telecom_data[\"Churn\"] == \"No\"]\n\ntelecom_data","6aa72368":"#Checking ratio of Churn to Non Churn in pie chart\nfig, ax = plt.subplots()\nlab = telecom_data[\"Churn\"].value_counts().keys().tolist()\nsizes= telecom_data[\"Churn\"].value_counts().values.tolist()\nax.pie(sizes, labels=lab, autopct='%1.1f%%', shadow=True)\nax.axis('equal')\nplt.show()","aac57be4":"#count of online services availed is identified by creating a new column \"Count_OnlineServices\" and then visualize those number of customers against it.\nplt.figure(figsize=(12,6))\ntelecom_data['Count_OnlineServices'] = (telecom_data[['OnlineSecurity', 'DeviceProtection', 'StreamingMovies', 'TechSupport',\n       'StreamingTV', 'OnlineBackup']] == 'Yes').sum(axis=1)\n\nax = sns.countplot(x='Count_OnlineServices', hue='Churn' , data=telecom_data)\nax.set_title('Number of Services Availed Vs Churn', fontsize=20)\nax.set_ylabel('Number of Customers', fontsize=15)\nax.set_xlabel('Number of Online Services', fontsize=15)","053f8d58":"#Calculating Average Monthly charged for those customer with respect to their number online services\naverage_monthly_data = telecom_data.groupby('Count_OnlineServices', as_index=False)[['MonthlyCharges']].mean()\n\nplt.figure(figsize=(12,6))\nax = sns.barplot(y='MonthlyCharges', x='Count_OnlineServices', data=average_monthly_data)\nax.set_xlabel('Number of Online Services Availed', fontsize=15)\nax.set_ylabel('Average Monthly Charges',  fontsize=15)\nax.set_title('Avg Monthly Charges vs Number of Services', fontsize=20)","618ba496":"#Visualizing Percentage of Churn based on their tenure(in months)\ntenure_month = telecom_data.replace('Yes', 1).replace('No', 0).groupby('tenure', as_index=False)[['Churn']].mean()\n\n\nplt.figure(figsize=(20,6))\n\nax = sns.barplot(x='tenure', y='Churn', data = tenure_month)\nax.set_title('Churn Percentage Over Tenure months', fontsize=20)\nax.set_ylabel('Percentage of Churn', fontsize = 15)\nax.set_xlabel('Tenure in Months', fontsize = 15)\n","ef4ff28f":"#Checking whether any particualr type of Internet service has any impact \nsns.catplot(x=\"InternetService\", y=\"tenure\",hue=\"Churn\",kind=\"box\",data=telecom_data);","fffd754c":"\nplt.figure(figsize=(12,6))\nax = sns.countplot(x=\"Churn\", hue=\"InternetService\", data=telecom_data);\nax.set_ylabel('Number of Customers', fontsize = 15)\nax.set_xlabel('Churn', fontsize = 15)\nax.set_title('Churn By Internet Service Type', fontsize=20)","ed95585d":"#To assess the strength and direction of the linear relationships between pairs of variables using Heat map\ndef corelation(telecom_data):\n    # Create Correlation df\n    corr = telecom_data.corr()\n    # Plot figsize\n    fig, ax = plt.subplots(figsize=(16, 16))\n    # Generate Color Map\n    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n    # Drop self-correlations\n    dropSelf = np.zeros_like(corr)\n    dropSelf[np.triu_indices_from(dropSelf)] = True# Generate Color Map\n    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n    # Generate Heat Map, allow annotations and place floats in map\n    sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n    # Apply xticks\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    # Apply yticks\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    # show plot\n    plt.show()\n\ncorelation(telecom_data)","aab85ed6":"#Churn By Count against Tenure \nfig, ax = plt.subplots(figsize=(20,10))\nsns.catplot(ax=ax ,x=\"tenure\", hue=\"Churn\", kind=\"count\", data=telecom_data);","d36a6182":"\nplt.figure(figsize=(20,6))\naverage_totalcharge_data = telecom_data.groupby('tenure', as_index=False)[['TotalCharges']].mean()\n\nax = sns.barplot(y='TotalCharges', x='tenure', data=average_totalcharge_data)\nax.set_xlabel('Number of Online Services Availed', fontsize=15)\nax.set_ylabel('Average Monthly Charges',  fontsize=15)\nax.set_title('Avg Monthly Charges vs Number of Services', fontsize=20)","73e81956":"telecom_data_copy = telecom_data.copy()\n\nId_col     = ['customerID']\n#Target columns\ntarget_col = [\"Churn\"]\n#telecom_data_copy = telecom_data_copy.drop('Churn',axis=1)\n#categorical columns\ncat_cols   = telecom_data_copy.nunique()[telecom_data_copy.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols   = [x for x in telecom_data_copy.columns if x not in cat_cols + target_col + Id_col]\n#Binary columns with 2 values\nbin_cols   = telecom_data_copy.nunique()[telecom_data_copy.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\ntelecom_data_copy","716117c1":"print (\"Dimension   : \" ,telecom_data_copy.shape)\nprint (\"\\nFeatures : \\n\" ,telecom_data_copy.columns.tolist())\nprint (\"\\nMissing values :  \",telecom_data_copy.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",telecom_data_copy.nunique())\nprint (\"\\nCategorical columns : \\n\",cat_cols)\nprint (\"\\nNumerical columns : \\n\",num_cols)\nprint (\"\\nBinary valued columns : \\n\",bin_cols)\nprint (\"\\nMultivalued columns : \\n\",multi_cols)","bc6558fc":"#Duplicating columns for multi value columns\ntelecom_data_copy = pd.get_dummies(data = telecom_data_copy,columns = multi_cols )","8e73a41d":"#label encoder for binary valud columns\nle = LabelEncoder()\nfor i in bin_cols :\n    telecom_data_copy[i] = le.fit_transform(telecom_data_copy[i])\n\ntelecom_data_copy.shape","a4c1aa21":"#Chi-Square test is a statistical method to determine if two categorical variables have a significant correlation between them.\nchi2_selector = SelectKBest(chi2, k=2)\ndf_chi = telecom_data_copy.copy()\n#Drop values for non negative columns and id column\ndf_chi = df_chi.drop(num_cols + Id_col ,axis=1)\n\ndf_chi","e94f044b":"X_kbest = chi2_selector.fit_transform(df_chi, telecom_data_copy[target_col])\nX_kbest.shape","159d9149":"#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(telecom_data_copy[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\ntelecom_data_copy = telecom_data_copy.drop(columns = num_cols,axis = 1)\ntelecom_data_copy = telecom_data_copy.merge(scaled,left_index=True,right_index=True,how = \"left\")\ntelecom_data_copy","a2c5398f":"#To get better interrelation between features after scaling\ncorelation(telecom_data_copy)","e9c51e55":"# To convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components.\n\n\n#Dataframe without chi square \nprechi2_data = telecom_data_copy[[i for i in telecom_data_copy.columns if i not in Id_col + target_col]]\n# Adding numeriacal columns which were dropped during chi squared statistics implementation\ndropped_column = ['tenure','Count_OnlineServices','MonthlyCharges']\ndroppedcolumnDF = telecom_data_copy[[i for i in telecom_data_copy.columns if i in dropped_column]]\nchi2dataframe = pd.DataFrame(X_kbest)\n#Dataframe with chi square\npostchi2_data = pd.concat([chi2dataframe, droppedcolumnDF], axis=1)\n\n#Sample feature and Target feature for PCA\nX = postchi2_data\nY = telecom_data_copy[target_col]\npca = PCA().fit(X)","16813b46":"#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Dataset Explained Variance')\nplt.show()","9e146516":"#PCA implentation with 2 component\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(X)\npca_data = pd.DataFrame(data = principal_components\n             , columns = ['principal component 1', 'principal component 2'])\n\n#Final dataframe for model implementation\nfinalDf = pd.concat([pca_data, telecom_data_copy[['Churn']]], axis = 1)\n","46655bd4":"finalDf.shape\nfinalDf.columns","8be9247e":"finalDF_X = finalDf.drop(\"Churn\",axis=1)\nfinalDF_Y = finalDf[\"Churn\"]","91747f3e":"fig = plt.figure(figsize = (15,15))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\nfinalDf[\"Churn\"] = finalDf[\"Churn\"].replace({1:\"Churn\",0:\"Not Churn\"})\ntargets = [\"Churn\",\"Not Churn\"]\ncolors = ['r', 'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['Churn'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","3f492ccd":"#The explained variance tells you how much information (variance) can be attributed to each of the principal components\npca.explained_variance_ratio_","62d6fa77":"X_train, X_test, Y_train, Y_test = train_test_split(finalDF_X, finalDF_Y, test_size=0.30, random_state=0)\n#KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach.\nknn = KNeighborsClassifier(n_neighbors= 3)\n \nknn.fit(X_train, Y_train)","2ebbc890":"prediction = knn.predict(X_test)","04819bce":"# Calculate the fpr and tpr for all thresholds of the classification\n# Predict the class labels for the provided data\nf1_score = round(f1_score(Y_test, prediction), 2)\nrecall_score = round(recall_score(Y_test, prediction), 2)\nfpr, tpr, thresholds = roc_curve(Y_test, prediction)\nroc_auc = auc(fpr, tpr)\n\nprint(' ')\nprint(\"Sensitivity\/Recall : {recall_score}\".format(recall_score = recall_score))\nprint(' ')\nprint(\"F1 Score : {f1_score}\".format(f1_score = f1_score))\nprint(' ')\nprint(\"AUC-ROC score : \", roc_auc_score(Y_test, prediction))\nprint(' ')\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\nprint(' ')\nprint(\"Confusion Matrix : \")\nprint(confusion_matrix(Y_test, prediction))\nprint(' ')\nprint(\"Classification Report : \")\nprint(classification_report(Y_test, prediction))\nprint(' ')\nprint(\"Accuracy : \", round(accuracy_score (Y_test, prediction)*100,2), \"%\")","626a873b":"## Data Cleaning\n","c33adf29":"# Chi square implementation","3d8f04b9":"# Corelation matrix after data refactoring","f8f23f0c":"## Average Monthly Charges vs Number of Services\n","9481f2fc":"# Data Preprocessing","05ca61a3":"## Churn vs Service Type","95d24082":"# KNN classifier\n","5bbf5db0":"## Avg total charges vs tenure","99e8786d":"# Data Loading and Data Analysis\n","cb5781f8":"\n# Customer Churn Prediction\n## Problem Statement\n### Acquiring a new customer typically costs anywhere between 5 and 25 times as much as it does to retain an existing customer, making it critical for any business to minimize customer churn. That\u2019s why it\u2019s so important for product managers to take a proactive approach to user retention, to both minimize acquisition cost and increase the chances of mass adoption over the long haul. But any product manager knows that they\u2019re going to experience serious roadblocks to adoptions, and in turn, retention. The more you can forecast churn, the better you can prevent it. \n## What to achieve?\n### With machine learning models,what\u2019s specifically causing churn.","911e9969":"## Co-relation matrix\n","a02285ac":"# PCA","5059752e":"## Tenure(in months) vs churn\n","6cda5acd":"## Churn vs Non-churn plot\n","b6966499":"## Online customer vs churn plot\n"}}