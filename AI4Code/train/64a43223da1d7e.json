{"cell_type":{"7295e937":"code","cc9ccc09":"code","4e91b6ab":"code","5044ef8a":"code","682df813":"code","c3dc6acd":"code","a48798f1":"code","c0f06726":"code","4e0e7b90":"code","93a6782c":"code","4b1bc439":"code","3fdbf26a":"code","ce1dc383":"code","437609e6":"code","0e23f016":"code","7463b562":"code","53d07100":"code","866f9667":"code","7ffe3e50":"code","9fc82314":"code","aa7368b2":"code","a8ff24af":"code","5723d80e":"code","21cebb01":"code","47bd520a":"code","668a6f7f":"code","91f9c49f":"code","8f17182f":"code","4996c221":"code","b4321089":"code","6536da0a":"code","cec71f48":"code","b8386202":"code","7c24a96e":"markdown","4510006f":"markdown","4f9f8178":"markdown","0ecc2392":"markdown","d8f3898f":"markdown","65580848":"markdown","78c8e1b8":"markdown","bbd8cbac":"markdown","58224462":"markdown","7a4792d1":"markdown","b0ac4037":"markdown","5359b08d":"markdown","d4f1b838":"markdown","a01791cc":"markdown","3ad25c09":"markdown","c7361f69":"markdown","b43dc293":"markdown","54769efe":"markdown"},"source":{"7295e937":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc9ccc09":"# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\nimport matplotlib                  # 2D Plotting Library        \nimport geopandas as gpd            # Python Geospatial Data Library\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n#import nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize\n\n#preprocessing\nfrom nltk.corpus import stopwords  #stopwords\nfrom nltk import word_tokenize,sent_tokenize # tokenizing\nfrom nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n\n# for named entity recognition (NER)\nfrom nltk import ne_chunk\n\n# vectorizers for creating the document-term-matrix (DTM)\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n#stop-words\nstop_words=set(nltk.corpus.stopwords.words('english'))","4e91b6ab":"df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","5044ef8a":"df.head()","682df813":"index = df.index\nnumber_of_rows = len(index)\nprint(number_of_rows)","c3dc6acd":"#df.drop(['sentiment'],axis=1,inplace=True)","a48798f1":"new1 = df[['review']].iloc[:10000].copy()","c0f06726":"print (new1)","4e0e7b90":"def clean_text(review):\n    le=WordNetLemmatizer()\n    word_tokens=word_tokenize(review)\n    tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n    cleaned_text=\" \".join(tokens)\n    return cleaned_text","93a6782c":"#it takes time\nnew1['cleaned_text']=new1['review'].apply(clean_text)","4b1bc439":"new1.head()","3fdbf26a":"new1.drop(['review'],axis=1,inplace=True)\n","ce1dc383":"vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)","437609e6":"vect_text=vect.fit_transform(new1['cleaned_text'])","0e23f016":"print(vect.get_feature_names())","7463b562":"print(vect_text.shape)\ntype(vect_text)\n","53d07100":"\nidf=vect.idf_","866f9667":"dd=dict(zip(vect.get_feature_names(), idf))\nl=sorted(dd, key=(dd).get)\n# print(l)\nprint(l[0],l[-1])\nprint(dd['like'])\nprint(dd['success'])  ","7ffe3e50":"new1['cleaned_text'].head()","9fc82314":"lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n\nlsa_top=lsa_model.fit_transform(vect_text)","aa7368b2":"print(lsa_top[0])\nprint(lsa_top.shape)  # (no_of_doc*no_of_topics)","a8ff24af":"l=lsa_top[0]\nprint(\"Document 0 :\")\nfor i,topic in enumerate(l):\n  print(\"Topic \",i,\" : \",topic*100)","5723d80e":"print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\nprint(lsa_model.components_)","21cebb01":"# most important words for each topic\nvocab = vect.get_feature_names()\n\nfor i, comp in enumerate(lsa_model.components_):\n    vocab_comp = zip(vocab, comp)\n    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_words:\n        print(t[0],end=\" \")\n    print(\"\\n\")","47bd520a":"from sklearn.decomposition import LatentDirichletAllocation\nlda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1) \n# n_components is the number of topics","668a6f7f":"lda_top=lda_model.fit_transform(vect_text)","91f9c49f":"print(lda_top.shape)  # (no_of_doc,no_of_topics)\nprint(lda_top[0])","8f17182f":"sum=0\nfor i in lda_top[0]:\n  sum=sum+i\nprint(sum)","4996c221":"# composition of doc 0 for eg\nprint(\"Document 0: \")\nfor i,topic in enumerate(lda_top[0]):\n  print(\"Topic \",i,\": \",topic*100,\"%\")","b4321089":"print(lda_model.components_[0])\nprint(lda_model.components_.shape)  # (no_of_topics*no_of_words)","6536da0a":"# most important words for each topic\nvocab = vect.get_feature_names()\n\nfor i, comp in enumerate(lda_model.components_):\n    vocab_comp = zip(vocab, comp)\n    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:5]\n    print(\"Topic \"+str(i)+\": \")\n    for t in sorted_words:\n        print(t[0],end=\" \")\n    print(\"\\n\")","cec71f48":"from wordcloud import WordCloud\n# Generate a word cloud image for given topic\ndef draw_word_cloud(index):\n  imp_words_topic=\"\"\n  comp=lda_model.components_[index]\n  vocab_comp = zip(vocab, comp)\n  sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:25]\n  for word in sorted_words:\n    imp_words_topic=imp_words_topic+\" \"+word[0]\n\n  wordcloud = WordCloud(width=900, height=600).generate(imp_words_topic)\n  plt.figure( figsize=(5,5))\n  plt.imshow(wordcloud)\n  plt.axis(\"off\")\n  plt.tight_layout()\n  plt.show()","b8386202":"# topic 0\ndraw_word_cloud(0)","7c24a96e":"### EXTRACTING THE FEATURES AND CREATING THE DOCUMENT-TERM-MATRIX ( DTM )\nA document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n\nSome important points:-\n\n1) LSA is generally implemented with Tfidf values everywhere and not with the Count Vectorizer.\n\n2) max_features depends on your computing power and also on eval. metric (coherence score is a metric for topic model). Try the value that gives best eval. metric and doesn't limits processing power.\n\n3) Default values for min_df & max_df worked well.\n\n4) Can try different values for ngram_range.","4510006f":"Now we can get a list of the important words for each of the 10 topics as shown. For simplicity here I have shown 10 words for each topic.","4f9f8178":"Latent Semantic Analysis (LSA)\nThe first approach that I have used is the LSA. LSA is basically singular value decomposition.\n\nSVD decomposes the original DTM into three matrices S=U.(sigma).(V.T). Here the matrix U denotes the document-topic matrix while (V) is the topic-term matrix.\n\nEach row of the matrix U(document-term matrix) is the vector representation of the corresponding document. The length of these vectors is the number of desired topics. Vector representation for the terms in our data can be found in the matrix V (term-topic matrix).\n\nSo, SVD gives us vectors for every document and term in our data. The length of each vector would be k. We can then use these vectors to find similar words and similar documents using the cosine similarity method.\n\nWe can use the truncatedSVD function to implement LSA. The n_components parameter is the number of topics we wish to extract. The model is then fit and transformed on the result given by vectorizer.\n\nLastly note that LSA and LSI (I for indexing) are the same and the later is just sometimes used in information retrieval contexts.","0ecc2392":"> As you can see amount of data is really huge so we will take small set of rows out of it so that it takes less time","d8f3898f":"We can now see the most frequent and rare words in the cleaned_text column based on idf score. The lesser the value; more common is the word in the column.","65580848":"> Similalry for other documents we can do this. However note that values don't add to 1 as in LSA it is not probabiltiy of a topic in a document.","78c8e1b8":"As we can see Topic 0 is dominantly present in document 0.","bbd8cbac":"### TOPIC MODELLING","58224462":"> we are only interested in Text column","7a4792d1":" To understand LDA in detail please refer to [this](https:\/\/www.analyticsvidhya.com\/blog\/2016\/08\/beginners-guide-to-topic-modeling-in-python\/) blog.\n ","b0ac4037":"> Note that the values in a particular row adds to 1. This is beacuse each value denotes the % contribution of the corressponding topic in the document.","5359b08d":"### DATA CLEANING & PRE-PROCESSING\nHere I have done the data pre-processing. We can use any among the lemmatization and the stemming but i prefer to use lemmatiation. Also the stop words have been removed along with the words with length shorter than 3 characters to reduce some stray words.","d4f1b838":"Most important words for a topic. (say 5 this time.)","a01791cc":"To better visualize words in a topic we can see the word cloud. For each topic top 25 words are plotted.","3ad25c09":"> We can therefore see that on the basis of the idf value , 'like' is the least frequent word while 'success' is most frequently occuring word.","c7361f69":"### Latent Dirichlet Allocation (LDA)\nLDA is the most popular technique.The topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.[](http:\/\/)","b43dc293":"> Now drop the unpre-processed column.","54769efe":"You can clearly see the difference after removal of stopwords and some shorter words."}}