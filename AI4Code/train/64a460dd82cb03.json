{"cell_type":{"e8464b3a":"code","7f9958bd":"code","e61e09ce":"code","c4555d09":"code","5f60852f":"code","5d67e893":"code","99a02528":"code","5d23b1f6":"code","8935a428":"code","39585d4e":"code","af75ce43":"code","a46aa1eb":"code","db97b78b":"code","14b5ac60":"code","22b25998":"code","7a9302d6":"code","ddd779a9":"code","8f59bbe2":"code","ff1487a9":"code","d02f6447":"code","14074487":"code","b565db94":"code","9033d8f9":"code","811b5c52":"code","2d5272fd":"code","abe0a599":"code","79c39b1a":"code","4fc41785":"code","1e2e0aba":"code","a3db7f9b":"code","92100033":"code","32444139":"code","a7a3b8ca":"code","079c54ea":"code","12dd25bf":"code","cc9e1cc5":"code","fbb547d8":"code","d3dcf372":"code","b1c0aef9":"code","1fe38fd3":"code","73327257":"code","cc95b5e1":"code","bdda174f":"code","ed07ea7a":"code","74042f24":"code","aa04b09e":"markdown","62c3c873":"markdown","5ff54c38":"markdown","efc03f70":"markdown","3a8b3c35":"markdown","654edab3":"markdown","684f16f2":"markdown","2fb5809c":"markdown","22701206":"markdown","728974be":"markdown","ca1dbc2e":"markdown","ae7110f3":"markdown","0d4bd067":"markdown","d5836602":"markdown","555d8795":"markdown","94f28058":"markdown","0a1d5c5a":"markdown","807266b1":"markdown","95ba011b":"markdown","4478cd95":"markdown","fb55679a":"markdown"},"source":{"e8464b3a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7f9958bd":"# Importa\u00e7\u00e3o dos pacotes\nimport pandas as ps\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport lightgbm as lgb","e61e09ce":"treino = pd.read_csv(\"..\/input\/kernelrandomensemble\/dataset_treino.csv\")\nteste = pd.read_csv(\"..\/input\/kernelrandomensemble\/dataset_teste.csv\")\nsubmission = pd.read_csv(\"..\/input\/kernelrandomensemble\/sample_submission.csv\")\ntransacoeshist = pd.read_csv(\"..\/input\/kernelrandomensemble\/transacoes_historicas.csv\")\nnewtransacoeshist = pd.read_csv(\"..\/input\/kernelrandomensemble\/novas_transacoes_comerciantes.csv\")","c4555d09":"treino.head()","5f60852f":"treino.info()","5d67e893":"teste.head()","99a02528":"teste.info()","5d23b1f6":"transacoeshist.head()","8935a428":"transacoeshist.info()","39585d4e":"newtransacoeshist.head()","af75ce43":"newtransacoeshist.info()","a46aa1eb":"#Representando numericamente valores da vari\u00e1vel \"first_active_month\"\ntreino['first_active_month']=pd.to_datetime(treino['first_active_month'])\nteste['first_active_month']=pd.to_datetime(teste['first_active_month'])\ntreino[\"ano\"] = treino[\"first_active_month\"].dt.year\nteste[\"ano\"] = teste[\"first_active_month\"].dt.year\ntreino[\"mes\"] = treino[\"first_active_month\"].dt.month\nteste[\"mes\"] = teste[\"first_active_month\"].dt.month","db97b78b":"treino.head(2)","14b5ac60":"teste.head(2)","22b25998":"treino.describe()","7a9302d6":"teste.describe()","ddd779a9":"#Distribui\u00e7\u00e3o da vari\u00e1vel target\nfig = plt.figure(figsize=(15,5))\nsns.distplot(treino['target'])","8f59bbe2":"fig = plt.figure(figsize=(15,5))\nsns.boxplot(data = treino[['target']], orient = \"h\")","ff1487a9":"sns.countplot(x = \"feature_1\", data = treino, palette = \"Greens_d\")","d02f6447":"sns.stripplot(x = \"feature_1\", y = \"target\", data = treino)","14074487":"sns.countplot(x = \"feature_2\", data = treino, palette = \"Greens_d\")","b565db94":"sns.stripplot(x = \"feature_2\", y = \"target\", data = treino)","9033d8f9":"sns.countplot(x = \"feature_3\", data = treino, palette = \"Greens_d\")","811b5c52":"sns.stripplot(x = \"feature_3\", y = \"target\", data = treino)","2d5272fd":"sns.countplot(x = \"mes\", data = treino, palette = \"Greens_d\")","abe0a599":"sns.stripplot(x = \"mes\", y = \"target\", data = treino)","79c39b1a":"treino.isnull().sum()","4fc41785":"teste.isnull().sum()","1e2e0aba":"#Substituindo valores nulos dos dados de teste pelos valores mais frequentes\nfrom statistics import mode\nteste['first_active_month'].fillna(mode(teste['first_active_month']), inplace=True)\nteste['ano'].fillna(mode(teste['ano']), inplace=True)\nteste['mes'].fillna(mode(teste['mes']), inplace=True)","a3db7f9b":"teste.isnull().sum()","92100033":"newtransacoeshist.isnull().sum()","32444139":"newtransacoeshist['category_2'].fillna(value=0.1,inplace=True)\nnewtransacoeshist['category_3'].fillna('B',inplace=True)\nnewtransacoeshist['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","a7a3b8ca":"transacoeshist.isnull().sum()","079c54ea":"transacoeshist['category_2'].fillna(value=0.1,inplace=True)\ntransacoeshist['category_3'].fillna('B',inplace=True)\ntransacoeshist['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)","12dd25bf":"hist = transacoeshist.groupby([\"card_id\"])\nhist= hist[\"purchase_amount\"].size().reset_index()\nhist.columns = [\"card_id\", \"hist_transactions\"]\ntreino = pd.merge(treino,hist, on=\"card_id\", how=\"left\")\nteste = pd.merge(teste,hist, on=\"card_id\", how=\"left\")\n\nhist = transacoeshist.groupby([\"card_id\"])\nhist = hist[\"purchase_amount\"].agg(['sum','mean','max','min','std']).reset_index()\nhist.columns = ['card_id','sum_hist_tran','mean_hist_tran','max_hist_tran','min_hist_tran','std_hist_tran']\ntreino = pd.merge(treino,hist,on='card_id',how='left')\nteste = pd.merge(teste,hist,on='card_id',how='left')","cc9e1cc5":"merchant = newtransacoeshist.groupby([\"card_id\"])\nmerchant= merchant[\"purchase_amount\"].size().reset_index()\nmerchant.columns = [\"card_id\", \"merchant_transactions\"]\ntreino = pd.merge(treino,merchant, on=\"card_id\", how=\"left\")\nteste = pd.merge(teste,merchant, on=\"card_id\", how=\"left\")\n\nmerchant= newtransacoeshist.groupby([\"card_id\"])\nmerchant= merchant[\"purchase_amount\"].agg(['sum','mean','max','min','std']).reset_index()\nmerchant.columns=['card_id','sum_merchant_tran','mean_merchant_tran','max_merchant_tran','min_merchant_tran','std_merchant_tran']\ntreino=pd.merge(treino,merchant,on='card_id',how='left')\nteste=pd.merge(teste,merchant,on='card_id',how='left')","fbb547d8":"#treino\ndumtreino_feature_1 = pd.get_dummies(treino['feature_1'],prefix = 'f1_')\ndumtreino_feature_2 = pd.get_dummies(treino['feature_2'],prefix = 'f2_')\ndumtreino_feature_3 = pd.get_dummies(treino['feature_3'],prefix = 'f3_')\n\n#teste\ndumteste_feature_1 = pd.get_dummies(teste['feature_1'], prefix = 'f1_')\ndumteste_feature_2 = pd.get_dummies(teste['feature_2'], prefix = 'f2_')\ndumteste_feature_3 = pd.get_dummies(teste['feature_3'], prefix = 'f3_')\n\n#concatenando dados\ntreino = pd.concat([treino, dumtreino_feature_1, dumtreino_feature_2, dumtreino_feature_3], axis = 1, sort = False)\nteste = pd.concat([teste, dumteste_feature_1, dumteste_feature_2, dumteste_feature_3], axis = 1, sort = False)","d3dcf372":"#visualizando o resultado dados de treino\ntreino.head()","b1c0aef9":"#visualizando o resultado dados de teste\nteste.head()","1fe38fd3":"target_col = treino['target']\ntreino.drop('target',axis=1,inplace=True)","73327257":"treino.columns","cc95b5e1":"cols_to_use = ['ano', 'mes', 'hist_transactions', 'sum_hist_tran', 'mean_hist_tran',\n       'max_hist_tran', 'min_hist_tran', 'std_hist_tran',\n       'merchant_transactions', 'sum_merchant_tran', 'mean_merchant_tran',\n       'max_merchant_tran', 'min_merchant_tran', 'std_merchant_tran', 'f1__1',\n       'f1__2', 'f1__3', 'f1__4', 'f1__5', 'f2__1', 'f2__2', 'f2__3', 'f3__0',\n       'f3__1']\n\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 30,\n        \"min_child_weight\" : 50,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_freq\" : 5,\n        \"bagging_seed\" : 2019,\n        \"verbosity\" : -1\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgval], \n        early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\ntrain_X = treino[cols_to_use]\ntest_X = teste[cols_to_use]\ntrain_y = target_col.values\n\npred_test = 0\nkf = model_selection.KFold(n_splits=5, random_state=2019, shuffle=True)\nfor dev_index, val_index in kf.split(treino):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    pred_test += pred_test_tmp\npred_test \/= 5.","bdda174f":"fig, ax = plt.subplots(figsize=(15,12))\nlgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax,color='lightblue')\nplt.title(\"Import\u00e2ncia das Vari\u00e1veis\",color='black', fontsize = 18)\nax.grid(False)\nplt.show()","ed07ea7a":"submission['target'] = pred_test\nsubmission.to_csv(\"submission1.csv\", index=False)","74042f24":"submission.head()","aa04b09e":"Este trabalho faz parte da edi\u00e7\u00e3o de junho de 2019 da competi\u00e7\u00e3o de machine learning promovida pela Data Science Academy. Tenho como objetivo colocar em pr\u00e1tica os conceitos aprendidos, melhorar a capacidade de resolu\u00e7\u00e3o de problemas e me auto avaliar num ambiente de aprendizado que simula problemas reais que os profissionais da \u00e1rea de ci\u00eancia de dados enfrentam no dia a dia.\n\nNossa miss\u00e3o \u00e9 prever o \u00edndice de lealdade para cada \"card_id\" representado no conjunto de dados \"dataset_teste\", um alto \u00edndice de lealdade indica que o cliente tem maior probabilidade de comprar novamente produtos ou servi\u00e7os da empresa, o \u00edndice \u00e9 muito utilizado por empresas para criar um servi\u00e7o personalizado, melhorando a satisfa\u00e7\u00e3o do cliente e consequentemente, as vendas da empresa.\n\nA m\u00e9trica de avalia\u00e7\u00e3o utilizada \u00e9 o RMSE(mean squared error), prevemos um valor de lealdade para cada card_id e no final \u00e9 calculado o RMSE da nossa previs\u00e3o.","62c3c873":"# Competi\u00e7\u00e3o DSA de Machine Learning\n### Competi\u00e7\u00e3o DSA de Machine Learning - Edi\u00e7\u00e3o Junho\/2019","5ff54c38":"Podemos perceber a exist\u00eancia de uma quantidade consider\u00e1vel de valores outliers \u00e1 esquerda do histograma, vamos ficar atentos com eles ao longo do processo.","efc03f70":"### Coleta de dados","3a8b3c35":"#### feature_1","654edab3":"## Cria\u00e7\u00e3o de vari\u00e1veis","684f16f2":"#### feature_3","2fb5809c":"#### m\u00eas","22701206":"#### One hot encoding","728974be":"#### Correla\u00e7\u00e3o das vari\u00e1veis categ\u00f3ricas com a vari\u00e1vel target","ca1dbc2e":"## Introdu\u00e7\u00e3o","ae7110f3":"### Valores nulos","0d4bd067":"## Modelagem preditiva","d5836602":"##### dataset novas transa\u00e7\u00f5es comerciais","555d8795":"## An\u00e1lise explorat\u00f3ria","94f28058":"### Descri\u00e7\u00e3o dos arquivos utilizados","0a1d5c5a":"##### dataset transa\u00e7\u00f5es hist\u00f3ricas","807266b1":"#### feature_2","95ba011b":"### Outliers\n\nDisponibilizo tamb\u00e9m uma abordagem para prever outliers nos dados de teste, n\u00e3o adicionei neste kernel pois n\u00e3o teve a acur\u00e1cia esperada.\nLink: [Combinando Regress\u00f5es para prever Outliers](https:\/\/www.kaggle.com\/andrehofreire\/combinando-regress-es-para-prever-outliers?scriptVersionId=16533800)","4478cd95":"* treino - *conjunto de dados de treinamento, possui informa\u00e7\u00f5es sobre o pr\u00f3prio cart\u00e3o: id do cart\u00e3o, primeiro m\u00eas que o cart\u00e3o estava ativo, target (\u00edndice de lealdade do cleinte) e mais outras features.\n*\n* teste - *conjunto de dados de teste, tamb\u00e9m possui informa\u00e7\u00f5es sobre o cart\u00e3o, mas n\u00e3o possui a vari\u00e1vel target (\u00e9 a que queremos prever).*\n\n* submission - *arquivo que possui os card_ids que vamos prever no formato correto para envio das previs\u00f5es*\n\n* transacoeshist - *at\u00e9 3 meses de informa\u00e7\u00f5es sobre transa\u00e7\u00f5es hist\u00f3ricas para cada card_id*\n\n* newtransacoeshist - *possui 2 meses de informa\u00e7\u00f5es sobre compras que cada card_id fez em comerciantes que n\u00e3o haviam sido visitados no transacoeshist*\n\nMais informa\u00e7\u00f5es sobre os dados ou a competi\u00e7\u00e3o, [clique aqui](https:\/\/www.kaggle.com\/c\/competicao-dsa-machine-learning-jun-2019).","fb55679a":"Podemos ver que nenhuma das 3 features dos dados de treino nem a vari\u00e1vel m\u00eas possuem algum tipo de correla\u00e7\u00e3o com a vari\u00e1vel target, e mais uma vez, fica claro a presen\u00e7a de outliers para todas as features do dataset."}}