{"cell_type":{"fe51f86c":"code","4c828ab7":"code","323761d6":"code","78da7c21":"code","1f71f4a4":"code","8eb72d57":"code","b8be714b":"code","04545a49":"code","cbebb529":"code","38e3a6cc":"code","b479038f":"code","b875815e":"code","96c7a7ac":"code","b9b1fdca":"code","f482c219":"code","d3ff58fa":"code","6c15f09b":"code","c2e83a0c":"code","12a15b86":"code","c1be1950":"code","9d25fd30":"code","fd52cc1c":"code","158ae3ee":"code","fa12e685":"code","413519cd":"code","0bc79b55":"code","32075bd0":"markdown","c889da37":"markdown"},"source":{"fe51f86c":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import randint\nimport seaborn as sns # used for plot interactive graph. \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom io import StringIO\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom IPython.display import display\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.calibration import CalibratedClassifierCV","4c828ab7":"dataset=pd.read_csv('..\/input\/website-classification\/website_classification.csv')\ndataset.shape","323761d6":"dataset.head()","78da7c21":"df = dataset[['website_url','cleaned_website_text','Category']].copy()\ndf.head()","1f71f4a4":"pd.DataFrame(df.Category.unique()).values","8eb72d57":"# Create a new column 'category_id' with encoded categories \ndf['category_id'] = df['Category'].factorize()[0]\ncategory_id_df = df[['Category', 'category_id']].drop_duplicates()\n\n\n# Dictionaries for future use\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'Category']].values)\n\n# New dataframe\ndf.head()","b8be714b":"category_id_df","04545a49":"from wordcloud import WordCloud,STOPWORDS\nplt.figure(figsize=(40,25))\nsubset = df[df['Category']=='Travel']\ntext = subset.cleaned_website_text.values\ncloud1=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,1)\nplt.axis('off')\nplt.title(\"Travel\",fontsize=40)\nplt.imshow(cloud1)\nsubset = df[df['Category']=='Social Networking and Messaging']\ntext = subset.cleaned_website_text.values\ncloud2=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,2)\nplt.axis('off')\nplt.title(\"Social Networking and Messaging\",fontsize=40)\nplt.imshow(cloud2)\nsubset = df[df['Category']=='News']\ntext = subset.cleaned_website_text.values\ncloud3=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,3)\nplt.axis('off')\nplt.title(\"News\",fontsize=40)\nplt.imshow(cloud3)\n\nsubset = df[df['Category']=='Streaming Services']\ntext = subset.cleaned_website_text.values\ncloud4=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,4)\nplt.axis('off')\nplt.title(\"Streaming Services\",fontsize=40)\nplt.imshow(cloud4)\n\nsubset = df[df['Category']=='Sports']\ntext = subset.cleaned_website_text.values\ncloud5=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,5)\nplt.axis('off')\nplt.title('Sports',fontsize=40)\nplt.imshow(cloud5)\n\nsubset = df[df['Category']=='Photography']\ntext = subset.cleaned_website_text.values\ncloud6=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,6)\nplt.axis('off')\nplt.title(\"Photography\",fontsize=40)\nplt.imshow(cloud6)\n\nsubset = df[df['Category']=='Law and Government']\ntext = subset.cleaned_website_text.values\ncloud7=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,7)\nplt.axis('off')\nplt.title(\"Law and Government\",fontsize=40)\nplt.imshow(cloud7)\n\nsubset = df[df['Category']=='Health and Fitness']\ntext = subset.cleaned_website_text.values\ncloud8=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,8)\nplt.axis('off')\nplt.title(\"Health and Fitness\",fontsize=40)\nplt.imshow(cloud8)\n\nsubset = df[df['Category']=='Games']\ntext = subset.cleaned_website_text.values\ncloud9=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,9)\nplt.axis('off')\nplt.title(\"Games\",fontsize=40)\nplt.imshow(cloud9)\n\nsubset = df[df['Category']=='E-Commerce']\ntext = subset.cleaned_website_text.values\ncloud10=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,10)\nplt.axis('off')\nplt.title(\"E-Commerce\",fontsize=40)\nplt.imshow(cloud10)\n\nsubset = df[df['Category']=='Forums']\ntext = subset.cleaned_website_text.values\ncloud11=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,11)\nplt.axis('off')\nplt.title(\"Forums\",fontsize=40)\nplt.imshow(cloud11)\n\nsubset = df[df['Category']=='Food']\ntext = subset.cleaned_website_text.values\ncloud12=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,12)\nplt.axis('off')\nplt.title(\"Food\",fontsize=40)\nplt.imshow(cloud12)\n\nsubset = df[df['Category']=='Education']\ntext = subset.cleaned_website_text.values\ncloud13=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,13)\nplt.axis('off')\nplt.title(\"Education\",fontsize=40)\nplt.imshow(cloud13)\n\nsubset =df[df['Category']=='Computers and Technology']\ntext = subset.cleaned_website_text.values\ncloud14=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,14)\nplt.axis('off')\nplt.title(\"Computers and Technology\",fontsize=40)\nplt.imshow(cloud14)\n\nsubset = df[df['Category']=='Business\/Corporate']\ntext = subset.cleaned_website_text.values\ncloud15=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,15)\nplt.axis('off')\nplt.title(\"Business\/Corporate\",fontsize=40)\nplt.imshow(cloud15)\n\nsubset = df[df['Category']=='Adult']\ntext = subset.cleaned_website_text.values\ncloud16=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap=\"Dark2\",collocations=False,width=2500,height=1800\n                       ).generate(\" \".join(text))\nplt.subplot(4,4,16)\nplt.axis('off')\nplt.title(\"Adult\",fontsize=40)\nplt.imshow(cloud16)\nplt.show()","cbebb529":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\n# We transform each cleaned_text into a vector\nfeatures = tfidf.fit_transform(df.cleaned_website_text).toarray()\n\nlabels = df.category_id\n\nprint(\"Each of the %d text is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))","38e3a6cc":"# Finding the three most correlated terms with each of the categories\nN = 3\nfor Category, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"\\n==> %s:\" %(Category))\n  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))","b479038f":"X = df['cleaned_website_text'] # Collection of text\ny = df['Category'] # Target or the labels we want to predict\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state = 0)","b875815e":"y_train.value_counts()","96c7a7ac":"y_test.value_counts()","b9b1fdca":"models = [\n    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    GaussianNB()\n]\n\n# 5 Cross-validation\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\n\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\n    \ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\ncv_df","f482c219":"mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\nstd_accuracy = cv_df.groupby('model_name').accuracy.std()\n\nacc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n          ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","d3ff58fa":"X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n                                                               labels, \n                                                               df.index, test_size=0.25, \n                                                               random_state=1)\nmodel = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\ncalibrated_svc = CalibratedClassifierCV(base_estimator=model,\n                                        cv=\"prefit\")\n\ncalibrated_svc.fit(X_train,y_train)\npredicted = calibrated_svc.predict(X_test)\nprint(metrics.accuracy_score(y_test, predicted))","6c15f09b":"# Classification report\nprint('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\nprint(metrics.classification_report(y_test,predicted,labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],target_names= df['Category'].unique()))","c2e83a0c":"conf_mat = confusion_matrix(y_test, predicted,labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\nfig, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(conf_mat, annot=True, cmap=\"OrRd\", fmt='d',\n            xticklabels=category_id_df.Category.values, \n            yticklabels=category_id_df.Category.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title(\"CONFUSION MATRIX - LinearSVC\\n\", size=16);","12a15b86":"for predicted in category_id_df.category_id:\n    for actual in category_id_df.category_id:\n        if predicted != actual and conf_mat[actual, predicted] >0:\n            print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual],id_to_category[predicted],\n                                                                   conf_mat[actual, predicted]))\n            display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['Category', \n                                                                'cleaned_website_text']])","c1be1950":"model.fit(features, labels)\n\nN = 4\nfor Category, category_id in sorted(category_to_id.items()):\n  indices = np.argsort(model.coef_[category_id])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n  print(\"\\n==> '{}':\".format(Category))\n  print(\"  * Top unigrams: %s\" %(', '.join(unigrams)))\n  print(\"  * Top bigrams: %s\" %(', '.join(bigrams)))","9d25fd30":"X_train, X_test, y_train, y_test = train_test_split(X, df['category_id'], \n                                                    test_size=0.25,\n                                                    random_state = 0)\n\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n                        ngram_range=(1, 2), \n                        stop_words='english')\n\nfitted_vectorizer = tfidf.fit(X_train)\ntfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n\nm = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)\nm1=CalibratedClassifierCV(base_estimator=m,\n                                        cv=\"prefit\").fit(tfidf_vectorizer_vectors, y_train)\n","fd52cc1c":"pip install BeautifulSoup4","158ae3ee":"from bs4 import BeautifulSoup\nimport bs4 as bs4\nfrom urllib.parse import urlparse\nimport requests\nfrom collections import Counter\nimport pandas as pd\nimport os\nclass ScrapTool:\n    def visit_url(self, website_url):\n        '''\n        Visit URL. Download the Content. Initialize the beautifulsoup object. Call parsing methods. Return Series object.\n        '''\n        #headers = {'User-Agent': 'Mozilla\/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/54.0.2840.71 Safari\/537.36'}\n        content = requests.get(website_url,timeout=60).content\n        \n        #lxml is apparently faster than other settings.\n        soup = BeautifulSoup(content, \"lxml\")\n        result = {\n            \"website_url\": website_url,\n            \"website_name\": self.get_website_name(website_url),\n            \"website_text\": self.get_html_title_tag(soup)+self.get_html_meta_tags(soup)+self.get_html_heading_tags(soup)+\n                                                               self.get_text_content(soup)\n        }\n        \n        #Convert to Series object and return\n        return pd.Series(result)\n    \n    def get_website_name(self,website_url):\n        '''\n        Example: returns \"google\" from \"www.google.com\"\n        '''\n        return \"\".join(urlparse(website_url).netloc.split(\".\")[-2])\n    \n    def get_html_title_tag(self,soup):\n        '''Return the text content of <title> tag from a webpage'''\n        return '. '.join(soup.title.contents)\n    \n    def get_html_meta_tags(self,soup):\n        '''Returns the text content of <meta> tags related to keywords and description from a webpage'''\n        tags = soup.find_all(lambda tag: (tag.name==\"meta\") & (tag.has_attr('name') & (tag.has_attr('content'))))\n        content = [str(tag[\"content\"]) for tag in tags if tag[\"name\"] in ['keywords','description']]\n        return ' '.join(content)\n    \n    def get_html_heading_tags(self,soup):\n        '''returns the text content of heading tags. The assumption is that headings might contain relatively important text.'''\n        tags = soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n        content = [\" \".join(tag.stripped_strings) for tag in tags]\n        return ' '.join(content)\n    \n    def get_text_content(self,soup):\n        '''returns the text content of the whole page with some exception to tags. See tags_to_ignore.'''\n        tags_to_ignore = ['style', 'script', 'head', 'title', 'meta', '[document]',\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"noscript\"]\n        tags = soup.find_all(text=True)\n        result = []\n        for tag in tags:\n            stripped_tag = tag.strip()\n            if tag.parent.name not in tags_to_ignore\\\n                and isinstance(tag, bs4.element.Comment)==False\\\n                and not stripped_tag.isnumeric()\\\n                and len(stripped_tag)>0:\n                result.append(stripped_tag)\n        return ' '.join(result)\n\nimport spacy as sp\nfrom collections import Counter\nsp.prefer_gpu()\nimport en_core_web_sm\n#anconda prompt ko run as adminstrator and copy paste this:python -m spacy download en\nnlp = en_core_web_sm.load()\nimport re\ndef clean_text(doc):\n    '''\n    Clean the document. Remove pronouns, stopwords, lemmatize the words and lowercase them\n    '''\n    doc = nlp(doc)\n    tokens = []\n    exclusion_list = [\"nan\"]\n    for token in doc:\n        if token.is_stop or token.is_punct or token.text.isnumeric() or (token.text.isalnum()==False) or token.text in exclusion_list :\n            continue\n        token = str(token.lemma_.lower().strip())\n        tokens.append(token)\n    return \" \".join(tokens) \n","fa12e685":"website='https:\/\/leetcode.com\/problemset\/all\/'\nscrapTool = ScrapTool()\ntry:\n    web=dict(scrapTool.visit_url(website))\n    text=(clean_text(web['website_text']))\n    t=fitted_vectorizer.transform([text])\n    print(id_to_category[m1.predict(t)[0]])\n    data=pd.DataFrame(m1.predict_proba(t)*100,columns=df['Category'].unique())\n    data=data.T\n    data.columns=['Probability']\n    data.index.name='Category'\n    a=data.sort_values(['Probability'],ascending=False)\n    a['Probability']=a['Probability'].apply(lambda x:round(x,2))\nexcept:\n    print(\"Connection Timedout!\")","413519cd":"a","0bc79b55":"sns.set(font_scale = 1.5)\nplt.figure(figsize=(10,5))\ni=list(a.index)\nax= sns.barplot(i,a['Probability'])\n\nplt.title(\"Probability Prediction for each Category of the URL\", fontsize=18)\nplt.ylabel('Probability', fontsize=16)\nplt.xlabel('Category ', fontsize=16)\n\n#adding the text labels\nrects = ax.patches\nlabels = a['Probability']\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 2, label, ha='center', va='bottom', fontsize=14)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, horizontalalignment='center')\nplt.show()","32075bd0":"Now we need to represent each category as a number, so as our predictive model can better understand the different categories.","c889da37":"Spliting the data into train and test sets\nThe original data was divided into features (X) and target (y), which were then splitted into train (75%) and test (25%) sets. Thus, the algorithms would be trained on one set of data and tested out on a completely different set of data (not seen before by the algorithm)."}}