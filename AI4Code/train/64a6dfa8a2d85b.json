{"cell_type":{"ba90ba14":"code","a8337bdb":"code","7ba7a0d7":"code","90871554":"code","41f79bc8":"code","c7f23836":"code","3af8fcab":"code","b74f3ec2":"code","3144a3db":"code","00e7964e":"code","3d6b40f4":"code","20a7e66e":"code","55526e33":"code","1e1b8f4a":"code","1048ac85":"code","13821c94":"code","d228cc61":"code","08c81c92":"code","b57567b0":"code","78aa1cb1":"code","79a94d7a":"code","933ef2c2":"code","c4913799":"code","8e96ec4b":"code","ccd03739":"code","96240370":"code","c292b4ae":"code","93cdfeff":"markdown","1ed6ddd8":"markdown","8d37cdd6":"markdown","02309187":"markdown","35d23814":"markdown","26480c42":"markdown","edbc9553":"markdown","9c1b5de7":"markdown","e3df7b2a":"markdown","8def205d":"markdown"},"source":{"ba90ba14":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","a8337bdb":"# import our dataset\ndata = pd.read_csv('..\/input\/avocado-prices\/avocado.csv')","7ba7a0d7":"# first 10 observations of our dataset\ndata.head(10)","90871554":"# renaming column names into meaningful names (refer kaggle's avacado dataset description)\ndata = data.rename(columns={'4046':'PLU_4046','4225':'PLU_4225','4770':'PLU_4770'})","41f79bc8":"# removing unnecessary column\ndata = data.drop(['Unnamed: 0'],axis = 1)\ndata.head(10)","c7f23836":"# convert the type of Date feature from obj to datetime type\ndata['Date'] = pd.to_datetime(data['Date'])","3af8fcab":"# categorizing into several seasons\ndef season_of_date(date):\n    year = str(date.year)\n    seasons = {'spring': pd.date_range(start='21\/03\/'+year, end='20\/06\/'+year),\n               'summer': pd.date_range(start='21\/06\/'+year, end='22\/09\/'+year),\n               'autumn': pd.date_range(start='23\/09\/'+year, end='20\/12\/'+year)}\n    if date in seasons['spring']:\n        return 'spring'\n    if date in seasons['summer']:\n        return 'summer'\n    if date in seasons['autumn']:\n        return 'autumn'\n    else:\n        return 'winter'","b74f3ec2":"# creating a new feature 'season' and assign the corresponding season for the Date using map function over our season_of_date function\ndata['season'] = data.Date.map(season_of_date)","3144a3db":"# now, we can see the season feature appended at the last\ndata.head(10)","00e7964e":"# no of observations for each seasons\ndata.season.value_counts()","3d6b40f4":"# droping date feature\ndata = data.drop(['Date'],axis = 1)","20a7e66e":"# converting categorical features of text data into model-understandable numerical data\nlabel_cols = ['type','region','season']\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ndata[label_cols] = data[label_cols].apply(lambda x : label.fit_transform(x))","55526e33":"# Scaling the features and \n# spliting the label encoded features into distinct features inorder to prevent our model to think that columns have data with some kind of order or hierarchy\n# column_tranformer allows us to combine several feature extraction or transformation methods into a single transformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nscale_cols = data.drop(['AveragePrice','type','year','region','season'],axis=1)\ncol_trans = make_column_transformer(\n            (OneHotEncoder(), data[label_cols].columns),\n            (StandardScaler(), scale_cols.columns),\n            remainder = 'passthrough')","1e1b8f4a":"# splitting our dataset into train and test set such that 20% of observations are considered as test set\nX = data.drop(['AveragePrice'],axis=1)\ny = data.AveragePrice\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","1048ac85":"from sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\npipe = make_pipeline(col_trans,linreg)\npipe.fit(X_train,y_train)","13821c94":"y_pred_test = pipe.predict(X_test)","d228cc61":"from sklearn.metrics import mean_absolute_error,mean_squared_error\nprint('MAE for testing set: {}'.format(mean_absolute_error(y_pred_test,y_test)))\nprint('MSE for testing set: {}'.format(mean_squared_error(y_pred_test,y_test)))\nprint('RMSE for testing set: {}'.format(np.sqrt(mean_squared_error(y_pred_test,y_test))))","08c81c92":"from sklearn.svm import SVR\nsvr = SVR()\npipe = make_pipeline(col_trans,svr)\npipe.fit(X_train,y_train)","b57567b0":"y_pred_test = pipe.predict(X_test)","78aa1cb1":"print('MAE for testing set: {}'.format(mean_absolute_error(y_pred_test,y_test)))\nprint('MSE for testing set: {}'.format(mean_squared_error(y_pred_test,y_test)))\nprint('RMSE for testing set: {}'.format(np.sqrt(mean_squared_error(y_pred_test,y_test))))","79a94d7a":"from sklearn.tree import DecisionTreeRegressor\ndr=DecisionTreeRegressor()\npipe = make_pipeline(col_trans,dr)\npipe.fit(X_train,y_train)","933ef2c2":"y_pred_test = pipe.predict(X_test)","c4913799":"print('MAE for testing set: {}'.format(mean_absolute_error(y_pred_test,y_test)))\nprint('MSE for testing set: {}'.format(mean_squared_error(y_pred_test,y_test)))\nprint('RMSE for testing set: {}'.format(np.sqrt(mean_squared_error(y_pred_test,y_test))))","8e96ec4b":"from sklearn.ensemble import RandomForestRegressor\nforest_model = RandomForestRegressor()\npipe = make_pipeline(col_trans,forest_model)\npipe.fit(X_train,y_train)","ccd03739":"y_pred_test = pipe.predict(X_test)","96240370":"print('MAE for testing set: {}'.format(mean_absolute_error(y_pred_test,y_test)))\nprint('MSE for testing set: {}'.format(mean_squared_error(y_pred_test,y_test)))\nprint('RMSE for testing set: {}'.format(np.sqrt(mean_squared_error(y_pred_test,y_test))))","c292b4ae":"sns.distplot((y_test-y_pred_test),bins=50)","93cdfeff":"## Regression Models\n\n### 1. Linear Regression","1ed6ddd8":"RandomForestRegressor outperfomed LinearRegression, SVR and DecisionTreeRegressor with an RMSE of 0.148.\n\nWe can increase the performance to some more extent by tweaking the parameters of the models (especially for the models like RandomForestRegressor and DecisionTreeRegressor) and can use hyperparameter tuning techniques such as GridSearchCV and RandomizedSearchCV to find out the best parameters for our models!\n\nThanks! Please do Upvote if you like my notebook. Any Suggestions are welcome!","8d37cdd6":"### 4. Random Forest Regressor","02309187":"## Train Test Split","35d23814":"## Data Preprocessing","26480c42":"### 2. Support Vector Regressor (SVR)","edbc9553":"## Importing Libraries","9c1b5de7":"### 3. Decision Tree Regressor","e3df7b2a":"Notice here that our residuals looked to be normally distributed and that's really a good sign which means that our model was a correct choice for the data.","8def205d":"## Feature Engineering"}}