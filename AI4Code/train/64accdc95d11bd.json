{"cell_type":{"933e784a":"code","09528371":"code","620d7cc2":"code","dc87c983":"code","526e4993":"code","7be4cc0e":"code","cfdf683f":"code","244bc850":"code","99c4ca35":"code","a9fc4c22":"code","818d14a6":"code","0aafa6d4":"code","e6e0984e":"code","2c837bd1":"code","145b5f4c":"code","7306556b":"code","f0471447":"code","2277a69b":"code","57dc8fb6":"code","459572ac":"code","b1972bec":"code","e4eb25c7":"code","9e857157":"code","9b9bcd26":"code","e622720c":"code","1ec27090":"code","86027edf":"code","305d920d":"code","f55b9911":"code","4c4f9545":"code","461d547a":"code","0e97fd00":"code","7bbdebf1":"code","7fefcd98":"code","cfefc286":"code","fa29dd08":"code","60d87afc":"code","be3fcbee":"code","8ccf76df":"code","49224fbf":"code","9a4994f4":"code","51f03aa3":"code","192186c4":"markdown","fbace8fb":"markdown","aa88eee8":"markdown","4de971e5":"markdown","e6202d36":"markdown","adcf080f":"markdown","c826760c":"markdown","eec7abe6":"markdown","b198a829":"markdown","8224a20b":"markdown","d8b7d1eb":"markdown","3d97185b":"markdown","e0d75d4e":"markdown","3173eb2a":"markdown","bde2bcef":"markdown","e54e9927":"markdown","d35f7b19":"markdown"},"source":{"933e784a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09528371":"data = pd.read_csv('\/kaggle\/input\/data_walmart.csv')","620d7cc2":"data.head()","dc87c983":"# dropping the Unwaned attribute\ndata.drop('Unnamed: 0', axis=1, inplace= True)","526e4993":"# check the info of data\ndata.info()","7be4cc0e":"# check 'type' attribute\ndata['type'].value_counts()","cfdf683f":" data['type'] =  pd.Categorical(data['type']) # type dtype changed into categorical\n data['date'] = pd.to_datetime(data['date'])  # date dtype changed into date format","244bc850":"data.info()","99c4ca35":"data.head()","a9fc4c22":"# sort with respect to weekly_sales\ndata.sort_values(\"weekly_sales\")","818d14a6":"# sort with respect to weekly_sales in descending \ndata.sort_values(\"weekly_sales\", ascending= False)","0aafa6d4":"# sort with respect to weekly_sales and fuel_price_usd_per_l with descending & ascending order\ndata.sort_values([\"weekly_sales\", 'fuel_price_usd_per_l'], ascending= [False, True])\n# it means whenever for the same value of 1st element there is multiple values in 2nd column, then in the 2nd column the values are \n# arrange in the given order mentioned in 2nd arg passes in ascending parameter.","e6e0984e":"# subset the data","2c837bd1":"data['weekly_sales']\n# for considering 1 column","145b5f4c":"data[['weekly_sales', 'fuel_price_usd_per_l']]\n# double square bracket is considered\n# the outer is responsible for creating the subset and the inner is responsible to create the list of column","7306556b":"#  subset the rows by providing logical condition to the data\n# selecting the rows where the weekly_sales are more than 500000\n\ncondition = data['weekly_sales'] > 500000 # returns the booleans \ndata[condition]","f0471447":"# subsetting wrt a given specific time\n\ndata[data.date ==  '2011-11-25'] # date is given as in string format","2277a69b":"# using multiple conditions\n\ndata[(data['weekly_sales'] > 600000) & (data['is_holiday'] == True)]","57dc8fb6":"con = (data['type'].isin(['A', 'C'])) & (data['is_holiday'] == False) \ndata[con]","459572ac":"data['temp_f'] = data.temperature_c * 1.8 + 32 \ndata.head(5)","b1972bec":"# We get the statistical values of the numeric columns\n# mean() ,mode(), median(), max(), min(), var(), std(), sum() quantile() etc\n\ndata[data['weekly_sales'] > 600000].mean()","e4eb25c7":"# get only the particular attribute\ndata[data['weekly_sales'] > 600000].weekly_sales.mean()","9e857157":"# getting the mean over the entire data field\ndata['weekly_sales'].mean()\n","9b9bcd26":"# getting the oldest and mewset time entries\n\nprint(data['date'].min())\nprint(data['date'].max())","e622720c":"def kel(column):\n    return column + 273.15","1ec27090":"data['tem_k'] = data.temperature_c.agg(kel) # agg call with the entire col and return the entie too\n                                # here we can pass any predefine functions also \ndata.head()","86027edf":"# mean of 2 attributes\ndata[['weekly_sales', 'fuel_price_usd_per_l']].agg(np.mean)","305d920d":"# mean and sum of 2 attributes ['and 2 function']\ndata[['weekly_sales', 'fuel_price_usd_per_l']].agg([np.mean, np.sum])","f55b9911":"d =  pd.DataFrame({'week_cum': data.weekly_sales.cumsum(), 'week_cummax': data.weekly_sales.cummax(), 'week_cummin': data.weekly_sales.cummin(), 'week_cumprod' : data.weekly_sales.cumprod()})\n# or\n# d['week_cum'] = data.weekly_sales.cumsum()\n# d['week_cummax'] = data.weekly_sales.cummax() ..\nd.head()\n\n#data.weekly_sales.agg(np.cumsum, np.cummax, np.cummin, np.cumprod)   \n#error","4c4f9545":"data['type'].value_counts(normalize= True) # normalize form ie propertion form","461d547a":"data.head()","0e97fd00":"data.groupby('is_holiday')['weekly_sales'].mean()","7bbdebf1":"data.groupby(['is_holiday', 'type'])['weekly_sales', 'fuel_price_usd_per_l'].mean()","7fefcd98":"# using agg()\ndata.groupby(['is_holiday', 'type'])['weekly_sales', 'fuel_price_usd_per_l'].agg([sum, max, min])","cfefc286":"data.pivot_table(values = 'weekly_sales', index = 'is_holiday')\n# In values paramater we pass the numeric arg and we can use the index and\/or columns parameter for our categorical features","fa29dd08":"# we can use the the 'margins' for add extra row\/columns having the mean of the associate values\n# and 'fill_value' parameters to fill the NaN values with the given value \ndata.pivot_table(values = 'weekly_sales', index = 'is_holiday', columns= 'type', fill_value= 0, margins= True)","60d87afc":"import matplotlib.pyplot as plt\n%matplotlib inline","be3fcbee":"data.weekly_sales.hist(bins= 30)","8ccf76df":"# plotting the barchart wrt types and mean of weekly_sales\ndata.groupby('type')['weekly_sales'].mean().plot(kind = 'bar' , rot = 45)","49224fbf":"data.plot(x = 'date', y = 'temperature_c', kind ='line')","9a4994f4":"data[data['is_holiday'] == False ]['weekly_sales'].hist(alpha = 0.7)\ndata[data['is_holiday'] == True ]['weekly_sales'].hist(alpha = 0.7)\nplt.legend(['Not holiday', 'Holiday'])\nplt.show()","51f03aa3":"# To save the datafile\ndata.to_csv('updated_data.csv')","192186c4":"# Summary statistics","fbace8fb":"### adding column","aa88eee8":"### Sorting and Subsetting","4de971e5":"# Some Visualization","e6202d36":"# Pivot table","adcf080f":"[](http:\/\/)","c826760c":"## agg() function the coolest feature:)","eec7abe6":"# **Thank You**\n## mail me at : aashirbad.maharana3@gmail.com\n## Look for your feedback.\n# Thanks again... :)","b198a829":"#### we can calculate the cumulative sum and cumsum(), cummax(), cumprod() <br>\nThese are DataFrame methods. So cant be involked by numpy or directly and cant use in agg() <br>i,e AttributeError: module 'numpy' has no attribute 'cummax'","8224a20b":"# grouped summary stats","d8b7d1eb":"Now it seems data is ready for preprocessing.\n<lr>We are ready to go....","3d97185b":"### rows subsetting using isin()","e0d75d4e":"This is similar to groupby.\n","3173eb2a":"<ul>\n    <li>Here we do not have any null values in our data<\/li>\n    <li>'type' is a catagory type, but encoded as object type<\/li>\n    <li>'date' should be in data format, but given as object<\/li>","bde2bcef":"# Using agg()\ndef cum_sum_max_min(col):\n    return col.cummax(), col.cummin(), col.cumsum()\n\nd1, d2, d3 = data.weekly_sales.agg(cum_sum_max_min)\nd123 = pd.DataFrame({\"cumMax\": d1, \"CumMax\" : d2, \"CumMin\"  : d3})\nd123.head()","e54e9927":"Line-plot","d35f7b19":"## dropping duplicate date\ndata_drop = data.drop_duplicates(subset='date')\ndata_drop.sort_values('date')"}}