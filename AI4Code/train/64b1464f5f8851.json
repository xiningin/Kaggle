{"cell_type":{"71db2b83":"code","e750cfba":"code","ddfc232a":"code","e3ffa1f0":"code","6f99d559":"code","45809f82":"code","54ad94d8":"code","5227076c":"code","128dfb91":"code","de6e9bb3":"code","a9397804":"code","59a1edfb":"code","8dfbcc67":"code","61597f83":"code","9b3bca6c":"code","3c16b532":"code","2684b0aa":"code","b11fc1a5":"code","719968a6":"code","2913e772":"code","aa5a6b4b":"code","a0635fab":"code","92022666":"code","ef7da737":"code","72fd7dc0":"code","49c06194":"code","b915c12e":"code","b852a980":"code","6da9c510":"code","6a3e662e":"code","3c9ff052":"code","cabb0c44":"code","d0fa5052":"code","30a5486a":"code","a9316fd6":"code","9889df76":"code","92029b75":"code","97003c43":"code","630f908b":"code","98e0fb89":"code","f1c22690":"code","fd82450e":"code","6fb00c6b":"code","4b79ca2b":"code","d448eb30":"code","d4661327":"code","1cf0bad0":"code","3e2c988f":"code","9e445844":"code","6070371b":"code","07fbc738":"code","c7f91c62":"code","09676451":"code","9278a759":"code","9d92d3b7":"code","0f595b62":"code","81f68b53":"code","0d2d5815":"code","6f9210ea":"code","d6aed2bf":"code","d04559c0":"code","46c0690a":"code","7bbdeac8":"code","36c0e690":"code","26192b0a":"code","24d27d7d":"code","4f75ed09":"code","b3a7e15e":"code","fcc37b1e":"code","1262cf58":"code","585e41de":"code","9d678afa":"code","0cb297e3":"code","b3ccec0c":"code","4b994222":"code","391ace2f":"code","9c653fa2":"code","46a84d05":"code","30ddf6d8":"code","a8128558":"code","7980d277":"code","0c985ba7":"code","da442449":"code","0764413c":"code","62e0f150":"code","85b06f5d":"code","e2ca4633":"code","6b09b07b":"code","9362b294":"code","4b9e95de":"code","c6ebb71a":"code","0e9da747":"code","02d6c7e9":"code","a16b938b":"code","aecc5f95":"code","5090af56":"code","37825940":"code","8007b51b":"code","8ecb6c50":"code","7cd26dd7":"code","0eac2f73":"code","99564a37":"code","4c39a855":"code","2d030087":"code","17f2f219":"code","78984c8b":"code","f38be316":"code","bdfad826":"code","560740fd":"code","7b37a494":"code","8b348476":"code","0f40c975":"code","b9967832":"code","11c71a16":"code","b89419b5":"code","4ba78b3e":"code","38d7aa9d":"code","e104e7cb":"code","96a4d872":"code","6a137bb3":"code","ba429aad":"code","a423b159":"code","a2a2dbd2":"code","7fa8d774":"code","b378e366":"code","92324edd":"code","2073fd14":"code","2b08dc22":"code","3e1fb978":"code","bff8b531":"code","d0b3c8d1":"code","33b5a172":"code","8e27f600":"code","d5a49c23":"code","32250fc8":"code","41488635":"code","8c0da80a":"code","9274c65e":"code","c7f3affa":"code","e5b23272":"code","3a02d126":"code","7c4cd1ff":"code","634363fa":"code","09459e94":"code","77e30306":"code","2d90fd74":"code","2ac1d474":"code","4c240124":"code","c267fda5":"code","9926a98f":"code","e68e1548":"code","64892e74":"code","ca5f62be":"code","e18101c9":"code","f81b6b91":"code","cc1f0e8d":"code","424c22dc":"code","c7266258":"code","57e84490":"code","9e236817":"code","c10accb5":"code","660f2b19":"code","c4598ca2":"code","1dbd33cc":"code","7f6dedeb":"code","6d999cee":"code","e369cdcd":"code","1948808b":"code","44cfdaa0":"code","a3c1e0c3":"code","fd018523":"code","3faadb51":"code","5a13fdbc":"code","c84f95e1":"code","64014038":"code","f09997af":"code","7d810976":"code","ffcb349b":"code","b98b6bd7":"code","f71ac196":"code","ad362f32":"code","eeee844d":"code","f6715f6f":"code","2ece79b2":"code","856e7b6d":"code","1ab41162":"code","cfb3d14a":"code","766b07d8":"code","205fb9ae":"code","5f88d19b":"code","47408dbb":"code","1ba43b98":"code","f4468b33":"code","73b1af8b":"code","efc88443":"code","08264519":"code","5687afbf":"markdown","1e8761c6":"markdown","075105fa":"markdown","6b7a8e5d":"markdown","3aa2feac":"markdown","4ab4efdf":"markdown","6f7153d4":"markdown","6e69979a":"markdown","7bc0c549":"markdown","0fb3ae00":"markdown","b037210e":"markdown","cdffab34":"markdown","1a0fb34f":"markdown","58ea64be":"markdown","e8c97f5a":"markdown","28f0f4ec":"markdown","b6ef553b":"markdown","99772e72":"markdown","9086500e":"markdown","2d4de19f":"markdown","919ccff3":"markdown","3851ae1b":"markdown","e8dc11af":"markdown","c9224165":"markdown","2dcc71f6":"markdown","550e845e":"markdown","67ab81ab":"markdown","9422cd4d":"markdown","a8839944":"markdown","2afed0fc":"markdown","c51a6c48":"markdown","7be001e5":"markdown","aed47484":"markdown","a595bd0a":"markdown","abda5c78":"markdown","905856d8":"markdown","2956ce5d":"markdown","8a6a6636":"markdown","8cf4b506":"markdown","f3c79f20":"markdown","0de75af4":"markdown","d0850efb":"markdown","75fdd73a":"markdown","10a5fef6":"markdown","aebdddbf":"markdown","45009e7b":"markdown","d96c8a81":"markdown","63652e96":"markdown","25c7c14c":"markdown","49939775":"markdown","3ce5db36":"markdown","5d2b3bf4":"markdown","95f4b11b":"markdown","459e9579":"markdown","4d0dc356":"markdown","8a356e1f":"markdown","68a44c0f":"markdown","e80a5dd1":"markdown","96f73722":"markdown","27922861":"markdown","80a4447b":"markdown","c4c164c9":"markdown","e4a2e7b4":"markdown","90db6fba":"markdown","a6ec286d":"markdown","ddc180c1":"markdown","a8a7eaa2":"markdown","dd6b8922":"markdown","cf0b062a":"markdown","0e3366ac":"markdown","b4fe2a8e":"markdown","14117c72":"markdown","db5f5abd":"markdown","512449c3":"markdown","5910dd20":"markdown","17ed9ef2":"markdown","9307fcbe":"markdown","084b6c6d":"markdown","aeab56f6":"markdown","e29598bb":"markdown","ecf51c2b":"markdown","b29493c5":"markdown","08f087a5":"markdown","034795ac":"markdown","612eb01e":"markdown","7cbb63b9":"markdown","c91a94b9":"markdown","ac7d3c33":"markdown","b69228f2":"markdown","94e43457":"markdown","f513770e":"markdown","d23759ae":"markdown","ff2aafd3":"markdown","f51343b0":"markdown","ece18390":"markdown","a1bef364":"markdown","4097b7be":"markdown","29dabc4e":"markdown","b991f3aa":"markdown","08a4da9c":"markdown","1e296a0f":"markdown","fdbdd796":"markdown","5c962a27":"markdown","3ff417eb":"markdown","44e57045":"markdown","f2149bfd":"markdown","2ca7bd51":"markdown","679861d4":"markdown","b4a06a41":"markdown","696b54b2":"markdown","a273679b":"markdown","f6a46f61":"markdown","9e1152b9":"markdown","edd67511":"markdown","96c59bd1":"markdown","b2ff9c81":"markdown","e13aa05e":"markdown","870a30bf":"markdown","cc2b52b5":"markdown","31ab2d65":"markdown","ded2d451":"markdown","9aa455ac":"markdown","3e38fec0":"markdown","17de9fc0":"markdown","1ac67743":"markdown","5ef6fa1d":"markdown","d16fca17":"markdown","69c81ce0":"markdown","ebb934ed":"markdown","bca8f1ad":"markdown"},"source":{"71db2b83":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))","e750cfba":"train = pd.read_csv(\"..\/input\/train.csv\") \ntest = pd.read_csv(\"..\/input\/test.csv\")","ddfc232a":"train.head()","e3ffa1f0":"train.info()","6f99d559":"test.info()","45809f82":"df_int_cols = train.select_dtypes('int').apply(lambda x:x.nunique()).reset_index()\ndf_int_cols.columns = ['var_name','count']\ndf_int_cols.groupby('count')['var_name'].size().plot(kind='bar')\n\nplt.xlabel('Number of Unique Values'); plt.ylabel('Count');\nplt.title('Count of Unique Values in Integer Columns');","54ad94d8":"from collections import OrderedDict\n\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\nfig,ax = plt.subplots(4,2,figsize=[20,16])\n\nfor i,col in enumerate(train.select_dtypes('float').columns):\n    ax = plt.subplot(4,2,i+1)\n    for proverty_level,color in colors.items():\n        sns.kdeplot(train.loc[train['Target']==proverty_level,col].dropna(),ax=ax,color=color,label=poverty_mapping[proverty_level])\n    \n    ax.set(title=f'{col.capitalize()} Distribution',xlabel=f'{col}',ylabel='Density')","5227076c":"train.select_dtypes('object').head()","128dfb91":"mapping = {'yes':1,'no':0}\n\nneedToChange = train.select_dtypes('object').columns[-1:1:-1]\n\nfor df in [train,test]:\n    for col in needToChange:\n        df[col] = df[col].replace(mapping).astype(np.float64)\n        \ntrain.loc[:,needToChange].describe()","de6e9bb3":"fig,ax = plt.subplots(3,1,figsize=[16,18])\n\nfor i,col in enumerate(needToChange):\n    ax = plt.subplot(3,1,i+1)\n    for proverty_level,color in colors.items():\n        sns.kdeplot(train.loc[train['Target']==proverty_level,col].dropna(),ax=ax,color=color,label=poverty_mapping[proverty_level])\n    \n    ax.set(title=f'{col.capitalize()} Distribution',xlabel=f'{col}',ylabel='Density')","a9397804":"# Add null Target column to test\ntest['Target'] = np.nan\ndata = train.append(test, ignore_index = True)","59a1edfb":"heads = data.loc[data['parentesco1'] == 1].copy()\n\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1),['Target','idhogar']]\n\nlabel_counts = train_labels['Target'].value_counts().sort_index()\n\nlabel_counts.plot(kind='bar',figsize=[8,6],color=colors.values(),edgecolor='k',linewidth = 2)\n\nplt.xlabel('Poverty Level'); plt.ylabel('Count'); \nplt.xticks([x - 1 for x in poverty_mapping.keys()], \n           list(poverty_mapping.values()), rotation = 60)\nplt.title('Poverty Level Breakdown');","8dfbcc67":"all_equal = train.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\n\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","61597f83":"train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","9b3bca6c":"household_leader = train.groupby('idhogar')['parentesco1'].sum()\n\nhousehold_nohead = train.loc[train['idhogar'].isin(household_leader[household_leader==0].index),:]\nprint('There are {} households without a head.'.format(household_nohead['idhogar'].nunique()))","3c16b532":"household_nohead_equal = household_nohead.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\nprint('{} Households with no head have different labels.'.format(sum(household_nohead_equal == False)))","2684b0aa":"for household in not_equal.index:\n    true_label = int(train.loc[(train['idhogar'] == household) & (train['parentesco1'] == 1)]['Target'])\n    train.loc[train['idhogar'] == household,'Target'] = true_label\n    \nall_equal = train.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\nnot_equal = all_equal[all_equal != True]\n\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","b11fc1a5":"missing = pd.DataFrame(data.isnull().sum()).rename(columns={0:'total'})\nmissing['percent'] = missing['total'] \/ len(data)\nmissing.sort_values(by='percent',ascending=False).head(10).drop('Target')","719968a6":"def plot_value_counts(df,col,head_only=False):\n    if head_only:\n        df = df.loc[df['parentesco1']==1,col]\n    plt.figure(figsize=[8,6])\n    df[col].value_counts().sort_index().plot(kind='bar',color='blue',edgecolor='k',linewidth=2)\n    plt.xlabel(f'{col}')\n    plt.title(f'{col} Value Counts')\n    plt.ylabel('Count')\n    plt.show()","2913e772":"plot_value_counts(heads,'v18q1')","aa5a6b4b":"heads.groupby('v18q')['v18q1'].apply(lambda x:x.isnull().sum())","a0635fab":"data['v18q1'] = data['v18q1'].fillna(0)","92022666":"own_variables = [x for x in data if x.startswith('tipo')]\n\ndata.loc[data['v2a1'].isnull(),own_variables].sum().plot(kind='bar',figsize=[10,8],color='green',edgecolor='k',linewidth=2)\n\nplt.xticks([0,1,2,3,4], ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'], rotation = 60)\nplt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);","ef7da737":"data.loc[data['tipovivi1'] == 1,'v2a1'] = 0\n\ndata['v2a1-missing'] = data['v2a1'].isnull()\ndata['v2a1-missing'].value_counts()","72fd7dc0":"data.loc[data['rez_esc'].notnull(),'age'].describe()","49c06194":"data.loc[data['rez_esc'].isnull(),'age'].describe()","b915c12e":"# If individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","b852a980":"data.loc[data['rez_esc']>5,'rez_esc'] = 5","6da9c510":"def plot_categoricals(x, y, data, annotate = True):\n    \"\"\"Plot counts of two categoricals.\n    Size is raw count for each grouping.\n    Percentages are for a given value of y.\"\"\"\n    \n    # Raw counts \n    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n    \n    # Calculate counts for each group of x and y\n    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n    \n    # Rename the column and reset the index\n    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n    counts['percent'] = 100 * counts['normalized_count']\n    \n    # Add the raw count\n    counts['raw_count'] = list(raw_counts['raw_count'])\n    \n    plt.figure(figsize = (14, 10))\n    # Scatter plot sized by percent\n    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',\n                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',\n                alpha = 0.6, linewidth = 1.5)\n    \n    if annotate:\n        # Annotate the plot with text\n        for i, row in counts.iterrows():\n            # Put text with appropriate offsets\n            plt.annotate(xy = (row[x] - (1 \/ counts[x].nunique()), \n                               row[y] - (0.15 \/ counts[y].nunique())),\n                         color = 'navy',\n                         s = f\"{round(row['percent'], 1)}%\")\n        \n    # Set tick marks\n    plt.yticks(counts[y].unique())\n    plt.xticks(counts[x].unique())\n    \n    # Transform min and max to evenly space in square root domain\n    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n    \n    # 5 sizes for legend\n    msizes = list(range(sqr_min, sqr_max,\n                        int(( sqr_max - sqr_min) \/ 5)))\n    markers = []\n    \n    # Markers for legend\n    for size in msizes:\n        markers.append(plt.scatter([], [], s = 100 * size, \n                                   label = f'{int(round(np.square(size) \/ 100) * 100)}', \n                                   color = 'lightgreen',\n                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))\n        \n    # Legend and formatting\n    plt.legend(handles = markers, title = 'Counts',\n               labelspacing = 3, handletextpad = 2,\n               fontsize = 16,\n               loc = (1.10, 0.19))\n    \n    plt.annotate(f'* Size represents raw count while % is for a given y value.',\n                 xy = (0, 1), xycoords = 'figure points', size = 10)\n    \n    # Adjust axes limits\n    plt.xlim((counts[x].min() - (6 \/ counts[x].nunique()), \n              counts[x].max() + (6 \/ counts[x].nunique())))\n    plt.ylim((counts[y].min() - (4 \/ counts[y].nunique()), \n              counts[y].max() + (4 \/ counts[y].nunique())))\n    plt.grid(None)\n    plt.xlabel(f\"{x}\"); plt.ylabel(f\"{y}\"); plt.title(f\"{y} vs {x}\");","6a3e662e":"plot_categoricals('rez_esc', 'Target', data);","3c9ff052":"plot_categoricals('escolari', 'Target', data, annotate = False)","cabb0c44":"plot_value_counts(data[(data['rez_esc-missing'] == 1)], 'Target')","d0fa5052":"plot_value_counts(data[(data['v2a1-missing'] == 1)], 'Target')","30a5486a":"id_ = ['Id', 'idhogar', 'Target']","a9316fd6":"ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']","9889df76":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","92029b75":"sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","97003c43":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n\nfrom collections import Counter\n\nprint('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\nprint('We covered every variable: ', len(x) == data.shape[1])","630f908b":"np.all(np.array(list(Counter(x).values())) == 1)","98e0fb89":"sns.lmplot('age','SQBage',data=data,fit_reg=False)\nplt.title('Squared Age versus Age')","f1c22690":"data = data.drop(columns = sqr_)\ndata.shape","fd82450e":"heads = data.loc[data['parentesco1']== 1,:]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","6fb00c6b":"corr_matrix = heads.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [col for col in upper.columns if any(abs(upper[col]) > 0.95)]\n\nto_drop","4b79ca2b":"corr_matrix.loc[corr_matrix['tamhog'].abs()>0.9,corr_matrix['tamhog'].abs()>0.9]","d448eb30":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],annot=True, cmap = plt.cm.autumn_r, fmt='.3f');","d4661327":"heads = heads.drop(columns = ['tamhog','hogar_total','r4t3'])","1cf0bad0":"sns.lmplot('tamviv','hhsize',data,fit_reg=False,size=8)\nplt.title('Household size vs number of persons living in the household');","3e2c988f":"heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']\nplot_categoricals('hhsize-diff','Target',heads)","9e445844":"corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9,corr_matrix['coopele'].abs() > 0.9]","6070371b":"elec = []\n\nfor i,row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()\n\nheads = heads.drop(columns = ['noelec','coopele','public','planpri'])","07fbc738":"plot_categoricals('elec','Target',heads)","c7f91c62":"corr_matrix.loc[corr_matrix['area2'].abs() >0.9, corr_matrix['area2'].abs()>0.9]","09676451":"heads = heads.drop(columns='area2')\n\nheads.groupby('area1')['Target'].value_counts(normalize=True)","9278a759":"heads['walls'] = np.argmax(np.array(heads[['epared1','epared2','epared3']]),axis=1)\nheads.drop(columns=['epared1','epared2','epared3'])\n\nplot_categoricals('walls','Target',heads)","9d92d3b7":"# Roof ordinal variable\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","0f595b62":"heads['wall+roof+floor'] = heads['walls']+heads['roof']+heads['floor']\n\nplot_categoricals('wall+roof+floor','Target',heads,annotate=False)","81f68b53":"count = pd.DataFrame(heads.groupby('wall+roof+floor')['Target'].value_counts(normalize=True)).rename(columns={'Target':'Normalized Count'}).reset_index()\ncount.head()","0d2d5815":"heads['warning'] = 1 * (heads['sanitario1'] + (heads['elec'] == 0) + heads['pisonotiene'] + heads['abastaguano'] + (heads['cielorazo'] == 0))","6f9210ea":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'warning', y = 'Target', data = heads);\nplt.title('Target vs Warning Variable');","d6aed2bf":"plot_categoricals('warning', 'Target', data = heads)","d04559c0":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","46c0690a":"heads['phones-per-capita'] = heads['qmobilephone'] \/ heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] \/ heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] \/ heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] \/ heads['tamviv']","7bbdeac8":"from scipy.stats import spearmanr\n\ndef plot_corrs(x,y):\n    spr = spearmanr(x,y).correlation\n    pcr = np.corrcoef(x,y)[0,1]\n    \n    data = pd.DataFrame({'x':x,'y':y})\n    plt.figure(figsize=[6,4])\n    sns.regplot('x','y',data=data,fit_reg=False)\n    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}');","36c0e690":"x = np.array(range(100))\ny = x ** 2\n\nplot_corrs(x, y)","26192b0a":"x = np.array([1, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 7, 8, 8, 9, 9, 9])\ny = np.array([1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 3, 2, 4, 2, 2, 4])\n\nplot_corrs(x, y)","24d27d7d":"x = np.array(range(-19, 20))\ny = 2 * np.sin(x)\n\nplot_corrs(x, y)","4f75ed09":"train_heads = heads.loc[heads['Target'].notnull(),:].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target':'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns= {'index':'feature'})\n\nprint('Most negatively correlated variables:')\nprint(pcorrs.head())\n\nprint('\\nMost positively correlated variables:')\nprint(pcorrs.dropna().tail())","b3a7e15e":"import warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\nfeats = []\nscorr = []\npvalues = []\n\n# Iterate through each column\nfor c in heads:\n    # Only valid for numbers\n    if heads[c].dtype != 'object':\n        feats.append(c)\n        \n        # Calculate spearman correlation\n        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n\nscorrs = pd.DataFrame({'feature': feats, 'scorr': scorr, 'pvalue': pvalues}).sort_values('scorr')","fcc37b1e":"print('Most negative Spearman correlations:')\nprint(scorrs.head())\nprint('\\nMost positive Spearman correlations:')\nprint(scorrs.dropna().tail())","1262cf58":"corrs = pcorrs.merge(scorrs, on = 'feature')\ncorrs['diff'] = corrs['pcorr'] - corrs['scorr']\n\ncorrs.sort_values('diff').head()","585e41de":"corrs.sort_values('diff').dropna().tail()","9d678afa":"sns.lmplot('dependency', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Dependency');","0cb297e3":"sns.lmplot('rooms-per-capita', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Rooms Per Capita');","b3ccec0c":"variables = ['Target', 'dependency', 'warning', 'wall+roof+floor', 'meaneduc',\n             'floor', 'r4m1', 'overcrowding']\n\n# Calculate the correlations\ncorr_mat = train_heads[variables].corr().round(2)\n\n# Draw a correlation heatmap\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n            cmap = plt.cm.RdYlGn_r, annot = True);","4b994222":"import warnings\nwarnings.filterwarnings('ignore')\n\nplot_data = train_heads[['Target', 'dependency', 'wall+roof+floor','meaneduc', 'overcrowding']]\n\ngrid = sns.PairGrid(data=plot_data,size=4,diag_sharey=False,hue='Target',hue_order=[4,3,2,1],vars = [col for col in list(plot_data.columns) if col != 'Target'])\n\ngrid.map_upper(plt.scatter,alpha=0.8,s=20)\n\ngrid.map_diag(sns.kdeplot)\n\ngrid.map_lower(sns.kdeplot,cmap=plt.cm.OrRd_r)\ngrid = grid.add_legend()\nplt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05);","391ace2f":"household_feats = list(heads.columns)","9c653fa2":"ind = data[id_ + ind_bool + ind_ordered]\nind.shape","46a84d05":"corr_matrix = ind.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [col for col in upper.columns if any(abs(upper[col]) > 0.95)]\n\nto_drop","30ddf6d8":"ind = ind.drop(columns = 'male')","a8128558":"ind[[col for col in ind.columns if col.startswith('instlevel')]].head()","7980d277":"ind['inst'] = np.argmax(np.array(ind[[col for col in ind.columns if col.startswith('instlevel')]]),axis=1)\n\nplot_categoricals('inst','Target',ind,annotate=False)","0c985ba7":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');","da442449":"# Drop the education columns\nind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\nind.shape","0764413c":"ind['escolari\/age'] = ind['escolari'] \/ ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari\/age', data = ind);","62e0f150":"ind['inst\/age'] = ind['inst'] \/ ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","85b06f5d":"range_ = lambda x:x.max() - x.min()\nrange_.__name__ = 'range_'\n\nind_agg = ind.drop(columns='Target').groupby('idhogar').agg(['min','max','sum','count','std',range_])\nind_agg.head()","e2ca4633":"new_cols = []\n\nfor col in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_cols.append(f'{col}-{stat}')\n        \nind_agg.columns = new_cols\nind_agg.head()","6b09b07b":"ind_agg.shape","9362b294":"corr_matrix = ind_agg.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [col for col in upper.columns if any(abs(upper[col]) > 0.95)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')","4b9e95de":"ind_agg = ind_agg.drop(columns=to_drop)\nind_feats = list(ind_agg.columns)\n\nfinal = heads.merge(ind_agg,on='idhogar',how='left')\n\nprint('Final features shape: ', final.shape)","c6ebb71a":"final.head()","0e9da747":"corrs = final.corr()['Target']","02d6c7e9":"corrs.sort_values().head()","a16b938b":"corrs.sort_values().dropna().tail()","aecc5f95":"plot_categoricals('escolari-max', 'Target', final, annotate=False);","5090af56":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","37825940":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","8007b51b":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'meaneduc', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Average Schooling by Target');","8ecb6c50":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'overcrowding', data = final);\nplt.xticks([0, 1, 2, 3], poverty_mapping.values())\nplt.title('Overcrowding by Target');","7cd26dd7":"head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\nfinal = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})","0eac2f73":"final.groupby('female-head')['Target'].value_counts(normalize=True)","99564a37":"sns.violinplot(x = 'female-head', y = 'Target', data = final);\nplt.title('Target by Female Head of Household');","4c39a855":"plt.figure(figsize = (8, 8))\nsns.boxplot(x = 'Target', y = 'meaneduc', hue = 'female-head', data = final);\nplt.title('Average Education by Target and Female Head of Household', size = 16);","2d030087":"final.groupby('female-head')['meaneduc'].agg(['mean', 'count'])","17f2f219":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# Custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')","78984c8b":"train_labels = np.array(list(final.loc[final['Target'].notnull(),'Target'].astype(np.uint8)))\n\ntrain_set = final[final['Target'].notnull()].drop(columns=['Id','idhogar','Target'])\n\ntest_set = final[final['Target'].isnull()].drop(columns=['Id','idhogar','Target'])\n\nsubmission_base = test[['Id', 'idhogar']].copy()","f38be316":"train_set.head()","bdfad826":"features = list(train_set.columns)\n\npipeline = Pipeline([('imputer',Imputer(strategy='median')),('scaler',MinMaxScaler())])\n\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","560740fd":"model = RandomForestClassifier(n_estimators=100, random_state=10, n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","7b37a494":"model.fit(train_set,train_labels)\n\nindices = np.argsort(model.feature_importances_)[::-1]\nimportances = model.feature_importances_\n\nfor i in range(len(features)):\n    print(\"%d) feature importances of %s %f\"%(i,features[indices[i]],importances[indices[i]]))","8b348476":"# def plot_feature_importances(df,feature_label,importance_label,n=10,ascending=False):\n    \n#     plt.style.use('fivethirtyeight')\n    \n#     df = df.sort_values(by=importance_label,ascending=ascending).reset_index()\n#     df.loc[list(range(n)),][importance_label].plot(kind='bar')\n#     plt.xticks(list(range(n)),df.loc[list(range(n)),][feature_label])","0f40c975":"def plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df","b9967832":"temp_df = pd.DataFrame({'feature':features,'importance':importances})","11c71a16":"norm_fi = plot_feature_importances(temp_df, threshold=0.95)","b89419b5":"def kde_target(df, variable):\n    \"\"\"Plots the distribution of `variable` in `df` colored by the `Target` column\"\"\"\n    \n    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n\n    plt.figure(figsize = (12, 8))\n    \n    df = df[df['Target'].notnull()]\n    \n    for level in df['Target'].unique():\n        subset = df[df['Target'] == level].copy()\n        sns.kdeplot(subset[variable].dropna(), \n                    label = f'Poverty Level: {level}', \n                    color = colors[int(subset['Target'].unique())])\n\n    plt.xlabel(variable); plt.ylabel('Density');\n    plt.title('{} Distribution'.format(variable.capitalize()));","4ba78b3e":"kde_target(final, 'meaneduc')","38d7aa9d":"kde_target(final, 'escolari\/age-range_')","e104e7cb":"# Model imports\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier","96a4d872":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\n# Filter out warnings from models\nwarnings.filterwarnings('ignore', category = ConvergenceWarning)\nwarnings.filterwarnings('ignore', category = DeprecationWarning)\nwarnings.filterwarnings('ignore', category = UserWarning)\n\n# Dataframe to hold results\nmodel_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n\ndef cv_model(train, train_labels, model, name, model_results=None):\n    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \n    if model_results is not None:\n        model_results = model_results.append(pd.DataFrame({'model': name, \n                                                           'cv_mean': cv_scores.mean(), \n                                                            'cv_std': cv_scores.std()},\n                                                           index = [0]),\n                                             ignore_index = True)\n\n        return model_results","6a137bb3":"model_results = cv_model(train_set,train_labels,LinearSVC(),'LSVC',model_results)","ba429aad":"model_results = cv_model(train_set,train_labels,GaussianNB(),'GNB',model_results)","a423b159":"model_results = cv_model(train_set,train_labels,MLPClassifier(hidden_layer_sizes=(32,64,128,64,32)),'MLP',model_results)","a2a2dbd2":"model_results = cv_model(train_set, train_labels, \n                          LinearDiscriminantAnalysis(), \n                          'LDA', model_results)","7fa8d774":"model_results = cv_model(train_set, train_labels, RidgeClassifierCV(), 'RIDGE', model_results)","b378e366":"for n in [5, 10, 20]:\n    print(f'\\nKNN with {n} neighbors\\n')\n    model_results = cv_model(train_set, train_labels, \n                             KNeighborsClassifier(n_neighbors = n),\n                             f'knn-{n}', model_results)","92324edd":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel_results = cv_model(train_set, train_labels, \n                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n                         'EXT', model_results)","2073fd14":"model_results = cv_model(train_set, train_labels, \n                         RandomForestClassifier(n_estimators = 100, random_state = 10),\n                         'RF', model_results)","2b08dc22":"models = model_results['model']","3e1fb978":"model_results_ = model_results.set_index('model')\nmodel_results_['cv_mean'].plot(kind='bar',color='orange',figsize=(8,6),edgecolor='k',linewidth=2, yerr=list(model_results['cv_std']))","bff8b531":"test_ids = list(final.loc[final['Target'].isnull(),'idhogar'])","d0b3c8d1":"def submit(model,train,train_labels,test,test_ids):\n    \n    model.fit(train,train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar':test_ids,'Target':predictions})\n    \n    submission = submission_base.merge(predictions,on='idhogar',how='left').drop(columns=['idhogar'])\n    \n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    return submission","33b5a172":"rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n                                              random_state=10, n_jobs = -1), \n                         train_set, train_labels, test_set, test_ids)\n\nrf_submission.to_csv('rf_submission.csv', index = False)","8e27f600":"train_set = pd.DataFrame(train_set,columns=features)","d5a49c23":"corr_matrix = train_set.corr()\n\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n\nto_drop = [column for column in upper.columns if any(abs(upper[column]) >0.95)]\n\nto_drop","32250fc8":"print(train_set.shape)","41488635":"train_set = train_set.drop(to_drop,axis=1)\nprint(train_set.shape)","8c0da80a":"test_set = pd.DataFrame(test_set,columns=features)\nprint(test_set.shape)\ntrain_set, test_set = train_set.align(test_set,axis=1,join='inner')\nfeatures = list(train_set.columns)\nprint(test_set.shape)","9274c65e":"from sklearn.feature_selection import RFECV\n\nestimator = RandomForestClassifier(random_state=10,n_estimators=100,n_jobs=-1)\n\nselector = RFECV(estimator,step=1,cv=5,scoring=scorer,n_jobs=-1)","c7f3affa":"selector.fit(train_set,train_labels)","e5b23272":"plt.plot(selector.grid_scores_)\n\nplt.xlabel('Number of Features')\nplt.ylabel('Macro F1 Score')\nplt.title('Feature Selection Scores')\nselector.n_features_","3a02d126":"rankings = pd.DataFrame({'feature':list(train_set.columns),'rank':list(selector.ranking_)}).sort_values(by='rank')\nrankings.head(10)","7c4cd1ff":"train_selected = selector.transform(train_set)\ntest_selected = selector.transform(test_set)","634363fa":"train_selected","09459e94":"# np.where(selector.ranking_==1)","77e30306":"selected_features = train_set.columns[np.where(selector.ranking_==1)]\ntrain_selected = pd.DataFrame(train_selected,columns=selected_features)\ntest_selected = pd.DataFrame(test_selected,columns=selected_features)","2d90fd74":"train_selected.shape","2ac1d474":"model_results = cv_model(train_selected,train_labels,model,'RF-SEL',model_results)","4c240124":"model_results.set_index('model', inplace = True)\nmodel_results_ = model_results\n\nmodel_results_['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\nmodel_results.reset_index(inplace = True)","c267fda5":"def macro_f1_score(labels, predictions):\n    # Reshape the predictions as needed\n    # \ud589\uc73c\ub85c\ub9cc \uc774\ub8e8\uc5b4\uc9c4 \uacb0\uacfc\uce58\ub97c 4\uc5f4\uc529 \ucc28\ub840\ub300\ub85c \ubc30\uce58\ud558\uace0 \uadf8 4\uc5f4\uc5d0\uc11c max\ub97c \uac01\uac01 \ucd94\ucd9c\n    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n    \n    metric_value = f1_score(labels, predictions, average = 'macro')\n    \n    # Return is name, value, is_higher_better\n    return 'macro_f1', metric_value, True","9926a98f":"# from sklearn.model_selection import StratifiedKFold\n# import lightgbm as lgb\n\n# feature_names = features\n\n# params = {\n#     'boosting_type':'dart',\n#     'colsample_bytree':0.88,\n#     'learning_rate':0.028,\n#     'min_child_samples':10,\n#     'num_leaves':36, 'reg_alpha':0.76,\n#     'reg_lambda':0.43,\n#     'subsample_for_bin':40000,\n#     'subsample':0.54,\n#     'class_weight':'balanced'\n# }\n\n# model = lgb.LGBMClassifier(**params,objective='multiclass',n_jobs=-1,n_estimators=1000,random_state=10)\n\n# SKFOLD = StratifiedKFold(n_splits=5,shuffle=True)\n\n# predictions = pd.DataFrame()\n# importances = np.zeros(len(feature_names))\n\n# features = np.array(train_set)\n# test_features = np.array(test_set)\n# labels = np.array(train_labels).reshape((-1))\n\n# valid_scores = []\n\n# for i,(train_indices,valid_indices) in enumerate(SKFOLD.split(features,labels)):\n    \n#     fold_predictions = pd.DataFrame()\n    \n#     X_train,X_valid = features[train_indices],features[valid_indices]\n#     y_train,y_valid = labels[train_indices],labels[valid_indices]\n    \n#     model.fit(X_train,y_train,early_stopping_rounds=100,eval_metric=macro_f1_score,eval_set=[(X_train,y_train),(X_valid,y_valid)],eval_names=['train','valid'],verbose=200)\n    \n#     valid_scores.append(model.best_score_['valid']['macro_f1'])\n    \n#     fold_probabilities = model.predict_proba(test_features)\n    \n#     for j in range(4):\n#         fold_predictions[(j+1)] = fold_probabilities[:,j]\n        \n#     fold_predictions['idhogar'] = test_ids\n#     fold_predictions['fold'] = (i+1)\n    \n#     predictions = predictions.append(fold_predictions)\n    \n#     importances += model.feature_importances_ \/ 5\n    \n#     display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n    \n#     predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n#     predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    \n#     submission = submission_base.merge(predictions[['idhogar','Target']],on='idhogar',how='left').drop(columns=['idhogar'])\n#     submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n#     break\n","e68e1548":"from sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom IPython.display import display\n\ndef model_gbm(features, labels, test_features, test_ids, \n              nfolds = 5, return_preds = False, hyp = None):\n    \"\"\"Model using the GBM and cross validation.\n       Trains with early stopping on each fold.\n       Hyperparameters probably need to be tuned.\"\"\"\n    \n    feature_names = list(features.columns)\n\n    # Option for user specified hyperparameters\n    if hyp is not None:\n        # Using early stopping so do not need number of esimators\n        if 'n_estimators' in hyp:\n            del hyp['n_estimators']\n        params = hyp\n    \n    else:\n        # Model hyperparameters\n        params = {'boosting_type': 'dart', \n                  'colsample_bytree': 0.88, \n                  'learning_rate': 0.028, \n                   'min_child_samples': 10, \n                   'num_leaves': 36, 'reg_alpha': 0.76, \n                   'reg_lambda': 0.43, \n                   'subsample_for_bin': 40000, \n                   'subsample': 0.54, \n                   'class_weight': 'balanced'}\n    \n    # Build the model\n    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n                               n_jobs = -1, n_estimators = 10000,\n                               random_state = 10)\n    \n    # Using stratified kfold cross validation\n    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n    \n    # Hold all the predictions from each fold\n    predictions = pd.DataFrame()\n    importances = np.zeros(len(feature_names))\n    \n    # Convert to arrays for indexing\n    features = np.array(features)\n    test_features = np.array(test_features)\n    labels = np.array(labels).reshape((-1 ))\n    \n    valid_scores = []\n    \n    # Iterate through the folds\n    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n        \n        # Dataframe for fold predictions\n        fold_predictions = pd.DataFrame()\n        \n        # Training and validation data\n        X_train = features[train_indices]\n        X_valid = features[valid_indices]\n        y_train = labels[train_indices]\n        y_valid = labels[valid_indices]\n        \n        # Train with early stopping\n        model.fit(X_train, y_train, early_stopping_rounds = 100, \n                  eval_metric = macro_f1_score,\n                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n                  eval_names = ['train', 'valid'],\n                  verbose = 200)\n        \n        # Record the validation fold score\n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        \n        # Make predictions from the fold as probabilities\n        fold_probabilitites = model.predict_proba(test_features)\n        \n        # Record each prediction for each class as a separate column\n        for j in range(4):\n            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n            \n        # Add needed information for predictions \n        fold_predictions['idhogar'] = test_ids\n        fold_predictions['fold'] = (i+1)\n        \n        # Add the predictions as new rows to the existing predictions\n        predictions = predictions.append(fold_predictions)\n        \n        # Feature importances\n        importances += model.feature_importances_ \/ nfolds   \n        \n        # Display fold information\n        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n\n    # Feature importances dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names,\n                                        'importance': importances})\n    \n    valid_scores = np.array(valid_scores)\n    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n    \n    # If we want to examine predictions don't average over folds\n    if return_preds:\n        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n        return predictions, feature_importances\n    \n    # Average the predictions over folds\n    predictions = predictions.groupby('idhogar', as_index = False).mean()\n    \n    # Find the class and associated probability\n    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n    predictions = predictions.drop(columns = ['fold'])\n    \n    # Merge with the base to have one prediction for each individual\n    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n        \n    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    # return the submission and feature importances along with validation scores\n    return submission, feature_importances, valid_scores","64892e74":"predictions, gbm_fi = model_gbm(train_set,train_labels,test_set,test_ids,return_preds=True)","ca5f62be":"predictions.head()","e18101c9":"plt.rcParams['font.size'] = 18\n\n# Kdeplot\ng = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', size = 3, aspect = 4)\ng.map(sns.kdeplot, 'confidence');\ng.add_legend();\n\nplt.suptitle('Distribution of Confidence by Fold and Target', y = 1.05);","f81b6b91":"plt.figure(figsize = (24, 12))\nsns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);","cc1f0e8d":"predictions.loc[predictions['idhogar'] == '000a08204',:]","424c22dc":"predictions.groupby('idhogar', as_index = False).mean().head()","c7266258":"# Average the predictions over folds\npredictions = predictions.groupby('idhogar', as_index = False).mean()\n\n# Find the class and associated probability\npredictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\npredictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\npredictions = predictions.drop(columns = ['fold'])","57e84490":"predictions.loc[predictions['idhogar'] == '000a08204',:]","9e236817":"# Plot the confidence by each target\nplt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'confidence', data = predictions);\nplt.title('Confidence by Target');\n\nplt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'confidence', data = predictions);\nplt.title('Confidence by Target');","c10accb5":"%%capture\nsubmission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, return_preds=False)\n\nsubmission.to_csv('gbm_baseline.csv')","660f2b19":"_ = plot_feature_importances(gbm_fi, threshold=0.95)","c4598ca2":"submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, \n                                                               test_selected, test_ids)","1dbd33cc":"model_results","7f6dedeb":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM\", \"GBM_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                                sort = True)\nmodel_results.reset_index(drop=True)","6d999cee":"model_results_ = model_results.set_index('model')\nmodel_results_['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');\n# model_results.reset_index(inplace = True)","e369cdcd":"submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, test_set, test_ids, \n                                             nfolds=10, return_preds=False)","1948808b":"submission.to_csv('gbm_10fold.csv', index = False)","44cfdaa0":"submission, gbm_fi_selected, valid_scores_selected = model_gbm(train_selected, train_labels, test_selected, test_ids,\n                                                               nfolds=10)","a3c1e0c3":"submission.to_csv('gmb_10fold_selected.csv', index = False)","fd018523":"model_results","3faadb51":"model_results = model_results.append(pd.DataFrame({'model': [\"GBM_10Fold\", \"GBM_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean(), valid_scores_selected.mean()],\n                                                   'cv_std':  [valid_scores.std(), valid_scores_selected.std()]}),\n                                    sort = True)","5a13fdbc":"model_results_ = model_results.set_index('model')\nmodel_results_['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n                                  yerr = list(model_results['cv_std']),\n                                 edgecolor = 'k', linewidth = 2)\nplt.title('Model F1 Score Results');\nplt.ylabel('Mean F1 Score (with error bar)');","c84f95e1":"model_results","64014038":"print(f\"There are {gbm_fi_selected[gbm_fi_selected['importance'] == 0].shape[0]} features with no importance.\")","f09997af":"from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\nfrom hyperopt.pyll.stochastic import sample","7d810976":"import csv\nimport ast\nfrom timeit import default_timer as timer","ffcb349b":"def objective(hyperparameters,nfolds=5):\n    \n    global ITERATION\n    ITERATION += 1\n    \n    subsample = hyperparameters['boosting_type'].get('subsample',1.0)\n    subsample_freq = hyperparameters['boosting_type'].get('subsample_freq',0)\n    \n    boosting_type = hyperparameters['boosting_type']['boosting_type']\n    \n    if boosting_type == 'dart':\n        hyperparameters['drop_rate'] = hyperparameters['boosting_type']['drop_rate']\n        \n    hyperparameters['subsample'] = subsample\n    hyperparameters['subsample_freq'] = subsample_freq\n    hyperparameters['boosting_type'] = boosting_type\n    \n    if not hyperparameters['limit_max_depth']:\n        hyperparameters['max_depth'] = -1\n        \n    for parameter_name in ['max_depth','num_leaves','subsample_for_bin','min_child_samples','subsample_freq']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n        \n    if 'n_estimators' in hyperparameters:\n        del hyperparameters['n_estimators']\n    \n    strkfold = StratifiedKFold(n_splits=nfolds, shuffle=True)\n    \n    features = np.array(train_selected)\n    # ????\n    labels = np.array(train_labels).reshape((-1))\n    \n    valid_scores = []\n    best_estimators = []\n    run_times =[]\n    \n    model = lgb.LGBMClassifier(**hyperparameters,class_weight='balanced',n_jobs=-1,metric='None',n_estimators=10000)\n    \n    for i,(train_indices,valid_indices) in enumerate(strkfold.split(features,labels)):\n        \n        X_train,X_valid = features[train_indices],features[valid_indices]\n        y_train,y_valid = labels[train_indices],labels[valid_indices]\n        \n        start = timer()\n        \n        model.fit(X_train,y_train,early_stopping_rounds=100,eval_metric=macro_f1_score,eval_set=[(X_train,y_train),(X_valid,y_valid)],eval_names=['train','valid'],verbose=400)\n        \n        end = timer()\n        \n        valid_scores.append(model.best_score_['valid']['macro_f1'])\n        best_estimators.append(model.best_iteration_)\n        \n        run_times.append(end-start)\n        \n    score = np.mean(valid_scores)\n    score_std = np.std(valid_scores)\n    loss = 1 - score\n    \n    run_time = np.mean(run_times)\n    run_time_std = np.std(run_times)\n    \n    estimators = int(np.mean(best_estimators))\n    hyperparameters['n_estimators'] = estimators\n    \n    of_connection = open(OUT_FILE,'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss,hyperparameters,ITERATION,run_time,score,score_std])\n    of_connection.close()\n    \n    if ITERATION % PROGRESS == 0:\n        display(f'Iteration: {ITERATION}, Current Score: {round(score, 4)}.')\n    \n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'time': run_time, 'time_std': run_time_std, 'status': STATUS_OK, \n            'score': score, 'score_std': score_std}","b98b6bd7":"# Define the search space\nspace = {\n    'boosting_type': hp.choice('boosting_type', \n                              [{'boosting_type': 'gbdt', \n                                'subsample': hp.uniform('gdbt_subsample', 0.5, 1),\n                                'subsample_freq': hp.quniform('gbdt_subsample_freq', 1, 10, 1)}, \n                               {'boosting_type': 'dart', \n                                 'subsample': hp.uniform('dart_subsample', 0.5, 1),\n                                 'subsample_freq': hp.quniform('dart_subsample_freq', 1, 10, 1),\n                                 'drop_rate': hp.uniform('dart_drop_rate', 0.1, 0.5)},\n                                {'boosting_type': 'goss',\n                                 'subsample': 1.0,\n                                 'subsample_freq': 0}]),\n    'limit_max_depth': hp.choice('limit_max_depth', [True, False]),\n    'max_depth': hp.quniform('max_depth', 1, 40, 1),\n    'num_leaves': hp.quniform('num_leaves', 3, 50, 1),\n    'learning_rate': hp.loguniform('learning_rate', \n                                   np.log(0.025), \n                                   np.log(0.25)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 2000, 100000, 2000),\n    'min_child_samples': hp.quniform('min_child_samples', 5, 80, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.5, 1.0)\n}","f71ac196":"sample(space)","ad362f32":"algo = tpe.suggest","eeee844d":"trials = Trials()\n\nOUT_FILE = 'optimization.csv'\nof_connection = open(OUT_FILE,'w')\nwriter = csv.writer(of_connection)\n\nMAX_EVALS = 100\nPROGRESS = 10\nN_FOLDS = 5\nITERATION = 0\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score', 'std']\nwriter.writerow(headers)\nof_connection.close()","f6715f6f":"display(\"Running Optimization for {} Trials.\".format(MAX_EVALS))\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)","2ece79b2":"import json\n\n# Save the trial results\nwith open('trials.json', 'w') as f:\n    f.write(json.dumps(str(trials)))","856e7b6d":"results = pd.read_csv(OUT_FILE).sort_values('loss', ascending = True).reset_index()\nresults.head()","1ab41162":"plt.figure(figsize = (8, 6))\nsns.regplot('iteration', 'score', data = results);\nplt.title(\"Optimization Scores\");\nplt.xticks(list(range(1, results['iteration'].max() + 1, 3)));","cfb3d14a":"best_hyp = ast.literal_eval(results.loc[0, 'hyperparameters'])\nbest_hyp","766b07d8":"submission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, \n                                             test_selected, test_ids, \n                                             nfolds = 10, return_preds=False)\n\nmodel_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold_SEL\"], \n                                                   'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True).sort_values('cv_mean', ascending = False)","205fb9ae":"submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n                                             test_set, test_ids, \n                                             nfolds = 10, return_preds=False)\n\nmodel_results = model_results.append(pd.DataFrame({'model': [\"GBM_OPT_10Fold\"], \n                                                   'cv_mean': [valid_scores.mean()],\n                                                   'cv_std':  [valid_scores.std()]}),\n                                    sort = True).sort_values('cv_mean', ascending = False)","5f88d19b":"model_results.head()","47408dbb":"submission, gbm_fi, valid_scores = model_gbm(train_selected, train_labels, \n                                             test_selected, test_ids, \n                                             nfolds = 10, return_preds=False)","1ba43b98":"submission.to_csv('gbm_opt_10fold_selected.csv', index = False)","f4468b33":"_ = plot_feature_importances(gbm_fi)","73b1af8b":"preds = submission_base.merge(submission,on='Id',how='left')\npreds = pd.DataFrame(preds.groupby('idhogar')['Target'].mean())\n\nfig,axes = plt.subplots(1,2,sharey=True,figsize=[12,6])\nheads['Target'].sort_index().plot.hist(normed=True,edgecolor=r'k',linewidth=2,ax=axes[0])\naxes[0].set_xticks([1,2,3,4])\naxes[0].set_xticklabels(poverty_mapping.values(),rotation=60)\naxes[0].set_title('Train Label Distribution')\n\n\npreds['Target'].sort_index().plot.hist(normed = True, \n                                       edgecolor = 'k',\n                                       linewidth = 2,\n                                       ax = axes[1])\naxes[1].set_xticks([1, 2, 3, 4]);\naxes[1].set_xticklabels(poverty_mapping.values(), rotation = 60)\nplt.subplots_adjust()\nplt.title('Predicted Label Distribution');","efc88443":"heads['Target'].value_counts()","08264519":"preds['Target'].value_counts()","5687afbf":"### 3. Algorithm\nThe algorithm for choosing the next values is the Tree Parzen Estimator which uses Bayes rule for constructing a surrogate model of the objective function. Instead of maximizing the objective function, the algorithm maximizes the Expected Improvement (EI) of the surrogate model. <br \/>\n=> \ub2e4\uc74c \uac12\uc744 \uace0\ub974\ub294 \uc54c\uace0\ub9ac\uc998\uc740 \ubaa9\uc801\ud568\uc218\uc758 \uc784\uc2dc \ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30\uc704\ud55c \ubca0\uc774\uc988\uc774\ub860\uc744 \ub530\ub974\ub294 Tree Parzen Estimator\uc774\ub2e4. \ubaa9\uc801\ud568\uc218\ub97c \ucd5c\ub300\ud654 \uc2dc\ud0a4\ub294 \uac83 \ub300\uc2e0\uc5d0 \uc54c\uace0\ub9ac\uc998\uc740 \uc784\uc2dc \ubaa8\ub378\uc758 Expected Improvement\ub97c \ucd5c\ub300\ud654\uc2dc\ud0a8\ub2e4. \n","1e8761c6":"## Id Variables<br \/>\nThese are pretty simple: they will be kept as is in the data since we need them for identification.<br \/>\n=> Id \uac12\uc740 \uac01\uac01\uc744 \uc2dd\ubcc4\ud558\uae30 \uc704\ud574 \ubc18\ub4dc\uc2dc \ud544\uc694\ud558\ubbc0\ub85c \uc0ad\uc81c\ub4f1\uc744 \ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.","075105fa":"## Exploring Household Variables [\uac00\uad6c\ubcc4 \ubcc0\uc218]<br \/>\nAfter going to all the trouble of getting our features in order, now we can take a look at them in relation to the Target. <br \/>\n=> \uc21c\uc11c\ub97c \uac00\uc9c4 \uceec\ub7fc\ub4e4\uc758 \ubb38\uc81c\ub4e4\uc744 \ubaa8\ub450 \ud574\uacb0 \ud588\uc73c\ub2c8 \uc774\uc81c \ud0c0\uac9f \ub77c\ubca8\uacfc\uc758 \uad00\uacc4\ub97c \uc0b4\ud3b4\ubd05\uc2dc\ub2e4<br \/>\nWe've already done a little of this, but now we can try to quantify relationships.<br \/>\n=> \uc774\ubbf8 \uc870\uae08\ud588\uc9c0\ub9cc, \uc591\uc801\uc778 \uad00\uacc4\ub97c \u314f\u3139\ud3b4\ubd05\uc2dc\ub2e4\n\n## Measuring Relationships<br \/>\nThere are many ways for measuring relationships between two variables. Here we will examine two of these:<br \/>\n=> \uad00\uacc4\ub97c \uce21\uc815\ud558\ub294 \ubc29\ubc95\uc5d0\ub294 \uc5ec\ub7ec\ubc29\ubc95\uc774 \uc788\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \uadf8\uc911 \ub450\uac00\uc9c0\ub9cc \ucc44\ud0dd\ud569\uc2dc\ub2e4\n\n   **The Pearson Correlation: from -1 to 1 measuring the linear relationship between two variables [\uc77c\ubc18\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \uc0c1\uad00\uacc4\uc218]<br \/>\n   The Spearman Correlation: from -1 to 1 measuring the monotonic relationship between two variables [\ud574\ub2f9 \uc9c0\ud45c\uac00 \uc810\ucc28\uc801\uc73c\ub85c \uc99d\uac00 \ub610\ub294 \uac10\uc18c\ud558\ub294 \uad00\uac8c\ub97c \ubcf4\uc5ec\uc8fc\ub294 \uc9c0\ud45c]<br \/>**\n\nThe Spearman correlation is 1 if as one variable increases, the other does as well, even if the relationship is not linear. \n\nOn the other hand, the Pearson correlation can only be one if the increase is exactly linear. These are best illustrated by example.","6b7a8e5d":"# Upgrading Our Model: Gradient Boosting Machine\nAfter using the Random Forest and getting decent scores, it's time to step up and use the gradient boosting machine. If you spend any time on Kaggle, you'll notice that the Gradient Boosting Machine (GBM) wins a high percentage of competitions where the data is structured (in tables) and the datasets are not that large (less than a million observations). <br \/>\n=> \uc6b0\ub9ac\uac00 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub97c \ud1b5\ud574 \uc88b\uc740 \uc810\uc218\ub97c \ubc1b\uc740 \uc774\ud6c4\uc5d0, \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305\ubaa8\ub378\uc744 \uc774\uc6a9\ud560 \ucc28\ub840\uc785\ub2c8\ub2e4. \ub9cc\uc57d \ub3c5\uc790\uac00 \uce90\uae00\uc5d0 \uc775\uc219\ud55c \uc0ac\uc6a9\uc790\ub77c\uba74 GBM\ubaa8\ub378\uc774 \ub9ce\uc740 \ub370\uc774\ud130\uac00 \uc815\ud615\ud654\ub418\uc5b4\uc788\uace0 \ub370\uc774\ud130\uc14b\uc774 \ud06c\uc9c0 \uc54a\uc740 \ucef4\ud53c\ud2f0\uc158\uc5d0\uc11c \uc2b9\ub9ac\ud55c \uac83\uc744 \ubcf4\uc558\uc744 \uac83\uc785\ub2c8\ub2e4.<br \/>\n\nI won't go too much into the details here, but instead will focus on the implementation. We'll use the GBM in LightGBM, although there are also options in Scikit-Learn, XGBOOST, and CatBoost. The first set of hyperparameters we'll use were based on those I've found have worked well for other problems. <br \/>\n=> \ub9ce\uc740 \uc790\uc138\ud55c \uac83\ub4e4\uc744 \uc5b8\uae09\ud558\uc9c4 \uc54a\uaca0\uc9c0\ub9cc \uc2e4\ud589\uc5d0 \ucd08\uc810\uc744 \ub9de\ucdb0\ubd05\uc2dc\ub2e4. \uc6b0\ub9ac\ub294 LightGBM\uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\uc9c0\ub9cc \ub2e4\ub978 \uc5ec\ub7ec \uc120\ud0dd\uc9c0\uac00 \uc0ac\uc774\ud0b7\ub7f0,XGBOOST,CatBoost\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uccab\ubc88 \uc9f8 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uc758 \uc138\ud2b8\ub294 \uc6b0\ub9ac\uac00 \uc704\uc5d0\uc11c \ucc3e\uc544\uc654\ub358 \uac83\ub4e4\uc744 \uae30\ubc18\uc73c\ub85c \uc138\ud305\ud558\uaca0\uc2b5\ub2c8\ub2e4.<br \/>\n\n## Choosing Number of Estimators with Early Stopping\nTo choose the number of estimators (the number of decision trees in the ensemble, called n_estimators or num_boost_rounds), we'll use early stopping with 5-fold cross validation. This will keep adding estimators until the performance as measured by the Macro F1 Score has not increased for 100 training rounds. To use this metric, we'll have to define a custom metric. <br \/>\n=>\ubc18\ubcf5\uc2dc\ud589\uc758 \uc218\ub97c \uc120\ud0dd\ud558\uae30 \uc704\ud574 \uc6b0\ub9ac\ub294 5\uacb9 \uad50\ucc28\uac80\uc99d\uc744 \uc774\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4. \uc774\uac83\uc740 \uc6b0\ub9ac\uac00 \uc774\uc6a9\ud558\ub294 \ucc99\ub3c4 Macro F1 \uc218\uce58\uac00 100\ubc88\uc758 \ud6c8\ub828 \ubc18\ubcf5\uc2dc\ud589\uc5d0 \uc788\uc5b4\uc11c \uc99d\uac00\ud558\uc9c0 \uc54a\uc744 \ub54c \uae4c\uc9c0 \ubc18\ubcf5\uc2dc\ud589\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4. \uc774 \uc218\uce58\ub97c \uc774\uc6a9\ud558\uae30 \uc704\ud574 custom metric\uc744 \uc774\uc6a9\ud569\uc2dc\ub2e4.<br \/>","3aa2feac":"## Recursive Feature Elimination with Random Forest\nThe RFECV in Sklearn stands for Recursive Feature Elimination with Cross Validation. The selector operates using a model with feature importances in an iterative manner. At each iteration, it removes either a fraction of features or a set number of features. The iterations continue until the cross validation score no longer improves. <br \/>\n=> RFECV\ub294 \uad50\ucc28\uac80\uc99d\uc744 \uc774\uc6a9\ud55c \ubc18\ubcf5\uc801 \ud2b9\uc9d5 \uc81c\uac70\ubc95\uc785\ub2c8\ub2e4. \uc120\ud0dd\uc790\ub294 \ubc18\ubcf5\uc801\uc778 \ubc29\ubc95\uc744 \ud1b5\ud574\uc11c \ubaa8\ub378\uc758 feature_importance\ub97c \uc9c4\ud589\ud569\ub2c8\ub2e4. \uac01\uac01\uc758 \ubc18\ubcf5\uc5d0\uc11c, RFECV\ub294 \ud2b9\uc9d5\uc740 \ubd80\ubd84\uc774\ub098 \ud2b9\uc9d5\uc758 \uac1c\uc218\ub97c \uc810\uc810 \uc904\uc5ec\uac11\ub2c8\ub2e4. \uc774 \ubc18\ubcf5\uc740 \ub354\uc774\uc0c1 \uad50\ucc28\uac80\uc99d \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc744 \ub54c \uae4c\uc9c0 \uc9c4\ud589\ud569\ub2c8\ub2e4.\n\nTo create the selector object, we pass in the the model, the number of features to remove at each iteration, the cross validation folds, our custom scorer, and any other parameters to guide the selection. <br \/>\n\uc120\ud0dd\uc790 \uac1d\uccb4\ub97c \uc0dd\uc131\ud558\uae30 \uc704\ud574, \uc6b0\ub9ac\ub294 \ubaa8\ub378\uc744 \ub118\uae30\uace0, \uac01\uac01\uc758 \ubc18\ubcf5\uc2dc\ud589\ub9c8\ub2e4 \uc5c6\uc568 \uceec\ub7fc\ub4e4\uc758 \uac1c\uc218, \uad50\ucc28\uac80\uc99d\uc758 \uacb9\uc218, \uc6b0\ub9ac\uc758 custom scorer \uadf8\ub9ac\uace0 \uc120\ud0dd\uc790\uac00 \ucc38\uace0\ud560 \uc218 \uc788\ub294 \ub2e4\ub978 \ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ub118\uaca8\uc90d\ub2c8\ub2e4.","4ab4efdf":"## Plot Two Categorical Variables [\ub450\uac1c\uc758 \uce74\ud14c\uace0\ub9ac\uceec\ud55c \ubcc0\uc218\uc5d0 \ub300\ud55c \uc2dc\uac01\ud654]<br \/>\nTo show how two categorical variables interact with one another, there are a number of plotting options: scatterplots, faceted bar plots, boxplots, etc. <br \/>\n=> \ub450 \uac1c\uc758 \uce74\ud14c\uace0\ub9ac\uceec\ud55c \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud574\uc11c \uc2dc\uac01\ud654\ud558\ub294 \ubc29\ubc95\uc5d0\ub294: \uc0b0\ud3ec\ub3c4, \ubc15\uc2a4\uadf8\ub9bc \ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4<br \/>\nI wasn't satisfied with any of these choices so I wrote the function below, which essentially is a scatterplot of two categoricals where the size of the points represent the percentage of a given y-value represented by each x-value.\n=>","6f7153d4":"We can see some of the variables that we made are highly correlated with the Target. <br \/>\n=> \ud0c0\uac9f \ub77c\ubca8\uacfc \ub192\uc740 \uad00\ub828\uc131\uc744 \ubcf4\uc774\ub294 \ubcc0\uc218\ub4e4\uc744 \ucc3e\uc544\ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nWhether these variables are actually useful will be determined in the modeling stage.<br \/>\n=> \uc774 \uc815\ubcf4\ub294 \ubaa8\ub378\ub9c1 \ub2e8\uacc4\uc5d0\uc11c \uc720\uc6a9\ud560\uc9c0 \uc544\ub2d0\uc9c0\uac00 \uacb0\uc815\ub420 \uac83\uc785\ub2c8\ub2e4.<br \/>","6e69979a":"To resume training, we can pass in the same trials object and increase the max number of iterations. For later use, the trials can be saved as json.","7bc0c549":"The size of the markers represents the raw count. <br \/>\n=> \ud558\ub098\uc758 \uc810\uc758 \ud06c\uae30\ub294 \uac1c\uc218\ub97c \ub73b\ud569\ub2c8\ub2e4<br \/>\nTo read the plot, choose a given y-value and then read across the row. <br \/>\n=> \ud45c\ub97c \uc77d\uc73c\ub824\uba74, \uc784\uc758\uc758 \uc138\ub85c\ucd95 \uac12\uc744 \uace0\ub974\uace0 \uac00\ub85c\ub85c \ud558\ub098\uc529 \uc77d\uc73c\uba74 \ub429\ub2c8\ub2e4.<br \/>\nFor example, with a poverty level of 1, 93% of individuals have no years behind with a total count of around 800 individuals and about 0.4% of individuals are 5 years behind with about 50 total individuals in this category.<br \/>\n=> \uc608\ub97c\ub4e4\uba74 \ube48\uace4\ub2e8\uacc4\uac00 1\uc778\uacbd\uc6b0\uc5d0\ub294, 93\ud37c\uc13c\ud2b8\uc758 \uac1c\uac1c\uc778\uc774 years behind\uac00 0\uc774\uba70 \uc774 \ud06c\uae30\ub294 \ub300\ub7b5 800\uba85 \uc815\ub3c4 \ub41c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.<br \/>\nThis plot attempts to show both the overall counts and the within category proportion; it's not perfect , but I gave it a shot!","0fb3ae00":"Higher levels of education seem to correspond to less extreme levels of poverty. <br \/>\n=> \ub192\uc740 \uad50\uc721\uc218\uc900\uc740 \ud0c0\uac9f\ub77c\ubca8\uc774 \ub192\uc740 \uc218\uc900\uc73c\ub85c \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\uba70<br \/>\nWe do need to keep in mind this is on an individual level though and we eventually will have to aggregate this data at the household level.<br \/>\n=> \uc6b0\ub9ac\ub294 \uac1c\ubcc4\uc801\uc778 \uceec\ub7fc\uc744 \uc804\uccb4 \uac00\uad6c\uc218\uc900\uc758 \uceec\ub7fc\uc73c\ub85c \ud569\uce58\ub824\ub294 \uc791\uc5c5\uc744 \ud574\uc57c \ud55c\ub2e4\ub294 \uac83\uc744 \uba85\uc2ec\ud574\uc57c\ud55c\ub2e4.","b037210e":"For the most part, the two methods of calculating correlations are in agreement. Just out of curiousity, we can look for the values that are furthest apart.","cdffab34":"We are dealing with an imbalanced class problem (which makes it intriguing why the contest organizers choose the macro F1 score as the metric instead of weighted F1!). <br \/>\n=> \uc704\uc758 \uadf8\ub9bc\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ud074\ub798\uc2a4 \ubd88\uade0\ud615 \ubb38\uc81c\ub97c \ub2e4\ub8e8\uace0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4 (\uc774\ub294 \uc65c \ucee8\ud14c\uc2a4\ud2b8\uc758 \uc8fc\ucd5c\uc790\uac00 \uac00\uc911 F1 \uc810\uc218\uac00 \uc544\ub2cc Macro F1\uc810\uc218\ub97c \ud0dd\ud588\ub294\uc9c0 \uc758\ubb38\uc810\uc744 \ub4e4\uac8c \ud569\ub2c8\ub2e4)<br \/>\nThere are many more households that classify as non vulnerable than in any other category. The extreme poverty class is the smallest (I guess this should make us optimistic!).<br \/>\n=> \ub2e4\ub978 \uce74\ud14c\uace0\ub9ac\ubcf4\ub2e4 \ucde8\uc57d\uacc4\uce35\uc774 \uc544\ub2cc \uac00\uad6c\ub4e4\uc774 \ub354 \ub9ce\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uadf9\ub3c4\uc758 \ube48\uace4\uac8c\uce35\uc740 \uac00\uc7a5 \uc791\uc74c\uc744 \ub610\ud55c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nOne problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. <br \/>\n=> \ud074\ub798\uc2a4 \ubd88\uade0\ud615\ubb38\uc81c\ub294 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc774 \uc18c\uc218\uc758 \ud074\ub798\uc2a4\ub4e4\uc744 \uc608\uce21\ud558\ub294\ub370 \ubb38\uc81c\uc810\uc774 \ubc1c\uc0dd\ud558\ub294 \uc6d0\uc778\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uadf8\uac83\uc5d0 \ub300\ud55c \uc608\uc81c\uac00 \uc801\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\nThink about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. [\uc778\uac04\uc758 \uad00\uc810\uc5d0\uc11c\uc758 \uc774\ud574\ub97c \ub3d5\ub294 \uc608\uc2dc]\n\nOne potential method to address class imbalanceds is through oversampling (which is covered in more advanced notebooks).<br \/>\n=> \ub530\ub77c\uc11c \uc774\ub7ec\ud55c \ud074\ub798\uc2a4 \ubd88\uade0\ud615 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc740 \uc624\ubc84\uc0d8\ud50c\ub9c1 \uc774\ub77c\ub294 \ubc29\ubc95\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4.","1a0fb34f":"## The next variable will be a warning about the quality of the house. <br \/>\n=> \ub2e4\uc74c\uc73c\ub85c\ub294 \uc9d1\uc758 \uc704\ud5d8\ub3c4 \uc815\ub3c4\ub97c \ub098\ud0c0\ub0b4\ubcf4\uc790<br \/>\nIt will be a negative value, with -1 point each for no toilet, electricity, floor, water service, and ceiling.","58ea64be":"Later on we'll calculate correlations between the variables and the Target to gauge the relationships between the features, <br \/>\n=> \ub098\uc911\uc5d0 \ud53c\uccd0\ub4e4 \uac04\uc5d0 \uad00\uac8c\ub97c \ubcf4\uae30 \uc704\ud574 \uac01\uac01\uc758 \ubcc0\uc218\ub4e4\uacfc \ud0c0\ucf13\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uacc4\uc0b0\ud560 \uac83\uc774\ub2e4 <br \/>\nbut these plots can already give us a sense of which variables may be most \"relevant\" to a model. <br \/>\n=> \uc774\ub7ec\ud55c \uadf8\ub9bc\uc740 \uc774\ubbf8 \uc5f0\uad00\uc131\uc5d0 \ub300\ud55c \uad00\uc810\uc744 \uc870\uae08\uc740 \ubcf4\uc5ec\uc8fc\ub294 \ub4ef\ud558\ub2e4<br \/>\nFor example, the meaneduc, representing the average education of the adults in the household appears to be related to the poverty level: a higher average adult education leads to higher values of the target which are less severe levels of poverty. <br \/>\n=> \uac04\ub2e8\ud788 \uc608\ub97c\ub4e4\uba74 meaneduc\uc758 \uacbd\uc6b0\uc5d0\ub294 \ube48\uace4\uacc4\uce35 \ub2e8\uacc4\uc640 \uad00\ub828\uc774 \uc788\uc5b4\ubcf4\uc774\ub294 \uac83 \uac19\ub2e4: \uad50\uc721 \uc218\uc900 \uc815\ub3c4\uac00 \ub192\uc744 \uc218\ub85d \ube48\uace4\uacc4\uce35 \ub2e8\uacc4\uac00 \ub354 4\ub85c \ub192\uc544\uc9c0\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc774\ub294 \uad50\uc721\uc218\uc900\uc774 \ub192\uc744 \uc218\ub85d \ube48\uace4\uc758 \uc218\uc900\uc774 \ub0ae\uac8c \ub428\uc744 \ubcfc \uc218 \uc788\ub2e4.<br \/>\nThe theme of the importance of education is one we will come back to again and again in this notebook!<br \/>\n=> \uad50\uc721\uc758 \uc911\uc694\uc131\uc744 \uc2e4\uac10\ud560 \uc218 \uc788\ub2e4.","e8c97f5a":"What we see here is that the confidence for each class if relatively low. It does appear that the model has greater confidence in Target=4 predictions which makes sense because of the class imbalance and the high prevalence of this label. <br \/>\n=> \uc6b0\ub9ac\uac00 \uc54c\uc218 \uc788\ub294 \uac83\uc740 \uac01 \ud074\ub798\uc2a4\uc758 confidence\uac00 \uc0c1\ub300\uc801\uc73c\ub85c \ub0ae\ub2e4\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\uac00 \uc774\uc804\uc5d0\ub3c4 \uc54c\uc544\ucc28\ub838\ub358 \uac83 \ucc98\ub7fc \uc774 \ubb38\uc81c\uc5d0\ub294 class imbalance\ubb38\uc81c\uac00 \uc788\uae30 \ub54c\ubb38\uc5d0 Target=4 \uc5d0 \ub300\ud55c confidence\uac00 \uad49\uc7a5\ud788 \ub192\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4.\n\nAnother way to look at the information is as a violinplot. This shows the same information, with the number of observations related to the width of the plot. <br \/>\n=> \uc774\ub97c \ubcf4\ub294 \ub2e4\ub978 \ubc29\ubc95\uc740 \ubc14\uc774\uc62c\ub9b0 \ud50c\ub86f\uc744 \uc774\uc6a9\ud558\ub294 \uac83\uc774\ub2e4.","28f0f4ec":"The Id and idhogar object types make sense because these are identifying variables. <br \/>\nId\uc640 idhogar\uac00 \uac1d\uccb4\ud615 \uc790\ub8cc\uc778 \uac83\uc740 \ub9de\ub294 \ub9d0\uc778 \uac83 \uac19\ub2e4<br \/>\nHowever, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. According to the documentation for these columns:<br \/>\n\uadf8\ub7ec\ub098 \ub2e4\ub978 \uceec\ub7fc\ub4e4\uc740 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc5d0 \ub123\uae30\uc804\uc5d0 \ucc98\ub9ac\ub97c \uac70\uccd0\uc57c \ud560 \ubb38\uc790\ud615\uacfc \uc22b\uc790\ud615 \uc790\ub8cc\uc758 \ud63c\ud569\ud615\uc778 \ub4ef\ud558\ub2e4. \ub2e4\uc74c\uc740 \ud574\ub2f9 \uceec\ub7fc\ub4e4\uc5d0 \ub300\ud55c \uc815\ubcf4\uc774\ub2e4<br \/>\ndependency: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n\nedjefe: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nedjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nThese explanations clear up the issue. For these three variables, \"yes\" = 1 and \"no\" = 0. We can correct the variables using a mapping and convert to floats.<br \/>\n\uc704\ub4e4 \uceec\ub7fc\uc744 \ubcf4\uba74 yes\ub294 1, no\ub294 0 \uc73c\ub85c mapping\ud560 \ud544\uc694\uac00 \uc788\uc5b4 \ubcf4\uc778\ub2e4.","b6ef553b":"Creating Ordinal Variables [\uc21c\uc11c\ucc99\ub3c4 \ub9cc\ub4e4\uae30]<br \/>\nMuch as we did with the household level data, we can map existing columns to an ordinal variable.<br \/>\n=> \uac00\uad6c\ub2e8\uc704\uc5d0\uc11c \ud588\ub4ef\uc774 \ub17c\ub9ac\ud615 \uceec\ub7fc\ub4e4\uc744 \uacb0\ud569\ud558\uc5ec \ud558\ub098\uc758 \uc21c\uc11c\ucc99\ub3c4 \uceec\ub7fc\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nHere we will focus on the instlevel_ variables which indicate the amount of education an individual has from instlevel1: no level of education to instlevel9: postgraduate education.<br \/>\n=> instlevel_ \ubcc0\uc218\ub4e4\uc740 \uac1c\uac1c\uc778\uc758 \uad50\uc721\uc758 \uc815\ub3c4\ub97c \ub098\ud0c0\ub0b4\ub294\ub370 1\ubd80\ud130 9\uae4c\uc9c0 \uc874\uc7ac\ud569\ub2c8\ub2e4.\n\nTo create the ordinal variable, for each individual, we will simply find which column is non-zero. <br \/>\n=> \uc21c\uc11c\ucc99\ub3c4\ud654 \ud558\uae30\uc704\ud574 \uac01 \uac1c\ubcc4 \ubcc0\uc218\ub4e4\uc5d0 0\uc774 \uc544\ub2cc \uceec\ub7fc\ub4e4\uc744 \ucc3e\uae30\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4.<br \/>\nThe education has an inherent ordering (higher is better) so this conversion to an ordinal variable makes sense in the problem context.<br \/>\n=> \uad50\uc721\uc218\uc900\uc740 \ub192\uc740 \uc21c\uc11c\uac00 \ub354 \uc88b\uc740 \uad50\uc721\uc218\uc900\uc744 \ub098\ud0c0\ub0b4\ub3c4\ub85d \ud569\uc2dc\ub2e4","99772e72":"If you run LinearDiscriminantAnalysis without filtering out the UserWarnings, you get many messages saying \"Variables are collinear.\" This might give us a hint that we want to remove some collinear features! We might want to try this model again after removing the collinear variables because the score is comparable to the random forest. <br \/>\n=> \ud574\ub2f9 \ubaa8\ub378\uc744 \uacbd\uace0\ucc3d\uc744 \uc5c6\uc560\uc9c0 \uc54a\uace0 \ud55c\ub2e4\uba74 \uc704\uc640 \uac19\uc740 \uacbd\uace0\uba54\uc2dc\uc9c0\ub97c \ub9cc\ub0a0 \uc218 \uc788\ub294\ub370 \uc774\ub294, \uc720\uc0ac\ud55c \uc131\uaca9\uc744 \uac00\uc9c4 \uceec\ub7fc\uc774 \ub9ce\uc544\uc11c \ub2e4\uc911 \uacf5\uc120\uc131\uc758 \ubb38\uc81c\uac00 \uc0dd\uae38 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\uac00 \uc81c\uc77c \ucc98\uc74c\uc5d0 \uac00\uc774\ub4dc \ub77c\uc778\uc73c\ub85c \uc0ac\uc6a9\ud588\ub358 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc640 \ube44\uc2b7\ud55c \uc810\uc218\ub97c \ub0b4\uae30 \ub54c\ubb38\uc5d0 \uacf5\uc120\uc131 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\uace0 \uc774 \ubaa8\ub378\uc744 \ub2e4\uc2dc \uc2dc\ud5d8\ud574 \ubcfc \uac83\uc785\ub2c8\ub2e4.","9086500e":"In most cases the accuracy gain is less than 10% so the worst model is probably not suddenly going to become the best model through tuning. <br \/>\n=> \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0\uc5d0 \uc815\ud655\ub3c4\ub97c \ub9ce\uc544\ub3c4 10\ud37c\uc13c\ud2b8\uae4c\uc9c0\ub9cc \uc62c\ub9b4 \uc218 \uc788\ub2e4. \n\nFor now we'll say the random forest does the best. Later we'll look at using the Gradient Boosting Machine, although not implemented in Scikit-Learn. <br \/>\n=> \uc9c0\uae08\uc740 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uac00 \ucd5c\uace0\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\uc9c0\ub9cc, \ub098\uc911\uc5d0 \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\uac8c\ub420 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubcfc \uac83\uc774\ub2e4. <br \/>\nInstead we'll be using the more powerful LightGBM version. Now, let's turn to making a submission using the random forest. <br \/>\n=> \uc6b0\ub9ac\ub294 \ub354 \uac15\ub825\ud55c LightGBM\ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\ub2e4.","2d4de19f":"We can have the function instead return the actual submission file. This takes the average predictions across the five folds, in effectm combining 5 different models, each one trained on a slghtly different subset of the data. <br \/>\n=> \uc6b0\ub9ac\ub294 \uc2e4\uc81c \uc81c\ucd9c\ud30c\uc77c\uc744 \ubc18\ud658\ud558\ub294 \ud568\uc218\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4. \uc774\ub294 5\uac1c\uc758 \uacc4\uce35\uc744 \ud3c9\uade0\ub0b4\uc5c8\uc73c\uba70, \uac01\uac01\uc758 \ubaa8\ub378\uc740 \ub370\uc774\ud130\uc758 \ub2e4\ub978 \uacb9\ub4e4\ub85c \ud6c8\ub828\ub418\uc5b4 \uc57d\uac04\uc529 \ucc28\uc774\uac00 \uc788\ub2e4.","919ccff3":"The function below takes in a model, a training set, the training labels, and a testing set and performs the following operations: <br \/>\n\nTrains the model on the training data using fit<br \/>\nMakes predictions on the test data using predict<br \/>\nCreates a submission dataframe that can be saved and uploaded to the competition<br \/>","3851ae1b":"The best model seems to be the Gradient Boosting Machine trained with 10 folds on the selected features. This model has not yet been optimized, but we might be able to get a little more performance through optimization. <br \/>\n\uac00\uc7a5 \ucd5c\uace0\uc758 \ubaa8\ub378\uc740 \uc120\ud0dd\ub41c \ud2b9\uc9d5\ub4e4\ub85c \uc774\uc6a9\ud55c 10\uacb9 \uad50\ucc28\uac80\uc99d\uc73c\ub85c \ud6c8\ub828\ub41c \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc778\uac83 \uac19\uc2b5\ub2c8\ub2e4. [\ubc88\uc5ed\uc790\ub294 \uadf8\ub9bc\uc5d0\uc11c \ubcf4\ub2e4\uc2dc\ud53c \ub2e4\ub974\uc9c0\ub9cc \ud574\uc11d\uc740 \uac19\uac8c \ud558\uaca0\uc2b5\ub2c8\ub2e4.] \uc774 \ubaa8\ub378\uc740 \uc544\uc9c1 \ucd5c\uc801\ud654\uac00 \ub418\uc5b4\uc788\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \ucd5c\uc801\ud654\ub97c \ud1b5\ud574\uc11c \uc57d\uac04\uc758 \uc131\ub2a5\ud5a5\uc0c1\uc744 \ud560 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4.","e8dc11af":"We can see that for every value of the Target, the most common source of electricity is from one of the listed providers.<br \/>\n=> \uadf8\ub9bc\uc5d0\uc11c\ub3c4 \ubcfc \uc218 \uc788\ub4ef\uc774 \uac00\uc7a5 \ud3c9\ubc94\ud55c \ubc29\ubc95\uc740 \uacf5\uc2dd\uc801\uc778 \uacf5\uae09\uc790\uc5d0\uac8c \ubc1b\ub294 \uac83\uc785\ub2c8\ub2e4.\n\nThe final redundant column is area2. <br \/>\n=> \ub9c8\uc9c0\ub9c9 \uceec\ub7fc\uc740 area2\uc785\ub2c8\ub2e4.<br \/>\nThis means the house is in a rural zone, but it's redundant because we have a column indicating if the house is in a urban zone. <br \/>\n=> \uc774 \uceec\ub7fc\uc740 \uc9d1\uc774 \uad50\uc678\uc5d0 \uc788\ub294\uc9c0\ub97c \ubb3c\uc5b4\ubcf4\ub294 \uac83\uc785\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc774\ubbf8 \uc9d1\uc774 \ub3c4\uc2ec\uc5d0 \uc788\ub294\uc9c0\uc5d0 \ub300\ud55c \uceec\ub7fc\uc774 \uc788\uae30 \ub54c\ubb38\uc5d0 \ud574\ub2f9 \uceec\ub7fc\uc740 \uc911\ubcf5\ub418\uc5c8\ub2e4\uace0 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nTherefore, we can drop this column.<br \/>\n=> \uadf8\ub7ec\ubbc0\ub85c \ud574\ub2f9 \uceec\ub7fc\uc744 \uc81c\uac70\ud569\ub2c8\ub2e4.","c9224165":"The columns with only 2 unique values represent Booleans (0 or 1). <br \/>\n=> \uac12\uc774 \ub450 \uac1c \ubc16\uc5d0 \uc5c6\ub2e4\ub294 \uac83\uc740 \ub17c\ub9ac\ud615\uc744 \ub73b\ud569\ub2c8\ub2e4.<br \/>\nIn a lot of cases, this boolean information is already on a household level.<br \/>\n=> \uc774\ub7ec\ud55c \ub17c\ub9ac\ud615 \uc815\ubcf4\ub294 \uac00\uad6c \uc804\uccb4\uc758 \ub2e8\uacc4\ub97c \uc704\ud574 \uc774\ubbf8 \uc900\ube44\ub418\uc5c8\uc2b5\ub2c8\ub2e4<br \/>\nFor example, the refrig column says whether or not the household has a refrigerator. <br \/>\n=> \uadf8 \uc911 \ud558\ub098\ub85c refig\ub294 \uac00\uad6c\uac00 \ub0c9\uc7a5\uace0\ub97c \uac00\uc9c0\ub294\uc9c0 \uc544\ub2cc\uc9c0\ub97c \uac00\ub974\ub294 \uc815\ubcf4\uc785\ub2c8\ub2e4.<br \/>\nWhen it comes time to make features from the Boolean columns that are on the household level, we will not need to aggregate these. <br \/>\n=> \uc774\ub7ec\ud55c \ub17c\ub9ac\ud615 \uc815\ubcf4\ub294 \uac00\uad6c\ub2f9 \uc815\ubcf4\uc774\uae30 \ub54c\ubb38\uc5d0 \ub530\ub85c \ucde8\ud569\ud560 \ud544\uc694\uc131\uc740 \uc5c6\ub294 \ub4ef \ud569\ub2c8\ub2e4<br \/>\nHowever, the Boolean columns that are on the individual level will need to be aggregated.<br \/>\n=> \uadf8\ub7ec\ub098 \uc77c\ubd80 \uac1c\uac1c\uc778\uc744 \uc704\ud55c \ub17c\ub9ac\ud615 \uc815\ubcf4\ub294 \ucde8\ud569\uc758 \uc5ec\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\nFloat Columns [\uc2e4\uc218\ud615 \uceec\ub7fc]<br \/>\nAnother column type is floats which represent continuous variables.<br \/>\n=> \uc5f0\uc18d\ud615 \ubcc0\uc218\ub97c \ub300\ud45c\ud558\ub294 \uc2e4\uc218\ud615 \uceec\ub7fc\uc785\ub2c8\ub2e4<br \/>\nWe can make a quick distribution plot to show the distribution of all float columns. <br \/>\n=> \ubaa8\ub4e0 \uc815\uc218\ud615 \uceec\ub7fc\uc758 \ubd84\ud3ec\ub97c \uac04\ub2e8\ud558\uac8c \uc2dc\uac01\ud654\ud574\ubcf4\ub824\uace0 \ud569\ub2c8\ub2e4<br \/>\nWe'll use an OrderedDict to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).<br \/>\n=> \uc6b0\ub9ac\ub294 \uc77c\ubc18 \ub515\uc154\ub108\ub9ac \uc169\ud0dc\uc640\ub294 \ub2ec\ub9ac \uc815\ub82c\ub41c \ub515\uc154\ub108\ub9ac\ub97c \uc0ac\uc6a9\ud558\ub824\uace0 \ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \ud0a4\uc640 \uac12\uc774 \uac19\uc740 \uac19\uc740 \uc21c\uc11c\ub85c \uc720\uc9c0\ud560 \ud544\uc694\uac00 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\nThe following graphs shows the distributions of the float columns colored by the value of the Target. <br \/>\n=> \ub2e4\uc74c\uc758 \uadf8\ub798\ud504\ub4e4\uc740 \ud0c0\ucf13\uc758 \uac12 \ubcc4\ub85c \uce60\ud574\uc9c4 \uc2e4\uc218\ud615 \uceec\ub7fc\uc758 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc904 \uac83\uc785\ub2c8\ub2e4.<br \/>\nWith these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level.<br \/>\n=> \uc774 \uadf8\ub798\ud504\ub4e4\ub85c \uac01\uac01\uc758 \ube48\uace4 \ub808\ubca8\uc5d0 \ub530\ub978 \uac1c\ubcc4 \uceec\ub7fc\uc758 \ubd84\ud3ec\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. <br \/>","2dcc71f6":"For this problem, the labels are imbalanced, which makes it a little strange to use macro averaging for the evaluation metric, <br \/>\n=> \uc6b0\ub9ac\uac00 \uc608\uce21\ud558\ub824\ub294 \ub77c\ubca8\uc774 \ubd88\uade0\ud615\uc774 \uc788\uc74c (\ud3c9\uac00 \uc9c0\ud45c\ub85c macro averaging\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc870\uae08 \uc774\uc0c1\ud558\uac8c \ub290\uaef4\uc9c8 \uc218 \uc788\uc74c)<br \/>\nbut that's a decision made by the organizers and not something we can change! In your own work, you want to be aware of label imbalances and choose a metric accordingly.<br \/>\n=> \uadf8\ub7ec\ub098 \uadf8\uac83\uc740 \uc774\ubbf8 \uc9c0\uc815\ub41c \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc0c1\uad00\ud560 \ud544\uc694 \uc5c6\uc74c \ub2e8\uc9c0 \ub77c\ubca8\uc758 \ubd84\uade0\ud615\uacfc \uadf8\uac83\uc5d0 \ud574\ub2f9\ud558\ub294 \ud3c9\uac00\uc9c0\ud45c \ubc29\ubc95\uc744 \uace0\ub978 \uac83\uc774\ub77c\uace0 \uc0dd\uac01\ud558\uba74 \ub41c\ub2e4.\n\nRoadmap<br \/>\nThe end objective is a machine learning model that can predict the poverty level of a household. <br \/>\n=> \uc6b0\ub9ac\uc758 \ucd5c\uc885\ubaa9\ud45c\ub294 \ube48\uace4\uac8c\uce35\uc744 \uc608\uce21\ud558\ub294 \uac83\uc774\ub2e4<br \/>\nHowever, before we get carried away with modeling, it's important to understand the problem and data. <br \/>\n=> \uc54c\uc544\ubcf4\uae30\uc804\uc5d0 \uc6b0\ub9ac\uac00 \uc9c1\uba74\ud55c \ubb38\uc81c\uc640 \ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uc790<br \/>\nAlso, we want to evaluate numerous models before choosing one as the \"best\" and after building a model, we want to investigate the predictions. Our roadmap is therefore as follows:<br \/>\n=> \uc6b0\ub9ac\uc758 \ub85c\ub4dc\ub9f5\uc740 \uc544\ub798\uc758 \uc21c\uc11c\uc640 \uac19\ub2e4\n\nUnderstand the problem (we're almost there already)<br \/>\nExploratory Data Analysis<br \/>\nFeature engineering to create a dataset for machine learning<br \/>\nCompare several baseline machine learning models<br \/>\nTry more complex machine learning models<br \/>\nOptimize the selected model<br \/>\nInvestigate model predictions in context of problem<br \/>\nDraw conclusions and lay out next steps\n\nThe steps laid out above are iterative meaning that while we will go through them one at a time, we might go back to an earlier step and revisit some of our decisions. <br \/>\nIn general, data science is a non-linear pracice where we are constantly evaluating our past decisions and making improvements. <br \/>\nIn particular, feature engineering, modeling, and optimization are steps that we often repeat because we never know if we got them right the first time!\n\nGetting Started<br \/>\nWe have a pretty good grasp of the problem, so we'll move into the Exploratory Data Analysis (EDA) and feature engineering. <br \/>\nFor the EDA we'll examine any interesting anomalies, trends, correlations, or patterns that can be used for feature engineering and for modeling. <br \/>\n=> \uc6b0\ub9ac\ub294 \ud2b9\uc131\uacf5\ud559\uacfc \ubaa8\ub378\ub9b4\uc5d0 \uc4f0\uc774\ub294 \ud765\ubbf8\ub85c\uc6b4 \uc798\ubabb\ub41c \uc810, \ucd94\uc138, \uc0c1\uad00\uad00\uacc4 \ub3c4\ub294 \ud328\ud134\uc744 \ud0d0\uad6c\ud560 \uac83\uc785\ub2c8\ub2e4<br \/>\nWe'll make sure to investigate our data both quantitatively (with statistics) and visually (with figures).<br \/>\n=> \uc6b0\ub9ac\uc758 \ub370\uc774\ud130\ub97c \uc591\uc801\uc73c\ub85c \uadf8\ub9ac\uace0 \uc2dc\uac01\uc801\uc73c\ub85c \ud0d0\uc0ac\ud574\ubd05\uc2dc\ub2e4.\n\nOnce we have a good grasp of the data and any potentially useful relationships, <br \/>\n=> \uc704\uc758 \ud0d0\uc0ac\ub97c \uc131\uacf5\uc801\uc73c\ub85c \ub05d\ub0b8 \ud6c4\uc5d0\uc57c<br \/>\nwe can do some feature engineering (the most important part of the machine learning pipeline) and establish a baseline model. <br \/>\n=> \ud2b9\uc131\uacf5\ud559\uc774\ub098 \uae30\ubc18\ubaa8\ub378\uc744 \uc124\uac8c\ud560 \uc218 \uc788\uc744 \uac83\uc785\ub2c8\ub2e4<br \/>\nThis won't get us to the top of the leaderboard, but it will provide a strong foundation to build on!<br \/>\n\nWith all that info in mind (don't worry if you haven't got all the details), let's get started!\n\nImports\nWe'll use a familiar stack of data science libraries: Pandas, numpy, matplotlib, seaborn, and eventually sklearn for modeling.","550e845e":"## Creating Ordinal Variables <br \/>\nFor the walls, roof, and floor of the house, there are three columns each: the first indicating 'bad', the second 'regular', and the third 'good'. <br \/>\n=> \uc9d1\uc758 \ubcbd, \uc9c0\ubd95, \ubc14\ub2e5\uc5d0 \ub300\ud574 \uc138\uac1c\uc758 \uceec\ub7fc\uc774 \uc788\uc2b5\ub2c8\ub2e4. <br \/>\nWe could leave the variables as booleans, but to me it makes more sense to turn them into ordinal variables because there is an inherent order: bad < regular < good. <br \/>\n=> \uc6b0\ub9ac\ub294 \uc774\ub4e4\uc744 \ub17c\ub9ac\ud615\uc73c\ub85c \ub0a8\uae38 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ud544\uc790\ub294 \uc774\ub97c \uc21c\uc11c\ud615\uc73c\ub85c \ubc14\uafb8\uc5c8\uc73c\uba74 \ud569\ub2c8\ub2e4 \ub098\uc068 \ubcf4\ud1b5 \uc88b\uc74c \uc21c\uc73c\ub85c<br \/>\nTo do this, we can simply find whichever column is non-zero for each household using np.argmax.\n\nOnce we have created the ordinal variable, we are able to drop the original variables.","67ab81ab":"The violinplot is not great here because it smooths out the categorical variable with the effect that it looks as if the Target can take on lesser and greater values than in reality. <br \/>\n=> \ubc14\uc774\uc62c\ub9b0 \ud50c\ub86f\uc740 \uc5ec\uae30\uc11c\ub294 \uadf8\ub9ac \uc88b\uc544\ubcf4\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \ub9cc\uc57d \ud0c0\uac9f\uc774 \uc2e4\uc81c\ubcf4\ub2e4 \uc791\uac70\ub098 \ud06c\uba74 \ubcf4\uc774\ub294\ub370 \uce74\ud14c\uace0\ub9ac \ubcc0\uc218\uac00 \uc601\ud5a5\uc744 \ubc1b\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.<br \/>\nNonetheless, we can see a high concentration of households that have no warning signs and have the lowest level of poverty. <br \/>\n=> \uadf8\ub7fc\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \uc6b0\ub9ac\ub294 \ub9ce\uc740 \uac00\uad6c\ub4e4\uc774 \uc704\ud5d8\uc694\uc18c\uac00 \uc801\uc73c\uba70 \ube48\uace4\uc5ec\ubd80\uac00 \uc801\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nIt looks as if this may be a useful feature, but we can't know for sure until we get to modeling!<br \/>\n=> \uc720\uc6a9\ud55c \uac83 \ucc98\ub7fc \ubcf4\uc774\uc9c0\ub9cc, \ubaa8\ub378\ub9c1\ub54c \uc4f0\uc5ec\uc9c8 \uc9c0\ub294 \ubaa8\ub974\uaca0\uc2b5\ub2c8\ub2e4.\n\nThe final household feature we can make for now is a bonus where a family gets a point for having a refrigerator, computer, tablet, or television.","9422cd4d":"Individual Level Variables [\uac1c\ubcc4 \ub2e8\uc704 \ubcc0\uc218]<br \/>\nThere are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering).<br \/>\n=> \uac1c\ubcc4 \ub2e8\uc704 \ubcc0\uc218\uc5d0\ub294 \ub17c\ub9ac\ud615\uacfc \uc21c\uc11c\ud615\uc774 \uc788\uc2b5\ub2c8\ub2e4.","a8839944":"Per Capita Features<br \/>\nAdditional features we can make calculate the number of certain measurements for each person in the household.","2afed0fc":"That performance is very poor. I don't think we need to revisit the Gaussian Naive Bayes method (although there are problems on which it can outperform the Gradient Boosting Machine). <br \/>\n=> \uac00\uc6b0\uc2dc\uc548 \ub098\uc774\ube0c \ubca0\uc774\uc988 \ubaa8\ub378\uc758 \uc131\ub2a5\uc740 \uad49\uc7a5\ud788 \ud615\ud3b8\uc5c6\uc5c8\ub2e4.","c51a6c48":"The gbm seems to think the most important features are those derived from ages. The education variables also show up in the most important features. <br \/>\n=> \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc740 \ub098\uc774\ub97c \uac00\uc7a5 \uc911\uc694\ud55c \ud2b9\uc9d5\u3147\ub77c\uace0 \uc0dd\uac01\ud55c \uac83\uac19\ub2e4. \uad50\uc721 \ubcc0\uc218 \ub610\ud55c \uc911\uc694\ud55c \ubcc0\uc218\ub85c \uc0dd\uac01\ud558\uace0 \uc788\ub2e4.\n\n## Try Selected Features\nThe next step with the LightGBM is to try the features that were selected through recursive feature elimination.<br \/>\n\ub2e4\uc74c\uc73c\ub85c\ub294 \uc6b0\ub9ac\uac00 Recursive Feature Elimination\uc744 \ud1b5\ud574 \uc5bb\uc740 \ud53c\uccd0\ub4e4\ub85c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4 \ubcf4\uc790","7be001e5":"**Model Selection** <br \/>\nNow that we have a good set of features, it's time to get into the modeling. <br \/>\n=> \uc774\uc81c \uc6b0\ub9ac\ub294 \uc88b\uc740 \ubcc0\uc218\ub4e4\uc758 \uc9d1\ud569\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc774\uc81c \ubaa8\ub378\ub9c1\uc744 \ud574\ubcfc \ucc28\ub840\uc785\ub2c8\ub2e4.\nWe already tried one basic model, the Random Forest Classifier which delivered a best macro F1 of 0.35. <br \/>\n=> \uc6b0\ub9ac\ub294 \uc774\ubbf8 RandomForestClassifier\ub85c 0.35\uc810\uc758 \uc131\uacfc\ub97c \ub0c8\uc2b5\ub2c8\ub2e4. <br \/>\nHowever, in machine learning, there is no way to know ahead of time which model will work best for a given dataset. <br \/>\n=> \uadf8\ub7ec\ub098 \uba38\uc2e0\ub7ec\ub2dd\uc5d0\uc11c \uc5b4\ub290 \ubaa8\ub378\uc774 \uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ucd5c\uace0\ub85c \uc791\ub3d9\uc744 \uc790\ub784\uc9c0 \uc544\ub294 \ubc29\ubc95\uc774 \uc5c6\uc2b5\ub2c8\ub2e4. <br \/>\nThe following plot shows that there are some problems where even Gaussian Naive Bayes will outperform a gradient boosting machine. <br \/>\n=> \ub2e4\uc74c\uc758 \uadf8\ub9bc\uc740 \uac00\uc6b0\uc2dc\uc548 \ub098\uc774\ube0c \ubca0\uc774\uc988\uac00 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305\ubcf4\ub2e4 \uc88b\uc740 \uc131\uc801\uc744 \ub0b8\uc801\uc774 \uc788\ub294 \ubb38\uc81c\ub3c4 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.<br \/>\nThis is from an excellent paper by Randal Olson that discusses many points of machine learning","aed47484":"### 4. Results History\nWe'll use two different methods for recording results: <br \/>\n\uacb0\uacfc\ub97c \uae30\ub85d\ud558\ub294\ub370 \uc788\uc5b4\uc11c \ub450\uac00\uc9c0 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\ub2e4.\n\n1. Trials object: stores everything returned from the objective function <br \/>\n\ubaa9\uc801\ud568\uc218\ub85c\ubd80\ud130 \ubc18\ud658\ub418\ub294 \ubaa8\ub4e0 \uac83\uc744 \uc800\uc7a5 <br \/>\n2. Write to a csv file on every iteration <br \/>\n\ubaa8\ub4e0 \ubc18\ubcf5\ub4e4\uc744 csv\ud30c\uc77c\ub85c \uc4f0\uae30 <br \/>\n\nI like using multiple methods for tracking progress because it means redundancy. One way may fail, but hopefully both will not! The csv file can be used to monitor the method while it is running and the Trials object can be saved and then reloaded to resume optimization. <br \/>\n=> \ub098\ub294 \uacb0\uacfc\ub97c \ucd94\uc801\ud558\ub294\ub370 \uc5ec\ub7ec\ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc744 \uc88b\uc544\ud558\ub294\ub370 \uc65c\ub0d0\ud558\uba74 \uacb0\uacfc \ucd94\uc801\uc740 \uc911\ubcf5\uc758 \uc218\ub2e8\uc774 \ub418\uae30 \ub54c\ubb38\uc774\ub2e4. \ud558\ub098\uc758 \ubc29\ubc95\uc774 \uc2e4\ud328\ud558\uba74 \ub2e4\ub978 \uac83\uc740 \uc2e4\ud328\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub2e4. csv\ud30c\uc77c\uc740 \uc2e4\ud589\ub418\ub294 \uac83\uc744 \uad00\ucc30\ud558\ub294 \ubc29\ubc95\uc774\uba70, \uc774\ub807\uac8c \uc218\ud589\ub4e4\uc758 \uac1d\uccb4\ub294 \uc800\uc7a5\ub418\uace0 \ucd5c\uc801\ud654\ub97c \uc7ac\uac1c\ud558\uae30 \uc704\ud574 \ub2e4\uc2dc \ub85c\ub4dc\ub41c\ub2e4.","a595bd0a":"Correlation Heatmap<br \/>\nOne of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 7 variables and show the correlations between themselves and with the target.","abda5c78":"These variables indicate where the electricity in the home is coming from. <br \/>\n=> \uc774 \ubcc0\uc218\ub4e4\uc740 \uc9d1\uc758 \uc804\uae30\uac00 \uc5b4\ub514\uc11c \uc624\ub294\uc9c0\ub97c \uac00\ub974\ud0b5\ub2c8\ub2e4.<br \/>\nThere are four options, and the families that don't have one of these two options either have no electricity (noelec) or get it from a private plant (planpri).<br \/>\n=> \uc5ec\uae30\uc5d0\ub294 \ub124\uac00\uc9c0 \uc635\uc158\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc804\uae30\uac00 \uc5c6\uac70\ub098 \uac1c\uc778 \ubc1c\uc804\uc744 \ud558\ub294 \uc120\ud0dd\uc9c0\ub97c \ub3d9\uc2dc\uc5d0 \uc120\ud0dd\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.","905856d8":"## rez_esc: years behind in school<br \/>\n=> \ud559\uad50\ub97c \ub5a0\ub09c \ud6c4\uc758 \ub144\uc218\n\nThe last column with a high percentage of missing values is rez_esc indicating years behind in school.<br \/>\nFor the families with a null value, is possible that they have no children currently in school.<br \/>\n=> \uc190\uc2e4 \uac12\uc744 \uac00\uc9c0\uace0 \uc788\ub294 \uac00\uc871\ub4e4\uc5d0 \ub300\ud558\uc5ec, \ucd5c\uadfc\uc5d0 \ud559\uad50\uc5d0 \uc9c4\ud559\ud55c \uc544\uc774\ub4e4\uc774 \uc5c6\uc744 \uc218 \uc788\ub2e4<br \/>\nLet's test this out by finding the ages of those who have a missing value in this column and the ages of those who do not have a missing value.<br \/>\n=> \uadf8\ub798\uc11c \ud574\ub2f9 \uceec\ub7fc\uc758 \uc190\uc2e4\uac12\uc744 \uac00\uc9c0\uace0\uc788\ub294 \uc0ac\ub78c\ub4e4\uc758 \uc5f0\ub839\uacfc \uc190\uc2e4\uac12\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\ub294 \uc0ac\ub78c\ub4e4\uc758 \uc5f0\ub839\uc744 \ube44\uad50\ud574\ubcf4\uc790","2956ce5d":"The largest discrepancy in the correlations is dependency. We can make a scatterplot of the Target versus the dependency to visualize the relationship. We'll add a little jitter to the plot because these are both discrete variables.","8a6a6636":"## Investigate Predictions\nAs a first attempt at looking into our model, we can visualize the distribution of predicted labels on the test data. We would expect these to show the same distribution as on the training data. Since we are concerned with household predictions, we'll look at only the predictions for each house and compare with that in the training data. <br \/>\n=>\uc6b0\ub9ac\uc758 \ubaa8\ub378\uc744 \ubcf4\ub294 \uccab\ubc88\uc9f8 \uc2dc\ub3c4\ub85c, \uc6b0\ub9ac\ub294 \ud14c\uc2a4\ud2b8\ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc608\uce21\ub41c \ub77c\ubca8\uac12\ub4e4\uc758 \ubd84\ud3ec\ub97c \uc2dc\uac01\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc774 \ubd84\ud3ec\uac00 \ud6c8\ub828\uc138\ud2b8\uc758 \ub77c\ubca8\ubd84\ud3ec\uc640 \ub3d9\uc77c\ud558\uae30\ub97c \ubc14\ub77c\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uac00\uad6c\uc608\uce21\uc744 \ud574\uc654\ub358\uac83\ucc98\ub7fc, \uc6b0\ub9ac\ub294 \uac01\uac01\uc758 \uac00\uad6c\uc758 \ub370\uc774\ud130\ub97c \ud6c8\ub828\uc138\ud2b8\uc758 \ub77c\ubca8\uacfc \ube44\uad50\ud560 \uac83\uc785\ub2c8\ub2e4.\n\nThe following histrograms are normalize meaning that they show the relative frequency instead of the absolute counts. This is necessary because the raw counts differ in the training and testing data. <br \/>\n=> \ub2e4\uc74c\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc740 \uc808\ub300\uc801\uc778 \uce74\uc6b4\ud2b8\uac00 \uc544\ub2cc \uc0c1\ub300\uc801\uc778 \ube48\ub3c4\ub97c \ub098\ud0c0\ub0b4\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc2e4\uc81c\uc801\uc778 \uce74\uc6b4\ud2b8\ub294 \ud14c\uc2a4\ud2b8\uc14b\uacfc \ud6c8\ub828\uc14b\uc774 \ub2e4\ub974\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.","8cf4b506":"The linear model (with ridge regularization) does surprisingly well. This might indicate that a simple model can go a long way in this problem (although we'll probably end up using a more powerful method). <br \/>\n=> \uc120\ud615\ubaa8\ub378\uc740 \uc0dd\uac01\ubcf4\ub2e4 \uc798 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \n","f3c79f20":"Education reigns supreme! The most important variable is the average amount of education in the household, followed by the maximum education of anyone in the household. I have a suspicion these variables are highly correlated (collinear) which means we may want to remove one of them from the data. The other most important features are a combination of variables we created and variables that were already present in the data. <br \/>\n=>  \uad50\uc721\uc740 \uc815\ub9d0 \uc911\uc694\ud574\ubcf4\uc5ec\uc694! \uac00\uc7a5 \uc911\uc694\ud55c \ubcc0\uc218\ub294 \uac01 \uac00\uc815\uc5d0\uc11c \uad50\uc721\uc758 \ud3c9\uade0\uc591\uc774\uc5c8\uc2a4\ube44\ub2e4. \uadf8\ub9ac\uace0 \ub098\ub294 \uc774\ub7ec\ud55c \ubcc0\uc218\ub4e4\uc774 \uc0c1\ub2f9\ud788 \uc5f0\uad00\uad00\uacc4\uac00 \ud074 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 \uadf8\ub4e4\uc744 \uc9c0\uc6cc\uc8fc\ub824\uace0 \ud569\ub2c8\ub2e4. \ub2e4\ub978 \uc911\uc694\ud55c \ud2b9\uc9d5\ud754 \uc6b0\ub9ac\uac00 \ub9cc\ub4e4\uc5c8\ub10c \ubcc0\uc218\ub4e4\uc758 \uc870\ud569\uc774\uace0 \uadf8\uac83\ub4e4\uc740 \uc774\ubbf8 \ub370\uc774\ud130 \uc548\uc5d0 \uc874\uc7ac\ud569\ub2c8\ub2e4 \n\nIt's interesting that we only need 106 of the ~180 features to account for 90% of the importance. This tells us that we may be able to remove some of the features. However, feature importances don't tell us which direction of the feature is important (for example, we can't use these to tell whether more or less education leads to more severe poverty) they only tell us which features the model considered relevant.<br \/>\n=>106\uc5d0\uc11c 180\uac1c \uc815\ub3c4\uc758 \ud53c\uccd0\ub9cc \uc788\uc5b4\ub3c4 90%\uc758 importance\ub97c \ub2ec\uc131\ud55c \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uc6b0\ub9ac\uac00 \uba87\uba87 feature\ub4e4\uc744 \uc9c0\uc6cc\uc918\uc57c \ud55c\ub2e4\ub294 \uc774\uc57c\uae30 \uc785\ub2c8\ub2e4. \uadf8\ub7ec\ub098 feature_importances\ub294 \ud2b9\uc9d5\uc758 \uc5b4\ub290 \ubc29\ud5a5\uc774 \uc911\uc694\ud55c\uc9c0 \ub9d0\ud574\uc8fc\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uadf8\ub4e4\uc740 \ub2e8\uc9c0 \uc5b4\ub290 \ud2b9\uc9d5\uc774 \ubaa8\ub378\uacfc \uad00\ub828\uc774 \uc788\uc5c8\ub294\uc9c0\ub9cc \uc54c\ub824\uc90d\ub2c8\ub2e4. (\uc5b4\ub290 \uceec\ub7fc\uc774 \ubd80\uc758 \uc218\uc900\uacfc \uad00\ub828\uc774 \uc788\ub2e4\ub294 \uac83\uc740 \uc774\uc57c\uae30 \ud574\uc8fc\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.)","0de75af4":"Because we are going to be comparing different models, we want to scale the features (limit the range of each column to between 0 and 1).<br \/>\n=> \uc6b0\ub9ac\ub294 \ub2e4\ub978 \ubaa8\ub378\uc744 \ube44\uad50\ud560 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc6b0\ub9ac\uc758 \ud2b9\uc9d5\ub4e4\uc744 scaling\ud560 \ud544\uc694\uac00 \uc788\uc5b4\ubcf4\uc785\ub2c8\ub2e4.<br \/>\nFor many ensemble models this is not necessary, but when we use models that depend on a distance metric, such as KNearest Neighbors or the Support Vector Machine, feature scaling is an absolute necessity. <br \/>\n=> \ub300\ubd80\ubd84\uc758 \uc559\uc0c1\ube14 \ubaa8\ub378\uc5d0\ub294 \ubd88\ud544\uc694\ud558\uc9c0\ub9cc, \uac70\ub9ac\uc640 \uad00\ub828\ub41c \ucc99\ub3c4\uc778 K \ucd5c\uadfc\uc811 \uc774\uc6c3\uc774\ub098 \uc11c\ud3ec\ud2b8 \ubca1\ud130\uba38\uc2e0 \uacfc \uac19\uc740 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \uc774\uc6a9\ud55c\ub2e4\uba74 \uc774\ub7f0 \uc2a4\ucf00\uc77c\ub9c1\uc740 \ud544\uc218\uc801\uc785\ub2c8\ub2e4.<br \/>\nWhen comparing different models, it's always safest to scale the features. <br \/>\n=> \ub2e4\ub978 \ubaa8\ub378\uc744 \ube44\uad50\ud560 \ub54c \uc2a4\ucf00\uc77c\ub9c1\ud558\ub294 \uac83\uc740 \uac00\uc7a5 \uc548\uc804\ud55c \ubc29\ubc95\uc911\uc5d0 \ud558\ub098\uc785\ub2c8\ub2e4.<br \/>\nWe also impute the missing values with the median of the feature.<br \/>\n=> \uadf8\ub9ac\uace0 \uc6b0\ub9ac\ub294 \uac01 \ud2b9\uc9d5\uc758 \uc911\uc559\uac12\uc744 \uc190\uc2e4\uce58\ub4e4\uc5d0 \ub123\uc5b4\uc904 \uac83\uc785\ub2c8\ub2e4.\n\n**For imputing missing values and scaling the features in one step, we can make a pipeline.** <br \/>\n**=> \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc774\uc6a9\ud558\uba74 \uc190\uc2e4\uce58\ub97c \ub123\uac70\ub098 \uc2a4\ucf00\uc77c\ub9c1\uc744 \ud558\ub294\ub370 \uac04\ub2e8\ud558\uac8c \ucc98\ub9ac\ud560 \uc218\uc788\uc2b5\ub2c8\ub2e4.**<br \/>\nThis will be fit on the training data and used to transform the training and testing data.","d0850efb":"## Comparing Model Performance\nWith the modeling results in a dataframe, we can plot them to see which model does the best.","75fdd73a":"We'll leave the feature engineering of the household variables for now. Later, we can come back to this step if we are not pleased with the model performance.","10a5fef6":"Well that's a relief! <br \/>\n=> \ub2e4\ud589\uc785\ub2c8\ub2e4<br \/>\nThis means that we don't have to worry about a household both where there is no head AND the members have different values of the label! <br \/>\n=> \uc704\ub97c \ud1b5\ud574 \uc6b0\ub9ac\uac00 \uc6b0\ub824\ud560 \ubed4 \ud588\ub358 \ub450 \uac00\uc9c0 \uc0c1\ud669\uc774 \ub3d9\uc2dc\uc5d0 \uc77c\uc5b4\ub098\ub294 \uacbd\uc6b0\ub294 \uc5c6\ub2e4\ub294 \uac83\uc744 \ud30c\uc545\ud588\uc2b5\ub2c8\ub2e4.<br \/>\nFor this problem, according to the organizers, if a household does not have a head, then there is no true label. <br \/>\n=> \uc774 \ubb38\uc81c \ub54c\ubb38\uc5d0, \ucef4\ud53c\ud2f0\uc158\uc758 \uacc4\ud68d\uc790\ub294 \uac00\uc7a5\uc774 \uc5c6\ub294 \uac00\uad6c\ub294 \uc81c\ub300\ub85c\ub41c \ub77c\ubca8\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\ub2e4\uace0 \uc5b8\uae09\ud588\uc2b5\ub2c8\ub2e4.<br \/>\nTherefore, we actually won't use any of the households without a head for training Nonetheless, it's still a good exercise to go through this process of investigating the data!<br \/>\n=> \uadf8\ub7ec\ubbc0\ub85c \uc6b0\ub9ac\ub294 \uac00\uc7a5\uc5c6\ub294 \uac00\uad6c\uc758 \uad6c\uc131\uc6d0\ub4e4\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc744 \uac83\uc785\ub2c8\ub2e4. \n\n## Correct Errors [\uc624\ub958\uc218\uc815]<br \/>\nNow we can correct labels for the households that do have a head AND the members have different poverty levels.<br \/>\n=> \uc774\uc81c \uc6b0\ub9ac\ub294 \uac01\uac01\uc758 \uac00\uad6c\uc758 \ud0c0\uac9f \ub77c\ubca8\uc744 \uac00\uc7a5\uc758 \ud0c0\uac9f\ub77c\ubca8\ub85c \ubc14\uafd4\uc8fc\ub3c4\ub85d \ud574\ubd05\uc2dc\ub2e4.","aebdddbf":"For most of the household level variables, we can simply keep them as is: since we want to make predictions for each household, we use these variables as features. <br \/>\n=> \ub300\ubd80\ubd84\uc758 \uac00\uad6c\ubcc4 \uc218\uc900\uc758 \ubcc0\uc218\ub4e4\uc5d0 \ub300\ud574, \uc6b0\ub9ac\uac00 \uac00\uad6c\ubcc4\ub85c \uc608\uce21\uc744 \ud558\ub824\uace0 \ud560 \ub54c\uc5d0\ub294 \uc704\uc758 \ubcc0\uc218\ub4e4\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4<br \/>\nHowever, we can also remove some redundant variables and also add in some more features derived from existing data.<br \/>\n=> \uadf8\ub7ec\ub098 \uc774 \ub370\uc774\ud130\ub4e4\uc5d0\uc11c\ub3c4 \uc911\ubcf5\ub418\ub294 \uac12\ub4e4\uc744 \uc81c\uac70\ud574\uc57c \ud558\uba70 \uba70\ucce7\uc758 \ud2b9\uc9d5\ub4e4\uc740 \uc874\uc7ac\ud558\ub294 \ub370\uc774\ud130\ub85c\ubd80\ud130 \ud30c\uc0dd\ub41c \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\n\nRedundant Household Variables [\uc911\ubcf5\ub41c \uac00\uad6c\ub2e8\uc704\uc758 \ubcc0\uc218]\n\nLet's take a look at the correlations between all of the household variables. <br \/>\n=> \uac00\uad6c\ub2e8\uc704\uc758 \ubcc0\uc218\ub4e4\uc758 \uc0c1\uad00\uad00\uac8c\ub97c \uc54c\uc544\ubcf4\ub3c4\ub85d \ud569\uc2dc\ub2e4<br \/>\nIf there are any that are too highly correlated, then we might want to remove one of the pair of highly correlated variables.<br \/>\n=>\ub9cc\uc57d \ub108\ubb34 \ub9ce\uc774 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\ub294 \uac83\uc774 \uc788\ub2e4\uba74, \uc6b0\ub9ac\ub294 \uadf8 \uc30d\uc911 \ud558\ub098\ub97c \uc0ad\uc81c\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\n\nThe following code identifies any variables with a greater than 0.95 absolute magnitude correlation.<br \/>\n=> \ub2e4\uc74c\uc758 \ucf54\ub4dc\ub294 \uc808\ub300\uac12 0.95\uc774\uc0c1\uc758 \uc0c1\uad00\uad00\uac8c\ub97c \uac00\uc9c0\ub294 \ubcc0\uc218\ub4e4\uc744 \uc54c\uc544\ub0b4\ub294 \ucf54\ub4dc\uc785\ub2c8\ub2e4.","45009e7b":"This new feature may be useful because it seems like a Target of 4 (the lowest poverty level) tends to have higher values of the 'house quality' variable. <br \/>\n=> \uc774\ub294 \uc0dd\uac01\ubcf4\ub2e4 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 4 \ud0c0\uac9f \ub77c\ubca8\uc744 \uac00\uc9c4 \uac12\ub4e4\uc774 \ub300\uccb4\ub85c \uc9d1\uc758 \ud488\uc9c8\uc774 \uc88b\ub2e4\uace0 \ud310\ub2e8\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.<br \/>\nWe can also look at this in a table to get the fine-grained details.","d96c8a81":"We see for a number of cases, there are more people living in the household than there are in the family. <br \/>\n=> \uc6b0\ub9ac\ub294 \ub2e4\uc591\ud55c \ucf00\uc774\uc2a4\ub4e4\uc744 \ubd24\uace0, \uac00\uc871\uc758 \uc218\ubcf4\ub2e4 \uac00\uad6c\uc5d0 \uc0b4\uace0 \uc788\ub294 \uc0ac\ub78c\ub4e4\uc774 \ub9ce\uc2b5\ub2c8\ub2e4<br \/>\nThis gives us a good idea for a new feature: the difference between these two measurements!<br \/>\n=> \uc774\ub294 \uc6b0\ub9ac\uac00 \ub450 \uc778\uc6d0\uc758 \ucc28\uc774\ub97c \uc0c8\ub85c\uc6b4 feature\ub85c \uc4f0\ub294 \uc778\uc0ac\uc774\ud2b8\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\n\nLet's make this new feature.<br \/>\n=> \uc0c8\ub85c\uc6b4 feature\ub97c \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4.","63652e96":"## Feature Engineering\n\nThere is plenty more exploratory data analysis we can do, but first we should work on consolidating our data at a household level. <br \/>\n=> \uc6b0\ub9ac\uac00 \ud560 \uc218 \uc788\ub294 EDA\ub294 \uad49\uc7a5\ud788 \ub9ce\uc9c0\ub9cc, \uc6b0\ub9ac\uc758 \ub370\uc774\ud130\ub4e4\uc744 \uac00\uad6c\ub2f9 \ub2e8\uc704\ub85c \ud569\ubcd1\ud558\ub294 \uc791\uc5c5\uc744 \uc9c4\ud589\ud574\uc57c\ud569\ub2c8\ub2e4.<br \/>\nWe already have some of the information for each household, but for training, we will need all of the information summarized for each household. <br \/>\n=> \uc6b0\ub9ac\ub294 \uac01\uac01\uc758 \uac00\uad6c\ubcc4 \uc815\ubcf4\ub97c \uc870\uae08 \uac00\uc9c0\uace0 \uc788\uc9c0\ub9cc, \ud6c8\ub828\uc744 \uc704\ud574\uc11c\ub294 \uac01\uac01\uc758 \uac00\uad6c\ub97c \uc704\ud574 \uc815\ubcf4\ub97c \uac04\ub7b5\ud654\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nThis means grouping the individuals in a house (groupby) and performing an aggregation (agg) of the individual variables.<br \/>\n=> \uc774\ub294 \uac1c\uac1c\uc778\uc744 \uac00\uad6c\ubcc4\ub85c \uadf8\ub8f9\uc744 \uc9d3\uace0 \uac1c\ubcc4 \uac12\uc744 \ubcd1\ud569\ud574\uc57c\ud569\ub2c8\ub2e4.\n\nIn another notebook, I show how we can use automated feature engineering to do this, and automated feature engineering should be a standard part of the machine learning workflow. <br \/>\n=> \ud544\uc790\ub294 feature engineering\uc5d0 \uc788\uc5b4\uc11c \uc790\ub3d9\ud654\ub41c \ubc29\ubc95\uc744\ud558\ub824\uace0 \ud558\uba70, \uc774\ub7ec\ud55c \ubc29\ubc95\uc740 \uba38\uc2e0\ub7ec\ub2dd \uc791\uc5c5 \ud750\ub984\uc5d0\uc11c \uad49\uc7a5\ud788 \uae30\ubcf8\uc801\uc778 \ubd80\ubd84\uc785\ub2c8\ub2e4<br \/>\nRight now, we'll stick to doing this by hand, but definitely take a look at automated feature engineering in Featuretools.<br \/>\n=> \uc6b0\ub9ac\ub294 \uc218\uc791\uc5c5\uc73c\ub85c \ud560 \uac83\uc774\uc9c0\ub9cc, \uc790\ub3d9\uc801\uc73c\ub85c \ud558\ub294 \ubc29\ubc95\uc744 \uaf2d \ucc38\uace0\ud574\ubcf4\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4\n\n## Column Definitions [\uceec\ub7fc\uc758 \uc758\ubbf8]<br \/>\nSometimes in data science we have to get our hands dirty digging through the data or do tedious tasks that take a lot of time. <br \/>\n=> \ub370\uc774\ud130 \uc0ac\uc774\uc5b8\uc2a4\uc5d0\uc11c \uc6b0\ub9ac\ub294 \uc9c1\uc811 \ub370\uc774\ud130\uc5d0\uc11c \ubb34\uc5b8\uac00\ub97c \ucc3e\uc544\ub0b4\uc57c\ud558\uace0 \uc2dc\uac04\uc774 \ub9ce\uc774\uac78\ub9ac\ub294 \uc9c0\ub8e8\ud55c \uc791\uc5c5\ub4e4\uc744 \ud574\uc57c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nThis is that part of the analysis: we have to define the columns that are at an individual level and at a household level using the data decsriptions. <br \/>\n=> \uc774\uac83\uc740 \ubd84\uc11d\uc758 \ub2e8\uacc4\uc785\ub2c8\ub2e4: \uac01\uac01\uc758 \uceec\ub7fc\uc744 \uc815\uc758\ud558\uace0 \uc5b4\ub290 \uac83\uc774 \uac1c\uac1c\uc778\uc758 \uc218\uc900\uc778\uac00 \ub610\ub294 \uac00\uad6c\ubcc4 \uc218\uc900\uc778\uc9c0 \uc124\uba85\uc744 \ud1b5\ud574 \uc54c\uc544\ub0b4\uc57c\ud569\ub2c8\ub2e4<br \/>\nThere is simply no other way to identify which variables at are the household level than to go through the variables themselves in the data description. <br \/>\n=> \ub370\uc774\ud130 \uc124\uba85\uc744 \ubcf4\uc9c0 \uc54a\uace0\uc11c\ub294 \uc774\uac83\uc744 \ubc1d\ud600\ub0bc\ub9cc\ud55c \ub2e4\ub978 \ubc29\ubc95\uc740 \uc5c6\uc744 \uac83\uc785\ub2c8\ub2e4.<br \/>\nExcept, I've already done this for you, so all you have to do is copy and paste!<br \/>\n=> \ud544\uc790\ub294 \uc774\ubbf8 \uc791\uc5c5\uc744 \ud574\ub450\uc5c8\uc73c\ub2c8, \ud574\uc57c\ud560 \uc77c\uc740 \uac00\uc838\ub2e4 \uc4f0\ub294 \uac83\uc785\ub2c8\ub2e4.\n\nWe'll define different variables because we need to treat some of them in a different manner. <br \/>\n=> \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c \uba87\uba87\uc774 \uc0ac\uc6a9\ub418\uae30 \ub54c\ubb38\uc5d0 \uc774\ub4e4\uc744 \uc704\ud574 \uba87\uba87\uc758 \ub2e4\ub978 \ubcc0\uc218\ub97c \uc120\uc5b8\ud560 \uac83\uc785\ub2c8\ub2e4<br \/>\nOnce we have the variables defined on each level, we can work to start aggregating them as needed.<br \/>\n=> \uac01\uac01\uc758 \ub808\ubca8\uc5d0\uc11c \uc4f0\uc5ec\uc9c8 \ubcc0\uc218\ub4e4\uc774 \uc815\uc758\ub418\uace0 \ub09c \ub4a4\uc5d0, \uc774\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud574\uc11c \ubcd1\ud569\uc744 \uc2dc\uc791\ud560 \uac83\uc785\ub2c8\ub2e4.\n\nThe process is as follows<br \/>\n=> \ud574\ub2f9 \uacfc\uc815\uc740 \uc544\ub798\uc758 \uacfc\uc815\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n\n1. Break variables into household level and invididual level<br \/>\n=> \ubcc0\uc218\ub97c \uac00\uad6c\ub2e8\uc704 \uc640 \uac1c\ubcc4\ub2e8\uc704\ub85c \uad6c\ubd84<br \/>\n2. Find suitable aggregations for the individual level data<br \/>\n=> \uac1c\uac1c\uc778 \uc218\uc900\uc758 \ub370\uc774\ud130\ub97c \uc704\ud574\uc11c \uc801\uc808\ud55c \ubcd1\ud569\ubc29\ubc95\uc744 \ucc3e\uc2b5\ub2c8\ub2e4<br \/>\n    Ordinal variables can use statistical aggregations<br \/>\n    => Ordinal \uac12\ub4e4\uc740 \ud1b5\uacc4\uc801 \ubcd1\ud569\uc5d0 \uc0ac\uc6a9\ub420 \uac83\uc774\uba70<br \/>\n    Boolean variables can also be aggregated but with fewer stats<br \/>\n    => \ub17c\ub9ac\ud615 \uc790\ub8cc\ub4e4\uc740 \uc801\uc740 \ud1b5\uacc4\uce58\ub4e4\uc5d0 \ubcd1\ud569\ub420 \uac83\uc774\uba70<br \/>\n3. Join the individual aggregations to the household level data<br \/>\n=> \uac1c\uc778 \uc218\uc900\uc758 \ub370\uc774\ud130\ub97c \uac00\uad6c\ubcc4 \uc218\uc900\uc758 \ub370\uc774\ud130\uc5d0 \ud569\uce58\uace0\n\n\n## Define Variable Categories[\ubcc0\uc218\uc758 \uce74\ud14c\uace0\ub9ac\ub4e4\uc744 \uc815\uc758]\n\nThere are several different categories of variables:\n\n1.Individual Variables: these are characteristics of each individual rather than the household<br \/>\n    Boolean: Yes or No (0 or 1)<br \/>\n    Ordered Discrete: Integers with an ordering<br \/>\n2. Household variables<br \/>\n    Boolean: Yes or No<br \/>\n    Ordered Discrete: Integers with an ordering<br \/>\n    Continuous numeric<br \/>\n3. Squared Variables: derived from squaring variables in the data<br \/>\n4. Id variables: identifies the data and should not be used as features\n\nBelow we manually define the variables in each category. This is a little tedious, but also necessary.<br \/>\n=> \uc544\ub798\uc5d0 \uc6b0\ub9ac\ub294 \uac01\uac01\uc758 \uce74\ud14c\uace0\ub9ac\uc758 \ubcc0\uc218\ub4e4\uc744 \uc815\uc758\ud574\ub450\uc5c8\uc2b5\ub2c8\ub2e4.","25c7c14c":"## Cross Validation with Early Stopping Notes\n\n> \ucc38\uace0. Variance\uc640 Bias https:\/\/www.slideshare.net\/freepsw\/boosting-bagging-vs-boosting [boosting \uae30\ubc95\uc758 \uc774\ud574]\n\nCross validation with early stopping is one of the most effective methods for preventing overfitting on the training set because it prevents us from continuing to add model complexity once it is clear that validation scores are not improving. Repeating this process across multiple folds helps to reduce the bias that comes from using a single fold. Early stopping also lets us train the model much quicker. Overall, early stopping with cross validation is the best method to select the number of estimators in the Gradient Boosting Machine and should be our default technique when we desig an implementation. <br \/>\n=> early stopping\uc744 Cross validation \ud558\ub294 \uac83\uc740 \ud6c8\ub828\uc138\ud2b8\uc5d0\uc11c \uacfc\uc801\ud569\uc744 \ub9c9\ub294 \uac00\uc7a5 \ud6a8\uacfc\uc801\uc778 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc774\ub294 \uc6b0\ub9ac\uac00 \ubaa8\ub378 \ubcf5\uc7a1\ub3c4\ub97c \uc9c0\uc18d\uc801\uc73c\ub85c \ucd94\uac00\ud558\ub294 \uac83\uc744 \ub9c9\uae30 \ub54c\ubb38\uc774\uba70, validation \uc810\uc218\uac00 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc744\ub54c \uc774\ub294 \uba85\ud655\ud574\uc9d1\ub2c8\ub2e4. \uc5ec\ub7ec\uacb9\uc744 \uac70\uccd0\uc11c \uc774 \uacfc\uc815\uc744 \ubc18\ubcf5\ud558\ub294 \uac83\uc740 \ud558\ub098\uc758 \ub2e8\uc77c \uacc4\uce35\uc73c\ub85c \ud588\uc744 \ub54c \ubc1c\uc0dd\ud560 \uc218 \uc788\ub294 bias\ub97c \uc904\uc5ec\uc900\ub2e4(bias\uac00 \ub0ae\uc73c\uba74 \uc62c\ubc14\ub978 \uc608\uce21\uce58\ub97c \ucc3e\uc544\uac04\ub2e4\ub294 \ub73b \uc0c1\ucda9\ub418\ub294 \uac1c\ub150\uc73c\ub85c variance\uac00 \uc788\uc74c \uc704\uc758 \ucc38\uace0 url\uc5d0 \ub450 \uc6a9\uc5b4\uc5d0 \ub300\ud55c \uac1c\ub150\uc774 \uc788\uc74c). early stopping\uc740 \ub610\ud55c \uc6b0\ub9ac\uac00 \ud6c8\ub828 \ubaa8\ub378 \ud559\uc2b5\uc744 \ub354 \ube60\ub974\uac8c \ud574\uc900\ub2e4. \uc804\ubc18\uc801\uc73c\ub85c \uad50\ucc28\uac80\uc99d\uc744 \uc774\uc6a9\ud55c early stopping\uc740 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc5d0\uc11c \ubc18\ubcf5\uc2dc\ud589\uc758 \uc218\ub97c \uc120\ud0dd\ud558\ub294 \ucd5c\uace0\uc758 \ubc29\ubc95\uc774\uba70 \uc2e4\ud589\uc744 \ud558\ub294\ub370 \uc788\uc5b4\uc11c \uac00\uc7a5 \uae30\ucd08\uc801\uc778 \uc9c0\uc2dd\uc774\uba70 \uae30\uc220\uc774\ub2e4.  <br \/>","49939775":"Before going into the Kernel,\n\nI REFERED ON THIS KERNEL https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\nI translated into Korean for my studying\n\nI'D LIKE TO GIVE HUGE THANKS TO HIM","3ce5db36":"Let's look at one example.\n=> \uadf8\ub807\ub2e4\uba74 \uc5b4\ub5a4 \ud55c \uac00\uc871\uc758 \uc608\uc2dc\ub97c \ubd05\uc2dc\ub2e4","5d2b3bf4":"This tells us there are 130 integer columns, 8 float (numeric) columns, and 5 object columns. <br \/>\n=> 130\uac1c\uc758 \uc815\uc218, 8\uac1c\uc758 \uc2e4\uc218 \uadf8\ub9ac\uace0 5\uac1c\uc758 \uac1d\uccb4 \uceec\ub7fc\uc774 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nThe integer columns probably represent Boolean variables (that take on either 0 or 1) or ordinal variables with discrete ordered values. <br \/>\n=> \uc815\uc218\uc778 \uceec\ub7fc\uc740 \ubd88\ub9ac\uc5b8 \uac12\uc77c \uc218\ub3c4 \uc788\uc9c0\ub9cc \uc21c\uc11c\ub97c \uac00\uc9c4 ordinal\ud55c \uac12\uc77c \uac00\ub2a5\uc131\uc774 \uc788\uc2b5\ub2c8\ub2e4<br \/>\nThe object columns might pose an issue because they cannot be fed directly into a machine learning model.<br \/>\n=> \uac1d\uccb4\uceec\ub7fc\uc740 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc5d0\uc11c \ud559\uc2b5\ub420 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc5b4\ub5a0\ud55c \uc870\uce58\ub97c \ucde8\ud574\uc8fc\uc5b4\uc57c \ud569\ub2c8\ub2e4\n\nLet's glance at the test data which has many more rows (individuals) than the train. It does have one fewer column because there's no Target!<br \/>\n=> \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc744 \ubcf4\uac8c \ub41c\ub2e4\uba74 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b\uc5d0\ub294 \ud0c0\uac9f \uac12\uc774 \uc5c6\uae30 \ub300\ubb38\uc5d0 \uc815\uc218\uceec\ub7fc\uc774 \ud558\ub098 \uc5c6\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","95f4b11b":"Even though most households do not have a difference, there are a few that have more people living in the household than are members of the household.<br \/>\n=> \ube44\ub85d \ub9ce\uc740 \uac00\uad6c\ub4e4\uc774 \ucc28\uc774\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\ub354\ub77c\ub3c4, \uba87\uba87\uc758 \ucc28\uc774\uac00 \ub098\ub294 \uac00\uad6c\ub4e4\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4.\n\nLet's move on to the other redundant variables. First we can look at coopele<br \/>\n=> \uc774\uc81c\ub294 \ub2e4\ub978 \ubcc0\uc218\ub85c \ub118\uc5b4\uac00 \ubd05\uc2dc\ub2e4","459e9579":"\"tamhog\ub294 \uac00\uad6c\uc758 \uc0ac\uc774\uc988\ub97c \ub73b\ud568\"<br \/>\n\nThere are several variables here having to do with the size of the house:<br \/>\n=> \uc9d1\uc758 \uc0ac\uc774\uc988\uc640 \uad00\ub828 \uc788\ub294 \ubcc0\uc218\ub4e4\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\nr4t3, Total persons in the household [\uc9d1\uc5d0 \uc0b4\uace0 \uc788\ub294 \ucd1d\uc778\uc6d0 \uc218]<br \/>\ntamhog, size of the household [\uac00\uad6c\uc758 \ud06c\uae30]<br \/>\ntamviv, number of persons living in the household [\uac00\uad6c\uc5d0 \uc0b4\uace0 \uc788\ub294 \uc0ac\ub78c\uc758 \uc218]<br \/>\nhhsize, household size [\uac00\uad6c\uc758 \ud06c\uae30]<br \/>\nhogar_total, # of total individuals in the household [\uac00\uad6c\uc5d0 \uc788\ub294 \uac1c\ubcc4 \uc778\uc6d0\uc758 \ucd1d\ud569]<br \/>\n\nThese variables are all highly correlated with one another. <br \/>\n=> \uc774\ub7ec\ud55c \uac12\ub4e4\uc740 \uc11c\ub85c \ud070 \uc5f0\uad00\uc131\uc744 \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nIn fact, hhsize has a perfect correlation with tamhog and hogar_total. <br \/>\n=> \uc2e4\uc81c\ub85c hhsize\ub294 tamhog\ub098 hogar_total\uacfc \uc644\ubcbd\ud55c \uc120\ud615\uc131\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4<br \/>\nWe will remove these two variables because the information is redundant. We can also remove r4t3 because it has a near perfect correlation with hhsize.<br \/>\n=> \uc774 \uc815\ubcf4\ub294 \uc911\ubcf5\uc774\uae30 \ub54c\ubb38\uc5d0 \ub450 \ubcc0\uc218\ub97c \uc81c\uac70\ud574 \uc904 \uac83\uc785\ub2c8\ub2e4. r4t3\ub610\ud55c \uac70\uc758 \uc644\ubcbd\ud55c \uc911\ubcf5\uc5d0 \uac00\uae5d\uae30 \ub54c\ubb38\uc5d0 \uc9c0\uc6cc\uc904 \uac83\uc785\ub2c8\ub2e4.\n\ntamviv is not necessarily the same as hhsize because there might be family members that are not living in the household. Let's visualize this difference in a scatterplot.<br \/>\n=>tamviv\ub294 hhsize\uc640 \ud544\uc218\uc801\uc73c\ub85c \uac19\uc744 \ud544\uc694\ub294 \uc5c6\uc2b5\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uac00\uad6c\uc5d0 \uc0b4\uace0 \uc788\uc9c0 \uc54a\ub294 \uac00\uc871\ub4e4\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\uc8e0","4d0dc356":"Since we are only going to use the heads of household for the labels, this step is not completely necessary but it shows a workflow for correcting data errors like you may encounter in real life. Don't consider it extra work, just practice for your career!<br \/>\n=> \uc6b0\ub9ac\uac00 \uac00\uc7a5\uc758 \ub77c\ubca8\ub9cc \uc0ac\uc6a9\ud558\uaca0\ub2e4\uace0 \ud588\uae30 \ub54c\ubb38\uc5d0, \uc774 \uacfc\uc815\uc740 \ubcc4\ub85c \ubd88\ud544\uc694\ud574\ubcf4\uc774\uc9c0\ub9cc \uae30 \uacfc\uc815\uc740 \uc2e4\uc81c \ub370\uc774\ud130\uc14b\uc5d0\uc11c \ub9cc\ub0a0\uc218 \uc788\ub294 \uc5d0\ub7ec\ub97c \uace0\uce58\ub294 \ubc29\uc548\uc911 \ud558\ub098\uc785\ub2c8\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \uadc0\ucc2e\uc740 \uc77c\uc744 \ud588\ub2e4\uace0 \uc0dd\uac01\ud558\uc9c0 \ub9d0\uc544\uc92c\uc73c\uba74 \ud569\ub2c8\ub2e4.\n\n## Missing Variables [\uc190\uc2e4 \uac12]<br \/>\nOne of the most important steps of exploratory data analysis is finding missing values in the data and determining how to handle them. <br \/>\n=> EDA\uc5d0\uc11c \uac00\uc7a5 \uc911\uc694\ud55c \uacfc\uc815\uc740 \uc190\uc2e4 \uac12\ub4e4\uc744 \ucc3e\uc544\ub0b4\uace0 \uc774\ub97c \uc5b4\ub5bb\uac8c \ub2e4\ub8f0\uc9c0 \uacb0\uc815\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n\nMissing values have to be filled in before we use a machine learning model and we need to think of the best strategy for filling them in based on the feature: this is where we'll have to start digging into the data definitions.<br \/>\n=> \uc190\uc2e4 \uac12\uc740 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\uae30 \uc804\uc5d0 \ucc44\uc6cc\uc838\uc57c \ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc774\ub97c \ucc44\uc6b0\ub294\ub370 \uc801\uc808\ud55c \uc804\ub7b5\uc744 \uc0dd\uac01\ud574\ubd10\uc57c\ud569\ub2c8\ub2e4. \uc774\ub294 \uc6b0\ub9ac\uac00 \ub370\uc774\ud130\uc758 \uc815\uc758\ub97c \uc0b4\ud3b4\ubd10\uc57c\ud558\ub294 \uacf3\uc785\ub2c8\ub2e4.<br \/>\n\nFirst we can look at the percentage of missing values in each column.<br \/>\n=> \uc77c\ub2e8\uc740 \uac01\uac01\uc758 \uceec\ub7fc\uc758 \uc190\uc2e4 \uac12\ub4e4\uc758 \ube44\uc728\uc744 \uacc4\uc0b0\ud574 \ubd05\uc2dc\ub2e4.","8a356e1f":"Feature Selection\nAs a first round of feature selection, we can remove one out of every pair of variables with a correlation greater than 0.95.","68a44c0f":"We don't have to worry about the Target becuase we made that NaN for the test data. However, we do need to address the other 3 columns with a high percentage of missing values.<br \/>\n=> \ud0c0\uac9f\uac12\uc5d0 \uc788\uc5b4\uc11c \uc190\uc2e4 \uac12\uc740 \uace0\ub824\ud558\uc9c0 \uc54a\uc544\ub3c4 \ub429\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc6b0\ub9ac\ub294 \ud14c\uc2a4\ud2b8\uc14b\uc5d0 \ub300\ud574\uc11c\ub294 \uc784\uc758\ub85c \uc190\uc2e4 \uac12\uc744 \ub9cc\ub4dc\uc5b4 \uc8fc\uc5c8\uae30 \ub54c\ubb38\uc774\uc8e0. \uadf8\ub7ec\ub098 3\uac1c\uc758 \ub192\uc740 \ube44\uc728\uc758 \uc190\uc2e4 \uac12\uc744 \uac00\uc9c4 \uceec\ub7fc\uc744 \ucc3e\uc544\ub0c8\uc2b5\ub2c8\ub2e4.\n\nv18q1: Number of tablets\n\nLet's start with v18q1 which indicates the number of tablets owned by a family. <br \/>\n=> \uac00\uc871\uc774 \uc18c\uc720\ud55c tablet\uc758 \uc218\ub97c \ub098\ud0c0\ub0b4\ub294 \uceec\ub7fc\uc5d0 \ub300\ud574\uc11c \uc54c\uc544 \ubd05\uc2dc\ub2e4<br \/>\nWe can look at the value counts of this variable. \n\nSince this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household.<br \/>\n=> \uc774\uac83\uc740 \uac00\uad6c\ubcc4 \ubcc0\uc218\uc774\uae30 \ub300\ubb38\uc5d0, \uac00\uad6c\ub2f9 \ub808\ubca8\uc5d0\uc11c \uc774\ud574\ub97c \ud558\ub294 \uac83\uc774 \uc633\uc544\ubcf4\uc785\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 \uac00\uc7a5\uc5d0 \ub300\ud574\uc11c\ub9cc \uc54c\uc544\ubcf4\ub3c4\ub85d \ud569\uc2dc\ub2e4\n\n## Function to Plot Value Counts [\ud574\ub2f9 \uceec\ub7fc\uc758 \uac12\uc758 \uc218\ub97c \uc2dc\uac01\ud654]<br \/>\nSince we might want to plot value counts for different columns, we can write a simple function that will do it for us!","e80a5dd1":"## Light Gradient Boosting Machine Implementation\nThe function below implements training the gradient boosting machine with Stratified Kfold cross validation and early stopping to prevent overfitting to the training data (although this can still occur). The function performs training with cross validation and records the predictions in probability for each fold. To see how this works, we can return the predictions from each fold and then we'll return a submission to upload to the competition. <br \/>\n=> \uc544\ub798\uc758 \ud568\uc218\ub294 K\uacb9 \uad50\ucc28\uac80\uc99d\uc744 \uc774\uc6a9\ud55c \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0a4\ub294 \ud568\uc218\uc774\uba70, early stopping\uc740 \ud6c8\ub828\ub370\uc774\ud130\uac00 \uacfc\uc801\ud569\ub418\ub294 \uac83\uc744 \ub9c9\ub294 \ud30c\ub77c\ubbf8\ud130\uc785\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \uad50\ucc28\uac80\uc99d\uc744 \ud1b5\ud574 \ud6c8\ub828\uc2dc\ud0a4\uace0 \uac01 \uacc4\uce35\ub9c8\ub2e4\uc758 \uc608\uce21\uc5d0 \ub300\ud55c \ud655\ub960\uc744 \uae30\ub85d\ud569\ub2c8\ub2e4. \uc774\uac83\uc774 \uc5b4\ub5bb\uac8c \uc791\ub3d9\ud558\ub294\uc9c0\ub97c \uc54c\uae30 \uc704\ud574\uc11c \uac01 \ud3f4\ub4dc\ubcc4 \uc608\uce21\uce58\ub97c \ubc18\ud658\ud558\ub294 prediction\uc744 \ubc18\ud658\ud558\uace0 \ucef4\ud53c\ud2f0\uc158\uc758 \uc5c5\ub85c\ub4dc\ub97c \uc704\ud574\uc11c submission \ub610\ud55c \uc5c5\ub85c\ub4dc \ud560 \uac83 \uc785\ub2c8\ub2e4.<br \/>\n\nChoosing hyperparameters for the Gradient Boosting Machine can be tough and generally is done through model optimization. In this notebook, we'll use a set of hyperparameters that I've found work well on previous problems (although they will not necessarily translate to this competition). <br \/>\n=> \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305\uc5d0\uc11c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uace0\ub974\ub294 \uac83\uc740 \uc5b4\ub824\uc6b4\uc77c\uc785\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc774\ub294 \ubaa8\ub378 \ucd5c\uc801\ud654\ub97c \ud1b5\ud574 \uc774\ub8e8\uc5b4\uc9c8 \uac83\uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \uc77c\ub828\uc758 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \uc138\ud305\ud560 \uac83\uc785\ub2c8\ub2e4.<br \/>\n\nWe set the n_estimators to 10000 but we won't actually reach this number because we are using early stopping which will quit training estimators when the cross validation metric does not improve for early_stopping_rounds. There's a lot going on in this function, and read through it carefully to make sure you have it all! I've tried to make the comments and code straightforward. (The display is used to show custom information during training in combination with %%capture so we don't have to see all the LightGBM information during training). <br \/>\n=> \uc6b0\ub9ac\ub294 n_estimators\ub97c 10000\uc73c\ub85c \uc138\ud305\ud558\uc600\uc9c0\ub9cc, \uc6b0\ub9ac\ub294 \uad50\ucc28\uac80\uc99d \uc810\uc218\uac00 \ub354 \uc774\uc0c1 \uc624\ub974\uc9c0 \uc54a\ub294 \uc21c\uac04\uc744 \uce90\uce58\ud558\uace0 \uc774\ub97c \uc911\ub2e8 \uc2dc\ud0a4\ub294 early_stopping\uc744 \uc124\uc815\ud558\uae30 \ub54c\ubb38\uc5d0 \ub9cc \ubc88\uc758 \ubc18\ubcf5 \uc2dc\ud589\uae4c\uc9c0\ub294 \uac00\uc9c0 \uc54a\uc744 \uac83\uc785\ub2c8\ub2e4. \ub9ce\uc740 \uac83\ub4e4\uc774 \uc774 \ud568\uc218\uc5d0\uc11c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4. \uc774\ub97c \ubaa8\ub450 \uc77d\uace0 \uc5ec\ub7ec\ubd84\uc758 \uac83\uc73c\ub85c \ub9cc\ub4dc\uc138\uc694. \ucf54\ub4dc\uc5d0 \uc8fc\uc11d\ub4e4\uc744 \ub2ec\uc544 \ub1a8\uc2b5\ub2c8\ub2e4.<br \/>","96f73722":"## Feature Engineering through Aggregations<br \/>\nIn order to incorporate the individual data into the household data, we need to aggregate it for each household. <br \/>\n=> \uac1c\ubcc4 \ub370\uc774\ud130\ub97c \uac00\uad6c\ubcc4 \ub370\uc774\ud130\ub85c \ubcc0\ud658\ud558\uae30 \uc704\ud574\uc11c \uac01\uac01\uc758 \uac00\uad6c\ubcc4\ub85c \ubcd1\ud569\uc744 \ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nThe simplest way to do this is to groupby the family id idhogar and then agg the data. <br \/>\n=> \uac00\uc7a5 \uac04\ub2e8\ud55c \ubc29\ubc95\uc73c\ub85c\ub294 \uac00\uad6c\ubcc4 \uc2dd\ubcc4\uc790\ub85c \uadf8\ub8f9\ud6c4\uc5d0 \uadf8\ub8f9\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4<br \/>\nFor the aggregations for ordered or continuous variables, we can use six, five of which are built in to pandas, and one of which we define ourselves range_. <br \/>\n=> \uc21c\uc11c \ubc0f \uc5f0\uc18d\uc801\uc778 \ubcc0\uc218\uc5d0 \ub300\ud574\uc11c \uadf8\ub8f9\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub294\ub370 \uc788\uc5b4\uc11c \uc5ec\uc12f\uac00\uc9c0\uac00 \uc788\uc73c\uba70, \ud55c\uac00\uc9c0\ub294 \uc9c1\uc811 \uc815\uc758\ud574\uc11c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.<br \/>\nThe boolean aggregations can be the same, but this will create many redundant columns which we will then need to drop. <br \/>\n=> \ub17c\ub9ac\ud615 \ubcc0\uc218\ub3c4 \uacb0\ud569\ud558\ub294 \uac83\uc774 \uac19\uc9c0\ub9cc, \uc774\ub294 \uc6b0\ub9ac\uac00 \ub098\uc911\uc5d0 \ubc84\ub9b4 \uc218\ub3c4 \uc788\ub294 \uc911\ubcf5\ub41c \uceec\ub7fc\ub4e4\uc774 \uc0dd\uae30\ub294 \uacbd\uc6b0\uac00 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nFor this case, we'll use the same aggregations and then go back and drop the redundant columns.<br \/>\n=> \uc6b0\ub9ac\ub294 \uac19\uc740 \uacb0\ud569\uc815\ucc45\uc744 \uc0ac\uc6a9\ud558\uace0 \ub098\uc911\uc5d0 \ub3cc\uc544\uac00\uc11c \uc911\ubcf5\ub418\ub294 \uceec\ub7fc\ub4e4\uc744 \uc81c\uac70 \ud560 \uac83\uc785\ub2c8\ub2e4.","27922861":"We'll drop the columns and then merge with the heads data to create a final dataframe.","80a4447b":"## Feature Importances<br \/>\nWith a tree-based model, we can look at the feature importances which show a relative ranking of the usefulness of features in the model. <br \/>\n=> \ud2b8\ub9ac \ubaa8\ub378\ub85c \uc6b0\ub9ac\ub294 \ubaa8\ub378\uc5d0\uc11c \ud2b9\uc9d5\uc758 \ud544\uc694\uc5c6\ub294 \uc815\ub3c4\ub97c \ub098\ud0c0\ub0b4\ub294 \uc0c1\ub300\uc801 \ub7ad\ud0b9\uc744 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nThese represent the sum of the reduction in impurity at nodes that used the variable for splitting, but we don't have to pay much attention to the absolute value. Instead we'll focus on relative scores.<br \/>\n=> \uc774\ub294 \ubcc0\uc218\ub97c \ubd84\uae30\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\ub294 \uac01\uac01\uc758 \ub178\ub4dc\uc5d0 \ubd88 \uc21c\uc218\uc131\uc5d0\uc11c reduction\uc758 \ud569\uc744 \ub098\ud0c0\ub0b4\uba70, \uc774\ub4e4\uc758 \uc2e4\uc81c\uac12\uc5d0\ub294 \ub9ce\uc740 \uad00\uc2ec\uc744 \ub450\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \ub300\uc2e0\uc5d0 \uc6b0\ub9ac\ub294 \uc0c1\ub300\uc801\uc778 \uc810\uc218\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd9c \uac83\uc785\ub2c8\ub2e4.\n\nIf we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances.<br \/>\n\ub9cc\uc57d feature_importances \uc758 \uc218\uce58\ub97c \ubcf4\uace0 \uc2f6\ub2e4\uba74, \ubaa8\ub378\uc744 \uc804\uccb4 \ud2b8\ub808\uc774\ub2dd \uc14b\uc5d0 \ud6c8\ub828\uc2dc\ud0a4\uace0 \ud574\ub77c.","c4c164c9":"## Feature Selection \nOne potential method for improving model performance is feature selection. This is the process where we try to keep only the most useful features for our model. \"Most useful\" can mean many different things, and there are numerous heuristics for selecting the most important features. For feature selection in this notebook, we'll first remove any columns with greater than 0.95 correlation (we already did some of this during feature engineering) and then we'll apply recursive feature elimination with the Scikit-Learn library. <br \/>\n\ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud55c \ud558\ub098\uc758 \ubc29\ubc95\uc740 feature_selection\uc774\ub2e4. \uc774\ub294 \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc5d0\uc11c \uc720\uc758\ubbf8\ud55c \uceec\ub7fc\ub4e4\ub9cc \ub0a8\uae30\uae30 \uc704\ud574\uc11c \uc9c4\ud589\ud558\ub294 \uac83\uc774\ub2e4. \uac00\uc7a5 \uc720\uc6a9\ud558\ub2e4\ub294 \uac83\uc740 \ub9ce\uc740 \ub2e4\ub978 \uac83\ub4e4\uc744 \uc758\ubbf8\ud558\uace0, \uc774\ub7f0 \uac00\uc7a5 \uc911\uc694\ud55c \uceec\ub7fc\ub4e4 \ucc3e\ub294\ub370 \ub9ce\uc740 \uacbd\ud5d8\uc801 \uc9c0\uc2dd\ub4e4\uc774 \uc788\ub2e4. \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 0.95\uc774\uc0c1\uc758 \uc0c1\uad00\uad00\uaed8 \uc774\uc0c1\uc744 \uac00\uc9c0\ub294 \uc784\uc758\uc758 \uceec\ub7fc\ub4e4\uc744 \uc9c0\uc6b0\uace0, **Recursive feature elimination** \uae30\ubc95\uc744 \uc774\uc6a9\ud560 \uac83\uc774\ub2e4\n\n\nFirst up are the correlations. 0.95 is an arbitrary threshold - feel free to change the values and see how the performance changes! <br \/>\n\uc77c\ub2e8\uc740 0.95\ub97c \uc784\uacc4\uc810\uc73c\ub85c \ud558\uc5ec \uceec\ub7fc\ub4e4\uc744 \ucc3e\uc544\ub0b4\uc790\n","e4a2e7b4":"Redundant Individual Variables [\uac1c\ubcc4 \ub2e8\uc704 \ubcc0\uc218\uc5d0 \ub300\ud55c \uc911\ubcf5\ucc98\ub9ac]<br \/>\nWe can do the same process we did with the household level variables to identify any redundant individual variables. <br \/>\n=> \uc6b0\ub9ac\ub294 \uac1c\ubcc4 \ub2e8\uc704 \ubcc0\uc218\uc5d0\ub3c4 \uc720\uc0ac\ud55c \ubc29\ubc95\uc744 \uc801\uc6a9\ud558\ub824\uace0 \ud569\ub2c8\ub2e4<br \/>\nWe'll focus on any variables that have an absolute magnitude of the correlation coefficient greater than 0.95.<br \/>\n=> \uc0c1\uad00\uacc4\uc218\uac00 0.95\uc774\uc0c1\uc778 \uac12\ub4e4\uc5d0 \ub300\ud574\uc11c\ub9cc \ud655\uc778\ud574 \ubd05\uc2dc\ub2e4","90db6fba":"We can also take our new variable, inst, and divide this by the age. The final variable we'll name tech: this represents the combination of tablet and mobile phones.","a6ec286d":"It seems like households in an urban area (value of 1) are more likely to have lower poverty levels than households in a rural area (value of 0).<br \/>\n=> \uc704\uc5d0 \uc758\ud558\uba74 \ub3c4\uc2ec\uc5d0 \uc0ac\ub294 \uac00\uad6c\uac00 \ucde8\uc57d\uacc4\uce35\ub4e4\uc744 \ubcc4\ub85c \uac00\uc9c0\uace0 \uc788\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","ddc180c1":"That's one model to cross off the list (although we didn't perform hyperparameter tuning so the actual performance could possibly be improved). <br \/>","a8a7eaa2":"Object Columns [\uac1d\uccb4\ud615 \uceec\ub7fc]<br \/>\nThe last column type is object which we can view as follows.","dd6b8922":"We can investigate the object to see the training scores for each iteration. The following code will plot the validation scores versus the number of features for the training. <br \/>\n=> \uc6b0\ub9ac\ub294 \uac01\uac01\uc758 \ubc18\ubcf5\ub9c8\ub2e4 \ud6c8\ub828 \uc810\uc218\ub97c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc758 \ucf54\ub4dc\ub294 \uad50\ucc28\uac80\uc99d \uc810\uc218\uc640 \uceec\ub7fc\uc758 \uac1c\uc218\ub97c \uadf8\ub9bc\ud654 \ud55c \uac83\uc785\ub2c8\ub2e4.","cf0b062a":"These variables are highly correlated, and we don't need to keep both in our data.<br \/>\n=> \uc704\uc640 \uac19\uc774 \uc774\ub7ec\ud55c \uc131\uaca9\uc758 \uac12\uc740 \uad34\uc55a\u3147\ud788 \uc0c1\uad00\uad00\uacc4\uac00 \ucee4\uc11c \ub458 \ub2e4 \ubaa8\ub450\ub97c \ub370\uc774\ud130\uc14b\uc5d0 \ub0a8\uaca8\ub458 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.","0e3366ac":"The data has no missing values and is scaled between zero and one. This means it can be directly used in any Scikit-Learn model.","b4fe2a8e":"Then we fit the selector on the training data as with any other sklearn model. This will continue the feature selection until the cross validation scores no longer improve. <br \/>\n=> \ub2e4\uc74c\uc73c\ub85c\ub294 \uc120\ud0dd\uc790\ub97c \ud6c8\ub828\uc138\ud2b8\ub97c \uc774\uc6a9\ud558\uc5ec \ud559\uc2b5\uc2dc\ud0b5\ub2c8\ub2e4. \uc774\ub294 \uad50\ucc28\uac80\uc99d\uc810\uc218\uac00 \ub354 \uc774\uc0c1 \ub0ae\uc544\uc9c8 \ub54c\uac00 \uc5c6\uc744 \ub54c \uae4c\uc9c0 \uacc4\uc18d\ud574\uc11c \uc9c4\ud589 \ud560 \uac83\uc785\ub2c8\ub2e4.","14117c72":"The multi-layer perceptron (a deep neural network) has decent performance. This might be an option if we are able to hyperparameter tune the network. However, the limited amount of data could be an issue with a neural network as these generally require hundreds of thousands of examples to learn effectively. <br \/>\n=> \ub2e4\uce35 \ud37c\uc138\ube0c\ub860\uc740 \uad49\uc7a5\ud55c \uc131\ub2a5\uc744 \ubcf4\uc5ec\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \ud29c\ub2dd\uc744 \uc880\ud55c\ub2e4\uba74 \ub354 \uc88b\uc544\uc9c8 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc2e0\uacbd\ub9dd\uc5d0\uc11c \uc791\uc740 \uc591\uc758 \ub370\uc774\ud130\ub294 \ubb38\uc81c\uac00 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","db5f5abd":"The massive advantage of the gradient boosting machine is on display here. For a final step, let's try using 10-folds with both sets and add them to the plot. <br \/>","512449c3":"### 2. Search Space\nThe domain is the entire range of values over which we want to search. The only difficult part is the subsample ratio which must be set to 1.0 if the boosting_type=\"goss\". Feel free to play around with the values here. <br \/>\n=> \ub3c4\uba54\uc778\uc740 \uc6b0\ub9ac\uac00 \ucc3e\uace0\uc790\ud558\ub294 \uac12\ub4e4\uc758 \uc804\uccb4 \ubc94\uc704\uc785\ub2c8\ub2e4. \uc11c\ube0c\uc0d8\ud50c \ube44\uc728\uc740 \uc5b4\ub824\uc6b4 \ubd80\ubd84\uc774\uba70, \ub9cc\uc57d boosting_type\uc774 'goss'\uc774\uba74 \ubb34\uc870\uac74 1.0\uc73c\ub85c \uc138\ud305\ud574\uc57c\ud569\ub2c8\ub2e4. \uc790\uc720\ub86d\uac8c \ub3c4\uba54\uc778\ub4e4\uc744 \ub458\ub7ec\ubcf4\ub3c4\ub85d \ud569\uc2dc\ub2e4.\n\n> https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin [hyperopt\uc5d0 \ub300\ud558\uc5ec]","5910dd20":"## Household Level Variables<br \/>\nFirst let's subset to the heads of household and then to the household level variables.<br \/>\n=> \uac00\uc7a5\ub4e4\uc758 \uc11c\ube0c\uc14b\uc744 \ub9cc\ub4e4\uace0 \uac00\uad6c\ubcc4 \uceec\ub7fc\ub4e4\uc744 \ucd94\ucd9c\ud569\ub2c8\ub2e4.","17ed9ef2":"The Spearman correlation is often considered to be better for ordinal variables such as the Target or the years of education. <br \/>\n=> Spearman \uc0c1\uad00\uacc4\uc218\ub294 \ud0c0\uac9f\uc774\ub098 \uad50\uc721\ub144\uc218\uc640 \uac19\uc740 \uc21c\uc11c\uc801\uc778 \ubcc0\uc218\ub4e4\uc5d0\uac8c \uc88b\uc744 \uac83\uc774\uba70<br \/>\nMost relationshisp in the real world aren't linear, and although the Pearson correlation can be an approximation of how related two variables are, it's inexact and not the best method of comparison.<br \/>\n\uc2e4\uc81c\ub85c\ub294 \ubaa8\ub4e0 \uad00\uacc4\uac00 \uc120\ud615\uc77c\ub9ac\ub294 \uc5c6\uc73c\uba70, \ube44\ub85d Pearson \uc0c1\uad00\uacc4\uc218\uac00 \ub450 \ubcc0\uc218\uac00 \uc5b4\ub5bb\uac8c \uad00\uacc4\uc788\ub294\uc9c0\uc5d0 \ub300\ud55c \ub300\ub7b5\uc801\uc778 \uac83\uc744 \ubcf4\uc5ec \uc904\uc9c0\ub77c\ub3c4 \ube44\uad50\uc5d0 \uc788\uc5b4\uc11c \ud56d\uc0c1 \ucd5c\uace0\uc758 \ubc29\ubc95\uc740 \uc544\ub2c8\ub2e4","9307fcbe":"This is simply the opposite of male! We can remove the male flag.","084b6c6d":"These show one out of each pair of correlated variables. To find the other pair, we can subset the corr_matrix.<br \/>\n=> \uad00\ub828\ub41c \ubcc0\uc218\uc758 \uac01\uac01\uc758 \uc30d\uc911 \ud558\ub098\ub97c \ubcf4\uc5ec\uc90c. \ub2e4\ub978 \uc30d\uc744 \ucc3e\uae30\uc704\ud574\uc11c\ub294 corr_matrix\ub97c \ub9cc\ub4e0\ub2e4","aeab56f6":"Overall, the average education of households with female heads is slightly higher than those with male heads. <br \/>\n=> \uc804\ubc18\uc801\uc73c\ub85c \uac00\uc7a5\uc774 \ub0a8\uc790\uc778 \uacbd\uc6b0\ubcf4\ub2e4 \uc5ec\uc790\uc778 \uacbd\uc6b0\uac00 \ub354 \uad50\uc721\uae30\uac04\uc774 \uae34 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.<br \/>\nI'm not too sure what to make of this, but it seems right to me.<br \/>\n=> \uc65c \uc774\ub807\uac8c \ub418\uc5c8\ub294\uc9c0\ub294 \ubaa8\ub974\uaca0\uc9c0\ub9cc \uc774\ub7ec\ud55c \ud1b5\ucc30\uc744 \uc5bb\uc744 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.","e29598bb":"Well, that solves the issue! Every family that has nan for v18q1 does not own a tablet. Therefore, we can fill in this missing value with zero.<br \/>\n=> v18q1\uc5d0\uc11c \ud14c\ube14\ub9bf\uc744 \uac00\uc9c0\uc9c0 \ubabb\ud55c \ubaa8\ub4e0 \uac00\uc871\ub4e4\uc774 nan\uac12\uc744 \uac00\uc9d0\uc744 \ucc3e\uc544\ub0c8\uc2b5\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc6b0\ub9ac\ub294 \ube48 \uac12\uc744 0\uc73c\ub85c \ucc44\uc6cc\uc904 \uac83\uc785\ub2c8\ub2e4.","ecf51c2b":"The remaining missing values in each column will be filled in, a process known as Imputation. <br \/>\n=> \ub0a8\uc740 \uac01 \uceec\ub7fc\uc758 \uc190\uc2e4 \uac12\ub4e4\uc740 Imputation\uc774\ub77c\uace0 \uac70\uce58\ub294 \uc190\uc2e4 \uac12\uc744 \ucc44\uc6b0\ub294 \uacfc\uc815\uc744 \uac70\uce58\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4.<br \/>\nThere are several types of imputation commonly used, and one of the simplest and most effective methods is to fill in the missing values with the median of the column.<br \/>\n=> \uba87\uba87\uc758 imputation \ud0c0\uc785\uc740 \uc8fc\ub85c \uc0ac\uc6a9\ub418\uba70, \uadf8\uc911 \uac00\uc7a5 \uac04\ub2e8\ud558\uace0 \ud6a8\uacfc\uc801\uc778 \ubc29\ubc95\uc740 \uc911\uc559\uac12\uc744 \ub123\ub294 \uac83\uc785\ub2c8\ub2e4.","b29493c5":"There is also one outlier in the rez_esc column. <br \/>\n=> rez_esc \uceec\ub7fc\uc5d0 \ud558\ub098\uc758 \uc774\uc0c1\uce58\ub97c \ucc3e\uc544 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nAgain, if we read through the competition discussions, we learn that the maximum value for this variable is 5. Therefore, any values above 5 should be set to 5.<br \/>\n=> \ub514\uc2a4\ucee4\uc158\uc744 \uc77d\uc5c8\ub2e4\uba74 \ud574\ub2f9 \uac12\uc758 \ucd5c\ub300\uce58\ub294 5\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c 5\uc758 \uac12\uc744 \ub118\ub294 \uac83\ub4e4\uc5d0\uac8c \ubaa8\ub450 5\ub97c \ud560\ub2f9\ud574 \uc90d\uc2dc\ub2e4","08f087a5":"Overall, these results show the issue with imbalanced class problems: our model cannot distinguish very well between the classes that are underrepresented. Later we'll look at predictions themselves and see where our model is \"confused\". For now, we can generate a submission file and submit it to the competition. <br \/>\n=> \uc774 \ub610\ud55c \uc6b0\ub9ac\uac00 \uc9c1\uba74\ud55c \ud074\ub798\uc2a4 \ubd88\uc77c\uce58 \ubb38\uc81c\uac00 \uc788\ub2e4. \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc740 \ube48\ub3c4\uc218\uac00 \uc801\uac8c \ub4f1\uc7a5\ud558\ub294 \ud074\ub798\uc2a4\ub4e4\uc744 \uc798 \uad6c\ubd84\ud558\uc9c0 \ubabb\ud55c\ub2e4. \ucd94\ud6c4\uc5d0 prediction\uc744 \ubcf4\uace0 \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc774 \ubb38\uc81c\uac00 \uc788\ub2e4\ub294\uac83\uc744 \uc0b4\ud3b4\ubcf4\uc790. \uc77c\ub2e8\uc740 \uc81c\ucd9c\uc744 \ud574\ubcf4\ub3c4\ub85d\ud558\uc790.\n\nWhen we actually make predictions for each household, we average the predictions from each of the folds. Therefore, we are essentially using multiple models since each one is trained on a slightly different fold of the data. The gradient boosting machine is already an ensemble machine learning model, and now we are using it almost as a meta-ensemble by averaging predictions from several gbms. <br \/>\n=> \uc6b0\ub9ac\uac00 \uac01 \uac00\uad6c\ubcc4\ub85c \uc608\uce21\uce58\ub97c \ub9cc\ub4e4\ub54c, \uc6b0\ub9ac\ub294 \uac01 \ud3f4\ub4dc\ub85c\ubd80\ud130 \ub098\uc624\ub294 \uc608\uce21\uce58\ub4e4\uc744 \ud3c9\uade0\ud588\ub2e4. \uadf8\ub7ec\ubbc0\ub85c \uac01\uac01\uc758 \ubaa8\ub378\uc774 \ub370\uc774\ud130\uc758 \ub2e4\ub978 \uacc4\uce35\uc73c\ub85c \ud6c8\ub828\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc5ec\ub7ec\uac1c\uc758 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc740 \ud544\uc218\uc801\uc774\ub2e4. \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc740 \uc774\ubbf8 \uc559\uc0c1\ube14 \ubaa8\ub378\uc774\uace0, \uc6b0\ub9ac\ub294 \uac00\ub9ac\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc744 \ub2e4\ub978 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\ub4e4\ub85c\ubd80\ud130 \uc608\uce21\uce58\ub4e4\uc744 \ud3c9\uade0\ud55c meta-ensemble\ub85c\uc368 \ud65c\uc6a9\ud55c\ub2e4.\n\nThis process is shown in the code below.","034795ac":"The meaning of the home ownership variables is below:<br \/>\n=> \ud574\ub2f9 \uac11\uc758 \uc815\ubcf4\ub294 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4\n\ntipovivi1, =1 own and fully paid house<br \/>\ntipovivi2, \"=1 own,  paying in installments\"<br \/>\ntipovivi3, =1 rented<br \/>\ntipovivi4, =1 precarious<br \/>\ntipovivi5, \"=1 other(assigned,  borrowed)\"\n\nWe've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. <br \/>\n=> \uc6b0\ub9ac\ub294 \ub610\ub2e4\ub978 \ubb38\uc81c\ub97c \ud574\uacb0\ud588\uc2b5\ub2c8\ub2e4. \ub9e4\ub2ec \ub0b4\ub294 \uc6d4\uc138\uc758 \uac12\uc774 \uc5c6\ub294 \uc774\uc720\ub294 \ub300\ubd80\ubd84 \uc774\ubbf8 \uadf8\ub4e4 \uc18c\uc720\uc758 \uc9d1\uc5d0 \uc0b4\uace0 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4<br \/>\nIn a few other situations, we are not sure of the reason for the missing information.<br \/>\n=> \uba87\uba87 \ub2e4\ub978 \uc0c1\ud669\ub4e4\uc5d0 \ub300\ud574\uc11c\ub294 \uc65c \uc190\uc2e4 \uac12\uc744 \uac00\uc9c0\ub294\uc9c0\ub294 \ud655\uc2e0\ud560 \uc218 \uc5c6\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4\n\nFor the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to zero. <br \/>\n=>\uadf8\ub798\uc11c \ud574\ub2f9\ud558\ub294 \uacbd\uc6b0\uc758 \uc6d4\uc138\uac12\uc740 0\uc73c\ub85c \uc124\uc815 \ud560 \uc218 \uc788\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4<br \/>\nFor the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values.<br \/>\n=> \ub2e4\ub978 \uc9d1\ub4e4\uc758 \uacbd\uc6b0\uc5d0\ub294 \uc190\uc2e4 \uac12\uc73c\ub85c \ub0a8\uaca8\ub450\uc9c0\ub9cc, \ud558\ub098\uc758 \ub17c\ub9ac\ud615 \uceec\ub7fc\uc744 \uc774\uc6a9\ud558\uc5ec \uc190\uc2e4 \uac12\uc744 \uac00\uc9c0\uace0 \uc788\uc74c\uc744 \ud45c\uc2dc\ud574\ub458 \uac83\uc785\ub2c8\ub2e4.","612eb01e":"## v2a1: Monthly rent payment<br \/>\n=> \ub9e4\ub2ec \uc9c0\ubd88\ud558\ub294 \uc6d4\uc138\n\nThe next missing column is v2a1 which represents the montly rent payment.<br \/>\n=> \ud574\ub2f9 \uceec\ub7fc\uc740 \ub9e4\ub2ec \uc9c0\ubd88\ud558\ub294 \uc6d4\uc138\uc758 \uc815\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4\n\nIn addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of tipovivi_, the columns showing the ownership\/renting status of the home. <br \/>\n=> \uc774 \uceec\ub7fc\uc758 \uc190\uc2e4 \uac12\uc744 \ubcf4\ub294\ub370 \uc788\uc5b4\uc11c, tipoviv_(\uc9d1\uc758 \uc18c\uc720\/\ub300\uc5ec \uad6c\ubd84\uc744 \ud0c0\ub098\ub0b4\ub294 \uceec\ub7fc)\uc744 \ucc38\uace0\ud574\ubd10\uc57c \ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4<br \/>\nFor this plot, we show the ownership status of those homes with a nan for the monthyl rent payment.<br \/>\n=> \uc774 \uadf8\ub9bc\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 \ub9e4\ub2ec \ub0b4\ub294 \uc6d4\uc138\uac00 nan\uc778 \uc9d1\uc758 \uc18c\uc720\uc0c1\ud0dc\ub97c \ubcf4\ub824\uace0 \ud569\ub2c8\ub2e4","7cbb63b9":"Finally, we select the features and then evaluate in cross validation. <br \/>\n=> \ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc6b0\ub9ac\ub4e4\uc758 \uad50\ucc28\uac80\uc99d \uacb0\uacfc\ub97c \ud655\uc778\ud574 \ubd05\uc2dc\ub2e4.","c91a94b9":"# Model Optimization\nModel optimization is the process of extracting the best performance from a machine learning model by tuning the hyperparameters through cross-validation. This is necessary because the best model hyperparameters are different for every dataset. <br \/>\n=> \ubaa8\ub378 \ucd5c\uc801\ud654\ub294 \uba38\uc2e0\ub7ec\ub2dd\ubaa8\ub378\uc5d0\uc11c \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \uad50\ucc28\uac80\uc99d\uc744 \ud1b5\ud574 \ud29c\ub2dd\ud558\uc5ec \ucd5c\uace0\uc758 \uc131\ub2a5\uc744 \ubf51\uc544\ub0b4\uae30 \uc704\ud55c \uacfc\uc815\uc785\ub2c8\ub2e4. \uc774\ub294 \ucd5c\uace0\uc758 \ubaa8\ub378 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\uac00 \ub370\uc774\ud130 \uc138\ud2b8\ub4e4\ub9c8\ub2e4 \ub2e4\ub974\uae30 \ub54c\ubb38\uc5d0 \ud544\uc218\uc801\uc785\ub2c8\ub2e4.\n\nIn this notebook, we won't get too far into model tuning, but there are multiple options:\n\uc5ec\uae30\uc11c\ub294 \ubaa8\ub378 \ud29c\ub2dd\uc5d0 \ub300\ud574\uc11c \uadf8\ub807\uac8c \uae4a\uac8c \ub2e4\ub8e8\uc9c0\ub294 \uc54a\uaca0\uc9c0\ub9cc \uc5ec\ub7ec\uac00\uc9c0 \uc635\uc158\ub4e4\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n1. Manual\n2. Grid Search\n3. Random Search\n4. Automated Optimization\n\nWe will stick to 4. because it is generally the most efficient method and can easily be implemented in a number of libraries, including Hyperopt, which uses a modified version of Bayesian Optimization with the Tree Parzen Estimator.<br \/>\n=> \uc6b0\ub9ac\ub294 \ub124\ubc88\uc9f8\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubcfc \uac83\uc774\uba70, \uc774\ub294 \uac00\uc7a5 \ud6a8\uc728\uc801\uc778 \ubc29\ubc95\uc774\uba74\uc11c \ub9ce\uc740 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4\ub85c \uc27d\uac8c \uc2e4\ud589\uac00\ub2a5\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 Hyperopt\ub97c \uc0ac\uc6a9\ud560 \uac83\uc778\ub370 \uc774\ub294 Tree Parzen Estimator\ub97c \ud65c\uc6a9\ud55c Bayesian Optimization\uc758 \uc218\uc815\ub41c \ubc84\uc804\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n## Model Tuning with Hyperopt\nBayesian optimization requires 4 parts: <br \/>\n=> Bayesian optimization\uc740 \ub124\uac00\uc9c0 \ud30c\ud2b8\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4.\n\n* Objective function: what we want to maximize (or minimize) <br \/>\n\ucd5c\ub300\ud654 \ucd5c\uc18c\ud654 \uc2dc\ud0a4\ub824\ub294 \ubaa9\uc801 \ud568\uc218 <br \/>\n* Domain space: region over which to search <br \/>\n\ucc3e\uc73c\ub824\ub294 region<br \/>\n* Algorithm for choosing next hyperparameters: uses past results to suggest next values <br \/>\n\ub2e4\uc74c \ud30c\ub77c\ubbf8\ud130\ub97c \uace0\ub974\ub294 \uc54c\uace0\ub9ac\uc998: \ub450\ubc88 \uc9f8\uc758 \uacb0\uacfc\uac12\uc744 \uac12\uc73c\ub85c \uc0ac\uc6a9\ud568<br \/>\n* Results history: saves the past results <br \/>\n\uacb0\uacfc history: \uacfc\uac70 \uacb0\uacfc\ub4e4\uc77c \uae30\ub85d\ud568 <br \/>\n\nI've written previously about using Hyperopt, so here we'll stick to the implementation.<br \/>\n\uc6b0\ub9ac\ub294 \uc774\ub97c \uc774\uc6a9\ud574\uc11c \uc2e4\ud589\ud574 \ubd05\uc2dc\ub2e4 <br \/>\n","ac7d3c33":"## Creating Ordinal Variable<br \/>\nI'm going to compress these four variables into one by creating an ordinal variable. I'm going to choose the mapping myself, based on the data decriptions:<br \/>\n=> \ub124\uac00\uc9c0 \uceec\ub7fc\uc744 \ud558\ub098\ub85c \ud569\uce60 \uac83\uc785\ub2c8\ub2e4. \ud569\uce58\ub294 \ub370 \uc788\uc5b4\uc11c \ub9e4\ud551\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n0: No electricity<br \/>\n1: Electricity from cooperative<br \/>\n2: Electricity from CNFL, ICA, ESPH\/JASEC<br \/>\n3: Electricity from private plant<br \/>\n\nAn ordered variable has an inherent ordering, and for this we choose our own based on the domain knowledge. <br \/>\n=> <br \/>\nAfter we create this new ordered variable, we can drop the four others. <br \/>\n=> \uc774\ub97c \ud1b5\ud574 \ud558\ub098\uc758 \uceec\ub7fc\uc744 \ub9cc\ub4e4\uba74 \ub2e4\ub978 \uceec\ub7fc\ub4e4\uc744 \uc5c6\uc560\uc90d\ub2c8\ub2e4<br \/>\nThere are several households that do not have a variable here, so we will use a nan (which will be filled in during imputation) and add a Boolean column indicating there was no measure for this variable.<br \/>\n=> \ub9cc\uc57d \uac12\uc774 \uc5c6\ub2e4\uba74 \ub098\uc911\uc5d0 \ucc44\uc6cc\uc904 \uac83\uc774\ubbc0\ub85c \ud574\ub2f9\ud558\ub294 \uac12\uc744 \uad6c\ubd84\ud558\uae30 \uc704\ud574\uc11c \ub17c\ub9ac\ud615 \uceec\ub7fc\uc744 \ucd94\uac00\ud558\uc5ec\uc90d\ub2c8\ub2e4","b69228f2":"## Addressing Wrong Labels [\uc798\ubabb\ub41c \ub77c\ubca8 \uc0b4\ud3b4\ubcf4\uae30]<br \/>\nAs with any realistic dataset, the Costa Rican Poverty data has some issues. <br \/>\n=> \uc6b0\ub9ac\uac00 \uc0b4\ud3b4\ubcfc \ub370\uc774\ud130\uc14b\uc5d0\ub3c4 \uc2e4\uc81c \ub370\uc774\ud130\uc14b\uc774 \uac00\uc9c0\uace0 \uc788\ub294 \ubb38\uc81c\uc640 \uac19\uc740 \uac83\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4.<br \/>\nTypically, 80% of a data science project will be spent cleaning data and fixing anomalies\/errors. <br \/>\n80%\uc758 \ub370\uc774\ud130 \uc0ac\uc774\uc5b8\uc2a4 \ud504\ub85c\uc81d\ud2b8\ub294 \ub370\uc774\ud130\ub97c \uc815\uc81c\ud558\uace0 \uc624\ub958\ub97c \uc218\uc815\ud558\ub294\ub370 \uc2dc\uac04\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.<br \/>\nThese can be either human entry errors, measurement errors, or sometimes just extreme values that are correct but stand out. <br \/>\n=> \uc774\uac83\uc740 \uc544\ub9c8\ub3c4 \uc0ac\ub78c\uc774 \ub370\uc774\ud130\ub97c \ub123\ub294\ub370 \uc788\uc5b4\uc11c \uc5d0\ub7ec\uc774\uac70\ub098, \uce21\uc815\uc624\ub958\uc774\uac70\ub098 \ub610\ub294 \uc5b4\ub5a4 \uadf9\ud55c\uc758 \uc774\uc0c1\uce58\uc778 \uacbd\uc6b0\uac00 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nFor this problem, some of the labels are not correct because individuals in the same household have a different poverty level. <br \/>\n=> \uc774\ubb38\uc81c\ub85c \uc778\ud574\uc11c, \uba87\uac1c\uc758 \ub77c\ubca8\uc740 \uac19\uc740 \uac00\uad6c\uc774\uc9c0\ub9cc \ud0c0\uac9f \ub77c\ubca8\uc774 \ub2e4\ub978 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.<br \/>\nWe're not told why this may be the case, but we are told to use the head of household as the true label.<br \/>\n=> \uc774 \ucef4\ud53c\ud2f0\uc158\uc5d0\uc11c\ub294 \uc65c\uc778\uc9c0\ub294 \ubaa8\ub974\uaca0\uc9c0\ub9cc, \uc6b0\ub9ac\ub294 \uac00\uad6c\uc758 \uac00\uc7a5\uc758 \ub77c\ubca8\uc774 \ub9de\ub2e4\ub294 \uac83\uc744 \uacc4\uc18d \uc5b8\uae09\ud574 \uc654\uc2b5\ub2c8\ub2e4.\n\nThat information makes our job much easier, but in a real-world problem, we would have to figure out the reason Why the labels are wrong and how to address the issue on our own. <br \/>\n=> \uc774\ub7ec\ud55c \uc815\ubcf4\ub294 \uc6b0\ub9ac\uac00 \ud558\ub294 \uc77c\uc744 \ub354\uc6b1\ub354 \uc27d\uac8c \ud558\uc9c0\ub9cc, \uc2e4\uc81c \uc601\uc5ed\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 \uc65c \ub77c\ubca8\ub4e4\uc774 \ub2e4\ub978\uc9c0\uc5d0 \ub300\ud574\uc11c \ucc3e\uc544\ub0b4\uc57c\ub9cc \ud569\ub2c8\ub2e4.<br \/>\nThis section fixes the issue with the labels although it is not strictly necessary: I kept it in the notebook just to show how we may deal with this issue.<br \/>\n=> \uc774\ub807\uac8c \ub77c\ubca8\uc744 \uace0\uce58\ub294 \uac83\uc774 \ud544\uc218\ub294 \uc544\ub2c8\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \uace0\uccd0\ubcf4\ub824\uace0\ud569\ub2c8\ub2e4.\n\nIdentify Errors<br \/>\nFirst we need to find the errors before we can correct them. <br \/>\n\uc77c\ub2e8 \uc624\ub958\ub97c \uc815\uc815\ud558\uae30 \uc804\uc5d0 \uc624\ub958\ub97c \uc218\uc9d1\ud574\ubd05\uc2dc\ub2e4.<br \/>\nTo find the households with different labels for family members, we can group the data by the household and then check if there is only one unique value of the Target.<br \/>\n\ub2e4\ub978 \ub77c\ubca8\uc744 \uac00\uc9c4 \uac00\uc871\uc744 \ucc3e\uc544\ub0b4\uae30 \uc704\ud574\uc11c \uc6b0\ub9ac\ub294 \ub370\uc774\ud130\ub4e4\uc744 \uac00\uad6c\ubcc4\ub85c \uadf8\ub8f9\uc744 \uc9d3\uace0 \ud0c0\uac9f\uc758 \uac12\uc774 \uace0\uc720\ud55c\uc9c0 \uc0b4\ud3b4\ubcf4\ub824\uace0\ud569\ub2c8\ub2e4.","94e43457":"It's hard to see the relationship, but it's slightly negative: as the dependency increases, the value of the Target decreases. <br \/>\n=> \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uae34 \uc5b4\ub835\uc9c0\ub9cc \uc57d\uac04\uc758 \ubd80\uc758 \uc0c1\uad00\uad00\uac8c\ub97c \uac00\uc9d0\uc744 \uc758\ubbf8\ud569\ub2c8\ub2e4.<br \/>\nThis makes sense: the dependency is the number of dependent individuals divided by the number of non-dependents. <br \/>\n=> \uc774\ub294 dependency\uac00 \ubd80\uc591\ud574\uc57c\ud560 \uac00\uc871 \/ \ubd80\uc591\ud558\ub294 \uac00\uc871 \uc744 \uc758\ubbf8\ud558\uae30 \ub54c\ubb38\uc5d0 \uc774\ub97c \uc720\ucd94\ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nAs we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members.","f513770e":"The predicted distribution looks close to the training distribution although there are some differences. Depending on the run of the notebook, the results you see may change, but for this edition, the 4s are underrepresented in the predictions and the 3s are overrepresented.\n\nOne potentially method for dealing with imbalanced classification problems is oversampling the minority class, which is easy to do in Python using the imbalanced learn library. We won't explore that option here.","d23759ae":"What this plot tells us is that we have to try out a number of different models to see which is optimal. <br \/>\n=> \uc774 \ud50c\ub86f\uc774 \ub9d0\ud558\uace0\uc790 \ud558\ub294 \ubc14\ub294 \uc5b4\ub290 \uac83\uc774 \ucd5c\uc801\uc778\uc9c0\ub97c \ucc3e\uae30\uc704\ud574\uc11c\ub294 \uc5ec\ub7ec\uac00\uc9c0 \ub2e4\ub978 \ubaa8\ub378\ub4e4\uc744 \ud14c\uc2a4\ud2b8 \ud574\ubd10\uc57c \ud55c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4. <br \/>\nMost people eventually settle on the gradient boosting machine and we will try that out, but for now we'll take a look at some of the other options. There are literally dozens (maybe hundreds) of multi-class machine <br \/>\n=>\ub9ce\uc740 \uc0ac\ub78c\ub4e4\uc774 \ub05d\ub0b4 \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305\uc5d0 \uba38\ubb3c\uc9c0\ub9cc, \uc6b0\ub9ac\ub294 \ub2e4\ub978 \uc120\ud0dd\uc9c0\ub4e4\uc744 \uc880 \uc0b4\ud3b4\ubcfc \ud544\uc694\uac00 \uc788\ub2e4. \uc5ec\uae30\uc5d0 \uad49\uc7a5\ud788 \ub9ce\uc740 \uc120\ud0dd\uc9c0\ub4e4\uc774 \uc788\ub2e4.<br \/>\nlearning models if we look at the Scikit-Learn documentation. We don't have to try them all, but we should sample from the options. <br \/>\n=> \ub2e4\ub294 \ud574\ubcf4\uc9c0 \ubabb\ud558\uaca0\uc9c0\ub9cc \uc77c\ubd80\ub77c\ub3c4 \uc2e4\ud5d8\ud574\ubcf4\ub3c4\ub85d \ud558\uc790 <br \/> \n\nWhat we want to do is write a function that can evaluate a model <br \/>. \n=> \uc6b0\ub9ac\ub294 \uac01 \ubaa8\ub378\uc744 \uc744 \ud3c9\uac00\ud560 \uc218 \uc788\ub294 \ud568\uc218\ub97c \uc9dc\uc57c\ud55c\ub2e4. <br \/>\nThis will be pretty simple since we already wrote most of the code. \n=> \uc774\ub294 \uac04\ub2e8\ud560 \uac83\uc774\ub2e4.\nIn addition to the Random Forest Classifier, we'll try eight other Scikit-Learn models. <br \/>\nLuckily, this dataset is relatively small and we can rapidly iterate through the models. <br \/>\n=> \uc6b4\uc88b\uac8c\ub3c4 \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc740 \uc791\uae30 \ub54c\ubb38\uc5d0 \uc0c1\ub300\uc801\uc73c\ub85c \ube60\ub974\uac8c \ub420 \uac83\uc774\ub2e4 <br \/>\nWe will make a dataframe to hold the results and the function will add a row to the dataframe for each model. <br \/>\n=> \uacb0\uacfc\ub97c \ub2f4\uc744 \ub370\uc774\ud130\ud504\ub808\uc784\uc744 \ub9cc\ub4e4\uace0 \uac01\ubaa8\ub378\uc5d0 \ub300\ud574\uc11c \uacb0\uacfc\ub97c \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0 \ud589\uc5d0 \ucd94\uac00\ud574 \uc904 \uac83\uc774\ub2e4.<br \/>","ff2aafd3":"That score is not great, but it will serve as a baseline and leaves us plenty of room to improve!","f51343b0":"We can keep using our plot_categoricals function to visualize these relationships, but seaborn also has a number of plotting options that can work with categoricals. <br \/>\nOne is the violinplot which shows the distribution of a variable on the y axis with the width of each plot showing the number of observations in that category.","ece18390":"The organizers tell us that the correct label is that for the head of household, where parentesco1 == 1. <br \/>\n=> \ub300\ud68c\uc758 \uc8fc\ucd5c\uc790\ub294 \uac00\uc7a5\uc758 \ub77c\ubca8\uc740 \uc815\ud655\ud558\ub2e4\uace0 \uc5b8\uae09\ud588\uc2b5\ub2c8\ub2e4..<br \/>\nFor this household, the correct label is 3 for all members. <br \/>\n=> \uadf8\ub798\uc11c \uc704\uc758 \uacbd\uc6b0\uc5d0\ub294 \ubaa8\ub4e0 \uac00\uc871\ub4e4\uc5d0\uac8c 3\uc774\ub77c\ub294 \uac12\uc744 \ud560\ub2f9\ud558\ub294 \uac83\uc774 \uc633\uc2b5\ub2c8\ub2e4.<br \/>\nWe can correct this (as shown later) by reassigning all the individuals in this household the correct poverty level.<br \/>\n=> \uc6b0\ub9ac\ub294 \uac00\uc7a5\uc758 \ub77c\ubca8 \uac12\uc744 \uc798\ubabb \uac1c\uc7ac\ub418\uc5b4 \uc788\ub294 \ub77c\ubca8\uc5d0 \ub123\uc5b4 \uc904\uac81\ub2c8\ub2e4.<br \/>\nIn the real-world, you might have to make the tough decision of how to address the problem by yourself (or with the help of your team).<br \/>\n=> \uc2e4\uc81c\ub85c\ub294, \uc5b4\ub5bb\uac8c \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \ud574\uacb0\ud560\uc9c0 \uace0\ubbfc\uc744 \ud574\ubd10\uc57c \ud560 \uac83\uc785\ub2c8\ub2e4.\n\n## Families without Heads of Household [\uac00\uc7a5\uc774 \uc5c6\ub294 \uac00\uad6c]<br \/>\nWe can correct all the label discrepancies by assigning the individuals in the same household the label of the head of household. <br \/>\n=> \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \ud0c0\uac9f \ub77c\ubca8 \ubd88\uc77c\uce58\ub97c \uac00\uc7a5\uc758 \ud0c0\uac9f \ub77c\ubca8\uc744 \ud560\ub2f9\ud574 \uc90c\uc73c\ub85c\uc368 \uc633\uac8c \ub9cc\ub4e4\uc5b4 \uc904 \uc218 \uc788\uc2b5\ub2c8\ub2e4<br \/>\nBut wait, you may ask: \"What if there are households without a head of household? And what if the members of those households have differing values of the label?\"<br \/>\n=> \uadf8\ub7ec\ub098 \uac04\ub2e8\ud55c \uc9c8\ubb38\uc744 \ud574\ubd05\uc2dc\ub2e4 \"\ub9cc\uc57d \uac00\uc7a5\uc774 \uc5c6\ub294 \uac00\uad6c\uac00 \uc788\ub2e4\uba74? \uadf8\ub9ac\uace0 \uadf8 \uad6c\uc131\uc6d0\ub4e4\uc774 \ub2e4\ub978 \ud0c0\uac9f \ub77c\ubca8\uc744 \uac00\uc9c0\uace0 \uc788\ub2e4\uba74?\"<br \/>\nWell, since you asked, let's investigate exactly that question!<br \/>\n=> \uc74c... \uadf8\ub807\ub2e4\uba74 \uc790\uc138\ud788 \uc774 \uc9c8\ubb38\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubcf4\ub3c4\ub85d \ud569\uc2dc\ub2e4","a1bef364":"Well that should make us feel better! All of the features we are using have some importance to the Gradient Boosting Machine. It might be a good idea to go back and retry feature selection but with the GBM since that is the model we are using. <br \/>\n=> \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \uba38\uc2e0\uc5d0 \ubaa8\ub4e0 \ud53c\uccd0\ub4e4\uc774 \uc0ac\uc6a9\ub418\uace0 \uc788\ub2e4\ub294 \uac83\uc740 \uc88b\uc740 \uc9d5\uc870\uc778 \uac83 \uac19\uc2b5\ub2c8\ub2e4! \ub2e4\uc2dc \ub3cc\uc544\uac00\uc11c feature selection\uc744 \uc7ac\uc2dc\ub3c4 \ud558\ub294 \uac83\uc740 \uc88b\uc740 \uc0dd\uac01\uc778 \uac83 \uac19\uc9c0\ub9cc \uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\uace0 \uc788\ub294 GBM\ubaa8\ub378\uc744 \uc774\uc6a9\ud574\uc57c \ud560 \uac83\uc785\ub2c8\ub2e4.\n\nThe next step to take is Model Optimization, the process of getting the most from a machine learning model. <br \/>\n\ub2e4\uc74c \ub2e8\uacc4\ub294 \uba38\uc2e0\ub7ec\ub2dd\ubaa8\ub378\uc5d0\uc11c \uac00\uc7a5 \uc911\uc694\ud55c \ubaa8\ub378 \ucd5c\uc801\ud654 \ub2e8\uacc4\uc785\ub2c8\ub2e4","4097b7be":"## Exploring Label Distribution<br \/>\nNext, we can get an idea of how imbalanced the problem is by looking at the distribution of labels.<br \/>\n=> \ub2e4\uc74c\uc73c\ub85c \uc5b4\ub5bb\uac8c \ub77c\ubca8 \ubd88\uade0\ud615 \ubb38\uc81c\ub97c \ud574\uacb0\ud574\uc57c \ud558\ub294\uc9c0\uc5d0 \ub300\ud55c \uc544\uc774\ub514\uc5b4\ub97c \uc5bb\uc744 \uc218 \uc788\uc744 \uac83\uc774\ub2e4.<br \/>\nThere are four possible integer levels, indicating four different levels of poverty. <br \/>\n=> \uc54c\ub2e4\uc2dc\ud53c \uc6b0\ub9ac\ub294 \ub124\uac1c\uc758 \ube48\ubd80\uc218\uc900\uc744 \ub098\ud0dc\ub0b4\ub294 \ub2e8\uacc4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4 <br \/>\nTo look at the correct labels, we'll subset only to the columns where parentesco1 == 1 because this is the head of household, the correct label for each household.<br \/>\n=> \uc815\ud655\ud55c \ub77c\ubca8\ub4e4\uc744 \ubcf4\uae30 \uc704\ud574\uc11c \uc6b0\ub9ac\ub294 \uac00\uc7a5\uc758 \uc11c\ube0c\uc14b\ub9cc\uc744 \uc774\uc6a9\ud560 \uac83\uc774\ub2e4\n\nThe bar plot below shows the distribution of training labels (since there are no testing labels).<br \/>\n\uc544\ub798\uc758 \uadf8\ub9bc\uc740 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc758 \ud0c0\uac9f \ub77c\ubca8\uc758 \uac12\uc758 \ubd84\ud3ec\ub97c \ubcf4\uc5ec\uc900\ub2e4","29dabc4e":"In most cases, the values are very similar.","b991f3aa":"That gives us a look at all of the columns which don't appear to be in any order. To get a quick overview of the data we use df.info().","08a4da9c":"## Machine Learning Modeling<br \/>\nOnce feature engineering\/construction is done, we can get started with the machine learning! <br \/>\n=> feature engineering\/construction\uc774 \ub05d\ub098\uace0 \ub09c\ud6c4\uc5d0 \uc6b0\ub9ac\ub294 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc744 \ub9cc\ub4e4 \uc2dc\uac04\uc785\ub2c8\ub2e4.<br \/>\nAll of our data (both training and testing) is aggregated for each household and so can be directly used in a model. <br \/>\n=> \uc6b0\ub9ac\uc758 \ubaa8\ub4e0 \ub370\uc774\ud130\ub294 \uac01\uac01\uc758 \uac00\uad6c\ubcc4\ub85c \uc774\ubbf8 \ub098\ub204\uc5b4\uc84c\uc2b5\ub2c8\ub2e4. \uc774\uc81c \uc774\ub97c \ubc14\ub85c \ubaa8\ub378\uc5d0 \uc0ac\uc6a9\ud558\uba74 \ub420 \uac83 \uac19\uc2b5\ub2c8\ub2e4.<br \/>\nTo first show the process of modeling, we'll use the capable Random Forest Classifier in Scikit-Learn.<br \/>\n=> \ubaa8\ub378\ub9c1\uc758 \uccab\ubc88\uc9f8 \ub2e8\uacc4\ub85c RandonForestClassifier\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.<br \/>\nThis probably won't get us to the top of the leaderboard, but it will allow us to establish a baseline.<br \/>\n=> \uc774\ub294 \uc810\uc218\ud310\uc5d0 \ucd5c\uc0c1\uc704\uc5d0 \uc624\ub974\uac8c\ub294 \ud558\uc9c0 \uc54a\uc9c0\ub9cc \uc801\uc808\ud55c \ucd08\uc11d\uc744 \ub2e4\uc9c0\uae30\uc5d4 \uc88b\uc2b5\ub2c8\ub2e4.<br \/>\nLater we'll try several other models including the powerful Gradient Boosting Machine.<br \/>\n=> \ucd94\ud6c4\uc5d0 Gradient Boosting\uc744 \ud65c\uc6a9\ud558\uc5ec \ub354 \uac15\ub825\ud55c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4\n\nTo assess our model, we'll use 10-fold cross validation on the training data.<br \/>\n=> \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc744 \ud3c9\uac00\ud558\uae30 \uc704\ud574\uc11c 10\uacb9 \uad50\ucc28\uac80\uc99d\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4<br \/>\nThis will essentially train and test the model 10 times using different splits of the training data. <br \/>\n=> \uc774\ub294 10\ubc88\uc5d0 \uac70\uccd0\uc11c 10\uacb9\uc5d0 \ub098\ub220\uc9c4 \ub370\uc774\ud130 \ubcc4\ub85c \ud6c8\ub828\uacfc \ud14c\uc2a4\ud2b8\ub97c \uc9c4\ud589\ud569\ub2c8\ub2e4.<br \/>\n10-fold cross validation is an effective method for estimating the performance of a model on the test set. <br \/>\n=> 10\uacb9 \uad50\ucc28\uac80\uc99d\uc740 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud558\ub294\ub370 \ud6a8\uacfc\uc801\uc778 \ubc29\ubc95\uc785\ub2c8\ub2e4.<br \/>\nWe want to look at the average performance in cross validation as well as the standard deviation to see how much scores change between the folds. <br \/>\n=> \uc6b0\ub9ac\ub294 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc744 \uac70\uce5c\uac83\uacfc 10\uacb9 \uad50\ucc28\uac80\uc99d\uc744 \uac70\uce5c \uac83\uc758 \uc810\uc218 \ub610\ud55c \ube44\uad50\ud574 \ubcfc \uac83\uc785\ub2c8\ub2e4.<br \/>\nWe use the F1 Macro measure to evaluate performance.","1e296a0f":"As one more attempt, we'll consider the ExtraTreesClassifier, a variant on the random forest using ensembles of decision trees as well. <br \/>\n=> \ub9c8\uc9c0\ub9c9\uc73c\ub85c\ub294 \uacb0\uc815\ud2b8\ub9ac\uc5d0 \uc559\uc0c1\ube14\uc744 \uc774\uc6a9\ud55c \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc758 \ubcc0\ud615\uc778 ExtraTreeClassifier\ub97c \uc774\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n","fdbdd796":"# Costa Rican Household Poverty Level Prediction<br \/>\nWelcome to another Kaggle challenge! The objective of the Costa Rican Household Poverty Level Prediction contest is to develop a machine learning model that can predict the poverty level of households using both individual and household characteristics. This \"data science for good\" project offers the opportunity to put our skills towards a task more beneficial to society than getting people to click on ads!<br \/>\n=> \uac00\uad6c \uac1c\uac1c\uc778 \uacfc \uac00\uad6c\uc758 \ud2b9\uc9d5\uc744 \uc774\uc6a9\ud558\uc5ec \uac00\uad6c\uc758 \ube48\uce35\ub2e8\uacc4\ub97c \uc608\uce21\ud558\ub294 \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294 \uac83\n\n* In this notebook, we will walk through a complete machine learning solution: <br \/>\n* first, get introduced to the problem, [\ubb38\uc81c\ub97c \ub3c4\ucd9c]<br \/>\n* then perform a thorough Exploratory Data Analysis of the dataset, [EDA\ub97c \ud1b5\ud55c \ub370\uc774\ud130 \ud0d0\uc0c9] <br \/>\n* work on feature engineering, <br \/>\n* try out multiple machine learning models, [\uc5ec\ub7ec\uac1c\uc758 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc5d0 \ub300\uc785]<br \/>\n* select a model, [\ubaa8\ub378\uc744 \uc120\ud0dd] <br \/>\n* work to optimize the model, [\ubaa8\ub378\uc744 \ucd5c\uc801\ud654] <br \/> \n* and finally, inspect the outputs of the model and draw conclusions. [\uacb0\ub860\ub3c4\ucd9c] <br \/>\n\n__While this notebook may not get us to the top of the leaderboard, it is meant to be used as a teaching tool to give you a solid foundation to build on for future machine learning projects. Kaggle projects can teach us a lot about machine learning, but several of the strategies used to get to the very top of the leaderboard are not best practices, so here we'll stick to building a very good - although not quite first place - machine learning solution. While Kaggle projects are competitions, I think they are best described as \"a machine learning education\" disguised as a contest!\"\n\nIf you are looking to follow-up on this work, I have additional work including a kernel on using Automated Feature Engineering with Featuretools for this problem (with slightly higher leaderboard score). (If you enjoy my writing style and explanations, I write for Towards Data Science)\n\nProblem and Data Explanation <br \/>\nThe data for this competition is provided in two files: train.csv and test.csv. <br \/>\nThe training set has 9557 rows and 143 columns [\ud6c8\ub828\uc138\ud2b8\uc758 \uc815\ubcf4] <br \/>\nwhile the testing set has 23856 rows and 142 columns. [\ud14c\uc2a4\ud2b8\uc14b\uc758 \uc815\ubcf4] <br \/>\nEach row represents one individual and each column is a feature, either unique to the individual[\uac1c\uc778\uc758 \uace0\uc720\uac12], or for the household of the individual.[\uac01 \uac00\uad6c\ubcc4 \uace0\uc733\uac12] <br \/>\nThe training set has one additional column,  Target, which represents the poverty level on a 1-4 scale and is the label for the competition. <br \/>\n[\uc6b0\ub9ac\uac00 \uc608\uce21\ud558\uace0\uc790 \ud558\ub294 \uacb0\uacfc \uac12\uc774 \ub4e4\uc5b4\uc788\ub294 \uceec\ub7fc] <br \/>\nA value of 1 is the most extreme poverty. <br \/>\n\nThis is a supervised multi-class classification machine learning problem: <br \/>\n=> \uc9c0\ub3c4\ud559\uc2b5 && \ub2e4\uc911\ubd84\ub958 <br \/>\n\nSupervised: provided with the labels for the training data <br \/>\nMulti-class classification: Labels are discrete values with 4 classes <br \/>\n\nObjective <br \/>\nThe objective is to predict poverty on a household level. <br \/> \n=> \ubaa9\ud45c\ub294 \uac00\uad6c\ub2e8\uc704\ub85c \ube48\uace4\ub2e8\uacc4\ub97c \uad6c\ubd84\ud558\ub294 \uac83 <br \/>\nWe are given data on the individual level with each individual having unique features but also information about their household. <br \/>\n=> \uc815\ubcf4\ub294 \uac00\uad6c\uc6d0 \uac1c\uac1c\uc778\uc5d0 \ub300\ud55c \uc720\ub2c8\ud06c\ud55c \uc815\ubcf4\ub97c \ud3ec\ud568\ud558\uba70 \uac00\uad6c \uc804\uccb4\uc758 \uc720\ub2c8\ud06c\ud55c \uc815\ubcf4\ub3c4 \ud3ec\ud568\ud55c\ub2e4 <br \/>\nIn order to create a dataset for the task, we'll have to perform some aggregations of the individual data for each household. <br \/> \n=> \uadf8\ub798\uc11c \uc704\ub97c \uc704\ud55c \ub370\uc774\ud130\uc14b\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c\ub294 \uac01\uac01\uc758 \uac00\uc815\uc744 \uac1c\uac1c\uc778\uc758 \ub370\uc774\ud130\ub85c \ud569\uccd0\ub458 \ud544\uc694\uac00 \uc788\ub2e4 <br \/>\nMoreover, we have to make a prediction for every individual in the test set,  <br \/>\n=> \ud14c\uc2a4\ud2b8\uc14b\uc5d0 \uc788\ub294 \ubaa8\ub4e0 \uac1c\ubcc4 \uc778\uc6d0\ub4e4\uc5d0\uac8c \uc608\uce21\uc744 \uc2e4\ud589\ud574\uc57c \ud55c\ub2e4 <br \/>\nbut \"ONLY the heads of household are used in scoring\" which means we want to predict poverty on a household basis <br \/>.\n=> \ud558\uc9c0\ub9cc \ubb34\uc870\uac74 \uac00\uc7a5\uc5d0 \ub300\ud574\uc11c\ub9cc \uc810\uc218\ub97c \ubc18\uc601\ud55c\ub2e4 <br \/>\n\nImportant note: <br \/>\nwhile all members of a household should have the same label in the training data, <br \/> \n=> \uac00\uad6c\uc758 \ubaa8\ub4e0 \uad6c\uc131\uc6d0\ub4e4\uc774 \ubb34\uc870\uac74 \uac19\uc740 \ub2e8\uacc4\ub97c \ubc1b\uc544\uc57c \ud55c\ub2e4 <br \/>\nthere are errors where individuals in the same household have different labels. <br \/> \n=> \ud558\uc9c0\ub9cc \uc77c\ubd80 \uac00\uad6c\uc758 \uc77c\ubd80 \uad6c\uc131\uc6d0\ub4e4\uc740 \ub2e4\ub978 \uad6c\uc131\uc6d0\uacfc\ub294 \ub2e4\ub978 \ub2e8\uacc4\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \uacbd\uc6b0\ub3c4 \uc788\ub2e4. <br \/>\nIn these cases, we are told to use the label for the head of each household, <br \/>\n=> \uc774\uacbd\uc6b0\uc5d0\ub294 \uc6b0\ub9ac\ub294 \uac00\uad6c\uc758 \uac00\uc7a5\uc758 label\uc744 \uc774\uc6a9\ud560 \uac83\uc774\ub2e4 <br \/>\nwhich can be identified by the rows where parentesco1 == 1.0. <br \/>\n\nWe will cover how to correct this in the notebook (for more info take a look at the competition main discussion). <br \/>\n=> \uc65c \uc774 \uc811\uadfc\ubc95\uc774 \ub9de\ub294\uc9c0\ub294 Discussion\uc5d0\uc11c \ucc38\uace0\ud558\uae38 \ubc14\ub780\ub2e4 <br \/>\n\nThe Target values represent poverty levels as follows: <br \/>\n=> \ub2e8\uacc4\ub294 \uc544\ub798\uc640 \uac19\ub2e4 <br \/>\n\n1 = extreme poverty [\uadf9\ube48\uce35] <br \/>\n2 = moderate poverty [\uc801\uc815 \uc218\uc900\uc758 \ube48\uace4]<br \/>\n3 = vulnerable households [\ucde8\uc57d\uacc4\uce35] <br \/>\n4 = non vulnerable households [\ucde8\uc57d\uacc4\uce35\uc774 \uc544\ub2d8]<br \/>\n\nThe explanations for all 143 columns can be found in the competition documentation, but a few to note are below: <br \/>\n=> 143\uac1c\uc758 columns\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4 \uadf8 \uc911 \uc77c\ubd80 \uceec\ub7fc\uc5d0 \ub300\ud558 \uc124\uba85 <br \/>\n\nId: a unique identifier for each individual, this should not be a feature that we use! <br \/>\n=> \uac1c\uc778\uc5d0\uac8c \uc8fc\uc5b4\uc9c0\ub294 \uc2dd\ubcc4\uc790 <br \/>\nidhogar: a unique identifier for each household. This variable is not a feature, but will be used to group individuals by household as all individuals in a household will have the same identifier. <br \/>\n=> \uac01\uac01\uc758 \uac00\uad6c\ub97c \uad6c\ubd84\ud558\ub294 \uc2dd\ubcc4\uc790 <br \/>\nparentesco1: indicates if this person is the head of the household. <br \/>\n=> \uac00\uc7a5\uc744 \ub098\ud0c0\ub0b4\ub294 \uceec\ub7fc<br \/>\nTarget: the label, which should be equal for all members in a household<br \/>\n=> \ubaa8\ub4e0 \uac00\uad6c\ub4e4\uc740 \ub77c\ubca8\uc774 \ubaa8\ub450 \ub3d9\uc77c\ud574\uc57c \ud55c\ub2e4.\n\nWhen we make a model, we'll train on a household basis with the label for each household the poverty level of the head of household. <br \/>\n=> \uc608\uce21\uc744 \ud558\ub294\ub370 \uc788\uc5b4\uc11c \uac00\uc7a5\uc758 \uc815\ubcf4\ub9cc \uc0ac\uc6a9\ud560 \uac83\uc784<br \/>\nThe raw data contains a mix of both household and individual characteristics and for the individual data, <br \/>\n=> \uae30\uc874\uc758 \ub370\uc774\ud130\ub294 \uac1c\uac1c\uc778\uc5d0 \ub300\ud55c \uc815\ubcf4\uc640 \uac00\uad6c\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ud568\uaed8\uac00\uc9c0\uace0 \uc788\uc74c<br \/>\nwe will have to find a way to aggregate this for each household. <br \/>\n=> \uac01\uac01\uc758 \uac00\uad6c\ubcc4\ub85c \ud655\uc778 \uac00\ub2a5\ud558\ub3c4\ub85d \ub370\uc774\ud130\ub97c \ud569\uce58\ub294 \ubc29\ubc95\uc744 \ubaa8\uc0c9\ud574\uc57c \ud55c\ub2e4<br \/>\nSome of the individuals belong to a household with no head of household which means that unfortunately we can't use this data for training. <br \/>\n=> \uba87\uba87\uc758 \uac00\uad6c\ub294 \uac00\uc7a5\uc774 \uc5c6\ub294 \uacbd\uc6b0\uac00 \uc788\ub294\ub370, \uc774 \uacbd\uc6b0\ub294 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub3c4\ub85d \ud55c\ub2e4<br \/>\nThese issues with the data are completely typical of real-world data and hence this problem is great preparation for the datasets you'll encounter in a data science job!<br \/>\n=> \uc704\uc758 \ubb38\uc81c\ub4e4\uc740 \uc2e4\uc81c\uc5d0\uc11c \uac00\uc838\uc624\ub294 \ub370\uc774\ud130\uc5d0\uc11c \ub9ce\uc774 \uc77c\uc5b4\ub0a0 \uc218 \uc788\ub294 \ubb38\uc81c\uc784.\n\nMetric [\ud3c9\uac00\uc9c0\ud45c]<br \/>\nUltimately we want to build a machine learning model that can predict the integer poverty level of a household. <br \/>\n\uad81\uadf9\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 \uc6d0\ud558\ub294 \uac83\uc740 \uac01\uac01\uc758 \uac00\uad6c\uc758 \ube48\uace4\ub808\ubca8\uc744 \uc608\uce21\ud558\ub294 \uac83<br \/>\nOur predictions will be assessed by the Macro F1 Score. You may be familiar with the standard F1 score for binary classification problems which is the harmonic mean of precision and recall:<br \/>\n\uc6b0\ub9ac\ub294 Macro F1 Score\ub97c \uc0ac\uc6a9\ud560 \uac83\uc774\ub2e4. \uae30\uc874\uc758 F1 score\ub294 \uc815\ubc00\ub3c4\uc640 \uc7ac\ud604\uc728\uc758 \uc870\ud654\ud3c9\uade0\ud55c \uac12\uc774\ub2e4\n\n** \uc870\ud654\ud3c9\uade0 [\uc5ed\uc218\ub4e4\uc744 \uc0b0\uc220\ud3c9\uade0(Average) \ud55c \uac83\ub4e4\uc758 \uc5ed\uc218]<br \/>\nhttps:\/\/blog.naver.com\/hyuiin\/221367087739 \n\nF1=21recall+1precision=2\u22c5precision\u22c5recallprecision+recall\n \nFor mutli-class problems, we have to average the F1 scores for each class. The macro F1 score averages the F1 score for each class without taking into account label imbalances.\n\nMacro F1=F1 Class 1+F1 Class 2+F1 Class 3+F1 Class 44\n \nIn other words, the number of occurrences of each label does not figure into the calculation when using macro (while it does when using the \"weighted\" score). <br \/>\n=> \uac01 \ub77c\ubca8\uc774 \ub4f1\uc7a5\ud55c \ud69f\uc218\uc758 \uacbd\uc6b0\uc5d0\ub294 \uacc4\uc0b0\uc5d0 \uc801\uc6a9\ub418\uc9c0 \uc54a\uc744 \uac83\uc774\ub2e4 macro\ub97c \uc0ac\uc6a9\ud558\uac8c \ub418\uba74<br \/>\n(For more information on the differences, look at the Scikit-Learn Documention for F1 Score or this Stack Exchange question and answers. If we want to assess our performance, we can use the code:\n\nfrom sklearn.metrics import f1_score<br \/>\nf1_score(y_true, y_predicted, average = 'macro`)","5c962a27":"The most likely candidate seems to be the Random Forest because it does best right out of the box. <br \/>\n=> \ub9ce\uc740 \uc0ac\ub78c\ub4e4\uc740 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uac00 \ucd5c\uace0\uc758 \ubc29\ubc95\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. <br \/>\nWhile we didn't tune any of the hyperparameters so the comparison between models is not perfect, these results reflect those of many other Kaggle competitiors finding that tree-based ensemble methods (including the Gradient Boosting Machine) perform very well on structured datasets. <br \/>\n=> \uadf8\ub7ec\ub098 \uc6b0\ub9ac\ub294 \uc544\ubb34\ub7f0 \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc744 \ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0 \uc774 \ubaa8\ub378\ub4e4\uc740 \uc644\ubcbd\ud558\uc9c0 \uc54a\ub2e4, \uc774 \uacb0\uacfc\ub294 \uc65c \ub9ce\uc740 \uce90\uae00\uc758 \uc774\uc6a9\uc790\ub4e4\uc774 \ud2b8\ub9ac\uae30\ubc18\uc758 \uc544\uc559\ube14 \uba54\uc18c\ub4dc\ub97c \uc4f0\ub294\uc9c0 \ubcf4\uc5ec\uc900\ub2e4 <br \/>\nHyperparameter performance does improve the performance of machine learning models, but we don't have time to try all possible combinations of settings for all models.  <br \/>\n=> \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc740 \uba38\uc2e0\ub7f0\uc774 \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ucf1c\uc8fc\uc9c0\ub9cc, \ubaa8\ub4e0 \ubaa8\ub378\uc758 \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc744 \ud558\uace0 \uc788\uae30\uc5d4 \uc2dc\uac04\uc774 \uc5c6\ub2e4 <br \/>\nThe graph below (from the paper by Randal Olson) shows the effect of hyperparameter tuning versus the default values in Scikit-Learn. ","3ff417eb":"We can see that the score improves as we add features up until 96 features. According to the selector, this is the optimal number of features. <br \/>\n\uc6b0\ub9ac\ub294 96\uac1c\uc758 feature\ub97c \ub123\uc744 \ub54c\uac00\uc9c0\ub9cc \ud574\ub3c4 \uc810\uc218\uc758 \ud5a5\uc0c1\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. selector\uc5d0 \ub530\ub974\uba74 \ucd5c\uc801\uc758 \ud53c\uccd0\uc758 \uc218\ub294 \uc815\ud574\uc84c\uc2b5\ub2c8\ub2e4.\n\nThe rankings of each feature can be found by inspecting the trained object. These represent essentially the importance of features averaged over the iterations. Features can share the same ranking, and only features with a rank of 1 are retained. <br \/>\n\uac01\uac01\uc758 \uceec\ub7fc\ub4e4\uc758 \ub7ad\ud0b9\uc740 \ud6c8\ub828\ub41c \uc120\ud0dd\uc790 \uac1d\uccb4\ub97c \ud1b5\ud574 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub4e4\uc740 \ubc18\ubcf5\uc744 \uac70\ub4ed\ud568\uc5d0 \ub530\ub77c \ud3c9\uade0\ub41c \uc911\uc694\ub3c4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \ud2b9\uc9d5\ub4e4\uc740 \uac19\uc740 \ub7ad\ud0b9\uc744 \uacf5\uc720\ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4. rank\uac00 1\uc778 \uceec\ub7fc\ub4e4\ub9cc \uc0ac\uc6a9\ub420 \uac83\uc785\ub2c8\ub2e4.","44e57045":"The power of the Gradient Boosting Machine can be seen here! The cross validation score blows away anything we've done previously. <br \/>\n=> \uadf8\ub77c\ub514\uc5b8\ud2b8 \ubd80\uc2a4\ud305 \ubaa8\ub378\uc758 \ud798\uc740 \uc5ec\uae30\uc11c \ubc1c\ud718\ub41c\ub2e4. <br \/>\n\nLet's take a look at the predictions to understand what is going on with the predictions in each fold. <br \/>\n=> \uac01\uac01\uc758 \uacc4\uce35\ubcc4 \uc9c4\ud589\ub41c \uc608\uce21\uc774 \uc5b4\ub5bb\uac8c \ub418\uc5c8\ub294\uc9c0 \uc774\ud574\ub97c \uc704\ud574 prediction \ubcc0\uc218\ub97c \uc0b4\ud3b4\ubcf4\uc790<br \/>","f2149bfd":"It looks like the most common number of tablets to own is 1 if we go only by the data that is present. <br \/>\n=> \uc874\uc7ac\ud558\ub294 \ub370\uc774\ud130\ub9cc \uace0\ub824\ud588\uc744 \ub54c 1\ub300\uc758 \ud14c\ube14\ub9bf\uc744 \ubcf4\uc720\ud55c \uc22b\uc790\uac00 \uac00\uc7a5 \ub9ce\uc544 \ubcf4\uc785\ub2c8\ub2e4.<br \/>\nHowever, we also need to think about the data that is missing. <br \/>\n=> \uadf8\ub7ec\ub098 \ub370\uc774\ud130\uac00 \uc190\uc2e4 \ub418\uc5c8\uc74c\uc744 \uc5fc\ub450\ud560 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nIn this case, it could be that families with a nan in this category just do not own a tablet! <br \/>\n=> \uadf8\ub798\uc11c, \ud574\ub2f9 \uc218\uce58\uac00 \uc190\uc2e4\ub41c \uac00\uc871\uc758 \uacbd\uc6b0\uc5d0\ub294 \ud14c\ube14\ub9bf\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\uace0 \ucd94\uce21\ub429\ub2c8\ub2e4.<br \/>\nIf we look at the data definitions, we see that v18q indicates whether or not a family owns a tablet. <br \/>\n=>v18q\uceec\ub7fc\uc744 \ubcf4\uac8c\ub418\uba74 \ud574\ub2f9 \uceec\ub7fc\uc740 \ud574\ub2f9\uac00\uc871\uc774 \ud14c\ube14\ub9bf\uc744 \uac00\uc9c0\uace0 \uc788\ub294\uc9c0 \uc5c6\ub294\uc9c0 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4<br \/>\nWe should investigate this column combined with the number of tablets to see if our hypothesis holds.<br \/>\n=> \uadf8\ub798\uc11c \uc6b0\ub9ac\uc758 \uac00\uc815\uc774 \ub9de\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574\uc11c \ud574\ub2f9 \uceec\ub7fc\uc744 \uac19\uc774 \uc54c\uc544\ubcfc \ud544\uc694\uac00 \uc788\uc744 \ub4ef\ud569\ub2c8\ub2e4.\n\nWe can groupby the value of v18q (which is 1 for owns a tablet and 0 for does not) and then calculate the number of null values for v18q1. <br \/>\n=> \uc6b0\ub9ac\ub294 \uadf8 \uceec\ub7fc\uc744 \uadf8\ub8f9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nThis will tell us if the null values represent that the family does not own a tablet.<br \/>\n=> \uc774 \uac12\uc774 null\uc778 \uacbd\uc6b0\ub294 \uac00\uc871\uc774 \ud14c\ube14\ub9bf\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","2ca7bd51":"It looks like at every value of the Target, households with female heads have higher levels of education. Yet, we saw that overall, households with female heads are more likely to have severe poverty.","679861d4":"Features Plot\nFor the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle.","b4a06a41":"This plot shows us that there are a number of variables that have a weak correlation with the Target. There are also high correlations between some variables (such as floor and walls+roof+floor) which could pose an issue because of collinearity.","696b54b2":"For the negative correlations, as we increase the variable, the Target decreases indicating the poverty severity increases. <br \/>\n=> \ubd80\uc758 \uc0c1\uad00\uad00\uacc4\ub294 \uc784\uc758\uc758 \ud574\ub2f9 \uc0c1\uad00\uad00\uac8c\uc758 \uac12\uc774 \uc99d\uac00\ud558\uba74 \ud0c0\uac9f\ub77c\ubca8\uc740 \uac10\uc18c\ud560 \uac83\uc785\ub2c8\ub2e4(\uc774\ub294 \ub354 \ube48\uace4\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud568)<br \/>\nTherefore, as the warning increases, the poverty level also increases which makes sense because this was meant to show potential bad signs about a house. <br \/>\n=> \uadf8\ub7ec\ubbc0\ub85c warning\uc774 \uc99d\uac00\ud558\uba74 \ubd80\uc758 \ud0c0\uac9f \ub77c\ubca8\uc740 \ub354 \ube48\uace4\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud560 \uac83\uc785\ub2c8\ub2e4..<br \/>\nThe hogar_nin is the number of children 0 - 19 in the family which also makes sense: younger children can be financial source of stress on a family leading to higher levels of poverty. <br \/>\n=> hogar_nin\uc740 0-19\uc138\uc758 \uc544\uc774\ub4e4\uc758 \uc218\uc778\ub370 \uc774 \ub610\ud55c \ub9d0\uc774 \ub9de\ub294\ub4ef\ud569\ub2c8\ub2e4: \uc5b4\ub9b0 \uc544\uc774\ub4e4\uc740 \uacbd\uc81c\uc801\uc778 \uc9c0\uc6d0\uc774 \ub9ce\uc774 \ud544\uc694\ud558\uae30 \ub54c\ubb38\uc5d0 \uac00\uc871\uc758 \ubd80\uc758 \uc218\uc900\uc744 \ub5a8\uc5b4\ub728\ub9b4 \uac83\uc785\ub2c8\ub2e4.<br \/>\nOr, families with lower socioeconomic status have more children in the hopes that one of them will be able to succeed.<br \/>\n=> \ub610\ub294 <br \/>\nWhatever the explanation, there is a real link between family size and poverty<br \/>\n=> \n\nOn the other hand, for the positive correlations, a higher value means a higher value of Target indicating the poverty severity decreases. <br \/>\n=> \ubc18\uba74 \uc591\uc758 \uad00\uacc4\uc5d0\uc11c\ub294, \uc704\uc758 \ubd80\uc758 \uad00\uacc4\uc640 \ubc18\ub300\ub85c \uc0dd\uac01\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.<br \/>\nThe most highly correlated household level variable is meaneduc, the average education level of the adults in the household. <br \/>\n=> \uac00\uc7a5 \ub192\uc740 \uc218\uce58\ub294 meaneduc\uc774\uba70, \uc774\ub294 \uac00\uc815\uc758 \uc5b4\ub978\ub4e4\uc758 \ud3c9\uade0 \uad50\uc721 \ub2e8\uac8c\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.<br \/>\nThis relationship between education and poverty intuitively makes sense: greater levels of education generally correlate with lower levels of poverty.<br \/>\n=> \uad50\uc721 \uc218\uc900\uacfc \ubd80\uc758 \ub2e8\uacc4\ub294 \ub610\ud55c \ub9d0\uc774 \ub9de\ub294 \uac83 \uac19\uc2b5\ub2c8\ub2e4.<br \/>\nWe don't necessarily know which causes which, but we do know these tend to move in the same direction.<br \/>\n=> \uc5b4\ub290\uac83\uc774 \ud0c0\uac9f\ub77c\ubca8\uc744 \uc6c0\uc9c1\uc774\ub294\uc9c0 \uba74\ubc00\ud788 \ud30c\uc545\ud560 \ud544\uc694\ub294 \uc5c6\uc9c0\ub9cc, \uc591\uc758 \ubc29\ud5a5\uc73c\ub85c \uac19\uc774 \uad00\uacc4\ub97c \uac00\uc9c4\ub2e4\ub294 \uc815\ub3c4\ub85c \uc54c\uc544\ub458 \ud544\uc694\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\nThe general guidelines for correlation values are below, but these will change depending on who you ask (source for these):<br \/>\n=> \uc544\ub798\ub294 \uc0c1\uad00\uad00\uacc4\ub97c \ud574\uc11d\ud558\ub294 \uc218\uce58\ubcc4 \uc815\ub3c4\uc785\ub2c8\ub2e4.\n\n.00-.19 \u201cvery weak\u201d<br \/>\n.20-.39 \u201cweak\u201d<br \/>\n.40-.59 \u201cmoderate\u201d<br \/>\n.60-.79 \u201cstrong\u201d<br \/>\n.80-1.0 \u201cvery strong\u201d<br \/>\nWhat these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n\nNow we can move on to the Spearman correlation.","a273679b":"One other feature that might be useful is the gender of the head of household. <br \/>\n=> \ub2e4\ub978 \uc911\uc694\ud560 \uac83\uc73c\ub85c \uc608\uc0c1\ub418\ub294 \ud2b9\uc9d5\uc740 \uac00\uc7a5\uc758 \uc131\ubcc4\uc785\ub2c8\ub2e4.<br \/>\nSince we aggregated the data, we'll have to go back to the individual level data and find the gender for the head of household.<br \/>\n=> \uc6b0\ub9ac\uac00 \ub370\uc774\ud130\ub97c \ud569\uce5c \uc774\ud6c4\ub85c, \uc774\uc81c \uac1c\ubcc4 \ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c \ub3cc\uc544\ubcf4\uace0 \uac00\uc7a5\uc758 \uc131\ubcc4\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubd05\uc2dc\ub2e4.","f6a46f61":"## Making a Submission\nIn order to make a submission, we need the test data. Fortunately, we have the test data formatted in exactly the same manner as the train data. <br \/>\n=> \uc81c\ucd9c\uc744 \uc704\ud574, \uc6b0\ub9ac\ub294 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uac00 \ud544\uc694\ud558\ub2e4. \uc6b4\uc88b\uac8c\ub3c4 \uc6b0\ub9ac\ub294 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \ud6c8\ub828\ub370\uc774\ud130\ub97c \uc804\ucc98\ub9ac\ud55c \uac83\uacfc \ub3d9\uc77c\ud558\uac8c \ucc98\ub9ac\ud574 \ub450\uc5c8\ub2e4.\n\nThe format of a testing submission is shown below. Although we are making predictions for each household, we actually need one row per individual (identified by the Id) but only the prediction for the head of household is scored. <br \/>\n=>  \ud14c\uc2a4\ud2b8 \uc14b\uc758 \uacb0\uacfc \uc81c\ucd9c\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \ube44\ub85d \uc6b0\ub9ac\uac00 \ubaa8\ub4e0 \uac00\uad6c\ub4e4\uc5d0 \ub300\ud574 \uacb0\uacfc\ub97c \uc608\uce21\ud588\ub354\ub77c\ub3c4 \uc6b0\ub9ac\ub294 \uac1c\uc778 row\ub2f9 \ud558\ub098\uc758 \ud0c0\uac9f \uac12\uc744 \uc6d0\ud55c\ub2e4. \uadf8\ub7ec\ub098 \uac00\uc7a5\uc758 \uac83\ub9cc \uc810\uc218\uc5d0 \ud569\uc0b0 \ub420 \uac83\uc774\ub2e4.\n\nId,Target <br \/>\nID_2f6873615,1 <br \/>\nID_1c78846d2,2 <br \/>\nID_e5442cf6a,3 <br \/>\nID_a8db26a79,4 <br \/>\nID_a62966799,4 <br \/>\n\nThe submission_base will have all the individuals in the test set since we have to have a \"prediction\" for each individual while the test_ids will only contain the idhogar from the heads of households. <br \/>\n=> submissin_base\ub294 \ubaa8\ub4e0 \uac1c\uac1c\uc778\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ub2e4 \uac00\uc9c0\uace0 \uc788\ub2e4. \uadf8\ub7f0\ub370 \uc6b0\ub9ac\uac00 \uae30\uc794 test_id\ub294 \uac00\uad6c\ubcc4\ub85c \uc608\uce21\uc774 \ub418\uc5b4\uc788\ub2e4 <br \/>\nWhen predicting, we only predict for each household and then we merge the predictions dataframe with all of the individuals on the household id (idhogar). <br \/>\n=> \uc608\uce21\uc2dc\uc5d0, \uc6b0\ub9ac\ub294 \uac00\uad6c \ub2e8\uc704\ub85c \uc608\uce21\uc744 \ud588\uace0, \uc6b0\ub9ac\ub294 \ub450 \uac1c\uc758 \ub370\uc774\ud130\ud504\ub808\uc784\uc744 left outer join\uc744 \ud1b5\ud574 \ubaa8\ub4e0 \uac00\uad6c\uc6d0\ub4e4\uc5d0\uac8c \uac00\uad6c\ubcc4 \ud0c0\uac9f \uac12\uc744 \ud560\ub2f9\ud560 \uac83\uc774\ub2e4. <br \/>\nThis will set the Target to the same value for everyone in a household. For the test households without a head of household, we can just set these predictions to 4 since they will not be scored. <br \/>\n=> \uc774\uac83\uc740 \ubaa8\ub4e0 \uad6c\uc131\uc6d0\ub4e4\uc5d0\uac8c \uac12\uc774 \ub3cc\uc544\uac00\uc9c0\ub9cc, \uac00\uc7a5\uc774 \uc5c6\ub294 \uac00\uad6c\uc758 \uacbd\uc6b0\uc5d0\ub294 \uc608\uce21\uc774 \ub418\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 4\ub85c \ubc30\uc815\ud55c\ub2e4.","9e1152b9":"It looks like households where the head is female are slightly more likely to have a severe level of poverty.","edd67511":"If we read through some of the discussions for this competition, we learn that this variable is only defined for individuals between 7 and 19. <br \/>\n=> \ub9cc\uc57d \uc774 \ucef4\ud53c\ud2f0\uc158\uc5d0 \ub300\ud55c \ub514\uc2a4\ucee4\uc158\uc744 \uc77d\uc5b4\ubd24\ub2e4\uba74, \uc6b0\ub9ac\ub294 \uc774 \uac12\uc774 7\uc0b4\uacfc 19\uc0b4 \uc0ac\uc774\uc5d0 \uc0ac\ub78c\ub4e4\uc5d0\uac8c\ub9cc \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nAnyone younger or older than this range presumably has no years behind and therefore the value should be set to 0. <br \/>\n=> \uadf8\ub798\uc11c \uc774 \uc5f0\ub839\uc5d0 \ud574\ub2f9\ud558\uc9c0 \ubabb\ud558\ub294 \uc0ac\ub78c\ub4e4\uc5d0\uac8c\ub294 0\uc758 \uac12\uc744 \ubd80\uc5ec\ud574\uc57c \ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4.<br \/>\nFor this variable, if the individual is over 19 and they have a missing value, or if they are younger than 7 and have a missing value we can set it to zero. <br \/>\nFor anyone else, we'll leave the value to be imputed and add a boolean flag.<br \/>\n=> \uadf8\ub798\uc11c \uadf8 \uc5f0\ub839\uc5d0 \ud574\ub2f9\ud558\uc9c0 \uc54a\ub294 \uc0ac\ub78c\ub4e4\uc758 \uacbd\uc6b0\uc5d0\ub294 \ub17c\ub9ac\ud615 \uceec\ub7fc\uc73c\ub85c \ub0a8\uaca8\ub458 \uac83\uc785\ub2c8\ub2e4.","96c59bd1":"Let's make sure we covered all of the variables and didn't repeat any.<br \/>\n=> \ud639\uc2dc \ubc18\ubcf5\ub418\ub294 \uac83\uc740 \uc5c6\ub294\uc9c0 \ud655\uc778\ud574\ubd05\uc2dc\ub2e4.","b2ff9c81":"Final Data Exploration\nWe'll do a little bit of exploration.","e13aa05e":"This looks like it could be an indicator of more poverty given the higher prevalence of 2: moderate poverty.\n\nThis represents an important point: sometimes the missing information is just as important as the information you are given.<br \/>\n=> \uac00\ub054\uc740 \uc190\uc2e4 \uac12\ub4e4\uc774 \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uac00\uc9c0\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\uae30\ub3c4 \ud569\ub2c8\ub2e4.","870a30bf":"The Spearman correlation coefficient calculation also comes with a pvalue indicating the significance level of the relationship. <br \/>\n=> Spearman \uc0c1\uad00\uacc4\uc218\ub294 \uad00\uacc4\uc758 \uc720\uc758\uc218\uc900\uc744 \uacb0\uc815\ud558\ub294 pvalue\uc640 \ud568\uaed8 \uacc4\uc0b0\ud569\ub2c8\ub2e4.<br \/>\nAny pvalue less than 0.05 is genearally regarded as significant, although since we are doing multiple comparisons, we want to divide the p-value by the number of comparisons, a process known as the Bonferroni correction.<br \/>\n=> \ub2e4\uc911 \ube44\uad50\ub97c \ud558\ub354\ub77c\ub3c4 \uc784\uc758\uc758 pvalue\uac00 0.05\ubcf4\ub2e4 \uc791\uc740 \uacbd\uc6b0 \uc6b0\ub9ac\ub294 \uc720\uc758\ud558\ub2e4\uace0 \ud310\ub2e8\ud569\ub2c8\ub2e4. \uadf8\ub798\uc11c \ube44\uad50 \ud69f\uc218\ubcc4\ub85c p-value\ub97c \ub098\ub215\ub2c8\ub2e4. \uc774 \uacfc\uc815\uc744 \ubca0\ub8e8\ub204\uc774 \uc0c1\uad00\uad00\uacc4\ub77c\uace0 \ud569\ub2c8\ub2e4.","cc2b52b5":"Feature Construction<br \/>\nWe can make a few features using the existing data. <br \/>\n=> \uc874\uc7ac\ud558\ub294 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574\uc11c \uba87\uac1c\uc758 \uceec\ub7fc\ub4e4\uc744 \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4.<br \/>\nFor example, we can divide the years of schooling by the age.","31ab2d65":"## Feature Construction<br \/>\nIn addition to mapping variables to ordinal features, we can also create entirely new features from the existing data, known as feature construction. <br \/>\n=> \uae30\uc874\uc5d0 \uc874\uc7ac\ud558\ub294 \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 \uc0c8\ub85c\uc6b4 \uceec\ub7fc\uc744 \ub9cc\ub4e4\uc5b4 \ub0bc \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nFor example, we can add up the previous three features we just created to get an overall measure of the quality of the house's structure.<br \/>\n=> \uc608\ub97c\ub4e4\uba74, \uc6b0\ub9ac\uac00 \ub9cc\ub4e4\uc5c8\ub358 \uc138\uac00\uc9c0\uc758 \uceec\ub7fc\uc744 \uacb0\ud569\ud55c \uceec\ub7fc\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4","ded2d451":"For each fold, the 1, 2, 3, 4 columns represent the probability for each Target. The Target is the maximum of these with the confidence the probability. We have the predictions for all 5 folds, so we can plot the confidence in each Target for the different folds. <br \/>\n\uac01\uac01\uc758 \uacc4\uce35\uc5d0 \ub300\ud574 1,2,3,4\ub294 \uac01 \ud0c0\uac9f\uc758 \ud655\ub960\uc744 \ub098\ud0c0\ub0b8\ub2e4. \uc774\ub4e4\uc758 \ucd5c\ub300 \uac12\uc740 \ud655\ub960\uc758 confidence\uac12\uc73c\ub85c \ub418\uc5b4\uc788\ub2e4. \uc6b0\ub9ac\ub294 5\uac1c\uc758 \uacc4\uce35\uc5d0 \ub300\ud574\uc11c \uc9c4\ud589\ud588\uace0 \uac01 \uacc4\uce35\ubcc4\ub85c \uc790\uc138\ud788 \ud655\uc778 \uac00\ub2a5\ud558\ub3c4\ub85d \uadf8\ub9bc\uc744 \uadf8\ub824\uc904 \uc218 \uc788\ub2e4.<br \/>","9aa455ac":"## Squared Variables [\uc81c\uacf1\ub41c \uac12\ub4e4] <br \/>\nFirst, the easiest step: we'll remove all of the squared variables.<br \/>\n=> \uac00\uc7a5 \uc26c\uc6b4 \ubc29\ubc95\uc73c\ub85c, \uc6b0\ub9ac\ub294 \ubaa8\ub4e0 \uc774\uc640 \uac19\uc740 \uac12\ub4e4\uc744 \uc9c0\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\nSometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. <br \/>\n=> \uadf8\ub7ec\ub098 \uac00\ub054\uc529 \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \uc131\uaca9\uc744 \uc9c0\ub2cc \uac12\ub4e4\uc774 \ube44\uc120\ud615\uad00\uacc4\ub97c \uc120\ud615\ubaa8\ub378\uc774 \ud559\uc2b5\uc744 \ud558\ub3c4\ub85d \ub3c4\uc640\uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.<br \/>\nHowever, since we will be using more complex models, these squared features are redundant. <br \/>\n=> \uadf8\ub7ec\ub098 \ub354\uc6b1 \ub354 \ubcf5\uc7a1\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0, \uc704\uc640 \uac19\uc740 \ud2b9\uc9d5\ub4e4\uc740 \uc911\ubcf5\uc774 \ub429\ub2c8\ub2e4.<br \/>\nThey are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.<br \/>\n=> \uc774 \uac12\ub4e4\uc740 \uc81c\uacf1\uc774 \ucde8\ud574\uc9c0\uae30 \uc804\uc758 \uceec\ub7fc\uacfc \uad49\uc7a5\ud788 \ub192\uc740 \uc0c1\uad00\uad00\uacc4\ub97c \ubcf4\uc774\uba70, \uc774\ub294 \ub354 \ub098\uc544\uac00 \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc5d0 \uc774\ub86d\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.<br \/>\n\nFor an example, let's take a look at SQBage vs age<br \/>\n=> \uc608\uc2dc\ub85c \uc54c\uc544\ubd05\uc2dc\ub2e4","3e38fec0":"What this tells us is that the oldest age with a missing value is 17.\n\nFor anyone older than this, maybe we can assume that they are simply not in school. Let's look at the ages of those who have a missing value.<br \/>\n=> 17\uc0b4\ubcf4\ub2e4 \ub098\uc774\uac00 \ub9ce\uc740 \uc0ac\ub78c\ub4e4\uc5d0 \ub300\ud574, \uc6b0\ub9ac\ub294 \ud559\uad50\uc5d0 \ub2e4\ub2c8\uace0 \uc788\uc9c0 \uc54a\ub2e4\uace0 \uac00\uc815\ud560 \uc218 \uc788\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc190\uc2e4\uac12\uc744 \uac00\uc9c0\uace0 \uc788\ub294 \uc0ac\ub78c\ub4e4\uc5d0 \ub300\ud574\uc11c \uc54c\uc544\ubcfc\uae4c\uc694?","17de9fc0":"At this point, to improve our performance, we can continue with the optimization, try more feature engineering, stack or ensemble additional models, or look at more experimental methods such as dimension reduction or oversampling. However, the scores we are getting are fairly good and I'm going to move on to a crucial part of machine learning: investigating the predictions to see where our model is wrong. <br \/>\n\uc6b0\ub9ac\uc758 \uc131\ub2a5\uc744 \ub192\uc774\uae30 \uc704\ud574 \ucd5c\uc801\ud654\ub97c \uacc4\uc18d\ud560 \uc218 \uc788\uace0, \ub2e4\ub978 feature engineering\uc744 \ud560\uc218\ub3c4 \uc788\uace0 \ucd94\uac00\uc801\uc778 \uc2a4\ud0dc\ud0b9 \ub610\ub294 \uc559\uc0c1\ube14 \ubaa8\ub378\uc774\ub098 \ub610\ub294 \ucc28\uc6d0\ucd95\uc18c \ub610\ub294 \uc624\ubc84\uc0d8\ud50c\ub9c1\uacfc \uac19\uc740 \uc2e4\ud5d8\uc801\uc778 \ubc29\ubc95\uc744 \ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc810\uc218\ub294 \uc0dd\uac01\ubcf4\ub2e4\ub3c4 \uc88b\uc73c\uba70, \uadf8\ub798\uc11c \uba38\uc2e0\ub7ec\ub2dd\uc5d0 \uc788\uc5b4\uc11c \uc880\ub354 \uc911\uc694\ud55c \ubd80\ubd84\uc778, \uc608\uce21\uce58\uc5d0\uc11c \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc774 \uc798\ubabb\ub41c \ubd80\ubd84\uc744 \uc870\uc0ac\ud574\ubcf4\ub824\uace0 \ud55c\ub2e4.<br \/>\n\nWhile Kaggle is great for presenting realistic datasets, the methods needed to get to the very top of the leaderboard are not generally used in the real-world. Past some level of performance, accuracy takes a back seat to interpretability. People want to know why a model makes the predictions it does, much as they would want a human to be able to explain her decisions. We could work on squeezing some more performance from our model, but right now, our energy is better spent investigating our model. <br \/>\n\uce90\uae00\uc740 \uc2e4\uc81c \ub370\uc774\ud130\uc14b\uc744 \uc81c\uacf5\ud558\ub294 \uc88b\uc740 \uacf3\uc774\uc9c0\ub9cc, \uc2e4\uc81c\ub85c \ub192\uc740 \uc810\uc218\ub97c \ucc28\uc9c0\ud558\ub294 \uc0ac\ub78c\ub4e4\uc774 \ud558\ub294 \ubc29\ubc95\ub4e4\uc740 \uc2e4\uc81c\ub85c \uc0ac\uc6a9\ub418\uc9c0 \uc54a\ub294\ub2e4. \uc77c\uc815 \uc218\uc900\uc758 \uc131\ub2a5\uc5d0\uc11c, \uc815\ud655\ub3c4\ub294 \uc758\ubbf8\ub97c\ud574\uc11d\ud558\ub294 \uac83\ubcf4\ub2e4 \ub4b7\uc804\uc5d0 \uc788\ub2e4. \uc0ac\ub78c\ub4e4\uc740 \uc65c \ubaa8\ub378\uc774 \uc608\uce21\uc744 \uadf8\ub807\uac8c \ud588\ub294\uc9c0 \uc54c\uace0 \uc2f6\uc5b4\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ubaa8\ub378\uc758 \ub354\uc88b\uc740 \uc131\ub2a5\uc744 \uc704\ud574 \uc9dc\ub0bc \uc218 \uc788\uc9c0\ub9cc, \uadf8\uac83 \ubcf4\ub2e4\ub294 \uc6b0\ub9ac\uc758 \uc5d0\ub108\uc9c0\ub97c \uc6b0\ub9ac\uc758 \ubaa8\ub378\uc744 \uc870\uc0ac\ud558\ub294\ub370 \ucd08\uc810\uc744 \ub9de\ucdb0\ubcf4\ub3c4\ub85d \ud558\uc790.","1ac67743":"First, we'll calculate the Pearson correlation of every variable with the Target.","5ef6fa1d":"These variables are now correctly represented as numbers and can be fed into a machine learning model.<br \/>\n=> \uc774\uc81c \uc774 \ubcc0\uc218\ub4e4\uc740 \uc644\uc804\ud788 \uc22b\uc800\ud654 \ub418\uc5c7\uc73c\uba70 \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378\uc5d0 \ub123\uc744 \uc900\ube44\uac00 \ub418\uc5c8\ub2e4\n\nTo make operations like that above a little easier, we'll join together the training and testing dataframes. <br \/>\n=> \uc704\uc758 \uc791\uc5c5\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ud6c8\ub828\ub370\uc774\ud130\uc640 \ud14c\uc2a4\ud2b8\uc14b \ub370\uc774\ud130\uc5d0 \ubaa8\ub450 \uc801\uc6a9\ud560 \uc218 \uc788\uc5c8\ub2e4<br \/>\nThis is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. <br \/>\n=> \uc774\ub294 feature engineering\ud558\ub294\ub370 \uc788\uc5b4\uc11c \ud575\uc2ec\uc801\uc774\ub2e4<br \/>\nLater we can separate out the sets based on the Target.<br \/>\n=> \uc774\ud6c4\uc5d0 \ud0c0\uac9f\uac12\uc744 \uae30\ucd08\ub85c\ud558\uc5ec \ud6c8\ub828\uc14b\uacfc \ud14c\uc2a4\ud2b8\uc14b\uc744 \ub2e4\uc2dc \ub098\ub20c \uc218 \uc788\uc744 \uac83\uc774\ub2e4.","d16fca17":"Integer Columns<br \/>\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot.<br \/>\n=> \uc704\uc5d0\uc11c \ub9d0\ud588\ub4ef\uc774 \uc815\uc218\uceec\ub7fc\uc758 \uc720\ud615\uc744 \uad6c\ubd84\ud558\uae30 \uc704\ud574\uc11c \uac01\uac01\uc758 \uceec\ub7fc\uc774 \uc5b4\ub5a4 \uc720\ub2c8\ud06c\ud55c \uac1c\ubcc4\uac12\uc744 \uac16\ub294\uc9c0\ub97c \ub098\ud0c0\ub0b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","69c81ce0":"Using Optimized Model\nOnce the optimization procedure has finished, we can use the best results for modeling.","ebb934ed":"As a final step with the missing values, we can plot the distribution of target for the case where either of these values are missing.<br \/>\n=> \uc190\uc2e4\uac12\uc758 \ub9c8\uc9c0\ub9c9 \ub2e8\uac8c\ub85c\uc11c, \uc6b0\ub9ac\ub294 \uc190\uc2e4\uac12\ub4e4\uc758 \ud0c0\uac9f \ub77c\ubca8\uc758 \ubd84\ud3ec\ub97c \uadf8\ub824\ubcfc \uac83\uc785\ub2c8\ub2e4.","bca8f1ad":"We can also look at the difference in average education by whether or not the family has a female head of household."}}