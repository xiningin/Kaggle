{"cell_type":{"00f2190d":"code","10762913":"code","907c9e1e":"code","2bfbc5eb":"code","fa6b3666":"code","4539fd8c":"code","c1de7cde":"code","1e547dbe":"code","b7e04398":"code","b2ced47c":"code","2da389da":"code","ae425672":"code","36def3a3":"code","1163182d":"code","7ad4ea49":"code","f3c34695":"markdown","7eb5788c":"markdown","c5761304":"markdown","0c5c5fe2":"markdown","52fb5ba3":"markdown","06c566a4":"markdown","c729a55e":"markdown","593e5acb":"markdown","a7c54fbd":"markdown","e4cb4394":"markdown","370ab02c":"markdown","10f04baa":"markdown","e2352099":"markdown","ceeb5298":"markdown","9aaf9fe7":"markdown","3faaa5e9":"markdown"},"source":{"00f2190d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10762913":"iris = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\niris = iris.drop('Id', axis = 1)","907c9e1e":"#Exploring the dataset for dtype and null values\niris.info()","2bfbc5eb":"import seaborn as sns\n\n#Visulaising data using paiplot with regression as the 'kind'.\nsns.pairplot(iris, hue = 'Species', kind = 'reg', height = 4)","fa6b3666":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n\ntrain, test = train_test_split(iris, test_size = 0.3, random_state = 5)\ntrain_X = train[X]\ntrain_y = train.Species\ntest_X = test[X]\ntest_y = test.Species","4539fd8c":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state = 5)\n\nclf.fit(train_X, train_y)\nprediction_clf = clf.predict(test_X)\n\nscore_clf = round(accuracy_score(prediction_clf, test_y), 5)\n\nprint('The prediction accuray of Random Forest Classifier with train, test split is ' + str(score_clf))","c1de7cde":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state = 5)\n\nlr.fit(train_X, train_y)\nprediction_lr = lr.predict(test_X)\n\nscore_lr = round(accuracy_score(prediction_lr, test_y), 5)\n\nprint('The prediction accuray of Logistic Regression with train, test split is ' + str(score_lr))","1e547dbe":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\nknn.fit(train_X, train_y)\nprediction_knn = knn.predict(test_X)\n\nscore_knn = round(accuracy_score(prediction_knn, test_y), 5)\n\nprint('The prediction accuray of K-Nearest Neighbours Classifier with train, test split is ' + str(score_knn))","b7e04398":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC(random_state = 5)\n\nsvc.fit(train_X, train_y)\nprediction_svc = svc.predict(test_X)\n\nscore_svc = round(accuracy_score(prediction_svc, test_y), 5)\n\nprint('The prediction accuray of Linear Support Vector Classifier with train, test split is ' + str(score_svc))","b2ced47c":"import matplotlib.pyplot as plt\n\n#Finding correlation in the dataset\ncorr = iris.corr()\n\n#Building a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype = np.bool))\n\n#Building a plot\nf, ax = plt.subplots(figsize = (11, 9))\n\n#Generationg custom divergig colour map\ncmap = sns.diverging_palette(10, 220, as_cmap = True)\n\nsns.heatmap(corr, mask = mask, cmap = cmap, vmax = 0.3, center = 0, square = True, linewidths = 0.5, annot = True, cbar_kws = {'shrink' : 0.75})","2da389da":"#Taking new parameters into account\nZ = ['PetalLengthCm', 'SepalWidthCm']\n\ntrain_Z = train[Z]\ntest_Z = test[Z]","ae425672":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state = 5)\n\nclf.fit(train_Z, train_y)\nprediction_clfz = clf.predict(test_Z)\n\nscore_clfz = round(accuracy_score(prediction_clfz, test_y), 5)\n\nprint('The prediction accuray of Random Forest Classifier with train, test split is ' + str(score_clfz))","36def3a3":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state = 5)\n\nlr.fit(train_Z, train_y)\nprediction_lrz = lr.predict(test_Z)\n\nscore_lrz = round(accuracy_score(prediction_lrz, test_y), 5)\n\nprint('The prediction accuray of Logistic Regression with train, test split is ' + str(score_lrz))","1163182d":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n\nknn.fit(train_Z, train_y)\nprediction_knnz = knn.predict(test_Z)\n\nscore_knnz = round(accuracy_score(prediction_knnz, test_y), 5)\n\nprint('The prediction accuray of K-Nearest Neighbours Classifier with train, test split is ' + str(score_knnz))","7ad4ea49":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC(random_state = 5)\n\nsvc.fit(train_Z, train_y)\nprediction_svcz = svc.predict(test_Z)\n\nscore_svcz = round(accuracy_score(prediction_svcz, test_y), 5)\n\nprint('The prediction accuray of Linear Support Vector Classifier with train, test split is ' + str(score_svcz))","f3c34695":"# Splitting Data into train and test subsets.","7eb5788c":"**Logistic Regression**","c5761304":"**Random Forest Classifier**","0c5c5fe2":"**K-Nearest Neighbours Classifier**","52fb5ba3":"**Linear Support Vector Classifier**","06c566a4":"Conclusion: The maximum accuracy score obtained is 0.97778, while Logistic Regression performed the same when modelled on all parameters and selective parameters.\nLinear SVC, KNN Classifier and Random Forest Clasifier performed better with selective parameters.","c729a55e":"**Logistic Regression**","593e5acb":"# Exploration and Visualisation of data.","a7c54fbd":"# Building Models based on all parameters","e4cb4394":"# Exploring possibilities of correlation of parameters","370ab02c":"# Building models based on correlation and distribution","10f04baa":"Considering the distribution of data shown in the pairplot and the correlations in the heatmap. We choose to consider two parameters i.e. Petal Length and Sepal Width. \n\n*Petal Length because it has a the least overlaps between different species and Sepal Width as it is not correlated to any other parameter.*","e2352099":"The heatmap shows that Sepal Length and Sepal Width are ***not correlated***.\nWhile Petal Length and Petal Width are ***highly correlated***.\nPetal Length and Sepal Legth are ***correlated*** as well.\nWhile Sepal Width and Petal width are ***not correlated***.\nSpeal Length and Petal Width are ***correlated*** as well.","ceeb5298":"**K-Nearest Neighbours Classifier**","9aaf9fe7":"**Linear Support Vector Classifier**","3faaa5e9":"**Random Forest Classifier**"}}