{"cell_type":{"3c08c406":"code","2fa7a694":"code","afb3294c":"code","42811709":"code","fdf1d114":"code","d9811808":"code","9798162c":"code","5e4260fd":"code","e9c98f44":"code","784374dc":"code","ec8332e3":"code","3b55e54f":"code","336d5fdd":"code","66274d64":"code","b65e6a59":"code","efdc1923":"code","cf77cc43":"code","f6379862":"code","c9e1aad9":"code","bfd884b1":"code","7eb3f005":"code","03901ed6":"code","5ff46316":"code","8a656e01":"code","4fd67f04":"code","2bed41d5":"code","98005bf4":"code","0b11aa49":"code","e8746ae9":"code","ab2373ee":"code","c96bcebd":"code","640dea8e":"code","d90e5d74":"code","551888ba":"code","d755a609":"code","b36ff147":"code","86443f60":"code","ed581be2":"code","d933110e":"code","a69b6a28":"code","e22b71cc":"code","fe434948":"code","cf1ea3d4":"code","3fd4cb17":"code","1754760d":"code","287e2c15":"code","45d90ba4":"code","91f2b76a":"code","60d6f127":"code","3dcc5dff":"code","d74b5468":"code","3b47e77e":"code","02f831a7":"code","ecef4c4e":"code","c1e26f21":"markdown","911c5be6":"markdown","3de98a4b":"markdown","4cd86228":"markdown","6f69cdac":"markdown","92cc1c19":"markdown","0b7e1fd0":"markdown","bfa3fb51":"markdown","932d856a":"markdown"},"source":{"3c08c406":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fa7a694":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","afb3294c":"pd.options.display.max_columns = 100\npd.options.display.max_rows = 100","42811709":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv', index_col = 'id')\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv', index_col = 'id')","fdf1d114":"print('Number of NA values in train data is {}'.format(train.isna().sum().sum()))\nprint('Number of NA values in test data is {}'.format(test.isna().sum().sum()))","d9811808":"X = train.copy()\ntarget = X.pop('target').copy()","9798162c":"train.head()","5e4260fd":"train.info()","e9c98f44":"X = train.copy()\ny = X.pop('target').copy()","784374dc":"cat_features = [col for col in X.select_dtypes('object')]\nnum_features = [col for col in X.select_dtypes(['float64'])]","ec8332e3":"#count unique values for each cat_col in train dataset\n#There are four features (cat5, cat7, cat8, cat10) with high cardinality (>20 , one very high with 299)\n\nfor col in train[cat_features]:\n    print(col, train[col].nunique())","3b55e54f":"#count unique values for each cat_col in test dataset\n#There are four features (cat5, cat7, cat8, cat10) with high cardinality (>20 , one very high with 295)\n\nfor col in test[cat_features]:\n    print(col, test[col].nunique())","336d5fdd":"# group columns according to cardinality\nlow_cardinality_col = []\nhigh_cardinality_col = []\n\nfor col in train[cat_features]:\n    if train[col].nunique() <= 20:\n        low_cardinality_col.append(col)\n    else:\n        high_cardinality_col.append(col)","66274d64":"print(f\"low_cardinality_col:\", low_cardinality_col)\nprint(f\"high_cardinality_col:\", high_cardinality_col)","b65e6a59":"def cat_feature_distribution(df):\n    for col in cat_features:\n        dfg = df.groupby(col).agg(freq=(col, lambda x :x.count())).sort_values('freq',ascending = False)\n        dfg.plot(kind = 'bar', figsize = (20,8))\n        plt.ylabel('Frequency of Taregt', fontsize = 20)\n        plt.title('{}'.format(col), fontsize =20)","efdc1923":"#check cat_features distribution in train dataset\ncat_feature_distribution(train)","cf77cc43":"#check cat_features distribution in test dataset\ncat_feature_distribution(test)","f6379862":"#check cat_features between train and test\n#combine both datasets\ntrain['source'] = 'train'\ntest['source'] = 'test'\ndfm = pd.concat([train, test], axis = 0)","c9e1aad9":"dfm.shape","bfd884b1":"#check cat_features diffence between train and test\nfor col in cat_features:\n    dfg = dfm.groupby(['source',col]).agg(freq=(col, lambda x :x.count())).sort_values('freq',ascending = False).reset_index()\n    plt.figure(figsize = (20,8))\n    sns.barplot(x=col, y ='freq', hue= 'source', data = dfg)\n    plt.title('The frequency distribution for {} between train and test'.format(col), fontsize = 30)\n    plt.ylabel('Freqency')\n    ","7eb3f005":"#distribution for num_feature\ntrain[num_features].describe()","03901ed6":"#histograph for num_feature\ntrain[num_features].hist(bins = 10, figsize = (20,15));\n#sns.distplot(train[num_features], kde= True)","5ff46316":"#correlation matrix\ndef corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n","8a656e01":"corrplot(train[num_features], annot = True)","4fd67f04":"def get_df_name(df):\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name","2bed41d5":"def boxplot_plot(df):\n    plt.figure(figsize = (20, 8))\n    sns.boxplot(x = 'variable', y = 'value', data = pd.melt(df[num_features]))\n    plt.xlabel('Num_features', fontsize = 20)\n    plt.ylabel('Value', fontsize = 20)\n    plt.title('Boxplot for numerical features in {} dataset'.format(get_df_name(df)), fontsize = 30)\n    ","98005bf4":"#boxplot for train dataset\nboxplot_plot(train)","0b11aa49":"#boxplot for test dataset\nboxplot_plot(test)","e8746ae9":"plt.figure(figsize = (10,6))\nax = sns.countplot(x= target, data = train)\nax.set_title(\"Target distrubution\", fontsize = 20, y = 1.05);","ab2373ee":"dfm['cat1'] =  dfm['cat1'].apply(lambda x : x if x not in ['D', 'E'] else 'other')\ndfm['cat2'] =  dfm['cat2'].apply(lambda x : x if x not in ['N','H','B','S','U','R','K','E'] else 'other')\ndfm['cat3'] =  dfm['cat3'].apply(lambda x : x if x not in ['K','G','L','J','H','I','N'] else 'other')\ndfm['cat4'] =  dfm['cat4'].apply(lambda x : x if x not in ['E','F','G','D','H','J','I','K','M'] else 'other')\ndfm['cat5'] =  dfm['cat5'].apply(lambda x : x if x in ['BI', 'AB', 'BU','K','G','BQ','N','CL','AL','BO','AY'] else 'other')\ndfm['cat6'] =  dfm['cat6'].apply(lambda x : x if x not in ['W', 'Q'] else 'other')\ndfm['cat9'] =  dfm['cat9'].apply(lambda x : x if x not in ['W','O','U','X','S'] else 'other')","c96bcebd":"#create a function to combine less frequence < 1%  as other in cat10 \ndef less_freq_other(df):\n    threshold_percent = 1\n    series = pd.value_counts(df['cat10'])\n    mask = (series \/ series.sum() *100).lt(threshold_percent)\n    #df['cat10'] = np.where(df['cat10'].isin(series[mask].index), 'other', df['cat10'])\n    df.loc[df['cat10'].isin(series[mask].index.tolist()), 'cat10'] = 'other'","640dea8e":"less_freq_other(dfm)","d90e5d74":"dfm.head()","551888ba":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostClassifier, Pool\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, make_scorer\nfrom sklearn.linear_model import LogisticRegression","d755a609":"df1 = dfm.drop('source', axis =1).copy()","b36ff147":"#label encode categorical features\nfor col in cat_features:\n    le = LabelEncoder()\n    df1[col] = le.fit_transform(df1[col])\n    label_train = df1.iloc[:len(train), :]\n    label_test = df1.iloc[len(train):, :]\n    label_test = label_test.drop('target', axis =1)","86443f60":"print('label_train:', label_train.shape)\nprint('label_test:', label_test.shape)","ed581be2":"x= label_train.drop('target', axis = 1)\ny = label_train['target']\nx_train,  x_valid, y_train, y_valid = train_test_split(x,y,test_size = 0.2, random_state = 0)","d933110e":"#xgboost\nxgb = XGBClassifier()\n\nxgb.fit(x_train, y_train, verbose=False)\npredictions = xgb.predict_proba(x_valid)[:,1]\n\nauc = roc_auc_score(y_valid, predictions)\n\nprint(f'Baseline Score: {auc}')","a69b6a28":"#LGBM\nlgbm = LGBMClassifier()\n\nlgbm.fit(x_train, y_train, eval_set=(x_valid,y_valid), early_stopping_rounds=150, verbose=False)\npredictions = lgbm.predict_proba(x_valid)[:,1]\n\nauc = roc_auc_score(y_valid, predictions)\n\nprint(f'Baseline Score: {auc}')","e22b71cc":"#catboost\ncat_model = CatBoostClassifier(verbose=0,\n                                eval_metric=\"AUC\",\n                                random_state=42)\ncat_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n\ncat_pred = cat_model.predict_proba(x_valid)[:,1]\ncat_auc = roc_auc_score(y_valid, cat_pred)\nprint(f'AUC score for catboost: {cat_auc}')","fe434948":"#random forest\nrf = RandomForestClassifier(n_estimators = 100)\nrf.fit(x_train, y_train)\nrf_pred = rf.predict_proba(x_valid)[:,1]\nrf_auc = roc_auc_score(y_valid, rf_pred)\nprint(f'AUC score for randomforest: {rf_auc}')","cf1ea3d4":"#logisticregression\nlg = LogisticRegression(solver='liblinear')\nlg.fit(x_train, y_train)\nlg_pred = lg.predict_proba(x_valid)[:,1]\nlg_auc = roc_auc_score(y_valid, cat_pred)\nprint(f'AUC score for Logistic Regression: {lg_auc}')","3fd4cb17":"clf = CatBoostClassifier()\nparams = {'iterations': [500],\n          'depth': [4, 5, 6],\n          'loss_function': ['Logloss', 'CrossEntropy'],\n          'l2_leaf_reg': np.logspace(-20, -19, 3),\n          'leaf_estimation_iterations': [10],\n#           'eval_metric': ['Accuracy'],\n#           'use_best_model': ['True'],\n          'logging_level':['Silent'],\n          'random_seed': [42]\n         }\nscorer = make_scorer(accuracy_score)\nclf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=5)\n","1754760d":"clf_grid.fit(x_train, y_train)\nbest_param = clf_grid.best_params_\nbest_param","287e2c15":"#fit the best model\ncat_model_final = CatBoostClassifier(iterations=500,\n                           loss_function=best_param['loss_function'],\n                           depth=best_param['depth'],\n                           l2_leaf_reg=best_param['l2_leaf_reg'],\n                           eval_metric='AUC',\n                           leaf_estimation_iterations=10,\n                           use_best_model=True,\n                           logging_level='Silent',\n                           random_seed=42\n                          )","45d90ba4":"train_pool = Pool(x_train, y_train)","91f2b76a":"cat_model_final.fit(train_pool, eval_set=(x_valid,y_valid))\n","60d6f127":"submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv', index_col = 'id')","3dcc5dff":"#get an ensemble model based on both catboost and logistic regression\npred = cat_model_final.predict(label_test)","d74b5468":"submission['target'] = pred","3b47e77e":"submission.reset_index(inplace = True)","02f831a7":"submission.head()","ecef4c4e":"submission.to_csv('submission.csv', index=False)","c1e26f21":"# target variable","911c5be6":"# #the nunique values were different for some features between train and test","3de98a4b":"# numerial features","4cd86228":"# categorical features exploration","6f69cdac":" # data preparation and modeling\n\n","92cc1c19":"## cat1: combine D,E as other\n## cat2: combine N,H,B,S,U,R,K,E as other\n## cat3: combine K,G,L,J,H,I,N as other\n## cat4: keep E,F,G,D,H,J,I,K,M\n## cat5: keep BI, AB, BU,K,G,BQ,N,CL,AL,BO,AY\n## cat6: combine Q,W as other\n## cat9: combine W,O,U,X,S as other","0b7e1fd0":"# model tuning","bfa3fb51":"## feature engineer\n### cat1: combine D,E as other\n### cat2: combine N,H,B,S,U,R,K,E as other\n### cat3: combine K,G,L,J,H,I,N as other\n### cat4: keep E,F,G,D,H,J,I,K,M\n### cat5: keep BI, AB, BU,K,G,BQ,N,CL,AL,BO,AY\n### cat6: combine Q,W as other\n### cat9: combine W,O,U,X,S as other","932d856a":"## among all models, catboost and logisticregression performs best with highest AUC"}}