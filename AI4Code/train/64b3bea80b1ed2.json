{"cell_type":{"70a3a39f":"code","bfa34898":"code","e41a75da":"code","45fe0471":"code","fb328b68":"code","cfecf67e":"code","a75c57af":"code","a5985e0d":"code","f8783681":"code","06c217dd":"code","c0211bb1":"code","a5d275bc":"markdown"},"source":{"70a3a39f":"import numpy as np","bfa34898":"class LogisticRegression:\n    EPS = 1e-5\n    def __ols_solve(self, x, y):\n        # uses the closed-form formula\n        rows, cols = x.shape\n        if rows >= cols == np.linalg.matrix_rank(x):\n            y = np.maximum(self.EPS, np.minimum(y.astype(np.float32), 1-self.EPS))\n            ols_y = -np.log(np.divide(1, y) - 1)\n            self.weights = np.matmul(\n                np.matmul(\n                    np.linalg.inv(\n                        np.matmul(x.transpose(), x)\n                    ),\n                    x.transpose()),\n                ols_y)\n        else:\n            print('Error! X has not full column rank.')\n    \n    def __sgd(self, x, y, grad_fn, learning_rate, iterations, batch_size):\n        rows, cols = x.shape\n        self.weights = np.random.normal(scale=1.0\/cols, size=(cols, 1))\n        num_batches = int(np.ceil(rows\/batch_size))\n        \n        for i in range(iterations):\n            xy = np.concatenate([x, y], axis=1)\n            np.random.shuffle(xy)\n            x, y = xy[:, :-1], xy[:, -1:]\n            for step in range(num_batches):\n                start, end = batch_size*step, np.min([batch_size*(step+1), rows])\n                xb, yb = x[start:end], y[start:end]\n                \n                grads = grad_fn(xb, yb)\n                \n                self.weights -= learning_rate*grads\n    \n    def __sse_grad(self, xb, yb):\n        # computes the gradient of the Sum of Squared Errors loss\n        yb = np.maximum(self.EPS, np.minimum(yb.astype(np.float32), 1-self.EPS))\n        ols_yb = -np.log(np.divide(1, yb) - 1)\n        \n        grads = 2*np.matmul(\n            xb.transpose(),\n            np.matmul(xb, self.weights) - ols_yb)\n        \n        return grads\n    \n    def __mle_grad(self, xb, yb):\n        # computes the gradient of the MLE loss\n        term1 = np.matmul(xb.transpose(), 1-yb)\n        exw = np.exp(-np.matmul(xb, self.weights))\n        term2 = np.matmul(\n            (np.divide(exw, 1+exw)*xb).transpose(),\n            np.ones_like(yb))\n        return term1-term2\n    \n    def fit(self, x, y, method, learning_rate=0.001, iterations=500, batch_size=32):\n        x = np.concatenate([x, np.ones_like(y, dtype=np.float32)], axis=1)\n        if method == \"ols_solve\":\n            self.__ols_solve(x, y)\n        elif method == \"ols_sgd\":\n            self.__sgd(x, y, self.__sse_grad, learning_rate, iterations, batch_size)\n        elif method == \"mle_sgd\":\n            self.__sgd(x, y, self.__mle_grad, learning_rate, iterations, batch_size)\n        else:\n            print(f'Unknown method: \\'{method}\\'')\n        \n        return self\n    \n    def predict(self, x):\n        if not hasattr(self, 'weights'):\n            print('Cannot predict. You should call the .fit() method first.')\n            return\n        \n        x = np.concatenate([x, np.ones((x.shape[0], 1), dtype=np.float32)], axis=1)\n        \n        if x.shape[1] != self.weights.shape[0]:\n            print(f'Shapes do not match. {x.shape[1]} != {self.weights.shape[0]}')\n            return\n        \n        xw = np.matmul(x, self.weights)\n        return np.divide(1, 1+np.exp(-xw))\n    \n    def accuracy(self, x, y):\n        y_hat = self.predict(x)\n        \n        if y.shape != y_hat.shape:\n            print('Error! Predictions don\\'t have the same shape as given y')\n            return\n        \n        zeros, ones = np.zeros_like(y), np.ones_like(y)\n        y = np.where(y >= 0.5, ones, zeros)\n        y_hat = np.where(y_hat >= 0.5, ones, zeros)\n        \n        return np.mean((y == y_hat).astype(np.float32))","e41a75da":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","45fe0471":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndf","fb328b68":"x, y = df.iloc[:, 0:-1].values, df.iloc[:, -1].values.reshape((-1, 1))","cfecf67e":"x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8)","a75c57af":"def print_acc(model):\n    print(f'Train accuracy = {model.accuracy(x_train, y_train)} ; '+\n          f'Test accuracy = {model.accuracy(x_test, y_test)}')","a5985e0d":"scaler = MinMaxScaler().fit(x_train)\nx_train, x_test = scaler.transform(x_train), scaler.transform(x_test)","f8783681":"lr_ols_solve = LogisticRegression().fit(x_train, y_train, 'ols_solve')\nprint_acc(lr_ols_solve)","06c217dd":"lr_ols_sgd = LogisticRegression().fit(x_train, y_train, 'ols_sgd')\nprint_acc(lr_ols_sgd)","c0211bb1":"lr_mle_sgd = LogisticRegression().fit(x_train, y_train, 'mle_sgd')\nprint_acc(lr_mle_sgd)","a5d275bc":"# Predicting heart disease with a logistic regression model made from scratch"}}