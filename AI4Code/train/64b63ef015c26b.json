{"cell_type":{"1b80bfc9":"code","f0f07665":"code","bb00b0b8":"code","589a2752":"code","30f93ba1":"code","2022fd46":"code","bf9a8154":"code","657e7fd9":"code","ad18590c":"code","0fa3abce":"code","5dd350e9":"code","edd1d3e8":"code","84ae388f":"code","eb83df5b":"code","0d9a0c36":"code","b173b4c0":"code","ef38cd9d":"code","258cac8f":"code","e2b5c7f2":"code","13744fba":"code","931f5f8c":"code","bb0af241":"code","8b32826e":"code","72516151":"code","fed6605d":"code","c6346c46":"code","d1d3b98a":"code","7519f626":"code","6256ec2d":"code","815d2838":"code","c408d98a":"code","ed13f1f7":"code","b77619e0":"code","731808c8":"code","1ad6e1e1":"code","5d760140":"code","dfda7832":"code","655d27e9":"code","73397052":"code","3692d3a8":"code","8b655be9":"code","1802b7ca":"code","95fe75b0":"code","7735e59d":"markdown"},"source":{"1b80bfc9":"train_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')","f0f07665":"train_data.head()","bb00b0b8":"train_data.shape","589a2752":"import numpy as np\nimport pandas as pd\nimport nltk\n# nltk.download('stopwords')\n# nltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n# !pip install contractions\n# import contractions\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud","30f93ba1":"def preprocess(sentence):\n    stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    \n    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n                        'even though', 'yet']\n    stop_words = [z for z in stop_words if z not in negative]\n    preprocessed_tokens = [lemmatizer.lemmatize(temp.lower()) for temp in sentence.split() if temp not in stop_words] #lemmatization\n    sentence_temp = re.sub('[^A-z]', ' ', ' '.join([x for x in preprocessed_tokens]).strip())\n    return sentence_temp","2022fd46":" train_data['excerpt'] = train_data['excerpt'].apply(lambda x: preprocess(x))","bf9a8154":"x = train_data['excerpt']\ny = train_data['target']\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)","657e7fd9":"# # !pip install transformers\n# import transformers","ad18590c":"# from transformers import BertTokenizer, RobertaTokenizer\n# tokenizer = BertTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased', do_lower_case=True)","0fa3abce":"# import numpy as np\n# import tensorflow as tf ","5dd350e9":"# def bert_encode(data,maximum_length) :\n#     input_ids = []\n#     attention_masks = []\n#     token_type_ids = []\n#     for i in data:\n#         encoded = tokenizer.encode_plus(\n\n#         i,\n#         add_special_tokens=True,\n#         max_length=maximum_length,\n#         pad_to_max_length=True,\n\n#         return_attention_mask=True,\n\n#         )\n\n#         input_ids.append(encoded['input_ids'])\n#         attention_masks.append(encoded['attention_mask'])\n# #         token_type_ids.append(encoded['token_type_ids'])\n        \n#     return np.array(input_ids),np.array(attention_masks)  #, np.array(token_type_ids)","edd1d3e8":"max_len = max([len(x) for x in train_data['excerpt']])\nmax_len\n","84ae388f":"max_len = 800","eb83df5b":"# train_input_ids,train_attention_masks = bert_encode(xtrain,max_len)\n# test_input_ids,test_attention_masks = bert_encode(xtest,max_len)","0d9a0c36":"# train_input_ids,train_attention_masks = bert_encode(train_data,max_len)\n# test_input_ids,test_attention_masks = bert_encode(test,60)","b173b4c0":"# import tensorflow as tf\n# from tensorflow.keras.optimizers import Adam\n# from keras.metrics import RootMeanSquaredError\n# def create_model(bert_model, max_len):\n#     input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n#     attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n# #     token_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n\n#     output = bert_model([input_ids, attention_masks])[0]\n#     output = output[:, 0, :]\n# #     output = tf.keras.layers.Dense(512,activation='relu')(output)\n# #     output = tf.keras.layers.Dropout(0.2)(output)\n    \n#     output = tf.keras.layers.Dense(256,activation='relu')(output)\n#     output = tf.keras.layers.Dropout(0.4)(output)\n    \n#     output = tf.keras.layers.Dense(64, activation='relu')(output)\n#     output = tf.keras.layers.Dropout(0.2)(output)\n\n#     output = tf.keras.layers.Dense(1,activation='linear')(output)\n#     model = tf.keras.models.Model(inputs = [input_ids, attention_masks],outputs = output)\n#     model.compile(Adam(lr=2e-6), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n#     return model","ef38cd9d":"# from transformers import TFBertModel, TFRobertaModel\n# bert_model = TFBertModel.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased')","258cac8f":"# model = create_model(bert_model, max_len)\n# model.summary()","e2b5c7f2":"# history = model.fit([train_input_ids, train_attention_masks], train_data.target, validation_split=0.2, epochs=4, batch_size=4)","13744fba":"# result = model.predict([test_input_ids, test_attention_masks])","931f5f8c":"# from sklearn.metrics import mean_squared_error\n# mean_squared_error(ytest, result)","bb0af241":"%cd ..\/input\/sentence-transformers\/sentence-transformers-master","8b32826e":"import sentence_transformers\nfrom sentence_transformers import SentenceTransformer\nsent_model = SentenceTransformer('..\/..\/sentence-transformer-models\/distilroberta-base-paraphrase-v1')","72516151":"xtrain_encode = sent_model.encode([temp for temp in xtrain])\nxtest_encode = sent_model.encode([temp for temp in xtest])","fed6605d":"# xtrain_encode = sent_model.encode([temp for temp in x])\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(xtrain_encode, ytrain)\nypred = reg.predict(xtest_encode)\n\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(ytest, ypred)","c6346c46":"from sklearn.linear_model import LinearRegression, Ridge, RidgeCV,Lasso,LassoCV\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nfrom xgboost import XGBRegressor\nmodel = XGBRegressor(n_estimators=100, max_depth=4)\n\nlr=0.1\nn = 100\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nsvr_model = SVR(C=1)\ndecision_tree_model = DecisionTreeRegressor()\nrandom_forest_model = RandomForestRegressor(n_estimators=n)\nadaboost_model = AdaBoostRegressor(n_estimators=n,learning_rate=lr)\ngradientboost_model = GradientBoostingRegressor(learning_rate=lr,n_estimators = n)\nridge_model = Ridge(alpha=0.1)\nridge_cv_model = RidgeCV(alphas=[0.1,0.01,0.001],cv=5)\nlasso_model = Lasso(alpha=0.1)\nlasso_cv_model = LassoCV(alphas=[0.1,0.01,0.001],cv=5)","d1d3b98a":"def metrics(ytest,ypred):\n    return np.sqrt(mean_squared_error(ytest,ypred))","7519f626":"xtrain_encode.shape","6256ec2d":"xtrain_encode = sent_model.encode([temp for temp in xtrain])\nxtest_encode = sent_model.encode([temp for temp in xtest])","815d2838":"result = pd.DataFrame([],columns=['Model','CV_rmse','Prediction_rmse'])\ndef compute(model,i):\n#     cv_rmse = np.sqrt(-cross_val_score(model, xtrain_encode, ytrain, scoring=\"neg_mean_squared_error\", cv=kf))\n    model.fit(xtrain_encode,ytrain)\n    ypred = model.predict(xtest_encode)\n    result.loc[i] = [str(model)[:str(model).index('(')] ,4,metrics(ytest,ypred)]","c408d98a":"models = [svr_model,decision_tree_model,reg,model,random_forest_model,adaboost_model,gradientboost_model,ridge_model,ridge_cv_model,lasso_model,lasso_cv_model]\nfor model in range(len(models)):\n    print(model)\n    compute(models[model],model)\nresult = result.sort_values('CV_rmse')\nwarnings.filterwarnings(\"ignore\")\nresult","ed13f1f7":"X = sent_model.encode(x) ","b77619e0":"from mlxtend.regressor import StackingCVRegressor\nstack_gen = StackingCVRegressor(regressors=(ridge_model,ridge_cv_model,lasso_cv_model,reg,model,gr),\n                                meta_regressor=svr_model,\n                                use_features_in_secondary=True,cv=20)\nstack_gen.fit(np.array(X),np.array(y))","731808c8":"ypred = stack_gen.predict(np.array(xtest_encode))\nwarnings.filterwarnings(\"ignore\")\nprint('StackingCV Regressor RMSE {}'.format(metrics(ytest,ypred)))","1ad6e1e1":"# reg.fit(xtrain_encode, y)","5d760140":"%cd ..\/..\/..\/working","dfda7832":"test_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ntest_data.head()","655d27e9":"test_data['excerpt'] = test_data['excerpt'].apply(lambda x: preprocess(x))\n# test_input_ids, test_attention_masks = bert_encode(test_data, max_len)","73397052":"xtest_encode = sent_model.encode([x for x in test_data['excerpt']])\nypred = stack_gen.predict(xtest_encode)\n# from sklearn.metrics import mean_squared_error\n# error = mean_squared_error(ytest, ypred)\n# error","3692d3a8":"# result = model.predict([test_input_ids, test_attention_masks])","8b655be9":"submission_df  = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsubmission_df.head()","1802b7ca":"submission_df['target'] = ypred\nsubmission_df","95fe75b0":"submission_df.to_csv('submission.csv', index = False)","7735e59d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}