{"cell_type":{"dca77bbe":"code","8f473359":"code","cb306247":"code","60902c9f":"code","37191958":"code","5d155d94":"code","50a753b1":"code","76a6e9fa":"code","1a71c613":"code","ab0fb190":"code","c51769ff":"code","1b1f67d2":"code","634ccf70":"code","e9b710a7":"code","3248bbaf":"code","68e0ecf4":"code","4b6112c1":"code","454585bb":"code","6d15a94c":"code","2d9d83f1":"code","ea9ce910":"markdown","172a51e0":"markdown","2ba6ddb7":"markdown","11c39413":"markdown","296783f6":"markdown","7cb0bcac":"markdown"},"source":{"dca77bbe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8f473359":"df = pd.read_csv('..\/input\/heart-patients\/US_Heart_Patients.csv')\ndf = df.sample(frac = 1,random_state = 3)","cb306247":"df.head()","60902c9f":"df['TenYearCHD'].value_counts()","37191958":"df['TenYearCHD'].value_counts(normalize = True).plot(kind = 'bar')","5d155d94":"ms = df.isnull().sum()\nms[ms > 0]","50a753b1":"df = df.fillna(method = 'ffill')","76a6e9fa":"y = df['TenYearCHD']\nX = df.drop('TenYearCHD',axis = 1)","1a71c613":"from sklearn.model_selection import train_test_split","ab0fb190":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 3)","c51769ff":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear')","1b1f67d2":"from sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve,accuracy_score\ndef model_eval(algo,Xtrain,ytrain,Xtest,ytest):\n    algo.fit(Xtrain,ytrain)\n    y_train_ypred = algo.predict(Xtrain)\n    y_train_prob = algo.predict_proba(Xtrain)[:,-1]\n\n    print('Confusion matrix - Train : \\n',confusion_matrix(ytrain,y_train_ypred))\n\n    print('Overall accuracy : ',accuracy_score(ytrain,y_train_ypred))\n\n    print('AUC - Train : ',roc_auc_score(ytrain,y_train_prob))\n    print()\n\n    #### TEST\n\n    y_test_ypred = algo.predict(Xtest)\n    y_test_prob = algo.predict_proba(Xtest)[:,-1]\n\n    print('Confusion matrix - Test : \\n',confusion_matrix(ytest,y_test_ypred))\n\n    print('Overall accuracy - Test : ',accuracy_score(ytest,y_test_ypred))\n\n    print('AUC - Test : ',roc_auc_score(ytest,y_test_prob))\n\n    fpr,tpr,thresholds = roc_curve(ytest,y_test_prob)\n\n    plt.plot(fpr,tpr)\n    plt.plot(fpr,fpr)\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.show()","634ccf70":"model_eval(lr,X_train,y_train,X_test,y_test)","e9b710a7":"Xy_train = pd.concat([X_train,y_train],axis = 1)\n\nprint('Before undersampling : \\n',Xy_train['TenYearCHD'].value_counts())\nprint()\n\nXy_train_0 = Xy_train[Xy_train['TenYearCHD'] == 0]\nXy_train_1 = Xy_train[Xy_train['TenYearCHD'] == 1]\n\nlen_0 = len(Xy_train_0)\nlen_1 = len(Xy_train_1)\n\n# Undersampling\nXy_train_0_us = Xy_train_0.sample(len_1,random_state = 3)\n\nXy_train_us = pd.concat([Xy_train_0_us,Xy_train_1])\n\nprint('After undersampling : \\n',Xy_train_us['TenYearCHD'].value_counts())\n\ny_train_us = Xy_train_us['TenYearCHD']\nX_train_us = Xy_train_us.drop('TenYearCHD',axis = 1)","3248bbaf":"model_eval(lr,X_train_us,y_train_us,X_test,y_test)","68e0ecf4":"Xy_train = pd.concat([X_train,y_train],axis = 1)\n\nprint('Before oversampling : \\n',Xy_train['TenYearCHD'].value_counts())\nprint()\n\nXy_train_0 = Xy_train[Xy_train['TenYearCHD'] == 0]\nXy_train_1 = Xy_train[Xy_train['TenYearCHD'] == 1]\n\nlen_0 = len(Xy_train_0)\nlen_1 = len(Xy_train_1)\n\n# OverSampling\nXy_train_1_os = Xy_train_1.sample(len_0,replace = True,random_state = 3)\n\nXy_train_os = pd.concat([Xy_train_1_os,Xy_train_0])\n\nprint('After oversampling : \\n',Xy_train_os['TenYearCHD'].value_counts())\n\ny_train_os = Xy_train_os['TenYearCHD']\nX_train_os = Xy_train_os.drop('TenYearCHD',axis = 1)","4b6112c1":"model_eval(lr,X_train_os,y_train_os,X_test,y_test)","454585bb":"# !pip install imblearn","6d15a94c":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy = 'minority',random_state = 3)\n\nX_train_sm, y_train_sm = smote.fit_sample(X_train,y_train)","2d9d83f1":"model_eval(lr,X_train_sm,y_train_sm,X_test,y_test)","ea9ce910":"### Using SMOTE (Synthetic Minority Over-sampling Technique)","172a51e0":"***Please UpVote if you like the work!!!***","2ba6ddb7":"The intention for the below code is just for us to get introduced to the techniques which can be used to handle the imbalanced data.\n\nIn my opinion, handling data to a complete 50 - 50 % is not a very good practice.\n\nHandling of imbalanced data should only be limited to how this imbalance is represented in our population. If our sample data follows the same imbalance as that of population, then there is no need to handle imbalanced data. The only problem this imbalanced data could have is when our model is highly biased, it could lead us to predicting a biased result.\n\nIf we have build our model which is quite accurate(handled the bias and variace error well), the imbalance will not affect our model prediction.","11c39413":"***Please UpVote if you like the work!!!***","296783f6":"### UNDERSAMPLING MAJORITY CLASS","7cb0bcac":"### OVERSAMPLING MINORITY CLASS"}}