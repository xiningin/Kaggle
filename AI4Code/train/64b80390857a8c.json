{"cell_type":{"189330f0":"code","bace1f0b":"code","ccd93625":"code","1855518a":"code","841a8e5f":"code","39dd8636":"code","61495e54":"code","5fed5b75":"code","e3094b71":"code","5cbe10bd":"code","169cfac5":"code","81f5ff90":"code","4109eda3":"code","0b5282cc":"code","7ccb8b0a":"code","98cb9d89":"code","bc42ed2f":"code","98578da3":"code","83673f4f":"code","1b454253":"code","a6583038":"code","00ec7d07":"code","2162dc26":"code","9505995b":"code","6fc318ab":"code","5eb3ce2a":"code","38e654fe":"code","278b35f6":"code","5bf186a5":"code","5b8b71f9":"code","b2e7f3c2":"code","7a448f8b":"code","b568e178":"markdown","1cc4a422":"markdown","882c9ed1":"markdown","67a6bbcc":"markdown","84249982":"markdown","c8a875aa":"markdown","dc54dfb9":"markdown","5438f00a":"markdown","248e7baa":"markdown","3e9e22e9":"markdown","e14ffb62":"markdown","e6879303":"markdown","4ea17cdb":"markdown","167d6876":"markdown","6dbd20f8":"markdown","4041e6c6":"markdown","d3c40733":"markdown","bfe9ccdd":"markdown","c229ae20":"markdown","f709a4cb":"markdown","635cfdc9":"markdown","f906c859":"markdown","57dcbbd0":"markdown"},"source":{"189330f0":"import pandas as pd\nimport datatable as dt\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, RobustScaler\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV\nfrom sklearn.neural_network import MLPClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import (AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn.model_selection import cross_validate,cross_val_score,train_test_split, KFold, GridSearchCV\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\n","bace1f0b":"#choose colors accordingly\npl1=px.colors.qualitative.swatches()\npl1.show()","ccd93625":"print(px.colors.qualitative.Plotly)","1855518a":"import datatable as dt\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","841a8e5f":"%%time\ntrain = dt.fread('..\/input\/tabular-playground-series-dec-2021\/train.csv').to_pandas().drop('Id', axis=1)\ntrain = reduce_memory_usage(train)\ntest = dt.fread('..\/input\/tabular-playground-series-dec-2021\/test.csv').to_pandas().drop('Id', axis=1)\ntest = reduce_memory_usage(test)\nss = dt.fread('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv').to_pandas()\nss = reduce_memory_usage(ss)","39dd8636":"categorical_features=[]\nnumerical_features=[]\nfor i in test.columns:\n    if test[i].dtype=='int16' or 'int8':\n        numerical_features.append(i)\n    elif i!='target':\n        categorical_features.append(i)\n\nprint(f'Continious features obtained {len(numerical_features)} and categorical features obtained are {len(categorical_features)}')","61495e54":"\npal=['#FFA15A','#00CC96']\n\nfig = px.histogram(train, x=\"Cover_Type\", color=\"Cover_Type\", color_discrete_sequence=px.colors.qualitative.Pastel1)\nfig.show()\n\n","5fed5b75":"%%time\nscale = RobustScaler()\n#scale = MinMaxScaler()\n#scale = StandardScaler()\ntrain[numerical_features]=scale.fit_transform(train[numerical_features])\ntest[numerical_features]= scale.transform(test[numerical_features])  \n\nprint('Data scaled using : ', scale)","e3094b71":"train['Cover_Type'].unique()\nprint(len(train[train['Cover_Type']==5]))\nprint(len(train[train['Cover_Type']==4]))\n","5cbe10bd":"#lets train only on 10% data and identify what models perform better and then later can develop a piepline for the best performing model.\nX=train.drop(['Cover_Type'],axis=1)\ny=train.Cover_Type\nX_train,X_valid,y_train,y_valid=train_test_split(X,y,train_size=0.1,test_size=0.05,random_state=2001)","169cfac5":"print(len(X_train))","81f5ff90":"def cnf_matrix_and_report(y_validation,y_prediction):\n    cf_matrix = confusion_matrix(y_validation, y_prediction)\n    sns_plot=sns.heatmap(cf_matrix, annot=True,  cmap='Blues')\n    fig = sns_plot.get_figure()\n    #fig.savefig(\"output.png\")\n    print(classification_report(y_validation, y_prediction))","4109eda3":"accuracy_list=[]\nmodel_list=[]","0b5282cc":"#1. Linear SVC\n\nmodel_type='Linear_svc'\nlinear_svc=LinearSVC()\nlinear_svc.fit(X_train,y_train)\nypreds=linear_svc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)","7ccb8b0a":"#2. Decision Tree\nmodel_type='DecisionTreeClassifier'\ndt=DecisionTreeClassifier()\ndt.fit(X_train,y_train)\nypreds=dt.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","98cb9d89":"#3. Random Forest\n\nmodel_type='RandomForestClassifier'\nclf=RandomForestClassifier()\nclf.fit(X_train,y_train)\nypreds=clf.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","bc42ed2f":"#4. AdaBoostClassifier\n\nmodel_type='AdaBoostClassifier'\nadb=AdaBoostClassifier()\nadb.fit(X_train,y_train)\nypreds=adb.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","98578da3":"#5. AdaBoostClassifier\n\nmodel_type='BaggingClassifier'\nbagging=BaggingClassifier()\nbagging.fit(X_train,y_train)\nypreds=bagging.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","83673f4f":"#6. ExtraTreesClassifier\nmodel_type='ExtraTreesClassifier'\netc=ExtraTreesClassifier()\netc.fit(X_train,y_train)\nypreds=etc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","1b454253":"#7. GradientBoostingClassifier\nmodel_type='GradientBoostingClassifier'\ngbc=GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\nypreds=gbc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","a6583038":"#8. LGBMClassifier\nmodel_type='LGBMClassifier'\nlgbm=LGBMClassifier()\nlgbm.fit(X_train,y_train)\nypreds=lgbm.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","00ec7d07":"#9 XGBClassifier\nmodel_type='XGBClassifier'\nxgb=XGBClassifier()\nxgb.fit(X_train,y_train)\nypreds=xgb.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","2162dc26":"#10 SGDClassifier\nmodel_type='SGDClassifier'\nsgd=SGDClassifier()\nsgd.fit(X_train,y_train)\nypreds=sgd.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","9505995b":"#11 Ridge Classifier\nmodel_type='Ridge Classifier'\nrgc=RidgeClassifierCV()\nrgc.fit(X_train,y_train)\nypreds=rgc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","6fc318ab":"#12 Perceptron\nmodel_type='Perceptron'\npercp=Perceptron()\npercp.fit(X_train,y_train)\nypreds=percp.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","5eb3ce2a":"#13 passive aggresive\nmodel_type='Passive Aggresive'\npac=PassiveAggressiveClassifier()\npac.fit(X_train,y_train)\nypreds=pac.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","38e654fe":"#14 MLP sklearn\nmodel_type='MLP'\nmlp=MLPClassifier()\nmlp.fit(X_train,y_train)\nypreds=mlp.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","278b35f6":"list_of_tuples = list(zip(model_list, accuracy_list))\ndf = pd.DataFrame(list_of_tuples, columns = ['Models', 'scores'])\ndf=df.sort_values(by='scores', ascending=False)\ndf=df.reset_index(drop=True)","5bf186a5":"fig = px.bar(df, x='Models', y='scores',color_discrete_sequence=px.colors.qualitative.Pastel)\nfig.show()","5b8b71f9":"sns.set(rc = {'figure.figsize':(12,8)})\nsns.heatmap(train.corr())","b2e7f3c2":"#Absolute corr\nabsoulte_corr = pd.DataFrame(np.abs(train).corr()['Cover_Type'])\nabsoulte_corr=absoulte_corr.sort_values(by='Cover_Type', ascending=False)\ntop15_absolute=absoulte_corr.head(15).index.tolist()\ntop15_absolute","7a448f8b":"#submitting 1 model.\n#NOTE - we haven't used the full dataset , not expecting anything from the LB(lol)\nxgb_submission = ss.copy()\nypreds=xgb.predict(test).tolist()\nxgb_submission['Cover_Type'] = ypreds\nxgb_submission.to_csv(\"xgbsub.csv\",index=None)\nxgb_submission.head()\n'''\nPS - got a score of .0.95056 in public I guess it will overfit on\nthe hidden test dataset as it was only trained on 10 percent of data with no Fine tuning.\nEasy improvements Ahead'''","b568e178":"<a id=\"3\"><\/a>\n## Target distribution","1cc4a422":"\n<a id=\"5.7\"><\/a>\n### Gradient Boosting\n","882c9ed1":"\n<a id=\"5.9\"><\/a>\n### XgBoost\n","67a6bbcc":"<a id=\"1\"><\/a>\n## Imports","84249982":"\n<a id=\"5.8\"><\/a>\n### Light Gradient Boosting\n","c8a875aa":"### Thanks for spending your time while going through this Kernel. \n#### Cheers","dc54dfb9":"\n<a id=\"5.12\"><\/a>\n### Perceptron\n","5438f00a":"\n<a id=\"5.6\"><\/a>\n### Extra Trees Classifier\n","248e7baa":"\n<a id=\"5.11\"><\/a>\n### Ridge Classifier\n","3e9e22e9":"\n<a id=\"5.14\"><\/a>\n### MLP\n","e14ffb62":"\n<a id=\"6\"><\/a>\n## Scores plot\n","e6879303":"\n<a id=\"5.2\"><\/a>\n### Decision Tree","4ea17cdb":"\n<a id=\"5.3\"><\/a>\n### Random Forest\n","167d6876":"\n<a id=\"5.5\"><\/a>\n### Bagging Classifier\n","6dbd20f8":"\n<a id=\"5.13\"><\/a>\n### Passive Aggresive\n","4041e6c6":"# [TPS]Dec-2021(14 Baseline Models)\n![Happy-Holidays-December-FB-Cover-Picture.jpg](attachment:f089f296-501c-4cf1-9e85-1c950864bde2.jpg)\n## Dataset\nFor each monthly competition, we'll be offering Kaggle Merchandise for the top three teams. And finally, because we want these competitions to be more about learning, we're limiting team sizes to 3 individuals.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. This dataset is based off of the original Forest Cover Type Prediction competition\n\n# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Imports](#1)\n- [2. Load Data](#2)\n- [3. Target Distribution](#3)\n- [4. Function for confusion matrix and classification report](#2)\n- [5. Modelling](#2)\n    - [5.1 Linear SVC](#5.1)\n    - [5.2 Decision Tree](#5.2)\n    - [5.3 Random Forest](#5.3)\n    - [5.4 ADA Boost](#5.4)\n    - [5.5 Bagging Classifier](#5.5)\n    - [5.6 Extra Trees](#5.6)\n    - [5.7 Gradient Boosting](#5.7)\n    - [5.8 Light Gradient Boosting](#5.8)\n    - [5.9 XGB](#5.9)\n    - [5.10 SGD Classifier](#5.10)\n    - [5.11 Ridge](#5.11)\n    - [5.12 Passive Aggresive](#5.12)\n    - [5.13 Perceptron](#5.13)\n    - [5.14 MLP](#5.14)\n- [6. Scores](#6)\n- [6. Feature Importance Analysis](#7)\n\n.\n## Observations\n- num_classes = 7\n- Highly Imbalanced dataset, with soil type 5 having only 1 example\n- Saw some correlations among the features with Elavation having a high Correlation with the target variable\n- No of models fitted - 14\n- Boosting and tree based classifiers performed much better although each one of them needs to be finetuned as lots of FP,FN .\n- Weighted classes shall be used as regularizers for better results.\n- Also a NN based model can be trained and then further optimized as we have seen that NN models are performing better these days in TPS.\n- A basic analyis of correlation was done in the end of the notebook, took top 15 features, SHAP notebook out soon.\n\n## Contact\n\n- [Click here ](https:\/\/twitter.com\/bambose_) - to connect me on twitter\n- [Click here](https:\/\/www.kaggle.com\/datastrophy\/tps-12-21-xgb-optuna-tutorial-study-vis\/edit) to visit  TPS-12\/21[Xgb-optuna-tutorial+(study vis)]\n\n\n\n\n\n\n\n\n\n","d3c40733":"\n<a id=\"5\"><\/a>\n## Modelling\n<br><hr>\n<br>\n<a id=\"5.1\"><\/a>\n### Linear SVC","bfe9ccdd":"\n<a id=\"5.4\"><\/a>\n### AdaBoost Classifier\n","c229ae20":"\n<a id=\"5.10\"><\/a>\n### Stocastic gradient Descent\n","f709a4cb":"# My Observations\n**14 models fitted.**\nWe can see that some models performed better than others but there is huge huge class imbalance and that is going to affect the f1 score, also Soil_type =5 has only one example, it's going to be a big task to detect that.\n\nThe next step for model training can be finetuning the hyperparams for top 5 models on a single fold and then further full training on 10 fold dataset with including weighted classes.\n\nAlso a NN based model can be trained and then further optimized as we have seen that NN models are performing better these days in TPS.\n\n\nFurther more the next step should be get the feature importance of this dataset, we might use a few tecniques to get the important features and then maybe \n\n\n","635cfdc9":"<a id=\"4\"><\/a>\n## Function for confusion matrix and classification report","f906c859":"<a id=\"2\"><\/a>\n## Load Data","57dcbbd0":"\n<a id=\"7\"><\/a>\n### Feature Importance Analysis"}}