{"cell_type":{"531255bd":"code","769bc6d8":"code","11243ce7":"code","65ce3d10":"code","10c4c981":"code","8774373e":"code","f8e42ccb":"code","45faf2c3":"code","4301b4e1":"code","20a0d3da":"code","a6ba6faf":"code","d167a432":"code","ddf20888":"code","fe2ff80e":"markdown","fbc5a6a3":"markdown","3e86f818":"markdown"},"source":{"531255bd":"import tensorflow as tf","769bc6d8":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","11243ce7":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.layers import (Activation, BatchNormalization, Concatenate, Dense,\n                          Embedding, Flatten, Input, Multiply, Reshape)\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam","65ce3d10":"img_rows = 28\nimg_cols = 28\nchannels = 1\n\n# Input image dimensions\nimg_shape = (img_rows, img_cols, channels)\n\n# Size of the noise vector, used as input to the Generator\nz_dim = 100\n\n# Number of classes in the dataset\nnum_classes = 10","10c4c981":"def build_generator(z_dim):\n\n    model = Sequential()\n\n    # Reshape input into 7x7x256 tensor via a fully connected layer\n    model.add(Dense(256 * 7 * 7, input_dim=z_dim))\n    model.add(Reshape((7, 7, 256)))\n\n    # Transposed convolution layer, from 7x7x256 into 14x14x128 tensor\n    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n\n    # Batch normalization\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Transposed convolution layer, from 14x14x128 to 14x14x64 tensor\n    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n\n    # Batch normalization\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Transposed convolution layer, from 14x14x64 to 28x28x1 tensor\n    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n\n    # Output layer with tanh activation\n    model.add(Activation('tanh'))\n\n    return model","8774373e":"def build_cgan_generator(z_dim):\n\n    # Random noise vector z\n    z = Input(shape=(z_dim, ))\n\n    # Conditioning label: integer 0-9 specifying the number G should generate\n    label = Input(shape=(1, ), dtype='int32')\n\n    # Label embedding:\n    # ----------------\n    # Turns labels into dense vectors of size z_dim\n    # Produces 3D tensor with shape (batch_size, 1, z_dim)\n    label_embedding = Embedding(num_classes, z_dim, input_length=1)(label)\n\n    # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, z_dim)\n    label_embedding = Flatten()(label_embedding)\n\n    # Element-wise product of the vectors z and the label embeddings\n    joined_representation = Multiply()([z, label_embedding])\n\n    generator = build_generator(z_dim)\n\n    # Generate image for the given label\n    conditioned_img = generator(joined_representation)\n\n    return Model([z, label], conditioned_img)","f8e42ccb":"def build_discriminator(img_shape):\n\n    model = Sequential()\n\n\n    model.add(\n        Conv2D(64,\n               kernel_size=3,\n               strides=2,\n               input_shape=(img_shape[0], img_shape[1], img_shape[2] + 1),\n               padding='same'))\n\n \n    model.add(LeakyReLU(alpha=0.01))\n\n  \n    model.add(\n        Conv2D(64,\n               kernel_size=3,\n               strides=2,\n               input_shape=img_shape,\n               padding='same'))\n\n\n    model.add(BatchNormalization())\n\n    # Leaky ReLU activation\n    model.add(LeakyReLU(alpha=0.01))\n\n    model.add(\n        Conv2D(128,\n               kernel_size=3,\n               strides=2,\n               input_shape=img_shape,\n               padding='same'))\n\n\n    model.add(BatchNormalization())\n\n\n    model.add(LeakyReLU(alpha=0.01))\n\n    # Output layer with sigmoid activation\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model","45faf2c3":"def build_cgan_discriminator(img_shape):\n\n\n    img = Input(shape=img_shape)\n\n    label = Input(shape=(1, ), dtype='int32')\n\n    # Label embedding:\n\n    label_embedding = Embedding(num_classes,\n                                np.prod(img_shape),\n                                input_length=1)(label)\n\n    \n    label_embedding = Flatten()(label_embedding)\n\n   \n    label_embedding = Reshape(img_shape)(label_embedding)\n\n   \n    concatenated = Concatenate(axis=-1)([img, label_embedding])\n\n    discriminator = build_discriminator(img_shape)\n\n    classification = discriminator(concatenated)\n\n    return Model([img, label], classification)","4301b4e1":"def build_cgan(generator, discriminator):\n\n \n    z = Input(shape=(z_dim, ))\n    label = Input(shape=(1, ))\n\n    img = generator([z, label])\n\n    classification = discriminator([img, label])\n\n\n    model = Model([z, label], classification)\n\n    return model","20a0d3da":"discriminator = build_cgan_discriminator(img_shape)\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=Adam(),\n                      metrics=['accuracy'])\n\n\ngenerator = build_cgan_generator(z_dim)\n\n\ndiscriminator.trainable = False\n\n\ncgan = build_cgan(generator, discriminator)\ncgan.compile(loss='binary_crossentropy', optimizer=Adam())","a6ba6faf":"accuracies = []\nlosses = []\n\n\ndef train(iterations, batch_size, sample_interval):\n\n    \n    (X_train, y_train), (_, _) = mnist.load_data()\n\n    \n    X_train = X_train \/ 127.5 - 1.\n    X_train = np.expand_dims(X_train, axis=3)\n\n    real = np.ones((batch_size, 1))\n\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n\n\n\n        # Get a random batch of real images and their labels\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        imgs, labels = X_train[idx], y_train[idx]\n\n       \n        z = np.random.normal(0, 1, (batch_size, z_dim))\n        gen_imgs = generator.predict([z, labels])\n\n       \n        d_loss_real = discriminator.train_on_batch([imgs, labels], real)\n        d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n       \n        z = np.random.normal(0, 1, (batch_size, z_dim))\n\n        \n        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n         # Train the Generator\n        g_loss = cgan.train_on_batch([z, labels], real)\n\n        if (iteration + 1) % sample_interval == 0:\n\n            # Output training progress\n            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n                  (iteration + 1, d_loss[0], 100 * d_loss[1], g_loss))\n\n            # Save losses and accuracies so they can be plotted after training\n            losses.append((d_loss[0], g_loss))\n            accuracies.append(100 * d_loss[1])\n\n            # Output sample of generated images\n            sample_images()","d167a432":"def sample_images(image_grid_rows=2, image_grid_columns=5):\n\n    # Sample random noise\n    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n\n    # Get image labels 0-9\n    labels = np.arange(0, 10).reshape(-1, 1)\n\n    # Generate images from random noise\n    gen_imgs = generator.predict([z, labels])\n\n    # Rescale image pixel values to [0, 1]\n    gen_imgs = 0.5 * gen_imgs + 0.5\n\n    # Set image grid\n    fig, axs = plt.subplots(image_grid_rows,\n                            image_grid_columns,\n                            figsize=(10, 4),\n                            sharey=True,\n                            sharex=True)\n\n    cnt = 0\n    for i in range(image_grid_rows):\n        for j in range(image_grid_columns):\n            # Output a grid of images\n            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n            axs[i, j].axis('off')\n            axs[i, j].set_title(\"Digit: %d\" % labels[cnt])\n            cnt += 1","ddf20888":"# Set hyperparameters\niterations = 12000\nbatch_size = 32\nsample_interval = 1000\n\n# Train the CGAN for the specified number of iterations\ntrain(iterations, batch_size, sample_interval)","fe2ff80e":"# Conditional GAN","fbc5a6a3":"> Generative adversarial nets can be extended to a conditional model if both the generator and discrim- inator are conditioned on some extra information y. y could be any kind of auxiliary information, such as class labels or data from other modalities. We can perform the conditioning by feeding y into the both the discriminator and generator as additional input layer.\n> In the generator the prior input noise pz(z), and y are combined in joint hidden representation, and the adversarial training framework allows for considerable flexibility in how this hidden representa- tion is composed. 1\nIn the discriminator x and y are presented as inputs and to a discriminative function (embodied again by a MLP in this case).\nThe objective function of a two-player minimax game would be as Eq 2\n\n****minmaxV(D,G) = Ex\u223cpdata(x)[logD(x|y)]+Ez\u223cpz(z)[log(1\u2212D(G(z|y)))].****","3e86f818":"**Refrences**\n* 1.https:\/\/learning.oreilly.com\/library\/view\/gans-in-action\/9781617295560\/\n* 2.[https:\/\/arxiv.org\/pdf\/1411.1784.pdf](http:\/\/)"}}