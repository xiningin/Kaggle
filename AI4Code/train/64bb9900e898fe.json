{"cell_type":{"99255525":"code","b1eb5932":"code","1ab1c2da":"code","586f615c":"code","8b796461":"code","6f9ba3c9":"code","4ecd5279":"code","43b50a97":"code","5bf99fc1":"code","04471558":"code","903daca0":"code","f7fc8147":"code","164e79c5":"code","1c2a16c6":"code","c583254a":"code","2bc254c5":"code","05ff3bbf":"code","f60c4ab9":"code","a8c129b5":"code","10d2e82f":"code","17a83e88":"code","9a4047b8":"code","9a639a19":"code","81191658":"code","ccbb45d2":"code","0516fcfd":"code","fcbe627b":"code","0dfccb18":"code","1ab30bbf":"code","58485ee3":"code","b4e836c5":"code","f3781db6":"code","48212e6a":"code","a9e6b817":"code","f9f564ac":"code","e98f1bb4":"code","e3e4b371":"code","da298c8a":"code","aec6839e":"code","d627ada3":"code","bc65eed1":"code","6ff8d856":"code","beacfe90":"code","bb13fe14":"code","95d3099c":"code","db76de14":"code","100d6610":"code","920f8117":"code","05140750":"code","d6638735":"code","f81dbc39":"code","6588be58":"code","534f0fff":"markdown","7ab027d9":"markdown"},"source":{"99255525":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nfrom collections import defaultdict\n# Visualization\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n\n# Modeling\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.datasets import make_regression\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n# Metrics\nfrom sklearn.metrics import r2_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/cryptocurrencypricehistory'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.simplefilter('ignore')","b1eb5932":"pd.set_option('max_colwidth', 200)","1ab1c2da":"# Download data\ndata = pd.read_csv('..\/input\/wq-southern-bug-river-01052021\/PB_All_2000_2021.csv', sep=';', header=0)\ndata","586f615c":"# Download data about monitoring stations\ndata_about = pd.read_csv('..\/input\/wq-southern-bug-river-01052021\/PB_stations.csv', sep=';', header=0, encoding='cp1251')\ndata_about.sort_values(by=['length'], ascending=False)","8b796461":"data['id'].value_counts().sort_values().plot(kind='barh')","6f9ba3c9":"data['ds'] = pd.to_datetime(data['date'])\ndata['year'] = data['ds'].dt.year\ndata.info()","4ecd5279":"data[['id', 'year']].groupby(by=['id']).min().sort_values(by=['year'], ascending=False)","43b50a97":"data[['id', 'year']].groupby(by=['id']).max().sort_values(by=['year'], ascending=False)","5bf99fc1":"\nstations_good = [14,15]\ndata_about[data_about['id'].isin(stations_good)]","04471558":"target_data_name = 'Suspended'\n#feature_target_all = ['NH4', 'BSK5', 'NO3', 'NO2', 'SO4', 'PO4', 'CL']\n#feature_target_all = ['Suspended', 'BSK5','NO3', 'NO2', 'SO4', 'PO4', 'CL']\n#feature_target_all = ['Suspended', 'BSK5', 'NO2']\nfeature_target_all = ['Suspended']\nfeature_data_all = feature_target_all #+ [target_data_name]\nfeature_data_all","903daca0":"df_indicator = data[['id', 'ds'] + feature_data_all]\ndf_indicator = df_indicator[df_indicator['id'].isin(stations_good)].dropna().reset_index(drop=True)\ndf_indicator","f7fc8147":"cols = []\nfor station in stations_good:\n    for feature in feature_data_all:\n        cols.append(str(station) + \"_\" + feature)\ncols","164e79c5":"df = pd.pivot_table(df_indicator, index=[\"ds\"], columns=[\"id\"], values=feature_data_all).dropna()\ndf.columns = cols\ndf","1c2a16c6":"df.info()","c583254a":"df.plot(figsize=(12,10))","2bc254c5":"df.describe([.05, .5, .96])","05ff3bbf":"cols_anomal = df.columns.tolist()   # All features\ncols_anomal.remove(target_name)     # All features without target\nprint(cols_anomal)","f60c4ab9":"df_len0 = len(df)\nfor col in cols_anomal:\n    df = df[df[col] <= float(df.quantile([.96])[col])]\ndf = df.reset_index(drop=True)\nprint(f\"The number of observational data before filtering anomalies - {df_len0} and the number after - {len(df)}\")\nprint(f\"It is desirable that after filtering the anomalies there is enough data: at least 65% - {int(0.65*df_len0)}\")\ndf.describe()","a8c129b5":"df.plot(figsize=(12,10))","10d2e82f":"#pp.ProfileReport(df)","17a83e88":"target_data_name = 'Suspended'\ntarget_name = '14_' + target_data_name\ntarget_data = df.pop(target_name)\ntarget_data","9a4047b8":"# Dividing data into training and test\ntrain, test, target, target_test = train_test_split(df, target_data, test_size=0.25, random_state=0)\nprint(train.shape, test.shape)","9a639a19":"# Standartization data\nscaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n\n# Display training data\ntrain","81191658":"test = pd.DataFrame(scaler.transform(test), columns = test.columns)","ccbb45d2":"train_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.2, random_state=0)","0516fcfd":"cv_train = ShuffleSplit(n_splits=3, test_size=0.5, random_state=0)\n#cv_train = KFold(n_splits=5, shuffle=True, random_state=0)","fcbe627b":"cv_train","0dfccb18":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['Linear Regression', 'Random Forest Regressor', \n                                  'XGBoost Regressor', 'LGB', 'Average prediction'], \n                       'train_score': 0, 'valid_score': 0})\nresult","1ab30bbf":"# Linear Regression\nlr = LinearRegression()\nlr.fit(train, target_train)\n\n# Prediction for training data\ny_train_lr = lr.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_lr), 2)\nprint(f'Accuracy of Linear Regression model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Linear Regression', 'train_score'] = r2_score_acc","58485ee3":"# Print rounded r2_lr = lr.predict(valid)\ny_val_lr = lr.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_lr),2)\nresult.loc[result['model'] == 'Linear Regression', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of Linear Regression model prediction for valid dataset is {r2_score_acc_valid}')","b4e836c5":"lgb_train = lgb.Dataset(train, target_train)\nlgb_eval = lgb.Dataset(valid, target_valid,  reference=lgb_train)\n\nparams = {\n    'boosting_type': 'dart',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'num_leaves': 31,\n    'learning_rate': 0.4,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 0\n}\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=100,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=10)\n\ny_train_lgb = gbm.predict(train)\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_lgb), 2)\nprint(f'Accuracy of Linear Regression model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'LGB', 'train_score'] = r2_score_acc","f3781db6":"#ret = mean_squared_error(valid, gbm.predict(target_valid))\ny_val_lgb = gbm.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_lgb),2)\nresult.loc[result['model'] == 'LGB', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of LGB model prediction for valid dataset is {r2_score_acc_valid}')\n#self.assertLess(ret, 16)\n#self.assertAlmostEqual(evals_result['valid_0']['l2'][-1], ret, places=5)","48212e6a":"ax = lgb.plot_importance(gbm, max_num_features=40, figsize=(15,15))\nplt.show()","a9e6b817":"%%time\n# XGBoost Regressor\nxgbr = xgb.XGBRegressor() \n# parameters = {'n_estimators': [60, 70, 80, 90, 95, 100, 105, 110, 120, 130, 140], \n#               'learning_rate': [0.005, 0.01, 0.05, 0.075, 0.1],\n#               'max_depth': [3, 5, 7, 9],\n#               'reg_lambda': [0.1, 0.3, 0.5]}\n\nparameters = {'n_estimators': [50, 60 ], \n              'learning_rate': [0.1, 0.15],\n              'max_depth': [5, 9],\n              'reg_lambda': [0.1, 0.3]}\n\n# Training model\nxgb_CV = GridSearchCV(estimator=xgbr, param_grid=parameters, cv=cv_train, n_jobs=-1)\nxgb_CV.fit(train, target_train)\nprint(\"Best score: %0.3f\" % xgb_CV.best_score_)\nprint(\"Best parameters set:\", xgb_CV.best_params_)\n\n# Prediction for training data\ny_train_xgb = xgb_CV.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_xgb),2)\nprint(f'Accuracy of XGBoost Regressor model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'XGBoost Regressor', 'train_score'] = r2_score_acc","f9f564ac":"y_val_xgb = xgb_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_xgb),2)\nresult.loc[result['model'] == 'XGBoost Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of XGBoost Regressor model prediction for valid dataset is {r2_score_acc_valid}')","e98f1bb4":"xgbr = xgb.XGBRegressor(**xgb_CV.best_params_)\nxgbr.fit(train, target_train)\nfig =  plt.figure(figsize = (10,8))\naxes = fig.add_subplot(111)\nxgb.plot_importance(xgbr,ax = axes,height = 0.5)\nplt.show();\nplt.close()","e3e4b371":"%%time\n# Random Forest Regressor\nrf = RandomForestRegressor()\nparam_grid = {'n_estimators': [15, 20, 25], 'min_samples_leaf': [i for i in range(4,7)], \n              'max_features': ['auto'], 'max_depth': [i for i in range(3,6)], \n              'criterion': ['mse'], 'bootstrap': [False]}\n\n# Training model\nrf_CV = GridSearchCV(rf, param_grid=param_grid, cv=cv_train, verbose=False)\nrf_CV.fit(train, target_train)\nprint(rf_CV.best_params_)\n\n# Prediction for training data\ny_train_rf = rf_CV.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_rf),2)\nprint(f'Accuracy of RandomForestRegressor model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Random Forest Regressor', 'train_score'] = r2_score_acc","da298c8a":"# Print rounded r2_score_acc to 2 decimal values after the text\ny_val_rf = rf_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_rf),2)\nresult.loc[result['model'] == 'Random Forest Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of RandomForestRegressor model prediction for valid dataset is {r2_score_acc_valid}')","aec6839e":"def agg(x1, x2, x3):\n    # Aggregation of x1, x2 and x3 predictions\n    #return (x1 + x2 + x3) \/ 3\n    #return list(np.maximum(np.array(x1), np.array(x2), np.array(x3)))\n    return list(np.minimum(np.array(x1), np.array(x2), np.array(x3)))","d627ada3":"# Average prediction for training dataset\ny_train = agg(y_train_lr, y_train_rf, y_train_xgb)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train),2)\nprint(f'Accuracy of Average prediction is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Average prediction', 'train_score'] = r2_score_acc","bc65eed1":"# Average prediction for validation dataset\ny_val = agg(y_val_lr, y_val_rf, y_val_xgb)\nr2_score_acc_valid = round(r2_score(target_valid, y_val),2)\nprint(f'Accuracy of Average prediction for valid dataset is {r2_score_acc_valid}')\nresult.loc[result['model'] == 'Average prediction', 'valid_score'] = r2_score_acc_valid","6ff8d856":"# Prediction of target for test data for all models\ny_test_lr = lr.predict(test)\ny_test_rf = rf_CV.predict(test)\ny_test_xgb = xgb_CV.predict(test)\ny_test_lgb = gbm.predict(test)\ny_test = agg(y_test_lr, y_test_rf, y_test_xgb)","beacfe90":"y_test_lgb = gbm.predict(test)\nr2_score_acc_valid = round(r2_score(target_test, y_test_lgb),2)\nresult.loc[result['model'] == 'Random Forest Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of RandomForestRegressor model prediction for valid dataset is {r2_score_acc_valid}')","bb13fe14":"x = np.arange(len(train))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_train, label = \"Target training data\", color = 'k')\nplt.scatter(x, y_train_lr, label = \"Linear Regression prediction\", color = 'b')\nplt.scatter(x, y_train_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_train_xgb, label = \"XGBoost Regressor prediction\", color = 'brown')\nplt.scatter(x, y_train_lgb, label = \"LGBM\", color = 'pink')\nplt.scatter(x, y_train, label = \"Average prediction\", color = 'g')\nplt.plot(x, np.full(len(train), 15), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the training data')\nplt.legend(loc='best')\nplt.grid(True)","95d3099c":"# Building plot for prediction for the valid data \nx = np.arange(len(valid))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_valid, label = \"Target valid data\", color = 'k')\nplt.scatter(x, y_val_lr, label = \"Linear Regression prediction\", color = 'b')\nplt.scatter(x, y_val_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_val_xgb, label = \"XGBoost Regressor prediction\", color = 'brown')\nplt.scatter(x, y_val_lgb, label = \"LGBM\", color = 'pink')\nplt.scatter(x, y_val, label = \"Average prediction\", color = 'g')\nplt.plot(x, np.full(len(valid), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the valid data')\nplt.legend(loc='best')\nplt.grid(True)","db76de14":"x = np.arange(len(test))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_test, label = \"Target test data\", color = 'k')\nplt.scatter(x, y_test_lr, label = \"Linear Regression prediction\", color = 'b')\nplt.scatter(x, y_test_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_test_xgb, label = \"XGBoost Regressor prediction\", color = 'brown')\nplt.scatter(x, y_test_lgb, label = \"LGBM\", color = 'pink')\nplt.scatter(x, y_test, label = \"Average prediction\", color = 'g')\nplt.plot(x, np.full(len(test), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the test data')\nplt.legend(loc='best')\nplt.grid(True)","100d6610":"x = np.arange(len(test))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_test, label = \"Target test data\", color = 'k')\nplt.scatter(x, y_test_lgb, label = \"LGBM\", color = 'pink')\nplt.plot(x, np.full(len(test), 15), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the test data')\nplt.legend(loc='best')\nplt.grid(True)","920f8117":"\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","05140750":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 0.05]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","d6638735":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","f81dbc39":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']\n","6588be58":"print(f'The best model is \"{best_model_name}\"')","534f0fff":"LGBM","7ab027d9":"\u041c\u043e\u0434\u0435\u043b\u044e\u0432\u0430\u043d\u043d\u044f "}}