{"cell_type":{"9cfc508f":"code","10f74348":"code","8853db59":"code","7abe7528":"code","4b17486b":"code","69c4aafa":"code","719fb58b":"code","d7686b85":"code","aeca8975":"code","cbb9157a":"code","8e68aa36":"code","c0353d31":"code","929a556b":"code","b1d413bc":"code","b1e1f5d7":"code","184110a7":"code","cab8d1b6":"code","1d33ef03":"code","22a3d3a1":"code","73e0a26b":"code","d1df409a":"code","f548cf0a":"code","917b379f":"code","9397affe":"code","0c61a09a":"code","6face465":"code","96107b22":"code","8577d4aa":"code","68c890f7":"code","6013dd92":"code","72930c70":"code","80cc4c69":"code","1db430d8":"code","36c3be84":"code","218b5633":"code","6464b925":"code","985e08df":"code","84e6d60b":"code","1786f9dd":"code","3be14261":"code","4a7f1db0":"code","679d6772":"code","bc394ab0":"code","8b0f7f8a":"code","4d64cf85":"code","c4467eec":"markdown","4ff6bb91":"markdown","ab5279c6":"markdown","5c76e571":"markdown","43816fd7":"markdown","c4457b9c":"markdown","cfca42d7":"markdown","1c923948":"markdown","88111860":"markdown","9f6fbf53":"markdown","7a43a376":"markdown","4745bb66":"markdown","655ff25b":"markdown","c9fb5000":"markdown","70835a61":"markdown","69394f82":"markdown","5b887241":"markdown","ca010427":"markdown","ae5f2c18":"markdown","a119741e":"markdown","61dca022":"markdown","1e2b6451":"markdown","09c6325c":"markdown","108124b8":"markdown","7e8505ae":"markdown","12cca952":"markdown","37c70c34":"markdown","668bdfa2":"markdown","68f81af7":"markdown","eea6c178":"markdown","22abb0d6":"markdown","63d5f156":"markdown","e2c02677":"markdown","c320bedb":"markdown","a5db4442":"markdown","9b7f099a":"markdown","8dcde0b4":"markdown","31794066":"markdown","0fbf6abc":"markdown","e02f4966":"markdown","4a7646e8":"markdown","5a6f4f0b":"markdown","17618940":"markdown","729c8e54":"markdown","9be6dfc2":"markdown","9e26bff7":"markdown","2ce52d62":"markdown","a37e4f1f":"markdown","2ed3c946":"markdown","43489745":"markdown","b9598654":"markdown","813c8395":"markdown","61474bea":"markdown","cb3c1db9":"markdown","107e6877":"markdown","cdf9eea9":"markdown","3a265766":"markdown","e721eb15":"markdown","e8ec770d":"markdown","57dc4d3c":"markdown","2527d13b":"markdown","2c8216ad":"markdown","b82cd4be":"markdown","de4989e1":"markdown","ebbb8bfe":"markdown","591fbc8b":"markdown","ad2c9d95":"markdown","aedab295":"markdown","e43fdacc":"markdown","8080eb54":"markdown","c6043e16":"markdown","64802387":"markdown","aedf2f53":"markdown","7e99f754":"markdown","07dd5fb8":"markdown","2c7d043a":"markdown","dfb925c9":"markdown","b575b4ad":"markdown","c584d98b":"markdown","461d1bc5":"markdown","8389a736":"markdown","f0e49a03":"markdown","5cdb7fc8":"markdown","079a4a80":"markdown","e1f71147":"markdown","13e5eb73":"markdown","1305f574":"markdown","3b21b74f":"markdown","2b4936d9":"markdown","8c3cf1a3":"markdown","967b4f46":"markdown","66654f9f":"markdown","0c9d0204":"markdown","6e708142":"markdown","8d40b7a1":"markdown","42eae061":"markdown","3f90ebd2":"markdown","40370ac1":"markdown","61f104ae":"markdown","6da8690d":"markdown"},"source":{"9cfc508f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import r2_score,mean_squared_error,confusion_matrix,classification_report,roc_auc_score,recall_score, precision_score, f1_score, accuracy_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","10f74348":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv', index_col=0)\ndf.head(20)","8853db59":"df.shape","7abe7528":"df.info()","4b17486b":"df.isnull().sum()","69c4aafa":"df.drop('Research', axis=1).describe()","719fb58b":"plt.figure(figsize=(10,6))\nplt.hist(df['Chance of Admit '])\nplt.title('Distribution of Target (Chance of Admit)')\npd.DataFrame(df['Chance of Admit '].describe()).T","d7686b85":"plt.figure(figsize=(10,6))\nplt.hist(df['GRE Score'], bins=20)\nplt.title('GRE Score Distribution')\npd.DataFrame(df['GRE Score'].describe()).T","aeca8975":"plt.figure(figsize=(10,6))\nplt.hist(df['TOEFL Score'], bins=12)\nplt.title('TOEFL Score Distribution')\npd.DataFrame(df['TOEFL Score'].describe()).T","cbb9157a":"plt.figure(figsize=(8,6))\nsns.countplot(df['University Rating'])\npd.DataFrame(df['University Rating'].astype('O').describe()).T","8e68aa36":"plt.figure(figsize=(10,6))\nsns.countplot(df['SOP'])\npd.DataFrame(df['SOP'].astype('O').describe()).T","c0353d31":"plt.figure(figsize=(10,6))\nsns.countplot(df['LOR '])\npd.DataFrame(df['LOR '].astype('O').describe()).T","929a556b":"plt.figure(figsize=(10,6))\nplt.hist(df['CGPA'])\nplt.title('GCPA Score Distribution')\npd.DataFrame(df['CGPA'].describe()).T","b1d413bc":"plt.figure(figsize=(6,6))\nsns.countplot(df['Research'])\npd.DataFrame(df['Research'].astype('O').describe()).T","b1e1f5d7":"pd.DataFrame(df.corr()['Chance of Admit '])[:-1]","184110a7":"sns.pairplot(df, hue='Research')","cab8d1b6":"plt.subplots(1,2 , figsize = (16,6))\nplt.subplot(1,2,1)\nsns.scatterplot(df['GRE Score'], df['Chance of Admit '])\nplt.subplot(1,2,2)\nsns.lineplot(df['GRE Score'], df['Chance of Admit '])","1d33ef03":"plt.subplots(1,2 , figsize = (16,6))\nplt.subplot(1,2,1)\nsns.scatterplot(df['TOEFL Score'], df['Chance of Admit '])\nplt.subplot(1,2,2)\nsns.lineplot(df['TOEFL Score'], df['Chance of Admit '])","22a3d3a1":"Uni_Mean = df['Chance of Admit '].groupby(df['University Rating']).agg(['mean'])\nsns.lineplot(df['University Rating'],df['Chance of Admit '])\nUni_Mean.plot(kind='bar')\nplt.ylabel('Mean Chances of Admit')\nUni_Mean","73e0a26b":"plt.subplots(2,2, figsize = (16,6))\nplt.subplot(1,2,1)\nsns.scatterplot(df['SOP'], df['Chance of Admit '])\nplt.subplot(1,2,2)\nsns.lineplot(df['SOP'], df['Chance of Admit '])","d1df409a":"plt.subplots(2,2, figsize = (16,6))\nplt.subplot(1,2,1)\nsns.scatterplot(df['LOR '], df['Chance of Admit '])\nplt.subplot(1,2,2)\nsns.lineplot(df['LOR '], df['Chance of Admit '])\ndf['LOR '].value_counts().tail(1)","f548cf0a":"plt.figure(figsize=(10,6))\nsns.scatterplot(df['CGPA'], df['Chance of Admit '])","917b379f":"Research_Mean = df['Chance of Admit '].groupby(df['Research']).agg(['mean'])\nResearch_Mean.plot(kind='bar')\nplt.ylabel('Mean Chance of Admit')\nResearch_Mean","9397affe":"pd.DataFrame(df.corr()['GRE Score'][1:-1])","0c61a09a":"plt.subplots(1,2,figsize=(16,6))\nplt.subplot(1,2,1)\nsns.scatterplot(df['GRE Score'],df['TOEFL Score'], color='r')\nplt.subplot(1,2,2)\nsns.lineplot(df['GRE Score'],df['University Rating'], color='g')\nplt.subplots(1,2,figsize=(16,6))\nplt.subplot(1,2,1)\nsns.lineplot(df['GRE Score'],df['SOP'], color='y')\nplt.subplot(1,2,2)\nsns.lineplot(df['GRE Score'],df['LOR '], color='b')\nplt.show()\nplt.figure(figsize=(7,6))\nsns.scatterplot(df['GRE Score'],df['CGPA'], color='y')\npd.DataFrame(df.groupby('Research')['GRE Score'].agg(['mean']))","6face465":"x = df.drop('Chance of Admit ', axis=1)\ny = df['Chance of Admit ']\n\nx_scaled = pd.DataFrame(StandardScaler().fit_transform(x), columns = x.columns)\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 1)\nx_train_s, x_test_s, y_train, y_test = train_test_split(x_scaled,y, test_size = 0.3, random_state = 1)\n\nprint(\"Training Set :\", x_train.shape[0], \"entries\")\nprint(\"Testing Set :\", x_test.shape[0], \"entries\")","96107b22":"l=[]\n\ndef models_lr(x,y):\n    mod = {}\n    model = LinearRegression().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Linear Regression'\n    mod['Train_Score'] = model.score(x,y)\n    mod['Test_accuracy'] = r2_score(y_test,ypred)\n    mod['RMSE'] = np.sqrt(mean_squared_error(y_test,ypred))\n    return mod\nl.append(models_lr(x_train,y_train))\n\ndef models_lr(x,y):\n    mod = {}\n    model = LinearRegression().fit(x,y)\n    ypred = model.predict(x_test_s)\n    mod['Model'] = 'Linear Regression with Scaled Data'\n    mod['Train_Score'] = model.score(x,y)\n    mod['Test_accuracy'] = r2_score(y_test,ypred)\n    mod['RMSE'] = np.sqrt(mean_squared_error(y_test,ypred))\n    return mod\nl.append(models_lr(x_train_s,y_train))\n\ndef models_dt(x,y):\n    mod = {}\n    model = DecisionTreeRegressor().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Decison Tree'\n    mod['Train_Score'] = model.score(x,y)\n    mod['Test_accuracy'] = r2_score(y_test,ypred)\n    mod['RMSE'] = np.sqrt(mean_squared_error(y_test,ypred))\n    return mod\nl.append(models_dt(x_train,y_train))\n\ndef models_rf(x,y):\n    mod = {}\n    model = RandomForestRegressor().fit(x,y)\n    ypred = model.predict(x_test)\n    mod['Model'] = 'Random Forest'\n    mod['Train_Score'] = model.score(x,y)\n    mod['Test_accuracy'] = r2_score(y_test,ypred)\n    mod['RMSE'] = np.sqrt(mean_squared_error(y_test,ypred))\n    return mod\nl.append(models_rf(x_train,y_train))\n\ndef models_rf(x,y):\n    mod = {}\n    model = KNeighborsRegressor().fit(x,y)\n    ypred = model.predict(x_test_s)\n    mod['Model'] = 'KNN'\n    mod['Train_Score'] = model.score(x,y)\n    mod['Test_accuracy'] = r2_score(y_test,ypred)\n    mod['RMSE'] = np.sqrt(mean_squared_error(y_test,ypred))\n    return mod\nl.append(models_rf(x_train_s,y_train))\n\npd.DataFrame(l)","8577d4aa":"x_train_c = sm.add_constant(x_train)\n\nvif = [ variance_inflation_factor(x_train_c.values, i ) for i in range(x_train_c.shape[1])]\nvif_df = pd.DataFrame({'vif' : vif[1:]} , index = x_train.columns)\nvif_df","68c890f7":"for i in range(1,df.shape[1]):\n   \n    rfe = RFE(LinearRegression(),i).fit(x,y)\n    print(x.columns[rfe.support_])\n    x_train1,x_test1,y_train,y_test = train_test_split(rfe.transform(x),y,test_size=0.3,random_state=1)\n    LR = LinearRegression()\n    LR.fit(x_train1,y_train)\n    y_pred1 = LR.predict(x_test1)\n    print(\"RMSE : \",np.sqrt(mean_squared_error(y_test,y_pred1)))\n    print(\"Accuracy : \",r2_score(y_test,y_pred1))\n    print('*'*100)\n    \nparams = [{'n_features_to_select' : list(range(1,8))}]\nmodel_cv = GridSearchCV(estimator = rfe, param_grid= params, scoring='r2', cv=5)\nmodel_cv.fit(x_train1, y_train)\nprint()\nprint(\"Number of Features : \",model_cv.best_params_);print()\nprint('*'*100); print()\n\nrfe1 = RFE(LinearRegression(), n_features_to_select = 6).fit(x_train1,y_train)\ny_pred2 = rfe1.predict(x_test)\nprint(\"Train Accuracy after RFE feature selection :\", rfe1.score(x_train1,y_train))\nprint(\"Test Accuracy after RFE feature selection :\", r2_score(y_test,y_pred2))\nprint(\"RMSE after RFE feature selection :\", np.sqrt(mean_squared_error(y_test,y_pred2)))\nprint(); print('*'*100)\n\ncols = pd.DataFrame({'features': x.columns, 'selection' : rfe1.support_, 'rank' : rfe1.ranking_ })\ncols","6013dd92":"pd.DataFrame(df.corr()['Chance of Admit '][:-1])","72930c70":"df[['GRE Score','TOEFL Score', 'Chance of Admit ']].corr()","80cc4c69":"df[['GRE Score','University Rating', 'Chance of Admit ']].corr()","1db430d8":"df[['GRE Score','SOP', 'Chance of Admit ']].corr()","36c3be84":"df[['GRE Score','LOR ', 'Chance of Admit ']].corr()","218b5633":"df[['GRE Score','CGPA', 'Chance of Admit ']].corr()","6464b925":"df[['CGPA','Research', 'Chance of Admit ']].corr()","985e08df":"x_PC = df[['CGPA']]\nx_train_PC, x_test_PC, y_train, y_test = train_test_split(x_PC, y, test_size= 0.3, random_state = 1)\n\nLR_PC = LinearRegression().fit(x_train_PC, y_train)\ny_pred_PC = LR_PC.predict(x_test_PC)\n\nprint(\"Train Accuracy After Pearson Correlation : \", LR_PC.score(x_train_PC, y_train))\nprint(\"Test Accuracy After Pearson Correlation : \", r2_score(y_test, y_pred_PC))\nprint(\"RMSE After Pearson Correlation : \",np.sqrt(mean_squared_error(y_test, y_pred_PC)))","84e6d60b":"cols = list(x.columns)\npmax=1\nwhile(len(cols)>0):\n    p = []\n    x_1 = x[cols]\n    x_1 = sm.add_constant(x_1)\n    model = sm.OLS(y, x_1).fit()\n    p = pd.Series(model.pvalues.values[1:], index = cols)\n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax > 0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break       \nselected_columns = cols\nprint(\"Choosed Features : \",selected_columns)","1786f9dd":"x_back = df[['GRE Score', 'TOEFL Score', 'LOR ', 'CGPA', 'Research']]\nx_train_back, x_test_back, y_train, y_test = train_test_split(x_back, y, test_size= 0.3, random_state = 1)\n\nLR_back = LinearRegression().fit(x_train_back, y_train)\ny_pred_back = LR_back.predict(x_test_back)\n\nprint(\"Train Accuracy After Backward Elimination : \", LR_back.score(x_train_back, y_train))\nprint(\"Test Accuracy After Backward Elimination : \", r2_score(y_test, y_pred_back))\nprint(\"RMSE After Backward Elimination : \",np.sqrt(mean_squared_error(y_test, y_pred_back)))","3be14261":"for i in [100,10,1,0.1, 0.01, 0.001, 0.0001,0.00001, 0.000001]:\n    rr = Ridge(alpha=i) \n    rr.fit(x_train_back, y_train) \n    print(\"Rigde with Learning Rate of\",i,\":\\n\")\n    print(\"Train Accuracy After Rigde : \", rr.score(x_train_back, y_train))\n    print(\"Test Accuracy After Rigde : \",rr.score(x_test_back, y_test),'\\n')\n    print(\"-\"*100,\"\\n\")\n    ","4a7f1db0":"for i in [0.01, 0.001, 0.0001,0.00001, 0.000001]:\n    lasso = Lasso(alpha=i) \n    lasso.fit(x_train_back, y_train) \n    print(\"Lasso with Learning Rate of\",i,\":\\n\")\n    print(\"Train Accuracy After Lasso : \", lasso.score(x_train_back, y_train))\n    print(\"Test Accuracy After Lasso : \",lasso.score(x_test_back, y_test),'\\n')\n    print(\"-\"*100,\"\\n\")","679d6772":"cv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], \n                        normalize=True,copy_X=True, positive=False,)\ncv_model.fit(x_train_back, y_train)\n\nEN = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\nEN.fit(x_train_back, y_train)\n\nprint(\"Train Accuracy After ElasticNet : \", EN.score(x_train_back, y_train))\nprint(\"Test Accuracy After ElasticNet : \",r2_score(y_test, EN.predict(x_test_back)))","bc394ab0":"Bagg = BaggingRegressor().fit(x_train_back, y_train)\ny_pred_bagg = Bagg.predict(x_test_back)\n\nprint(\"Train Accuracy After Bagging : \", Bagg.score(x_train_back, y_train))\nprint(\"Test Accuracy After Bagging : \", r2_score(y_test, y_pred_bagg))\nprint(\"RMSE After Bagging : \",np.sqrt(mean_squared_error(y_test, y_pred_bagg)))","8b0f7f8a":"AdaBoost = AdaBoostRegressor().fit(x_train_back, y_train)\ny_pred_ada = AdaBoost.predict(x_test_back)\n\nprint(\"Train Accuracy After AdaBoost : \", AdaBoost.score(x_train_back, y_train))\nprint(\"Test Accuracy After AdaBoost : \", r2_score(y_test, y_pred_ada))\nprint(\"RMSE After AdaBoost : \",np.sqrt(mean_squared_error(y_test, y_pred_ada)))","4d64cf85":"Gradient = GradientBoostingRegressor().fit(x_train_back, y_train)\ny_pred_grad = Gradient.predict(x_test_back)\n\nprint(\"Train Accuracy After Gradient Boost : \", Gradient.score(x_train_back, y_train))\nprint(\"Test Accuracy After Gradient Boost : \", r2_score(y_test, y_pred_grad))\nprint(\"RMSE After Gradient Boost: \",np.sqrt(mean_squared_error(y_test, y_pred_grad)))","c4467eec":"* Students with research experience have a average of 78.9 % chances of admit while those who doesn't  have any research experience are expected to get 63.5% chance of admit","4ff6bb91":"* Most of the students have secured GCPA between 8.1 and 9\n* More than 50% of the students have CGPA of 8.56 and above","ab5279c6":"# Exploring the Data","5c76e571":"* From the chart, we can view that the students with SOP score of 4 are highest in number.\n* Most of the students have a SOP score ranging between 2.5 to 4.\n* Also more than 50% of the students have got SOP scores of 3.5 and above (value)","43816fd7":"* Since there is a greater correlation b\/w GRE - SOP, and GRE has greater correlation with Chance of Admit, we have to remove SOP","c4457b9c":"* Backward Elimination seems to be the better feature selection method as the test accuracy has improved from 81.5767 % to 81.8942 % although there is a slight decrease in training (82.0984 % to 91.8329 %) and also RMSE has reduced to a certain good amount from 0.06423 to 0.06367","cfca42d7":"### iv) Statistical Inference","1c923948":"### iii) Missing Values","88111860":"* The table above gives us some intuition about all the columns and how are their values spread and other basic statistics values like mean, max, min etc..","9f6fbf53":"* The mean GRE score of Research oriented students is 322 which others had mean GRE score of 309.\n* The plots shows the same as correlaltion table that there is a good positive correlation among features with GRE.\n\n* Multicollearity exists among the features.\n* For ex, University Rating vs SOP Score : We know University Rating has higher correlation with GRE score. Hence higher the rating, higher is the GRE score which in turns leads to higher SOP score as GRE and SOP are positively correlalated.\n* Same is the case with every other predictor feature.","7a43a376":"#### 4) SOP","4745bb66":"* From the chart we can conclude that students with some kind of research are out-numbered by those without any research work","655ff25b":"###### Inference :\n\n* Ridge with learning rate of 0.00001 and  0.000001 gave the highest accuracy among all and also we can notice that more or less the accuracy has neither increased nor decreased after performing Ridge (Same accuracy after feature selection).\n* So, Ridge won't be of much useful here\n","c9fb5000":"* The dataset contains 500 applicants and the corresponding information of their academic scores. The dataset is inspired from the UCLA Graduate Dataset and is owned by Mr.Mohan S Acharya (https:\/\/www.kaggle.com\/mohansacharya).\n\n* The dataset contains seven different featues which are considered important during the application for Masters Programs.\n\nThe features included are :\n\n1. GRE Scores ( out of 340 ) \n2. TOEFL Scores ( out of 120 ) \n3. University Rating ( out of 5 ) \n4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) \n5. Undergraduate GPA ( out of 10 ) \n6. Research Experience ( either 0 or 1 ) \n7. Chance of Admit ( ranging from 0 to 1 )\n\n\n* The dataset can be downloaded from the Kaggle website : https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions\n\n","70835a61":"#### ii) TOEFL Score vs Chance of Admit","69394f82":"* From the chart, we can view that the students from universities that have got a rating of 3 are more in number among those who have applied for MS program.\n* Also more than 50% of the universities have got rating of 3 and above (median value)","5b887241":"* Same as GRE Score, students with a high TOEFL score has a greater chance of getting admit.\n    \n    ","ca010427":"* So, RFE has eliminated 'SOP' from the set of features to be used for modelling.\n* The 'best r2_score' and 'least rmse' in RFE was achieved when only 6 features (excluding SOP) were used.\n* There is a very little decrease in the training (82.0984 % to 82.0965 %) and a good increase in the test accuracy (81.5767 % to 81. 6071 %) and RMSE has reduced a liitle bit.\n* But still, the other 6 features have multi collinearity i.e there is data redundancy in place (when 2 features convey the same information. for ex : we know if students either high GRE or high TOEFL, the chances are high). So, both GRE and TOEFL conveys the same linear information.\n* As we know, one of the assumptions for linear regression is that there shouldn't be multi collinearity.\n* Hence we have to look up for other possible feature selection techniques as well.","ae5f2c18":"### Individual Independent Features vs Target","a119741e":"### ElasticNet","61dca022":"* One of the simplest method for understanding a feature\u2019s relation to the target variable is Pearson correlation coefficient, which measures linear correlation between two variables and pearson correlation is merely a number between -1 and 1.\n* Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable.\n* So, when two features have high correlation among themselves, we can drop that feature which has lesser correlation with the target.\n* Formula to calculate pearson correlation : \n![image.png](attachment:image.png)","1e2b6451":"# Model Building ","09c6325c":"### Feature Selection","108124b8":"##### Inferences : \n\n* The ensemble models especially Bagging and Gradient Boosting have greatly increased the training accuracy but on the other hand the test accuracy and RMSE has gotten worse.\n* So it won't be a great idea to make use of Ensembling techniques to be performed over the dataset.\n* Moreover the model that we got after feature selection is indeed not a weak learner and hence it justifies our assumption.","7e8505ae":"# Final Choosed Model and Conclusion\n\n* We have choosed Linear Regresion after feature selection using Backward Eliminiation as our final Predictive model.\n* We have finally built a model with Training Accuracy of 81.83 % and test accuracy of 81.89 % and with a low RMSE of 0.06367\n* This predictive model would be a huge asset for all those students who are literally confused or worried about their admission acceptance and would give them a clear picture of where they stand in respect to their score in the exams and other aspects.\n* This model can also be used by higher education consulting companies for their analysis and the model would serve them better results.","12cca952":"* Generally considering 5 as the threshold, we can see that all the features came out with VIF values lesser than the threshold.\n* Hence VIF method of feature selection suggests to go with all the features for building models.\n* So, we will try some other feature selection techniques too\n* Hence Accuracy will be still the same as base model (0.815767)","37c70c34":"#### Gradient Boosting","668bdfa2":"#### AdaBoosting","68f81af7":"* Students who have secured higher score for their Statement of Purpose (SOP), have an upper hand in getting an admit.","eea6c178":"* Variance Inflation Factor (VIF) is used to detect the presence of multicollinearity\n* The VIF is widely used as a measure of the degree of multi-collinearity of the i-th independent variable with the other independent variables in a regression model. \n* If we have indepedent variables X1, X2, X3, \u2026 Xi, the VIF for variable X1 can be calculated by running an ordinary least square regression that has X1 as a function of all the other variables X2 \u2026 Xi. \n* The VIF is than computed following Equation :\n![image.png](attachment:image.png)\n* Higher the VIF, higher the R2 which means the variable X1 is collinear with other indepedent X2...Xi variables.\n* As a rule of thumb, VIF values in excess of 5 or 10 are often considered an indication that multicollinearity may by a cause of problem.","22abb0d6":"#### 1) VIF Method","63d5f156":"* The students with high CGPA are likely to get more chance of admit than those who scored low CGPA.","e2c02677":"##### Results After Feature Selection :\n    \nVIF\/ Base Model (all features) :\n\n* Accuracy : 0.815767\t\n* RMSE : 0.064233\n    \nRFE (eliminated SOP) :\n\n* Accuracy : 0.81607\n* RMSE : 0.06418\n\nPearson (only CGPA) :\n\n* Accuracy : 0.79769\n* RMSE : 0.067311\n\nBackward Elimination (eliminated SOP, University Rating) :\n\n* Accuracy : 0.8189428\n* RMSE : 0.06367\n","c320bedb":"* The above charts depicts that chance of admit is higher for the students from top rated universities.\n* Students from universitites rated '5' have a average of whopping 88.8% chances of admit whilst students from '1' rated universities have not a great value of 56.2 % chances","a5db4442":"* All the features have a good positive correlation with the target.\n* GRE tops the list and has a very high correlation of 0.81 and Research has got the table spoon with 0.54 which is still a good correlation value\n* On a whole GRE score will play a vital role in one's chances of getting an MS admit","9b7f099a":"# Prediction of \"Chances of Admit\" of Master Graduate Programs in US Universities from an Indian Perspective","8dcde0b4":"* The training and test accuracy are lower than of RFE and VIF (base model) and RMSE is higher.\n* Hence Pearson Correlation underperforms for this dataset.","31794066":"#### Chance of Admit - Target Variable","0fbf6abc":"### Segregating x & y and generating train- test sets","e02f4966":"# Exploratory Data Analysis (EDA)","4a7646e8":"### Analysis Among Independent Features","5a6f4f0b":"#### Bagging Regressor","17618940":"* All the predictor variables have a good positive correlation with GRE.\n* Hence we can sense there exists multi collinearity among the variables which needs to be taken care of, while model building.\n* So, students who scored higher GRE score will tend to score higher in other aspects too.","729c8e54":"* Recursive Feature Elimination (RFE) as its title suggests recursively removes features, builds a model using the remaining attributes and calculates model accuracy.\n* RFE is able to work out the combination of attributes that contribute to the prediction on the target variable.\n* It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.","9be6dfc2":"#### 4) Backward Elimination","9e26bff7":"* Since there is a greater correlation b\/w GRE - LOR, and GRE has greater correlation with Chance of Admit, we have to remove LOR","2ce52d62":"##### Inference After Feature Selection :\n\n* So finally we have concluded that 'Backward Elimination' is the best feature selection technique fot the dataset as it increased the accuracy and reduced the error.","a37e4f1f":"* Larger group of students have secured GRE score between 308 and 325\n* More than 50% of the students scored more 316    ","2ed3c946":"- So, finally only 'CGPA feature' is choosed by Pierson Correlation method \n* Lets build model using CGPA and check out the results","43489745":"#### 3) University Rating","b9598654":" * There are no null values present in the dataset","813c8395":"* Predominant of the students have got 65% to 75% chances of getting admit. \n* More than 50% of students have got more than 72% chance of admit (using median value)","61474bea":"* The columns are all of numeric datatypes.","cb3c1db9":"#### v) LOR vs Chance of Admit","107e6877":"#### 5) LOR","cdf9eea9":"### ii) Datatypes of the features","3a265766":"* In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n* For more details about backward elimination, please refer : https:\/\/towardsdatascience.com\/backward-elimination-for-feature-selection-in-machine-learning-c6a3a8f8cef4","e721eb15":"### Lasso","e8ec770d":"#### 3) Pearson Correlation Method","57dc4d3c":"* From the distributions, we can infer that students with research experience have got better score in the exams and better SOP, LOP scores and most importantly they are from higher ranked (high rating) universities","2527d13b":"* Similary Students who have secured higher score for their Letter of Recommendation (LOR), have an upper hand in getting an admit.\n* Another interesting thing we've found is that out of 500 students, only one has got poor score of 1 for LOR. Must be a unlucky folk.","2c8216ad":"#### 7) Research","b82cd4be":"# Reading and Understanding the Dataset","de4989e1":"### Ridge","ebbb8bfe":"### i) Shape ","591fbc8b":"## Regularization Techniques","ad2c9d95":"* Most of the students have scored between 103 and 112 in TOEFL exam\n* More than 50% of the students scored TOEFL more 107","aedab295":"#### vii) Research vs Chance of Admit","e43fdacc":"* Since there is a greater correlation b\/w CGPA - Research, and CGPA has greater correlation with Chance of Admit, we have to remove Research","8080eb54":"#### 6) CGPA","c6043e16":"## Ensemble Techniques\n\n* Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n* So lets try out ensemble models and check out if there's an improvement in model accuracy","64802387":"* At the first step, we took first 2 features and examined multicollinearity within them and the process is continued until there is no data redundancy","aedf2f53":"## Bivariate Analysis","7e99f754":"# Importing Required Libraries","07dd5fb8":"* Since there is a greater correlation b\/w GRE - University Rating, and GRE has greater correlation with Chance of Admit, we have to remove University Rating.","2c7d043a":"#### 2) RFE Method","dfb925c9":"* We are choosing Linear Regression as our base model as it has the greater training and testing accuracy and also lower mean squared error than the other models.\n\n","b575b4ad":"### Independent Variables \n\n#### 1) GRE Score","c584d98b":"* Students are often worried about their chances of admission in graduate school.\n* The objective is to determine the most important factors that contribute to a student's chance of admission, and select the most accurate model to predict the probability of admission.\n* The aim here is to help students in shortlisting universities with their profiles by building a predictive machine learning model.\n* The predicted output gives them a fair idea about their admission chances in a particular university. \n* This analysis should also help students who are currently preparing or will be preparing to get a better idea about their chances of getting awarded an admission with respect to their scores and other aspects.","461d1bc5":"#### The Correlation of the predictor variables on target variable ","8389a736":"#### iv) SOP vs Chance of Admit","f0e49a03":"#### i) GRE Score vs Chance of Admit","5cdb7fc8":"### Base Model","079a4a80":"# Problem Statement :","e1f71147":"## Univariate Analysis","13e5eb73":"* As already found from correlation table, the chart too shows that higher the GRE score, higher the chance of getting admit ","1305f574":"* From the chart, we can view that the students with LOR score of 3 are highest in number.\n* Most of the students have a LOR score of either 3, 3.5 or 4\n* Also more than 50% of the students have got LOR scores of 3.5 and above (median value)","3b21b74f":"###### Inference  :\n\n* Both training and test accuracy have decreased after ElasticNet.\n* So much like the the above 2 regularization methods, ElasticNet too won't be useful ","2b4936d9":"* Since there is a greater correlation b\/w GRE - CGPA, and CGPA has greater correlation with Chance of Admit, we have to remove GRE","8c3cf1a3":"#### 2) TOEFL Score","967b4f46":"* Regularization is generally performed to avoid over fitting of the data.\n* Eventhough there is no overfitting in our model, still lets try out the regularization models to see that whether the accuracy gets better.","66654f9f":"# Final Model and Optimizations","0c9d0204":"#### 1) GRE vs other independent features","6e708142":"* The dataset contains 500 entries and 8 features (exlcuding Serial Number)","8d40b7a1":"#### vi) GCPA vs Chance of Admit","42eae061":"#### iii) University Rating vs Chance of Admit","3f90ebd2":"* I've made Serial Number as the index.\n* Serial number is just like a unique number to each student entry just similar to primary key in SQL.\n* Serial number won't play any role in the prediction model\n* Hence instead of removing it, we've made that as the index so that it can be intrepreted in future if neccessary.","40370ac1":"# Dataset","61f104ae":"###### Inference :\n\n* Lasso with learning rate of 0.000001 gave the highest accuracy among all and also we can notice that the test accuracy has very very minutely decreased after performing lasso ( 81.89428 % to 81.89402 i.e decreased by 0.00026).\n* So, Lassso too more less gave the same accuracy as of feature selection and hence won't be of much useful.","6da8690d":"* Since there is a greater correlation b\/w GRE - TOEFL (above abs(0.5)), and GRE has greater correlation with Chance of Admit, we have to remove TOEFL Score."}}