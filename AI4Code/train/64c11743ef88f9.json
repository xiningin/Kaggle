{"cell_type":{"2dd4d6ba":"code","dd7be7be":"code","821166ef":"code","cfbbd40c":"code","6725f3aa":"code","451a4a51":"code","cdc7ddfa":"code","21716920":"code","370f68c9":"code","e6025e45":"markdown","a0142d25":"markdown","ffd05ebe":"markdown","da134c4e":"markdown","57da8d25":"markdown","5411bc88":"markdown","2a6c7dd9":"markdown","31a17bf1":"markdown","cc99a105":"markdown"},"source":{"2dd4d6ba":"!pip install nltk -q\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nsent= '''Prime Minister Jacinda Ardern has claimed that New Zealand had won a big \nbattle over the spread of coronavirus. Her words came as the country begins to exit from its lockdown.'''\nwords= word_tokenize(sent)\npostags=pos_tag(words)\nne_tree = nltk.ne_chunk(postags,binary=False)\nprint(ne_tree)","dd7be7be":"ne_tree = nltk.ne_chunk(postags,binary=True)\nprint(ne_tree)","821166ef":"!pip install spacy -q\n!python -m spacy download en_core_web_sm","cfbbd40c":"from pprint import pprint\nimport spacy \nfrom spacy import displacy\n  \nnlp = spacy.load('en_core_web_sm') \n  \nsentence = '''Prime Minister Jacinda Ardern has claimed that New Zealand had won a big \nbattle over the spread of coronavirus. Her words came as the country begins to exit from its lockdown.'''\n \n  \nentities= nlp(sentence)\n#to print all the entities with iob tags \npprint([ (X, X.ent_iob_, X.ent_type_) for X in entities] )\n \n#to print just named entities use this code\nprint(\"Named entities in this text are\\n\")\nfor ent in entities.ents: \n    print(ent.text,ent.label_)\n \n# visualize named entities\ndisplacy.render(entities, style='ent', jupyter=True)","6725f3aa":"import random\nfrom spacy.gold import GoldParse\n#the training data with named entity NE needs to be of this format \n# ( \"training example\",[(strat position of NE,end position of NE,\"type of NE\")] )\ntrain_data = [\n    (\"Uber blew through $1 million a week\", [(0, 4, 'ORG')]),\n    (\"Android Pay expands to Canada\", [(0, 11, 'PRODUCT'), (23, 30, 'GPE')]),\n    (\"Spotify steps up Asia expansion\", [(0, 8, \"ORG\"), (17, 21, \"LOC\")]),\n    (\"Google Maps launches location sharing\", [(0, 11, \"PRODUCT\")]),\n    (\"Google rebrands its business apps\", [(0, 6, \"ORG\")]),\n    (\"look what i found on google! \ud83d\ude02\", [(21, 27, \"PRODUCT\")])]\n#An optimizer is set to update the model\u2019s weights.\noptimizer = nlp.begin_training()\nfor itn in range(100):\n    random.shuffle(train_data)\n    for raw_text, entity_offsets in train_data:\n        doc = nlp.make_doc(raw_text)\n        gold = GoldParse(doc, entities=entity_offsets)\n        nlp.update([doc], [gold], drop=0.25, sgd=optimizer)\n#setting drop makes it harder for the model to just memorize the data.\nnlp.to_disk(\"\/model\")","451a4a51":"from pprint import pprint\nsentence_1 = '''i use google to search. Google is a large IT company'''\nsentence_2='''Prime Minister Jacinda Ardern has claimed that New Zealand had won a big \nbattle over the spread of coronavirus. Her words came as the country begins to exit from its lockdown.'''\n  \nentities_1= nlp(sentence_1)\nentities_2=nlp(sentence_2)\n#to print just named entities use this code\nprint(\"Named entities in this text are\\n\")\nfor ent in entities_1.ents: \n    print(ent.text,ent.label_)\nfor ent in entities_2.ents: \n    print(ent.text,ent.label_)","cdc7ddfa":"#Install Apex for Mix Precision\n!cd ..\/input\/apex-master\/apex\/ && pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .","21716920":"!pip install simpletransformers -q","370f68c9":"from simpletransformers.ner import NERModel\n\n\n# Create a NERModel\nmodel = NERModel('bert', 'bert-base-cased')\n# model.args = {\n#     \"output_dir\": \"\/kaggle\/working\/\",\n#     \"cache_dir\": \"\/kaggle\/working\/\",\n\n#     \"fp16\": True,\n#     \"fp16_opt_level\": \"O1\",\n#     \"max_seq_length\": 128,\n#     \"train_batch_size\": 8,\n#     \"gradient_accumulation_steps\": 1,\n#     \"eval_batch_size\": 8,\n#     \"num_train_epochs\": 1,\n#     \"weight_decay\": 0,\n#     \"learning_rate\": 4e-5,\n#     \"adam_epsilon\": 1e-8,\n#     \"warmup_ratio\": 0.06,\n#     \"warmup_steps\": 0,\n#     \"max_grad_norm\": 1.0,\n\n#     \"logging_steps\": 50,\n#     \"save_steps\": 2000,\n\n#     \"overwrite_output_dir\": False,\n#     \"reprocess_input_data\": False,\n#     \"evaluate_during_training\": False,\n\n#     \"process_count\": 2,\n#     \"n_gpu\": 1,\n# }\n# model = NERModel('bert', 'bert-base-cased', args={'learning_rate': 2e-5, 'overwrite_output_dir': True, 'reprocess_input_data': True})\nmodel.train_model('..\/input\/conll003-englishversion\/train.txt')\n\n#read model\n#model = NERModel('bert', 'outputs\/')\n\n#evaluate\nresults, model_outputs, predictions = model.eval_model('..\/input\/conll003-englishversion\/valid.txt')\n\n# Check predictions\nprint(predictions[:5])","e6025e45":"### 3 approches pour la recherche d'entit\u00e9s nomm\u00e9es","a0142d25":"Tutorial comes from https:\/\/towardsdatascience.com\/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0\nThank you for reading.","ffd05ebe":"#### 3eme approche : avec les transformers","da134c4e":"predictions, raw_outputs = model.predict([\"Some arbitary sentence\"])","57da8d25":"Here, the three return values are:\n\n\nresult: Dictionary containing evaluation results. (eval_loss, precision, recall, f1_score)\n\nmodel_outputs: List of raw model outputs\n\npreds_list: List of predicted tags\n\nThe evaluation results I obtained are given here for reference.\n\n{'eval_loss': 0.10684790916955669, 'precision': 0.9023580786026201, 'recall': 0.9153082919914954, 'f1_score': 0.9087870525112148}\n\nNot too shabby for a single run with default hyperparameter values!\n","5411bc88":"model_type: The type of model (bert, roberta)\n\nmodel_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).\n\nlabels (optional): A list of all Named Entity labels. If not given, [\u201cO\u201d, \u201cB-MISC\u201d, \u201cI-MISC\u201d, \u201cB-PER\u201d, \u201cI-PER\u201d, \u201cB-ORG\u201d, \u201cI-ORG\u201d, \u201cB-LOC\u201d, \u201cI-LOC\u201d] will be used.\n\nargs (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n\nuse_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.","2a6c7dd9":"### Approche 1 : avec NLTK","31a17bf1":"As you can see, Jacinda Ardern is chunked together and classified as a person. Also, note that the binary parameter in the ne_chunck has been set to \u2018False\u2019.If this parameter is set to True, the output just points out the named entity as NE  instead of the type of named entity as shown below:","cc99a105":"#### Approche 2  : avec Spacy"}}