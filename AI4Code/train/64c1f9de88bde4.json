{"cell_type":{"31014217":"code","7a0ac2e7":"code","3898f2a8":"code","b857ab70":"code","696c319e":"code","2c51797b":"code","dcd2d94d":"code","a7382ba3":"code","1908f351":"code","d83a013c":"code","4224c39c":"code","6d93cbfa":"code","677772fe":"code","7ba8af49":"code","46cc9892":"code","91dc40bc":"markdown","277931ea":"markdown","2d5afde6":"markdown","052d8515":"markdown","dc217834":"markdown","d9ceaed9":"markdown","137001bc":"markdown","6e9629c7":"markdown","41a184d0":"markdown","6c2c30fb":"markdown","b5b61221":"markdown","1a2b3540":"markdown"},"source":{"31014217":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # data processing\n\nfrom sklearn.model_selection import train_test_split # train_test_split\nfrom sklearn.ensemble import RandomForestClassifier # classifier model\nfrom sklearn.metrics import classification_report # model performance analysis\nfrom sklearn.model_selection import cross_val_score # model performance analysis\n\nimport seaborn as sns # data visualization","7a0ac2e7":"training_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\n# merge the two for convenience in data preprocessing\ndata = pd.concat([training_data, test_data], sort=False)\n\n# Take a quick look at it\ndata.head()","3898f2a8":"# STEP 1\n# Remove unrelated columns\ndata.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)\n\n# STEP 2\n# Before we start, let's take a look at where these missing values are\ndef check_missing_values(data):\n    for col in data:\n        num_of_na = data[col].isna().sum()\n        # Test data does not have 'Survived' column\n        if num_of_na > 0 and col != 'Survived': \n            print('Found', num_of_na, 'missing values in column', col)\n            percentage = num_of_na \/ 1309 * 100    # 1309 instances in total\n            print('That\\'s', '%.2f' % percentage + '% missing.\\n')\n            \ncheck_missing_values(data)","b857ab70":"# Remove column 'Cabin' from data\ndata.drop('Cabin', axis=1, inplace=True)","696c319e":"correlations = data.copy().corr().abs().unstack().sort_values()\ncorrelations = correlations['Age'].drop('Age')\ncorrelations_chart = correlations.plot(kind='barh', title='Correlation With Other Attributes')","2c51797b":"# Relationship with passenger class\nfor pclass in range(1, 4):\n    analysis = data[data['Pclass'] == pclass]\n    mean = analysis['Age'].mean()\n    print('Passengers in class', pclass, 'are on average', round(mean), 'y\/o')\n\n# Relationship with number of siblings and spouse\nfor sibsp in np.sort(data['SibSp'].unique()):\n    analysis = data[data['SibSp'] == sibsp]    \n    mean = analysis['Age'].mean()\n    \n    if np.isnan(mean): \n        print('Average age of passengers with', sibsp, 'siblings and spouse are unknown')\n    else:\n        print('Passengers with', sibsp, 'siblings and spouse are on average', round(mean), 'y\/o')","dcd2d94d":"# if number of sibling and spouse > 2:\n    # age = 16\/9\/10\/14 accordingly\n# else:\n    # age = mean(average(pclass) + average(sibsp))\n    \n# Impute age based on the above log\ndef impute_age(columns):\n    sibsp_age_map = {0.0: 31, \n                     1.0: 31, \n                     2.0: 24, \n                     3.0: 16, \n                     4.0: 9, \n                     5.0: 10,\n                     8.0: 14,}\n    \n    pclass_age_map = {1.0: 39,\n                      2.0: 30,\n                      3.0: 25}\n    age = columns[0]\n    pclass = columns[1]\n    sibsp = columns[2]\n    \n    if pd.isna(age):\n        if sibsp > 2.0:\n            age = sibsp_age_map[sibsp]\n        else:\n            age = (pclass_age_map[pclass] + sibsp_age_map[sibsp]) \/ 2\n            \n    return age\n\n# Now fill in the missing age\ndata['Age'] = data[['Age', 'Pclass', 'SibSp']].apply(impute_age, axis=1)\n\n# Then fill in missing 'Fare' and 'Embarked'\n# Since the amount missing is very small, \n# we simply use 'mode' value (most often seen value)\ndata['Fare'].fillna(data['Fare'].mode()[0], inplace=True)\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)","a7382ba3":"# STEP 3\ngender =  pd.get_dummies(data['Sex'], drop_first=True)\nembarked_location = pd.get_dummies(data['Embarked'], drop_first=True)\ndata = pd.concat([data, gender, embarked_location], axis=1, sort=False)\n\n# Now we can safely drop both the 'Sex' column and the 'Embarked' column\ndata.drop(['Sex', 'Embarked'], axis=1, inplace=True)\n\n# Lastly we have to handle the 'Name' column\n# NOTE: \n# We are only interested in the title of the passenger,\n# as it may have implications with socio-economic status as well as age of the passenger.\n# Title is located between the comma and the period of the name.\ntitles = [name.split(\",\")[1].split(\".\")[0].strip() for name in data[\"Name\"]]\ndata[\"Title\"] = pd.Series(titles)\n\n# Since some titles are really rare, we group them together into one category.\ndata[\"Title\"] = data[\"Title\"] \\\n                .replace(['Lady','the Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n\n# Then we group the equivalent ones\ndata[\"Title\"] = data[\"Title\"].replace('Mlle','Miss')\ndata[\"Title\"] = data[\"Title\"].replace('Ms','Miss')\ndata[\"Title\"] = data[\"Title\"].replace('Mme','Mrs')\n\n# convert using one-hot encoding\none_hot_titles = pd.get_dummies(data['Title'], drop_first=True)\ndata = pd.concat([data, one_hot_titles], axis=1, sort=False)\n\n# Now we can safely remove both the 'Name' column and the 'Title' column\ndata.drop(['Name', 'Title'], axis=1, inplace=True)\n\n# Let's take a look at what our training data becomes!\ndata.head()","1908f351":"# STEP 4\n# In this case, variable 'Fare' is positively skewed.\n# A skewed distribution will affect model's performance.\n# We need to fix the skewness by either applying a boxcox transformation or log transformation.\n\n# First let's take a look\nbefore = sns.distplot(data['Fare'])","d83a013c":"# Apply a log transformation\ndata[\"Fare\"] = data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\n# Let's see what it looks like now\nafter = sns.distplot(data['Fare'])","4224c39c":"# Create column 'FamilySize' based on number of family members\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\n# Now we can safely remove 'SibSp' and 'Parch'\ndata.drop(['SibSp', 'Parch'], axis=1, inplace=True)","6d93cbfa":"# split the data\ntraining_data = data.copy().iloc[:890]\ntest_data = data.copy().iloc[891:].drop('Survived', axis=1)\n\n# 'Survived' column contains all the training labels\nx = training_data.drop('Survived', axis=1)\ny = training_data['Survived']\n\n# take 30% of the training set as validation set\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=1)\n\n# Build our machine learning model\n# Feel free to explore hyperparameters\nmodel = RandomForestClassifier(random_state=0,\n                               n_estimators=450,\n                               criterion='gini',\n                               n_jobs=-1,\n                               max_depth = 8,\n                               min_samples_leaf=1,\n                               min_samples_split= 11)","677772fe":"# Training\nmodel.fit(x_train, y_train)\n# Make prediction on the validation set\npredictions = model.predict(x_val).astype(int)\n\n# Report on how the model performs on the validation set\nprint(classification_report(y_val, predictions))\naccuracies = cross_val_score(estimator=model, X= x_train, y=y_train, cv=10)\n\n# average cross validation accuracy\nprint('Cross validation average: %.4f' % accuracies.mean())","7ba8af49":"feature_importance = pd.DataFrame({'Feature': x_train.columns, 'Importance': model.feature_importances_})\nfeature_importance_chart = feature_importance.sort_values(by='Importance').plot(x='Feature', kind='barh', title='Feature Importance')","46cc9892":"passenger_id = pd.read_csv('..\/input\/test.csv')['PassengerId'].values.tolist()\npredictions = model.predict(test_data).astype(int)\nsubmission = pd.DataFrame({'PassengerId':passenger_id, \n                           'Survived':predictions}).to_csv('prediction.csv', index=False)","91dc40bc":"### After log transformation","277931ea":"### Feature Importance Visualization\nAfter training, we examine feature importance to see what features matter the most","2d5afde6":"## Training and Validation","052d8515":"## Training Preparation","dc217834":"## Preprocess Data\n1. Remove columns that has no effect on prediction of survival. (e.g. : `PassengerId` and `Ticket`)\n2. Filling in missing values with statistical approach.\n3. Convert categorical column using one-hot encoding. (e.g. : convert `male` -> 1 and `female` -> 0 in `Sex` column)\n4. Normalize variables with skewed distributions.","d9ceaed9":"### >2 sibling and spouse indicate possible children\nTherefore we fill missing `Age` by the following logic","137001bc":"## Feature Engineering\nSum `SibSp` and `Parch` into `FamilySize`","6e9629c7":"## Import Data\nRead input data then merge them","41a184d0":"## Apply on Test Data","6c2c30fb":"### Drop `Cabin` since it's over 75% incomplete","b5b61221":"### Explore correlations before filling missing values","1a2b3540":"### Inspired by [Basic Approach for Top 3% in the Titanic](https:\/\/www.kaggle.com\/danielv7\/basic-approach-for-top-3-in-the-titanic)\n\n### Feature Importance Visualization Inspired by [Feature Engineering Tutorial with Titanic](https:\/\/www.kaggle.com\/gunesevitan\/feature-engineering-tutorial-with-titanic)\n\n#### Feel free to comment below and upvote if you find this solution helpful to you in anyway :)"}}