{"cell_type":{"125e2317":"code","31b5b493":"code","e81edbd3":"code","46424dbd":"code","01b34c69":"code","44c0a257":"code","77532196":"code","e8f3447a":"code","f13d0ae2":"code","b0e32e82":"code","3639b40f":"code","7e432bb6":"code","89f3f4ae":"code","e53dcfc9":"code","e99c56c2":"code","2aa672c0":"code","6405142b":"code","815e8a9e":"code","b27ecb4e":"code","a0b297c7":"code","5b21c66f":"code","c37af95a":"code","56f078de":"code","f9d6a491":"code","cc4d2181":"code","7e2bda72":"code","93483c34":"code","bc4694bf":"code","c0097a55":"code","11200f6a":"code","1a14c327":"code","6902058c":"code","09f9d62b":"code","df0b2aae":"code","a794eb1b":"code","141b60f2":"code","cac14056":"code","9169a26d":"code","7aaffe4b":"code","6c1935c5":"code","26da9f6f":"code","5d93b954":"code","67d47ecc":"code","ab84b261":"code","2fa4f162":"code","5d340bf3":"code","d243c69b":"code","ce8d69ff":"code","91fa434c":"code","46559536":"code","2eec1c42":"code","ca0c0bda":"code","a20694ba":"code","c80fa90d":"code","9c2982bf":"code","533f4343":"code","ca680bc7":"code","6d856053":"code","fb786c17":"code","94b1b258":"code","38cd9c12":"code","d1d3a2ab":"code","535a963c":"code","04c07871":"code","457167cf":"code","e8af4b2e":"code","f6f93dff":"code","0c890989":"code","59a9f018":"code","378c1146":"code","fd4b248d":"code","398515b3":"code","2a9445ed":"code","70b601e4":"code","e762c3e5":"code","c5bc74a7":"code","51143281":"code","009b437d":"code","3c0a4ae4":"code","090c35d0":"code","09e7145d":"code","bd76649c":"code","05702926":"code","fd132348":"code","0e3fd0fd":"code","7354130f":"code","64bb08ba":"code","8d3a34fa":"code","92dc31b9":"code","40c2d863":"code","28c2df9c":"code","9b4dab86":"code","6cd3edda":"code","890c37ad":"code","a91baa43":"code","275501f1":"code","e2a5f6c5":"code","544546c4":"code","58745a9b":"code","cab84bdc":"code","8fa07be7":"code","3cec7ca8":"code","a860bf01":"code","4162c4c5":"code","3afb2b20":"code","ac2bec92":"code","591d88f8":"code","d11f75f9":"code","8ba31926":"code","1941037f":"code","895bd90c":"code","597f075c":"code","71010c75":"code","16f85980":"code","e11f7ca4":"code","fa4e0ad8":"code","ef8bf3d7":"code","ec97d609":"code","24aa8ac3":"code","46c39f9c":"code","9263ce02":"code","e9e0331c":"markdown","55113883":"markdown","e4b1d1aa":"markdown","edaede03":"markdown","e2a8ef6e":"markdown"},"source":{"125e2317":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31b5b493":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom sklearn import preprocessing\n\n","e81edbd3":"df = pd.read_csv(\"\/kaggle\/input\/amphibians-data-set\/dataset.csv\",delimiter=';', header=1, index_col='ID')\n","46424dbd":"df.head()\n","01b34c69":"df.dtypes","44c0a257":"len(df)","77532196":"# Replace space to underline\ncols = df.columns\ncols = cols.map(lambda x: x.replace(' ', '_') if isinstance(x, (str)) else x)\ndf.columns = cols\n","e8f3447a":"# checking dataset\ndf.head()\n","f13d0ae2":"# Converting several columns to category or object\ndf2 = df[['Motorway','TR', 'VR', 'SUR1', 'SUR2', 'SUR3', 'UR', 'FR',\n       'OR', 'RR', 'BR', 'MR', 'CR']].astype(\"category\")\ndf = df.merge(df2)\n\ndf","b0e32e82":"# Checking types\ndf.dtypes","3639b40f":"# No Nan values\ndf.isnull().sum()","7e432bb6":"df.describe()","89f3f4ae":"# Copy \ndf2 = df.copy()\ndf2.head()","e53dcfc9":"#  create a sum column of the species\ndf2['Species'] = df2.iloc[:,15:].sum(axis=1)\ndf2.head()","e99c56c2":"# Drop columns frog's species \ndf2=df2.drop(['Green_frogs', 'Brown_frogs','Common_toad',\n              'Fire-bellied_toad','Tree_frog','Common_toad',\n              'Fire-bellied_toad','Tree_frog','Common_newt','Great_crested_newt'], axis=1)\ndf2.head()","2aa672c0":"# Replace value in SUR1 columns\ndf2['SUR1'].replace({1:'forest_areas', 2:'meadows', 4:'gardens',\n                   6:'industrial_areas',10:'river_valleys', 7:'orchards', 9:'roads',\n                   14:'agricultural'}, inplace=True)","6405142b":"df2.SUR1.value_counts()","815e8a9e":"# Replace value in SUR2 columns\ndf2['SUR2'].replace({1:'forest_areas', 2:'meadows',6:'industrial_areas',\n                     10:'river_valleys', 7:'orchards', 9:'roads',\n                     11:'agricultural'}, inplace=True)","b27ecb4e":"df2.SUR2.value_counts()","a0b297c7":"# Replace value in SUR3 columns\ndf2['SUR3'].replace({1:'forest areas', 2:'meadows', 5:'parks',\n                   6:'industrial areas',10:'river valleys', 7:'orchards', 9:'roads',\n                   11:'agricultural'}, inplace=True)","5b21c66f":"df2.SUR3.value_counts()","c37af95a":"# Replace value in UR columns\ndf2['UR'].replace({0:'unused', 1:'scenic', 3:'technological'}, inplace=True)","56f078de":"df2.UR.value_counts()","f9d6a491":"# Replace value in FR columns\ndf2['FR'].replace({0:'lack', 1:'intense fishing', 2:'breeding reservoirs',3:'remove',4:'remove'}, inplace=True)","cc4d2181":"df2.FR.value_counts()","7e2bda72":"# Replace value in OR columns\ndf2['OR'].replace({25:'poor access', 50:'low access', \n                   75:'medium access',100:'large access',99:'remove',80:'remove'}, inplace=True)","93483c34":"df2.OR.value_counts()","bc4694bf":"# Replace value in RR columns\ndf2['RR'].replace({0:'<50 m', 1:'50-100 m', 2:'100-200 m',5:'200-500 m',9:'500-1000 m',10:'>10000'}, inplace=True)","c0097a55":"df2.RR.value_counts()","11200f6a":"# Replace value in BR columns\ndf2['BR'].replace({0:'<50 m', 1:'50-100 m', 2:'100-200 m',5:'200-500 m',9:'500-1000 m',10:'>10000'}, inplace=True)","1a14c327":"df2.BR.value_counts()","6902058c":"# Replace value in MR columns\ndf2['MR'].replace({0:'Clean', 1:'slightly littered', 2:'heavily littered'}, inplace=True)","09f9d62b":"df2.MR.value_counts()","df0b2aae":"# Replace value in CR columns\ndf2['CR'].replace({1:'Natural', 2:'Concrete'}, inplace=True)","a794eb1b":"df2.CR.value_counts()","141b60f2":"df3_dimmies = pd.get_dummies(df2)","cac14056":"df3_dimmies.head()","9169a26d":"# split our dataframe into two other dataframes: numerical and categorical\n\nnumerical = df3_dimmies[['Species','SR','NR']]\ncategorical = df3_dimmies.drop([\"SR\",\"NR\",\"Species\"],axis=1)","7aaffe4b":"# Standardization of numerical data\nscaler = StandardScaler()\nnumerical = pd.DataFrame(scaler.fit_transform(numerical))\nnumerical.columns = [\"Species\",\"SR\",\"NR\"]\nnumerical","6c1935c5":"# Concat dataframes in df4\ndf4 = pd.concat([numerical, categorical], axis=1, join='inner')","26da9f6f":"# Separating the array into input and output components\n\ny= df4.Species.values\ny = y.reshape(-1, 1)\nx=df4.drop([\"Species\"],axis=1)","5d93b954":"# train test split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.30, random_state = 5)","67d47ecc":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR","ab84b261":"# Metrics used\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nfrom matplotlib import pyplot","2fa4f162":"\n# Preparing the list of models\nmodels = []\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Random Forest Regressor', RandomForestRegressor()))\nmodels.append(('Linear Regression', LinearRegression()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNNR', KNeighborsRegressor()))\nmodels.append(('SVR', SVR()))\n\n# Evaluating each model in a loop\nresultados = []\nname = []\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    r2 = r2_score(Y_test, Y_pred)\n    mae = mean_absolute_error(Y_test, Y_pred)\n    mse = mean_squared_error(Y_test, Y_pred)\n    rmse = np.sqrt(mse)\n    print(name)\n    print(\"r2_score: %.2f\" % r2)\n    print(\"mean absolute error: %.2f\" % mae)\n    print(\"mean squared error: %.2f\" % mse)\n    print(\"root mean squared error: %.2f\" % rmse)\n    print(\"\\n\")\n","5d340bf3":"model = RandomForestRegressor(n_estimators=100,max_depth=3,random_state=42)","d243c69b":"model = model.fit(X_train, Y_train)","ce8d69ff":"# Printing feature importances\ncol_feature_importances = []\ncol_name = []\ncol_value = []\nfor coef, var in sorted(zip(map(abs, model.feature_importances_.reshape(-1,1)), (df4.columns[1:])), reverse = True):\n    col_name.append(var)\n    col_value.append(coef)\n    if coef>0.002:\n        col_feature_importances.append(var)\n    print (\"%.4f %s\" % (coef,var))\ncol_feature_importances    ","91fa434c":"# data with several columns to improve r2\ndf_feature_importances = pd.DataFrame(df4, columns= col_feature_importances)","46559536":"# With this selection of attribute importance the result was not as good as the previous one, \n# because the RandomForest Regressor r2 was 0.32 and the previous two was 0.37\n\nY = df4.Species\nX = df_feature_importances\n\n# train test split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\nY_train = Y_train.values.ravel()\n\n#Y_train = Y_train.reshape((n_samples,))\n#Y_test = Y_test.reshape((n_samples,))\n\n# Preparing the list of templates\nmodels = []\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Random Forest Regressor', RandomForestRegressor()))\nmodels.append(('Linear Regression', LinearRegression()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNNR', KNeighborsRegressor()))\nmodels.append(('SVR', SVR()))\n\n# Evaluating each model in a loop\nresultados = []\nname = []\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    r2 = r2_score(Y_test, Y_pred)\n    mae = mean_absolute_error(Y_test, Y_pred)\n    mse = mean_squared_error(Y_test, Y_pred)\n    rmse = np.sqrt(mse)\n    print(name)\n    print(\"r2_score: %.2f\" % r2)\n    print(\"mean absolute error: %.2f\" % mae)\n    print(\"mean squared error: %.2f\" % mse)\n    print(\"root mean squared error: %.2f\" % rmse)\n    print(\"\\n\")\n","2eec1c42":"modelo = RandomForestRegressor()","ca0c0bda":"def r2_est(x,y):\n    return r2_score(y, modelo.fit(x,y).predict(x))","a20694ba":"y = y.ravel()\n","c80fa90d":"print ('Coeficiente R2: %0.3f' %  r2_est(x,y))","9c2982bf":"# impact of each attribute on R2\ncol_imp_r2 = list()\nr2_impact = list()\nfor j in range(x.shape[1]):\n    selection = [i for i in range(x.shape[1]) if i!=j]\n    r2_impact.append(((r2_est(x,y) - r2_est(x.values[:,selection],y)), df4.drop('Species', axis=1).columns[j]))\n    \nfor imp, varname in sorted((r2_impact), reverse = True):\n    if imp > 0.000001:\n        col_imp_r2.append(varname)\n        \n    print ('%.6f %s' %  (imp, varname))\ncol_imp_r2","533f4343":"# data with several columns to improve r2\ndf_r2 = pd.DataFrame(df4, columns= col_imp_r2)","ca680bc7":"# In this analysis we can see a considerable increase in r2,\n# because the columns with the greatest potential for r2 were selected\n\nY = df4.Species\nX = df_r2\n\n# train test split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\nY_train = Y_train.values.ravel()\n\n#Y_train = Y_train.reshape((n_samples,))\n#Y_test = Y_test.reshape((n_samples,))\n\n# Preparing the list of templates\nmodels = []\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Random Forest Regressor', RandomForestRegressor()))\nmodels.append(('Linear Regression', LinearRegression()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNNR', KNeighborsRegressor()))\nmodels.append(('SVR', SVR()))\n\n# Evaluating each model in a loop\nresultados = []\nname = []\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    r2 = r2_score(Y_test, Y_pred)\n    mae = mean_absolute_error(Y_test, Y_pred)\n    mse = mean_squared_error(Y_test, Y_pred)\n    rmse = np.sqrt(mse)\n    print(name)\n    print(\"r2_score: %.2f\" % r2)\n    print(\"mean absolute error: %.2f\" % mae)\n    print(\"mean squared error: %.2f\" % mse)\n    print(\"root mean squared error: %.2f\" % rmse)\n    print(\"\\n\")\n","6d856053":"pip install pydotplus","fb786c17":"import pydotplus","94b1b258":"from ipywidgets import Image\nfrom io import StringIO\nimport graphviz\nfrom sklearn.tree import export_graphviz","38cd9c12":"# Defining x and y\ny= df4.Species.values\ny = y.reshape(-1, 1)\nx=df4.drop([\"Species\"],axis=1)","d1d3a2ab":"# train test split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 5)\nY_train = Y_train.values.ravel()","535a963c":"rfr=RandomForestRegressor(random_state=5)\nrfr= rfr.fit(X_train,Y_train)","04c07871":"d_tree99 = rfr.estimators_[99]\ndot_data = StringIO()\nexport_graphviz(d_tree99, feature_names = X.columns,\n               out_file = dot_data, filled = True, rounded=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value = graph.create_png())","457167cf":"rfr_params = {\"max_depth\":[11],\n             \"max_features\":[\"auto\", \"sqrt\", \"log2\"],\n             \"n_estimators\":[950,1000,1050],\n             \"min_samples_split\":[2]}\n","e8af4b2e":"from sklearn.model_selection import GridSearchCV","f6f93dff":"rfr_model = RandomForestRegressor()\nrfr_cv_model = GridSearchCV(rfr_model,\n                        rfr_params,\n                        cv=10,\n                        n_jobs=-1,\n                        verbose=2,\n                        return_train_score=True)","0c890989":"rfr_cv_model.fit(X_train,Y_train)\nprint(\"Best parameters: \" + str(rfr_cv_model.best_params_))","59a9f018":"#from sklearn.ensemble import RandomForestRegressor","378c1146":"rfr_tuned = RandomForestRegressor(max_depth=11,\n                                  max_features='auto',\n                                  min_samples_split=2,\n                                  n_estimators=1000, random_state=11)\nrfr_tuned = rfr_tuned.fit(X_train,Y_train)","fd4b248d":"rfr_pred = rfr_tuned.predict(X_test)\nr2 = r2_score(Y_test, rfr_pred)\nmae = mean_absolute_error(Y_test, rfr_pred)\nmse = mean_squared_error(Y_test, rfr_pred)\nrmse = np.sqrt(mse)\n\nprint(\"r2_score : %.2f\" %r2)\nprint(\"mean absolute error: %.2f\" % mae)\nprint(\"mean squared error: %.2f\" % mse)\nprint(\"root mean squared error: %.2f\" % rmse)","398515b3":"Importance=pd.DataFrame({\"Importance\":rfr_tuned.feature_importances_*100},\n                       index=X_train.columns)","2a9445ed":"Importance.sort_values(by=\"Importance\",\n                      axis=0,\n                      ascending=True).plot(kind=\"barh\",color=\"green\")\nplt.xlabel(\"Importance level of values\")\n","70b601e4":"## from sklearn.metrics import r2_score\nprint(r2_score(Y_test,rfr_tuned.predict(X_test)))","e762c3e5":"import statsmodels.api as sm","c5bc74a7":" \nXc = sm.add_constant(x)\nmodelo = sm.OLS(y, Xc)\nmodelo_v1 = modelo.fit()","51143281":"modelo_v1.summary()","009b437d":"Y = df4.Species\nX = df4.drop('Species',axis=1)\n# train test split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 5)\nY_train = Y_train.values.ravel()\n\n#Y_train = Y_train.reshape((n_samples,))\n#Y_test = Y_test.reshape((n_samples,))\n\n# Preparing the list of templates\nmodels = []\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Random Forest Regressor', RandomForestRegressor()))\nmodels.append(('Linear Regression', LinearRegression()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('ElasticNet', ElasticNet()))\nmodels.append(('KNNR', KNeighborsRegressor()))\nmodels.append(('SVR', SVR()))\n\n# Evaluating each model in a loop\nresultados = []\nname = []\n\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    r2 = r2_score(Y_test, Y_pred)\n    mae = mean_absolute_error(Y_test, Y_pred)\n    mse = mean_squared_error(Y_test, Y_pred)\n    rmse = np.sqrt(mse)\n    print(name)\n    print(\"r2_score: %.2f\" % r2)\n    print(\"mean absolute error: %.2f\" % mae)\n    print(\"mean squared error: %.2f\" % mse)\n    print(\"root mean squared error: %.2f\" % rmse)\n    print(\"\\n\")","3c0a4ae4":"from itertools import islice\nfrom sklearn.ensemble import GradientBoostingRegressor","090c35d0":"# Regressor GBRT\nest = GradientBoostingRegressor(n_estimators = 1000, max_depth = 3, learning_rate = 1.0)\nest.fit(X_train, Y_train)","09e7145d":"Y_pred = est.predict(X_test)\nnp.sqrt(mean_squared_error(Y_test, Y_pred))","bd76649c":"\nmae = mean_absolute_error(Y_test, Y_pred)\nmse = mean_squared_error(Y_test, Y_pred)\nrmse = np.sqrt(mse)\nprint(name)\nprint(\"mean absolute error: %.2f\" % mae)\nprint(\"mean squared error: %.2f\" % mse)\nprint(\"root mean squared error: %.2f\" % rmse)\nprint(\"\\n\")","05702926":"gbm_params = {\n    \"learning_rate\": [ 0.001 ,0.01, 0.015, 0.1],\n    'max_depth': [ 1, 2, 3, 4, 5,10],\n    'n_estimators': [100, 200, 300, 400],\n    'subsample': [1.25 , 0.5, 0.60, 0.75],\n}\n","fd132348":"gbr = GradientBoostingRegressor()\ngbr_cv_model = GridSearchCV(gbr, gbm_params, cv=10, n_jobs = -1, verbose =2)\ngbr_cv_model.fit(X_train, Y_train)","0e3fd0fd":"gbr_cv_model.best_params_","7354130f":"gbr_tuned = GradientBoostingRegressor(learning_rate = 0.01,\n                                      max_depth = 2,\n                                      n_estimators = 300,\n                                      subsample = 0.75)\ngbr_tuned = gbr_tuned.fit(X_train, Y_train)","64bb08ba":"Y_pred = gbr_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(Y_test, Y_pred))","8d3a34fa":"Importance=pd.DataFrame({\"Importance\":gbr_tuned.feature_importances_*100},\n                       index= X_train.columns)","92dc31b9":"Importance.sort_values(by=\"Importance\",\n                      axis=0,\n                      ascending=True).plot(kind=\"barh\",color=\"green\")\nplt.xlabel(\"Importance level of values\")\nplt.show()\ndf5=pd.DataFrame(Importance)\ndf5.head(64)","40c2d863":"df5.iloc[40:64]","28c2df9c":"model=sm.OLS(gbr_tuned.predict(X_test),X_test)\nmodel.fit().summary()","9b4dab86":"df.head()","6cd3edda":"df[\"Species\"]= df.iloc[:,15:].sum(axis=1)\ndf2_Species = df.drop(['Green_frogs', 'Brown_frogs','Common_toad',\n              'Fire-bellied_toad','Tree_frog','Common_toad',\n              'Fire-bellied_toad','Tree_frog','Common_newt','Great_crested_newt'], axis=1)\ndf2_Species.head()","890c37ad":"t = df2_Species.Species\na = df2_Species.drop(['Motorway','Species'], axis=1)\na.head()","a91baa43":"plt.figure(figsize=(12,8))\nsns.distplot(t)\nplt.show()","275501f1":"a_train, a_test, t_train, t_test=train_test_split(a,t,test_size=0.33,random_state=11)","e2a5f6c5":"rfr2=RandomForestRegressor(n_estimators=300,max_depth=2,random_state=42)\nrfr2.fit(a_train,t_train)","544546c4":"mae = mean_absolute_error(rfr2.predict(a_test),t_test)\nmse = mean_squared_error(rfr2.predict(a_test),t_test)\nrmse = np.sqrt(mse)\n\nprint(\"mean absolute error: %.2f\" % mae)\nprint(\"mean squared error: %.2f\" % mse)\nprint(\"root mean squared error: %.2f\" % rmse)","58745a9b":"d_tree99 = rfr2.estimators_[99]\ndot_data = StringIO()\nexport_graphviz(d_tree99, feature_names = a.columns,\n               out_file = dot_data, filled = True, rounded=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(value = graph.create_png())","cab84bdc":"rf2_params = {\"max_depth\":[8,10,11],\n             \"max_features\":[1,2,3],\n             \"n_estimators\":[10,500,600],\n             \"min_samples_split\":[2,5,10]}","8fa07be7":"rf2_model = RandomForestRegressor()","3cec7ca8":"rf2_cv_model = GridSearchCV(rf2_model,\n                        rf2_params,\n                        cv=10,\n                        n_jobs=-1,\n                        verbose=2)\nrf2_cv_model.fit(a_train,t_train)\nprint(\"Best parameters: \" + str(rf2_cv_model.best_params_))","a860bf01":"rf2_tuned = RandomForestRegressor(max_depth=8,\n                                  max_features=2,\n                                  min_samples_split=2,\n                                  n_estimators=10)\nrf2_tuned.fit(a_train,t_train)","4162c4c5":"mae=mean_absolute_error(rf2_tuned.predict(a_test),t_test)\nmse=mean_squared_error(rf2_tuned.predict(a_test),t_test)\nrmse=np.sqrt(mse)\n\nprint(\"mean_absolute_error: %.2f\"%mae)\nprint(\"mean squared error: %.2f\" %mse)\nprint(\"root mean squared error: %.2f\" %rmse)","3afb2b20":"Importance=pd.DataFrame({\"Importance\":rf2_tuned.feature_importances_*100},\n                       index=a_train.columns)\nImportance.sort_values(by=\"Importance\",\n                      axis=0,\n                      ascending=True).plot(kind=\"barh\",color=\"green\")\nplt.xlabel(\"Importance level of values\")","ac2bec92":"model2=sm.OLS(rf2_tuned.predict(a_test),a_test.astype(float))\nmodel2.fit().summary()","591d88f8":"!pip install xgboost","d11f75f9":"import xgboost as xgb","8ba31926":"from xgboost import XGBRegressor","1941037f":"xgb = XGBRegressor().fit(a_train.astype(float), t_train)\n","895bd90c":"t_pred = xgb.predict(a_test.astype(float))","597f075c":"np.sqrt(mean_squared_error(t_test, t_pred))","71010c75":"xgb","16f85980":"xgb_grid = {\n    'learning_rate': [0.001,0.1, 0.01, 0.05],\n    'max_depth': [2, 3, 4],\n    'n_estimators': [100, 200, 300,400],\n    'subsample': [1, 0.25, 0.5, 0.75]    \n}","e11f7ca4":"xgb = XGBRegressor()","fa4e0ad8":"xgb_cv = GridSearchCV(xgb, \n                     param_grid = xgb_grid,\n                     cv=10,\n                     n_jobs = -1,\n                     verbose = 2)\nxgb_cv.fit(a_train.astype(float), t_train)","ef8bf3d7":"xgb_cv.best_params_","ec97d609":"xgb_tuned = XGBRegressor(colsample_bytree = 0.4,\n                         learning_rate = 0.01,\n                         max_depth = 2,\n                         n_estimators = 500)","24aa8ac3":"xgb_tuned = xgb_tuned.fit(a_train.astype(float), t_train)","46c39f9c":"k_pred = xgb_tuned.predict(a_test.astype(float))\nnp.sqrt(mean_squared_error(t_test, t_pred))","9263ce02":"Importance=pd.DataFrame({\"Importance\":xgb_tuned.feature_importances_*100},\n                       index = a_train.columns)\nImportance.sort_values(by=\"Importance\",\n                      axis=0,\n                      ascending=True).plot(kind=\"barh\",color=\"green\")\nplt.xlabel(\"Importance level of values\")\nmodel=sm.OLS(xgb_tuned.predict(a_test.astype(float)), a_test.astype(float))\nmodel.fit().summary()\n","e9e0331c":"# Aplying the Gradient Boosting Regressor","55113883":"# Testing Regression models","e4b1d1aa":"# Using R Squared","edaede03":"#  Testing Regression models with columns to improve r2","e2a8ef6e":"# XGBoost With Label Encoding "}}