{"cell_type":{"c9c260a1":"code","d899199c":"code","e34a081b":"code","5190d2e4":"code","994b4d92":"code","64ef244e":"code","bf58b677":"code","16fb409e":"code","feddfa54":"code","d8b7efb4":"code","058c51d7":"code","4f9feb28":"markdown","f7c022fe":"markdown","49d0f340":"markdown","e391e308":"markdown","5d744840":"markdown","8a96c5d1":"markdown","edb002cc":"markdown","3f56af51":"markdown","b08c5336":"markdown","81f323cb":"markdown","513b67bb":"markdown"},"source":{"c9c260a1":"import pandas as pd\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","d899199c":"train_shape = train_data.shape\ntest_shape = test_data.shape\n\nprint('train_data has {examples} images, shape of each image is {img_shape}'.format(examples=train_shape[0], img_shape=train_shape[1]))\nprint('test_data has {examples} images, shape of each image is {img_shape}'.format(examples=test_shape[0], img_shape=test_shape[1]))","e34a081b":"import matplotlib.pyplot as plt\nimport random\nfrom tensorflow import keras\nfrom keras import layers\nfrom tensorflow.keras import regularizers","5190d2e4":"y_train = train_data.label\nX_train = train_data.drop('label', axis=1)\n\nX_test = np.array(test_data)\nX_train = np.array(X_train)\n\nX_train = X_train.reshape(-1, 28, 28)\nX_test = X_test.reshape(-1, 28, 28)\n\nX_train = X_train.astype(\"float32\") \/ 255.0\nX_test = X_test.astype(\"float32\") \/ 255.0\n\nX_test = np.expand_dims(X_test, -1)\nX_train = np.expand_dims(X_train, -1)\n\nnum_classes = 10\n\ny_train = keras.utils.to_categorical(y_train, num_classes)","994b4d92":"y_train","64ef244e":"random_indices = random.sample(range(0, len(X_train)), 9)\n\nfor i, idx in enumerate(random_indices):\n    img = X_train[idx]\n    label = train_data['label'][idx]\n    \n    plt.subplot(330 + 1 + i)    \n    plt.tick_params(left=False,\n            bottom=False,\n            labelleft=False,\n            labelbottom=False)\n    \n    plt.title('Label is {label}'.format(label=label))\n    plt.tight_layout()\n    plt.imshow(img, cmap='gray') \n    \nplt.show()","bf58b677":"input_shape = (28, 28, 1)\n\nmodel = keras.Sequential([\n    # Input layer\n    keras.Input(shape=input_shape, name='input'),\n    \n    layers.Conv2D(filters=32, kernel_size=5, strides=1, \n                  activation='relu', kernel_regularizer=regularizers.l2(0.0005), name='conv1'),\n    \n    layers.Conv2D(filters=32, kernel_size=5, strides=1, use_bias=False, name='conv2'),\n    \n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.MaxPool2D(pool_size=2, strides=2, name='pool1'),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n                  kernel_regularizer=regularizers.l2(0.0005), name='conv3'),\n    \n    layers.Conv2D(filters=64, kernel_size=3, use_bias=False, name='conv4'),\n    \n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.MaxPool2D(pool_size=2, strides=2, name='pool2'),\n    layers.Dropout(0.25),\n    \n    layers.Flatten(name='flatten'),\n    layers.Dense(256, use_bias=False, name='dense1'),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dropout(0.25),\n    \n    layers.Dense(128, use_bias=False, name='dense2'),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n\n    layers.Dense(10, activation='softmax', name='output')\n])","16fb409e":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=30, batch_size=128)","feddfa54":"predictions = model.predict(x = X_test)\npredicted_class_indices = np.argmax(predictions, axis=1)","d8b7efb4":"# I know that that's almost the exact copy of the code that we used earlier\n# but I don't feel like creating a function right now, sorry\n\nrandom_indices = random.sample(range(0, len(predictions)), 9)\nprediction_classes = np.argmax(predictions[random_indices], axis=1)\n\nfor i, idx in enumerate(random_indices):\n    img = X_test[idx]\n    prediction_label = prediction_classes[i]\n    \n    plt.subplot(330 + 1 + i)    \n    plt.tick_params(left=False,\n            bottom=False,\n            labelleft=False,\n            labelbottom=False)\n    \n    plt.title('Prediction is {prediction_label}'.format(prediction_label=prediction_label))\n    plt.tight_layout()\n    plt.imshow(img, cmap='gray') \n    \nplt.show()","058c51d7":"image_id = np.arange(start=1, stop=len(predicted_class_indices) + 1)\n\noutput = pd.DataFrame({'ImageId': image_id, 'Label': predicted_class_indices})\noutput.to_csv('my_submission.csv', index=False)","4f9feb28":"### Now let's compile our model. We use categorical_crossentropy for multi-class classification. We use Adam optimizer, because it's usually a good fit for any kind of network and it controls the learning rate by itself (one less hyperparameter to worry about). 30 epochs is kind of arbitrary, but I found that that's a good balance between underfiting and overfiting.","f7c022fe":"## Hello, this notebook will show you how to get a score of more than **0.993** in Digit Recognizer competition. \n### Let's start off by loading the training and testing data.","49d0f340":"### As you can see the results kind of good (at least I hope so). We are ready to submit!","e391e308":"## The next step is to preprocess our data. This consists from the several steps:\n1. Separate labels from the values in training set.\n2. Normalizing images by dividing them by 255. This way all of the pixel values are now in range [0, 1]. \n   This helps our model to converge faster.\n3. Reshaping our data. Initially our data is an form of a DataFrame where each row is 784 pixels. But our model needs the data in form of 28x28 numpy array.\n4. Use expand_dims to make the shape of our data match the shape that Keras expects.\n5. And the last, but not least. We need to convert number representation of our classes 1, 2, 3, ..., 9 to vector representation.","5d744840":"### Now let's create our model. We use Keras Sequential API to create our Convolutional Neural Network.\n### Some other things to note:\n* We are using L2 regulariation and Dropout layers to avoid overfitting\n* We use BatchNormalization layer to speed up learning. Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n* We use relu (rectified linear unit) as an activation function. It works really well with convolutional neural networks.","8a96c5d1":"### Let's visualize some of the predictions.","edb002cc":"### Let's make the predictions","3f56af51":"### As you can see y_train is now a matrix, where each row encodes one of the classes. This representation called one hot encoding, you can learn more about it [here](https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/).","b08c5336":"## Let's look at dimensions of the data","81f323cb":"### As you can see we have 42000 images in train dataset and 28000 images in test dataset. Each image consists of 784 pixels (28x28) and the first column in train_data is label, that we will use to train our model","513b67bb":"### Now let's visualize our data"}}