{"cell_type":{"5f12d1a1":"code","a92eba60":"code","9fc638b2":"code","6a8d4687":"code","b9ccf877":"code","1837a0f7":"code","789fc201":"code","692a2384":"code","f1e3e9a5":"code","3469e42b":"code","ed963c1a":"code","bd327152":"code","5ea02623":"code","442c6c31":"markdown","e3f2cbd6":"markdown","b1d93612":"markdown","147feb99":"markdown","64b737b9":"markdown"},"source":{"5f12d1a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a92eba60":"data = pd.read_csv(\"..\/input\/candy-data.csv\")","9fc638b2":"data.info()\n","6a8d4687":"# I do not need \"competitorname\" so I drop it\ndata.drop(\"competitorname\", inplace = True, axis=1)","b9ccf877":"# initialize x and y\ny = data.chocolate.values\nx_data = data.drop([\"chocolate\"], axis = 1)","1837a0f7":"x = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","789fc201":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train shape\", x_train.shape)\nprint(\"x_test shape\", x_test.shape)\nprint(\"y_train shape\", y_train.shape)\nprint(\"y_test shape\", y_test.shape)","692a2384":"def initialize_weights_and_bias(dimension):\n    \"\"\"\n    Input\n    dimension => number of train_data's features\n    \n    Output\n    w => weights\n    b => bias\n    \"\"\"\n    w = np.full((dimension,1), 0.01)\n    b = 0\n    return w, b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n    ","f1e3e9a5":"def forward_backward_propagation(w, b, x_train, y_train):\n    \"\"\"\n    Input\n    w => weights\n    b => bias\n    x_train => x of the data we want the train\n    y_train => y of the data we want the train\n    \n    Output\n    cost => loss of function\n    gradients => derivative of weights and bias\n    \"\"\"\n    \n    # forward propagation\n    z = (np.dot(w.T, x_train)+b)\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-((1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss)\/x_train.shape[1]\n    \n    # backward propagation\n    \n    derivative_weights = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weights\":derivative_weights, \"derivative_bias\":derivative_bias}\n    \n    return cost, gradients\n    ","3469e42b":"def update(w, b, x_train, y_train, learning_rate, num_iteration):\n    \"\"\"\n    Input\n    w => weights\n    b => bias\n    x_train => x of the data we want the train\n    y_train => y of the data we want the train\n    learning_rate => learn speed (if speed is too big, function can not be run properly)\n    num_iteration => how many times we want to run forward_backward_propagation()\n    \n    Output\n    paremeter => last values of weights and bias\n    cost_list => all cost(loss) values we got\n    \"\"\"\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(num_iteration):\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate*gradients[\"derivative_weights\"]\n        b = b - learning_rate*gradients[\"derivative_bias\"]\n    \n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n    \n    parameters = {\"weights\":w, \"bias\":b}        \n    plt.plot(index, cost_list2)\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list\n            ","ed963c1a":"def predict(w, b, x_test):\n    \"\"\"\n    Input\n    w => last values of weights\n    b => last value of bias\n    x_test => x of the data we want to test\n    \n    Output\n    y_predict => prediction of the test data\n    \"\"\"\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    y_predict = np.zeros((1, x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0\n        else:\n            y_predict[0,i] = 1\n    return y_predict","bd327152":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iteration):\n    w, b = initialize_weights_and_bias(x_train.shape[0])\n    \n    parameter, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iteration)\n    \n    y_predict = predict(parameter[\"weights\"], parameter[\"bias\"], x_test)\n    print(\"accuracy: {}\".format(100 - np.mean(np.abs(y_predict - y_test))))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, num_iteration = 100)","5ea02623":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nlr.fit(x_train.T, y_train.T)\n\nprint(\"Test accuracy: {}\".format(lr.score(x_test.T, y_test.T)))\nprint(\"Train accuracy: {}\".format(lr.score(x_train.T, y_train.T)))","442c6c31":"## EDA (Exploratory Data Analysis)","e3f2cbd6":"## Conclusion\n\nIf you see my wrong spelling please ignore them :)\n\nIf you like it, please upvote :)\n\nIf you have any question, I will be appreciate to hear it.","b1d93612":"## Train-Test-Split","147feb99":"In this kernel we try to predict if a candy is chocolate based or not, based on its others features\nif its chocolate based, result will be 1 else it will be 0","64b737b9":"## Normalization"}}