{"cell_type":{"21a634cd":"code","70797778":"code","80ac1ca4":"code","24197eb5":"code","bce96db2":"code","669791e7":"code","b569c874":"code","2cf55d1a":"code","d50405eb":"code","8c9b640c":"code","4b341fee":"code","2c547066":"code","0ae47636":"code","cd1571dc":"code","b53a8400":"code","c5b5538f":"code","cf2fdf34":"code","a68d1051":"code","fc698b95":"code","a02829bb":"code","56ba36d0":"code","f066f09b":"code","66712fa2":"code","228a9e35":"code","e71b5825":"code","7a0c5ef6":"code","b5728f84":"code","20447897":"code","7435b48f":"code","426c2e55":"code","028ac618":"code","ca82ed0c":"code","4eb4d56e":"code","45ca9c26":"code","c4998be7":"code","fcfca25d":"code","825da96c":"code","436ec188":"code","1f2c8c1b":"code","4d62ebef":"code","89d56985":"code","c96a6c7a":"code","de134372":"code","d1544954":"code","95240d9a":"markdown","a15004ac":"markdown","9fbb81ec":"markdown","edaf9444":"markdown","7a5e46b9":"markdown","3222b3e8":"markdown","b218410b":"markdown","114c066c":"markdown","69568bc0":"markdown","4122f599":"markdown","49e9473f":"markdown","4805c61b":"markdown","b9c083ca":"markdown","bb1e0b71":"markdown","cc7ae3ca":"markdown","ad837491":"markdown","1604d295":"markdown","125667c3":"markdown","5924ecda":"markdown","1a42a3ba":"markdown","0059969e":"markdown","4a9d7094":"markdown","d87945be":"markdown","69d58032":"markdown","f5245813":"markdown","669e1726":"markdown","99e4fae4":"markdown","02454d1f":"markdown","2034efe3":"markdown","1936137b":"markdown","5e141564":"markdown","b6a733c8":"markdown","9229cf3c":"markdown","11617138":"markdown","3df9845b":"markdown","532b22b3":"markdown","022fbee6":"markdown"},"source":{"21a634cd":"!pip install https:\/\/github.com\/Yard1\/pycaret\/archive\/one_hot_for_boolean_categories.zip\n!pip install --upgrade --no-deps --force-reinstall https:\/\/github.com\/Yard1\/pycaret\/archive\/one_hot_for_boolean_categories.zip\nimport pandas as pd\nimport numpy as np\nfrom pycaret.classification import *\n\nimport random as rn\nSEED = 440\n!PYTHONHASHSEED=0\nnp.random.seed(SEED)\nrn.seed(SEED)","70797778":"raw_data = pd.read_excel('..\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx')\nraw_data","80ac1ca4":"raw_data['AGE_PERCENTIL'] = raw_data['AGE_PERCENTIL'].replace(r'[^0-9]', '', regex=True)","24197eb5":"def get_na_table(df):\n    na_counts = pd.DataFrame([(x, df[x].isna().sum(), df[x].isna().sum()\/len(df)) for x in df.columns])\n    na_counts.columns = ['Variable', 'NAs', 'Percentage']\n    return na_counts[na_counts['NAs'] > 0].sort_values(by=['NAs'], ascending=False)","bce96db2":"get_na_table(raw_data)","669791e7":"# https:\/\/www.kaggle.com\/fernandoramacciotti\/early-icu-detection-only-0-2-window\nraw_data = raw_data.sort_values(by=['PATIENT_VISIT_IDENTIFIER', 'WINDOW']).groupby('PATIENT_VISIT_IDENTIFIER', as_index=False).fillna(method='ffill').fillna(method='bfill')\n\nget_na_table(raw_data)","b569c874":"# https:\/\/www.kaggle.com\/fernandoramacciotti\/early-icu-detection-only-0-2-window\n\n# dropping the patients which were admitted to ICU in the first window\ndata = raw_data.loc[~((raw_data['WINDOW'] == '0-2') & (raw_data['ICU'] == 1))]\n\n\n# getting the patients that were eventually admitted to ICU\nicu_above_2 = data.groupby('PATIENT_VISIT_IDENTIFIER').agg({'ICU': max}).reset_index().rename(columns={'ICU': 'ICU_NEW'})\n    \ndata = data.merge(icu_above_2, on=['PATIENT_VISIT_IDENTIFIER'], how='left')\ndata = data.loc[data['WINDOW'] == '0-2']","2cf55d1a":"pd.crosstab(data['WINDOW'], data['ICU_NEW'])","d50405eb":"modifiers = ['_DIFF', '_MEAN', '_MEAN_REL', '_MIN', '_MAX', '_MEDIAN']\nlab_columns = [x for x in data.columns if any(y in x for y in modifiers)]\nvital_signs = [x.replace('_MEAN', '') for x in lab_columns if '_MEAN' in x]\nvital_signs = {x:[y for y in lab_columns if y.startswith(x)] for x in vital_signs}\nfor k, v in vital_signs.items():\n    display(data[v].corr())","8c9b640c":"lab_columns_to_ignore = [x for x in lab_columns if not (x.endswith('_DIFF_REL') or x.endswith('_MEDIAN'))]","4b341fee":"data['MEAN_AERTIAL_PRESSURE'] = (2*data['BLOODPRESSURE_DIASTOLIC_MEAN'])\/3 + (data['BLOODPRESSURE_SISTOLIC_MEAN'])\/3\ndata['BLOOD_PRESSURE_MEDIAN'] = data['BLOODPRESSURE_SISTOLIC_MEDIAN'] - data['BLOODPRESSURE_DIASTOLIC_MEDIAN']","2c547066":"experiment = setup(\n    data, \n    target='ICU_NEW',\n    ignore_features=['PATIENT_VISIT_IDENTIFIER', 'ICU', 'WINDOW']+lab_columns_to_ignore,\n    ordinal_features={'AGE_PERCENTIL': sorted(list(data.AGE_PERCENTIL.unique()))}, # converting AGE_PERCENTIL to an ordinal feature instead of categorical\n    fix_imbalance=True, # fixing train-test split imbalances\n    feature_selection=True, feature_selection_threshold=0.95, # conservative important feature selection\n    remove_perfect_collinearity=True, # in case we missed any perfectly collinear features\n    session_id=SEED, # seed for reproductibility\n    silent=True # for kaggle compatibility\n    )","0ae47636":"transformed_data = get_config('X')\ntransformed_data","cd1571dc":"display(sorted(transformed_data.columns))","b53a8400":"# helper function to get a nice model name\ndef get_model_name(e) :\n    mn = str(e).split(\"(\")[0]\n\n    if 'catboost' in str(e):\n        mn = 'CatBoostClassifier'\n    \n    model_dict_logging = {'ExtraTreesClassifier' : 'Extra Trees Classifier',\n                        'GradientBoostingClassifier' : 'Gradient Boosting Classifier', \n                        'RandomForestClassifier' : 'Random Forest Classifier',\n                        'LGBMClassifier' : 'Light Gradient Boosting Machine',\n                        'XGBClassifier' : 'Extreme Gradient Boosting',\n                        'AdaBoostClassifier' : 'Ada Boost Classifier', \n                        'DecisionTreeClassifier' : 'Decision Tree Classifier', \n                        'RidgeClassifier' : 'Ridge Classifier',\n                        'LogisticRegression' : 'Logistic Regression',\n                        'KNeighborsClassifier' : 'K Neighbors Classifier',\n                        'GaussianNB' : 'Naive Bayes',\n                        'SGDClassifier' : 'SVM - Linear Kernel',\n                        'SVC' : 'SVM - Radial Kernel',\n                        'GaussianProcessClassifier' : 'Gaussian Process Classifier',\n                        'MLPClassifier' : 'MLP Classifier',\n                        'QuadraticDiscriminantAnalysis' : 'Quadratic Discriminant Analysis',\n                        'LinearDiscriminantAnalysis' : 'Linear Discriminant Analysis',\n                        'CatBoostClassifier' : 'CatBoost Classifier',\n                        'BaggingClassifier' : 'Bagging Classifier',\n                        'VotingClassifier' : 'Voting Classifier'} \n\n    return model_dict_logging.get(mn)","c5b5538f":"models = compare_models(sort='F1', n_select=8)\ncompare_cv_results = [get_config('display_container')[-1]]","cf2fdf34":"models.append(create_model('rf'))\ncompare_cv_results.append(get_config('display_container')[-1])","a68d1051":"tuned_models_auc_f1 = []\ntuned_models_f1_auc = []\ntuned_models_auc_f1_cv = []\ntuned_models_f1_auc_cv = []","fc698b95":"for model in models:\n    model_tuned = tune_model(tune_model(model, optimize='AUC', n_iter=60, choose_better=True), optimize='F1', n_iter=20, choose_better = True) \n    tuned_models_auc_f1.append(model_tuned)\n    tuned_models_auc_f1_cv.append(get_config('display_container')[-1])\nfor model in models:\n    model_tuned = tune_model(tune_model(model, optimize='F1', n_iter=60, choose_better=True), optimize='AUC', n_iter=20, choose_better = True) \n    tuned_models_f1_auc.append(model_tuned)\n    tuned_models_f1_auc_cv.append(get_config('display_container')[-1])","a02829bb":"cv_results = compare_cv_results[0].iloc[0:8, :].reset_index().rename({'index': 'Index'}, axis=1).drop('TT (Sec)', axis=1)\ncv_results.insert(2, 'Optimized for', '',)\ncv_results.loc[len(cv_results)] = [9, get_model_name(models[-1]), ''] + list(compare_cv_results[-1].loc['Mean', :])\n\ntuned_models_auc_f1_mean = []\nfor i, x in enumerate(tuned_models_auc_f1):\n    df = pd.DataFrame(tuned_models_auc_f1_cv[i].loc['Mean', :].to_frame().T.reset_index(drop=True), columns=['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa', 'MCC'])\n    df.insert(0, 'Index', i)\n    df.insert(1, 'Model', get_model_name(tuned_models_auc_f1[i]))\n    df.insert(2, 'Optimized for', 'AUC-F1')\n    tuned_models_auc_f1_mean.append(df)\ntuned_models_f1_auc_mean = []\nfor i, x in enumerate(tuned_models_f1_auc):\n    df = pd.DataFrame(tuned_models_f1_auc_cv[i].loc['Mean', :].to_frame().T.reset_index(drop=True), columns=['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa', 'MCC'])\n    df.insert(0, 'Index', i)\n    df.insert(1, 'Model', get_model_name(tuned_models_f1_auc[i]))\n    df.insert(2, 'Optimized for', 'F1-AUC')\n    tuned_models_f1_auc_mean.append(df)\n    \ncv_results = pd.concat([cv_results] + tuned_models_auc_f1_mean + tuned_models_f1_auc_mean).reset_index(drop=True)\ncv_results.sort_values(by='F1',ascending=False)","56ba36d0":"metrics = []\n\nfor i, model in enumerate(models):\n    x = predict_model(model)\n    real = x[\"ICU_NEW\"]\n    predicted = x[\"Label\"]\n    out = get_config('display_container')[-1]\n    out_metrics = [i, get_model_name(model), ''] + [out.iloc[0,x] for x in range(1,8)]\n    metrics.append(out_metrics)\nfor i, model in enumerate(tuned_models_auc_f1):\n    x = predict_model(model)\n    real = x[\"ICU_NEW\"]\n    predicted = x[\"Label\"]\n    out = get_config('display_container')[-1]\n    out_metrics = [i, get_model_name(model), 'AUC-F1'] + [out.iloc[0,x] for x in range(1,8)]\n    metrics.append(out_metrics)\nfor i, model in enumerate(tuned_models_f1_auc):\n    x = predict_model(model)\n    real = x[\"ICU_NEW\"]\n    predicted = x[\"Label\"]\n    out = get_config('display_container')[-1]\n    out_metrics = [i, get_model_name(model), 'F1-AUC'] + [out.iloc[0,x] for x in range(1,8)]\n    metrics.append(out_metrics)\n\ntest_results = pd.DataFrame(metrics, columns=['Index','Model', 'Optimized for', 'Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa', 'MCC']).reset_index(drop=True)\ntest_results.sort_values(by='F1',ascending=False)","f066f09b":"cv_test_average = test_results.copy()\nvalue_columns = cv_test_average.columns[-7:]\nfor x in value_columns:\n    cv_test_average[x] = np.absolute(cv_test_average[x] - cv_results[x])\/cv_results[x]\ncv_test_average['Mean Difference'] = cv_test_average.drop('Index', axis=1).mean(numeric_only=True, axis=1)\ncv_test_average['Max Difference'] = cv_test_average.drop('Index', axis=1).max(numeric_only=True, axis=1)\ncv_test_average.sort_values(by='Mean Difference', ascending=True)","66712fa2":"blend_stack_cv_results = []\nblend_stack_models = []\nmodels_to_blend = [tuned_models_f1_auc[8], tuned_models_f1_auc[1], models[0]]\n\nblend = blend_models(models_to_blend, method='soft')\nblend_stack_models.append(blend)\nblend_stack_cv_results.append(get_config('display_container')[-1])","228a9e35":"models_to_stack = [tuned_models_f1_auc[8], tuned_models_f1_auc[1], models[0], tuned_models_f1_auc[4]]\n\nstack_catboost = stack_models(models_to_stack, meta_model = models[4])\nblend_stack_models.append(stack_catboost)\nblend_stack_cv_results.append(get_config('display_container')[-1])","e71b5825":"stack_rf = stack_models(models_to_stack, meta_model = models[-1])\nblend_stack_models.append(stack_rf)\nblend_stack_cv_results.append(get_config('display_container')[-1])","7a0c5ef6":"names = ['Voting Classifier', 'Stack CatBoost', 'Stack Random Forest']\nblend_stack_cv_results_mean = []\nfor i, x in enumerate(blend_stack_cv_results):\n    df = pd.DataFrame(blend_stack_cv_results[i].loc['Mean', :].to_frame().T.reset_index(drop=True), columns=['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa', 'MCC'])\n    df.insert(0, 'Index', i)\n    df.insert(1, 'Model', names[i])\n    df.insert(2, 'Optimized for', '')\n    blend_stack_cv_results_mean.append(df)\n    \nblend_stack_cv_results_mean = pd.concat(blend_stack_cv_results_mean).reset_index(drop=True)\nblend_stack_cv_results_mean.sort_values(by='F1',ascending=False)","b5728f84":"blend_stack_test_results = []\nfor i, model in enumerate(blend_stack_models):\n    x = predict_model(model)\n    real = x[\"ICU_NEW\"]\n    predicted = x[\"Label\"]\n    out = get_config('display_container')[-1]\n    out_metrics = [i, names[i], ''] + [out.iloc[0,x] for x in range(1,8)]\n    blend_stack_test_results.append(out_metrics)\n\nblend_stack_test_results = pd.DataFrame(blend_stack_test_results, columns=['Index','Model', 'Optimized for', 'Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa', 'MCC']).reset_index(drop=True)\nblend_stack_test_results.sort_values(by='F1',ascending=False)","20447897":"blend_stack_cv_test_average = blend_stack_test_results.copy()\nvalue_columns = blend_stack_cv_test_average.columns[-7:]\nfor x in value_columns:\n    blend_stack_cv_test_average[x] = np.absolute(blend_stack_cv_test_average[x] - blend_stack_cv_results_mean[x])\/blend_stack_cv_results_mean[x]\nblend_stack_cv_test_average['Mean Difference'] = blend_stack_cv_test_average.drop('Index', axis=1).mean(numeric_only=True, axis=1)\nblend_stack_cv_test_average['Max Difference'] = blend_stack_cv_test_average.drop('Index', axis=1).max(numeric_only=True, axis=1)\nblend_stack_cv_test_average.sort_values(by='Mean Difference', ascending=True)","7435b48f":"summary_cv_results = pd.concat([blend_stack_cv_results_mean, cv_results.sort_values(by='F1',ascending=False).iloc[0:10,:]]).reset_index(drop=True)\ndisplay(summary_cv_results.sort_values(by='F1',ascending=False))\ndisplay(summary_cv_results.sort_values(by='AUC',ascending=False))","426c2e55":"plot_model(blend, plot='confusion_matrix')","028ac618":"plot_model(blend, plot='class_report')","ca82ed0c":"interpret_model(tuned_models_auc_f1[4])","4eb4d56e":"interpret_model(models[0])","45ca9c26":"plot_model(models[0], plot='confusion_matrix')","c4998be7":"plot_model(models[0], plot='class_report')","fcfca25d":"plot_model(models[0], plot='auc')","825da96c":"plot_model(tuned_models_f1_auc[1], plot='confusion_matrix')","436ec188":"plot_model(tuned_models_f1_auc[1], plot='class_report')","1f2c8c1b":"plot_model(tuned_models_f1_auc[1], plot='auc')","4d62ebef":"interpret_model(tuned_models_f1_auc[-1])","89d56985":"plot_model(tuned_models_f1_auc[-1], plot='confusion_matrix')","c96a6c7a":"plot_model(tuned_models_f1_auc[-1], plot='class_report')","de134372":"plot_model(tuned_models_f1_auc[-1], plot='auc')","d1544954":"!rm -rf *.pkl\nfrom datetime import date\nfinal_selected_models = models_to_stack + [blend]\nfor model in final_selected_models:\n    save_model(finalize_model(model), f'covid19_{get_model_name(model).replace(\" \", \"_\")}_{date.today()}')\n    \n!rm -rf catboost_info\n!rm -rf cb_model.json","95240d9a":"`compare_models()` function of PyCaret runs 18 different models on provided data. It makes it very easy to see which models work best with the data we have. Everything is cross-validated with k-folds=10. In my opinion, this function is what sells this library.\n\nIn order to create clinically relevant models, we will focus on the F1 score and AUC. Accuracy is not as important, since the cost of False Negatives is high.","a15004ac":"We'll add those new features to our dataset.","9fbb81ec":"### F1-AUC XGBoost\n\nSadly, for some reason `interpret_model()` errors out for XGBoost on Kaggle. It works locally on my machine, though!","edaf9444":"# Modelling with PyCaret\n\nFinally, our data is ready for modelling. PyCaret makes everything a breeze. First, we need to setup the PyCaret experiment using `setup()`. That function can do a lot of preprocessing itself, but we will not need much, considering that the data is already standardized.","7a5e46b9":"# Preprocessing","3222b3e8":"First, we'll remove the ordinal suffixes from the `AGE_PERCENTIL` column, so that we are left with just numbers. A simple regex to get rid of all non-digits will suffice.","b218410b":"# Preparation\n\nI will be using the nightly version of the PyCaret library, as it has several new features and improvements not present in the stable version. When the new stable version comes out (2.x.x), feel free to switch to `pycaret` instead of `pycaret-nightly`.\n\nI'll be using my own fork of `pycaret-nightly`, as I changed the way boolean category columns are encoded. In base `pycaret`, even boolean columns are split into two (eg. `GENDER` with values `[0,1]` will be split into `GENDER_0` and `GENDER_1`). I think that's unnecessary, and it causes issues with modelling. Hopefully my PR will be accepted!","114c066c":"## Blending and stacking\n\nThe metrics are not the best, but that's to be expected with how little data we have. They could most likely be improved with further feature engineering and tuning. There is, however, one thing we can still do - blend and stack models. \n\nBlending is, in essence, averaging the predictions of various models. Stacking takes the output of the models, and feeds it to a meta-model in order to classify data. Both can be 'soft' or 'hard' - in the former, the probabilities obtained are used, and in the latter, the binary result is used. In most cases, 'soft' method gives better results.\n\nLet's take the best models - RF, CatBoost, XGBoost and GBC - and do that. CatBoost cannot be blended with PyCaret, therefore we'll omit it from blending.\n\nSince the choice of meta model can highly impact the performance of the stacked model, we will use both CatBoost and Random Forest for comparison.","69568bc0":"### Untuned GBC","4122f599":"I won't bore you with any long winded descriptions about COVID-19 itself, as I am no medical professional - and everyone knows what's happening right now. I'd like to instead try and use the PyCaret library (which is really awesome, go get it) to create several models attempting to predict whether a patient will be admitted to ICU based on their characteristcs in the first measurment window.\n\nThe main objective of this kernel is to showcase the ease of use of the PyCaret library, and to try and produce a viable model. In-depth tnterpretation of the model will not be performed.\n\nI'd like to thank [Fernando Ramacciotti](https:\/\/www.kaggle.com\/fernandoramacciotti\/early-icu-detection-only-0-2-window) and [Leonardo Canela](https:\/\/www.kaggle.com\/leocanela\/random-forest-to-predict-not-admitted-to-icu) for their kernels - they have been a great inspiration and help. Some of the code in this notebook has been copied from them.","49e9473f":"We can check how PyCaret transformed the data.","4805c61b":"I am not dropping anything at this stage, as it will be handled by PyCaret itself.","b9c083ca":"### F1-AUC Random Forest","bb1e0b71":"353 rows. Not the biggest sample size, but it's the best we can get, if we want our data to be clinically relevant.","cc7ae3ca":"## Missing values\n\nLet's take a look at missing values.","ad837491":"## Finalization\n\nAll that is left is to finalize the models - which will retrain it using the entire dataset - and save them as a files ready for deployment.","1604d295":"We can now select several models for hyperparameter tuning, and see if we can improve them further. We will choose the top 8 models - they have been already saved into the `models` list thanks to the `n_select=8` parameter.\n\n## Hyper parameter tuning\n\nThe `tune_model()` function of PyCaret conducts a randomized search on hyper parameters, using `sklearn.model_selection.RandomizedSearchCV` under the hood. The default grids give decent results for most datasets. We will do two double optimizations - AUC followed by F1 and the other way around. The number of iterations (`n_iter`) can be increased to get better hyperparameters, at a cost of longer computing time. In my experience, 100 is a good number, as higher numbers give diminishing results (that being said, tuning of some models is fast enough that their `n_iter` can be increased). By specifiying `choose_better=True`, if the tuned model turns out to have worse metric value than the passed model, the passed model will be saved. However, the overall performance may still be worse, even if a single metric is improved. In my experience, this often happens with CatBoost and XGBoost, which give very good results for most datasets without tuning. The output of each cell will be additionally saved into a list, so that the CV results can be compared. We will also add Random Forest to the models we have, as it often gives much better results after tuning.","125667c3":"Now we can compare the new models like we did before.","5924ecda":"### F1-AUC CatBoost","1a42a3ba":"## Performance on test set\nPyCaret automatically splits the data into the train-test sets, using a 0.7-0.3 ratio by default. By calling `predict_model()` function without the `data` parameter, the model will be tested on the test set. Let's compare all the models by their performance on the test set, sorting by F1 - I'd normally sort by AUC, but Ridge doesn't provide an AUC metric.","0059969e":"The blended model seems to have the best performance out of the three. However, due to its nature, its interpretation is a challenge. Let's see how it compares against the best previous models.","4a9d7094":"## Removing highly correlated features\n\nThe various vital signs are represented by several columns each, providing information such as the mean, median, minimum value, maximum value and the difference between the last two. However, if we look at the columns, we can clearly see they are highly correlated with each other, and thus not providing any extra information. We can simplify the model by just using the mean or median of those results.","d87945be":"I hope this presentation of the possibilities of PyCaret library has been enjoyable. The saved models, especially Random Forest and CatBoost give quite good results, comparable, or even slightly better than the results that have been submitted so far.","69d58032":"It looks like XGBoost has overfit the least, though other models are not terrible on that account either. Random Forest and CatBoost have little variance too. Tuned GBC models seem to have overfit.\n\nTaking everything we have learned so far, it looks like Random Forest, XGBoost, CatBoost and GBC are the top models for this problem.","f5245813":"A lot of measurments are missing - however, as the authors of the dataset pointed out, we can use the measurments from neighboring windows.","669e1726":"## Interpretation\n\nSadly, due to the intricacies of the Blended Model's `VotingClassifier` class, most plots cannot be easily made, making it a bit of a black box. Other models - CatBoost, XGBoost, Random Forest and GBC will be interpreted using PyCaret's `interpret_model()` function. It is a wrapper for SHAP's `TreeEvaluator`, returning the SHAP plots we all know and love. For models other than CatBoost, we can also use the `plot_model()` function, which provides a selection of various plots.\n\n### Blended Model","99e4fae4":"# Predicting ICU admission using with PyCaret","02454d1f":"The best models by F1:\n* F1-AUC tuned Random Forest\n* Untuned GBC\n* AUC-F1 tuned CatBoost\n* Blended Model (GBC, F1-AUC tuned XGBoost, F1-AUC tuned Random Forest)\n\nThe best models by AUC:\n* F1-AUC tuned Random Forest\n* AUC-F1 tuned CatBoost\n* Blended Model (GBC, F1-AUC tuned XGBoost, F1-AUC tuned Random Forest)\n* Untuned GBC","2034efe3":"Much better! Now we can proceed with further processing.","1936137b":"## Removing unusable data\n\nThere are two issues we need to deal with:\n* We cannot use any data after the patient has already been admitted to ICU (that includes the first row with `ICU == 1`).\n* We need to select patients which were admitted to ICU eventually.\n\nThankfully, pandas makes it easy to get the rows we want.","5e141564":"It looks like RF, GBC, CatBoost and XGBoost are by far the best models. The tuned Random Forest classifier has much better performance than its untuned counterpart, too - though other models don't have as much of a dramatic change.","b6a733c8":"If the difference between the metrics obtained during cross-validation and prediction on test data is too big, then that most likely means the model is overfitted. Let's check the percentage difference.","9229cf3c":"## Feature Engineering\n\nAs [Leonardo Canela](https:\/\/www.kaggle.com\/leocanela\/random-forest-to-predict-not-admitted-to-icu) points out, there are two more potentially clinically relevant vital signs that can be created through feature engineering - namely Mean Aertial Pressure (MAP) and Blood Pressure (BP). According to Wikipedia, they are caluclated as following: ","11617138":"## Summary","3df9845b":"By looking at SHAP plots for the models, we can clearly see that age, resporatory rate, lymphocytes count and blood pressure and the most important features, no matter the model. It is not surprising - even with my limited medical knowledge, it lines up with what I know about COVID-19.","532b22b3":"We can choose to keep either `MEDIAN` or `MEAN`. In my experience, `MEDIAN` gave slightly better results than `MEAN`.\n\nAs we can see, the only vitals where it's worth keeping any additional vital sign columns other than median are `BLOODPRESSURE`, `HEART_RATE`, `RESPIRATORY_RATE`, `TEMPERATURE` and `OXYGEN_SATURATION`. We'll keep `DIFF_REL` from those too, as it's not highly correlated with `MEDIAN`.","022fbee6":"We are eventually left with:"}}