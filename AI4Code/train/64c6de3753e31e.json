{"cell_type":{"ab837a92":"code","3529ecc2":"code","04c23d07":"code","d7db6ac8":"code","e428b7b0":"code","b9080022":"code","cbee0cd5":"code","bb69aab8":"code","65fe43b5":"code","54181235":"code","9f73deab":"code","d0f72870":"code","bb7eabe1":"code","c5819618":"code","3a0c1975":"code","2dfc69c9":"code","e7dfac92":"code","2a23aa91":"code","f87666e0":"code","28d6d373":"code","4b642acf":"code","eb422ac8":"code","074b9b08":"code","8d26c2bb":"code","607072e4":"code","009fd0b3":"code","1812188c":"code","40c4fb70":"code","09a41242":"code","59dba107":"code","f84af6a6":"markdown","efa4eb03":"markdown","a4ff855b":"markdown","a2a09426":"markdown","86f3d68e":"markdown","9132a6fa":"markdown","0888f92a":"markdown","ed989e5c":"markdown","f24c25de":"markdown","cc93005c":"markdown","e98169e2":"markdown","43023374":"markdown","e7dbde57":"markdown","0763a8ad":"markdown","c9c4238c":"markdown","1e74faa7":"markdown","f7e2655a":"markdown","8485a8d0":"markdown","c4cb9fc8":"markdown","c693b05a":"markdown","e97c5fa0":"markdown","048d33ba":"markdown","2b5bb737":"markdown","f2efb8e4":"markdown","93ec7b10":"markdown","d4e435a1":"markdown","b782f792":"markdown","38feed92":"markdown","2efab6b5":"markdown","f2551375":"markdown","b46a9cbc":"markdown","a29d4a8a":"markdown","9e8b506c":"markdown","577f8487":"markdown","71720f17":"markdown","f9e7aafb":"markdown","f6bf2ea9":"markdown","4ebef2ae":"markdown","45f24506":"markdown","e747f160":"markdown","688f22f4":"markdown"},"source":{"ab837a92":"# Python libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as ss\n%matplotlib inline\nimport itertools\nimport lightgbm as lgbm\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nfrom datetime import datetime\nimport lightgbm as lgbm\nimport warnings\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nimport warnings\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\nwarnings.filterwarnings('ignore') #ignore warning messages ","3529ecc2":"data = pd.read_csv(r\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","04c23d07":"display(data.head())\ndisplay(data.describe())\ndisplay(data.shape)\ndisplay(data.info())","d7db6ac8":"# Reassign target\ndata.Churn.replace(to_replace = dict(Yes = 1, No = 0), inplace = True)\n\n# Encode as object\ncol_name = ['SeniorCitizen', 'Churn']\ndata[col_name] = data[col_name].astype(object)\n\n# Encode as float\ndata['TotalCharges'] = data['TotalCharges'].replace(\" \", 0).astype('float64')","e428b7b0":"churn = data[(data['Churn'] != 0)]\nno_churn = data[(data['Churn'] == 0)]\n\n#------------COUNT-----------------------\ntrace = go.Bar(\n        x = (data['Churn'].value_counts().values.tolist()), \n        y = ['Churn : no', 'Churn : yes'], \n        orientation = 'h', opacity = 0.8, \n        text=data['Churn'].value_counts().values.tolist(), \n        textfont=dict(size=15),\n        textposition = 'auto',\n        marker=dict(\n        color=['lightblue','gold'],\n        line=dict(color='#000000',width=1.5)\n        ))\n\nlayout = dict(title =  'Count of attrition variable',\n                        autosize = False,\n                        height  = 500,\n                        width   = 800)\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)\n\n#------------PERCENTAGE-------------------\ntrace = go.Pie(labels = ['Churn : no', 'Churn : yes'], values = data['Churn'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightblue','gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of attrition variable',\n                        autosize = False,\n                        height  = 500,\n                        width   = 800)\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","b9080022":"def plot_distribution(var_select, bin_size) : \n    tmp1 = churn[var_select]\n    tmp2 = no_churn[var_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Churn : yes', 'Churn : no']\n    colors = ['gold', 'lightblue']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, curve_type='kde', bin_size = bin_size)\n    \n    fig['layout'].update(title = var_select, autosize = False,\n                        height  = 500,\n                        width   = 800)\n\n    py.iplot(fig, filename = 'Density plot')","cbee0cd5":"plot_distribution('tenure', False)\nplot_distribution('MonthlyCharges', False)\nplot_distribution('TotalCharges', False)","bb69aab8":"def plot_distribution_num(data_select) : \n    sns.set_style(\"ticks\")\n    s = sns.FacetGrid(data, hue = 'Churn',aspect = 2.5, palette ={0 : 'lightblue', 1 : 'gold'})\n    s.map(sns.kdeplot, data_select, shade = True, alpha = 0.8)\n    s.set(xlim=(0, data[data_select].max()))\n    s.add_legend()\n    s.set_axis_labels(data_select, 'proportion')\n    s.fig.suptitle(data_select)\n    plt.show()\n    \nplot_distribution_num('tenure')\nplot_distribution_num('MonthlyCharges')\nplot_distribution_num('TotalCharges')","65fe43b5":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'grey'\nfig = plt.figure(figsize=(18,5))\nalpha = 0.8\n\nplt.subplot(131)\nax1 = sns.scatterplot(x = data['TotalCharges'], y = data['tenure'], hue = \"Churn\",\n                    data = data, palette = palette, edgecolor=edgecolor, alpha = alpha)\nplt.title('TotalCharges vs tenure')\n\nplt.subplot(132)\nax2 = sns.scatterplot(x = data['TotalCharges'], y = data['MonthlyCharges'], hue = \"Churn\",\n                    data = data, palette =palette, edgecolor=edgecolor, alpha = alpha)\nplt.title('TotalCharges vs MonthlyCharges')\n\nplt.subplot(133)\nax2 = sns.scatterplot(x = data['MonthlyCharges'], y = data['tenure'], hue = \"Churn\",\n                    data = data, palette =palette, edgecolor=edgecolor, alpha = alpha)\nplt.title('MonthlyCharges vs tenure')\n\nfig.suptitle('Numeric features', fontsize = 20)\nplt.savefig('1')\nplt.show()","54181235":"df_quant = data.select_dtypes(exclude=[object])\ndf_quant.head()\ncorr_quant = df_quant.corr()\n\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_quant, annot=True, cmap = 'viridis', linewidths = .1, linecolor = 'grey', fmt=\".2f\")\nax.invert_yaxis()\nax.set_title(\"Correlation\")\nplt.show()","9f73deab":"def barplot(var_select, x_no_numeric) :\n    tmp1 = data[(data['Churn'] != 0)]\n    tmp2 = data[(data['Churn'] == 0)]\n    tmp3 = pd.DataFrame(pd.crosstab(data[var_select],data['Churn']), )\n    tmp3['Attr%'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n    if x_no_numeric == True  : \n        tmp3 = tmp3.sort_values(1, ascending = False)\n\n    trace1 = go.Bar(\n        x=tmp1[var_select].value_counts().keys().tolist(),\n        y=tmp1[var_select].value_counts().values.tolist(),\n        text=tmp1[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='Churn : yes',opacity = 0.8, marker=dict(\n        color='gold',\n        line=dict(color='#000000',width=1)))\n\n    trace2 = go.Bar(\n        x=tmp2[var_select].value_counts().keys().tolist(),\n        y=tmp2[var_select].value_counts().values.tolist(),\n        text=tmp2[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='Churn : no', opacity = 0.8, marker=dict(\n        color='lightblue',\n        line=dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x=tmp3.index,\n        y=tmp3['Attr%'],\n        yaxis = 'y2',\n        name='% Churn', opacity = 0.6, marker=dict(\n        color='black',\n        line=dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select),  autosize = False,\n                        height  = 500,\n                        width   = 800,\n              xaxis=dict(), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [-0, 75], \n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= '% Churn'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)","d0f72870":"barplot('gender', True)\nbarplot('SeniorCitizen', True)\nbarplot('Dependents', True)\nbarplot('PhoneService', True)\nbarplot('MultipleLines', True)\nbarplot('InternetService', True)\nbarplot('OnlineSecurity', True)\nbarplot('Partner', True)\nbarplot('OnlineBackup', True)\nbarplot('DeviceProtection', True)\nbarplot('TechSupport', True)\nbarplot('StreamingTV', True)\nbarplot('StreamingMovies', True)\nbarplot('Contract', True)\nbarplot('PaperlessBilling', True)\nbarplot('PaymentMethod', True)","bb7eabe1":"def plot_distribution_cat(feature1,feature2, df): \n    plt.figure(figsize=(18,5))\n    plt.subplot(121)\n    s = sns.countplot(x = feature1, hue='Churn', data = df, \n                      palette = {0 : 'lightblue', 1 :'gold'}, alpha = 0.8, \n                      linewidth = 0.4, edgecolor='grey') \n    s.set_title(feature1)\n    for p in s.patches:\n        s.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+30))\n    \n    plt.subplot(122)\n    s = sns.countplot(x = feature2, hue='Churn', data = df, \n                      palette = {0 : 'lightblue', 1 :'gold'}, alpha = 0.8, \n                      linewidth = 0.4, edgecolor='grey') \n    s.set_title(feature2)\n    for p in s.patches:\n        s.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+30))\n    plt.show()","c5819618":"plot_distribution_cat('SeniorCitizen', 'gender', data)\nplot_distribution_cat('Partner', 'Dependents', data)\n#plot_distribution_cat('MultipleLines', 'InternetService', data)\n#plot_distribution_cat('OnlineSecurity', 'TechSupport', data)\n#plot_distribution_cat('DeviceProtection', 'StreamingTV',data)\n#plot_distribution_cat('StreamingMovies', 'PaperlessBilling',data)\n#plot_distribution_cat('PaymentMethod', 'Contract',data)","3a0c1975":"data.loc[:,'Engaged']=1 \ndata.loc[(data['Contract']=='Month-to-month'),'Engaged']=0\n\ndata.loc[:,'YandNotE']=0\ndata.loc[(data['SeniorCitizen']==0) & (data['Engaged']==0),'YandNotE']=1\n\ndata.loc[:,'ElectCheck']=0 \ndata.loc[(data['PaymentMethod']=='Electronic check') & (data['Engaged']==0),'ElectCheck']=1\n\ndata.loc[:,'fiberopt']=1 \ndata.loc[(data['InternetService']!='Fiber optic'),'fiberopt']=0\n\ndata.loc[:,'StreamNoInt']=1 \ndata.loc[(data['StreamingTV']!='No internet service'),'StreamNoInt']=0\n\ndata.loc[:,'NoProt']=1 \ndata.loc[(data['OnlineBackup']!='No') | (data['DeviceProtection']!='No') | (data['TechSupport']!='No'),'NoProt']=0\n\ndata['TotalServices'] = (data[['PhoneService', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']]== 'Yes').sum(axis=1)","2dfc69c9":"data['tenure'] = pd.cut(data['tenure'], 3)","e7dfac92":"data = data.drop(columns = [\n                            'Contract',\n                            'DeviceProtection', \n                            'Partner'\n                           ])","2a23aa91":"#customer id col\nId_col     = ['customerID']\n#Target columns\ntarget_col = [\"Churn\"]\n#categorical columns\ncat_cols   = data.nunique()[data.nunique() < 10].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col]\n#numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col + Id_col]\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")\ndata = data.drop(['customerID'],axis = 1)","f87666e0":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 210,\n                                           t = 25,b = 210,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)","28d6d373":"correlation_plot()","4b642acf":"#Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = data.corr().abs()\ncorr_matrix.head()\n\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove :' % (len(to_drop)))\n\ndata = data.drop(columns = to_drop)\n\nto_drop","eb422ac8":"correlation_plot()","074b9b08":"# Def X and Y\ny = np.array(data.Churn.tolist())\ndata = data.drop('Churn', 1)\nX = np.array(data.as_matrix())","8d26c2bb":"# Train_test split\nrandom_state = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state)","607072e4":"def model_performance(model) : \n    #Conf matrix\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    trace1 = go.Heatmap(z = conf_matrix  ,x = [\"0 (pred)\",\"1 (pred)\"],\n                        y = [\"0 (true)\",\"1 (true)\"],xgap = 2, ygap = 2, \n                        colorscale = 'Viridis', showscale  = False)\n\n    #Show metrics\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    fp = conf_matrix[0,1]\n    tn = conf_matrix[0,0]\n    Accuracy  =  ((tp+tn)\/(tp+tn+fp+fn))\n    Precision =  (tp\/(tp+fp))\n    Recall    =  (tp\/(tp+fn))\n    F1_score  =  (2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn)))))\n\n    show_metrics = pd.DataFrame(data=[[Accuracy , Precision, Recall, F1_score]])\n    show_metrics = show_metrics.T\n\n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue']\n    trace2 = go.Bar(x = (show_metrics[0].values), \n                    y = ['Accuracy', 'Precision', 'Recall', 'F1_score'], text = np.round_(show_metrics[0].values,4),\n                    textposition = 'auto', textfont=dict(color='black'),\n                    orientation = 'h', opacity = 1, marker=dict(\n            color=colors,\n            line=dict(color='#000000',width=1.5)))\n    \n    #Roc curve\n    model_roc_auc = round(roc_auc_score(y_test, y_score) , 3)\n    fpr, tpr, t = roc_curve(y_test, y_score)\n    trace3 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2), fill='tozeroy')\n    trace4 = go.Scatter(x = [0,1],y = [0,1],\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n    \n    # Precision-recall curve\n    precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n    trace5 = go.Scatter(x = recall, y = precision,\n                        name = \"Precision\" + str(precision),\n                        line = dict(color = ('lightcoral'),width = 2), fill='tozeroy')\n    \n    #Feature importance\n    coefficients  = pd.DataFrame(eval(model).feature_importances_)\n    column_data   = pd.DataFrame(list(data))\n    coef_sumry    = (pd.merge(coefficients,column_data,left_index= True,\n                              right_index= True, how = \"left\"))\n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    coef_sumry = coef_sumry[coef_sumry[\"coefficients\"] !=0]\n    trace6 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\", \n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Viridis\",\n                                  line = dict(width = .6,color = \"black\")))\n    \n    #Cumulative gain\n    pos = pd.get_dummies(y_test).as_matrix()\n    pos = pos[:,1] \n    npos = np.sum(pos)\n    index = np.argsort(y_score) \n    index = index[::-1] \n    sort_pos = pos[index]\n    #cumulative sum\n    cpos = np.cumsum(sort_pos) \n    #recall\n    recall = cpos\/npos \n    #size obs test\n    n = y_test.shape[0] \n    size = np.arange(start=1,stop=369,step=1) \n    #proportion\n    size = size \/ n \n    #plots\n    model = model\n    trace7 = go.Scatter(x = size,y = recall,\n                        line = dict(color = ('gold'),width = 2), fill='tozeroy') \n    \n    #Subplots\n    fig = tls.make_subplots(rows=4, cols=2, print_grid=False,\n                          specs=[[{}, {}], \n                                 [{}, {}],\n                                 [{'colspan': 2}, None],\n                                 [{'colspan': 2}, None]],\n                          subplot_titles=('Confusion Matrix',\n                                          'Metrics',\n                                          'ROC curve'+\" \"+ '('+ str(model_roc_auc)+')',\n                                          'Precision - Recall curve',\n                                          'Cumulative gains curve',\n                                          'Feature importance'\n                                          ))\n    \n    \n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,2,1)\n    fig.append_trace(trace4,2,1)\n    fig.append_trace(trace5,2,2)\n    fig.append_trace(trace6,4,1)\n    fig.append_trace(trace7,3,1)\n    \n    fig['layout'].update(showlegend = False, title = '<b>Model performance report<\/b><br>'+str(model),\n                        autosize = False, height = 1500,width = 830,\n                        plot_bgcolor = 'black',\n                        paper_bgcolor = 'black',\n                        margin = dict(b = 195), font=dict(color='white'))\n    fig[\"layout\"][\"xaxis1\"].update(dict(color = 'white'),showgrid=False)\n    fig[\"layout\"][\"yaxis1\"].update(dict(color = 'white'),showgrid=False)\n    fig[\"layout\"][\"xaxis2\"].update((dict(range=[0, 1], color = 'white')),showgrid=False)\n    fig[\"layout\"][\"yaxis2\"].update(dict(color = 'white'),showgrid=False)\n    fig[\"layout\"][\"xaxis3\"].update(dict(title = \"false positive rate\"),showgrid=False)\n    fig[\"layout\"][\"yaxis3\"].update(dict(title = \"true positive rate\"),color = 'white',showgrid=False)\n    fig[\"layout\"][\"xaxis4\"].update(dict(title = \"recall\"), range = [0,1.05],color = 'white',showgrid=False)\n    fig[\"layout\"][\"yaxis4\"].update(dict(title = \"precision\"), range = [0,1.05],color = 'white',showgrid=False)\n    fig[\"layout\"][\"xaxis5\"].update(dict(title = \"Percentage contacted\"),color = 'white',showgrid=False)\n    fig[\"layout\"][\"yaxis5\"].update(dict(title = \"Percentage positive targeted\"),color = 'white',showgrid=False)\n    fig[\"layout\"][\"xaxis6\"].update(dict(color = 'white'),showgrid=False)\n    fig[\"layout\"][\"yaxis6\"].update(dict(color = 'white'),showgrid=False)\n    for i in fig['layout']['annotations']:\n        i['font'] = titlefont=dict(color='white', size = 14)\n    py.iplot(fig)","009fd0b3":"# Cross val metric\ndef cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","1812188c":"%%time\nlgbm_clf = lgbm.LGBMClassifier(n_estimators=1500, random_state = 42)\n\nlgbm_clf.fit(X_train, y_train)\nlgbm_clf.fit(X_train, y_train)\ny_pred = lgbm_clf.predict(X_test)\ny_score = lgbm_clf.predict_proba(X_test)[:,1]\n\nmodel_performance('lgbm_clf')","40c4fb70":"fit_params = {\"early_stopping_rounds\" : 50, \n             \"eval_metric\" : 'binary', \n             \"eval_set\" : [(X_test,y_test)],\n             'eval_names': ['valid'],\n             'verbose': 0,\n             'categorical_feature': 'auto'}\n\nparam_test = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n              'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000, 3000, 5000],\n              'num_leaves': sp_randint(6, 50), \n              'min_child_samples': sp_randint(100, 500), \n              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n              'subsample': sp_uniform(loc=0.2, scale=0.8), \n              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\n#number of combinations\nn_iter = 200\n\n#intialize lgbm and lunch the search\nlgbm_clf = lgbm.LGBMClassifier(random_state=random_state, silent=True, metric='None', n_jobs=4)\ngrid_search = RandomizedSearchCV(\n    estimator=lgbm_clf, param_distributions=param_test, \n    n_iter=n_iter,\n    scoring='accuracy',\n    cv=5,\n    refit=True,\n    random_state=random_state,\n    verbose=True)\n\ngrid_search.fit(X_train, y_train, **fit_params)\nprint('Best params: {} '.format(grid_search.best_params_))\n\nopt_parameters =  grid_search.best_params_","09a41242":"%%time\nlgbm_clf = lgbm.LGBMClassifier(**opt_parameters)\n\nlgbm_clf.fit(X_train, y_train)\ny_pred = lgbm_clf.predict(X_test)\ny_score = lgbm_clf.predict_proba(X_test)[:,1]\n\nmodel_performance('lgbm_clf')","59dba107":"cross_val_metrics(lgbm_clf)","f84af6a6":"* **customerID** : Customer ID\n* **gende**r : Whether the customer is a male or a female\n* **SeniorCitizen** : Whether the customer is a senior citizen or not (1, 0)\n* **Partner** : Whether the customer has a partner or not (Yes, No)\n* **Dependents** : Whether the customer has dependents or not (Yes, No)\n* **tenure** : Number of months the customer has stayed with the company\n* **PhoneService** : Whether the customer has a phone service or not (Yes, No)\n* **MultipleLines** : Whether the customer has multiple lines or not (Yes, No, No phone service)\n* **InternetService** : Customer\u2019s internet service provider (DSL, Fiber optic, No)\n* **OnlineSecurity** : Whether the customer has online security or not (Yes, No, No internet service)\n* **OnlineBackup** : Whether the customer has online backup or not (Yes, No, No internet service)\n* **DeviceProtection** : Whether the customer has device protection or not (Yes, No, No internet service)\n* **TechSupport** : Whether the customer has tech support or not (Yes, No, No internet service)\n* **StreamingTV** : Whether the customer has streaming TV or not (Yes, No, No internet service)\n* **StreamingMovies** : Whether the customer has streaming movies or not (Yes, No, No internet service)\n* **Contract** : The contract term of the customer (Month-to-month, One year, Two year)\n* **PaperlessBilling** : Whether the customer has paperless billing or not (Yes, No)\n* **PaymentMethod** : The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n* **MonthlyCharges** : The amount charged to the customer monthly\n* **TotalCharges** : The total amount charged to the customer\n* **Churn** : Whether the customer churned or not (Yes or No)","efa4eb03":"![](https:\/\/image.noelshack.com\/fichiers\/2019\/02\/4\/1547131561-report.jpg)","a4ff855b":"## <a id='1.3'>1.3. Head, describe, shape and info<\/a>","a2a09426":"----------\n**Telco - EDA + LightGBM + Stylized Report**\n=====================================\n\n* ***LGBM : Test - Roc_auc              =  .86***\n* ***LGBM : CV - Roc_auc (5 folds) =  .85***\n\n***Vincent Lugat***\n\n*January 2019*\n\n----------","86f3d68e":"# <a id='3'>3. Feature engineering and selection<\/a>","9132a6fa":"## <a id='3.4'>3.4. Correlation Matrix<\/a>","0888f92a":"To measure the performance of a model, we need several elements :\n\nThis part is essential\n\n* **Confusion matrix** : also known as the error matrix, allows visualization of the performance of an algorithm :\n\n    * true positive (TP) : Diabetic correctly identified as diabetic\n    * true negative (TN) : Healthy correctly identified as healthy\n    * false positive (FP) : Healthy incorrectly identified as diabetic\n    * false negative (FN) : Diabetic incorrectly identified as healthy\n\n![](https:\/\/image.noelshack.com\/fichiers\/2018\/20\/5\/1526651914-cs-heezweaa5hp7.jpg)\n\n* **Metrics ** :\n\n    * Accuracy : (TP +TN) \/ (TP + TN + FP +FN)\n    * Precision : TP \/ (TP + FP)\n    * Recall : TP \/ (TP + FN)\n    * F1 score : 2 x ((Precision x Recall) \/ (Precision + Recall))\n\n* **Roc Curve** : The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549284841-0-0-0-0-0-0-0-0-0-0.png)\n\n* **Precision Recall Curve** :  shows the tradeoff between precision and recall for different threshold","ed989e5c":"#  <a id='6'>6. Credits<\/a>","f24c25de":"## <a id='1.1'>1.1. Load libraries<\/a>","cc93005c":"## <a id='4.3'>4.3. Stylized report with Plotly (confusion matrix, metrics, roc, precision-recall, etc\u2026)<\/a>","e98169e2":"## <a id='5.3'>5.3. LightGBM - After RandomizedSearchCV<\/a>","43023374":"## <a id='3.3'>3.3. Features encoding and scaling<\/a>","e7dbde57":"## <a id='5.1'>5.1. LightGBM - Before RandomizedSearchCV<\/a>","0763a8ad":"## <a id='1.4'>1.4. Reassign target, encode variables and replace missing values<\/a>","c9c4238c":"## <a id='2.2'>2.2. Numeric features : Plotly<\/a>","1e74faa7":"#  <a id='5'>5. Light GBM Model<\/a>","f7e2655a":"## <a id='2.4'>2.4. Numeric features : Pairplots<\/a>","8485a8d0":"# <a id='2'>2. Exploratory Data Analysis (EDA)<\/a>","c4cb9fc8":"**Thank you all ! Merci \u00e0 tous ! :)**","c693b05a":"https:\/\/www.kaggle.com\/pavanraj159 (plotly master)","e97c5fa0":"# <a id='1'>1. Load libraries and read the data<\/a>","048d33ba":"## <a id='4.1'>4.1. Define (X,  y)<\/a>","2b5bb737":"## <a id='1.2'>1.2. Read the data<\/a>","f2efb8e4":"## <a id='2.6'>2.6. Object features : Plotly <\/a>","93ec7b10":"## <a id='5.2'>5.2. LightGBM - RandomizedSearchCV to optimise hyperparameters  (1000 fits)<\/a>","d4e435a1":"## <a id='3.5'>3.5. Remove collinear features<\/a>","b782f792":"## <a id='5.4'>5.4. LightGBM \u2013 Cross validation (5 folds)<\/a>","38feed92":"## <a id='3.2'>3.2. Drop some features<\/a>","2efab6b5":"## <a id='2.5'>2.5. Numeric features : Correlation<\/a>","f2551375":"![](http:\/\/image.noelshack.com\/fichiers\/2019\/06\/1\/1549285917-0000000000000000000.png)\n\n** LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel and GPU learning.\n* Capable of handling large-scale data.","b46a9cbc":"## <a id='3.1'>3.1. New features<\/a>","a29d4a8a":"To find the best hyperparameters, we'll use Random Search CV.\n\nRandom search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. \nGenerally RS is more faster and accurate than GridSearchCV who calculate all possible combinations. With Random Grid we specify the number of combinations that we want","9e8b506c":"**Telcom Customer Churn**\n\nEach row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\nThe raw data contains 7043 rows (customers) and 21 columns (features).\nThe \u201cChurn\u201d column is our target.","577f8487":"## <a id='4.4'>4.4. Define cross validation metrics<\/a>","71720f17":"## <a id='2.7'>2.7. Object features : Seaborn <\/a>","f9e7aafb":"## <a id='2.1'>2.1. Target distribution (number and %)<\/a>","f6bf2ea9":"## <a id='2.3'>2.3. Numeric features : Seaborn <\/a>","4ebef2ae":"## <a id='4.2'>4.2. Train test split <\/a>","45f24506":"# <a id='4'>4. Prepare dataset and stylized report<\/a>","e747f160":"* **LightGBM : Hyperparameters ** :\n\n    * learning_rate : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates\n    * n_estimators : number of trees (or rounds)\n    * num_leaves : number of leaves in full tree, default: 31\n    * min_child_samples : minimal number of data in one leaf. Can be used to deal with over-fitting\n    * min_child_weight : minimal sum hessian in one leaf.\n    * subsample : randomly select part of data without resampling\n    * max_depth : It describes the maximum depth of tree. This parameter is used to handle model overfitting.\n    * colsample_bytree : LightGBM will randomly select part of features on each iteration if colsample_bytree smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree\n    * reg_alpha : regularization\n    * reg_lambda : regularization\n    \n    * early_stopping_rounds : This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds. This will reduce excessive iterations","688f22f4":"- <a href='#1'>1. Load libraries and read the data<\/a>  \n    - <a href='#1.1'>1.1. Load libraries<\/a> \n    - <a href='#1.2'>1.2. Read the data<\/a> \n    - <a href='#1.3'>1.3. Head, describe, shape and info<\/a> \n    - <a href='#1.4'>1.4. Reassign target, encode variables and replace missing values<\/a> \n- <a href='#2'>2. Exploratory Data Analysis (EDA)<\/a> \n    - <a href='#2.1'>2.1. Target distribution (number and %)<\/a> \n    - <a href='#2.2'>2.2. Numeric features : Plotly<\/a> \n    - <a href='#2.3'>2.3. Numeric features : Seaborn <\/a> \n    - <a href='#2.4'>2.4. Numeric features : Pairplots<\/a> \n    - <a href='#2.5'>2.5. Numeric features : Correlation<\/a>\n    - <a href='#2.6'>2.6. Object features : Plotly <\/a>\n    - <a href='#2.7'>2.7. Object features : Seaborn <\/a>\n- <a href='#3'>3. Feature engineering and selection<\/a>\n    - <a href='#3.1'>3.1. New features<\/a> \n    - <a href='#3.2'>3.2. Drop some features<\/a> \n    - <a href='#3.3'>3.3. Features encoding and scaling<\/a>\n    - <a href='#3.4'>3.4. Correlation Matrix<\/a>\n    - <a href='#3.5'>3.5. Remove collinear features<\/a>\n- <a href='#4'>4. Prepare dataset and stylized report<\/a>\n    - <a href='#4.1'>4.1. Define (X,  y)<\/a> \n    - <a href='#4.2'>4.2. Train test split <\/a> \n    - <a href='#4.3'>4.3. Stylized report with Plotly (confusion matrix, metrics, roc, precision-recall, etc\u2026)<\/a>\n    - <a href='#4.4'>4.4. Define cross validation metrics<\/a>\n- <a href='#5'>5. Light GBM Model<\/a> \n    - <a href='#5.1'>5.1. LightGBM - Before RandomizedSearchCV<\/a> \n    - <a href='#5.2'>5.2. LightGBM - RandomizedSearchCV to optimise hyperparameters  (1000 fits)<\/a>\n    - <a href='#5.3'>5.3. LightGBM - After RandomizedSearchCV<\/a>\n    - <a href='#5.4'>5.4. LightGBM \u2013 Cross validation (5 folds)<\/a>\n- <a href='#6'>6. Credits<\/a> "}}