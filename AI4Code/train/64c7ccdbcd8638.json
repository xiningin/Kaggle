{"cell_type":{"a5c8bc4d":"code","79929e35":"code","02ed58f1":"code","93386eb9":"code","4010c0e2":"code","aea17702":"code","34880949":"code","0f8dc8c3":"code","aa253bf2":"code","eb4c471a":"code","58f32b36":"code","e4edfe71":"code","0fdcea5f":"code","647e4de0":"code","b5fc2568":"code","b670a0e9":"code","71cc345f":"code","ffaf493d":"code","01c44e2d":"code","e4685786":"code","2d10bfcb":"code","8f02c4ac":"code","2e6adb40":"markdown"},"source":{"a5c8bc4d":"import nltk.data;\n\nfrom gensim.models import word2vec;\n\nfrom sklearn.cluster import KMeans;\nfrom sklearn.neighbors import KDTree;\n\nimport pandas as pd;\nimport numpy as np;\n\nimport os;\nimport re;\nimport logging;\nimport sqlite3;\nimport time;\nimport sys;\nimport multiprocessing;\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt;\nfrom itertools import cycle;","79929e35":"model = word2vec.Word2Vec.load('..\/input\/reddit_word2vec\/model_full_reddit');","02ed58f1":"Z = model.wv.vectors","93386eb9":"print(Z[0].shape)\nprint(Z[0])","4010c0e2":"def clustering_on_wordvecs(word_vectors, num_clusters):\n    # Initalize a k-means object and use it to extract centroids\n    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n    idx = kmeans_clustering.fit_predict(word_vectors);\n    \n    return kmeans_clustering.cluster_centers_, idx;","aea17702":"start = time.time();\ncenters, clusters = clustering_on_wordvecs(Z, 50);\nprint('Total time: ' + str((time.time() - start)) + ' secs')","34880949":"start = time.time();\ncentroid_map = dict(zip(model.wv.index2word, clusters));\nprint('Total time: ' + str((time.time() - start)) + ' secs')","0f8dc8c3":"def get_top_words(index2word, k, centers, wordvecs):\n    tree = KDTree(wordvecs);\n\n    #Closest points for each Cluster center is used to query the closest 20 points to it.\n    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n    closest_words_idxs = [x[1] for x in closest_points];\n\n    #Word Index is queried for each position in the above array, and added to a Dictionary.\n    closest_words = {};\n    for i in range(0, len(closest_words_idxs)):\n        closest_words['Cluster #' + str(i+1).zfill(2)] = [index2word[j] for j in closest_words_idxs[i][0]]\n\n    #A DataFrame is generated from the dictionary.\n    df = pd.DataFrame(closest_words);\n    df.index = df.index+1\n\n    return df;\n","aa253bf2":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","eb4c471a":"top_words = get_top_words(model.wv.index2word, 20, centers, Z);","58f32b36":"top_words","e4edfe71":"def display_cloud(cluster_num, cmap):\n    wc = WordCloud(background_color=\"black\", max_words=2000, max_font_size=80, colormap=cmap);\n    wordcloud = wc.generate(' '.join([word for word in top_words['Cluster #' + str(cluster_num).zfill(2)]]))\n\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.savefig('cluster_' + str(cluster_num), bbox_inches='tight')","0fdcea5f":"cmaps = cycle([\n            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg', 'hsv',\n            'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar'])\n\n\ncol = next(cmaps)\ndisplay_cloud(48, col)","647e4de0":"def get_word_table(table, key, sim_key='similarity', show_sim = True):\n    if show_sim == True:\n        return pd.DataFrame(table, columns=[key, sim_key])\n    else:\n        return pd.DataFrame(table, columns=[key, sim_key])[key]","b5fc2568":"get_word_table(model.wv.most_similar_cosmul(positive=['king', 'woman'], negative=['queen']), 'Analogy')","b670a0e9":"model.wv.doesnt_match(\"apple microsoft samsung tesla\".split())","71cc345f":"model.wv.doesnt_match(\"trump clinton sanders obama\".split())","ffaf493d":"model.wv.doesnt_match(\"joffrey cersei tywin lannister jon\".split())","01c44e2d":"model.wv.doesnt_match(\"daenerys rhaegar viserion aemon aegon jon targaryen\".split())","e4685786":"keys = ['musk', 'modi', 'hodor', 'martell', 'apple', 'neutrality', 'snowden', 'batman', 'hulk', 'warriors', 'falcons', 'pizza', ];\ntables = [];\nfor key in keys:\n    tables.append(get_word_table(model.wv.similar_by_word(key), key, show_sim=False))","2d10bfcb":"pd.concat(tables, axis=1)","8f02c4ac":"from gensim.models.keyedvectors import KeyedVectors\nmodel.wv.save_word2vec_format('reddit_word2vec.txt', binary=False)","2e6adb40":"From https:\/\/github.com\/ravishchawla\/word_2_vec"}}