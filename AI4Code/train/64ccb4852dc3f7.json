{"cell_type":{"3a8f5296":"code","cbc1f88d":"code","615a4e61":"code","a58389ee":"code","ee80bccb":"code","a86785c8":"code","3ce54647":"code","0ac4071d":"code","8f00ed85":"code","4fcf6ae6":"code","05f80137":"code","a234266e":"code","13656b77":"code","1fe04739":"code","ded01f81":"code","4b25b469":"code","5cd474b4":"code","a678d468":"code","3290ac7d":"code","66bb02e9":"code","265ebffe":"code","bdc7b584":"code","1d63ed73":"code","2c3a019c":"code","a965466a":"code","a4bf5e18":"code","fdb95e05":"code","85e6be5c":"code","daecc96d":"code","b60f8595":"code","5fe9f0ce":"code","bb2d92ee":"code","e1dfefa7":"code","32d54434":"code","1a4e52c8":"code","8dad433e":"code","46a88e1b":"code","c47c8678":"code","a362b13e":"code","2dfdbf02":"code","2635dd0e":"code","51d2f2ad":"code","f5f1dcb1":"code","de9c5c05":"code","b2d04a69":"code","55200091":"code","19a1f4b1":"code","5e2707b5":"code","4423c665":"code","dfd40b3a":"code","aa7fa9dc":"code","04bc2070":"code","03b91752":"markdown","89eedaaa":"markdown","47ba08df":"markdown","756442b1":"markdown","3c6767f2":"markdown","f576f149":"markdown","d285d0f2":"markdown","71407404":"markdown","bd6fad69":"markdown","a926c875":"markdown","4446bcba":"markdown","898018b4":"markdown","59a835c5":"markdown","190941b3":"markdown","54460ede":"markdown","03585c44":"markdown","a164d23a":"markdown","ab0fab7d":"markdown","1dc8cd2c":"markdown","0c8e97e3":"markdown","045d76a1":"markdown","02106877":"markdown","893a591a":"markdown","d52f217f":"markdown","e3bd3064":"markdown","1c0c6d07":"markdown","cc4819df":"markdown","804eac17":"markdown","a6d6aa6c":"markdown","0a6d2cd8":"markdown","196f62d6":"markdown","81d369a3":"markdown","7ebec083":"markdown","1714fa63":"markdown","6bfe4d3a":"markdown","0cf7280f":"markdown","a3df5d9f":"markdown"},"source":{"3a8f5296":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\")   #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\nimport warnings\nwarnings.simplefilter(action='ignore')","cbc1f88d":"# let's import the data\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\nprint('The number of samples into the train data is {}.'.format(train_df.shape[0]))\nprint('The number of samples into the test data is {}.'.format(test_df.shape[0]))","615a4e61":"train_df.head()","a58389ee":"train_df.isnull().sum()","ee80bccb":"temp_df = train_df[['Sex','Age']].groupby(['Sex'],as_index=False).mean().sort_values(by = 'Age', ascending = False)\ntemp_df['Age'] = round(temp_df.Age)\na_dict = dict(temp_df.values)\nprint(a_dict)\n# filling null values with mean values group by gender\ntrain_df_m = train_df[train_df['Sex'] == 'male']\ntrain_df_f = train_df[train_df['Sex'] == 'female']\ntrain_df_m['Age'].fillna(round(a_dict['male']),inplace = True)\ntrain_df_f['Age'].fillna(round(a_dict['female']),inplace= True)\n\n# Concatenating the dataframes\ntrain_df = pd.concat([train_df_m, train_df_f])\n# Sort the datafeame based on PassengerId \ntrain_df = train_df.sort_values(by='PassengerId',ascending='True')\n\ntrain_df.head()","a86785c8":"print(train_df['Cabin'].mode())\n## replacing missing value in Cabin field with mode value\ntrain_df['Cabin'].fillna(train_df['Cabin'].mode()[0], inplace=True)","3ce54647":"print(train_df['Embarked'].mode())\n## replacing missing value in Embarked field with mode value\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)","0ac4071d":"train_df.isnull().sum()","8f00ed85":"print(train_df['Survived'].value_counts())\nsns.countplot(x='Survived', data=train_df, hue='Survived')","4fcf6ae6":"# percentage of Survived passenger in train dataset\nprint('percentage of Survived passenger in train dataset is %.2f%%' %(((train_df['Survived'] == 1).sum()\/train_df.shape[0])*100))\n# percentage of fraud claim reported in train dataset\nprint('percentage of non Survived passenger in train dataset is %.2f%%' %(((train_df['Survived'] == 0).sum()\/train_df.shape[0])*100))\n","05f80137":"train_df.describe()","a234266e":"# check the correlation \ntrain_df.corr()","13656b77":"## plotting correlation headmap \nf, ax = plt.subplots(figsize=(10,5))\ndataplots = sns.heatmap(train_df.corr(),cmap=\"YlGnBu\", annot=False)","1fe04739":"f, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='Pclass', data=train_df, hue = 'Survived')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=0)     # for readibility\nplt.show()","ded01f81":"f, ax = plt.subplots(figsize=(15, 10))\nplot = sns.countplot(x='Age', data=train_df, hue='Survived')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90)     # for readibility\nplt.show()","4b25b469":"# Count plot for SibSp - Siblings and spouse onboared\nf, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='SibSp', data=train_df, hue='Survived')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90)     # for readibility\nplt.show()","5cd474b4":"print(train_df['Embarked'].value_counts())\n# Count plot for Cabin and survivals\nf, ax = plt.subplots(figsize=(10, 10))\nplot = sns.countplot(x='Embarked', data=train_df,hue='Survived')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=90)     # for readibility\nplt.show()","a678d468":"f, ax = plt.subplots(figsize=(10, 5))\nplot = sns.countplot(x='Sex', data=train_df, hue='Survived')\nplot.set_xticklabels(plot.get_xticklabels(), rotation=0)     # for readibility\nplt.show()","3290ac7d":"#### Step 4.1 Encode the Sex Columns to numberical value\ntrain_df['Sex'] = train_df['Sex'].replace(('male','female'),(1,0))","66bb02e9":"# The Cabin suggest probably the location of passenger.\n# The first Character of Cabin might tells about the desk number or \n# location of passenger and might be an importent featue for passenger survival\ntrain_df['Cabin']= train_df['Cabin'].str.slice(0, 1)","265ebffe":"train_df['Cabin'].value_counts","bdc7b584":"### Step 4.2 Correlation between Cabin and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['Cabin','Survived']].groupby(['Cabin'],as_index=False).mean().sort_values(by = 'Cabin', ascending = False)\na_dict_cab = dict(temp_df.values)\n#Lets perform target encoding for Cabin\ntrain_df['Cabin'] = train_df['Cabin'].replace(a_dict_cab)","1d63ed73":"### Step 4.3 Correlation between Embarked and target variable, perfrom target encoding using correlation factor. \ntemp_df = train_df[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values(by = 'Embarked', ascending = False)\na_dict_emb = dict(temp_df.values)\n#Lets perform target encoding for Cabin\ntrain_df['Embarked'] = train_df['Embarked'].replace(a_dict_emb)","2c3a019c":"## Drop the name and Ticket columns\n#### Step 4.1 Drop the dates columns\ntrain_df.drop(['Name','Ticket'],axis=1, inplace=True)","a965466a":"train_df.head()","a4bf5e18":"### Split the train dataset into dependent and independent variable\nX = train_df.drop(['Survived'], axis = 1)\ny = train_df['Survived']\nprint(\"Shape of X :\", X.shape)\nprint(\"Shape of y :\", y.shape)","fdb95e05":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, stratify=y, random_state=42)\nprint(\"Shape of X_train :\", X_train.shape)\nprint(\"Shape of X_test :\", X_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","85e6be5c":"y_train.value_counts()","daecc96d":"import imblearn\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\ncounter = Counter(y_train)\nprint('Before Counter:', counter)\n#Oversampling the train dataset using SMOTE\nsmt = SMOTE()\nX_train, y_train = smt.fit_resample(X_train, y_train)\ncounter = Counter(y_train)\nprint('After Counter:', counter)","b60f8595":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nestimator = LogisticRegression(solver='liblinear')\nselector = RFE(estimator, n_features_to_select=7, step=1)\nselector = selector.fit(X_train, y_train)\n# summarize the selection of the attributes\nSelected_Feature = list(X_train.columns[selector.support_])\nprint('Selected features: %s' % Selected_Feature)\nprint('Feature ranking: %s' % selector.ranking_)","5fe9f0ce":"## Feature ranking with recursive feature elimination and cross-validation\nfrom sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator,step=1,cv=10, scoring='accuracy')\nrfecv.fit(X_train, y_train)\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X_train.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","bb2d92ee":"X_train = X_train[Selected_Feature]   # select only features extracted by RFE\nX_test = X_test[Selected_Feature]     # select only features extracted by RFE","e1dfefa7":"logreg = LogisticRegression()\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nlogreg.fit(X_train,y_train)     # model training\ny_pred = logreg.predict(X_test) # model testing \n\nprint(\"Training Accuracy: \", logreg.score(X_train, y_train))\nprint('Testing Accuarcy: ', logreg.score(X_test, y_test))\n\n# making a classification report\ncr = classification_report(y_test,  y_pred)\nprint(cr)\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (8,5))\nsns.set(font_scale=1.5)      #for label size\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16},fmt='d')    # font size","32d54434":"from sklearn.model_selection import cross_val_score\n# 10-fold cross-validation logistic regression\nlogreg_crossval = LogisticRegression(solver='liblinear')\n# Use cross_val_score function\n# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the data\n# cv=10 for 10 folds\n# scoring = {'accuracy', 'neg_log_loss', 'roc_auc'} for evaluation metric - althought they are many\nscores_accuracy = cross_val_score(logreg_crossval, X, y, cv=10, scoring='accuracy')\nscores_log_loss = cross_val_score(logreg_crossval, X, y, cv=10, scoring='neg_log_loss')\nscores_auc = cross_val_score(logreg_crossval, X, y, cv=10, scoring='roc_auc')\nprint('K-fold cross-validation results:')\nprint(logreg.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\nprint(logreg.__class__.__name__+\" average log_loss is %2.3f\" % scores_log_loss.mean())\nprint(logreg.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())","1a4e52c8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y,test_size=0.20, stratify=y, random_state=42)\nprint(\"Shape of X_train :\", X_train_rf.shape)\nprint(\"Shape of X_test :\", X_test_rf.shape)\nprint(\"Shape of y_train :\", y_train_rf.shape)\nprint(\"Shape of y_test :\", y_test_rf.shape)","8dad433e":"import imblearn\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\ncounter = Counter(y_train_rf)\nprint('Before Counter:', counter)\n#Oversampling the train dataset using SMOTE\nsmt = SMOTE()\nX_train_rf, y_train_rf = smt.fit_resample(X_train_rf, y_train_rf)\ncounter = Counter(y_train_rf)\nprint('After Counter:', counter)","46a88e1b":"\nsel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\nsel.fit(X_train_rf, y_train_rf)\n\nsel.get_support()    # to get which feature is important\n\nselected_feat= X_train_rf.columns[(sel.get_support())]\nselected_feat","c47c8678":"X_train_rf = X_train_rf[selected_feat]\nX_test_rf  = X_test_rf[selected_feat]","a362b13e":"# creating a RF classifier\nclf = RandomForestClassifier(n_estimators = 200) \n\n# Training the model on the training dataset\n# fit function is used to train the model using the training sets as parameters\nclf.fit(X_train_rf, y_train_rf)\n\n# performing predictions on the test dataset\ny_pred_rf = clf.predict(X_test_rf)\n\n# metrics are used to find accuracy or error\nfrom sklearn import metrics \n  \n# using metrics module for accuracy calculation\nprint(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test_rf, y_pred_rf))\n","2dfdbf02":"from sklearn.metrics import mean_squared_error\nfinal_mse = mean_squared_error(y_test_rf, y_pred_rf)\nfinal_rmse = np.sqrt(final_mse)\nprint('The final RMSE on the test set is', round(final_rmse, 2))","2635dd0e":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in range(200,2000,200)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","51d2f2ad":"# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 10 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train_rf, y_train_rf)\nrf_random.best_params_","f5f1dcb1":"# creating a RF classifier\nclf_hyp = RandomForestClassifier(n_estimators=1400, criterion='gini', min_samples_split=10, min_samples_leaf=1,\n                                 max_features='auto', bootstrap=True, n_jobs=-1, random_state=42)\n\n# Training the model on the training dataset\n# fit function is used to train the model using the training sets as parameters\nclf_hyp.fit(X_train_rf, y_train_rf)\n\n# performing predictions on the test dataset\ny_pred_rf = clf_hyp.predict(X_test_rf)\n\n# metrics are used to find accuracy or error\nfrom sklearn import metrics  \n  \n# using metrics module for accuracy calculation\nprint(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test_rf, y_pred_rf))\n\nfinal_mse = mean_squared_error(y_test_rf, y_pred_rf)\nfinal_rmse = np.sqrt(final_mse)\nprint('The final RMSE on the test set is', round(final_rmse, 2))","de9c5c05":"from xgboost import XGBClassifier\nxgbcl = XGBClassifier(n_estimators=100)\nxgbcl.fit(X_train_rf,y_train_rf)   # train the model using Xgboost\n\n#predict the values\ny_xgbcl = xgbcl.predict(X_test_rf)\n\n## Accuracy of XGBoost\nprint(\"XGBoost Accuracy: \", accuracy_score(y_xgbcl,y_test_rf))","b2d04a69":"from xgboost import XGBRFClassifier\nmodel_XGBRF = XGBRFClassifier(n_estimators=100, subsample=0.9)\nmodel_XGBRF.fit(X_train_rf,y_train_rf)   # train the model using XGBRF\n#predict the values\ny_xgbrf = model_XGBRF.predict(X_test_rf)\n## Accuracy of XGBRF\nprint(\"XGBRF Accuracy: \", accuracy_score(y_xgbrf,y_test_rf)) ","55200091":"models = pd.DataFrame({\n    'Model': ['Logistic regression', 'K fold cross validation', 'Random Forest', \n              'XGBoost', 'XGBoost rf'],\n    'Score': [logreg.score(X_train, y_train), scores_accuracy.mean(), metrics.accuracy_score(y_test_rf, y_pred_rf), \n              accuracy_score(y_xgbcl,y_test_rf), accuracy_score(y_xgbrf,y_test_rf)]})\nmodels.sort_values(by='Score', ascending=False)","19a1f4b1":"test_df_bkp = test_df.copy()","5e2707b5":"\n## Applying same imputation on test data sets\ntemp_df = test_df[['Sex','Age']].groupby(['Sex'],as_index=False).mean().sort_values(by = 'Age', ascending = False)\ntemp_df['Age'] = round(temp_df.Age)\na_dict = dict(temp_df.values)\n### resolving null values in test dataset \ntest_df_m = test_df[test_df['Sex'] == 'male']\ntest_df_f = test_df[test_df['Sex'] == 'female']\ntest_df_m['Age'].fillna(round(a_dict['male']),inplace = True)\ntest_df_f['Age'].fillna(round(a_dict['female']),inplace= True)\n\n# Concatenating the dataframes\ntest_df = pd.concat([test_df_m, test_df_f])\n# Sort the datafeame based on PassengerId \ntest_df = test_df.sort_values(by='PassengerId',ascending='True')\n\n## replacing missing value in Cabin field with mode value\ntest_df['Cabin'].fillna(test_df['Cabin'].mode()[0], inplace=True)\n\n## replacing missing value in Embarked field with mode value\ntest_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace=True)\n\n#### Step 5.1 Encode the Sex Columns to numberical value\ntest_df['Sex'] = test_df['Sex'].replace(('male','female'),(1,0))\n\n# The first Character of location of passenger and might be an importent featue for passenger survival\ntest_df['Cabin']= test_df['Cabin'].str.slice(0, 1)\n\n#Lets perform target encoding for Cabin\ntest_df['Cabin'] = test_df['Cabin'].replace(a_dict_cab)\n\n#Lets perform target encoding for Cabin\ntest_df['Embarked'] = test_df['Embarked'].replace(a_dict_emb)\n\n## Drop the name and Ticket columns\n#### Step 4.1 Drop the dates columns\ntest_df.drop(['Name','Ticket'],axis=1, inplace=True)\n\ntest_df_val = test_df[Selected_Feature]","4423c665":"test_df_val.head(3)","dfd40b3a":"#predict the values\npred_test = logreg.predict(test_df_val)","aa7fa9dc":"ids = test_df['PassengerId']\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': pred_test })\noutput.head(15)","04bc2070":"output.to_csv('submission.csv', index=False)","03b91752":"**Observation : It looks like the chances of passenger to survive is more if he have no siblings or spouse**","89eedaaa":"### Step 9.4.3 : Best feature selection from model. ","47ba08df":"### Filling null values for Age column","756442b1":"### Step 9.4.2 : Oversampling of minority class using SMOTE to resolve underfitting due to bias sampling.","3c6767f2":"### Step 6 : Feature selection using recursive feature elemination ( rfe )","f576f149":"### Step 9.6 : Use the random grid to search for best hyperparameters","d285d0f2":"### Step 11 : model tuning using XGBoost for random forest classifier \n","71407404":"### Step 12 : Based on above score I have decided to use Logistic regression model for testing data","bd6fad69":"**Observation : From above count plot it looks like that probability of passenger survival is high for Pclass = '1'**","a926c875":"### Step 9.7 Random Forest evaluation with hyperparamters ","4446bcba":"**Observation : Looks like the embarked = 'S' have higher probability of passenger survival**","898018b4":"## Step 1 : Import ML packages and Libraries","59a835c5":"### Step3.2 : Descriptive Analysis\nIt will demonstrate the count, mean, std dev, min, max, etc values for the Numerical features present in the data.","190941b3":"### Import titanic (train and test) dataset for model building","54460ede":"### Step 5 : Split the training dataset into train and test\n","03585c44":"#### Lets compare the accuracy of each model","a164d23a":"### Step 10 : model tuning using XGBoost\n","ab0fab7d":"### 9.5 RandomTreeClassifier hyperparameter tuning by RandomizedSearchCV\n","1dc8cd2c":"### Step 13 : Creating submission file","0c8e97e3":"## Step 2 : Data quality and missing value assessment","045d76a1":"Note : Please review, suggested and vote.","02106877":"### FIlling null values for Embarked column","893a591a":"### Oversampling the minority class using SMOTE\n","d52f217f":"### Step 7.1 : Lets use K-fold cross-validation using cross_val_score() function for model evaluation","e3bd3064":"**The sinking of the Titanic is one of the most infamous shipwrecks in history.\nIn this assignment, we build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).**","1c0c6d07":"## Step 7 : Model evaluation using logistic regression","cc4819df":"**Observation: It looks like the more survived passenger belongs to younger population**","804eac17":"### Step 3.3 Indenpendent variable visualization \n","a6d6aa6c":"## Step 3 : Exploratory data analysis","0a6d2cd8":"#### RMSE for random forest model ","196f62d6":"### Filling null values for Cabin columns","81d369a3":"## Step 4: Data imputation","7ebec083":"**Observation: There is a strong correlation between Fare and Survival, Parch and Survival, SibSp and Parch, Fare abd Parch**","1714fa63":"### Step 3.1 Dependent Variable","6bfe4d3a":"**Observation : looks like female passesnger have more chances of survival than male passenger**","0cf7280f":"**Observation : It looks like the target(lable) varible is class imbalance, so it might perform well for majority class and bad for minority class. to resolve this we need to resampling for minority class using SMOTE**","a3df5d9f":"### Step 9.4 : Model evaluation using RandomForstClassifier"}}