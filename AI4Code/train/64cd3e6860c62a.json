{"cell_type":{"daa94524":"code","ad5bf737":"code","8c47d0a3":"code","9050e4b4":"code","2d077fbf":"code","22668328":"code","77df3bf9":"code","dd58666f":"code","7e6a17a9":"code","bbd1a60e":"code","1e28f815":"code","6eb9aeea":"code","3cd50ddb":"code","4f73e929":"code","2e050c92":"code","703915f7":"code","846e65de":"code","3abd4475":"code","75511281":"code","b4a8e18e":"code","7a26afbf":"code","0d0b5691":"code","9d9bb0b8":"code","621a19b0":"code","513f3265":"code","86c20dbd":"code","2b3f5d6b":"code","14395f14":"code","b3d08817":"code","053962cc":"code","2d8fab25":"markdown","9fd1758d":"markdown","af909904":"markdown","7b7c9350":"markdown","f37fd639":"markdown","f9f132c5":"markdown","1a907ac9":"markdown","6cbb9427":"markdown"},"source":{"daa94524":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad5bf737":"# Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\npd.set_option('max_colwidth', 400)","8c47d0a3":"# Read CSVs into pandas\ntrain_data = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nitems_info = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n\n# Name info uneeded in this model\n#category_info = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n#shops_info = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")","9050e4b4":"# Uncomment to see length, dtypes, nulls, memory\nif False:\n    train_data.info()\n    items_info.info()\n    category_info.info()\n    shops_info.info()","2d077fbf":"# View data sample\ntrain_data.head()","22668328":"# Format date\ntrain_data[\"date\"] = pd.to_datetime(train_data[\"date\"], dayfirst=True)","77df3bf9":"# Due to 28 duplucates groupby on \"date\", \"shop_id\", \"item_id\"\nitems_info = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\ngrouped_data = train_data.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_price\": [\"mean\", \"max\", \"min\"], \"item_cnt_day\": [\"sum\", \"max\", \"min\", \"median\"]})\ngrouped_data = grouped_data.reset_index()\ngrouped_data = grouped_data.merge(items_info[[\"item_id\", \"item_category_id\"]].drop_duplicates(), left_on = \"item_id\", right_on = \"item_id\", how = \"left\")\ngrouped_data.drop(\"item_id\", axis = 1, inplace = True)","dd58666f":"grouped_data.columns = ['date_block_num', 'shop_id', 'item_id', 'item_price_mean', 'item_price_max', \n                        'item_price_min', 'item_cnt_day_sum', 'item_cnt_day_max', 'item_cnt_day_min',\n                        'item_cnt_day_median', 'item_category_id']","7e6a17a9":"# Merge with prev month data for prediction\nt_minus1_data = grouped_data.loc[grouped_data.date_block_num != 33, [\"date_block_num\", \"shop_id\", \"item_id\", \"item_price_mean\", \"item_cnt_day_sum\", \"item_cnt_day_median\"]]\nt_minus1_data[\"date_block_num\"] += 1\ndriver_data = grouped_data[grouped_data.date_block_num != 0]\ndriver_data = driver_data.merge(t_minus1_data, left_on = [\"date_block_num\", \"shop_id\", \"item_id\"], right_on = [\"date_block_num\", \"shop_id\", \"item_id\"], suffixes = (\"\", \"_tminus1\"))","bbd1a60e":"# Create deltas\nfor col in [\"item_price_mean\", \"item_cnt_day_sum\", \"item_cnt_day_median\"]:\n    driver_data[col + \"_tminus1\"] = np.log(driver_data[col + \"_tminus1\"]) - np.log(driver_data[col])","1e28f815":"# Copy data with all features for later\nsubmission_data = driver_data.copy()","6eb9aeea":"# Join with T+1 response data\nresponse_data = grouped_data.loc[grouped_data.date_block_num != 0, [\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_day_sum\"]]\ndriver_data = driver_data[driver_data.date_block_num != 33]\nresponse_data[\"date_block_num\"] -= 1\ndriver_data = driver_data.merge(response_data, left_on = [\"date_block_num\", \"shop_id\", \"item_id\"], right_on = [\"date_block_num\", \"shop_id\", \"item_id\"], suffixes = (\"\", \"_realised\"))","3cd50ddb":"xgb_model = XGBRegressor(colsample_bylevel=0.8, colsample_bynode=0.8, colsample_bytree=0.8, subsample=0.8, \n                         gamma=10, learning_rate=0.1, max_depth=12, min_child_weight=1, \n                         n_estimators=500, random_state=27, reg_alpha=50, reg_lambda=200)\ntraining_cols = driver_data.drop([\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_day_sum_realised\"], axis = 1).columns\nX, y = driver_data.drop([\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_day_sum_realised\"], axis = 1).values, driver_data[\"item_cnt_day_sum_realised\"].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=27, shuffle=True)","4f73e929":"eval_set = [(X_test, y_test)]\nxgb_model.fit(X_train, y_train, early_stopping_rounds=20, eval_set=eval_set, verbose=True)","2e050c92":"# make predictions for test data\ny_pred = xgb_model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\nrmse = mean_squared_error(y_test, predictions)\n# Default: 103.00\n# Manual tuned 1: 89.51","703915f7":"# Install Gpyopt\n!pip install GPyOpt\nimport GPyOpt","846e65de":"# https:\/\/machinelearningapplied.com\/hyperparameter-search-with-gpyopt-part-2-xgboost-classification-and-ensembling\/\n\n# hyper param tuning bayes\n# cross validation hyper param tunin","3abd4475":"search_space = [\n\n       {'name':'colsample_bylevel', 'type':'continuous', 'domain':(0.5, 0.99)},\n       {'name':'colsample_bynode', 'type':'continuous', 'domain':(0.5, 0.99)},\n       {'name':'colsample_bytree', 'type':'continuous', 'domain':(0.5, 0.99)},\n       {'name':'subsample', 'type':'continuous', 'domain':(0.6, 0.99)},\n\n       {'name':'min_child_weight', 'type':'discrete', 'domain':(1, 10)},\n       {'name':'max_depth', 'type':'discrete', 'domain':(8, 9, 10, 11, 12, 13, 14, 15)},\n\n       {'name':'gamma', 'type':'continuous', 'domain':(0.1, 10)},\n       {'name':'reg_alpha', 'type':'discrete', 'domain':(0,1,10,50)},\n       {'name':'reg_lambda', 'type':'discrete', 'domain':(1,10,50,100,150,200,500,1000)}\n               ]","75511281":"def xgb_function(params):\n    \"\"\"\n    \n    Training data defined outside function\n    \n    Inputs\n    ------\n    \n    Outputs\n    -------\n    \n    \"\"\"\n    \n     \n    dict_parameters = {'colsample_bylevel':params[0][0], \n                         'colsample_bynode':params[0][1],\n                         'colsample_bytree':params[0][2],\n                         'subsample':params[0][3],\n                         'min_child_weight':int(params[0][4]),\n                         'max_depth':int(params[0][5]),\n                         'gamma':params[0][6],\n                         'reg_alpha':int(params[0][7]),\n                         'reg_lambda':int(params[0][8]),\n                         \"random_state\": 27,\n                         \"n_estimators\": 500}\n          \n    opt_model = XGBRegressor(**dict_parameters)\n    opt_model.fit(X_train, y_train, early_stopping_rounds=20, eval_set=eval_set, verbose=False)\n\n    y_pred = opt_model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    rmse = mean_squared_error(y_test, predictions)\n     \n    print('\\ndict_parameters:',dict_parameters)\n    print('best_iteration =',opt_model.best_iteration)\n    print('rmse =',rmse)\n     \n    return rmse","b4a8e18e":"gpyopt_bo = GPyOpt.methods.BayesianOptimization(f=xgb_function, domain=search_space, \n                model_type='GP', initial_design_numdata=5, \n                initial_design_type='random', acquisition_type='EI', \n                normalize_Y=True, exact_feval=False, \n                acquisition_optimizer_type='lbfgs', \n                model_update_interval=1, evaluator_type='sequential', \n                batch_size=1, num_cores=os.cpu_count(), verbosity=True, \n                verbosity_model=False, maximize=False, de_duplication=True)","7a26afbf":"gpyopt_bo.run_optimization(max_iter=10)","0d0b5691":"header_params = []\nfor param in search_space:\n    header_params.append(param['name'])\n     \ndf_results = pd.DataFrame(data=gpyopt_bo.X, columns=header_params)\ndf_results[\"rmse\"] = gpyopt_bo.Y","9d9bb0b8":"df_results","621a19b0":"xgb_opt_model = XGBRegressor(**{'colsample_bylevel': 0.5095174790289587, 'colsample_bynode': 0.6375048884962791, 'colsample_bytree': 0.6727943521088571, 'subsample': 0.6096441166795321, \n                                'min_child_weight': 10, 'max_depth': 13, 'gamma': 2.797555559145436, 'reg_alpha': 1, 'reg_lambda': 100, 'random_state': 27, 'n_estimators': 500,\n                                'learning_rate': 0.1})\nxgb_opt_model.fit(X_train, y_train, early_stopping_rounds=20, eval_set=eval_set, verbose=False)","513f3265":"# make predictions for test data\ny_pred = xgb_opt_model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\nmean_squared_error(y_test, predictions)","86c20dbd":"# Read test CSVs and ID labels\nsample_submission = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\ntest_labels = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")","2b3f5d6b":"# Make predictions and keep last ones\nsubmissions = submission_data.sort_values([\"date_block_num\", \"shop_id\", \"item_id\"], ascending = True).drop_duplicates([\"shop_id\", \"item_id\"], keep = \"last\")\nsubmissions[\"y_predict\"] = xgb_opt_model.predict(submissions[training_cols].values)","14395f14":"test_labels = test_labels.merge(submissions[[\"shop_id\", \"item_id\", \"y_predict\"]], left_on = [\"shop_id\", \"item_id\"], right_on = [\"shop_id\", \"item_id\"], how = \"right\")\nsample_submission = sample_submission.merge(test_labels[[\"ID\", \"y_predict\"]], left_on = \"ID\", right_on = \"ID\", how = \"left\")\nsample_submission[\"y_predict\"].fillna(1, inplace = True)\nsample_submission.drop(\"item_cnt_month\", axis = 1, inplace = True)\nsample_submission.rename(columns = {\"y_predict\": \"item_cnt_month\"}, inplace = True)","b3d08817":"sample_submission.head()","053962cc":"sample_submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","2d8fab25":"# Bayesian Optimisation","9fd1758d":"#### Problem Statement\n\nPredict total sales for every product and store in the next month.","af909904":"# Data Preperation & Feature Creation","7b7c9350":"#### Data files\n\n|File|descriptions|\n|---|---|\n|sales_train.csv | the training set. Daily historical data from January 2013 to October 2015.|\n|test.csv | the test set. You need to forecast the sales for these shops and products for November 2015.|\n|sample_submission.csv | a sample submission file in the correct format.|\n|items.csv | supplemental information about the items\/products.|\n|item_categories.csv  | supplemental information about the items categories.|\n|shops.csv | supplemental information about the shops.|","f37fd639":"# Submission","f9f132c5":"# Download and View Data","1a907ac9":"# Fit Xgboost Model","6cbb9427":"#### Data fields\n\n|Field| Meaning|\n| --- | --- |\n|ID | an Id that represents a (Shop, Item) tuple within the test set|\n|shop_id | unique identifier of a shop|\n|item_id | unique identifier of a product|\n|item_category_id | unique identifier of item category|\n|item_cnt_day | number of products sold. You are predicting a monthly amount of this measure|\n|item_price | current price of an item|\n|date | date in format dd\/mm\/yyyy|\n|date_block_num | a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33|\n|item_name | name of item|\n|shop_name | name of shop|\n|item_category_name | name of item category|"}}