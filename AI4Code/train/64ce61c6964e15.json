{"cell_type":{"d7fc42d5":"code","d630edfe":"code","eef0a169":"code","76260dae":"code","29ffe52f":"code","dca22284":"code","7ec0477f":"code","81885f75":"code","631f254a":"code","8bcae15e":"code","85349dea":"code","0a5b8ea7":"code","24cf9218":"code","dae8ed24":"code","a42c6887":"code","d3965012":"code","84137b4a":"code","90b62d67":"code","46608981":"code","4369f702":"code","ee48a0ed":"code","81666116":"code","99768381":"code","89018243":"code","098a0908":"code","4bbf794d":"markdown","51f1cdf2":"markdown","5176365d":"markdown","b481ac2d":"markdown","c74a23dd":"markdown","c07d589c":"markdown"},"source":{"d7fc42d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d630edfe":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns","eef0a169":"data = pd.read_csv(\"\/kaggle\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\")\ndata.head()","76260dae":"data.columns = [\"mean_intrgt\", \"std_intrgt\", \"kurtosis_intrgt\", \"skew_intrgt\", \"mean_dmsnr\", \"std_dmsnr\", \"kurtosis_dmsnr\", \"skew_dmsnr\", \"class\"]","29ffe52f":"sns.kdeplot(data[data[\"class\"] == 0][\"mean_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"mean_intrgt\"], label = \"Class 1\")\nplt.title(\"Mean of the integrated profile\")\nplt.show()","dca22284":"# DM-SNR curve\nsns.kdeplot(data[data[\"class\"] == 0][\"mean_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"mean_dmsnr\"], label = \"Class 1\")\nplt.title(\"Mean of the DM-SNR curve\")\nplt.show()","7ec0477f":"sns.kdeplot(data[data[\"class\"] == 0][\"std_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"std_intrgt\"], label = \"Class 1\")\nplt.title(\"Std. Dev. of the integrated profile\")\nplt.show()","81885f75":"# DM-SNR curve\nsns.kdeplot(data[data[\"class\"] == 0][\"std_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"std_dmsnr\"], label = \"Class 1\")\nplt.title(\"Std. Dev. of the DM-SNR curve\")\nplt.show()","631f254a":"sns.kdeplot(data[data[\"class\"] == 0][\"kurtosis_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"kurtosis_intrgt\"], label = \"Class 1\")\nplt.title(\"Excess Kurtosis of Integrated Profile\")\nplt.show()","8bcae15e":"sns.kdeplot(data[data[\"class\"] == 0][\"kurtosis_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"kurtosis_dmsnr\"], label = \"Class 1\")\nplt.title(\"Excess Kurtosis of DM-SNR Curve\")\nplt.show()","85349dea":"sns.kdeplot(data[data[\"class\"] == 0][\"skew_intrgt\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"skew_intrgt\"], label = \"Class 1\")\nplt.title(\"Skewness of Integrated Profile\")\nplt.show()","0a5b8ea7":"sns.kdeplot(data[data[\"class\"] == 0][\"skew_dmsnr\"], label = \"Class 0\")\nsns.kdeplot(data[data[\"class\"] == 1][\"skew_dmsnr\"], label = \"Class 1\")\nplt.title(\"Skewness of the DM-SNR Curve\")\nplt.show()","24cf9218":"sns.countplot(data[\"class\"])","dae8ed24":"from sklearn.decomposition import PCA \npca = PCA(n_components=2)\npca_data = pca.fit_transform(data.iloc[:, :-1])\n\npca_data = pd.DataFrame(pca_data)\npca_data[\"class\"] = data[\"class\"]","a42c6887":"plt.scatter(pca_data[pca_data[\"class\"] == 1].iloc[:, 0], pca_data[pca_data[\"class\"] == 1].iloc[:, 1], color = \"green\",label = \"Class 1\" )\nplt.scatter(pca_data[pca_data[\"class\"] == 0].iloc[:, 0], pca_data[pca_data[\"class\"] == 0].iloc[:, 1], color = \"red\", label = \"Class 0\" )\nplt.legend()\nplt.show()","d3965012":"plt.scatter(pca_data[pca_data[\"class\"] == 0].iloc[:, 0], pca_data[pca_data[\"class\"] == 0].iloc[:, 1], color = \"red\", label = \"Class 0\" )\nplt.scatter(pca_data[pca_data[\"class\"] == 1].iloc[:, 0], pca_data[pca_data[\"class\"] == 1].iloc[:, 1], color = \"blue\", alpha= .2,label = \"Class 1\" )\nplt.legend()\nplt.show()","84137b4a":"sns.pairplot(data)","90b62d67":"from scipy.stats import pearsonr\nfor i in data.columns[:-1]:\n    for j in data.columns[:-1]:\n        corr = pearsonr(data[i], data[j])[0]\n        if corr > .7 or corr < -.7:\n            print(i, \" \", j, pearsonr(data[i], data[j])[0])","46608981":"X = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, shuffle = True, random_state  = 8)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .1, shuffle = True, random_state  = 8)","4369f702":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","ee48a0ed":"preds = model.predict(X_val)","81666116":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_val, preds)\nsns.heatmap(confusion_matrix(y_val, preds), annot = True, cmap=\"YlGnBu\")","99768381":"test_preds = model.predict(X_test)\nconfusion_matrix(y_test, test_preds)","89018243":"print(\"Accuracy: \", (4846+ 404)\/(4846+ 404 + 95 + 25))","098a0908":"precision = (4846)\/(4846+ 25)\nrecall = (4846)\/(4846+ 95)\nprint(\"Precision: \",precision)\nprint(\"Recall: \", recall)\nprint(\"f1 score: \", (2*precision*recall)\/(precision + recall))","4bbf794d":"The data is highly unbalanced and in unbalanced accuracy is not a good metric. We will be using precision, recall & F1 Score as our metric.","51f1cdf2":"## EDA","5176365d":"Feedbacks are highly appreciated. <br>\n\nUpvote if you like this kernel.","b481ac2d":"## Model ","c74a23dd":"## What we will be doing? \n1. Doing some basic EDA over the data. \n3. Fitting a Logistic regression model over the model.","c07d589c":"## What you can try? \n* Feature Selection -> Using one of the features from the features having high correlation \n* Using PCA with different number of components and training on that data. \n* Trying other classifiers"}}