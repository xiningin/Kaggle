{"cell_type":{"33198669":"code","fa6d6c4f":"code","c01da346":"code","63d85914":"code","79c32e40":"code","5cf5ef3f":"code","891a22be":"code","e4453d9f":"code","565f5bfc":"code","31394e06":"code","4148312a":"code","90b50d18":"code","fb9bf0fe":"code","6ddaa2df":"markdown","4e75053f":"markdown","b6bec877":"markdown","1c9e3be0":"markdown","9a009893":"markdown","1bdddcf8":"markdown","e0f6c898":"markdown","4aefc6d5":"markdown","09ce84b5":"markdown","ae62e895":"markdown","d50d379c":"markdown","b1347acd":"markdown","d6b6ed7b":"markdown"},"source":{"33198669":"!nvidia-smi","fa6d6c4f":"!cat \/usr\/local\/cuda\/version.txt","c01da346":"## Passing Y as input while conda asks for confirmation, we use yes command\n!yes Y | conda install faiss-gpu cudatoolkit=10.0 -c pytorch","63d85914":"# !wget https:\/\/anaconda.org\/CannyLab\/tsnecuda\/2.1.0\/download\/linux-64\/tsnecuda-2.1.0-cuda100.tar.bz2\n# !tar xvjf tsnecuda-2.1.0-cuda100.tar.bz2\n# !cp -r site-packages\/* \/opt\/conda\/lib\/python3.6\/site-packages\/\n# # !export LD_LIBRARY_PATH=\"\/kaggle\/working\/lib\/\" \n# !cp \/kaggle\/working\/lib\/libfaiss.so \/usr\/local\/cuda\/lib64\/","79c32e40":"!wget https:\/\/anaconda.org\/CannyLab\/tsnecuda\/2.1.0\/download\/linux-64\/tsnecuda-2.1.0-cuda100.tar.bz2\n!tar xvjf tsnecuda-2.1.0-cuda100.tar.bz2 --wildcards 'lib\/*'\n!tar xvjf tsnecuda-2.1.0-cuda100.tar.bz2 --wildcards 'site-packages\/*'\n!cp -r site-packages\/* \/opt\/conda\/lib\/python3.6\/site-packages\/\n# !export LD_LIBRARY_PATH=\"\/kaggle\/working\/lib\/\" \n!cp \/kaggle\/working\/lib\/libfaiss.so \/usr\/local\/cuda\/lib64\/","5cf5ef3f":"!apt search openblas\n!yes Y | apt install libopenblas-dev \n# !printf '%s\\n' 0 | update-alternatives --config libblas.so.3 << 0\n# !apt-get install libopenblas-dev \n!","891a22be":"import faiss\nfrom tsnecuda import TSNE\nimport pandas as pd\nimport numpy as np\nfrom  sklearn.manifold import TSNE as sktsne\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e4453d9f":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_train = df_train.head(10000)\nY = df_train[['label']]\nX = df_train.drop('label', axis=1)\n","565f5bfc":"def plot_digit(digits):\n    fig, axs = plt.subplots(1,len(digits),figsize=(2,2))\n    for i, pixels in enumerate(digits):\n        ax = axs[i]\n        digit_data = pixels.values.reshape(28,28)\n        ax.imshow(digit_data,interpolation=None, cmap='gray')\n    plt.show()","31394e06":"plot_digit([X.iloc[0], X.iloc[20], X.iloc[201]])","4148312a":"tsne_model = TSNE(n_components=2, perplexity=40.0, n_iter=2000).fit_transform(X)","90b50d18":"tsne_df = pd.DataFrame(tsne_model)\ntsne_df = pd.concat([tsne_df,Y], axis=1)","fb9bf0fe":"sns.FacetGrid(tsne_df, hue=\"label\" , size=6).map(plt.scatter, 0, 1).add_legend()\nplt.show()","6ddaa2df":"3. We found another library missing, *openblas*, we install that now.","4e75053f":"Plotting 3 digits for example in training set","b6bec877":"First of all, let us see if GPU is present","1c9e3be0":"## Data Load and Acquaintance\nFirst of all, let us load the train and test data from Kaggle\n\nWe can observe that Training Dataset has 42k records and 785 features, Testing Dataset has 28k data records with 784 fetures. We can intuite the additional feature in training dataset is the laballed class. \n\nLet us try to understand the data by displaying top few rows. We can see that, first Column, named **label** is the Y variable and rest 784 columns are independent variables, X. That means the first row corresponds to the digit 1 and pixel data is given by *pixel0* to *pixel783*","9a009893":"2. Now let us install *tsnecuda* from the sources\nAlso we found *libfaiss.so* was not found while running, but this file comes as part of sources. So, we move that to ```\/usr\/local\/cuda\/lib64```","1bdddcf8":"Next step is to build the t-SNE model with following hyperparameter\n\nPerplexity = 50 <br>\nNo. of Iterations = 1000","e0f6c898":"# Working with MNIST data and Visualize using t-SNE\n\n## Background\nMNIST data is most famous dataset available for us which has 60,000 samples of hand-written digits (0 to 9). Out of which, 42,000 in the training set and 28,000 is testing test. \nThe digits are already converted to vectors of 784 data points. Each data point will  be considered as feature in this dataset\n\n\n## Problem statement\n\n### Higher Dimension\nIn this kernel I would like to focus on visualization of data points of these samples more than actual classifications. As we all know the task of representation of datapoints with features more than 2 or 3 is not new. It is commom problem to plot the graphs with higher dimentional features. Here we have 784 features. \n\n### Expensive TSNE Computation\nWhoever working with SKLearn might have already figured out, TSNE computation is very expensive. For example, fitting 10,000 MNIST data with perplexity=30 and 1000 iteration would take around 3\/4 min to complete. \n\n## Available solutions \nBest solution to represent the higher dimensions data is to perform Dimensionality reduction exercise. Following are solutions available\n\n1. **Feature Eliminations**: Each and every feature is removed and\/or added to the solution and check the error results. Which ever feature had least positive impact on results or most negative impact, are eliminated from the set of features. \n\n2. **Factor Analysis**: Among the features, check the correlations between the features themselves. If we find correlated features, we can keep one among them, rest can be removed \n\n3. **Principal Component Analysis PCA**:  Most of common approach is PCA, where, we can project the data points orthogonally on principal component and reduce dimensions\n\n4. **T Distributed Stochastic Neighbourhood Embedding (T-SNE)**: This is most advanced and latest technology where data points are embedded on lower dimesion by keeping local topology and not concerning with gloabl configuration. Also this is similar to graph based technic. \n\n### GPU Solution\nTo tackle the problem of slow TSNE computation, we need to find a library which uses GPU effectively. One of the best available library which uses GPU for tsne computation is  ```tsnecuda``` https:\/\/github.com\/CannyLab\/tsne-cuda. However, Kaggle does not comes with tsnecuda and other pre-requisites installed in GPU enabled kernels. This notebook helps to install required libraries and pre-prequisites for running ```tsnecuda```\n\n\n## Basic checks\nLet us practice on TSNE techniche in this notebook and try to reduce 784 dimensions of MNIST to 2 dimensions and plot it on 2D graphics. Also let us perform those tsne computations using GPU using ```tsnecuda``` library","4aefc6d5":"## Start the main objective of notebook","09ce84b5":"Once the model is ready and fit the data, it has produced the new dataset of only 2 dimesions.\nLet us Label column for graphical representation in the next step","ae62e895":"Plotting the result of TSNE ","d50d379c":"Now let us try to plot the character from the pixel data provided. For reusability purpose, let us define a method","b1347acd":"Next check what is the version of cuda in this GPU ","d6b6ed7b":"## Installation of Pre-requisites\n\n1. One of the pre-requisite library for tsnecuda to run is ```faiss```. Let us isntall the same for the version of *cuda*"}}