{"cell_type":{"d4e6fc30":"code","6cc6d5bc":"code","0c3ca4b5":"code","e0222591":"code","ec894859":"code","95b4622e":"code","b2721ab5":"code","d6f9e51c":"code","ae486f23":"code","3f4b3567":"code","d0e4bd9a":"code","a666011e":"code","af1b3704":"code","fc898c9a":"code","f666b387":"code","a070005e":"code","59db08db":"code","a17d3997":"code","9ee76822":"code","b1284951":"code","2c04a606":"code","69dd8534":"code","6ee8c6ae":"code","9a4a788d":"code","3775d5fb":"code","d9836d35":"code","da04bcd2":"code","038504e0":"markdown","6e7fe2e7":"markdown","31cae624":"markdown","b6cf6e53":"markdown","0699844b":"markdown","17463ec8":"markdown","db726fe1":"markdown","6f360d13":"markdown","33dc060f":"markdown","e863dbd7":"markdown","b9b4e100":"markdown","68c76588":"markdown","6762b4f1":"markdown","a9e24635":"markdown","beeb1eca":"markdown","01741d60":"markdown","95c65979":"markdown","dc8b3506":"markdown"},"source":{"d4e6fc30":"# set this to True before commiting\nCOMMIT = True\n\nif COMMIT:\n    upscaling_steps=13\n    opt_steps=20\n    verbose=1\n    TOP = 4\nelse:\n    upscaling_steps=2\n    opt_steps=10\n    verbose=2\n    TOP = 1\n    \nINTERACTIVE = not COMMIT","6cc6d5bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torchvision import models, transforms\nimport cv2\nimport matplotlib.pyplot as plt\nimport zipfile\nimport PIL\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nif INTERACTIVE:\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","0c3ca4b5":"# VGG19 BN\nmodel = models.vgg19_bn()\nmodel.load_state_dict(torch.load('\/kaggle\/input\/pytorch-pretrained-models\/vgg19_bn-c79401a0.pth'))\nmodel.eval()\nmodel = model.double()","e0222591":"# list cnn layers\nlayers = [layer for layer in model.children()]\nprint('Layers: {}'.format(len(layers)))\nprint('Layers[0]: {}'.format(len(layers[0])))\nif INTERACTIVE:\n    for l in layers:\n        print(l)","ec894859":"# global variable to work with GPU if possible\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available(): print('Thanks Kaggle! Guau!!!')\nprint(device)","95b4622e":"class SaveFeatures():\n    def __init__(self, module, device=None):\n        # we are going to hook some model's layer (module here)\n        self.hook = module.register_forward_hook(self.hook_fn)\n        self.device = device\n\n    def hook_fn(self, module, input, output):\n        # when the module.forward() is executed, here we intercept its\n        # input and output. We are interested in the module's output.\n        self.features = output.clone()\n        if self.device is not None:\n            self.features = self.features.to(device)\n        self.features.requires_grad_(True)\n\n    def close(self):\n        # we must call this method to free memory resources\n        self.hook.remove()","b2721ab5":"class FeatureMapVisualizer():\n    def __init__(self, cnn, device, channels=3, layers_base=None, norm=None, denorm=None, save=None):\n        self.model = cnn\n\n        if layers_base is None:\n            self.layers = self.model\n        else:\n            self.layers = layers_base\n        \n        self.channels = channels\n        self.device = device\n\n        mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n        std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n            \n        self.norm = norm\n        self.denorm = denorm\n\n        if norm is None:\n            self.norm = transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n\n        if denorm is None:\n            self.denorm = transforms.Normalize((-mean \/ std).tolist(), (1.0 \/ std).tolist())\n            \n        self.save = save\n\n    def set_layers_base(self, layers):\n        # sometime we want to access to layers in deeper levels\n        # so we could call something like:\n        # featureMap.set_layers_base([module for module in model.children()][5][1])\n        self.layers = layers\n        \n    def optimize_img(self, activations, filter, img, learning_rate, opt_steps, verbose):\n        \n        size = img.shape[1]\n        img = torch.from_numpy(img.astype(np.float32).transpose(2,0,1))\n        \n        img = self.norm(img).double()\n        img_input = img.clone().detach().reshape(1, self.channels, size, size).to(self.device).requires_grad_(True)\n        optimizer = torch.optim.Adam([img_input], lr=learning_rate, weight_decay=1e-6)\n\n        for n in range(opt_steps):\n            optimizer.zero_grad()\n            self.output = self.model(img_input)\n            # TODO: the idea is to find an input image that\n            #       'illuminate' ONLY ONE feature map (filter here)\n            # TODO: 1 test a loss function that punish current\n            #       activation filter with the rest of the\n            #       filters mean values in the layer\n            # TODO: 2 test a loss function that punish current activation\n            #       filter with all the rest of the filters mean value\n            #       of more layers (all?)\n            loss = -1 * activations.features[0, filter].mean()\n            loss.backward()\n            if verbose > 1:\n                print('.', end='')\n            #print(loss.clone().detach().cpu().item())\n            optimizer.step()\n        if verbose > 1:\n            print()\n        img = self.denorm(img_input.clone().detach()[0].type(torch.float32))\n        img = img.cpu().numpy().transpose(1,2,0)\n        return img\n        \n\n    def visualize(self, layer, filter, size=56, upscaling_steps=12, upscaling_factor=1.2, lr=0.1, opt_steps=20, blur=None, verbose=2):\n        training = self.model.training\n        self.model.eval()\n        self.model = self.model.double().to(self.device)\n        # generate random image\n        img = np.uint8(np.random.uniform(100, 160, (size, size, self.channels)))\/255\n        # register hook\n        activations = SaveFeatures(self.layers[layer], self.device)\n        if verbose > 0:\n            print('Processing filter {}...'.format(filter))\n\n        for i in range(upscaling_steps):\n            if verbose > 1:\n                print('{:3d} x {:3d}'.format(size,size), end='')\n\n            img = self.optimize_img(activations, filter, img, learning_rate=lr, opt_steps=opt_steps, verbose=verbose)\n\n            if i < upscaling_steps-1:\n                size = int(size*upscaling_factor)\n                # scale image up\n                img = cv2.resize(img, (size, size), interpolation = cv2.INTER_CUBIC)\n                # blur image to reduce high frequency patterns\n                if blur is not None: img = cv2.blur(img,(blur,blur))\n            img = np.clip(img, 0, 1)\n\n        if verbose > 0:\n            print('preparing image...')\n        activations.close()\n        self.model.train(training)\n        if self.save != None:\n            self.save(\"layer_{:02d}_filter_{:03d}.jpg\".format(layer, filter), img)\n        return img\n    \n    # We return the mean of every activation value, but this could\n    # be other metric based on convolutional output values.\n    def get_activations(self, monitor, input, mean=True):\n\n        training = self.model.training\n        self.model.eval()\n        self.model = self.model.double().to(self.device)\n\n        activations = {}\n        mean_acts = {}\n\n        print('hooking layers {}'.format(monitor))\n        for layer in monitor:\n            activations[layer] = SaveFeatures(self.layers[layer], device=self.device)\n\n        self.output = self.model(input.to(self.device))\n\n        for layer in activations.keys():\n            filters = activations[layer].features.size()[1]\n            mean_acts[layer] = [activations[layer].features[0,i].mean().item() for i in range(filters)]\n\n        print('unhooking layers.')        \n        for layer in activations.keys():\n            activations[layer].close()\n            \n        self.model.train(training)\n        \n        if mean:\n            return mean_acts\n        \n        return activations","d6f9e51c":"# We will save all generated images in this directory\nimages_dir = '.\/images\/'\n# create images directory\nif not os.path.exists(images_dir):\n    os.makedirs(images_dir)\n!ls","ae486f23":"# We save images only when commiting\nif COMMIT:\n    def save(name, img):\n        global image_dir\n        plt.imsave(images_dir + name, img)\nelse:\n    # do not waste time saving images\n    save = None","3f4b3567":"with open('\/kaggle\/input\/imagenet-1000-labels\/imagenet1000_clsidx_to_labels.txt','r') as f:\n    class_labels = eval(f.read())","d0e4bd9a":"# we save in the variable 'monitor' every ReLU layers that appears\n# after every convolutional layer (they present non negative data)\n#monitor = [2,5,9,12,16,19,22,25,29,32,35,38,42,45,48,51]\n#monitor = [i for i, layer in enumerate(layers[0]) if isinstance(layer, torch.nn.Conv2d)]\nmonitor = [i for i, layer in enumerate(layers[0]) if isinstance(layer, torch.nn.ReLU)]\n\n# define mean and std used for most famous images datasets\nmean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\nstd = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n\n# define global transformations based on previous mean and std\nnormalize = transforms.Normalize(mean=mean.tolist(), std=std.tolist())\ndenormalize = transforms.Normalize((-mean \/ std).tolist(), (1.0 \/ std).tolist())\n\n# The input images will be prepared with this transformation\n# Minimum image size recommended for input is 224\nimg2tensor = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize])","a666011e":"def top(k, output, labels):\n    values, indices = torch.topk(output, k, 1)\n    values, indices = values[0].tolist(), indices[0].tolist()\n    print('Top {} predicted classes:'.format(k))\n    for i, idx in enumerate(indices):\n        print('- {}: {}'.format(labels[idx], values[i]))","af1b3704":"def show_filters(mean_act, filter, layer=None):\n    filters = len(mean_act)\n    plt.figure(figsize=(16,6))\n    extraticks=[filter]\n    act = plt.bar(list(range(filters)), mean_act)\n    ax = act[0].axes\n    ax.set_xticks([0,int(filters\/2),filters] + extraticks)\n    ax.set_xlim(0,filters)\n    ax.plot(filter, mean_act[filter], 'ro')\n    plt.axvline(x=filter, color='grey', linestyle='--')\n    ax.set_xlabel(\"feature map\")\n    ax.set_ylabel(\"mean activation\")\n    if layer is not None:\n        ax.set_title('Features maps of layer {}'.format(layer))\n    plt.show()","fc898c9a":"# Lets generate feature map visualization for filter 91 at layer 48\n# to discover the best (or a good one) 'texture image' that activate this filter\nfmv = FeatureMapVisualizer(model, device, layers_base=layers[0], save=None)\n\nsteps = 15\nif str(device) == 'cpu': steps = 5\n    \nfeature_map_48_91_img = fmv.visualize(\n                            layer=48, filter=91, size=56,\n                            upscaling_steps=steps, opt_steps=20, blur=3, verbose=2\n                        )\nplt.imsave(images_dir + 'input_for_L48F091.jpg', feature_map_48_91_img)","f666b387":"# And now we plot it\nfig = plt.figure(figsize=(10,10))\nplt.imshow(feature_map_48_91_img);","a070005e":"activation = SaveFeatures(layers[0][48], device=device)\n# transform image to tensor and generate input for the model\ninput = img2tensor(PIL.Image.fromarray(np.uint8(feature_map_48_91_img*255))).unsqueeze(0).double()\noutput = model(input.to(device))\ntop(10, output, class_labels)\nprint(activation.features[0,91].shape)\nplt.imshow(activation.features[0,91].clone().detach().cpu().numpy())\nactivation.close()\ndel activation","59db08db":"# load snake image\nsnake = PIL.Image.open('\/kaggle\/input\/tini-image-dataset\/snake.jpg')\nfig = plt.figure(figsize=(6,10))\nplt.imshow(snake)\nplt.show()","a17d3997":"activation = SaveFeatures(layers[0][48], device=device)\n# transform image to tensor and generate input for the model\ninput = img2tensor(snake).unsqueeze(0).double()\noutput = model(input.to(device))\ntop(10, output, class_labels)\nprint(activation.features[0,91].shape)\nplt.imshow(activation.features[0,91].clone().detach().cpu().numpy())\nactivation.close()\ndel activation","9ee76822":"# transform image to tensor and generate input for the model\ninput = img2tensor(snake).unsqueeze(0).double()","b1284951":"# get activations \nmean_acts = fmv.get_activations(monitor, input)","2c04a606":"top_filters = {}\nn = len(mean_acts)\n# we are monitoring 16 layers\nrows, cols = 8,2\nfig = plt.figure(figsize=(cols*10, rows*5))\nprint('Generating visualization for filters activations per layer', end='')\nfor i,k in enumerate(mean_acts.keys()):\n    print('.', end='')\n    ax = plt.subplot(rows, cols, i+1)\n    top_filters[k] = sorted(range(len(mean_acts[k])), key=lambda idx: mean_acts[k][idx], reverse=True)[:TOP]\n    for filter in top_filters[k]:\n        plt.axvline(x=filter, color='grey', linestyle='--')\n    act = plt.bar(list(range(len(mean_acts[k]))), mean_acts[k])\n    ax = act[0].axes\n    ax.set_xlim(0,len(mean_acts[k]))\n    ax.plot(top_filters[k], [mean_acts[k][f] for f in top_filters[k]], 'ro')\n    ax.set_xlabel(\"feature maps\")\n    ax.set_ylabel(\"mean activation\")\n    f_desc = ''\n    for f in top_filters[k]:\n        f_desc += ' ' + str(f)\n    ax.set_title('layer {} - filters{}'.format(k, f_desc))\nprint('tight_layout...')\nplt.tight_layout()\nfig.savefig(images_dir + 'top_3_activated_filters_per_layer.jpg')","69dd8534":"# Let plot it with zoom\nshow_filters(mean_acts[48], 91, '48')","6ee8c6ae":"fms = {}\nfmv = FeatureMapVisualizer(model, device, layers_base=layers[0], save=save)\nfor layer in top_filters.keys():\n    filters = top_filters[layer]\n    fms[layer] = []\n    for filter in filters:\n        fms[layer].append(fmv.visualize(\n            layer=layer, filter=filter,\n            size=56, upscaling_steps=upscaling_steps, upscaling_factor=1.2,\n            lr=0.1, opt_steps=opt_steps, blur=3, verbose=verbose))\n        ","9a4a788d":"for layer in fms.keys():\n    filters = top_filters[layer]\n    fig = plt.figure(figsize=(20,10))    \n    for i in range(len(filters)):\n        ax = plt.subplot(1,len(filters), i+1)\n        ax.imshow(fms[layer][i])\n        ax.set_title('L{:02d} F{:03d}'.format(layer, filters[i]))\n    plt.show()\n        ","3775d5fb":"def zip_images(name):\n    ziph = zipfile.ZipFile('{}.zip'.format(name), 'w', zipfile.ZIP_DEFLATED)\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk('.\/images\/'):\n        for file in files:\n            ziph.write(os.path.join(root, file))\n    ziph.close()","d9836d35":"zip_images('snake_feature_maps')","da04bcd2":"# remove all image files\n!rm .\/images\/*.*\n!rmdir .\/images\/ ","038504e0":"# Generate 3 input images for the top 3 activated filters\nThis is done per layer","6e7fe2e7":"# Define some important variables","31cae624":"# This class serves to hook cnn layers\nWe need to \"see\" what happens step per step, layer per layer, filter per filter.**","b6cf6e53":"# Visualize generated input images","0699844b":"# This function show top K predicted classes","17463ec8":"# Save the generated images in one zip file","db726fe1":"# This class let us visualize the feature maps\nIt generates an image (for model's input) that obtain a feature map\nwith a high mean value for a given filter at a given layer.","6f360d13":"# Obtain all filters activations of monitored layers","33dc060f":"# Import libraries","e863dbd7":"# Load ImageNet labels","b9b4e100":"# Load an image","68c76588":"# Define some global variables","6762b4f1":"# Let see the feature map generated by filter 91 at layer 48 using the image found\n\nThe image found was created by the FeatureMapVisualizer.\nAn interesting observation is that if we pay atention to the top predicted classes\nof this image, we will know that this filter is a 'feature' related to other classes.\nThis image could be some kind of a cluster characteristic representation.\nWe could select the top 3 filters per layer (based on its activations) for 100\ndifferents snakes and then check which are the filters that the snakes images have\nin common and which are not. Based on this idea:\n- We could detect common filters per class.\n- We could detect related (different) classes based on common filters presented on them.\n- We could define names for clusters of filters (some kind of language representation?)\n- We could use this information to 'edit' a CNN to do other things.\nIf we want to create a model capable of distinguishing between just two classes.\ni.e. frogs and cars, then we could discover in this networks that there are only 30\nimportant filters that are activated with frogs and another 20 with cars, and then\nuse them to build a new model. Or may be we could discover that is better to use the\ntransfer learning technique to avoid wasting our time :D.","a9e24635":"We can see at layer 48 that the best mean activations is generated by filter 91","beeb1eca":"# Load a trained CNN","01741d60":"# Plot all filters activated values\n1. Plot all feature maps mean values per layer (highlight top 3 activations)","95c65979":"I do not know if you see the same thing that I did... Do you see some kind of snake?\nWe can check if the filter 91 at layer 48 is activated when the model receive a snake image as input.","dc8b3506":"# This function plot the filters mean values of a layer"}}