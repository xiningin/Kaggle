{"cell_type":{"70beb071":"code","e3d8c237":"code","12bf9973":"code","f92ba9bf":"code","4873bb3f":"code","d1c16631":"code","f359a1dc":"code","30a2373f":"code","a9b830e8":"code","3838a1c0":"code","f5000cb7":"code","d9bc2188":"code","ac88ca96":"code","a4cffc9a":"code","184ded43":"code","307d1e98":"code","73e8ed0d":"code","305860cd":"code","bdf3e98e":"code","11b7577b":"code","a2174783":"code","88720d4a":"code","54e4a131":"code","ba05a2ad":"code","ec386878":"code","90fff4fe":"code","415cb7c3":"code","1c867fae":"code","206116a9":"code","21450f4a":"code","48fa3916":"code","3ecd7936":"code","2b7c985d":"code","80a6f486":"code","00d768b7":"markdown","3c070a84":"markdown","207c6471":"markdown","ff2363a4":"markdown","e82c3207":"markdown","343d0b76":"markdown","600b5677":"markdown","f68abc8a":"markdown","17d64311":"markdown","e8759915":"markdown","0ea3a70d":"markdown","55cadac3":"markdown","ed48e3fc":"markdown","7e522d9f":"markdown","48d75da1":"markdown","e515c3a8":"markdown","f7477a2e":"markdown","7e2abac0":"markdown","19f1ac21":"markdown","283a067c":"markdown","1d2027ac":"markdown","7acb94be":"markdown","eea47512":"markdown","4cdea484":"markdown","afea960a":"markdown","c29e9c26":"markdown","72015375":"markdown","d397154e":"markdown","45d3816c":"markdown","cb7a50bc":"markdown","548166bb":"markdown","6b775bf2":"markdown","735d4ce2":"markdown","2513fc3e":"markdown","1a8a1223":"markdown","eace1b91":"markdown","4617009e":"markdown","8bac3e04":"markdown","450b98e9":"markdown","fb884a1f":"markdown","89511a9b":"markdown","5d71a561":"markdown","48556376":"markdown","935e6629":"markdown","3f1bdb7e":"markdown","59052559":"markdown","f69b6f5d":"markdown","725fa3a6":"markdown"},"source":{"70beb071":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport os\nfrom subprocess import check_output\nfrom wordcloud import wordcloud, STOPWORDS\nimport warnings\nfrom collections import Counter\nimport datetime\nimport glob","e3d8c237":"#hiding warnings for cleaner display\nwarnings.filterwarnings('ignore')\n\n#Configuring some options\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n#For interactive plots\n%matplotlib notebook","12bf9973":"df = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\ndf_ca = pd.read_csv(\"..\/input\/youtube-new\/CAvideos.csv\")\ndf_de = pd.read_csv(\"..\/input\/youtube-new\/DEvideos.csv\")\ndf_fr = pd.read_csv(\"..\/input\/youtube-new\/FRvideos.csv\")\ndf_gb = pd.read_csv(\"..\/input\/youtube-new\/GBvideos.csv\")","f92ba9bf":"PLOT_COLORS = [\"#268bd2\", \"#0052CC\", \"#FF5722\", \"#b58900\", \"#003f5c\"]\npd.options.display.float_format = '{:.2f}'.format\nsns.set(style=\"ticks\")\nplt.rc('figure', figsize=(8, 5), dpi=100)\nplt.rc('axes', labelpad=20, facecolor=\"#ffffff\", linewidth=0.4, grid=True, labelsize=14)\nplt.rc('patch', linewidth=0)\nplt.rc('xtick.major', width=0.2)\nplt.rc('ytick.major', width=0.2)\nplt.rc('grid', color='#9E9E9E', linewidth=0.4)\nplt.rc('font', family='Arial', weight='400', size=10)\nplt.rc('text', color='#282828')\nplt.rc('savefig', pad_inches=0.3, dpi=300)\n","4873bb3f":"df.head()","d1c16631":"df.describe()","f359a1dc":"df.shape","30a2373f":"df.info()","a9b830e8":"df[df[\"description\"].apply(lambda x: pd.isna(x))].head(3)","3838a1c0":"cdf = df[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts()\\\n.to_frame().reset_index()\\\n.rename(columns={\"index\": \"year\", \"trending_date\": \"No_of_videos\"})\n\nfig, ax = plt.subplots()\n_ = sns.barplot(x=\"year\", y=\"No_of_videos\", data = cdf, \n                palette = sns.color_palette(['#ff764a','#ffa600'], n_colors=7), ax=ax)\n_ = ax.set(xlabel=\"Year\", ylabel=\"No. of videos\")","f5000cb7":"df[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts(normalize=True)","d9bc2188":"columns_show = ['views', 'likes', 'dislikes', 'comment_count']\nf, ax = plt.subplots(figsize=(8, 8))\ndfe = df[columns_show].corr()\nsns.heatmap(dfe,mask=np.zeros_like(dfe, dtype=np.bool), cmap='RdYlGn',linewidth=0.30,annot=True)","ac88ca96":"fig, ax = plt.subplots()\n_ = plt.scatter(x=df['views'], y=df['likes'], color = PLOT_COLORS[2], edgecolors=\"#000000\",\n               linewidth=0.5)\n_ = ax.set(xlabel=\"views\", ylabel=\"likes\")","a4cffc9a":"fig, ax = plt.subplots()\n_ = sns.distplot(df[\"views\"], kde=False, color=PLOT_COLORS[4],\n                hist_kws={'alpha': 1},bins=np.linspace(0, 2.3e8,47),ax=ax)\n_ = ax.set(xlabel=\"Views\",ylabel=\"No. of videos\", xticks=np.arange(0, 2.4e8, 1e7))\n_ = ax.set_xlim(right=2.5e8)\n_ = plt.xticks(rotation=90)","184ded43":"fig, ax = plt.subplots()\n_ = sns.distplot(df[df[\"views\"]<25e6][\"views\"], kde=False, color=PLOT_COLORS[1], hist_kws={'alpha': 1}, ax=ax)\n_ = ax.set(xlabel=\"Views\", ylabel=\"No. of videos\")","307d1e98":"df[df['views']<1e6]['views'].count()\/df['views'].count() * 100","73e8ed0d":"plt.rc('figure.subplot',wspace=0.9)\nfig, ax = plt.subplots()\n_ = sns.distplot(df[\"likes\"], kde=False, color=PLOT_COLORS[3],\n                hist_kws={'alpha':1}, bins=np.linspace(0, 6e6, 61), ax=ax)\n_ = ax.set(xlabel=\"Likes\", ylabel=\"No. of Videos\")\n_ = plt.xticks(rotation=90)","305860cd":"df['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')\ndf['publish_time'] = pd.to_datetime(df['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\n\n#Separate date and time into two columns from 'publish_time' column\ndf.insert(4, 'pub_date', df['publish_time'].dt.date)\ndf['publish_time'] = df['publish_time'].dt.time\ndf['pub_date'] = pd.to_datetime(df['pub_date'])","bdf3e98e":"us_views = df.groupby(['video_id'])['views'].agg('sum')\nus_likes = df.groupby(['video_id'])['likes'].agg('sum')\nus_dislikes = df.groupby(['video_id'])['dislikes'].agg('sum')\nus_comment_count = df.groupby(['video_id'])['comment_count'].agg('sum')","11b7577b":"df_usa_sdtr = df.drop_duplicates(subset='video_id', keep=False, inplace=False)\ndf_usa_mdtr = df.drop_duplicates(subset='video_id', keep='first', inplace=False)\n\nframes = [df_usa_sdtr, df_usa_mdtr]\ndf_usa_without_duplicates = pd.concat(frames)\n\ndf_usa_comment_disabled = df_usa_without_duplicates[df_usa_without_duplicates['comments_disabled']==True].describe()\ndf_usa_rating_disabled = df_usa_without_duplicates[df_usa_without_duplicates['ratings_disabled']==True].describe()\ndf_usa_video_error = df_usa_without_duplicates[df_usa_without_duplicates['video_error_or_removed']==True].describe()","a2174783":"df_usa_sdtr.head()","88720d4a":"df_usa_mdtr.head()","54e4a131":"df_usa_mdtr = df.groupby(by=['video_id'],as_index=False).count().sort_values(by='title',ascending=False).head()\n\nplt.figure(figsize=(8,8))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=df_usa_mdtr['video_id'],y=df_usa_mdtr['trending_date'], data=df_usa_mdtr)\nplt.xlabel(\"Video_Id\")\nplt.ylabel(\"Count\")\nplt.title(\"Top 5 videos that trended maximum days in USA\")","ba05a2ad":"df_us_max_views = us_views['j4KvrAUjn6c']\ndf_us_max_likes = us_likes['j4KvrAUjn6c']\ndf_us_max_dislikes = us_dislikes['j4KvrAUjn6c']\ndf_us_max_comment = us_comment_count['j4KvrAUjn6c']","ec386878":"cdf = df.groupby(\"channel_title\").size().reset_index(name=\"video_count\") \\\n.sort_values(\"video_count\", ascending=False).head(20)\n\nfig, ax = plt.subplots(figsize=(8,8))\n_ = sns.barplot(x=\"video_count\", y=\"channel_title\", data=cdf, \n                palette=sns.cubehelix_palette(n_colors=20, reverse=True),ax=ax)\n_ = ax.set(xlabel=\"No. of videos\", ylabel=\"Channel\")","90fff4fe":"usa_category_id = df_usa_without_duplicates.groupby(by=['category_id'],as_index=False).count().sort_values(by='title',ascending=False).head(5)\n\nplt.figure(figsize=(7,7))\nsns.kdeplot(usa_category_id['category_id']);\nplt.xlabel(\"category IDs\")\nplt.ylabel(\"Count\")\nplt.title(\"Top 5 categories IDs for USA\")","415cb7c3":"import simplejson as json\nwith open(\"..\/input\/youtube-new\/US_category_id.json\") as f:\n    categories = json.load(f)[\"items\"]\ncat_dict = {}\nfor cat in categories:\n    cat_dict[int(cat[\"id\"])] = cat[\"snippet\"][\"title\"]\ndf['category_name'] = df['category_id'].map(cat_dict)","1c867fae":"cdf = df[\"category_name\"].value_counts().to_frame().reset_index()\ncdf.rename(columns={\"index\": \"category_name\", \"category_name\": \"No_of_videos\"}, inplace=True)\nfig, ax = plt.subplots()\n_ = sns.barplot(x=\"category_name\", y=\"No_of_videos\", data=cdf, \n                palette=sns.cubehelix_palette(n_colors=16, reverse=True), ax=ax)\n_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n_ = ax.set(xlabel=\"Category\", ylabel=\"No. of videos\")","206116a9":"def Capitalized_word(s):\n    for m in s.split():\n        if m.isupper():\n            return True\n    return False\n\ndf[\"contains_capital_words\"] = df[\"title\"].apply(Capitalized_word)\n\nvalue_counts = df[\"contains_capital_words\"].value_counts().to_dict()\nfig, ax = plt.subplots()\n_ = ax.pie([value_counts[False], value_counts[True]], labels=['No','Yes'],\n          colors=['#003f5c', '#ffa600'], textprops={'color':'#040204'}, startangle=45)\n_ = ax.axis('equal')\n_ = ax.set_title('Title Contains Capitalized word?')","21450f4a":"df[\"contains_capital_words\"].value_counts(normalize=True)","48fa3916":"df[\"title_length\"] = df[\"title\"].apply(lambda x: len(x))\n\nfig, ax = plt.subplots()\n_ = sns.distplot(df[\"title_length\"], kde = False, rug = False, color=PLOT_COLORS[4], hist_kws={'alpha':1},\n                ax=ax)\n_ = ax.set(xlabel=\"Title Lenth\", ylabel = \"Number of Videos\", xticks=range(0, 110, 10))","3ecd7936":"fig, ax = plt.subplots()\n_ = ax.scatter(x=df['views'], y=df['title_length'], color=PLOT_COLORS[2], edgecolors=\"#000000\", \n               linewidth=0.5)\n_ = ax.set(xlabel = \"views\", ylabel = \"Title_Length\")","2b7c985d":"title_words = list(df[\"title\"].apply(lambda x: x.split()))\ntitle_words = [x for y in title_words for x in y]\nCounter(title_words).most_common(25)","80a6f486":"#wc = wordcloud.WordCloud(width=1200, height=600, collocations=False, Stopwords=None, \n#background_color=\"white\",colormap=\"tab20b\").generate_from_frequencies(dist(Counter(title_words).most_common(500)))\n\nwc = wordcloud.WordCloud(width=1200, height=500, collocations=False, background_color=\"white\",\n                        colormap=\"tab20b\").generate(\" \".join(title_words))\nplt.figure(figsize=(8,6))\nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")","00d768b7":"**Now we see the majority of trending videos have 1million views or less. let's see the exact percentage of videos less than 1million views**","3c070a84":"Now let's see some information about our dataset","207c6471":"Since, it's around 60%. Similarly, we see the percentage of videos with less than 1.5 million views is approx 71%, and that the percentage of videos with less than 5million views is around 91%","ff2363a4":"In the above scatter plot,we can see there is no relationship between these two variable i.e. title_length and number of views. \n\nThere is one interesting thing - videos that have 100,00,000 views have more than title length between 33 and 55 characters approximately.","e82c3207":"# Likes Histogram\n\nAfter views, we will plot the histogram for likes column","343d0b76":"**To see the correlation between the likes, dislikes, comments,and views lets plot a correlation matrix**","600b5677":"We note that the vast majority of trending videos have 5million views or less.\n\nNow lets us plot the histogram for videos with 25million views or less to get a closer look at the distribution of the data","f68abc8a":"# Video Title Lengths\nWe will add another column to dataset to represent the length of each video title,and visualize it in histogram to understand about the lengths of trending video title","17d64311":"# Which Video Category has the largest number of trending videos?\n\nFirst, we will add a column that contains category names based on the values in category_id column. We will use a category JSON file provided with the dataset which contains information about each category.","e8759915":"Notes from above table:\n1. Average number of likes - 74266, whereas dislikes are 3711\n2. Average number of views - 2,360,784 and median is 681861\n3. Average comment count - 8446 and max - 13,61,580","0ea3a70d":"# which videos were trending on maximum days and what is the title, likes, dislikes, comments, and views.","55cadac3":"In the dataset, the Trending Date and Published Time are not in the Unix date-time format. Let's fix this first.","ed48e3fc":"Approx 4079 videos were trending per day in USA","7e522d9f":"In the correlation plot matrix, for USA dataset, the columns with:\n1. High correlation - Views and likes, comment_count and Dislikes\n2. Medium Correlation - Views and comment_count, comment_count and dislikes\n3. Low Correlation - Likes and Dislike","48d75da1":"**Videos were trended for maximum days**\nThe Maximum no. of days a video trended is 30 i.e. for 'j4KvrAUjn6c' video id. Now, the below script gives its likes, dislikes, views and comments.","e515c3a8":"we note that the vast majority of trending videos have between 0 and 100,000 likes","f7477a2e":"# **TOP 5 USA Categories**","7e2abac0":"We see that the Entertainment category contains the largest number of trending videos among other categories: around 10,000 videos, followed by Music category with around 6,200 videos, followed by Howto & Style category with around 4100 videos and so on.","19f1ac21":"# **Top Trending Channel in USA**","283a067c":"We can see that dataset was collected in 2017-18\n\n77% in 2017 and 23% in 2018","1d2027ac":"# How many video titles contain Capitalized word?\n\nNow we want to see how many trending video titles contain atleast a capitalized word(e.g.HOW). To do that, we will add a new variable (column) to the dataset whose value is True if the video title has at least a capitalized word in it, and False otherwise","7acb94be":"we can see that 44% of trending video titles contain atleast a capitalized word. we will later use this added new column contains_capital_words in analyzing correlation between variables","eea47512":"we can see that views and likes are truly positively correlated: as one increases, the other increases too-mostly.","4cdea484":"There are 40379 entries in the dataset and all the columns in the dataset are complete(i.e. they have 40,949 non null entries) except description column which has some null values; it only has 40,379 non null values","afea960a":"Importing Libraries and Loading Data","c29e9c26":"# Description\nThe dataset includes data gathered from 40949 videos on YouTube that are contained within the trending category each day.\n\nThere are two kinds of data files, one includes comments (JSON) and one includes video statistics (CSV). They are linked by the unique video_id field.\n\nThe columns in the video file are:\n1. title\n2. channel_title\n3. video_id(Unique id of each video)\n4. trending_date\n5. title\n6. channel_title\n7. category_id (Can be looked up using the included JSON file)\n8. publish_time\n9. tags (Separated by | character, [none] is displayed if there are no tags)\n10. views\n11. likes\n12. dislikes\n13. comment_count\n14. thumbnail_link\n15. comments_disabled\n16. ratings_disabled\n17. video_error_or_removed\n18. description","72015375":"DATA CLEANING\nThe Description column has some null values. These are some of the rows whose description values are null. We can see that null values are denoted by NaN\n\n**For Data cleaning, and to get rid of null values that were set to empty string in place of each nullvalue in the description Column**","d397154e":"Approx 544 videos were trending per day in USA","45d3816c":"In 40949 videos - \"-\" and \"|\" have appeared 11452 and 10663 times respectively.\nWe notice also that words \"Video\",\"Trailer\",\"How\" and \"2018\" are common in trending video titles; each occured in 1613-1901 video titles.\n\n**We will draw a word cloud for most frequent words used**","cb7a50bc":"# Statistical Information about the numerical columns of dataset!","548166bb":"**Since, a video could be in trending for several days. There might be multiple rows of a particular video. In order to calculate the total_views, comments, likes, dislikes, of a video. we need to groupby with video_id. The below script will give the total no. of views\/comment\/likes, and dislikes of a video**","6b775bf2":"We can see that title-length distribution resembles a normal distribution, where most videos have title lengths between 30 and 60 characters approximately.\n\nNow let's draw a scatter plot between title length and number of views to see the relationship between these two variables","735d4ce2":"# Data Exploration","2513fc3e":"# Let's see the Collection year of Data","1a8a1223":"Let's verify that by plotting a scatter plot between views and likes to visualize the relationship between these variables","eace1b91":"Removing duplicates to get the correct numbers otherwise there will be redundancy\nGetting the number of videos on which comments disabled\/rating disabled\/video error","4617009e":"# Videos trending for more than a day","8bac3e04":"ESPN is in the Top list of channels in USA","450b98e9":"# Introduction\nYoutube videos are not only produced by vbloger or their other name, \"Youtubers\". Media corporations including Disney, CNN, BBC, and Hulu also offer some of their material via YouTube as part of the YouTube partnership program.\n\nIf your company, or yourself, a potential million-view youtuber, intend to employ this huge platform to publish your video, it is essential to enhance the content quality, and to increase its visibility. But why Youtube? Because it offers the possibility to monetize your videos, by adding ads during the video progression. With an in-depth analysis of thousands of videos, we could find several keys to increase views, likes, and the most important of all, your incomes.\n\nThe data used in this report can be found at: https:\/\/www.kaggle.com\/datasnaek\/youtube-new\/\n\nThe website says that it was last updated on May, 2019; however the lastest publish date in the data in 2018\/06\/14","fb884a1f":"# Setting up configuration for visuals","89511a9b":"This graph visualizes 24 category Id to be maximum in the range of 22-27","5d71a561":"Now we can see which category had the largest number of trending videos","48556376":"# Histogram of Views to verify-\n1. How many videos are between 10 million and 20 million views\n2. How many videos have between 20 million and 30 million views, and so on","935e6629":"# Trending Videos and Publishing Time\n\nPublish time is Cordinated Universal Time(UTC) time zone\n\nWe will add the date and hour of publishing each video, remove original publishing time column as we will not need it","3f1bdb7e":"# Data Preparation","59052559":"How many videos trending per day?","f69b6f5d":"# Most Common Words in video Title\nwe will verify significant words in trending video titles. we will display the 25 most common words in all trending video titles","725fa3a6":"# To be Continuedd...."}}