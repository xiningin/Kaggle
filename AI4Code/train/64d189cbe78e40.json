{"cell_type":{"19693f08":"code","fcf194af":"code","0716e11e":"code","62f4216d":"code","35b282a7":"code","2cc2a90e":"code","da112574":"code","60d612ff":"code","2a787539":"code","1699787d":"code","7b81a9e5":"code","4771b16d":"code","e0d2de71":"code","232b161b":"code","f50b82dd":"code","ea7cba40":"code","d30165b7":"code","7ff86d7e":"code","8272c7c1":"code","7c5b0e49":"code","fb191645":"code","3c1cda78":"code","c7f86c14":"code","a42f9f72":"code","87dee5a4":"code","707f3e46":"code","ba997135":"code","51abd92a":"markdown","027798c6":"markdown","cf079ff1":"markdown","6d6b9251":"markdown","208ba941":"markdown","d0be2cb6":"markdown","1ed376a2":"markdown","0c88bf1b":"markdown","89bf0ce6":"markdown"},"source":{"19693f08":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport optuna \nfrom optuna.visualization import plot_optimization_history\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom statistics import mean\n\nfrom vecstack import StackingTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nRS = 69 # :)","fcf194af":"df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ndf.drop('id', axis=1, inplace=True)\ndf","0716e11e":"# Remove Outliers\ndf = df.sort_values(by='target')\ndf = df.iloc[2:,:]","62f4216d":"# Split into Feat & Targets\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\nX.shape, y.shape","35b282a7":"# Train-Test-Validation Split, 60-20-20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= RS)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state= RS)\nevals = [(X_val, y_val)]","2cc2a90e":"# The XGBRegressor is built using GPU for accelerated computing! You can remove the variables and it will default to CPU\nmodel = XGBRegressor(predictor = 'gpu_predictor',\n                     tree_method = 'gpu_hist',\n                     eval_metric = 'rmse',\n                     verbosity=1)","da112574":"# Train Model with early stopping to prevent overfitting\nmodel.fit(X_train, y_train, eval_set = evals, eval_metric = 'rmse', early_stopping_rounds = 15)","60d612ff":"# Predict\ny_pred = model.predict(X_test)\ny_valpred = model.predict(X_val)\n\n# Compute Metrics\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nval_rmse = mean_squared_error(y_val, y_valpred, squared=False)\n\n# As a general rule of thumb expect your submission RMSE to be slightly higher than mean\nprint(\"The mean RMSE of the base model is {}\".format(mean((val_rmse, test_rmse))))","2a787539":"# Optuna iterates through this function\n\ndef objective(trial: Trial, X, y) -> float:\n    \n    # Split into Train-Test-Validation, 60, 20, 20\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RS)\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RS)\n    evals = [(X_val, y_val)]\n    \n    # Assign Parameter Dict\n    param = {\n                \"n_estimators\":trial.suggest_int('n_estimators', 0, 1000),\n                'max_depth':trial.suggest_int('max_depth', 2, 25),\n                'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n                'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n                'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n                'gamma':trial.suggest_int('gamma', 0, 5),\n                'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n                'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01)\n            }\n    \n    # Build Model\n    model = XGBRegressor(**param,\n                         predictor = 'gpu_predictor',\n                         tree_method = 'gpu_hist',\n                         eval_metric = 'rmse',\n                         verbosity=1)\n    \n    # Fit Model\n    model.fit(X_train, y_train, eval_set = evals, eval_metric = 'rmse', early_stopping_rounds = 15)\n    \n    # Predict\n    y_pred = model.predict(X_test)\n    y_valpred = model.predict(X_val)\n    \n    # Compute Metrics\n    test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n    val_rmse = mean_squared_error(y_val, y_valpred, squared=False)\n    \n    return mean((val_rmse, test_rmse))","1699787d":"%%time\n# To conserve computing time I have limited the trials here to 10, but I used 100 to develop my model\n\nstudy = optuna.create_study(study_name='Kaggle_Tabular_Compeition',\n                            direction='minimize',\n                            sampler=TPESampler())\n\n# Iterates through the function\nstudy.optimize(lambda trial : objective(trial, X, y), n_trials= 10)\n\nprint('Best trial: RMSE {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))","7b81a9e5":"# Dataframe of study \nhist = study.trials_dataframe()\nhist.head()","4771b16d":"# Plots a curve of how the mean RMSE changed through the 'epochs' (shoutout to all my ML nerds)\n# Deselect Objective Value to see the curve more clearly!\nplot_optimization_history(study)","e0d2de71":"print('Best mean trial: RMSE {},\\nparams {}'.format(study.best_trial.value, study.best_trial.params))","232b161b":"# Takes the 5 best optimized models to ensemble\n\n# This was the threshold I used to develop my model, choose one which suits you best and comment below!\n# bestTrials = hist[hist['value'] <= 0.699]\n\nbestTrials = hist.sort_values(by='value', ascending=True)\nbestTrials = bestTrials.head()\nbestTrials\n\n# Create a dictionary of best params for each bestTrialsis is a slightly 'hacky' way to do this, if you have a more elegant solution please let me know\nparam1 = bestTrials.iloc[0:1, 5:-1]\nparam1 = param1.to_dict()\n\nparam2 = bestTrials.iloc[1:2, 5:-1]\nparam2 = param2.to_dict()\n\nparam3 = bestTrials.iloc[2:3, 5:-1]\nparam3 = param3.to_dict()\n\nparam4 = bestTrials.iloc[3:4, 5:-1]\nparam4 = param4.to_dict()\n\nparam5 = bestTrials.iloc[4:5, 5:-1]\nparam5 = param5.to_dict()","f50b82dd":"# You now have to go through each param dictionary and manually edit it to work by copying, pasting and then reassigning! Please tell me there is a better way!\nparam1\n# param2\n# param3\n# param4\n# param5","ea7cba40":"param1 = {'params_colsample_bytree':0.91,\n          'params_gamma':0,\n          'params_learning_rate':0.16424108802390555,\n          'params_max_depth':6,\n          'params_min_child_weight':2,\n          'params_n_estimators':172,\n          'params_reg_alpha':0,\n          'params_reg_lambda':5\n         }\n\nparam2 = {'params_colsample_bytree':0.49,\n          'params_gamma':3,\n          'params_learning_rate':0.07499692005931666,\n          'params_max_depth':10,\n          'params_min_child_weight':5,\n          'params_n_estimators':865,\n          'params_reg_alpha':2,\n          'params_reg_lambda':1\n         }\n\nparam3 = {'params_colsample_bytree':0.36,\n          'params_gamma':4,\n          'params_learning_rate':0.015149948444442753,\n          'params_max_depth':22,\n          'params_min_child_weight':5,\n          'params_n_estimators':932,\n          'params_reg_alpha':5,\n          'params_reg_lambda':0\n         }\n\nparam4 = {'params_colsample_bytree':0.98,\n          'params_gamma':0,\n          'params_learning_rate':0.18311988381440686,\n          'params_max_depth':9,\n          'params_min_child_weight':5,\n          'params_n_estimators':507,\n          'params_reg_alpha':1,\n          'params_reg_lambda':2\n         }\n\nparam5 = {'params_colsample_bytree':0.56,\n          'params_gamma':5,\n          'params_learning_rate':0.02768954031767648,\n          'params_max_depth':13,\n          'params_min_child_weight':4,\n          'params_n_estimators':421,\n          'params_reg_alpha':0,\n          'params_reg_lambda':2\n         }","d30165b7":"print(\"Building Models...\")\nxgboost1 = XGBRegressor(**param1,\n                       predictor = 'gpu_predictor',\n                       tree_method = 'gpu_hist',\n                       random_state = RS,\n                       verbosity=0)\n\nxgboost2 = XGBRegressor(**param2,\n                       predictor = 'gpu_predictor',\n                       tree_method = 'gpu_hist',\n                       random_state = RS,\n                       verbosity=0)\n\nxgboost3 = XGBRegressor(**param3,\n                       predictor = 'gpu_predictor',\n                       tree_method = 'gpu_hist',\n                       random_state = RS,\n                       verbosity=0)\n\nxgboost4 = XGBRegressor(**param4,\n                       predictor = 'gpu_predictor',\n                       tree_method = 'gpu_hist',\n                       random_state = RS,\n                       verbosity=0)\n\nxgboost5 = XGBRegressor(**param5,\n                       predictor = 'gpu_predictor',\n                       tree_method = 'gpu_hist',\n                       random_state = RS,\n                       verbosity=0)\n\n# Notice how xgboost5 is not within the stack, that is for a reason\nmodels = [\n            ('XGB1', xgboost1),\n            ('XGB2', xgboost2),\n            ('XGB3', xgboost3),\n            ('XGB4', xgboost4)\n         ]\n\nprint(\"Built Models!\")","7ff86d7e":"# I used 10 folds but to save Compute time it is again only 2 here\nstack = StackingTransformer(estimators= models,\n                            regression= True,\n                            metric= mean_squared_error,\n                            n_folds= 2, \n                            shuffle= True,  \n                            random_state= RS,    \n                            verbose= 2)","8272c7c1":"%%time\nprint(\"Training Stack...\")\nstack = stack.fit(X_train, y_train)\nprint(\"Stack trained!\")","7c5b0e49":"# Create Stacked Train-Test\nS_train = stack.transform(X_train)\nS_test = stack.transform(X_test)","fb191645":"# Train Final Predictor\nxgboost5 = xgboost5.fit(S_train, y_train)\ny_pred = xgboost5.predict(S_test)\nprint('Final RMSE: %.6f' % mean_squared_error(y_test, y_pred, squared= False))\nprint(\"Stacking and HyperParam tuning decreased RMSE by {}%\".format(100-(study.best_trial.value\/mean_squared_error(y_test, y_pred, squared= False)*100)))\n\n# Obviously I have tuned down the model to save upon compute time but I urge you to fiddle with the parameters and see what you get!","3c1cda78":"test_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")","c7f86c14":"submission = pd.DataFrame(test_df.iloc[:, 0])\nX = test_df.drop('id', axis=1).values","a42f9f72":"X_test = stack.transform(X)\npreds = xgboost5.predict(X_test)","87dee5a4":"submission['target'] = preds\nsubmission = submission.set_index('id')","707f3e46":"submission","ba997135":"# Press Save All & Run All, after that has finished go to the submissions page on the tournament and you should be able to submit the version of the notebook you saved!\nsubmission.to_csv('submission.csv',index='id')","51abd92a":"# **Stacking the best XGBoost models**","027798c6":"# **Submitting Results**","cf079ff1":"Hello everybody! I learnt so much from so many differant people on Kaggle and so I thought it to be only right for me to give back to the community with my model development process. This is my first notebook and so any comments and upvotes are really appreciated!","6d6b9251":"**Building the Models**","208ba941":"# **Importing & Shaping the Data**","d0be2cb6":"# **Base Accuracy Score on XGBoost**","1ed376a2":"# **Hyperparameter Tuning using Optuna**","0c88bf1b":"# **Importing the Libraries**","89bf0ce6":"**Model Parameter Dictionaries**"}}