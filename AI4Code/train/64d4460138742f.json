{"cell_type":{"a9fc26cd":"code","07aab5b9":"code","653abd1c":"code","8587f9c6":"code","b81bf7aa":"code","1a1d7f89":"code","2eb382e6":"code","510eecf1":"code","2f98e34a":"code","17beca65":"code","275729d5":"code","747ee8e4":"code","cc4d9b5b":"code","9dbf3b52":"code","b2909064":"code","c4e6c407":"code","d0754306":"code","aa920ad5":"code","3434757a":"code","96d32ec9":"code","162f3de3":"code","3a73dba7":"code","45b0b95b":"code","1d0b53db":"code","d5d78ccd":"code","4515812e":"code","68808795":"code","c7251daa":"code","45839e20":"code","8056a3a7":"code","242e0d9c":"code","4fd575ff":"code","8e183be5":"code","65ba0a2f":"code","9de8102c":"code","f6f5d6d9":"code","9f784199":"code","29a3a3b2":"code","39da19b1":"code","7a574d48":"code","7f517663":"code","4e50b64e":"code","93560dab":"code","ba300d5f":"code","518147f7":"code","f070cd00":"code","bf9a7f02":"code","c158f394":"code","9754b283":"code","181a3732":"code","64d3a2af":"code","fa6b5c27":"code","1d4f9247":"code","5e5e79aa":"code","e6b45f28":"code","1518a688":"code","2553a80b":"code","860180ab":"code","3bbf3af4":"code","589094a0":"code","0ef47f47":"code","ae5b6fef":"code","3afcc711":"code","f7b92a5c":"code","cab05be1":"code","4e96579d":"code","95170769":"code","8baa9024":"code","eeb47161":"code","5591775e":"code","97e837a5":"code","327b68dd":"markdown","539bb8d5":"markdown","8ba3dcde":"markdown","c0a2fa42":"markdown","6635c419":"markdown","8faa042d":"markdown","95ae4dfa":"markdown","32e418d3":"markdown","ddea1196":"markdown","b5787aa6":"markdown","c594fa48":"markdown","b2f5b6c3":"markdown","9ae5ed83":"markdown","2352732c":"markdown","cd0bcccf":"markdown","a776f083":"markdown","1cc94e95":"markdown","a46e57bb":"markdown","3ac82150":"markdown","1b8590d9":"markdown","c20ad944":"markdown","dea6ed6f":"markdown","2658913f":"markdown","52ac1a6d":"markdown","c893be4c":"markdown","2ec8b58f":"markdown","9ff507a4":"markdown","c22c0d9e":"markdown","256660bb":"markdown","33893fe4":"markdown","da0468c4":"markdown","e77c54c8":"markdown","385f6255":"markdown","c801afd3":"markdown","ce043591":"markdown","527ac366":"markdown","4802183b":"markdown","7e1f254e":"markdown","2901be64":"markdown","a906f3de":"markdown","4ab2abdd":"markdown","12d04099":"markdown","b585bef0":"markdown","65a69ac3":"markdown","9aea14bb":"markdown","b1fdcc6a":"markdown","001e24eb":"markdown","2e1d50f9":"markdown","5072b2bd":"markdown","3c98ce25":"markdown","9c14e258":"markdown","7d567883":"markdown","fb791e1a":"markdown","6850340b":"markdown","6031444e":"markdown","541cf08e":"markdown","d58d213c":"markdown","5c491184":"markdown","c57f813e":"markdown"},"source":{"a9fc26cd":"#Imports\nimport numpy as np\nimport pandas as pd\n\n#Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [12,6]\nsns.set(style=\"darkgrid\")\n\n#Statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\n\n#Float Output to 2 decimals\npd.set_option('display.float_format', lambda x: '{:.2f}'.format(x))\n\n#Models\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n#Quality of life\nimport warnings\nwarnings.filterwarnings('ignore')","07aab5b9":"#Importing datasets\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","653abd1c":"#First look training set\ntrain.head()","8587f9c6":"#First look test set\ntest.head()","b81bf7aa":"#Saving then dropping Id column as it does not impact our prediction\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","1a1d7f89":"#visualizing outliers, seen in bottom right.\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice');","2eb382e6":"#the vast difference between 2 values of GrLivArea over 4000 and SalePrice under 200000 and the rest of datapoints makes me believe these outliers are safe to remove from the training set\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<200000)].index)","510eecf1":"#visualizing outliers, seen in bottom right.\nsns.scatterplot(data=train, x='GrLivArea', y='SalePrice');","2f98e34a":"sns.distplot(train['SalePrice'] , fit=norm);\n\n#Fitted parameters used by function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Visualize Normal Distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.0f} and $\\sigma=$ {:.0f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Visualize Quantile-Quantile plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","17beca65":"#Using numpy log1p fuction, which applies log(1+x) to elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n#Fitted parameters used by function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Visualize Normal Distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Visualize Quantile-Quantile plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","275729d5":"#Correlation map to understand how other features relate to SalePrice\ncorrmat = train.corr()\nmask = np.zeros_like(corrmat)\nmask[np.triu_indices_from(mask)] = True\n\nwith sns.axes_style(\"white\"):\n\n    f, ax = plt.subplots(figsize=(10, 10))\n    sns.heatmap(corrmat, mask=mask, ax=ax, cbar_kws={\"shrink\": .82},vmax=.9, cmap='coolwarm', square=True)","747ee8e4":"#Creating index for splitting datasets\ni_train = train.shape[0]\ni_test = test.shape[0]\n\n#Setting y_train\ny_train = train.SalePrice.values\n\n#Concatenating datasets\ndf_concat = pd.concat((train, test)).reset_index(drop=True)\ndf_concat.drop(['SalePrice'], axis=1, inplace=True)\n\n#Printing shape of datasets\nprint(train.shape)\nprint(test.shape)\nprint(df_concat.shape)","cc4d9b5b":"#Creating dataframe with percentages of missing values\ndf_concat_na = (df_concat.isnull().sum() \/ len(df_concat)) * 100\n\n#Sorting by percentages of missing values\ndf_concat_na = df_concat_na.sort_values(ascending=False)[:20] #values of missing percentages after index 20 drop to around 0.03.. \n\nmissing_values = pd.DataFrame({'Missing %' :df_concat_na})\nmissing_values.head()","9dbf3b52":"#Visualizing missing values by feauture\nsns.barplot(x=df_concat_na.index, y=df_concat_na)\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('% of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15);","b2909064":"df_concat['PoolQC'].fillna('None', inplace=True)","c4e6c407":"df_concat['MiscFeature'].fillna('None', inplace=True)","d0754306":"df_concat['Alley'].fillna('None', inplace=True)","aa920ad5":"df_concat['Fence'].fillna('None', inplace=True)","3434757a":"df_concat['FireplaceQu'].fillna('None', inplace=True)","96d32ec9":"df_concat[\"LotFrontage\"] = df_concat.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","162f3de3":"'''\nv6 - Possible direction for future notebooks to compute missing values seperately\n\ntrain_copy = train.copy(deep=True)\ntrain_copy.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nLotF_median_train = train_copy['LotFrontage']\n\ndf_concat.iloc[:i_train]['LotFrontage'] = LotF_median_train\n'''","3a73dba7":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_concat[col].fillna('None', inplace=True)","45b0b95b":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    df_concat[col].fillna(0, inplace=True)","1d0b53db":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):    \n    df_concat[col].fillna('None', inplace=True)","d5d78ccd":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtHalfBath', 'BsmtFullBath'):\n    df_concat[col].fillna(0, inplace=True)","4515812e":"df_concat['MasVnrType'].fillna('None', inplace=True)","68808795":"df_concat['MasVnrArea'].fillna(0, inplace=True)","c7251daa":"df_concat['MSZoning'].value_counts()","45839e20":"df_concat['MSZoning'].fillna('RL', inplace=True)","8056a3a7":"df_concat['Utilities'].value_counts()","242e0d9c":"df_concat.drop(['Utilities'], axis=1, inplace=True)","4fd575ff":"df_concat['Functional'].fillna('Typ', inplace=True)","8e183be5":"df_concat['Electrical'].fillna('SBrkr', inplace=True)","65ba0a2f":"df_concat['KitchenQual'].fillna('TA', inplace=True)","9de8102c":"df_concat['Exterior1st'].fillna('TA', inplace=True)\ndf_concat['Exterior2nd'].fillna('TA', inplace=True)","f6f5d6d9":"df_concat['SaleType'].fillna('WD', inplace=True)","9f784199":"#Creating dataframe with percentages of missing values\ndf_concat_na = (df_concat.isnull().sum() \/ len(df_concat)) * 100\n\n#Sorting by percentages of missing values\ndf_concat_na = df_concat_na.sort_values(ascending=False) \n\nmissing_values = pd.DataFrame({'Missing %' :df_concat_na})\nmissing_values.head()","29a3a3b2":"'''\n#Ordinal features\nordinal_to_encode=['LandSlope','YearBuilt','YearRemodAdd','CentralAir','GarageYrBlt','PavedDrive','YrSold']\n\n#Encoding features for model with LabelEncoder\nfor field in ordinal_to_encode:\n    le = LabelEncoder()\n    df_concat[field] = le.fit_transform(df_concat[field].values)\n'''","39da19b1":"#ordinal_ready = ['OverallQual','OverallCond','MoSold','FullBath','KitchenAbvGr','TotRmsAbvGrd']","7a574d48":"'''ordinal_mappings = {\n    \"MSSubClass\": ['None', 20, 30,40,45,50,60,70,75,80,85, 90,120,150,160,180,190], \n    \"ExterQual\": ['None','Fa','TA','Gd','Ex'], \n    \"LotShape\": ['None','Reg','IR1' ,'IR2','IR3'], \n    \"BsmtQual\": ['None','Fa','TA','Gd','Ex'], \n    \"BsmtCond\": ['None','Po','Fa','TA','Gd','Ex'], \n    \"BsmtExposure\": ['None','No','Mn','Av','Gd'], \n    \"BsmtFinType1\": ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], \n    \"BsmtFinType2\": ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], \n    \"HeatingQC\": ['None','Po','Fa','TA','Gd','Ex'], \n    \"Functional\": ['None','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], \n    \"FireplaceQu\": ['None','Po','Fa','TA','Gd','Ex'], \n    \"KitchenQual\": ['None','Fa','TA','Gd','Ex'], \n    \"GarageFinish\": ['None','Unf','RFn','Fin'], \n    \"GarageQual\": ['None','Po','Fa','TA','Gd','Ex'], \n    \"GarageCond\": ['None','Po','Fa','TA','Gd','Ex'], \n    \"PoolQC\": ['None','Fa','Gd','Ex'], \n    \"Fence\": ['None','MnWw','GdWo','MnPrv','GdPrv'],\n}\n\n# transform to a suitable format for ce.OrdinalEncoder\nce_ordinal_mappings = []\nfor col, unique_values in ordinal_mappings.items():\n    local_mapping = {val:idx for idx, val in enumerate(unique_values)}\n    ce_ordinal_mappings.append({\"col\":col, \"mapping\":local_mapping})\n    \nencoder = ce.OrdinalEncoder(mapping=ce_ordinal_mappings, return_df=True)\nencoder.fit_transform(train_X)\nencoder.transform(test)\n'''","7f517663":"#MSSubClass\ndf_concat['MSSubClass'].astype(str)\n\n\n#Changing OverallCond into a categorical variable\ndf_concat['OverallCond'] = df_concat['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndf_concat['YrSold'] = df_concat['YrSold'].astype(str)\ndf_concat['MoSold'] = df_concat['MoSold'].astype(str)","4e50b64e":"#List of features to encode\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n#Applying LabelEncoder to categorical features\nfor i in cols:\n    le = LabelEncoder() \n    le.fit(list(df_concat[i].values)) \n    df_concat[i] = le.transform(list(df_concat[i].values))","93560dab":"df_concat['TotalSF'] = df_concat['TotalBsmtSF'] + df_concat['1stFlrSF'] + df_concat['2ndFlrSF']","ba300d5f":"numeric_feats = df_concat.dtypes[df_concat.dtypes != \"object\"].index\n\n#Check the skew of all numerical features\nskewed_feats = df_concat[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","518147f7":"skewness = skewness[abs(skewness.Skew)>0.75]\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #df_concat[feat] += 1\n    df_concat[feat] = boxcox1p(df_concat[feat], lam)","f070cd00":"df_concat = pd.get_dummies(df_concat)","bf9a7f02":"train = df_concat[:i_train]\ntest = df_concat[i_train:]","c158f394":"#Creating cross validation strategy with shuffle\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=0).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9754b283":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=0))","181a3732":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=0))","64d3a2af":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","fa6b5c27":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =0)\n\n","1d4f9247":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =0, nthread = -1,\n                             verbosity = 0)","5e5e79aa":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11,\n                              verbose_eval = -1)\n","e6b45f28":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1518a688":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2553a80b":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","860180ab":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3bbf3af4":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","589094a0":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","0ef47f47":"class AverageModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    #Define clones of original models to fit the data\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        #Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Predictions for cloned models and get average\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","ae5b6fef":"averaged_models = AverageModels(models = (lasso, ENet, GBoost, KRR))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3afcc711":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    #Fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        #Train the cloned meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Predictions of all base models on test data and use the averaged predictions as \n    #meta-features for the final prediction\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","f7b92a5c":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","cab05be1":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","4e96579d":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","95170769":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","8baa9024":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","eeb47161":"print(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","5591775e":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","97e837a5":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","327b68dd":"### Missing Data","539bb8d5":"### Stacking Models\n\n\n**Average Base Models**\n\nBuilding a new scaleable class to extend scikit-learn with our model","8ba3dcde":"### Outliers","c0a2fa42":"### Data Correlation","6635c419":"**KitchenQual**: Kitchen quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       \nWe replace its only missing value by the mode of TA       ","8faa042d":"**GarageType**: Garage location\n\t\t\n       2Types\tMore than one type of garage\n       Attchd\tAttached to home\n       Basment\tBasement Garage\n       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n       CarPort\tCar Port\n       Detchd\tDetached from home\n       NA\tNo Garage\n\t\t\n**GarageFinish**: Interior finish of the garage\n\n       Fin\tFinished\n       RFn\tRough Finished\t\n       Unf\tUnfinished\n       NA\tNo Garage\n\n**GarageQual**: Garage quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n\t\t\n**GarageCond**: Garage condition\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical\/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n       \nNA means lack of this feature, so we fill missing values with None       ","95ae4dfa":"**GarageYrBlt**: Year garage was built\n\n**GarageCars**: Size of garage in car capacity\n\n**GarageArea**: Size of garage in square feet\n\nMissing numerical values will be replaced by 0","32e418d3":"### Feature Engineering\nConcatenating training and test set into same dataframe for preprocessing. Our feature engineering mainly revolves around harmonizing values so they more accurately reflect reality, whilst increasing predictions.\n\n*version 6 note: **data leakage will occur** since we impute values based on test+train datasets, leading to a too optimistic score. In future version we will calculate compute missing values and apply Box-Cox seperately.*","ddea1196":"**Creating new training and test sets**","b5787aa6":"**Elastic Net Regression**","c594fa48":"**MiscFeature**: Miscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\tNone\n\nEqually here, NA refers to lack of this feature. No misc. features means we can again apply a 'None' fill.","b2f5b6c3":"**SaleType**: Type of sale\n\t\t\n       WD \tWarranty Deed - Conventional\n       CWD\tWarranty Deed - Cash\n       VWD\tWarranty Deed - VA Loan\n       New\tHome just constructed and sold\n       COD\tCourt Officer Deed\/Estate\n       Con\tContract 15% Down payment regular terms\n       ConLw\tContract Low Down payment and low interest\n       ConLI\tContract Low Interest\n       ConLD\tContract Low Down\n       Oth\tOther\n       \nMissing values will be filled with most frequent type WD       ","9ae5ed83":"The target variable is right skewed, which is a problem for our model as it needs to be normally distributed. This means we need to apply a Log transformation.","2352732c":"**Utilities**: Type of utilities available\n\t\t\n       AllPub\tAll public Utilities (E,G,W,& S)\t\n       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n       NoSeWa\tElectricity and Gas Only\n       ELO\tElectricity only\n       \nThe value of this feauture is AllPub for most of the dataset, with only 1 NoSeWa feauture. We can fately assume this feauture will not be helpful in predictive modelling, and will remove it.      ","cd0bcccf":"### Log transformation","a776f083":"**Stacking Averaged Base Models**","1cc94e95":"**Average Base Models Score**","a46e57bb":"We clearly see on where we need to focus our work.\n1. Coming up with a strategy to the missing values with over 80% NAs\n2. Defining approach in imputing other missing values ","3ac82150":"**Meta-model**\n\nIn this model of models we use our base models to train a new meta-model. That's a lot of models.\n\n1. Create train and holdout sets\n2. Train base models on train set\n3. Test base models on holdout\n4. Use predictions as output to train meta-model","1b8590d9":"**Kernel Ridge Regression**","c20ad944":"**LightGBM**","dea6ed6f":"**MSZoning**: Identifies the general zoning classification of the sale.\n\t\t\n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density\n       \nAs RL is by far the most common, we take the mode and fill in missing values with RL.       ","2658913f":"**FireplaceQu**: Fireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n       \nNA means lack of this feature, so we fill missing values with None       ","52ac1a6d":"**Adding a Total sqf feature**\n\nSince we've seen the correlation between square footage on SalePrice, adding a feature combing all seperate parameters together seems logical as a great predictor. ","c893be4c":"Even the simplest stacking approach offers great improvement to our score, and encouragement to keep exploring further options.","2ec8b58f":"**BsmtQual**: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement\n\t\t\n**BsmtCond**: Evaluates the general condition of the basement\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement\n\t\n**BsmtExposure**: Refers to walkout or garden level walls\n\n       Gd\tGood Exposure\n       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n       Mn\tMimimum Exposure\n       No\tNo Exposure\n       NA\tNo Basement\n\t\n**BsmtFinType1**: Rating of basement finished area\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\t\t\n**BsmtFinType2**: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n       \nNA means lack of this feature, so we fill missing values with None.       ","9ff507a4":"**Stacked Regressor**","c22c0d9e":"### Modelling","256660bb":"**LASSO Regression**","33893fe4":"### Credits & Inspiration\n* [Stacked Regression by Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook) - Absolutely stunning notebook that I've used as the template for my own explorations here. Taught me a lot about feature engineering and stacked modelling.\n* [Stacking Ensemble Machine Learning With Python by Jason Brownlee](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/) - Great tutorial on stacking models\n* [Comprehensive data exploration with Python by Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) - In-depth EDA on this dataset\n* [A study on Regression applied to the Ames dataset by juliencs](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) - Great notebook using linear regression on this dataset\n* [Missing Values, Ordinal data and stories\n by mitra mirshafiee](https:\/\/www.kaggle.com\/mitramir5\/missing-values-ordinal-data-and-stories) - Illustrative and educational notebook on handling missing values","da0468c4":"**Ensembling StackedRegressor, XGBoost and LightGBM** ","e77c54c8":"**PoolQC:** Pool quality\n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage\/Typical\n       Fa\tFair\n       NA\tNo Pool\n       \nAs NA means 'No Pool' we can safely assume a missing value equates to None","385f6255":"**Electrical**: Electrical system\n\n       SBrkr\tStandard Circuit Breakers & Romex\n       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n       Mix\tMixed\n       \nIt's only NA value will be set to SBrkr.       ","c801afd3":"### Model Scores","ce043591":"The SalePrice has been transformed to be normally distributed.","527ac366":"**XGBoost**","4802183b":"**Gradient Boosting**","7e1f254e":"**Functional**: Home functionality (Assume typical unless deductions are warranted)\n\n       Typ\tTypical Functionality\n       Min1\tMinor Deductions 1\n       Min2\tMinor Deductions 2\n       Mod\tModerate Deductions\n       Maj1\tMajor Deductions 1\n       Maj2\tMajor Deductions 2\n       Sev\tSeverely Damaged\n       Sal\tSalvage only\n       \nAny missing values will be transformed to Typ.     ","2901be64":"**Dummy Variables**","a906f3de":"**RMSLE score**","4ab2abdd":"**Stacking Averaged Models Score**","12d04099":"**MasVnrType**: Masonry veneer type\n\n       BrkCmn\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       None\tNone\n       Stone\tStone\n       \nMissing values will be categorized as None       ","b585bef0":"Fence: Fence quality\n\t\t\n       GdPrv\tGood Privacy\n       MnPrv\tMinimum Privacy\n       GdWo\tGood Wood\n       MnWw\tMinimum Wood\/Wire\n       NA\tNo Fence\n\nNA means lack of this feature, so we fill missing values with None ","65a69ac3":"We can immediately spot how the OverallQual and GrLivArea features have the high positive correlation with SalePrice.","9aea14bb":"### Credits & Inspiration\n* [Stacked Regression by Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook) - Absolutely stunning notebook that I've used as the template for my own explorations here. Taught me a lot about feature engineering and stacked modelling.\n* [Stacking Ensemble Machine Learning With Python by Jason Brownlee](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/) - Great tutorial on stacking models\n* [Comprehensive data exploration with Python by Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) - In-depth EDA on this dataset\n* [A study on Regression applied to the Ames dataset by juliencs](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) - Great notebook using linear regression on this dataset\n* [Missing Values, Ordinal data and stories\n by mitra mirshafiee](https:\/\/www.kaggle.com\/mitramir5\/missing-values-ordinal-data-and-stories) - Illustrative and educational notebook on handling missing values","b1fdcc6a":"# Regression\n\n### Housing Price Prediction\n\nThe goal of this competition entry is apply concepts I've learnt so far, and gain more practise in feature engineering and stacked models.\n\nAs the missing features of this dataset are rather sparse, engineering them will mainly mean:\n\n* Imputing missing values\n* Dealing with outliers\n* Transforming variables\n* Label Encoding\n* Stabilizing variance\/skewness\n* Aquiring dummy variables\n\nModel will be a sklearn base + XGBoost + LightGBM stack predicting the **SalePrice**.","001e24eb":"**XGBoost**","2e1d50f9":"**MasVnrArea** : Masonry veneer area in square feet\nAs an architect I'm astonished this feature even exists. We assume missing values mean no masonry veneer, meaning 0.","5072b2bd":"**Missing Values Check**","3c98ce25":"### Skewness and Box Cox Transformation\nCalculating degree of skewness and computing Box Cox transformation of 1 + x\n\n*Note: applying Box Cox transformation to both datasets at once will lead to data leakage and will be addressed in future versions.*","9c14e258":"**BsmtFinSF1**: Type 1 finished square feet\n\n**BsmtFinSF2**: Type 2 finished square feet\n\n**BsmtUnfSF**: Unfinished square feet of basement area\n\n**TotalBsmtSF**: Total square feet of basement area\n\n**BsmtFullBath**: Basement full bathrooms\n\n**BsmtHalfBath**: Basement half bathrooms\n\nMissing numerical values will be replaced by 0","7d567883":"**LightGBM**","fb791e1a":"**Exterior1st**: Exterior covering on house\n**Exterior2nd**: Exterior covering on house (if more than one material)\n\n       AsbShng\tAsbestos Shingles\n       AsphShn\tAsphalt Shingles\n       BrkComm\tBrick Common\n       BrkFace\tBrick Face\n       CBlock\tCinder Block\n       CemntBd\tCement Board\n       HdBoard\tHard Board\n       ImStucc\tImitation Stucco\n       MetalSd\tMetal Siding\n       Other\tOther\n       Plywood\tPlywood\n       PreCast\tPreCast\t\n       Stone\tStone\n       Stucco\tStucco\n       VinylSd\tVinyl Siding\n       Wd Sdng\tWood Siding\n       WdShing\tWood Shingles\n\nSince both have a single missing value we again replace the most common string as the missing value.","6850340b":"**LotFrontage**: Linear feet of street connected to property\n\nWe assume here that neighborhoods contrain similar houses to fill in missing values by median LotFrontage per Neighborhood. Computing median has to be split per dataset, so as to not cause data leakage.","6031444e":"### Univariate Analysis SalePrice","541cf08e":"**Alley**: Type of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access\n       \nNA means lack of this feature, so we fill missing values with None ","d58d213c":"### Label Encoding\nSimple (simplified) encoding.\n\nFuture version: Transforming ordinal and nominal features","5c491184":"**Ensemble Prediction**","c57f813e":"### Imputing Missing Values\nWe will handle missing values by descending order of %NA. For each feature we'll first learn what we can from the description provided with the datasets, and secondly, provide our method for dealing with the missing values."}}