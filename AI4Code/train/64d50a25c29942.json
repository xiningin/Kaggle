{"cell_type":{"a7df33df":"code","d6f53025":"code","f1297724":"code","ea094a44":"code","bc4e2475":"code","a76968ce":"code","0d20781b":"code","d738aa56":"code","7514b2e1":"code","2a5ef657":"code","20bc8d09":"code","2e80a11e":"code","e91fb5ac":"code","7043b38f":"code","bff73736":"code","0e0ee12d":"code","20bcfbdd":"code","12872cd5":"code","8eb38528":"code","9fcd6fe4":"code","3d2b8b83":"code","b3da3b41":"code","a8c424c3":"code","28cb285d":"code","a0ee3338":"code","270c5147":"code","67aaf204":"code","8c0532a7":"code","82346a36":"code","7e08d504":"markdown","5a8171aa":"markdown","51e64d68":"markdown","42fddd0e":"markdown","e0c4a2e9":"markdown","5c212534":"markdown","71356fd8":"markdown","890dd468":"markdown","d6ed4f55":"markdown","505f9eb6":"markdown","d9800906":"markdown"},"source":{"a7df33df":"# Parameters:\n\n#stockName_str = stockName_strLst[3] # google\n\n# Test Set Percentage\ntestSet_pct = .01\n\n# Validation Set Percentage\nvalSet_pct = .2\n\n# Years of stock history to download\nhistoryYears_int = 8\n\n# PCA Factor for the preprocessing phase\npcaFactor_r = .95\n\n# Forecast History Length\nhistoryLength_int = 30\n\n# download data in daily intervals\ninterval_str = [ '1m', '5m', '15m', '30m', '60m', '1h', '1d', '1wk', '1mo'][6]\n\n# Figsize\nfigSize_tpl = (24,4.5)","d6f53025":"# Stock Tickers\n\nstockName_strLst = [\n    'SPY',    # SNP 500\n    'TSLA',   # TESLA\n    'GOOGL',  # GOOGLE\n    'MSFT',   # MICROSOFT\n    'ORCL',   # OCRACLE\n    'FB',     # FACEBOOK\n    'IBM',    # IBM\n    'AMZN',   # AMAZON\n    'CSCO',   # CISCO    \n]\n","f1297724":"# Import of the relevant libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nfrom pandas_datareader.data import DataReader\nfrom datetime import datetime\n\n# import the necessary libraries to connect to Yahoo\n!pip install yfinance > \/dev\/null\nimport yfinance as yf\n\nfrom sklearn.metrics import mean_squared_error\n\n","ea094a44":"# Support class to measure the runtime of model training\n# Not terribly necessary but a nice to have\nimport time\n\nclass Timer():\n    # Static variable for verbosity\n    quiet = False\n    \n    # Init the class\n    def __init__(self, str): \n        # print('init method called') \n        self.str = str\n        \n    # Enter the context\n    def __enter__(self):\n        # print('enter method called') \n        self.tick = time.time()\n        return self\n    \n    # Leave the context\n    def __exit__(self, exc_type, exc_value, exc_traceback): \n        if not Timer.quiet:\n            print(\"%s: \\t%.3f s\"%(self.str, time.time() - self.tick))\n","bc4e2475":"# Download the stock data from Yahoo Finance API\n\nend_dt = datetime.now()\nstart_dt = datetime(end_dt.year - historyYears_int, end_dt.month, end_dt.day)\n\n# API call\ndata_df = yf.download(\n    stockName_strLst, \n    start=start_dt, \n    end=end_dt, \n    progress=False,\n    interval = interval_str,\n)\nsequence_df = data_df['Close']\n\n\nsequence_df.drop(\n  index=sequence_df.index[0], \n  axis=0, \n  inplace=True\n)\n\n# Check for NaNs\nassert not sequence_df.isna().any().any()\n\n# Extract the numpy array from the dataframe\nsequence_np = sequence_df.values\n\ndataN_int = sequence_np.shape[0]","a76968ce":"# Present the downloaded data\n\nplt.figure(figsize=figSize_tpl)\nplt.plot(sequence_np)","0d20781b":"# Perform the split to Train\/Validation\/Test datasets\n\n# Get the row-counts for each set\ntestN_int = int(np.round(dataN_int * testSet_pct))\nvalN_int = int(np.round(dataN_int * valSet_pct))\ntrainN_int = dataN_int - testN_int - valN_int\ntrainSet_pct = 1 - testSet_pct - valSet_pct\n\nprint(f\"dataN_int: {dataN_int}\")\nprint(f\"testN_int: {testN_int}\")\nprint(f\"valN_int: {valN_int}\")\nprint(f\"trainN_int: {trainN_int}\")\n\n# Extract the 'raw' numpy arrays\ntrain_np = sequence_np[:trainN_int,:]\nval_np = sequence_np[trainN_int:trainN_int+valN_int,:]\ntest_np = sequence_np[trainN_int+valN_int:,:]\n\n# Extract the 'time' fields for each array (useful for plotting)\ntrainT_np = np.arange(0,trainN_int)\nvalT_np = np.arange(trainN_int,trainN_int + valN_int)\ntestT_np = np.arange(trainN_int + valN_int,dataN_int)\n\nplt.figure(figsize=figSize_tpl)\nplt.plot(trainT_np, train_np, 'r')\nplt.plot(valT_np, val_np, 'b')\nplt.plot(testT_np, test_np, 'g')\nplt.legend(['Training Set', 'Validation Set', 'Test Set'])\n#plt.axis(xmin=200, xmax=250, ymin=1900, ymax=2000)\n\n# Confirm I did not botch the indices\nx = np.concatenate((train_np, val_np, test_np))\n\n# assert np.all(np.equal(sequence_np,x))\nassert np.array_equal(sequence_np,x,equal_nan=True)\n\n","d738aa56":"# Imports \n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import FastICA\n","7514b2e1":"# Moving Average Module\n\nclass MovingAverageModule:\n\n  def __init__(self, windowLength=0): \n    self.windowLength_int = windowLength\n\n  def transform(self, sequence_np):\n    if self.windowLength_int > 0:\n      return pd.DataFrame(sequence_np).rolling(window=self.windowLength_int,min_periods=1).mean().values\n    else:\n      return sequence_np\n\n  def fit_transform(self, sequence_np):\n    return self.transform(sequence_np)\n  \n  def inverse_transform(self, sequencePP_np):\n    return sequencePP_np\n","2a5ef657":"# Differencing Module\n\nclass DiffModule:\n\n  def __init__(self): \n    pass\n\n  def transform(self, sequence_np):\n    return np.diff(sequence_np, axis=0)\n\n  def fit_transform(self, sequence_np):\n    return self.transform(sequence_np)\n  \n  def inverse_transform(self, sequencePP_np):\n    return np.cumsum(sequencePP_np, axis=0)\n\nx = DiffModule().transform(train_np)\ny = DiffModule().inverse_transform(x)\n","20bc8d09":"# Define the Power Module\n# calculates the powers of each feature \n# and concatenates it to the feature matrix\n\nclass PowerModule():\n  \n  def __init__(self, n): \n    self.n=n\n\n  def transform(self, sequence_np):\n    sequence_np = np.concatenate((sequence_np,np.power(sequence_np, self.n)),axis=1)\n    return sequence_np\n\n  def fit_transform(self, sequence_np):\n    return self.transform(sequence_np)\n  \n  def inverse_transform(self, sequencePP_np):\n    return sequencePP_np[:,sequencePP_np.shape[1]\/2]\n","2e80a11e":"# Define the preprocessor class\n\n\nclass Preprocessor:\n  \n    def __init__(self, modules=None): \n      if modules!=None:\n        self.modules_lst = modules\n      else:\n        self.modules_lst = [\n#          MovingAverageModule(2),\n          DiffModule(),\n#          PowerModule(3),\n          StandardScaler(),\n          PCA(pcaFactor_r, whiten=True),\n          FastICA(whiten=True),\n#          MinMaxScaler((0,1)),\n#          StandardScaler(),\n          RobustScaler(),\n        ]\n\n    def transform(self, sequence_np):\n      sequencePP_np = sequence_np\n      for module in self.modules_lst:\n        sequencePP_np = module.transform(sequencePP_np)\n      return sequencePP_np\n        \n    def fit_transform(self,sequence_np):\n      sequencePP_np = sequence_np\n      for module in self.modules_lst:\n#        print(f\"Performing {type(module)} on {sequencePP_np.shape}\")\n        sequencePP_np = module.fit_transform(sequencePP_np)\n      return sequencePP_np\n\n    def inverse_transform(self, sequencePP_np):\n      sequence_np = sequencePP_np\n      for module in reversed(self.modules_lst):\n        \n        sequence_np = module.inverse_transform(sequence_np)\n        print(f\"Performing {type(module)} on {sequence_np.shape}\")\n      return sequence_np\n\n","e91fb5ac":"# Preprocess the data \n\npreprocessor = Preprocessor()\ntrainPP_np = preprocessor.fit_transform(train_np)\nvalPP_np = preprocessor.transform(val_np)\n","7043b38f":"# Present the resulting time-series to be forecasted\n\nN, M = train_np.shape\nrows_int = int(np.ceil(np.sqrt(M)))\ncols_int = rows_int\nplt.figure(figsize=figSize_tpl)\nplt.suptitle(\"Original\")\nfor idx in range(M):\n  if len(stockName_strLst) == M:\n      plt.title(stockName_strLst[idx])\n  plt.subplot(rows_int, cols_int, idx+1)\n  plt.plot(train_np[:,idx])\n\nN, M = trainPP_np.shape\nrows_int = int(np.ceil(np.sqrt(M)))\ncols_int = rows_int\nplt.figure(figsize=figSize_tpl)\nplt.suptitle(\"Preprocessed Signal\")\nfor idx in range(M):\n    plt.subplot(rows_int, cols_int, idx+1)\n    plt.plot(trainPP_np[:,idx])\n","bff73736":"# Show the histogram of the each time series\n\ndef plotHistograms(sequence_np, labels_strLst=None):\n  N, M = sequence_np.shape\n  plt.figure(figsize=figSize_tpl)\n\n  rows_int = int(np.ceil(np.sqrt(M)))\n  cols_int = rows_int\n  plt.figure(figsize=figSize_tpl)\n  #plt.suptitle(title_str)\n  for idx in np.arange(M):\n    plt.subplot(rows_int, cols_int, idx+1)\n    plt.hist(sequence_np[:,idx],bins=100)\n    if labels_strLst!=None:\n      plt.title(labels_strLst[idx])\n\nplotHistograms(train_np,stockName_strLst)\nplotHistograms(trainPP_np)\n","0e0ee12d":"# Show the spectra of the features\n\ndef plotSpectra(sequence_np, labels_strLst=None):\n  N, M = sequence_np.shape\n  trainSpect_np = np.fft.fftshift(np.fft.fft(sequence_np, axis=0)) \/ N\n  df = 1.0\/(60*60*24)\n  f_np = np.linspace(-df\/2,df\/2,N)\n\n  rows_int = int(np.ceil(np.sqrt(M)))\n  cols_int = rows_int\n  plt.figure(figsize=figSize_tpl)\n  for idx in np.arange(M):\n    plt.subplot(rows_int, cols_int, idx+1)\n    #plt.plot(f_np, np.real(trainSpect_np[:,idx]),'b')\n    #plt.plot(f_np, np.imag(trainSpect_np[:,idx]),'r')\n    plt.plot(f_np, np.abs(trainSpect_np[:,idx]),'g')\n    if labels_strLst!=None:\n      plt.title(labels_strLst[idx])\n\n\nplotSpectra(train_np, stockName_strLst)\nplotSpectra(trainPP_np)\n","20bcfbdd":"# Plot the autocorrelation of each component\n\ndef plotAutocorr(sequence_np, labels_strLst=None):\n  N, M = sequence_np.shape\n  \n  autoCorr_np = np.zeros((2*N-1,M))\n  lag_np = np.linspace(-N,N,2*N-1)\n  rows_int = int(np.ceil(np.sqrt(M)))\n  cols_int = rows_int\n  plt.figure(figsize=figSize_tpl)\n  for idx in np.arange(M):\n\n    plt.subplot(rows_int, cols_int, idx+1)\n    autoCorr_np[:,idx] = np.correlate(sequence_np[:,idx], sequence_np[:,idx], mode='full')\n    plt.plot(lag_np, autoCorr_np[:,idx])\n    \n    if labels_strLst!=None:\n      plt.title(labels_strLst[idx])\n\nplotAutocorr(train_np, stockName_strLst)\nplotAutocorr(trainPP_np)\n\n\n","12872cd5":"# Vectorize the series\n\ndef vectorize(series_np, historyLength_int=historyLength_int):\n\n    # initialize\n    N,M = series_np.shape\n    seriesX_arr = []\n    seriesY_arr = []\n\n    for i in range(historyLength_int,N):\n      subSeriesX_np = series_np[i-historyLength_int:i,:]\n      subSeriesY_np = series_np[i,:]\n      seriesX_arr.append(subSeriesX_np)\n      seriesY_arr.append(subSeriesY_np)\n\n\n    seriesX_np = np.array(seriesX_arr)\n    seriesY_np = np.array(seriesY_arr)\n\n    return seriesX_np, seriesY_np\n\n\ntrainX_np, trainY_np = vectorize(trainPP_np)\nvalX_np, valY_np = vectorize(valPP_np)\n\nprint(trainY_np.shape)","8eb38528":"# randomize the order of the training set\n\nperm_idxLst = np.random.permutation(trainX_np.shape[0])\ntrainX_np = trainX_np[perm_idxLst]\ntrainY_np = trainY_np[perm_idxLst]\n","9fcd6fe4":"# Calculate a baseline forecast from the naive model \n\ntrainNaiveH_np = trainPP_np[historyLength_int:] * 0\nvalNaiveH_np = valPP_np[historyLength_int-1:-1] * 0\n\ntrainNaiveH_np = np.zeros(trainY_np.shape)\nvalNaiveH_np = np.zeros(valY_np.shape)\n\nM = valY_np.shape[1]\nrows_int = int(np.ceil(np.sqrt(M)))\ncols_int = rows_int\n\nplt.figure(figsize = figSize_tpl)\nplt.suptitle(\"Naive Model\")\nfor idx in np.arange(M):\n  plt.subplot(rows_int, cols_int, idx+1)\n  plt.plot(valNaiveH_np[:,idx])\n  plt.plot(valY_np[:,idx])\n  plt.legend(['Naive Forecast', 'Actual'])\n","3d2b8b83":"# build the model\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\n\n\n# Parameters:\nepochs_int = int(1e2)\nbatchSize_int = 2**3\nverbosity_int = 0\n\n\nN, M = trainX_np.shape[0:2]\nL = trainY_np.shape[1]\n\nprint(trainX_np.shape)\nprint(trainY_np.shape)\n\nearlyStopping_callback = EarlyStopping(\n    min_delta=1e-13, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = Sequential()\n\n\n# ------------\n# Define the sequence of layers\n# ------------\n\n# First LTSM\nmodel.add(LSTM(\n  units=historyLength_int,\n  return_sequences=True,\n  activation='tanh',\n))\n    \n# A dropout layer for overfitting\nmodel.add(Dropout(0.2))\n\n# The final LTSM\nmodel.add(LSTM(\n  units=historyLength_int,\n  activation='tanh',\n))\n\n## Another dropout layer to prevent overfitting\nmodel.add(Dropout(0.2))\n\n# A dense layer\nmodel.add(Dense(\n  units=20,\n  activation = \"relu\"\n))\n\n\n# The final layer\nmodel.add(Dense(\n    units=L,\n))\n\n# ------------\n# Finalize the model\n# ------------\n\nmodel.compile(\n    optimizer = 'adam',\n    loss = tf.keras.losses.Huber(),\n    metrics=[\"mae\", \"mse\"],\n)\n\nwith Timer(\"Training DNN\"):\n  history = model.fit(\n      trainX_np,\n      trainY_np,\n      epochs=epochs_int,\n      batch_size=batchSize_int,\n      verbose=verbosity_int,\n      validation_data=(\n          valX_np, \n          valY_np,\n      ),\n      callbacks = [earlyStopping_callback],\n  )\n\nprint(history.history.keys())\nloss = history.history['loss']\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\n","b3da3b41":"# Do a cursory review of the learning curve\n\n# Parameter\nzoomFactor_int = 5\n\nplt.figure(figsize = figSize_tpl)\nplt.plot(history.history['mae'][zoomFactor_int:])\nplt.plot(history.history['val_mae'][zoomFactor_int:])\n","a8c424c3":"# Perform train and validation forecasting\ntrainH_np = model.predict(trainX_np)\nvalH_np = model.predict(valX_np)\n","28cb285d":"# Display metrics\n\nfrom sklearn.metrics import r2_score\n\n# Make a function to call it twice\ndef displayMetrics(Y_np, H_np):\n    M = Y_np.shape[1]\n    error_np = Y_np - H_np\n    errorMean_np = np.average(error_np, axis=0)\n    errorSTD_np = np.std(error_np, axis=0)\n    mae_np = np.average(np.abs(error_np), axis=0)\n    r2_np = np.zeros(M)\n    for idx in np.arange(M):\n        h = H_np[:,idx]\n        y = Y_np[:,idx]\n        r2_np[idx] = r2_score(h, y)\n    \n    print(\" ME\\t STD\\t MAE\")\n    for idx in np.arange(M):\n        print(\"%7.4f\\t%7.4f\\t%7.4f\"%(errorMean_np[idx], errorSTD_np[idx], mae_np[idx]))\n    return mae_np\n    \nprint(\"\\n Validation Set\")\nvalMAE_np = displayMetrics(valY_np, valH_np)\nprint(\"\\n Naive Model\")\nvalNaiveMAE_np = displayMetrics(valY_np, valNaiveH_np)\n\n# Display the difference between the MAE of the naive forecaster and the LSTM\n# It should be positive\nprint(\"\\nNill MAE DIFF\")\nfor idx in range(valNaiveMAE_np.shape[0]):\n  temp = valNaiveMAE_np[idx] - valMAE_np[idx]\n  # Magnify the difference\n  temp*=1000\n  print(\"%7.5f\"%(temp))\n","a0ee3338":"# Plot the Error of the training and validation sets\n\n# Function since it will be called twice\ndef plotError(Y_np, H_np, title_str):\n    M = Y_np.shape[1]\n    error_np = Y_np - H_np\n    errorMean_np = np.average(error_np, axis=0)\n    errorSTD_np = np.std(error_np, axis=0)\n\n    rows_int = int(np.ceil(np.sqrt(M)))\n    cols_int = rows_int\n\n    plt.figure(figsize=figSize_tpl)\n    plt.suptitle(title_str)\n    for idx in np.arange(M):\n        plt.subplot(rows_int, cols_int, idx+1)\n        plt.plot(error_np[:,idx])\n\n        plt.axhline(errorMean_np[idx], color='gray', linestyle='-')\n        plt.axhline(errorMean_np[idx] - 1.96*errorSTD_np[idx], color='gray', linestyle='--')\n        plt.axhline(errorMean_np[idx] + 1.96*errorSTD_np[idx], color='gray', linestyle='--')\n        #plt.xlim(0, 1)\n        #plt.ylim(-3*np.max(errorSTD_np), 3*np.max(errorSTD_np))\n        plt.ylim(np.min(error_np), np.max(error_np))\n\n#plotError(trainY_np, trainH_np, \"Training Set Error\")\nplotError(valY_np, valNaiveH_np, \"Naive Model Error\")\nplotError(valY_np, valH_np, \"Validation Set Error\")\n","270c5147":"# Plot the scatterplot of Actual(Y) vs Forecasted(H) values\n\n# Function since it will be called twice\ndef plotScatter(Y_np, H_np, title_str):\n    M = Y_np.shape[1]\n    rows_int = int(np.ceil(np.sqrt(M)))\n    cols_int = rows_int\n\n    plt.figure(figsize=figSize_tpl)\n    plt.suptitle(title_str)\n    for idx in np.arange(M):\n        xrange = (np.min(H_np[:,idx]), np.max(H_np[:,idx]))\n        yrange = (np.min(Y_np[:,idx]), np.max(Y_np[:,idx]))\n        plt.subplot(rows_int, cols_int, idx+1)\n        plt.plot(H_np[:,idx],Y_np[:,idx],'.')\n        # Visual aid for the identity line\n        #plt.plot(yrange,yrange,'k--')\n        #plt.xlim(-1, 1)\n        #plt.ylim(-1, 1)\n        plt.xlabel('Forecasted')\n        plt.ylabel('Actual')\n\nplotScatter(trainY_np, trainH_np, \"Training Set Scatterplot\")\nplotScatter(valY_np, valH_np, \"Validation Set Scatterplot\")\nplotScatter(valY_np, valNaiveH_np, \"Naive Model Scatterplot\")\n","67aaf204":"# Plot the Bland-Altman plot of Actual(Y) vs Forecasted(H) values\n\n# Function since it will be called twice\ndef plotBA(Y_np, H_np, title_str):\n    M = Y_np.shape[1]\n    error_np = Y_np - H_np\n    miny_f = np.min(error_np)\n    maxy_f = np.max(error_np)\n    errorMean_np = np.average(error_np, axis=0)\n    errorSTD_np = np.std(error_np, axis=0)\n\n    rows_int = int(np.ceil(np.sqrt(M)))\n    cols_int = rows_int\n    \n    plt.figure(figsize=figSize_tpl)\n    plt.suptitle(title_str)\n    #plt.title(\"Modified Bland-Altman plot\")\n    for idx in np.arange(M):\n        plt.subplot(rows_int, cols_int, idx+1)\n        plt.plot(Y_np[:,idx], error_np[:,idx],'.')\n        plt.ylim(miny_f, maxy_f)\n        plt.xlabel('Actual')\n        plt.ylabel('Error')\n\n        # Draw lines with 95% confidence intervals and bias\n        plt.axhline(errorMean_np[idx] - 1.96*errorSTD_np[idx], color='gray', linestyle='--')\n        plt.axhline(errorMean_np[idx] + 1.96*errorSTD_np[idx], color='gray', linestyle='--')\n        plt.axhline(errorMean_np[idx] , color='gray', linestyle='-')\n    \n\n\n#plotBA(trainY_np, trainH_np, \"BA plot of the Training Set\")\nplotBA(valY_np, valH_np, \"BA plot of the Validation Set\")\nplotBA(valY_np, valNaiveH_np, \"BA plot of the Naive Model\")\n","8c0532a7":"def plotHY(H_np, Y_np, title_str):\n    \n    # Plot the reconstructed data set\n    rows_int = int(np.ceil(np.sqrt(H_np.shape[1])))\n    cols_int = rows_int\n    plt.figure(figsize=figSize_tpl)\n    plt.suptitle(title_str)\n    for idx in np.arange(H_np.shape[1]):\n        plt.subplot(rows_int, cols_int, idx+1)\n        plt.plot(H_np[:,idx])\n        plt.plot(Y_np[:,idx])\n        plt.legend([\"Forecasted\", \"Original\"])\n        # plt.title(stockName_strLst[idx])\n\n#plotHY(trainH_np, trainY_np, \"Train Results\")\nplotHY(valH_np, valY_np, \"Validation Results\")\nplotHY(valNaiveH_np, valY_np, \"Naive Model\")\n\n","82346a36":"# Reconstruct the stock closing prices from the forecasted data\n\ntrainR_np = preprocessor.inverse_transform(trainH_np)\nprint(trainR_np.shape)\n\nvalR_np = preprocessor.inverse_transform(valH_np)\nprint(valR_np.shape)\n\nvalNaiveR_np = preprocessor.inverse_transform(valNaiveH_np)\nprint(valNaiveR_np.shape)\n\ndef plotReconstructed(reconstructed_np, raw_np, title_str):\n    \n    # Plot the reconstructed data set\n    rows_int = int(np.ceil(np.sqrt(reconstructed_np.shape[1])))\n    cols_int = rows_int\n    plt.figure(figsize=(32, 16))\n    plt.suptitle(title_str)\n    for idx in np.arange(reconstructed_np.shape[1]):\n        plt.subplot(rows_int, cols_int, idx+1)\n        plt.plot(reconstructed_np[:,idx])\n        plt.plot(raw_np[historyLength_int:,idx]-raw_np[historyLength_int,idx])\n        plt.legend([\"Reconstructed\", \"Original\"])\n        plt.title(stockName_strLst[idx])\n\n# Plot the Train set\nplotReconstructed(trainR_np, train_np, \"Trainset Results\")\n\n#print(valR_np.shape)\n#print(val_np.shape)\n\n# Plot the Validation set\nplotReconstructed(valR_np, val_np, \"Validation Results\")\n\n# Plot the Naive model results\nplotReconstructed(valNaiveR_np, val_np, \"Naive Results\")\n","7e08d504":"From the training test results the overfitting at the training set is evident while the validation test presents a more realistic image of the method","5a8171aa":"## Discussion:\n\nIt appears that the model does not perform better than the naive forecaster. Looks like the LSTM model is unable to find an underlying structure in the sequence data and the outputs are stuck at 0 as the midpoint between the output range. \n\nAs a next step I'd like to do some feature engineering. The feature histograms seem quite clustered around the central value. Perhaps spreading more into the range of -1,1 would help the Neural Network calculations\n\nProspects for future improvement\n- ~Naive Forecast~\n- ~Shuffle datasets~\n- [Hyperparameter optimization](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html)\n- ~Inclusion of more tickers~\n- Stock clustering\n- Feature engineering (traded volume, daily returns, etc)\n- Sentiment analysis from Twitter\n- ~LSTM integration~\n- Handle as a data stream\n- Check out [this article](https:\/\/towardsdatascience.com\/lstm-time-series-forecasting-predicting-stock-prices-using-an-lstm-model-6223e9644a2f) for LSTM Stock forecasting\n- Check out  on feature scaling\n","51e64d68":"## Step 2: Preprocessing","42fddd0e":"## Note:\nIt is worth noting that even though this is only a next-day forecast, the model is trained only on the training set. This means that the further into the validation set we get, the more the model sees data it has never seen before, even though it would have been reasonable to train the model with the data up to this point.\n\nGiven this, I expected to the forecast quality to deteriorate on later forecasts. However this is not the case as we do not see an upward trend to the error","e0c4a2e9":"## Step 5: Model Evaluation","5c212534":"In this plot the points should follow the identity diagonal line","71356fd8":"### Modules\nThe following are pre-processing modules aiming to improve model performance. \nNot all of them are deemed useful in the final product","890dd468":"# Stock forecasting Notebook\n\n**Aim**: This notebook aims to perform next-day stock price forecasting\n\n**Methods**: The implemented approach will use the following steps:\n- Data acquisition\n  - Yahoo Finance API\n- Train-Test Split\n- Preprocessing\n  - Scale to zero mean and unit deviation\n  - PCA\n  - ICA\n  - Rescale to account for outliers\n- Vectorization (transform the time-series into matrix form)  \n- Regression\n  - Make use of LTSM model\n- Evaluation\n  - Compare  the results to the naive model","d6ed4f55":"## Step 4: LSTM Regression\n","505f9eb6":"## References\n\n1. [How and why to Standardize your data: A python tutorial](https:\/\/towardsdatascience.com\/how-and-why-to-standardize-your-data-996926c2c832) by \nSerafeim Loukas","d9800906":"## Step 1: Download the data"}}