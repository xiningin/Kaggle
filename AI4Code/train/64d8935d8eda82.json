{"cell_type":{"0840a461":"code","5833ca89":"code","4435c7f1":"code","b028e706":"code","505052e9":"code","0a93370e":"code","94d0ae2e":"code","f40f3e98":"code","24a53fae":"code","07cceba4":"code","9fa4028d":"code","9b73d52f":"code","1c2f1d37":"code","f884b6c9":"code","96970edf":"code","c3b44dd3":"code","fe396f0f":"code","703c94f9":"code","39e1726a":"code","9e98a2be":"code","786963c4":"code","9227601c":"code","310a3915":"code","089c93e5":"code","53703d0a":"code","4daf3a07":"code","85f73d02":"code","ff5d10cd":"code","969da79e":"code","0c475bd6":"code","0e6886e8":"code","f7c9f325":"code","10e0e9a9":"code","cad24320":"code","5d131ce2":"code","b2ecc44c":"code","2e4d1b25":"code","2aec4d8e":"code","265f69a4":"code","fad2254f":"markdown","b12550a1":"markdown","0899931b":"markdown","b016973f":"markdown","ed76fd10":"markdown","2a47d7dc":"markdown","6a93ad9b":"markdown","46e78a80":"markdown","71cc38e0":"markdown","89ee09f0":"markdown","a6be9822":"markdown","b759655b":"markdown","f6abab60":"markdown","dc2e4acb":"markdown","de4d6676":"markdown","1f61f9c4":"markdown","ae531222":"markdown","c66d7221":"markdown","1a6e4e37":"markdown","41c1482b":"markdown","8ecbd0f1":"markdown","2dea0e27":"markdown","f94e7a46":"markdown","44c8e52b":"markdown","6ab77bb6":"markdown","e5ff561d":"markdown","90eab675":"markdown","3ff097fa":"markdown","b736d55c":"markdown","1eb0cc86":"markdown","d032d46c":"markdown"},"source":{"0840a461":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport cufflinks as cf\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport seaborn as sns\nfrom math import pi\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","5833ca89":"df = pd.read_csv('\/kaggle\/input\/source-based-news-classification\/news_articles.csv')\ndf.head()","4435c7f1":"print(\"Number of rows and columns present in the dataset are: \", df.shape)","b028e706":"print(\"The number and categories of unique type of articles are: \", len(df['type'].unique()), df['type'].unique())","505052e9":"fig = px.pie(df,names='type',title='Types of Articles')\nfig.show()","0a93370e":"def msv(data, thresh = 20, color = 'black', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n    plt.axhline(y = thresh, color = 'r', linestyle = '-')\n    \n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    \n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+2.5, f'Columns with more than {thresh}% missing values', fontsize=10, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 0.5, f'Columns with less than {thresh}% missing values', fontsize=10, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    \n    return plt.show()\nmsv(df, 10, color=sns.color_palette('Oranges',10))","94d0ae2e":"df_orig = df.copy()\ndf.dropna(inplace = True)\nmsv(df, thresh = 2, color=sns.color_palette('Reds',15))","f40f3e98":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]\n\ntop_unigram = get_top_n_words(df['text_without_stopwords'], 20)\ntop_bigram = get_top_n_bigram(df['text_without_stopwords'], 20)\nwords = [i[0] for i in top_unigram]\ncount = [i[1] for i in top_unigram]\n\nplt.figure(figsize=(15,10))\nplt.bar(words, count,align='center')\nplt.xticks(rotation=90)\nplt.ylabel('Number of Occurences')\nplt.show()","24a53fae":"from wordcloud import WordCloud \nwc = WordCloud(background_color=\"white\", max_words=100,\n               max_font_size=256,\n               random_state=42, width=1000, height=1000)\nwc.generate(' '.join(df['text_without_stopwords']))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","07cceba4":"fig = px.pie(df,names='language',title='Languages of Articles')\nfig.show()","9fa4028d":"fig = px.bar(df, x='hasImage', y='label',title='Articles Including Images vs Label')\nfig.show()","9b73d52f":"from IPython.core.display import HTML\n\ndef convert(path):\n    return '<img src=\"'+ path + '\" width=\"80\">'\ndf_sources = df[['site_url','label','main_img_url', 'title_without_stopwords']]\ndf_r = df_sources.loc[df['label']== 'Real'].iloc[6 : 10,:]\ndf_f = df_sources.loc[df['label']== 'Fake'].head(6)","1c2f1d37":"HTML(df_r.to_html(escape = False, formatters = dict(main_img_url = convert)))","f884b6c9":"HTML(df_f.to_html(escape = False, formatters = dict(main_img_url = convert)))","96970edf":"fig = px.pie(df,names='label',title='Proportion of Real vs. Fake News',color_discrete_sequence=px.colors.sequential.Viridis_r)\nfig.show()","c3b44dd3":"print(f\"Sites printing Fake news are: {r_}{df[df['label'] == 'Fake']['site_url'].unique()}\")","fe396f0f":"print(f\"Sites printing Fake news are: {g_}{df[df['label'] == 'Real']['site_url'].unique()}\")","703c94f9":"real = set(df[df['label'] == 'Real']['site_url'].unique())\nfake = set(df[df['label'] == 'Fake']['site_url'].unique())\nprint(f\"Websites publishing both real & fake news are {m_}{real & fake}\")","39e1726a":"df[df['label'] == 'Fake']['site_url'].value_counts().tail(10)","9e98a2be":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder() ## Converting the type column from object datatype to numerical datatype\ndf['type'] = le.fit_transform(df['type'])\ndf.head()","786963c4":"le.classes_\nle.transform(['bias', 'bs', 'conspiracy', 'fake', 'hate', 'junksci', 'satire' , 'state'])\nmapping = {}\nfor i in le.classes_:\n    mapping[i] = le.transform([i])[0]\nprint(mapping)\n\nfig = px.sunburst(df, path=['label', 'type'])\nfig.show()","9227601c":"def sites_type(df):\n    types = df['type'].unique()\n    for type in types:\n        df_type = df[df['type'] == type]\n        type = le.inverse_transform([type])\n        print(f\"{r_}The unique sites publishing article of type {type[0]} are: {g_}{df_type['site_url'].unique()}\")\n        print()\n        \nsites_type(df)","310a3915":"df = df.sample(frac = 1)\ndf.head()","089c93e5":"urls = []\nfor url in df['site_url']:\n    urls.append(url.split('.')[0])\ndf['site_url'] = urls","53703d0a":"features = df[['site_url', 'text_without_stopwords']]\nfeatures['url_text'] = features[\"site_url\"].astype(str) + \" \" + features[\"text_without_stopwords\"]\nfeatures.drop(['site_url', 'text_without_stopwords'], axis = 1, inplace = True)\nfeatures.head()","4daf3a07":"X = features\ny = df['type']\ny = y.tolist()","85f73d02":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\ntfidf_vectorizer = TfidfVectorizer(use_idf = True, stop_words = 'english')\n\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train['url_text'])\nX_test_tfidf = tfidf_vectorizer.transform(X_test['url_text'])","ff5d10cd":"tfidf_train = pd.DataFrame(X_train_tfidf.A, columns = tfidf_vectorizer.get_feature_names())\ntfidf_train.head()","969da79e":"rfc = RandomForestClassifier(n_estimators=100,random_state=42)\nrfc.fit(tfidf_train, y_train)\ny_pred = rfc.predict(X_test_tfidf)\nRFscore = metrics.accuracy_score(y_test, y_pred)\nprint(\"The accuracy is : \", RFscore)","0c475bd6":"print(\"The Weighted F1 score is: \", metrics.f1_score(y_test, y_pred, average = 'weighted'))","0e6886e8":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ny_train = np.array(y_train)\ny_test = np.array(y_test)","f7c9f325":"vocab_size = 10000\noov_token = \"<OOV>\"\nembedding_dim = 32\nmax_length = 120\npadding = 'post' # \ntrunc_type = 'post'","10e0e9a9":"tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\ntokenizer.fit_on_texts(X_train['url_text'])\n# tokenizer.word_index # Mapping of words to numbers","cad24320":"training_sequences = tokenizer.texts_to_sequences(X_train['url_text'])\ntesting_sequences = tokenizer.texts_to_sequences(X_test['url_text']) # Converting the test data to sequences","5d131ce2":"train_padded = pad_sequences(training_sequences, maxlen = max_length, padding = 'post', truncating = trunc_type)\ntrain_padded.shape","b2ecc44c":"testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = 'post', truncating = trunc_type)\ntesting_padded.shape","2e4d1b25":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation = 'relu'),\n    tf.keras.layers.Dense(8, activation = 'softmax')\n])\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","2aec4d8e":"num_epochs = 15\nhistory = model.fit(train_padded, y_train, epochs = num_epochs, validation_data = (testing_padded, y_test))","265f69a4":"print(\"The Training Accuracy we get is: \", history.history['accuracy'][14])\nprint(\"The Testing Accuracy we get is: \", history.history['val_accuracy'][14])","fad2254f":"- We will set the maximum number of words allowed to be 10000.\n\n- In the test dataset, there can be words that are Out of Vocabulary(OOV), we will encode those words as OOV\n\n- Embedding Dimension has been set to 32\n\n- A lot of sentences might be very long, we will keep the maximum length to be 120.\n\n- You might be wondering that all the sentences are not neccessarry to be of the same length, in order to tackle that we will use the concept of padding i.e to add zeros before or after the sentence to keep the length uniform.","b12550a1":"### Language","0899931b":"Thus, we see that most of the fake news is being given by the **21stcenturywire.com**.","b016973f":"There are **1533 sentences** and each sentence has **length of 120 words**.","ed76fd10":"Thus, we have **removed all Null Values**.","2a47d7dc":"**LabelEncoder has labeled bias as 0, bs as 1, and conspiracy as 2 etc.**","6a93ad9b":"**Let's combine both of them together to form a new column, url_text.**","46e78a80":"**These are the 8 different types of articles:**\n- bias\n- conspiracy\n- fake\n- bs (i.e. bullshit)\n- satire\n- hate\n- junksci(i.e. junk science)\n- state","71cc38e0":"Lets try to boost this using a different approach called **Embedding**.","89ee09f0":"**Once we get the dictionary of word indexes, we need to convert the whole sentence into numerical representation, for that we use 'texts_to_sequences'**","a6be9822":"### Null values\n\nLet's now check for Null Values in this dataset","b759655b":"**Let's check which are the sites deliver fake news**","f6abab60":"Above is the representation of the **tf-idf matrix**. The first represents the **'first url_text'** and corresponding column values represent the value of that column for 1st document. One point to note here is the presence of a very large number of zeros. We will be dealing with that in the next section","dc2e4acb":"There are less than **2.5% values** which are missing in the columns 'text_without_stopwords' and 'text' and close to 0.5% missing values in columns like 'title_without_stop_words', 'language' etc. We will be dropping these null values, since they wont have much effect value on our model.","de4d6676":"**Using HTML to view the images given in form of image URL**","1f61f9c4":"**TF-IDF stands for \u201cTerm Frequency \u2014 Inverse Document Frequency\u201d. This is a technique to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word in the document and corpus. This method is a widely used technique in Information Retrieval and Text Mining.**\n\nTF-IDF for a word in a document is calculated by multiplying two different metrics:\n\n- The term frequency of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n\n- The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n\nSo, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1.\n\n**The importance increases proportionally to the number of times a word appears in the document but is balanced by the frequency of the word in the corpus.**\n\nTypically, the tf-idf weight is composed of two terms: Term Frequency (TF), aka. The number of times a word appears in a document, divided by the total number of words in that document and **Inverse Document Frequency (IDF)**, computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.","ae531222":"## EDA | Modelling | Embedding\n![](https:\/\/beconnected.esafety.gov.au\/pluginfile.php\/52815\/mod_resource\/content\/12\/fake-news-hero-img.jpg)\n<br>\nWe come across different type of news, out of which some can be bullshit, some news can be fake, some news are shown to express hate, and some news are published to make fun of others. Given the news content, we as humans are able to classify that article into different categories but can computers do it?\n\nWe first explore the data in depth and then draw certain conclusions from that. Then we will perform some text preprocessing using TF-IDF, once the data is preprocessed we apply Random Forest for modelling. During this, we will reliase the drawback of TF-IDF, which will take us to using Embeddings using TensorFlow.\n\n**Do upvote if you liked it!**","c66d7221":"## Visualization and EDA","1a6e4e37":"## Embedding\n\nBy applying the tf-idf method we used above, we observed that we get a lot of zeros for sentence representation, i.e we got a sparse matrix. Sparse Matrix is not a true represen****tation for the corpus, and it doesn't take into account the similarity of the words. That is where Embeddings come to our rescue.\n\nA word **embedding is a class of approaches for representing words and documents using a dense vector representation.**\n\nIt is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.","41c1482b":"### Articles Including Images vs. Label","8ecbd0f1":"## TF-IDF","2dea0e27":"**Let's remove '.com' from the main urls.**","f94e7a46":"### Real vs. Fake News","44c8e52b":"There are **512 sentences** and each sentence has **length of 120 words**.","6ab77bb6":"## Modelling\n\n**In the dataset, all the values are ordered. Therefore, we need to reshuffle these values.**","e5ff561d":"Websites that publish fake news is fine, but it might be possible that there are sites where only 1 or 2 news were fake, let's take these into consideration as well.","90eab675":"**Hope you liked the notebook, any suggestions would be highly appreciated.**\n\n**Please upvote if you liked it!**","3ff097fa":"p**ad_sequences** is used to ensure that all sequences in a list have the same length. **By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.**","b736d55c":"## Websites and the Types of Stories They Publish","1eb0cc86":"**Let's also check if there exists websites that publish both real and fake news**","d032d46c":"**Tokenizer updates internal vocabulary based on the given list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2**"}}