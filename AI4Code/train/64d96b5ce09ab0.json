{"cell_type":{"2d3148a8":"code","dba99841":"code","a0888d5e":"code","fb85d0ce":"code","455d3f4c":"code","32ed3b3b":"code","f012538e":"code","9be871f5":"code","d8909b6d":"code","6fbc5d22":"code","03577a35":"code","87913540":"code","c551640b":"code","641a6b39":"code","aa29daa4":"code","25bb8168":"code","fac7162e":"markdown"},"source":{"2d3148a8":"#Tensorflow and Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Import of keras model and hidden layers for our convolutional network\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Flatten, Dropout\n\n#Image handling libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport pandas as pd\n\n#Sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n#Initialize a list of paths for images\nimagepaths = []\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n        #print(os.path.join(dirname, filename))\n        if path.endswith(\"png\"):\n            imagepaths.append(path)\n\nprint(len(imagepaths))\n#print(imagepaths)","dba99841":"#Defining a function that plots the image selected from a path\n\ndef img_plot(img_path):\n    img = cv2.imread(img_path)\n    #convert to RGB space\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    #check the shape of the image\n    print(\"Shape of the image is \", img_rgb.shape)\n    #Display the image\n    plt.grid(False)\n    plt.imshow(img_rgb)\n    plt.xlabel(\"Width\")\n    plt.ylabel(\"Height\")\n    plt.title(\"Image \" + img_path)","a0888d5e":"#Example image plot\n#Plotting the first image from the dataset\nimg_plot(imagepaths[0])","fb85d0ce":"#### Creating Training Set and Labels ####\n# X for image data\nX = []\n# y for the labels\ny = []\n\n#Load the images into X by doing the necessary conversions and resizing of images\n#Resizing is done to reduce the size of image to increase the speed of training\nfor path in imagepaths[:19999]:\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.resize(img, (128,128))\n    X.append(img)  \n    #Getting the labels from the image path\n    category = path.split(\"\/\")[7]\n    #print(category)\n    label = int(category.split(\"_\")[0][1])\n    #print(label)\n    y.append(label)\n\n#print(label)\n#Turning X & y into numpy arrays\nX = np.array(X)\nX = X.reshape(len(imagepaths[:19999]), 128, 128, 1)\ny = np.array(y)\n\nprint(\"Images loaded: \", len(X))\nprint(\"Labels loaded: \", len(y))\n\nprint(y[0], imagepaths[0]) #To debug","455d3f4c":"# Make the test train split\nthreshold = 0.3\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = threshold, random_state = 42)","32ed3b3b":"# Create a CNN Sequential Model\nmodel = Sequential()\nmodel.add(Conv2D(32, (5,5), activation = 'relu', input_shape=(128,128,1)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu')) \nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu')) \nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu')) \nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))","f012538e":"#Model configuration for training purpose\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","9be871f5":"model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=2, \n         validation_data=(X_test, y_test))","d8909b6d":"model.save('handgesturerecog_model.h5')","6fbc5d22":"#calculate loss and accuracy on test data\n\ntLoss, tAccuracy = model.evaluate(X_test, y_test)\n\nprint('Test accuracy: {:2.2f}%'.format(tAccuracy*100))","03577a35":"# Making predictions on test data\nprediction = model.predict(X_test)","87913540":"#Lets compare the predicted value with actual label value\n# Ideally both prediction[0] and y_test[0] should be same\nnp.argmax(prediction[0]), y_test[0]","c551640b":"# Function to plot images and labels for validation purposes\ndef validate_gestures(predictions_array, true_label_array, img_array):\n  # Array for pretty printing and then figure size\n  class_names = [\"down\", \"palm\", \"l\", \"fist\", \"fist_moved\", \"thumb\", \"index\", \"ok\", \"palm_moved\", \"c\"] \n  plt.figure(figsize=(15,5))\n  \n  for i in range(1, 10):\n    # Just assigning variables\n    prediction = predictions_array[i]\n    true_label = true_label_array[i]\n    img = img_array[i]\n    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    \n    # Plot in a good way\n    plt.subplot(3,3,i)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(img, cmap=plt.cm.binary)\n\n    predicted_label = np.argmax(prediction) # Get index of the predicted label from prediction\n    \n    # Change color of title based on good prediction or not\n    if predicted_label == true_label:\n      color = 'blue'\n    else:\n      color = 'red'\n\n    plt.xlabel(\"Predicted: {} {:2.0f}% (Actual: {})\".format(class_names[predicted_label],\n                                  100*np.max(prediction),\n                                  class_names[true_label]),\n                                  color=color)\n  plt.show()","641a6b39":"# Plot testing based on predictions and their actual values\nvalidate_gestures(prediction, y_test, X_test)","aa29daa4":"#Transform predictions into 1D array \ny_pred = np.argmax(prediction, axis=1)","25bb8168":"#Create a Confusion Matrix for Evaluation\n# H = Horizontal\n# V = Vertical\npd.DataFrame(confusion_matrix(y_test, y_pred), \n             columns=[\"Predicted Thumb Down\", \"Predicted Palm (H)\", \"Predicted L\", \"Predicted Fist (H)\", \"Predicted Fist (V)\", \"Predicted Thumbs up\", \"Predicted Index\", \"Predicted OK\", \"Predicted Palm (V)\", \"Predicted C\"],\n             index=[\"Actual Thumb Down\", \"Actual Palm (H)\", \"Actual L\", \"Actual Fist (H)\", \"Actual Fist (V)\", \"Actual Thumbs up\", \"Actual Index\", \"Actual OK\", \"Actual Palm (V)\", \"Actual C\"])","fac7162e":"**TESTING THE MODEL**"}}