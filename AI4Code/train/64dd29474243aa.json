{"cell_type":{"ffaf307e":"code","de61f4da":"code","3b71416f":"code","34a1e5bf":"code","2233aec3":"code","4e4ddd4c":"code","c22c06d4":"code","860e37a3":"code","cb66398b":"code","48206a24":"code","b7821c7b":"code","23b3a7b2":"code","94d7043e":"code","786af81f":"code","083dfd4f":"code","d45e550b":"code","9a4c9981":"code","17eb5900":"code","1cd29a5b":"code","8ca72547":"code","341485bb":"code","fb218805":"code","b9343bfe":"code","5b6106e3":"code","f1767d38":"code","28de2e60":"code","83adc79e":"code","fe25aaa2":"code","d22b07d2":"markdown","1fa99139":"markdown","782b268e":"markdown","9fecdfa5":"markdown","93aa2079":"markdown","ecc06a6a":"markdown","76162f5c":"markdown","3a46b340":"markdown","105d0485":"markdown","11bfe31c":"markdown","15ae9b4e":"markdown","7de59679":"markdown","b85c406c":"markdown","c18d7d16":"markdown","0459b891":"markdown","b68b6f26":"markdown","9769534a":"markdown","d5f5f7af":"markdown","896d1a3c":"markdown","9380d40a":"markdown","4a384831":"markdown","64fe3118":"markdown","0d9f8759":"markdown","164d31cd":"markdown","5a808155":"markdown","a09cd672":"markdown","cac41ac9":"markdown","435c1cf2":"markdown"},"source":{"ffaf307e":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","de61f4da":"from fastai import *\nfrom fastai.vision import *","3b71416f":"bs = 64\n# bs = 16   # uncomment this line if you run out of memory even after clicking Kernel->Restart","34a1e5bf":"path = untar_data(URLs.PETS); path","2233aec3":"path_img = path\/'images'\nfnames = get_image_files(path_img)\nfnames[:5]","4e4ddd4c":"fnames = sorted(fname for fname in fnames if fname.name[0].lower() in 'abc')\nlen(fnames)","c22c06d4":"np.random.seed(2)\npat = re.compile(r'\/([^\/]+)_\\d+.jpg$')\ndata = ImageDataBunch.from_name_re(path_img, fnames, pat,\n                                   ds_tfms=get_transforms(),\n                                   size=224, bs=bs, num_workers=0\n                                  ).normalize(imagenet_stats)\ndata","860e37a3":"data.show_batch(rows=3, figsize=(7,6))","cb66398b":"print(data.classes)\nlen(data.classes),data.c","48206a24":"def badLearner() :    \n    model = nn.Sequential(\n        Flatten(),\n        nn.Linear(224*224*3,data.c)\n    )\n    return Learner(data,model,metrics=accuracy)","b7821c7b":"def randomSeedForTraining(seed) :\n    \"This is to make the training demos below repeatable.\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n\nrandomSeedForTraining(3141);\nlearn = badLearner()","23b3a7b2":"print(learn.summary())","94d7043e":"learn.fit(3,1.0)","786af81f":"randomSeedForTraining(314);\nlearn = badLearner()\nlearn.lr_find(); learn.recorder.plot()","083dfd4f":"learn.fit(5,3e-5)","d45e550b":"randomSeedForTraining(3141);\nlearn = create_cnn(data, models.resnet34, metrics=accuracy, pretrained=False)","9a4c9981":"learn.lr_find(); learn.recorder.plot()","17eb5900":"learn.fit(8,5e-3)","1cd29a5b":"randomSeedForTraining(3141);\nlearn = create_cnn(data, models.resnet34, metrics=accuracy)","8ca72547":"print(learn.summary())","341485bb":"learn.fit_one_cycle(3,5e-3)","fb218805":"learn.save('stage-1')","b9343bfe":"learn.unfreeze()\nlearn.fit_one_cycle(1,5e-3)","5b6106e3":"learn.load('stage-1')  # forget about the training above and revert to stage 1\nlearn.unfreeze()\nlearn.lr_find(); learn.recorder.plot()","f1767d38":"learn.fit_one_cycle(2,slice(1e-5,5e-4))","28de2e60":"randomSeedForTraining(3141);\nlearn = create_cnn(data, models.resnet34, metrics=accuracy, wd=0.1)\nlearn.fit_one_cycle(3,5e-3)","83adc79e":"randomSeedForTraining(3141);\nlearn = create_cnn(data, models.resnet34, metrics=accuracy, ps=[0.1,0.2])\nlearn.fit_one_cycle(3,5e-3)","fe25aaa2":"data = ImageDataBunch.from_name_re(path_img, fnames, pat,\n                                   ds_tfms=get_transforms(flip_vert=True,max_warp=0.1),\n                                   size=224, bs=bs, num_workers=0\n                                  ).normalize(imagenet_stats)","d22b07d2":"**This notebook revisits the first lesson in the Fastai 2019 Part 1 course.** In this lesson, the Fastai team showed how we could create a remarkably good image classifier for pet breeds using only a few lines of code. While this was really cool, I thought it would be interesting to revisit the lesson and see some of the things that can go wrong when you're developing a model, rather than having everything work right away! Perhaps, as you start developing your own models, this will help you recognize some of the things that happen ...\n\nLesson goal - a quick review of the overall Fastai process for developing a model, including:\n- recognizing underfitting and overfitting\n- fastai innovations including `lr_find` and `fit_one_cycle`\n- transfer learning and fine tuning\n- how to adjust different types of regularization\n","1fa99139":"## A bad model","782b268e":"Let's look at what's going on here - how was this model created?","9fecdfa5":"Since we're underfitting, let's try using a more powerful model! We'll use a powerful neural network model called Resnet34, but unlike in the original lesson we'll start from scratch (no pretraining).","93aa2079":"Let's define a simple linear model on all the input pixel values. This won't do well, since pet breed recognition isn't a linear function!","ecc06a6a":"And now we suddenly have pretty good performance!\nLet's save the model at this stage so we can revert to it below after we screw things up ...","76162f5c":"## Step 3 - Transfer Learning  ","3a46b340":"In transfer learning, we use a powerful model trained on a task that's related to what we want,\nbut for which there's a lot more data available, so we can train the model well.\nWe then modify the pretrained model a little to adapt it to our task,\nand train the modified part to do our task -\nwe can avoid overfitting in this stage because the part we're training is relatively small.\nIn a third step, we may fine-tune the whole model, but here we'll again be in danger of overfitting.\nFastai has a clever way to do this fine-tuning that usually improves performance a little.\nIn fact, Fastai is generally set up to do transfer learning very conveniently and with good performance.\n\nA classic use of transfer learning is in image classification, so let's go through that in some detail\n(for a use of transfer learning in another domain, see the ULMFit algorithm in lesson 3).\nWe'll again use the powerful NN model Resnet34, but this time we won't add `pretrained=False`,\nso we'll get a version of the model which has been pretrained on the Imagenet dataset\n(a large training set of images containing different objects that's used in ML competitions).","105d0485":"## Step 1: Underfitting","11bfe31c":"### Data Augmentation\n\nThis means we modify our training data to generate new examples,\nhopefully ones that are \"realistic\".\nIf done well, this is almost like getting more training data for free.\nAnd, as we said above the best way to prevent overfitting is to get more data!\n\nFastai has a good built-in system for doing this for images,\nand the defaults are set up to work well for the common case\nof photos taken from handheld cameras.\nIf you do want to get the best possible performance,\nit's worth understanding how this works in some detail,\nand how you might want to change things for different image types.\n\nThe basic ML training step is to take a mini-batch\n(a random selection of `batchsize` examples) from the training set,\ncalculate the loss function and gradient,\nand update weights using the gradient.\nWhen we do this for images, fastai randomly selects and applies a group of transforms\nto each image before feeding it into the mini-batch,\nas specified by the `tfms=` argument when the dataset is created.\nYou'll generally use `get_transforms()` to create this argument, and this default\nwill work well for common photos.\nHowever, if you want to get the best possible performance it may be worth looking\nat the documentation for `get_transforms`\n\nhttps:\/\/docs.fast.ai\/vision.transform.html#get_transforms\n\nand carefully considering what transforms to use - specifically, how realistic\nis each transform for the type of images you'll be predicting?\nSome examples:\n- `do_flip`, `flip_vert` say whether to apply random horizontal and vertical flips. For \"regular\" photos, you probably want horizontal but not vertical flips (the default). For satellite images, both. For chest X-rays, neither!\n- `max_warp` applies a perspective warp that's similar to taking a photo from a higher or lower angle. Regular photos - yes; Satellite images - no.\n- `max_lighting` randomly changes lighting and contrast. To predict photos taken by cameraphone you probably want a fair amount of this; for photos taken in a studio, not as much.\n\nExample of using different arguments to `get_transforms`:","15ae9b4e":"What we see now is that the training loss does go down for a while and then approaches a minimum, but that minimum is still bad! This situation (not even doing well on your training set) is called **underfitting**. You can also start to look at the other two columns, which give you stats on the validation set after each epoch - these are examples that your model wasn't trained on - and see the performance on these is bad as well.\n\nOne explanation for underfitting is that your model isn't powerful enough, but there are a couple of other possibilities. Why else might you get something that looks like underfitting?\n\n- Maybe what you're trying to predict isn't that predictable! If you tried to predict the results of a coin flip based on the coin flipper's age, weight, and so forth, you'd probably wind up with something that looks like underfitting.\n- Maybe you have a bug in constructing the dataset! Suppose you randomly mixed up the labels before applying them to the training data. The result would likely look like underfitting. Or, suppose you have a bug in your image preprocessing code and accidentally clip out much of the data relevant for prediction. This is why we look at the data! See the code above that calls `show_batch`.","7de59679":"## Step 2: Overfitting","b85c406c":"So, now that we have a good learning rate for our bad model, let's train it!","c18d7d16":"One confusing thing - you may see something to the effect that linear training provably produces an optimal result,\nwhile optimally training a neural network is intractable (NP-complete) even for very shallow architectures.\nWhat's going on here?\n\n\"A horse than can count to ten is a remarkable horse, not a remarkable mathematician.\" - Samuel Johnson?\n\nOur model is a remarkable linear model (close to the best possible), but not a remarkable predictor!\nOur function isn't close to linear, so even the optimal (best possible) linear model will be a lousy predictor.\nOn the other hand, when we train a neural network, while we may not get an optimal result for our neural network architecture,\nwe will in practice do much better than the linear model.","0459b891":"## Regularization\n\nRegularization isn't a specific method, but a catch-all term for a group of methods.\nIt refers to a bunch of different techniques that **prevent or reduce overfitting**.\nFastai makes heavy use of several of these by default,\napplied unobtrusively under the hood.\n\nWe'll quickly review a grab-bag of these regularization methods,\ncommenting in particular on how and when you may want to try changing the defaults to help performance.","b68b6f26":"Now we see a different pattern. The loss values for the training set (the leftmost column) go down steadily, but those for the validation set don't seem to progress over a considerable number of epochs, or may even get worse. This indicates **overfitting**. Intuitively, overfitting can happen when our model makes its classification function too complicated, for example by using some irrelevant feature that randomly happens to be associated with a category in our training set. This is likely to occur when we use an architecture that has lots of capacity for complication, and we don't have enough training data to make those incorrect associations unlikely.\n\nUnfortunately, many ML problems have complicated classification functions that probably require a model with lots of capacity for complication in order to do well, and in many cases training data is expensive and hard to get so you don't have that much to work with! So for many realistic problems we may face overfitting when we try to use powerful models.\n\nThe \"traditional\" ML response is to try simpler models. Fastai disagrees with this in an interesting way - they say to use more powerful models, and then fix the overfitting problem. In fact, they say that **you should always try to get to the point of overfitting** - in other words, if you don't get to the point of overfitting you're probably leaving something on the table! This strikes me as similar to business negotiators who believe that if you don't get walked out on in disgust at some point, you haven't negotiated hard enough.\n\nSo, how do we fix overfitting when it happens? The most reliable way to fix it is to get more data! But, as we just said, that may not be possible. So, the rest of this revisited lesson basically talks about what else you can do - in other words, how to use powerful models when you don't have enough data to train them from scratch. In a nutshell there are two things that can help a lot - **transfer learning** and **regularization** - and Fastai makes heavy use of both.","9769534a":"### Dropout\n\nDropout is a slightly more mysterious form of regularization - it works by, at each minibatch,\n\"dropping\" (zeroing out) a random selection of the activations, as specified by a probability p.\nThis has been found to reduce overfitting; intuitively, it may prevent the model from\ndepending on any particular activation for too much of its prediction.\nThe higher the probability, the stronger the regularizing effect,\nso again if you see overfitting you may want to try a higher dropout probability,\nand if you think your model may be underfitting you may want to try a lower one.\n\nIn the example above, dropout is used on the two new Linear layers (see the model summary above).\nYou can set the dropout probability for these layers using the `ps=` argument to `create_cnn\/cnn_learner` - if this is a float, the final layer will have that probability and\nthe previous layers will have `ps\/2`; you can also use a list to set the probability directly\nfor each of the new layers.\n\nThe default value for `ps` is 0.5. Example of using a lower value (less regularization):","d5f5f7af":"Looks like we got worse! And this isn't too surprising, because we're now training a much larger number of weights (since we unfroze the pretrained weights). So we can't usually improve performance if we just train the whole network at the same rate as before. But, it turns out we can often improve performance a bit by training the whole network \"just a little\" - i.e. fine-tuning it. Fastai provides a clever way of doing this called discriminative learning rates. Instead of a single learning rate, we pass two learning rates, the first smaller than the second (we choose these learning rates as described in lesson 1 - the first is usually 1\/10 of the learning rate used in the first training stage above, while the second is chosen by running `lr_find` again). Fastai then uses the smaller learning rate on the shallower layers of the network, and the larger rate on the deeper layers - this seems to work best in practice. Let's try this.","896d1a3c":"## Setting up the data","9380d40a":"# Lesson 1 Revisited - Fastai review","4a384831":"This concludes our quick review of transfer learning and fine-tuning, which is the first way Fastai lets us use more powerful models on relatively small training sets. But, there's more going on under the hood, and some of it may help you further improve performance in many cases. So, let's move on to the second bag of tricks - **regularization**.","64fe3118":"Sometimes you'll see values that stay ridiculously high for the training loss column (this shows the average value of the loss function on the training set at each epoch). In extreme cases, you may see the loss values \"blow up\" or even wind up with mysterious printouts like `NaN`, which means the values can't even be represented anymore.\n\nThis shouldn't happen! Mathematically, the training process essentially consists of repeatedly changing your model's weights as follows:  \n`weights -= learning_rate*gradient`  \nwhere the gradient is a vector the same size as the weights that says which direction the loss function goes as we slightly change each of the weights. So, as long as your learning rate is small enough, the loss on your training set should go down pretty consistently until you get close to a minimum value. (One exception is that when using `fit_one_cycle` you may see the training loss go up for a while at the beginning of the training - this may actually help training by exploring the weight space more thoroughly).\n\nIf you do see ridiculously big values or a blow up, the most likely reason is that your learning rate is too high, but you also might have bugs in how your model is coded.\n\nLuckily, Fastai provides the `lr_find()` function to help you pick a good learning rate!","0d9f8759":"In the interest of time I'll skip the part of Lesson 1 where we look at\nthe results in detail using `ClassificationInterpretation`,\nbut you should definitely check that out if you haven't yet!\n\nLet's look at the fine-tuning process. What we want to do is tune the rest of the model in addition\nto the part we added. What happens if we do this in a simple way?","164d31cd":"### BatchNorm\n\nThis one is a bit mysterious - a combination of regularization and training helper,\nand it's not clear why it has a regularizing effect.\nTo apply it, we take for each activation the mean and variance over each mini-batch,\nand do a modified form of normalization.\nWe then do a linear rescaling using two **trainable** weights for each activation.\nThis may actually be the more important step, since (intuitively) it makes it\neasier for the training to rescale activations into a better range -\notherwise this could require modifying many shallower weights in a complicated way.\n\nWe won't talk about BatchNorm much more because:\n- There's not much call for varying it - you almost always want to use it, because it almost always improves training! If you look at summaries of practically any model in Fastai, you'll see that it's generally invoked after a layer's activation function (you'll see ReLU, then BatchNorm).\n- It's not really clear how it works (the original paper said it \"reduces covariate shift\", but that turned out to be wrong).","5a808155":"## Step 0: Loss values ridiculously big or blow up","a09cd672":"The part of the model that was newly created for our problem is the last few lines. The main items we added are the two Linear layers:\n```\nLinear               [64, 512]            524800     True\n```\nand\n```\nLinear               [64, 11]             5643       True\n```\nthat go from the activations in the pretrained network to\na new hidden layer of 512 nodes,\nand then to an output layer of 11 nodes (we can tell how big we need to make\nthis output layer from the dataset we passed to `create_cnn` - in this case\nwe need 11 outputs since we're\ndoing classification into 11 categories).\n\nThese new parts are the ones that we'll now train -\nthe rest of the network will be frozen to the pretrained weights from training on Imagenet.\nNote that `create_cnn` allows for a considerable amount of customization\nin how to modify the pretrained network; for example, you can specify a different set of linear hidden layers using `lin_ftrs=`,\nor even specify your own model for the added part using `custom_head=`. For details, see the documentation!\n\nNote: `create_cnn` has been changed to `cnn_learner` in later versions of Fastai.\n\nNote: you can easily see the documentation for any function by running `doc(fname)` -\nthis will pop up a summary, and give you links to the full documentation and to the source code\n(which can also be instructive)!\n\nLet's try training with transfer learning.\nWe'll also start using `fit_one_cycle`, which is a modified form of training that changes the learning rate over time,\ngradually increasing it during the first part of training and reducing it toward the end.\nThis has been found to generally improve the final results.","cac41ac9":"### Weight Decay\n\nWeight decay (AKA L2 regularization) is a more understandable form of regularization,\nand also one that could be worth experimenting with.\nIt works by simply adding to the loss function a factor `wd` * the sum-of-squares of all your model's weights.\nEffectively, this penalizes your model for having weights significantly different from zero -\nthe higher the value of wd, the greater the penalty, and the more the regularizing effect.\n\nSo, if you see overfitting you may want to try a higher value of wd,\nand if you think your model may be underfitting you can try a lower value.\nFastai makes this easy - functions that construct a learner have an optional `wd=` argument.\nThe default value for wd is 0.01, which is quite low -\nit's usually worth trying some higher values (0.05, 0.1, 0.2). For example:","435c1cf2":"We'll again use the [Oxford-IIIT Pet Dataset](http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/pets\/) by [O. M. Parkhi et al., 2012](http:\/\/www.robots.ox.ac.uk\/~vgg\/publications\/2012\/parkhi12a\/parkhi12a.pdf) which features 12 cat breeds and 25 dogs breeds. However, we'll cut the dataset down to 11 breeds so we can run through the notebook faster."}}