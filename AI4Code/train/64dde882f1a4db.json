{"cell_type":{"ddd86989":"code","1f17ab37":"code","c65f419d":"code","15c99dd7":"code","c117fdc3":"code","4e93731f":"code","86871f73":"code","7911b2d6":"code","88dc3e85":"code","ff610652":"code","b8f4f673":"code","a841f323":"code","296542b1":"code","3f80ced8":"code","8bf85659":"code","5a244a5f":"code","ecd3b04b":"code","18d6ddb5":"code","32aa3416":"code","aa666d74":"code","eece4008":"code","bce63cd8":"code","a2ca7ba5":"code","cf70a5b5":"code","9a46b655":"code","942f3bde":"code","d8e2369c":"code","a5565f72":"code","a382e38d":"code","d34f1fe1":"code","d33574d0":"code","dd43dde7":"code","446365bc":"code","877397d1":"code","c64c1af8":"code","3afa8b1a":"code","05af5c99":"code","4fb768ce":"code","74ab2399":"code","d22750f2":"code","e1e5b309":"code","b68fb3b1":"code","56a53db8":"code","19f5f6ea":"code","3d52809b":"code","1010e598":"code","49932cc7":"code","9bc2e727":"code","38ae91c2":"code","2c686a66":"code","81291adc":"code","ff965797":"code","93f7b687":"code","ff853afb":"code","5a84fbf8":"code","a624c7cb":"code","19aa257a":"code","d58ca71e":"code","6afa63c0":"code","e100a994":"code","ac851016":"code","6fbc99a5":"code","b878cca0":"code","12c69d2c":"code","63eb8971":"code","3e08e49b":"code","696fc62b":"code","58dfe6f5":"code","fdca344f":"markdown","c3c627d7":"markdown","1501f180":"markdown","46656c7f":"markdown","51a5cfde":"markdown","865e849c":"markdown","fd5998ad":"markdown","dd2ad887":"markdown","b415a19f":"markdown","876eda09":"markdown","d35d0248":"markdown","6b220733":"markdown","3ee87758":"markdown","0634b889":"markdown","80058219":"markdown","76484a58":"markdown","289a0602":"markdown","5006ca16":"markdown","489da32d":"markdown","5a449ac3":"markdown","f74964ca":"markdown","0749e541":"markdown","f48ebe6f":"markdown","60fdb965":"markdown","e39c17d5":"markdown","31591f29":"markdown","e4b96f6f":"markdown","9b2e304b":"markdown","83d5c5bc":"markdown","8659b7bf":"markdown","070a8c03":"markdown","7cca6a5e":"markdown","89bc261f":"markdown","abdc9198":"markdown","19f29113":"markdown","82f62c50":"markdown","33ee0710":"markdown","bf4ef383":"markdown","0b8173dd":"markdown","397293cb":"markdown","23525c89":"markdown","087d4ce1":"markdown","8fe82a5b":"markdown","59555bfb":"markdown","8a54ba33":"markdown","b64a8887":"markdown","ea0f7536":"markdown","3d9b26c3":"markdown","25736d97":"markdown","0f12eba2":"markdown"},"source":{"ddd86989":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1f17ab37":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score","c65f419d":"train=pd.read_csv(\"..\/input\/titanic\/train.csv\" )\ntit1=train.select_dtypes(include=['float64','int64','object'])\ntrain.info()\n\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntit2=test.select_dtypes(include=['float64','int64','object'])\ntest.info()","15c99dd7":"print(\"train shape:\",train.shape)\nprint(\"test shape :\",test.shape)","c117fdc3":"tit1.head()","4e93731f":"tit2.head()","86871f73":"tit2['survived']=np.nan\ntit2.head()","7911b2d6":"plt.figure(figsize=(4,4))\nplt.title('SURVIVED',size=20)\ntit1.Survived.value_counts().plot.bar(color=['red','green'])\n\nplt.figure(figsize=(4,4))\nplt.title('SEX',size=20)\ntit1.Sex.value_counts().plot.bar(color=['skyblue','pink'])\n\n","88dc3e85":"percent=round(np.mean(train['Survived']),3)*100\nprint(\"Percentage of Survivors:\",percent)","ff610652":"total=train['Survived'].sum()\ntotal\nmen=train[train['Sex']=='male']\nwomen=train[train['Sex']=='female']\nm=men['Sex'].count()\nw=women['Sex'].count()\nprint(\"male:\",m)\nprint(\"female:\",w)\nprint(\"percentage of women:\",round(w\/(m+w)*100))\nprint(\"percentage of men:\",round(m\/(m+w)*100))","b8f4f673":"train.isnull().sum()","a841f323":"train['Cabin'] = train['Cabin'].fillna('X')\ntest['Cabin']=test['Cabin'].fillna('X')","296542b1":"train['Age'].hist(bins=40,color='salmon')\nplt.title(\"AGE\",size=20)\n","3f80ced8":"plt.figure(figsize=(5,5))\nplt.title(\"CLASS DIVISION\",size=20)\ntit1.Pclass.value_counts().plot.bar(color=['olive','coral','gold'])","8bf85659":"train['Fare'].hist(bins = 80, color = 'orange')\nplt.title(\"FARE\",size=20)","5a244a5f":"plt.figure(figsize=(5,5))\nplt.title(\"Embarked\",size=20)\ntit1.Embarked.value_counts().plot.bar(color=['olive','coral','gold'])","ecd3b04b":"sns.heatmap(train.corr(), annot = True)","18d6ddb5":"\nplt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Sex', data = train)\nplt.title(\"SURVIVED AND SEX\",size=20)","32aa3416":"plt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train)\nplt.title(\"SURVIVED AND PCLASS\",size=20)","aa666d74":"plt.figure(figsize=(5,5))\nsns.countplot(x = 'Survived', hue = 'Embarked', data = train)\nplt.title(\"SURVIVED AND EMBARKED\",size=20)","eece4008":"age_group = train.groupby(\"Pclass\")[\"Age\"]\nprint(age_group.median())","bce63cd8":"age_group = train.groupby(\"Embarked\")[\"Age\"]\nprint(age_group.median())","a2ca7ba5":"train.loc[train.Age.isnull(),'Age']=train.groupby(\"Pclass\").Age.transform('median')\ntest.loc[test.Age.isnull(),'Age']=test.groupby(\"Pclass\").Age.transform('median')\nprint(train['Age'].isnull().sum())","cf70a5b5":"test['Cabin'].unique().tolist()","9a46b655":"cab = test.groupby(\"Cabin\")[\"Age\"]\nprint(cab.median())","942f3bde":"\ntrain['Cabin'].unique().tolist()","d8e2369c":"\nimport re\n\ntest['Cabin'] = test['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ntest['Cabin'].unique().tolist()\n","a5565f72":"category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'X':8}\ntest['Cabin'] = test['Cabin'].map(category)\ntest['Cabin'].unique().tolist()","a382e38d":"cab = train.groupby(\"Cabin\")[\"Age\"]\nprint(cab.median())","d34f1fe1":"\n\ntrain['Cabin'] = train['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ntrain['Cabin'].unique().tolist()","d33574d0":"category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'X':8, 'T':9}\ntrain['Cabin'] = train['Cabin'].map(category)\ntrain['Cabin'].unique().tolist()","dd43dde7":"print(train.isnull().sum())","446365bc":"from statistics import mode\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(mode(train[\"Embarked\"]))","877397d1":"print(train.isnull().sum())","c64c1af8":"train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\ntrain[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n\ntest[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\ntest[\"Sex\"][test[\"Sex\"] == \"female\"] = 1\n\ntrain[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\ntrain[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\ntrain[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n\ntest[\"Embarked\"][test[\"Embarked\"] == \"S\"] = 0\ntest[\"Embarked\"][test[\"Embarked\"] == \"C\"] = 1\ntest[\"Embarked\"][test[\"Embarked\"] == \"Q\"] = 2\n","3afa8b1a":"train['fam']=train['SibSp']+train['Parch']+1\ntest['fam']=test['SibSp']+test['Parch']+1","05af5c99":"train['Age']=train['Age'].astype(str)\ntest['Age']=test['Age'].astype(str)","4fb768ce":"import re\ntrain['Age'] = train['Age'].map(lambda x: re.compile(\"[0-9]\").search(x).group())\ntrain['Age'].unique().tolist()","74ab2399":"\n\ncat={'2':1, '3':2 , '5':3, '1':4 ,'4':5,'8':6,'6':7,'7':8,'0':9,'9':10}\ntrain['Age']=train['Age'].map(cat)\ntrain['Age'].unique().tolist()","d22750f2":"test['Age'] = test['Age'].map(lambda x: re.compile(\"[0-9]\").search(x).group())\ntest['Age'].unique().tolist()","e1e5b309":"\ncat={'2':1, '3':2 , '5':3, '1':4 ,'4':5,'8':6,'6':7,'7':8,'0':9,'9':10}\ntest['Age']=test['Age'].map(cat)\ntest['Age'].unique().tolist()","b68fb3b1":"train['Title'] = train['Name'].map(lambda x: re.compile(\"([A-Za-z]+)\\.\").search(x).group())\ntest['Title'] = test['Name'].map(lambda x: re.compile(\"([A-Za-z]+)\\.\").search(x).group())\nprint(train['Title'].unique())\n    ","56a53db8":"print(test['Title'].unique())","19f5f6ea":"\n    train['Title'] = train['Title'].replace(['Lady.', 'Capt.', 'Col.',\n    'Don.', 'Dr.', 'Major.', 'Rev.', 'Jonkheer.', 'Dona.'], 'Rare.')\n    \n    train['Title'] = train['Title'].replace(['Countess.', 'Lady', 'Sir'], 'Royal')\n    train['Title'] = train['Title'].replace('Mlle.', 'Miss.')\n    train['Title'] = train['Title'].replace('Ms.', 'Miss.')\n    train['Title'] = train['Title'].replace('Mme.', 'Mrs.')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","3d52809b":"\n    test['Title'] = test['Title'].replace(['Lady.', 'Capt.', 'Col.',\n    'Don.', 'Dr.', 'Major.', 'Rev.', 'Jonkheer.', 'Dona.'], 'Rare.')\n    \n    test['Title'] = test['Title'].replace(['Countess.', 'Lady.', 'Sir.'], 'Royal.')\n    test['Title'] = test['Title'].replace('Mlle.', 'Miss')\n    test['Title'] = test['Title'].replace('Ms.', 'Miss.')\n    test['Title'] = test['Title'].replace('Mme.', 'Mrs.')\n\n","1010e598":"    title_mapping = {\"Mr.\": 1, \"Miss.\": 2, \"Mrs.\": 3, \"Master.\": 4, \"Royal.\": 5, \"Rare.\": 6}\n\n    train['Title'] = train['Title'].map(title_mapping)\n    train['Title'] = train['Title'].fillna(0)\n\n    train.head()","49932cc7":"    title_mapping = {\"Mr.\": 1, \"Miss.\": 2, \"Mrs.\": 3, \"Master.\": 4, \"Royal.\": 5, \"Rare.\": 6}\n\n    test['Title'] = test['Title'].map(title_mapping)\n    test['Title'] = test['Title'].fillna(0)\n\n   ","9bc2e727":"print(train['Age'])","38ae91c2":"print(train['Cabin'])","2c686a66":"print(train['Sex'])","81291adc":"print(train['Embarked'])","ff965797":"print(train['fam'])","93f7b687":"\ntest = test.drop(['Ticket'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\ntest = test.drop(['Parch'], axis = 1)\ntest = test.drop(['Fare','SibSp'], axis = 1)\n\ntrain.drop(['Name', 'Ticket'], axis = 1, inplace = True)\ntrain = train.drop(['Parch'], axis = 1)\ntrain = train.drop(['Fare','SibSp'], axis = 1)","ff853afb":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived','PassengerId'], axis=1), \n                                                    train['Survived'], test_size = 0.2, \n                                                    random_state = 0)","5a84fbf8":"from sklearn.linear_model import LogisticRegression\nlogisticRegression = LogisticRegression(max_iter = 30000)\nlogisticRegression.fit(X_train, y_train)\n\n\n","a624c7cb":"predictions = logisticRegression.predict(X_test)\nacc_LOG = round(accuracy_score(predictions, y_test) * 100, 2)\nprint(acc_LOG)\nprint(predictions)\n\n\n","19aa257a":"round(np.mean(predictions), 3)","d58ca71e":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, predictions))","6afa63c0":"accuracy=((88+50)\/(88+50+22+19))\nprint('accuracy is: ', (round(accuracy, 2)*100))","e100a994":"from sklearn.ensemble import RandomForestClassifier\n\n# Define our optimal randomForest algo\nrandomforest = RandomForestClassifier(random_state = 5, criterion = 'gini', max_depth = 10, max_features = 'auto', n_estimators = 500)\nrandomforest.fit(X_train, y_train)\npred = randomforest.predict(X_test)\nacc_randomforest = round(accuracy_score(pred, y_test) * 100, 2)\nprint(acc_randomforest)\n","ac851016":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(pred, y_test) * 100, 2)\nprint(acc_gbk)","6fbc99a5":"see={'TECHNIQUE':['RANDOM FOREST','LOGISTIC REGRESSION','GRADIENT BOOSTING'],'ACCURACY':[acc_randomforest,acc_LOG,acc_gbk]}\nmod=pd.DataFrame(see)\nmod","b878cca0":"mod['Rank']=[2,1,3]\nmod","12c69d2c":"mod['Weighted Rank']=[2\/6,1\/6,3\/6]\nmod","63eb8971":"final_pred=np.dot(mod['ACCURACY'],mod['Weighted Rank'])\nfinal_pred","3e08e49b":"train.head()","696fc62b":"test.head()","58dfe6f5":"ids = test['PassengerId']\npredictions = randomforest.predict(test.drop('PassengerId', axis=1))\n\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","fdca344f":"**Lets create a new column of fam using SibSp which means number of Siblings or Spouse and Parch which means number of Parents or Children,later we will be dropping SibSp and Parch from our data set since these values are alreday being used in Fam**","c3c627d7":"![image.png](attachment:image.png)\n","1501f180":"**Lets check for the number of Null Values in our DATA SET**","46656c7f":"**Checking out the accuracy of our predictions**","51a5cfde":"**So now we have filled the NAN values of embarked too.Lets check the null values again!**","865e849c":"**We should know the size of the data we are working with.**","fd5998ad":"**Now we are assigning values to the initials that we had found in the above step and replace them with integers by mapping them.\nSame step will be repeated for train and test data**","dd2ad887":"**Now we will do the same thing that we did with cabin so that we are left with the initials and can assign them numeric values accordingly.**","b415a19f":"**First we start by checking the counts of survived(1) and dead(0).\nthus from the below graoh it is clear that there were more deaths than the ratio of survivors.\nWe also plot of graph for the division of genders, to see the ratio between men and women.\nwhen the graph i splotted we see that the range of women seem more equivalent to the range of survivors and the range of deaths seem more closely related to the range of men.\nSo thus that mean that there were more women who survuived?\nWe shall se that further.**","876eda09":"**Lets check out the missing values again**","d35d0248":"**The Data that we are dropping from the dataset**","6b220733":"**Checking out the distribution of Fares**","3ee87758":"# **LOGISTIC REGRESSION**","0634b889":"**Searching for the titles and extracting them from the names in the given data**","80058219":"**Lets convert our categorical data to numeric form**","76484a58":"**AGE and CABIN have the higest number of Null Values,so they will not be of major help since most of the values are missing,especially CABIN.\nBut lets see the Maximum age groups present.**","289a0602":"**SUBMISSION FILE(choosing Gradient Boosting)**","5006ca16":"# **EXPLORING FEATURES**","489da32d":"**Mapping new numerical values onto Titles**","5a449ac3":"**Replacing to make the categories narrower and accurate**","f74964ca":"**Mapping values**","0749e541":"# **CLEANING DATA**","f48ebe6f":"**Since we have explored all the features in our dataset,now we shall draw close comparisons with \"SURVIVED\" feature,to help us draw some inference.**","60fdb965":"# **RANDOM FOREST**","e39c17d5":"**Lets examine the types of classes that were present**","31591f29":"**First we are converting float to string for both the datasets namely test and train**","e4b96f6f":"**Lets find out the percentage of Women and Men**","9b2e304b":"**GG!!So no more missing values in our dataset**","83d5c5bc":"**train_test_split :Split arrays or matrices into random train and test subsets**","8659b7bf":"Doing Rank Averaging ensembling modelling","070a8c03":"**Lets play a little with Age as well**","7cca6a5e":"**Now only \"Embarked\" has two missing values in it.**","89bc261f":"**Lets check whether the conversion has worked or not**","abdc9198":"**Checking out Embarked Attribute.\n  It has 3 discrete Divisions,namely S , C ,Q.**","19f29113":"**Mean survival rate according to the Titles assigned**","82f62c50":"**Lets start predicting,we will be using Logistic Regression.Logistic Regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).  Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.**","33ee0710":"**Visualizing the data in our dataframe into a correlation heatmap **","bf4ef383":"# **GRADIENT BOOSTING**","0b8173dd":"**Lets assign X value to all the NAN values**","397293cb":"**Lets play around with Name as well!**","23525c89":"**Lets take help of confusion matrix to find out TP TN FP FN.A confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made.\n**\n","087d4ce1":"**We will be searching for the initials of the cabin numbers like A,B,C,etc**","8fe82a5b":"**Lets try using Random Forest.A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.**","59555bfb":"**Calculating median values of \"Age\" by using \"Pclass\" and \"Embarked\" to fill up the missing values.**","8a54ba33":"**This mean is pretty close to the one that we had calculated earlier(0.384)**","b64a8887":"**Making and Printing our predictions**","ea0f7536":"**Now we have no missing values for AGE**","3d9b26c3":"**Lets work out with the Cabin numbers**","25736d97":"**Adding a column for Survived which has to be predicted in the test data.**","0f12eba2":"**Lets find out the Survival Rate**"}}