{"cell_type":{"f0bc13f0":"code","7e4bbf5c":"code","47fe8604":"code","bf709ef7":"code","183bdf62":"code","6fcfb712":"code","6a97fa18":"code","8ccb4308":"code","d94a891b":"code","b0d69006":"code","d0d69b1b":"code","3f0db1e1":"code","71a34e0b":"code","70920528":"code","a8c94ceb":"code","e1b0fe34":"code","6f209238":"code","70655657":"code","d669a1ae":"code","1aadb413":"code","34b22235":"code","ced94124":"code","9d9c5e21":"code","3ccc78f3":"code","04dc0c9b":"code","0a767200":"code","ab8fb6cb":"code","86efb385":"code","58475640":"code","0c68338c":"code","befd69c9":"markdown","5afbc989":"markdown","b11bd566":"markdown","9f4d2f9a":"markdown","8d397a0b":"markdown","4a084538":"markdown","65666869":"markdown","7bbafc91":"markdown","7dd12b0d":"markdown","5443763e":"markdown","cdae024a":"markdown","ebe5775c":"markdown","cebd625d":"markdown","dae2065b":"markdown","512fb639":"markdown","aab36f7c":"markdown"},"source":{"f0bc13f0":"import pandas as pd\nimport numpy as np\nimport os\n\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Load data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","7e4bbf5c":"train.shape, test.shape","47fe8604":"train.head(3)","bf709ef7":"# Check for NaN values and data types\ntrain.info()","183bdf62":"# Check for correlation between features\npd.get_dummies(train.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis = 1)).corr()[\"Survived\"].sort_values(ascending = False)","6fcfb712":"train_age = train[[\"Survived\", \"Age\"]]\ntrain_age = train_age.fillna(train_age[\"Age\"].median())\ntrain_age[[\"Survived\", \"Age\"]].corr()[\"Survived\"].sort_values(ascending = False)","6a97fa18":"pd.get_dummies(train[[\"Survived\", \"Cabin\"]]).corr()[\"Survived\"].sort_values(ascending = False)","8ccb4308":"pd.get_dummies(train[[\"Survived\", \"Ticket\"]]).corr()[\"Survived\"].sort_values(ascending = False)","d94a891b":"# Create a feature called \"FamilySize\" combining \"SibSp\" and \"Parch\"\ntrain_custom = train.copy()\ntrain_custom[\"FamilySize\"] = train_custom[\"SibSp\"] + train_custom[\"Parch\"]\ntrain_custom[[\"Survived\", \"FamilySize\"]].corr()[\"Survived\"].sort_values(ascending = False) # Low correlation","b0d69006":"# Create a \"NameTitle\" feature extracting the title of each passenger\n!pip install nameparser\nfrom nameparser import HumanName\n\ndef name_title(X):\n    names = []\n    for name in X[\"Name\"]:\n        names.append(HumanName(name).title)\n    X[\"NameTitle\"] = names\n    \ndef best_title(X):\n    min_titles = (X[\"NameTitle\"].value_counts() < 50)\n    for title in X[\"NameTitle\"]:\n        if min_titles.loc[title] == True:\n            X.loc[X[\"NameTitle\"] == title, \"NameTitle\"] = \"Misc\"\n            \n# Solution using lambda functions:\n# train_custom[\"NameTitle\"] = train_custom[\"Name\"].apply(lambda x: HumanName(x).title)\n# min_titles = (train_custom[\"NameTitle\"].value_counts() < 50)\n# train_custom[\"NameTitle\"] = train_custom[\"NameTitle\"].apply(lambda x: \"Misc\" if min_titles.loc[x] == True else x)\n\n# Check correlation\nname_title(train_custom)\nbest_title(train_custom)\npd.get_dummies(train_custom[[\"Survived\", \"NameTitle\"]]).corr()[\"Survived\"].sort_values(ascending = False)","d0d69b1b":"def alone(X):\n    X[\"Single\"] = 0\n    for i, value in enumerate(X[\"FamilySize\"]):\n        if value == 0:\n            X[\"Single\"][i] = 1\n            \nalone(train_custom)\ntrain_custom[[\"Survived\", \"Single\"]].corr()[\"Survived\"].sort_values(ascending = False)","3f0db1e1":"# Select independent and dependent variables\nX = train[[\"Sex\", \"Fare\", \"Embarked\", \"Pclass\", \"Name\", \"Age\", \"Parch\", \"SibSp\"]]\ny = train[\"Survived\"]\n\n# Convert Pclass data type from int to object\npd.options.mode.chained_assignment = None\nX[\"Pclass\"] = X[\"Pclass\"].astype(\"object\")","71a34e0b":"X.info()","70920528":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nfrom matplotlib.pyplot import plot\n\n# Set style for visualizations\nstyle.use(\"seaborn-whitegrid\")\n%matplotlib inline\n%config InlineBackend.figure_format = \"retina\"","a8c94ceb":"from scipy import stats\n\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\naxes[0].hist(X[\"Fare\"], bins = 50)\naxes[0].set_title(\"Skewness = \" + str(round(stats.skew(X[\"Fare\"]), 4)))\naxes[1].hist(np.log(1 + X[\"Fare\"]), bins = 50)\naxes[1].set_title(\"Log Transformed Skewness = \" + str(round(stats.skew(np.log(1 + X[\"Fare\"])), 4)))\nfig.suptitle(\"Fare Feature Skewness\", fontsize = 14)\nfig.tight_layout()","e1b0fe34":"# No need for transformation\nfig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\naxes[0].hist(train_age[\"Age\"], bins = 50)\naxes[0].set_title(\"Skewness = \" + str(round(stats.skew(train_age[\"Age\"]), 4)))\naxes[1].hist(np.log(1 + train_age[\"Age\"]), bins = 50)\naxes[1].set_title(\"Log Transformed Skewness = \" + str(round(stats.skew(np.sqrt(train_age[\"Age\"])), 4)))\nfig.suptitle(\"Age Feature Skewness\", fontsize = 14)\nfig.tight_layout()","6f209238":"# Custom pipeline for data preparation\n\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\n# Select numerical and categorical features\nnumerical_features = X.select_dtypes(include = [\"int64\", \"float64\"]).columns\ncategorical_features = X.select_dtypes(include = [\"object\", \"bool\"]).columns\n\n# Define custom transformer for log transformation of \"Fare\" feature\nclass log_transform(BaseEstimator, TransformerMixin):        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X[:, 0] = np.log(1 + X[:, 0])\n        return X\n    \n# Define custom transformer for name title extraction   \nclass name_transform(BaseEstimator, TransformerMixin):        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X): \n        names = []\n        for name in X[:, 3]:\n            names.append(HumanName(name).title)\n        names = np.array(names)\n        names = np.expand_dims(names, axis = 1)\n        X = np.append(X, names, axis = 1)\n        X = np.delete(X, 3, 1)\n        min_titles = [\"Mr.\", \"Mrs.\", \"Miss.\"]\n        for i, title in enumerate(X[:, 3]):\n                if title in min_titles:\n                    X[:, 3][i] = title\n                else:\n                    X[:, 3][i] = \"Misc\"\n        return X\n\n# Define custom transformer to create single passenger feature\nclass single(BaseEstimator, TransformerMixin):\n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        # FamilySize\n        family_size = X[:, 2] + X[:, 3]\n        family_size = np.expand_dims(family_size, axis = 1)\n        X = np.append(X, family_size, axis = 1)\n        \n        # SinglePassenger\n        z = np.zeros((X.shape[0], 1), dtype = \"int64\")\n        X = np.append(X, z, axis = 1)\n        for i, value in enumerate(X[:, 4]):\n            if value == 0:\n                X[:, 5][i] = 1\n        \n        # Drop Parch, SibSp, FamilySize features\n        X = np.delete(X, [2, 3, 4], 1)\n        return X\n\n# Impute all missing values with the median value,\n# Rescale features using Min-Max scaler, range 0-1\nnumerical_transformer = Pipeline(steps = [\n    (\"impute\", SimpleImputer(strategy = \"median\")),\n    (\"log_transform\", log_transform()),\n    (\"single_passenger\", single()),\n    (\"scaler\", MinMaxScaler())])\n\n# Impute all missing values using the most frequent value,\n# One-Hot encode all categorical values\ncategorical_transformer = Pipeline(steps = [\n    (\"impute\", SimpleImputer(strategy = \"most_frequent\")),\n    (\"names\", name_transform()),\n    (\"onehot\", OneHotEncoder())])\n\n# Combine numerical and categorical transformers for final pipeline\ntransformer = ColumnTransformer(transformers = [\n    (\"transform_numeric\", numerical_transformer, numerical_features),\n    (\"transform_categorical\", categorical_transformer, categorical_features)], \n                                verbose = True)","70655657":"from sklearn.model_selection import train_test_split\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(transformer.fit_transform(X), y, \n                                                    test_size = 0.2, shuffle = True, random_state = 10)","d669a1ae":"import graphviz\n\n# Draw basic architecture of model\ndef gv(x): \n    return graphviz.Source('digraph G{ rankdir=\"LR\"' + x + '; }')\n\n# Predictions are calculated from the independent variables, which is the data not including the labels.\n# The results of the model are called predictions, the measure of performance is called the loss.\n# The loss depends not only on the predictions, but also the correct labels.\n\ngv('''ordering=in\nmodel[shape=box3d width=1 height=0.7 label=model]\ninputs->model->predictions; weights->model; labels->loss; predictions->loss\nloss->weights[constraint=false label=update]''')","1aadb413":"from sklearn.linear_model import Perceptron\n\n# Fit and train perceptron algorithm\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nprint(\"Perceptron Benchmark Performance : \" + str(round(perceptron.score(X_test, y_test), 4)))","34b22235":"# Create predictions for submission\nbench_preds = perceptron.predict(transformer.fit_transform(\n    test.drop([\"PassengerId\", \"Cabin\", \"Ticket\"], axis = 1)))","ced94124":"# Create submission file\nbench_sub = pd.DataFrame({\"PassengerId\" : test.PassengerId, \"Survived\" : bench_preds})\nbench_sub.to_csv(\"bench_sub.csv\", index = False)\nbench_sub.head(5)\n\n# Performance ~ 0.77","9d9c5e21":"import tensorflow as tf\nfrom tensorflow import keras\nimport kerastuner as kt\n\ndef MLP(hp):\n    \n    # Create a sequential MLP model which consists of a single \n    # stack of layers connected sequentially\n    model = keras.Sequential()\n    \n    # Input layer with one neuron for each feature\n    model.add(keras.layers.Input(shape = X_train.shape[1]))\n    \n    # Add batch normalization layer for normalizing input\n    # Add dropout layer for regularization\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dropout(0.2))\n    \n    # Tune the number of TLU's in the first dense layer\n    # First hidden layer with ReLU activation function and \"He\" initialization,\n    # each dense layer manages it's own weight matrix and bias vector\n    hp_units_1 = hp.Int(\"units_1\", min_value = 16, max_value = 256, step = 16)\n    model.add(keras.layers.Dense(units = hp_units_1, activation = \"relu\", \n                                 kernel_initializer = \"he_normal\", use_bias = False))\n      \n    # Add batch normalization layer for normalizing input\n    # Add dropout layer for regularization\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dropout(0.2))\n    \n    # Tune the number of TLU's in the second dense layer\n    # Second hidden layer with ReLU activation function and \"He\" initialization, \n    # each dense layer manages it's own weight matrix and bias vector\n    hp_units_2 = hp.Int(\"units_2\", min_value = 16, max_value = 256, step = 16)\n    model.add(keras.layers.Dense(units = hp_units_2, activation = \"relu\", \n                                 kernel_initializer = \"he_normal\", use_bias = False))\n    \n    # Add batch normalization layer for normalizing input\n    # Add dropout layer for regularization\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dropout(0.2))\n    \n    # Tune the number of TLU's in the third dense layer\n    # Third hidden layer with ReLU activation function and \"He\" initialization, \n    # each dense layer manages it's own weight matrix and bias vector\n    hp_units_3 = hp.Int(\"units_3\", min_value = 16, max_value = 256, step = 16)\n    model.add(keras.layers.Dense(units = hp_units_3, activation = \"relu\", \n                                 kernel_initializer = \"he_normal\", use_bias = False))\n    \n    # Add batch normalization layer for normalizing input\n    # Add dropout layer for regularization\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dropout(0.2))\n    \n    # Output layer with 1 TLU per class and sigmoid activation function\n    model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n    \n    # Tune the learning rate for the optimizer \n    hp_learning_rate = hp.Choice(\"learning_rate\", values = [1e-1, 1e-2, 1e-3, 1e-4]) \n    model.compile(optimizer = keras.optimizers.RMSprop(learning_rate = hp_learning_rate),\n                loss = keras.losses.BinaryCrossentropy(), metrics = [\"accuracy\"])\n  \n    return model","3ccc78f3":"# Select hyperparameter optimization technique\ntuner = kt.Hyperband(MLP, \n                     objective = \"val_accuracy\", \n                     max_epochs = 100, \n                     factor = 3, \n                     overwrite = True)","04dc0c9b":"import IPython\n\n# Define a callback to clear the training outputs at the end of every training step\nclass ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)\n\n# Run the hyperparameter search with early stopping\ntuner.search(X_train, y_train, epochs = 100, batch_size = 16, validation_split = 0.3, \n             callbacks = [ClearTrainingOutput(), keras.callbacks.EarlyStopping(patience = 20)], verbose = 0)\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\nprint(f\"\"\"The hyperparameter search is complete. The optimal number of TLU's in the first densely-connected\nlayer is {best_hps.get(\"units_1\")}, the optimal number of TLU's in the second densely-connected\nlayer is {best_hps.get(\"units_2\")} and the optimal number of TLU's in the third densely-connected\nlayer is {best_hps.get(\"units_3\")}. The learning rate for the optimizer is {best_hps.get(\"learning_rate\")}.\"\"\")","0a767200":"# Retrain the model with the optimal hyperparameters\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X_train, y_train, epochs = 500, batch_size = 16, validation_split = 0.2, verbose = 0)","ab8fb6cb":"# Plot loss and accuracy\npd.DataFrame(history.history).plot(figsize = (10,5), linewidth = 2)\nplt.gca().set_ylim(0, 1)\nplt.show()","86efb385":"# Evaluate performance on test data\nprint(\"MLP Performance on Testset: \" + str(round(model.evaluate(X_test, y_test)[1], 4)))","58475640":"# Create predictions for submission\npredictions = model.predict(transformer.fit_transform(\n    test.drop([\"PassengerId\", \"Cabin\", \"Ticket\"], axis = 1)))","0c68338c":"# Create submission file\npredictions = (predictions.ravel() > 0.5).astype(int)\nsubmission = pd.DataFrame({\"PassengerId\" : test.PassengerId, \"Survived\" : predictions})\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head(5)\n\n# Performance ~ 0.77\n# Same performance as simple perceptron","befd69c9":"### Data Preparation Pipeline","5afbc989":"## Import Data","b11bd566":"### Data Split","9f4d2f9a":"### Missing Values","8d397a0b":"### Benchmark Predictions","4a084538":"## Custom Features","65666869":"### Family Size","7bbafc91":"### Predictions","7dd12b0d":"## Training Data Exploration","5443763e":"### Single Passenger","cdae024a":"## MLP with Hyperparameter Optimization","ebe5775c":"### Name Titles","cebd625d":"### Numerical Features Distributions","dae2065b":"## Perceptron Benchmark","512fb639":"### Correlation","aab36f7c":"## Data Preparation"}}