{"cell_type":{"d1179b5c":"code","74206cae":"code","ed5ee2e1":"code","ba0b5624":"code","3688c3a8":"code","011bf934":"code","fc13c30b":"code","b13c6900":"code","00cb7c34":"code","accb6476":"code","e515855f":"code","3ae849fc":"code","d7b457b4":"code","95b891ff":"code","2eb59bf5":"code","c11cc499":"code","845d24ca":"code","ed9ce5b5":"code","11541149":"code","98458d95":"code","1122a670":"code","6631df53":"code","f4d28059":"code","cded4a9a":"code","e563f4a6":"markdown","bfb47704":"markdown","249664b0":"markdown","52d24635":"markdown","a7dc88b3":"markdown","f9742d45":"markdown","3e7b0823":"markdown","731d99e3":"markdown","08d9fea7":"markdown","3d13981a":"markdown","faa94dbe":"markdown"},"source":{"d1179b5c":"import warnings\nwarnings.filterwarnings('ignore')\n\n#the basics\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json, os, random\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#tensorflow basics\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.cluster import KMeans","74206cae":"def seed_everything(seed = 34):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything()","ed5ee2e1":"#get comp data\ntrain = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_sub = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')\naug_df = pd.read_csv('..\/input\/aug-data\/aug_data.csv')","ba0b5624":"#Exploring signal_to_noise and SN_filter distributions\nfig, ax = plt.subplots(1, 2, figsize = (15, 5))\nsns.kdeplot(train['signal_to_noise'], shade = True, ax = ax[0])\nsns.countplot(train['SN_filter'], ax = ax[1])\n\nax[0].set_title('Signal\/Noise Distribution')\nax[1].set_title('Signal\/Noise Filter Distribution');","3688c3a8":"#target columns\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","011bf934":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}","fc13c30b":"def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea,bpps_nb_fea], 2)","b13c6900":"def rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score","00cb7c34":"# additional features\ndef read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # from https:\/\/www.kaggle.com\/tuckerarrants\/openvaccine-gru-lstm \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr \n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)","accb6476":"# clustering for  GroupKFold\nkmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[:,:,0])\ntrain['cluster_id'] = kmeans_model.labels_","e515855f":"def gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef build_model(gru=1, seq_len=107, pred_len=68, dropout=0.5,\n                embed_dim=100, hidden_dim=128, layers=3):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, 3))\n\n    embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    hidden = tf.keras.layers.SpatialDropout1D(.2)(reshaped)  \n    \n    \n    if gru==1:\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        \n    elif gru==0:\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        \n    elif gru==3:\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        \n    elif gru==4:\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        \n    elif gru==5:\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n    #only making predictions on the first part of each sequence\n    truncated = hidden[:, :pred_len]\n    \n    out = tf.keras.layers.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    #some optimizers\n    adam = tf.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n    \n    model.compile(optimizer=adam, loss=mcrmse)\n    \n    return model","3ae849fc":"def aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df\ntrain = aug_data(train)\ntest = aug_data(test)","d7b457b4":"#basic training configuration\nFOLDS = 5\nEPOCHS = 75\nREPEATS = 1\nBATCH_SIZE = 64\nVERBOSE = 2\nSEED = 34","95b891ff":"#get different test sets and process each\npublic_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","2eb59bf5":"lr_callback = tf.keras.callbacks.ReduceLROnPlateau()","c11cc499":"# gru_histories = []\n# gru_private_preds = np.zeros((private_df.shape[0], 130, 5))\n# gru_public_preds = np.zeros((public_df.shape[0], 107, 5))\n\n# gkf = GroupKFold(n_splits=FOLDS)\n# for cv, (tr_idx, vl_idx) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n\n#     sv_gru = tf.keras.callbacks.ModelCheckpoint(f'gru-{cv}.h5')\n\n#     trn = train.iloc[tr_idx]\n#     x_trn = preprocess_inputs(trn)\n#     y_trn = np.array(trn[target_cols].values.tolist()).transpose((0, 2, 1))\n#     w_trn = np.log(trn.signal_to_noise+1.1)\/2\n\n#     val = train.iloc[vl_idx]\n#     x_val_all = preprocess_inputs(val)\n#     val = val[val.SN_filter == 1]\n#     x_val = preprocess_inputs(val)\n#     y_val = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n#     gru = build_model(gru=1)\n#     history = gru.fit(x_trn, y_trn, \n#                       validation_data=(x_val, y_val),\n#                       batch_size=BATCH_SIZE,\n#                       sample_weight=w_trn\/2,\n#                       epochs=EPOCHS,\n#                       callbacks=[lr_callback,sv_gru],\n#                       verbose = VERBOSE)  \n\n#     gru_histories.append(history)\n\n#     #load best model and predict\n#     gru_short = build_model(gru=1, seq_len=107, pred_len=107)\n#     gru_short.load_weights(f'gru-{cv}.h5')\n#     gru_public_pred = gru_short.predict(public_inputs) \/ FOLDS * REPEATS\n\n#     gru_long = build_model(gru=1, seq_len=130, pred_len=130)\n#     gru_long.load_weights(f'gru-{cv}.h5')\n#     gru_private_pred = gru_long.predict(private_inputs) \/ FOLDS * REPEATS\n\n#     gru_public_preds += gru_public_pred\n#     gru_private_preds += gru_private_pred\n\n#     del gru_short, gru_long","845d24ca":"lstm_histories = []\nlstm_private_preds = np.zeros((private_df.shape[0], 130, 5))\nlstm_public_preds = np.zeros((public_df.shape[0], 107, 5))\n\ngkf = GroupKFold(n_splits=FOLDS)\nfor cv, (tr_idx, vl_idx) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n\n    sv_gru = tf.keras.callbacks.ModelCheckpoint(f'lstm-{cv}.h5')\n\n    trn = train.iloc[tr_idx]\n    x_trn = preprocess_inputs(trn)\n    y_trn = np.array(trn[target_cols].values.tolist()).transpose((0, 2, 1))\n    w_trn = np.log(trn.signal_to_noise+1.1)\/2\n\n    val = train.iloc[vl_idx]\n    x_val_all = preprocess_inputs(val)\n    val = val[val.SN_filter == 1]\n    x_val = preprocess_inputs(val)\n    y_val = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n    lstm = build_model(gru=1)\n    history = lstm.fit(x_trn, y_trn, \n                      validation_data=(x_val, y_val),\n                      batch_size=BATCH_SIZE,\n                      sample_weight=w_trn\/2,\n                      epochs=EPOCHS,\n                      callbacks=[lr_callback,sv_gru],\n                      verbose = VERBOSE)  \n\n    lstm_histories.append(history)\n\n    #load best model and predict\n    lstm_short = build_model(gru=1, seq_len=107, pred_len=107)\n    lstm_short.load_weights(f'lstm-{cv}.h5')\n    lstm_public_pred = lstm_short.predict(public_inputs) \/ FOLDS * REPEATS\n\n    lstm_long = build_model(gru=1, seq_len=130, pred_len=130)\n    lstm_long.load_weights(f'lstm-{cv}.h5')\n    lstm_private_pred = lstm_long.predict(private_inputs) \/ FOLDS * REPEATS\n\n    lstm_public_preds += lstm_public_pred\n    lstm_private_preds += lstm_private_pred\n\n    del lstm_short, lstm_long","ed9ce5b5":"fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n\n# for history in gru_histories:\n#     ax[0].plot(history.history['loss'], color='C0')\n#     ax[0].plot(history.history['val_loss'], color='C1')\nfor history in lstm_histories:\n    ax[1].plot(history.history['loss'], color='C0')\n    ax[1].plot(history.history['val_loss'], color='C1')\n\n# ax[0].set_title('GRU')\nax[1].set_title('LSTM')\n\n# ax[0].legend(['train', 'validation'], loc = 'upper right')\nax[1].legend(['train', 'validation'], loc = 'upper right')\n\n# ax[0].set_ylabel('MCRMSE')\n# ax[0].set_xlabel('Epoch')\nax[1].set_ylabel('MCRMSE')\nax[1].set_xlabel('Epoch');","11541149":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","98458d95":"# preds_gru = []\n\n# for df, preds in [(public_df, gru_public_preds), (private_df, gru_private_preds)]:\n#     for i, uid in enumerate(df.id):\n#         single_pred = preds[i]\n\n#         single_df = pd.DataFrame(single_pred, columns=target_cols)\n#         single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n#         preds_gru.append(single_df)\n\n# preds_gru_df = pd.concat(preds_gru).groupby('id_seqpos').mean().reset_index()\n# preds_gru_df.head()","1122a670":"preds_lstm = []\n\nfor df, preds in [(public_df, lstm_public_preds), (private_df, lstm_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_lstm.append(single_df)\n\npreds_lstm_df = pd.concat(preds_lstm).groupby('id_seqpos').mean().reset_index()\npreds_lstm_df.head()","6631df53":"# blend_preds_df = pd.DataFrame()\n# blend_preds_df['id_seqpos'] = preds_gru_df['id_seqpos']\n# blend_preds_df['reactivity'] = 0.5*preds_gru_df['reactivity'] + 0.5*preds_lstm_df['reactivity']\n# blend_preds_df['deg_Mg_pH10'] = 0.5*preds_gru_df['deg_Mg_pH10'] + 0.5*preds_lstm_df['deg_Mg_pH10']\n# blend_preds_df['deg_pH10'] = 0.5*preds_gru_df['deg_pH10'] + 0.5*preds_lstm_df['deg_pH10']\n# blend_preds_df['deg_Mg_50C'] = 0.5*preds_gru_df['deg_Mg_50C'] + 0.5*preds_lstm_df['deg_Mg_50C']\n# blend_preds_df['deg_50C'] = 0.5*preds_gru_df['deg_50C'] + 0.5*preds_lstm_df['deg_50C']","f4d28059":"submission = sample_sub[['id_seqpos']].merge(preds_lstm_df, on=['id_seqpos'])\n#sanity check\nsubmission.head()","cded4a9a":"submission.to_csv('submission.csv', index=False)\nprint('Submission saved')","e563f4a6":"# Training\n\n**Create train\/val split now so both models are trained and evaluated on the same samples:**","bfb47704":"## Processing","249664b0":"**Now we just need to change the shape of each sample to the long format:**","52d24635":"### 1. GRU","a7dc88b3":"Inspired by:\n1. [openvaccine-gru-lstm](https:\/\/www.kaggle.com\/tuckerarrants\/openvaccine-gru-lstm)\n2. [GRU+LSTM with feature engineering and augmentation](https:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation\/notebook)\n\n### Reviews appreciated:)","f9742d45":"### 2. LSTM","3e7b0823":"# Inference and Submission","731d99e3":"**And now we blend:**","08d9fea7":"**We will use a simple learning rate callback for now:**","3d13981a":"# Model Evaluation","faa94dbe":"**Now we do the same for the LSTM model so we can blend their predictions:**"}}