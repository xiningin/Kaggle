{"cell_type":{"36bd4126":"code","43492ffc":"code","bdb983e3":"code","017b9949":"code","03cf107e":"code","3bdd7e0d":"code","d9d41057":"code","164bcf47":"code","80ad2af7":"code","5d2b29e6":"code","a0f76352":"code","14588856":"code","ac4829f0":"code","20c0f659":"code","82b3ef51":"code","5f7a2abb":"code","6683d059":"code","06b4fefc":"code","c3a279f9":"code","a2dd5ed0":"code","38a6550d":"code","fd00f920":"code","845fe581":"code","a5cc7635":"code","789fd128":"code","f774301a":"code","d9b8928e":"code","7cc1fa5d":"code","1faf655b":"code","db08abeb":"code","b0896fcf":"code","ff7695ba":"code","4494dfb7":"code","7d647143":"code","851d2a65":"code","8b3fbc9f":"code","48908158":"code","ea371822":"code","d63a62fa":"code","e70e4239":"code","1e05dbf3":"code","e82dd481":"code","31589bd8":"code","3fbb1326":"code","2dd365dc":"code","88e4b772":"code","411bae49":"markdown","4e819697":"markdown","52f6bfd8":"markdown","e2c4e8b6":"markdown","8a85c8c6":"markdown","d14251db":"markdown","3b37f57e":"markdown","db1d4ea9":"markdown","32b990ac":"markdown","cde300b0":"markdown","5e3df641":"markdown","0da873d0":"markdown","d18e8d7c":"markdown","321ad61b":"markdown","7a0bec81":"markdown"},"source":{"36bd4126":"import math\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn.metrics as metrics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","43492ffc":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","bdb983e3":"train","017b9949":"test","03cf107e":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","3bdd7e0d":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n","d9d41057":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","164bcf47":"# Concatenate train and test data to make c\ndata = pd.concat([train, test])","80ad2af7":"data.set_index('Id', inplace = True)","5d2b29e6":"data.isnull().sum()[:50]","a0f76352":"data.info()","14588856":"data['PoolQC'].fillna('None', inplace=True) # NaN values mean 'No Pool'\ndata['MiscFeature'].fillna('None', inplace=True) # NaN values mean 'No MiscFeature'\ndata['Alley'].fillna('None', inplace=True) # NaN values mean 'No alley access'\ndata['Fence'].fillna('None', inplace=True) # NaN values mean 'No Fence'\ndata['FireplaceQu'].fillna('None', inplace=True) # NaN values mean 'No Fireplace'","ac4829f0":"# GSince the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood, we can fill in missing values by the median LotFrontage of the neiborhood.\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","20c0f659":"# GarageType, GarageFinish, GarageQual and GarageCond: Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')","82b3ef51":"# GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)","5f7a2abb":"# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)","6683d059":"# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')","06b4fefc":"# MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. \ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)","c3a279f9":"# MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])","a2dd5ed0":"# Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\ndata = data.drop(['Utilities'], axis=1)","38a6550d":"# Functional : data description says NA means typical\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")","fd00f920":"# Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])","845fe581":"# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])","a5cc7635":"# Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])","789fd128":"# SaleType : Fill in again with most frequent which is \"WD\"\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","f774301a":"# MSSubClass : Na most likely means No building class. We can replace missing values with None\ndata['MSSubClass'] = data['MSSubClass'].fillna(\"None\")","d9b8928e":"data.info()","7cc1fa5d":"# Transforming some numerical variables that are really categorical\n\n#MSSubClass=The building class\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ndata['OverallCond'] = data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","1faf655b":"# Label Encoding some categorical variables that may contain information in their ordering set\n\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[c].values)) \n    data[c] = lbl.transform(list(data[c].values))\n\n# shape        \nprint('Shape data: {}'.format(data.shape))","db08abeb":"data = pd.get_dummies(data)\nprint(data.shape)","b0896fcf":"train = data[:1460]\ntest = data[1460:].drop('SalePrice', axis = 1)","ff7695ba":"train.shape, test.shape","4494dfb7":"X = train.drop('SalePrice', axis = 1)\ny = train['SalePrice']","7d647143":"# Split in train and validation\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)","851d2a65":"xgb = XGBRegressor(booster='gbtree', colsample_bylevel=1,\n                   colsample_bynode=1,colsample_bytree=0.6,\n                   gamma=0, importance_type='gain',\n                   learning_rate=0.01, max_delta_step=0,\n                   max_depth=4,min_child_weight=1.5,\n                   n_estimators=2500, n_jobs=1, nthread=None,\n                   objective='reg:linear', reg_alpha=0.4640,\n                   reg_lambda=0.6, scale_pos_weight=1,\n                   silent=None, subsample=0.8, verbosity=1)","8b3fbc9f":"lgbm = LGBMRegressor(objective='regression',\n                    num_leaves=4,\n                    learning_rate=0.01,\n                    n_estimators=11000,\n                    max_bin=200,\n                    bagging_fraction=0.75,\n                    bagging_freq=5,\n                    bagging_seed=7,\n                    feature_fraction=0.4)","48908158":"gboost = GradientBoostingRegressor(n_estimators=3000, \n                                   learning_rate=0.05,\n                                   max_depth=4, \n                                   max_features='sqrt',\n                                   min_samples_leaf=15, \n                                   min_samples_split=10, \n                                   loss='huber', \n                                   random_state =5)","ea371822":"xgb.fit(X_train, y_train)\nlgbm.fit(X_train, y_train, eval_metric='rmsle')\ngboost.fit(X_train, y_train)","d63a62fa":"pred2 = xgb.predict(X_test)\npred3 = lgbm.predict(X_test)\npred4 = gboost.predict(X_test)","e70e4239":"print('Root Mean Square Logarithmic Error test (XGB) = ' + str(math.sqrt(metrics.mean_squared_log_error(y_test, pred2))))\nprint('Root Mean Square Logarithmic Error test (LGBM) = ' + str(math.sqrt(metrics.mean_squared_log_error(y_test, pred3))))\nprint('Root Mean Square Logarithmic Error test (GBoost) = ' + str(math.sqrt(metrics.mean_squared_log_error(y_test, pred4))))","1e05dbf3":"lgbm.fit(X, y)   # 0.12269 \nxgb.fit(X ,y)    # 0.12495\ngboost.fit(X, y) # 0.12333","e82dd481":"prediction_lgbm =  np.expm1(lgbm.predict(test))\nprediction_xgb = np.expm1(xgb.predict(test))\nprediction_gboost = np.expm1(gboost.predict(test))","31589bd8":"\"\"\"\nprediction = ( prediction_lgbm * 0.38 + prediction_gboost * 0.35 + prediction_xgb * 0.27)   # 0.12006\nprediction = ( prediction_lgbm * 0.4 + prediction_gboost * 0.35 + prediction_xgb * 0.25)    # 0.12007\nprediction = ( prediction_lgbm * 0.45 + prediction_gboost * 0.35 + prediction_xgb * 0.2)    # 0.12012\nprediction = ( prediction_lgbm * 0.55 + prediction_gboost * 0.45)                           # 0.12061\nprediction = ( prediction_lgbm * 0.45 + prediction_gboost * 0.55)                           # 0.12069\nprediction = ( prediction_gboost * 0.15 + prediction_lgbm * 0.7 + prediction_gboost * 0.15) # 0.12086\nprediction = ( prediction_gboost * 0.2 + prediction_lgbm * 0.5 + prediction_gboost * 0.3)   # 0.12154\nprediction = ( prediction_lgbm * 0.55 + prediction_xgb * 0.45)                              # 0.12155\n\"\"\"","3fbb1326":"prediction = ( prediction_lgbm * 0.38 + prediction_gboost * 0.35 + prediction_xgb * 0.27)   # 0.12006","2dd365dc":"submission = pd.DataFrame({\"Id\": test.index,\"SalePrice\": prediction})","88e4b772":"submission.to_csv('submission.csv', index=False)","411bae49":"# Imports","4e819697":"## Fitting and validating the models","52f6bfd8":"### Normalization","e2c4e8b6":"## Train and Predict with all data set","8a85c8c6":"## Feature Engineering","d14251db":"**If liked this notebook, please give a upvote!**\n\nIf you encounter any problems or have any tips, leave them in the comments.","3b37f57e":"## Submission","db1d4ea9":"# Modelling","32b990ac":"#  Data Preprocessing","cde300b0":"### Input missing values","5e3df641":"* XGBRegressor","0da873d0":"No missing values","d18e8d7c":"* GradientBoostingRegressor","321ad61b":"# References:\n    \n1. https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling\n2. https:\/\/www.kaggle.com\/fedi1996\/house-prices-data-cleaning-viz-and-modeling","7a0bec81":"* LGBMRegressor"}}