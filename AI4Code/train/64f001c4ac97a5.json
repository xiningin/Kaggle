{"cell_type":{"7144f92d":"code","bd46601d":"code","dad6abe4":"code","ac3b747d":"code","3cbbd52f":"code","6429a855":"code","a5054470":"code","31b612fb":"code","d35281df":"code","5a66ea32":"code","d2940ee4":"code","168c868f":"code","5b31755e":"code","47abfb96":"code","f6db0386":"code","57c03029":"code","4aec131a":"code","16f7226b":"code","6514f701":"code","9b53d30f":"code","711f93ad":"code","bc29831e":"code","7b3f3ed5":"code","84e2ba11":"code","9a0634b8":"code","7bc63718":"code","229ef55b":"code","9cd3a52d":"code","17cd4972":"code","5bce6909":"code","c6b386af":"code","aecbf612":"code","55aecc5b":"code","6d98daf8":"code","bfb5792f":"code","db786749":"code","cc3c5253":"code","218f36fc":"code","f9054768":"code","cb4c85c9":"code","6ee9c2a4":"code","664c3591":"code","93052bb0":"code","bef0693a":"code","08cf761a":"code","6d7964e6":"markdown","aebf180f":"markdown","ee7eb66e":"markdown","c77194d0":"markdown","04acd820":"markdown","a4cfc5c8":"markdown","c7204a71":"markdown","bdd3bf97":"markdown","aa79d33d":"markdown","2032282b":"markdown","ec6adda3":"markdown","228816de":"markdown","87b42984":"markdown","a7b4e81d":"markdown","8a0910bb":"markdown","76e83352":"markdown","a4bc7ed9":"markdown","818ef6d5":"markdown","0eecc697":"markdown","f02a180d":"markdown","f83af3c4":"markdown","ea627bae":"markdown","a506ef65":"markdown","6ea8da14":"markdown","22056175":"markdown","453e9406":"markdown","319b2529":"markdown","25793f6f":"markdown","529ae666":"markdown","e10f1066":"markdown","2168e65c":"markdown","2117d03f":"markdown","04af68f7":"markdown","8a488eef":"markdown","94829978":"markdown","dbbc7d16":"markdown","a79af68a":"markdown","60d58a39":"markdown","0c27cb8b":"markdown","c25262d5":"markdown","c69f89f0":"markdown","9cb644ee":"markdown","adc6eaa5":"markdown","32dd7e46":"markdown"},"source":{"7144f92d":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n%matplotlib inline \n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nimport os\nimport gc\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport time","bd46601d":"DATA_FOLDER = '..\/input\/'\nsales = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\nshops_df = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\ncat_df = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nitems_df = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))","dad6abe4":"print(sales.isnull().sum())\nprint(shops_df.isnull().sum())\nprint(cat_df.isnull().sum())\nprint(items_df.isnull().sum())\nprint(test_df.isnull().sum())","ac3b747d":"sales.head()","3cbbd52f":"sns.boxplot(sales['item_cnt_day'])","6429a855":"sales = sales[sales['item_cnt_day'] < 900]\nsns.boxplot(sales['item_cnt_day'])","a5054470":"sns.boxplot(sales['item_price'])","31b612fb":"sales = sales[(sales['item_price']<100000) & (sales['item_price']>0)]\n# uper, lower = np.percentile(sales['item_price'], [1, 99])\n# sales['item_price'] = np.clip(sales['item_price'], uper, lower)\nsns.boxplot(sales['item_price'])","d35281df":"scaler = MinMaxScaler().fit(sales[['item_price']])\nsales['item_price'] = scaler.transform(sales[['item_price']])","5a66ea32":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales.loc[sales['shop_id'] == 0, 'shop_id'] = 57\ntest_df.loc[test_df['shop_id'] == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales.loc[sales['shop_id'] == 1, 'shop_id'] = 58\ntest_df.loc[test_df['shop_id'] == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales.loc[sales['shop_id'] == 10, 'shop_id'] = 11\ntest_df.loc[test_df['shop_id'] == 10, 'shop_id'] = 11","d2940ee4":"shops_df.head()","168c868f":"shops_df.loc[shops_df['shop_name'] == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops_df['city'] = shops_df['shop_name'].str.split(' ').map(lambda x: x[0])\nshops_df.loc[shops_df['city'] == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops_df['city_code'] = LabelEncoder().fit_transform(shops_df['city'])\nshops_df.loc[shops_df['shop_id'].isin([9,12, 55]), 'city_code'] = 999\nshops_df = shops_df[['shop_id','city_code']]","5b31755e":"cat_df.head()","47abfb96":"cat_df.loc[cat_df['item_category_id'] == 8, 'item_category_name'] = '\u0411\u0438\u043b\u0435\u0442\u044b'\ncat_df.loc[cat_df['item_category_id'] == 26, 'item_category_name'] = '\u0418\u0433\u0440\u044b Android'\ncat_df.loc[cat_df['item_category_id'] == 27, 'item_category_name'] = '\u0418\u0433\u0440\u044b MAC'\ncat_df.loc[cat_df['item_category_id'] == 31, 'item_category_name'] = '\u0418\u0433\u0440\u044b PC'\ncat_df.loc[cat_df['item_category_id'] == 34, 'item_category_name'] = '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b - Live!'\ncat_df.loc[cat_df['item_category_id'] == 36, 'item_category_name'] = '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b - Windows'\ncat_df.loc[cat_df['item_category_id'] == 44, 'item_category_name'] = '\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b - Windows'\ncat_df.loc[cat_df['item_category_id'] == 74, 'item_category_name'] = '\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b - MAC'\n#43 equals 44\ncat_df.drop(cat_df[cat_df['item_category_id'] == 44].index, inplace=True)\n#75 == 76\ncat_df.drop(cat_df[cat_df['item_category_id'] == 76].index, inplace=True)\n#77 == 78\ncat_df.drop(cat_df[cat_df['item_category_id'] == 78].index, inplace=True)\n\nitems_df.loc[items_df['item_category_id'] == 44, 'item_category_id'] = 43\nitems_df.loc[items_df['item_category_id'] == 76, 'item_category_id'] = 75\nitems_df.loc[items_df['item_category_id'] == 78, 'item_category_id'] = 77","f6db0386":"cat_df['split'] = cat_df['item_category_name'].str.split('-')\ncat_df['type'] = cat_df['split'].map(lambda x: x[0].strip())\ncat_df['type_code'] = LabelEncoder().fit_transform(cat_df['type'])\n# if subtype is nan then type\ncat_df['subtype'] = cat_df['split'].map(lambda x: x[1].strip() if len(x) > 1 else 'unknown')\ncat_df['subtype_code'] = LabelEncoder().fit_transform(cat_df['subtype'])\ncat_df = cat_df[['item_category_id','type_code', 'subtype_code']]\n\nitems_df.drop(['item_name'], axis=1, inplace=True)","57c03029":"sales['date_block_num'] = sales['date_block_num'].astype(np.int8)\nsales['shop_id'] = sales['shop_id'].astype(np.int16)\nsales['item_id'] = sales['item_id'].astype(np.int32)\n\ntest_df['date_block_num'] = 34\ntest_df['date_block_num'] = test_df['date_block_num'].astype(np.int8)\ntest_df['shop_id'] = test_df['shop_id'].astype(np.int16)\ntest_df['item_id'] = test_df['item_id'].astype(np.int32)","4aec131a":"print('number of unique items in test: {0}'.format(test_df['item_id'].nunique()))\ntest_df.groupby('shop_id')['item_id'].nunique()\n","16f7226b":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    #create all posible tuples [shop, item, month]. Shop and item that used in particular month\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] ","6514f701":"# Groupby data to get shop-item-month aggregates\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n#it was created 2 layer columns after aggregating. So let's use the last.\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n\n# Join it to the grid\nall_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\nall_data['target'] = all_data['target'].astype(np.float16)","9b53d30f":"#joining test data to all_data\nall_data = pd.concat([all_data, test_df[index_cols]], ignore_index=True, sort=False, keys=index_cols)\nall_data.fillna(0, inplace=True)","711f93ad":"all_data['date_block_num'] = all_data['date_block_num'].astype(np.int8)\nall_data['shop_id'] = all_data['shop_id'].astype(np.int16)\nall_data['item_id'] = all_data['item_id'].astype(np.int32)","bc29831e":"del grid, gb \ngc.collect()","7b3f3ed5":"all_data['target'] = np.clip(all_data['target'], 0, 20)","84e2ba11":"all_data['target'] = all_data['target'].astype(np.float64) # we cannot calculate sum with float16\nsales_per_month = all_data.groupby('date_block_num')['target'].sum()\nall_data['target'] = all_data['target'].astype(np.float16)\nsales_per_month.plot(figsize=(10,8))","9a0634b8":"decembers = [11, 23]\nall_data['is_december'] = all_data['date_block_num'].isin(decembers).astype(np.int8)","7bc63718":"all_data = pd.merge(all_data, items_df, how='left', on='item_id')\nall_data = pd.merge(all_data, cat_df, how='left', on='item_category_id')\nall_data = pd.merge(all_data, shops_df, how='left', on='shop_id')","229ef55b":"all_data['item_category_id'] = all_data['item_category_id'].astype(np.int8)\nall_data['type_code'] = all_data['type_code'].astype(np.int8)\nall_data['subtype_code'] = all_data['subtype_code'].astype(np.int8)\nall_data['city_code'] = all_data['city_code'].astype(np.int16)","9cd3a52d":"del items_df\ndel cat_df\ndel shops_df\ngc.collect()","17cd4972":"#function to create lag features\n\ndef create_lag(df, cols_to_lag, shift_range):\n    print(cols_to_lag)\n    \n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = df[index_cols + cols_to_lag].copy()\n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n\n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_lag else x\n        train_shift = train_shift.rename(columns=foo)\n\n        df = pd.merge(df, train_shift, on=index_cols, how='left')\n\n    del train_shift\n    gc.collect()\n    return df","5bce6909":"mean_enc = all_data.groupby('date_block_num').agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num'])","c6b386af":"all_data = create_lag(all_data, ['date_block_num_enc'], [1, 2, 3, 6, 12])","aecbf612":"ts = time.time()\nmean_enc = all_data.groupby(['date_block_num', 'item_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_item_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'item_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'item_category_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_cat_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'item_category_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_cat_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id','item_category_id'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_type_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id','type_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_shop_subtype_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'shop_id','subtype_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'city_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_city_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'city_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'item_id','city_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_item_city_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'item_id','city_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'type_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_type_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'type_code'])\n\nmean_enc = all_data.groupby(['date_block_num', 'subtype_code']).agg({'target':['mean']})\nmean_enc.columns = ['date_block_num_subtype_enc']\nall_data = pd.merge(all_data, mean_enc, how='left', on=['date_block_num', 'subtype_code'])\n\ndel mean_enc\ngc.collect()\ntime.time() - ts","55aecc5b":"lag_features = ['date_block_num_item_enc', 'date_block_num_shop_enc', 'date_block_num_cat_enc',\n               'date_block_num_shop_cat_enc', 'date_block_num_shop_type_enc', 'date_block_num_shop_subtype_enc',\n               'date_block_num_city_enc', 'date_block_num_item_city_enc', 'date_block_num_type_enc',\n               'date_block_num_subtype_enc']\nall_data = create_lag(all_data, lag_features, [1, 2, 3, 6, 12])","6d98daf8":"all_data.drop(lag_features + ['date_block_num_enc'], inplace=True, axis=1)","bfb5792f":"#find out shop mean price for all time and shop mean price for every month\nts = time.time()\n\ngroup = sales.groupby(['shop_id']).agg({'item_price':['mean']})\ngroup.columns = ['shop_price_mean']\n\n#another way to get rid of 2 layer columns\ngroup.reset_index(inplace=True)\nall_data = pd.merge(all_data, group, how='left', on=['shop_id'])\nall_data['shop_price_mean'] = all_data['shop_price_mean'].astype(np.float32)\n\ngroup = sales.groupby(['date_block_num', 'shop_id']).agg({'item_price':['mean']})\ngroup.columns = ['date_shop_price_mean']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, how='left', on=['date_block_num', 'shop_id'])\nall_data['date_shop_price_mean'] = all_data['date_shop_price_mean'].astype(np.float16)\n\n# create lags \nlags = [1, 2, 3, 4, 5, 6]\nall_data = create_lag(all_data, ['date_shop_price_mean'],lags)\ntime.time() - ts","db786749":"# find out how much shop mean month price different from the 'all time' shop mean price , and normilize it.\nts = time.time()\n\nfor i in lags:\n    all_data['delta_shop_price_lag_'+str(i)] = \\\n        (all_data['date_shop_price_mean_lag_'+str(i)] - all_data['shop_price_mean']) \/ all_data['shop_price_mean']\n\n\ndel group\ngc.collect()\n\ndelta_cols = [col  for col in all_data.columns.values if col.startswith('delta_shop_price_lag_')]\ndate_item_price_cols = [col  for col in all_data.columns.values if col.startswith('date_shop_price_mean_lag_')]\n\n#fillna(method='backfill') doesn't support float16, so change it to float32\nall_data[delta_cols] = all_data[delta_cols].astype(np.float32) \n\n#get first non nan value in a row.\nall_data['delta_shop_price_lag'] = all_data[delta_cols].fillna(method='backfill', axis=1).iloc[:, 0]\n\n#fill it with zeros if it did not found non nan\nall_data['delta_shop_price_lag'] = all_data['delta_shop_price_lag'].fillna(0).astype(np.float16)\n\n#and remove feature we used to calclulate this\ncols_to_drop = delta_cols + date_item_price_cols +['date_shop_price_mean', 'shop_price_mean']\nall_data.drop(cols_to_drop, axis=1, inplace=True)\ntime.time()-ts","cc3c5253":"ts = time.time()\ngroup = sales.groupby(['item_id']).agg({'item_price':['mean']})\ngroup.columns = ['item_price_mean']\ngroup.reset_index(inplace=True)\nall_data = pd.merge(all_data, group, how='left', on=['item_id'])\nall_data['item_price_mean'] = all_data['item_price_mean'].astype(np.float16)\n\ngroup = sales.groupby(['date_block_num', 'item_id']).agg({'item_price':['mean']})\ngroup.columns = ['date_item_price_mean']\ngroup.reset_index(inplace=True)\n\nall_data = pd.merge(all_data, group, how='left', on=['date_block_num', 'item_id'])\nall_data['date_item_price_mean'] = all_data['date_item_price_mean'].astype(np.float16)\nlags = [1,2,3,4,5,6]\n\nall_data = create_lag(all_data, ['date_item_price_mean'],lags)\n\ndel group\ngc.collect()\n\nfor i in lags:\n    all_data['delta_item_price_lag_'+str(i)] = \\\n        (all_data['date_item_price_mean_lag_'+str(i)] - all_data['item_price_mean']) \/ all_data['item_price_mean']\n\ndelta_cols = [col  for col in all_data.columns.values if col.startswith('delta_item_price_lag_')]\ndate_item_price_cols = [col  for col in all_data.columns.values if col.startswith('date_item_price_mean_lag_')]\n\nall_data[delta_cols] = all_data[delta_cols].astype(np.float32) \nall_data['delta_item_price_lag'] = all_data[delta_cols].fillna(method='backfill', axis=1).iloc[:, 0]\n\nall_data['delta_item_price_lag'] = all_data['delta_item_price_lag'].fillna(0).astype(np.float16)\ncols_to_drop = delta_cols + date_item_price_cols +['date_item_price_mean', 'item_price_mean']\nall_data.drop(cols_to_drop, axis=1, inplace=True)\nall_data['delta_item_price_lag'].head()\n\ntime.time()-ts","218f36fc":"all_data['month'] = all_data['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nall_data['days'] = all_data['month'].map(days).astype(np.int8)","f9054768":"ts = time.time()\ncache = {} # key is 'item_id shop_id', values is date_block_num\nall_data['item_shop_last_sale'] = -1\nall_data['item_shop_last_sale'] = all_data['item_shop_last_sale'].astype(np.int8)\nfor idx, row in all_data.iterrows():    \n    key = str(row['item_id'])+' '+str(row['shop_id'])\n    if key not in cache:\n        if row['target']!=0:\n            cache[key] = row['date_block_num']\n    else:\n        last_date_block_num = cache[key]\n        all_data.at[idx, 'item_shop_last_sale'] = row['date_block_num'] - last_date_block_num\n        cache[key] = row['date_block_num']         \n\ndel cache\ngc.collect()        \ntime.time() - ts","cb4c85c9":"ts = time.time()\ncache = {}\nall_data['item_last_sale'] = -1\nall_data['item_last_sale'] = all_data['item_last_sale'].astype(np.int8)\nfor idx, row in all_data.iterrows():    \n    key = row['item_id']\n    if key not in cache:\n        if row['target']!=0:\n            cache[key] = row['date_block_num']\n    else:\n        last_date_block_num = cache[key]\n        if row['date_block_num']>last_date_block_num:\n            all_data.at[idx, 'item_last_sale'] = row['date_block_num'] - last_date_block_num\n            cache[key] = row['date_block_num']   \n\n            \ndel cache\ngc.collect()\ntime.time() - ts","6ee9c2a4":"ts = time.time()\nall_data['item_shop_first_sale'] = all_data['date_block_num'] - all_data.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nall_data['item_first_sale'] = all_data['date_block_num'] - all_data.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","664c3591":"all_data = all_data[all_data['date_block_num'] > 11]","93052bb0":"all_data.fillna(0, inplace=True, axis=1)","bef0693a":"all_data.info()","08cf761a":"all_data.to_pickle('all_data.pkl')","6d7964e6":"Create number of days in month features","aebf180f":"**Extra features**","ee7eb66e":"Join information about items, shops and categories","c77194d0":"Make another features in this way","04acd820":"**Useful note**\n\nSince we have a lot of rows and in future we will have new features we will use more and more RAM to hold this data. So I encourage you to manage memory very carefully. For example we have column date_block_num and it has type int64. But range of values are only from 0 to 34, so we simply can change type to int8 for saving memory. I have some cases when notebook kernel was died because of run out of memory. So downcasting types is very important when you have a lot of data.","a4cfc5c8":"**<font size=\"4\">Shops and category preprocessing<\/font>**","c7204a71":"Compute target value. This value we will be predicted for test data.","bdd3bf97":"Number of month since the last sale for shop\/item and for just item. Stay calm, this will take some time to calculate.","aa79d33d":"![](http:\/\/)Here we can see two peaks. This peaks related to 11 and 23 monthes. It is december. There are much more sales before New Year. So it will be reasonable to include this information in our set. ","2032282b":"First we import everything we need","ec6adda3":"Remove old data","228816de":"Since we have competition requirement to clip our prediction from 0 to 20. Then let's make it for train data. That's because I didn't make scaling item_cnt_day in previous steps","87b42984":"Also it is a good practice to scale numeric features (gradient descent will converge faster). In this case I will use MinMax scaler, so price range will be from 0 to 1.\nI will not scale item_cnt_day. I will deal with it later.","a7b4e81d":"There is no column that describes month sales for specific item\/shop. But there is only column that describe daily sales. So in future we need to calculate how much items was sold in month with respect to shops. But for now let make some analysis.","8a0910bb":"and create lags for this feature for previous month, month before previous and further.","76e83352":"**<font size=4>Lag features and mean encoding<\/font>**","a4bc7ed9":"And create lags features for this mean encodings","818ef6d5":"Monthes since first sale for shop\/item and for item only","0eecc697":"At this point I will introduce **mean encoding** features. But it will be adapted for time series problem. Instead of simply group data by some category and calculate mean of target value and then replace it with category I will do it with respect to month.","f02a180d":"Price trend for the last six months for shops.","f83af3c4":"So now we need compute our target values (month sales) for item\/shop. But first let's take a look on our test data. This can help understand in what way we need to construct train data that they would be the same. Let's find out how many unique items in test and how many unique items per shop","ea627bae":"Make the same calculation for items","a506ef65":"Categories as shops also have duplicates. Get rid of them. Don't forget to alter item_df because it contains column with category id. Some of category names look like \"blabla (\u0426\u044b\u0444\u0440\u0430)\" and \"blabla\". There are equal.","6ea8da14":"Let's check missing values","22056175":"**<font size=4>Dealing with outliers<\/font>**\n\nFirst of all we need to check our data for outliers. Seaborn will help us. Boxplot is a good tool to use for this purpose","453e9406":"this also help with fighting RAM issues. Don't forget to delete havy objects that you don't need anymore.","319b2529":"**<font size=4>Generating target<\/font>**","25793f6f":"You can see that we have 2 poits that are very far from another poits. So we consider them as outliers and can remove them.","529ae666":"Save it to pickle. I chose pickle instead of csv because pkl format loads faster.","e10f1066":"And After creating lags we have a lot of nans. So let's fill it zeros","2168e65c":"**<font size=4>Create super interesting features<\/font>**","2117d03f":"Do the same for price","04af68f7":"Now see how categories data looks like","8a488eef":"\nLet's get acquainted with the shops. \nSeveral shops was duplicated. So remove them from our train and test","94829978":"This is where the fun begins. Since it is time series based problem it is recomended to add lag features. Lag features it is feature that describe values in previous points of time. For example one month before, two, three or six.","dbbc7d16":"Most of the category names has stucte \"type - subtype\". Let's extract it and apply label encoding","a79af68a":"Compute some helpful features","60d58a39":"As you can see we generate quite a lot of new features and it's is almost 900 MB. So at this point our jorney of eda and feature engineering is finished but new road of building a predictive model is opening.","0c27cb8b":"Let's see how our target values are changing with respcect to time","c25262d5":"As you can see every shop has the same items. So let use this knowledge for building our train data","c69f89f0":"Remove mean encoding for current month since for test data it is unknown.","9cb644ee":"This notebook describes how to make basic eda, data preparation and generating features for predicting month sales. A lot of inspiration and good tips and tricks I got from https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n\nPipeline:\n* Check missing values\n* Handle outliers\n* Cleaning shops\/categories\n* Define our target\n* Lags and mean encodings\n* Price trending features\n* Extra features","adc6eaa5":"Let's make some cleaning in shops_df. I noticed that first word in shop_name means city name. Extract it. And make label encoding since city is a categorical type of feature.\nNote that shops with id 9, 12, 55 have not city so I encoded it with 999 which means unknown.","32dd7e46":"As you can see there is no missing values so we can move forward and start to get acquainted with the data. Let's print our sales"}}