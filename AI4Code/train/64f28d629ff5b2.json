{"cell_type":{"aae2eec6":"code","695b6f64":"code","70052d0e":"code","b286c6ef":"code","b38efd97":"code","747dc56a":"code","5ec460a7":"code","54020203":"code","d4b9ef38":"code","1865e855":"code","bf8482f8":"code","b7e7ecc4":"code","bd20f351":"code","34f07b73":"code","3f5b8f03":"code","62787902":"code","98fee211":"code","96d0fc34":"code","99d68037":"code","39105f48":"code","262772e9":"code","b16aa165":"code","56860a84":"code","870b109c":"code","0bd58e99":"code","38b2d2b8":"code","19bcc2d6":"code","19624be8":"code","148cde65":"code","9662a407":"code","6ee66778":"code","43e4e6c1":"code","cfed561b":"markdown"},"source":{"aae2eec6":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata= pd.read_csv(\"..\/input\/trainandtest\/trainandtest.csv\")\npd.pandas.set_option('display.max_columns',None)\ndata.shape","695b6f64":"# calculate the correlation matrix\ncorr = data.corr()\n\n# display the correlation matrix\ndisplay(corr)\n\n# plot the correlation heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, cmap='RdBu')","70052d0e":"## First lets handle Categorical features which are missing\nfeatures_nan=[feature for feature in data.columns if data[feature].isnull().sum()>1 and data[feature].dtypes=='O']\n\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(data[feature].isnull().mean(),4)))","b286c6ef":"## taking care of missing values\ndata['Alley']=data['FireplaceQu'].fillna('MissingAlley')\ndata['FireplaceQu']=data['FireplaceQu'].fillna('MissingFireplace')\ndata['Fence']=data['Fence'].fillna('MissingFENCE')\ndata['PoolQC']=data['PoolQC'].fillna('MissingPOOL')\ndata['MiscFeature']=data['MiscFeature'].fillna('MissingMISC')","b38efd97":"## Replace missing value with a new label for train\ndef replace_cat_feature(data,features_nan):\n    dataset=data.copy()\n    dataset[features_nan]=dataset[features_nan].fillna('Missing')\n    return dataset\n\ndata=replace_cat_feature(data,features_nan)\n\ndata[features_nan].isnull().sum()","747dc56a":"## Checking numerical missing values\nnumerical_with_nan=[feature for feature in data.columns if data[feature].isnull().sum()>1 and data[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(data[feature].isnull().mean(),4)))","5ec460a7":"## Replacing the numerical Missing Values \n\nfor feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=data[feature].median()\n    \n    ## create a new feature to capture nan values\n    #data[feature+'nan']=np.where(data[feature].isnull(),1,0)\n    data[feature].fillna(median_value,inplace=True)\n    \ndata[numerical_with_nan].isnull().sum()\n","54020203":"## (Date Time Variables)\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    data[feature]=data['YrSold']-data[feature]\n    ","d4b9ef38":"### log transorm for outliers\nimport numpy as np\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n\nfor feature in num_features:\n    data[feature]=np.log(data[feature])","1865e855":"## making a new categorical variable for variables occuring rarely\ncategorical_features=[feature for feature in data.columns if data[feature].dtype=='O']\n\nfor feature in categorical_features:\n    temp=data.groupby(feature)['SalePrice'].count()\/len(data)\n    temp_df=temp[temp>0.01].index\n    data[feature]=np.where(data[feature].isin(temp_df),data[feature],'Rare_var')","bf8482f8":"#one hot enceoding\nfor feature in categorical_features:\n    dummy = pd.get_dummies(data[feature])\n    data=pd.concat([data,dummy],axis=1)\n    data=data.drop(feature,axis=1)","b7e7ecc4":"## feature scaling \nfeature_scale=[feature for feature in data if feature not in ['Id','SalePrice']]\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(data[feature_scale])\n","bd20f351":"## removing duplicate columns if any\ndef remove_dup_columns(data):\n     keep_names = set()\n     keep_icols = list()\n     for icol, name in enumerate(data.columns):\n          if name not in keep_names:\n               keep_names.add(name)\n               keep_icols.append(icol)\n     return data.iloc[:, keep_icols]","34f07b73":"data = remove_dup_columns(data)","3f5b8f03":"## set up a customer filter to differentiate  test and train (for test columns, sales is 12345678)\n## wanted to do eda and feature engineering on train and test together\nfiltertest = data.SalePrice== 12345678\nfiltertrain = data.SalePrice!= 12345678","62787902":"## setting up test and train\ntest=data[filtertest]\ntrain=data[filtertrain]\nx_train = train.drop(['Id','SalePrice'],axis=1)\ny_train = train['SalePrice']\nx_test=test.drop(['Id','SalePrice'],axis=1)","98fee211":"## FEATURE SELECTION","96d0fc34":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","99d68037":"## finding alpha for lasso\nfrom sklearn.linear_model import LassoCV\nfrom yellowbrick.regressor import AlphaSelection\nfrom yellowbrick.datasets import load_concrete\n# Create a list of alphas to cross-validate against\n#alphas = np.logspace(-10,1,50,100)\nalphas=(1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25,25,26,26,27,27,28,28,29,29,30,30,31,31,32,32,33,33,34,34,35,35,36,36,37,37,38,38,39,39,40,40,41,41,42,42,43,43,44,44,45,45,46,46,47,47,48,48,49,49,50,50,51,51,52,52,53,53,54,54,55,55,56,56,57,57,58,58,59,59,60,60,61,61,62,62,63,63,64,64,65,65,66,66,67,67,68,68,69,69,70,70,71,71,72,72,73,73,74,74,75,75,76,76,77,77,78,78,79,79,80,80,81,81,82,82,83,83,84,84,85,85,86,86,87,87,88,88,89,89,90,90,91,91,92,92,93,93,94,94,95,95,96,96,97,97,98,98,99,99,100,100,101,101,102,102,103,103,104,104,105,105,106,106,107,107,108,108,109,109,110,110,111,111,112,112,113,113,114,114,115,115,116,116,117,117,118,118,119)\n# Instantiate the linear model and visualizer\nmodel = LassoCV(alphas=alphas,cv=6,max_iter=1500)\nvisualizer = AlphaSelection(model)\nvisualizer.fit(x_train,y_train)\nvisualizer.show()","39105f48":"sel_= SelectFromModel(Lasso(alpha=200)) #\nsel_.fit(x_train, y_train)","262772e9":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = x_train.columns[(sel_.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((x_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\n\nprint('features with coefficients shrank to zero: {}'.format(np.sum(sel_.estimator_.coef_ == 0)))","b16aa165":"x_train = x_train[selected_feat]\nx_test= x_test[selected_feat]","56860a84":"###hyper parameter tuning(naivebayes)","870b109c":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","0bd58e99":"import  xgboost as xgb\nimport  xgboost as xgb\nfrom hyperopt import hp, tpe, fmin\nfrom sklearn.model_selection import cross_val_score\nestimator = xgb.XGBRegressor()","38b2d2b8":"parameters = {\n    'max_depth': range (2, 10, 1),\n    'learning_rate': [0.07,0.1,0.5,1]\n}","19bcc2d6":"space = {'n_estimators':hp.quniform('n_estimators', 1000, 4000, 100),\n         'gamma':hp.uniform('gamma', 0.01, 0.05),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.025),\n         'max_depth':hp.quniform('max_depth', 3,7,1),\n         'subsample':hp.uniform('subsample', 0.60, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.60, 0.98),\n         'colsample_bylevel':hp.uniform('colsample_bylevel', 0.60, 0.98),\n         'reg_lambda': hp.uniform('reg_lambda', 1, 20)\n        }","19624be8":"def objective(params):\n    params= {'n_estimators': int(params['n_estimators']),\n             'gamma': params['gamma'],\n             'learning_rate': params['learning_rate'],\n             'max_depth': int(params['max_depth']),\n             'subsample': params['subsample'],\n             'colsample_bytree': params['colsample_bytree'],\n             'colsample_bylevel': params['colsample_bylevel'],\n             'reg_lambda': params['reg_lambda']}\n    xb_a= xgb.XGBRegressor(**params)\n    score = cross_val_score(xb_a,x_train,y_train,scoring='neg_mean_squared_error', cv=7, n_jobs=-1).mean()\n    return -score","148cde65":"best = fmin(fn= objective, space= space, max_evals=20, rstate=np.random.RandomState(1), algo=tpe.suggest)","9662a407":"print(best)","6ee66778":"## using parameters selected\nxb_b = xgb.XGBRegressor(random_state=0,\n                        n_estimators=int(best['n_estimators']), \n                        colsample_bytree= best['colsample_bytree'],\n                        gamma= best['gamma'],\n                        learning_rate= best['learning_rate'],\n                        max_depth= int(best['max_depth']),\n                        subsample= best['subsample'],\n                        colsample_bylevel= best['colsample_bylevel'],\n                        reg_lambda= best['reg_lambda']\n                       )\n\nxb_b.fit(x_train, y_train)","43e4e6c1":"y_pred= xb_b.predict(x_test)\ny_pred= pd.DataFrame(y_pred)\n#y_pred_a=y_pred.to_csv('C:\\\\Users\\\\Desktop\\\\Kaggle\\\\House Pred\\\\finalsubmission.csv',header= True)","cfed561b":"**Step 1 is combining test and train data so that EDS and feature engineering can be done together **"}}