{"cell_type":{"b278506a":"code","b043ddac":"code","545e2183":"code","6a06de9b":"code","764f45c9":"code","8308cc87":"code","3d5f2070":"code","2181f697":"code","5d691a43":"code","ef776f79":"code","28d834ab":"code","831f0772":"code","6ba81e05":"code","54dbd3ac":"code","d005d418":"code","82cd20e7":"code","10195161":"code","0f569614":"code","c8e6d3a1":"code","980ebde1":"code","d5b9239d":"code","6d81b042":"code","99e2f900":"code","e404b9bc":"code","452e00ff":"code","8f410e77":"code","c6613fad":"code","5d7edc32":"code","7b1d3530":"code","367d1ec6":"code","c8b5a81a":"code","9694fd4a":"code","1f14aa12":"code","8e086d1c":"code","6a442a64":"code","6a875fe5":"code","d987aa01":"code","99d0044e":"code","d421cecd":"markdown","5997577f":"markdown","9b64ae7a":"markdown","81a2e2a6":"markdown","12c3237e":"markdown","063fe638":"markdown","35ad251a":"markdown","b8cd686a":"markdown","09468ac4":"markdown","afe1a00f":"markdown","03d1ac0e":"markdown","e2704103":"markdown","a50d047f":"markdown","5a8c87ba":"markdown","0afe5888":"markdown","3e9c14cd":"markdown","3113c3e2":"markdown"},"source":{"b278506a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b043ddac":"# import visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns","545e2183":"# read data\ndata = pd.read_csv(os.path.join(dirname, filename))","6a06de9b":"# describe the data in general terms\ndata.describe()","764f45c9":"# get some info\ndata.info()","8308cc87":"# Let's start by looking at the distribution of alive vs. deceased.\nn_alive = len(data[data[\"DEATH_EVENT\"]==0])\nn_deceased = len(data[data[\"DEATH_EVENT\"]==1])\nplt.figure(figsize=(10, 10))\nplt.pie((n_alive, n_deceased), labels=(\"alive\", \"deceased\"), autopct='%1.2f%%')\nplt.show()","3d5f2070":"# Let's plot the number of deceased vs. alive people sorted by gender\nsns.countplot(x=\"DEATH_EVENT\", data=data, hue=\"sex\")","2181f697":"# number of male vs. female in the dataset\nsns.countplot(x=\"sex\", data=data)","5d691a43":"# Let's plot the correlation degree wrt the death event\ndata.corr()[\"DEATH_EVENT\"].sort_values().plot(kind=\"bar\")","ef776f79":"# final DataFrame without uncorrelated features\ndropped = [\"sex\", \"time\", \"smoking\", \"diabetes\"]\ndata_notime = data.drop(\"time\", axis=1)\nfinal_data = data.drop(dropped, axis=1)\nfinal_data","28d834ab":"# Let's look at the effet of high blood pressure more precisely in the case of the deceased population\ndeceased = final_data[final_data[\"DEATH_EVENT\"]==1]\nalive = final_data[final_data[\"DEATH_EVENT\"]==0]\nsns.boxplot(data=deceased, x=\"high_blood_pressure\", y=\"age\")\nplt.show()","831f0772":"# Let's plot the age distribution for the deceased and alive populations\nsns.distplot(alive[\"age\"], label=\"alive\")\nsns.distplot(deceased[\"age\"], label=\"deceased\")\nplt.legend()\nplt.show()","6ba81e05":"# Finally, let's group by status and look at the mean values of the different features\nfinal_data.groupby(\"DEATH_EVENT\").mean()","54dbd3ac":"# Let's first preprocess the data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","d005d418":"# X is the features, y is the target class\nX = final_data.drop(\"DEATH_EVENT\", axis=1)\ny = final_data[\"DEATH_EVENT\"]","82cd20e7":"# We split the dataset into training (80%) and testing (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","10195161":"# The features are scaled between 0 and 1\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","0f569614":"# We'll first try with the kmeans algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nkNeighbors = KNeighborsClassifier()\nkNeighbors.fit(X_train, y_train)\ntrain_pred_kneigh = kNeighbors.predict(X_train)\ntest_pred_kneigh = kNeighbors.predict(X_test)","c8e6d3a1":"# Let's print some metrics\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix\nprint(f\"KneighborsClassifier:\\nTest:\\n{classification_report(y_test, test_pred_kneigh)}\\nTrain:\\n{classification_report(y_train, train_pred_kneigh)}\")\nplt.figure(figsize=(12, 12))\nplot_confusion_matrix(kNeighbors, X_test, y_test, cmap=\"coolwarm\")\nplt.show()","980ebde1":"from sklearn.linear_model import LogisticRegression\nLogRegression = LogisticRegression()\nLogRegression.fit(X_train, y_train)\ntrain_pred_logreg = LogRegression.predict(X_train)\ntest_pred_logreg = LogRegression.predict(X_test)","d5b9239d":"# Let's print some metrics\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\nprint(f\"Logistic Regression:\\nTest:\\n{classification_report(y_test, test_pred_logreg)}\\nTrain:\\n{classification_report(y_train, train_pred_logreg)}\")\nplt.figure(figsize=(12, 12))\nplot_confusion_matrix(LogRegression, X_test, y_test, cmap=\"coolwarm\")\nplt.show()","6d81b042":"# we prediction probabilities now and manually change the threshold\nprobabilities = pd.DataFrame(LogRegression.predict_proba(X_test))\nthresholds = np.arange(0.1, 0.91, 0.02)\naccuracies = np.array([])\nf1scores = np.array([])\nprecisions = np.array([])\nrecalls = np.array([])\n\nfor thresh in thresholds:\n    pred_logreg_newthresh = np.empty(len(probabilities), dtype=float)\n    for i in range(len(probabilities)):\n        if probabilities.iloc[i,0] <= thresh:\n            pred_logreg_newthresh[i] = 1\n        else:\n            pred_logreg_newthresh[i] = 0\n    accuracies = np.append(accuracies, accuracy_score(y_test, pred_logreg_newthresh))\n    f1scores = np.append(f1scores, f1_score(y_test, pred_logreg_newthresh))\n    precisions = np.append(precisions, precision_score(y_test, pred_logreg_newthresh))\n    recalls = np.append(recalls, recall_score(y_test, pred_logreg_newthresh))\n# DataFrame containing accuracies, f1scores and precisions for these different thresholds\nd = {\"thresholds\": thresholds, \"accuracies\": accuracies, \"f1scores\": f1scores, \"precision\": precisions, \"recall\": recalls}\ndf = pd.DataFrame(d)*100","99e2f900":"# plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(df.thresholds, df.accuracies, color=\"blue\", label=\"accuracy\")\nplt.scatter(df.thresholds, df.f1scores, color=\"orange\", label=\"f1 score\")\nplt.scatter(df.thresholds, df.precision, color=\"green\", label=\"precision\")\nplt.scatter(df.thresholds, df.recall, color=\"red\", label=\"recall\")\nplt.xlabel(\"probability threshold [%]\")\nplt.ylabel(\"metric [%]\")\nplt.axvline(x=50, color=\"black\", linestyle=\"--\", linewidth=0.5, zorder=-34)\nplt.legend()\nplt.show()","e404b9bc":"probabilities = pd.DataFrame(LogRegression.predict_proba(X_test))\npred_logreg_bestthresh = np.empty(len(probabilities), dtype=float)\nthreshold = 0.55\nfor i in range(len(probabilities)):\n    if probabilities.iloc[i,0] <= threshold:\n        pred_logreg_bestthresh[i] = 1\n    else:\n        pred_logreg_bestthresh[i] = 0\nprint(confusion_matrix(y_test, pred_logreg_bestthresh))\nprint(f\"Accuracy score: {accuracy_score(y_test, pred_logreg_bestthresh)*100:.2f}%\")\nprint(f\"f1 score: {f1_score(y_test, pred_logreg_bestthresh)*100:.2f}%\")","452e00ff":"# We use tensforflow with the keras API\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","8f410e77":"# Sequential neural network that performs classification\nnn = Sequential()\nnn.add(Dense(units=8, activation=\"relu\"))\nnn.add(Dropout(0.3))\nnn.add(Dense(units=16, activation=\"relu\"))\nnn.add(Dropout(0.3))\nnn.add(Dense(units=32, activation=\"relu\"))\nnn.add(Dropout(0.3))\nnn.add(Dense(units=16, activation=\"relu\"))\nnn.add(Dropout(0.3))\nnn.add(Dense(units=8, activation=\"relu\"))\nnn.add(Dropout(0.3))\nnn.add(Dense(units=1, activation=\"sigmoid\"))\nnn.compile(optimizer=\"adam\", metrics=[\"acc\"], loss=\"binary_crossentropy\")","c6613fad":"# Early stop if accuracy does not improve over 10 epochs\nearly_stop = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=15)","5d7edc32":"nn.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stop])","7b1d3530":"# plot the metrics\nmetrics = pd.DataFrame(nn.history.history)\nmetrics.plot()\nplt.show()","367d1ec6":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import GenericUnivariateSelect\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","c8b5a81a":"X = final_data.drop(\"DEATH_EVENT\", axis=1)\ny = final_data[\"DEATH_EVENT\"]","9694fd4a":"pipeline = Pipeline([(\"feature_selec\",\"passthrough\"), (\"clf\", LogisticRegression())])","1f14aa12":"grid = [{\"feature_selec\": [GenericUnivariateSelect()],\n         \"feature_selec__mode\": [\"percentile\"],\n        \"feature_selec__param\": np.arange(10, 101, 10),\n        \"clf\": [RandomForestClassifier(), DecisionTreeClassifier()],\n        \"clf__min_samples_split\": np.arange(2, 8, 2)\n        },\n        {\"feature_selec\": [PCA()],\n        \"feature_selec__n_components\": np.arange(2, 6, 1),\n        \"clf\": [RandomForestClassifier(), DecisionTreeClassifier()],\n        \"clf__min_samples_split\": np.arange(2, 8, 2)\n        }]\nmodels = GridSearchCV(pipeline, grid, scoring=\"accuracy\", cv=5, return_train_score=True)","8e086d1c":"models_results = models.fit(X, y)","6a442a64":"final_results = pd.DataFrame(models_results.cv_results_)","6a875fe5":"final_results.sort_values(by=\"rank_test_score\")","d987aa01":"models_results.best_params_","99d0044e":"models_results.best_score_","d421cecd":"There's about one third of the cases that resulted in death. The distribution is not completely skewed towards alive nor dead.","5997577f":"# 2. Model","9b64ae7a":"As we suspected, gender is not correlated with the death event, so it's probably not super relevant to use it as a feature for our machine learning model. Diabetes also looks like it's not an important factor. We will drop sex, diabetes and smoking based on this correlation bar plot. We will also drop time, as it's irrelevant for future pre-diagnosis (time is set by the death_event, not the other way around!). ","81a2e2a6":"The data seems clean, nothing is missing and everything is either integer or float; no need to create dummy variables and the preprocessing seems rapid. Still need to select our features carefully!","12c3237e":"Logically, high blood pressure is an aggravating factor and leads to deaths at a younger median age, while broadening the age range aswell.","063fe638":"The kneighbors classifier has a decent degree of accuracy (70 to 78% on test\/training sets) but we can surely do better, epecially on the false negative (which we want to minimize here!). Let's try a linear regression algorithm.","35ad251a":"### This data exploration showed that the remaining features (8 in total) seem to be of relative importance to determine wether or not a patient will suffer heart failure. Let's now build a model that learns how to predict this outcome!","b8cd686a":"Women are more present in both categories, which indicates that there's more women in the dataset. Distribution seems similar between deceased and alive, which suggests gender is not super important. A correlation analysis might show that more precisely.","09468ac4":"# 1. Data exploration","afe1a00f":"Best results show more than 90% accuracy on train set & 76% on test set.","03d1ac0e":"This also points out that people prone to suffer heart failure have a tendency to have higher levels of ceatinine_phosphokinase and serum_creatinine.","e2704103":"## 3. Deep Learning - Neural Network","a50d047f":"Deep Learning doesn't really help here, maybe because we lack more interesting features that would help better estimating chances of heart failure.","5a8c87ba":"### Although while considering time as a feature models can reach accuracies of 90-95%, without it the few different models tested here have reached max accuracies of about 70 to 80%. Let's do automatic feature selection\/dimensionality reduction + hyperparameter sweep-up to increase accuracy!","0afe5888":"This distribution plot seems to show that older people are more prone to suffer heart failure, typically at age > 65-70, while still being at a real risk after 40.","3e9c14cd":"We want to avoid false negatives. I think it's always better to have false positives when it comes to medical diagnosis. We can trade between recall and precision by changing the threshold in order to minimize false negatives. The Logistic Regression model is far from perfect, and adapting the threshold is a limiter approach. Another algorithm might be more suited fo this problem, or even Deep Learning.","3113c3e2":"Algorithm is a bit better, there's less false positive, but still too much false negative. A possible solution would be to decrase the probability threshold. Let's try to evaluate the different metrics in function of the threshold parameter."}}