{"cell_type":{"39afa0fc":"code","7692a704":"code","d9ce1bdf":"code","9707892d":"code","9404b5a3":"code","fe9b1708":"code","82ea49ac":"code","d5e5927c":"code","2529df80":"code","cb4117ef":"markdown","0a4c13e5":"markdown","cd0f392b":"markdown","9793f921":"markdown","35e0ce65":"markdown","9c185427":"markdown","cab8ed9d":"markdown","48038876":"markdown","c074699e":"markdown"},"source":{"39afa0fc":"import numpy as np\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)","7692a704":"# load dataset\ndataframe = read_csv(\"..\/input\/sonar.all-data.csv\", header=None)\ndataset = dataframe.values\n# split into input and output variables\nX = dataset[:,0:60].astype(float)\nY = dataset[:,60]\ndataframe.head()\n","d9ce1bdf":"#one hot encode the targets\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)","9707892d":"encoded_Y","9404b5a3":"# baseline model\ndef create_baseline():\n    # create model\n    model = Sequential()\n    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","fe9b1708":"# evaluate model with standardized dataset\nestimator = KerasClassifier(build_fn=create_baseline, epochs=10, batch_size=5, verbose=0)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, encoded_Y, cv=kfold)\nprint(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n","82ea49ac":"# evaluate baseline model with standardized dataset\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=10,batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n","d5e5927c":"# baseline model\ndef create_smaller():\n    # create model\n    model = Sequential()\n    model.add(Dense(30, input_dim=60, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\n# evaluate baseline model with standardized dataset\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=10,batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","2529df80":"# larger model\ndef create_larger():\n    # create model\n    model = Sequential()\n    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=10, batch_size=5,verbose=0)))\npipeline = Pipeline(estimators)\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\nprint(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","cb4117ef":"Standardisation has produced a small lift in performance","0a4c13e5":"## Keras Binary Classification\n\nUse Keras with the standard sonar dataset.\n\nDataset describes\nsonar chirp returns bouncing off different surfaces. The 60 input variables are the strength of\nthe returns at different angles. It is a binary classification problem that requires a model to\ndifferentiate rocks from metal cylinders.\nAll of the variables are continuous and generally in the\nrange of 0 to 1. The output variable is a string M for mine and R for rock, which will need to be\nconverted to integers 1 and 0. The dataset contains 208 observations.","cd0f392b":"#### Try a larger network\nAdd one new layer to the network - another hidden layer with 30 neurons:\n60 inputs --> [60 -->30] --> 1 output\nGive the network the opportunity to model all the input variables before being bottlenecked","9793f921":"Larger network sees increase in performance ","35e0ce65":"If you liked this kernel then please upvote","9c185427":"### Data Preperation\nStandardize the data - rescale such that the mean value for each attribute is 0 and the standard\ndeviation is 1. This preserves Gaussian and Gaussian-like distributions whilst normalizing the\ncentral tendencies for each attribute.\n\nRather than performing the standardization on the entire dataset, it is\ngood practice to train the standardization procedure on the training data within the pass of a\ncross-validation run and to use the trained standardization instance to prepare the unseen test makes standardization a step in model preparation in the cross-validation process\nand it prevents the algorithm having knowledge of unseen data during evaluation, knowledge\nthat might be passed from the data preparation scheme like a crisper distribution\nCan use the Pipeline class to define a StandardScaler followed by neural network model.","cab8ed9d":"### Tuning of Network\n#### Try a smaller network\nTake baseline model with 60 neurons above and reduce to 30 - force type of feature extraction by restricting the representational space in the first hidden layer.","48038876":"Small increase in performance with smaller network, lets try making bigger network","c074699e":"### Define and evaluate the model\nThe weights are initialized using a small Gaussian random number. The Rectifier activation\nfunction is used. The output layer contains a single neuron in order to make predictions. Use the sigmoid activation function in order to produce a probability output in the range of\n0 to 1 that can easily and automatically be converted to crisp class values. Use the logarithmic loss function (binary crossentropy) during training, the preferred loss\nfunction for binary classification problems."}}