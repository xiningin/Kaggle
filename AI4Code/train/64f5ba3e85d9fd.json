{"cell_type":{"b6dee4f3":"code","f328941b":"code","f7ace17c":"code","ad39156c":"code","1451c4ea":"code","a7787c2a":"code","30d11b88":"code","68e8392e":"code","06bcc142":"code","c5bccc24":"code","fb27fecb":"code","0f476fd7":"code","fdb744f7":"code","7d5f022c":"code","175f6d99":"code","a156c4f4":"code","19af1f10":"code","fa479704":"code","3b802fbe":"code","6450ca0c":"code","2ed5a41b":"code","3f6837c0":"code","b91b8018":"code","005b1122":"code","f49adfa4":"code","e624d83b":"code","27657e62":"code","f42e8501":"code","3420c4bf":"code","b46738c7":"code","bb6f30fe":"code","43164f57":"code","617370f3":"code","ca0b1c0c":"code","f122e1ab":"code","bd1a7326":"code","20b58c9c":"code","7bfcb7fc":"code","126ecb49":"code","1d401509":"code","1a92f3eb":"code","3666aecc":"code","9e2fa688":"code","b1c5b342":"code","ec0d72e0":"code","734abcd8":"code","34eb6964":"code","4a79af3a":"code","d4885ab8":"code","1f81b64f":"code","6f6e4a6c":"code","53689ef8":"code","5f4a7aa8":"code","956d4e98":"code","16684e08":"code","d706fcec":"code","f3a34153":"code","60195541":"code","22e79f14":"code","63792147":"code","7cef6e07":"code","1f3d8426":"code","21e9c9b0":"code","0fcc342e":"code","25507193":"code","c513c04d":"markdown","76151e92":"markdown","4bdb1534":"markdown","bf835773":"markdown","e2234215":"markdown","0a065959":"markdown","6c38f94d":"markdown","cead203e":"markdown","e0b75ac2":"markdown","5fca41f0":"markdown","4da1e170":"markdown","05c02a33":"markdown","fc926876":"markdown","3619d638":"markdown","5f8e409c":"markdown","01797eef":"markdown","0527e13c":"markdown","a3a93dd8":"markdown","e87e143e":"markdown","4983cde7":"markdown","36bb2f55":"markdown","7da133bb":"markdown","c15fc338":"markdown","85a73e09":"markdown","e69ac100":"markdown","be9fc73d":"markdown","7bc63d89":"markdown","e3ce07ce":"markdown"},"source":{"b6dee4f3":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy.stats as st\nfrom sklearn import ensemble, tree, linear_model\nimport missingno as msno\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nplt.style.use('classic')","f328941b":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(train.head())\n#print(train.columns.value_counts())\nprint(train.shape)\nprint(test.shape)","f7ace17c":"test_Id = test['Id']\ntrain.drop('Id',axis=1,inplace=True)\ntest.drop('Id',axis=1,inplace=True)","ad39156c":"sns.distplot(train['SalePrice'], bins = 30)","1451c4ea":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","a7787c2a":"#train['SalePrice']= np.log1p(train['SalePrice'])\n#sns.distplot(train['SalePrice'], bins = 30)","30d11b88":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.columns","68e8392e":"numeric_features.head()","06bcc142":"date_feature = [feature for feature in numeric_features if 'Yr' in feature or 'Year' in feature]\nprint(date_feature)\n\nfor feature in date_feature:\n  print(feature, train[feature].unique())","c5bccc24":"#plt.scatter(x = (train['YrSold'] - train['YearBuilt']), y = train['SalePrice'])\n\nfor feature in date_feature:\n  if feature != 'YrSold':\n    data = train.copy()\n    plt.scatter(x = (data['YrSold'] - data[feature]), y = data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('Sale Price')\n    plt.show()\n","fb27fecb":"discrete_feature = [ feature for feature in numeric_features if len(train[feature].unique()) < 25 \n                    and feature not in date_feature + ['Id']]\n\nprint(len(discrete_feature))","0f476fd7":"train[discrete_feature].head()","fdb744f7":"for feature in discrete_feature:\n  data = train.copy(feature)\n  data = data.groupby(feature)['SalePrice']\n  data.median().plot.bar()\n  plt.xlabel(feature)\n  plt.ylabel('SalePrice')\n  plt.title(feature)\n  plt.show()","7d5f022c":"continious_feature = [ feature for feature in numeric_features if feature not in date_feature + discrete_feature + ['Id']]\nprint(len(continious_feature))","175f6d99":"for feature in continious_feature:\n  data = train.copy(feature)\n  data[feature].hist(bins = 30)\n  plt.xlabel(feature)\n  plt.ylabel('Count')\n  plt.title(feature)\n  plt.show()","a156c4f4":"fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(14,10))\nOverallQual_scatter_plot = pd.concat([train['SalePrice'],train['OverallQual']],axis = 1)\nsns.regplot(x='OverallQual',y = 'SalePrice',data = OverallQual_scatter_plot,scatter= True, fit_reg=True, ax=ax1)\nTotalBsmtSF_scatter_plot = pd.concat([train['SalePrice'],train['TotalBsmtSF']],axis = 1)\nsns.regplot(x='TotalBsmtSF',y = 'SalePrice',data = TotalBsmtSF_scatter_plot,scatter= True, fit_reg=True, ax=ax2)\nGrLivArea_scatter_plot = pd.concat([train['SalePrice'],train['GrLivArea']],axis = 1)\nsns.regplot(x='GrLivArea',y = 'SalePrice',data = GrLivArea_scatter_plot,scatter= True, fit_reg=True, ax=ax3)\nGarageArea_scatter_plot = pd.concat([train['SalePrice'],train['GarageArea']],axis = 1)\nsns.regplot(x='GarageArea',y = 'SalePrice',data = GarageArea_scatter_plot,scatter= True, fit_reg=True, ax=ax4)\nFullBath_scatter_plot = pd.concat([train['SalePrice'],train['FullBath']],axis = 1)\nsns.regplot(x='FullBath',y = 'SalePrice',data = FullBath_scatter_plot,scatter= True, fit_reg=True, ax=ax5)\nYearBuilt_scatter_plot = pd.concat([train['SalePrice'],train['YearBuilt']],axis = 1)\nsns.regplot(x='YearBuilt',y = 'SalePrice',data = YearBuilt_scatter_plot,scatter= True, fit_reg=True, ax=ax6)","19af1f10":"YearRemodAdd_scatter_plot = pd.concat([train['SalePrice'],train['YearRemodAdd']],axis = 1)\nYearRemodAdd_scatter_plot.plot.scatter('YearRemodAdd','SalePrice')","fa479704":"catagorical_features = train.select_dtypes(include = np.object)\ncatagorical_features.columns","3b802fbe":"sns.boxplot(x='OverallQual', y=\"SalePrice\", data=train)","6450ca0c":"plt.figure(figsize=(20,5))\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=train)\nplt.xticks(rotation=90)","2ed5a41b":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\n\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=catagorical_features)#check the output using f.head()\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=5,sharex=False, sharey=False)\ng = g.map(boxplot, \"value\", \"SalePrice\")\nplt.xticks(rotation=90)\n#sns.FacetGrid(data, row=None, col=None, hue=None, col_wrap=None, sharex=True, sharey=True)","3f6837c0":"plt.figure(figsize=(20,5))\nsns.boxplot(x='GarageYrBlt', y=\"SalePrice\", data=train)\nplt.xticks(rotation=90)","b91b8018":"plt.figure(figsize = (12,12))\nsns.heatmap(train.corr())\n\ncorrelation = train.corr()","005b1122":"k= 15\ncols = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)","f49adfa4":"plt.figure(figsize = (12,12))\n\nsns.heatmap(train[cols].corr(), vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\")","e624d83b":"train=train.drop(['GarageCars','1stFlrSF','TotRmsAbvGrd'],axis=1)\ntest=test.drop(['GarageCars','1stFlrSF','TotRmsAbvGrd'],axis=1)","27657e62":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","f42e8501":"df = train\n# Keep only the rows with at least 4 non-na values\ndf.dropna(axis =1 ,inplace = True, thresh = 300)\n\nprint('No. of Columns:' +str(len(df.columns)))\n\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))\n\n#We dropped Four Columns\n\nprint('No of rows:' +str(len(df)))","3420c4bf":"test=test.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1)","b46738c7":"# Will drop the rows only if all of the values in the row are missing\ndf.dropna(how = 'all',inplace = True)\n\nprint('No. of rows:' +str(len(df))) #No rows dropped as condition not satisfied","bb6f30fe":"df.dropna(how='any', subset=['MasVnrArea', 'MasVnrType', 'Electrical'], inplace = True)\nprint('No. of rows:' +str(len(df)))\n\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))","43164f57":"catagorical_features = df.select_dtypes(include = np.object)\ncatagorical_features.columns","617370f3":"for feature in catagorical_features:\n  df.fillna('0', inplace = True)\n  test.fillna('0', inplace = True)\n\n\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(20))","ca0b1c0c":"numeric_features = df.select_dtypes(include=[np.number])","f122e1ab":"date_feature = [feature for feature in numeric_features if 'Yr' in feature or 'Year' in feature]\n\nfor feature in date_feature:\n  df[feature].fillna(df[feature].mode()[0],inplace=True)","bd1a7326":"discrete_feature = [ feature for feature in numeric_features if len(train[feature].unique()) < 25 \n                    and feature not in date_feature + ['Id']]\n\nfor feature in date_feature:\n  df[feature].fillna(df[feature].mode()[0],inplace=True)","20b58c9c":"continious_feature = [ feature for feature in numeric_features if feature not in date_feature + discrete_feature + ['Id']]\n\nfor feature in continious_feature:\n  df[feature].fillna(df[feature].mean(),inplace=True)\n","7bfcb7fc":"print('No. of rows:' +str(len(df)))\nprint('No. of columns:' +str(len(df.columns)))","126ecb49":"train = df\nprint(train.shape)\nprint(test.shape)","1d401509":"num_train=train[['OverallCond','BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','KitchenAbvGr',\n'BedroomAbvGr','Fireplaces','MoSold','YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']]\nnum_test=test[['OverallCond','BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','KitchenAbvGr',\n'BedroomAbvGr','Fireplaces','MoSold','YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']]\nprint(num_train.shape)\nprint(num_test.shape)\nnumerical_categorical_feature=c = pd.concat((num_train,num_test),sort=False)\nprint(numerical_categorical_feature.shape)\nnumerical_categorical_feature=numerical_categorical_feature.astype('O')","1a92f3eb":"numerical_categorical_feature = pd.get_dummies(numerical_categorical_feature)\nnum_train_dummy = numerical_categorical_feature[:1451]\nnum_test_dummy = numerical_categorical_feature[1451:]\nprint(num_train_dummy.shape)\nprint(num_test_dummy.shape)","3666aecc":"train=train.drop(['OverallCond','BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','KitchenAbvGr',\n'BedroomAbvGr','Fireplaces','MoSold','YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'],axis=1)\n\ntest=test.drop(['OverallCond','BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath','KitchenAbvGr',\n'BedroomAbvGr','Fireplaces','MoSold','YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'],axis=1)\n\nprint(train.shape)\nprint(test.shape)\n","9e2fa688":"#skew_features = train.apply(lambda x : x.skew()).sort_values(ascending = False)\n#high_skew = skew_features[skew_features>0.5]\n#high_skew = high_skew.index\n#print('There are '+str(skew_index.shape[0]) +' highly skewed numerical features in training set')\n#print(high_skew)","b1c5b342":"# Normalize skewed features\n#train[high_skew] = np.log1p(train[high_skew])","ec0d72e0":"#skew_features = test.apply(lambda x : x.skew()).sort_values(ascending = False)\n#high_skew_test = skew_features[skew_features>0.5]\n#high_skew_test = high_skew_test.index\n#print('There are '+str(high_skew_test.shape[0]) +' highly skewed numerical features in test set')\n#print(high_skew_test)","734abcd8":"#test[high_skew_test] = np.log1p(test[high_skew_test])","34eb6964":"final_train = train.merge(num_train_dummy,left_index=True,right_index=True)\nfinal_test = test.merge(num_test_dummy,left_index=True,right_index=True)\n\nprint(final_train.shape)\nprint(final_test.shape)","4a79af3a":"from sklearn.model_selection import train_test_split\n\nX = final_train.drop('SalePrice',axis=1)\ny = final_train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)","d4885ab8":"from sklearn.tree import DecisionTreeRegressor\ndtree = DecisionTreeRegressor()\ndtree.fit(X_train, y_train)","1f81b64f":"predictions = dtree.predict(X_test)","6f6e4a6c":"plt.scatter(y_test,predictions)","53689ef8":"sns.distplot((y_test-predictions),bins=50);","5f4a7aa8":"predictions = dtree.predict(final_test)","956d4e98":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 501, random_state = 0)","16684e08":"rf.fit(X_train, y_train)","d706fcec":"predictions = rf.predict(X_test)","f3a34153":"predictions = rf.predict(X_test)","60195541":"plt.scatter(y_test,predictions)","22e79f14":"sns.distplot((y_test-predictions),bins=50);","63792147":"predictions = rf.predict(final_test)","7cef6e07":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.ensemble import GradientBoostingRegressor","1f3d8426":"model = GradientBoostingRegressor()\n# define the evaluation procedure\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate the model\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# report performance\nprint('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","21e9c9b0":"model.fit(X, y)","0fcc342e":"predictions = model.predict(final_test)","25507193":"sub = pd.DataFrame()\nsub['Id'] = test_Id\nsub['SalePrice'] = predictions\nsub.to_csv('submission.csv',index=False)","c513c04d":"3.1 Zoomed Correlation between 15 highly correlated variables with Sale Price","76151e92":"2.2 Let us Examine the Catagorical Features","4bdb1534":"2.1.2. Numeric Variables - Dircrete Type","bf835773":"2.1.3. Continious Variables","e2234215":"Then we drop these features both from the train and test data set.","0a065959":"Droping the ID Variable in both the Train & Test Dataset","6c38f94d":"2.1 Let's examine the numeric features in the data","cead203e":"We should drop PoolQC, MiscFeature, Alley, Fence, FireplaceQu\tas these variables contain huge missing data( more than 80%).\n\nThe only case that it may worth deleting a variable is when its missing values are more than 60% of the observations but only if that variable is insignificant.Taking this into consideration, imputation is always a preferred choice over deleting variables.","e0b75ac2":"First we make a separate data frame out of these variables and also with all the other catagorical variables each for the test and the train data. We then concat these two data frames to create a single data frame.","5fca41f0":"2.1.1. Date Time Variables \/ Temporal Variables","4da1e170":"4. Missing Value Treatment","05c02a33":"3. Correlation","fc926876":"Removing highly correlated values from the train as well as the Test Data","3619d638":"5. Get Dummies","5f8e409c":"7.1 Decision Tree - Baseline Model","01797eef":"2. Next, we explore the input variables. These are also called independent variables","0527e13c":"1. Let's have a look at the **Target** Variable. These is also called the Dependent Variable.","a3a93dd8":"7.2 RandomForest Regressor","e87e143e":"Importing Python Libraries","4983cde7":"6. Splitting the Training data","36bb2f55":"##Overview\n\nThere are 1460 instances of training data and 1460 of test data. Total number of attributes equals 81, of which 36 is quantitative, 43 categorical with and SalePrice.\n\nQuantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n\nQualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities,\n","7da133bb":"We have some numerical features which looked like categorical features. Now we will work on it in this section.\n","c15fc338":"Importing Training and Test Data","85a73e09":"Merge train with num_train_dummy\n\nMerge test with num_test_dummy","e69ac100":"7. Modelling","be9fc73d":"Now, we should also drop the same Variables from the Test Data as well.\n","7bc63d89":"4. Get Dummy Variables","e3ce07ce":"Gradient Boosting"}}