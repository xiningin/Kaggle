{"cell_type":{"895e4001":"code","33f2a889":"code","e3a35cef":"code","e5c65d96":"code","470463e2":"code","f3dcec40":"code","8ae8e307":"code","ae2be1cb":"code","155eec7a":"code","dd9735ea":"code","9664f252":"code","1163f2ca":"code","b8d413dc":"code","45f896df":"code","d48a353c":"code","0d15d642":"code","f3c9d823":"code","14f3d5c5":"code","1171edcd":"code","b3346a81":"markdown","bbd16eab":"markdown","d3374822":"markdown","0b596b64":"markdown","8c5a1c82":"markdown","e52a28e1":"markdown","f722f221":"markdown","6a022bcc":"markdown","2645d4ff":"markdown","63f9929b":"markdown"},"source":{"895e4001":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33f2a889":"df = pd.read_csv(\"\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")\ndf.head()","e3a35cef":"df.columns=[\"ID\",\"Gender\",\"Age\",\"Income\",\"Score\"]\ndf.head()","e5c65d96":"df.isnull().sum()","470463e2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.swarmplot(x=\"Gender\",y=\"Age\",data=df)\nplt.show()","f3dcec40":"sns.swarmplot(x=\"Gender\",y=\"Income\",data=df)\nplt.show()","8ae8e307":"fig, axs = plt.subplots(ncols=2,figsize=(12,4))\nsns.scatterplot(x=\"Age\",y=\"Income\",data=df,hue=\"Gender\",ax=axs[0])\nsns.scatterplot(x=\"Age\",y=\"Score\",data=df,hue=\"Gender\",ax=axs[1])\nplt.tight_layout()\nplt.show()","ae2be1cb":"sns.relplot(\"Income\",\"Score\",data=df,hue=\"Gender\")","155eec7a":"#Dividing into train and test\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, test_size=0.25, random_state=1234)\ndf_train = df_train.drop([\"ID\"],axis=1)\ndf_test = df_test.drop([\"ID\"],axis=1)\ndf_train.shape","dd9735ea":"df_train_2col = df_train[[\"Income\",\"Score\"]]\ndf_test_2col = df_test[[\"Income\",\"Score\"]]\ntmp_train = df_train_2col.copy()","9664f252":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=4)\nmodel.fit(df_train_2col)\ndf_train_2col[\"labels\"] = model.predict(df_train_2col).copy()\ndf_test_2col[\"labels\"] = model.predict(df_test_2col).copy()\ndf_train_2col.head()","1163f2ca":"fig, axs = plt.subplots(ncols=2,figsize=(12,4))\naxs[0].scatter(df_train_2col[\"Income\"],df_train_2col[\"Score\"],c=df_train_2col[\"labels\"],alpha=0.5)\naxs[0].set_title(\"Train Data\")\naxs[1].scatter(df_test_2col[\"Income\"],df_test_2col[\"Score\"],c=df_test_2col[\"labels\"],alpha=0.5)\naxs[1].set_title(\"Test Data\")\ncentroids_x = model.cluster_centers_[:,0]\ncentroids_y = model.cluster_centers_[:,1]\naxs[0].scatter(centroids_x,centroids_y,marker=\"D\",s=50,c=\"black\")\naxs[1].scatter(centroids_x,centroids_y,marker=\"D\",s=50,c=\"black\")","b8d413dc":"model.inertia_","45f896df":"#Inertia vs Number of Clusters\ninertia = []\nclus_size=range(1,11)\nfor k in clus_size:\n    model = KMeans(n_clusters=k)\n    model.fit(tmp_train)\n    inertia.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(clus_size, inertia, '-o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.xticks(clus_size)\nplt.show()","d48a353c":"df_train = pd.get_dummies(df_train,columns=[\"Gender\"],drop_first=True)\ndf_test = pd.get_dummies(df_test,columns=[\"Gender\"],drop_first=True)\nmodel = KMeans(n_clusters=5)\nmodel.fit(df_train)\nlabels_train = model.predict(df_train).copy()\nlabels_test = model.predict(df_test).copy()\nres_train.head()","0d15d642":"model.inertia_","f3c9d823":"#Inertia vs Number of Clusters\ninertia = []\nclus_size=range(1,11)\nfor k in clus_size:\n    model = KMeans(n_clusters=k)\n    model.fit(df_train)\n    inertia.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(clus_size, inertia, '-o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.xticks(clus_size)\nplt.show()","14f3d5c5":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nkmeans = KMeans(n_clusters=5)\npipeline = make_pipeline(scaler,kmeans)\npipeline.fit(df_train)\nlabels_train_std = pipeline.predict(df_train)\nlabels_test_std = pipeline.predict(df_test)","1171edcd":"labels_train_std","b3346a81":"# **Evaluation of the clustering**\n\nInertia is a good way to evaluate K-Means clustering.","bbd16eab":"From the above graph, we can see that females shop more than males. For females, 20-30 age group happens to shop the most","d3374822":"The inertia started decreasing slowly after five number of clusters. So, n_clusters = 4 is a decent parameter\n\nLet's do the similar analysis for the whole dataset","0b596b64":"We can see that n_clusters = 5 is a good parameter from the above graph","8c5a1c82":"There is no proper relation between Income, Score and age from above scatter plots","e52a28e1":"There is no segregation in terms of gender. But the score and income have a good relation. The data also can be segregated into 5 clusters.\n\nLet's make a K-Means cluster with just Income and spending score variables","f722f221":"Nothing much to infer from the above graph","6a022bcc":"Now we have clusters for both train and test data. Let's visualize these clusters","2645d4ff":"There are no missing values\n\nLet's do some visualization","63f9929b":"# Standardization\n\nk-Means algorithm doesn't work well with variables of different variances. In these cases, it is better to standardize the variables such that the variance of the variables are similar.\n\nGenerally, standard scalar is used to standardize. Let's make a pipeline to first standardize and then fit K-Means"}}