{"cell_type":{"f1a39504":"code","48286f46":"code","f9323b0a":"code","6a840cba":"code","f7385ed7":"code","9fc6e87a":"code","db4b9276":"code","120a0885":"code","fd48a660":"code","66251709":"code","1a92af6d":"code","e1879a87":"code","2a6221cf":"code","d378741c":"code","2ac63e6b":"code","4e95faf8":"code","0139c0b3":"code","420502e7":"code","6acd1d98":"code","832d71b3":"code","b9be1e05":"code","33e0eccd":"code","56575f0a":"code","9a26005e":"code","4ac1aa6e":"code","5d34f608":"code","e78ac76c":"code","dca52429":"code","2663fcdb":"code","57512043":"code","67ff384c":"code","6a9b358b":"code","11aac8f1":"code","67cd76e2":"code","e606a3c7":"code","2e8460aa":"code","de9b6db8":"code","a6d43c40":"code","41c5ec59":"code","49126bd7":"code","b4838217":"code","afc4581e":"code","bdde99ef":"code","39a0c918":"code","fbddc9de":"code","747502ea":"code","e02ffd52":"code","4e7e6d59":"code","1ce769e8":"code","949a134d":"code","3c1919a9":"code","8d717646":"code","f31ea209":"code","ab2cb19c":"code","fc8c88ec":"code","5390f856":"markdown","248cbb53":"markdown","11deaf76":"markdown","4ef3439e":"markdown","bd8fe649":"markdown","d63707bd":"markdown","2b707b8b":"markdown","35729002":"markdown","b9b70ae5":"markdown","33d96851":"markdown","020ab25d":"markdown","c1170277":"markdown","d2efcafc":"markdown","b36f2371":"markdown","9fbad631":"markdown","c600b1c8":"markdown","0c7e92a4":"markdown","12b68fd9":"markdown","43920444":"markdown","b6d8aa52":"markdown","fd299d56":"markdown","4f65e644":"markdown","a301d4d9":"markdown","40630130":"markdown","e9644251":"markdown","491de222":"markdown","a674f8c0":"markdown","a8622ae8":"markdown","d2dbbc53":"markdown","efde762a":"markdown","bc09dce3":"markdown","0c51a8fe":"markdown","38256773":"markdown","4ca66646":"markdown","62824bbf":"markdown","93aed60b":"markdown","177f269b":"markdown","90961e49":"markdown","8542c49a":"markdown","efae3b9f":"markdown","2752f4f4":"markdown","27752076":"markdown","7d341756":"markdown","71c07756":"markdown","0363ef9f":"markdown","b052affa":"markdown","2b838801":"markdown","fde62be9":"markdown","d211999e":"markdown","97737518":"markdown","0d542474":"markdown","c3b5ceca":"markdown","c1b4f3ee":"markdown","b2855175":"markdown","be411948":"markdown","a0274c93":"markdown","a460b1ac":"markdown"},"source":{"f1a39504":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('..\/input\/17k-apple-app-store-strategy-games\/appstore_games.csv')\ndata.head()","48286f46":"data.shape","f9323b0a":"data.isna().sum()","6a840cba":"data = data.drop('Subtitle', axis=1)\nfiltered_data = data.dropna()\nfiltered_data.shape","f7385ed7":"filtered_data.dtypes == 'object'","9fc6e87a":"filtered_data['Genres'].value_counts()","db4b9276":"genres = filtered_data['Genres'].str.split(', ')\ngenres","120a0885":"from sklearn.preprocessing import MultiLabelBinarizer\n\n# Binarise labels\n\nmlb = MultiLabelBinarizer()\nexpandedLabelData = mlb.fit_transform(filtered_data['Genres'].str.split(', '))\nlabelClasses = mlb.classes_\nprint(labelClasses)\n\n# Create a pandas.DataFrame from our output\nexpandedLabels = pd.DataFrame(expandedLabelData, columns=labelClasses)\nexpandedLabels.head()","fd48a660":"expandedLabels.sum()","66251709":"categories = list(expandedLabels.columns.values)\nplt.figure(figsize=(20,10))\nf = sns.barplot(categories, expandedLabels.sum().values)\nf.set_xticklabels(f.get_xticklabels(), rotation='25', ha=\"right\");","1a92af6d":"expanded_labels_minus_games_strategy = expandedLabels.drop([\"Strategy\", \"Games\"], axis=1)\ncategories = list(expanded_labels_minus_games_strategy.columns.values)\nplt.figure(figsize=(20,10))\nf = sns.barplot(categories, expanded_labels_minus_games_strategy.sum().values)\nf.set_xticklabels(f.get_xticklabels(), rotation='25', ha=\"right\");","e1879a87":"print(filtered_data.shape)\nprint(expandedLabels.shape)\ndf = pd.concat([filtered_data.reset_index(drop=True),expandedLabels.reset_index(drop=True)], axis=1)\nprint(df.shape)\ndf.head()","2a6221cf":"df = df.drop(['URL', 'ID', 'Icon URL'], axis=1)\ndf.head()","d378741c":"df['In-app Purchases'].value_counts()","2ac63e6b":"mlb_inapp = MultiLabelBinarizer()\nexpandedLabelData = mlb_inapp.fit_transform(df['In-app Purchases'].str.split(', '))\nlabelClasses = mlb_inapp.classes_\nprint(labelClasses)\n\n# Create a pandas.DataFrame from our output\nexpandedLabels = pd.DataFrame(expandedLabelData, columns=labelClasses)\nexpandedLabels.head()","4e95faf8":"max_10_prices = expandedLabels.sum().sort_values(ascending=False)[:10]\nsns.barplot(max_10_prices.index, max_10_prices.values)","0139c0b3":"df = pd.concat([df.reset_index(drop=True),expandedLabels.reset_index(drop=True)], axis=1)","420502e7":"y = df['Average User Rating']","6acd1d98":"desc = df['Description']\ndesc_lengths = [len(de) for de in desc]\ndf['desc_lengths'] = desc_lengths","832d71b3":"df = df.drop(['Name', 'Average User Rating', 'In-app Purchases', 'Languages', 'Genres', 'Description'], axis=1)\ndf.head()","b9be1e05":"from datetime import datetime\ndate_format = \"%d\/%m\/%Y\"\n(datetime.strptime(df['Current Version Release Date'][1], date_format) - datetime.strptime(df['Original Release Date'][1], date_format)).days","33e0eccd":"dataset_date = datetime.strptime(\"3\/7\/2019\", date_format)\ndataset_date","56575f0a":"curr_minus_orig = []\n\nfor i in range(len(df)):\n    curr = df['Current Version Release Date'][i]\n    orig = df['Original Release Date'][i]\n    diff = datetime.strptime(curr, date_format) - datetime.strptime(orig, date_format)\n    curr_minus_orig.append(diff.days)\n    \n\ndf['Current minus Original'] = np.array(curr_minus_orig)\ndf['Original Release Date'] = np.array([ (dataset_date - datetime.strptime(date, date_format)).days for date in df['Original Release Date']])\ndf['Current Version Release Date'] =  np.array([ (dataset_date - datetime.strptime(date, date_format)).days for date in df['Current Version Release Date']])","9a26005e":"df.dtypes[df.dtypes == object]","4ac1aa6e":"df['Age Rating'].unique()","5d34f608":"from sklearn import preprocessing\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))","e78ac76c":"df['Age Rating'].unique()","dca52429":"df['Age Rating'][df['Age Rating'] == 2] = 4\ndf['Age Rating'][df['Age Rating'] == 3] = 9\ndf['Age Rating'][df['Age Rating'] == 0] = 12\ndf['Age Rating'][df['Age Rating'] == 1] = 17","2663fcdb":"df['Age Rating'].unique()","57512043":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=123)","67ff384c":"X_train = X_train.fillna(0)\nX_test = X_test.fillna(0)","6a9b358b":"X_train.shape","11aac8f1":"sns.distplot(y, hist=True, kde=False, bins=9, hist_kws={'edgecolor':'#000000'})","67cd76e2":"X_train.head()","e606a3c7":"X_test.head()","2e8460aa":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX_train_preprocessed = preprocessing.normalize(X_train)\nX_test_preprocessed = preprocessing.normalize(X_test)\n\nlin_model = LinearRegression()\nlin_model.fit(X_train_preprocessed, y_train)\n\ny_train_predict = lin_model.predict(X_train_preprocessed)\n\nrmse = (np.sqrt(mean_squared_error(y_train, y_train_predict)))\n\nprint(\"The model performance for training set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint(\"\\n\")\n\n# model evaluation for testing set\ny_test_predict = lin_model.predict(X_test_preprocessed)\nrmse = (np.sqrt(mean_squared_error(y_test, y_test_predict)))\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))","de9b6db8":"error_frame = pd.DataFrame({'Actual': np.array(y_test).flatten(), 'Predicted': y_test_predict.flatten()})\nerror_frame.head(10)","a6d43c40":"df1 = error_frame[0:20]\ndf1.plot(kind='bar',figsize=(24,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","41c5ec59":"import xgboost as xgb\n\nxgr = xgb.XGBRegressor(           \n                 learning_rate=0.05,\n                 max_depth=8,\n                 min_child_weight=1.5,\n                 n_estimators=500,                                                                    \n                 seed=42,\n                 num_boost_rounds=50,\n                 objective=\"reg:squarederror\",\n                 tree_method='gpu_hist',  #IMPORTANT. GPU_HIST NEEDS GPU, OTHERWISE ERROR WILL BE THROWN.\n                )\nxgr.fit(X_train, y_train)","49126bd7":"train_pred = xgr.predict(data= X_train)\ntest_pred = xgr.predict(data= X_test)\n\nmse_train = mean_squared_error(y_train, train_pred)\nmse_test = mean_squared_error(y_test, test_pred)\n\nprint('RMSE train : {:.3f}'.format(np.sqrt(mse_train)))\nprint('RMSE test : {:.3f}'.format(np.sqrt(mse_test)))","b4838217":"from sklearn.model_selection import GridSearchCV\n\nparams = {'learning_rate': [0.01, 0.03, 0.06],\n          'max_depth' : [4, 6, 8],\n          'n_estimators' : [250, 500, 1000, 1500, 2000],\n          'num_boost_rounds' : [5, 20],}","afc4581e":"%%time\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nxgr_optimized = xgb.XGBRegressor(objective=\"reg:squarederror\",\n                                 min_child_weight=1.5,\n                                 tree_method='gpu_hist')\n\ngrid = GridSearchCV(estimator=xgr_optimized, scoring=\"neg_mean_squared_error\", param_grid = params, verbose=1, cv=3)\ngrid.fit(X_train, y_train)","bdde99ef":"print(grid.best_params_)","39a0c918":"xgr_best = xgb.XGBRegressor(learning_rate = 0.01, max_depth  = 6, n_estimators =  500, num_boost_rounds =  5,  min_child_weight=1.5, tree_method='gpu_hist') #, tree_method='gpu_hist') # You canuncomment this tree method part if you are using GPU\n\nxgr_best.fit(X_train, y_train)","fbddc9de":"train_pred = xgr_best.predict(data= X_train)\ntest_pred = xgr_best.predict(data= X_test)\n\nmse_train = mean_squared_error(y_train, train_pred)\nmse_test = mean_squared_error(y_test, test_pred)\n\nprint('RMSE train : {:.3f}'.format(np.sqrt(mse_train)))\nprint('RMSE test : {:.3f}'.format(np.sqrt(mse_test)))","747502ea":"error_frame = pd.DataFrame({'Actual': np.array(y_test).flatten(), 'Predicted': test_pred.flatten()})\nerror_frame.head(10)","e02ffd52":"df1 = error_frame[:20]\ndf1.plot(kind='bar',figsize=(24,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","4e7e6d59":"ax = xgb.plot_importance(xgr_best, max_num_features=10)\nplt.figure(figsize=(15,25))\nplt.show()","1ce769e8":"# ax = xgb.plot_importance(xgr, max_num_features=10)\n# plt.figure(figsize=(15,25))\n# plt.show()","949a134d":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='11G')","3c1919a9":"X_train['target'] = y_train\nh2o_train = h2o.H2OFrame(X_train)\n\nX_test['target'] = y_test\nh2o_test = h2o.H2OFrame(X_test)","8d717646":"h2o_train.head()","f31ea209":"aml = H2OAutoML(max_runtime_secs=1200, seed=1)\naml.train(x=list(X_train.columns), y=\"target\", training_frame=h2o_train, validation_frame=h2o_test)","ab2cb19c":"lb = aml.leaderboard\nlb.head()","fc8c88ec":"aml.predict(h2o_test)","5390f856":"sklearn's MultiLabelBinarizer makes it very easy to create a multi label dataframe.","248cbb53":"Set up an H2OAutoML class and specify a max_runtime_secs. I've given it 1200 to make it comparable to the GridSearchCV() we did with XGBoost (that generally takes me about 20 minutes).\n\nThen we train and pass\nx = column names of features\ny = column name of the target (it's target here)\ntraining_frame = our h2o_train frame\nvalidation_frame = our h2o_test frame","11deaf76":"Going to fill the missing values with 0.","4ef3439e":"From H2O's blog:\n\n>Automated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. AutoML tends to automate the maximum number of steps in an ML pipeline \u2014 with a minimum amount of human effort \u2014 without compromising the model\u2019s performance.\n\nWe don't have to do most of the things in AutoML. Just let it run for a predetermined time, and it will output with (in most cases) an ensembled model that does a very good job of predicting.","bd8fe649":"Now, this makes no sense. 4+ is mapped to 2, 9+ is mapped to 1 but 17+ is mapped to 1. To make the mapping better, we should use our own predefined mapping in this case.\n\nI'm going to convert 4+, 9+, 12+ and 17+ to 4,9, 12 and 17 respectively.","d63707bd":"Dropping all irrelevant\/already-encoded\/target columns...","2b707b8b":"Import the XGBoost API and create our Regressor with some parameters, then fit it to our data. I picked these parameters quite arbitarily, but after this we will be using GridSearchCV to find better parameters.\n\n### Important - the line `tree_method='gpu_hist'` will throw an error if you are not running a GPU kernel, so you will have to comment that line and run the kernel.","35729002":"Checking for missing values...","b9b70ae5":"Now we'll merge the expandedLabels with our dataframe.","33d96851":"I'm going to plot them for a more intuitive understanding.","020ab25d":"Seems like Games and Strategy are the most common genres, which is not surprising because the dataset itself is named 'Mobile Strategy Games'. Let's try removing them and check...","c1170277":"XGBoost is one of the most popular libraries for using the Gradient Boosting technique. \nFrom Wikipedia,\n\n>Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. \n\nEssentially XGB creates a lot of weak predictors and then combines them to give you a good prediction model. This process of combining predictors\/models is called Ensembling.","d2efcafc":"Import the h2o library. We will have to initialize the h2o cluster. Since Kaggle kernels are 16GB RAM, I have decided to go with a maximum memory size of 11GB. This will change depending on your machine.","b36f2371":"So we added a new feature called Current minus Original, and converted two existing features (Original Release Date and Current Version Release Date) into number of days since the dataset release.","9fbad631":"# H2O AutoML","c600b1c8":"Instead of fitting to X_train and y_train like sklearn's fit\/predict method, H2O needs to be provided with a H2OFrame (similar to pandas DataFrame but for H2O). In that, the X equivalent to sklearn's y is the column names used for training, and the y equivalent to sklearn's y is the column name used to predict (that is, our target).","0c7e92a4":"I've imported warnings to ignore FutureWarning\/Depreciation warnings as they will occur 270 times. Kaggle seems to be using old version of XGBoost where this bug hasn't been fixed, so we will have to ignore them.\n\nWe take the scoring as neg_mean_squared_error because by default sklearn assumes that the more mean squared error we have, the better model it is, even though that is not the case here. So since we want to minimize the error, we use neg_mean_squared_error.\n\n### The following code will take about 20 minutes to run WITH a GPU. It might take an hour or longer without one. If you want to skip this, comment the following cells until I tell you not to.","12b68fd9":"Since the RMSE of train is significantly lower than the RMSE of test, we have overfit. Let us try to find better parameters that won't overfit. However, even with overfitting, the 0.621 RMSE is quite a lot better than the 0.688 RMSE of the scikit-learn's linear regressor.\n\nOne of the ways of finding ideal hyperparameters is through GridSearchCV, in which we specify the possible each hyperparameter values. GridSearchCV goes through all possible combinations of these parameter values.","43920444":"So for example, the age rating column now has the following unique values.","b6d8aa52":"Plotting the first 20 comparisons of our error frame...","fd299d56":"### Stop commenting here if you want to skip the hyperoptimization step.\n\nIf you want to look at the feature importances and you haven't done hyperoptimization, you can uncomment the following line to look at it.","4f65e644":"Let's plot the most popular microtransaction costs in mobile games. I'm going to plot only the top 10 microtransactions.","a301d4d9":"We first normalize the data using sklearn's preprocessing library, then use the LinearRegression class of sklearn to initialize our model. Then we fit the linear regression model to our preprocessed train data and train labels.\nFinally, we predict the train and the test labels and calculate the RMSE (root mean squared error) score for each.","40630130":"`$0.99` is the most common, followed by `$4.99`, `$1.99` and `$9.99`.\n\nInterestly there's also quite a lot of '$49.99' and `$99.99` transactions, so seems like almost (round) numbers like 5, 10, 50,100, etc are more popular.","e9644251":"There are primarily two types of encodings used for such type of data - \n\n1) Label Encoding  \n2) One Hot Encoding\n\nIf you are not familiar with them, you can check this intuitive table. In Label Encoding, we assign each seperate value a particular value. In One Hot Encoding, we create multiple features from one feature (genre) and fill the value with 0 or 1 depending whether that genre is present in the value or not.\n![](https:\/\/www.dropbox.com\/s\/cl8dfb7c55wgvcp\/onehotencoding.png?dl=1)\n\nSince each entry in the genre column can have multiple genres, we will have to use one hot encoding.  Otherwise, after encoding, for example, \"Games, Strategy, Word\" will be completely different than \"Games, Strategy, Role Playing\" even though they have two genres in common.","491de222":"Our final 2 features are Original Release Date and Current Version Release Date.\nI'm going to convert them both to the number of days since the dataset was extracted.\n\nBut before that, I'm going to make a new category that finds the difference between the current version release date and the original version release date using datetime.\n\nFor example, look at the difference between current version release date and the original version release date of the entry at index 1 in our df.","a674f8c0":"I'm going to add a new feature -- The lengths of descriptions. It could be that half-hearted devs don't make long descriptions. Or it may not be. We'll see.","a8622ae8":"Finding the best score and the best parameters...","d2dbbc53":"That's 2409 days. Seems like this game hasn't been updated in a long time! So this feature could tell us that maybe since the game hasn't been updated, it isn't very popular or good. Logically, it would make more sense for highly rated games to be frequently updated, but this may not be the case. We will see.\n\nNow, Original Release Date and Current Version Release Date will be converted to the number of days since the dataset was extracted. Dataset was extracted on 3rd August, 2019 (from the dataset description).","efde762a":"Let's look at the most important features.\nSeems a few features that we inserted turned out to be quite useful - `Current minus Original` and `Description length`.\n\nOverall, the linear regression models depends a lot on the user ratings.\n\nIn case you are curious about how this feature importance score is obtained (F-score) in XGBoost, these are simply the number of times was this variable split on in all XGBoost trees.","bc09dce3":"In this kernel, we'll go over the following topics - \n\n1) Preparing the dataset for regression (i.e. encoding)\n\n2) Using scikit learn's linear regression to predict the rating\n\n3) Using XGBoost to predict the rating\n\n4) Using H2O AutoML to predict rating","0c51a8fe":"# Regression using XGBoost","38256773":"Most readings lie between 4.0 and 4.5 in our dataset, which will likely result in most predictions to be between 4.0 and 4.5. \n\nBefore we move in to regression, let us take one final look at our dataframes.","4ca66646":"Let's start with the Genres field.","62824bbf":"We're going to split our dataframe and y to check how well our model generalizes in the future.","93aed60b":"Finally, we will check the RMSE scores.","177f269b":"Finally, since we're going to be predicting the Average User Rating, we'll store it in a seperate variable called y. This is our target.","90961e49":"We'll use the best params as arguments for our XGBRegressor, then fit our training data.","8542c49a":"Now, let's convert the remaining object categories to something that algorithms can use (numbers) using Label Encoding.","efae3b9f":"Let's look at how the models performed!\nThe leaderboard shows you the best models, sorted from the best to the worst.","2752f4f4":"A lot of our predictions lie between 4 and 4.5, which is mostly because the dataset majorly had those values. Let us see if XGBoost can give us a lower RMSE than sklearn's linear regression.","27752076":"We will now proceed to one hot encode the In-app Purchases as well.","7d341756":"Even lower!\nNow, the same thing as we did earlier-- Create an error frame and plot it.","71c07756":"Calculating our losses...[RMSE scores]","0363ef9f":"So the best ensemble gave use a rmse of  0.580096, which is a further improvement. The longer you let it run, the better your chance of getting good models, so you can definitely try for longer than 22 minutes.\n\nFinally, we can use the model to predict.","b052affa":"# Linear Regression with Sklearn","2b838801":"I'm going to make a dataframe of the actual value vs the predicted value.","fde62be9":"So in the above example, since we have `3,3,5,2` possible values (3 for learning_rate, 3 for max_depth, 5 for n_estimators and 2 for num_boost_rounds), we will get \n`3*3*5*2` = `90` candidates. The best one amongst these will be chosen. If we decide to cross validate them with 3 folds, we will have to fit a total of `90*3` = `270` times.\n\nCross validation's folds can be easily understood through this image - \n![](https:\/\/miro.medium.com\/max\/1400\/1*rgba1BIOUys7wQcXcL4U5A.png)\n\nThis image shows how cv works for 5 fold. Here, we will be splitting the training data into 3 instead and then training on two of those, then validating on the one that is left. ","d211999e":"After label encoding, the Age Rating column looks like","97737518":"Dropping some columns that we are not going to use... These seem unlikely to affect our classification. These are unique to each game, and unique IDs should not be used for regression as they are different for each reading and often provide no inference.","0d542474":"It's quite highly dominated by Entertainment.\n\nWe're going to merge the two dataframes -- our filtered data frame with no null values and the expanded label dataframes with a one hot encoded genres. So our new dataframe will have the columns of the original dataframe and the expanded label dataframe.","c3b5ceca":"Dropping Subtitle column as it has too many null values.\nWe also drop rows that have any null values.","c1b4f3ee":"# Preparing the dataset for regression\n![](http:\/\/)Import the neccessary libraries and the dataset.","b2855175":"Before we proceed with regression, let us check how well distributed the data is.","be411948":"Check what fields we cannot use right now. Since it is regression, we should only use numbers, and the object fields are not numbers. So we will have to either drop them or convert them to something we can use.","a0274c93":"Let's see how many of each genre is present.","a460b1ac":"Here we're going to seperate the genres using the ', ' as the delimiter\/seperator. You can see that  we have split the multiple genres into individual genres that are in a list."}}