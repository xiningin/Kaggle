{"cell_type":{"ddedadb4":"code","6b99ec95":"code","7c43987b":"code","0650c124":"code","5708fb1a":"code","fd3901a2":"code","8f8fe849":"code","7f441f84":"code","b301aaa7":"code","b8792f66":"code","c726ddb1":"code","05704c28":"code","f44d51b3":"code","97b25f5e":"code","0086afbe":"code","67e4b923":"code","860b7459":"code","7c481f87":"code","46ff3272":"code","7f260b38":"code","f766b991":"code","b46b51d6":"code","41973978":"code","fbccec2a":"code","cc06a8a3":"code","09d2a1bf":"code","f700c4ef":"code","a9c14985":"code","4376ace2":"code","7634e2c8":"code","1739bdef":"code","1b8941d8":"code","c2f8833a":"code","62741d22":"code","525f7769":"code","41a10658":"code","459b96e8":"code","7408787c":"code","d7bc9769":"code","9dcde5fb":"code","02e06d28":"code","590aa7c9":"code","37f8a6a4":"code","bb034705":"code","d695f0cb":"code","22d89fe6":"code","466094a1":"code","2c74c82d":"code","797e673d":"code","b9f833be":"code","e0624d68":"code","f043ea25":"code","d0f5f752":"code","3fe60d4c":"code","f622c104":"code","98d258a4":"code","16592dd8":"code","0c3c5648":"code","09414fce":"code","d2bb949b":"code","28ffcfee":"code","15ba05ee":"code","02bc96bb":"code","26e8cf17":"code","28242232":"code","cd7ece81":"code","f84ecab2":"code","5892dadd":"code","756150a7":"code","03beab46":"code","11faca4b":"code","67c81b46":"code","a9fca8e6":"code","3fceae58":"code","c83619a4":"code","eea0b575":"code","229ac335":"code","e81097cd":"code","74bca6e9":"code","a9bada3e":"code","9ac8cced":"code","ac457ce4":"code","4ad3ae7d":"code","70d2676c":"code","9366edbf":"code","cd609917":"code","de639e79":"code","36696e1b":"code","9600096e":"code","6fc91654":"code","0fd3532f":"code","24c1094c":"code","b728b1b3":"code","a3e1b555":"code","5a3dc85a":"code","1b0f9c43":"code","f27b8876":"code","0b237595":"code","e21db681":"code","3513ba55":"code","5b08cd6e":"code","4a72caf8":"code","a4dbe705":"code","b576d2e2":"code","160540df":"code","3f3c02c6":"code","af994d5d":"code","ee410156":"code","8451362b":"code","2ff83726":"code","2187c836":"code","d5aed4c1":"code","70c9ed78":"code","c6509883":"code","b7f69754":"code","4d720041":"code","241e94c2":"code","b36eb874":"code","0374c7b2":"markdown","8dc56f9c":"markdown","817ffd4b":"markdown","a8498e82":"markdown","be446890":"markdown","0b57e806":"markdown","fc347d5b":"markdown","1e4ad6e4":"markdown","387afb7c":"markdown","ebee02b7":"markdown","3875f560":"markdown","acb61a0e":"markdown","de215c44":"markdown","98cc1ee4":"markdown","84f2af59":"markdown","784a438c":"markdown","d32bd46e":"markdown","fb687679":"markdown","49922f8f":"markdown","380641e9":"markdown","08b8f3fe":"markdown","96d5de12":"markdown","cd4786f3":"markdown","a0693ebe":"markdown","e448ca91":"markdown","847a9036":"markdown","f154730f":"markdown","895c74f1":"markdown","e905b0ed":"markdown","e6498d4d":"markdown","81c00c1b":"markdown","d49fc707":"markdown","c56dc227":"markdown","8a91bfaa":"markdown","b7268238":"markdown","c2351fc7":"markdown","deb7534f":"markdown","bf64e00a":"markdown","bfd2e981":"markdown","817066df":"markdown","e4bf535f":"markdown","a86bf969":"markdown","9b96d791":"markdown","87215550":"markdown","bce947f3":"markdown","0b1e7ae8":"markdown","7d8ddb63":"markdown","80190918":"markdown","458cdc3d":"markdown","a016713b":"markdown","b9f2ed26":"markdown","2dc6645f":"markdown","4911dbde":"markdown","51ffdd21":"markdown","d5ac2555":"markdown","3eb052f3":"markdown","ace2a4f1":"markdown","bf767b2e":"markdown","910b5e64":"markdown","281ae25b":"markdown","20906bae":"markdown","edb05745":"markdown","d8541a18":"markdown","f5aa6807":"markdown","08580a61":"markdown"},"source":{"ddedadb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport altair as alt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6b99ec95":"train = pd.read_csv('..\/input\/sales_train.csv')\nitems = pd.read_csv('..\/input\/items.csv')\ncategories = pd.read_csv('..\/input\/item_categories.csv')\nshops = pd.read_csv('..\/input\/shops.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","7c43987b":"train.head()","0650c124":"train.shape","5708fb1a":"test.head()","fd3901a2":"test.shape","8f8fe849":"submission.head()","7f441f84":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntrain['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Training Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntrain['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram')\n\nplt.subplot2grid((3,3), (1,1))\ntrain['item_price'].plot(kind='hist', alpha=0.7, color='orange')\nplt.title('Item Price Histogram')\n\nplt.subplot2grid((3,3), (1,2))\ntrain['item_cnt_day'].plot(kind='hist', alpha=0.7, color='green')\nplt.title('Item Count Day Histogram')\n\nplt.subplot2grid((3,3), (2,0), colspan = 3)\ntrain['date_block_num'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Month (date_block_num) Values in the Training Set (Normalized)')\n\nplt.show()","b301aaa7":"train['item_id'].value_counts(ascending=False)[:5]","b8792f66":"items.loc[items['item_id']==20949]","c726ddb1":"categories.loc[categories['item_category_id']==71]","05704c28":"test.loc[test['item_id']==20949].head(5)","f44d51b3":"train['item_cnt_day'].sort_values(ascending=False)[:5]","97b25f5e":"train[train['item_cnt_day'] == 2169]","0086afbe":"items[items['item_id'] == 11373]","67e4b923":"train[train['item_id'] == 11373].head(5)","860b7459":"train = train[train['item_cnt_day'] < 2000]","7c481f87":"train['item_price'].sort_values(ascending=False)[:5]","46ff3272":"train[train['item_price'] == 307980]","7f260b38":"items[items['item_id'] == 6066]","f766b991":"train[train['item_id'] == 6066]","b46b51d6":"train = train[train['item_price'] < 300000]","41973978":"train['item_price'].sort_values()[:5]","fbccec2a":"train[train['item_price'] == -1]","cc06a8a3":"train[train['item_id'] == 2973].head(5)","09d2a1bf":"price_correction = train[(train['shop_id'] == 32) & (train['item_id'] == 2973) & (train['date_block_num'] == 4) & (train['item_price'] > 0)].item_price.median()\ntrain.loc[train['item_price'] < 0, 'item_price'] = price_correction","f700c4ef":"fig = plt.figure(figsize=(18,8))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntest['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Test Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntest['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram - Test Set')\n\nplt.show()","a9c14985":"shops_train = train['shop_id'].nunique()\nshops_test = test['shop_id'].nunique()\nprint('Shops in Training Set: ', shops_train)\nprint('Shops in Test Set: ', shops_test)","4376ace2":"shops_train_list = list(train['shop_id'].unique())\nshops_test_list = list(test['shop_id'].unique())\n\nflag = 0\nif(set(shops_test_list).issubset(set(shops_train_list))): \n    flag = 1\n      \nif (flag) : \n    print (\"Yes, list is subset of other.\") \nelse : \n    print (\"No, list is not subset of other.\") ","7634e2c8":"shops.T","1739bdef":"train.loc[train['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\n\ntrain.loc[train['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\n\ntrain.loc[train['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11","1b8941d8":"cities = shops['shop_name'].str.split(' ').map(lambda row: row[0])\ncities.unique()","c2f8833a":"shops['city'] = shops['shop_name'].str.split(' ').map(lambda row: row[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'","62741d22":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit_transform(shops['city'])","525f7769":"shops['city_label'] = le.fit_transform(shops['city'])\nshops.drop(['shop_name', 'city'], axis = 1, inplace = True)\nshops.head()","41a10658":"items_train = train['item_id'].nunique()\nitems_test = test['item_id'].nunique()\nprint('Items in Training Set: ', items_train)\nprint('Items in Test Set: ', items_test)","459b96e8":"items_train_list = list(train['item_id'].unique())\nitems_test_list = list(test['item_id'].unique())\n\nflag = 0\nif(set(items_test_list).issubset(set(items_train_list))): \n    flag = 1\n      \nif (flag) : \n    print (\"Yes, list is subset of other.\") \nelse : \n    print (\"No, list is not subset of other.\") ","7408787c":"len(set(items_test_list).difference(items_train_list))","d7bc9769":"categories_in_test = items.loc[items['item_id'].isin(sorted(test['item_id'].unique()))].item_category_id.unique()","9dcde5fb":"# categories.loc[categories['item_category_id'].isin(categories_in_test)]\ncategories.loc[~categories['item_category_id'].isin(categories_in_test)].T","02e06d28":"le = preprocessing.LabelEncoder()\n\nmain_categories = categories['item_category_name'].str.split('-')\ncategories['main_category_id'] = main_categories.map(lambda row: row[0].strip())\ncategories['main_category_id'] = le.fit_transform(categories['main_category_id'])\n\n# Some items don't have sub-categories. For those, we will use the main category as a sub-category\ncategories['sub_category_id'] = main_categories.map(lambda row: row[1].strip() if len(row) > 1 else row[0].strip())\ncategories['sub_category_id'] = le.fit_transform(categories['sub_category_id'])","590aa7c9":"categories.head()","37f8a6a4":"train['date'] =  pd.to_datetime(train['date'], format='%d.%m.%Y')","bb034705":"from itertools import product","d695f0cb":"# Testing generation of cartesian product for the month of January in 2013\n\nshops_in_jan = train.loc[train['date_block_num']==0, 'shop_id'].unique()\nitems_in_jan = train.loc[train['date_block_num']==0, 'item_id'].unique()\njan = list(product(*[shops_in_jan, items_in_jan, [0]]))","22d89fe6":"print(len(jan))","466094a1":"# Testing generation of cartesian product for the month of February in 2013\n\nshops_in_feb = train.loc[train['date_block_num']==1, 'shop_id'].unique()\nitems_in_feb = train.loc[train['date_block_num']==1, 'item_id'].unique()\nfeb = list(product(*[shops_in_feb, items_in_feb, [1]]))","2c74c82d":"print(len(feb))","797e673d":"cartesian_test = []\ncartesian_test.append(np.array(jan))\ncartesian_test.append(np.array(feb))","b9f833be":"cartesian_test","e0624d68":"cartesian_test = np.vstack(cartesian_test)","f043ea25":"cartesian_test_df = pd.DataFrame(cartesian_test, columns = ['shop_id', 'item_id', 'date_block_num'])","d0f5f752":"cartesian_test_df.head()","3fe60d4c":"cartesian_test_df.shape","f622c104":"from tqdm import tqdm_notebook\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float16)\n    df[int_cols]   = df[int_cols].astype(np.int16)\n    \n    return df","98d258a4":"months = train['date_block_num'].unique()","16592dd8":"cartesian = []\nfor month in months:\n    shops_in_month = train.loc[train['date_block_num']==month, 'shop_id'].unique()\n    items_in_month = train.loc[train['date_block_num']==month, 'item_id'].unique()\n    cartesian.append(np.array(list(product(*[shops_in_month, items_in_month, [month]])), dtype='int32'))","0c3c5648":"cartesian_df = pd.DataFrame(np.vstack(cartesian), columns = ['shop_id', 'item_id', 'date_block_num'], dtype=np.int32)","09414fce":"cartesian_df.shape","d2bb949b":"x = train.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\nx.head()","28ffcfee":"x.shape","15ba05ee":"new_train = pd.merge(cartesian_df, x, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)","02bc96bb":"new_train['item_cnt_month'] = np.clip(new_train['item_cnt_month'], 0, 20)","26e8cf17":"del x\ndel cartesian_df\ndel cartesian\ndel cartesian_test\ndel cartesian_test_df\ndel feb\ndel jan\ndel items_test_list\ndel items_train_list\ndel train","28242232":"new_train.sort_values(['date_block_num','shop_id','item_id'], inplace = True)\nnew_train.head()","cd7ece81":"test.insert(loc=3, column='date_block_num', value=34)","f84ecab2":"test['item_cnt_month'] = 0","5892dadd":"test.head()","756150a7":"new_train = new_train.append(test.drop('ID', axis = 1))","03beab46":"new_train = pd.merge(new_train, shops, on=['shop_id'], how='left')\nnew_train.head()","11faca4b":"new_train = pd.merge(new_train, items.drop('item_name', axis = 1), on=['item_id'], how='left')\nnew_train.head()","67c81b46":"new_train = pd.merge(new_train, categories.drop('item_category_name', axis = 1), on=['item_category_id'], how='left')\nnew_train.head()","a9fca8e6":"def generate_lag(train, months, lag_column):\n    for month in months:\n        # Speed up by grabbing only the useful bits\n        train_shift = train[['date_block_num', 'shop_id', 'item_id', lag_column]].copy()\n        train_shift.columns = ['date_block_num', 'shop_id', 'item_id', lag_column+'_lag_'+ str(month)]\n        train_shift['date_block_num'] += month\n        train = pd.merge(train, train_shift, on=['date_block_num', 'shop_id', 'item_id'], how='left')\n    return train","3fceae58":"del items\ndel categories\ndel shops\ndel test","c83619a4":"new_train = downcast_dtypes(new_train)","eea0b575":"import gc\ngc.collect()","229ac335":"%%time\nnew_train = generate_lag(new_train, [1,2,3,4,5,6,12], 'item_cnt_month')","e81097cd":"%%time\ngroup = new_train.groupby(['date_block_num', 'item_id'])['item_cnt_month'].mean().rename('item_month_mean').reset_index()\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'item_id'], how='left')\nnew_train = generate_lag(new_train, [1,2,3,6,12], 'item_month_mean')\nnew_train.drop(['item_month_mean'], axis=1, inplace=True)","74bca6e9":"%%time\ngroup = new_train.groupby(['date_block_num', 'shop_id'])['item_cnt_month'].mean().rename('shop_month_mean').reset_index()\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'shop_id'], how='left')\nnew_train = generate_lag(new_train, [1,2,3,6,12], 'shop_month_mean')\nnew_train.drop(['shop_month_mean'], axis=1, inplace=True)","a9bada3e":"%%time\ngroup = new_train.groupby(['date_block_num', 'shop_id', 'item_category_id'])['item_cnt_month'].mean().rename('shop_category_month_mean').reset_index()\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nnew_train = generate_lag(new_train, [1, 2], 'shop_category_month_mean')\nnew_train.drop(['shop_category_month_mean'], axis=1, inplace=True)","9ac8cced":"%%time\ngroup = new_train.groupby(['date_block_num', 'main_category_id'])['item_cnt_month'].mean().rename('main_category_month_mean').reset_index()\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'main_category_id'], how='left')\n\nnew_train = generate_lag(new_train, [1], 'main_category_month_mean')\nnew_train.drop(['main_category_month_mean'], axis=1, inplace=True)","ac457ce4":"%%time\ngroup = new_train.groupby(['date_block_num', 'sub_category_id'])['item_cnt_month'].mean().rename('sub_category_month_mean').reset_index()\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'sub_category_id'], how='left')\n\nnew_train = generate_lag(new_train, [1], 'sub_category_month_mean')\nnew_train.drop(['sub_category_month_mean'], axis=1, inplace=True)","4ad3ae7d":"new_train.tail()","70d2676c":"new_train['month'] = new_train['date_block_num'] % 12","9366edbf":"holiday_dict = {\n    0: 6,\n    1: 3,\n    2: 2,\n    3: 8,\n    4: 3,\n    5: 3,\n    6: 2,\n    7: 8,\n    8: 4,\n    9: 8,\n    10: 5,\n    11: 4,\n}","cd609917":"new_train['holidays_in_month'] = new_train['month'].map(holiday_dict)","de639e79":"# ruble_dollar = { 12: 33.675610, 13: 35.245171, 14: 36.195442, 15: 35.658811, 16: 34.918525, 17: 34.392044, 18: 34.684944, 19: 36.144526, 20: 37.951523, 21: 40.815324, 22: 46.257598, 23: 55.966912, 24: 63.676710, 25: 64.443511, 26: 60.261687, 27: 53.179035, 28: 50.682796, 29: 54.610770, 30: 57.155767, 31: 65.355082, 32: 66.950360, 33: 63.126499, 34: 65.083095, }","36696e1b":"# new_train['ruble_value'] = new_train.date_block_num.map(ruble_dollar)","9600096e":"moex = {\n    12: 659, 13: 640, 14: 1231,\n    15: 881, 16: 764, 17: 663,\n    18: 743, 19: 627, 20: 692,\n    21: 736, 22: 680, 23: 1092,\n    24: 657, 25: 863, 26: 720,\n    27: 819, 28: 574, 29: 568,\n    30: 633, 31: 658, 32: 611,\n    33: 770, 34: 723,\n}","6fc91654":"new_train['moex_value'] = new_train.date_block_num.map(moex)","0fd3532f":"new_train = downcast_dtypes(new_train)","24c1094c":"import xgboost as xgb","b728b1b3":"new_train = new_train[new_train.date_block_num > 11]\n\n# x_train = new_train[new_train.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n# y_train = new_train[new_train.date_block_num < 33]['item_cnt_month']\n\n# x_valid = new_train[new_train.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n# y_valid = new_train[new_train.date_block_num == 33]['item_cnt_month']\n\n# x_test = new_train[new_train.date_block_num == 34].drop(['item_cnt_month'], axis=1)","a3e1b555":"import gc\ngc.collect()","5a3dc85a":"def fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            df[col].fillna(0, inplace=True)         \n    return df\n\nnew_train = fill_na(new_train)","1b0f9c43":"def xgtrain():\n    regressor = xgb.XGBRegressor(n_estimators = 5000,\n                                 learning_rate = 0.01,\n                                 max_depth = 10,\n                                 subsample = 0.5,\n                                 colsample_bytree = 0.5)\n    \n    regressor_ = regressor.fit(new_train[new_train.date_block_num < 33].drop(['item_cnt_month'], axis=1).values, \n                               new_train[new_train.date_block_num < 33]['item_cnt_month'].values, \n                               eval_metric = 'rmse', \n                               eval_set = [(new_train[new_train.date_block_num < 33].drop(['item_cnt_month'], axis=1).values, \n                                            new_train[new_train.date_block_num < 33]['item_cnt_month'].values), \n                                           (new_train[new_train.date_block_num == 33].drop(['item_cnt_month'], axis=1).values, \n                                            new_train[new_train.date_block_num == 33]['item_cnt_month'].values)\n                                          ], \n                               verbose=True,\n                               early_stopping_rounds = 50,\n                              )\n    return regressor_","f27b8876":"%%time\nregressor_ = xgtrain()","0b237595":"predictions = regressor_.predict(new_train[new_train.date_block_num == 34].drop(['item_cnt_month'], axis = 1).values)","e21db681":"from matplotlib import rcParams\nrcParams['figure.figsize'] = 11.7,8.27\n\ncols = new_train.drop('item_cnt_month', axis = 1).columns\nplt.barh(cols, regressor_.feature_importances_)\nplt.show()","3513ba55":"submission['item_cnt_month'] = predictions","5b08cd6e":"submission.to_csv('sales_faster_learn.csv', index=False)","4a72caf8":"from IPython.display import FileLinks\nFileLinks('.')","a4dbe705":"import json  # need it for json.dumps\nfrom IPython.display import HTML\n\n# Create the correct URLs for require.js to find the Javascript libraries\nvega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + alt.SCHEMA_VERSION\nvega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\nvega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\nnoext = \"?noext\"\n\naltair_paths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n    paths: {paths}\n}});\n\"\"\"\n\n# Define the function for rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        \"\"\"Render an altair chart directly via javascript.\n        \n        This is a workaround for functioning export to HTML.\n        (It probably messes up other ways to export.) It will\n        cache and autoincrement the ID suffixed with a\n        number (e.g. vega-chart-1) so you don't have to deal\n        with that.\n        \"\"\"\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay defined and keep track of the unique div Ids\n    return wrapped\n\n\n@add_autoincrement\ndef render_alt(chart, id=\"vega-chart\"):\n    # This below is the javascript to make the chart directly using vegaEmbed\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vegaEmbed) {{\n        const spec = {chart};     \n        vegaEmbed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n    }});\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(paths=json.dumps(altair_paths)),\n    \"<\/script>\"\n)))","b576d2e2":"alt.data_transformers.enable('default', max_rows=None)","160540df":"train = pd.read_csv('..\/input\/sales_train.csv')","3f3c02c6":"alt_df = train.groupby('date_block_num')['item_cnt_day'].sum().rename('sum').reset_index()\nalt_df.head(2)","af994d5d":"# Generate a mapping for date_block_num to month and year.\ndict_date_block_num = {0: 'January 2013', 1: 'February 2013', 2: 'March 2013', 3: 'April 2013',\n4: 'May 2013', 5: 'June 2013', 6: 'July 2013', 7: 'August 2013',\n8: 'September 2013', 9: 'October 2013', 10: 'November 2013', 11: 'December 2013',\n12: 'January 2014', 13: 'February 2014', 14: 'March 2014', 15: 'April 2014',\n16: 'May 2014', 17: 'June 2014', 18: 'July 2014', 19: 'August 2014',\n20: 'September 2014', 21: 'October 2014', 22: 'November 2014', 23: 'December 2014',\n24: 'January 2015', 25: 'February 2015', 26: 'March 2015', 27: 'April 2015',\n28: 'May 2015', 29: 'June 2015', 30: 'July 2015', 31: 'August 2015',\n32: 'September 2015', 33: 'October 2015'}","ee410156":"alt_df['date_block_num'].replace(dict_date_block_num, inplace=True)\nalt_df.head(2)","8451362b":"split = alt_df['date_block_num'].str.split(\" \", n = 1, expand = True) ","2ff83726":"alt_df_two = alt_df.copy()\nalt_df_two['month'] = split[0]\nalt_df_two['year'] = split[1]\nalt_df_two.head()","2187c836":"chart1 = alt.Chart(alt_df_two).mark_area().encode(\n    alt.X('date_block_num:T', axis = alt.Axis(labelAngle=-45), title='Time'),\n    alt.Y('sum:Q', title='Items Sold'),\n    alt.Color('month:N', scale=alt.Scale(scheme='category20'), sort = ['Januray'], title='Month')\n).properties(width=1000, title='Kaggle Data Science Competition: Sales Prediction in Russian Stores')","d5aed4c1":"alt.themes.enable('opaque')","70c9ed78":"render_alt(chart1)","c6509883":"chart2 = alt.Chart(alt_df_two).mark_bar().encode(\n    alt.X('year:N'),\n    alt.Y('sum(sum):Q'),\n    alt.Color('year:N')\n).properties(width=1000)","b7f69754":"render_alt(chart2)","4d720041":"alt_df_three = train.groupby(['date_block_num', 'shop_id'])['item_cnt_day'].sum().rename('sum').reset_index()\nalt_df_three['date_block_num'].replace(dict_date_block_num, inplace=True)\n\nsplit = alt_df_three['date_block_num'].str.split(\" \", n = 1, expand = True) \nalt_df_three['month'] = split[0]\nalt_df_three['year'] = split[1]\n\nalt_df_three.head(2)","241e94c2":"chart3 = alt.Chart(alt_df_three).mark_rect().encode(\n    alt.Y('month'),\n    alt.X('shop_id:N'),\n    alt.Color('mean(sum):Q')\n)\n\nchart4 = alt.Chart(alt_df_three).mark_rect().encode(\n    alt.Y('year'),\n    alt.X('shop_id:N'),\n    alt.Color('mean(sum):Q')\n)\n\nchart = (chart3 & chart4)","b36eb874":"render_alt(chart)","0374c7b2":"Let's perform a similar train-test analysis for item_ids","8dc56f9c":"Once again, using Google translate, I found that this is an antivirus sold to 522 people and the price is probabaly the cost of one installation times 522. Let's see if there are other transactions related to this software in our training set.","817ffd4b":"# EDA","a8498e82":"### Grouping Common Categories and Extracting Sub-Categories","be446890":"Viola! That worked. Time to extend this to all months using some neat code","0b57e806":"Before we do that, let's find out more about the 5100 items in the test set. What categories to they belong to? What categories do we not have to make predictions in the test set for","fc347d5b":"### Add month feature!","1e4ad6e4":"As we can see, the test set is different in size and structure when compared to the training set. We have the features 'shop_id' and 'item_id' in the test set, which are present in the trianing set as well. Each observation in the test set has an ID associated with it. If we look at our submission file, we need to submit the monthly count (item_cnt_month) for that particular ID. This means we need to predict a number for the monthly sale quantity of a particular item at a particular shop.","387afb7c":"Using google translate, I understood that this item is related to point of delivery and the Russian shipment company 'Boxberry'. Let's look at some of the other daiy sales for this item","ebee02b7":"Using [Google translate API](https:\/\/pypi.org\/project\/googletrans\/) and measuring pair wise Levenstein distance with a python [library](https:\/\/github.com\/luozhouyang\/python-string-similarity), I generated a matrix plot to observe similarity with the shop names\n\n![Matrix Plot](https:\/\/i.imgur.com\/wIvpacw.png) ![Threshold](https:\/\/i.imgur.com\/cH94yt2.png)","3875f560":"Upon careful observation, the city Yakutsk is represented as *'!\u042f\u043a\u0443\u0442\u0441\u043a'* and *'\u042f\u043a\u0443\u0442\u0441\u043a'*. We should make sure they are the same so when we label encode these city names, they will fall under the same cateogory. For reference, the english tranlation of the cities is below.\n\n'!Yakutsk', 'Adygea', 'Balashikha', 'Volzhsky', 'Vologda', 'Voronezh', 'Outbound', 'Zhukovsky', 'Online', 'Kazan', 'Kaluga', 'Kolomna', 'Krasnoyarsk', 'Kursk', 'Moscow', 'Mytishchi', 'N.Novgorod', 'Novosibirsk', 'Omsk', 'RostovNaDonu', 'St.', 'Samara', 'Sergiev', 'Surgut', 'Tomsk', 'Tyumen', 'Ufa', 'Khimki', 'Digital', 'Chekhov', 'Yakutsk', 'Yaroslavl'\n\nYou will notice we only extract the St. from St. Petersburg but that's okay as we will label encode these categorical variables. ","acb61a0e":"Well then, this means there are certain items that are present in the test set but completely absent in the training set! Can we put a number to this to get an intuition?","de215c44":"new_train = new_train.reset_index(drop = True)\nnew_train.to_feather('new_train_baller')","98cc1ee4":"### Lag for monthly Item-Target mean","84f2af59":"Let's look at the distribution of the training and test set to understand our dataset better and check for overlap, or lack thereof.","784a438c":"### Duplicate Shops","d32bd46e":"## Dealing with Outliers","fb687679":"Great, that worked! Let's just add these codes to the shops dataframe. We can get rid of *'shop_name'* and *'city'* while preserving the *'shop_id'* and *'city_label'*","49922f8f":"Now, we will inspect the 'item_price' field for low priced outliers","380641e9":"Upon examing the above matrices, one can spot a trend of main and sub-categories. These are in the dataframe, before the hyphen. Let's extract them!","08b8f3fe":"Having a look at the item id 20949 that has been sold the most number of times, it is a plastic bag!","96d5de12":"# Training","cd4786f3":"And just to satisfy my curiosity bowels, there is only one item under item category 71 and it is indeed present in the test set!","a0693ebe":"1. The Shop Id's are evenly spread out, unlike the training set. The font size of labels quickly tells me that there are certain Shop Id's missing in the test set as the bars in the training set 'shop_id' plot were more tightly packed.\n2. While item id's in the histogram are binned, the spikes are less in the test set. The test set is much smaller in shape than the training set, and naturally, the frequency values are significantly lower. It is tough to be draw more insights from this histogram.\n\nIt seems there might be some values of shop_id and item_id completely missing in the test set. Let's have a closer look and put some numbers or percentages to these missing values.","e448ca91":"Another outlier! This time with 'item_price'. Let's inspect this a little further before we decide to remove it.","847a9036":"### Russian Stock Exchange Trading Volume (in Trillions)","f154730f":"### Merging Shops, Items, Categories dataframes to add the city label, category_id, main category and sub-category feature","895c74f1":"1. Plotting Ideas: Number of unique items sold each month\n2. Sales graph, Revenue graph","e905b0ed":"Great, so all shops id's in the test set are also present in the training set. But there is an issue regarding duplicate shops that was talked about in the competition discussions. Let's have a look at the shops dataframe to identify this issue.","e6498d4d":"Now, let's do the distribution analysis for the test set and see if we can spot any differences","81c00c1b":"Using the same method, I generated a matrix plot to observe similarity with the shop names\n\n![Category Matrix Plot](https:\/\/i.imgur.com\/O5iTFj2.png) ![Category Matrix Threhsold](https:\/\/i.imgur.com\/YZQrVRM.png)","d49fc707":"And yes, there exists a value of -1. This is an error and we should inspect this observation further","c56dc227":"## Appending Test Set to Training Set","8a91bfaa":"## Items Analysis","b7268238":"### Russian Ruble Price per month!","c2351fc7":"# Reading Data","deb7534f":"Upon examing the above matrixes and 'similar' shop names, one observes that shops start with their city name! This can be a useful feature. Also, 'shop_id's 0 and 1 are the same as 57 and 58 respectively. The only difference is the Russian word '\u0444\u0440\u0430\u043d' attached to both shops 0 and 1. Shops 10 and 11 have the exact same name excpet the last character ('?' vs '2'). These duplicates should be removed!","bf64e00a":"Let's see if there are other observations for this item (2973) which can help us determine its price","bfd2e981":"### Extracting City ","817066df":"### Lag for Target Variable","e4bf535f":"## Test Set Distribution","a86bf969":"## Generating prodcuct of Shop-Item pairs for each month in the training data","9b96d791":"### Lag for monthly Shop-Category mean","87215550":"By default, pandas fills the dataframes with NaN. That's why we use fillna to replace all NaN's with zero.","bce947f3":"As we can see, January 2013 contains 365,175 intersections of shops and items. Most of these will have no sales and we can verify this once we check our training set, which has been grouped by month, to see which 'items' x 'shops' combinations have a sale count associated with them. \n\nBut before that, we need to generate this cartesian product for all 33 months in the training set. Before generating it for all months, I will generate it for February 2013, concatenate it with January 2013 and produce a dataframe.","0b1e7ae8":"Now, let's generate sales data for each shop and item present in the training set. We should do this for each month, as the final prediction is for the monthly count of sales for a particular shop and item","7d8ddb63":"Ran out of memory when trying to create a dataframe from january and february lists. The trick was to convert the lists to a numpy array. The handy numpy method 'vstack' will shape the cartesian_test object in the right manner, so we can convert it into a long form dataframe.","80190918":"### Add Holiday feature!","458cdc3d":"There are 363 items that are present in the test set but completely absent in the training set. This doesn't mean that the sales prediction against those items must be zero as new items can be added to the market or we simply didn't possess the data for those items before. The fascinating questions pops though, how do you go about predicting them? ","a016713b":"## Aggregating sales to a monthly level and clipping target variable","b9f2ed26":"There are a lot many more items in the training set than there are in the test set. However, this doesn't mean that the training set contains all of the items in the test set. For that, we need to see if every element of the test set is present in the training set. Let's write some simple code to achieve this.","2dc6645f":"## Shops Analysis","4911dbde":"### Lag for Monthly Sub-Category mean","51ffdd21":"The above graphs are a nice way to look at the raw distribtion of the test dataset. Here are some observations:\n\n1. We have 60 'shop_id's but there is an uneven distribtution of these in the dataset. Four (<7%) of these shops make up ~25 percent of this dataset. These are shops (31, 25, 54, 28).\n\n\n2. The Item IDs seem to have variations in frequency. We can't attribute a reason to this yet but we can inspect this further. Certain cateogeries are bound to sell better and maybe items under the same category are closer to each other as far as their ID distributions are concerned\n\n\n3. From the vast empty spaces in the histograms of 'item_price' and 'item_cnt_day', we can infer that there are outliers in their distribution. **Let's write some simple code below to put a value to these outliers.**\n\n\n4. Plotting the individual months from January 2013 to October 2015, it is interesting to see that the block month 12, corresponding to December 2013, had the highest number of sales. Month 23, which corresponds to December 2014, had the second highest number of sales. Shortly, we will use some better graphs to observe the monthly sale trends.","d5ac2555":"## Generating Lag Features and Mean-Encodings","3eb052f3":"First, let's insert the date_block_num feature for the test set! Using insert method of pandas to place this new column at a specific index. This will allow us to concatenate the test set easily to the training set before we generate mean encodings and lag features","ace2a4f1":"Now we need to merge our two dataframes. For the intersecting, we will simply put the values that exist in the dataframe x. For the remaining rows, we will sub in zero. Remember, the columns you want to merge on are the intersection of shop_id, item_id, and date_block_num","bf767b2e":"However, this doesn't mean that the training set contains all of the shops present in the test set. For that, we need to see if every element of the test set is present in the training set. Let's write some simple code to see if the test set list is a subset of the training set list.","910b5e64":"Indeed, this item seems to have been sold for quite a high price so its value of -1 should be changed. We should replace it with the median of its price, but this should be calculated for the shop (ID 12) for which this outlier exists. If there are other sales for this item in that shop for the same month (date_block_num 4), then we should calculate the median using that","281ae25b":"### Lag for monthly Shop-Target mean","20906bae":"Code to render Altair Charts","edb05745":"### Lag for Monthly Main-Category mean","d8541a18":"Nope! The outlier is the only transaction. It is justifiable to remove this observation from the training set.","f5aa6807":"So item 11373 was sold 2169 times at shop 12 on a single day in October. Let's inspect this outlier a bit more.","08580a61":"We can see that item 11373 usually has sells much less. I calculated the median of it's 'item_cnt_day' value to be 4. This confirms that the high value of 2169 is an anomaly and we should get rid of it."}}