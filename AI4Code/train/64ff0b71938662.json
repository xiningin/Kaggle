{"cell_type":{"7ab2cfd0":"code","976673f4":"code","e844101e":"code","0bfbcf90":"code","72ffebc4":"code","df7baf6a":"code","7a91559b":"code","7c4377d0":"code","742a4689":"code","dcbd0318":"code","257c29ce":"code","3f63cb13":"code","14e82eec":"code","9b1754cb":"code","e36720d1":"code","5de505af":"code","98b4e788":"code","2aa9673d":"code","cd2957fc":"code","756f3893":"code","df557950":"code","492d7873":"code","77a3f2dc":"code","e97c8708":"code","043a5370":"code","a8481365":"code","b2a0f37f":"code","b7a7592f":"code","1815136f":"code","2eaae822":"code","a4bf8f31":"code","29868e46":"code","c0fb6aa4":"code","9fe62593":"code","52ff1721":"code","7b1e1a0a":"code","82a6ef3f":"code","65d0205c":"code","a8b0590d":"code","9f7026c2":"code","15e01431":"code","48dbf808":"code","44f91868":"code","6edc69a8":"markdown","fab47a32":"markdown","0d4b0a43":"markdown","115643a8":"markdown","cead94ed":"markdown","e2c259ca":"markdown","84d0f81a":"markdown","3bcfe4a7":"markdown","de0bdbbf":"markdown","437347df":"markdown","f7814f4b":"markdown","97d84fd9":"markdown","062ba5a6":"markdown","a93a58b0":"markdown","8a11eeb6":"markdown","ce7f022b":"markdown","82ad33b7":"markdown","fc5e0307":"markdown","420b495a":"markdown"},"source":{"7ab2cfd0":"import pandas as pd","976673f4":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","e844101e":"df.head()","0bfbcf90":"df.tail()","72ffebc4":"df.describe().T","df7baf6a":"df.describe(include=['object']).T","7a91559b":"    df.shape","7c4377d0":"df.target.value_counts()","742a4689":"df['length_text'] = df['text'].apply(len)\ndf.head()","dcbd0318":"df.info()","257c29ce":"df.isnull().sum()","3f63cb13":"import seaborn as se\nimport matplotlib.pyplot as plt","14e82eec":"# visualize on target variable\nse.countplot(df.target)\nplt.show()","9b1754cb":"#Create visualization of the distribution of text length in comparision to target feature\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True,figsize=(14,6))\nse.distplot(df[(df['target'] == 1)]['length_text'], ax=ax1, kde=False, color='green',label='Disater Tweets')\nse.distplot(df[(df['target'] == 0)]['length_text'],ax=ax2, kde=False, color='red',label='Non-Disater Tweets');\nf.suptitle('Tweet length distribution')\nf.legend(loc='upper right')\nax1.grid()\nax2.grid()\nplt.show()","e36720d1":"plt.figure(figsize=(12,6))\nse.barplot(y=df['keyword'].value_counts()[:20].index,x=df['keyword'].value_counts()[:20])\nplt.show()","5de505af":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","98b4e788":"def word_cloud(target):\n    words = ''\n    for msg in df[df['target']==target]['text']:\n        msg = msg.lower()\n        msg = re.sub('\\[.*?\\]', '', msg)\n        msg = re.sub('https?:\/\/\\S+|www\\.\\S+', '', msg)\n        msg = re.sub('<.*?>+', '', msg)\n        msg = re.sub('[%s]' % re.escape(string.punctuation), '', msg)\n        msg = re.sub('\\w*\\d\\w*', '', msg)\n        msg = re.sub('\\n', '', msg)\n        msg = re.sub('\\x89\u00fb','',msg)\n        msg = re.sub('rt','',msg)\n        words += msg + ''\n    wordcloud = WordCloud(background_color='black', width = 700, height = 400).generate(words)\n    plt.figure(figsize = (15,7))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","2aa9673d":"## Not disaster\nword_cloud(0)","cd2957fc":"## Real Disaster\nword_cloud(1)","756f3893":"def text_preprocess(text):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",text)\n    letters_only = letters_only.lower()\n    words = letters_only.split()             \n    words = [w for w in words if not w in stopwords.words(\"english\")]\n    token = [word_tokenize(word) for word in words]\n    token = [''.join(ele) for ele in token]\n    \n    clean_text = []\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in token]\n    clean_text.append(\" \".join(words))\n    \n    return (clean_text[0])","df557950":"from tqdm import tqdm, tqdm_notebook\ntqdm.pandas()","492d7873":"df[\"lemme_text\"] = df['text'].progress_apply(lambda x: text_preprocess(x))","77a3f2dc":"df['lemme_text']","e97c8708":"df.head()","043a5370":"from sklearn.feature_extraction.text import CountVectorizer","a8481365":"vec = CountVectorizer(analyzer='word')\nX = vec.fit_transform(df[\"lemme_text\"])","b2a0f37f":"X.toarray()","b7a7592f":"from sklearn.feature_extraction.text import TfidfTransformer","1815136f":"tfifd_X = TfidfTransformer().fit_transform(X)","2eaae822":"tfifd_X","a4bf8f31":"from sklearn.model_selection import train_test_split\nmsg_train,msg_test,label_train,label_test = train_test_split(tfifd_X.toarray(),df['target'],test_size = 0.25,random_state=15)","29868e46":"msg_test","c0fb6aa4":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","9fe62593":"lr = LogisticRegression()\nmn = MultinomialNB()\ndtc = DecisionTreeClassifier()\nknn = KNeighborsClassifier(n_neighbors=3)\nrfc = RandomForestClassifier()\nmodels = {lr: 'LogisticRegression',\n          mn: 'MultinomialNB',\n          dtc: 'DecisionTreeClassifier',\n          knn:'KNeighborsClassifier',\n          rfc: 'RandomForestClassifier'}","52ff1721":"def model_performance(mod, mod_name, X_test, y_test, y_predict):\n    print(mod_name)\n    print('\\n')\n    print('Accuracy score is : {}'.format(accuracy_score(y_test, y_predict)))\n    print('\\n')\n    print('confusion_matrix : ')\n    print(confusion_matrix(y_test, y_predict))\n    print('\\n')\n    print('classification_report :')\n    print(classification_report(y_test, y_predict))\n    print('*'*60)","7b1e1a0a":"dic = {\"algo\":[],\"accuracy_score\":[]}\nfor mod, mod_name in models.items():\n    fit = mod.fit(msg_train,label_train)\n    # Making predictions\n    predict = fit.predict(msg_test)\n    # Reviewing the metrics\n    model_performance(mod, mod_name, msg_test, label_test, predict)\n    dic[\"algo\"].append(mod_name)\n    dic[\"accuracy_score\"].append(accuracy_score(label_test,predict))","82a6ef3f":"mode = LogisticRegression()\nmode.fit(tfifd_X,df['target'])","65d0205c":"from pickle import dump\ndump(vec, open('disaster_countvectorizer.pkl', 'wb'))\ndump(mode, open('disaster_decision_model.pkl', 'wb'))","a8b0590d":"from pickle import load\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string","9f7026c2":"def text_preprocess(pre_text):\n    # Removing special characters and digits\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",pre_text)\n    \n    # change sentence to lower case\n    letters_only = letters_only.lower()\n    words = letters_only.split()\n    \n    # remove stop words                \n    words = [w for w in words if not w in stopwords.words(\"english\")]\n    #Tokenization\n    token = [word_tokenize(word) for word in words]\n    token = [''.join(ele) for ele in token]\n    \n    clean_text = []\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in token]\n    clean_text.append(\" \".join(words))\n    \n    return (clean_text[0])","15e01431":"def predict(text):\n    \n    # Loading pretrained CountVectorizer from pickle file\n    vectorizer = load(open('disaster_countvectorizer.pkl', 'rb'))\n    \n    # Loading pretrained logistic classifier from pickle file\n    classifier = load(open('disaster_decision_model.pkl', 'rb'))\n    \n    clean_text = text_preprocess(text)\n    \n    # Converting text to numerical vector\n    clean_text_vec = vectorizer.transform([clean_text])\n    \n    # Converting sparse matrix to dense matrix\n    text_input = clean_text_vec.toarray()\n    \n    # Prediction\n    prediction = classifier.predict(text_input)\n    \n    return prediction","48dbf808":"text = input(\"Enter a tweet to check its sentiment: \")\n\nprediction = predict(text)\n\nprint(prediction)","44f91868":"if(prediction == 0):\n    print(\"It's not Disaster\")\nelse:\n    print(\"It's Disaster\")","6edc69a8":"> Train Test Split :","fab47a32":"# TFIDF Features :\n> * Term Frequency: is a scoring of the frequency of the word in the current document.\n> * Inverse Document Frequency: is a scoring of how rare the word is across documents.","0d4b0a43":"> It seems train dataset has 'NaN' values for keyword and location columns. Let's explore the dataset further for missing data values.\n","115643a8":"# Building a Text Classification model :","cead94ed":"> The train CSV contain 7613 Rows and 5 columns.","e2c259ca":"# Data Exploration :","84d0f81a":"> Creating a new column which will have the length of each text.","3bcfe4a7":"# Data Preview :","de0bdbbf":"> It seems like the distribution of both kind of tweets to be almost same. About 120 to 140 characters in a tweet are the most common among both.","437347df":"## Prediction on Future Data Points :","f7814f4b":"# Bag of Words - Countvectorizer :-\n> The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.","97d84fd9":"> * Number of records with missing location: 2533\n> * Number of records with missing keywords: 61","062ba5a6":"> In this Dataset 3271 tweets is a real disaster (1) tweet whereas 4342 tweets is not a disaster (0) tweet.","a93a58b0":"# Loading the Dataset :","8a11eeb6":"# Real or Not? NLP with Disaster Tweets\nPredict which Tweets are about real disasters and which ones are not.\n\n**Competition Description :**\n* Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). But, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\n**Acknowledgments :**\n* This dataset was created by the company figure-eight and originally shared on their \u2018Data For Everyone\u2019 [website here.](https:\/\/appen.com\/resources\/datasets\/)\n\n**Columns :**\n* `id` - a unique identifier for each tweet\n\n* `text` - the text of the tweet\n\n* `location` - the location the tweet was sent from (may be blank)\n\n* `keyword` - a particular keyword from the tweet (may be blank)\n\n* `target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","ce7f022b":"## Saving the Model and Vectorizer in Pickle :","82ad33b7":"# Data Cleaning :\n> * Removing special characters and digits.\n> * Making all text to lower case.\n> * `Stopwords` Removal.\n> * `Tokenization`: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want.\n> * `Lemmatization` : Returns the base or dictionary form of a word, which is known as the lemma.","fc5e0307":"## Conclusion :\n> In this Kaggle challenge to classify tweets into disaster tweets in real or not?. \n    > * First, I have analyzed and explored all the provided tweets data to visualize the statistical and other properties of the presented data.\n    > * Next, I performed some exploratory analysis of the data to check type of the data, whether there are unwanted features and if features have missing data. Based on the analysis.\n    > * The \u2018text\u2019 columns is all text data along with alpha numeric, special characters and embedded URLs.The \u2018text\u2019 column data needs to be cleaned and pre processed and vectorized before it can be used with a machine learning algorithm for the classification of the tweets.\n    > * After pre processing the train data, the data was vectorized using `CountVectorizer` and `TFIDF` features.\n    > * Then various classifiers were fit on the data and predictions were made.\n    > *`LogisticRegression()` fits are model best with less time complexity and with Accuracy Score of 80.25 %.","420b495a":"# WordCloud :"}}