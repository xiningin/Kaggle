{"cell_type":{"79cb609f":"code","6c97d2ec":"code","24b3123d":"code","72899790":"code","0ab65cd3":"code","8eb34532":"code","10436f3b":"code","6077d3e7":"code","cf1950cf":"code","bdd2106e":"code","ca9628ea":"code","8eec9e03":"code","6af376ce":"code","c7c7c5e3":"code","2269dfd6":"code","ff6f9e5b":"code","2c1f706d":"code","e604573b":"code","3e244eed":"code","e66f5d8d":"code","c31b90b9":"code","8034b472":"code","acc33fd7":"code","2679e5a5":"code","063e8877":"code","a6b980e3":"code","e7067128":"markdown","06cd39f7":"markdown","ee023fdc":"markdown","b8f3e674":"markdown","8b8cb841":"markdown","3b5d0aa2":"markdown","d5a9c7df":"markdown","866f6d3f":"markdown","2ad02dea":"markdown","2156011f":"markdown","637c7fe1":"markdown","819d2632":"markdown"},"source":{"79cb609f":"import gc\nimport os\nimport random\nimport time\nfrom contextlib import contextmanager\n\nimport fasttext\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as torchdata\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","6c97d2ec":"def set_seed(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    \n    \n@contextmanager\ndef timer(name: str):\n    t0 = time.time()\n    print(f\"[{name}] start\")\n    yield\n    print(f\"[{name}] done in {time.time() - t0:.2f} s\")","24b3123d":"set_seed(1213)","72899790":"with timer(\"Load dataset\"):\n    train = pd.read_csv(\"..\/input\/rkcup-1\/train.csv\")\n    test = pd.read_csv(\"..\/input\/rkcup-1\/test.csv\")\n    \ntrain.head()","0ab65cd3":"train[\"word_count\"] = train[\"separated_text\"].map(lambda x: len(x.split()))\ntest[\"word_count\"] = test[\"separated_text\"].map(lambda x: len(x.split()))\n\ntrain.head()","8eb34532":"train[\"word_count\"].max(), test[\"word_count\"].max()","10436f3b":"train_unique_words = set(train[\"separated_text\"].map(lambda x: x.split()).explode().unique())\ntest_unique_words = set(test[\"separated_text\"].map(lambda x: x.split()).explode().unique())\nlen(train_unique_words.union(test_unique_words))","6077d3e7":"# \u5358\u8a9e\u3092\u9023\u756a\u306e\u6570\u5024\u306b\u5909\u63db\u3059\u308b\u8f9e\u66f8\nword_index_dict = {\n    word: i + 1 for i, word in enumerate(train_unique_words.union(test_unique_words))\n}","cf1950cf":"class TextDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, word_index_dict: dict, max_length=80):\n        self.texts = df[\"separated_text\"].map(lambda x: x.split()).values\n        \n        # target\u3068\u3044\u3046\u30ab\u30e9\u30e0\u306e\u6709\u7121\u3067\u5224\u5b9a\u3059\u308b\n        if \"target\" in df.columns:\n            self.target = df[\"target\"].values\n            self.train = True\n        else:\n            self.train = False\n            \n        self.word_index_dict = word_index_dict\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx: int):\n        text = self.texts[idx]\n        idx_list = [self.word_index_dict[w] for w in text]\n        n_padding = self.max_length - len(text)\n        idx_list += [0] * n_padding\n        if self.train:\n            target = self.target[idx]\n            # long\u578b\u306fPyTorch\u306e\u6574\u6570\u578b\n            return torch.tensor(idx_list).long(), target\n        else:\n            return torch.tensor(idx_list).long()","bdd2106e":"train_dataset = TextDataset(train, word_index_dict)\ntest_dataset = TextDataset(test, word_index_dict)\n\n# __len__\u306e\u51fa\u529b\u304c\u78ba\u8a8d\u3067\u304d\u308b\nlen(train_dataset), len(test_dataset)","ca9628ea":"train_dataset[0]","8eec9e03":"test_dataset[0]","6af376ce":"with timer(\"Load fastText model\"):\n    ft_model = fasttext.load_model(\n        \"..\/input\/japanese-word-embeddings\/cc.ja.300.bin\")","c7c7c5e3":"embedding_matrix = np.zeros((len(word_index_dict) + 1, 300))\nfor word in train_unique_words.union(test_unique_words):\n    index = word_index_dict[word]\n    embedding_matrix[index] = ft_model.get_word_vector(word)\n    \nembedding_matrix","2269dfd6":"del ft_model\ngc.collect();","ff6f9e5b":"class Model(nn.Module):\n    def __init__(self, embedding_matrix: np.ndarray, padding_idx=0):\n        super().__init__()\n        vocab_size = len(embedding_matrix)\n        embedding_dim = embedding_matrix.shape[1]\n\n        # Embedding\u306e\u7528\u610f\n        self.embedding = nn.Embedding(\n            vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n        self.embedding.weight = nn.Parameter(\n            torch.tensor(embedding_matrix).float(), requires_grad=True)\n        \n        # CNN\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim \/\/ 2, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv1d(in_channels=embedding_dim \/\/ 2, out_channels=embedding_dim \/\/ 4, kernel_size=3),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool1d(1))\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(embedding_dim \/\/ 4, 1))\n        \n    def forward(self, x):\n        x = self.embedding(x).transpose(1, 2)\n        x = self.cnn(x).squeeze(2)\n        return self.classifier(x)","2c1f706d":"def train_kfold(train: pd.DataFrame, \n                test: pd.DataFrame, \n                word_index_dict: dict,\n                embedding_matrix: np.ndarray,\n                n_splits=5,\n                n_epochs=20):\n    # Out-of-fold\u306a\u4e88\u6e2c\u3092\u5165\u308c\u3066\u3044\u304f\u305f\u3081\u306e\u914d\u5217\n    oof = np.zeros(len(train), dtype=float)\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u63a8\u8ad6\u7d50\u679c\u3092\u5165\u308c\u3066\u3044\u304f\u305f\u3081\u306e\u914d\u5217\n    test_pred = np.zeros(len(test), dtype=float)\n    \n    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n    test_dataset = TextDataset(test, word_index_dict)\n    test_loader = torchdata.DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n    for i, (trn_idx, val_idx) in enumerate(kf.split(train)):\n        with timer(f\"Fold {i + 1} training\"):\n            X_trn = train.loc[trn_idx, :].reset_index(drop=True)\n            X_val = train.loc[val_idx, :].reset_index(drop=True)\n            train_dataset = TextDataset(X_trn, word_index_dict)\n            valid_dataset = TextDataset(X_val, word_index_dict)\n            \n            train_loader = torchdata.DataLoader(train_dataset, batch_size=64, shuffle=True)\n            valid_loader = torchdata.DataLoader(valid_dataset, batch_size=64, shuffle=False)\n            \n            model = Model(embedding_matrix)\n            criterion = nn.BCEWithLogitsLoss()\n            optimizer = optim.Adam(model.parameters(), lr=1e-3)\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, verbose=True)\n            \n            best_auc = 0.0\n            for epoch in range(n_epochs):\n                print(f\"Epoch [{epoch + 1}\/{n_epochs}]\")\n                for x_batch, y_batch in train_loader:\n                    output = model(x_batch).view(-1)\n                    loss = criterion(output, y_batch.float())\n                    \n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n                    \n                avg_val_loss = 0.0\n                targets = []\n                preds = []\n                for x_batch, y_batch in valid_loader:\n                    targets.append(y_batch.detach().cpu().numpy())\n                    with torch.no_grad():\n                        output = model(x_batch).view(-1)\n                        loss = criterion(output, y_batch.float())\n                        \n                        avg_val_loss += loss.item() \/ len(valid_loader)\n                        preds.append(output.sigmoid().detach().cpu().numpy())\n                print(f\"Loss: {avg_val_loss:.5f}\")\n                auc = roc_auc_score(y_true=np.concatenate(targets), y_score=np.concatenate(preds))\n                print(f\"AUC: {auc:.5f}\")\n                if auc > best_auc:\n                    print(\"Save best model\")\n                    torch.save(model.state_dict(), \"best.pth\")\n                    best_auc = auc\n                scheduler.step()\n                    \n            model.load_state_dict(torch.load(\"best.pth\"))\n            preds = []\n            for x_batch, _ in valid_loader:\n                with torch.no_grad():\n                    output = model(x_batch).sigmoid().view(-1).detach().cpu().numpy()\n                    preds.append(output)\n            preds = np.concatenate(preds)\n            oof[val_idx] = preds\n            \n            preds = []\n            for x_batch in test_loader:\n                with torch.no_grad():\n                    output = model(x_batch).sigmoid().view(-1).detach().cpu().numpy()\n                    preds.append(output)\n            preds = np.concatenate(preds)\n            test_pred += preds \/ n_splits\n    return oof, test_pred","e604573b":"oof_cnn, test_pred_cnn = train_kfold(train, test, word_index_dict, embedding_matrix, n_epochs=30)","3e244eed":"score = roc_auc_score(y_true=train[\"target\"].values, y_score=oof_cnn)\nprint(f\"OOF AUC: {score:.4f}\")","e66f5d8d":"class Model(nn.Module):\n    def __init__(self, embedding_matrix: np.ndarray, padding_idx=0):\n        super().__init__()\n        vocab_size = len(embedding_matrix)\n        embedding_dim = embedding_matrix.shape[1]\n\n        # Embedding\u306e\u7528\u610f\n        self.embedding = nn.Embedding(\n            vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n        self.embedding.weight = nn.Parameter(\n            torch.tensor(embedding_matrix).float(), requires_grad=True)\n        \n        encoder = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4)\n        self.transformer = nn.TransformerEncoder(encoder, num_layers=1)\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(embedding_dim, 1))\n        \n    def forward(self, x):\n        x = self.embedding(x).transpose(0, 1)\n        x = self.transformer(x)\n        x, _ = torch.max(x, dim=0)\n        return self.classifier(x)","c31b90b9":"oof_trns, test_pred_trns = train_kfold(train, test, word_index_dict, embedding_matrix, n_epochs=30)","8034b472":"score = roc_auc_score(y_true=train[\"target\"].values, y_score=oof_trns)\nprint(f\"OOF AUC: {score:.4f}\")","acc33fd7":"oof = 0.8 * oof_trns + 0.2 * oof_cnn\nscpre = roc_auc_score(y_true=train[\"target\"].values, y_score=oof)\nprint(f\"OOF AUC: {score:.4f}\")","2679e5a5":"sample = pd.read_csv(\"..\/input\/rkcup-1\/sample_submission.csv\")\nsample.head()","063e8877":"sample[\"target\"] = 0.2 * test_pred_cnn + 0.8 * test_pred_trns\nsample.to_csv(\"fasttext_nn_submission.csv\", index=False)\nsample.head()","a6b980e3":"pd.read_csv(\"fasttext_nn_submission.csv\").tail()","e7067128":"## \u30e2\u30c7\u30ea\u30f3\u30b0\u6e96\u5099\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f`fastText`\u306eEmbedding\u3092\u5165\u529b\u5c64\u3068\u3057\u305fCNN\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3057\u3066\u3044\u304d\u307e\u3059\u3002CNN\u30e2\u30c7\u30eb\u306fPyTorch\u3067\u5b9f\u88c5\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n\u30d0\u30c3\u30c1\u3067\u5165\u529b\u3057\u3066\u3044\u304f\u305f\u3081\u306b\u6587\u7ae0\u306e\u5358\u8a9e\u6570\u3092\u78ba\u8a8d\u3057\u3066\u304a\u304d\u3001\u6700\u5927\u306e\u5358\u8a9e\u6570\u3067\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3088\u3046\u306b\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u306e\u3067\u3001\u307e\u305a\u5404\u6587\u7ae0\u306e\u5358\u8a9e\u6570\u3092\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","06cd39f7":"train\u306e\u65b9\u306f\u30bf\u30fc\u30b2\u30c3\u30c8\u3082\u8fd4\u3063\u3066\u6765\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059","ee023fdc":"\u5b9f\u969b\u306b\u51fa\u529b\u304c\u3069\u3046\u306a\u308b\u304b\u78ba\u8a8d\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","b8f3e674":"## \u5b66\u7fd2\u306e\u30eb\u30fc\u30d7\n\n\u3055\u3066\u3001\u5b66\u7fd2\u3092\u3057\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002","8b8cb841":"## PyTorch\u306e\u30e2\u30c7\u30eb\u306e\u7528\u610f\n\n\u6700\u521d\u306e\u5c64\u304c`nn.Embedding`\u306b\u306a\u3063\u305fCNN\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002","3b5d0aa2":"### fastText\u306e\u30e2\u30c7\u30eb\u306e\u7528\u610f\n\n\u4eca\u5ea6\u306f\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u307e\u305a\u3001`fastText`\u306e\u30e2\u30c7\u30eb\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5fc5\u8981\u306a\u5206\u306e\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u3092numpy\u306e\u914d\u5217\u306b\u5165\u308c\u3066\u304a\u304d\u307e\u3059\u3002\u3053\u306e\u914d\u5217\u306f\u5f8c\u307b\u3069`nn.Embedding`\u306e\u521d\u671f\u5316\u306b\u7528\u3044\u307e\u3059\u3002","d5a9c7df":"## \u306f\u3058\u3081\u306b\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f[\u524d\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af](https:\/\/www.kaggle.com\/hidehisaarai1213\/rkcup-1-fasttext-swem-lightgbm)\u306b\u5f15\u304d\u7d9a\u304d`fastText`\u3092\u7528\u3044\u3066\u30dd\u30b8\u30cd\u30ac\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\n\u4eca\u56de\u306f\u3001[GiNZA\u3067\u304a\u624b\u8efd\u306b\u65e5\u672c\u8a9eNLP + \u4e88\u6e2c\u7d50\u679c\u306e\u5206\u6790](https:\/\/www.kaggle.com\/shionhonda\/ginza-nlp)\u306e\u5206\u6790\u3092\u53c2\u8003\u306b\u5358\u8a9e\u9806\u3092\u3042\u308b\u7a0b\u5ea6\u8003\u616e\u3067\u304d\u305f\u308a\u3001\u4fc2\u53d7\u3051\u69cb\u9020\u3092\u8003\u616e\u3067\u304d\u308b\u30e2\u30c7\u30eb\u3092\u7528\u3044\u3066\u30e2\u30c7\u30ea\u30f3\u30b0\u3092\u3057\u3066\u3044\u304d\u307e\u3059\u3002","866f6d3f":"## Dataset\u306e\u4f5c\u6210\n\n\u3053\u3053\u3067\u306fPyTorch\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002\n\nPyTorch\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u30b5\u30f3\u30d7\u30eb\u6570\u3092\u8fd4\u308a\u5024\u3068\u3057\u3066\u8fd4\u3059`__len__`\u30e1\u30bd\u30c3\u30c9\u3068\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d7\u3051\u53d6\u3063\u3066\u30c7\u30fc\u30bf\u30b5\u30f3\u30d7\u30eb\u3092\u8fd4\u3059`__getitem__`\u3092\u5b9f\u88c5\u3057\u3066\u3042\u308c\u3070\u3088\u3044\u3067\u3059\u3002\n\n\u4eca\u56de\u306f`__getitem__`\u30e1\u30bd\u30c3\u30c9\u3067\u3001\u6587\u7ae0\u3092\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5165\u3063\u305f\u914d\u5217\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u3053\u306e\u969b\u306b\u30d1\u30c7\u30a3\u30f3\u30b0\u3082\u884c\u3044\u307e\u3059\u3002\u5177\u4f53\u7684\u306b\u306f\n\n```txt\n\u5168\u3066 \u306b \u6e80\u8db3 \u3067\u3059 \u3002\n```\n\n\u306e\u3088\u3046\u306a\u6587\u7ae0\u3092\n\n```txt\n[10 21 5 3 101 0 0 0 ... 0]\n```\n\n\u306e\u3088\u3046\u306a\u914d\u5217(`torch.tensor`)\u306b\u5909\u63db\u3057\u307e\u3059(\u6570\u5024\u306e\u4f8b\u306f\u9069\u5f53\u3067\u3059\u3002)\u3002","2ad02dea":"\u30e6\u30cb\u30fc\u30af\u306a\u5358\u8a9e\u6570\u306e\u7dcf\u6570\u3082\u30ab\u30a6\u30f3\u30c8\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\u3053\u308c\u306f`nn.Embedding`\u306b\u884c\u5217\u3092\u4e0e\u3048\u308b\u969b\u306b\u4f7f\u3044\u307e\u3059\u3002","2156011f":"## Transformer\u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n\nCNN\u30e2\u30c7\u30eb\u306f\u8fd1\u508d\u306e\u30c8\u30fc\u30af\u30f3\u306e\u60c5\u5831\u3092\u3082\u3068\u306b\u5b66\u7fd2\u3059\u308b\u3053\u3068\u306f\u3067\u304d\u308b\u306e\u3067\u3059\u304c\u3001\u6587\u7ae0\u9577\u304c\u9577\u304f\u306a\u308a\u3001\u4fc2\u53d7\u3051\u306a\u3069\u306e\u95a2\u4fc2\u304c\u96e2\u308c\u305f\u5358\u8a9e\u540c\u58eb\u3067\u751f\u3058\u3066\u3044\u308b\u5834\u5408\u306b\u306f\u3001\u305d\u306e\u95a2\u4fc2\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093\u3002\u3053\u306e\u3088\u3046\u306a\u9577\u8ddd\u96e2\u306e\u4f9d\u5b58\u6027\u3078\u306e\u5f31\u3055\u3068\u3044\u3046\u554f\u984c\u306b\u5bfe\u3057\u3066\u63d0\u6848\u3055\u308c\u305f\u306e\u304cTransformer\u30e2\u30c7\u30eb\u3067\u3059\u3002","637c7fe1":"## Test\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u306e\u63d0\u51fa","819d2632":"## \u4fbf\u5229\u95a2\u6570\u306e\u7528\u610f"}}