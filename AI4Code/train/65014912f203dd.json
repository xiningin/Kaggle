{"cell_type":{"95bcab62":"code","0037f190":"code","7d6c3126":"code","bd7c99d1":"code","c9fd9703":"code","425a18ab":"code","1a877034":"code","3bc628cd":"code","71ec49db":"code","fc067935":"code","2ef7cd5e":"code","a0b1f1bf":"code","3d297e4b":"code","ffdb8dc1":"code","5e953275":"code","cb18ea10":"code","208313d4":"code","30ea56f5":"code","9df49647":"markdown","96cb3bf8":"markdown","5a29f330":"markdown","ee77b266":"markdown","94ab31a1":"markdown","8ee508a2":"markdown","5755c633":"markdown","c9d98dfa":"markdown","58224341":"markdown","371f6f42":"markdown","54fcb15e":"markdown","354e5098":"markdown","3fe4c2d1":"markdown","0891cb32":"markdown","3bdd4eb8":"markdown","3233af31":"markdown","efda46fd":"markdown","0418e006":"markdown"},"source":{"95bcab62":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport tensorflow as tf\nfrom math import pi","0037f190":"x_train = pd.read_csv(\"..\/input\/csvTrainImages 13440x1024.csv\" )\ny_train = pd.read_csv(\"..\/input\/csvTrainLabel 13440x1.csv\")\nx_test  = pd.read_csv(\"..\/input\/csvTestImages 3360x1024.csv\")\ny_test  = pd.read_csv(\"..\/input\/csvTestLabel 3360x1.csv\")","7d6c3126":"x_train.info()","bd7c99d1":"x_train = x_train.iloc[:,:].values\ny_train = y_train.iloc[:,:].values\nx_test = x_test.iloc[:,:].values\ny_test = y_test.iloc[:,:].values","c9fd9703":"def visualize_images(df, img_size, number_of_images):\n    plt.figure(figsize=(8,8))\n    \n    n_rows = df.shape[0]\n    reshaped_df = df.reshape(df.shape[0], img_size, img_size)\n    number_of_rows = number_of_images\/5 if number_of_images%5 == 0 else (number_of_images\/5) +1\n    for i in range(number_of_images):\n        plt.subplot(number_of_rows, 5, i+1, xticks=[], yticks=[])\n        plt.imshow(reshaped_df[i], cmap='gray')\n","425a18ab":"visualize_images(x_train, 32, 16)","1a877034":"def visualize_input(img, ax):\n    img = img.reshape(32, 32)\n    ax.imshow(img, cmap='gray')\n    width, height = img.shape\n    thresh = img.max()\/2.5\n    for x in range(width):\n        for y in range(height):\n            ax.annotate(str(round(img[x][y],2)), xy=(y,x),\n                        horizontalalignment='center',\n                        verticalalignment='center',\n                        color='white' if img[x][y]<thresh else 'black')\n\nfig = plt.figure(figsize = (12,12)) \nax = fig.add_subplot(111, xticks=[], yticks=[])\nvisualize_input(x_train[0], ax)","3bc628cd":"# rescale [0,255] --> [0,1]\nx_train = x_train.astype('float32')\/255\nx_test = x_test.astype('float32')\/255\ny_train = y_train.astype('int32')\ny_test = y_test.astype('int32')","71ec49db":"max_ = y_train.max()+1","fc067935":"import keras\nfrom keras.utils import np_utils\n\n# break training set into training and validation sets\n(x_train, x_valid) = x_train[1000:], x_train[:1000]\n(y_train, y_valid) = y_train[1000:], y_train[:1000]\n\n# one-hot encode the labels\nnum_classes = len(np.unique(y_train))+1\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\ny_valid = keras.utils.to_categorical(y_valid, num_classes)\n\n# print shape of training set\nprint('x_train shape:', x_train.shape)\n\n# print number of training, validation, and test images\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\nprint(x_valid.shape[0], 'validation samples')","2ef7cd5e":"x_train = x_train.reshape([-1, 32, 32, 1])\nx_test = x_test.reshape([-1, 32, 32, 1])\nx_valid = x_valid.reshape([-1, 32, 32, 1])","a0b1f1bf":"x_train.shape","3d297e4b":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', \n                        input_shape=x_train.shape[1:]))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(500, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(29, activation='softmax'))\n\nmodel.summary()","ffdb8dc1":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n                  metrics=['accuracy'])","5e953275":"from keras.preprocessing.image import ImageDataGenerator\n\n# create and configure augmented image generator\ndatagen_train = ImageDataGenerator(\n    width_shift_range=0.1,  # randomly shift images horizontally (10% of total width)\n    height_shift_range=0.1,  # randomly shift images vertically (10% of total height)\n    horizontal_flip=True) # randomly flip images horizontally\n\n# fit augmented image generator on data\ndatagen_train.fit(x_train)","cb18ea10":"from keras.callbacks import ModelCheckpoint   \n\nbatch_size = 128\nepochs = 100\n\n# train the model\ncheckpointer = ModelCheckpoint(filepath='aug_model.weights.best.hdf5', verbose=1, \n                               save_best_only=True)\nmodel.fit_generator(datagen_train.flow(x_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=x_train.shape[0] \/\/ batch_size,\n                    epochs=epochs, verbose=2, callbacks=[checkpointer],\n                    validation_data=(x_valid, y_valid),\n                    validation_steps=x_valid.shape[0] \/\/ batch_size)","208313d4":"model.load_weights('aug_model.weights.best.hdf5')","30ea56f5":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('\\n', 'Test accuracy:', score[1]*100)","9df49647":"# Table of Contents\n\n- [1. Introduction](#1)\n- [2. Importing the packages needed](#2)\n- [3. Load and prepare the data](#3)\n- [4. Initial Data Inspection](#4)\n- [5. Changing the dataframe to include \"reshape\"](#5)\n- [6. Visualizing the dataset](#6)\n    - [6.1 Deep Inspection](#6.1)\n- [7. Normalization](#7)\n- [8. Breaking it into test, validation and train sets](#8)\n    - [8.1 Reshaping the data to 32x32 image](#8.1)\n- [9. Define the model architecture](#9)\n- [10. Compile the Model](#10)\n- [11. Augmenting the Images](#11)\n- [12. Train the Model](#12)\n- [13. Load the Model with the Best Validation Accuracy](#13)\n- [14. Calculate Classification Accuracy on Test Set](#14)","96cb3bf8":"# 1. Introduction <a class=\"anchor\" id=\"1\"><\/a>\nThe data-set is composed of 16,800 characters written by 60 participants, the age range is between 19 to 40 years, and 90% of participants are right-hand. Each participant wrote each character (from \u2019alef\u2019 to \u2019yeh\u2019) ten times on two forms as shown in Fig. 7(a) & 7(b). The forms were scanned at the resolution of 300 dpi. Each block is segmented automatically using Matlab 2016a to determining the coordinates for each block. The database is partitioned into two sets: a training set (13,440 characters to 480 images per class) and a test set (3,360 characters to 120 images per class). Writers of training set and test set are exclusive. Ordering of including writers to test set are randomized to make sure that writers of test set are not from a single institution (to ensure variability of the test set).","5a29f330":"# 14. Calculate Classification Accuracy on Test Set<a class=\"anchor\" id=\"14\"><\/a>","ee77b266":"# 13. Load the Model with the Best Validation Accuracy<a class=\"anchor\" id=\"13\"><\/a>","94ab31a1":"# 5. Changing the dataframe to include \"reshape\" <a class=\"anchor\" id=\"5\"><\/a>\n- Dataframes doesn't have \"replace\" option implicitely\n- You could achieve the same by using values option","8ee508a2":"# 11. Augmenting the Images<a class=\"anchor\" id=\"11\"><\/a>\n- Rotational Invariance = Rotating the images randomly so that CNN can learn those images \n- Translation Invariance = Translate the images randomly so that CNN can learn those images\n- Augmentation reduces overfitting","5755c633":"# 3. Load and prepare the data <a class=\"anchor\" id=\"3\"><\/a>","c9d98dfa":"# 2. Importing the packages needed <a class=\"anchor\" id=\"2\"><\/a>","58224341":"# 7. Normalization<a class=\"anchor\" id=\"7\"><\/a>\n- Since the image is in RCG format, we could downgrade it to grayscale \n- Black woudl be 0 and white would be 255\n- Gray would vary between black and white\n- We would also change the labels to be int32 \n    - It will be useful when we are categorizing them","371f6f42":"# 4. Initial Data Inspection <a class=\"anchor\" id=\"4\"><\/a>","54fcb15e":"# 8. Breaking it into test, validation and train sets<a class=\"anchor\" id=\"8\"><\/a>\n\n- Here, we break the data into train, test and validation sets\n- Validation is really important because the trainig set checks the validation set \n- If the training set has less error and validation set has more error, it means that we are overfitting the data\n- To see if we are overfitting, validation set helps us really well","354e5098":"# 10. Compile the Model<a class=\"anchor\" id=\"10\"><\/a>\n- Since there is 28 different alphabets, we use **categorical_crossentropy** as loss","3fe4c2d1":"## 6.1 Deep Inspection <a class=\"anchor\" id=\"6.1\"><\/a>","0891cb32":"# 12. Train the Model<a class=\"anchor\" id=\"12\"><\/a>","3bdd4eb8":"### Inference\nThe trainIamges csv has 1024 columns and 13439 rows. Each column represents a pixel in an image and each row represents an individual grayscale image. The value of each pixel varies from 0 -255.","3233af31":"## 8.1 Reshaping the data to 32x32 image<a class=\"anchor\" id=\"8\"><\/a>\n- We could reshape it to 32x32 before introcing it to CNN\n- We do this becase CNN uses spacial relationships and filters the data","efda46fd":"# 6. Visualizing the dataset <a class=\"anchor\" id=\"6\"><\/a>","0418e006":"# 9. Define the model architecture<a class=\"anchor\" id=\"9\"><\/a>\n\n- The CNN model is defined below:\n\n- **filters**\u00a0- The number of filters. (Helps in extracting different features)\n- **kernel_size**\u00a0- Number specifying both the height and width of the (square) convolution window.\n  There are some additional, optional arguments that you might like to tune:\n- **strides**\u00a0- The stride of the convolution. If you don't specify anything,\u00a0strides\u00a0is set to\u00a01.\n- **padding**\u00a0- One of\u00a0'valid'\u00a0or\u00a0'same'. If you don't specify anything,\u00a0padding\u00a0is set to\u00a0'valid'.\n"}}