{"cell_type":{"f8470c25":"code","ec3627bd":"code","b3717e5d":"code","b35a3470":"code","1220c401":"code","18a2c1e2":"code","7d37be90":"code","b249e504":"code","a1e67af4":"code","74594d21":"code","98d3db37":"code","47661dbb":"code","cbadd673":"code","7936e560":"code","f76b96f5":"code","0afedae2":"code","268626b9":"code","0cb59cad":"code","597bf05b":"code","aca29b18":"code","9c20c37a":"code","680a8d50":"code","8efa011b":"code","cd537495":"code","ea53c0c4":"code","da0c63f7":"code","74ddda27":"code","f8ef7fd2":"code","09d1e8c1":"code","cb8723d2":"code","e9a371cb":"code","9b695742":"code","5cfaa4a5":"code","dc917135":"code","0887f669":"markdown","b2324da4":"markdown","c18084c9":"markdown","0c2504bb":"markdown","452e07f8":"markdown","711e35bd":"markdown","7a2958e9":"markdown","d804ad37":"markdown","86d7179a":"markdown","499d6d4d":"markdown","113dee48":"markdown","5dc59742":"markdown","84e745e4":"markdown","5e5ff885":"markdown","e579708d":"markdown","7b1facf0":"markdown","4c35b1a2":"markdown","9d0bb920":"markdown","bf5c3710":"markdown","802bd63a":"markdown","a9edf624":"markdown","693dbec6":"markdown","849e990b":"markdown","aa8f189a":"markdown","d03d8b33":"markdown","09e2eeff":"markdown","353dc0ea":"markdown","f41329b6":"markdown","27a8881e":"markdown","1b655e4a":"markdown","c8978402":"markdown","26a5a987":"markdown"},"source":{"f8470c25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec3627bd":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np","b3717e5d":"# Read CSV file\ndata = pd.read_csv(\"\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndata.head()","b35a3470":"data.shape","1220c401":"# Identify NULL Values if any\ndata.isnull().sum()","18a2c1e2":"data.describe()","7d37be90":"# Check Unique Values for Categorical data\nprint('Unique Attrition Values: ' + str(data.Attrition.unique()) + '\\n')\nprint('Unique Business Travel Values: ' + str(data.BusinessTravel.unique()) + '\\n')\nprint('Unique Dept Values: ' + str(data.Department.unique()) + '\\n')\nprint('Unique Education Field Values: ' + str(data.EducationField.unique()) + '\\n')\nprint('Unique Gender Values: ' + str(data.Gender.unique()) + '\\n')\nprint('Unique Job Role Values: ' + str(data.JobRole.unique()) + '\\n')\nprint('Unique Marital Status Values: ' + str(data.MaritalStatus.unique()) + '\\n')\nprint('Unique Over18 Values: ' + str(data.Over18.unique()) + '\\n')\nprint('Unique OverTime Values: ' + str(data.OverTime.unique()) + '\\n')","b249e504":"# Remove unnecessary columns\ndata.drop(columns=[\"EmployeeCount\", \"StandardHours\", \"Over18\", \"EmployeeNumber\"], inplace=True)\ndata.shape  # Now we have 31 columns only","a1e67af4":"# Convert Columns - Attrition, BusinessTravel, Dept, Gender, MaritalStatus, OverTime\ndata.Attrition.replace({'Yes': 1, 'No': 0}, inplace=True)\ndata.BusinessTravel.replace({'Non-Travel': 0, 'Travel_Rarely': 1, 'Travel_Frequently': 2}, inplace=True)\ndata.Department.replace({'Sales': 0, 'Research & Development': 1, 'Human Resources': 2}, inplace=True)\ndata.Gender.replace({'Female': 0, 'Male': 1}, inplace=True)\ndata.MaritalStatus.replace({'Single': 0,'Married': 1, 'Divorced': 2}, inplace=True)\ndata.OverTime.replace({'No': 0, 'Yes': 1}, inplace=True)\ndata.EducationField.replace({'Life Sciences': 0, 'Medical': 1, 'Marketing': 2, 'Technical Degree': 3, 'Human Resources': 4, 'Other': 5}, inplace=True)\ndata.JobRole.replace({\n    'Sales Executive': 0, \n    'Research Scientist': 1, \n    'Laboratory Technician': 2,\n    'Manufacturing Director': 3,\n    'Healthcare Representative': 4,\n    'Manager': 5,\n    'Sales Representative': 6,\n    'Research Director': 7,\n    'Human Resources': 8\n}, inplace=True)","74594d21":"data.describe()","98d3db37":"temp = None\ntemp = pd.DataFrame({\n    \"Median Values\": data.median(), \n    \"Mean Values\": data.mean(), \n    \"Standard Deviation\": data.std(),\n    \"Skewness\": data.skew() # Ideal range: -1 to +1\n})\n\n# Sort values based on skewness\ntemp = temp.sort_values(by='Skewness')\n\ntemp.head(50)","47661dbb":"data.describe()","cbadd673":"total_rows = data.shape[0]\nno_val = data.Attrition.value_counts()[0]\nyes_val = data.Attrition.value_counts()[1]\nprint('Percentage of NO Values: ' + str((no_val\/total_rows) * 100))\nprint('Percentage of YES Values: ' + str((yes_val\/total_rows) * 100))\n\nplt.figure(figsize = (7, 7))\nplt.pie([yes_val, no_val], labels=['YES', 'NO'], autopct='%1.0f%%', colors = ['lightgreen','#66b3ff'])\nplt.title(\"Observed Class Imbalance in Attrition values\", fontsize=20)\nplt.show()","7936e560":"print(data.Gender.value_counts())\n\ntotal = data.Gender.value_counts()[0] + data.Gender.value_counts()[1]\nper_men_data = (data.Gender.value_counts()[1] \/ total) * 100 \nper_women_data = (data.Gender.value_counts()[0] \/ total) * 100 \n\nplt.figure(figsize = (7, 7))\nplt.pie([per_men_data, per_women_data], labels=['Men', 'Women'], autopct='%1.0f%%')\nplt.title('Gender wise Data Biasness', fontsize=20)\nplt.show()","f76b96f5":"plt.figure(figsize=(8, 5))\nax = sns.barplot(x = data.Gender , y = data.WorkLifeBalance, estimator = np.sum, hue = data.Attrition)\nax.set_xticklabels(('Women', 'Men'))\nplt.title('How Worklife Balance affects Attrition Rate')\nplt.show()","0afedae2":"plt.figure(figsize=(8, 5))\nax = sns.barplot(x = data.Gender , y = data.WorkLifeBalance, estimator = np.sum, hue = data.MaritalStatus)\nax.set_xticklabels(('Women', 'Men'))\nplt.show()","268626b9":"plt.figure(figsize=(8, 5))\nax = sns.barplot(x = data.Gender , y = data.BusinessTravel, estimator = np.sum, hue = data.Attrition)\nax.set_xticklabels(('Women', 'Men'))\nplt.title('How Business Travel affects Attrition Rate')\nplt.show()","0cb59cad":"plt.figure(figsize=(10, 6))\nax = sns.boxplot(x=data.Gender, y=data.Age, hue=data.Attrition, data=data, linewidth=2.5)\nax.set_xticklabels(('Women', 'Men'))\nplt.title('Age-wise Attrition Rate')\nplt.show()","597bf05b":"plt.figure(figsize=(10, 6))\nplt.title('How Distance from Home affects Attrition Rate')\nax = sns.barplot(x = data.BusinessTravel , y = data.DistanceFromHome, estimator = np.median, hue = data.Attrition, palette='Set1')\nax.set_xticklabels(('Non-Travel', 'Travel_Rarely', 'Travel_Frequently'))\nplt.show()","aca29b18":"plt.figure(figsize=(10, 6))\nplt.title('Department wise Attrition Rate')\nax = sns.barplot(x=data.Department, y=data.JobRole, hue=data.Attrition, orient='v', palette='Set2')\nax.set_xticklabels(('Sales', 'Research & Development', 'Human Resources'))\nplt.show()","9c20c37a":"plt.figure(figsize=(10, 6))\nax = sns.boxplot(x=data.Department, y=data.MonthlyIncome, hue=data.Attrition, data=data, linewidth=2.5, palette='Set2')\nax.set_xticklabels(('Sales', 'Research & Development', 'Human Resources'))\nplt.title('Department wise Monthly Income')\nplt.show()","680a8d50":"plt.figure(figsize=(30, 20))\nplt.title('Correlation between variables')\nsns.heatmap(data.corr(), annot=True, cmap='Blues')","8efa011b":"data.columns","cd537495":"from sklearn.model_selection import train_test_split\ndata_x = data.iloc[:, 0:30]\ndata_y = data.iloc[:, 1]\n\ndata_x.drop(columns=[\"Attrition\"], inplace=True)","ea53c0c4":"data_x_train, data_x_test, data_y_train, data_y_test = train_test_split(data_x, data_y, test_size = .2, random_state=20)\nprint(\"-----------------------\")\nprint(data_x_train.shape)\nprint(data_y_train.shape)\nprint(\"-----------------------\")\nprint(data_x_test.shape)\nprint(data_y_test.shape)\nprint(\"-----------------------\")","da0c63f7":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(data_y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(data_y_train == 0))) \n  \n# import SMOTE module from imblearn library \nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=0)\noversampled_x_train, oversampled_y_train = sm.fit_resample(data_x_train, data_y_train)\n\nprint(\"After OverSampling, the shape of oversampled_x_train: {}\".format(oversampled_x_train.shape))\nprint(\"After OverSampling, the shape of oversampled_y_train: {} \\n\".format(oversampled_y_train.shape))\n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(oversampled_y_train == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(oversampled_y_train == 0))) \n\noversampled_y_train = pd.Series(oversampled_y_train)\n\n\n# Plot on a Pie chart\ntotal_rows = data.shape[0]\n\nplt.figure(figsize = (7, 7))\nplt.pie([sum(oversampled_y_train == 1), sum(oversampled_y_train == 0)], labels=['YES', 'NO'], autopct='%1.0f%%', colors = ['lightgreen','#66b3ff'])\nplt.title(\"Resolved Class Imbalance in Attrition values\", fontsize=20)\nplt.show()","74ddda27":"oversampled_x_train.head()","f8ef7fd2":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(class_weight='balanced')\n\nlog_reg.fit(oversampled_x_train, oversampled_y_train)\n\n# Predict values based on this fittest model\nlog_pred = log_reg.predict(data_x_test)\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nlog_conf = confusion_matrix(data_y_test, log_pred)\n\n# Visulize this Confusion Matrix neatly using seaborn\nplt.figure(figsize = (8,5))\nsns.heatmap(log_conf, annot=True, cmap='Blues', linewidths=.5)\n\nlog_accuracy = log_conf.diagonal().sum() \/ log_conf.sum()\nprint(\"Accuracy: \" + str(log_accuracy))\n\nlog_prec = log_conf[1,1] \/ (log_conf[0,1] + (log_conf[1,1]))\nprint(\"Precision: \" + str(log_prec))\n\nlog_sens = log_conf[1,1] \/ (log_conf[1,0] + (log_conf[1,1]))\nprint(\"Sensitivity: \" + str(log_sens))\n\nlog_spec = log_conf[0,0] \/ (log_conf[0,0] + (log_conf[0,1]))\nprint(\"Specificity: \" + str(log_spec))\n\nlog_roc_auc_score = roc_auc_score(data_y_test, log_pred)\nprint(\"ROC AUC Score: \" + str(log_roc_auc_score))\n\nlog_F1 = 2 * (log_prec * log_sens) \/ (log_prec + log_sens)\nprint('F1 Score: ' + str(log_F1))\n\nresults_normalized = pd.DataFrame({\"Actual Values\":data_y_test,\"Predicted Values\":log_pred})\nresults_normalized.tail()","09d1e8c1":"# Applying XGBoost to Logistic Regression\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n# fit model no training data\nxg_model = XGBClassifier()\nxg_model.fit(oversampled_x_train, oversampled_y_train)\n\n\n# Prediction using XGBoost\ny_pred = xg_model.predict(data_x_test)\npredictions = [round(value) for value in y_pred]\n\n\n# Evaluate predictions\nxg_log_conf = confusion_matrix(data_y_test, predictions)\n\n# Visulize this Confusion Matrix neatly using seaborn\nplt.figure(figsize = (8,5))\nsns.heatmap(xg_log_conf, annot=True, cmap='Blues', linewidths=.5)\n\nxg_log_accuracy = xg_log_conf.diagonal().sum() \/ log_conf.sum()\nprint(\"Accuracy: \" + str(xg_log_accuracy))\n\nxg_log_prec = xg_log_conf[1,1] \/ (xg_log_conf[0,1] + (xg_log_conf[1,1]))\nprint(\"Precision: \" + str(xg_log_prec))\n\nxg_log_sens = xg_log_conf[1,1] \/ (xg_log_conf[1,0] + (xg_log_conf[1,1]))\nprint(\"Sensitivity: \" + str(xg_log_sens))\n\nxg_log_spec = xg_log_conf[0,0] \/ (xg_log_conf[0,0] + (xg_log_conf[0,1]))\nprint(\"Specificity: \" + str(xg_log_spec))\n\nxg_log_roc_auc_score = roc_auc_score(data_y_test, log_pred)\nprint(\"ROC AUC Score: \" + str(xg_log_roc_auc_score))\n\nxg_log_F1 = 2 * (xg_log_prec * xg_log_sens) \/ (xg_log_prec + xg_log_sens)\nprint('F1 Score: ' + str(xg_log_F1))\n\nresults_normalized = pd.DataFrame({\"Actual Values\":data_y_test,\"Predicted Values\":log_pred})\nresults_normalized.tail()","cb8723d2":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(oversampled_x_train, oversampled_y_train)\n\ndt_pred = dt.predict(data_x_test)\ndt_conf = confusion_matrix(data_y_test, dt_pred)\n\n# Visulize this Confusion Matrix neatly using seaborn\nplt.figure(figsize = (8, 5))\nsns.heatmap(dt_conf, annot=True, cmap='Blues', linewidths=.5)\n\ndt_accuracy = dt_conf.diagonal().sum() \/ dt_conf.sum()\nprint(\"Accuracy: \" + str(dt_accuracy))\n\ndt_prec = dt_conf[1,1] \/ (dt_conf[0,1] + (dt_conf[1,1]))\nprint(\"Precision: \" + str(dt_prec))\n\ndt_sens = dt_conf[1,1] \/ (dt_conf[1,0] + (dt_conf[1,1]))\nprint(\"Sensitivity: \" + str(dt_sens))\n\ndt_spec = dt_conf[0,0] \/ (dt_conf[0,0] + (dt_conf[0,1]))\nprint(\"Specificity: \" + str(dt_spec))\n\ndt_roc_auc_score = roc_auc_score(data_y_test, dt_pred)\nprint(\"ROC AUC Score: \" + str(dt_roc_auc_score))\n\ndt_F1 = 2 * (dt_prec * dt_sens) \/ (dt_prec + dt_sens)\nprint('F1 Score: ' + str(dt_F1))\n\nresults_normalized = pd.DataFrame({\"Actual Values\":data_y_test,\"Predicted Values\":dt_pred})\nresults_normalized.tail()","e9a371cb":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(oversampled_x_train, oversampled_y_train)\n\nrf_pred = rf.predict(data_x_test)\nrf_conf = confusion_matrix(data_y_test, rf_pred)\n\n# Visulize this Confusion Matrix neatly using seaborn\nplt.figure(figsize = (8,5))\nsns.heatmap(rf_conf, annot=True, cmap='Blues', linewidths=.5)\n\nrf_accuracy = rf_conf.diagonal().sum() \/ rf_conf.sum()\nprint(\"Accuracy: \" + str(rf_accuracy))\n\nrf_prec = rf_conf[1,1] \/ (rf_conf[0,1] + (rf_conf[1,1]))\nprint(\"Precision: \" + str(rf_prec))\n\nrf_sens = rf_conf[1,1] \/ (rf_conf[1,0] + (rf_conf[1,1]))\nprint(\"Sensitivity: \" + str(rf_sens))\n\nrf_spec = rf_conf[0,0] \/ (rf_conf[0,0] + (rf_conf[0,1]))\nprint(\"Specificity: \" + str(rf_spec))\n\nrf_roc_auc_score = roc_auc_score(data_y_test, rf_pred)\nprint(\"ROC AUC Score: \" + str(rf_roc_auc_score))\n\nrf_F1 = 2 * (rf_prec * rf_sens) \/ (rf_prec + rf_sens)\nprint('F1 Score: ' + str(rf_F1))\n\nresults_normalized = pd.DataFrame({\"Actual Values\":data_y_test,\"Predicted Values\":rf_pred})\nresults_normalized.tail()","9b695742":"results = [\n    [\"Logistic Regression\", log_accuracy, log_spec, log_prec, log_sens, log_roc_auc_score, log_F1],\n    [\"XGBoost\", xg_log_accuracy, xg_log_spec, xg_log_prec, xg_log_sens, xg_log_roc_auc_score, xg_log_F1],\n    [\"Decision Tree\", dt_accuracy, dt_spec, dt_prec, dt_sens, dt_roc_auc_score, dt_F1],\n    [\"Random Forest\", rf_accuracy, rf_spec, rf_prec, rf_sens, rf_roc_auc_score, rf_F1],\n]\n\nresults_df = pd.DataFrame(results, columns=[\"Name\", \"Accuracy\", \"Specificity\", \"Precision\", \"Sensitivity\", \"ROC AUC Score\", \"F1 Score\"])\nresults_df","5cfaa4a5":"from sklearn.metrics import roc_curve\n\n# Calculate FPR, TPR, THRESHOLD for Logistic Regression Model \nlog_pred_prob = log_reg.predict_proba(data_x_test)  # Build on our Test data\nlog_fpr, log_tpr, log_thre = roc_curve(data_y_test, log_pred_prob[:, 1])\n\n# Calculate FPR, TPR, THRESHOLD for Decision Tree Model \ndt_pred_prob = dt.predict_proba(data_x_test)  # Build on our Test data\ndt_fpr, dt_tpr, dt_thre = roc_curve(data_y_test, dt_pred_prob[:, 1])\n\n# Calculate FPR, TPR, THRESHOLD for Random Forest Model \nrf_pred_prob = rf.predict_proba(data_x_test)  # Build on our Test data\nrf_fpr, rf_tpr, rf_thre = roc_curve(data_y_test, rf_pred_prob[:, 1])\n\nroc_plot_df = pd.DataFrame({\n    \"FPR\": [log_fpr, dt_fpr, rf_fpr],\n    \"TPR\": [log_tpr, dt_tpr, rf_tpr],\n})\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\nax.plot(log_fpr,log_tpr)\nax.plot(dt_fpr,dt_tpr)\nax.plot(rf_fpr,rf_tpr)\n\nax.grid(True)\nax.set_title('Fpr vs Tpr on the Attrition Dataset')\nax.legend(['XGBoost', 'Decision Tree', 'Random Forest'])\nax.xaxis.set_label_text('Fpr Value')\nax.yaxis.set_label_text('Tpr Value')\n\nplt.show()\n\n\nprint(\"AUCROC Score using XGBoost: \" + str(log_roc_auc_score))\nprint(\"AUCROC Score for Decision Tree: \" + str(dt_roc_auc_score))\nprint(\"AUCROC Score for Random Forest: \" + str(rf_roc_auc_score))","dc917135":"from sklearn.model_selection import cross_val_score\ntemp_df = []\n\n################ Calculate Score for Logistic Regression\nscore_log_reg = cross_val_score(log_reg, data_x, data_y, scoring='accuracy', cv=5)\n\n\n################ Calculate Score for XGBoost\nscore_xg_reg = cross_val_score(xg_model, data_x, data_y, scoring='accuracy', cv=5)\n\n\n################ Calculate Score for Decision Tree\nscore_dt = cross_val_score(dt, data_x, data_y, scoring='accuracy', cv=5)\n\n\n############### Calculate Score for Random Forest\nscore_rf = cross_val_score(rf, data_x, data_y, scoring='accuracy', cv=5)\n\n\n# Create a Dataframe\nresults = [\n    [\"Logistic Regression\", score_log_reg.mean(), score_log_reg.min(), score_log_reg.max()],\n    [\"XGBoost\", score_xg_reg.mean(), score_xg_reg.min(), score_xg_reg.max()],\n    [\"Decision Tree\", score_dt.mean(), score_dt.min(), score_dt.max()],\n    [\"Random Forest\", score_rf.mean(), score_rf.min(), score_rf.max()],\n]\n\nresults_df = pd.DataFrame(results, columns=[\"Name\", \"Mean Accuracy\", \"Minimum Accuracy\", \"Maximum Accuracy\"])\nresults_df","0887f669":"### Logistic Regression","b2324da4":"### Attrition Rate is higher in people who are Non-Business Travelers  and who stay far from office","c18084c9":"### Improving Accuracy of Logistic Regression using XGBoost","0c2504bb":"## Observations - \n\n* Constant data Columns (Can be removed) - Over18\n* Can be converted from Categorical - Numerical - Attrition, BusinessTravel, Dept, Gender, MaritalStatus, OverTime\n\n","452e07f8":"### Women are not willing to switch jobs more often as men do. Also, their work-life balance is very low as compared to men.","711e35bd":"### Resolving Target Class Imbalance using SMOTE","7a2958e9":"### Let us identify whether our dataset is gender biased or not. If yes, then by how much","d804ad37":"Identify Difference between mean and median values.\n[This helps to understand whether columns are skewed or not]","86d7179a":"### Observations - \n\n* Job-Level, Monthly Income increases as Age and the total number of working years (i.e. Work Experience) increases.\n* The more the amount of Over Time employees do, that more their chances of switching jobs\n* Salary Hike is observed the most among those who have the highest performance ratings","499d6d4d":"### Conclusion: Logistic Regression using XGBoost seems to be the best fit","113dee48":"### Let's see how Age affects Attrition Rate:\n* Women within 25-35 age have more attrition rate, whereas men show higher attrition rates until late 40s\n* Later, it is only after women reach their 50s, they show higher attrition rates and start switching jobs","5dc59742":"### Identify NA Values (if any)","84e745e4":"### Remove unnecessary columns & identify the total no. of remaining columns","5e5ff885":"### Read Input file","e579708d":"### Further comparison using Cross-Validation of scores","7b1facf0":"* Sales Employees have the highest paid Jobs\n* R&D Dept Employees have mid-pay jobs\n* The lesser the Salaries, the more their Attrition Rate\n* The amount of people residing in the higher-salary segment are more in Sales Dept and least in the R&D Dept","4c35b1a2":"## Model Development","9d0bb920":"### Let us identify whether our target variable is biased or not. If yes, then by how much","bf5c3710":"### Replace Categorical - Numerical (wherever needed)","802bd63a":"## Exploratory Data Analysis","a9edf624":"### Import necessary libraries","693dbec6":"* Sales Dept has lowest employee retention rate, since most attrition rate is observed in this dept.\n* R&D Dept has the highest employee retention rate, since least attrition rate is seen in this dept.","849e990b":"### Identifying Correlation furthermore using Correlation plot","aa8f189a":"### Decision Tree","d03d8b33":"### More men do Business Travels than women. Also, as seen above their Work-Life Balance is higher than women.","09e2eeff":"### Visualizing AUROC Curve","353dc0ea":"### Random Forest","f41329b6":"### Identify Columns which can be removed","27a8881e":"### Married people have a maximum Work-Life Balance rate than others.","1b655e4a":"Describe our data and confirm whether changes are reflected","c8978402":"## Observations - \n\n\n* Constant data Columns (Can be removed) - EmployeeCount, StandardHours\n* Category-type variables - EnvironmentSatisfaction, JobInvolvement, JobLevel, RelationshipSatisfaction, WorkLifeBalance\n* Unique Identifiers - EmployeeNumber\n\nNow let's check for categorical variables if they have uniqueness or not:","26a5a987":"### Model output parameters comparison"}}