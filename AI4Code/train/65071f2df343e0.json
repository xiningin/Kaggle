{"cell_type":{"4f044700":"code","a4d36589":"code","27113ac2":"code","82a8c4be":"code","84b14c9f":"code","3ea9c68f":"code","19450267":"code","f3029c7c":"code","d382dee1":"code","431cfd6d":"code","42ff12b6":"code","ddaccd37":"code","e6feafef":"code","522a072d":"code","e607ff00":"code","a2163323":"code","f22c4688":"code","e2e62096":"code","6a81244c":"code","05479558":"code","c856391e":"code","96963716":"code","e94bd8d3":"code","822ce560":"code","38cc971f":"code","f60f1e2f":"code","e596cb53":"code","bb919523":"code","f8780bc3":"code","f9bcbfaa":"code","74f36e30":"code","13e1d7eb":"code","00ee037e":"code","0c9344a4":"code","235d8d38":"code","1565da2d":"code","e8571348":"markdown","3c6d586c":"markdown","41668425":"markdown","f8bedde8":"markdown","fe1ca7ba":"markdown","d1f78e47":"markdown","7e079c54":"markdown","379cedf9":"markdown","a907397b":"markdown","6dc78169":"markdown","8f9a551f":"markdown","f761345d":"markdown","2e229238":"markdown","5346abf7":"markdown","fdc9a008":"markdown","eaa3cfad":"markdown","24899046":"markdown","a465abdf":"markdown","d7552e57":"markdown","aeb52430":"markdown","9c85af54":"markdown","1300636d":"markdown","3979a94a":"markdown","c5b72b09":"markdown","acb2ec84":"markdown"},"source":{"4f044700":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","a4d36589":"dftrain=pd.read_csv('..\/input\/titanic\/train.csv')\ndftest=pd.read_csv('..\/input\/titanic\/test.csv')","27113ac2":"dftrain.head()","82a8c4be":"dftest.head()","84b14c9f":"sns.countplot(data=dftrain,x='Survived',palette='Set2')","3ea9c68f":"sns.countplot(data=dftrain,x='Survived',hue='Sex',palette='Set2')","19450267":"sns.countplot(data=dftrain,x='Survived',hue='Pclass',palette='Set2')","f3029c7c":"sns.countplot(data=dftrain,x='Survived',hue='Embarked',palette='Set2')","d382dee1":"plt.figure(figsize=(10,5))\nplt.subplot(2,2,1)\nsns.distplot(dftrain['Age'],bins=50)\nplt.subplot(2,2,2)\nsns.distplot(dftrain['Fare'],bins=50,color='red')","431cfd6d":"plt.figure(figsize=(15,10))\nsns.heatmap(dftrain.corr(),annot=True,linewidth=0.2)","42ff12b6":"dftrain.isnull().sum()","ddaccd37":"dftrain.drop('Cabin',axis=1,inplace=True)","e6feafef":"dftest.isnull().sum()","522a072d":"dftest.drop('Cabin',axis=1,inplace=True)","e607ff00":"dftrain.head()","a2163323":"dftrain.Sex[dftrain.Sex=='male']=0\ndftrain.Sex[dftrain.Sex=='female']=1\ndftest.Sex[dftest.Sex=='male']=0\ndftest.Sex[dftest.Sex=='female']=1","f22c4688":"dftrain.Embarked[dftrain.Embarked=='S']=0\ndftrain.Embarked[dftrain.Embarked=='C']=1\ndftrain.Embarked[dftrain.Embarked=='Q']=2\n\ndftest.Embarked[dftest.Embarked=='S']=0\ndftest.Embarked[dftest.Embarked=='C']=1\ndftest.Embarked[dftest.Embarked=='Q']=2\n","e2e62096":"finalId=dftest['PassengerId']","6a81244c":"dftrain.drop(['PassengerId','Ticket'],axis=1,inplace=True)\ndftest.drop(['PassengerId','Ticket'],axis=1,inplace=True)","05479558":"dftest['Fare']=dftest['Fare'].fillna(np.mean(dftest['Fare']))\ndftest['Age']=dftest['Age'].fillna(np.mean(dftest['Age']))\ndftrain['Age']=dftrain['Age'].fillna(np.mean(dftrain['Age']))\ndftrain['Embarked']=dftrain['Embarked'].fillna(0)","c856391e":"dftrain.isna().sum()","96963716":"dftest.isna().sum()","e94bd8d3":"df_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dftrain[\"Name\"]]\ndftrain[\"Title\"] = pd.Series(df_title)\ndftrain[\"Title\"] = dftrain[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndftrain[\"Title\"] = dftrain[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndftrain.drop('Name',axis=1,inplace=True)\ndftrain['Familysize']=dftrain['SibSp']+dftrain['Parch']+1\ndftrain['Alone'] = dftrain['Familysize'].map(lambda s: 1 if s == 1 else 0)","822ce560":"df_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dftest[\"Name\"]]\ndftest[\"Title\"] = pd.Series(df_title)\ndftest[\"Title\"] = dftest[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndftest[\"Title\"] = dftest[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndftest.drop('Name',axis=1,inplace=True)\ndftest['Familysize']=dftest['SibSp']+dftest['Parch']+1\ndftest['Alone'] = dftest['Familysize'].map(lambda s: 1 if s == 1 else 0)","38cc971f":"dftrain.head()","f60f1e2f":"dftest.head()","e596cb53":"X=dftrain.drop('Survived',axis=1)\ny=dftrain['Survived']\nXtrain,Xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=42)","bb919523":"scaler=StandardScaler()\nXtrainscaled=scaler.fit_transform(Xtrain)\nXtestscaled=scaler.transform(Xtest)\ndftestscaled=scaler.transform(dftest)","f8780bc3":"model=LogisticRegression()\nmodel.fit(Xtrainscaled,ytrain)\nypred=model.predict(Xtestscaled)\nn=accuracy_score(ytest,ypred)\nprint('Accuracy of Logistic regression model: {}%'.format(round(n*100,2)))","f9bcbfaa":"mod=svm.SVC()\nmod.fit(Xtrainscaled,ytrain)\ny1pred=mod.predict(Xtestscaled)\nsc=accuracy_score(ytest,y1pred)\nprint('Accuracy of SVM model: {}%'.format(round(sc*100,2)))","74f36e30":"base=DecisionTreeClassifier(max_depth=4,min_samples_split=2,criterion='gini')\nbase.fit(Xtrainscaled,ytrain)\ny2pred=base.predict(Xtestscaled)\nscr=accuracy_score(ytest,y2pred)\nprint('Accuracy of decision tree classifier: {}%'.format(round(scr*100,2)))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(ytest,y2pred)))","13e1d7eb":"mdl=RandomForestClassifier()\nmdl.fit(Xtrainscaled,ytrain)\nyr=model.predict(Xtestscaled)\nscc=accuracy_score(ytest,yr)\nprint('Accuracy of Random forest classifier: {}%'.format(round(scc*100,2)))","00ee037e":"bs=DecisionTreeClassifier(max_depth=4,min_samples_split=2,criterion='entropy',splitter='best')\ngb=XGBClassifier(base_estimator=bs,n_estimators=200,learning_rate=0.001,max_depth=3)\ngb.fit(Xtrainscaled,ytrain)\nypred=gb.predict(Xtestscaled)\nscor=accuracy_score(ytest,ypred)\nprint('Accuracy score: {}%'.format(round(scor,2)*100))","0c9344a4":"print(confusion_matrix(ytest,ypred))","235d8d38":"print(classification_report(ytest,ypred))","1565da2d":"prediction=gb.predict(dftestscaled)\nanswerdf=pd.DataFrame({'PassengerId':finalId,'Survived':prediction})\nanswerdf.to_csv('TitanicSubmission.csv',index=False)","e8571348":"# Random forest classifier","3c6d586c":"# Visualizing data","41668425":"Converting the string and character values of Sex and Embarked columns to numbers","f8bedde8":"Create a variable to store the passenger id to be used for submission","fe1ca7ba":"# Splitting the training and testing data","d1f78e47":"Check the presence of null values once before continuing","7e079c54":"First lets encode the title column depending on the prefixes used. Then, let's perform feature engineering and bring in 2 new columns related to number of family members on board. All this is done on both training and testing sets.","379cedf9":"No. of people who did and did not survive","a907397b":"Convert the gender strings from male and female to numbers 0 and 1 for predictions","6dc78169":"# Support vector machine","8f9a551f":"Fill in the NaN values in different columns with the mean value","f761345d":"# Using different classification models to check accuracy","2e229238":"People who boarded the Titanic from different ports who did and did not survive","5346abf7":"# Scaling the data","fdc9a008":"# XGBoost classifier with a base model.","eaa3cfad":"The no. of male and female members who did and did not survive","24899046":"Drop unwanted columns","a465abdf":"The age range of people on Titanic","d7552e57":"# Data cleaning\nLet's get rid of null\/unwanted data from the dataset before moving on to model prediction so as to make it easier and more accurate.\nCheck for null values first","aeb52430":"Drop cabin as it has the maximum number of null values","9c85af54":"We get a pretty good accuracy of 83% with XGBoost classifier. Let's now look at the classification report and confusion matrix.","1300636d":"# Decision tree","3979a94a":"People belonging to different passenger classes who did and did not survive","c5b72b09":"# Predicting the survival status of passengers in the test dataset","acb2ec84":"# Logistic Regression"}}