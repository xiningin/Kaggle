{"cell_type":{"ac257f31":"code","b64928ba":"code","18f0ad24":"code","64c20944":"code","47d36e1b":"code","a31bc1ff":"code","a86ef51d":"code","b14defce":"code","4cb9f2f7":"code","d74377ec":"code","48a486d7":"code","ffb46351":"code","325d8b76":"code","b12a1aa5":"code","ded81cf4":"code","bbb1d4a5":"code","cf067dbb":"code","c38d3eff":"code","ff8aac4f":"code","feb7b064":"code","ef242ce0":"code","f205d391":"code","77ddecd2":"code","6a62bef8":"code","02c7f18f":"code","22e9560f":"code","d535052a":"code","aea6d89a":"code","e1fa8443":"code","24c9771d":"code","b35cc695":"code","0643458c":"code","a669a3c5":"code","11f438b0":"code","6c5e18ec":"code","83f78743":"code","a534c760":"code","70b79533":"code","e5113cc1":"code","de420c0b":"code","d99fc61b":"code","7dbc9aab":"code","8d671a34":"code","f0e5c5f5":"code","008bb2f8":"code","0c71cdd8":"code","43f0e449":"code","1daa711b":"code","5195f99f":"code","f2570dd7":"code","21f0d805":"code","40920ccb":"code","231a6b8b":"code","a2eeed02":"code","cc86d5c6":"code","730a8a18":"code","eb0f4798":"code","c2f3f2e0":"code","90a3d565":"code","a376ddb0":"code","8b1e8794":"code","e2595360":"code","079e5ea8":"code","4572984f":"code","f12bb116":"code","2bfc5d20":"code","6f940801":"code","cd636b47":"code","13832935":"code","5ee797e4":"code","5441eb8c":"code","a4670008":"code","3ddffc84":"code","7309ff38":"code","2537442d":"code","23275a5a":"code","9f640cb8":"code","780910ed":"code","31dde9d7":"code","96d36fef":"code","59d8a4c3":"markdown","c7178616":"markdown","ebd64785":"markdown","c21dc6f9":"markdown","618980ed":"markdown","fb345937":"markdown","f7dc287f":"markdown","57ad5d0a":"markdown","8e42e9c3":"markdown","96bdbfd6":"markdown","d6ff7dd5":"markdown","2546e2f4":"markdown","4d39386f":"markdown","57c52c89":"markdown","12b9fbab":"markdown","f485d6b7":"markdown","564b50a8":"markdown","63fcec84":"markdown","db6472d2":"markdown","c0201f83":"markdown","ccf27595":"markdown","62921621":"markdown","00266bd4":"markdown","a3e9b365":"markdown","b797b695":"markdown","ae397460":"markdown","add80745":"markdown","eb1b8fa9":"markdown","0e624d39":"markdown","18dda48f":"markdown","486eed53":"markdown","026df668":"markdown","1f0b54f7":"markdown","df78bb77":"markdown","3d67d554":"markdown","b03d9154":"markdown"},"source":{"ac257f31":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","b64928ba":"df=pd.read_csv(\"..\/input\/diamonds\/diamonds.csv\")\ndf.head()","18f0ad24":"df.columns","64c20944":"df.price.head()","47d36e1b":"# Droping the 'Unnamed: 0' column.\ndf.drop(['Unnamed: 0'],axis=1,inplace=True)","a31bc1ff":"df.head() #Rechecking the dataframe","a86ef51d":"df.shape # checking the shape of the dataframe","b14defce":"df.info() #To get overall information of the dataset","4cb9f2f7":"df.describe() #To check the statistical data","d74377ec":"df.cut.value_counts() # to check unique values of cut variable.","48a486d7":"df.color.value_counts() #to get unique values of color variable.","ffb46351":"df.clarity.value_counts() #to get unique values of clarity variable.","325d8b76":"df.columns # Viewing the columns ","b12a1aa5":"# Calling categorical columns as cat & continous variables as cont\ncont=[ \"carat\",\"depth\",\"table\",'x', 'y','z','price']\ncat=['cut','color', 'clarity']","ded81cf4":"sns.pairplot(data=df,x_vars=[ \"carat\",\"depth\",\"table\",'x', 'y','z'],y_vars=\"price\",diag_kind=None)","bbb1d4a5":"plt.figure(figsize=(15,10))\nsns.heatmap(df[cont].corr(),annot=True,cmap=\"Greens\")","cf067dbb":"def plot1(i):\n    plt.figure(figsize=(15,5))\n    ax1=plt.subplot(121)\n    sns.countplot(df[i],ax=ax1)\n    plt.title(\"Count distribution of {} type in diamond dataset\".format(i))\n    \n    ax2=plt.subplot(122)\n    sns.barplot(x=df[i],y=df[\"price\"],ax=ax2)\n    plt.title(\"Total price distribution for each {} type\".format(i))\n    plt.show()\n\n    \n","c38d3eff":"for i in cat:\n    plot1(i)\n    print(\"*\"*75)","ff8aac4f":"# Creating dummies for 3 categorical variables and dropping th first reductant variable.\ncut=pd.get_dummies(df[\"cut\"],drop_first=True)\ncolor=pd.get_dummies(df[\"color\"],drop_first=True)\nclarity=pd.get_dummies(df[\"clarity\"],drop_first=True)","feb7b064":"# Adding the dummy variables to the dataframe.\ndf=pd.concat([cut,color,clarity,df],axis=1)\ndf.head()","ef242ce0":"#Dropping the main 3 categorical variables from dataframe\ndf.drop(cat,axis=1,inplace=True)","f205d391":"df.shape #Once again viewing the shape","77ddecd2":"#Importing scikit library\nimport sklearn\nfrom sklearn.model_selection import train_test_split","6a62bef8":"df_train,df_test=train_test_split(df,train_size=0.7,random_state=100)","02c7f18f":"df.shape # Viewing the shape of main dataframe","22e9560f":"df_train.shape #Viewing the shape of train dataset.","d535052a":"df_test.shape #Viewing ths shape of test dataset.","aea6d89a":"df_train[cont].head() #Viewing the train dataset continous variables.","e1fa8443":"# Importing library\nfrom sklearn.preprocessing import MinMaxScaler\nScaler=MinMaxScaler()","24c9771d":"#Scaling the continous variables to 0-1\ndf_train[cont]=Scaler.fit_transform(df_train[cont]) ","b35cc695":"df_train[cont].head() #Cross checking the test dataset once again.","0643458c":"df_train[cont].describe() #Checking the descriptive statistics of the scaled dataset.","a669a3c5":"y_train=df_train.pop(\"price\")","11f438b0":"y_train","6c5e18ec":"X_train=df_train","83f78743":"X_train.head()","a534c760":"X_train.shape","70b79533":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","e5113cc1":"# Running RFE with output variable is equal to 13.\nlm=LinearRegression()\nlm.fit(X_train,y_train)\n\nrfe=RFE(lm,13)\nrfe=rfe.fit(X_train,y_train)","de420c0b":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","d99fc61b":"col=X_train.columns[rfe.support_] # Storing the acceptable variables into col list","7dbc9aab":"X_train.columns[~rfe.support_]","8d671a34":"len(col) # checking the length of acceptable variables.","f0e5c5f5":"# Creating X_train_rfe dataframe with RFE selected variables\nX_train_rfe = X_train[col]","008bb2f8":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","0c71cdd8":"X_train_rfe.head()","43f0e449":"# Running the linear model\nlm = sm.OLS(y_train,X_train_rfe).fit()","1daa711b":"#Let's see the summary of our linear model\nlm.summary()","5195f99f":"X_vif1=X_train_rfe.drop([\"const\"],axis=1)\n\n# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Checking the Vif's:\nvif = pd.DataFrame()\nX = X_vif1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f2570dd7":"X_train_new1=X_train_rfe.drop([\"x\"],axis=1)","21f0d805":"X_train_new1.columns","40920ccb":"# Running the linear model\nlm = sm.OLS(y_train,X_train_new1).fit()\nlm.summary()","231a6b8b":"### Checking the VIF:\nX_vif2=X_train_new1.drop([\"const\"],axis=1)\n\nvif = pd.DataFrame()\nX = X_vif2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a2eeed02":"X_train_new2=X_train_new1.drop([\"depth\"],axis=1)","cc86d5c6":"len(X_train_new2.columns)","730a8a18":"lm = sm.OLS(y_train,X_train_new2).fit()\nlm.summary()","eb0f4798":"### Checking the VIF:\nX_vif3=X_train_new2.drop([\"const\"],axis=1)\n\nvif = pd.DataFrame()\nX = X_vif3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c2f3f2e0":"X_train_new3=X_train_new2.drop([\"table\"],axis=1)","90a3d565":"len(X_train_new3.columns)","a376ddb0":"lm = sm.OLS(y_train,X_train_new3).fit()\nlm.summary()","8b1e8794":"### Checking the VIF:\nX_vif4=X_train_new3.drop([\"const\"],axis=1)\n\nvif = pd.DataFrame()\nX = X_vif4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e2595360":"# Finding the price predicted values using the regression model built already:\ny_train_pred = lm.predict(X_train_new3) ","079e5ea8":"res=y_train-y_train_pred","4572984f":"res.head()","f12bb116":"plt.figure(figsize=(15,5))\nax1=plt.subplot(121)\nsns.distplot(res,ax=ax1)\nplt.title(\"Error density distribution\",fontdict={\"fontsize\":20,\"fontweight\":-0.5,\"color\":\"Red\"})\nplt.xlabel(\"Error terms\")\nax2=plt.subplot(122)\nsns.scatterplot(x=y_train,y=res,ax=ax2)\nplt.title(\"Error terms vs price_train\",fontdict={\"fontsize\":20,\"fontweight\":-0.5,\"color\":\"Red\"})\nplt.ylabel(\"Error terms\")\nplt.show()","2bfc5d20":"df_test.head()","6f940801":"df_test[cont]=Scaler.transform(df_test[cont])","cd636b47":"df_test[cont].head()","13832935":"df_test[cont].describe()","5ee797e4":"y_test=df_test.pop(\"price\")","5441eb8c":"y_test.head()","a4670008":"X_test=df_test","3ddffc84":"X_test.head()","7309ff38":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new=X_test[X_vif4.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","2537442d":"X_test_new.head()","23275a5a":"y_pred_lm = lm.predict(X_test_new)","9f640cb8":"X_test_new.shape","780910ed":"#Evaluating the r-square for test data:\nfrom sklearn.metrics import r2_score\nr2_score(y_true=y_test,y_pred=y_pred_lm)","31dde9d7":"n=16182\nr2=0.907\nk=10\nAdjusted_R2 = 1 - float((1-r2)*(n-1)\/(n-k-1))\nprint(Adjusted_R2)","96d36fef":"# Scatter plot:\nsns.scatterplot(x=y_test,y=y_pred_lm)\nplt.xlabel(\"y_actual on test data\")\nplt.ylabel(\"y_predict on test data\")\nplt.title(\"y_actual vs y_predict\",fontdict={\"fontsize\":20,\"fontweight\":-0.5,\"color\":\"Red\"})\nplt.show()","59d8a4c3":"### Business Insights:\n### Train set:\n1. rsquare: __0.91__\n2. adjusted square: __0.90__\n3. Rsqure is 0.91 which tells the corelation  between price of diamonds vs different independent varaiables explained by __91%__\n4. If we see the final model:\n   1. price=-0.3022-0.05*(I)-0.10*(J)+0.3026*(IF)+0.205*(Sl1)+0.154*(Sl2)+0.257*(VS1)+0.241*(VS2)+0.284*(VVS1)+0.2817*(VVS2)+2.27*(carat)\n   2. Co-efficient of the __Carat__ is highest most, which signifies if there is increase of one unit of carat there will increase of 2.27 in price.\n   3. Next most positive effecting indepenedent variable is __IF__ clarity type variable.\n   4. Most Negatively effecting parameter is __J__ color type diamonds ,means a loss of 0.10 will occur with increase of one unit of J color type diamonds.\n    \n   \n### Test set:\n1. rsquare: __0.907__\n2. Adjusted square: __0.906__\n\n* Finally, Our linear model is __good__ as the r-square difference in train & test dataset is less than __5%__.","c7178616":"### Model Visualization:","ebd64785":"#### By inspection:\n* Continous\/Numerical variables are: \"carat\",\"depth\",\"table\",\"price\",'x', 'y','z'\n* Categorical variables are: 'cut','color', 'clarity'","c21dc6f9":"### Takeways from pairplot:\n* There is some sought of good linear relationship between x,y,z,carat and price variables.| \n","618980ed":"### Vif for \"table\" variable is high. It means effected by mutlicollinearity. So, we shall drop \"table\" variable.","fb345937":"### Divide train dataset into X and y datasets.","f7dc287f":"### Making Predictions on Test data","57ad5d0a":"### Creating X_test, y_test variables","8e42e9c3":"### Vif for \"x\" variable is high. It means effected by mutlicollinearity. So, we shall drop \"x\" variable.","96bdbfd6":"### Takeways from above plots-\n* __Cut type__- Even Ideal cut type diamonds are high in the dataset, it doesn't have high price in total.\n* __Cut type__- Even Fair diamonds are least in the dataset, it secures 2nd position in total price distribution of cut category.\n* __Cut type__- Premium cut type diamonds in the dataset have high price in total.\n* __Color type__- J type has least in number of diamonds in dataset, but it has high price in total price distribution.\n* __Color type__- G type are maximum in number in dataset but it doesnot have high price in total price distribution in the dataset.\n* __Clarity type__- L1 diamonds are least in number in the dataset but it considerably has high price in total price distribution of clarity type diamonds.\n* __Clarity type__- Sl1 diamonds are maximum in the dataset but it doesn't have high price in total price distribution of clairty type diamonds.","d6ff7dd5":"### Take aways from heatmap:\n* Strongest correlation between \"x\" & \"y\" that is 0.98.\n* Good correlation between \"price\" and \"carat\" that is 0.92\n* Poor correlation between \"depth\" and \"price\" that is -0.11\n* Poor correlation between \"table\" and \"price\" that is 0.13.","2546e2f4":"### Checking the multicollinearity of variables by Variance Inflation Factor:","4d39386f":"* first column \"Unnamed:0\" is just as the index type so we can drop this column.\n* carat weight of the diamond-Continous type variable\n* cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)- categorical type variable \n* color diamond colour, from J (worst) to D (best)-categorical type variable \n* clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) - categorical type variable \n* x length in mm (0--10.74)-Continous variable type\n* y width in mm (0--58.9)-continous variable type\n* z depth in mm (0--31.8)-continous variable type\n* depth total depth percentage = z \/ mean(x, y)-Continous variable type\n* table width of top of diamond relative to widest point (43--95)-Continous variable type","57c52c89":"### Exploratory Data Analysis:\n#### Numerical Analysis ( Target variable vs remaing continous variables):","12b9fbab":"####  Thanks for opportunity :)\n####  Your reviews are always welcome.\n","f485d6b7":"### Building model using statsmodel, for the detailed statistics","564b50a8":"### Building our model","63fcec84":"### Validating linear regression assumptions:\n#### Insights from above two graphs:\n* Errors terms were normally distributed(from left graph 1).\n* Errors terms are more or less randomly distrubted with x values. \n* Variance is also fine..but for some data points error term's variance is quite high.","db6472d2":"### Heatmap\n#### To find the correlation between the numerical variables we will plot heatmap.","c0201f83":"### Analysis on Categorical variables:","ccf27595":"### Vif for \"depth\" variable is high. It means effected by mutlicollinearity. So, we shall drop \"depth\" variable.","62921621":"### Now finally, Co-efficients are significant (p values are less than 5% confidence interval) and VIF's also in considerable range(<5). let's write in an equation form:\n* price=-0.3022-0.05*(I)-0.10*(J)+0.3026*(IF)+0.205*(Sl1)+0.154*(Sl2)+0.257*(VS1)+0.241*(VS2)+0.284*(VVS1)+0.2817*(VVS2)+2.27*(carat)","00266bd4":"# Scaling the Continous variable in train dataset:","a3e9b365":"### Dummy creation for categorical variables:","b797b695":"### Understanding the data dictionary","ae397460":"* No data is missing..it is good to go","add80745":"#### Scaling the Test dataset","eb1b8fa9":"### Understanding the target variable (Price)","0e624d39":"### Residual Analysis on train_dataset:","18dda48f":"* Here we shall use Minmax approach also called as normal distribution to squeeze the values to 0 to 1.","486eed53":"* It is of continous datatype so we should use \"Supervised linear regression model\"","026df668":"### Intially importing some of the necessary librabries:","1f0b54f7":"### Data sanity check","df78bb77":"### Importing the dataset","3d67d554":"### Mixed Feature elimination\n* As the number of independent variables are 23 that is high. Initially we shall use Recursive Feature Elimination.\n* We will be using the LinearRegression function from SciKit Learn for its compatibility with RFE.\n* Once in the model the number of variables are 13, we shall use Manual Elimination method.","b03d9154":"### Train_Test_Split"}}