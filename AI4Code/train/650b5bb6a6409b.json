{"cell_type":{"f50e0ef5":"code","11005502":"code","693db488":"code","3f83eb3b":"code","bb0febfe":"code","02ae0825":"code","60776e96":"code","b9919036":"code","c956e4d0":"code","ddae8779":"code","3ee6f163":"code","3a32bba6":"code","24352a34":"code","254f9eeb":"code","06de2c0a":"code","da8e560a":"code","da2bc769":"code","3b6f1f34":"code","8c6bf63a":"code","ffa761bd":"code","05f73768":"code","8df60451":"code","c063f5db":"code","602dda34":"code","5ae98acc":"code","57ecfbf9":"code","099102aa":"markdown","52064fae":"markdown","e2939024":"markdown","1bd7c4b2":"markdown","53a7b61d":"markdown","6201d96e":"markdown","36a123e1":"markdown","dbb5325f":"markdown","e9548ac0":"markdown","cee44847":"markdown","2a79a0a8":"markdown","9db7712f":"markdown"},"source":{"f50e0ef5":"import numpy as np\nimport pandas as pd\n\n# Encoders\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier\n\n# Voting Classifier\nfrom sklearn.ensemble import VotingClassifier","11005502":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","693db488":"train_data","3f83eb3b":"train_data.describe()","bb0febfe":"print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))","02ae0825":"missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))","60776e96":"print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()\/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()\/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()\/len(train_data))))","b9919036":"duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))","c956e4d0":"categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))","ddae8779":"for data in [train_data, test_data]:\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    # Probably will not provide some useful information\n    data.drop(['Ticket', 'Fare'], axis=1, inplace=True)","3ee6f163":"train_data.tail()","3a32bba6":"# Find the women and boys\nfor data in [train_data, test_data]:\n    data['Title'] = data.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\n    data['Woman_Or_Boy'] = (data.Title == 'Master') | (data.Sex == 'female')\n    data.drop('Title', axis=1, inplace=True)\n    data.drop('Name', axis=1, inplace=True)","24352a34":"# Encode 'Sex' and 'Woman_Or_Boy' columns\nlabel_encoder = LabelEncoder()\nfor data in [train_data, test_data]:\n    data['Sex'] = label_encoder.fit_transform(data['Sex'])\n    data['Woman_Or_Boy'] = label_encoder.fit_transform(data['Woman_Or_Boy'])","254f9eeb":"# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\nfor data in [train_data, test_data]:\n    data.fillna(value={'Age': average}, inplace=True)","06de2c0a":"# Get the most common Embark and fill the column\nmost_common = all_data.Embarked.mode()\nprint(\"Most common Embarked value: {0}\".format(most_common[0]))\nfor data in [train_data, test_data]:\n    data.fillna(value={'Embarked': most_common[0]}, inplace=True)","da8e560a":"# Create categorical variable for traveling alone\n# Credits to https:\/\/www.kaggle.com\/vaishvik25\/titanic-eda-fe-3-model-decision-tree-viz\nfor data in [train_data, test_data]:\n    data['TravelAlone'] = np.where(data[\"SibSp\"] + data[\"Parch\"] > 0, 0, 1)\n    data.drop('SibSp', axis=1, inplace=True)\n    data.drop('Parch', axis=1, inplace=True)","da2bc769":"# Encode 'Embarked' column\none_hot_encoder = OneHotEncoder(sparse=False)\ndef encode_embarked(data):\n    encoded = pd.DataFrame(one_hot_encoder.fit_transform(data[['Embarked']]))\n    encoded.columns = one_hot_encoder.get_feature_names(['Embarked'])\n    data.drop(['Embarked'], axis=1, inplace=True)\n    data = data.join(encoded)\n    return data\ntrain_data = encode_embarked(train_data)\ntest_data = encode_embarked(test_data)","3b6f1f34":"train_data.tail()","8c6bf63a":"# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)","ffa761bd":"# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters,\n                               n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model","05f73768":"print(\"Features: \\n{0} \".format(X.columns.tolist()))","8df60451":"# I couldn't find a way to set fit_params of XGBClasssifier through GridSearchCV, so did a little trick.\n# https:\/\/stackoverflow.com\/questions\/35545733\/how-do-you-use-fit-params-for-randomizedsearch-with-votingclassifier-in-sklearn\nclass MyXGBClassifier(XGBClassifier):\n    def fit(self, X, y=None):\n        return super(XGBClassifier, self).fit(X, y,\n                                              verbose=False,\n                                              early_stopping_rounds=40,\n                                              eval_metric='logloss',\n                                              eval_set=[(val_X, val_y)])","c063f5db":"# Models from, https:\/\/www.kaggle.com\/sfktrkl\/titanic-hyperparameter-tuning-gridsearchcv\nrandomForest = RandomForestClassifier(random_state=1, n_estimators=20, max_features='auto',\n                                      criterion='gini', max_depth=4, min_samples_split=2,\n                                      min_samples_leaf=3)\nxgbClassifier = MyXGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor',\n                                use_label_encoder=False, learning_rate=0.4, gamma=0.4,\n                                max_depth=4, reg_lambda=0, reg_alpha=0.1)\nlgbmClassifier = LGBMClassifier(random_state=1, device='gpu', boosting_type='dart',\n                                num_leaves=8, learning_rate=0.1, n_estimators=100,\n                                reg_alpha=1, reg_lambda=1)\n\nclassifiers = [\n    ('randomForest', randomForest),\n    ('xgbClassifier', xgbClassifier),\n    ('lgbmClassifier', lgbmClassifier)\n]","602dda34":"hyperparameters = {\n    'n_jobs'  : [-1],\n    'voting'  : ['hard', 'soft'],\n    'weights' : [(1, 1, 1),\n                (2, 1, 1), (1, 2, 1), (1, 1, 2),\n                (2, 2, 1), (1, 2, 2), (2, 1, 2),\n                (3, 2, 1), (1, 3, 2), (2, 1, 3), (3, 1, 2)]\n}\nestimator = VotingClassifier(estimators=classifiers)\nbest_model_voting = get_best_model(estimator, hyperparameters)","5ae98acc":"evaluate_model(best_model_voting.best_estimator_, 'voting')","57ecfbf9":"# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)","099102aa":"# 7. Submission","52064fae":"## Check for duplicates","e2939024":"# 4. Data cleaning","1bd7c4b2":"# 1. Importing libraries and loading datasets","53a7b61d":"# 2. Explore data","6201d96e":"## [VotingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html)\n\n* **estimators: list of (str, estimator) tuples**  \n    Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored in the class attribute self.estimators_. An estimator can be set to 'drop' using set_params.\n\n* **voting: {\u2018hard\u2019, \u2018soft\u2019}, default=\u2019hard\u2019**  \n    If \u2018hard\u2019, uses predicted class labels for majority rule voting. Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n    \n* **weights: array-like of shape (n_classifiers,), default=None**  \n    Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.\n    \n* **n_jobs: int, default=None**  \n    The number of jobs to run in parallel for fit. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.","36a123e1":"# 6. Modelling\n\nUse all top scoring models to test them for GridSearch VotingClassifier.  \nhttps:\/\/www.kaggle.com\/sfktrkl\/titanic-hyperparameter-tuning-gridsearchcv","dbb5325f":"# 3. Basic data check","e9548ac0":"# 5. Feature engineering","cee44847":"## Classifiers","2a79a0a8":"## Missing values","9db7712f":"## Categorical variables"}}