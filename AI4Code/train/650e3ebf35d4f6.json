{"cell_type":{"4ee2661c":"code","9a00b729":"code","17701673":"code","56169c6b":"code","41d85087":"code","bba3da8c":"code","1f7f2376":"code","ea107c9b":"code","ede81507":"code","b7dff491":"code","a05e4dad":"code","ccbe47ee":"code","230c8d1c":"code","c9776a92":"code","e5e1ed81":"code","71ce73d5":"code","c5dad19a":"code","9589f294":"code","425136ca":"code","72ca939a":"code","e21b87c2":"code","cc15fa2e":"code","bb10c858":"code","46c14cf5":"code","1f81a3ab":"code","75ba9a4e":"markdown","b53183ec":"markdown","beecb247":"markdown","92062f10":"markdown","3d9f593f":"markdown","072764e0":"markdown","24820728":"markdown","e5322c53":"markdown","0542ebc4":"markdown","6f4f4939":"markdown","1e06b25a":"markdown","2a3580fb":"markdown","68e5ff12":"markdown","4536f3ec":"markdown","320ca5d2":"markdown"},"source":{"4ee2661c":"import pandas as pd\nimport datatable as dt\n\n# confirming the default pandas doesn't work (running the below code should result in a memory error)\n# data = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\")","9a00b729":"%%time\n\ndtypes = {\n    \"row_id\": \"int64\",\n    \"timestamp\": \"int64\",\n    \"user_id\": \"int32\",\n    \"content_id\": \"int16\",\n    \"content_type_id\": \"boolean\",\n    \"task_container_id\": \"int16\",\n    \"user_answer\": \"int8\",\n    \"answered_correctly\": \"int8\",\n    \"prior_question_elapsed_time\": \"float32\", \n    \"prior_question_had_explanation\": \"boolean\"\n}\n\ndata = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\", dtype=dtypes)\n\nprint(\"Train size:\", data.shape)","17701673":"data.head()","56169c6b":"# free memory\ndel data\nimport gc\ngc.collect()","41d85087":"import dask.dataframe as dd","bba3da8c":"%%time\n\ndtypes = {\n    \"row_id\": \"int64\",\n    \"timestamp\": \"int64\",\n    \"user_id\": \"int32\",\n    \"content_id\": \"int16\",\n    \"content_type_id\": \"boolean\",\n    \"task_container_id\": \"int16\",\n    \"user_answer\": \"int8\",\n    \"answered_correctly\": \"int8\",\n    \"prior_question_elapsed_time\": \"float32\", \n    \"prior_question_had_explanation\": \"boolean\"\n}\n\ndata = dd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\", dtype=dtypes).compute()\n\nprint(\"Train size:\", data.shape)","1f7f2376":"data.head()","ea107c9b":"# free memory\ndel data\nimport gc\ngc.collect()","ede81507":"# datatable installation with internet\n# !pip install datatable==0.11.0 > \/dev\/null\n\n# datatable installation without internet\n# !pip install ..\/input\/python-datatable\/datatable-0.11.1-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null\n\nimport datatable as dt","b7dff491":"%%time\n\ndata = dt.fread(\"..\/input\/riiid-test-answer-prediction\/train.csv\")\n\nprint(\"Train size:\", data.shape)","a05e4dad":"data.head()","ccbe47ee":"# free memory\ndel data\nimport gc\ngc.collect()","230c8d1c":"# rapids installation (make sure to turn on GPU)\nimport sys\n!cp ..\/input\/rapids\/rapids.0.19.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","c9776a92":"import cudf","e5e1ed81":"%%time\n\ndata = cudf.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\")\n\nprint(\"Train size:\", data.shape)","71ce73d5":"data.head()","c5dad19a":"# reading data from csv using datatable and converting to pandas\ndata = dt.fread(\"..\/input\/riiid-test-answer-prediction\/train.csv\").to_pandas()\n\n# writing dataset as csv\n# data.to_csv(\"riiid_train.csv\", index=False)\n\n# writing dataset as hdf5\n# data.to_hdf(\"riiid_train.h5\", \"riiid_train\")\n\n# writing dataset as feather\n# data.to_feather(\"riiid_train.feather\")\n\n# writing dataset as parquet\n# data.to_parquet(\"riiid_train.parquet\")\n\n# writing dataset as pickle\ndata.to_pickle(\"riiid_train.pkl.gzip\")\n\n# writing dataset as jay\n# dt.Frame(data).to_jay(\"riiid_train.jay\")","9589f294":"free memory\ndel data\nimport gc\ngc.collect()","425136ca":"%%time\n\ndtypes = {\n    \"row_id\": \"int64\",\n    \"timestamp\": \"int64\",\n    \"user_id\": \"int32\",\n    \"content_id\": \"int16\",\n    \"content_type_id\": \"boolean\",\n    \"task_container_id\": \"int16\",\n    \"user_answer\": \"int8\",\n    \"answered_correctly\": \"int8\",\n    \"prior_question_elapsed_time\": \"float32\", \n    \"prior_question_had_explanation\": \"boolean\"\n}\n\ndata = pd.read_csv(\"..\/input\/riiid-test-answer-prediction\/train.csv\", dtype=dtypes)\n\nprint(\"Train size:\", data.shape)","72ca939a":"%%time\n\ndata = pd.read_feather(\"..\/input\/riiid-train-data-multiple-formats\/riiid_train.feather\")\n\nprint(\"Train size:\", data.shape)","e21b87c2":"%%time\n\ndata = pd.read_hdf(\"..\/input\/riiid-train-data-multiple-formats\/riiid_train.h5\", \"riiid_train\")\n\nprint(\"Train size:\", data.shape)","cc15fa2e":"import datatable as dt","bb10c858":"%%time\n\ndata = dt.fread(\"..\/input\/riiid-train-data-multiple-formats\/riiid_train.jay\")\n\nprint(\"Train size:\", data.shape)","46c14cf5":"%%time\n\ndata = pd.read_parquet(\"..\/input\/riiid-train-data-multiple-formats\/riiid_train.parquet\")\n\nprint(\"Train size:\", data.shape)","1f81a3ab":"%%time\n\ndata = pd.read_pickle(\"..\/input\/riiid-train-data-multiple-formats\/riiid_train.pkl.gzip\")\n\nprint(\"Train size:\", data.shape)","75ba9a4e":"## There is no winner or loser\n![choice.png](attachment:choice.png)\n\nEach method has it's own set of pros and cons. Some examples are:   \n* **Pandas** requires a lot more RAM to handle large datasets.\n* **Dask** can be slow at times especially with transformations that cannot be parallelized.\n* **Datatable** doesn't have a very exhaustive set of data processing functions.\n* **Rapids** is not useful if you don't have a GPU.\n\nSo it's a good idea to explore various options and finally choose whichever appropriately fits the requirements. I strongly believe in not marrying a technology and continuously adapting to newer ideas, better approaches and ultimately the best possible solutions for building data science pipelines.\n\nEven in my personal experience I've found different approaches working well on different datasets. So don't shy away from experimentation.\n\n> Data Science is blooming under the blessings of open source packages and communities","b53183ec":"## Format: feather\nIt is common to store data in feather (binary) format specifically for pandas. It significantly improves reading speed of datasets.\n\nRead more: https:\/\/arrow.apache.org\/docs\/python\/feather.html","beecb247":"## Format: csv\nMost Kaggle datasets are available in csv format and is pretty much the standard format in which datasets are shared. Almost all methods can be used to read data from csv.\n\nRead more: https:\/\/en.wikipedia.org\/wiki\/Comma-separated_values","92062f10":"## Method: Pandas\n![pandas.png](attachment:pandas.png)\n\n[Pandas](https:\/\/pandas.pydata.org\/) is probably the most popular method of reading datasets and is also the default on Kaggle. It has a lot of options, flexibility and functions for reading and processing data.\n\nOne of the challenges with using pandas for reading large datasets is it's conservative nature while infering data types of the columns of a dataset often resulting in unnecessary large memory usage for the pandas dataframe. You can pre-define optimal data types of the columns (based on prior knowledge or sample inspection) and provide it explicitly while reading the dataset.\n\nThis is the method used in the [official starter notebook of the RiiiD competition](https:\/\/www.kaggle.com\/sohier\/competition-api-detailed-introduction) as well.\n\nDocumentation: https:\/\/pandas.pydata.org\/docs\/","3d9f593f":"## Methods\nBefore exploring various methods let's once confirm that reading the dataset using the default pandas setting fails.","072764e0":"## Method: Rapids\n![rapids.png](attachment:rapids.png)\n\n[Rapids](https:\/\/rapids.ai\/) is a great option to scale data processing on GPUs. With a lot of machine learning modelling moving to GPUs, Rapids enables to build end-to-end data science solutions on one or more GPUs.\n\nDocumentation: https:\/\/docs.rapids.ai\/","24820728":"## Format: parquet\nIn the Hadoop ecosystem, parquet was popularly used as the primary file format for tabular datasets and is now extensively used with Spark. It has become more available and efficient over the years and is also supported by pandas.\n\nRead more: https:\/\/parquet.apache.org\/documentation\/latest\/","e5322c53":"## Method: Datatable\n![py_datatable_logo.png](attachment:py_datatable_logo.png)\n\n[Datatable](https:\/\/github.com\/h2oai\/datatable) (heavily inspired by R's data.table) can read large datasets fairly quickly and is often faster than pandas. It is specifically meant for data processing of tabular datasets with emphasis on speed and support for large sized data.\n\nDocumentation: https:\/\/datatable.readthedocs.io\/en\/latest\/index.html\n","0542ebc4":"## Format: pickle\nPython objects can be stored in the form of pickle files and pandas has inbuilt functions to read and write dataframes as pickle objects.\n\nRead more: https:\/\/docs.python.org\/3\/library\/pickle.html","6f4f4939":"## Reading Different file formats","1e06b25a":"## Format: hdf5\nHDF5 is a high-performance data management suite to store, manage and process large and complex data.\n\nRead more: https:\/\/www.hdfgroup.org\/solutions\/hdf5","2a3580fb":"## Large datasets\n![dataset.png](attachment:dataset.png)\n\n> As a Data Scientist or Kaggler, we crave for more data\n\nHow many times have you complained about not having good enough data for working on a particular problem? Plenty I'm sure.   \nHow many times have you complained about having too much data to work with? Maybe not many but it's still a better problem to have.\n\nThe most common resolution in the scenario of having too much data is using a part\/sample of it that fits in the RAM available. But that leads to wastage of the unused data and sometimes loss of information. Many times there are ways to overcome this challenge without the need of subsampling. A single solution might not cater to all requirements and so different solutions can work in different scenarios.\n\nThis notebook aims to describe and summarize some of these techniques. The [Riiid! Answer Correctness Prediction](https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction) dataset is quite a nice sample to experiment on since the plain vanilla ***pd.read_csv*** will result in an out-of-memory error on Kaggle Notebooks. It has over 100 million rows and 10 columns.\n\nDifferent packages have their own way of reading data. The methods explored in the notebook (Default ***pandas*** and rest alphabetically):\n\n* [Pandas](#Method:-Pandas)\n* [Dask](#Method:-Dask)\n* [Datatable](#Method:-Datatable)\n* [Rapids](#Method:-Rapids)\n\nApart from methods of reading data from the raw csv files, it is also common to convert the dataset into another format which uses lesser disk space, is smaller in size and\/or can be read faster for subsequent reads. The file types explored in the notebook (Default ***csv*** and rest alphabetically):\n\n* [csv](#Format:-csv)\n* [feather](#Format:-feather)\n* [hdf5](#Format:-hdf5)\n* [jay](#Format:-jay)\n* [parquet](#Format:-parquet)\n* [pickle](#Format:-pickle)\n\nNote that just reading data is not the end of the story. The final decision of which method to use should also consider the downstream tasks and processes of the data that will be required to run. But that is outside the scope of this notebook.\n\nYou will also find that for different datasets or different environments, there will be different methods that work best. So there is no clear winner as such.\n\nFeel free to share other approaches that can be added to this list.","68e5ff12":"## Format: jay\nDatatable uses .jay (binary) format which makes reading datasets blazing fast. An example notebook is shared [here](https:\/\/www.kaggle.com\/rohanrao\/riiid-with-blazing-fast-rid) and also shown below which reads the entire dataset in less than a second!\n\nRead more: https:\/\/datatable.readthedocs.io\/en\/latest\/api\/frame\/to_jay.html","4536f3ec":"## File Formats\nIt is common to convert a dataset into a format which is easier or faster to read or smaller in size to store. There are various formats in which datasets can be stored though not all will be readable across different packages. Let's look at how these datasets can be converted into different formats.\n\nMost of them are available in pandas: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/io.html\n","320ca5d2":"## Method: Dask\n![dask.png](attachment:dask.png)\n\n[Dask](https:\/\/dask.org\/) provides a framework to scale pandas workflows natively using a parallel processing architecture. For those of you who have used [Spark](https:\/\/spark.apache.org\/), you will find an uncanny similarity between the two.\n\nDocumentation: https:\/\/docs.dask.org\/en\/latest\/"}}