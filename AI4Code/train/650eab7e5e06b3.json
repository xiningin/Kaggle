{"cell_type":{"ac3a9a3e":"code","9d062aca":"code","36b48bb4":"code","f9ee31f2":"code","922994cd":"code","36b67681":"code","e8e758b8":"code","29d4a224":"code","be306139":"code","14de4aae":"code","ca1e8863":"code","9c13034b":"code","543c23eb":"code","67783c55":"code","60aa7e33":"code","08fe5698":"code","fb38b115":"code","b48d1ab1":"code","a1286f61":"code","49f7e6b6":"code","d948875a":"code","9706b8e1":"code","20fe4070":"code","313e5329":"code","b8048f37":"code","41ea7daa":"code","2446933e":"code","2a19e5ba":"code","9ff08a93":"code","76532c93":"markdown","f8906c9a":"markdown"},"source":{"ac3a9a3e":"#import the required libaries for reading the csv file and for plotting the data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9d062aca":"# Read the data using pandas\ndata = pd.read_csv('\/kaggle\/input\/pyramid-scheme-profit-or-loss\/pyramid_scheme.csv')","36b48bb4":"# Check for the top 5 rows of data\ndata.head()","f9ee31f2":"# This provides the basic information the data\ndata.info()","922994cd":"# this explains the basic stat behind the data\ndata.describe()","36b67681":"# Remove the column unnamed \ndata = data.iloc[:,1:]\ndata.head()","e8e758b8":"# paiplot the data to check the linearity\nplt.figure(figsize=(12,6))\nsns.pairplot(data,kind='scatter')\nplt.show()","29d4a224":"# Correlation matrix\nplt.figure(figsize=(12,6))\ncor = data.corr()\nsns.heatmap(cor,annot=True)","be306139":"# oulier detection\nsns.boxplot(x=data['cost_price'])","14de4aae":"sns.boxplot(x=data['profit_markup'])","ca1e8863":"sns.boxplot(x=data['depth_of_tree'])","9c13034b":"sns.boxplot(data['sales_commission'])","543c23eb":"sns.boxplot(data['profit'])","67783c55":"# Hence there are no ouliers cook the data\nX = data[['cost_price','profit_markup','depth_of_tree','sales_commission']]\nX.head(2)","60aa7e33":"y = data['profit']\ny.head(2)","08fe5698":"# Import Linear Regresion model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()","fb38b115":"# Split the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)","b48d1ab1":"# Fit the model with the train dataset\nlr.fit(X_train,y_train)","a1286f61":"# predict the model with test data\ny_pred = lr.predict(X_test)","49f7e6b6":"# Coeff and intercept\nprint('coef:',lr.coef_)\nprint('Intercept:',lr.intercept_)","d948875a":"# Model evaluation\nfrom sklearn.metrics import r2_score,mean_squared_error\nmse = mean_squared_error(y_test,y_pred)\nrsq = r2_score(y_test,y_pred)","9706b8e1":"# R square value provides how accurate the model is for the data\nprint('mean sq error:',mse)\nprint('r square:',rsq)","20fe4070":"# visualising the actual and predicted\nplt.figure(figsize=(12,6))\nc = [i for i in range(1,len(y_test)+1,1)]\nplt.plot(c,y_test,color='b',linestyle='-')\nplt.plot(c,y_pred,color='r',linestyle='-')\nplt.xlabel('index')\nplt.ylabel('Profit')\nplt.title('Actual vs Predicted')\nplt.show()","313e5329":"# Plot the error value\nplt.figure(figsize=(12,6))\nc = [i for i in range(1,len(y_test)+1,1)]\nplt.plot(c,(y_test-y_pred),color='b',linestyle='-')\n#plt.plot(c,y_pred,color='r',linestyle='-')\nplt.xlabel('index')\nplt.ylabel('Profit')\nplt.title('Actual vs Predicted')\nplt.show()","b8048f37":"# import stat model\nimport statsmodels.api as sm","41ea7daa":"# Add constant to the train data\nX_train_new = X_train\nX_train_new = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_new).fit()","2446933e":"lm.params","2a19e5ba":"# This helps to identify the column which are really significant\nprint(lm.summary())","9ff08a93":"# Finally the predicted and the actual plot\nplt.figure(figsize=(12,6))\nplt.plot(y_test,y_pred,color='green',linestyle='-',linewidth=1.5)\nplt.show()","76532c93":"# **Multiple Linear Regression on Pyramid scheme data**","f8906c9a":" ***` If you like this approach please give this kernel an UPVOTE to show your appreciation `***"}}