{"cell_type":{"8f451433":"code","f5876576":"code","6efc3773":"code","5b225d51":"code","5239f827":"code","8feae1f3":"code","06a68d44":"code","5222c8a3":"code","b9916ace":"code","5a8eb07f":"code","0bf5fdd9":"code","3e0c2978":"code","aa7f3289":"code","ed03eaf3":"code","d8fd13a9":"code","1e9451b9":"code","42d40c70":"code","e295087f":"code","d4662b98":"code","13fc48d4":"code","f31ea7f2":"code","2b9b50b8":"code","f93b7375":"code","95e366ef":"code","642349ed":"code","cd0aa2bb":"code","4d083259":"code","dfd47e85":"code","b7f95af4":"code","5ae47606":"code","315fc363":"code","37c0f81b":"markdown","6339407b":"markdown","8f196fce":"markdown","50c54ce6":"markdown","859b861a":"markdown","4918d115":"markdown","a7802b39":"markdown","ac68bb7a":"markdown","63feb3ec":"markdown","64b46cc7":"markdown","5ad6235b":"markdown","25d842fc":"markdown","04fb7775":"markdown","ce93081a":"markdown","8092d3e2":"markdown","079a77d2":"markdown","6f49bd72":"markdown","48f7140c":"markdown"},"source":{"8f451433":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib\nfrom scipy.stats import norm\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f5876576":"train_df=pd.read_csv('..\/input\/train.csv')\ntest_df=pd.read_csv('..\/input\/test.csv')\nprint(train_df.shape)\nprint(test_df.shape)","6efc3773":"train_df.head()","5b225d51":"train_df.describe()","5239f827":"missing_data=pd.DataFrame(train_df.isnull().sum().reset_index())\n\nmissing_data.columns=[\"index\",\"missingcount\"]\nmissing_data=missing_data[missing_data[\"missingcount\"]>0]\nmissing_data['missingper']=(missing_data['missingcount']\/train_df.shape[0])*100\nmissing_data","8feae1f3":"train_df.drop(['MiscFeature','PoolQC','Fence','FireplaceQu','Alley','LotFrontage'],axis=1,inplace=True)\nprint(train_df.shape)","06a68d44":"cor=train_df.corr()\ncor['SalePrice'].sort_values(ascending=False)[0:20]\n","5222c8a3":"list_cor=list(cor['SalePrice'].sort_values(ascending=False)[0:20].index)\nfinal_df=train_df[list_cor]\nfinal_df.shape","b9916ace":"#list_cor=list(cor['SalePrice'].sort_values(ascending=False)[0:20].index)\nfinal_df[list_cor].isnull().sum()","5a8eb07f":"print(final_df['GarageYrBlt'].dtype)\nprint(final_df['MasVnrArea'].dtype)","0bf5fdd9":"final_df['GarageYrBlt']=final_df['GarageYrBlt'].fillna(final_df['GarageYrBlt'].mode()[0])\nfinal_df['MasVnrArea']=final_df['MasVnrArea'].fillna(final_df['MasVnrArea'].mode()[0])\nfinal_df[list_cor].isnull().sum()","3e0c2978":"plt.figure(figsize=(12,8))\nsns.distplot(final_df['SalePrice'], color='r')\nplt.title('Distribution of Sales Price', fontsize=18)\n\nplt.show()","aa7f3289":"print(final_df['SalePrice'].skew())\nprint(final_df['SalePrice'].kurt())","ed03eaf3":"final_df['SalePrice']=np.log(final_df.loc[:,'SalePrice'])\nprint(final_df['SalePrice'].skew())","d8fd13a9":"plt.figure(figsize=(12,8))\nsns.distplot(final_df['SalePrice'], color='r')\nplt.title('Distribution of Sales Price', fontsize=18)\nplt.show()\n\nfig = plt.figure(figsize=(12,8))\nres = stats.probplot(final_df['SalePrice'], plot=plt)\nplt.show()","1e9451b9":"corrmat = final_df.corr()\nf, ax = plt.subplots(figsize=(22, 9))\nsns.heatmap(corrmat, vmax=.8, square=True,annot=True,cmap='YlOrRd',linewidths=0.2,annot_kws={'size':10})\nplt.title(\"Heat map\",fontsize=20)\n","42d40c70":"final_df=final_df.drop([\"GarageArea\",\"TotRmsAbvGrd\",\"2ndFlrSF\",\"1stFlrSF\",\"GarageYrBlt\"],axis=1)\nfinal_df.shape","e295087f":"corrmat = final_df.corr()\nf, ax = plt.subplots(figsize=(22, 9))\nsns.heatmap(corrmat, vmax=.8, square=True,annot=True,cmap='YlOrRd',linewidths=0.2,annot_kws={'size':10})\nplt.title(\"Heat map\",fontsize=20)\n","d4662b98":"plt.figure(figsize=(10,8))\nplt.scatter(x='OverallQual',y='SalePrice',data =final_df)\nplt.ylabel(\"SalesPrice\")\nplt.xlabel(\"OverallQual\")\nplt.title(\"OverallQual vs SalePice\")","13fc48d4":"plt.figure(figsize=(10,8))\nplt.scatter(x='GrLivArea',y='SalePrice',data =final_df)\nplt.ylabel(\"SalesPrice\")\nplt.xlabel(\"GrLivArea\")\nplt.title(\"GrLivArea vs SalePice\")","f31ea7f2":"plt.figure(figsize=(10,8))\nplt.scatter(x='GarageCars',y='SalePrice',data =final_df)\nplt.ylabel(\"SalesPrice\")\nplt.xlabel(\"GarageCars\")\nplt.title(\"GarageCars vs SalePice\")","2b9b50b8":"plt.figure(figsize=(10,8))\nplt.scatter(x='TotalBsmtSF',y='SalePrice',data =final_df)\nplt.ylabel(\"SalesPrice\")\nplt.xlabel(\"TotalBsmtSF\")\nplt.title(\"TotalBsmtSF vs SalePice\")","f93b7375":"finaltest_df=final_df[\"SalePrice\"]\nfinaltrain_df=final_df.drop(\"SalePrice\",axis=1)\nprint(finaltrain_df.shape)\nprint(finaltest_df.shape)","95e366ef":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n","642349ed":"x_train,x_test,y_train,y_test=train_test_split(finaltrain_df,finaltest_df,test_size=0.3)\n\nlr=LinearRegression()\nlr.fit(x_train,y_train)\ny_pred=lr.predict(x_test)","cd0aa2bb":"score = r2_score(y_test,y_pred)\nscore","4d083259":"from sklearn.preprocessing import Imputer\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(x_train)\ntest_X = my_imputer.transform(x_test)","dfd47e85":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(x_train, y_train, verbose=False)","b7f95af4":"predictions = my_model.predict(x_test)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, y_test)))","5ae47606":"my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(x_train, y_train, early_stopping_rounds=5, \n             eval_set=[(x_test, y_test)], verbose=False)","315fc363":"score = r2_score(y_test,predictions)\nscore","37c0f81b":"See there is skewness in the SalePrice variable since it has a positive value it has right Skewness (ofcourse we can tell it from the graph). Here we can remove the skewness by log transforming the values of the SalePrice","6339407b":"Here i have printed top 20 features which make huge contrubution to target variables and now i will check whether any of these variables have null values or not if there are any i will fill them with most common values","8f196fce":"**4.TotalBsmtSF vs SalePrice**","50c54ce6":"Now we have handled missing values for the variables which are necessary so lets see how target variable values are and how these variable relate to the target variable which is SalePrice through visualizations","859b861a":"Now we will see what features have missing values and how many missing values each of these features contain","4918d115":"**Introduction :**\n    ","a7802b39":"**1.OveralQual vs SalePrice**","ac68bb7a":"**2.GrLivArea vs SalePrice ** ","63feb3ec":"So there are about 19 features which has null values and from this there are some feauters whose missing percent is very large so instead of filling these null vaues with average values it is better to remove those particular features because they dont provide much information. so i will remove the features whose missing percentage is greater.","64b46cc7":"Even after doing this we still have so many feautures which are filled with null values and luckily their missing percentage is low . so instead of deleting the features, we can do two things one is removing those paricular rows and another one is filling these null values with average values. but i have another option you wanna know what it is? welll ","5ad6235b":":**Conlclusion**","25d842fc":" HI friends !! in this kernal i will be doing Exploratory data analysis and data visualisation and modelling and this is my first kernal i hope you will like it  ","04fb7775":"We have two variables which are in good correlation with target variable and as well as has null values","ce93081a":"if observing heat map thoroughly we can see that there is a colinearity between the varibles.collinearity means having some relationship between the independent variables which may leads to so many problems when we interpret the results so it is better to remove the variables which are highly correlated between themselves","8092d3e2":"**3.GarageCars vs SalePrice **","079a77d2":"**Modelling**","6f49bd72":"In this kernal i try do some new things and i hope this will be useful to the beginers like me and there is more yet to come and i will modify this kernal again and agian till i make best out of it stay tune!!","48f7140c":"As we can see there are about 80 features that affect the variable we are going to predict. \nso we will see how data is..."}}