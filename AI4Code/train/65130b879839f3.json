{"cell_type":{"7783837c":"code","cb08059e":"code","aacbfd3e":"code","f7f10e26":"code","8b8d1828":"code","703bca55":"code","daf04498":"code","b29f5416":"code","17c51c6e":"code","e585ab50":"code","2c78ab75":"code","d1a29372":"code","5f4cf057":"code","6ffd0a60":"code","cb990a2b":"code","92c57796":"code","fe811248":"code","e8e790c3":"markdown","0eaf1b7e":"markdown","a6efbf4e":"markdown","d1acaf31":"markdown","8a74de50":"markdown","1810d187":"markdown","c9cd7266":"markdown","7e01f680":"markdown","963d6555":"markdown","b0c268a5":"markdown","ff797589":"markdown","8e7c3d4a":"markdown","4fe1d0c7":"markdown","00abad20":"markdown","25f4e21b":"markdown","8e2b446c":"markdown","689a316e":"markdown"},"source":{"7783837c":"import time\nimport torch\nfrom torch import nn, optim\nimport torchvision\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","cb08059e":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","aacbfd3e":"def vgg_block(num_conv, in_channels, out_channels):\n    blk = []\n    for i in range(num_conv):\n        if i == 0:\n            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        else:\n            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        blk.append(nn.ReLU())\n    blk.append(nn.MaxPool2d(kernel_size=2, stride=2))   # \u8fd9\u91cc\u4f1a\u4f7f\u5bbd\u9ad8\u51cf\u534a\n    return nn.Sequential(*blk)                          # \u8fd9\u91cc\u7684*\u662f\u62c6\u5305\uff0c\u76f8\u5f53\u4e8e\u628a\u5217\u8868\u4e2d\u7684\u5143\u7d20\u62c6\u6210\u4e00\u4e2a\u4e2a\u7684\u8fdb\u884c\u4f20\u53c2","f7f10e26":"# vgg\u5757\u7684\u53c2\u6570\uff1a num_conv ,in_channel, out_channel\nconv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n# \u7ecf\u8fc75\u4e2avgg_block\uff0c\u5bbd\u9ad8\u4f1a\u51cf\u534a5\u6b21\uff0c\u53d8\u6210224\/32 = 7\nfc_features = 512 * 7 * 7   # c * w * h\nfc_hidden_units = 4096      # \u4efb\u610f","8b8d1828":"class FlattenLayer(nn.Module):\n    def __init__(self):\n        super(FlattenLayer, self).__init__()\n    def forward(self, x): # x shape: (batch, *, *, ...)\n        return x.view(x.shape[0], -1)\n    \n# \u5b9e\u73b0VGG-11\ndef vgg(conv_arch, fc_features, fc_hidden_units=4096):\n    net = nn.Sequential()\n    # \u5377\u79ef\u5c42\u90e8\u5206\n    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n        # \u6bcf\u7ecf\u8fc7\u4e00\u4e2avgg_block\u90fd\u4f1a\u4f7f\u5bbd\u9ad8\u51cf\u534a\n        net.add_module(\"vgg_block\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n    # \u5168\u8fde\u63a5\u5c42\u90e8\u5206\n    net.add_module(\"fc\", nn.Sequential(\n        FlattenLayer(),\n        nn.Linear(fc_features, fc_hidden_units),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(fc_hidden_units, fc_hidden_units),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(fc_hidden_units, 10)\n    ))\n    return net","703bca55":"net = vgg(conv_arch, fc_features, fc_hidden_units)\nX = torch.rand(1, 1, 224, 224)\n\n# named_children\u83b7\u53d6\u4e00\u7ea7\u5b50\u6a21\u5757\u53ca\u5176\u540d\u5b57\uff08named_modules\u4f1a\u8fd4\u56de\u6240\u6709\u5b50\u6a21\u5757\uff0c\u5305\u62ec\u5b50\u6a21\u5757\u7684\u5b50\u6a21\u5757\uff09\nfor name, blk in net.named_children():\n    X = blk(X)\n    print(name, 'output shape: ', X.shape)","daf04498":"ratio = 8\nsmall_conv_arch = [(1, 1, 64\/\/ratio), (1, 64\/\/ratio, 128\/\/ratio), (2, 128\/\/ratio, 256\/\/ratio), \n                   (2, 256\/\/ratio, 512\/\/ratio), (2, 512\/\/ratio, 512\/\/ratio)]\nnet = vgg(small_conv_arch, fc_features \/\/ ratio, fc_hidden_units \/\/ ratio)\nprint(net)","b29f5416":"# \u6ce8\uff1a\u6b64\u7248\u672c\u7528\u5230\u4e86PIL\u5e93\uff0c\u5229\u7528\u63d2\u503c\u5c06\u56fe\u50cf\u653e\u5927\u4e86\uff01\nclass MyFashionMnistDataset(Dataset):\n    def __init__(self, train=True, transform=None):\n        super(MyFashionMnistDataset, self).__init__()\n        if train == True:\n            self.csv_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n        else:\n            self.csv_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n        self.labels = torch.tensor(self.csv_data.label.values)\n        self.data = self.csv_data.iloc[:, 1:].values.reshape((-1, 28, 28)).astype('uint8')  # pd\u8f6cnumpy\n        self.transform = transform\n    \n    def __getitem__(self, index):       # \u8fd9\u4e2a\u65b9\u6cd5\u662f\u5fc5\u987b\u8981\u6709\u7684\uff0c\u7528\u4e8e\u6309\u7167\u7d22\u5f15\u8bfb\u53d6\u6bcf\u4e2a\u5143\u7d20\u7684\u5177\u4f53\u5185\u5bb9\n        img = Image.fromarray(self.data[index], 'L')    # numpy\u8f6cpillow img\n        tag = self.labels[index]\n        if(self.transform is not None):\n            data_tensor = self.transform(img)\n        return data_tensor, tag     # return\u5f88\u5173\u952e\uff0creturn\u56de\u54ea\u4e9b\u5185\u5bb9\uff0c\u90a3\u4e48\u6211\u4eec\u5728\u8bad\u7ec3\u65f6\u5faa\u73af\u8bfb\u53d6\u6bcf\u4e2abatch\u65f6\uff0c\u5c31\u80fd\u83b7\u5f97\u54ea\u4e9b\u5185\u5bb9!\n        \n    def __len__(self):                  # \u8fd9\u4e2a\u51fd\u6570\u4e5f\u5fc5\u987b\u8981\u5199\uff0c\u5b83\u8fd4\u56de\u7684\u662f\u6570\u636e\u96c6\u7684\u957f\u5ea6\n        return len(self.data)\n\ndef get_fashion_mnist_dataset(batch_size, resize, num_workers=0):\n    trans = []\n    if resize:\n        trans.append(transforms.Resize(size=resize))\n    trans.append(transforms.ToTensor())\n    transform = transforms.Compose(trans)\n\n    train_dataset = MyFashionMnistDataset(train=True, transform=transform)\n    test_dataset = MyFashionMnistDataset(train=False, transform=transform)\n    \n    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n    \n    return train_iter, test_iter","17c51c6e":"batch_size = 128\ntrain_iter, test_iter = get_fashion_mnist_dataset(batch_size, resize=224)","e585ab50":"X, y = iter(test_iter).next()\nplt.subplot('141')\nplt.imshow(X[0][0])  ","2c78ab75":"def train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):\n    net = net.to(device)\n    print(\"training on \", device)\n    loss = torch.nn.CrossEntropyLoss()\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()\n        bar = tqdm(train_iter)\n        for X, y in bar:\n            bar.set_description(f\"epoch {epoch + 1}\")\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            train_l_sum += l.cpu().item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n            n += y.shape[0]\n            batch_count += 1\n        test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n              % (epoch + 1, train_l_sum \/ batch_count, train_acc_sum \/ n, test_acc, time.time() - start))\n        \ndef evaluate_accuracy(data_iter, net, device=None):\n    if device is None and isinstance(net, torch.nn.Module):\n        # \u5982\u679c\u6ca1\u6307\u5b9adevice\u5c31\u4f7f\u7528net\u7684device\n        device = list(net.parameters())[0].device\n    acc_sum, n = 0.0, 0\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(net, torch.nn.Module):\n                net.eval() # \u8bc4\u4f30\u6a21\u5f0f, \u8fd9\u4f1a\u5173\u95eddropout\n                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n                net.train() # \u6539\u56de\u8bad\u7ec3\u6a21\u5f0f\n            else: # \u81ea\u5b9a\u4e49\u7684\u6a21\u578b, 3.13\u8282\u4e4b\u540e\u4e0d\u4f1a\u7528\u5230, \u4e0d\u8003\u8651GPU\n                if('is_training' in net.__code__.co_varnames): # \u5982\u679c\u6709is_training\u8fd9\u4e2a\u53c2\u6570\n                    # \u5c06is_training\u8bbe\u7f6e\u6210False\n                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n                else:\n                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n            n += y.shape[0]\n    return acc_sum \/ n","d1a29372":"# \u8bad\u7ec3\nlr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\ntrain_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","5f4cf057":"# \u4fdd\u5b58\u6a21\u578b\ntorch.save(net.state_dict(), '.\/vgg11_model_mini.pt')","6ffd0a60":"from IPython import display\ndef use_svg_display():\n    display.set_matplotlib_formats('svg')\n    \ndef get_fashion_mnist_labels(labels):\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]\n\ndef show_fashion_mnist(images, labels):\n    use_svg_display()\n    # \u8fd9\u91cc\u7684_\u8868\u793a\u6211\u4eec\u5ffd\u7565\uff08\u4e0d\u4f7f\u7528\uff09\u7684\u53d8\u91cf\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((224, 224)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n    plt.show()","cb990a2b":"X, y = iter(test_iter).next()","92c57796":"X.shape","fe811248":"true_labels = get_fashion_mnist_labels(y.numpy())\npred_labels = get_fashion_mnist_labels(net(X.to(device)).argmax(dim=1).cpu().numpy())\ntitles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\nshow_fashion_mnist(X[0:10], titles[0:10])","e8e790c3":"AlexNet\u5728LeNet\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e863\u4e2a\u5377\u79ef\u5c42\u3002\u4f46AlexNet\u4f5c\u8005\u5bf9\u5b83\u4eec\u7684\u5377\u79ef\u7a97\u53e3\u3001\u8f93\u51fa\u901a\u9053\u6570\u548c\u6784\u9020\u987a\u5e8f\u5747\u505a\u4e86\u5927\u91cf\u7684\u8c03\u6574\u3002**\u867d\u7136AlexNet\u6307\u660e\u4e86\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u53d6\u5f97\u51fa\u8272\u7684\u7ed3\u679c\uff0c\u4f46\u5e76\u6ca1\u6709\u63d0\u4f9b\u7b80\u5355\u7684\u89c4\u5219\u4ee5\u6307\u5bfc\u540e\u6765\u7684\u7814\u7a76\u8005\u5982\u4f55\u8bbe\u8ba1\u65b0\u7684\u7f51\u7edc\u3002**\u6211\u4eec\u5c06\u5728\u672c\u7ae0\u7684\u540e\u7eed\u51e0\u8282\u91cc\u4ecb\u7ecd\u51e0\u79cd\u4e0d\u540c\u7684\u6df1\u5ea6\u7f51\u7edc\u8bbe\u8ba1\u601d\u8def\u3002","0eaf1b7e":"`VGG\u5757`\u7684\u7ec4\u6210\u89c4\u5f8b\u662f\uff1a**\u8fde\u7eed\u4f7f\u7528\u6570\u4e2a\u76f8\u540c\u7684\u586b\u5145\u4e3a1\u3001\u7a97\u53e3\u5f62\u72b6\u4e3a3\u00d73\u7684\u5377\u79ef\u5c42\u540e\u63a5\u4e0a\u4e00\u4e2a\u6b65\u5e45\u4e3a2\u3001\u7a97\u53e3\u5f62\u72b6\u4e3a2\u00d72\u7684\u6700\u5927\u6c60\u5316\u5c42\u3002\u5377\u79ef\u5c42\u4fdd\u6301\u8f93\u5165\u7684\u9ad8\u548c\u5bbd\u4e0d\u53d8\uff0c\u800c\u6c60\u5316\u5c42\u5219\u5bf9\u5176\u51cf\u534a\u3002**","a6efbf4e":"\u56e0\u4e3aVGG-11\u8ba1\u7b97\u4e0a\u6bd4AlexNet\u66f4\u52a0\u590d\u6742\uff0c\u51fa\u4e8e\u6d4b\u8bd5\u7684\u76ee\u7684\u6211\u4eec\u6784\u9020\u4e00\u4e2a\u901a\u9053\u6570\u66f4\u5c0f\uff0c\u6216\u8005\u8bf4\u66f4\u7a84\u7684\u7f51\u7edc\u5728Fashion-MNIST\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002","d1acaf31":"\u4e0b\u9762\u6784\u9020\u4e00\u4e2a\u9ad8\u548c\u5bbd\u5747\u4e3a224\u7684\u5355\u901a\u9053\u6570\u636e\u6837\u672c\u6765\u89c2\u5bdf\u6bcf\u4e00\u5c42\u7684\u8f93\u51fa\u5f62\u72b6\u3002","8a74de50":"# 5.\u9884\u6d4b","1810d187":"# 4. \u83b7\u53d6\u6570\u636e\u548c\u8bad\u7ec3\u6a21\u578b","c9cd7266":"**\u4e0eAlexNet\u548cLeNet\u4e00\u6837\uff0cVGG\u7f51\u7edc\u7531\u5377\u79ef\u5c42\u6a21\u5757\u540e\u63a5\u5168\u8fde\u63a5\u5c42\u6a21\u5757\u6784\u6210\u3002\u5377\u79ef\u5c42\u6a21\u5757\u4e32\u8054\u6570\u4e2avgg_block\uff0c\u5176\u8d85\u53c2\u6570\u7531\u53d8\u91cf`conv_arch`\u5b9a\u4e49\u3002\u8be5\u53d8\u91cf\u6307\u5b9a\u4e86\u6bcf\u4e2aVGG\u5757\u91cc\u5377\u79ef\u5c42\u4e2a\u6570\u548c\u8f93\u5165\u8f93\u51fa\u901a\u9053\u6570\u3002\u5168\u8fde\u63a5\u6a21\u5757\u5219\u8ddfAlexNet\u4e2d\u7684\u4e00\u6837\u3002**","7e01f680":"\u53ef\u4ee5\u770b\u5230\uff0c\u6bcf\u6b21\u6211\u4eec\u5c06\u8f93\u5165\u7684\u9ad8\u548c\u5bbd\u51cf\u534a\uff0c\u76f4\u5230\u6700\u7ec8\u9ad8\u548c\u5bbd\u53d8\u62107\u540e\u4f20\u5165\u5168\u8fde\u63a5\u5c42\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u8f93\u51fa\u901a\u9053\u6570\u6bcf\u6b21\u7ffb\u500d\uff0c\u76f4\u5230\u53d8\u6210512\u3002**\u56e0\u4e3a\u6bcf\u4e2a\u5377\u79ef\u5c42\u7684\u7a97\u53e3\u5927\u5c0f\u4e00\u6837\uff0c\u6240\u4ee5\u6bcf\u5c42\u7684\u6a21\u578b\u53c2\u6570\u5c3a\u5bf8\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u8f93\u5165\u9ad8\u3001\u8f93\u5165\u5bbd\u3001\u8f93\u5165\u901a\u9053\u6570\u548c\u8f93\u51fa\u901a\u9053\u6570\u7684\u4e58\u79ef\u6210\u6b63\u6bd4**\u3002**VGG\u8fd9\u79cd\u9ad8\u548c\u5bbd\u51cf\u534a\u4ee5\u53ca\u901a\u9053\u7ffb\u500d\u7684\u8bbe\u8ba1\u4f7f\u5f97\u591a\u6570\u5377\u79ef\u5c42\u90fd\u6709\u76f8\u540c\u7684\u6a21\u578b\u53c2\u6570\u5c3a\u5bf8\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002**","963d6555":"\u672c\u8282\u4ecb\u7ecdVGG\uff0c\u5b83\u7684\u540d\u5b57\u6765\u6e90\u4e8e\u8bba\u6587\u4f5c\u8005\u6240\u5728\u7684\u5b9e\u9a8c\u5ba4Visual Geometry Group [1]\u3002**VGG\u63d0\u51fa\u4e86\u53ef\u4ee5\u901a\u8fc7\u91cd\u590d\u4f7f\u7528\u7b80\u5355\u7684\u57fa\u7840\u5757\u6765\u6784\u5efa\u6df1\u5ea6\u6a21\u578b\u7684\u601d\u8def\u3002**","b0c268a5":"VGG-11\u901a\u8fc75\u4e2a\u53ef\u4ee5\u91cd\u590d\u4f7f\u7528\u7684\u5377\u79ef\u5757\u6765\u6784\u9020\u7f51\u7edc\u3002\u6839\u636e\u6bcf\u5757\u91cc\u5377\u79ef\u5c42\u4e2a\u6570\u548c\u8f93\u51fa\u901a\u9053\u6570\u7684\u4e0d\u540c\u53ef\u4ee5\u5b9a\u4e49\u51fa\u4e0d\u540c\u7684VGG\u6a21\u578b\u3002","ff797589":"# 2. VGG\u5757","8e7c3d4a":"# 6. \u5c0f\u7ed3","4fe1d0c7":"\u6211\u4eec\u4f7f\u7528vgg_block\u51fd\u6570\u6765\u5b9e\u73b0\u8fd9\u4e2a\u57fa\u7840\u7684VGG\u5757\uff0c\u5b83\u53ef\u4ee5\u6307\u5b9a\u5377\u79ef\u5c42\u7684\u6570\u91cf\u548c\u8f93\u5165\u8f93\u51fa\u901a\u9053\u6570\u3002","00abad20":"**\uff08\u7cbe\u534e\u90e8\u5206\uff01\uff01\uff01\uff09\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u611f\u53d7\u91ce\uff08\u4e0e\u8f93\u51fa\u6709\u5173\u7684\u8f93\u5165\u56fe\u7247\u7684\u5c40\u90e8\u5927\u5c0f\uff09\uff0c\u91c7\u7528\u5806\u79ef\u7684\u5c0f\u5377\u79ef\u6838\u4f18\u4e8e\u91c7\u7528\u5927\u7684\u5377\u79ef\u6838\uff0c\u56e0\u4e3a\u53ef\u4ee5\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\u6765\u4fdd\u8bc1\u5b66\u4e60\u66f4\u590d\u6742\u7684\u6a21\u5f0f\uff0c\u800c\u4e14\u4ee3\u4ef7\u8fd8\u6bd4\u8f83\u5c0f\uff08\u53c2\u6570\u66f4\u5c11\uff09\u3002\u4f8b\u5982\uff0c\u5728VGG\u4e2d\uff0c\u4f7f\u7528\u4e863\u4e2a3x3\u5377\u79ef\u6838\u6765\u4ee3\u66ff7x7\u5377\u79ef\u6838\uff0c\u4f7f\u7528\u4e862\u4e2a3x3\u5377\u79ef\u6838\u6765\u4ee3\u66ff5*5\u5377\u79ef\u6838\uff0c\u8fd9\u6837\u505a\u7684\u4e3b\u8981\u76ee\u7684\u662f\u5728\u4fdd\u8bc1\u5177\u6709\u76f8\u540c\u611f\u77e5\u91ce\u7684\u6761\u4ef6\u4e0b\uff0c\u63d0\u5347\u4e86\u7f51\u7edc\u7684\u6df1\u5ea6\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u6548\u679c\u3002**","25f4e21b":"# 1. VGG\u80cc\u666f","8e2b446c":"\u73b0\u5728\u6211\u4eec\u6784\u9020\u4e00\u4e2aVGG\u7f51\u7edc\u3002\u5b83\u67095\u4e2a\u5377\u79ef\u5757\uff0c\u524d2\u5757\u4f7f\u7528\u5355\u5377\u79ef\u5c42\uff0c\u800c\u540e3\u5757\u4f7f\u7528\u53cc\u5377\u79ef\u5c42\u3002**\u7b2c\u4e00\u5757\u7684\u8f93\u5165\u8f93\u51fa\u901a\u9053\u5206\u522b\u662f1\uff08\u56e0\u4e3a\u4e0b\u9762\u8981\u4f7f\u7528\u7684Fashion-MNIST\u6570\u636e\u7684\u901a\u9053\u6570\u4e3a1\uff09\u548c64**\uff0c\u4e4b\u540e\u6bcf\u6b21\u5bf9\u8f93\u51fa\u901a\u9053\u6570\u7ffb\u500d\uff0c\u76f4\u5230\u53d8\u4e3a512\u3002**\u56e0\u4e3a\u8fd9\u4e2a\u7f51\u7edc\u4f7f\u7528\u4e868\u4e2a\u5377\u79ef\u5c42\u548c3\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u6240\u4ee5\u7ecf\u5e38\u88ab\u79f0\u4e3aVGG-11**","689a316e":"# 3. VGG\u7f51\u7edc"}}