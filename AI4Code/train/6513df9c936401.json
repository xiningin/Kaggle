{"cell_type":{"5f4b92e9":"code","a045627a":"code","09706161":"code","56faa396":"code","fa316c5f":"code","fc4a5b3c":"code","71d25770":"code","d3702217":"code","b51b8267":"code","fff46ee2":"code","7efa1346":"code","b3286a30":"code","47a6ecaf":"code","49394467":"code","78710eaf":"code","11839c6e":"code","c470cdb8":"code","49c0c526":"code","8f9159bc":"code","78441445":"code","5f923f86":"code","fa85ab93":"code","92f70ebf":"code","aaa722d9":"code","91170740":"code","bc73d255":"code","8fc6f916":"code","45e5d5b4":"code","2b779e77":"code","242eef89":"code","edd8e4c7":"code","303b7b4e":"code","7316fdc2":"code","c073e9b6":"code","d579445f":"code","c48e276a":"code","0ed25178":"code","515ecc3c":"code","805cdf38":"code","09431a11":"code","1cd05c66":"code","cb4c932d":"code","a22fc539":"code","5d0452b9":"code","cb30d670":"code","837f1ff1":"code","0b236309":"code","67da7434":"code","a736bb44":"code","80d2c875":"code","de5e8cc6":"code","d93b7f8c":"code","177d970e":"code","85a4c42c":"code","8b86eb50":"code","81817bcc":"code","23d0c1c8":"code","7d4af055":"code","bbaa907c":"code","aaced829":"code","7fdedfc7":"code","1e1b898e":"code","9b901ae7":"code","90812a39":"code","9f4a2c7e":"code","de9103a9":"code","8b84f60c":"code","3a4d33f1":"code","b681b76b":"code","11e2ed9a":"code","e7e4f1c1":"markdown","7429c44a":"markdown","66c8c458":"markdown","b2d24423":"markdown","b11b66cf":"markdown","64518164":"markdown","ec33aecd":"markdown","529274cb":"markdown","3f3971f7":"markdown","dbebbdf3":"markdown","b73771c2":"markdown","030171ae":"markdown","f974ea1c":"markdown","5e7d734e":"markdown","3e6a3079":"markdown","ca3e574b":"markdown","743be5f8":"markdown","2acd7c78":"markdown","027d2a01":"markdown","4a2cb3fd":"markdown","d799df9f":"markdown","8ac0c68a":"markdown","58a7f13e":"markdown","7a4823c2":"markdown","f594314c":"markdown","ea78bcd0":"markdown","c7d2d23b":"markdown","27741268":"markdown","3491f8f3":"markdown","5cd065b8":"markdown","0b690f13":"markdown","2dae5350":"markdown","853b83e7":"markdown","8c888867":"markdown","68d92363":"markdown","b9d1e8b5":"markdown","6096e528":"markdown","aacf2ede":"markdown","83a82432":"markdown"},"source":{"5f4b92e9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\n\nfrom tqdm import tqdm\n\nimport os\nimport gc\nfrom itertools import combinations, chain\nfrom datetime import datetime\nprint(os.listdir(\"..\/input\"))\n\n\n\n# Any results you write to the current directory are saved as output.","a045627a":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nsmpsb = pd.read_csv(\"..\/input\/sample_submission.csv\")","09706161":"# First of all, let's see the distribution of each variable.\n# You can see that there is a big difference in distribution between training data and test data.\n\nfrom scipy.stats import gaussian_kde\n\ndef compare_dist(ax, feature, i=0):\n    sns.kdeplot(train_df[feature], label=\"train\", ax=ax)\n    sns.kdeplot(test_df[feature], label=\"test\", ax=ax)\n\n\ndef numeric_tile(plot_func):\n    fig, axs = plt.subplots(2, 5, figsize=(24, 6))\n    axs = axs.flatten()\n    \n    for i, (ax, col) in enumerate(zip(axs, train_df.columns.tolist()[1:11])):\n        plot_func(ax, col, i)\n        ax.set_title(col)\n    plt.tight_layout()\n    \nnumeric_tile(compare_dist)","56faa396":"# For the training data, display the distribution of variables for each target.\n\n# Please pay attention to \"Elevation\". The difference between the training data and the test data distribution is\n# thought to be due to the difference between the proportion of the target variables in the training data and the test data.\n\ndef compare_target(ax, feature, i=0):\n    sns.kdeplot(train_df.loc[:, feature], label=\"train\", ax=ax)\n    sns.kdeplot(test_df.loc[:, feature], label=\"test\", ax=ax)\n    for target in range(1, 8):\n        sns.kdeplot(train_df.loc[train_df[\"Cover_Type\"] == target, feature], label=target, alpha=0.5, lw=1, ax=ax)\n\nnumeric_tile(compare_target)        ","fa316c5f":"# I was able to obtain the distribution of the test data by submitting prediction data with all the same purpose variables.\n\n\"\"\"\nsmpsb = pd.read_csv(\"..\/input\/sample_submission.csv\")\nfor i in range(1, 8):\n    smpsb[\"Cover_Type\"] = i\n    smpsb.to_csv(\"all_{}.csv\".format(i), index=None)\"\"\"\n\n# and this is the magic number of this competition.\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}\n\n# By using these numbers, you can mimic the distribution of the test data from the training data.\ndef compare_balanced_dist(ax, feature, i=0):\n    min_ = min(train_df[feature].min(), test_df[feature].min())\n    max_ = max(train_df[feature].max(), test_df[feature].max())\n    X = np.linspace(min_, max_, 1000)\n\n    sns.kdeplot(train_df[feature], label=\"train\", ax=ax)\n    sns.kdeplot(test_df[feature], label=\"test\", ax=ax)\n    btest = np.zeros(1000)\n    \n    for target in range(1, 8):\n        btest += gaussian_kde(train_df.loc[train_df[\"Cover_Type\"] == target, feature])(X) * type_ratio[target-1]\n    \n    ax.plot(X, btest, label=\"balanced\")\n    ax.legend()\n\nnumeric_tile(compare_balanced_dist)","fc4a5b3c":"# By using the following functions, it is possible to perform almost the same evaluation\n# as the leader board even in the local environment.\n\ndef balanced_accuracy_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred, sample_weight=np.apply_along_axis(lambda x: type_ratio[x], 0, y_true-1))\n","71d25770":"# The angle can be divided into sine and cosine\nsin_ = np.sin(np.pi*train_df[\"Aspect\"]\/180)\ncos_ = np.cos(np.pi*train_df[\"Aspect\"]\/180)\n\n# However, if this feature quantity alone, the effect seems to be light.\nplt.figure(figsize=(5, 4))\nfor i in range(1, 8):\n    cat = np.where(train_df[\"Cover_Type\"] == i)[0]\n    r = (.5+0.2*i)\n    plt.scatter(cos_[cat]*(r), sin_[cat]*(r), alpha=0.02*r, s=6, label=i)\nplt.xlim(-2, 3)\nplt.legend()\nplt.savefig(\"aspect.png\")","d3702217":"# this may be good feature but unfortunally i forgot to add my data\nhydro_h = train_df[\"Vertical_Distance_To_Hydrology\"]\nhydro_v = train_df[\"Horizontal_Distance_To_Hydrology\"]","b51b8267":"plt.scatter(hydro_h, hydro_v, s=1, c=train_df[\"Cover_Type\"], cmap=\"Set1\", alpha=0.3)","fff46ee2":"hydro_arctan = np.arctan((hydro_h+0.0001) \/ (hydro_v+0.0001))\nfor i in range(1, 8):\n    cat = np.where(train_df[\"Cover_Type\"] == i)[0]\n    sns.kdeplot(hydro_arctan[cat])","7efa1346":"plt.scatter(hydro_arctan, np.pi*train_df[\"Slope\"]\/180, c=train_df[\"Cover_Type\"], cmap=\"Set1\", s=1.5, alpha=0.7)","b3286a30":"# this is the ratio of Wilderness_Area\nplt.figure(figsize=(6, 6))\ntrain_df.filter(regex=\"Wilder\").sum(axis=0).plot(\"pie\")","47a6ecaf":"# and this is ratio of \"over_Type\" in each \"Wildereness_area\"\nwilder = (train_df.filter(regex=\"Wilder\") * np.array([1, 2, 3, 4])).sum(axis=1)\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\naxs = axs.flatten()\nfor i, ax in enumerate(axs, start=1):\n    train_df.loc[wilder==i, \"Cover_Type\"].value_counts().sort_index().plot(\"pie\", ax=ax)\n    ax.set_title(i)","49394467":"# This shows the expression of Soil_Type for the objective variable.\nplt.figure(figsize=(12, 4))\nsns.heatmap(train_df.iloc[:, -41:].sort_values(by=\"Cover_Type\").iloc[:, :-1].T, cmap=\"Greys_r\")\nfor i in np.linspace(0, train_df.shape[0], 8)[1:]:\n    plt.axvline(i, c=\"r\")","78710eaf":"# this is the code\ndef categorical_post_mean(x):\n    p = (x.values)*type_ratio\n    p = p\/p.sum()*x.sum() + 10*type_ratio\n    return p\/p.sum()","11839c6e":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nsmpsb = pd.read_csv(\"..\/input\/sample_submission.csv\")\n\ndef main(train_df, test_df):\n    # this is public leaderboard ratio\n    start = datetime.now()\n    type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\n    \n    total_df = pd.concat([train_df.iloc[:, :-1], test_df])\n    \n    # Aspect\n    total_df[\"Aspect_Sin\"] = np.sin(np.pi*total_df[\"Aspect\"]\/180)\n    total_df[\"Aspect_Cos\"] = np.cos(np.pi*total_df[\"Aspect\"]\/180)\n    print(\"Aspect\", (datetime.now() - start).seconds)\n    \n    # Hillshade\n    hillshade_col = [\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\"]\n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) \/ (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    \n    total_df[\"Hillshade_mean\"] = total_df[hillshade_col].mean(axis=1)\n    total_df[\"Hillshade_std\"] = total_df[hillshade_col].std(axis=1)\n    total_df[\"Hillshade_max\"] = total_df[hillshade_col].max(axis=1)\n    total_df[\"Hillshade_min\"] = total_df[hillshade_col].min(axis=1)\n    print(\"Hillshade\", (datetime.now() - start).seconds)\n    \n    # Hydrology ** I forgot to add arctan\n    total_df[\"Degree_to_Hydrology\"] = ((total_df[\"Vertical_Distance_To_Hydrology\"] + 0.001) \/\n                                       (total_df[\"Horizontal_Distance_To_Hydrology\"] + 0.01))\n    \n    # Holizontal\n    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n                      \"Horizontal_Distance_To_Roadways\",\n                      \"Horizontal_Distance_To_Fire_Points\"]\n    \n    \n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) \/ (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n    print(\"Holizontal\", (datetime.now() - start).seconds)\n    \n    \n    def categorical_post_mean(x):\n        p = (x.values)*type_ratio\n        p = p\/p.sum()*x.sum() + 10*type_ratio\n        return p\/p.sum()\n    \n    # Wilder\n    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    wilder.columns = [\"Wilder_Type\", \"Cover_Type\"]\n    wilder[\"one\"] = 1\n    piv = wilder.pivot_table(values=\"one\",\n                             index=\"Wilder_Type\",\n                             columns=\"Cover_Type\",\n                             aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Wilder_Type\"] + [\"Wilder_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Wilder_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Wilder_Type\"] = (total_df.filter(regex=\"Wilder\") * np.arange(1, 5)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Wilder_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n    print(\"Wilder_type\", (datetime.now() - start).seconds)\n    \n    \n    # Soil type\n    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n    soil[\"one\"] = 1\n    piv = soil.pivot_table(values=\"one\",\n                           index=\"Soil_Type\",\n                           columns=\"Cover_Type\",\n                           aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n    print(\"Soil_type\", (datetime.now() - start).seconds)\n    \n    icol = total_df.select_dtypes(np.int64).columns\n    fcol = total_df.select_dtypes(np.float64).columns\n    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n    return total_df\n\ntotal_df = main(train_df, test_df)\none_col = total_df.filter(regex=\"(Type\\d+)|(Area\\d+)\").columns\ntotal_df = total_df.drop(one_col, axis=1)","c470cdb8":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)","49c0c526":"gc.collect()","8f9159bc":"all_set =  [['Elevation', 500],\n            ['Horizontal_Distance_To_Roadways', 500],\n            ['Horizontal_Distance_To_Fire_Points', 500],\n            ['Horizontal_Distance_To_Hydrology', 500],\n            ['Hillshade_9am', 500],\n            ['Aspect', 500],\n            ['Hillshade_3pm', 500],\n            ['Slope', 500],\n            ['Hillshade_Noon', 500],\n            ['Vertical_Distance_To_Hydrology', 500],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 200],\n            ['Elevation_PLUS_Aspect', 200],\n            ['Elevation_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_9am', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Roadways', 100],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Wilder_Type_PLUS_Elevation', 500],\n            ['Wilder_Type_PLUS_Hillshade_Noon_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Degree_to_Hydrology', 200],\n            ['Wilder_Type_PLUS_Hillshade_9am_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Aspect_Cos', 500],\n            ['Hillshade_9am_dif_Hillshade_Noon_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Hillshade_Noon_PLUS_Hillshade_3pm', 200],\n            ['Hillshade_Noon_add_Hillshade_3pm_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200]]\n\n\ndef simple_feature_scores2(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    scl = StandardScaler().fit(X.loc[:, cols])\n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = scl.transform(X.loc[train, cols])\n        X_val = scl.transform(X.loc[val, cols])\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = scl.transform(X_test.loc[:, cols])\n        C = clf(**params)\n        C.fit(scl.transform(X.loc[:, cols]), y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","78441445":"import warnings\nimport gc\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings(\"ignore\")\n\npreds = []\ntest_preds = []\nfor colname, neighbor in tqdm(all_set):\n    gc.collect()\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores2(KNeighborsClassifier,\n                                                          colname.split(\"_PLUS_\"),\n                                                          test=True,\n                                                          n_neighbors=neighbor)\n    preds.append(pred)\n    test_preds.append(test_pred)","5f923f86":"cols = list(chain.from_iterable([[col[0] + \"_KNN_{}\".format(i) for i in range(1, 8)] for col in all_set]))\nknn_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\nknn_train_df.columns = cols\nknn_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\nknn_test_df.columns = cols\n","fa85ab93":"all_set = [['Elevation', 4],\n           ['Horizontal_Distance_To_Roadways', 4],\n           ['Horizontal_Distance_To_Fire_Points', 3],\n           ['Horizontal_Distance_To_Hydrology', 4],\n           ['Hillshade_9am', 3],\n           ['Vertical_Distance_To_Hydrology', 3],\n           ['Slope', 4],\n           ['Aspect', 4],\n           ['Hillshade_3pm', 3],\n           ['Hillshade_Noon', 3],\n           ['Degree_to_Hydrology', 3],\n           ['Hillshade_Noon_dif_Hillshade_3pm', 3],\n           ['Hillshade_Noon_abs_Hillshade_3pm', 3],\n           ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 5],\n           ['Elevation_PLUS_Hillshade_max', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Aspect_Sin_PLUS_Aspect_Cos_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Fire_Points', 5],\n           ['Wilder_Type_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Hillshade_9am', 5],\n           ['Elevation_PLUS_Degree_to_Hydrology', 5],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Roadways', 5],\n           ['Wilder_Type_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Wilder_Type_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_std', 4],\n           ['Hillshade_9am_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_Noon_add_Hillshade_3pm', 5]]\n\ndef simple_feature_scores(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    \n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = X.loc[train, cols]\n        X_val = X.loc[val, cols]\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = X_test.loc[:, cols]\n        C = clf(**params)\n        C.fit(X.loc[:, cols], y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","92f70ebf":"preds = []\ntest_preds = []\nfor colname, depth in tqdm(all_set):\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores(DecisionTreeClassifier,\n                                                         colname.split(\"_PLUS_\"),\n                                                         test=True,\n                                                         max_depth=depth)\n    preds.append(pred)\n    test_preds.append(test_pred)\n\ncols = list(chain.from_iterable([[col[0] + \"_DT_{}\".format(i) for i in range(1, 8)] for col in all_set]))\ndt_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\ndt_train_df.columns = cols\n\ndt_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\ndt_test_df.columns = cols","aaa722d9":"# target encoding features(1.2.3)\nte_train_df = total_df.filter(regex=\"ctype\").iloc[:len(train_df)]\nte_test_df = total_df.filter(regex=\"ctype\").iloc[len(train_df):]\n","91170740":"train_level2 = train_df[[\"Id\"]]\ntest_level2 = test_df[[\"Id\"]]","bc73d255":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}","8fc6f916":"RFC1_col = [\"RFC1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in RFC1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","45e5d5b4":"rfc = RandomForestClassifier(n_estimators=150,\n                             max_depth=12,\n                             class_weight=class_weight,\n                             n_jobs=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.iloc[train, :]\n    X_val = X.iloc[val, :]\n\n    y_train = y[train]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n    y_val_pred = rfc.predict(X_val)\n    y_val_proba = rfc.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, RFC1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nrfc.fit(X, y)\ntest_level2.loc[:, RFC1_col] = rfc.predict_proba(X_test)\n#smpsb.loc[:, \"Cover_Type\"] = rfc.predict(X_test)\n#smpsb.to_csv(\"RFC1.csv\", index=None)","2b779e77":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","242eef89":"KNN1_col = [\"KNN1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNN1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","edd8e4c7":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]","303b7b4e":"knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n\nscl = StandardScaler().fit(X_test.drop(cat_col, axis=1))\nX_scl = scl.transform(X.drop(cat_col, axis=1))\nX_test_scl = scl.transform(X_test.drop(cat_col, axis=1))\npca = PCA(n_components=23).fit(X_test_scl)\nX_pca = pca.transform(X_scl)\nX_test_pca = pca.transform(X_test_scl)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X_pca[train]\n    X_val = X_pca[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    knn.fit(X_train, y_train)\n    y_val_pred = knn.predict(X_val)\n    y_val_proba = knn.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, KNN1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nknn.fit(X_pca, y)\ntest_level2.loc[:, KNN1_col] = knn.predict_proba(X_test_pca)\n#smpsb.loc[:, \"Cover_Type\"] = knn.predict(X_test_pca)\n#smpsb.to_csv(\"KNN1.csv\", index=None)","7316fdc2":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","c073e9b6":"LGBM1_col = [\"LGBM1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in LGBM1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","d579445f":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]\ncategorical_feature = [29, 38]\nlgbm_col = X.drop(cat_col[:-2], axis=1).columns.tolist()\nclass_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}","c48e276a":"gbm = lgb.LGBMClassifier(n_estimators=15,\n                         num_class=7,\n                         learning_rate=0.1,\n                         bagging_fraction=0.6,\n                         num_boost_round=370,\n                         max_depth=8,\n                         max_cat_to_onehot=40,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.loc[train, lgbm_col]\n    X_val = X.loc[val, lgbm_col]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n            verbose=50, categorical_feature=categorical_feature)\n\n    y_val_pred = gbm.predict(X_val)\n    y_val_proba = gbm.predict_proba(X_val)\n    \n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n    train_level2.loc[val, LGBM1_col] = y_val_proba\n\n\nX_all = X.loc[:, lgbm_col]\nX_test_lgbm = X_test.loc[:, lgbm_col]\ngbm.fit(X_all, y, verbose=50, categorical_feature=categorical_feature)\ntest_level2.loc[:, LGBM1_col] = gbm.predict_proba(X_test_lgbm)\n#smpsb[\"Cover_Type\"] = gbm.predict(X_test_lgbm)\n#smpsb.to_csv(\"LGBM1.csv\")","0ed25178":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","515ecc3c":"X_p = pd.concat([knn_train_df, dt_train_df, te_train_df], axis=1).astype(np.float32)\nX_test_p = pd.concat([knn_test_df, dt_test_df, te_test_df.reset_index(drop=True)], axis=1).astype(np.float32)","805cdf38":"KNNDT_RF_col = [\"KNNDT_RF_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_RF_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","09431a11":"rfc = RandomForestClassifier(n_jobs=-1,\n                             n_estimators=200,\n                             max_depth=None,\n                             max_features=.7,\n                             max_leaf_nodes=220,\n                             class_weight=class_weight)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n\n    y_pred = rfc.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_RF_col] = rfc.predict_proba(X_val)\n\nrfc.fit(X_p, y)\ntest_level2.loc[:, KNNDT_RF_col] = rfc.predict_proba(X_test_p)","1cd05c66":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","cb4c932d":"KNNDT_LR_col = [\"KNNDT_LR_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LR_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","a22fc539":"confusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=-1, multi_class=\"multinomial\", C=10**9, solver=\"saga\", class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_val_pred = lr.predict(X_val)\n    train_level2.loc[val, KNNDT_LR_col] = lr.predict_proba(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n\nlr.fit(X_p, y)\ntest_level2.loc[:, KNNDT_LR_col] = lr.predict_proba(X_test_p)","5d0452b9":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","cb30d670":"KNNDT_LGB_col = [\"KNNDT_LGB_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LGB_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","837f1ff1":"X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1).reset_index(drop=True)\n\nX_d = pd.concat([X.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_train_df,\n                 dt_train_df], axis=1)\n\nX_test_d = pd.concat([X_test.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_test_df,\n                 dt_test_df], axis=1)\n\nfcol = X_d.select_dtypes(np.float64).columns\nX_d.loc[:, fcol] = X_d.loc[:, fcol].astype(np.float32)\nX_d = X_d.values.astype(np.float32)\nX_test_d.loc[:, fcol] = X_test_d.loc[:, fcol].astype(np.float32)\nX_test_d = X_test_d.values.astype(np.float32)","0b236309":"class_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}\n\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=32,\n                         feature_fraction=0.3,\n                         min_child_samples=20,\n                         learning_rate=0.05,\n                         num_boost_round=430,\n                         max_depth=-1,                         \n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_d[train]\n    X_val = X_d[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, categorical_feature=[33, 42])\n\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_LGB_col] = gbm.predict_proba(X_val)\n    \ngbm.fit(X_d, y, categorical_feature=[33, 42])\ntest_level2.loc[:, KNNDT_LGB_col] = gbm.predict_proba(X_test_d)","67da7434":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","a736bb44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#import warnings\n#warnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nimport lightgbm as lgb","80d2c875":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","de5e8cc6":"#drop columns that have the same value in every row\ntrain.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)","d93b7f8c":"train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) \/ 3  \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) \/ 2 ","177d970e":"test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) \/ 3  \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) \/ 2","85a4c42c":"#Id for later use\nId_train=train['Id']\nId_test=test['Id']\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","8b86eb50":"x_train=train.drop('Cover_Type', axis=1)\ny_train=train['Cover_Type']","81817bcc":"#prepare df to store pred proba\nx_train_L2=pd.DataFrame(Id_train)\nx_test_L2=pd.DataFrame(Id_test)\nrf_cul=['rf'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in rf_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nrf=RandomForestClassifier(max_depth=None, max_features=20,n_estimators=500, random_state=1)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    rf.fit(x_train_L1, y_train_L1)\n    y_val_proba=rf.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, rf_cul]=y_val_proba\n\nrf.fit(x_train, y_train)\nx_test_L2.loc[:, rf_cul]=rf.predict_proba(test)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(rf.predict(test))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('rf.csv', index=False)\n\n#0.75604","23d0c1c8":"#prepare df to store pred proba\n#x_train_L2=pd.DataFrame(Id_train)\n#x_test_L2=pd.DataFrame(Id_test)\nlgbm_cul=['lgbm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lgbm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nlgbm=lgb.LGBMClassifier(learning_rate=0.3, max_depth=-1, min_child_samples=20, n_estimators=300, num_leaves=200, random_state=1, n_jobs=4)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    lgbm.fit(x_train_L1, y_train_L1)\n    y_val_proba=lgbm.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lgbm_cul]=y_val_proba\n\nlgbm.fit(x_train, y_train)\nx_test_L2.loc[:, lgbm_cul]=lgbm.predict_proba(test)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(lgbm.predict(test))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('lgbm.csv', index=False)","7d4af055":"lr_cul=['lr'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lr_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \npca=PCA(n_components=40)\nx_train_pca=pd.DataFrame(pca.fit_transform(x_train))\ntest_pca=pd.DataFrame(pca.transform(test))\n\npipeline=Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=10, solver='newton-cg', multi_class='multinomial',max_iter=500))])\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lr_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, lr_cul]=pipeline.predict_proba(test_pca)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('lr.csv', index=False)","bbaa907c":"svm_cul=['svm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in svm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \n#pca=PCA(n_components=40)\n#x_train_pca=pca.fit_transform(x_train)\n#test_pca=pca.transform(test)\n\npipeline=Pipeline([('scaler', StandardScaler()), ('svm', SVC(C=10, gamma=0.1, probability=True))])\n\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, svm_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, svm_cul]=pipeline.predict_proba(test_pca)\n\n#prepare df for submission\n#submit_df=pd.DataFrame(pipeline.predict(test_pca))\n#submit_df.columns=['Cover_Type']\n#submit_df['Id']=Id_test\n#submit_df=submit_df.loc[:, ['Id', 'Cover_Type']]\n#submit_df.to_csv('svm.csv', index=False)","aaced829":"# concatenate two data\ntrain_L2 = pd.concat([x_train_L2.iloc[:, 1:].reset_index(drop=True), train_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntest_L2 = pd.concat([x_test_L2.iloc[:, 1:].reset_index(drop=True), test_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntrain_L2.to_csv(\"Wtrain_L2.csv\", index=False)\ntest_L2.to_csv(\"Wtest_L2.csv\", index=False)","7fdedfc7":"# each models score\n\ny = pd.read_csv(\"..\/input\/train.csv\")[\"Cover_Type\"].values\nmodel_scores = {}\ntext = []\n\nfor i in range(10):\n    y_pred = np.argmax(train_L2.iloc[:, 7*i:7*(i+1)].values, axis=1) + 1\n    score = balanced_accuracy_score(y, y_pred)\n    model_scores[cols[i*7]] = score\n    text.append(\"{}\\t{:<.5}\".format(train_L2.columns[i*7], score))\n\nprint(*text[::-1], sep=\"\\n\")\npd.Series(model_scores).plot(kind=\"barh\")\nplt.savefig(\"model_summary.png\")","1e1b898e":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = train_level2.iloc[train, 1:]\n    X_val = train_level2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\n    #print(score[-1])\nprint(np.mean(score))","9b901ae7":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = x_train_L2.iloc[train, 1:]\n    X_val = x_train_L2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\nprint(np.mean(score))\n\nlr = LogisticRegression(n_jobs=1, class_weight=class_weight)\nlr.fit(x_train_L2, y)\n","90812a39":"score = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = train_L2.iloc[train, 1:]\n    X_val = train_L2.iloc[val, 1:]\n    y_train = y[train]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=1, class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_val)\n    score.append(balanced_accuracy_score(y_val, y_pred))\nprint(np.mean(score))","9f4a2c7e":"# this is 0.83266 on public LB\n\"\"\"\nsmpsb = pd.read_csv(\"..\/input\/sample_submission.csv\")\nlr = LogisticRegression(n_jobs=1, class_weight=class_weight)\nlr.fit(train_L2, y)\nsmpsb[\"Cover_Type\"] = lr.predict(test_L2)\nsmpsb.to_csv(\"W_ensemble_LR.csv\", index=False)\"\"\"","de9103a9":"wtrain = train_L2.values.astype(np.float32)\nwtest = test_L2.values.astype(np.float32)\ny = pd.read_csv(\"..\/input\/train.csv\")[\"Cover_Type\"].values\nsmpsb = pd.read_csv(\"..\/input\/sample_submission.csv\")\ncols = train_L2.columns","8b84f60c":"# this is our final submission which is 0.84806 on Public LB\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=4000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\ngbm.fit(wtrain, y, verbose=-1)\nsmpsb[\"Cover_Type\"] = gbm.predict(wtest)\nsmpsb.to_csv(\"final_submission.csv\", index=False)","3a4d33f1":"plt.figure(figsize=(6, 12))\nplt.barh(cols, gbm.feature_importances_)\nplt.savefig(\"feature_importances.png\")","b681b76b":"# bagging with k-fold\nscores = []\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=8000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=-1,\n                         silent=-1,\n                         verbose=-1)\n\nproba = np.zeros((wtest.shape[0], 7))\nfor train, val in tqdm(StratifiedKFold(n_splits=5, shuffle=True, random_state=2434).split(wtrain, y)):\n    X_train = wtrain[train]\n    X_val = wtrain[val]\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, verbose=-1, \n            eval_set=[(X_train, y_train), (X_val, y_val)], early_stopping_rounds=20)\n    proba += gbm.predict_proba(wtest) \/ 10\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n\nprint(np.mean(scores))","11e2ed9a":"smpsb[\"Cover_Type\"] = np.argmax(proba, axis=1) + 1\nsmpsb.to_csv(\"final_submission_bagging.csv\", index=False)","e7e4f1c1":"I will explain some of the features I consider important or unique.","7429c44a":"## stacking with Logistic Regression","66c8c458":"I have created 6 models\n\nwithout KNN&DT features\n* Random Forest Classifier\n* PCA & K-nearest Neighbors Classifier\n* LightGBM\n\nwith KNN & DT features\n* Random Forest Classifier\n* Logistic Regression\n* LightGBM\n\nUsing these learning machines, data for stacking was created using 10-fold cross validation.","b2d24423":"#### PCA & KNN","b11b66cf":"#### KNN_feature","64518164":"### double simple stacking","ec33aecd":"#### Logistic Regression","529274cb":"## LR","3f3971f7":"#### DT_features","dbebbdf3":"### feature engineering 1","b73771c2":"## stacking with LightGBM","030171ae":"#### LightGBM","f974ea1c":"### EDA & leader board hacking","5e7d734e":"#### RandomForestClassifier","3e6a3079":"### with KNN & DT features","ca3e574b":"# summary","743be5f8":"# nadare's kernel","2acd7c78":"# stacking","027d2a01":"## randomforest","4a2cb3fd":"## modeling","d799df9f":"### nadare's simple stacking","8ac0c68a":"## SVM","58a7f13e":"#### Random forest classifier","7a4823c2":"#### Aspect","f594314c":"#### summarizes preprocessing","ea78bcd0":"# ykskks's kernel","c7d2d23b":"## Level1 summary","27741268":"### ykskks's simple stacking","3491f8f3":"### KNN features and Decision tree feature","5cd065b8":"## model summary\nWe created a total of 10 learning models and stacked their predicted by LightGBM.\n\ntable of contents\n\n","0b690f13":"For the variable created up to the above, the decision tree and the k-nearest neighbor method are applied after narrowing down the number of variables and adding the prediction probability as the feature amount. \n\nI decided the combination of variables to be used last and the setting of parameters based on Multi-class logarithmic loss while considering diversity.","2dae5350":"## LightGBM","853b83e7":"## preprocessing","8c888867":"#### LightGBM","68d92363":"As indicated above, category values are considered to have a major role in classification.\n\nTherefore, in order to handle category values effectively, the ratio of object variables in each category value is added as a feature quantity.\n\nIn order to prevent data leakage and not to excessively trust category values which have only a small number, we added values for 10 data as prior distribution to each category.","b9d1e8b5":"#### target_encoding ","6096e528":"The feature enginnering ideas I used here are based on [Lathwal's amazing kernel ](https:\/\/www.kaggle.com\/codename007\/forest-cover-type-eda-baseline-model).\n\nI removed 'slope_hyd' feature from the original one beacause it did'nt seem to be that useful for prediction.\n","aacf2ede":"### without KNN&DT feature","83a82432":"#### degree to hydrology"}}