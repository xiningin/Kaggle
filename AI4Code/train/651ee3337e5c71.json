{"cell_type":{"f91cedba":"code","a94b3031":"code","3bff4f75":"code","a7584107":"code","28b53a83":"code","429adad1":"code","907562bf":"code","6c638463":"code","d7eb5308":"code","2eef0067":"code","c9e1b53e":"code","4e9f5056":"code","faa17989":"code","fb31359b":"code","b0e2776f":"code","0c18615b":"code","07651f73":"code","8b390e7e":"code","70bd7362":"code","e8c2718b":"code","b89b5233":"code","92e1fd50":"code","b22b4fcd":"code","c3dd76e4":"code","60dc89e1":"code","57936bb8":"code","7f58805a":"code","0881a72d":"code","9b5e9e20":"code","66059b8d":"code","ab2c5f7f":"code","fc88914a":"code","50cd1939":"code","34fdfc0d":"code","da8c397b":"code","14183a38":"code","9bfc05c9":"code","42ba31d6":"code","94299d29":"code","fb8f8a28":"code","71185da8":"code","64ff122d":"code","d0bd1215":"code","57568915":"code","4cfb7a79":"code","f980b1d7":"code","f00f3e38":"code","6d2f458d":"code","79d2dc16":"code","b91b2bfa":"code","ce71393b":"code","357556d0":"code","fed5fbdf":"code","25340f14":"code","346f63ea":"code","1513e3d7":"code","2e562249":"code","50e4a463":"code","ff7e318b":"markdown","f0f704dc":"markdown","8984b407":"markdown","eb37d4ab":"markdown","f2fd2aea":"markdown","f23987a3":"markdown","1f83efdd":"markdown","be0844c6":"markdown","b1aba867":"markdown","747ee30f":"markdown","90809c57":"markdown","2c902a42":"markdown","dadabf5c":"markdown","1352022a":"markdown","72edacaa":"markdown","41b7c70b":"markdown","7a6072ce":"markdown","3efe5a9c":"markdown","315c3ed3":"markdown","13983d84":"markdown","6befe3e2":"markdown","8e1a479a":"markdown","1802f656":"markdown","3d01a1a5":"markdown","a02eb2f8":"markdown","b56c9caa":"markdown","46665b05":"markdown","73827e21":"markdown","6da448b0":"markdown","0e29587f":"markdown","f13400b6":"markdown","4788f12b":"markdown","7905e1ef":"markdown","4e1315f7":"markdown","6a1fe1c7":"markdown","5dba843b":"markdown","8ccf7420":"markdown","3dbc36ef":"markdown","6fd330c5":"markdown","4db33ec3":"markdown","7465e330":"markdown","8cf8cad6":"markdown","410bd8f7":"markdown","2a517aa3":"markdown","58c4b5d6":"markdown","3c776810":"markdown","f909eacf":"markdown","0d0d55d5":"markdown","5af97696":"markdown","7bd72b9c":"markdown","6bf16de8":"markdown","3db51de4":"markdown","9f62e705":"markdown","e03cbbb0":"markdown","2fc96d8b":"markdown","86f4a245":"markdown","75ba2daf":"markdown","f19191f8":"markdown","a5d96538":"markdown","8a2eb2de":"markdown","6e400f84":"markdown","bfd29c27":"markdown","383eff19":"markdown","adfb7959":"markdown","d43df181":"markdown","60c8b384":"markdown","871c2690":"markdown","f1091395":"markdown","27aae4e4":"markdown","1b53bf4e":"markdown","d8b470b5":"markdown","47cc85d3":"markdown","04cb79f9":"markdown","6ae34ee9":"markdown","cb9480f9":"markdown","db645c08":"markdown","0abd2a1b":"markdown","eb7a2fc5":"markdown","46b2c707":"markdown","9935d94f":"markdown","053e3cae":"markdown","a7c82de4":"markdown","e4b2b847":"markdown","8be144bc":"markdown","9264200b":"markdown","72d67654":"markdown","3cac764b":"markdown","d4f5f7f8":"markdown","201d6e5b":"markdown","6f48d632":"markdown","18193751":"markdown","69c78c01":"markdown"},"source":{"f91cedba":"%%sh\npip install -q albumentations\npip install -q --upgrade wandb","a94b3031":"import gc\nimport os\nimport glob\nimport sys\nimport cv2\nimport imageio\nimport joblib\nimport math\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.stats import kstest\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.image as mpimg\n\nfrom PIL import Image\n\nfrom statsmodels.graphics.gofplots import qqplot\n\nplt.rcParams.update({'font.size': 18})\nplt.style.use('fivethirtyeight')\n\nimport seaborn as sns\nimport matplotlib\n\nfrom termcolor import colored\n\nfrom multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import pearsonr\n\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\nfrom albumentations.pytorch import ToTensorV2\n\n# Core layers\nfrom keras.layers \\\n    import Activation, Dropout, Flatten, Dense, Input, LeakyReLU\n\n# Normalization layers\nfrom keras.layers import BatchNormalization\n\n# Merge layers\nfrom keras.layers import concatenate, multiply\n\n# Embedding Layers\nfrom keras.layers import Embedding\n\n# Keras models\nfrom keras.models import Model, Sequential\n\n# Keras optimizers\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Activate pandas progress apply bar\ntqdm.pandas()","3bff4f75":"# Wandb Login\nimport wandb\nwandb.login()","a7584107":"class config:\n    DIRECTORY_PATH = \"..\/input\/petfinder-pawpularity-score\"\n    TRAIN_FOLDER_PATH = DIRECTORY_PATH + \"\/train\"\n    TRAIN_CSV_PATH = DIRECTORY_PATH + \"\/train.csv\"\n    TEST_CSV_PATH = DIRECTORY_PATH + \"\/test.csv\"\n    \n    SEED = 42","28b53a83":"# wandb config\nWANDB_CONFIG = {\n     'competition': 'PetFinder', \n              '_wandb_kernel': 'neuracort'\n    }","429adad1":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nset_seed()","907562bf":"def return_filpath(name, folder=config.TRAIN_FOLDER_PATH):\n    path = os.path.join(folder, f'{name}.jpg')\n    return path","6c638463":"train = pd.read_csv(config.TRAIN_CSV_PATH)\ntrain['image_path'] = train['Id'].apply(lambda x: return_filpath(x))","d7eb5308":"train.head()","2eef0067":"def plot_augments(title, color_space):\n    \"\"\"\n    Function which plots a 2*6 plot of images of specified color space.\n    \n    params: title(str)  - Title of the Plot\n            color_space - color_space to which we convert the images\n    \"\"\"\n    \n    # Define subplots\n    fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(16,6))\n    plt.suptitle(title, fontsize = 16)\n    \n    # Loop through images\n    for i in range(0, 2*6):\n        image = cv2.imread(train['image_path'][i])\n\n        # The function converts an input image from one color space to another.\n        image = cv2.cvtColor(image, color_space)\n        image = cv2.resize(image, (200,200))\n\n        x = i \/\/ 6\n        y = i % 6\n        \n        axes[x, y].imshow(image, cmap=plt.cm.bone) \n        axes[x, y].axis('off')","c9e1b53e":"plot_augments(\"Original\", cv2.COLOR_BGR2RGB)","4e9f5056":"plot_augments(\"B&W\", cv2.COLOR_RGB2GRAY)","faa17989":"plot_augments(\"Without Gaussian Blur\", cv2.COLOR_RGB2HSV)","fb31359b":"plot_augments(\"With Gaussian Blur\", cv2.COLOR_RGB2HSV)","b0e2776f":"plot_augments(\"Hue, Saturation, Brightness\", cv2.COLOR_RGB2HLS)","0c18615b":"plot_augments(\"LUV Color Space\", cv2.COLOR_RGB2LUV)","07651f73":"plot_augments(\"Aplha Channel\", cv2.COLOR_RGB2RGBA)","8b390e7e":"plot_augments(\"CIE XYZ\", cv2.COLOR_RGB2XYZ)","70bd7362":"plot_augments(\"Luma Chroma\", cv2.COLOR_RGB2YCrCb)","e8c2718b":"plot_augments(\"CIE Lab\", cv2.COLOR_RGB2Lab)","b89b5233":"plot_augments(\"YUV Color Space\", cv2.COLOR_RGB2YUV)","92e1fd50":"# Select a small sample of the image paths\nimage_list = train.sample(12)['image_path']\nimage_list = image_list.reset_index()['image_path']\n\n# Show the sample\nplt.figure(figsize=(16,6))\nplt.suptitle(\"Original View\", fontsize = 16)\n    \nfor k, path in enumerate(image_list):\n    image = mpimg.imread(path)\n        \n    plt.subplot(2, 6, k+1)\n    plt.imshow(image)\n    plt.axis('off') ","b22b4fcd":"class PetDataset(Dataset):\n    def __init__(self, image_list, transforms = None):\n        self.image_list = image_list\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self, i):\n        image = plt.imread(self.image_list[i])\n        image = Image.fromarray(image).convert('RGB')\n        image = np.asarray(image).astype(np.uint8)\n        \n        if self.transforms is not None:\n            image = self.transforms(image)\n            \n        return torch.tensor(image, dtype = torch.float)","c3dd76e4":"# Function to display applied transformations\n\ndef show_transform(image, title):\n    \"\"\"\n    Function to Plot the Transformed Images\n    \"\"\"\n    plt.figure(figsize = (16,6))\n    plt.suptitle(title, fontsize = 16)\n    \n    #Unnormalize\n    image = image \/ 2 + 0.5\n    npimg = image.numpy()\n    npimg = np.clip(npimg, 0., 1.)\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()   ","60dc89e1":"def apply_transform(transform_layer, title):\n    \"\"\"\n    Function to apply Torchvision Transforms to set of Images\n    \n    params: transform_layer - The transform to be applied\n            title (str)     - Title of the Plot\n    \"\"\"\n    transform = transforms.Compose(\n        [\n            transforms.ToPILImage(),\n            transforms.Resize((300 , 300)),\n            transform_layer,\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n    # Dataset\n    train_dataset = PetDataset(image_list = image_list, transforms = transform)\n\n    # DataLoader\n    train_dataloader = DataLoader(dataset = train_dataset, batch_size = 12, shuffle = True)\n\n    # Select Data\n    images = next(iter(train_dataloader))\n\n    # Show Images\n    show_transform(torchvision.utils.make_grid(images, nrow = 6), title = title)","57936bb8":"apply_transform(transforms.CenterCrop((100, 100)), \"Center Crop\")","7f58805a":"apply_transform(transforms.RandomCrop((100, 100)), \"Random Crop\")","0881a72d":"apply_transform(transforms.RandomResizedCrop(size = 60), \"Random Resized Crop\")","9b5e9e20":"apply_transform(transforms.ColorJitter(brightness=0.7, contrast=0.7, saturation=0.7, hue=0.5), \"Color Jitter\")","66059b8d":"apply_transform(transforms.Pad(padding = 5), \"Padding\")","ab2c5f7f":"apply_transform(transforms.RandomAffine(degrees=45), \"Random Affine\")","fc88914a":"apply_transform(transforms.RandomHorizontalFlip(p=0.7), \"Random Horizontal Flip\")","50cd1939":"apply_transform(transforms.RandomVerticalFlip(p=0.7), \"Random Vertical Flip\")","34fdfc0d":"apply_transform(transforms.RandomPerspective(p=0.7), \"Random Perspective\")","da8c397b":"apply_transform(transforms.RandomRotation(degrees = 180), \"Random Rotation\")","14183a38":"apply_transform(transforms.RandomInvert(p=0.7), \"Random Invert\")","9bfc05c9":"apply_transform(transforms.RandomPosterize(bits = 4, p=0.7), \"Random Posterize\")","42ba31d6":"apply_transform(transforms.RandomSolarize(threshold = 30, p=0.7), \"Random Solarize\")","94299d29":"apply_transform(transforms.RandomAutocontrast(p=0.7), \"Random AutoContrast\")","fb8f8a28":"apply_transform(transforms.RandomEqualize(p=0.7), \"Random Equalize\")","71185da8":"latent_dim = 100 # dimension of the latent space\nn_samples = 1000 # size of our dataset\nn_classes = 3\nn_features = 2 # we use 2 features since we'd like to visualize them","64ff122d":"from sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=n_samples, centers=n_classes, n_features=n_features, random_state=123)\n\nprint('Size of our dataset:', len(X))\nprint('Number of features:', X.shape[1])\nprint('Classes:', set(y))","d0bd1215":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\nscaled_X = scaler.fit_transform(X)","57568915":"fig, ax = plt.subplots(figsize=(15, 4))\nlegend = []\n\nfor i in range(n_classes):\n    plt.scatter(scaled_X[:, 0][np.where(y==i)], scaled_X[:, 1][np.where(y==i)], )\n    legend.append('Class %d' % i)\n\nax.set_xlabel('Feature 1')\nax.set_ylabel('Feature 2')\nax.legend(legend)","4cfb7a79":"def build_discriminator(optimizer=Adam(0.0002, 0.5)):\n    '''\n    Defines and compiles discriminator model.\n    This architecture has been inspired by:\n    https:\/\/github.com\/eriklindernoren\/Keras-GAN\/blob\/master\/cgan\/cgan.py\n    and adapted for this problem.\n    \n    Params:\n        optimizer=Adam(0.0002, 0.5) - recommended values\n    '''\n    features = Input(shape=(n_features,))\n    label = Input(shape=(1,), dtype='int32')\n    \n    # Using an Embedding layer is recommended by the papers\n    label_embedding = Flatten()(Embedding(n_classes, n_features)(label))\n    \n    # We condition the discrimination of generated features \n    inputs = multiply([features, label_embedding])\n    \n    x = Dense(512)(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dense(512)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dropout(0.4)(x)\n    x = Dense(512)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = Dropout(0.4)(x)\n    \n    valid = Dense(1, activation='sigmoid')(x)\n    \n    model = Model([features, label], valid)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.summary()\n\n    return model","f980b1d7":"def build_generator():\n    '''\n    Defines the generator model.\n    This architecture has been inspired by:\n    https:\/\/github.com\/eriklindernoren\/Keras-GAN\/blob\/master\/cgan\/cgan.py\n    and adapted for this problem.\n    '''\n    \n    noise = Input(shape=(latent_dim,))\n    label = Input(shape=(1,), dtype='int32')\n    \n    # Using an Embedding layer is recommended by the papers\n    label_embedding = Flatten()(Embedding(n_classes, latent_dim)(label))\n    \n    # We condition the generation of features\n    inputs = multiply([noise, label_embedding])\n    \n    x = Dense(256)(inputs)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization(momentum=0.8)(x)\n    x = Dense(512)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization(momentum=0.8)(x)\n    x = Dense(1024)(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization(momentum=0.8)(x)\n    \n    features = Dense(n_features, activation='tanh')(x)\n    \n    model = Model([noise, label], features)\n    model.summary()\n\n    return model","f00f3e38":"def build_gan(generator, discriminator, optimizer=Adam(0.0002, 0.5)):\n    '''\n    Defines and compiles GAN model. It bassically chains Generator\n    and Discriminator in an assembly-line sort of way where the input is\n    the Generator's input. The Generator's output is the input of the Discriminator,\n    which outputs the output of the whole GAN.\n    \n    Params:\n        optimizer=Adam(0.0002, 0.5) - recommended values\n    '''\n    \n    noise = Input(shape=(latent_dim,))\n    label = Input(shape=(1,))\n    \n    features = generator([noise, label])\n    valid = discriminator([features, label])\n    \n    # We freeze the discriminator's layers since we're only \n    # interested in the generator and its learning\n    discriminator.trainable = False\n    \n    model = Model([noise, label], valid)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.summary()\n    \n    return model","6d2f458d":"discriminator = build_discriminator()","79d2dc16":"generator = build_generator()","b91b2bfa":"gan = build_gan(generator, discriminator)","ce71393b":"def get_random_batch(X, y, batch_size):\n    '''\n    Will return random batches of size batch_size\n    \n    Params:\n        X: numpy array - features\n        y: numpy array - classes\n        batch_size: Int\n    '''\n    idx = np.random.randint(0, len(X))\n    \n    X_batch = X[idx:idx+batch_size]\n    y_batch = y[idx:idx+batch_size]\n    \n    return X_batch, y_batch","357556d0":"def train_gan(gan, generator, discriminator, \n              X, y, \n              n_epochs=100, batch_size=32, \n              hist_every=10, log_every=1):\n    '''\n    Trains discriminator and generator (last one through the GAN) \n    separately in batches of size batch_size. The training goes as follow:\n        1. Discriminator is trained with real features from our training data\n        2. Discriminator is trained with fake features generated by the Generator\n        3. GAN is trained, which will only change the Generator's weights.\n        \n    Params:\n        gan: GAN model\n        generator: Generator model\n        discriminator: Discriminator model\n        X: numpy array - features\n        y: numpy array - classes\n        n_epochs: Int\n        batch_size: Int\n        hist_every: Int - will save the training loss and accuracy every hist_every epochs\n        log_every: Int - will output the loss and accuracy every log_every epochs\n    \n    Returns:\n        loss_real_hist: List of Floats\n        acc_real_hist: List of Floats\n        loss_fake_hist: List of Floats\n        acc_fake_hist: List of Floats\n        loss_gan_hist: List of Floats\n        acc_gan_hist: List of Floats\n    '''\n    \n    half_batch = int(batch_size \/ 2)\n    \n    acc_real_hist = []\n    acc_fake_hist = []\n    acc_gan_hist = []\n    loss_real_hist = []\n    loss_fake_hist = []\n    loss_gan_hist = []\n    \n    # Initialize W&B\n    run = wandb.init(project='PetFinder-GANs', \n                 config= WANDB_CONFIG)\n    \n    for epoch in range(n_epochs):\n        \n        X_batch, labels = get_random_batch(X, y, batch_size)\n        \n        # train with real values\n        y_real = np.ones((X_batch.shape[0], 1))\n        loss_real, acc_real = discriminator.train_on_batch([X_batch, labels], y_real)\n        \n        # train with fake values\n        noise = np.random.uniform(0, 1, (labels.shape[0], latent_dim))\n        X_fake = generator.predict([noise, labels])\n        y_fake = np.zeros((X_fake.shape[0], 1))\n        loss_fake, acc_fake = discriminator.train_on_batch([X_fake, labels], y_fake)\n        \n        y_gan = np.ones((labels.shape[0], 1))\n        loss_gan, acc_gan = gan.train_on_batch([noise, labels], y_gan)\n        \n        if (epoch+1) % hist_every == 0:\n            acc_real_hist.append(acc_real)\n            acc_fake_hist.append(acc_fake)\n            acc_gan_hist.append(acc_gan)\n            loss_real_hist.append(loss_real)\n            loss_fake_hist.append(loss_fake)\n            loss_gan_hist.append(loss_gan)\n            \n            wandb.log({'acc_real': acc_real,\n                      'acc_fake': acc_fake,\n                      'acc_gan': acc_gan,\n                      'loss_real': loss_real,\n                      'loss_fake': loss_fake,\n                      'loss_gan': loss_gan,\n                     })\n\n        if (epoch+1) % log_every == 0:\n            lr = 'loss real: {:.3f}'.format(loss_real)\n            ar = 'acc real: {:.3f}'.format(acc_real)\n            lf = 'loss fake: {:.3f}'.format(loss_fake)\n            af = 'acc fake: {:.3f}'.format(acc_fake)\n            lg = 'loss gan: {:.3f}'.format(loss_gan)\n            ag = 'acc gan: {:.3f}'.format(acc_gan)\n\n            print('{}, {} | {}, {} | {}, {}'.format(lr, ar, lf, af, lg, ag))\n    \n    # Close W&B run\n    wandb.finish()\n        \n    return loss_real_hist, acc_real_hist, loss_fake_hist, acc_fake_hist, loss_gan_hist, acc_gan_hist","fed5fbdf":"loss_real_hist, acc_real_hist, loss_fake_hist, acc_fake_hist, loss_gan_hist, acc_gan_hist = train_gan(gan, generator, discriminator, scaled_X, y)","25340f14":"def generate_samples(class_for, n_samples=20):\n    '''\n    Generates new random but very realistic features using\n    a trained generator model\n    \n    Params:\n        class_for: Int - features for this class\n        n_samples: Int - how many samples to generate\n    '''\n    \n    noise = np.random.uniform(0, 1, (n_samples, latent_dim))\n    label = np.full((n_samples,), fill_value=class_for)\n    return generator.predict([noise, label])","346f63ea":"features_class_0 = generate_samples(0)","1513e3d7":"def visualize_fake_features(fake_features, figsize=(15, 6), color='r'):\n    ax, fig = plt.subplots(figsize=figsize)\n    \n    # Let's plot our dataset to compare\n    for i in range(n_classes):\n        plt.scatter(scaled_X[:, 0][np.where(y==i)], scaled_X[:, 1][np.where(y==i)])\n\n    plt.scatter(fake_features[:, 0], fake_features[:, 1], c=color)\n    plt.title('Real and fake features')\n    plt.legend(['Class 0', 'Class 1', 'Class 2', 'Fake'])","2e562249":"visualize_fake_features(features_class_0)","50e4a463":"features_class_1 = generate_samples(1)\nvisualize_fake_features(features_class_1)","ff7e318b":"--- \n\n## **<span style=\"color:orange;\">Let's have a Talk!<\/span>**\n> ### Reach out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/ishandutta0098)\n\n---","f0f704dc":"---","8984b407":"In this section we will see what are the different **color spaces** in which we can transform the images. Based on your target you can select suitable color spaces and train model on that dataset.\n  \n**I will demonstrate 9 different variations of color spaces namely:**\n1. [Black and White](#1.1)   \n2. [Ben Graham: Greyscale + Gaussian Blur](#1.2)\n3. [Hue, Saturation, Brightness](#1.3) \n4. [LUV Color Space](#1.4) \n5. [Alpha Channel](#1.5)\n6. [XYZ Color Space](#1.6)\n7. [Luma Chroma](#1.7)\n8. [CIE Lab](#1.8)\n9. [YUV Color Space](#1.9)","eb37d4ab":"<a id=\"what-is-data-augmentation\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>What is Data Augmentation?<\/center><\/h2>","f2fd2aea":"We are installing all dependencies at one place.","f23987a3":"---","1f83efdd":"<a id=\"load-datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","be0844c6":"<a id=\"intermediate-image-augmentations\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Intermediate Image Augmentations<\/center><\/h2>","b1aba867":"**Weights & Biases** is the machine learning platform for developers to build better models faster.\n\nYou can use W&B's lightweight, interoperable tools to\n\n- quickly track experiments,\n- version and iterate on datasets,\n- evaluate model performance,\n- reproduce models,\n- visualize results and spot regressions,\n- and share findings with colleagues.\n  \nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.\n\nIn this notebook I will use Weights and Biases's amazing features to perform wonderful visualizations and logging seamlessly.","747ee30f":"Now we have the dataframe ready and we can move to the augmentations part.","90809c57":"<a id=\"1.3\"><\/a>\n## **<span style=\"color:orange;\">3. Hue, Saturation, Brightness <\/span>**\n\n- **Hue:**   \nHue, in the context of color and graphics, refers to the attribute of a visible light due to which it is differentiated from or similar to the primary colors: red, green and blue. The term is also used to refer to colors that have no added tint or shade.\n  \n- **Saturation:**  \nColor saturation refers to the intensity of color in an image. As the saturation increases, the colors appear to be more pure. As the saturation decreases, the colors appear to be more washed-out or pale.\n  \n- **Brightness:**  \nBrightness is the relative lightness or darkness of a particular color, from black (no brightness) to white (full brightness). Brightness is also called Lightness in some contexts\n  \n>`cv2.COLOR_RGB2HLS` converts `RGB\/BGR` to `HLS` (hue lightness saturation) with `H` range `0..180` if `8 bit` image","2c902a42":"<a id=\"2.11\"><\/a>\n## **<span style=\"color:orange;\">11. Random Invert<\/span>**  \n`transforms.RandomInvert()` Inverts the colors of the given image randomly with a given probability. \n  \n- If img is a Tensor, it is expected to be in `[\u2026, 1 or 3, H, W]` format, where `\u2026` means it can have an arbitrary number of leading dimensions. \n- If img is PIL Image, it is expected to be in mode \u201cL\u201d or \u201cRGB\u201d.","dadabf5c":"---","1352022a":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","72edacaa":"<a id=\"1.7\"><\/a>\n## **<span style=\"color:orange;\">7. Luma Chroma<\/span>**\n**Chroma** is the color and the saturation of that color. **Luma** is the brightness of the pixel.\n  \n>`cv2.COLOR_RGB2YCrCb` convert RGB\/BGR to luma-chroma (aka YCC)","41b7c70b":"We define a function `return_filepath` which returns the file_path of an image which we require.","7a6072ce":"We start by creating random clusters of points, n_classes, with features, n_features. We make use of make_blobs from scikit learn that generates gaussian blobs","3efe5a9c":"> ### **<span style=\"color:orange;\">Training Data<\/span>**\n> - train\/ - Folder containing training set photos of the form {id}.jpg, where {id} is a unique Pet Profile ID.\n> - train.csv - Metadata (described below) for each photo in the training set as well as the target, the photo's Pawpularity score. The Id column gives the photo's unique Pet Profile ID corresponding the photo's file name.\n> \n> ### **<span style=\"color:orange;\">Example Test Data<\/span>**\n> In addition to the training data, we include some randomly generated example test data to help you author submission code. When your submitted notebook is scored, this example data will be replaced by the actual test data (including the sample submission).\n> \n> - test\/ - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\n> - test.csv - Randomly generated metadata similar to the training set metadata.\n> - sample_submission.csv - A sample submission file in the correct format.\n>\n>---\n\n> ### **<span style=\"color:orange;\">Photo Metadata<\/span>**\n> The train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n> \n> - Focus - Pet stands out against uncluttered background, not too close \/ far.\n> - Eyes - Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear.\n> - Face - Decently clear face, facing front or near-front.\n> Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n> - Action - Pet in the middle of an action (e.g., jumping).\n> - Accessory - Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash.\n> - Group - More than 1 pet in the photo.\n> - Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n> - Human - Human in the photo.\n> - Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n> - Info - Custom-added text or labels (i.e. pet name, description).\n> - Blur - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n>\n>---","315c3ed3":"> | S.No       |                   Heading                |\n> | :------------- | :-------------------:                |         \n> |  01 |  [**Competition Overview**](#competition-overview)  |                   \n> |  02 |  [**Libraries**](#libraries)                        |  \n> |  03 |  [**Global Config**](#global-config)                |\n> |  04 |  [**Weights and Biases**](#weights-and-biases)      |\n> |  05 |  [**Load Datasets**](#load-datasets)                |\n> |  06 |  [**What is Data Augmentation?**](#what-is-data-augmentation)  |\n> |  07 |  [**Basic Image Augmentations**](#basic-image-augmentations)   |\n> |  08 |  [**Intermediate Image Augmentations**](#intermediate-image-augmentations)   |\n> |  09 |  [**Advanced Data Augmentations**](#intermediate-image-augmentations) |\n> |  10 |  [**References**](#references)                      |","13983d84":"<a id=\"basic-image-augmentations\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Basic Image Augmentations<\/center><\/h2>","6befe3e2":"<a id=\"1.9\"><\/a>\n## **<span style=\"color:orange;\">9. YUV Color Space<\/span>**\n\nYUV is a color encoding system typically used as part of a color image pipeline. It encodes a color image or video taking human perception into account, allowing reduced bandwidth for chrominance components, thereby typically enabling transmission errors or compression artifacts to be more efficiently masked by the human perception than using a \"direct\" RGB-representation. \n  \n>","8e1a479a":"<h1><center>PetFinder: Image Augmentations Master Notebook<\/center><\/h1>\n                                                      \n<center><img src = \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcS1Dxr7XNyny7wDQS3i4nhDy3sZamt8bfKrZg&usqp=CAU\" width = \"750\" height = \"500\"\/><\/center>                                                                          ","1802f656":"I have come across a lot of Kaggle Notebooks using a variety of Image Augmentations some of which might feels a bit advanced and difficult to understand in the first go.  \n  \nThe purpose of this notebook is to have the best and important augmentations all at one place. The notebook will be in continuous development as long as I keep on finding interesting stuff to add to it.","3d01a1a5":"---","a02eb2f8":"<a id=\"2.14\"><\/a>\n## **<span style=\"color:orange;\">14. Random Auto Contrast<\/span>** \n`transforms.RandomAutocontrast()` Autocontrasts the pixels of the given image randomly with a given probability. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, 1 or 3, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions. \n- If img is PIL Image, it is expected to be in mode \u201cL\u201d or \u201cRGB\u201d.","b56c9caa":"---","46665b05":"<a id=\"2.5\"><\/a>\n## **<span style=\"color:orange;\">5. Pad<\/span>**\n`transforms.Pad()` pad the given image on all sides with the given \u201cpad\u201d value. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means at most 2 leading dimensions for mode reflect and symmetric, at most 3 leading dimensions for mode edge, and an arbitrary number of leading dimensions for mode constant","73827e21":"## [Check out the run page here $\\rightarrow$](https:\/\/wandb.ai\/ishandutta\/PetFinder-GANs?workspace=user-ishandutta)","6da448b0":"We shall declare all the required configurations in the `config` class so that it comes in handy throughout the code.","0e29587f":"## **<span style=\"color:orange;\">PyTorch Dataset Class<\/span>**","f13400b6":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","4788f12b":"---","7905e1ef":"<a id=\"references\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References<\/center><\/h2>","4e1315f7":"<a id=\"1.6\"><\/a>\n## **<span style=\"color:orange;\">6. XYZ Color Space<\/span>**","6a1fe1c7":"## **<span style=\"color:orange;\">Original Images<\/span>**\nFirst let us plot the original images so that the augmentations become more clear to us. After this one by one we will implement different augmentations.","5dba843b":"Following we normalize our features to help with the learning","8ccf7420":"<a id=\"2.6\"><\/a>\n## **<span style=\"color:orange;\">6. Random Affine<\/span>**\n`transforms.RandomAffine()` applies Random affine transformation of the image keeping center invariant. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions.","3dbc36ef":"## **<span style=\"color:orange;\">How does Data Augmentation Work?<\/span>**\n\nComputer vision applications use common data augmentation methods for training data. There are classic and advanced techniques in data augmentation for image recognition and natural language processing.  \n  \n![](https:\/\/research.aimultiple.com\/wp-content\/uploads\/2021\/04\/dataaugmentaion_simpleimage_stanford-800x289.png)\n  \nFor data augmentation, making simple alterations on visual data is popular. In addition, generative adversarial networks (GANs) are used to create new synthetic data. \n  \n### **<span style=\"color:orange;\">1. Classical Methods<\/span>**\n**Classic image processing activities for data augmentation are:**\n\n- Padding\n- Random rotating\n- Re-scaling,\n- Vertical and horizontal flipping\n- Translation ( image is moved along X, Y direction)\n- Cropping\n- Zooming\n- Darkening & brightening\/color modification\n- Grayscaling\n- Changing contrast\n- Adding noise\n- Random erasing\n\n  \n    \n![](https:\/\/research.aimultiple.com\/wp-content\/uploads\/2021\/04\/dataaugmention_image_alletranitons.png)\n  \n### **<span style=\"color:orange;\">2. Advanced Methods<\/span>**\n**Advanced models for data augmentation are:**\n  \n- Adversarial training\/Adversarial machine learning:   \nIt generates adversarial examples which disrupt a machine learning model and injects them into dataset to train.\n- [Generative adversarial networks (GANs)](https:\/\/research.aimultiple.com\/synthetic-data\/):   \nGAN algorithms can learn patterns from input datasets and automatically create new examples which resemble into training data.\n- [Neural style transfer](https:\/\/www.tensorflow.org\/tutorials\/generative\/style_transfer):   Neural style transfer models can blend content image and style image and separate style from content.\n- [Reinforcement learning](https:\/\/research.aimultiple.com\/learning-algorithms\/):    \nReinforcement learning models train software agents to reach attain their goals and make decisions in a virtual environment.\n  \nPopular open source python packages for data augmentation in computer vision are Keras ImageDataGenerator, Skimage and OpenCV.\n\n---","6fd330c5":"<a id=\"1.8\"><\/a>\n## **<span style=\"color:orange;\">8. CIE Lab<\/span>**\nThe CIELAB color space also referred to as `L*a*b*`. \n  \nIt expresses color as three values: \n- `L*` for perceptual lightness, and \n- `a*` and `b*` for the four unique colors of human vision: red, green, blue, and yellow. \n  \nCIELAB was intended as a perceptually uniform space, where a given numerical change corresponds to similar perceived change in color. While the LAB space is not truly perceptually uniform, it nevertheless is useful in industry for detecting small differences in color.\n  \n>`cv2.COLOR_RGB2Lab` convert RGB\/BGR to CIE Lab","4db33ec3":"<a id=\"2.9\"><\/a>\n## **<span style=\"color:orange;\">9. Random Perspective<\/span>**\n`transforms.RandomPerspective()` Performs a random perspective transformation of the given image with a given probability. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions.","7465e330":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","8cf8cad6":"<a id=\"2.3\"><\/a>\n## **<span style=\"color:orange;\">3. Random Resized Crop<\/span>**\n`transforms.RandomResizedCrop` Crops a random portion of image and resize it to a given size.\n  \n- If the image is torch Tensor, it is expected to have [\u2026, H, W] shape, where \u2026 means an arbitrary number of leading dimensions\n  \nA crop of the original image is made: the crop has a random area `(H * W)` and a random aspect ratio. This crop is finally resized to the given size. This is popularly used to train the **Inception networks**.","410bd8f7":"<a id=\"1.5\"><\/a>\n## **<span style=\"color:orange;\">5. Alpha Channel<\/span>**\nRGBA color values are an extension of RGB color values with an alpha channel - which specifies the opacity for a color.\n  \nAn RGBA color value is specified with: `rgba(red, green, blue, alpha)`. The `alpha` parameter is a number between `0.0` (fully transparent) and `1.0` (fully opaque).\n  \n>`cv2.COLOR_RGB2RGBA` adds alpha channel to RGB or BGR image","2a517aa3":"---","58c4b5d6":"---","3c776810":"## **<span style=\"color:orange;\">Definition<\/span>**\n\nData Augmentation is a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n\n## **<span style=\"color:orange;\">Why is Data Augmentation Important?<\/span>**\nData augmentation is useful to improve performance and outcomes of machine learning models by forming new and different examples to train datasets. If dataset in a machine learning model is rich and sufficient, the model performs better and more accurate.\n  \nFor machine learning models, collecting and labeling of data can be exhausting and costly processes. Transformations in datasets by using data augmentation techniques allow companies to reduce these operational costs.\n\n---","f909eacf":"---","0d0d55d5":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","5af97696":"---","7bd72b9c":"**In this section I will demonstrate 15 Torchvision Transforms with demos namely:**\n1. [Center Crop](#2.1)\n2. [Random Crop](#2.2)\n3. [Random Resized Crop](#2.3)\n4. [Color Jitter](#2.4)\n5. [Pad](#2.5)\n6. [Random Affine](#2.6)\n7. [Random Horizontal Flip](#2.7)\n8. [Random Vertical Flip](#2.8)\n9. [Random Perspective](#2.9)\n10. [Random Rotation](#2.10)\n11. [Random Invert](#2.11)\n12. [Random Posterize](#2.12)\n13. [Random Solarize](#2.13)\n14. [Random Autocontrast](#2.14)\n15. [Random Equalize](#2.15)","6bf16de8":"## **<span style=\"color:orange;\">Torchvision Transforms<\/span>**\n\nTransforms are common image transformations. They can be chained together using [`Compose`](https:\/\/pytorch.org\/vision\/stable\/transforms.html#torchvision.transforms.Compose). \n  \nMost transform classes have a function equivalent: [functional transforms](https:\/\/pytorch.org\/vision\/stable\/transforms.html#functional-transforms) give fine-grained control over the transformations. This is useful if you have to build a more complex transformation pipeline (e.g. in the case of segmentation tasks).\n  \nMost transformations accept both [PIL](https:\/\/pillow.readthedocs.io\/) images and tensor images, although some transformations are [PIL-only](https:\/\/pytorch.org\/vision\/stable\/transforms.html#transforms-pil-only) and some are [tensor-only](https:\/\/pytorch.org\/vision\/stable\/transforms.html#transforms-tensor-only). The [Conversion Transforms](https:\/\/pytorch.org\/vision\/stable\/transforms.html#conversion-transforms) may be used to convert to and from PIL images.\n  \nThe transformations that accept tensor images also accept batches of tensor images. A Tensor Image is a tensor with `(C, H, W)` shape, where `C` is a number of channels, `H` and `W` are image height and width. A batch of Tensor Images is a tensor of `(B, C, H, W)` shape, where B is a number of images in the batch.\n  \nThe expected range of the values of a tensor image is implicitly defined by the tensor dtype. Tensor images with a float dtype are expected to have values in `[0, 1)`. Tensor images with an integer dtype are expected to have values in `[0, MAX_DTYPE]` where `MAX_DTYPE` is the largest value that can be represented in that dtype.\n\nRandomized transformations will apply the same transformation to all the images of a given batch, but they will produce different transformations across calls. For reproducible transformations across calls, you may use functional transforms.\n\n---","3db51de4":">- [What is Data Augmentation? Techniques, Benefit and Examples](https:\/\/research.aimultiple.com\/data-augmentation\/)\n>- [\ud83e\uddecSIIM Melanoma Competition: EDA + Augmentations](https:\/\/www.kaggle.com\/andradaolteanu\/siim-melanoma-competition-eda-augmentations)\n>- [Data Augmentation using Conditional GAN (cGAN)](https:\/\/medium.com\/@jscriptcoder\/data-augmentation-using-conditional-gan-cgan-d5e8d33ad032)\n>\n>---","9f62e705":"---","e03cbbb0":"<a id=\"1.4\"><\/a>\n## **<span style=\"color:orange;\">4. LUV Color Space<\/span>**\n\nLUV decouple the \"color\" (chromaticity, the UV part) and \"lightness\" (luminance, the L part) of color. \n\n>Hue, Saturation, Brightness \n`cv2.COLOR_RGB2LUV` is used to convert RGB\/BGR to CIE Luv.","2fc96d8b":"We shall define a simple function which will plot the augmented images for us. ","86f4a245":"<center><img src = \"https:\/\/i.imgur.com\/1sm6x8P.png\" width = \"750\" height = \"500\"\/><\/center>        ","75ba2daf":"We set the seed to a standard value. ","f19191f8":"---","a5d96538":"---","8a2eb2de":"---","6e400f84":"<a id=\"2.1\"><\/a>\n## **<span style=\"color:orange;\">1. Center Crop<\/span>**\n`transforms.CenterCrop` crops the given image at the center. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions. \n- If image size is smaller than output size along any edge, image is padded with 0 and then center cropped.","bfd29c27":"---","383eff19":"The CIE XYZ color space encompasses all color sensations that are visible to a person with average eyesight. That is why CIE XYZ (Tristimulus values) is a device-invariant representation of color.[5] It serves as a standard reference against which many other color spaces are defined.\n  \n>`cv2.COLOR_RGB2XYZ` convert RGB\/BGR to CIE XYZ","adfb7959":"<a id=\"advanced-data-augmentations\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Advanced Data Augmentations<\/center><\/h2>","d43df181":"---","60c8b384":"Let us have a brief walkthrough about the competition first.","871c2690":"<a id=\"2.12\"><\/a>\n## **<span style=\"color:orange;\">12. Random Posterize<\/span>**  \n`transforms.RandomPosterize()` Posterize the image randomly with a given probability by reducing the number of bits for each color channel. \n  \n- If the image is torch Tensor, it should be of type `torch.uint8`, and it is expected to have `[\u2026, 1 or 3, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions. - If img is PIL Image, it is expected to be in mode \u201cL\u201d or \u201cRGB\u201d.","f1091395":"---","27aae4e4":"<a id=\"2.13\"><\/a>\n## **<span style=\"color:orange;\">13. Random Solarize<\/span>** \n`transforms.RandomSolarize()` Solarizes the image randomly with a given probability by inverting all pixel values above a threshold. \n  \n- If img is a Tensor, it is expected to be in `[\u2026, 1 or 3, H, W]` format, where `\u2026` means it can have an arbitrary number of leading dimensions. \n- If img is PIL Image, it is expected to be in mode \u201cL\u201d or \u201cRGB\u201d.","1b53bf4e":"---","d8b470b5":"<a id=\"2.8\"><\/a>\n## **<span style=\"color:orange;\">8. Random Vertical Flip<\/span>**\n`transforms.RandomVerticalFlip()` Vertically flip the given image randomly with a given probability. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions.","47cc85d3":"<a id=\"2.2\"><\/a>\n## **<span style=\"color:orange;\">2. Random Crop<\/span>**\n`transforms.RandomCrop` Crop the given image at a random location. \n  \n- If the image is torch Tensor, it is expected to have [\u2026, H, W] shape, where \u2026 means an arbitrary number of leading dimensions, but if non-constant padding is used, the input is expected to have at most 2 leading dimensions","04cb79f9":"The idea is to generate new and realistic fetures based on labels. GANs are excellent at generating realistic data. We can condition this generation by using [Conditional Generative Adversarial Networks](https:\/\/arxiv.org\/abs\/1411.1784)","6ae34ee9":"<a id=\"global-config\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Global Config<\/center><\/h2>","cb9480f9":"<a id=\"2.4\"><\/a>\n## **<span style=\"color:orange;\">4. Color Jitter<\/span>**\n`transforms.ColorJitter()` randomly change the brightness, contrast, saturation and hue of an image. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, 1 or 3, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions. \n- If img is PIL Image, mode `\u201c1\u201d, \u201cI\u201d, \u201cF\u201d` and modes with transparency (alpha channel) are not supported.","db645c08":"<a id=\"2.10\"><\/a>\n## **<span style=\"color:orange;\">10. Random Rotation<\/span>**  \n`transforms.RandomRotation()` Rotates the image by angle. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions.","0abd2a1b":"---","eb7a2fc5":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","46b2c707":"<a id=\"2.7\"><\/a>\n## **<span style=\"color:orange;\">7. Random Horizontal Flip<\/span>**\n`transforms.RandomHorizontalFlip` Horizontally flips the given image randomly with a given probability. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions","9935d94f":"---","053e3cae":"### **<span style=\"color:orange;\">#1 Without Gaussian Blur<\/span>**","a7c82de4":"### **<span style=\"color:orange;\">#2 With Gaussian Blur<\/span>**","e4b2b847":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:maroon; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>If you find this notebook useful, do give me an upvote, it helps to keep up my motivation. This notebook will be updated frequently so keep checking for furthur developments.<\/center><\/h3>","8be144bc":"<a id=\"1.2\"><\/a>\n## **<span style=\"color:orange;\">2. Ben Graham: Greyscale + Gaussian Blur<\/span>**\n\nAs in any other signals, images also can contain different types of noise, especially because of the source (camera sensor). Image Smoothing techniques help in reducing the noise.  \n  \n**Gaussian filters** have the properties of having no overshoot to a step function input while minimizing the rise and fall time. In terms of image processing, any sharp edges in images are smoothed while minimizing too much blurring.\n  \n> OpenCV provides `cv2.gaussianblur()` function to apply Gaussian Smoothing on the input source image","9264200b":"---","72d67654":"---","3cac764b":"<a id=\"1.1\"><\/a>\n## **<span style=\"color:orange;\">1. Black and White<\/span>**","d4f5f7f8":"<h1><center>More Plots coming soon!<\/center><\/h1>\n                                                      \n<center><img src = \"https:\/\/static.wixstatic.com\/media\/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\/v1\/fill\/w_934,h_379,al_c,q_90\/5f8fae_7581e21a24a1483085024f88b0949a9d~mv2.jpg\" width = \"750\" height = \"500\"\/><\/center> ","201d6e5b":"## **<span style=\"color:orange;\">Description<\/span>**\n\n\nIn this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.\n  \nIf successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their \"furever\" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created..\n  \n---\n\n## **<span style=\"color:orange;\">Evaluation Metric<\/span>**\n\nSubmissions are scored on the root mean squared error.\n\n---","6f48d632":"---","18193751":"<a id=\"2.15\"><\/a>\n## **<span style=\"color:orange;\">15. Random Equalize<\/span>** \n`transforms.RandomEqualize()` Equalizes the histogram of the given image randomly with a given probability. \n  \n- If the image is torch Tensor, it is expected to have `[\u2026, 1 or 3, H, W]` shape, where `\u2026` means an arbitrary number of leading dimensions. \n- If img is PIL Image, it is expected to be in mode \u201cP\u201d, \u201cL\u201d or \u201cRGB\u201d.","69c78c01":"---"}}