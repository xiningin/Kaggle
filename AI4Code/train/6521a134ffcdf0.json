{"cell_type":{"080062bb":"code","0b837810":"code","bef356e2":"code","0b40ffaf":"code","34784dcc":"code","8d5f4ea2":"code","cdd76fd4":"code","fce250e8":"code","af65cbf2":"code","6bdcf3e4":"code","7fdbcd8d":"code","fd417e3f":"code","07d5436b":"code","4a997d87":"code","87f8b66a":"code","3d24d18c":"code","357465ba":"code","4c34d559":"code","9b38a888":"code","e3c588ba":"code","57e3511c":"code","56fa8100":"code","6439450b":"code","2fc42a56":"code","9638ed7e":"code","5ef42c27":"code","a2465caa":"code","202ca8c9":"code","7be91ce9":"markdown","328ec4fb":"markdown","2f8eeec5":"markdown","56047c80":"markdown","b218d88e":"markdown","d5973b58":"markdown","6e92b9aa":"markdown","a105baa6":"markdown","3244b132":"markdown","23e9513e":"markdown","342a06eb":"markdown"},"source":{"080062bb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom time import time\nimport string\n#import itertools\nfrom pprint import pprint\nfrom nltk import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report","0b837810":"df = pd.read_csv(\"..\/input\/textdb3\/fake_or_real_news.csv\")\nprint(df.shape)","bef356e2":"df.head()","0b40ffaf":"df.describe()","34784dcc":"df.info ()","8d5f4ea2":"df.isnull().any()","cdd76fd4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport math as math\nfrom pylab import rcParams\n\n%matplotlib inline","fce250e8":"plt.figure (figsize=(6,6))\np = sns.countplot(data=df,x = 'label',)","af65cbf2":"df.loc[df['label']== 0, 'label'] = 'REAL'\ndf.loc[df['label']== 1, 'label'] = 'FAKE'\ndf.columns\ndf['label'].value_counts()","6bdcf3e4":"# Draw a graph of text length verse frequency\n\nimport matplotlib\n%matplotlib inline\ndf['text'].str.len().plot(kind = 'hist', bins = 1000, figsize = (12,5))","7fdbcd8d":"y = df.label \ndf.drop(\"label\", axis=1) \nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=42)","fd417e3f":"print(X_train.shape)\nprint(type(X_train))\nprint(X_train.head())\nprint(X_test.shape)\nprint(type(X_test))\nprint(X_test.head())","07d5436b":"tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7) \ntfidf_train = tfidf_vectorizer.fit_transform(X_train) \ntfidf_test = tfidf_vectorizer.transform(X_test)","4a997d87":"print(tfidf_train.shape)\nprint(tfidf_test.shape)","87f8b66a":"print(tfidf_vectorizer.get_feature_names()[-10:])","3d24d18c":"tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())","357465ba":"count_vectorizer = CountVectorizer(stop_words='english')\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)","4c34d559":"count_vectorizer.get_feature_names()[:10]","9b38a888":"tfidf_df.head()","e3c588ba":"import sklearn.metrics as metrics","57e3511c":"def classify_and_fit(clf, X_train, y_train, X_test, y_test, class_labels = ['FAKE', 'REAL']):\n    print(\"Classifier : \", clf )\n    clf.fit(X_train, y_train)\n    pred = clf.predict(X_test)\n    score = metrics.accuracy_score(y_test, pred)\n    print(\"Accuracy:   %0.3f\" % score)\n    print(\"\\nConfusion Matrix :\")\n    #print(pd.crosstab(y_test, pred, rownames=['True'], colnames=['Predicted'], margins=True))\n    cm = metrics.confusion_matrix(y_test, pred, labels=class_labels)\n    print(cm)\n    print(\"\\nReport :\")    \n    print(classification_report(y_test, pred, target_names=class_labels))\n    return clf","56fa8100":"clf = MultinomialNB() \nclassify_and_fit(clf, tfidf_train, y_train, tfidf_test, y_test)","6439450b":"linear_clf = PassiveAggressiveClassifier()\nclassify_and_fit(linear_clf, tfidf_train, y_train, tfidf_test, y_test)","2fc42a56":"log_reg = LogisticRegression()\nclassify_and_fit(log_reg, tfidf_train, y_train, tfidf_test, y_test)","9638ed7e":"from sklearn.ensemble import RandomForestClassifier\nran_class= RandomForestClassifier()\nclassify_and_fit(ran_class, tfidf_train, y_train, tfidf_test, y_test)","5ef42c27":"def most_informative_feature_for_binary_classification(vectorizer, classifier, n=100):\n    class_labels = classifier.classes_\n    feature_names = vectorizer.get_feature_names()\n    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n    for coef, feat in topn_class1:\n        print(class_labels[0], coef, feat)\n    print()\n    for coef, feat in reversed(topn_class2):\n        print(class_labels[1], coef, feat)\n\n\nmost_informative_feature_for_binary_classification(tfidf_vectorizer, linear_clf, n=30)","a2465caa":"from wordcloud import WordCloud \n","202ca8c9":"# Start with one review:\ntext = df.text[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","7be91ce9":"Let\u2019s initialize a TfidfVectorizer with stop words from the English language and a maximum document frequency of 0.7 (terms with a higher document frequency will be discarded). Stop words are the most common words in a language that are to be filtered out before processing the natural language data. And a TfidfVectorizer turns a collection of raw documents into a matrix of TF-IDF features.","328ec4fb":"Importing libraries ","2f8eeec5":"# Comparing Models","56047c80":"# Extracting the training data","b218d88e":"# Random Forest classifier ","d5973b58":"# Data Exploration","6e92b9aa":"# Fake news detection ","a105baa6":"# Logistic regression ","3244b132":"# Count versus TF-IDF Features\n","23e9513e":"**Wordcloud**","342a06eb":"# Testing Linear Models"}}