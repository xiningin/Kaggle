{"cell_type":{"7d0b9f8b":"code","620179e1":"code","65f2fdf6":"code","c6526259":"code","f8eba269":"code","9fdff3c0":"code","a86144d5":"code","a84b7b52":"code","44095ebc":"code","35050641":"code","cd850ab2":"code","70110301":"code","02155cb3":"code","3eea26a0":"code","10dc8727":"code","a11092ae":"code","34d64b7c":"code","ed6520f7":"code","2ed05f8c":"code","6c8ae807":"code","f8dc2270":"code","fefe9474":"code","248c75d7":"code","7e38a14b":"code","05930cd8":"code","566459e7":"code","ce4ace56":"code","eaa542f4":"code","8bf0a48d":"code","d5da3ace":"code","a231e142":"code","935c267d":"code","d21435a8":"code","15762397":"code","06660b6c":"code","6b41fa75":"code","9f9ae47b":"code","bc198172":"code","be4605ee":"code","2f358a0c":"code","3ffb539c":"code","18f7249e":"code","a46cd233":"code","88fef0fc":"code","693f22a0":"code","f40624cb":"code","f787911f":"code","85df2818":"code","91aabd96":"code","60bcd366":"code","b5a535b2":"code","7e43ce8f":"code","0a46f1ec":"code","7d5eb2cf":"code","22526711":"markdown","9aaf2113":"markdown","a51384db":"markdown","c294f568":"markdown","2f461ddc":"markdown","2ead8d1f":"markdown","7a526418":"markdown","c54b7c2f":"markdown","430ec60e":"markdown","8ae77f43":"markdown","781e3296":"markdown","348e54a8":"markdown","ab8f1441":"markdown","17ebf8a9":"markdown","b9b74b9a":"markdown","62cec90c":"markdown","9783c909":"markdown","7ccf05a8":"markdown","b01e1f5b":"markdown","265eb77a":"markdown","93ca561b":"markdown","da8b5cae":"markdown","86838cbb":"markdown","b13c3eb0":"markdown","24daae38":"markdown","41d8ba20":"markdown","7d2b33a3":"markdown","bd377b29":"markdown","53d028a1":"markdown","00227c15":"markdown","f9ff1219":"markdown","22abed3e":"markdown","f0b90d25":"markdown","16b60132":"markdown","66f4b264":"markdown","be65f481":"markdown","2eda193b":"markdown","c71e2476":"markdown","c0f3981d":"markdown","d8eeeca4":"markdown","36279c0d":"markdown","2c62828f":"markdown","b6dc0cca":"markdown","244a9135":"markdown","442d6fb5":"markdown","5ca0185e":"markdown","f6064000":"markdown","110bc83d":"markdown","92990798":"markdown","99fecedb":"markdown","c03407e6":"markdown","c30bc097":"markdown","527417af":"markdown","7fbab3fe":"markdown","d5f5cca6":"markdown","3bcd9830":"markdown","182586cd":"markdown","aec479d1":"markdown","8b0e21f7":"markdown","b734a6c0":"markdown","2d573203":"markdown","cb832988":"markdown","692c8dd3":"markdown","52e6263b":"markdown","472a6546":"markdown","ea5fa4d2":"markdown","9f9de9d2":"markdown","83c57be9":"markdown","24c8e4d3":"markdown","c1e90177":"markdown","d34ef567":"markdown","3ed21202":"markdown","335f22f8":"markdown"},"source":{"7d0b9f8b":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 3)\n\n# Data Visualisation Libraries\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\n!pip install seaborn --upgrade\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Statistics\nfrom scipy.stats import chi2_contingency\nfrom imblearn.over_sampling import SMOTE\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nprint('\u2714\ufe0f Libraries Imported!')","620179e1":"font_size = 20\nplt.rcParams['axes.labelsize'] = font_size\nplt.rcParams['axes.titlesize'] = font_size + 2\nplt.rcParams['xtick.labelsize'] = font_size - 2\nplt.rcParams['ytick.labelsize'] = font_size - 2\nplt.rcParams['legend.fontsize'] = font_size - 2\n\ncolors = ['#00A5E0', '#DD403A']\ncolors_cat = ['#E8907E', '#D5CABD', '#7A6F86', '#C34A36', '#B0A8B9', '#845EC2', '#8f9aaa', '#FFB86F', '#63BAAA', '#9D88B3', '#38c4e3']\ncolors_comp = ['steelblue', 'seagreen', 'black', 'darkorange', 'purple', 'firebrick', 'slategrey']\n\nrandom_state = 42\nscoring_metric = 'recall'\ncomparison_dict, comparison_test_dict = {}, {}\n\nprint('\u2714\ufe0f Default Parameters and Variables Set!')","65f2fdf6":"def plot_continuous(feature):\n    '''Plot a histogram and boxplot for the churned and retained distributions for the specified feature.'''\n    df_func = train_df.copy()\n    df_func['Exited'] = df_func['Exited'].astype('category')\n\n    fig, (ax1, ax2) = plt.subplots(2,\n                                   figsize=(9, 7),\n                                   sharex=True,\n                                   gridspec_kw={'height_ratios': (.7, .3)})\n\n    for df, color, label in zip([df_retained, df_churned], colors, ['Retained', 'Churned']):\n        sns.histplot(data=df,\n                     x=feature,\n                     bins=15,\n                     color=color,\n                     alpha=0.66,\n                     edgecolor='firebrick',\n                     label=label,\n                     kde=False,\n                     ax=ax1)\n    ax1.legend()\n\n    sns.boxplot(x=feature, y='Exited', data=df_func, palette=colors, ax=ax2)\n    ax2.set_ylabel('')\n    ax2.set_yticklabels(['Retained', 'Churned'])\n\n    plt.tight_layout();\n\n\nprint('\u2714\ufe0f Function Defined!')","c6526259":"def plot_categorical(feature):\n    '''For a categorical feature, plot a seaborn.countplot for the total counts of each category next to a barplot for the churn rate.'''\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    sns.countplot(x=feature,\n                  hue='Exited',\n                  data=train_df,\n                  palette=colors,\n                  ax=ax1)\n    ax1.set_ylabel('Count')\n    ax1.legend(labels=['Retained', 'Churned'])\n\n    sns.barplot(x=feature,\n                y='Exited',\n                data=train_df,\n                palette=colors_cat,\n                ax=ax2)\n    ax2.set_ylabel('Churn rate')\n\n    if (feature == 'HasCrCard' or feature == 'IsActiveMember'):\n        ax1.set_xticklabels(['No', 'Yes'])\n        ax2.set_xticklabels(['No', 'Yes'])\n\n    plt.tight_layout();\n\n\nprint('\u2714\ufe0f Function Defined!')","f8eba269":"def plot_conf_mx(cm, ax):\n    '''Plot a confusion matrix in the specified axes object.'''\n    sns.heatmap(data=cm,\n                annot=True,\n                cmap='Blues',\n                annot_kws={'fontsize': 30},\n                ax=ax)\n\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Retained', 'Churned'])\n\n    ax.set_ylabel('True Label')\n    ax.set_yticks([0.25, 1.25])\n    ax.set_yticklabels(['Retained', 'Churned']);\n\n\nprint('\u2714\ufe0f Function Defined!')","9fdff3c0":"def plot_learning_curve(estimator,\n                        X,\n                        y,\n                        ax,\n                        cv=None,\n                        train_sizes=np.linspace(0.1, 1.0, 5)):\n    '''Plot the learning curves for an estimator in the specified axes object.'''\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=-1,\n        train_sizes=train_sizes,\n        scoring='accuracy')\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.fill_between(train_sizes,\n                    train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std,\n                    alpha=0.1,\n                    color='dodgerblue')\n    ax.fill_between(train_sizes,\n                    test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std,\n                    alpha=0.1,\n                    color='darkorange')\n\n    ax.plot(train_sizes,\n            train_scores_mean,\n            color='dodgerblue',\n            marker='o',\n            linestyle='-',\n            label='Training Score')\n    ax.plot(train_sizes,\n            test_scores_mean,\n            color='darkorange',\n            marker='o',\n            linestyle='-',\n            label='Cross-validation Score')\n\n    ax.set_xlabel('Training Examples')\n    ax.set_ylabel('Score')\n    ax.legend(loc='best', fontsize=14);\n\n\nprint('\u2714\ufe0f Function Defined!')","a86144d5":"def clf_performance(classifier, classifier_name, classifier_name_abv):\n    '''Display the overall performance of a classifier with this template.'''\n    print('\\n', classifier_name)\n    print('-------------------------------')\n    print('   Best Score ({}): '.format(scoring_metric) + str(np.round(classifier.best_score_, 3)))\n    print('   Best Parameters: ')\n    for key, value in classifier.best_params_.items():\n        print('      {}: {}'.format(key, value))\n\n    y_pred_pp = cross_val_predict(estimator=classifier.best_estimator_,\n                                  X=X_train,\n                                  y=y_train,\n                                  cv=5,\n                                  method='predict_proba')[:, 1]\n    y_pred = y_pred_pp.round()\n\n    cm = confusion_matrix(y_train, y_pred, normalize='true')\n\n    fpr, tpr, _ = roc_curve(y_train, y_pred_pp)\n    comparison_dict[classifier_name_abv] = [\n        accuracy_score(y_train, y_pred),\n        precision_score(y_train, y_pred),\n        recall_score(y_train, y_pred),\n        roc_auc_score(y_train, y_pred_pp), fpr, tpr\n    ]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    plot_conf_mx(cm, ax1)\n    plot_learning_curve(classifier.best_estimator_, X_train, y_train, ax2)\n\n    plt.tight_layout();\n\n\nprint('\u2714\ufe0f Function Defined!')","a84b7b52":"def plot_feature_imp(classifier, classifier_name, color, ax):\n    '''Plot the importance of features for a classifier as a barplot.'''\n    importances = pd.DataFrame({'Feature': X_train.columns,\n                                'Importance': np.round(classifier.best_estimator_.feature_importances_, 3)})\n\n    importances = importances.sort_values('Importance', ascending=True).set_index('Feature')\n\n    importances.plot.barh(color=color,\n                          edgecolor='firebrick',\n                          legend=False,\n                          ax=ax)\n    ax.set_title(classifier_name)\n    ax.set_xlabel('Importance');\n\n\nprint('\u2714\ufe0f Function Defined!')","44095ebc":"def test_func(classifier, classifier_name, ax):\n    '''Assess the performance on the test set and plot the confusion matrix.'''\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred, normalize='true')\n\n    comparison_test_dict[classifier_name] = [accuracy_score(y_test, y_pred),\n                                             precision_score(y_test, y_pred),\n                                             recall_score(y_test, y_pred)]\n\n    sns.heatmap(cm,\n                annot=True,\n                annot_kws={'fontsize': 24},\n                cmap='Blues',\n                ax=ax)\n\n    ax.set_title(classifier_name)\n\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Retained', 'Churned'])\n\n    ax.set_ylabel('True Label')\n    ax.set_yticks([0.2, 1.4])\n    ax.set_yticklabels(['Retained', 'Churned']);\n\n\nprint('\u2714\ufe0f Function Defined!')","35050641":"df = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')\n\nprint('\u2714\ufe0f Dataset Imported Successfully!\\n')\nprint('It contains {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\ndf.head()","cd850ab2":"df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\ndf.columns","70110301":"df.info()","02155cb3":"df.describe().T","3eea26a0":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=random_state)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\nprint('Train set: {} rows x {} columns'.format(train_df.shape[0],\n                                               train_df.shape[1]))\nprint(' Test set: {} rows x {} columns'.format(test_df.shape[0],\n                                               test_df.shape[1]))","10dc8727":"fig, ax = plt.subplots(figsize=(6, 6))\n\nsns.countplot(x='Exited', data=train_df, palette=colors, ax=ax)\n\nfor index, value in enumerate(train_df['Exited'].value_counts()):\n    label = '{}%'.format(round((value \/ train_df['Exited'].shape[0]) * 100, 2))\n    ax.annotate(label,\n                xy=(index, value + 250),\n                ha='center',\n                va='center',\n                color=colors[index],\n                fontweight='bold',\n                size=font_size + 4)\n\nax.set_xticklabels(['Retained', 'Churned'])\nax.set_xlabel('Status')\nax.set_ylabel('Count')\nax.set_ylim([0, 7000]);","a11092ae":"continuous = ['Age', 'CreditScore', 'Balance', 'EstimatedSalary']\ncategorical = ['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\n\nprint('Continuous: ', ', '.join(continuous))\nprint('Categorical: ', ', '.join(categorical))","34d64b7c":"train_df[continuous].hist(figsize=(12, 10),\n                          bins=20,\n                          layout=(2, 2),\n                          color='steelblue',\n                          edgecolor='firebrick',\n                          linewidth=1.5);","ed6520f7":"fig, ax = plt.subplots(figsize=(7, 6))\n\nsns.heatmap(train_df[continuous].corr(),\n            annot=True,\n            annot_kws={'fontsize': 16},\n            cmap='Blues',\n            ax=ax)\n\nax.tick_params(axis='x', rotation=45)\nax.tick_params(axis='y', rotation=360);","2ed05f8c":"df_churned = train_df[train_df['Exited'] == 1]\ndf_retained = train_df[train_df['Exited'] == 0]\n\nplot_continuous('Age')","6c8ae807":"plot_continuous('CreditScore')","f8dc2270":"plot_continuous('Balance')","fefe9474":"plot_continuous('EstimatedSalary')","248c75d7":"df_cat = train_df[categorical]\n\nfig, ax = plt.subplots(2, 3, figsize=(12, 8))\n\nfor index, column in enumerate(df_cat.columns):\n\n    plt.subplot(2, 3, index + 1)\n    sns.countplot(x=column, data=train_df, palette=colors_cat)\n\n    plt.ylabel('Count')\n    if (column == 'HasCrCard' or column == 'IsActiveMember'):\n        plt.xticks([0, 1], ['No', 'Yes'])\n\nplt.tight_layout();","7e38a14b":"plot_categorical('Geography')","05930cd8":"plot_categorical('Gender')","566459e7":"plot_categorical('Tenure')","ce4ace56":"plot_categorical('NumOfProducts')","eaa542f4":"plot_categorical('HasCrCard')","8bf0a48d":"plot_categorical('IsActiveMember')","d5da3ace":"chi2_array, p_array = [], []\nfor column in categorical:\n\n    crosstab = pd.crosstab(train_df[column], train_df['Exited'])\n    chi2, p, dof, expected = chi2_contingency(crosstab)\n    chi2_array.append(chi2)\n    p_array.append(p)\n\ndf_chi = pd.DataFrame({\n    'Variable': categorical,\n    'Chi-square': chi2_array,\n    'p-value': p_array\n})\ndf_chi.sort_values(by='Chi-square', ascending=False)","a231e142":"features_drop = ['Tenure', 'HasCrCard', 'EstimatedSalary']\ntrain_df = train_df.drop(features_drop, axis=1)\n\nprint('\u2714\ufe0f Features Dropped!')","935c267d":"train_df['Gender'] = LabelEncoder().fit_transform(train_df['Gender'])\n\ntrain_df['Geography'] = train_df['Geography'].map({\n    'Germany': 1,\n    'Spain': 0,\n    'France': 0\n})\n\nprint('\u2714\ufe0f Features Encoded!')","d21435a8":"scaler = StandardScaler()\n\nscl_columns = ['CreditScore', 'Age', 'Balance']\ntrain_df[scl_columns] = scaler.fit_transform(train_df[scl_columns])\n\nprint('\u2714\ufe0f Features Scaled!')","15762397":"y_train = train_df['Exited']\nX_train = train_df.drop('Exited', 1)\n\nprint('\u2714\ufe0f Sets Created!')","06660b6c":"y_train.value_counts()","6b41fa75":"over = SMOTE(sampling_strategy='auto', random_state=random_state)\nX_train, y_train = over.fit_resample(X_train, y_train)\n\ny_train.value_counts()","9f9ae47b":"clf_list = [('Gaussian Naive Bayes', GaussianNB()),\n            ('Logistic Regression', LogisticRegression(random_state=random_state))]\n\ncv_base_mean, cv_std = [], []\nfor clf in clf_list:\n\n    cv = cross_val_score(estimator=clf[1],\n                         X=X_train,\n                         y=y_train,\n                         scoring=scoring_metric,\n                         cv=5,\n                         n_jobs=-1)\n\n    cv_base_mean.append(cv.mean())\n    cv_std.append(cv.std())\n\nprint('Baseline Models (Recall):')\n\nfor i in range(len(clf_list)):\n    print('   {}: {}'.format(clf_list[i][0], np.round(cv_base_mean[i], 2)))","bc198172":"lr = LogisticRegression(random_state=random_state)\n\nparam_grid = {\n    'max_iter': [100],\n    'penalty': ['l1', 'l2'],\n    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n    'solver': ['lbfgs', 'liblinear']\n}\n\nlr_clf = GridSearchCV(estimator=lr,\n                      param_grid=param_grid,\n                      scoring=scoring_metric,\n                      cv=5,\n                      verbose=False,\n                      n_jobs=-1)\n\nbest_lr_clf = lr_clf.fit(X_train, y_train)\nclf_performance(best_lr_clf, 'Logistic Regression', 'LR')","be4605ee":"svc = SVC(probability=True, random_state=random_state)\n\nparam_grid = tuned_parameters = [{'kernel': ['rbf'],\n                                  'gamma': ['scale', 'auto'],\n                                  'C': [.1, 1, 2]},\n                                 {'kernel': ['linear'],\n                                  'C': [.1, 1, 10]}\n                                ]\n\nsvc_clf = GridSearchCV(estimator=svc,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_svc_clf = svc_clf.fit(X_train, y_train)\nclf_performance(best_svc_clf, 'Support Vector Classifier', 'SVC')","2f358a0c":"rf = RandomForestClassifier(random_state=random_state)\nparam_grid = {\n    'n_estimators': [100],\n    'criterion': ['entropy', 'gini'],\n    'bootstrap': [True, False],\n    'max_depth': [6],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2, 3, 5],\n    'min_samples_split': [2, 3, 5]\n}\n\nrf_clf = GridSearchCV(estimator=rf,\n                      param_grid=param_grid,\n                      scoring=scoring_metric,\n                      cv=5,\n                      verbose=False,\n                      n_jobs=-1)\n\nbest_rf_clf = rf_clf.fit(X_train, y_train)\nclf_performance(best_rf_clf, 'Random Forest', 'RF')","3ffb539c":"gbc = GradientBoostingClassifier(random_state=random_state)\nparam_grid = {\n    'n_estimators': [600],\n    'subsample': [0.66, 0.75],\n    'learning_rate': [0.001, 0.01],\n    'max_depth': [3],  # default=3\n    'min_samples_split': [5, 7],\n    'min_samples_leaf': [3, 5],\n    'max_features': ['auto', 'log2', None],\n    'n_iter_no_change': [20],\n    'validation_fraction': [0.2],\n    'tol': [0.01]\n}\n\ngbc_clf = GridSearchCV(estimator=gbc,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_gbc_clf = gbc_clf.fit(X_train, y_train)\nclf_performance(best_gbc_clf, 'Gradient Boosting Classifier', 'GBC')","18f7249e":"best_gbc_clf.best_estimator_.n_estimators_","a46cd233":"xgb = XGBClassifier(random_state=random_state)\n\nparam_grid = {\n    'n_estimators': [50],\n    'learning_rate': [0.001, 0.01],\n    'max_depth': [3, 4],  # default=6\n    'reg_alpha': [1, 2],\n    'reg_lambda': [1, 2],\n    'subsample': [0.5, 0.75],\n    'colsample_bytree': [0.50, 0.75],\n    'gamma': [0.1, 0.5, 1],\n    'min_child_weight': [1]\n}\n\nxgb_clf = GridSearchCV(estimator=xgb,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_xgb_clf = xgb_clf.fit(X_train, y_train)\nclf_performance(best_xgb_clf, 'XGBoost Classifier', 'XGB')","88fef0fc":"lgbmc = LGBMClassifier(random_state=random_state)\n\nparam_grid = {\n    'max_depth': [5],\n    'num_leaves': [5, 10],\n    'learning_rate': [0.001, 0.01],\n    'n_estimators': [200],\n    'feature_fraction': [0.5],\n    'min_child_samples': [5, 10],\n    'reg_alpha': [0.1, 0.5],\n    'reg_lambda': [0.1, 0.5]\n}\n\nlgbmc_clf = GridSearchCV(estimator=lgbmc,\n                         param_grid=param_grid,\n                         scoring=scoring_metric,\n                         cv=5,\n                         verbose=False,\n                         n_jobs=-1)\n\nbest_lgbmc_clf = lgbmc_clf.fit(X_train, y_train)\nclf_performance(best_lgbmc_clf, 'LGBMClassifier', 'LGBMC')","693f22a0":"estimators = [('LR', best_lr_clf.best_estimator_),\n              ('SCV', best_svc_clf.best_estimator_),\n              ('RF', best_rf_clf.best_estimator_),\n              ('GBC', best_gbc_clf.best_estimator_),\n              ('XGB', best_xgb_clf.best_estimator_),\n              ('LGBMC', best_lgbmc_clf.best_estimator_)]\n\ntuned_voting_soft = VotingClassifier(estimators=estimators[1:],\n                                     voting='soft',\n                                     n_jobs=-1)\nestimators.append(('SoftV', tuned_voting_soft))\n\ny_pred_pp = cross_val_predict(tuned_voting_soft,\n                              X_train,\n                              y_train,\n                              cv=5,\n                              method='predict_proba')[:, 1]\ny_pred = y_pred_pp.round()\n\ncm = confusion_matrix(y_train, y_pred, normalize='true')\nfpr, tpr, _ = roc_curve(y_train, y_pred_pp)\ncomparison_dict['SVot'] = [\n    accuracy_score(y_train, y_pred),\n    precision_score(y_train, y_pred),\n    recall_score(y_train, y_pred),\n    roc_auc_score(y_train, y_pred_pp), fpr, tpr\n]\n\nprint('Soft Voting\\n-----------------')\nprint('  Recall: ', np.round(recall_score(y_train, y_pred), 3))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_conf_mx(cm, ax1)\nplot_learning_curve(tuned_voting_soft, X_train, y_train, ax2)","f40624cb":"colors_fi = ['steelblue', 'darkgray', 'cadetblue', 'bisque']\n\nfig = plt.subplots(2, 2, figsize=(12, 10))\n\nfor i, (name, clf) in enumerate(zip(['RF', 'GB', 'XGB', 'LGBM'],\n                                    [best_rf_clf, best_gbc_clf, best_xgb_clf, best_lgbmc_clf])):\n\n    ax = plt.subplot(2, 2, i + 1)\n    plot_feature_imp(clf, name, colors_fi[i], ax)\n    plt.ylabel('')\n\nplt.tight_layout();","f787911f":"comparison_matrix = {}\nfor key, value in comparison_dict.items():\n    comparison_matrix[str(key)] = value[0:4]\n\ncomparison_df = pd.DataFrame(comparison_matrix,\n                             index=['Accuracy', 'Precision', 'Recall', 'AUC']).T\ncomparison_df.style.highlight_max(color='indianred', axis=0)","85df2818":"comparison_df.plot(kind='bar',\n                   figsize=(10, 5),\n                   fontsize=12,\n                   color=['#5081DE', '#A7AABD', '#D85870', '#424656'])\n\nplt.legend(loc='upper center',\n           fontsize=font_size - 6,\n           ncol=len(comparison_df.columns),\n           bbox_to_anchor=(0.5, 1.12))\nplt.xticks(rotation=0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y=0.70, color='red', linestyle='--')\nplt.text(x=-0.5, y=0.73, s='0.70', size=font_size + 2, color='red');","91aabd96":"fig, ax = plt.subplots(figsize=(10, 5))\n\nfor index, key in enumerate(comparison_dict.keys()):\n    auc, fpr, tpr = comparison_dict[key][3], comparison_dict[key][4], comparison_dict[key][5]\n    ax.plot(fpr,\n            tpr,\n            color=colors_comp[index],\n            label='{}: {}'.format(key, np.round(auc, 3)))\n\nax.plot([0, 1], [0, 1], 'k--', label='Baseline')\n\nax.set_title('ROC Curve')\nax.set_xlabel('False Positive Rate')\nax.set_xticks([0, 0.25, 0.5, 0.75, 1])\nax.set_ylabel('False Positive Rate')\nax.set_yticks([0, 0.25, 0.5, 0.75, 1])\nax.autoscale(axis='both', tight=True)\nax.legend(fontsize=14);","60bcd366":"print('Soft Voting:')\n\ny_pred = cross_val_predict(tuned_voting_soft,\n                           X_train,\n                           y_train,\n                           cv=5,\n                           method='predict_proba')\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nskplt.metrics.plot_cumulative_gain(y_train, y_pred, ax=ax)\n\nax.plot([0.5, 0.5], [0, 0.8], color='firebrick')\nax.plot([0.0, 0.5], [0.8, 0.8], color='firebrick')\n\nax.set_title('Cumulative Gains Curve', size=font_size)\nax.set_xlabel('Percentage of Sample', size=font_size)\nax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_xticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=font_size - 2)\n\nax.set_ylabel('Gain', size=font_size)\nax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0], fontsize=font_size - 2)\n\nax.text(0.15, 0.81, '80%', size=font_size, color='firebrick')\nax.legend(fontsize=14);","b5a535b2":"test_df = test_df.drop(features_drop, axis=1)\n\ntest_df['Gender'] = LabelEncoder().fit_transform(test_df['Gender'])\ntest_df['Geography'] = test_df['Geography'].map({\n    'Germany': 1,\n    'Spain': 0,\n    'France': 0\n})\n\ntest_df[scl_columns] = scaler.transform(test_df[scl_columns])  # not fit_transform, scaler has already been trained\n\ny_test = test_df['Exited']\nX_test = test_df.drop('Exited', 1)\n\nprint('\u2714\ufe0f Preprocessing Complete!')","7e43ce8f":"tuned_voting_soft.fit(X_train, y_train)\n\nfig, ax = plt.subplots(7, 1, figsize=(5, 30))\n\nfor i, (name, clf) in enumerate(zip(['LR', 'SVC', 'RF', 'GB', 'XGB', 'LGBM', 'SVot'], \n                                    [best_lr_clf.best_estimator_, best_svc_clf.best_estimator_, best_rf_clf.best_estimator_, best_gbc_clf.best_estimator_, best_xgb_clf.best_estimator_, best_lgbmc_clf.best_estimator_, tuned_voting_soft])):\n    test_func(clf, name, ax=ax[i])\n\nplt.tight_layout();","0a46f1ec":"comparison_test_df = pd.DataFrame(comparison_test_dict,\n                                  index=['Accuracy', 'Precision', 'Recall']).T\ncomparison_test_df.style.highlight_max(color='indianred', axis=0)","7d5eb2cf":"comparison_test_df.plot(kind='bar',\n                        figsize=(10, 5),\n                        fontsize=12,\n                        color=['#5081DE', '#A7AABD', '#D85870'])\n\nplt.legend(loc='upper center',\n           ncol=len(comparison_test_df.columns),\n           bbox_to_anchor=(0.5, 1.11))\nplt.xticks(rotation=0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y=0.70, color='red', linestyle='--')\nplt.text(x=-0.5, y=0.72, s='0.70', size=font_size + 2, color='red');","22526711":"Our DataFrame has 14 features\/attributes and 10K customers\/instances. The last feature, '**Exited**', is the **target variable** and indicates whether the customer has churned (0 = No, 1 = Yes). The meaning of the rest of the features can be easily inferred from their name.\n\nFeatures 'RowNumber', 'CustomerId', and 'Surname' are specific to each customer and can be dropped:","9aaf2113":"Interestingly, there is a clear difference between age groups since older customers are more likely to churn. This observation could potentially indicate that preferences change with age, and the bank hasn't adapted its strategy to meet the requirements of older customers.","a51384db":"Finally, we will split the train set into 'X_train' and 'y_train' sets:","c294f568":"### Active Members","2f461ddc":"<br>\n\nWe will use the same method for comparing our classifiers as we did in the training set.","2ead8d1f":"- 'Age' is slightly tail-heavy, i.e. it extends more further to the right of the median than to the left,\n- Most values for 'CreditScore' are above 600,\n- If we ignore the first bin, 'Balance' follows a fairly normal distribution, and\n- The distribution of 'EstimatedSalary' is more or less uniform and provides little information.\n\n## Looking for Correlations\n\nWe can compute the standard correlation coefficient between every pair of (continuous) features using the pandas' `corr()` method and plot it as a matrix:","7a526418":"<br>\n\n# Objective\n\nThe goal of this notebook is to understand and predict customer churn for a bank. Specifically, we will initially perform **Exploratory Data Analysis** (**EDA**) to identify and visualise the factors contributing to customer churn. This analysis will later help us build **Machine Learning** models to predict whether a customer will churn or not. \n\nThis problem is a typical **classification** task. The task does not specify which performance metric to use for optimising our machine learning models. I decided to use **recall** since correctly classifying elements of the positive class (customers who will churn) is more critical for the bank.\n\n<br>\n\n*Skills: Exploratory Data Analysis, Data Visualisation, Data Preprocessing (Feature Selection, Encoding Categorical Features, Feature Scaling), Addressing Class Imbalance (SMOTE), Model Tuning.*\n\n*Models Used: Logistic Regression, Support Vector Machines, Random Forests, Gradient Boosting, XGBoost, and Light Gradient Boosting Machine.*\n","c54b7c2f":"<br>\n\n# Creating a Test Set\n\nWe will split our dataset into a train and test set using scikit-learn's `train_test_split()` function, which implements random sampling. Our dataset is large enough (especially relative to the number of features), so we do **not** risk introducing *sampling bias*.","430ec60e":"### Support Vector Classifier","8ae77f43":"Both churned and retained customers display a similar uniform distribution in their salaries. Consequently, we can conclude that salary doesn't have a significant effect on the likelihood to churn.","781e3296":"### Random Forest","348e54a8":"<br>\n\n# Results\n\n## Learning Curves\n\nFor all models, there is a tiny gap between the two curves at the end of training. This observation indicates that we do **not** overfit the training set. \n\n## Feature Importance\n\nSome classifiers allow us to visualise feature importance:","ab8f1441":"## Continuous Variables\n\nBy calling the `hist()` method, we can plot a histogram for each of the four continuous numeric features:","17ebf8a9":"Class imbalance is usually a problem and occurs in many real-world tasks. Classification using imbalanced data is biased in favour of the majority class, meaning that machine learning algorithms will likely result in models that do little more than predict the most common class. Additionally, common metrics can be misleading when handling class-imbalanced data (e.g. if a dataset contains 99.9% 0s and 0.01% 1s, a classifier that always predicts 0 will have 99.9% accuracy).\n\nThankfully, some strategies can address this problem. I decided to use the SMOTE ('Synthetic Minority Oversampling Technique') algorithm, which, as we read in [[2](#Bibliography)], <br>\n'*finds a record that is similar to the record being upsampled and creates a synthetic record that is a randomly weighted average of the original record and the neighbouring record, where the weight is generated separately for each predictor*'.\n\nI\u2019ll use the `SMOTE` function from [imblearn](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html) with the `sampling_strategy` set to 'auto'.","b9b74b9a":"Thankfully, there are **no missing values** in our DataFrame. The `describe()` method gives us a statistical summary of the numerical features:","62cec90c":"## Performance Comparison\n\nInitially, we can compare the performance of our classifiers in terms of four individual metrics (Accuracy, precision, recall, and area under the ROC curve or simply AUC):","9783c909":"## `plot_feature_imp()`","7ccf05a8":"It's not a surprise that inactive customers are more likely to churn. A significant portion of the clientele is inactive; therefore, the bank will benefit from changing its policy so that more customers become active.","b01e1f5b":"### Card Holders","265eb77a":"### Balance","93ca561b":"<br>\n\n# Future Development\n\n-  I tried combining existing features to produce more useful ones (Feature Engineering). However, this didn\u2019t increase the predictive performance of my models. Feature engineering can prove quite important when done right, so it is worth exploring in a future version of this notebook.","da8b5cae":"**Note**: We could have used more (powerful) classifiers such as Random Forests or\/and XGBoost. However, I preferred to exclude them at this stage as their default parameters make them more susceptible to overfitting the training set and hence provide inaccurate baseline performance. \n\n## Model Tuning\n\nWe are now ready to start building machine learning models. The six classifiers I have selected are the following:\n\n1) [Logistic Regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html), <br>\n2) [Support Vector Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC), <br>\n3) [Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html), <br> \n4) [Gradient Boosting Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html), <br>\n5) [Xtreme Gradient Boosting Classifier](https:\/\/xgboost.readthedocs.io\/en\/latest\/), and <br>\n6) [Light Gradient Boosting Machine](https:\/\/lightgbm.readthedocs.io\/en\/latest\/).\n\nI won't go into detail about how these algorithms work. You can read more in [[1](#Bibliography)] or the corresponding documentation.\n\nUsing default hyperparameters usually results in non-optimised models that overfit or underfit the dataset. **Hyperparameter tuning** is the process of finding the set of hyperparameter values that achieves optimal performance. For this purpose, we will first define which hyperparameters we want to experiment with and what values to try out. We will pass this information to Scikit-Learn\u2019s `GridSearchCV`, which then evaluates all the possible combinations of hyperparameter values. As mentioned [earlier](#Objective), **recall** will be used as the scoring metric for optimising our models. Note that `GridSearchCV` evaluates performance by performing k-fold cross-validation (therefore, a number of folds, `cv`, needs to be provided).\n\nApart from a confusion matrix, a plot of the **learning curves** will be provided for each classifier. Learning curves are plots of a model's performance on the training set and the validation set as a function of the training set size. They can help us visualise overfitting\/underfitting and the effect of the training size on a model's error.","86838cbb":"<br>\n\n# Evaluating the Test Set\n\nNow is the time to evaluate the models on unseen data. First, we need to perform the same preprocessing steps as the training set.","b13c3eb0":"There is no significant difference between retained and churned customers in terms of their credit scores.","24daae38":"Interestingly, having 3 or 4 products significantly increases the likelihood of churn. I am not sure how to interpret this result. It could potentially mean that the bank cannot properly support customers with more products which in turn increases customer dissatisfaction.","41d8ba20":"### Geography","7d2b33a3":"'Age' and 'NumOfProducts' seem like the most useful features for all classifiers, followed by 'IsActiveMember' and 'Balance'. On the other hand, 'CreditScore' is the least important feature with a small value close to zero for all estimators apart from LGBM. ","bd377b29":"## `test_func()`","53d028a1":"'Tenure' and 'HasCrCard' have a small chi-square and a p-value greater than 0.05 (the standard cut-off value), confirming our initial hypothesis that these two features do not convey any useful information.\n\nWe can use the `drop()` method to remove these three features from the train set:","00227c15":"<br>\n\n# Parameters and Variables\n\nIt is convenient to set some (default) parameters and variables for the whole notebook.  ","f9ff1219":"### Gradient Boosting Classifier","22abed3e":"### Credit Score","f0b90d25":"The most important things to note are:\n\n- The age of customers ranges from 18 to 92, with a mean value approximately equal to 40,\n- The mean (and median) tenure is 5 years, so the majority of customers is loyal (tenure > 3), and\n- Approximately 50% of customers are active.\n\nEDA will help us understand our dataset better. However, before we look at the data any further, we need to create a **test set**, put it aside, and use it only to evaluate our Machine Learning models. This practice protects our models from **data snooping bias** (you can read more on page 51 of [[1](#Bibliography)]) and ensures that evaluation will be performed using unseen data. ","16b60132":"All other classifiers have a recall higher than 70% (baseline performance). XGB is the model with the highest recall (78.5 %). However, the LGBM classifier has the best overall performance with the highest accuracy, precision, and AUC.\n\nUsing single metrics is not the only way of comparing the predictive performance of classification models. The ROC curve (Receiver Operating Characteristic curve) is a graph showing the performance of a classifier at different classification thresholds. It plots the true positive rate (another name for recall) against the false positive rate.","66f4b264":"Having a credit card does not seem to affect the churn rate.","be65f481":"<br>\n\n# Conclusions\n\nOur notebook just came to an end! Our final report to the bank should be based on two main points:\n\n- **EDA** can help us identify which features contribute to customer churn. Additionally, **feature importance** analysis can quantify the importance of each feature in predicting the likelihood of churn. Our results reveal that the most significant feature is **age** (older customers are more likely to churn), followed by the **number of products** (having more products increases a customer\u2019s likelihood to churn). The bank could use our findings to adapt and improve its services in a way that increases satisfaction for those customers more likely to churn.\n\n- We can build several **machine learning models** with **recall** approximately equal to **78%**, meaning that they can successfully detect almost 80% of those customers more luckily to churn. Perhaps, adding more features or\/and records could help us improve predictive performance. Therefore, the bank could benefit from investing in gathering more data.\n\n<br>\n\nPlease feel free to make any suggestions for improving my analysis. Also,  please consider <font size=+0 color=\"red\"><b>upvoting<\/b><\/font> if you found this notebook useful. Thank you! \ud83d\ude09","2eda193b":"## `plot_categorical()`","c71e2476":"There is no significant intercorrelation between our features, so we do **not** have to worry about multicollinearity.\n\nLet's look at these features in greater detail.\n\n### Age","c0f3981d":"The number of years (tenure) does not seem to affect the churn rate.","d8eeeca4":"The `info()` method can give us valuable information such as the number of non-null values and the type of each feature:","36279c0d":"## `plot_conf_mx()`","2c62828f":"<br>\n\n# Building Machine Learning Models\n\n## Baseline Models\n\nWe start this section by first creating two simple models to estimate the **baseline performance** on the training set. Specifically, we will use Gaussian Na\u00efve Bayes and Logistic Regression. We will use their default parameters and evaluate their (mean) recall by performing **k-fold cross-validation**. The idea behind k-fold cross-validation, which is illustrated in [this figure](https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png), is simple: it splits the (training) set into k subsets\/folds, trains the models using k-1 folds, and evaluates the model on the remaining one fold. This process is repeated until every fold is tested once.","b6dc0cca":"### Number of Products","244a9135":"The dashed diagonal line represents a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\n\nIn our case, all classifiers, apart from Logistic Regression, perform similarly. It seems that LGBM performs marginally better, as evidenced by the slightly higher AUC (0.888).","442d6fb5":"## Addressing Class Imbalance\n\nAs we have seen previously, there is an imbalance in the classes to be predicted, with one class (0 \u2013 retained) much more prevalent than the other (1 - churned):","5ca0185e":"### Estimated Salary","f6064000":"## Scaling\n\nFeature scaling is a technique used to normalise the range of features in a dataset. Some algorithms are sensitive to feature scaling (e.g. SVMs), while others are invariant (e.g. Random Forests). \n\nI decided to use `StandardScaler()`, which standardises features by subtracting the mean and dividing by the standard deviation. This transformation results in features with zero mean and unit variance.","110bc83d":"### Tenure","92990798":"<br>\n\n# Libraries","99fecedb":"Female customers are more likely to churn.","c03407e6":"<br>\n\n# Functions\n\nSince we will reuse parts of the code, it will be helpful to define some functions.\n\n## `plot_continuous()`","c30bc097":"### Ensemble Learning\n\nWe can combine the predictions of all these classifiers to determine if we get better predictive performance compared to each individual constituent classifier. This practice is the main motivation behind Ensemble Learning.\n\nSpecifically, I will use **Soft Voting**. In this case, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n","527417af":"The bank kept 80% of its clientele.\n\nNotice that our dataset is **skewed\/imbalanced** since the number of instances in the 'Retained' class outnumbers the number of instances in the 'Churned' class by a lot. Therefore, accuracy is probably not the best metric for model performance.\n\n\nDifferent visualisation techniques apply to different types of variables, so it's helpful to differentiate between continuous and categorical variables and look at them separately.","7fbab3fe":"### Logistic Regression","d5f5cca6":"The performance on the test set for all models is fairly similar to the training set, which proves that we do **not** overfit the training set. Therefore, we can predict customer churn with a recall approximately equal to **78%**.","3bcd9830":"This chart shows that if we target 50% of the customers most likely to churn (according to the model), the model will pick 80% of customers who will actually churn, while the random pick would pick only 50% of the targets.","182586cd":"The number of estimators after early stopping is: ","aec479d1":"Important points:\n\n- The bank has customers in three countries (France, Spain, and Germany). Most customers are in France.\n- There are more male customers than females,\n- Only a small percentage leaves within the first year. The count of customers in tenure years between 1 and 9 is almost the same,\n- Most of the customers have purchased 1 or 2 products, while a small portion has purchased 3 and 4,\n- A significant majority of customers has a credit card, and\n- Almost 50% of customers are not active.\n\nAgain, we will look at these features in greater detail.","8b0e21f7":"<br>\n\n# Exploratory Data Analysis\n\n## Target Variable: Exited\n\nAs we mentioned earlier, the target variable is already encoded and can take two possible values:\n\n- Zero (0) for a customer that has **not** churned, and\n- One (1) for a customer that has churned.","b734a6c0":"### Gender","2d573203":"<br>\n\n# Data Preprocessing\n\nData preprocessing is the process of converting raw data into a well-readable format that is suitable for building and training Machine Learning models.\n\nLet's complete this process step-by-step.\n\n## Feature Selection\n\nWe have already performed feature selection by dropping columns 'RowNumber', 'CustomerId', and 'Surname' at the beginning of our notebook. EDA revealed several more features that can be dropped as they do not provide any value in predicting our target variable:\n\n- 'EstimatedSalary' displays a uniform distribution for both types of customers and can be dropped.\n- The categories in 'Tenure' and 'HasCrCard' have a similar churn rate and are deemed redundant. This can be confirmed from a chi-square test [[2](#Bibliography)]:","cb832988":"### XGBoost Classifier","692c8dd3":"<br>\n\n# A Quick Look at our Data\n\nWe start by importing the dataset as a Pandas DataFrame. We can also take a look at the top five rows using the `head()` method:","52e6263b":"<font size=+3 color=\"#3D3D3D\"><center><b>Predicting Customer Churn with Machine Learning \ud83c\udfe6\ud83d\udcb0<\/b><\/center><\/font>\n\n<img src=\"https:\/\/images.unsplash.com\/photo-1520033906782-1684d0e7498e?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1334&q=80\" width = 600>\n<center><em>Photo by Michael Jasmund (Unsplash)<\/em><\/center>\n\n<br>\n\n**Table of Contents**\n\n- [Introduction](#Introduction)\n- [Objective](#Objective)\n- [Libraries](#Libraries)\n- [Parameters and Variables](#Parameters-and-Variables)\n- [Functions](#Functions)\n- [A Quick Look at our Data](#A-Quick-Look-at-our-Data)\n- [Creating a Test Set](#Creating-a-Test-Set)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    - [Target Variable: Exited](#Target-Variable:-Exited)\n    - [Continuous Variables](#Continuous-Variables)\n    - [Categorical Variables](#Categorical-Variables)\n- [Data Preprocessing](#Data-Preprocessing)\n    - [Feature Selection](#Feature-Selection)\n    - [Encoding Categorical Features](#Encoding-Categorical-Features)\n    - [Scaling](#Scaling)\n    - [Addressing Class Imbalance](#Addressing-Class-Imbalance)\n- [Building Machine Learning Models](#Building-Machine-Learning-Models)\n    - [Baseline Models](#Baseline-Models)\n    - [Model Tuning](#Model-Tuning)\n- [Results](#Results)\n    - [Learning Curves](#Learning-Curves)\n    - [Feature Importance](#Feature-Importance)\n    - [Performance Comparison](#Performance-Comparison)\n- [Evaluating the Test Set](#Evaluating-the-Test-Set)\n- [Bibliography](#Bibliography)\n- [Future Development](#Future-Development)\n- [Conclusions](#Conclusions)\n  \n\n<br>\n\n# Introduction\n\n\nCustomer churn (also known as customer attrition) occurs when a customer stops using a company's products or services. \n\nCustomer churn affects profitability, especially in industries where revenues are heavily dependent on subscriptions (e.g. banks, telephone and internet service providers, pay-TV companies, insurance firms, etc.). It is estimated that acquiring a new customer can cost up to five times more than retaining an existing one.\n\nTherefore, customer churn analysis is essential as it can help a business:\n\n- identify problems in its services (e.g. poor quality product\/service, poor customer support, wrong target audience, etc.), and \n- make correct strategic decisions that would lead to higher customer satisfaction and consequently higher customer retention.\n\nIf you would like to know more about this topic, please refer to the references in the [Bibliography](#Bibliography) section.","472a6546":"## `plot_learning_curve()`","ea5fa4d2":"<br>\n\n# Bibliography\n\nThe main resources I used are the following two books:\n\n[1] **[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/), by Aur\u00e9lien G\u00e9ron (2019)**\n\n[2] **[Practical Statistics for Data Scientists, 2nd Edition](https:\/\/www.oreilly.com\/library\/view\/practical-statistics-for\/9781492072935\/), by Peter Bruce, Andrew Bruce, and Peter Gedeck (2020)**\n\n<br>\n\nThe following resources (Retrieved on mid-December 2020) also helped me in my analysis:\n\n[3] [Bank Customer Churn](https:\/\/rstudio-pubs-static.s3.amazonaws.com\/565148_6e82a5c320f14869bf63e23bcf59ce9b.html#compare-models-performance), by Zicheng Shi (same dataset but analysis in R)\n\n[4] [Metrics and scoring: quantifying the quality of predictions](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html) on [scikit-learn.org](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)\n\n[5] [Easy Guide To Data Preprocessing In Python](https:\/\/www.kdnuggets.com\/2020\/07\/easy-guide-data-preprocessing-python.html), by Ahmad Anis\n\n[6] [Meaningful Metrics: Cumulative Gains and Lyft Charts](https:\/\/towardsdatascience.com\/meaningful-metrics-cumulative-gains-and-lyft-charts-7aac02fc5c14), by Raffi Sahakyan\n\n<br>\n\nIf you would like to read more about customer churn:\n\n[7] [Churn Rate](https:\/\/www.investopedia.com\/terms\/c\/churnrate.asp), by Jake Frankenfield on [Investopedia](https:\/\/www.investopedia.com\/)\n\n[8] [Customer attrition](https:\/\/en.wikipedia.org\/wiki\/Customer_attrition) on [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Main_Page)\n\n[9] [A Survey on Churn Prediction Techniques in Communication Sector](http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.278.4171&rep=rep1&type=pdf), by N.Kamalraj and A.Malathi\n\n[10] [Customer Attrition](https:\/\/www.optimove.com\/resources\/learning-center\/customer-attrition) on [optimove](https:\/\/www.optimove.com\/resources\/learning-center\/customer-attrition)","9f9de9d2":"Recently, I came across another tool for assessing the performance of a classifier model. Simply put, a Cumulative Gain shows the percentage of targets reached when considering a certain percentage of the population with the highest probability to be target according to the model (see [here](https:\/\/towardsdatascience.com\/meaningful-metrics-cumulative-gains-and-lyft-charts-7aac02fc5c14) and [here](http:\/\/mlwiki.org\/index.php\/Cumulative_Gain_Chart)). The `scikitplot` library offers an easy way of plotting this chart:","83c57be9":"Again, the two distributions are quite similar. There is a big percentage of non-churned customers with a low account balance.","24c8e4d3":"## Encoding Categorical Features\n\nMachine learning algorithms usually require that all input (and output) features are numeric. Consequently, categorical features need to be converted (encoded) to numbers before building models.\n\nOur dataset contains two features that require encoding.\n\n- For 'Gender', we will use scikit-learn's `LabelEncoder()` which maps each unique label to an integer (Male --> 1 and Female --> 0).\n- For 'Geography', we will manually map values so that customers in Germany have the value of one (1) and all other customers (France and Spain) have zero (0). I chose this method since the churn rate for customers in the other two countries is almost equal and considerably lower than in Germany. Therefore, it makes sense to encode this feature so that it differentiates between German and non-German customers. Additionally, I tried one-hot encoding (`get_dummies()`) this feature, and the two new features for France and Spain had small feature importance.\n","c1e90177":"## `clf_performance()`","d34ef567":"## Categorical Variables\n\nLet's plot a seaborn.countplot for each categorical feature:","3ed21202":"Customers in Germany are more likely to churn than customers in the other two countries (the churn rate is almost double compared to Spain and France). Many reasons could explain this finding, such as higher competition or different preferences for German customers.","335f22f8":"### LGBMClassifier"}}