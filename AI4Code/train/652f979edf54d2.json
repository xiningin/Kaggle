{"cell_type":{"766687d2":"code","871c45a3":"code","55d77bc5":"code","9a591a00":"code","af91e04d":"code","623683df":"code","fc9822cb":"code","2f960e70":"code","bfba9691":"code","95ddca04":"code","64ec1a13":"code","94d439b3":"code","05d36ead":"code","49092ec8":"code","0439ebc4":"code","51c4707a":"code","3326a271":"code","baa1c670":"code","c0c965e0":"code","cdf83149":"code","bc2c2294":"code","1ad3c97f":"code","adfac374":"code","4f0186d6":"code","b0e2fe4d":"code","c2c88912":"code","2b4db2ee":"code","34dffd92":"code","9366999e":"code","6d6f024c":"code","dd52e2d7":"code","bda752ed":"code","fb5f1818":"code","c365f98a":"code","203f79b4":"code","2d48da9f":"code","31238bb0":"code","c348f26c":"code","b5520070":"code","5ef53354":"markdown","76fa7ee4":"markdown","38b5e058":"markdown","d77d0347":"markdown","12d8b62d":"markdown","33e25db5":"markdown","73697628":"markdown","39b1f8f8":"markdown","ab55a1ee":"markdown","867cc704":"markdown","d36e56e7":"markdown","752eb993":"markdown","edd18cb7":"markdown","a106a8a1":"markdown","10a24be6":"markdown","dfb2a22e":"markdown","8ca2a54a":"markdown","51fa1848":"markdown","9c42fd1f":"markdown","db4e6b99":"markdown","df7b5b4f":"markdown","7731975f":"markdown","09945774":"markdown","948b7173":"markdown","09c46eeb":"markdown","a83484f2":"markdown","ec466278":"markdown","489588b3":"markdown","2edff733":"markdown","ff4e9412":"markdown","6227146a":"markdown","d31b4228":"markdown","ded94f03":"markdown","8ef69c9b":"markdown","aacacd2a":"markdown","6ea62638":"markdown","c8d2cbd7":"markdown","7073d272":"markdown","21752011":"markdown","8e3276d9":"markdown","4e406672":"markdown"},"source":{"766687d2":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\n\nfrom copy import deepcopy","871c45a3":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score","55d77bc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a591a00":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","af91e04d":"y = df['diagnosis']\ndf = df.drop(['id', 'Unnamed: 32', 'diagnosis'], axis=1)\n\nfeatures_mean= list(df.columns[:10])\nfeatures_se= list(df.columns[10:19])\nfeatures_worst=list(df.columns[20:])","623683df":"df.describe()","fc9822cb":"# I want to thanks @masumrumi for sharing this amazing plot!\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );","2f960e70":"ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\nb, m = y.value_counts()\nprint(b, m)","bfba9691":"# Explore Fare distribution \ng = sns.distplot(df[\"radius_mean\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"radius_mean\"].skew()))\ng = g.legend(loc=\"best\")","95ddca04":"num_rows, num_cols = 5,2\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 12))\nfig.tight_layout()\n\nfor index, column in enumerate(df[features_mean].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    g = sns.distplot(df[column], color=\"m\", label=\"%.2f\"%(df[column].skew()), ax=axes[i,j])\n    g = g.legend(loc=\"best\")","64ec1a13":"# Explore Age distibution \ng = sns.kdeplot(df[\"radius_mean\"][(y == 'B') & (df[\"radius_mean\"].notnull())], color=\"Red\", shade=True)\ng = sns.kdeplot(df[\"radius_mean\"][(y == 'M') & (df[\"radius_mean\"].notnull())], ax=g, color=\"Blue\", shade=True)\ng.set_xlabel(\"radius_mean\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","94d439b3":"df_b = df[y == 'B']\ndf_m = df[y == 'M']\n\nnum_rows, num_cols = 5,2\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 12))\nfig.tight_layout()\n\nfor index, column in enumerate(df[features_mean].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    g = sns.kdeplot(df_b[column], color=\"Red\", shade=True, ax=axes[i,j])\n    g = sns.kdeplot(df_m[column], ax=g, color=\"Blue\", shade=True)\n    g.set_xlabel(column)\n    g = g.legend([\"Benign\",\"Malignant\"])","05d36ead":"sns.set(rc={'figure.figsize':(16,12)})","49092ec8":"corr = df[features_mean].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8})\n# yticks\nplt.yticks(rotation=0)\nplt.show()","0439ebc4":"# Selecting upper triangle of correlation matrix\nupper_tri = corr.where(np.triu(np.ones(corr.shape),\n                                  k=1).astype(np.bool))\n\n# Finding index of feature columns with correlation greater than 0.95\nmean_set_to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\nprint('Features to drop: {}'.format(mean_set_to_drop))","51c4707a":"corr = df.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=False, fmt=\".2f\", cmap='coolwarm',\n           cbar_kws={\"shrink\": .8})\n# yticks\nplt.yticks(rotation=0)\nplt.show()","3326a271":"# Selecting upper triangle of correlation matrix\nupper_tri = corr.where(np.triu(np.ones(corr.shape),\n                                  k=1).astype(np.bool))\n\n# Finding index of feature columns with correlation greater than 0.95\nfull_set_to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\nprint('Features to drop: {}'.format(full_set_to_drop))","baa1c670":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint(\"Accuracy: %.4f%%\" % (ac * 100.0))","c0c965e0":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(df[features_mean], y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint(\"Accuracy: %.4f%%\" % (ac * 100.0))","cdf83149":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny = le.fit_transform(y)","bc2c2294":"def fix_skew(features):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numerical_columns = features.select_dtypes(include=['int64','float64']).columns\n\n    # Check the skew of all numerical features\n    skewed_features = features[numerical_columns].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n    high_skew = skewed_features[abs(skewed_features) > 0.5]\n    skewed_features = high_skew.index\n\n    # Perform the Box-Cox transformation\n    for column in skewed_features:\n        features[column] = boxcox1p(features[column], boxcox_normmax(features[column] + 1))\n        \n    return features","1ad3c97f":"plotting_3_chart(df, 'concavity_mean')","adfac374":"df_skew = fix_skew(df)","4f0186d6":"plotting_3_chart(df, 'concavity_mean')","b0e2fe4d":"num_rows, num_cols = 5,2\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(25, 12))\nfig.tight_layout()\n\nfor index, column in enumerate(df[features_mean].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    g = sns.distplot(df_skew[column], color=\"m\", label=\"%.2f\"%(df_skew[column].skew()), ax=axes[i,j])\n    g = g.legend(loc=\"best\")","c2c88912":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(df_skew, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint(\"Accuracy: %.4f%%\" % (ac * 100.0))","2b4db2ee":"from sklearn.preprocessing import StandardScaler\nsscal = StandardScaler()\nsscal.fit(df)\ndf_scaled = sscal.transform(df)","34dffd92":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(df_scaled)\n\nplt.figure(1, figsize=(8, 7))\nplt.clf()\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","9366999e":"pca = PCA(n_components=3)\npca.fit(df)\n\ndf_pca = pca.transform(df)","6d6f024c":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(df_pca, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint(\"Accuracy: %.4f%%\" % (ac * 100.0))","dd52e2d7":"from datetime import datetime\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier","bda752ed":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(df_pca, y, test_size=0.3, random_state=42)\n\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)","fb5f1818":"# make predictions for test data\ny_pred = model.predict(x_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.4f%%\" % (accuracy * 100.0))","c365f98a":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 10, 40],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 10]\n        }\n\nxgb = XGBClassifier(\n    learning_rate=0.02, \n    n_estimators=600,\n    silent=True, \n    nthread=1\n)","203f79b4":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","2d48da9f":"folds = 5\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state=42)\n\nrandom_search = RandomizedSearchCV(\n    xgb, \n    param_distributions=params, \n    n_iter=param_comb, \n    scoring='roc_auc', \n    n_jobs=4, \n    cv=skf.split(df_pca, y), \n    verbose=3, \n    random_state=42\n)\n\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(df_pca, y)\ntimer(start_time) # timing ends here for \"start_time\" variable","31238bb0":"best_params = random_search.best_params_\nbest_params","c348f26c":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(df_pca, y, test_size=0.3, random_state=42)\n\nmodel = XGBClassifier(\n    subsample=0.6,\n    min_child_weigth=1,\n    max_depth=5,\n    gamma=1.5,\n    colsample_bytree=0.8,\n)\nmodel.fit(x_train, y_train)","b5520070":"# make predictions for test data\ny_pred = model.predict(x_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.4f%%\" % (accuracy * 100.0))","5ef53354":"![pca.jpeg](attachment:pca.jpeg)","76fa7ee4":"## Target\n\nLets see the target distribution","38b5e058":"## Independent variables: study the distribution","d77d0347":"As we can see in this plot, the distribution skewness has changhed, but the Probability Plots are very similar.","12d8b62d":"Now that the data is scaled, we can use PCA.","33e25db5":"## Final model","73697628":"As we can see, the classes are not equaly balanced so we have to take it into consideration in the future.\n\nThe imbalanced ratio is not so hard, so for now we will let the variable as it is.","39b1f8f8":"As we can see, this transformation hasn't have any impact in the model poerformance, so we can discard it.","ab55a1ee":"# 2. Feature engineering \ud83d\udee0","867cc704":"![skewness.png](attachment:skewness.png)","d36e56e7":"## Mean subset","752eb993":"As we can see, the [BoxCox function](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html) have completlely fixed the skewness of the variables.\n\nSo now, we have to check if this transformacion have had any impact in the model performace.","edd18cb7":"As we can see, many of the variables have distinct distributions from Benign to Malignant so we could think that these are going to be relevant variables.\n\nOn the other hand, those variables with almost the same distribution in Benign and Malignant probably wont be as much important as others are.","a106a8a1":"## Encode label","10a24be6":"As we can see, it seems that we can explain all the data with **3 or more features**.","dfb2a22e":"## Hyperparameter tunning","8ca2a54a":"As we can see, there are some correlated variables. So we need to drop some of them. In this case, we decided to drop: ['perimeter_mean', 'area_mean']","51fa1848":"## Fix Skewness","9c42fd1f":"The first thing we are going to try (to see how it impacts the modle performance) is to reduce the distributions skewness with boxcox transformation.","db4e6b99":"Now we are comparing all the variables in the dataset so there are a higher number of features correlated.\n\nAs we can see, there are some correlated variables. So we need to drop some of them. In this case, we decided to drop: ['perimeter_mean', 'area_mean', 'perimeter_se', 'area_se', 'radius_worst', 'perimeter_worst', 'area_worst']","df7b5b4f":"![DNA-purple-banner_.png](attachment:DNA-purple-banner_.png)","7731975f":"As we can see, with only 3 features, we can explain almost the 100% of the information in the original plot!!","09945774":"## Second try model","948b7173":"### Full set","09c46eeb":"As we can see, its a really high performace without doing anything at the dataset, so it seems to be a easy problem.\n\nIn the next steps we will try to get a higher performance and reduce the dimensionality.","a83484f2":"In this case, it seems that we are losing information. Anycase, it's still a good result.","ec466278":"## PCA: Principal Component Analysis","489588b3":"## Independent variables: kernel density estimation","2edff733":"As we can see, some of the variables are little skewed. As all the values are below 1.5, we can ommit it.\n\nThe other clear conclusion is that the data is not scaled and standarized. We could try to this options.","ff4e9412":"# 1. Data Visualization \ud83d\udcca","6227146a":"### Third try model","d31b4228":"Firstly, we can check that id and Unamed: 32 are useless features so we can drop them.\n\nThen we can check dtypes","ded94f03":"## First try model\n\nAt this point, I think that we have so many things to do, but i want to see how the model works with all the features and only the `mean`ones.\n\nSo, the first thing we are going to do is to train a naive model (Random Forest) with both sets and save the accuracy.\n\nIn the next steps, we will compare these accuracy to see which is the impact in the model performance.","8ef69c9b":"## Correlation","aacacd2a":"As we can see, all the variables are numerical.\n\n* In this kind of problemas we have to be aware the numiercal datya distributions, skew, outliers, and so on. \n* We won't need to feature encoding \n* I have the intuition that some variables are going to be correlated so i will have to do a feature selection\n\nSo, lets go for it!","6ea62638":"The first thing we are going to do is to Standarize the data so we can ","c8d2cbd7":"# 3. Feature selection \ud83d\udd2d","7073d272":"As we can see, we have earned a very good result. So now we can train the final model.","21752011":"## Test XGBoost model","8e3276d9":"# 4. Modeling \ud83d\udcda\ud83d\udd2e\n\nNow with the final subset of variables, we are going to select and tune the final model.","4e406672":"# \ud83e\udda0 Breast Cancer \ud83e\ude7a EDA and Classification \ud83e\uddec"}}