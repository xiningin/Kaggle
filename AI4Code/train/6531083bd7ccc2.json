{"cell_type":{"40481fee":"code","1f5b1349":"code","7d417eb3":"code","1eb04108":"code","21663635":"code","3e8c596e":"code","905b9e14":"code","b34a34da":"code","1630a074":"code","5aaf826b":"code","a0a953e1":"code","20d9b831":"markdown","c6674636":"markdown","7bf145ba":"markdown","77d5627e":"markdown","e5c1f477":"markdown","019fd9ea":"markdown"},"source":{"40481fee":"import torch\n\ndef try_gpu():\n    if torch.cuda.device_count() >= 1:\n        return torch.device('cuda:0')\n    else: return torch.device('cpu')\n\ntry_gpu()","1f5b1349":"# \u8bfb\u53d6\u6570\u636e,\u5c55\u5e73\u4e3a list\nimport re\n\ndef read_file(path, sub=None):\n    '''sub:\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u88ab\u5ffd\u7565\u7684\u5b57\u7b26'''\n    text = []\n    with open(path, 'r', encoding='utf-8') as f:\n        if sub:\n            return [re.sub(sub, '', i) for i in f]\n        else:\n            return [i for i in f]\n\ndef tokenize(lines, token='char'):\n    \"\"\"\u5c06\u6587\u672c\u884c\u62c6\u5206\u4e3a\u5355\u8bcd\u6216\u5b57\u7b26\u8bcd\u5143\u3002\n    (token:'char'or'word)\"\"\"\n    if token == 'word':\n        return [word for line in lines for word in line.split()]\n    elif token == 'char':\n        return [char for line in lines for char in list(line)]\n    else:\n        raise RuntimeError('\u9519\u8bef:\u672a\u77e5\u8bcd\u5143\u7c7b\u578b:' + token)\n\ndef counter(tokens): # \u8bcd\u9891\u7edf\u8ba1\n    worddict = dict()\n    for w in tokens:\n        worddict[w]= worddict.get(w,0)+1\n    return worddict\n\n\nclass Vocab:\n    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n        self.token_freqs = counter(tokens)\n        wordlist = sorted(self.token_freqs,key=lambda w:self.token_freqs[w])\n        self.unk = 0\n        uniq_tokens = [i for i in wordlist if (self.token_freqs[i]>=min_freq) or i in reserved_tokens]\n        self.idx_to_token, self.token_to_idx = ['<unk>'], dict()\n        for token in uniq_tokens[::-1]:\n            self.idx_to_token.append(token)\n            self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n    \n    def __getitem__(self,tokens):\n        if not isinstance(tokens, (list, str)):\n            raise ValueError(\"tokens must be <list> or <str>\")\n        return [self.token_to_idx.get(token,self.unk) for token in tokens]\n\n    def to_tokens(self,indices):\n        return [self.idx_to_token[int(i)] for i in indices]\n\n","7d417eb3":"import torch \nimport random\nfrom torch.nn import functional as F\n#\u987a\u5e8f\ndef seq_data_iter_sequential(corpus, batch_size, num_steps, device): \n    offset = random.randint(0,num_steps-1)\n    for i in range(offset,len(corpus)\/\/batch_size-1-num_steps,num_steps):\n        X = [corpus[i*batch: i*batch + num_steps] for batch in range(1,batch_size+1)]\n        Y = [corpus[i*batch + 1: i*batch + num_steps + 1] for batch in range(1,batch_size+1)]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n\n","1eb04108":"# \u5bfc\u5165\u5305\nfrom torch import nn\nfrom torch.nn import functional as F","21663635":"# \u57fa\u7840\u957f\u77ed\u671f\u6a21\u578b\n\nclass rnnlstm(nn.Module):\n    def __init__(self, num_hiddens, vocab_size, num_layers, device) -> None:\n        super().__init__()\n        self.device=device \n        self.vocab_size = vocab_size\n        self.num_layers = num_layers\n        self.num_hiddens = num_hiddens\n        self.state = None\n        self.c_0 = None\n        self.rnn_layer = nn.LSTM(vocab_size,num_hiddens,num_layers)\n        self.linear = nn.Linear(num_hiddens,vocab_size)\n        self.to(device)\n\n    def forward(self, inputs):\n        X_0 = F.one_hot(inputs.T.long(), self.vocab_size).to(torch.float32)\n        X_1, self.state = self.rnn_layer(X_0,self.state)\n        outputs = self.linear(X_1.reshape((-1, X_1.shape[-1])))\n        return outputs\n\n    def init_state(self, batch_size=None):\n        if batch_size:\n            self.state = (torch.zeros((self.num_layers, batch_size, self.num_hiddens),device=self.device),\n                          torch.zeros((self.num_layers, batch_size, self.num_hiddens),device=self.device))\n        else:\n            self.state = (torch.zeros_like(self.state[0], device=self.device),\n                          torch.zeros_like(self.state[1], device=self.device))\n","3e8c596e":"# \u57fa\u7840rnn\u6a21\u578b\n\nclass rnn0(nn.Module):\n    def __init__(self, num_hiddens, vocab_size, num_layers, device) -> None:\n        super().__init__()\n        self.device=device\n        self.vocab_size = vocab_size\n        self.num_layers = num_layers\n        self.num_hiddens = num_hiddens\n        self.state = None\n        self.rnn_layer = nn.RNN(vocab_size,num_hiddens,num_layers)\n        self.linear = nn.Linear(num_hiddens,vocab_size)\n        self.to(self.device)\n    def forward(self, inputs):\n        state = self.state\n        X_0 = F.one_hot(inputs.T.long(), self.vocab_size).to(torch.float32)\n        X_1, self.state = self.rnn_layer(X_0,state)\n        outputs = self.linear(X_1.reshape((-1, X_1.shape[-1])))\n        return outputs\n\n    def init_state(self, batch_size=None):\n        if batch_size:\n            self.state = torch.zeros((self.num_layers, batch_size, self.num_hiddens),device=self.device)\n        else:\n            self.state = torch.zeros_like(self.state, device=self.device)","905b9e14":"# \u521d\u59cb\u5316\u6a21\u578b\u53c2\u6570\n\ndef init_parameters(p):\n    pass\n","b34a34da":"def predict(prefix, num_preds, net, vocab, device):  # @save\n    \"\"\"\u5728`prefix`\u540e\u2faf\u2f63\u6210\u65b0\u5b57\u7b26\u3002\"\"\"\n    net.init_state(1)\n    if isinstance(prefix,list):\n        prefix = [[i] for i in prefix]\n    outputs = [*vocab[prefix[0]]]\n\n    def get_input(): return torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n    for y in prefix[1:]:  # \u9884\u70ed\u671f\n        _= net(get_input())\n        outputs.append(*vocab[y])\n        \n    for _ in range(num_preds):  # \u9884\u6d4b`num_preds`\u6b65\n        y = net(get_input())\n        outputs.append(int(y.argmax(dim=-1).reshape(1)))\n    return ''.join(vocab.to_tokens(outputs))\n","1630a074":"#\u8bad\u7ec3\nfrom tqdm import tqdm\n\ndef train_step(net, data, optimizer):\n    loss = nn.CrossEntropyLoss()\n    l=0\n    for x, y in data():\n        net.init_state()\n        y_hat = net(x)\n        Y = y.T.reshape(-1)\n        l = loss(y_hat,Y.long())\n        optimizer.zero_grad()\n        l.backward()\n        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.5, norm_type=2)\n        optimizer.step()\n    return l\n\ndef train_epoch(epoch, net, data, optimizer):\n    bar = tqdm(range(epoch))\n    for i in bar:\n        l = train_step(net, data, optimizer)\n        bar.set_description(F'epoch:{i+1:<2d} Perplexity:{l:.3f} Process')\n","5aaf826b":"#load data\ntokens = tokenize(read_file('..\/input\/chinese1\/zh.txt','[\\n ]'))\nvocab = Vocab(tokens)\ncorpus = vocab[tokens]\n#init model\ndevice = try_gpu()\n\nbatch_size, num_steps, num_hiddens = 64, 50, 2048\nnet = rnn0(num_hiddens, len(vocab), 2, device)\n#net = net.to(device)\nnet.init_state(batch_size)\noptimizer = torch.optim.Adadelta(net.parameters(), 1)\ndef get_iter():\n    return seq_data_iter_sequential(corpus, batch_size, num_steps, device)\n\ntrain_epoch(100, net, get_iter, optimizer)","a0a953e1":"start = ['\u6218\u4e89\u521a\u521a\u7ed3\u675f','\u4eca\u5e74','\u5bb6\u91cc','\u6211\u4eec\u5e94\u8be5']\nfor t in start: \n    print(predict(t, 100, net, vocab, device))","20d9b831":"## \u9884\u6d4b&\u8bad\u7ec3","c6674636":"## \u6587\u672c\u9884\u5904\u7406 \n\n1. \u8bfb\u53d6\u6587\u4ef6\n2. \u62c6\u5206\u8bcd\u5143\n3. \u5efa\u7acb\u8bcd\u6c47\u8868","7bf145ba":"## \u5c1d\u8bd5GPU","77d5627e":"## \u6570\u636e","e5c1f477":"# \u8bad\u7ec3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u57fa\u7840\u529f\u80fd\u7684\u4ee3\u7801\u5757\n   \n    ","019fd9ea":"## \u6a21\u578b"}}