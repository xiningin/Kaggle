{"cell_type":{"c678eb46":"code","7ae95854":"code","c06ca7a1":"code","6853b943":"code","b6bfecc8":"code","bd761b25":"code","b550e8bc":"code","48e733a1":"code","c7217c3c":"code","46e329b2":"code","9dca3a21":"code","9c31e805":"code","82c16fb1":"code","c9b20e0b":"code","687f21f4":"code","ac431489":"code","2bb77352":"code","ce933266":"code","032bee6b":"code","8763577b":"code","320e15c1":"code","5d021d92":"code","279bd446":"code","6101c1de":"code","9496596b":"code","9b5b7d80":"code","bfd29e16":"code","12243d7e":"code","c1d05484":"code","6b961acb":"code","48233104":"code","a9c7b7ff":"code","93a4a1c3":"code","66f15cf4":"code","fca84f50":"code","691735c1":"code","f1cc84e6":"code","27dad58e":"code","92c36ce6":"code","61a41191":"code","33c9ac62":"code","7a4e8cc9":"code","647d3986":"code","a66a635b":"code","6e6d1663":"code","62347e08":"code","ead8ea20":"code","52f09174":"code","06d49bf5":"code","0405bd8d":"code","5288f817":"code","d6b78de3":"code","f07b0c2f":"code","acac3265":"code","f5cf417b":"code","627f6e83":"code","1c129ef7":"code","e368bcc0":"code","1161147c":"code","170b3fad":"code","cd9ce06d":"code","e1b7ae63":"code","6d81dc17":"code","0fef1e19":"code","952c4cd5":"code","b7e3f884":"code","a97c1b41":"code","ec7cf37f":"code","59cc3839":"code","56e3a6a2":"code","02169198":"code","c6896be1":"code","9c2e1df3":"code","df8d48af":"code","ab432954":"code","e5cf790b":"code","d2c04390":"code","b2a7a534":"code","ba43ddbc":"code","45c99329":"code","b9c47de3":"code","b1b3fe0e":"markdown","b209e14e":"markdown","8f011938":"markdown","782b8517":"markdown","f7d7c70b":"markdown","ece7b0c7":"markdown","4047f5fb":"markdown","ca8f8073":"markdown","199b77b3":"markdown","f8aa6be4":"markdown","94a505f7":"markdown","25308de9":"markdown","bba0c7b0":"markdown","e11ed351":"markdown","c99a0df8":"markdown","d53e1b37":"markdown","b659dbc5":"markdown","0e52b652":"markdown","bfa64573":"markdown","c55d0191":"markdown","37a2cfc1":"markdown","a8e6effe":"markdown","870432d9":"markdown","059da276":"markdown","6df8f8b1":"markdown","e2e3c332":"markdown","2a8c401f":"markdown","e618a6ec":"markdown","47110301":"markdown","de0d8548":"markdown","a4c61094":"markdown","dcc9376c":"markdown","ba3c8129":"markdown","540c86f7":"markdown","cd035949":"markdown","ef3ad2b2":"markdown","86257a21":"markdown","e1869b08":"markdown","f0863295":"markdown","aad89afe":"markdown","8b0ae7af":"markdown","0cfd4bf7":"markdown","cdfa192f":"markdown","12691ec2":"markdown"},"source":{"c678eb46":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nstyle.use('dark_background')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ae95854":"data_dir='\/kaggle\/input\/titanic\/'","c06ca7a1":"train=pd.read_csv(data_dir+'train.csv')\ntest=pd.read_csv(data_dir+'test.csv')","6853b943":"train.describe()","b6bfecc8":"test.describe()","bd761b25":"test","b550e8bc":"otest=test.copy()\notrain=train.copy()","48e733a1":"otrain","c7217c3c":"feature_nan=[feature for feature in train.columns if train[feature].isnull().sum()>=1]\nfor feature in feature_nan:\n    print(f' {feature} : {np.round(np.round(train[feature].isnull().mean(),3)*100,2) } % missing')","46e329b2":"train.columns","9dca3a21":"fig,axes=plt.subplots(nrows=1,ncols=2,figsize=(15,8))\nwomen=train[train['Sex']=='female']\nmen=train[train['Sex']=='male']\nax=sns.distplot(women[women['Survived']==1].Age.dropna(),bins=18,label='Survived',ax=axes[0],kde=False)\nax=sns.distplot(women[women['Survived']==0].Age.dropna(),bins=18,label='Not_Survived',ax=axes[0],kde=False)\nax.legend()\nax.set_title('Female')\nax=sns.distplot(men[men['Survived']==1].Age.dropna(),bins=18,label='Survived',ax=axes[1],kde=False)\nax=sns.distplot(men[men['Survived']==0].Age.dropna(),bins=18,label='Not_Survived',ax=axes[1],kde=False)\nax.legend()\nax.set_title('Male')","9c31e805":"train.columns","82c16fb1":"sns.FacetGrid(train,row=\"Embarked\",size=7,aspect=1.6).map(sns.pointplot,'Pclass','Survived','Sex',palette=None,  order=None, hue_order=None ).add_legend()","c9b20e0b":"sns.barplot(x='Pclass',y='Survived',data=train)\nplt.legend()","687f21f4":"sns.FacetGrid(train,row='Pclass',col='Survived',size=5,aspect=1.6).map(sns.distplot,'Age',bins=20,kde=False).add_legend()\n","ac431489":"train.columns","2bb77352":"data=[train,test]\nfor dataset in data:\n    dataset['relatives']=dataset['SibSp']+dataset['Parch']\n    dataset.loc[dataset['relatives']>0,'not_alone']=1\n    dataset.loc[dataset['relatives']==0,'not_alone']=0\n    dataset['not_alone']=dataset['not_alone'].astype(int)\n    \ntrain['not_alone'] .value_counts()   ","ce933266":"train.columns","032bee6b":"sns.pointplot('relatives','Survived',data=train,size=100,aspect=150)","8763577b":"train=train.drop(['PassengerId'],axis=1)","320e15c1":"train","5d021d92":"train['Cabin'].unique()","279bd446":"import re\ndeck={\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6,\"G\":7,\"T\":8,\"U\":0}\ndata=[train,test]\nfor dataset in data:\n    dataset['Cabin']=dataset[\"Cabin\"].fillna(\"U\");\n    dataset[\"Deck\"]=dataset[\"Cabin\"].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset[\"Deck\"]=dataset[\"Deck\"].map(deck)\n    dataset[\"Deck\"]=dataset[\"Deck\"].astype(int)\n\ntrain=train.drop(['Cabin'],axis=1)\ntest=test.drop(['Cabin'],axis=1)\n    \n    ","6101c1de":"train","9496596b":"data=[train,test]\n\nfor dataset in data:\n    means=np.mean(dataset[\"Age\"])\n    std=np.std(dataset[\"Age\"])\n    nulls=dataset[\"Age\"].isnull().sum()\n    rand_age=np.random.randint(means-std,means+std,size=nulls)\n    imputed_age=dataset[\"Age\"].copy()\n    #an_age=dataset[\"Age\"].copy()\n    imputed_age[np.isnan(imputed_age)]=rand_age\n    dataset[\"Age\"]=imputed_age\n    dataset[\"Age\"]=dataset[\"Age\"].astype(int)\n\n    ","9b5b7d80":"train[\"Embarked\"].mode()","bfd29e16":"cv=\"S\"\ndata=[train,test]\nfor dataset in data:\n    dataset[\"Embarked\"]=dataset[\"Embarked\"].fillna(cv)","12243d7e":"train.isnull().sum()","c1d05484":"test.isnull().sum()","6b961acb":"train.info()","48233104":"data=[train,test]\nfor dataset in data:\n    dataset[\"Fare\"]=dataset[\"Fare\"].fillna(0)\n   ","a9c7b7ff":"train[\"Name\"]","93a4a1c3":"titles={\"Mr\":1,\"Miss\":2,\"Mrs\":3,\"Master\":4,\"Rare\":5}\ndata=[train,test]\nfor dataset in data:\n    dataset['Title']=dataset.Name.str.extract(\"([a-zA-Z]+)\\.\")\n    dataset[\"Title\"]=dataset[\"Title\"].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n    dataset['Title']=dataset['Title'].replace('Mile','Miss')\n    dataset['Title']=dataset['Title'].replace('Ms','Miss')\n    dataset['Title']=dataset['Title'].replace('Mme','Mrs')\n    dataset['Title']=dataset['Title'].map(titles)\n    dataset['Title']=dataset['Title'].fillna(0)\n    \n    \ntrain=train.drop(['Name'],axis=1)\ntest=test.drop(['Name'],axis=1)","66f15cf4":"train['Title']=train['Title'].astype(int)","fca84f50":"train","691735c1":"train.isnull().sum()","f1cc84e6":"gender={\"male\":0,\"female\":1}\ndata=[train,test]\nfor dataset in data:\n    dataset['Sex']=dataset['Sex'].map(gender)\n    ","27dad58e":"train","92c36ce6":"train[\"Ticket\"].describe()","61a41191":"train=train.drop(['Ticket'],axis=1)\ntest=test.drop(['Ticket'],axis=1)","33c9ac62":"train","7a4e8cc9":"ports={\"S\":0,\"C\":1,\"Q\":2}\ndata=[train,test]\nfor dataset in data:\n    dataset[\"Embarked\"]=dataset[\"Embarked\"].map(ports)\n    ","647d3986":"train","a66a635b":"train.isnull().sum()","6e6d1663":"train.info()","62347e08":"data=[train,test]\nfor dataset in data:\n    dataset['Age']=dataset['Age'].astype(int)\n    dataset.loc[dataset['Age']<=11,'Age']=0\n    dataset.loc[(dataset['Age']>11) & (dataset['Age']<=18),'Age']=1\n    dataset.loc[(dataset['Age']>18) & (dataset['Age']<=22),'Age']=2\n    dataset.loc[(dataset['Age']>22) & (dataset['Age']<=27),'Age']=3\n    dataset.loc[(dataset['Age']>27) & (dataset['Age']<=33),'Age']=4\n    dataset.loc[(dataset['Age']>33) & (dataset['Age']<=40),'Age']=5\n    dataset.loc[(dataset['Age']>40) & (dataset['Age']<=66),'Age']=6\n    dataset.loc[ (dataset['Age']>66),'Age']=7\n    \n\n","ead8ea20":"train['Age'].value_counts()","52f09174":"data=[train,test]\nfor dataset in data:\n    #dataset['Age']=dataset['Age'].astype(int)\n    dataset.loc[dataset['Fare']<=7.9,'Fare']=0\n    dataset.loc[(dataset['Fare']>7.9) & (dataset['Fare']<=14.454),'Fare']=1\n    dataset.loc[(dataset['Fare']>14.454) & (dataset['Fare']<=31),'Fare']=2\n    dataset.loc[(dataset['Fare']>31) & (dataset['Fare']<=99),'Fare']=3\n    dataset.loc[(dataset['Fare']>99) & (dataset['Fare']<=250),'Fare']=4\n    dataset.loc[ (dataset['Fare']>250),'Fare']=5\n    dataset['Fare']=dataset['Fare'].astype(int)","06d49bf5":"train","0405bd8d":"data=[train,test]\nfor dataset in data:\n    dataset['Age_class']=dataset['Age']*dataset['Pclass']","5288f817":"train","d6b78de3":"data=[train,test]\nfor dataset in data:\n    dataset['Fare_per_person']=dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_per_person']=dataset['Fare_per_person'].astype(int)","f07b0c2f":"train","acac3265":"X=train.drop(['Survived'],axis=1)\ny=train['Survived']\nX","f5cf417b":"train['Survived'].value_counts()","627f6e83":"processed_df=X","1c129ef7":"test=test.drop(['PassengerId'],axis=1)","e368bcc0":"test","1161147c":"from sklearn.preprocessing import StandardScaler\nstd_data=StandardScaler().fit_transform(X)\nsample_data=std_data\nprint(np.round(np.mean(std_data),2))\nprint(np.std(std_data))","170b3fad":"std_data_test=StandardScaler().fit_transform(test)\nprint(np.mean(std_data_test))\nprint(np.std(std_data_test))","cd9ce06d":"from sklearn.manifold import TSNE\nmodel=TSNE(n_components=2,perplexity=50,n_iter=5000,random_state=0)\ntsne_data=model.fit_transform(std_data)\ntsne_data=np.vstack((tsne_data.T,y)).T\ntsne_df=pd.DataFrame(tsne_data,columns=['D1','D2','Class'])\nprint(f' Perplexity : {50} and iterations : {5000} ')\nsns.FacetGrid(tsne_df,hue='Class',size=7).map(plt.scatter,'D1','D2').add_legend()\nplt.show()","e1b7ae63":"model=TSNE(n_components=2,perplexity=30,n_iter=5000,random_state=0)\ntsne_data=model.fit_transform(std_data)\ntsne_data=np.vstack((tsne_data.T,y)).T\ntsne_df=pd.DataFrame(tsne_data,columns=['D1','D2','Class'])\nprint(f' Perplexity: {30} and iterations: {5000} ')\nsns.FacetGrid(tsne_df,hue='Class',size=7).map(plt.scatter,'D1','D2').add_legend()\nplt.show()","6d81dc17":"model=TSNE(n_components=2,perplexity=80,n_iter=5000,random_state=0)\ntsne_data=model.fit_transform(std_data)\ntsne_data=np.vstack((tsne_data.T,y)).T\ntsne_df=pd.DataFrame(tsne_data,columns=['D1','D2','Class'])\nprint(f' Perplexity: {80} and iterations: {5000} ')\nsns.FacetGrid(tsne_df,hue='Class',size=7).map(plt.scatter,'D1','D2').add_legend()\nplt.show()","0fef1e19":"model=TSNE(n_components=2,perplexity=1000,n_iter=5000,random_state=0)\ntsne_data=model.fit_transform(std_data)\ntsne_data=np.vstack((tsne_data.T,y)).T\ntsne_df=pd.DataFrame(tsne_data,columns=['D1','D2','Class'])\nprint(f' Perplexity: {100} and iterations: {5000} ')\nsns.FacetGrid(tsne_df,hue='Class',size=7).map(plt.scatter,'D1','D2').add_legend()\nplt.show()","952c4cd5":"from sklearn import decomposition\npca=decomposition.PCA()\npca.n_components=13\npca_data=pca.fit_transform(std_data)\npercent_explained_variance=pca.explained_variance_\/np.sum(pca.explained_variance_)\ncummulative_explained_variance=np.cumsum(percent_explained_variance)\nplt.plot(cummulative_explained_variance,linewidth=2)\nplt.grid()\nplt.xlabel('dimensions')\nplt.ylabel('% explained variance')\nplt.show()","b7e3f884":"pca=decomposition.PCA()\npca.n_components=8\nfinal_data_train=pca.fit_transform(sample_data)\n","a97c1b41":"pca=decomposition.PCA()\npca.n_components=8\nfinal_data_test=pca.fit_transform(std_data_test)","ec7cf37f":"final_data_train.shape,final_data_test.shape","59cc3839":"x_train=final_data_train\ny_train=y\nx_test=final_data_test","56e3a6a2":"x_train.shape,y_train.shape,x_test.shape","02169198":"from sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score","c6896be1":"\nfrom sklearn.neighbors import KNeighborsClassifier\nml=list(range(100))\nlst_k=[i for i in ml if i%2!=0 ]\ncv_scores=[]\nMSE=[]\nfor k in lst_k:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    scores=cross_val_score(knn,x_train,y_train,cv=10,scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \nMSE=[1-x for x in cv_scores]\nbest_k=lst_k[MSE.index(min(MSE))]\nprint(f'best k of knn is {best_k}')\nplt.plot(lst_k,MSE)\nplt.xlabel('K value of KNN ->')\nplt.ylabel('Mean Squared Error')\nplt.show()\n","9c2e1df3":"knn=KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(x_train,y_train)\ny_pred_knn=knn.predict(x_test)\nacc_knn=np.round(knn.score(x_train,y_train)*100,2)\nprint(f' The accuracy on train set of KNN model is {acc_knn}')","df8d48af":"from sklearn.ensemble import RandomForestClassifier\n\n# random_forest.fit(x_train,y_train)\n# y_pred=random_forest.predict(x_test)\n# acc_random_forest=np.round(random_forest.score(x_train,y_train)*100,2)\n# print(f' The accuracy of random forest is {acc_random_forest}')\n\n\nml=list(range(150))\nlst_n=[i for i in ml if i!=0]\ncv_scores=[]\nMSE=[]\nfor k in lst_n:\n    #print(k,end='')\n    random_forest=RandomForestClassifier(n_estimators=k)\n    scores=cross_val_score(random_forest,x_train,y_train,cv=10,scoring='accuracy')\n    cv_scores.append(scores.mean())\n    \nMSE=[1-x for x in cv_scores]\nbest_n=lst_n[MSE.index(min(MSE))]\nprint(f'best n of Random Forest is {best_n}')\nplt.plot(lst_n,MSE)\nplt.xlabel('n value of Random Forest ->')\nplt.ylabel('Mean Squared Error')\nplt.show()\n","ab432954":"random_forest=RandomForestClassifier(n_estimators=best_n)\nrandom_forest.fit(x_train,y_train)\ny_pred_rf=random_forest.predict(x_test)\nacc_random_forest=np.round(random_forest.score(x_train,y_train)*100,2)\nprint(f' The accuracy of random forest is {acc_random_forest}')","e5cf790b":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_pred_dt=dt.predict(x_test)\nacc_dt=np.round(dt.score(x_train,y_train)*100,2)\nprint(f' The accuracy of random forest is {acc_dt}')","d2c04390":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb=nb.predict(x_test)\nacc_nb=np.round(nb.score(x_train,y_train)*100,2)\nprint(f' The accuracy of random forest is {acc_nb}')","b2a7a534":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\n\ny_pred_lr = logreg.predict(x_test)\n\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nprint(f'The accuracy of logistic Regression is {acc_log}')","ba43ddbc":"from sklearn.svm import SVC, LinearSVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\n\nY_pred_svm = linear_svc.predict(x_test)\n\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nprint(f'The accuracy of SVM is {acc_linear_svc}')","45c99329":"from sklearn.linear_model import SGDClassifier\nsgd =SGDClassifier(max_iter=5, tol=None)\nsgd.fit(x_train, y_train)\nY_pred_sgd= sgd.predict(x_test)\n\nsgd.score(x_train, y_train)\n\nacc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\nprint(f'The accuracy of SGD is {acc_sgd}')","b9c47de3":"results=pd.DataFrame({\n    'Model':['SVM','KNN',\"Logistic regression\",'Random_forests',\"Decision Tree\",'SGD',\"Naive Bayes\"],\n    'score':[acc_linear_svc,acc_knn,acc_log,acc_random_forest,acc_dt,acc_sgd,acc_nb]})\nresults=results.sort_values(by='score',ascending=False)\nresults=results.set_index('score')\nresults\n    ","b1b3fe0e":"## lets convert float type Fare to int","b209e14e":"# categorizing Fare","8f011938":"# % of missing datas","782b8517":"# Embarked to Numeric","f7d7c70b":"# From graph we can see that, only top 8 features can explain greater than 95% of total variance of our dataset and  to explain 100% we just need 10.","ece7b0c7":"# Separating feature matrix and target vectors","4047f5fb":"# Applying Knn","ca8f8073":"# best tree value is {}","199b77b3":"# well person in class 1 has the better survival but person beloging to class 3 has least survival chance","f8aa6be4":"## so 8 can be a good value for dimension of our dataset","94a505f7":"### standardizing test data","25308de9":"# finding best dimensional value that can explain the maximum variance in dataset","bba0c7b0":"# summing up all sibsp and parch to relatives","e11ed351":"# Applying tsne","c99a0df8":"# Applying Random Forest","d53e1b37":"#  Applying SVM","b659dbc5":"# Applying Logistic-Regression","0e52b652":"# Applying Naive-Bayes","bfa64573":"## lets create our final dataset with those top 8 features on which we'll run our ml algorithms ","c55d0191":"# For class S ,Q women have higher chance of survival \n# men have higher chance in port C","37a2cfc1":"# lets create new features like age_class and Fare Per Person","a8e6effe":"# Applying Decision Tree","870432d9":"# 9 is the best K value for KNN, as for k=9 we got the lowest error possible","059da276":"# Handling missing values of Embarked","6df8f8b1":"# lets use the designation of name as a feature","e2e3c332":"# women have higher chance of survival between 14 and 40 while chance of survival for men is very very low from 15 to 40 and also after 40 to 65 their chance is low infants in both of these cases have high survival chance","2a8c401f":"# splitting the dataset in train and test","e618a6ec":"# Lets drop Passengerid","47110301":"# Lets first use the cross_validation to determine the right value of k for our KNN model and then we'll use that k to predict our class label by KNN","de0d8548":"# This is train accuracy","a4c61094":"# Fare is of float type so we need to convert to int and also there are 4 categorical features Name,Sex,Age,Ticket and Embarked","dcc9376c":"# Visualizing the data","ba3c8129":"# catgorizing age into different categories","540c86f7":"# standardizing the data both train and test","cd035949":"### standardizing train data","ef3ad2b2":"# lets create new Features","86257a21":"## lots of unique features so lets drop them","e1869b08":"# converting sex to int","f0863295":"# Lets handle the missing cabin data","aad89afe":"# 1st class has the highest survival chance followed by 2 and 3","8b0ae7af":"# Lets determine the best no of trees in Random Forest","0cfd4bf7":"# Applying SGD","cdfa192f":"# Well higher no of relatives of more than 3 has lower survival chance","12691ec2":"# Handling missing values of Age"}}