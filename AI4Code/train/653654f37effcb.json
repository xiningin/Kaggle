{"cell_type":{"c2b1ee68":"code","5a40c201":"code","4ec8eeb5":"code","d23452c3":"code","81b8dc5b":"code","bcc1c98a":"code","99606454":"code","c49dfbd7":"code","64be5421":"code","21a279cf":"code","1c7f080f":"code","bd00841e":"markdown","f0140596":"markdown","bbfce0b3":"markdown","b50b2e40":"markdown","8b8dae2b":"markdown","29d20e82":"markdown","02f3cf1a":"markdown","f846f616":"markdown","fbb1fd58":"markdown"},"source":{"c2b1ee68":"import datetime\nimport uuid\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport sys\nimport subprocess\n\nfrom tensorflow.keras import datasets, layers, models\nfrom sklearn.model_selection import train_test_split\n\n# If in Colab install tensorflow_cloud\nif \"google.colab\" in sys.modules and not os.environ.get(\n    \"TF_KERAS_RUNNING_REMOTELY\"):\n    subprocess.run(\n        ['python3', '-m', 'pip', 'install', 'tensorflow-cloud', '-q'])\n\nimport tensorflow_cloud as tfc\nprint(tfc.__version__)","5a40c201":"# Set Google Cloud Specific parameters\n\n# TODO: Please set GCP_PROJECT_ID to your own Google Cloud project ID.\nGCP_PROJECT_ID = 'YOUR_PROJECT_ID'  #@param {type:\"string\"}\n\n# TODO: Change the Service Account Name to your own Service Account\nSERVICE_ACCOUNT_NAME = 'YOUR_SERVICE_ACCOUNT_NAME' #@param {type:\"string\"}\nSERVICE_ACCOUNT = f'{SERVICE_ACCOUNT_NAME}@{GCP_PROJECT_ID}.iam.gserviceaccount.com'\n\n# TODO: set GCS_BUCKET to your own Google Cloud Storage (GCS) bucket.\nGCS_BUCKET = 'YOUR_GCS_BUCKET_NAME' #@param {type:\"string\"}\n\n# DO NOT CHANGE: Currently only the 'us-central1' region is supported.\nREGION = 'us-central1'","4ec8eeb5":"# Set Tuning Specific parameters\n\n# OPTIONAL: You can change the project name to any string.\nJOB_NAME = 'cifar10' #@param {type:\"string\"}\n\n# OPTIONAL:  Set Number of concurrent tuning jobs that you would like to run.\nNUM_JOBS = 5 #@param {type:\"string\"}\n\n# TODO: Set the study ID for this run. Study_ID can be any unique string.\n# Reusing the same Study_ID will cause the Tuner to continue tuning the\n# Same Study parameters. This can be used to continue on a terminated job,\n# or load stats from a previous study.\n# Note Study ID MUST NOT be auto generated for example using random or time,\n# as at run time each job will rerun this code and will create a separate study.\nSTUDY_NUMBER = '00001' #@param {type:\"string\"}\nSTUDY_ID = f'{GCP_PROJECT_ID}_{JOB_NAME}_{STUDY_NUMBER}'\n\n# Setting location were training logs and checkpoints will be stored\nGCS_BASE_PATH = f'gs:\/\/{GCS_BUCKET}\/{JOB_NAME}\/{STUDY_ID}'\nTENSORBOARD_LOGS_DIR = os.path.join(GCS_BASE_PATH,\"logs\")","d23452c3":"# Using tfc.remote() to ensure this code only runs in notebook\nif not tfc.remote():\n\n    # Authentication for Kaggle Notebooks\n    if \"kaggle_secrets\" in sys.modules:\n        from kaggle_secrets import UserSecretsClient\n        UserSecretsClient().set_gcloud_credentials(project=GCP_PROJECT_ID)\n\n    # Authentication for Colab Notebooks\n    if \"google.colab\" in sys.modules:\n        from google.colab import auth\n        auth.authenticate_user()\n        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID","81b8dc5b":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n# Setting input specific parameters\n# The model expects input of dimetions of (INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3)\nINPUT_IMG_SIZE = 32\nNUM_CLASSES = 10","bcc1c98a":"import kerastuner\nfrom tensorflow.keras import layers\n\n# Configure the search space\nHPS = kerastuner.engine.hyperparameters.HyperParameters()\n\nHPS.Int('conv_blocks', 3, 5, default=3)\nfor i in range(5):\n    HPS.Int('filters_' + str(i), 32, 256, step=32)\n    HPS.Choice('pooling_' + str(i), ['avg', 'max'])\nHPS.Int('hidden_size', 30, 100, step=10, default=50)\nHPS.Float('dropout', 0, 0.5, step=0.1, default=0.5)\nHPS.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n\ndef build_model(hp):\n    inputs = tf.keras.Input(shape=(INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3))\n    x = inputs\n    for i in range(hp.get('conv_blocks')):\n        filters = hp.get('filters_'+ str(i))\n        for _ in range(2):\n            x = layers.Conv2D(\n              filters, kernel_size=(3, 3), padding='same')(x)\n            x = layers.BatchNormalization()(x)\n            x = layers.ReLU()(x)\n        if hp.get('pooling_' + str(i)) == 'max':\n            x = layers.MaxPool2D()(x)\n        else:\n            x = layers.AvgPool2D()(x)\n    x = layers.GlobalAvgPool2D()(x)\n    x = layers.Dense(hp.get('hidden_size'),\n      activation='relu')(x)\n    x = layers.Dropout(hp.get('dropout'))(x)\n    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs, outputs)\n    model.compile(\n      optimizer=tf.keras.optimizers.Adam(\n        hp.get('learning_rate')),\n      loss='sparse_categorical_crossentropy',\n      metrics=['accuracy'])\n    return model","99606454":"from tensorflow_cloud import CloudTuner\n\ndistribution_strategy = None\nif not tfc.remote():\n    # Using MirroredStrategy to use a single instance with 4 GPUs\n    # during remote execution while using no strategy for local.\n    distribution_strategy = tf.distribute.MirroredStrategy()\n\ntuner = CloudTuner(\n    build_model,\n    project_id=GCP_PROJECT_ID,\n    project_name= JOB_NAME,\n    region=REGION,\n    objective='accuracy',\n    hyperparameters=HPS,\n    max_trials=100,\n    directory=GCS_BASE_PATH,\n    study_id=STUDY_ID,\n    overwrite=False,\n    distribution_strategy=distribution_strategy)","c49dfbd7":"# Configure Tensorboard logs\ncallbacks=[\n    tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_LOGS_DIR)]\n\n# Setting to run tuning remotely, you can run tuner locally to validate it works first.\nif tfc.remote():\n    tuner.search(x=x_train, y=y_train, epochs=30, validation_split=0.2, callbacks=callbacks)","64be5421":"if not tfc.remote():\n    print('Training on TensorFlow Cloud...')\n\n    # If you are using a custom image you can install modules via requirements txt file.\n    with open('requirements.txt','w') as f:\n        f.write('pandas==1.1.5\\n')\n        f.write('numpy==1.18.5\\n')\n        f.write('tensorflow-cloud==0.1.11\\n')\n        f.write('keras-tuner==1.0.2\\n')\n\n    # Optional: Some recommended base images. If you provide none the system will choose one for you.\n    TF_GPU_IMAGE= \"tensorflow\/tensorflow:latest-gpu\"\n    TF_CPU_IMAGE= \"tensorflow\/tensorflow:latest\"\n\n\n    tfc.run_cloudtuner(\n        distribution_strategy='auto',\n        requirements_txt='requirements.txt',\n        docker_config=tfc.DockerConfig(\n            parent_image=TF_GPU_IMAGE,\n            image_build_bucket=GCS_BUCKET\n            ),\n        chief_config=tfc.COMMON_MACHINE_CONFIGS['K80_4X'],\n        job_labels={'job': JOB_NAME},\n        service_account=SERVICE_ACCOUNT,\n        num_jobs=NUM_JOBS\n    )","21a279cf":"# %load_ext tensorboard\n# %tensorboard --logdir TENSORBOARD_LOGS_DIR","1c7f080f":"if not tfc.remote():\n    tuner.results_summary(1)\n    best_model = tuner.get_best_models(1)[0]\n    best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n\n    # References to best trial assets\n    best_trial_id = tuner.oracle.get_best_trials(1)[0].trial_id\n    best_trial_dir = tuner.get_trial_dir(best_trial_id)","bd00841e":"You can access the training assets as follows:","f0140596":"## Authenticating the notebook to use your Google Cloud Project\n\nFor Kaggle Notebooks click on \"Add-ons\"->\"Google Cloud SDK\" before running the cell below.","bbfce0b3":"## Define the model architecture and hyperparameters","b50b2e40":"# Training Results\nWhile the training is in progress you can use Tensorboard to view the results.","8b8dae2b":"# HP Tuning CIFAR10 on Google Cloud with tensorflow_cloud and CloudTuner\n\nThis example is based on [Keras-Tuner CIFAR10 sample](https:\/\/github.com\/keras-team\/keras-tuner\/blob\/master\/examples\/cifar10.py) to demonstrate how to run HP tuning jobs using\n[tensorflow_cloud](https:\/\/github.com\/tensorflow\/cloud) and Google Cloud Platform at scale.\n\n<table align=\"left\">\n    <td>\n        <a href=\"https:\/\/colab.research.google.com\/github\/tensorflow\/cloud\/blob\/master\/examples\/hp_tuning_cifar10_using_google_cloud.ipynb\">\n            <img width=\"50\" src=\"https:\/\/cloud.google.com\/ml-engine\/images\/colab-logo-32px.png\" alt=\"Colab logo\">Run in Colab\n        <\/a>\n    <\/td>\n    <td>\n        <a href=\"https:\/\/github.com\/tensorflow\/cloud\/blob\/master\/examples\/hp_tuning_cifar10_using_google_cloud.ipynb\">\n            <img src=\"https:\/\/cloud.google.com\/ml-engine\/images\/github-logo-32px.png\" alt=\"GitHub logo\">View on GitHub\n        <\/a>\n     <\/td>\n    <td>\n        <a href=\"https:\/\/www.kaggle.com\/nitric\/hp-tuning-cifar10-using-google-cloud\">\n            <img width=\"90\" src=\"https:\/\/www.kaggle.com\/static\/images\/site-logo.png\" alt=\"Kaggle logo\">Run in Kaggle\n        <\/a>\n     <\/td>\n<\/table>","29d20e82":"## Configure a CloudTuner\nIn this section we configure the cloud tuner for both remote and local execution. The main difference between the two is the distribution strategy.","02f3cf1a":"## Load and prepare data\nRead raw data and split to train and test data sets.","f846f616":"Set project parameters For Google Cloud Specific parameters refer to [Google Cloud Project Setup Instructions](https:\/\/www.kaggle.com\/nitric\/google-cloud-project-setup-instructions\/).","fbb1fd58":"## Start the remote training\n\nThis step will prepare your code from this notebook for remote execution and start NUM_JOBS parallel runs remotely to train the model. Once the jobs are submitted you can go to the next step to monitor the jobs progress via Tensorboard.\n"}}