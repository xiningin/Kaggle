{"cell_type":{"e344b3ce":"code","75a1787d":"code","ab19b80f":"code","0bb5209b":"code","dd3298ae":"code","2731e47e":"code","0c893b17":"code","ec9255dd":"code","1b151ced":"code","aaa35a69":"code","2b5ccf7e":"code","432f83a0":"code","8012d2ef":"code","11b1ec44":"code","fd2b2e14":"code","022c2988":"code","8459415c":"code","ab415541":"code","bb9750fc":"code","81caebff":"code","a1f831ad":"code","21eba4ed":"code","c46d0f44":"code","a70a63a1":"code","0c40ccba":"code","4bc47d67":"code","20f4164a":"code","a41d7809":"code","9a5aad11":"code","f89582eb":"code","994c9f6a":"code","43d4e059":"code","34296823":"code","4f19f465":"markdown","965c26db":"markdown","59a73a24":"markdown","09d6f785":"markdown","eedabbdc":"markdown","00a2aac0":"markdown","c13b3eb6":"markdown","c5caef8d":"markdown","fc2d7701":"markdown","40818eab":"markdown","bb6a2fc0":"markdown","7effb8d6":"markdown","f08fbf0e":"markdown","2280be39":"markdown","6572a1bc":"markdown","e365e36f":"markdown","5f4ce59c":"markdown","0d71555f":"markdown","60c32442":"markdown","25e680ac":"markdown","b7002d78":"markdown","698c9413":"markdown","424c6a2b":"markdown","15af5af7":"markdown","efb02d26":"markdown","a45b535e":"markdown","65908c56":"markdown","8783fe4b":"markdown","e6aa32e7":"markdown","2d6674a3":"markdown","a8fcd3fb":"markdown","fd80d348":"markdown","ab2ed1e2":"markdown","38520514":"markdown","31e4d9d4":"markdown","5fda37bc":"markdown","4f3492fe":"markdown","21e0a533":"markdown","ff777d99":"markdown","083adc9f":"markdown","144c8b52":"markdown","a5ddcdb6":"markdown","671b7bc8":"markdown","a66aa1ef":"markdown","77de8fad":"markdown","651e081f":"markdown","5fc85a82":"markdown","b183097b":"markdown","aec7afb1":"markdown","5ebf84dc":"markdown","a8c27465":"markdown","69103720":"markdown","e8203049":"markdown","24169d06":"markdown","e09843b4":"markdown","6f9c4cc4":"markdown","a696799e":"markdown","761a9ded":"markdown","88a3e3dd":"markdown"},"source":{"e344b3ce":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(1)\nseed(1)","75a1787d":"train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])","ab19b80f":"train.describe()","0bb5209b":"train.head()","dd3298ae":"print('Min date from train set: %s' % train['date'].min().date())\nprint('Max date from train set: %s' % train['date'].max().date())","2731e47e":"lag_size = (test['date'].max().date() - train['date'].max().date()).days\nprint('Max date from train set: %s' % train['date'].max().date())\nprint('Max date from test set: %s' % test['date'].max().date())\nprint('Forecast lag size', lag_size)","0c893b17":"daily_sales = train.groupby('date', as_index=False)['sales'].sum()\nstore_daily_sales = train.groupby(['store', 'date'], as_index=False)['sales'].sum()\nitem_daily_sales = train.groupby(['item', 'date'], as_index=False)['sales'].sum()","ec9255dd":"daily_sales_sc = go.Scatter(x=daily_sales['date'], y=daily_sales['sales'])\nlayout = go.Layout(title='Daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=[daily_sales_sc], layout=layout)\niplot(fig)","1b151ced":"store_daily_sales_sc = []\nfor store in store_daily_sales['store'].unique():\n    current_store_daily_sales = store_daily_sales[(store_daily_sales['store'] == store)]\n    store_daily_sales_sc.append(go.Scatter(x=current_store_daily_sales['date'], y=current_store_daily_sales['sales'], name=('Store %s' % store)))\n\nlayout = go.Layout(title='Store daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=store_daily_sales_sc, layout=layout)\niplot(fig)","aaa35a69":"item_daily_sales_sc = []\nfor item in item_daily_sales['item'].unique():\n    current_item_daily_sales = item_daily_sales[(item_daily_sales['item'] == item)]\n    item_daily_sales_sc.append(go.Scatter(x=current_item_daily_sales['date'], y=current_item_daily_sales['sales'], name=('Item %s' % item)))\n\nlayout = go.Layout(title='Item daily sales', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=item_daily_sales_sc, layout=layout)\niplot(fig)","2b5ccf7e":"train = train[(train['date'] >= '2017-01-01')]","432f83a0":"train_gp = train.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\ntrain_gp = train_gp.agg({'sales':['mean']})\ntrain_gp.columns = ['item', 'store', 'date', 'sales']\ntrain_gp.head()","8012d2ef":"def series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Target timestep (t=lag)\n    cols.append(data.shift(-lag))\n    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","11b1ec44":"window = 29\nlag = lag_size\nseries = series_to_supervised(train_gp.drop('date', axis=1), window=window, lag=lag)\nseries.head()","fd2b2e14":"last_item = 'item(t-%d)' % window\nlast_store = 'store(t-%d)' % window\nseries = series[(series['store(t)'] == series[last_store])]\nseries = series[(series['item(t)'] == series[last_item])]","022c2988":"columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['item', 'store']]\nfor i in range(window, 0, -1):\n    columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\nseries.drop(columns_to_drop, axis=1, inplace=True)\nseries.drop(['item(t)', 'store(t)'], axis=1, inplace=True)","8459415c":"# Label\nlabels_col = 'sales(t+%d)' % lag_size\nlabels = series[labels_col]\nseries = series.drop(labels_col, axis=1)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(series, labels.values, test_size=0.4, random_state=0)\nprint('Train set shape', X_train.shape)\nprint('Validation set shape', X_valid.shape)\nX_train.head()","ab415541":"epochs = 40\nbatch = 256\nlr = 0.0003\nadam = optimizers.Adam(lr)","bb9750fc":"model_mlp = Sequential()\nmodel_mlp.add(Dense(100, activation='relu', input_dim=X_train.shape[1]))\nmodel_mlp.add(Dense(1))\nmodel_mlp.compile(loss='mse', optimizer=adam)\nmodel_mlp.summary()","81caebff":"mlp_history = model_mlp.fit(X_train.values, Y_train, validation_data=(X_valid.values, Y_valid), epochs=epochs, verbose=2)","a1f831ad":"X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\nprint('Train set shape', X_train_series.shape)\nprint('Validation set shape', X_valid_series.shape)","21eba4ed":"model_cnn = Sequential()\nmodel_cnn.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_cnn.add(MaxPooling1D(pool_size=2))\nmodel_cnn.add(Flatten())\nmodel_cnn.add(Dense(50, activation='relu'))\nmodel_cnn.add(Dense(1))\nmodel_cnn.compile(loss='mse', optimizer=adam)\nmodel_cnn.summary()","c46d0f44":"cnn_history = model_cnn.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, verbose=2)","a70a63a1":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(50, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(loss='mse', optimizer=adam)\nmodel_lstm.summary()","0c40ccba":"lstm_history = model_lstm.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=epochs, verbose=2)","4bc47d67":"subsequences = 2\ntimesteps = X_train_series.shape[1]\/\/subsequences\nX_train_series_sub = X_train_series.reshape((X_train_series.shape[0], subsequences, timesteps, 1))\nX_valid_series_sub = X_valid_series.reshape((X_valid_series.shape[0], subsequences, timesteps, 1))\nprint('Train set shape', X_train_series_sub.shape)\nprint('Validation set shape', X_valid_series_sub.shape)","20f4164a":"model_cnn_lstm = Sequential()\nmodel_cnn_lstm.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, X_train_series_sub.shape[2], X_train_series_sub.shape[3])))\nmodel_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel_cnn_lstm.add(TimeDistributed(Flatten()))\nmodel_cnn_lstm.add(LSTM(50, activation='relu'))\nmodel_cnn_lstm.add(Dense(1))\nmodel_cnn_lstm.compile(loss='mse', optimizer=adam)","a41d7809":"cnn_lstm_history = model_cnn_lstm.fit(X_train_series_sub, Y_train, validation_data=(X_valid_series_sub, Y_valid), epochs=epochs, verbose=2)","9a5aad11":"fig, axes = plt.subplots(2, 2, sharex=True, sharey=True,figsize=(22,12))\nax1, ax2 = axes[0]\nax3, ax4 = axes[1]\n\nax1.plot(mlp_history.history['loss'], label='Train loss')\nax1.plot(mlp_history.history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('MLP')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('MSE')\n\nax2.plot(cnn_history.history['loss'], label='Train loss')\nax2.plot(cnn_history.history['val_loss'], label='Validation loss')\nax2.legend(loc='best')\nax2.set_title('CNN')\nax2.set_xlabel('Epochs')\nax2.set_ylabel('MSE')\n\nax3.plot(lstm_history.history['loss'], label='Train loss')\nax3.plot(lstm_history.history['val_loss'], label='Validation loss')\nax3.legend(loc='best')\nax3.set_title('LSTM')\nax3.set_xlabel('Epochs')\nax3.set_ylabel('MSE')\n\nax4.plot(cnn_lstm_history.history['loss'], label='Train loss')\nax4.plot(cnn_lstm_history.history['val_loss'], label='Validation loss')\nax4.legend(loc='best')\nax4.set_title('CNN-LSTM')\nax4.set_xlabel('Epochs')\nax4.set_ylabel('MSE')\n\nplt.show()","f89582eb":"mlp_train_pred = model_mlp.predict(X_train.values)\nmlp_valid_pred = model_mlp.predict(X_valid.values)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, mlp_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, mlp_valid_pred)))","994c9f6a":"cnn_train_pred = model_cnn.predict(X_train_series)\ncnn_valid_pred = model_cnn.predict(X_valid_series)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_valid_pred)))","43d4e059":"lstm_train_pred = model_lstm.predict(X_train_series)\nlstm_valid_pred = model_cnn.predict(X_valid_series)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, lstm_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, lstm_valid_pred)))","34296823":"cnn_lstm_train_pred = model_cnn_lstm.predict(X_train_series_sub)\ncnn_lstm_valid_pred = model_cnn_lstm.predict(X_valid_series_sub)\nprint('Train rmse:', np.sqrt(mean_squared_error(Y_train, cnn_lstm_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_valid, cnn_lstm_valid_pred)))","4f19f465":"### Comparing models","965c26db":"[Jaehoon]\n- Process each subsequence with Conv1D.\n- Process entire sequence with LSTM which gets the features from Conv1D(subsequence).\n- Get the final output of LSTM, predict the value at t+lag using Linear layer.","59a73a24":"[Jaehoon]\n- Run the above function with window=29, lag=90.\n- Create a dataset to predict the value of t+lag using t-window ~ t data.\n- print out the first row of the created dataset.","09d6f785":"### Rearrange dataset so we can apply shift methods","eedabbdc":"[Jaehoon]\n- Load the data from CSV Files. (pd.read_csv)\n- The CSV files have 4 columns, [date, store, item, sales]\n- \"parse_dates=['date']\" Converts value(string)s in 'date' column in the format of python datetime.","00a2aac0":"### Loading data","c13b3eb6":"[Jaehoon]\n- plot train loss and validation loss of each model. (MLP, CNN, LSTM, CNN-LSTM)\n\n### My Opinion\n- MLP's performance is the best among models.\n  - I think this experiment shows limit of LSTM-based (or Sliding window) models.\n  - In this task, I conjecture that the importance of old data is much higher than recent data.\n  - As MLP does not forget the old values, it worked best. Also CNN model works well because it also observes whole sequence without forgetting.\n  - However, MLP cannot be generalized to input sequence length.\n- CNN-LSTM's stable fitting\n  - I think Conv1d on subsequence smoothes the noise in each time-step.\n  - Therefore, CNN-LSTM fits stably than LSTM model.","c5caef8d":"[Jaehoon]\n- Set Training Configuration\n- 40 epochs with minibatch_size=256, learning_rate = 0.0003 training with Adam optimizer.","fc2d7701":"[Jaehoon]\n- For each store, (ignore redundant store id by .unique() function)\n    - Scatter the number of sales in a day of given store. Make date-sales graph for each store.\n- layout: specify the title of the figure, the graph's x axis and y axis.\n- fig: joint the graph and the title layout.\n- plot the figure on ipynb.","40818eab":"[Jaehoon]\n- printout the root mean square error of the LSTM model on Train \/ Val dataset","bb6a2fc0":"[Jaehoon]\n- Get statistic features of given data.\n- Compute count, mean, std, min\/max, quantile values.","7effb8d6":"### CNN for Time Series Forecasting\n\n* For the CNN model we will use one convolutional hidden layer followed by a max pooling layer. The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n* The convolutional layer should be able to identify patterns between the timesteps.\n* Input shape **[samples, timesteps, features]**.\n\n#### Data preprocess\n* Reshape from [samples, timesteps] into [samples, timesteps, features].\n* This same reshaped data will be used on the CNN and the LSTM model.","f08fbf0e":"#### We will use the current timestep and the last 29 to forecast 90 days ahead","2280be39":"[Jaehoon]\n- Get train data which is more recent than 2017-01-01","6572a1bc":"### Overall daily sales","e365e36f":"#### Remove unwanted columns","5f4ce59c":"[Jaehoon]\n- Define t+lag values to target values, and drop the target values from inputs.\n- split train and validation data. (6:4 split)\n- print out the input_shape of trainset, val_set.\n- print out first 5 rows from trainset_input.","0d71555f":"### Train\/validation split","60c32442":"[Jaehoon]\n- daily_sales: sum of sales in each day.\n- store_daily_sales: sum of sales in each day, at each store.\n- item_daily_sales: total sales of each item in each day.","25e680ac":"[Jaehoon]\n- Let n=window.\n  - shift(i) shifts the row of data with i steps. Fill NaN value for empty slots.\n  - By shifting and concatenating the columns, we can use single row as training data sample. It contains the data from t-n to t, and target value t+lag.\n  - Drop rows with NaN values which was created by shift() function.\n  - After droping rows, the data starts with index == window.","b7002d78":"[Jaehoon]\n- Get minimum(oldest) and maximum(latest) date from the data.","698c9413":"### LSTM for Time Series Forecasting\n\n* Now the LSTM model actually sees the input data as a sequence, so it's able to learn patterns from sequenced data (assuming it exists) better than the other ones, especially patterns from long sequences.\n* Input shape **[samples, timesteps, features]**.","424c6a2b":"### Basic EDA\n\nTo explore the time series data first we need to aggregate the sales by day","15af5af7":"[Jaehoon]\n- collect values of each (item, store, date) by groupby(). sort the data by 'date'.\n- get the mean 'sales' of each (item, store, date).\n- set the layout of dataframe (4 columns, [item, store, date, sales])\n- print out first 5 rows","efb02d26":"[Jaehoon]\n- Scatter the number of sales in a day. Plot the date-sales graph.\n- layout: specify the title of the figure, the graph's x axis and y axis.\n- fig: joint the graph and the title layout.\n- plot the figure on ipynb.","a45b535e":"[Jaehoon]\n- For each item, (ignore redundant item id by .unique() function)\n    - Scatter the number of sales in a day of given store. Make date-sales graph for each store.\n- layout: specify the title of the figure, the graph's x axis and y axis.\n- fig: joint the graph and the title layout.\n- plot the figure on ipynb.","65908c56":"[Jaehoon]\n- printout the root mean square error of the MLP model on Train \/ Val dataset","8783fe4b":"#### Let's find out what's the time gap between the last day from training set from the last day of the test set, this will be out lag (the amount of day that need to be forecast)","e6aa32e7":"[Jaehoon]\n- Define a MLP model.\n- 2 layer MLP with ReLU activation, trained with mse loss.","2d6674a3":"[Jaehoon]\n- drop the information about item and store.\n- We only want to model the sales value.","a8fcd3fb":"[Jaehoon]\n- Construct a LSTM model.\n- Process the Sequential model with LSTM.\n- Predict t+lag value by process the last output of LSTM.","fd80d348":"### CNN-LSTM for Time Series Forecasting\n* Input shape **[samples, subsequences, timesteps, features]**.\n\n#### Model explanation from the [article](https:\/\/machinelearningmastery.com\/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course\/)\n> \"The benefit of this model is that the model can support very long input sequences that can be read as blocks or subsequences by the CNN model, then pieced together by the LSTM model.\"\n>\n> \"When using a hybrid CNN-LSTM model, we will further divide each sample into further subsequences. The CNN model will interpret each sub-sequence and the LSTM will piece together the interpretations from the subsequences. As such, we will split each sample into 2 subsequences of 2 times per subsequence.\"\n>\n> \"The CNN will be defined to expect 2 timesteps per subsequence with one feature. The entire CNN model is then wrapped in TimeDistributed wrapper layers so that it can be applied to each subsequence in the sample. The results are then interpreted by the LSTM layer before the model outputs a prediction.\"\n\n#### Data preprocess\n* Reshape from [samples, timesteps, features] into [samples, subsequences, timesteps, features].","ab2ed1e2":"### Time period of the train dataset","38520514":"### Conclusion\n\nHere you could see some approaches to a time series problem, how to develop and the differences between them, this is not meant to have a great performance, so if you want better results, you are more than welcomed to try a few different hyper-parameters, especially the window size and the networks topology, if you do, please let me know the results.\n\nI hope you learned a few things here, leave a feedback and if you liked what you saw make sure to check the [article](https:\/\/machinelearningmastery.com\/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course\/) that I used as source.\n\nIf you want to check out how you can use LSTM as autoencoders and create new features that represent a time series take a look at my other kernel [Time-series forecasting with deep learning & LSTM autoencoders](https:\/\/www.kaggle.com\/dimitreoliveira\/time-series-forecasting-with-lstm-autoencoders\/data).","31e4d9d4":"[Jaehoon]\n- printout the root mean square error of the CNN-LSTM model on Train \/ Val dataset","5fda37bc":"#### CNN on train and validation","4f3492fe":"### MLP for Time Series Forecasting\n\n* First we will use a Multilayer Perceptron model or MLP model, here our model will have input features equal to the window size.\n* The thing with MLP models is that the model don't take the input as sequenced data, so for the model, it is just receiving inputs and don't treat them as sequenced data, that may be a problem since the model won't see the data with the sequence patter that it has.\n* Input shape **[samples, timesteps]**.","21e0a533":"### Daily sales by store","ff777d99":"[Jaehoon]\n- Train the above CNN-LSTM model.","083adc9f":"### Transform the data into a time series problem","144c8b52":"[Jaehoon]\n- Reshape the data to fit CNN-LSTM model.\n- Split the entire sequence to sequence of 2-step subsequences. ","a5ddcdb6":"[Jaehoon]\n- printout the root mean square error of the CNN model on Train \/ Val dataset","671b7bc8":"<h2><center>Deep Learning for Time Series Forecasting<\/center><\/h2>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/Store%20Item%20Demand%20Forecasting%20Challenge\/time-series%20graph.png\" width=\"800\">\n\n### The goal of this notebook is to develop and compare different approaches to time-series problems.\n\n#### Content:\n* [Time series visualization with ploty](#Basic-EDA).\n* [How to transform a time series dataset into a supervised learning problem](#Transform-the-data-into-a-time-series-problem).\n* [How to develop a Multilayer Perceptron model for a univariate time series forecasting problem](#MLP-for-Time-Series-Forecasting).\n* [How to develop a Convolutional Neural Network model for a univariate time series forecasting problem](#CNN-for-Time-Series-Forecasting).\n* [How to develop a Long Short-Term Memory network model for a univariate time series forecasting problem](#LSTM-for-Time-Series-Forecasting).\n* [How to develop a Hybrid CNN-LSTM model for a univariate time series forecasting problem](#CNN-LSTM-for-Time-Series-Forecasting).\n\n#### The content here was inspired by this article at **machinelearningmastery.com**, [How to Get Started with Deep Learning for Time Series Forecasting (7-Day Mini-Course)](https:\/\/machinelearningmastery.com\/how-to-get-started-with-deep-learning-for-time-series-forecasting-7-day-mini-course\/)\n\n#### Dependencies","a66aa1ef":"#### LSTM on train and validation","77de8fad":"### Train set","651e081f":"#### MLP on train and validation","5fc85a82":"[Jaehoon]\n- fit the mlp model on trainset. test the performance with validation_data.","b183097b":"#### Sub-sample train set to get only the last year of data and reduce training time","aec7afb1":"[Jaehoon]\n- Construct a CNN Model.\n- Use Conv1D and Maxpool to get a feature of given sequential data. Process this hidden feature with 2 Layer MLP. Predict the value of t+lag, and training the model with mse loss.","5ebf84dc":"[Jaehoon]\n- Train the above CNN model with trainset. Validate with the validation_set.","a8c27465":"[Jaehoon]\n- filter the data to have same item-id, store-id over a column.","69103720":"[Jaehoon]\n- reshape the data as sequential data with scalar feature.\n- printout the data shape.","e8203049":"#### CNN-LSTM on train and validation","24169d06":"[Jaehoon]\n- Train the above LSTM model.","e09843b4":"[Jaehoon]\n* Load necessary modules.\n* numpy: array processing\n* pandas: DataFrame\n* matplotlib, plotly: visualization\n* keras, tensorflow: ML\n* sklearn: data processing utils","6f9c4cc4":"#### Drop rows with different item or store values than the shifted columns","a696799e":"[Jaehoon]\n- Get first 5 (if not clarifying the number to get) rows from given DataFrame.","761a9ded":"### Daily sales by item","88a3e3dd":"[Jaehoon]\n- lag_size means the time gap between train data and test data.\n- We can obtain the lag_size by compute the difference between the trainset's latest date and the testset's latest date.\n- In this case, we need to predict the values of lag_size==90 days."}}