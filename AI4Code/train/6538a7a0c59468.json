{"cell_type":{"5ae11da7":"code","eb3f8584":"code","e35d7dcf":"code","b6cdea04":"code","3d2d3c81":"code","9f72137a":"code","a30c8178":"code","86825515":"code","c7a5feae":"code","43fcec34":"code","eb38e23f":"markdown","f0636688":"markdown","95e37842":"markdown","ecf603d8":"markdown","00162a70":"markdown","a5bbdbcb":"markdown","9fba4177":"markdown","1b33e1dd":"markdown","8c0b0395":"markdown","c391c869":"markdown","a42c4f59":"markdown","d4397217":"markdown"},"source":{"5ae11da7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb3f8584":"train_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\nsubmission_data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')","e35d7dcf":"print(train_data.isna().sum(), '\\n', submission_data.isna().sum())","b6cdea04":"print(train_data['pubchem_id'].value_counts())","3d2d3c81":"train_data = train_data.drop(columns=['id', 'pubchem_id'])\nsubmission_data = submission_data.drop(columns=['pubchem_id'])","9f72137a":"train_labels = train_data['Eat']\ntrain_data = train_data.drop(columns=['Eat'])\n\nprint(train_data.shape, '\\n', train_labels.shape)","a30c8178":"from keras.models import Sequential\nfrom keras.layers import Dense, LayerNormalization\nfrom keras.metrics import RootMeanSquaredError\nfrom keras.callbacks import History, EarlyStopping\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\ntrain_data, test_data, train_labels, test_labels = train_test_split(train_data, train_labels, test_size=0.2)\n\nmodel_1 = Sequential()\nhistory = History()\noptimizer = RMSprop(0.001)\n\nmodel_1.add(LayerNormalization())\nmodel_1.add(Dense(256, activation='relu', input_shape=(1275,)))\nmodel_1.add(Dense(128, activation='relu'))\nmodel_1.add(Dense(1))\n\nmodel_1.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\nhistory = model_1.fit(train_data, train_labels, epochs=100, validation_split=0.1)\n\nhistory_df = pd.DataFrame(history.history['root_mean_squared_error'])\n\nplt.plot(history_df)\nplt.show()\nprint(history_df.describe())","86825515":"from keras.layers import Dropout\n\nmodel_2 = Sequential()\nhistory = History()\noptimizer = Adam(0.01)\n\nmodel_2.add(LayerNormalization())\nmodel_2.add(Dense(1024, activation='relu', input_shape=(1275,)))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(256, activation='relu'))\nmodel_2.add(Dense(128, activation='relu'))\nmodel_2.add(Dense(32, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(Dense(1))\n\nmodel_2.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\nhistory = model_2.fit(train_data, train_labels, epochs=100, validation_split=0.1)\n\nhistory_df = pd.DataFrame(history.history['root_mean_squared_error'])\n\nplt.plot(history_df)\nplt.show()\nprint(history_df.describe())","c7a5feae":"model_3 = Sequential()\nhistory = History()\noptimizer = Adam(0.001)\nearly_stopping = EarlyStopping(patience=2)\n\nmodel_3.add(LayerNormalization())\nmodel_3.add(Dense(500, activation='sigmoid', input_shape=(1275,)))\nmodel_3.add(Dropout(0.3))\nmodel_3.add(Dense(100, activation='sigmoid'))\nmodel_3.add(Dense(60, activation='sigmoid'))\nmodel_3.add(Dense(50, activation='relu'))\nmodel_3.add(Dense(20, activation='sigmoid'))\nmodel_3.add(Dropout(0.3))\nmodel_3.add(Dense(1))\n\nmodel_3.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n\nhistory = model_3.fit(train_data, train_labels, epochs=100, validation_split=0.1, callbacks=[early_stopping])\n\nhistory_df = pd.DataFrame(history.history['root_mean_squared_error'])\n\nplt.plot(history_df)\nplt.show()\nprint(history_df.describe())","43fcec34":"submission_pred = model_1.predict(submission_data.drop(columns=['id']))\n\nsubmission_pred = submission_pred.reshape(-1)\n\noutput = pd.DataFrame({'id': submission_data['id'], 'Eat': submission_pred})\noutput.to_csv('submission_csv', index=False)\nprint(output)\nprint('Your submission was successfully saved!')","eb38e23f":"# Drop 'id', 'pubchem_id' from Data as it is not needed (only Coulomb Matrix is Needed)","f0636688":"# Split Training Data\/Labels","95e37842":"# Check for Missing Values","ecf603d8":"# Create Third Neural Network","00162a70":"# Load Data from CSV","a5bbdbcb":"*Simple 2 layer Neural Network with Normalization*","9fba4177":"*4 layer Neural Network with Normalization and Dropout between layers*","1b33e1dd":"# Create First Neural Network","8c0b0395":"# Choose Best Network For Submission","c391c869":"*5 layer Neural Network with Normalization, Dropout, and EarlyStopping between layers*","a42c4f59":"# Create Second Neural Network","d4397217":"# Check for any repeating values of 'pubchem_id' (possible duplicate entries)"}}