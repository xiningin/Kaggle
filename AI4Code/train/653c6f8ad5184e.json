{"cell_type":{"5814ebbb":"code","1aa71f93":"code","d29e9d3c":"code","cd89f6af":"code","786a3e4e":"code","94132a29":"code","b2aff4e9":"code","78eb54e7":"code","c8c7a58a":"code","02466147":"code","d9ddf75e":"code","7c22ca97":"code","5d5a8b30":"code","f3933aa6":"markdown","98db4cef":"markdown","5c87064a":"markdown","8ca78c32":"markdown","06a60ec9":"markdown","670d99f3":"markdown","2b9d5a25":"markdown","88747cb9":"markdown","84f323ca":"markdown","48033e5f":"markdown","6ef86373":"markdown","65d80601":"markdown"},"source":{"5814ebbb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1aa71f93":"import pandas as pd","d29e9d3c":"data_df = pd.read_csv(\"\/kaggle\/input\/excess-mortality-in-europe-in-20202021\/demo_mexrt.tsv\", sep='\\t')","cd89f6af":"print(list(data_df.columns))","786a3e4e":"pivot_data_col = data_df.columns[0]\ntime_columns = data_df.columns[1:]","94132a29":"data_df['unit']     = data_df[pivot_data_col].apply(lambda x: x.split(\",\")[0])\ndata_df['country'] = data_df[pivot_data_col].apply(lambda x: x.split(\",\")[1])","b2aff4e9":"selected_columns = list(['unit', 'country']) +  list(time_columns)\ndata_df = data_df[selected_columns]","78eb54e7":"data_tr_df = data_df.melt(id_vars=['unit', 'country'], \n        var_name=\"date\", \n        value_name=\"value\")\ndata_tr_df['value'] = data_tr_df['value'].apply(lambda x: str(x).replace(\"p\", \"\"))\ndata_tr_df['value'] = data_tr_df['value'].apply(lambda x: str(x).replace(\": \", \"NAN\"))\ndata_tr_df['value'] = data_tr_df['value'].apply(lambda x: float(x))","c8c7a58a":"from datetime import datetime\ndef strip_date(date_string, test=False):\n    year, month = int(date_string[0:4]), int(date_string[5:7])\n    if test:\n        print(f\"From: {date_string} -> Year: {year}, Month: {month}\")\n    try:\n        d = datetime(year, month, 1)\n        return d\n    except Exception as ex:\n        print(\"Error, wrong data: \", year, month)\n        return None\n    \n\nprint(f\"Tests:\") \nstrip_date('2021M06 ', test=True)\nstrip_date('2019M13 ', test=True)\nstrip_date('1971M01 ', test=True)\n\nprint(f\"\\nFull data processing...\\n\")\ndata_tr_df['date'] = data_tr_df['date'].apply(lambda x: strip_date(x))\nprint(\"done.\")","02466147":"print(f\"Transformed data shape: {data_tr_df.shape} (rows\/columns)\")\ndata_tr_df.head()","d9ddf75e":"data_tr_df.tail()","7c22ca97":"import pandas_profiling\npandas_profiling.ProfileReport(data_tr_df)","5d5a8b30":"data_tr_df.to_csv(\"excess_mortality_eu.csv\", index=False)","f3933aa6":"## Load the data\n\nThe datafiles are in TSV format. We will read the files using pandas, just include in the function call the `sep` (tab separator data).\nWe demonstrate first how to read and process the Annual data.","98db4cef":"# Analysis preparation\n\n## Load packages","5c87064a":"We select now only the new columns resulted from splitting the `pivot_data_col` and the time columns.","8ca78c32":"Let's glimpse the data columns.","06a60ec9":"# Introduction\n\nWe show in this Kernel how we can process the data to prepare it for easier further processing. \nWe also use this Kernel to generate the transformed data, from the original one. \nLet's check the data files.","670d99f3":"The first column is a composed one, containing 2 different information (the unit and the geography). The next columns are the year\/month value, from current available month in 2021 to first in 2020.","2b9d5a25":"# Save transformed data","88747cb9":"Then, we split from `pivot_data_col` the 2 separate fields:\n* unit (NAC only);\n* geography.","84f323ca":"Let's inspect the result.","48033e5f":"Next, we pivot the time columns using `melt` operation in pandas.  \nWe also make sure we transform `date` to be an integer (here is a year data).  \nWe set `value` to be a float, after we replace \": \" (for N\/A) with `NAN`.","6ef86373":"# A very preliminary exploratory data analysis\n\nThis would be a very short exploratory data analysis. The role of this Kernel is just to show how we can prepare the annual data for analysis and we already did this.","65d80601":"# Data pre-processing\n\nWe start by defining two working lists."}}