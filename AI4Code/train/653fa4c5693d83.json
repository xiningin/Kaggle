{"cell_type":{"7cc088a0":"code","01d8bb4f":"code","9ffe533a":"code","4038fcda":"code","45e2bdfd":"code","e01819c6":"code","538e8097":"code","cdb030c9":"code","bdcf7c91":"code","831a32a9":"code","f5f904a6":"code","d416fb4c":"code","aac18c59":"code","2089174c":"code","316f7fde":"code","6986ac61":"code","87271b2e":"code","a040f2f3":"code","a6e1b34b":"code","17ac2438":"code","86f2b005":"code","a8bcbdc1":"markdown","3fbfef78":"markdown","82e2d608":"markdown"},"source":{"7cc088a0":"!pip install -qU '..\/input\/libraries\/tokenizers-0.7.0-cp37-cp37m-linux_x86_64.whl'\n!pip install -qU '..\/input\/libraries\/pytorch_lightning-0.7.5-py3-none-any.whl'\n!pip install -qU '..\/input\/libraries\/wandb-0.8.35-py2.py3-none-any.whl'\n!pip install -qU '..\/input\/libraries\/transformers-2.8.0-py3-none-any.whl'","01d8bb4f":"%matplotlib inline\n\nimport random\nimport os\nimport gc\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport shutil\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn import model_selection\n\nfrom transformers import RobertaModel, RobertaConfig, RobertaTokenizerFast, get_linear_schedule_with_warmup\n\nimport pytorch_lightning as pl\n\nimport warnings\nwarnings.filterwarnings('ignore')","9ffe533a":"def seed_everything(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True","4038fcda":"class GlobalConfig:\n    def __init__(self):\n        self.seed = 7\n        self.path = Path('..\/input\/tweet-sentiment-extraction\/')\n        \n        self.max_length = 128\n        self.roberta_path = Path('..\/input\/roberta-base\/')\n        \n        self.num_workers = os.cpu_count()\n        self.batch_size = 64\n\n        self.accum_steps = 1\n        self.epochs = 5\n        self.warmup_steps = 0\n        self.lr = 3e-5\n        \n        self.offline = True\n        self.saved_model_path = Path('\/kaggle\/working\/final_models')\n        self.n_splits = 5\n        \n        self.saved_model_path.mkdir(parents=True, exist_ok=True)","45e2bdfd":"GCONF = GlobalConfig()\n\n# If you want to use wandb offline then login with a random 40 character string\nif not GCONF.offline: wandb.login(key=UserSecretsClient().get_secret('WANDB_KEY'))\nelse: wandb.login(key='X'*40)\n\nseed_everything(GCONF.seed)\n[f.name for f in GCONF.path.iterdir()]","e01819c6":"spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0', '\\u202f']","538e8097":"def remove_space(text):\n    for space in spaces: text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()","cdb030c9":"train_df = pd.read_csv(GCONF.path\/'train.csv')\ntrain_df = train_df.dropna().reset_index(drop=True)\nprint(train_df.shape)\ntrain_df.head()","bdcf7c91":"train_df['text'] = train_df['text'].apply(remove_space)\ntrain_df['selected_text'] = train_df['selected_text'].apply(remove_space)","831a32a9":"def get_callbacks(name):\n    mc_cb = pl.callbacks.ModelCheckpoint(\n        filepath='\/kaggle\/working\/models\/{epoch}',\n        monitor=f'avg_valid_loss_{name}',\n        mode='min',\n        save_top_k=1,\n        prefix=f'{name}_',\n        save_weights_only=True\n    )\n    \n    es_cb = pl.callbacks.EarlyStopping(\n        monitor=f'avg_valid_loss_{name}',\n        min_delta=0.,\n        patience=2,\n        verbose=1,\n        mode='min'\n    )\n    return mc_cb, es_cb\n\ndef get_best_model_fn(mc_cb, fold_i):\n    for k, v in mc_cb.best_k_models.items():\n        if (v == mc_cb.best) and Path(k).stem.startswith(str(fold_i)):\n            return k\n        \ndef move_to_device(x, device):\n    if callable(getattr(x, 'to', None)): return x.to(device)\n    if isinstance(x, (tuple, list)): return [move_to_device(o, device) for o in x]\n    elif isinstance(x, dict): return {k: move_to_device(v, device) for k, v in x.items()}\n    return x\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = self.avg = self.sum = self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","f5f904a6":"class TweetRobertaDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length, is_testing=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_testing = is_testing\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def _improve_st_span(self, start_token, end_token, input_ids, selected_text):\n        tok_st = ' '.join(self.tokenizer.tokenize(selected_text))\n        tok_ids = self.tokenizer.convert_ids_to_tokens(input_ids)\n        \n        for new_start in range(start_token, end_token+1):\n            for new_end in range(end_token, new_start-1, -1):\n                text_span = ' '.join(tok_ids[new_start:new_end+1])\n                if text_span == tok_st: return new_start, new_end\n        \n        return start_token, end_token\n    \n    def __getitem__(self, ix):\n        text = self.df.iloc[ix]['text']\n        sentiment = self.df.iloc[ix]['sentiment']\n        start_ix, end_ix = 0, 0\n        \n        encoded_text = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_token_type_ids=False,\n            return_attention_mask=False,\n            return_offsets_mapping=True\n        )\n        \n        input_ids = encoded_text['input_ids']\n        offsets = encoded_text['offset_mapping']\n\n        sentiment_map = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n        \n        init_offset = 4\n        if not self.is_testing:\n            selected_text = self.df.iloc[ix]['selected_text']\n            \n            for i in (j for j, c in enumerate(text) if c == selected_text[0]):\n                if text[i:i+len(selected_text)] == selected_text:\n                    start_ix = i\n                    end_ix = i + len(selected_text) - 1\n                    break\n            \n            for i, offset in enumerate(offsets):\n                if start_ix < offset[1]:\n                    start_token = i\n                    break\n                \n            for i, offset in enumerate(offsets):\n                if end_ix < offset[1]:\n                    end_token = i\n                    break\n                    \n            start_token, end_token = self._improve_st_span(start_token, end_token, input_ids, selected_text)        \n            start_token += init_offset\n            end_token += init_offset\n                    \n        input_ids = [0] + [sentiment_map[sentiment]] + [2]*2 + input_ids + [2]\n        attn_mask = [1]*len(input_ids)\n        token_type_ids = [0]*len(input_ids)\n        offsets = [[0, 0]]*init_offset + offsets + [(0, 0)]\n        \n        pad_len = self.max_length - len(input_ids)\n        input_ids += [0]*pad_len\n        attn_mask += [0]*pad_len\n        token_type_ids += [0]*pad_len\n        offsets += [(0, 0)]*pad_len\n        \n        input_ids, attn_mask, token_type_ids, offsets = map(torch.LongTensor, [input_ids, attn_mask, token_type_ids, offsets])\n        \n        encoded_dict = {\n            'input_ids': input_ids,\n            'attn_mask': attn_mask,\n            'token_type_ids': token_type_ids,\n            'offsets': offsets,\n            'orig_text': text,\n            'sentiment': sentiment\n        }\n        \n        if not self.is_testing:\n            start_token = torch.tensor(start_token, dtype=torch.long)\n            end_token = torch.tensor(end_token, dtype=torch.long)\n            encoded_dict['start_token'] = start_token\n            encoded_dict['end_token'] = end_token\n            encoded_dict['orig_selected_text'] = selected_text\n        \n        return encoded_dict","d416fb4c":"class TweetRobertaModel(nn.Module):\n    def __init__(self, roberta_path):\n        super().__init__()\n        \n        roberta_config = RobertaConfig.from_pretrained(roberta_path)\n        roberta_config.output_hidden_states = True\n        self.roberta = RobertaModel.from_pretrained(roberta_path, config=roberta_config)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(roberta_config.hidden_size * 2, 2)\n        \n        torch.nn.init.normal_(self.linear.weight, std=0.02)\n    \n    def forward(self, input_ids, attn_mask, token_type_ids):\n        _, _, out = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attn_mask,\n            token_type_ids=token_type_ids\n        )\n        out = torch.cat([out[-1], out[-2]], dim=2)\n        out = self.linear(self.dropout(out))\n        \n        start_logits, end_logits = out.split(1, dim=2)\n        start_logits = start_logits.squeeze(2)\n        end_logits = end_logits.squeeze(2)\n        return start_logits, end_logits","aac18c59":"class TweetRoberta(pl.LightningModule):\n    def __init__(self, roberta_path, fold_i=None, train_len=None):\n        super().__init__()\n        \n        self.fold_i = fold_i\n        self.train_len = train_len\n        \n        self.model = TweetRobertaModel(roberta_path)\n        \n        self.am_tloss = AverageMeter()\n        self.am_vloss = AverageMeter()\n        self.am_jscore = AverageMeter()\n        \n    def _compute_loss(self, start_logits, end_logits, batch):\n        ignore_ix = start_logits.size(1)\n        start_logits.clamp_(0., ignore_ix)\n        end_logits.clamp_(0., ignore_ix)\n        \n        loss_fn = nn.CrossEntropyLoss(ignore_index=ignore_ix)\n        start_loss = loss_fn(start_logits, batch['start_token'])\n        end_loss = loss_fn(end_logits, batch['end_token'])\n        return start_loss + end_loss\n    \n    def _jaccard_score(self, text1, text2):\n        a = set(text1.lower().split())\n        b = set(text2.lower().split())\n        if (len(a)==0) & (len(b)==0): return 0.5\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n    \n    def _convert_tokens_to_text(self, start_token, end_token, text, sentiment, offsets):\n        if end_token < start_token: end_token = start_token\n        filtered_text = ''\n        for i in range(start_token, end_token+1):\n            filtered_text += text[offsets[i, 0]:offsets[i, 1]]\n            if ((i+1) < len(offsets)) and (offsets[i, 1] < offsets[i+1, 0]): filtered_text += ' '\n    \n        if (sentiment == 'neutral') or (len(text.split()) < 2): filtered_text = text\n        return filtered_text\n    \n    def batch_convert_logits_to_text(self, start_logits, end_logits, batch, return_score=True):\n        filtered_texts = []\n        jac_scores = []\n        \n        start_token_preds = torch.argmax(start_logits, dim=1)\n        end_tokens_preds = torch.argmax(end_logits, dim=1)\n        \n        for i, text in enumerate(batch['orig_text']):\n            sentiment = batch['sentiment'][i]\n            start_token_pred = start_token_preds[i]\n            end_token_pred = end_tokens_preds[i]\n            filtered_text = self._convert_tokens_to_text(\n                start_token_pred, end_token_pred, text,\n                sentiment, batch['offsets'][i]\n            )\n            \n            if return_score:\n                selected_text = batch['orig_selected_text'][i]\n                jac = self._jaccard_score(selected_text.strip(), filtered_text.strip())\n                jac_scores.append(jac)\n            else:\n                filtered_texts.append(filtered_text)\n        \n        if return_score: return torch.tensor(jac_scores).mean()\n        else: return filtered_texts\n    \n    def forward(self, batch):\n        return self.model(batch['input_ids'], batch['attn_mask'], batch['token_type_ids'])\n    \n    def training_step(self, batch, batch_nb):\n        start_logits, end_logits = self.forward(batch)\n        train_loss = self._compute_loss(start_logits, end_logits, batch)\n        self.am_tloss.update(train_loss.item(), len(batch['input_ids']))\n        return {'loss': train_loss, 'log': {f'train_loss_{self.fold_i}': train_loss}}\n    \n    def validation_step(self, batch, batch_nb):\n        start_logits, end_logits = self.forward(batch)\n        start_logits, end_logits = start_logits.detach(), end_logits.detach()\n        \n        valid_loss = self._compute_loss(start_logits, end_logits, batch)\n        jac_score = self.batch_convert_logits_to_text(start_logits, end_logits, batch)\n\n        self.am_vloss.update(valid_loss.item(), len(batch['input_ids']))\n        self.am_jscore.update(jac_score, len(batch['input_ids']))\n    \n    def validation_epoch_end(self, _):\n        tqdm_dict = {\n            f'avg_train_loss_{self.fold_i}': self.am_tloss.avg,\n            f'avg_valid_loss_{self.fold_i}': self.am_vloss.avg,\n            f'avg_jaccard_{self.fold_i}': self.am_jscore.avg\n        }\n        \n        self.am_tloss.reset()\n        self.am_vloss.reset()\n        self.am_jscore.reset()\n        return {'progress_bar': tqdm_dict, 'log': tqdm_dict}\n    \n    def predict(self, batch, device, mode='test'):\n        self.eval()\n        \n        with torch.no_grad():\n            batch = move_to_device(batch, device)\n            start_logits, end_logits = self.forward(batch)\n            return start_logits.detach(), end_logits.detach()\n    \n    def configure_optimizers(self):\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_parameters = [\n            {\n                'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                'weight_decay': 1e-3\n            },\n            {\n                'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.\n            }\n        ]\n        optimizer = optim.AdamW(optimizer_parameters, lr=GCONF.lr)\n        train_steps = (self.train_len * GCONF.epochs)\/\/GCONF.accum_steps\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=int(GCONF.warmup_steps*train_steps),\n            num_training_steps=train_steps\n        )\n        \n        scheduler_dict = {\n            'scheduler': scheduler,\n            'interval': 'step'\n        }\n        return [optimizer], [scheduler_dict]","2089174c":"wandb_logger = pl.loggers.WandbLogger(\n    name='roberta_uncased',\n    save_dir='\/kaggle\/working\/',\n    project='kaggle_tweet_extraction',\n    offline=GCONF.offline\n)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntokenizer = RobertaTokenizerFast.from_pretrained(str(GCONF.roberta_path), do_lower_case=True, add_prefix_space=True)","316f7fde":"skf = model_selection.StratifiedKFold(n_splits=GCONF.n_splits, shuffle=True, random_state=GCONF.seed)\n\nfor fold_i, (train_ix, valid_ix) in enumerate(skf.split(train_df, train_df['sentiment'].values)):\n    train_ds = TweetRobertaDataset(train_df.iloc[train_ix], tokenizer, GCONF.max_length, is_testing=False)\n    train_dl = DataLoader(train_ds, batch_size=GCONF.batch_size, shuffle=True)\n    \n    valid_ds = TweetRobertaDataset(train_df.iloc[valid_ix], tokenizer, GCONF.max_length, is_testing=False)\n    valid_dl = DataLoader(valid_ds, batch_size=GCONF.batch_size*4, shuffle=False)\n\n    mc_cb, es_cb = get_callbacks(fold_i)\n    model = TweetRoberta(GCONF.roberta_path, fold_i, len(train_dl))\n    trainer = pl.Trainer(\n        checkpoint_callback=mc_cb,\n        early_stop_callback=es_cb,\n        logger=wandb_logger,\n        gpus='0',\n        max_epochs=GCONF.epochs,\n        accumulate_grad_batches=GCONF.accum_steps,\n#         fast_dev_run=True\n    )\n    trainer.fit(model, train_dl, valid_dl)\n    torch.cuda.empty_cache()\n    \n    best_model_fn = get_best_model_fn(mc_cb, fold_i)\n    \n    # currently there is a bug in PytorchLightning ModelCheckpoint callback that\n    # the parameter save_weights_only does not work properly so we will load\n    # and save the weights somewhere else to avoid disk full error\n    trainer.restore(best_model_fn, on_gpu=False)\n    torch.save(model.state_dict(), GCONF.saved_model_path\/Path(best_model_fn).name)\n    \n    model.to('cpu')\n    del train_ds, valid_ds, mc_cb, es_cb, model, trainer\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    # to avoid disk full\n    shutil.rmtree('\/kaggle\/working\/models')","6986ac61":"test_df = pd.read_csv(GCONF.path\/'test.csv')\nprint(test_df.shape)\ntest_df.head()","87271b2e":"test_df['text'] = test_df['text'].apply(remove_space)","a040f2f3":"test_ds = TweetRobertaDataset(test_df, tokenizer, GCONF.max_length, is_testing=True)\ntest_dl = DataLoader(test_ds, batch_size=GCONF.batch_size*4, shuffle=False)","a6e1b34b":"models = []\n\nfor mf_path in tqdm(GCONF.saved_model_path.iterdir()):\n    model = TweetRoberta(GCONF.roberta_path)\n    model.load_state_dict(torch.load(mf_path, map_location=lambda storage, loc: storage))\n    model.to(device)\n    models.append(model)","17ac2438":"test_results = []\n\nfor batch in tqdm(test_dl):\n    start_logits, end_logits = 0, 0\n    \n    for model in models:\n        tmp_start_logits, tmp_end_logits = model.predict(batch, device)\n        start_logits += tmp_start_logits\n        end_logits += tmp_end_logits\n    \n    start_logits \/= len(models)\n    end_logits \/= len(models)\n    pred_texts = model.batch_convert_logits_to_text(start_logits, end_logits, batch, return_score=False)\n    test_results.extend(pred_texts)","86f2b005":"sub_df = pd.read_csv(GCONF.path\/'sample_submission.csv')\nsub_df.loc[:, 'selected_text'] = test_results\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head()","a8bcbdc1":"## Testing","3fbfef78":"## 5 Fold training","82e2d608":"Trying out [PyTorchLightning](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning) along with [Wandb](https:\/\/app.wandb.ai\/) to track the experiments. Thanks to [Abhishek's Kernel](https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds) for data preprocessing steps. Currently this kernel is running in offline mode so it won't log anything to the wandb servers. But still the logs are saved in the output and can be synced later. If you want to log them live then login and configure wandb account, put the key in secrets, turn on the internet and change the `offline` parameter in `GCONF` to `False`. Train and save the weights here and move the testing code to another kernel with no internet access to make a submission."}}