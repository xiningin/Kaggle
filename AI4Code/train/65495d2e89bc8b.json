{"cell_type":{"1664c5ce":"code","0562c76c":"code","ac056140":"code","5ada4cba":"code","90fc6b12":"code","2ed42a64":"code","eb1fe609":"code","25707fcc":"code","769ea3e1":"code","f8c08bbc":"code","4f1e553d":"code","8a2113fa":"code","efb0a250":"markdown","4190dfb5":"markdown","9340edfb":"markdown","03499155":"markdown","4b51baba":"markdown","74f7051b":"markdown","c0ba032a":"markdown","e4ffb56f":"markdown"},"source":{"1664c5ce":"import numpy as np\nimport pandas as pd\n\nDATASET_DIR = '\/kaggle\/input\/titanic'\nX_tr_df = pd.read_csv(DATASET_DIR + '\/train.csv', index_col='PassengerId')\nX_ts = pd.read_csv(DATASET_DIR + '\/test.csv', index_col='PassengerId')\ny_tr = X_tr_df['Survived']\nX_tr = X_tr_df.drop(columns='Survived')\n\nprint('\\nTraining data:')\nX_tr.describe()","0562c76c":"print('\\nTest data:')\nX_ts.describe()","ac056140":"def parse_ticket(X):\n    header = []\n    number = []\n    for tt in X['Ticket']:\n        if tt == 'LINE': # for several passengers\n            header.append(tt)\n            number.append(0)\n        else:\n            space = tt.rfind(' ')\n            if space >= 0:\n                header.append(tt[:space])\n            else:\n                header.append(None)\n            number.append(int(tt[space+1:]))\n    X['Ticket_header'] = header\n    X['Ticket_number'] = number\n    X.drop(columns='Ticket', inplace=True)\n\nparse_ticket(X_tr)\nX_tr.head()","5ada4cba":"parse_ticket(X_ts)\nX_ts.head()","90fc6b12":"from sklearn.model_selection import train_test_split\nX_tr_t, X_tr_v, y_tr_t, y_tr_v = train_test_split(X_tr, y_tr, test_size=0.25)\n\ncolumns_num = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Ticket_number']\ncolumns_cat = ['Sex', 'Embarked']","2ed42a64":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer","eb1fe609":"steps_num = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n                            ('scaler', StandardScaler()),\n                           ])","25707fcc":"steps_cat = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='EMPTY')),\n                            ('encoder', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n                           ])","769ea3e1":"preprocessor = ColumnTransformer(transformers=[\n    ('numeric_transformer', steps_num, columns_num),\n    ('categorical_transformer', steps_cat, columns_cat),\n    ], sparse_threshold=0.)\nX_tr_t_pre = preprocessor.fit_transform(X_tr_t)\nX_tr_v_pre = preprocessor.transform(X_tr_v)","f8c08bbc":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\noptimizers = (['svm_rbf', SVC(kernel='rbf', gamma='scale')],\n              ['svm_poly2', SVC(kernel='poly', degree=2, gamma='scale')],\n              ['svm_poly3', SVC(kernel='poly', degree=3, gamma='scale')],\n              ['kNN3', KNeighborsClassifier(n_neighbors=3)],\n              ['kNN5', KNeighborsClassifier(n_neighbors=5)],\n              ['kNN10', KNeighborsClassifier(n_neighbors=10)],\n              ['kNN20', KNeighborsClassifier(n_neighbors=20)],\n              ['naive_bayes', BernoulliNB()],\n              ['random_forest', RandomForestClassifier(n_estimators=3)],\n              ['ada_boost', AdaBoostClassifier(base_estimator=None, n_estimators=30)])","4f1e553d":"from sklearn.model_selection import cross_validate\n\ncv_results = {}\nfor name, estimator in optimizers:\n    estimator.fit(X_tr_t_pre, y_tr_t)\n    print(f'      * {name:16}: training -> {estimator.score(X_tr_t_pre, y_tr_t):5.3}'\n          f' \/ test -> {estimator.score(X_tr_v_pre, y_tr_v):5.3}')\ndel name\ndel estimator","8a2113fa":"X_tr_pre = preprocessor.transform(X_tr)\nX_ts_pre = preprocessor.transform(X_ts)\nfor name, estimator in optimizers:\n    estimator.fit(X_tr_pre, y_tr)\n    out = pd.DataFrame({'PassengerID': X_ts.index,\n                        'Survived': estimator.predict(X_ts_pre)})\n    out.to_csv(f'model_v1_{name}.csv', index=False)","efb0a250":"# Categorical features\n\n## Missing values:\n   Missing values are treated as \"empty\" here. This must NOT an optimal solution,\n   but it avoids errors at least.\n   \n## One-hot encoding:\n   One-hot encoder is used for each categorical feauture.\n   This method is accceptable as the first trial\n   since we are currently dealing with rather simple set of values:\n   * sex ( one out of two )\n   * embarked ( one out of three )","4190dfb5":"# Accuracy\n\n1. Support vector machines in general give us pretty good test performance.\n2. Random forest has a significant difference between training and test accuracy,\n   which indicate the possibility of over-fitting.","9340edfb":"# Optimizers to try:\n## Support vector machine\n  1. Radial basis function kernel\n  2. second-order polynomial\n  3. third-order polynomial\n\n## k-Nearest Neighbors\n  1. k=3 (easier to keep track of training samples, prone to over-fitting)\n  2. k=5\n  3. k=10\n  4. k=20 (safer against over-fitting, more computation needed to predict)\n\n## Naive Bayes\n## Random forest\n## Ada Boost","03499155":"# Preprocessing -- Column transform\n\nEach column is transormed according to its type: numeric or categorical","4b51baba":"# Numeric features\n\n## Missing values:\n   There may be missing values (often expressed as NaN) in each of the numeric feature.\n   One of the simplest solutions to these kind of errors is\n   to impute all missing values with their average or median value.\n   We will try the median value of each feature this time.\n\n## Scaling:\n   Many of the algorithms used in machine learning is NOT scale-invariant.\n   Scaling each feature so that its average is 0 and its standard deviation is 1\n   is necessary for those \"scale-variant\" algorithms to work.","74f7051b":"# Classification of each feature as either of:\n* *numeric*\n* *categorical*","c0ba032a":"# Input data\n\n* Data under \/kaggle\/input\/titanic\n* Loaded as Pandas DataFrames","e4ffb56f":"# Final models with all training data used"}}