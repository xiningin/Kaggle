{"cell_type":{"195f2772":"code","50915cf1":"code","a9501a92":"code","0dbbdcc3":"code","4d6af75f":"code","2841bb6b":"code","4fed151f":"code","d3ff4d6d":"code","16bc75a7":"code","3ff1021c":"markdown","acdbad03":"markdown","19d5f518":"markdown","da43a4fd":"markdown","05ae6027":"markdown","42737318":"markdown","ed13ae7a":"markdown","2b1a4d18":"markdown","b70666bd":"markdown","a5f7f66f":"markdown","1b3bb6b7":"markdown","b5fbfc54":"markdown","865efe2d":"markdown","29497f80":"markdown","55d665ec":"markdown","11a9daf3":"markdown","8f432136":"markdown","7d6c6c42":"markdown","f58a0bb2":"markdown","d97eac5c":"markdown","2df3e45d":"markdown","95c70ccc":"markdown","cd19b2b3":"markdown","2581e82b":"markdown","637f22e2":"markdown","5103e7ef":"markdown","10fc183c":"markdown","7de6f013":"markdown","65ae028c":"markdown","54627fb5":"markdown","6d53e45b":"markdown","f4d939fa":"markdown","a51d32cb":"markdown","4e52f2a1":"markdown","9d0c0695":"markdown","b2fbc3bb":"markdown","52e3513d":"markdown","b13bf473":"markdown","5e75deec":"markdown","6da8a467":"markdown","d5ccc8ee":"markdown","35c7bc73":"markdown","e5f1d83b":"markdown","b86c5f88":"markdown","2e6e90ca":"markdown","fde0a050":"markdown","a4ec4f2b":"markdown","8041d887":"markdown","28cb9f90":"markdown"},"source":{"195f2772":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","50915cf1":"dataset = pd.read_csv('..\/input\/50-startups\/50_Startups.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\nprint(dataset.head())","a9501a92":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\nprint(X)\n","0dbbdcc3":"X=X[:,1:] \nprint(X)","4d6af75f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","2841bb6b":"# here to avoid the inv error transform your values to float\nX_train =X_train.astype('float') \ny_train=y_train.astype('float')\n\n# first we add X_0 to our X\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# the we find theta using the normal equation\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n","4fed151f":"# predictions\n# training set \ny_pred_train=np.matmul(X_train_0,theta)\n# test set\ny_pred_test =np.matmul(X_test_0,theta)\n","d3ff4d6d":"#Evaluation training\n\n#MSE\nJ_mse_train = np.sum((y_pred_train - y_train)**2)\/ X_test_0.shape[0]\n\n# R_square\nsse_train = np.sum((y_pred_train - y_train)**2)\nsst_train = np.sum((y_train - y_train.mean())**2)\nR_square_train = 1 - (sse_train\/sst_train)\nprint('The Mean Square Error(MSE) or J(theta) for our trainig set is: ',J_mse_train)\nprint('R square obtain for normal equation method for our training set is :',R_square_train)\n\n###\nprint()\n#Evaluation testing: \n\n#MSE\nJ_mse_test= np.sum((y_pred_test - y_test)**2)\/ X_test_0.shape[0]\n\n# R_square \nsse_test = np.sum((y_pred_test - y_test)**2)\nsst_test = np.sum((y_test - y_test.mean())**2)\nR_square_test= 1 - (sse_test\/sst_test)\nprint('The Mean Square Error(MSE) or J(theta) for our test set is: ',J_mse_test)\nprint('R square obtain for normal equation method for our test set is :',R_square_test)\n","16bc75a7":"from sklearn.linear_model import LinearRegression\n\n# fit is trainig our model to the training data\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# predicting our test results\ny_pred_test = regressor.predict(X_test)\n# predicting our training results\ny_pred_train = regressor.predict(X_train)\n\n\n# Evaluation of the trainig set\n#MSE and R_squared\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Evaluation of the test set\n#MSE and R_squared\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_train,y_pred_train))\n# The coefficient of determination: 1 is perfect prediction\nprint('R_squred: %.2f'\n      % r2_score(y_train,y_pred_train))\n\n###\nprint()\n###\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Evaluation of the test set\n#MSE and R_squared\nprint('Mean squared error: %.2f'\n      % mean_squared_error(y_test,y_pred_test))\n# The coefficient of determination: 1 is perfect prediction\nprint('R_squred: %.2f'\n      % r2_score(y_test,y_pred_test))\n\n\n","3ff1021c":"### a simple intuition for the cost function is that we are calcuting the differnce between the \\\\(\\hat{y_i}\\\\) the predicted y and \\\\(y_i\\\\) the real y and we are taking the average dividing by m <br> now you might ask why didn't i take the absolute value <br> the answer is simple it's the math, the function above makes the math happier and easier,some times we need to do dervation and other math operation which might be complicated using the abs() function","acdbad03":"### we don't need to apply feature scalling on linear regression","19d5f518":"### we will first read our data set and spread it to X and y (look at the formulas above to remember what's X and what's y)","da43a4fd":"### to find the best line we need to find \\\\(b_0,b_1,b_2,.....,b_n\\\\) so we need to find the coefficients <br \/> let's think a littel what is the best line that fits our data ? <br \/> the line which gives us the least error let's stop here and change the notation a littel .","05ae6027":"### \\\\[\\mathbf{ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\\\]<br> it can also be written as \\\\[\\mathbf{J(\\theta) =  \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x_i) - y_i)^2}\\\\] <br> if we considered \\\\(h_\\theta(x_i)\\\\) as the function which gives us the prediction ","42737318":"## **how we take care of the data**","ed13ae7a":"### code syntax notes\n##### numpy.linalg.inv(a) Compute the (multiplicative) inverse of a matrix.\n##### The numpy.matmul() function returns the matrix product of two arrays.","2b1a4d18":"don't forget we added \\\\(\\mathbf{X_0}\\\\) to our matrix \\\\(\\mathbf{X}\\\\)<br> \n### \\\\[\\mathbf{X} = \\left( \\begin{smallmatrix} 1 & x_{11} & x_{12} &.&.&.&.& x_{1n}\\\\1 & x_{21} & x_{22} &.&.&.&.& x_{2n}\\\\1 & x_{31} & x_{32} &.&.&.&.& x_{3n}\\\\.&.&.&.&. &.&.&.& \\\\.&.&.&.&. &.&.&.& \\\\1 & x_{m1} & x_{m2} &.&.&.&.&.x_{mn}\\\\\\end{smallmatrix} \\right)_{(m,n+1)}\\\\]","b70666bd":"### notice the size is (m,n+1) as we added the first column x0=1 ","a5f7f66f":"## **Model evaluation**\n### (Mean Square Error using formula) \\\\[\\mathbf{ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\\\]\n### \\\\[\\mathbf{R^2 = 1 - \\frac{SSE}{SST}}\\\\] \n### **SSE = Sum of Square Error** \n### **SST = Sum of Square Total**\n### \\\\[\\mathbf{SSE = \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\\\]\n### \\\\[\\mathbf{SST = \\sum_{i=1}^{m}(y_i - \\bar{y}_i)^2}\\\\]\n### \\\\(\\mathbf{\\hat{y}}\\\\) is predicted value and \\\\(\\mathbf{\\bar{y}}\\\\) is mean value of  y","1b3bb6b7":"### let's right our cost function again \n### \\\\[\\mathbf{J(\\theta) =  \\frac{1}{m} \\sum_{i=1}^{m}(h_\\theta(x_i) - y_i)^2}\\\\] <br> now let's write it as a matrix notation <br> \\\\[\\mathbf{J(\\theta) = \\frac{1}{m} (X\\theta - y)^T(X\\theta - y)}\\\\] <br> \n### T is tranpose why we are taking the tranpose ? , if we want to multiply two matrices the number of rows in the first matrix should be equal to the number of columns in the second matrix , and the tranpose converts rows to columns and columns to rows , you can google the matrix transpose and watch how it's done for further understanding.\n### let's simplify without \\\\(\\mathbf{\\frac {1}{m}}\\\\) <br> \\\\[\\mathbf{= (X\\theta)^T - y^T)(X\\theta -y)}\\mathbf{= (\\theta^T X^T - y^T)(X\\theta - y)}\\\\]\n### let's simplify further \\\\[\\mathbf{= \\theta^T X^T X \\theta - y^T X \\theta - \\theta^T X^T y + y^T y}\\\\]\n### \\\\[\\mathbf{ = \\theta^T X^T X \\theta  - 2\\theta^T X^T y + y^T y}\\\\]","b5fbfc54":"## **the normal equation**","865efe2d":"# **Multiple linear regression Math and intuitoin**  \n### we have a data in a plot just like the image below and all we want to do is to find the best line that **fits the data**.\n![linear regression](https:\/\/www.gatevidyalay.com\/wp-content\/uploads\/2020\/01\/Representing-Linear-Regression-Model.png)","29497f80":"## **how to find y ?**","55d665ec":"### untill now we reached a simplified term of our cost function the normal equation is a solution to our linear regression using ordinary least square cost function (the cost function in our hand)<br> now we need to take the dervative of our cost function and make it equal to zero ,this is the normal equation it's an analytical solution to our problem to find theta  <br> don't forget theta is now a matrix so we will find all of our thetas. \n### take partial derivative of  \\\\(J(\\theta)\\\\)  with respect to  \u03b8  and equate to  0 . The derivative of function is nothing but if a small change in input what would be the change in output of function\n### in math terms \\\\[\\mathbf{min_{\\theta_0,\\theta_1..\\theta_n} J({\\theta_0,\\theta_1..\\theta_n})}\\\\] \n### \\\\[\\mathbf{\\frac{\\partial J(\\theta_j)}{\\partial\\theta_j} =0}\\\\]\n### where \\\\(\\mathbf{j = 0,1,2,....n}\\\\)","11a9daf3":"### now we have  \\\\[\\mathbf{b_0x_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]\n### using theta notation \\\\[\\mathbf{\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+.....+\\theta_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]","8f432136":"## **Data preprocessing**","7d6c6c42":"### look at our third column in python we start from 0 so 3 is the state column it's categorical (machine learning understands numerical values only so we will encode our categorical data to numbers)\n### we will use binary vector here we have 3 states (california ,folrida,new york) <br \/> so we will have a binary vector of [0 0 0]<br \/> we will encode california as [1 0 0]<br \/>we will encode florida as    [0 1 0]<br \/>we will encode new york as   [0 0 1]<br \/>so we will add 3 columns to our dataset and remove the categorical data column so in total we added two columns ","f58a0bb2":"### after substitution we will have our predicted y now after understanding the core math of the model let's recap by a problem to illustrate further  \n### in a real life example we are given a data where we have two columns experience and salary so we have a column for the experience and a column for the salary for our company we are hiring a new canidates so we want to detrimne if we knew the canidate exprience what should it's salary be \n### so our formula now is like this \\\\[\\mathbf{y=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+.....+\\theta_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\] \n### y is what we want to predict which is the salary (we call that dependant variable ) and X in this problem is the experience (we call that independant variable) then we will use the normal equation to predict theta and substitute to get our predicted y","d97eac5c":"### after encoding our categorical data , we now have a problem called the dummy variable trap","2df3e45d":"### We will predict our y value and compare it with actual value in the test set \n### do you remember our cost function (Mean Square Error using formula) \\\\[\\mathbf{ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\\\]\n### \\\\(\\mathbf{R^2}\\\\) is what we will be using to evalute our data \n### \\\\(\\mathbf{R^2}\\\\) is statistical measure of how close data are to the fitted regression line\n### the intuition behind it is that we compare our line to another line the average line \n### \\\\[\\mathbf{R^2 = 1 - \\frac{SSE}{SST}}\\\\] \n### **SSE = Sum of Square Error** \n### **SST = Sum of Square Total**\n### \\\\[\\mathbf{SSE = \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2}\\\\]\n### \\\\[\\mathbf{SST = \\sum_{i=1}^{m}(y_i - \\bar{y}_i)^2}\\\\]\n### \\\\(\\mathbf{\\hat{y}}\\\\) is predicted value and \\\\(\\mathbf{\\bar{y}}\\\\) is mean value of  y","95c70ccc":"### now let's write theta and y as a matrix \\\\[\\theta = \\left (\\begin{matrix} \\theta_0 \\\\ \\theta_1 \\\\ .\\\\.\\\\ \\theta_j\\\\.\\\\.\\\\ \\theta_n \\end {matrix}\\right)_{(n+1,1)}\\mathbf{ y } = \\left (\\begin{matrix} y_1\\\\ y_2\\\\. \\\\. \\\\ y_i \\\\. \\\\. \\\\ y_m \\end{matrix} \\right)_{(m,1)}\\\\]\n","cd19b2b3":"### machine learning works like this we first train our model to solve a known data (training set) and then we evalute our model by unknown data (the test set) ","2581e82b":"# **Multiple linear regression coding using scikit learn**","637f22e2":"### okay let's write all our equations to be clear\n### \\\\[\\mathbf{\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} (\\theta^T X^T X \\theta  - 2\\theta^T X^T y + y^T y )=0}\\\\]\n### \\\\[\\mathbf{ = X^T X \\frac {\\partial \\theta^T \\theta}{\\partial\\theta} - 2 X^T y \\frac{\\partial \\theta^T}{\\partial\\theta} + \\frac {\\partial y^T y}{\\partial\\theta}=0}\\\\]\n### \\\\[\\mathbf{\\frac {\\partial \\theta^T \\theta}{\\partial\\theta}= 2\\theta \\ \\ , \\ \\ \\frac{\\partial \\theta^T}{\\partial\\theta}=I \\ \\ , \\ \\ \\frac {\\partial y^T y}{\\partial\\theta}=0}\\\\]\n### \\\\(\\mathbf{I}\\\\) is the identity matrix it's like 1 look at the photo below it does nothing in multiplication it is a matrix that contains one in the left diagonal and all the other elments is equal to zero look at the example below multipling any matrix by the identity matrix will give us the same matrix\n### \\\\[\\left( \\begin{smallmatrix}a_{11} & a_{12} &a_{13}\\\\a_{21}&a_{22} &a_{23}\\\\a_{31} &a_{32} &a_{33}\\\\\\end{smallmatrix} \\right)_{(3,3)} \\ \\ * \\left( \\begin{smallmatrix}1 & 0 &0\\\\0&1 &0\\\\0 &0 &1\\\\\\end{smallmatrix} \\right)_{(3,3)}=\\left( \\begin{smallmatrix}a_{11} & a_{12} &a_{13}\\\\a_{21}&a_{22} &a_{23}\\\\a_{31} &a_{32} &a_{33}\\\\\\end{smallmatrix} \\right)_{(3,3)}\\\\]","5103e7ef":"## **Model evaluation**","10fc183c":"**Importing the dataset**","7de6f013":"### the intution behind it is this we can predict a dummy variable from another dummy variable by saying \\\\(\\mathbf{D2 = 1 \u2212 D1}\\\\) we are using binary vectors only 0 and 1 now do you remember our original formula\n### \\\\[\\mathbf{y=b_0x_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]\n### let's plug in our \\\\(\\mathbf{D_1}\\\\) and \\\\(\\mathbf{D_2}\\\\) into the formula \n### \\\\[\\mathbf{y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4D_1 + b_5D_2}\\\\]\n### now let's substitute D2 when \\\\(\\mathbf{D2 = 1 \u2212 D1}\\\\)  \\\\[\\mathbf{= b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4D_1 + b_5(1 \u2212 D_1)\\\\= b_0 + b_5 + b_1x_1 + b_2x_2 + b_3x_3 + (b_4 \u2212 b_5)D_1\\\\= b^\u2217_0+b_1x_1+b_2x_2+b_3x_3+b^\u2217_4D_1}\\\\]\n### with \\\\(\\mathbf{b^\u2217_0 = b_0 + b_5 \\ \\ and \\ \\ b^\u2217_4 = b4 \u2212 b5\\\\}\\\\)\n\n","65ae028c":"### code syntax notes\n##### Label encoding refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.\n\n##### A One hot encoding is a representation of categorical variable as binary vectors.It allows the representation of categorical data to be more expresive. This first requires that the categorical values be mapped to integer values, that is label encoding. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n\n","54627fb5":"### let's write our y again \\\\[\\mathbf{y=b_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] <br> now y is equal to \\\\[\\mathbf{b_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] <br> it is the same as \\\\[\\mathbf{b_0x_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\] <br> let's right  X as a matrix <br> \n### \\\\[\\mathbf{X} = \\left( \\begin{smallmatrix} 1 & x_{11} & x_{12} &.&.&.&.& x_{1n}\\\\1 & x_{21} & x_{22} &.&.&.&.& x_{2n}\\\\1 & x_{31} & x_{32} &.&.&.&.& x_{3n}\\\\.&.&.&.&. &.&.&.& \\\\.&.&.&.&. &.&.&.& \\\\1 & x_{m1} & x_{m2} &.&.&.&.&.x_{mn}\\\\\\end{smallmatrix} \\right)_{(m,n+1)}\\\\]","6d53e45b":"### so in the cost function \\\\(\\mathbf{ h_\\theta{(x)} = X\\theta}\\\\) where \\\\(\\mathbf{h_\\theta(x_i)}\\\\) ","f4d939fa":"### now let's predict our training and test answers","a51d32cb":"Splitting the dataset into the Training set and Test set","4e52f2a1":" ### \\\\[\\mathbf{y=b_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] \\\\[y \\ is \\ our \\ line\\\\]  <br \/> \\\\[\\mathbf{x_1,2x_2,.....,x_n}\\\\] \\\\[are \\ the \\ features \\ of \\ our \\ data \\ ,the \\ independent \\ variables (given)\\\\]\n","9d0c0695":"## **notes**","b2fbc3bb":"### consider our \\\\(b_0,b_1,b_2,.....,b_n\\\\) as \\\\(\\theta_0,\\theta_1,\\theta_2,.....,\\theta_n\\\\) we just changed the notation for explaination purposes <br \/> so how to calc the error , the function which calcute the error is called the cost function <br \/> the y we are talking about untill this moment is the y which we are going to predict let's call it \\\\(\\hat{y}\\\\) <br \/> there is another y the dependant variable (given to us) let's call it \\\\(y\\\\)","52e3513d":"### we don't have to put our \\\\(\\mathbf{\\frac {1}{m}}\\\\) back since we are equating our function to zero we will throw it any way and start our dervative  \\\\[\\mathbf{\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} (\\theta^T X^T X \\theta  - 2\\theta^T X^T y + y^T y )=0}\\\\]\n### \\\\[\\mathbf{ = X^T X \\frac {\\partial \\theta^T \\theta}{\\partial\\theta} - 2 X^T y \\frac{\\partial \\theta^T}{\\partial\\theta} + \\frac {\\partial y^T y}{\\partial\\theta}=0}\\\\]","b13bf473":"### Therefore the information of the redundant dummy variable \\\\(\\mathbf{D_2}\\\\) is going into the constant \\\\(\\mathbf{b_0}\\\\) and that's the math behind the dummy variable trap .\n### **the solution** to the dummy variable trap is to drop a column from our encoded data zeros and one the intuition behind the solution is that now we can't predict a dummy variable from another dummy variable <br>now remember to drop a column from your encoded to avoid the dummy variable trap","5e75deec":"avoiding the dummy variable trap","6da8a467":"**Importing the libraries**","d5ccc8ee":"## **the cost function** ","35c7bc73":"## **Model building** \n### don't forget our intial goal was to find theta and we did it by taking the dervative of the cost function now we found theta all we have to do is to substitute in our intial formula\n### \\\\[\\mathbf{y=b_0x_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]\n### using theta notation \\\\[\\mathbf{y=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+.....+\\theta_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]\n### all we need is the normal equation  \\\\[\\mathbf{ \\theta = (X^TX)^{-1} X^Ty }\\\\]\n### now we have our theta let's substitute in y to get our predicted y","e5f1d83b":"### the dervative rules used here is matrix dervative rules you can download this cheat sheet from google to know the dervative rules but i will leave a photo here any way\nhttp:\/\/www.gatsby.ucl.ac.uk\/teaching\/courses\/sntn\/sntn-2017\/resources\/Matrix_derivatives_cribsheet.pdf\n![Capture](https:\/\/user-images.githubusercontent.com\/24479105\/101497869-19fe0700-3974-11eb-86f6-5c29516e663a.PNG)\n### you can also use this calculator to calcute matrix dervatives http:\/\/www.matrixcalculus.org\/\n\n","b86c5f88":"### don't forget our intial goal was to find theta and we did it by taking the dervative of the cost function now we found theta all we have to do is to substitute in our intial formula\n### \\\\[\\mathbf{y=b_0x_0+b_1x_1+b_2x_2+.....+b_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]\n### using theta notation \\\\[\\mathbf{y=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+.....+\\theta_nx_n}\\\\] \\\\[when \\ x_0=1 \\\\]","2e6e90ca":"### our final equations after dervative \\\\[\\mathbf{\\frac{\\partial J(\\theta)}{\\partial\\theta} = X^T X 2\\theta - 2X^T y +0=0}\\\\] \n### \\\\[\\mathbf{ 0 = 2X^T X \\theta - 2X^T y}\\\\]\n### \\\\[\\mathbf{ X^T X \\theta = X^T }\\\\]\n### and now for the normal equation  \\\\[\\mathbf{ \\theta = (X^TX)^{-1} X^Ty }\\\\]","fde0a050":"### now we let's code what we have so far ","a4ec4f2b":"### let's see what's our data set is about we have 50 startups and some information about them the independant variables we want to analyze the given data and predict the profit the dependant variable ","8041d887":"## **the dummy variable trap**","28cb9f90":"# Multiple linear regression from scratch coding"}}