{"cell_type":{"be236dad":"code","ae44341e":"code","debcaa4e":"code","a0bfe236":"code","7ee2599d":"code","f1b5c892":"code","317f7938":"code","7c29844d":"code","e747efa5":"code","b4a075dc":"code","cea16d4c":"code","1c293ca0":"code","8e8b96e1":"code","8fca698d":"code","aab74453":"code","0630944e":"code","3eb21147":"code","c9b3af5a":"code","6aa40aae":"code","28379646":"code","4ea4a84b":"code","27c6993d":"code","1005afc9":"code","b5eaacd0":"code","118033db":"code","8cc2cb53":"code","1a1be393":"code","fc3c8c9e":"code","d4aa9c23":"code","60aa263a":"code","fe419a1e":"code","a5323121":"code","8f168895":"markdown","8c4417fa":"markdown","3aa5b3ea":"markdown","b2685b8f":"markdown","a629924b":"markdown","2ce96655":"markdown","61b79d5b":"markdown","cfb016ae":"markdown","0af704a0":"markdown","55424332":"markdown","4704c256":"markdown","a17028e4":"markdown","12ca05a8":"markdown","e9ca90ef":"markdown","882aa2e7":"markdown","a780bf13":"markdown","387f2a96":"markdown","fb788032":"markdown","dc904531":"markdown"},"source":{"be236dad":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom torchtext import data\nfrom torchtext.data import TabularDataset\nfrom tqdm.notebook import tqdm","ae44341e":"import torch\nimport torchtext\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","debcaa4e":"import random","a0bfe236":"SEED = 5\nrandom.seed(SEED)\ntorch.manual_seed(SEED)","7ee2599d":"# dataset\nTRAIN_SIZE = 0.7\n# model\nBATCH_SIZE = 64\nLEARNING_RATE = 0.001\nEPOCHS = 30\nDROP_RATE = 0.3","f1b5c892":"USE_CUDA = torch.cuda.is_available()\nDEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\nprint(\"cpu\uc640 cuda \uc911 \ub2e4\uc74c \uae30\uae30\ub85c \ud559\uc2b5\ud568:\", DEVICE)","317f7938":"### 1> set several paths\nPATH_TRAIN = '..\/input\/nlp-getting-started\/train.csv'\nPATH_TEST = '..\/input\/nlp-getting-started\/test.csv'\n\n### 2> read_csv\ndf_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)","7c29844d":"print(df_train.shape)\nprint(df_test.shape)","e747efa5":"df = df_train.append(df_test, sort=False)\ndf.shape","b4a075dc":"import re\nimport string","cea16d4c":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    \n    return url.sub('', text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    \n    return html.sub('', text)\n    \ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    \n    return text.translate(table)","1c293ca0":"df['text'] = df['text'].apply(lambda x: remove_URL(x))\ndf['text'] = df['text'].apply(lambda x: remove_html(x))\ndf['text'] = df['text'].apply(lambda x: remove_emoji(x))\ndf['text'] = df['text'].apply(lambda x: remove_punct(x))","8e8b96e1":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\nnltk.download('wordnet')","8fca698d":"keywords = df_train.keyword.unique()[1:]\nkeywords = list(map(lambda x: x.replace('%20', ' '), keywords))\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize_sentence(sentence):\n    sentence_words = sentence.split(' ')\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        sentence_word = sentence_word.replace('#', '')\n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence","aab74453":"df['text'] = df['text'].apply(lambda x: lemmatize_sentence(x))","0630944e":"df_train = df.iloc[:len(df_train)]\ndf_test = df.iloc[len(df_train):]","3eb21147":"df_train = df_train[['id','text','target']]\ndf_test = df_test[['id','text']]","c9b3af5a":"import os","6aa40aae":"if not os.path.exists('preprocessed_train.csv'):\n    df_train.to_csv('preprocessed_train.csv', index = False)\n    \nif not os.path.exists('preprocessed_test.csv'):\n    df_test.to_csv('preprocessed_test.csv', index = False)","28379646":"TEXT = torchtext.data.Field(sequential=True, \n                            tokenize='spacy', \n                            lower=True, \n                            include_lengths=True, \n                            batch_first=True, \n                            fix_length=25)\nLABEL = torchtext.data.Field(use_vocab=True,\n                           sequential=False,\n                           dtype=torch.float16)\nID = torchtext.data.Field(use_vocab=False,\n                         sequential=False,\n                         dtype=torch.float16)","4ea4a84b":"from torchtext.data import TabularDataset","27c6993d":"trainset = TabularDataset(path='preprocessed_train.csv', format='csv', skip_header=True,\n                            fields=[('id', ID), ('text', TEXT), ('target', LABEL)])\ntestset = TabularDataset(path='preprocessed_test.csv', format='csv', skip_header=True,\n                            fields=[('id', ID), ('text', TEXT)])","1005afc9":"from torchtext.vocab import Vectors, GloVe","b5eaacd0":"TEXT.build_vocab(trainset, testset, \n                 max_size=20000, min_freq=10,\n                 vectors=GloVe(name='6B', dim=300))  # We use it for getting vocabulary of words\nLABEL.build_vocab(trainset)\nID.build_vocab(trainset, testset)","118033db":"trainset, valset = trainset.split(split_ratio = TRAIN_SIZE, random_state=random.getstate(),\n                                  strata_field = 'target', stratified=True)","8cc2cb53":"train_iter = torchtext.data.Iterator(dataset = trainset, batch_size = BATCH_SIZE, device = DEVICE,\n                                     train=True, shuffle=True, repeat=False, sort = False)\nval_iter = torchtext.data.Iterator(dataset = valset, batch_size = BATCH_SIZE, device = DEVICE,\n                                  train=True, shuffle=True, repeat=False)\ntest_iter = torchtext.data.Iterator(dataset = testset, batch_size = BATCH_SIZE, device = DEVICE,\n                                   train=False, shuffle=False, repeat=False)","1a1be393":"word_embeddings = TEXT.vocab.vectors\nvocab_size = len(TEXT.vocab)\nn_classes = 2","fc3c8c9e":"class LSTM_model(nn.Module):\n    def __init__(self, n_layers, hidden_dim, n_vocab, embedding_dim, n_classes, dropout_p = DROP_RATE):\n        super(LSTM_model, self).__init__()\n        self.n_layers = n_layers\n        self.embed = nn.Embedding(n_vocab, embedding_dim)\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(dropout_p)\n        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers = self.n_layers, batch_first = True)\n        self.out = nn.Linear(self.hidden_dim, n_classes)\n        \n    def forward(self, x):\n        # x = [64, 27]\n        x = self.embed(x)\n        # x = [64, 27, 128]\n        h_0 = self._init_state(batch_size = x.size(0))#\uccab \ubc88\uc9f8 \uc740\ub2c9 \ubca1\ud130 \uc815\uc758\n        # h_0 = [1, 64, 256]\n        x, _ = self.lstm(x,(h_0,h_0))\n        # x = [64, 27, 256]\n        h_t = x[:,-1,:]\n        # h_t = [64, 256]\n        self.dropout(h_t)\n        logit = self.out(h_t)\n        # logit = [64, 2]\n        return logit\n    \n    def _init_state(self, batch_size = 1):\n        weight = next(self.parameters()).data\n        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()","d4aa9c23":"def train(model, optimizer, train_iter):\n    model.train()\n    acc, total_loss = 0, 0\n    for b,batch in enumerate(train_iter):\n        x, y = batch.text[0], batch.target\n        y.sub_(1)\n        y = y.type(torch.LongTensor)\n        x = x.to(DEVICE)\n        y = y.data.to(DEVICE)\n        optimizer.zero_grad()# \uae30\uc6b8\uae30 0\uc73c\ub85c \ucd08\uae30\ud654\n        logit = model(x)\n        loss = F.cross_entropy(logit, y, reduction = 'mean')\n        total_loss += loss.item()\n        acc += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n        loss.backward()\n        optimizer.step()\n    size = len(train_iter.dataset)\n    avg_loss = total_loss \/ size\n    avg_accuracy = 100. * acc \/ size\n    return avg_loss, avg_accuracy","60aa263a":"def evaluate(model, val_iter):\n    model.eval()\n    acc, total_loss = 0., 0.\n    for batch in val_iter:\n        x, y = batch.text[0], batch.target\n        y.sub_(1)\n        y = y.type(torch.LongTensor)\n        x = x.to(DEVICE)\n        y = y.data.to(DEVICE)\n        logit = model(x)\n        loss = F.cross_entropy(logit, y, reduction = 'sum')#\uc624\ucc28\uc758 \ud569 \uad6c\ud558\uace0 total_loss\uc5d0 \ub354\ud574\uc90c\n        total_loss += loss.item()\n        acc += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n    size = len(val_iter.dataset)\n    avg_loss = total_loss \/ size\n    avg_accuracy = 100. * acc \/ size\n    return avg_loss, avg_accuracy","fe419a1e":"model = LSTM_model(1, 256, vocab_size, 128, n_classes, 0.5).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)","a5323121":"best_val_loss = None\nfor e in tqdm(range(1, EPOCHS + 1)):\n    train_loss, train_accuracy = train(model, optimizer, train_iter)\n    val_loss, val_accuracy = evaluate(model, val_iter)\n\n    print(\"<<e : %d>> <<train_loss : %f>> <<train_accuracy : %f>> <<val_loss : %f>> <<val_accuracy : %f>>\"%(e, train_loss, train_accuracy, val_loss, val_accuracy))","8f168895":"-------\n\n## 5) split dataset","8c4417fa":"------\n------","3aa5b3ea":"## 2) Hyperparameter \uc124\uc815","b2685b8f":"-------\n# Pytorch\n-------\n\n# 3. Data Preprocess\n\n## 1) prepare dataset","a629924b":"## 3) GPU","2ce96655":"# 1. EDA\n\n## 0) get DataFrame","61b79d5b":"-------\n\n## 2) Field \uc815\uc758\ud558\uae30\n\n### 1> Field\ub780?\n\n- datatype\uc744 define: TEXT\uc640 LABEL \uac1d\uccb4\ub97c \uc815\uc758\ud574\uc900\ub2e4. (input\uacfc output\uc744 \uc815\uc758\ud574\uc900\ub2e4.)\n- \ucd94\ud6c4 \uc5b4\ub5a4 \uc804\ucc98\ub9ac\ub97c \ud560 \uac83\uc778\uc9c0 \uc815\uc758\ud55c\ub2e4.\n\n(data\ub97c \ubc1b\uc744 \uac1d\uccb4\ub97c \ubbf8\ub9ac \uc120\uc5b8\ud558\ub294 \uac83 \uac19\ub2e4.)\n\n### 2> \ub0b4\ubd80 \uae30\ub2a5\n\n- sequential: \ud574\ub2f9 data\uac00 sequential data\uc778\uc9c0 \uc5ec\ubd80 (default\ub294 True)\n- use_vocab: \ub2e8\uc5b4 \uc9d1\ud569\uc744 \ub9cc\ub4e4 \uac83\uc778\uac00 \uc5ec\ubd80\n    - \ub2e8\uc5b4 \uc9d1\ud569: \uc911\ubcf5\uc744 \uc81c\uac70\ud55c \ud14d\uc2a4\ud2b8\uc758 \ucd1d \ub2e8\uc5b4\uc758 \uc9d1\ud569(set)\n- tokenize: \uc5b4\ub5a4 token\ud654 \ud568\uc218\ub97c \uc0ac\uc6a9\ud560 \uac83\uc778\uc9c0 \uc9c0\uc815 (default\ub294 string.split)\n- lower : \uc601\uc5b4 \ub370\uc774\ud130\ub97c \uc804\ubd80 \uc18c\ubb38\uc790\ud654\ud55c\ub2e4. (default\ub294 False)\n    - \ucef4\ud4e8\ud130\ub294 \ub300\ubb38\uc790\uc640 \uc18c\ubb38\uc790\ub97c \ub2e4\ub974\uac8c \ud310\ub2e8\ud558\ubbc0\ub85c \ud544\uc694\ud55c \uae30\ub2a5\n- batch_first : \ubbf8\ub2c8 \ubc30\uce58 \ucc28\uc6d0\uc744 \ub9e8 \uc55e\uc73c\ub85c \ud558\uc5ec \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc62c \uac83\uc778\uc9c0 \uc5ec\ubd80. (default\ub294 False)\n- is_target : \ub808\uc774\ube14 \ub370\uc774\ud130 \uc5ec\ubd80. (default\ub294 False)\n- fix_length : \ucd5c\ub300 \ud5c8\uc6a9 \uae38\uc774. \uc774 \uae38\uc774\uc5d0 \ub9de\ucdb0\uc11c \ud328\ub529 \uc791\uc5c5(Padding)\uc774 \uc9c4\ud589\ub429\ub2c8\ub2e4.","cfb016ae":"## 2) Lemmatization (\ud45c\uc81c\uc5b4 \ucd94\ucd9c)","0af704a0":"-------","55424332":"## 1) Cleaning by Regular expression","4704c256":"----------\n\n## 6) Build the Iterator(dataloader)","a17028e4":"--------\n\n## 3) CSV -> TabularDataset\n\n### 1> TabularDataset\n\n- \ub370\uc774\ud130\ub97c \ubd88\ub7ec\uc624\uba74\uc11c \n- \ud544\ub4dc\uc5d0\uc11c \uc815\uc758\ud588\ub358 \ud1a0\ud070\ud654 \ubc29\ubc95\uc73c\ub85c \ud1a0\ud070\ud654\ub97c \uc218\ud589\ud569\ub2c8\ub2e4 (\uc774\ub54c, \uc18c\ubb38\uc790\ud654 \uac19\uc740 \uae30\ubcf8\uc801\uc778 \uc804\ucc98\ub9ac\ub3c4 \ud568\uaed8 \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4.)\n\n### 2> \ub0b4\ubd80 \uae30\ub2a5\n\n- path : \ud30c\uc77c\uc774 \uc704\uce58\ud55c \uacbd\ub85c.\n- format : \ub370\uc774\ud130\uc758 \ud3ec\ub9f7.\n- fields : \uc704\uc5d0\uc11c \uc815\uc758\ud55c \ud544\ub4dc\ub97c \uc9c0\uc815. \uccab\ubc88\uc9f8 \uc6d0\uc18c\ub294 \ub370\uc774\ud130 \uc14b \ub0b4\uc5d0\uc11c \ud574\ub2f9 \ud544\ub4dc\ub97c \ud638\uce6d\ud560 \uc774\ub984, \ub450\ubc88\uc9f8 \uc6d0\uc18c\ub294 \uc9c0\uc815\ud560 \ud544\ub4dc.\n<font color = 'red'> (DataFrame\uc740 column \uc21c\uc11c\ub300\ub85c field \uc9c0\uc815\ud574\uc57c \ud55c\ub2e4.) <\/font>\n- skip_header : \ub370\uc774\ud130\uc758 \uccab\ubc88\uc9f8 \uc904\uc740 \ubb34\uc2dc. <font color = 'red'> \uaf2d \ud574\uc57c\ud55c\ub2e4. \uc548 \uadf8\ub7ec\uba74 column\uc774 data\ub85c \ub07c\uc5b4\ub4e0\ub2e4. <\/font>","12ca05a8":"------\n------","e9ca90ef":"# 0. \ucd08\uae30 \uc124\uc815\n\n## 1) seed \uc124\uc815","882aa2e7":"-------","a780bf13":"------\n\n## 4) \ub2e8\uc5b4 \uc9d1\ud569(\ub2e8\uc5b4\uc7a5, vocabulary) \uc0dd\uc131\n\n### 1> \ub2e8\uc5b4 \uc9d1\ud569\uc774\ub780?\n\n- \uc911\ubcf5\uc744 \uc81c\uac70\ud55c \ucd1d \ub2e8\uc5b4\ub4e4\uc758 \uc9d1\ud569\n- \ub2e8\uc5b4 \uc9d1\ud569\uc73c\ub85c \ucd94\ud6c4 \uc5b4\ub5a4 \uae30\uc900\uc744 \uac00\uc9c0\uace0(\uba87 \ubc88 \ub4f1\uc7a5?, \ub4f1\uc7a5 \uc21c\uc704\ubcc4) \uc815\uc218 \uc778\ucf54\ub529\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n\n### 2> \ub0b4\ubd80 \uae30\ub2a5\n\n- min_freq: \ub2e8\uc5b4 \uc9d1\ud569\uc5d0 \ucd94\uac00 \uc2dc \ub2e8\uc5b4\uc758 \ucd5c\uc18c \ub4f1\uc7a5 \ube48\ub3c4 \uc870\uac74\uc744 \ucd94\uac00\n- max_size: \ub2e8\uc5b4 \uc9d1\ud569\uc758 \ucd5c\ub300 \ud06c\uae30\ub97c \uc9c0\uc815","387f2a96":"-------\n\n## 7) and so on","fb788032":"# 2. Data Preprocess (Cleaning) \n\n\n## 0) concat","dc904531":"---------\n---------\n\n# 4. Build the LSTM"}}