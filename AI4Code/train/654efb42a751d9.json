{"cell_type":{"749a6dd2":"code","f3026bcf":"code","20abd96f":"code","ac719dea":"code","f6f42700":"code","59a6a7e0":"code","3f4224c5":"code","84954536":"code","514345f5":"code","c305f9e4":"code","27e3b759":"code","2917bfd8":"code","a1439cf2":"code","77287b57":"code","5a9ea720":"code","42a2e7fc":"code","62d315e9":"code","129be7ac":"code","c3432c65":"code","57e7d6c9":"code","bf1c3fa4":"code","25852ec5":"code","b17c6869":"code","54a381c1":"code","fb1c771c":"code","1b7a020a":"code","dcfecfa9":"code","3dab8c26":"code","fcf9410a":"code","0b52f325":"code","f553bbac":"code","52c483ce":"code","8c8ceafb":"code","4519f3ea":"code","d6c8b10b":"code","8f797e42":"code","df07a128":"code","17d5906f":"code","a36e4790":"code","1483356a":"code","8bdf58d7":"code","1a31f7a9":"code","faef6a23":"code","6712bb7a":"code","a4477d57":"code","5204f537":"code","97fd6e23":"code","e486172a":"code","82d0f120":"code","bf17f4bc":"code","77f2ced2":"code","154ec1b6":"code","2febf6c3":"code","0077fa3c":"code","cbb367e2":"code","7f865843":"code","258c468e":"code","138a1f0b":"code","1ae9ee4f":"code","69173cc1":"code","ad63ee16":"code","b523c777":"code","bb34c087":"code","eacb9a9b":"code","f7877754":"code","2c52e6c1":"code","1fc78aa1":"markdown","b9db14c6":"markdown","0ed504d8":"markdown","c77a8691":"markdown","1c94b741":"markdown","19ffeb3e":"markdown","2d165ddd":"markdown","6952230f":"markdown","c8373db4":"markdown","17038ccd":"markdown","e8897caf":"markdown","a8eb39b6":"markdown","e6150d6e":"markdown","4f9423e3":"markdown","52fc5b14":"markdown","df2557fa":"markdown","4fad10ea":"markdown","881f6134":"markdown","43fa9a1b":"markdown","4f388360":"markdown","920b13b0":"markdown","15e05eaf":"markdown","4c8b6988":"markdown","f350e720":"markdown","9f9a5078":"markdown","cebd8495":"markdown","6d51e3e9":"markdown","a8a07189":"markdown","8cb7d96b":"markdown","5d58348b":"markdown","2ac7b788":"markdown","5f80b990":"markdown","ab9e7121":"markdown","9d523803":"markdown","19f6b8d6":"markdown","a0640937":"markdown","fd566d75":"markdown","a3972fd9":"markdown","d18aaa0c":"markdown","bba04f15":"markdown","082edc6d":"markdown","c6a67177":"markdown","ad6819eb":"markdown","79416c1b":"markdown","4ff9a52d":"markdown","062a3de0":"markdown","555a4b5e":"markdown","119a65bd":"markdown","f2db0648":"markdown","d96ba61c":"markdown","d55c8f12":"markdown","d5b7b21c":"markdown","41398617":"markdown","5107446a":"markdown","f8a20205":"markdown","e1f201d4":"markdown","01fd6270":"markdown","82ba72c3":"markdown","a0d19f0c":"markdown","f615328d":"markdown","a78b75f1":"markdown","5fc829c1":"markdown","35a2f16d":"markdown","c8bbdde0":"markdown","d71d8cbd":"markdown","9e3baafe":"markdown","1135eccd":"markdown","6939b9d8":"markdown","db5658ba":"markdown","e580c947":"markdown","0aec0f4b":"markdown","bf196e45":"markdown","6e8a41b8":"markdown","f32fa031":"markdown"},"source":{"749a6dd2":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport plotly.graph_objs as go\nfrom sklearn import datasets\nimport plotly.plotly as py\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os","f3026bcf":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","20abd96f":"sns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\nnp.random.seed(1337)\n#show plot inline\n%matplotlib inline","ac719dea":"print(os.listdir(\"..\/input\/\"))","f6f42700":"# import Dataset to play with it\ndataset = pd.read_csv('..\/input\/Iris.csv')","59a6a7e0":"type(dataset)","3f4224c5":"# Modify the graph above by assigning each species an individual color.\nsns.FacetGrid(dataset, hue=\"Species\", size=5) \\\n   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n   .add_legend()\nplt.show()","84954536":"dataset.plot(kind='box', subplots=True, layout=(2,3), sharex=False, sharey=False)\nplt.figure()\n#This gives us a much clearer idea of the distribution of the input attributes:","514345f5":"# To plot the species data using a box plot:\n\nsns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset )\nplt.show()","c305f9e4":"ax= sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset)\nax= sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset, jitter=True, edgecolor=\"gray\")\nplt.show()","27e3b759":"ax= sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset)\nax= sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset, jitter=True, edgecolor=\"gray\")\n\nboxtwo = ax.artists[2]\nboxtwo.set_facecolor('red')\nboxtwo.set_edgecolor('black')\nboxthree=ax.artists[1]\nboxthree.set_facecolor('yellow')\nboxthree.set_edgecolor('black')\n\nplt.show()","2917bfd8":"# histograms\ndataset.hist(figsize=(15,20))\nplt.figure()","a1439cf2":"dataset[\"PetalLengthCm\"].hist();","77287b57":"\n# scatter plot matrix\npd.plotting.scatter_matrix(dataset,figsize=(10,10))\nplt.figure()","5a9ea720":"# violinplots on petal-length for each species\nsns.violinplot(data=dataset,x=\"Species\", y=\"PetalLengthCm\")","42a2e7fc":"# Using seaborn pairplot to see the bivariate relation between each pair of features\nsns.pairplot(dataset, hue=\"Species\")","62d315e9":"# updating the diagonal elements in a pairplot to show a kde\nsns.pairplot(dataset, hue=\"Species\",diag_kind=\"kde\")","129be7ac":"# seaborn's kdeplot, plots univariate or bivariate density estimates.\n#Size can be changed by tweeking the value used\nsns.FacetGrid(dataset, hue=\"Species\", size=5).map(sns.kdeplot, \"PetalLengthCm\").add_legend()\nplt.show()","c3432c65":"# Use seaborn's jointplot to make a hexagonal bin plot\n#Set desired size and ratio and choose a color.\nsns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=dataset, size=10,ratio=10, kind='hex',color='green')\nplt.show()","57e7d6c9":"#In Pandas use Andrews Curves to plot and visualize data structure.\n#Each multivariate observation is transformed into a curve and represents the coefficients of a Fourier series.\n#This useful for detecting outliers in times series data.\n#Use colormap to change the color of the curves\n\nfrom pandas.tools.plotting import andrews_curves\nandrews_curves(dataset.drop(\"Id\", axis=1), \"Species\",colormap='rainbow')\nplt.show()","bf1c3fa4":"# we will use seaborn jointplot shows bivariate scatterplots and univariate histograms with Kernel density \n# estimation in the same figure\nsns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=dataset, size=6, kind='kde', color='#800000', space=0)","25852ec5":"plt.figure(figsize=(7,4)) \nsns.heatmap(dataset.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","b17c6869":"# A final multivariate visualization technique pandas has is radviz\n# Which puts each feature as a point on a 2D plane, and then simulates\n# having each sample attached to those points through a spring weighted\n# by the relative value for that feature\nfrom pandas.tools.plotting import radviz\nradviz(dataset.drop(\"Id\", axis=1), \"Species\")","54a381c1":"dataset['Species'].value_counts().plot(kind=\"bar\");","fb1c771c":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nfrom plotly import tools\nimport plotly.figure_factory as ff\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\ntrace = go.Scatter(x=X[:, 0],\n                   y=X[:, 1],\n                   mode='markers',\n                   marker=dict(color=np.random.randn(150),\n                               size=10,\n                               colorscale='Viridis',\n                               showscale=False))\n\nlayout = go.Layout(title='Training Points',\n                   xaxis=dict(title='Sepal length',\n                            showgrid=False),\n                   yaxis=dict(title='Sepal width',\n                            showgrid=False),\n                  )\n \nfig = go.Figure(data=[trace], layout=layout)","1b7a020a":"py.iplot(fig)","dcfecfa9":"# shape\nprint(dataset.shape)","3dab8c26":"#columns*rows\ndataset.size","fcf9410a":"dataset.isnull().sum()","0b52f325":"# remove rows that have NA's\ndataset = dataset.dropna()","f553bbac":"print(dataset.info())","52c483ce":"dataset['Species'].unique()","8c8ceafb":"dataset[\"Species\"].value_counts()\n","4519f3ea":"dataset.head(5) ","d6c8b10b":"dataset.tail() ","8f797e42":"dataset.sample(5) ","df07a128":"dataset.describe() ","17d5906f":"dataset.isnull().sum()","a36e4790":"dataset.groupby('Species').count()","1483356a":"dataset.columns","8bdf58d7":"dataset.where(dataset ['Species']=='Iris-setosa')","1a31f7a9":"dataset[dataset['SepalLengthCm']>7.2]","faef6a23":"# Seperating the data into dependent and independent variables\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","6712bb7a":"cols = dataset.columns\nfeatures = cols[0:4]\nlabels = cols[4]\nprint(features)\nprint(labels)","a4477d57":"#Well conditioned data will have zero mean and equal variance\n#We get this automattically when we calculate the Z Scores for the data\n\ndata_norm = pd.DataFrame(dataset)\n\nfor feature in features:\n    dataset[feature] = (dataset[feature] - dataset[feature].mean())\/dataset[feature].std()\n\n#Show that should now have zero mean\nprint(\"Averages\")\nprint(dataset.mean())\n\nprint(\"\\n Deviations\")\n#Show that we have equal variance\nprint(pow(dataset.std(),2))","5204f537":"#Shuffle The data\nindices = data_norm.index.tolist()\nindices = np.array(indices)\nnp.random.shuffle(indices)\n","97fd6e23":"# One Hot Encode as a dataframe\nfrom sklearn.model_selection import train_test_split\ny = get_dummies(y)\n\n# Generate Training and Validation Sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n\n# Convert to np arrays so that we can use with TensorFlow\nX_train = np.array(X_train).astype(np.float32)\nX_test  = np.array(X_test).astype(np.float32)\ny_train = np.array(y_train).astype(np.float32)\ny_test  = np.array(y_test).astype(np.float32)","e486172a":"#Check to make sure split still has 4 features and 3 labels\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","82d0f120":"\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","bf17f4bc":"# K-Nearest Neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","77f2ced2":"from sklearn.neighbors import  RadiusNeighborsClassifier\nModel=RadiusNeighborsClassifier(radius=8.0)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n#summary of the predictions made by the classifier\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accouracy score\nprint('accuracy is ', accuracy_score(y_test,y_pred))","154ec1b6":"# LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","2febf6c3":"from sklearn.linear_model import PassiveAggressiveClassifier\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","0077fa3c":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nModel = GaussianNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","cbb367e2":"# BernoulliNB\nfrom sklearn.naive_bayes import BernoulliNB\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","7f865843":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\nModel = SVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","258c468e":"# Support Vector Machine's \nfrom sklearn.svm import NuSVC\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","138a1f0b":"# Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\n\nModel = LinearSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","1ae9ee4f":"# Decision Tree's\nfrom sklearn.tree import DecisionTreeClassifier\n\nModel = DecisionTreeClassifier()\n\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","69173cc1":"# ExtraTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\n\nModel = ExtraTreeClassifier()\n\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","ad63ee16":"from sklearn.neural_network import MLPClassifier\nModel=MLPClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n# Summary of the predictions\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","b523c777":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","bb34c087":"from sklearn.ensemble import BaggingClassifier\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","eacb9a9b":"from sklearn.ensemble import AdaBoostClassifier\nModel=AdaBoostClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","f7877754":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","2c52e6c1":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","1fc78aa1":"<a id=\"75\"><\/a> <br>\n##  7-5 Radius Neighbors Classifier","b9db14c6":" <a id=\"1\"><\/a> <br>\n## 1- Introduction\nThis is a **comprehensive ML techniques with python** , that I have spent for more than two months to complete it.\n\nit is clear that everyone in this community is familiar with IRIS dataset but if you need to review your information about the dataset please visit this [link](https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris).\n\nI have tried to help **beginners**  in Kaggle how to face machine learning problems. and I think it is a great opportunity for who want to learn machine learning workflow with python completely.\nI have covered most of the methods that are implemented for iris until **2018**, you can start to learn and review your knowledge about ML with a simple dataset and try to learn and memorize the workflow for your journey in Data science world.\n <a id=\"11\"><\/a> <br>\n## 1-1 Courses\n\nThere are alot of Online courses that can help you develop your knowledge, here I have just  listed some of them:\n\n1. [Machine Learning Certification by Stanford University (Coursera)](https:\/\/www.coursera.org\/learn\/machine-learning\/)\n\n2. [Machine Learning A-Z\u2122: Hands-On Python & R In Data Science (Udemy)](https:\/\/www.udemy.com\/machinelearning\/)\n\n3. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https:\/\/www.coursera.org\/specializations\/deep-learning)\n\n4. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n\n5. [Mathematics for Machine Learning by Imperial College London](https:\/\/www.coursera.org\/specializations\/mathematics-machine-learning)\n\n6. [Deep Learning A-Z\u2122: Hands-On Artificial Neural Networks](https:\/\/www.udemy.com\/deeplearning\/)\n\n7. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https:\/\/www.udemy.com\/complete-guide-to-tensorflow-for-deep-learning-with-python\/)\n\n8. [Data Science and Machine Learning Tutorial with Python \u2013 Hands On](https:\/\/www.udemy.com\/data-science-and-machine-learning-with-python-hands-on\/)\n\n9. [Machine Learning Certification by University of Washington](https:\/\/www.coursera.org\/specializations\/machine-learning)\n\n10. [Data Science and Machine Learning Bootcamp with R](https:\/\/www.udemy.com\/data-science-and-machine-learning-bootcamp-with-r\/)\n\n\n5- [https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n <a id=\"12\"><\/a> <br>\n## 1-2 Ebooks\nSo you love reading , here is **10 free machine learning books**\n\n1. [Probability and Statistics for Programmers](http:\/\/www.greenteapress.com\/thinkstats\/)\n\n1. [Bayesian Reasoning and Machine Learning](http:\/\/web4.cs.ucl.ac.uk\/staff\/D.Barber\/textbook\/091117.pdf)\n\n1. [An Introduction to Statistical Learning](http:\/\/www-bcf.usc.edu\/~gareth\/ISL\/)\n\n1. [Understanding Machine Learning](http:\/\/www.cs.huji.ac.il\/~shais\/UnderstandingMachineLearning\/index.html)\n\n1. [A Programmer\u2019s Guide to Data Mining](http:\/\/guidetodatamining.com\/)\n\n1. [Mining of Massive Datasets](http:\/\/infolab.stanford.edu\/~ullman\/mmds\/book.pdf)\n\n1. [A Brief Introduction to Neural Networks](http:\/\/www.dkriesel.com\/_media\/science\/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n\n1. [Deep Learning](http:\/\/www.deeplearningbook.org\/)\n\n1. [Natural Language Processing with Python](https:\/\/www.researchgate.net\/publication\/220691633_Natural_Language_Processing_with_Python)\n\n1. [Machine Learning Yearning](http:\/\/www.mlyearning.org\/)\n \n <a id=\"13\"><\/a> <br>\n \n## 1-3 Cheat Sheets\nSome perfect cheatsheet [26]:\n1. [top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data](https:\/\/www.analyticsvidhya.com\/blog\/2017\/02\/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data\/)\n\n\nI am open to getting your feedback for improving this **kernel**\n<br>\n[go to top](#top)","0ed504d8":"<a id=\"6213\"><\/a> <br>\n### 6-2-13 Visualization with Plotly","c77a8691":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n*   1-  [Introduction](#1)\n    * [1-1 Courses](#11)\n    * [1-2 Ebooks](#12)\n    * [1-3 Cheat Sheets](#13)\n*   2- [Machine learning workflow](#2)\n*       2-1 [Real world Application Vs Competitions](#21)\n*   3- [Problem Definition](#3)\n*       3-1 [Problem feature](#31)\n*       3-2 [Aim](#32)\n*       3-3 [Variables](#33)\n*   4-[ Inputs & Outputs](#4)\n*   4-1 [Inputs ](#41)\n*   4-2 [Outputs](#42)\n*   5- [Loading Packages](#5)\n*   6- [Exploratory data analysis](#6)\n*       6-1 [Data Collection](#61)\n*       6-2 [Visualization](#62)\n*           6-2-1 [Scatter plot](#621)\n*           6-2-2 [Box](#622)\n*           6-2-3 [Histogram](#623)\n*           6-2-4 [Multivariate Plots](#624)\n*           6-2-5 [Violinplots](#625)\n*           6-2-6 [Pair plot](#626)\n*           6-2-7 [Kde plot](#627)\n*           6-2-8 [Joint plot](#628)\n*           6-2-9 [Andrews curves](#629)\n*           6-2-10 [Heatmap](#6210)\n*           6-2-11 [Radviz](#6211)\n*           6-2-12 [Bar Plot](#6212)\n*           6-2-13 [Visualization with Plotly](#6213)\n*           6-2-14 [Conclusion](#6214)\n*       6-3 [Data Preprocessing](#63)\n*           6-3-1 [Features](#631)\n*           6-3-2 [Explorer Dataset](#632)\n*       6-4 [Data Cleaning](#64)\n*   7- [Model Deployment](#7)\n*       7-1[ Families of ML algorithms](#71)\n*       7-2[ Prepare Features & Targets](#72)\n*       7-3[ Accuracy and precision](#73)\n*       7-4[ KNN](#74)\n*       7-5 [Radius Neighbors Classifier](#75)\n*       7-6 [Logistic Regression](#76)\n*       7-7 [Passive Aggressive Classifier](#77)\n*       7-8 [Naive Bayes](#78)\n*       7-9 [MultinomialNB](#79)\n*       7-10 [BernoulliNB](#710)\n*       7-11 [SVM](#711)\n*       7-12 [Nu-Support Vector Classification](#712)\n*       7-13 [Linear Support Vector Classification](#713)\n*       7-14 [Decision Tree](#714)\n*       7-15 [ExtraTreeClassifier](#715)\n*       7-16 [Neural network](#716)\n*            7-16-1 [What is a Perceptron?](#7161)\n*       7-17 [RandomForest](#717)\n*       7-18 [Bagging classifier ](#718)\n*       7-19 [AdaBoost classifier](#719)\n*       7-20 [Gradient Boosting Classifier](#720)\n*   8- [Conclusion](#8)\n*   9- [References](#9)","1c94b741":"<a id=\"717\"><\/a> <br>\n## 7-17 Bagging classifier ","19ffeb3e":"<a id=\"30\"><\/a> <br>\n## 6-3-1 Features","2d165ddd":"To give a statistical summary about the dataset, we can use **describe()","6952230f":"<a id=\"628\"><\/a> <br>\n### 6-2-8 jointplot","c8373db4":"<a id=\"6211\"><\/a> <br>\n### 6-2-11 radviz","17038ccd":"<a id=\"6\"><\/a> <br>\n## 6- Exploratory Data Analysis(EDA)\n In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n\nBy the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n\n*   5-1 Data Collection\n*   5-2 Visualization\n*   5-3 Data Preprocessing\n*   5-4 Data Cleaning\n<img src=\"http:\/\/s9.picofile.com\/file\/8338476134\/EDA.png\">","e8897caf":"Fork and Run this Notebook on GitHub:\n\n> #### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n> #### [ Kaggle](https:\/\/www.kaggle.com\/mjbahmani)\n\n--------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated** ","a8eb39b6":"<a id=\"629\"><\/a> <br>\n###  6-2-9 andrews_curves","e6150d6e":"To check the first 5 rows of the data set, we can use head(5).","4f9423e3":"**<< Note >>**\n>Preprocessing and generation pipelines depend on a model type.","52fc5b14":"<a id=\"6\"><\/a> <br>\n## 6-3 Data Preprocessing","df2557fa":"To check out how many null info are on the dataset, we can use **isnull().sum().","4fad10ea":"To check out last 5 row of the data set, we use tail() function","881f6134":"<a id=\"713\"><\/a> <br>\n## 7-13 Decision Tree","43fa9a1b":"<a id=\"711\"><\/a> <br>\n## 7-11 Nu-Support Vector Classification\n","4f388360":"<a id=\"719\"><\/a> <br>\n## 7-19 Gradient Boosting Classifier","920b13b0":"**<< Note 2 >>**\n<br>\n> in pandas's data frame you can perform some query such as \"where\".","15e05eaf":"<a id=\"6210\"><\/a> <br>\n### 6-2-10 Heatmap","4c8b6988":"<a id=\"74\"><\/a> <br>\n## 7-4 K-Nearest Neighbours","f350e720":"<a id=\"4\"><\/a> <br>\n## 4- Inputs & Outputs\n<a id=\"41\"><\/a> <br>\n### 4-1 Inputs\n**Iris** is a very popular **classification** and **clustering** problem in machine learning and it is such as \"Hello world\" program when you start learning a new programming language. then I decided to apply Iris on  20 machine learning method on it.\nAs a result, **iris dataset is used as the input of all algorithms**.\n<a id=\"42\"><\/a> <br>\n### 4-2 Outputs\nthe outputs for our algorithms totally depend on the type of classification or clustering algorithms.\nthe outputs can be the number of clusters or predict for new input.\n\n**setosa**: Iris setosa, true or false, used as target.\n**versicolour**: Iris versicolour, true or false, used as target.\n**virginica**: Iris virginica, true or false, used as a target.","9f9a5078":"<a id=\"626\"><\/a> <br>\n### 6-2-6 pairplot","cebd8495":"<a id=\"621\"><\/a> <br>\n### 6-2-1 Scatter plot","6d51e3e9":"For getting some information about the dataset you can use **info()** command","a8a07189":"From the plot, we can see that the species setosa is separataed from the other two across all feature combinations\n\nWe can also replace the histograms shown in the diagonal of the pairplot by kde.","8cb7d96b":"<a id=\"52\"><\/a> <br>\n### 5-2 Version","5d58348b":"<a id=\"6214\"><\/a> <br>\n### 6-2-14 Conclusion\nwe have used Python to apply data visualization tools to the Iris dataset. Color and size changes were made to the data points in scatterplots. I changed the border and fill color of the boxplot and violin, respectively.","2ac7b788":"<a id=\"718\"><\/a> <br>\n##  7-18 AdaBoost classifier","5f80b990":"<a id=\"715\"><\/a> <br>\n## 7-15 Neural network","ab9e7121":"<a id=\"623\"><\/a> <br>\n### 6-2-3 Histogram\nWe can also create a **histogram** of each input variable to get an idea of the distribution.\n\n[go to top](#top)","9d523803":"<a id=\"710\"><\/a> <br>\n## 7-10 SVM","19f6b8d6":"<a id=\"632\"><\/a> <br>\n### 6-3-2 Explorer Dataset","a0640937":"<a id=\"6212\"><\/a> <br>\n### 6-2-12 Bar Plot","fd566d75":"<a id=\"627\"><\/a> <br>\n###  6-2-7 kdeplot","a3972fd9":"It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\n\n","d18aaa0c":"<a id=\"720\"><\/a> <br>\n## 7-20 Linear Discriminant Analysis","bba04f15":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:","082edc6d":"<a id=\"5\"><\/a> <br>\n## 5 Loading Packages\nIn this kernel we are using the following packages:","c6a67177":"<a id=\"53\"><\/a> <br>\n### 5-3 Setup\n\nA few tiny adjustments for better **code readability**","ad6819eb":"<a id=\"78\"><\/a> <br>\n## 7-8 Naive Bayes","79416c1b":" <img src=\"http:\/\/s8.picofile.com\/file\/8338227868\/packages.png\">\n","4ff9a52d":"<a id=\"624\"><\/a> <br>\n### 6-2-4 Multivariate Plots\nNow we can look at the interactions between the variables.\n\nFirst, let\u2019s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables.","062a3de0":"<a id=\"76\"><\/a> <br>\n## 7-6 Logistic Regression","555a4b5e":"<a id=\"32\"><\/a> <br>\n## 7- Model Deployment\nIn this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.","119a65bd":"<a id=\"9\"><\/a> <br>\n# 9- References & Credits\n1. [Iris image](https:\/\/rpubs.com\/wjholst\/322258)\n1. [IRIS](https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris)\n1. [https:\/\/skymind.ai\/wiki\/machine-learning-workflow](https:\/\/skymind.ai\/wiki\/machine-learning-workflow)\n1. [IRIS-wiki](https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris)\n1. [Problem-define](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n1. [Sklearn](http:\/\/scikit-learn.org\/)\n1. [machine-learning-in-python-step-by-step](https:\/\/machinelearningmastery.com\/machine-learning-in-python-step-by-step\/)\n1. [Data Cleaning](http:\/\/wp.sigmod.org\/?p=2288)\n1. [competitive data science](https:\/\/www.coursera.org\/learn\/competitive-data-science\/)\n1. [Top 28 Cheat Sheets for Machine Learning](https:\/\/www.analyticsvidhya.com\/blog\/2017\/02\/top-28-cheat-sheets-for-machine-learning-data-science-probability-sql-big-data\/)\n1. [geeksforgeeks](https:\/\/www.geeksforgeeks.org\/data-preprocessing-machine-learning-python\/)\n1. [https:\/\/wp.sigmod.org\/?p=2288](https:\/\/wp.sigmod.org\/?p=2288)\n1. [https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall)\n1. [medium](https:\/\/medium.com\/@jebaseelanravi96\/machine-learning-iris-classification-33aa18a4a983)\n-------------\n","f2db0648":"<a id=\"714\"><\/a> <br>\n## 7-14 ExtraTreeClassifier","d96ba61c":"Go to first step: [**Course Home Page**](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\nGo to next step : [**Mathematics and Linear Algebra**](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)","d55c8f12":"<a id=\"51\"><\/a> <br>\n###   5-1 Import","d5b7b21c":"<a id=\"712\"><\/a> <br>\n## 7-12 Linear Support Vector Classification","41398617":"\nWe can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n\nYou should see 150 instances and 5 attributes:","5107446a":"As you can see in the below in python, it is so easy perform some query on the dataframe:","f8a20205":"-----------------\n<a id=\"8\"><\/a> <br>\n# 8- Conclusion","e1f201d4":"<a id=\"34\"><\/a> <br>\n## 7-2 Prepare Features & Targets\nFirst of all seperating the data into independent variable(Feature) and dependent(Target) variables.\n\n**<< Note 4 >>**\n1. X==>>Feature\n1. y==>>Target","01fd6270":"## <div style=\"text-align: center\"> +20 ML Algorithms +15 Plot for Beginners<\/div>\n<div style=\"text-align: center\"><b>Quite Practical and Far from any Theoretical Concepts<\/b><\/div>\n<div style=\"text-align:center\">last update: <b>12\/02\/2019<\/b><\/div>\n\n> You are reading **10 Steps to Become a Data Scientist** and are now in the 9th step : \n\n1. [Leren Python](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-1)\n2. [Python Packages](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-2)\n3. [Mathematics and Linear Algebra](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)\n4. [Programming &amp; Analysis Tools](https:\/\/www.kaggle.com\/mjbahmani\/20-ml-algorithms-15-plot-for-beginners)\n5. [Big Data](https:\/\/www.kaggle.com\/mjbahmani\/a-data-science-framework-for-quora)\n6. [Data visualization](https:\/\/www.kaggle.com\/mjbahmani\/top-5-data-visualization-libraries-tutorial)\n7. [Data Cleaning](https:\/\/www.kaggle.com\/mjbahmani\/machine-learning-workflow-for-house-prices)\n8. [How to solve a Problem?](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-2)\n9. <font color=\"red\">You are in the ninth step<\/font>\n10. [Deep Learning](https:\/\/www.kaggle.com\/mjbahmani\/top-5-deep-learning-frameworks-tutorial)\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","82ba72c3":"In this kernel, I have tried to cover all the parts related to the process of **Machine Learning** with a variety of Python packages and I know that there are still some problems then I hope to get your feedback to improve it.\n<br>\n[go to top](#top)","a0d19f0c":"<a id=\"716\"><\/a> <br>\n## 7-16 RandomForest","f615328d":"You see number of unique item for Species with command below:","a78b75f1":"Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship.","5fc829c1":"How many NA elements in every column\n","35a2f16d":"<a id=\"77\"><\/a> <br>\n##  7-7 Passive Aggressive Classifier","c8bbdde0":"**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)","d71d8cbd":"<a id=\"3\"><\/a> <br>\n## 3- Problem Definition\nProblem Definition has four steps that have illustrated in the picture below:\n<img src=\"http:\/\/s8.picofile.com\/file\/8338227734\/ProblemDefination.png\">\n<a id=\"31\"><\/a> <br>\n### 3-1 Problem Feature\nwe will use the classic Iris data set. This dataset contains information about three different types of Iris flowers:\n\n1. Iris Versicolor\n1. Iris Virginica\n1. Iris Setosa\n\nThe data set contains measurements of four variables :\n\n1. sepal length \n1. sepal width\n1. petal length \n1. petal width\n\n<a id=\"32\"><\/a> <br>\n### 3-2 Aim\nThe aim is to classify iris flowers among three species (setosa, versicolor or virginica) from measurements of length and width of sepals and petals\n<a id=\"33\"><\/a> <br>\n### 3-3 Variables\nThe variables are :\n**sepal_length**: Sepal length, in centimeters, used as input.\n**sepal_width**: Sepal width, in centimeters, used as input.\n**petal_length**: Petal length, in centimeters, used as input.\n**petal_width**: Petal width, in centimeters, used as input.\n**setosa**: Iris setosa, true or false, used as target.\n**versicolour**: Iris versicolour, true or false, used as target.\n**virginica**: Iris virginica, true or false, used as target.\n\n**<< Note >>**\n> You must answer the following question:\nHow does your company expact to use and benfit from your model.\n<br>\n[go to top](#top)","9e3baafe":"<a id=\"79\"><\/a> <br>\n##  7-9 BernoulliNB","1135eccd":"<a id=\"61\"><\/a> <br>\n## 6-1 Data Collection\n**Iris dataset**  consists of 3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n\nThe rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n","6939b9d8":"To print dataset **columns**, we can use columns atribute","db5658ba":"<a id=\"64\"><\/a> <br>\n## 6-4 Data Cleaning","e580c947":"<a id=\"625\"><\/a> <br>\n### 6-2-5 violinplots","0aec0f4b":"<a id=\"62\"><\/a> <br>\n## 6-2 Visualization\nIn this section I show you  **+15  plots** with **matplotlib** and **seaborn** that is listed in the blew picture:\n <img src=\"http:\/\/s8.picofile.com\/file\/8338475500\/visualization.jpg\" \/>\n <\/br>\n[go to top](#top)","bf196e45":"<a id=\"622\"><\/a> <br>\n### 6-2-2 Box","6e8a41b8":"<a id=\"2\"><\/a> <br>\n## 2- Machine Learning Workflow\nField of \tstudy \tthat \tgives\tcomputers\tthe\tability \tto\tlearn \twithout \tbeing\nexplicitly \tprogrammed.\n\n**Arthur\tSamuel, 1959**\n\nIf you have already read some [machine learning books](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/tree\/master\/Ebooks). You have noticed that there are different ways to stream data into machine learning.\n\nmost of these books share the following steps (checklist):\n1. Define the Problem(Look at the big picture)\n1. Specify Inputs & Outputs\n1. Data Collection\n1. Exploratory data analysis\n1. Data Preprocessing\n1. Model Design, Training, and Offline Evaluation\n1. Model Deployment, Online Evaluation, and Monitoring\n1. Model Maintenance, Diagnosis, and Retraining\n\n**You can see my workflow in the below image** :\n <img src=\"http:\/\/s9.picofile.com\/file\/8338227634\/workflow.png\" \/>\n\n**you should\tfeel free\tto\tadapt \tthis\tchecklist \tto\tyour needs**\n<br>\n[go to top](#top)","f32fa031":"To pop up 5 random rows from the data set, we can use **sample(5)**  function"}}