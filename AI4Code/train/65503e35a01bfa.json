{"cell_type":{"742ed2fb":"code","f49cbcf5":"code","af542391":"code","ac2158aa":"code","abd9cd07":"code","4981307c":"code","2b1a6e0a":"code","1040027e":"code","e110306a":"code","6c0a57fc":"code","06dbb6c1":"code","5433f98f":"code","f49ecbe5":"code","0e470f46":"code","1ff059d9":"code","e7881736":"code","538d40b2":"code","da34e191":"code","c59ac9bc":"code","23e6532f":"code","8eb84640":"code","45c319bd":"code","145d66c1":"code","9289133c":"code","0a2f5b0f":"code","2ab171b2":"code","689c990c":"code","61621f48":"code","eb7f1fc3":"code","4e3d0a8f":"code","7667d1a8":"code","a121d607":"code","dec782b7":"code","a0dd83e9":"code","e0ad1e18":"code","cf278acf":"code","5dd2c0ac":"code","791efb8c":"markdown","266c57b2":"markdown","febfe81a":"markdown","c7f6263b":"markdown","145cf9a6":"markdown","fc91e68d":"markdown","671a9739":"markdown","d2a54fd0":"markdown","f8e58e69":"markdown","4983a59d":"markdown","2e6060ed":"markdown","540593c8":"markdown"},"source":{"742ed2fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f49cbcf5":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer","af542391":"\ndef remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)\n\n\n# extracting the stopwords from nltk library\nsw = stopwords.words('english')\n# displaying the stopwords\nnp.array(sw);\n\n\ndef stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)\n\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) \n    \n\n\ndef clean_loc(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_loc:\n        return x\n    else: return 'Others'\n","ac2158aa":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n","abd9cd07":"# importing data\n\nnew_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nfinal_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n","4981307c":"new_df['keyword'] = new_df['keyword'].fillna('unknown')\nnew_df['location'] = new_df['location'].fillna('unknown')\n\n\nnew_df = new_df[['target', 'location', 'text', 'keyword']]\nfinal_test = final_test[['location', 'text', 'keyword']]\n\n\n\nnew_df['text'] = new_df['text'].apply(remove_punctuation)\nnew_df['text'] = new_df['text'].apply(stopwords)\nnew_df['text'] = new_df['text'].apply(stemming)\nnew_df['text'] = new_df['text'].apply(remove_URL)\nnew_df['text'] = new_df['text'].apply(remove_html)\nnew_df['text'] = new_df['text'].apply(remove_emoji)\nnew_df['text'] = new_df['text'].apply(remove_punct)\n\n\n\nfinal_test['text'] = final_test['text'].apply(remove_punctuation)\nfinal_test['text'] = final_test['text'].apply(stopwords)\nfinal_test['text'] = final_test['text'].apply(stemming)\nfinal_test['text'] = final_test['text'].apply(remove_URL)\nfinal_test['text'] = final_test['text'].apply(remove_html)\nfinal_test['text'] = final_test['text'].apply(remove_emoji)\nfinal_test['text'] = final_test['text'].apply(remove_punct)\n","2b1a6e0a":"raw_loc = new_df.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\nnew_df['location_clean'] = new_df['location'].apply(lambda x: clean_loc(str(x)))\nfinal_test['location_clean'] = final_test['location'].apply(lambda x: clean_loc(str(x)))\n","1040027e":"from bs4 import BeautifulSoup\n\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\n","e110306a":"new_df['text'] = new_df['text'].apply(cleanText)\nnew_df['keyword'] = new_df['keyword'].apply(cleanText)\nnew_df['location_clean'] = new_df['location_clean'].apply(cleanText)\n\nfinal_test['text'] = final_test['text'].apply(cleanText)\nfinal_test['keyword'] = final_test['keyword'].fillna('unknown')\nfinal_test['keyword'] = final_test['keyword'].apply(cleanText)\nfinal_test['location_clean'] = final_test['location_clean'].apply(cleanText)\n","6c0a57fc":"keyword_df = new_df.groupby(['keyword']).count().reset_index()\nkeyword_test = final_test.groupby(['keyword']).count().reset_index()\n\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\npath = get_tmpfile(\"word2vec.model\")\nmodel = Word2Vec(common_texts, size=100, window=1, min_count=1, workers=4)\n\nmodel = Word2Vec([list(keyword_df['keyword']) + list(keyword_test['keyword'])], min_count=1)\n","06dbb6c1":"# traing and test split\ntrain, test = train_test_split(new_df, test_size=0.2, random_state=42)\n\n\n# another encoding technique for location and keyword, with the event rate\nkeyword_val = train.groupby('keyword').agg({'target': 'mean'})\nlocation_val = train.groupby('location_clean').agg({'target': 'mean'})\n\n\nnew_train = pd.merge(train, keyword_val, how='left', on = 'keyword')\nnew_train = pd.merge(new_train, location_val, how='left', on = 'location_clean')\n\nnew_test = pd.merge(test, keyword_val, how='left', on = 'keyword')\nnew_test = pd.merge(new_test, location_val, how='left', on = 'location_clean')\n","5433f98f":"new_train['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\nnew_train['target'].fillna(new_train['target'].mean(), inplace=True)\n\nnew_test['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\nnew_test['target'].fillna(new_train['target'].mean(), inplace=True)\n","f49ecbe5":"# now back to creating word embeddings vector for keywords\nwords = list(model.wv.vocab)\n\ntrain_w2v = []\ntest_w2v = []\nfinal_test_w2v = []\n\nfor elem in train['keyword']:\n    train_w2v.append(model.wv[elem])\n    \nfor elem in test['keyword']:\n    test_w2v.append(model.wv[elem])\n    \nfor elem in final_test['keyword']:\n    final_test_w2v.append(model.wv[elem])\n\n\n# below code to create doc2vec vector for text variable\n\nimport multiprocessing\ncores = multiprocessing.cpu_count()\n\nimport nltk\nfrom nltk.corpus import stopwords\n\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    return tokens\n\ntrain_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r.target]), axis=1)\ntest_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']),tags=[r.target]), axis=1)\n\n\nmodel_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\nmodel_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n\n","0e470f46":"%%time\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n\n\ndef vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n    return targets, regressors\n\ny_train, X_train = vec_for_learning(model_dbow, train_tagged)\ny_test, X_test = vec_for_learning(model_dbow, test_tagged)\n\n\n\n# now combining the doc2vec vector, with word2vec vector and keyword and location encoding \n\n\n\n","1ff059d9":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nvectorizer = CountVectorizer(analyzer='word', binary=True)\nvectorizer.fit(new_df['text'])","e7881736":"X_train_cvec = vectorizer.transform(train['text']).todense()\nX_test_cvec = vectorizer.transform(test['text']).todense()\n\n# y = tweets['target'].values\n# X.shape, y.shape","538d40b2":"X_train_cvec.shape, X_test_cvec.shape","da34e191":"tfidf = TfidfVectorizer(analyzer='word', binary=True)\ntfidf.fit(new_df['text'])","c59ac9bc":"X_train_tfidf = tfidf.transform(train['text']).todense()\nX_test_tfidf = tfidf.transform(test['text']).todense()","23e6532f":"new_X_train = list(map(lambda x,y: np.append(x,y),X_train, new_train['target_y']))\nnew_X_train_2 = list(map(lambda x,y: np.append(x,y),new_X_train, new_train['target']))\nnew_X_train_3 = list(map(lambda x,y: np.append(x,y),new_X_train_2, train_w2v))\n\n\nnew_X_test = list(map(lambda x,y: np.append(x,y),X_test, new_test['target_y']))\nnew_X_test_2 = list(map(lambda x,y: np.append(x,y),new_X_test, new_test['target']))\nnew_X_test_3 = list(map(lambda x,y: np.append(x,y),new_X_test_2, test_w2v))\n","8eb84640":"# CountVectorizer\n\n\nnew_X_train_4 = list(map(lambda x,y: np.append(x,y),new_X_train_3, X_train_cvec))\nnew_X_test_4 = list(map(lambda x,y: np.append(x,y),new_X_test_3, X_test_cvec))\n\n\n","45c319bd":"# TFIDF\n\nnew_X_train_5 = list(map(lambda x,y: np.append(x,y),new_X_train_4, X_train_tfidf))\nnew_X_test_5 = list(map(lambda x,y: np.append(x,y),new_X_test_4, X_test_tfidf))\n\n","145d66c1":"new_X_test_5[0].dtype","9289133c":"# Simple logistic regression\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\n\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(new_X_train_5, y_train)\ny_pred = logreg.predict(new_X_test_5)\nprint ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\nprint ('Testing F1 score : {}'.format(f1_score(y_test, y_pred, average='weighted')))\n\n","0a2f5b0f":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_jobs=50, random_state=0)\nclf.fit(new_X_train_5, y_train)\n","2ab171b2":"from sklearn.metrics import accuracy_score, f1_score\n\ny_pred = clf.predict(new_X_test_5)\nprint ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\nprint ('Testing F1 score : {}'.format(f1_score(y_test, y_pred, average='weighted')))\n\n","689c990c":"var_lst = ['var_'+str(i) for i in range(len(new_X_train_4[0]))]\n\nimport xgboost as xgb\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'binary:logistic',\n    'silent': 1,\n    'seed' : 0,\n    'n_estimators': 200,\n    'eval_metric': 'logloss'\n}\ndtrain = xgb.DMatrix(new_X_train_4, y_train, feature_names=var_lst)\nxgb_model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n\n\ndtest = xgb.DMatrix(new_X_test_4,feature_names = var_lst )\n\ny_pred = xgb_model.predict(dtest)\n# print ('Testing accuracy : {}'.format(accuracy_score(y_test, y_pred)))\nprint ('Testing F1 score : {}'.format(f1_score(y_test, y_pred.round(), average='weighted')))","61621f48":"new_X_all_4 = new_X_train_4+new_X_test_4\nnew_y_all_4 = y_train+y_test","eb7f1fc3":"new_X_all_4_np = np.array(new_X_all_4)\nnew_y_all_4_np = np.array(new_y_all_4)","4e3d0a8f":"from sklearn.model_selection import StratifiedKFold\n\nmodels = []\nn_splits = 5\nfold = 0 \nfor train_index, test_index in StratifiedKFold(n_splits=n_splits).split(new_X_all_4_np, new_y_all_4_np):\n\n    X_train, X_test = new_X_all_4_np[train_index], new_X_all_4_np[test_index]\n    y_train, y_test = new_y_all_4_np[train_index], new_y_all_4_np[test_index]\n\n    clf = LogisticRegression(max_iter=400)\n\n    clf.fit(X_train,y_train)\n\n\n    models.append(clf)\n    fold += 1\n    print(fold)","7667d1a8":"from sklearn.model_selection import StratifiedKFold\n\nmodels = []\nn_splits = 5\nfold = 0 \nfor train_index, test_index in StratifiedKFold(n_splits=n_splits).split(new_X_all_4_np, new_y_all_4_np):\n\n    X_train, X_test = new_X_all_4_np[train_index], new_X_all_4_np[test_index]\n    y_train, y_test = new_y_all_4_np[train_index], new_y_all_4_np[test_index]\n\n    clf = RandomForestClassifier(n_jobs=50, random_state=0)\n\n    clf.fit(X_train,y_train)\n\n\n    models.append(clf)\n    fold += 1\n    print(fold)","a121d607":"final_test['target'] = 0\n\nfinal_test = pd.merge(final_test, keyword_val, how='left', on = 'keyword')\nfinal_test = pd.merge(final_test, location_val, how='left', on = 'location_clean')\nfinal_test['target_y'].fillna(new_train['target_y'].mean(), inplace=True)\nfinal_test['target'].fillna(new_train['target'].mean(), inplace=True)\n\nfinal_test_tagged = final_test.apply(lambda r: TaggedDocument(words=tokenize_text(r['text']),tags=[r.target]), axis=1)\nf_y_test, f_X_test = vec_for_learning(model_dbow, final_test_tagged)\n\nfinal_X_test = list(map(lambda x,y: np.append(x,y), f_X_test, final_test['target_y']))\nfinal_X_test_2 = list(map(lambda x,y: np.append(x,y),final_X_test, final_test['target']))\nfinal_X_test_3 = list(map(lambda x,y: np.append(x,y),final_X_test_2, final_test_w2v))\n","dec782b7":"X_f_test_cvec = vectorizer.transform(final_test['text']).todense()\nX_f_test_tfidf = tfidf.transform(final_test['text']).todense()\n\n\nfinal_X_test_4 = list(map(lambda x,y: np.append(x,y),final_X_test_3, X_f_test_cvec))\n# final_X_test_5 = list(map(lambda x,y: np.append(x,y),final_X_test_4, X_f_test_tfidf))","a0dd83e9":"new_final_4 = list((lambda x: map(x, float), final_X_test_4))","e0ad1e18":"new_final_4_np = np.array(final_X_test_4)\n\n# new_y_all_4_np = np.array(new_y_all_4)\n# y_hat = clf.predict(new_final_4_np)\n\nfinal = np.zeros((new_final_4_np.shape[0]))\n\nfor i in range(n_splits):\n        clf = models[i]\n        preds = clf.predict(new_final_4_np)\n        \n        final += preds\/n_splits\n\n    \nfinal = np.where(final>=0.5,1,0)","cf278acf":"# xgboost\n\n# dtest = xgb.DMatrix(final_X_test_4,feature_names = var_lst )\n\n# y_pred_xgb = xgb_model.predict(dtest)","5dd2c0ac":"# creating the submissions file\nsub_sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsubmit = sub_sample.copy()\nsubmit.target = final\nsubmit.to_csv('submit_rf_cv.csv',index=False)\n\n","791efb8c":"# Random Forest - Cross Validation","266c57b2":"# Random Forest","febfe81a":"## Bringing in all the variables","c7f6263b":"# Final Test Prediction","145cf9a6":"After having learned from so many great people and their work on here, making my first public notebook. \n\nTested a few things\n1. Feature creation techniques for text\n2. ML algos\n\nNext up: Deep learning models, and creating cleaner notebooks","fc91e68d":"### Word2Vec ","671a9739":"### Count Vectorizer and TFIDF","d2a54fd0":"Please let me know if there are any questions. And also, would appreciate an upvote if you found the notebook helpful.\n","f8e58e69":"# Logistic Regression","4983a59d":"### Categorical Variable Encoding","2e6060ed":"# Logistic Regression - Cross Validation","540593c8":"# XG Boost"}}