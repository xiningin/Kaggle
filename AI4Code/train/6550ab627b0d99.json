{"cell_type":{"e5702f91":"code","360048f1":"code","6089f420":"code","19916a2c":"code","3e9fafd3":"code","754a3ed0":"code","855a20d1":"code","faccdda0":"code","915b95b2":"code","5f4c6899":"code","9efc70a9":"code","757a1e76":"code","27325fa7":"code","64891e31":"code","8d8d38f0":"code","349a6825":"code","d09667ff":"code","23f1e1c3":"code","f4f99240":"code","056b6536":"code","1c84ba5a":"markdown","f57e3e41":"markdown","869a84cb":"markdown","049cbde0":"markdown","d2e629d3":"markdown","95970838":"markdown","31f90e1b":"markdown","e208e04c":"markdown","34dcb0d3":"markdown","709b6071":"markdown","1dd1487c":"markdown","201348c2":"markdown","ba0a728f":"markdown","eb4025ec":"markdown","f569c73b":"markdown","5302e16a":"markdown","ddfd2ff0":"markdown","7455f5ce":"markdown","1b9ef96a":"markdown","a96c1b28":"markdown","2ac57ba6":"markdown","162709c0":"markdown","28e43eec":"markdown","047ed123":"markdown","b466d162":"markdown","11be6433":"markdown","ce1e69ae":"markdown","c7993930":"markdown"},"source":{"e5702f91":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nmatplotlib.style.use('ggplot')","360048f1":"# Load mtcars data set\nmtcars = pd.read_csv(\"..\/input\/mtcars\/mtcars.csv\")\n\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\");","6089f420":"from sklearn import linear_model","19916a2c":"# Initialize model\nregression_model = linear_model.LinearRegression()\n\n# Train the model using the mtcars data\nregression_model.fit(X = pd.DataFrame(mtcars[\"wt\"]), \n                     y = mtcars[\"mpg\"])\n\n# Check trained model y-intercept\nprint(regression_model.intercept_)\n\n# Check trained model coefficients\nprint(regression_model.coef_)","3e9fafd3":"regression_model.score(X = pd.DataFrame(mtcars[\"wt\"]), \n                       y = mtcars[\"mpg\"])","754a3ed0":"train_prediction = regression_model.predict(X = pd.DataFrame(mtcars[\"wt\"]))\n\n# Actual - prediction = residuals\nresiduals = mtcars[\"mpg\"] - train_prediction\n\nresiduals.describe()","855a20d1":"SSResiduals = (residuals**2).sum()\n\nSSTotal = ((mtcars[\"mpg\"] - mtcars[\"mpg\"].mean())**2).sum()\n\n# R-squared\n1 - (SSResiduals\/SSTotal)","faccdda0":"\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\",\n           xlim = (0,7))\n\n# Plot regression line\nplt.plot(mtcars[\"wt\"],      # Explanitory variable\n         train_prediction,  # Predicted values\n         color=\"blue\");","915b95b2":"mtcars_subset = mtcars[[\"mpg\",\"wt\"]]\n\nsuper_car = pd.DataFrame({\"mpg\":50,\"wt\":10}, index=[\"super\"])\n\nnew_cars = mtcars_subset.append(super_car)\n\n# Initialize model\nregression_model = linear_model.LinearRegression()\n\n# Train the model using the new_cars data\nregression_model.fit(X = pd.DataFrame(new_cars[\"wt\"]), \n                     y = new_cars[\"mpg\"])\n\ntrain_prediction2 = regression_model.predict(X = pd.DataFrame(new_cars[\"wt\"]))\n\n# Plot the new model\nnew_cars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\", xlim=(1,11), ylim=(10,52))\n\n# Plot regression line\nplt.plot(new_cars[\"wt\"],     # Explanatory variable\n         train_prediction2,  # Predicted values\n         color=\"blue\");","5f4c6899":"plt.figure(figsize=(9,9))\n\nstats.probplot(residuals, dist=\"norm\", plot=plt);","9efc70a9":"def rmse(predicted, targets):\n    \"\"\"\n    Computes root mean squared error of two numpy ndarrays\n    \n    Args:\n        predicted: an ndarray of predictions\n        targets: an ndarray of target values\n    \n    Returns:\n        The root mean squared error as a float\n    \"\"\"\n    return (np.sqrt(np.mean((targets-predicted)**2)))\n\nrmse(train_prediction, mtcars[\"mpg\"])","757a1e76":"from sklearn.metrics import mean_squared_error\n\nRMSE = mean_squared_error(train_prediction, mtcars[\"mpg\"])**0.5\n\nRMSE","27325fa7":"# Initialize model\npoly_model = linear_model.LinearRegression()\n\n# Make a DataFrame of predictor variables\npredictors = pd.DataFrame([mtcars[\"wt\"],           # Include weight\n                           mtcars[\"wt\"]**2]).T     # Include weight squared\n\n# Train the model using the new_cars data\npoly_model.fit(X = predictors, \n               y = mtcars[\"mpg\"])\n\n# Check trained model y-intercept\nprint(\"Model intercept\")\nprint(poly_model.intercept_)\n\n# Check trained model coefficients (scaling factor given to \"wt\")\nprint(\"Model Coefficients\")\nprint(poly_model.coef_)\n\n# Check R-squared\nprint(\"Model Accuracy:\")\nprint(poly_model.score(X = predictors, \n                 y = mtcars[\"mpg\"]))","64891e31":"# Plot the curve from 1.5 to 5.5\npoly_line_range = np.arange(1.5, 5.5, 0.1)\n\n# Get first and second order predictors from range\npoly_predictors = pd.DataFrame([poly_line_range,\n                               poly_line_range**2]).T\n\n# Get corresponding y values from the model\ny_values = poly_model.predict(X = poly_predictors)\n\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\",\n           xlim = (0,7))\n\n# Plot curve line\nplt.plot(poly_line_range,   # X-axis range\n         y_values,          # Predicted values\n         color=\"blue\");","8d8d38f0":"preds = poly_model.predict(X=predictors)\n\nrmse(preds , mtcars[\"mpg\"])","349a6825":"# Initialize model\npoly_model = linear_model.LinearRegression()\n\n# Make a DataFrame of predictor variables\npredictors = pd.DataFrame([mtcars[\"wt\"],           \n                           mtcars[\"wt\"]**2,\n                           mtcars[\"wt\"]**3,\n                           mtcars[\"wt\"]**4,\n                           mtcars[\"wt\"]**5,\n                           mtcars[\"wt\"]**6,\n                           mtcars[\"wt\"]**7,\n                           mtcars[\"wt\"]**8,\n                           mtcars[\"wt\"]**9,\n                           mtcars[\"wt\"]**10]).T     \n\n# Train the model using the new_cars data\npoly_model.fit(X = predictors, \n               y = mtcars[\"mpg\"])\n\n# Check trained model y-intercept\nprint(\"Model intercept\")\nprint(poly_model.intercept_)\n\n# Check trained model coefficients (scaling factor given to \"wt\")\nprint(\"Model Coefficients\")\nprint(poly_model.coef_)\n\n# Check R-squared\npoly_model.score(X = predictors, \n                 y = mtcars[\"mpg\"])","d09667ff":"p_range = np.arange(1.5, 5.45, 0.01)\n\npoly_predictors = pd.DataFrame([p_range, p_range**2, p_range**3,\n                              p_range**4, p_range**5, p_range**6, p_range**7, \n                              p_range**8, p_range**9, p_range**10]).T  \n\n# Get corresponding y values from the model\ny_values = poly_model.predict(X = poly_predictors)\n\nmtcars.plot(kind=\"scatter\",\n           x=\"wt\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\",\n           xlim = (0,7))\n\n# Plot curve line\nplt.plot(p_range,   # X-axis range\n         y_values,          # Predicted values\n         color=\"blue\");","23f1e1c3":"# Initialize model\nmulti_reg_model = linear_model.LinearRegression()\n\n# Train the model using the mtcars data\nmulti_reg_model.fit(X = mtcars.loc[:,[\"wt\",\"hp\"]], \n                     y = mtcars[\"mpg\"])\n\n# Check trained model y-intercept\nprint(multi_reg_model.intercept_)\n\n# Check trained model coefficients (scaling factor given to \"wt\")\nprint(multi_reg_model.coef_)\n\n# Check R-squared\nmulti_reg_model.score(X = mtcars.loc[:,[\"wt\",\"hp\"]], \n                      y = mtcars[\"mpg\"])","f4f99240":"mtcars.plot(kind=\"scatter\",\n           x=\"hp\",\n           y=\"mpg\",\n           figsize=(9,9),\n           color=\"black\");","056b6536":"# Initialize model\nmulti_reg_model = linear_model.LinearRegression()\n\n# Include squared terms\npoly_predictors = pd.DataFrame([mtcars[\"wt\"],\n                                mtcars[\"hp\"],\n                                mtcars[\"wt\"]**2,\n                                mtcars[\"hp\"]**2]).T\n\n# Train the model using the mtcars data\nmulti_reg_model.fit(X = poly_predictors, \n                    y = mtcars[\"mpg\"])\n\n# Check R-squared\nprint(\"R-Squared\")\nprint( multi_reg_model.score(X = poly_predictors , \n                      y = mtcars[\"mpg\"]) )\n\n# Check RMSE\nprint(\"RMSE\")\nprint(rmse(multi_reg_model.predict(poly_predictors),mtcars[\"mpg\"]))","1c84ba5a":"The quadratic function seems to fit the data a little better than the linear one. Let's investigate further by using the new model to make predictions on the original data and check the root mean squared error:","f57e3e41":"# Wrap Up","869a84cb":"Variables often exhibit non-linear relationships that can't be fit well with a straight line. In these cases, we can use linear regression to fit a curved line the data by adding extra higher order terms (squared, cubic, etc.) to the model. A linear regression that involves higher order terms is known as \"polynomial regression.\"","049cbde0":"The output of the score function for linear regression is \"R-squared\", a value that ranges from 0 to 1 which describes the proportion of variance in the response variable that is explained by the model. In this case, car weight explains roughly 75% of the variance in mpg.\n\nThe R-squared measure is based on the residuals: differences between what the model predicts for each data point and the actual value of each data point. We can extract the model's residuals by making a prediction with the model on the data and then subtracting the actual value from each prediction:","d2e629d3":"R-squared is calculated as 1 - (SSResiduals\/SSTotal) were SSResiduals is the sum of the squares of the model residuals and SSTotal is the sum of the squares of the difference between each data point and the mean of the data. We could calculate R-squared by hand like this:","95970838":"When residuals are normally distributed, they tend to lie along the straight line on the Q-Q plot. In this case residuals appear to follow a slightly non-linear pattern: the residuals are bowed a bit away from the normality line on each end. This is an indication that simple straight line might not be sufficient to fully describe the relationship between weight and mpg.\n\nAfter making model predictions, it is useful to have some sort of metric to evaluate oh well the model performed. Adjusted R-squared is one useful measure, but it only applies to the regression model itself: we'd like some universal evaluation metric that lets us compare the performance of different types of models. Root mean squared error (RMSE) is a common evaluation metric for predictions involving real numbers. Root mean squared error is square root of the average of the squared error (residuals.). If you recall, we wrote a function to calculate RMSE way back in lesson lesson 12:","31f90e1b":"The improved R-squared score suggests horsepower has a linear relationship with mpg. Let's investigate with a plot:","e208e04c":"The new R-squared and lower RMSE suggest this is a better model than any we made previously and we wouldn't be too concerned about overfitting since it only includes 2 variables and 2 squared terms. Note that when working with multidimensional models, it becomes difficult to visualize results, so you rely heavily on numeric output.\n\nWe could continue adding more explanatory variables in an attempt to improve the model. Adding variables that have little relationship with the response or including variables that are too closely related to one another can hurt your results when using linear regression. You should also be wary of numeric variables that take on few unique values since they can act more like categorical variables than numeric ones.","34dcb0d3":"The output shows us that including the weight squared term appears to improve the model's performance because the R-squared increased from 0.75 to 0.8190. It should be noted, however, that adding more variables to a linear regression model can never cause R-squared to decrease, so we only want to add variables if there is a substantial improvement in performance.\n\nLet's plot the curved line defined by the new model to see if the fit looks better than the old one. To start off, let's create a function that takes an array of x values, model coefficients and an intercept term and returns the x values and fitted y values corresponding to those x values.","709b6071":"# Linear Regression Basics","1dd1487c":"The scatterplot shows a roughly linear relationship between weight and mpg, suggesting a linear regression model might work well.\n\nPython's scikit-learn library contains a wide range of functions for predictive modeling. Let's load its linear regression training function and fit a line to the mtcars data:","201348c2":"Linear regression is one of the most common techniques for making real numbered predictions from data. It is a good place to start any time you need to make a numeric prediction. Next time we'll revisit the titanic survival data set and focus classification: assigning observations to categories.","ba0a728f":"# Multiple Linear Regression","eb4025ec":"Now that we have a linear model, let's plot the line it fits on our scatterplot to get a sense of how well it fits the data:","f569c73b":"Although this is an extreme, contrived case, the plot above illustrates how much influence a single outlier can have on a linear regression model.\n\nIn a well-behaved linear regression model, we'd like the residuals to be roughly normally distributed. That is, we'd like a roughly even spread of error above and below the regression line. We can investigate the normality of residuals with a Q-Q (quantile-quantile) plot. Make a qqplot by passing the residuals to the stats.probplot() function in the scipy.stats library:","5302e16a":"Since the RMSE of the quadratic model is lower than the old one and the adjusted R-squared is higher, it is probably a better model. We do, however, have to be careful about overfitting the training data.\n\nOverfitting describes a situation where our model fits the data we use to create it (training data) too closely, resulting in poor generalization to new data. This is why we generally don't want to use training data to evaluate a model: it gives us a biased, usually overly optimistic evaluation. One of the strengths of first and second order linear regression is that they are so simple, they are unlikely to overfit data very much. The more complex the model we create and the more freedom it has to fit the training data, the greater risk we run of overfitting. For example, we could keep including more polynomial terms in our regression model to fit the training data more closely and achieve lower RMSE scores against the training set, but this would almost certainly not generalize well to new data. Let's illustrate this point by fitting a 10th order model to the mtcars data:","ddfd2ff0":"In the last few lessons we learned about statistical inference techniques including the t-test, chi-squared test and ANOVA which let you analyze differences between data samples. Predictive modeling--using a data samples to make predictions about unseen data, such as data that has yet to be generated--is another common data analytics task. Predictive modeling is a form of machine learning, which describes using computers to automate the process of finding patterns in data.\n\nMachine learning is the driving force behind all kinds of modern conveniences and automation systems like ATMs that can read handwritten text, smartphones that translate speech to text and self-driving cars. The methods used in such cutting-edge applications are more advanced than anything we'll cover in this introduction, but they are all based on the principles of taking data and applying some learning algorithm to it to arrive at some sort of prediction.\n\nThis lesson is intended to provide a high level overview of linear regression and how to begin using it in Python.","7455f5ce":"# Python for Data 27: Linear Regression\n[back to index](https:\/\/www.kaggle.com\/hamelg\/python-for-data-analysis-index)","1b9ef96a":"Notice how the 10th order polynomial model curves wildly in some places to fit the training data. While this model happens to yield a closer fit to the training data, it will almost certainly fail to generalize well to new data as it leads to absurd predictions such as a car having less than 0 mpg if it weighs 5000lbs.","a96c1b28":"When faced with a predictive modeling task, you'll often have several variables in your data that may help explain variation in the response variable. You can include more explanatory variables in a linear regression model by including more columns in the data frame you pass to the model training function. Let's make a new model that adds the horsepower variable to our original model:","2ac57ba6":"\nLinear regression is a predictive modeling technique for predicting a numeric response variable based on one or more explanatory variables. The term \"regression\" in predictive modeling generally refers to any modeling task that involves predicting a real number (as opposed classification, which involves predicting a category or class.). The term \"linear\" in the name linear regression refers to the fact that the method models data with linear combination of the explanatory variables. A linear combination is an expression where one or more variables are scaled by a constant factor and added together. In the case of linear regression with a single explanatory variable, the linear combination used in linear regression can be expressed as:\n\nresponse = intercept + constant \u2217 explanatory\n\nThe right side if the equation defines a line with a certain y-intercept and slope times the explanatory variable. In other words, linear regression in its most basic form fits a straight line to the response variable. The model is designed to fit a line that minimizes the squared differences (also called errors or residuals.). We won't go into all the math behind how the model actually minimizes the squared errors, but the end result is a line intended to give the \"best fit\" to the data. Since linear regression fits data with a line, it is most effective in cases where the response and explanatory variable have a linear relationship.\n\nLet's revisit the mtcars data set and use linear regression to predict vehicle gas mileage based on vehicle weight. First, let's load some libraries and look at a scatterplot of weight and mpg to get a sense of the shape of the data:","162709c0":"The output above shows the model intercept and coefficients used to create the best fit line. In this case the y-intercept term is set to 37.2851 and the coefficient for the weight variable is -5.3445. In other words, the model fit the line: mpg = 37.2851 - 5.3445 * wt.\n\nWe can get a sense of how much of the variance in the response variable is explained by the model using the model.score() function:","28e43eec":"# Polynomial Regression","047ed123":"# Next Lesson: [Python for Data 28: Logistic Regression](https:\/\/www.kaggle.com\/hamelg\/python-for-data-28-logistic-regression)\n[back to index](https:\/\/www.kaggle.com\/hamelg\/python-for-data-analysis-index)","b466d162":"While mpg does tend to decline with horsepower, the relationship appears more curved than linear so adding polynomial terms to our multiple regression model could yield a better fit:","11be6433":"The regression line looks like a reasonable fit and it follows our intuition: as car weight increases we would expect fuel economy to decline.\n\nOutliers can have a large influence on linear regression models: since regression deals with minimizing squared residuals, large residuals have a disproportionately large influence on the model. Plotting the result helps us detect influential outliers. In this case there does not appear to be any influential outliers. Let's add an outlier--a super heavy fuel efficient car--and plot a new regression model:","ce1e69ae":"Notice the R-squared score has increased substantially from our quadratic model. Let's plot the best fit line to investigate what the model is doing:","c7993930":"Instead of defining your own RMSE function, you can use the scikit-learn library's mean squared error function and take the square root of the result:"}}