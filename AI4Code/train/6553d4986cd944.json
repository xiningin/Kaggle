{"cell_type":{"d4bd561e":"code","f16b2cbb":"code","6134c943":"code","a150271d":"code","7448bfa0":"code","494cd15c":"code","88af8554":"code","a8b96f4b":"code","a5e71d91":"code","1785928f":"code","7dff09f4":"code","4edd4b2d":"code","0057b1fd":"code","895a4e5d":"code","7a564393":"code","db356d5a":"code","8db42f6c":"code","6a91f9ac":"code","421eb9ec":"code","2f25b97f":"code","e8dcf07b":"code","9e85d9ff":"code","c441acad":"code","56bc18cc":"markdown","fa69aeb0":"markdown","9f53569c":"markdown","4a728203":"markdown","a590b8ae":"markdown","bc48316c":"markdown","27ae0c5e":"markdown","5eea066a":"markdown","26ba91c1":"markdown","3414d6d7":"markdown","7b8e36a2":"markdown","4643458d":"markdown","f43ef5c6":"markdown","6134a779":"markdown","6fb8bf96":"markdown","1fed2765":"markdown","2dd3b332":"markdown","1a74904b":"markdown","ce3c8271":"markdown"},"source":{"d4bd561e":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, metrics\nimport lightgbm as lgb\nimport pickle\nimport gc\n\nimport os\nprint(pd.__version__)\n\ndirpath = os.getcwd()\nprint(\"current directory is : \" + dirpath)\nprint(os.listdir(\"\/kaggle\/input\"))\nPATH = \"..\/input\/cat-in-the-dat\/\"\n\ntrain_raw = pd.read_csv(PATH + 'train.csv')\ntest_raw = pd.read_csv(PATH + 'test.csv')\n\nprint(train_raw.shape)\nprint(test_raw.shape)\ntrain_raw.head(3)\nprint(train_raw.axes)\nprint(train_raw.dtypes)\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n\n    #dataframe = dataframe.sort_values('click_time')  #uncomment if you have time-series data\n    valid_rows = int(len(dataframe) * valid_fraction)\n    train = dataframe[:-valid_rows * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_rows * 2:-valid_rows]\n    test = dataframe[-valid_rows:]\n    \n    return train, valid, test\n\ndef train_model(train, valid, test=None, feature_cols=None):\n    if feature_cols is None:\n        feature_cols = train.columns.drop(['id', 'target'])\n    dtrain = lgb.Dataset(train[feature_cols], label=train['target'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['target'])\n    \n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    num_round = 1000\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n                    early_stopping_rounds=20, verbose_eval=False)\n    \n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['target'], valid_pred)\n    print(f\"Validation AUC score: {valid_score}\")\n    \n    if test is not None: \n        test_pred = bst.predict(test[feature_cols])\n        test_score = metrics.roc_auc_score(test['target'], test_pred)\n        return bst, valid_score, test_score\n    else:\n        return bst, valid_score","f16b2cbb":"## replacing extra levels in test with mode:\ntest_all_feat = test_raw.copy()\n\nfor iter in ('nom_8','nom_9'):\n    s = pd.Series(list(set(test_all_feat[iter].values) - set(train_raw[iter].values)))\n    s1 = pd.Series(test_all_feat[iter].isin(s))\n    test_all_feat.loc[s1,iter] = test_all_feat[iter].mode().values\nfor iter in ('nom_8','nom_9'):\n    s = pd.Series(list(set(test_all_feat[iter].values) - set(train_raw[iter].values)))\n    s1 = pd.Series(test_all_feat[iter].isin(s))\n    print(s1.value_counts())\n","6134c943":"train_bench = train_raw.copy()\ntrain_bench = train_bench.assign(bin_3 = (train_bench[\"bin_3\"]==\"T\").astype(int))\ntrain_bench = train_bench.assign(bin_4 = (train_bench[\"bin_4\"]==\"Y\").astype(int))\nprint(train_bench.iloc[:4,:6])\n      \ntest_all_feat = test_all_feat.assign(bin_3 = (test_all_feat[\"bin_3\"]==\"T\").astype(int))\ntest_all_feat = test_all_feat.assign(bin_4 = (test_all_feat[\"bin_4\"]==\"Y\").astype(int))\nprint(test_all_feat.iloc[:4,:6])\n\ncat_features =list(train_bench.columns[6:22])\nprint(cat_features)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfor iter in cat_features:\n    encoder = LabelEncoder()\n    encoder.fit(train_bench[iter])\n    train_bench[iter] = encoder.transform(train_bench[iter])\n    test_all_feat[iter] = encoder.transform(test_all_feat[iter])\n    \ntest_all_feat.head(2)\n\ntrain_all_feat = train_bench.copy()","a150271d":"train, valid, test = get_data_splits(train_bench)\nfor each in [train, valid, test]:\n    print(f\"Target fraction = {each.target.mean():.4f}\")\n    \n_, baseline_score, test_score = train_model(train, valid, test)\n\n# peeking at test score just once. we will re-check it after we are done with Feature enginering.\nprint(f\"Test AUC score: {test_score}\")","7448bfa0":"cat_features = list(train_bench.columns[6:24])\nprint(cat_features)\nbin_features = list(train_bench.columns[1:6])\nprint(bin_features)\n\nimport category_encoders as ce\ncount_enc = ce.CountEncoder()\ncount_encoded = count_enc.fit_transform(train_bench[cat_features])\n\ndata = train_bench.join(count_encoded.add_suffix(\"_count\"))\nprint(data.shape)\ndata.head(3)\n\ndata_all_feat = data.copy() ## collecting all features for subsequent feature selection\n\n# Training a model on the baseline data\ntrain, valid, test = get_data_splits(data)\nprint(\"Score with count encoding\")\nbst = train_model(train, valid)\n","494cd15c":"train_bench.head(3)\nimport category_encoders as ce\ncat_features = list(train_bench.columns[6:24])\nprint(cat_features)\n\n# Create the encoder itself\ntarget_enc = ce.TargetEncoder(cols=cat_features)\n\ntrain, valid, _ = get_data_splits(train_bench)\n\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(train[cat_features], train['target'])\n\n# Transform the features, rename the columns with _target suffix, and join to dataframe\ntrain = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\nvalid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n\nprint(train.shape)\nprint(\"Score with target encoding\")\nbst = train_model(train, valid)","88af8554":"target_enc = ce.CatBoostEncoder(cols=cat_features)\n\ntrain, valid, _ = get_data_splits(train_bench)\ntarget_enc.fit(train[cat_features], train['target'])\n\ntrain = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\nvalid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n\nprint(train.shape)\nprint(\"Score with CatBoost target encoding\")\nbst = train_model(train, valid)","a8b96f4b":"data_all_feat = data_all_feat.join(target_enc.transform(train_bench[cat_features]).add_suffix('_cb'))","a5e71d91":"import itertools\n\ncat_features = list(train_bench.columns[1:24])\nprint(cat_features)\n\ninteractions = pd.DataFrame(index=train_bench.index)\nlist(itertools.combinations(cat_features,2))[0][1]\nfor iter in list(itertools.combinations(cat_features,2)):\n    naming = (iter[0]+\"_\"+iter[1])\n    interactions = interactions.assign(**{naming : (train_bench[iter[0]].astype(str) + \"_\" + train_bench[iter[1]].astype(str))})\n    encoder = preprocessing.LabelEncoder()\n    interactions[naming] = encoder.fit_transform(interactions[naming])\ninteractions.head()","1785928f":"data = train_bench.join(interactions)\nprint(data.shape)\n\nprint(\"Score with interactions\")\ntrain, valid, test = get_data_splits(data)\n_ = train_model(train, valid)\n\ndel data","7dff09f4":"data_all_feat = data_all_feat.join(interactions)\ndata_all_feat.shape","4edd4b2d":"from sklearn.feature_selection import SelectKBest, f_classif\nfeature_cols = data_all_feat.columns.drop(['target'])\ntrain, valid, test = get_data_splits(data_all_feat)\nprint(train.shape)\n\nselector = SelectKBest(f_classif, k=110)  # Create the selector, keeping 110 features, k should be optimized further\n\n# Use the selector to retrieve the best features\nX_new = selector.fit_transform(train[feature_cols],train['target'])\n\n# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), index=train.index, columns=feature_cols)\n#selected_features.head()\n\n# Find the columns that were dropped\ndropped_columns = selected_features.columns[selected_features.var(axis=0)==0]\nprint(len(dropped_columns))\nprint(dropped_columns)\ndropped_columns = dropped_columns.drop('id')\ndropped_columns_KBest = dropped_columns.copy()  ##saving for final model\n\n_, baseline_score, test_score = train_model(train.drop(dropped_columns, axis=1), valid.drop(dropped_columns, axis=1),test.drop(dropped_columns, axis=1))\nprint(f\"Test AUC score after LogReg_L1 feature selection: {test_score}\")\n\ndel X_new","0057b1fd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\ndef select_features_l1(X, y):\n    \"\"\" Return selected features using logistic regression with an L1 penalty \"\"\"\n    logistic = LogisticRegression(C=0.03, random_state=7, penalty=\"l1\")\n    logistic_fit = logistic.fit(X,y)\n    \n    selector = SelectFromModel(logistic_fit, prefit=True)\n    X_new = selector.transform(X)\n\n    # Get back the features we've kept, zero out all other features\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                    index=X.index, \n                                    columns=X.columns)\n\n    # Dropped columns have values of all 0s, so var is 0, drop them\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    \n    return selected_columns","895a4e5d":"feature_cols = data_all_feat.columns.drop(['target'])\ntrain, valid, test = get_data_splits(data_all_feat, valid_fraction=0.1)  #####  use smaller train subset (e.g. valid_fraction=0.3 ) to speed up L1 feature selection\nprint(train.shape)\n\nfeature_cols = data_all_feat.columns.drop(['target'])\nselected = select_features_l1(train[feature_cols], train['target'])\n#print(selected)\ndropped_columns = feature_cols.drop(selected)\ndropped_columns = dropped_columns.drop('id')\nprint(len(dropped_columns))\nprint(dropped_columns)\ndropped_columns_L1 = dropped_columns.copy()  ##saving for final model\n\ntrain, valid, test = get_data_splits(data_all_feat, valid_fraction=0.1)\n\n_, baseline_score, test_score = train_model(train.drop(dropped_columns, axis=1), valid.drop(dropped_columns, axis=1),test.drop(dropped_columns, axis=1))\nprint(f\"Test AUC score after LogReg_L1 feature selection: {test_score}\")","7a564393":"import category_encoders as ce\ncat_features = list(train_bench.columns[6:24])\n#print(cat_features)\nbin_features = list(train_bench.columns[1:6])\n#print(bin_features)\n\ntrain_encoded = train_bench[cat_features].copy()\ntest_encoded = test_all_feat[cat_features].copy()\n\nenc = ce.CountEncoder(cols=cat_features).fit(train_bench[cat_features])\ntrain_encoded = enc.transform(train_bench[cat_features])\ntest_encoded = enc.transform(test_all_feat[cat_features])\n\nprint(train_encoded.shape)\nprint(test_encoded.shape)\n\ntrain_all_feat = train_all_feat.join(train_encoded.add_suffix(\"_count\"))\ntest_all_feat = test_all_feat.join(test_encoded.add_suffix(\"_count\"))\n\nprint(train_all_feat.shape)\n\ndel train_encoded\ndel test_encoded","db356d5a":"target_enc = ce.CatBoostEncoder(cols=cat_features)\n\ntarget_enc.fit(train_bench[cat_features], train_bench['target'])\n\ntrain_all_feat = train_all_feat.join(target_enc.transform(train_bench[cat_features]).add_suffix('_cb'))\ntest_all_feat = test_all_feat.join(target_enc.transform(test_all_feat[cat_features]).add_suffix('_cb'))\n\nprint(train_all_feat.shape)\nprint(test_all_feat.shape)","8db42f6c":"import itertools\n\ncat_features = list(train_bench.columns[1:24])\nprint(cat_features)\n\ninteractions = pd.DataFrame(index=train_bench.index)\ninteractions_test = pd.DataFrame(index=test_all_feat.index)\n\nlist(itertools.combinations(cat_features,2))[0][1]\nfor iter in list(itertools.combinations(cat_features,2)):\n    naming = (iter[0]+\"_\"+iter[1])\n    interactions = interactions.assign(**{naming : (train_bench[iter[0]].astype(str) + \"_\" + train_bench[iter[1]].astype(str))})\n    interactions_test = interactions_test.assign(**{naming : (test_all_feat[iter[0]].astype(str) + \"_\" + test_all_feat[iter[1]].astype(str))})\n    \nprint(interactions.shape)\nprint(interactions_test.shape)\n\ngc.collect()\n\ndel data_all_feat\ngc.collect()\n\ndata = pd.concat([interactions,interactions_test],axis=0, ignore_index = True)\n\nfor iter in list(interactions.columns):\n    encoder = LabelEncoder()\n    encoder.fit(data[iter])\n    data[iter] = encoder.transform(data[iter])\n\ninteractions = data.iloc[0:len(interactions)]\ninteractions_test = data.iloc[len(interactions):len(data)]\ninteractions_test.reset_index(drop=True,inplace = True)\n\nprint(interactions.shape)\nprint(interactions_test.shape)","6a91f9ac":"train_all_feat = train_all_feat.join(interactions)\ntest_all_feat = test_all_feat.join(interactions_test)\n\nprint(train_all_feat.shape)\nprint(test_all_feat.shape)\n\ndel interactions, interactions_test\ngc.collect()","421eb9ec":"dropped_columns = dropped_columns_KBest\n#print(dropped_columns)\n\nfeature_cols = train_all_feat.drop(dropped_columns, axis=1).columns.drop(['id', 'target'])\nprint(feature_cols)\n\nvalid_fraction = 0.1\nvalid_rows = int(len(train_all_feat) * valid_fraction)\ntrain = train_all_feat[:-valid_rows]\nvalid = train_all_feat[-valid_rows:]\nprint(train.shape)\nprint(valid.shape)\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['target'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['target'])\n    \nparam = {'num_leaves': 64, 'objective': 'binary', 'metric': 'auc', 'seed': 77}\nnum_round = 1000\nprint(\"Training model!\")\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval= 50)\n\nnum_iteration=bst.best_iteration\nprint(num_iteration)\n    \nvalid_pred = bst.predict(valid[feature_cols])\nvalid_score = metrics.roc_auc_score(valid['target'], valid_pred)\nprint(f\"Validation AUC score: {valid_score}\")\n    \n## training on full train set\ndtrain = lgb.Dataset(train_all_feat[feature_cols], label=train_all_feat['target'])\nparam = {'num_leaves': 64, 'objective': 'binary', 'metric': 'auc', 'seed': 77}\nnum_round = num_iteration\nprint(\"Training model!\")\nbst = lgb.train(param, dtrain, num_round,  verbose_eval=True)\n\ntest_pred = bst.predict(test_all_feat[feature_cols])\n\nsub1 = pd.read_csv(PATH + 'sample_submission.csv')\nsub1.head()\nsub1['target'] =test_pred\nsub1.to_csv(\"submission_KBest.csv\",index=False)","2f25b97f":"dropped_columns = dropped_columns_L1\n#print(dropped_columns)\n\nfeature_cols = train_all_feat.drop(dropped_columns, axis=1).columns.drop(['id', 'target'])\nprint(feature_cols)\n\nvalid_fraction = 0.1\nvalid_rows = int(len(train_all_feat) * valid_fraction)\ntrain = train_all_feat[:-valid_rows]\nvalid = train_all_feat[-valid_rows:]\nprint(train.shape)\nprint(valid.shape)\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['target'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['target'])\n    \nparam = {'num_leaves': 64, 'objective': 'binary', 'metric': 'auc', 'seed': 77}\nnum_round = 1000\nprint(\"Training model!\")\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval=50)\n\nnum_iteration=bst.best_iteration\nprint(num_iteration)\n\nvalid_pred = bst.predict(valid[feature_cols])\nvalid_score = metrics.roc_auc_score(valid['target'], valid_pred)\nprint(f\"Validation AUC score: {valid_score}\")\n    \n## training on full train set\ndtrain = lgb.Dataset(train_all_feat[feature_cols], label=train_all_feat['target'])\nparam = {'num_leaves': 64, 'objective': 'binary', 'metric': 'auc', 'seed': 77}\nnum_round = num_iteration\nprint(\"Training model!\")\nbst = lgb.train(param, dtrain, num_round,  verbose_eval=True)\n\ntest_pred = bst.predict(test_all_feat[feature_cols])\n\nsub2 = pd.read_csv(PATH + 'sample_submission.csv')\nsub2.head()\nsub2['target'] =test_pred\nsub2.to_csv(\"submission_L1.csv\",index=False)","e8dcf07b":"sub3 = pd.read_csv(PATH + 'sample_submission.csv')\nsub3['target'] = (sub1['target']+sub2['target'])\/2\nsub3.to_csv(\"submission_1_2.csv\",index=False)","9e85d9ff":"feature_cols = train_all_feat.columns.drop(['id', 'target'])\n\nvalid_fraction = 0.1\nvalid_rows = int(len(train_all_feat) * valid_fraction)\ntrain = train_all_feat[:-valid_rows]\nvalid = train_all_feat[-valid_rows:]\nprint(train.shape)\nprint(valid.shape)\n\ndtrain = lgb.Dataset(train[feature_cols], label=train['target'])\ndvalid = lgb.Dataset(valid[feature_cols], label=valid['target'])\n    \nparam = {'num_leaves': 64, 'objective': 'binary', 'metric': 'auc', 'seed': 77}\nnum_round = 1000\nprint(\"Training model!\")\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=20, verbose_eval=50)\n\nnum_iteration=bst.best_iteration\nprint(num_iteration)\n\nvalid_pred = bst.predict(valid[feature_cols])\nvalid_score = metrics.roc_auc_score(valid['target'], valid_pred)\nprint(f\"Validation AUC score: {valid_score}\")\n    \n## training on full train set\ndtrain = lgb.Dataset(train_all_feat[feature_cols], label=train_all_feat['target'])\nparam = {'num_leaves': 64, 'objective': 'binary', 'metric': 'auc', 'seed': 77}\nnum_round = num_iteration\nprint(\"Training model!\")\nbst = lgb.train(param, dtrain, num_round,  verbose_eval=True)\n\ntest_pred = bst.predict(test_all_feat[feature_cols])\n\nsub4 = pd.read_csv(PATH + 'sample_submission.csv')\nsub4.head()\nsub4['target'] =test_pred\nsub4.to_csv(\"submission_full_set.csv\",index=False)","c441acad":"train_all_feat.to_csv(\"train_all_feat.csv.zip\", index = False, compression = 'zip')\ntest_all_feat.to_csv(\"test_all_feat.csv.zip\", index = False, compression = 'zip')\n\nwith open('dropped_columns_L1.data', 'wb') as filehandle:\n    pickle.dump(dropped_columns_L1, filehandle)\nwith open('dropped_columns_KBest.data', 'wb') as filehandle:\n    pickle.dump(dropped_columns_KBest, filehandle)","56bc18cc":"## Univariate Feature Selection\n\nUsing `SelectKBest` with the `f_classif` scoring function to choose 140 features. Will compare this feature selection method with selection using `LogisticRegression` classifier model with an L1 penalty (see below). `k` should be optimized further using a grid of `k` values.","fa69aeb0":"### L1 regularization for feature selection\n\nUse a `LogisticRegression` classifier model with an L1 penalty to select the features. For the model, set the random state to 7 and the regularization parameter to 0.001. Fit the model then use `SelectFromModel` to return a model with the selected features.  C needs to be further optimized to narrow down the best feature subset.","9f53569c":"Both feature selection algos produce comparable scores. We will use each sets of features to train two final models. It is appears that we did not overtrain because our small test set score show gains consistent with that of the validation set for both feature subsets.","4a728203":"## Target encoding","a590b8ae":"### Re-creating the rest of the features for Test set","bc48316c":"## Feature encoding\nBelow, I will use several encoding and feature engineering approaches followed by feature selection. Everything will be done with train set (split in train, validation,and test subsets). Later I will redo encoding using the whole train and test set with the selected features to train and submit final model(s).","27ae0c5e":"## CatBoost target encoding","5eea066a":"## Baseline score.","26ba91c1":"## Count encoding","3414d6d7":"## Import packages, define functions. ","7b8e36a2":"## Convert categorical to int. \nI'm ignoring the order of the ordinal features for now. May come back to this in later versions.","4643458d":"I adapted the code from the excellent Feature Encoding mini-course by super-GM Pavel Pleskov (@ppleskov) and Instructional Designer at Kaggle Mat Leonard (@matleonard). I found their course very well laid out. For full benefit, I suggest going through their examples and then solving the exercises yourself.\n\nCheck it out! **[Feature Engineering Home Page](https:\/\/www.kaggle.com\/learn\/feature-engineering)**\n\n\n<p><font color=\"red\"> Work in progress...<\/font><\/p>\n\n<p><font color=\"red\">Please wear hard hats at all times!<\/font><\/p>\n\n**Version notes:**\n\nv1: C = 0.001 param needs updating because L1_selection with this value actually has worse score (too many features dropped)","f43ef5c6":"## Training on L1_selected features\n\nFirst, I will use train-validation split to find the best round via early stopping, then will train on the whole set.","6134a779":"## Training models for submissions","6fb8bf96":"## Training on full feature set\n\nFirst, I will use train-validation split to find the best round via early stopping, then will train on the whole set.","1fed2765":"## Save feature datasets for future use in other kernels","2dd3b332":"I will keep CatBoost target encoding and won't use plain target encoding since they are largely redundunt but CatBoost uses leave-one-out algo that should be less prone to overfitting.","1a74904b":"## Training on KBest selected features\n\nFirst, I will use train-validation split to find the best round via early stopping, then will train on the whole set.","ce3c8271":"## Combining all features together"}}