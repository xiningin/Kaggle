{"cell_type":{"91ee8f89":"code","c81d5999":"code","7bdd257e":"code","5d199a5f":"code","ec0c3d01":"code","9d2f3940":"code","a861c21f":"code","281b8567":"code","ab4f22f3":"code","7cc34dab":"code","22b51f34":"code","3f12fa32":"code","89c16728":"code","1d9a7110":"code","bc1e955d":"code","0845311b":"code","483ca0ca":"code","ac3e6a0e":"code","eb51bb47":"code","ce2892e4":"code","2a434c3d":"code","1e925a8f":"code","c39f2e1a":"code","01dc4793":"code","de9d9d21":"code","1f40c27b":"code","541bcab8":"code","086f670b":"markdown","a1e0ce81":"markdown","21789ab9":"markdown","ec3f7aa1":"markdown","be2e04f3":"markdown","2712c37f":"markdown","06d7c6ec":"markdown","a68f0688":"markdown","56244797":"markdown","a9fe35e5":"markdown","7ea3e5d4":"markdown","48b73128":"markdown","008d6012":"markdown","4e372506":"markdown","a23660b4":"markdown","7482888d":"markdown"},"source":{"91ee8f89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c81d5999":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","7bdd257e":"display(df.head(),df.tail(),df.describe(),df.info())","5d199a5f":"#Seperation of Data and Target\nx_data,y = df.loc[:,df.columns != 'target'],df.loc[:,'target'].values\n#normalize\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state=42)\n","ec0c3d01":"\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred_lr = lr.predict(x_test)\nprint('LR accuracy = {}'.format(lr.score(x_test,y_test)))\ncm_lr = confusion_matrix(y_test,y_pred_lr)","9d2f3940":"#Logistic Regression Visiualization\nsns.set_palette('muted')\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_lr,annot=True,linewidths=0.5,linecolor='black',fmt='.0f',ax=ax,)\nplt.show()","a861c21f":"#K=3\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nprint('With K = {}, Accuracy is {}'.format(3,knn.score(x_test,y_test)))","281b8567":"#finding best K value\nscore_test = []\nscore_train =[]\nfor i in range(1,30):\n    knn2 = KNeighborsClassifier(i)\n    knn2.fit(x_train,y_train)\n    score_test.append(knn2.score(x_test,y_test))\n    score_train.append(knn2.score(x_train,y_train))\nf, ax = plt.subplots(figsize=(15,10))\nplt.plot(range(1,30),score_test,label='Test Data Accuracy')\nplt.plot(range(1,30),score_train,label='Train Data Accuracy')\nplt.legend()\nplt.xlabel('K values')\nplt.ylabel('Accuracy')\nplt.show()\nprint('Best Accuracy is = {} with the K Value of = {}'.format(np.max(score_test),+1+score_test.index(np.max(score_test))))\ny_pred_knn = knn2.predict(x_test)\ncm_knn = confusion_matrix(y_test,y_pred_knn)\n\n","ab4f22f3":"f , ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_knn,annot=True,linewidths=0.5,ax=ax)\nplt.show()","7cc34dab":"from sklearn.svm import SVC\nsvc = SVC(random_state=42)\nsvc.fit(x_train,y_train)\ny_pred_svc = svc.predict(x_test)\nprint('Svc score is = {}'.format(svc.score(x_test,y_test)) )\ncm_svc = confusion_matrix(y_test,y_pred_svc)\n\n","22b51f34":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svc,annot=True,cmap='coolwarm',linewidth=0.5,ax=ax)\nplt.show()","3f12fa32":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\ny_pred_nb = nb.predict(x_test)\nprint('Accuracy with Naive Bayes = {}'.format(nb.score(x_test,y_test)))\ncm_nb = confusion_matrix(y_test,y_pred_nb)\n\nf, ax= plt.subplots(figsize=(5,5))\nsns.heatmap(cm_nb,annot=True,linewidth=0.5,ax=ax)\nplt.show()","89c16728":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train)\ny_pred_dtc = dtc.predict(x_test)\ncm_dtc = confusion_matrix(y_test,y_pred_dtc)\n\nprint('Accuracy with Decision Tree Classifier {}'.format(dtc.score(x_test,y_test)))\n\n","1d9a7110":"f,ax= plt.subplots(figsize=(5,5))\nsns.heatmap(cm_dtc,annot=True,linewidths=0.5,ax=ax)\nplt.show()","bc1e955d":"#finding best Estimator value\nfrom sklearn.ensemble import RandomForestClassifier\nscore_list = []\nfor i in range(1,101):\n    rfc2= RandomForestClassifier(n_estimators=i,random_state=42)\n    rfc2.fit(x_train,y_train)\n    score_list.append(rfc2.score(x_test,y_test))\n    print('Accuracy with Random forest is {} with {} Trees'.format(rfc2.score(x_test,y_test),i))\n","0845311b":"print('Best Accuracy with Random Forest Classifier is {} with the Decision Tree number of {} '.format(np.max(score_list),+1+score_list.index(np.max(score_list))))\nplt.plot(range(1,101),score_list,c='orange',label='RF Accuracy')\nplt.legend()\nplt.xlabel('# of Decision Trees')\nplt.ylabel('Accuracy')\nplt.show()\nbest_estimator = score_list.index(np.max(score_list))+1\nrfc = RandomForestClassifier(n_estimators=best_estimator,random_state=42)\nrfc.fit(x_train,y_train)\ny_pred_rfc = rfc.predict(x_test)\ncm_rfc = confusion_matrix(y_test,y_pred_rfc)\n","483ca0ca":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_rfc, annot=True, linewidths=0.5,ax=ax)\nplt.show()","ac3e6a0e":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlinr = LinearDiscriminantAnalysis()\nlinr.fit(x_train,y_train)\nprint('Accuracy with Linear Disc Analysis is {}'.format(linr.score(x_test,y_test)))\ny_pred_linr = linr.predict(x_test)\n\ncm_linr = confusion_matrix(y_test,y_pred_linr) ","eb51bb47":"f,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm_linr,annot=True)\nplt.show()","ce2892e4":"#ML models in Automation\n\nmodels = []\n\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"NB\",GaussianNB()))\nmodels.append((\"KNN\",KNeighborsClassifier(n_neighbors=5)))\nmodels.append((\"DT\",DecisionTreeClassifier()))\nmodels.append((\"SVM\",SVC()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('RF',RandomForestClassifier(n_estimators=13)))\nmodels\n#Results for ML Models\n\n\nfor name, model in models:\n    \n    clf=model\n\n    clf.fit(x_train, y_train)\n\n    y_pred =clf.predict(x_test)\n    print(10*\"=\",\"{} i\u00e7in Sonu\u00e7lar\".format(name).upper(),10*\"=\")\n    print(\"Accuracy Score:{:0.2f}\".format(accuracy_score(y_test, y_pred)))\n    print(\"Confusion Matrix:\\n{}\".format(confusion_matrix(y_test, y_pred)))\n    print(\"Classification Report:\\n{}\".format(classification_report(y_test,y_pred)))\n    print(30*\"=\")\n","2a434c3d":"print(\"Logistic Regression Accuracy:\", 100*lr.score(x_test, y_test), \"%\")\nprint(\"KNN Prediction Accuracy:\", 100*knn.score(x_test, y_test), \"%\")\nprint(\"SVM Prediction Accuracy:\", 100*svc.score(x_test, y_test), \"%\")\nprint(\"Naive Bayes Prediction Accuracy:\", 100*nb.score(x_test, y_test), \"%\")\nprint(\"Decision Trees Prediction Accuracy:\", 100*dtc.score(x_test, y_test), \"%\")\nprint(\"Random Forest Prediction Accuracy:\", 100*rfc.score(x_test, y_test), \"%\")\nprint('Linear Disc Analysis Accuracy:',100*linr.score(x_test,y_test),'%')\n","1e925a8f":"dfu_data = df.loc[:, df.columns != 'target']","c39f2e1a":"dfu_data.head()","01dc4793":"#normalize data\ndfu = (dfu_data-np.min(dfu_data))\/(np.max(dfu_data)-np.min(dfu_data))\n","de9d9d21":"#Kmeans Clustering\nfrom sklearn.cluster import KMeans\ninertia_list = np.empty(10)\nfor i in range(1,10):\n    kmeans = KMeans(n_clusters = i)\n    kmeans.fit_predict(dfu)\n    inertia_list[i]= kmeans.inertia_\nplt.plot(range(0,10),inertia_list,'-o',c='r')\nplt.xlabel('# of clusters')\nplt.ylabel('Inertia')\nplt.grid(True)\nplt.show()\n","1f40c27b":"#graph suggests Cluster number to be 2, as we know from the labeled data there were 2 labels\nkmeans2= KMeans(n_clusters=2)\n\n\nlabels = kmeans2.fit_predict(dfu)\ncheckdata = pd.DataFrame({'labels': labels,'target':df.target})\nct = pd.crosstab(checkdata.labels,checkdata.target)\nct\n","541bcab8":"#Standartization and making pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\npipe = make_pipeline(scalar,kmeans2)\npipe.fit(dfu)\nlabels = pipe.predict(dfu)\npipedf = pd.DataFrame({'labels':labels,'target':df.target})\nct = pd.crosstab(pipedf.labels,pipedf.target)\nct\n\n#Hierarchical Clustering\nfrom scipy.cluster.hierarchy import linkage,dendrogram\nmerg = linkage(dfu,method='ward')\ndendrogram(merg,leaf_rotation=90)\nplt.show","086f670b":"# Unsupervised Learning Work<a id = \"14\"><\/a><br>\nTo practice Unsupervised Learning I will drop target column and try to apply Unsupervised Learning","a1e0ce81":"# Conclusion<a id = \"13\"><\/a><br>\n* In conclusion Random Forest Classifier is the best tool for Heart Disease UCI data.","21789ab9":"# Appliying ML Algorithms on Heart Disease Data\n* [Reading Data](#1)\n* [Understanding Data](#2)\n* [Normalize & Seperation of Data and Target](#3)\n* [Creating ML models](#4)\n    * [Logistics Regression](#5)\n    * [KNN Neighbours](#6)\n    * [Support Vector Machine](#7)\n    * [Naive Bayes](#8)\n    * [Decision Tree Classifier](#9)\n    * [Random Forest Classifier](#10)\n    * [Linear Discriminant Analysis](#11)\n* [Trying to Automate the Model Selection](#12)\n* [Conclusion](#13)\n* [Unsupervised Learning Work](#14)","ec3f7aa1":"## Logistics Regression<a id = \"5\"><\/a><br>\n* Logistic Regression resulted with 79% accuracy","be2e04f3":"* There is no missing values.\n* Normalization is needed since feature means have big difference which can result in dominance of some fetures against others.\n* Target variable type is integer and in a binary form, which means no need for any actions\n* There is no object type variable ","2712c37f":"# Understanding Data <a id = \"2\"><\/a><br>","06d7c6ec":"# Reading Data  <a id = \"1\"><\/a><br>","a68f0688":"# Trying to Automate the Model Selection<a id = \"12\"><\/a><br>\n","56244797":"## Support Vector Machine <a id = \"7\"><\/a><br>","a9fe35e5":"# Normalize & Seperation of Data and Target & Train Test Split<a id = \"3\"><\/a><br>\n","7ea3e5d4":"## Naive Bayes <a id = \"8\"><\/a><br>","48b73128":"## Random Forest Classifier<a id = \"10\"><\/a><br>","008d6012":"## KNN Neighbours<a id = \"6\"><\/a><br>","4e372506":"## Linear Discriminant Analysis<a id = \"11\"><\/a><br>","a23660b4":"## Decision Tree Classifier<a id = \"9\"><\/a><br>","7482888d":"# Applying ML models<a id = \"4\"><\/a><br>"}}