{"cell_type":{"e1859a59":"code","7961dc7d":"code","638e5eda":"code","494659ed":"code","68e31aaf":"code","a6999c1e":"code","69ae75c5":"code","f15578a5":"code","5afb6d45":"code","439fadae":"code","7a0f35d5":"code","f49527f4":"code","d79088d8":"code","8737a3c7":"code","269261fe":"code","be087a3e":"code","d9045fb9":"code","8f631f54":"code","67925358":"code","96f227a9":"code","ea2caba2":"code","f2b458ee":"code","8db11743":"code","f7a9c89d":"code","aee47e75":"code","c844c9c0":"code","2f7acbc9":"code","456b4e85":"code","b67422ad":"code","ccc151fc":"code","2953d35d":"code","bdb19af0":"code","3e35c90d":"code","22d16613":"markdown","3a6ed351":"markdown","3b4c117c":"markdown","a88b3e04":"markdown","ad881fe9":"markdown","21dc5e28":"markdown","380f0194":"markdown","759ce01f":"markdown","c1cfb52c":"markdown","f83ace89":"markdown","2d402d2b":"markdown","dcedd0a4":"markdown"},"source":{"e1859a59":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree","7961dc7d":"# Load the dataset.\nle = pd.read_csv('\/kaggle\/input\/life-expectancy-who\/Life Expectancy Data.csv', sep=',')\nle.dataframeName = 'Life Expectancy Data.csv'\nle.head()","638e5eda":"# Modify the original names of the features using a standard format for all the features.\norig_cols = list(le.columns) \nnew_cols = [] \nfor col in orig_cols:     \n    new_cols.append(col.strip().replace('  ', ' ').replace(' ', '_').lower()) \n\nle.columns = new_cols\n\n# Compute a summary of statistics only for the numerical features.\nle.describe()","494659ed":"# Discard the metadata (country and year).\nle = le.drop(['country','year'], axis=1)","68e31aaf":"# For each feature count all rows with NULL values.\nle.isnull().sum()","a6999c1e":"# For each feature delete all rows with NULL values.\nle.dropna(inplace=True)\nle.isnull().sum()","69ae75c5":"#Change column order to better perform splits\nnew_order = [1,0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\nle = le[le.columns[new_order]]\nle.head()","f15578a5":"# Get a concise summary of the dataset.\nle.info()","5afb6d45":"# Create a dictionary of columns representing the features of the dataset.\ncol_dict = {'life_expectancy':1,'adult_mortality':2,'infant_deaths':3,'alcohol':4,'percentage_expenditure':5,'hepatitis_b':6,'measles':7,'bmi':8,\n            'under-five_deaths':9,'polio':10,'total_expenditure':11,'diphtheria':12,'hiv\/aids':13,'gdp':14,'population':15,'thinness_1-19_years':16,\n            'thinness_5-9_years':17,'income_composition_of_resources':18,'schooling':19}\n\n# Visualize the data for each feature using box plots.\nplt.figure(figsize=(18,30))\n\nfor variable,i in col_dict.items():\n                     plt.subplot(5,4,i)\n                     plt.boxplot(le[variable],whis=1.5)\n                     plt.title(variable)\n\nplt.show()\nle.shape","439fadae":"# Remove the outliers using the interquartile range (IQR).\nQ1 = le.quantile(0.25)\nQ3 = le.quantile(0.75)\nIQR = Q3 - Q1\n\nle = le[~((le < (Q1 - 1.5 * IQR)) |(le > (Q3 + 1.5 * IQR))).any(axis=1)]\n\n#Replace Status into boolean variables\nle[\"status\"].replace({\"Developing\": 1, \"Developed\": 0}, inplace=True)\n\n# Print the dimensions of the cleaned dataset.\nle.shape","7a0f35d5":"# Visualize the cleaned data for each feature using box plots.\nplt.figure(figsize=(18,30))\n\nfor variable,i in col_dict.items():\n                     plt.subplot(5,4,i)\n                     plt.boxplot(le[variable],whis=1.5)\n                     plt.title(variable)\nplt.show()","f49527f4":"# Plot heatmap to visualize the correlations.\nplt.figure(figsize = (14, 12))\nsns.heatmap(le.corr(), annot = True)\nplt.title('Correlation between different features');","d79088d8":"# Create a vector containing all the features of the dataset.\nall_col = ['adult_mortality','infant_deaths','alcohol','percentage_expenditure','hepatitis_b','measles','bmi',\n         'under-five_deaths','polio','total_expenditure','diphtheria','hiv\/aids','gdp','population','thinness_1-19_years',\n         'thinness_5-9_years','income_composition_of_resources','schooling']\n\nplt.figure(figsize=(15,30))\n\n# Plot each feature in function of the target variable (life_expectancy) using scatter plots.\nfor i in range(len(all_col)):\n    plt.subplot(7,3,i+1)\n    plt.scatter(le[all_col[i]], le['life_expectancy'])\n    plt.xlabel(all_col[i])\n    plt.ylabel('Life Expectancy')\n\nplt.show()","8737a3c7":"# Separate the features from the labels.\nX = le.iloc[:,1:].values\ny = le.iloc[:,0].values #Life Expectancy","269261fe":"# Normalize the data.\nX_std= StandardScaler().fit_transform(X)\nmean_vec = np.mean(X_std, axis=0)\n\n# Compute covariance matrix.\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","be087a3e":"# Compute eigenvalues and eigenvectors.\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","d9045fb9":"# Compute the variance for every eigenvalue.\ntot = sum(eig_vals)\n\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\n\nvar_exp","8f631f54":"# Plot the principal components.\nplt.figure(figsize=(10,4))\nplt.bar(range(19), var_exp, alpha=0.7, align='center', label='Individual Variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.xticks(np.arange(0, 19, 1.0))\nplt.tight_layout()","67925358":"# Plot the cumulative variance.\npca = PCA(n_components=19).fit(X_std)\nplt.figure(figsize=(12, 4))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), label='Cumulative Variance')\nplt.xlim(0,18,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.legend(loc='best')\nplt.grid(color='#E3E3E3')\nplt.xticks(np.arange(0, 19, 1.0));","96f227a9":"# Take the the values of the target variable and of the most correlated feature with the target variable.\nle_features = le['income_composition_of_resources'].values.reshape(-1,1)\nle_labels = le['life_expectancy'].values.reshape(-1,1)\n\n# Normalize the data.\nmin_max_scaler = MinMaxScaler()\nle_features = min_max_scaler.fit_transform(le_features)\n\n# Split the dataset in training and test set.\nle_features_train, le_features_test, le_labels_train, le_labels_test = train_test_split(le_features, le_labels, train_size = 0.7, test_size = 0.3)","ea2caba2":"linear_model = LinearRegression()\n\n# Train the model.\nlinear_model.fit(le_features_train, le_labels_train);","f2b458ee":"# Test the model.\nlinear_model_score = linear_model.predict(le_features_test)\n\n# Plot the result.\nplt.figure(figsize=(10, 6))\nplt.scatter(le_features_test, le_labels_test,  color='black')\nplt.plot(le_features_test, linear_model_score, color='blue', linewidth=2)\nplt.xlabel('Income Composition of Resources')\nplt.ylabel('Life Expectancy')\nplt.show()\n\nprint('Coefficients: \\n', linear_model.coef_)\nprint(\"Mean squared error: %.2f\" % mean_squared_error(le_labels_test,linear_model_score))\nprint(\"R^2 score : %.2f\" % r2_score(le_labels_test,linear_model_score))","8db11743":"# Separate the features from the labels.\nle_features = le.iloc[:, 1:].values\nle_labels = le.iloc[:,0] #Life Expectancy\n\n# Normalize the data.\nmin_max_scaler = MinMaxScaler()\nle_features = min_max_scaler.fit_transform(le_features)\n\n# Split the dataset in training and test set.\nle_features_train, le_features_test, le_labels_train, le_labels_test = train_test_split(le_features, le_labels, train_size = 0.7, test_size = 0.3)","f7a9c89d":"# Train the model.\nlinear_model.fit(le_features_train, le_labels_train);\n\n# Test the model.\nlinear_model_score = linear_model.predict(le_features_test)\n\nprint('Coefficients: \\n', linear_model.coef_)\nprint(\"Mean squared error: %.2f\" % mean_squared_error(le_labels_test,linear_model_score))\nprint(\"R^2 score : %.2f\" % r2_score(le_labels_test,linear_model_score))","aee47e75":"# Calculate the life expeectancy average\nle_avg = le['life_expectancy'].mean()\nle_lr = le.copy()\n\n# Replace 1 if life expectancy > avg, 0 otherwise\nle_lr['life_expectancy'] = (le_lr['life_expectancy'] > le_avg).astype(int)\n\n# Separate the features from the labels.\nle_features_lr = le_lr.iloc[:, 1:].values\nle_labels_lr = le_lr.iloc[:,0] #Life Expectancy\n\n# Normalize the data.\nmin_max_scaler = MinMaxScaler()\nle_features_lr = min_max_scaler.fit_transform(le_features_lr)\n\n# Split the dataset in training and test set.\nle_features_train_lr, le_features_test_lr, le_labels_train_lr, le_labels_test_lr = train_test_split(le_features_lr, le_labels_lr, train_size = 0.7, test_size = 0.3)","c844c9c0":"logistic_model = LogisticRegression(solver='liblinear')\n\n#Train The Model\nlogistic_model.fit(le_features_train_lr, le_labels_train_lr);","2f7acbc9":"logistic_score = logistic_model.predict(le_features_test_lr)\n\n#Perform confusion matrix\nconfusion_matrix = confusion_matrix(le_labels_test_lr, logistic_score)\n\n#Print confusion matrix as heatmap\nclass_names=[0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix),cmap='YlGnBu',annot=True,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.xlabel('Predicted label')\nplt.ylabel('Actual label')\nplt.show()","456b4e85":"print(\"Score on the train data: %.2f\" % logistic_model.score(le_features_train_lr, le_labels_train_lr))\nprint(\"Score on the test data: %.2f\" % logistic_model.score(le_features_test_lr, le_labels_test_lr))","b67422ad":"# Perform DecisionTreeRegresson with three different depths\ndecision_tree_model3 = DecisionTreeRegressor(max_depth=3)\ndecision_tree_model5 = DecisionTreeRegressor(max_depth=5)\ndecision_tree_model7 = DecisionTreeRegressor(max_depth=7)\n\n# Train the model.\ndecision_tree_model3 = decision_tree_model3.fit(le_features_train, le_labels_train)\ndecision_tree_model5 = decision_tree_model5.fit(le_features_train, le_labels_train)\ndecision_tree_model7 = decision_tree_model7.fit(le_features_train, le_labels_train)\n\nprint(\"Score on the train data with depth 3: %.2f\" % decision_tree_model3.score(le_features_train, le_labels_train))\nprint(\"Score on the test data with depth 3: %.2f\" % decision_tree_model3.score(le_features_test, le_labels_test))\nprint(\"Score on the train data with depth 5: %.2f\" % decision_tree_model5.score(le_features_train, le_labels_train))\nprint(\"Score on the test data with depth 5: %.2f\" % decision_tree_model5.score(le_features_test, le_labels_test))\nprint(\"Score on the train data with depth 7: %.2f\" % decision_tree_model7.score(le_features_train, le_labels_train))\nprint(\"Score on the test data with depth 7: %.2f\" % decision_tree_model7.score(le_features_test, le_labels_test))","ccc151fc":"# Plot the result.\ndot_data = tree.export_graphviz(decision_tree_model3, \n                                filled=True, \n                                rounded=True, \n                                out_file=None, \n                                feature_names=le.iloc[:, 1:].columns)\ngraph = graphviz.Source(dot_data)\ngraph","2953d35d":"random_forest_model = RandomForestRegressor(n_estimators=100,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\n# Train the model.\nrandom_forest_model.fit(le_features_train, le_labels_train);","bdb19af0":"df_ = pd.DataFrame(le.iloc[:, 1:].columns, columns = ['feature'])\ndf_['fscore'] = random_forest_model.feature_importances_[:, ]\n\n# Plot the relative importance of the top 10 features.\ndf_['fscore'] = df_['fscore'] \/ df_['fscore'].max()\ndf_.sort_values('fscore', ascending = False, inplace = True)\ndf_ = df_[0:19]\ndf_.sort_values('fscore', ascending = True, inplace = True)\nax = df_.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(15, 10))\n\n# Plot the result.\nplt.title('Random forest feature importance')\nplt.xlabel('')\nplt.ylabel('')\nplt.xticks([], [])\nplt.yticks()\n\n# Create a list to collect the plt.patches data.\ntotals = []\n\n# Find the values and append to list.\nfor i in ax.patches:\n    totals.append(i.get_width())\n\n# Set individual bar lables using above list.\ntotal = sum(totals)\n\n# Set individual bar lables using above list.\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down.\n    ax.text(i.get_width(), i.get_y()+.13, \\\n            str(round((i.get_width()\/total)*100, 2))+'%', fontsize=10,\ncolor='#505050')\n\nplt.show()","3e35c90d":"# Test the model.\nrandom_forest_score = random_forest_model.predict(le_features_test)\n\nprint(\"Score on the train data: %.2f\" % random_forest_model.score(le_features_train, le_labels_train))\nprint(\"Score on the test data: %.2f\" % random_forest_model.score(le_features_test, le_labels_test))","22d16613":"- **Heatmap**","3a6ed351":"# - Features Extraction\n- **PCA**","3b4c117c":"- **Scatter Plots**","a88b3e04":"# LOAD DATASET","ad881fe9":"# - Multiple Linear Regression","21dc5e28":"# - Decision Tree","380f0194":"# - Logistic Regression","759ce01f":"# DATA PREPROCESSING\n\n# - Data Cleaning","c1cfb52c":"# - Data Exploration","f83ace89":"# - Random Forest","2d402d2b":"# DATA ANALYSIS\n\n# - Linear Regression","dcedd0a4":"- **Box Plots**"}}