{"cell_type":{"dc28b26d":"code","a54b90ac":"code","2d5ad5a7":"code","b9f9af26":"code","bf68c85b":"code","c94e4b32":"code","d20f2653":"code","ae5a4b68":"code","8e60ea28":"code","8b2e4648":"code","de9205e1":"code","94bfea0d":"code","6230ed7d":"code","0b33e294":"code","33d504c8":"code","c52e5265":"code","b65f5d64":"code","92c1279e":"code","1f1021e7":"code","2df703f8":"code","3be2c4f0":"code","27660793":"markdown","30b11f74":"markdown","216b1129":"markdown","239502fc":"markdown","3b953073":"markdown","4d48c149":"markdown","3e0dfdec":"markdown","59839b1e":"markdown","a0ee4cb6":"markdown","13e1d502":"markdown","b9100685":"markdown","c55e8215":"markdown","8060e280":"markdown"},"source":{"dc28b26d":"import matplotlib.pyplot as plt\nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import precision_recall_curve, roc_curve, accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nplt.style.use('fivethirtyeight')\nimport pickle \nimport os \nimport numpy as np\nimport cv2 \n%matplotlib inline","a54b90ac":"labels = ['PNEUMONIA', 'NORMAL']\nimg_size = 200\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","2d5ad5a7":"train = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train')\ntest = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test')\nval = get_training_data('..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val')","b9f9af26":"pnenumonia = 0 \nnormal = 0 \n\nfor i, j in train:\n    if j == 0:\n        pnenumonia+=1\n    else:\n        normal+=1\n        \nprint('Pneumonia:', pnenumonia)\nprint('Normal:', normal)\nprint('Pneumonia - Normal:', pnenumonia-normal)","bf68c85b":"plt.imshow(train[1][0], cmap='gray')\nplt.axis('off')\nprint(labels[train[1][1]])","c94e4b32":"X = []\ny = []\n\nfor feature, label in train:\n    X.append(feature)\n    y.append(label)\n\nfor feature, label in test:\n    X.append(feature)\n    y.append(label)\n    \nfor feature, label in val:\n    X.append(feature)\n    y.append(label)\n\n\n# resize data for deep learning \nX = np.array(X).reshape(-1, img_size, img_size, 1)\ny = np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=32)","d20f2653":"X_train = X_train \/ 255\nX_test = X_test \/ 255\nX_val = X_val \/ 255","ae5a4b68":"# good for balancing out disproportions in the dataset \ndatagen = ImageDataGenerator(\n        featurewise_center=False, \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range=90, \n        zoom_range = 0.1, \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip=True,  \n        vertical_flip=True)  \n\ndatagen.fit(X_train)","8e60ea28":"model = Sequential()\n\nmodel.add(Conv2D(256, (3, 3), input_shape=X_train.shape[1:], padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(16, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nearly_stop = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\nadam = Adam(learning_rate=0.0001)\nmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc'])","8b2e4648":"model.summary()","de9205e1":"history = model.fit(datagen.flow(X_train, y_train, batch_size=10), callbacks=[early_stop], validation_data=(X_val, y_val), epochs=15)","94bfea0d":"model.evaluate(X_test, y_test)","6230ed7d":"plt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['acc'])\nplt.title('Model Accuracy')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['loss'])\nplt.title('Model Loss')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['val_acc'])\nplt.title('Model Validation Accuracy')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['val_loss'])\nplt.title('Model Validation Loss')\nplt.legend(['train'], loc='upper left')\nplt.show()","0b33e294":"pred = model.predict(X_train)\nprecisions, recalls, thresholds = precision_recall_curve(y_train, pred)\nfpr, tpr, thresholds2 = roc_curve(y_train, pred)","33d504c8":"def plot_precision_recall(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--')\n    plt.plot(thresholds, recalls[:-1], 'g-')\n    plt.title('Precision vs. Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(['Precision', 'Recall'], loc='best')\n    plt.show()\n\ndef plot_roc(fpr, tpr):\n    plt.plot(fpr, tpr)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.title('FPR (False Positive rate) vs TPR (True Positive Rate)')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Recall)')\n    plt.show()\n    \nplot_precision_recall(precisions, recalls, thresholds)\nplot_roc(fpr, tpr)","c52e5265":"predictions = model.predict(X_test)","b65f5d64":"binary_predictions = []\nthreshold = thresholds[np.argmax(precisions >= 0.80)]\nfor i in predictions:\n    if i >= threshold:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0) ","92c1279e":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))","1f1021e7":"matrix = confusion_matrix(binary_predictions, y_test)\nplt.figure(figsize=(16, 9))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=20)\nax.set_ylabel('True Labels', size=20)\nax.set_title('Confusion Matrix', size=20) \nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)","2df703f8":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X_train.reshape(-1, img_size, img_size)[i], cmap='gray')\n    if(binary_predictions[i]==y_test[i]):\n        plt.xlabel(labels[binary_predictions[i]], color='blue')\n    else:\n        plt.xlabel(labels[binary_predictions[i]], color='red')\nplt.show()","3be2c4f0":" model.save('pneumonia_detection_ai_version_3.h5')","27660793":"<h2>Process the images and resize them to the preferred size <\/h2>","30b11f74":"<h2>Visualize training images<\/h2>","216b1129":"<h2>Preparing the training and testing data<\/h2>","239502fc":"Data augmentation ","3b953073":"<h2>View some results from a sample of 25 images<\/h2>","4d48c149":"<h2>Plotting the confusion matrix. Here is how we interpet one. <\/h2>\n\nImage source: https:\/\/silvrback.s3.amazonaws.com\/uploads\/4ab81a17-4a77-4e9e-b092-de5fac2afa07\/confusionmatrix_large.png","3e0dfdec":"<h2 >We are incoprating the validation data into the training data because it does not contain enough examples. <\/h2>","59839b1e":"<h2>Prepare data for precision vs. recall and ROC<\/h2>","a0ee4cb6":"<h2 >CNN (Convolutional Neural Network) <\/h2>\nImage source: https:\/\/www.researchgate.net\/publication\/321286547\/figure\/download\/fig6\/AS:564402564472832@1511575465150\/A-convolutional-neural-networks-CNN.png","13e1d502":"<h2>Set thresholds for our model, we want the results to be precise while not sacraficing too much recall <\/h2>","b9100685":"<h2>Download the model<\/h2>","c55e8215":"<h2>Visualizing our training progress<\/h2>","8060e280":"<h2>Reflection: <\/h2>\n<h3> There are a lot of rooms to improve. If you have any questions or suggestions, please leave a comment below.<\/h3>"}}