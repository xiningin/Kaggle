{"cell_type":{"38adc1a1":"code","3cb7b0f0":"code","bd93c25a":"code","cee35918":"code","8f172e08":"code","d6414204":"code","9cbb6b36":"code","c87e5241":"code","a9814ea3":"code","557771ce":"code","aa12c100":"code","7e41ccfc":"code","136782af":"code","650ae9fa":"code","b828e284":"code","2d7e1bfb":"code","1f0d2df8":"code","4099e3c1":"code","c33c4765":"code","d7863ed2":"code","1a83d858":"code","49ab5706":"code","61f15292":"code","fd3fd4a5":"code","97748419":"code","ecc3d160":"code","a266cae9":"code","ca61a780":"code","f4257282":"code","4182492c":"code","3d460f6d":"code","2b9bbb39":"code","4a149c34":"code","ad72a8b5":"code","4cbf7733":"code","ed607fa2":"code","aa090922":"code","fb577a7f":"code","7a6cb919":"code","e0de83ea":"code","24ae3328":"code","499b844f":"code","bcec1b1f":"code","7535d967":"code","2ea9e908":"code","425b9583":"code","d253ae40":"code","d52a05ad":"code","ccaecb67":"code","3cc1670a":"code","4a53a989":"code","1b184745":"code","3238520b":"code","dfcacecd":"code","f6ae0353":"code","c65cb95e":"code","859eecd3":"code","e574760b":"code","95fba1d6":"code","7f9e27aa":"markdown","e5a493d6":"markdown","a376acd1":"markdown","a1eb5ef9":"markdown","7e36e6b2":"markdown","4695245f":"markdown","7f447b78":"markdown","57bc791c":"markdown","e748745f":"markdown","f704da38":"markdown","8802b9b0":"markdown","8471c92f":"markdown","f05ae84b":"markdown","4b33aa08":"markdown","0aa55051":"markdown","6f87fd54":"markdown","d84513a0":"markdown","13df3240":"markdown","2265c826":"markdown","09f0dd7e":"markdown","21bab3e5":"markdown","95576478":"markdown","11593ca5":"markdown","802546db":"markdown","bd1ecc22":"markdown","3530cf02":"markdown","7a13dbbf":"markdown","a2a9f8eb":"markdown","c59c00aa":"markdown"},"source":{"38adc1a1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3cb7b0f0":"#Importing Data\ndata = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","bd93c25a":"#Showing first 5 rows of data\ndata.head()","cee35918":"#Checking types of columns\ndata.dtypes","8f172e08":"data.describe()","d6414204":"#Correlation matrix\nsns.set(style=\"white\")\n\ncorr = data.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1,\n            square=True, annot=True)\n\nplt.show()","9cbb6b36":"#Histogram\ndata.hist(column='output')","c87e5241":"#counting values of chest pain\nsns.countplot(data=data, x=\"cp\")","a9814ea3":"#Violin plot\nsns.violinplot(x = 'cp', y = 'output', data = data)","557771ce":"#Taking heart attack chance percantages for every chest pain\ncppercent0 = (100)*data[data['cp']==0]['output'].sum()\/len(data[data['cp']==0]['output'])\ncppercent1= (100)*data[data['cp']==1]['output'].sum()\/len(data[data['cp']==1]['output'])\ncppercent2= (100)*data[data['cp']==2]['output'].sum()\/len(data[data['cp']==2]['output'])\ncppercent3= (100)*data[data['cp']==3]['output'].sum()\/len(data[data['cp']==3]['output'])\n\ncppercentages=[cppercent0,cppercent1,cppercent2,cppercent3]","aa12c100":"#showing percentage disturbituon of every value of cp in itself (for example percantage of output=1 in cp=2)\nfig = plt.figure()\nplt.bar([0,1,2,3],cppercentages)","7e41ccfc":"pd.get_dummies(data['cp'], prefix='cp')","136782af":"#replacing integer encoding with real names\ndata['cp'] = data['cp'].replace(0,'typical')\ndata['cp'] = data['cp'].replace(1,'atypical')\ndata['cp'] = data['cp'].replace(2,'non-anginal')\ndata['cp'] = data['cp'].replace(3,'asymptomatic')","650ae9fa":"data.head()","b828e284":"#getting dummies and adding them to the dataset\ndata = pd.concat([data,pd.get_dummies(data['cp'], prefix='cp')],axis=1)\n#dropping old cp\ndata.drop(['cp'],axis=1, inplace=True)\n#reordering features\ndata=data[['age','sex','cp_typical','cp_atypical','cp_non-anginal','cp_asymptomatic','trtbps','chol','fbs','restecg','thalachh','exng','oldpeak','slp','caa','thall','output']]","2d7e1bfb":"data.head()","1f0d2df8":"sns.countplot(data=data, x=\"sex\")","4099e3c1":"sexpercentf = (100)*data[data['sex']==0]['output'].sum()\/len(data[data['sex']==0]['output'])\nsexpercentm = (100)*data[data['sex']==1]['output'].sum()\/len(data[data['sex']==1]['output'])\nsexpercentages=[sexpercentf,sexpercentm]\nfig = plt.figure()\nplt.bar(['Female','Male'],sexpercentages)","c33c4765":"sns.catplot(x=\"output\", y=\"thalachh\", hue=\"sex\", data=data)","d7863ed2":"sns.boxplot(x=\"output\", y=\"thalachh\",hue='sex', data=data)","1a83d858":"data.hist(column='age')","49ab5706":"sns.catplot(x=\"output\", y=\"age\", hue=\"sex\", data=data)","61f15292":"#counting values of restecg\nsns.countplot(data=data, x=\"restecg\")","fd3fd4a5":"#Violin plot\nsns.violinplot(x = 'restecg', y = 'output', data = data)","97748419":"#Taking heart attack chance percantages for every restecg\nrestecgpercent0 = (100)*data[data['restecg']==0]['output'].sum()\/len(data[data['restecg']==0]['output'])\nrestecgpercent1= (100)*data[data['restecg']==1]['output'].sum()\/len(data[data['restecg']==1]['output'])\nrestecgpercent2= (100)*data[data['restecg']==2]['output'].sum()\/len(data[data['restecg']==2]['output'])\n\nrestecgpercentages=[restecgpercent0,restecgpercent1,restecgpercent2]","ecc3d160":"#showing percentage disturbituon of every value of restecg in itself (for example percantage of output=1 in cp=2)\nfig = plt.figure()\nplt.bar([0,1,2],restecgpercentages)","a266cae9":"#replacing integer encoding with real names\ndata['restecg'] = data['restecg'].replace(0,'normal')\ndata['restecg'] = data['restecg'].replace(1,'abnormaly')\ndata['restecg'] = data['restecg'].replace(2,'probable')","ca61a780":"data.head()","f4257282":"#getting dummies and adding them to the dataset\ndata = pd.concat([data,pd.get_dummies(data['restecg'], prefix='restecg')],axis=1)\n#dropping old cp\ndata.drop(['restecg'],axis=1, inplace=True)\n#reordering features\ndata=data[['age','sex','cp_typical','cp_atypical','cp_non-anginal','cp_asymptomatic','trtbps','chol','fbs','restecg_normal','restecg_abnormaly','restecg_probable','thalachh','exng','oldpeak','slp','caa','thall','output']]","4182492c":"data.head()","3d460f6d":"data.shape","2b9bbb39":"#Taking values from data\nM=data.values\nX = M[:,0:18]\ny = M[:,18]","4a149c34":"X.shape","ad72a8b5":"y.shape","4cbf7733":"#Train Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","ed607fa2":"# prepare the Kfold cross-validation procedure\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\ncv = KFold(n_splits=10, random_state=42, shuffle=True)","aa090922":"from sklearn.neighbors import KNeighborsClassifier\nx=[5,6,7,8,9,10,11,12,13]\nfor k in x:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train,y_train)\n    print(knn.score(X_test, y_test))","fb577a7f":"#Cross validation\nknncross = KNeighborsClassifier(n_neighbors=11)\nknnscores = cross_val_score(knncross, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(knnscores)","7a6cb919":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nlr.score(X_test, y_test)","e0de83ea":"#Cross validation\nlrcross = LogisticRegression()\nlrscores = cross_val_score(lrcross, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(lrscores)","24ae3328":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(random_state=42)\ndtree.fit(X_train,y_train)\ndtree.score(X_test, y_test)","499b844f":"params={\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(1,10),\n    \"min_samples_split\":range(1,10),\n    \"min_samples_leaf\":range(1,10)\n    \n}","bcec1b1f":"grid= GridSearchCV(dtree,\n                  param_grid=params,\n                  cv=10)\ngrid.fit(X_train,y_train)","7535d967":"grid.best_params_","2ea9e908":"dtree2 = DecisionTreeClassifier(random_state=42,criterion='gini',max_depth= 5,min_samples_leaf= 8,min_samples_split= 2)\ndtree2.fit(X_train,y_train)\ndtree2.score(X_test, y_test)","425b9583":"data2=data.copy()","d253ae40":"data2=data2[['cp_typical','cp_atypical','cp_non-anginal','cp_asymptomatic','thalachh','exng','oldpeak','slp','caa','thall','output']]","d52a05ad":"data2.head()","ccaecb67":"#Taking values from data\nM2=data2.values\nX2 = M2[:,0:10]\ny2 = M2[:,10]","3cc1670a":"#Train Test split\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.20, random_state=42)","4a53a989":"x=[5,6,7,8,9,10,11,12,13]\nfor k in x:\n    knn2 = KNeighborsClassifier(n_neighbors=k)\n    knn2.fit(X2_train,y2_train)\n    print(knn2.score(X2_test, y2_test))","1b184745":"#Cross validation\nknn2cross = KNeighborsClassifier(n_neighbors=10)\nknn2scores = cross_val_score(knn2cross, X2, y2, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(knn2scores)","3238520b":"lr2 = LogisticRegression()\nlr2.fit(X2_train,y2_train)\nlr2.score(X2_test, y2_test)","dfcacecd":"#Cross validation\nlr2cross = LogisticRegression()\nlr2scores = cross_val_score(lr2cross, X2, y2, scoring='accuracy', cv=cv, n_jobs=-1)\nprint(lr2scores)","f6ae0353":"dtree3 = DecisionTreeClassifier(random_state=42)\ndtree3.fit(X2_train,y2_train)\ndtree3.score(X2_test, y2_test)","c65cb95e":"params={\n    \"criterion\":['gini','entropy'],\n    \"max_depth\":range(1,10),\n    \"min_samples_split\":range(1,10),\n    \"min_samples_leaf\":range(1,10)\n    \n}","859eecd3":"grid2= GridSearchCV(dtree,\n                  param_grid=params,\n                  cv=10)\ngrid2.fit(X2_train,y2_train)","e574760b":"grid2.best_params_","95fba1d6":"dtree4 = DecisionTreeClassifier(random_state=42,criterion='entropy',max_depth= 4,min_samples_leaf= 6,min_samples_split= 2)\ndtree4.fit(X2_train,y2_train)\ndtree4.score(X2_test, y2_test)","7f9e27aa":"# Chest Pain","e5a493d6":"# Logistic Regression","a376acd1":"# Data","a1eb5ef9":"# restecg","7e36e6b2":"With grid search performance of decision tree classifier improved","4695245f":"According to graphs chest pain values does not show in order effect to the output so we decided to change integer encoding to one-hot encoding (dummies).","7f447b78":"# Decision Tree","57bc791c":"Our data is maden for predicting heart attack chance of people.\n\nAge : Age of the patient\n\nSex : Sex of the patient (male=1, female=0)\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n  Value 0: typical angina\n  Value 1: atypical angina\n  Value 2: non-anginal pain\n  Value 3: asymptomatic\n\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n  Value 0: normal\n  Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n  Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\nthalach : maximum heart rate achieved\n\noutput : 0= less chance of heart attack 1= more chance of heart attack\n","e748745f":"We choose to use features which has higher correlattion then 0.30","f704da38":"Results of second features:\n\nKNN(best):\n0.7868852459016393\n\nKNN_Cross(best):\n0.80645161\n\nLogistic Regression:\n0.8688524590163934\n\nLogistic Regression_Cross(best):\n0.90322581\n\nDecision Tree:\n0.819672131147541\n\nDecision Tree(gridcv):\n0.819672131147541","8802b9b0":"According to our tests logistic regression with cross validation with first features shows best results and second best results with second features. Logistic Regression withot cross validation shows third and fourt best options. Its expected due to its very efficient when output is 2 class problem. Tests shows decision tree comes second in total after logistic regression. In first features decision tree gets better results with grid search but the results doesn't change with second features. KNN methods shows worst results between this 3 models. According to test changing features gives best effects with KNN. It shows with better feature chooses KNN can get better effects.","8471c92f":"We are going to predict output so its a binary classification target. The methods will we use are Logistic Regression, KNN and Decision Tree","f05ae84b":"# Age","4b33aa08":"# Sex","0aa55051":"In first experiment Logistic Regression gave the best result. Logistic Regression has advantages most of time in high correlated data its only disadvantage is its only usable in 2 class outputs but it is not a problem in our dataset. Decision tree has given second best result we believe decision tree is mostly superior model against knn but its a more complicated model due to knn. Decision trees result chances in its random state and mostly gives better results with more improved hyperparameters.","6f87fd54":"# Logistic Regression","d84513a0":"# KNN","13df3240":"# Decision Tree","2265c826":"Our performance is upgraded with cross validation","09f0dd7e":"# Models","21bab3e5":"# Conclusion","95576478":"Results of first features:\n\nKNN(best):\n0.7540983606557377\n\nKNN_Cross(best):\n0.73333333\n\nLogistic Regression:\n0.8688524590163934\n\nLogistic Regression_Cross(best):\n0.93548387\n\nDecision Tree:\n0.8360655737704918\n\nDecision Tree(gridcv):\n0.8524590163934426","11593ca5":"According to visualizations restecg values doesn't show any in order effect like chest pain feature so we decided to use dummies for this feature too.","802546db":"# KNN","bd1ecc22":"According to correlation matrix every features have higher correlation then 0.1 with the target except fbs and chol","3530cf02":"Now we try different hyperparameters, we try to use gridseach for decision tree","7a13dbbf":"# Different Features","a2a9f8eb":"Firstly we wanted to check chest pain because of it has high correlation with our target and its values are integer encoded.","c59c00aa":"# Thalachh"}}