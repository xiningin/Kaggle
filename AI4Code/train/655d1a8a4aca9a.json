{"cell_type":{"73da2cdf":"code","41789ece":"code","03a1d11f":"code","67c76d1f":"code","3a0c3ca4":"code","01bf831c":"code","95262278":"code","417ff6dd":"code","a87b072f":"code","fd23034b":"code","3cb44ffc":"code","dd2c0705":"code","75f13d07":"code","81c1216c":"code","65a9a335":"code","2d557631":"code","b8656089":"code","6691cf1a":"code","9a03bd91":"code","b98cdcd4":"code","e342a8c5":"code","af8c1f5c":"code","651494ee":"code","9f3609a8":"code","36c04748":"code","439f678a":"code","4edf4b01":"code","be2240b3":"code","82cf59a4":"code","0467dc9c":"code","e6c7368c":"code","24a2e40c":"code","91191b8b":"code","63a4a39a":"code","31fc17ba":"code","f849138f":"code","59d3b6a3":"code","982af6aa":"code","fb59fa55":"code","ae5bdd3b":"code","22197c53":"code","74508565":"code","ff31f55c":"code","0d15b1ab":"code","8e79ec92":"code","93b9daf2":"code","90747a12":"code","28a74a4a":"markdown","27ed72a1":"markdown","e0725523":"markdown","4aa6ad1e":"markdown","5fda45fd":"markdown","acf3ea4d":"markdown","02dee282":"markdown","12efbfa9":"markdown","1a63a409":"markdown","51514570":"markdown","dc0f682a":"markdown","923fa39a":"markdown","1b2b33c3":"markdown","db153b82":"markdown","c300c168":"markdown","3f459c3c":"markdown","acfa2ff5":"markdown","b63f0a4b":"markdown","dea14021":"markdown","32eae7ef":"markdown","df0406d2":"markdown","615d7554":"markdown","f3f9a31a":"markdown","1b0cd539":"markdown","241dc534":"markdown","5faf9144":"markdown","878f37e5":"markdown","f14a0971":"markdown","140fb6f4":"markdown","e9bd499c":"markdown","752331ce":"markdown","484fb100":"markdown","7cf425ce":"markdown","8588e685":"markdown","0a48f03a":"markdown","52c9b746":"markdown","4c57e84e":"markdown"},"source":{"73da2cdf":"# Here are all imports that you will need\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics \nfrom sklearn.metrics import accuracy_score","41789ece":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","03a1d11f":"iris = pd.read_csv(r'..\/input\/iris\/Iris.csv')","67c76d1f":"# first 5 rows of data\niris.head()","3a0c3ca4":"iris['Species'].value_counts()","01bf831c":"iris.info()","95262278":"iris.describe()","417ff6dd":"# Create a pairplot of the data set. Which flower species seems to be the most separable?\nsns.pairplot(iris ,hue='Species')\n# Iris setosa seems most separable from the other two species","a87b072f":"fig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","fd23034b":"sns.lmplot(x = 'SepalLengthCm', y = 'SepalWidthCm', data = iris, hue = 'Species', col = 'Species')","3cb44ffc":"fig = iris[iris.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')\niris[iris.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","dd2c0705":"sns.lmplot(x = 'PetalLengthCm', y = 'PetalWidthCm', data = iris, hue = 'Species', col = 'Species')","75f13d07":"plt.figure(figsize=(10,8)) \nsns.heatmap(iris.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","81c1216c":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=iris)","65a9a335":"iris.hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","2d557631":"X = iris.drop(['Species'], axis=1)\ny = iris['Species']\n# print(X.head())\nprint(X.shape)\n# print(y.head())\nprint(y.shape)","b8656089":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","6691cf1a":"train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=10)\nprint(train_X.shape)\nprint(train_y.shape)\nprint(test_X.shape)\nprint(test_y.shape)","9a03bd91":"train_X.head(2)","b98cdcd4":"test_X.head(2)","e342a8c5":"# Instantiate learning model (k = 3)\nknc = KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\n# Fitting the model\nknc.fit(train_X,train_y)\n# Predicting the Test set results\npred_y = knc.predict(test_X)\n# Getting the accuracy score\naccuracy = metrics.accuracy_score(pred_y,test_y)\nprint('The accuracy of the KNN is', accuracy)","af8c1f5c":"#Counting the correct predicitions:\nprint(np.count_nonzero(pred_y == test_y))","651494ee":"from sklearn.metrics import classification_report\nprint(classification_report(pred_y,test_y))","9f3609a8":"from sklearn.metrics import accuracy_score\nN = [] #No. of Neighbours\nk_scores = [] #Accuracy Score\ncorr_pred = [] #correct prediction\nk_list = [1,3,5,7,10,20,30,40,50]\n\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(train_X, train_y)\n    y_pred = knn.predict(test_X)\n    k_scores.append(accuracy_score(y_pred,test_y))\n    corr_pred.append(np.count_nonzero(y_pred == test_y))\n    N.append(k)\n    \nprint('Accuracy score for all values of k is:',k_scores)\nprint('Correct number of predictions for all the values of k is:',corr_pred)\n    \nplt.grid(True)\nplt.plot(N,k_scores)","36c04748":"from sklearn.model_selection import cross_val_score\nkn = KNeighborsClassifier(n_neighbors=3)\nscores = cross_val_score(kn, X, y, cv=10, scoring='accuracy')\nprint(scores)\nprint(scores.mean())","439f678a":"k_list = [1,3,5,7,10,20,30,40,50]\n# creating list of cv scores\ncv_scores = []\ncorrect_pred = []\n\n# perform 10-fold cross validation\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, train_X, train_y, cv=10, scoring='accuracy')\n    correct = np.count_nonzero(y_pred == test_y)\n    cv_scores.append(scores.mean())\n    correct_pred.append(correct)\n    \n    \nprint (cv_scores)\nprint (correct_pred)\n","4edf4b01":"# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(k_list, cv_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')","be2240b3":"# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\nplt.xlabel('Number of Neighbors K', fontsize=15)\nplt.ylabel('Misclassification Error', fontsize=15)\nsns.set_style(\"whitegrid\")\nplt.plot(k_list, MSE)\n\nplt.show()\n","82cf59a4":"# finding best k\nbest_k = k_list[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d.\" % best_k)","0467dc9c":"error_rate = []\nk_list = [1,3,5,7,10,20,30,40,50]\n# Will take some time\nfor i in k_list:\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(train_X,train_y)\n    pred_i = knn.predict(test_X)\n    error_rate.append(np.mean(pred_i != test_y))\n    \nprint (error_rate)","e6c7368c":"plt.figure(figsize=(10,6))\nplt.plot(k_list,error_rate,color='black', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","24a2e40c":"from sklearn.model_selection  import GridSearchCV","91191b8b":"k_list = [1,3,5,7,10,20,30,40,50]\nprint (k_list)","63a4a39a":"param_grid=dict(n_neighbors=k_list)\nprint (param_grid)","31fc17ba":"grid=GridSearchCV(knn,param_grid,cv=10,scoring='accuracy')","f849138f":"grid.fit(X,y)","59d3b6a3":"grid.cv_results_","982af6aa":"print (grid.cv_results_['params'][0])\nprint (grid.cv_results_['split0_test_score'])\nprint (grid.cv_results_['mean_test_score'][0])\nprint (grid.cv_results_['std_test_score'][0])","fb59fa55":"#create a list of the mean scores only\ncv_results = pd.DataFrame(grid.cv_results_)\n\n\nprint(cv_results['mean_test_score'])","ae5bdd3b":"# examine the best model\n\n# Single best score achieved across all params (k)\nprint(grid.best_score_)\n\n# Dictionary containing the parameters (k) used to generate that score\nprint(grid.best_params_)\n\n# Actual model object fit with those best parameters\n# Shows default parameters that we did not specify\nprint(grid.best_estimator_)","22197c53":"plt.plot(k_list, cv_results['mean_test_score'])\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')","74508565":"weight_options=['uniform','distance']","ff31f55c":"param_grid=dict(n_neighbors=k_list,weights=weight_options)\nprint (param_grid)","0d15b1ab":"grid=GridSearchCV(knn,param_grid,cv=10,scoring='accuracy')","8e79ec92":"grid.fit(X,y)","93b9daf2":"grid.cv_results_","90747a12":"print (grid.best_score_)\nprint (grid.best_params_)","28a74a4a":"# Run the fit and predict the class from features","27ed72a1":"# Data Visualization","e0725523":"view the complete results (list of named tuples)","4aa6ad1e":"This gives us the count of correct predictions which is 44","5fda45fd":"The Sepal Width and Length are not correlated The Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n\nThen we will use 1 Petal Feature and 1 Sepal Feature to check the accuracy of the algorithm as we are using only 2 features that are not correlated. Thus we can have a variance in the dataset which may help in better accuracy.","acf3ea4d":"We can also see through the numbers that k = 20 is some how giving us best test score so the GridSearchCv has also choosen the same.","02dee282":"OUR goal here is to find the optimal value of K for KNN which we set using the n_neighbors parameter. Thus we will loop through a range of reasonable values for K and for each value use 10-fold cross-validation to estimate the out-of-sample accuracy.","12efbfa9":"There are 150 observations with 4 features each (sepal length, sepal width, petal length, petal width). There are no null values, so we don't have to worry about that. There are 50 observations of each species (setosa, versicolor, virginica).","1a63a409":"we will examine the individual tuples, just in case you need to do so in the future. I am going to slice the list to select the first tuple using the [0] notation.\n\nexamine the first tuple","51514570":"By using train_test_split method we have got k = 7 as best option.","dc0f682a":"# 3. More efficient parameter tuning using GridSearchCV\nGridSearchCV allows you to define a set of parameters that you want to try with a given model and it will automatically run cross-validation using each of those parameters keeping track of the resulting scores. Essentially it replaces the for loop above as well as providing some additional functionality.","923fa39a":"It turns out to be about 97%. Because we used cross-validation and arrived at this result, we are more confident that it is an accurate estimate of out-of-sample accuracy than we would be if we had used train\/test split.","1b2b33c3":"After looking at this we can see that k = 3 is giving us best result.","db153b82":"We can see that error rate of k = 7 is 0 so that is the best option according to cross validation.","c300c168":"That means, it compared the testing responses with the predicted responses and calculated that 97.7% of our predictions were correct.","3f459c3c":"# Prepare the data for classification.","acfa2ff5":"# Classification","b63f0a4b":"# Preview of Data","dea14021":"# Prepare 2 scatter plots","32eae7ef":"We have used three methods to see how accuracy score and the number of correct predictions change with the number of neighbors k.\n1. Train_test_split method - it gave us result as k = 7\n2. Cross validation method - it gave us result as k = 3 and the error rate of k = 7 was least that is 0.\n3. GridSearchCV method - it gave us the best result as k = 20. We also tried Weight Parameter and still got the result as k =20 as best option. Using the best parameter we predicted using out of sample data and got the coprrect prediciton.\n\nAs GridSearchCv is the best method to check values of k so i will go with k = 20.","df0406d2":"we fit the grid with data, with just the X and y objects.","615d7554":"# 1.Train_test_split method","f3f9a31a":"We create what is known as a parameter grid. It is simply a python dictionary, in which the key is the parameter name, and the value is a list of values that should be searched for that parameter.","1b0cd539":"# Data inspection","241dc534":"Next we will instantiate the grid. You will notice that it has the same parameters as cross_val_score except it does not have the X and y but it does include the param_grid\n\ninstantiate the grid","5faf9144":"To split the data into training and testing sets, we are going to use scikit-learn\u2019s built-in, train_test_split function","878f37e5":" # See how accuracy score and the number of correct predictions change with the number of neighbors k","f14a0971":"We are getting k = 20 as the best value after observing all the parameters.","140fb6f4":"The dataset was first introduced by statistician R. Fisher and consists of 50 observations from each of three species Iris (_Iris setosa_, _Iris virginica_ and _Iris versicolor_). For each sample, 4 features are given: the sepal length and width, and the petal length and width.\n\nThe goal is to train kNN algorithm to distinguish the species from one another.","e9bd499c":"We see that during the first iteration, the model achieved an accuracy of 100%\nIn the second iteration, the model achieved an accuracy of 93.3%\nAs mentioned above, we will average the 10 scores and use that as our out-of-sample accuracy.\n","752331ce":"# Label encoding\nAs we can see labels are categorical. KNeighborsClassifier does not accept string labels. We need to use LabelEncoder to transform them into numbers. Iris-setosa correspond to 0, Iris-versicolor correspond to 1 and Iris-virginica correspond to 2.","484fb100":"As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal.","7cf425ce":"# Split Data","8588e685":"The 1st tuple indicates that when the n_neighbors parameter was set to 1, the mean cross-validated accuracy was 0.96 and the standard deviation of the accuracy scores was 0.053. While the mean is usually what we pay attention, the standard deviation is something to keep in mind, because if the standard deviation is, then that means the cross-validated accuracy might not be as reliable.","0a48f03a":"# 2. Cross Validation method\n\nWe want to choose the tuning parameters for KNN which will produce a model that best generalizes the out-of-sample data. We will focus on tuning the k in K-NearestNeighbors which represents the number of nearest neighbors that are taken into account when making a prediction.","52c9b746":"# Data download and giving names to columns","4c57e84e":"# Weight parameter used\nThis weights parameter controls how the K-nearest neighbors are weighted when making a prediction. The default option is uniform which means that all points in the neighborhood weighted equally but another option is distance which weights closer neighbors more heavily than further neighbors."}}