{"cell_type":{"f259b25e":"code","80a27d23":"code","b37d53d6":"code","2c06ed90":"code","d13abf19":"code","99c997b5":"code","5d31b36a":"code","be2d2ac2":"code","f540885b":"code","e3430565":"code","f02234c5":"code","86bcba8f":"code","9ff8ead8":"code","55a34e37":"code","421493b5":"code","4c881bab":"code","1b4e2b13":"code","b7ca2f86":"code","68af4495":"code","2ff73c8d":"code","82a04d25":"code","77a6d097":"code","669eafd0":"code","877317e3":"code","46e4ee6c":"code","4ab8054f":"code","ce352f92":"code","84ab877e":"code","c4cb0046":"code","7ccfba19":"code","537b1bf0":"code","c723434f":"code","cb41c4ff":"code","61983169":"code","3ddf2774":"markdown","d1ed50b6":"markdown","e905feef":"markdown","efda38f4":"markdown","44b3cbf7":"markdown","a3d09687":"markdown","4b7deffb":"markdown","cb7379e9":"markdown","8cc47daa":"markdown","b346de86":"markdown","aa326eb0":"markdown","4b75bbda":"markdown","aaf1ba36":"markdown","a614f7b3":"markdown","7272861d":"markdown","a66786d0":"markdown","dd2ccc9c":"markdown","73508e31":"markdown","b67b393c":"markdown","c447d724":"markdown","7960bce9":"markdown","48a56743":"markdown","ef2c457f":"markdown","d000ebf6":"markdown","350400a5":"markdown","aab38950":"markdown","3ab78d50":"markdown","f41a2b29":"markdown","6f80f7e0":"markdown"},"source":{"f259b25e":"# imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.eval_measures import rmse\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# allow plots to appear directly in the notebook\n%matplotlib inline\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","80a27d23":"# Advertising data set contains information about money spent on advertisement (TV, Radio and Newspaper) and their generated Sales.\n\ndf_advertising = pd.read_csv(\"..\/input\/advertising-dataset\/advertising.csv\")\ndf_advertising.head(2)","b37d53d6":"# shape of the DataFrame\ndf_advertising.shape","2c06ed90":"# visualize the relationship between the features and the target using scatterplots\nsns.pairplot(df_advertising, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size = 4, aspect = 1)","d13abf19":"sns.pairplot(df_advertising[['TV','Radio','Newspaper']])","99c997b5":"sns.heatmap(df_advertising[['TV','Radio','Newspaper']].corr(), annot = True)","5d31b36a":"# This function will regress y on x and then draw a scatterplot of the residuals.\n\nsns.residplot(x = df_advertising['TV'], y = df_advertising[\"Sales\"], lowess = True)","be2d2ac2":"Ststsmodels_model = smf.ols(formula='Sales ~ TV', data = df_advertising)\nStstsmodels_result = Ststsmodels_model.fit()\n\n# print the coefficients\nStstsmodels_result.params","f540885b":"### SCIKIT-LEARN ###\n\nX = df_advertising[['TV']]\ny = df_advertising[[\"Sales\"]]\n\nSkLearn_model = LinearRegression()\nSkLearn_result = SkLearn_model.fit(X, y)\n\n# print the coefficients\nprint(SkLearn_result.intercept_)\nprint(SkLearn_result.coef_)","e3430565":"# manually calculate the prediction\nSales = 6.97482149 + 0.05546477*50\nSales * 1000","f02234c5":"### STATSMODELS ###\n\nX_new = pd.DataFrame({'TV': [50]})\n\n# predict for a new observation\nSales = Ststsmodels_result.predict(X_new)\nSales * 1000","86bcba8f":"### SCIKIT-LEARN ###\n\n# predict for a new observation\nSales = SkLearn_result.predict(np.array(50).reshape(1,-1))\nSales * 1000","9ff8ead8":"sns.pairplot(df_advertising, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=4, aspect = 1, kind='reg')","55a34e37":"Ststsmodels_residual = Ststsmodels_result.resid\nax = sm.qqplot(Ststsmodels_residual, fit = True, line = \"45\")","421493b5":"### STATSMODELS ###\n\n# print the confidence intervals for the model coefficients\nStstsmodels_result.conf_int()","4c881bab":"### STATSMODELS ###\n\n# print the p-values for the model coefficients\nStstsmodels_result.pvalues","1b4e2b13":"### STATSMODELS ###\n\n# print a summary of the fitted model\nStstsmodels_result.summary()","b7ca2f86":"### SCIKIT-LEARN ###\n\n# print the R-squared value for the model\nSkLearn_result.score(X, y)","68af4495":"### STATSMODELS ###\n\n# create a fitted model with all three features\nStstsmodels_model = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=df_advertising)\nStstsmodels_result = Ststsmodels_model.fit()\n\n# print the coefficients\nStstsmodels_result.params","2ff73c8d":"### SCIKIT-LEARN ###\n\nfeature_cols = ['TV', 'Radio', 'Newspaper']\nX = df_advertising[feature_cols]\ny = df_advertising[[\"Sales\"]]\n\n# instantiate and fit\nSkLearn_model = LinearRegression()\nSkLearn_result = SkLearn_model.fit(X, y)\n\n# print the coefficients\nprint(SkLearn_result.intercept_)\nprint(SkLearn_result.coef_)","82a04d25":"### STATSMODELS ###\n\n# print a summary of the fitted model\nStstsmodels_result.summary()","77a6d097":"# only include TV and Radio in the model\n\n# instantiate and fit model\nStstsmodels_model = smf.ols(formula='Sales ~ TV + Radio', data=df_advertising)\nStstsmodels_result = Ststsmodels_model.fit()\n\n# print a summary of the fitted model\nStstsmodels_result.summary()","669eafd0":"# exclude Newspaper\nX = df_advertising[['TV', 'Radio']]\ny = df_advertising.Sales\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n# Instantiate model\nlm2 = LinearRegression()\n\n# Fit model\nlm2.fit(X_train, y_train)\n\n# Predict\ny_pred = lm2.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","877317e3":"# include Newspaper\nX = df_advertising[['TV', 'Radio', 'Newspaper']]\ny = df_advertising.Sales\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n\n# Instantiate model\nlm2 = LinearRegression()\n\n# Fit Model\nlm2.fit(X_train, y_train)\n\n# Predict\ny_pred = lm2.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","46e4ee6c":"from sklearn.linear_model import Ridge\n\nridgeReg = Ridge(alpha=0.1, normalize=True)\n\nridgeReg.fit(X_train,y_train)\n\ny_pred = ridgeReg.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","4ab8054f":"print(ridgeReg.intercept_)\nprint(ridgeReg.coef_)","ce352f92":"ridgeReg = Ridge(alpha=0.9, normalize=True)\n\nridgeReg.fit(X_train,y_train)\n\ny_pred = ridgeReg.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","84ab877e":"print(ridgeReg.intercept_)\nprint(ridgeReg.coef_)","c4cb0046":"from sklearn.linear_model import Lasso\n\nlassoReg = Lasso(alpha=0.1, normalize=True)\n\nlassoReg.fit(X_train,y_train)\n\ny_pred = lassoReg.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","7ccfba19":"print(lassoReg.intercept_)\nprint(lassoReg.coef_)","537b1bf0":"lassoReg = Ridge(alpha=0.9, normalize=True)\n\nlassoReg.fit(X_train,y_train)\n\ny_pred = lassoReg.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","c723434f":"print(lassoReg.intercept_)\nprint(lassoReg.coef_)","cb41c4ff":"from sklearn.linear_model import ElasticNet\n\nelsticNetReg = ElasticNet( l1_ratio=0.2, normalize=True)\n\nelsticNetReg.fit(X_train,y_train)\n\ny_pred = elsticNetReg.predict(X_test)\n\n# RMSE\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","61983169":"print(elsticNetReg.intercept_)\nprint(elsticNetReg.coef_)","3ddf2774":"### <u> Model Interpretation <\/u>\n\n- For a given amount of Radio and Newspaper ad spending, an increase of Rs.1000 in TV ad spending is associated with an increase in Sales of 54.444 items.\n- TV and Radio have small p-values, whereas Newspaper have a large p-value\n- Reject the null hypothesis for TV and Radio, Fail to reject the null hypothesis for Newspaper\n- This model has a higher R-squared (0.903) than the previous model(0.812). This model provides a better fit to the data than a model that only includes TV.","d1ed50b6":"<b> The Confidence Interval is interpreted as:  <\/b>\n\nIf the population from which the sample (dataset) was drawn is sampled 100 times, Approximately 95% of times the value of intercept would fall in the range: \"6.338740 - 7.610903\" and the value of the slope would fall in range: \"0.051727 - 0.059203\".","e905feef":"In a large dataset (10,000 features) some of the independent features correlates with other independent features.\n\nIf we apply ridge regression to it, it will retain all of the features but will shrink the coefficients. But the model will still remain complex (10,000 features), thus may lead to poor performance.\n\nIf we apply lasso regression to it, it retains only one correlated variable and sets other correlated variables to zero. That will possibly lead to some loss of information resulting in lower accuracy in the model.\n\nWe will use Elastic Net Regression to solve the problem, which is a hybrid of ridge and lasso regression.","efda38f4":"### Ridge Regression:\n\nIn ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients.\n\n$$\\sum_{i = 1}^{n}(y_i - \\hat{y_i})^2 + \\lambda \\sum_{j = 0}^{p}(w_j)^2$$\n\nThis is equivalent to saying minimizing the cost function $\\sum_{i = 1}^{n}(y_i - \\hat{y_i})^2$ under the condition:\n\n$$\\text{For some c > 0, }\\sum_{j = 0}^{p}(w_j)^2 \\text{ < c}$$\n\nRidge regression puts constraint on the coefficients (w). It shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity. When $\\lambda$ \u2192 0 , the cost function becomes similar to the linear regression cost function.\n\nRidge regression can be used when the number of predictor variables exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables).","44b3cbf7":"## Regularization Techniques\n\nOnce we use linear regression on a data-set divided in to training and test set, calculating the scores on training and test set can give us a rough idea about whether the model is suffering from over-fitting or under-fitting.\n\nIf we have very few features on a data-set and the score is poor for both training and test set then it\u2019s a problem of under-fitting. On the other hand if we have large number of features and test score is relatively poor than the training score then it\u2019s the problem of over-fitting.\n\nRegularization is acsimple technique to reduce model complexity and prevent over-fitting which may result from simple linear regression.\n\n### Types of Regularization Techniques\n\nThere are different Regularization Techniques with different choices of order of the parameter in the regularization term, which is denoted by $\\sum_{i}(\\theta_i)^p$ . This is more generally known as $L_p$ regularizer.\n\n\nLet us try to visualize some by plotting them in a 2D space. For that we suppose that we just have two parameters $\\theta_1$ and $\\theta_2$.\n\n![Screen%20Shot%202019-12-26%20at%2012.33.43.png](attachment:Screen%20Shot%202019-12-26%20at%2012.33.43.png)\n\n- $p = 0.5$, $\\sum_{i}(|\\theta_i|)^p$ = ${\\theta_0}^{\\frac{1}{2}} + {\\theta_1}^{\\frac{1}{2}}$\n- $p = 1$, $\\sum_{i}(|\\theta_i|)^p$ = ${|\\theta_0|} + {|\\theta_1|}$\n- $p = 2$, $\\sum_{i}(|\\theta_i|)^p$ = ${\\theta_0}^2 + {\\theta_1}^2$\n- $p = 4$, $\\sum_{i}(|\\theta_i|)^p$ = ${\\theta_0}^4 + {\\theta_1}^4$\n\nIn the above plots, axis denote the parameters($\\theta_1$ and $\\theta_2$).\n\nFor $p = 0.5$, we can only get large values of one parameter only if other parameter is too small. For $p = 1$, we get sum of absolute values where the increase in one parameter $\\theta$ is exactly offset by the decrease in other. For $p = 2$, we get a circle and for larger p values, it approaches a round square shape.\n\n<b> Ridge ($L_2$) and Lasso ($L_1$) regression are some of the simple Regularization Techniques. <\/b>\n\nIn the figure given below, the blue shape refers the regularization term and other shape present refers to the least square error. The cost function method determine coefficients by finding the first point where the elliptical contours hit the region of constraints.\n\n![Screen%20Shot%202019-12-26%20at%2012.43.17.png](attachment:Screen%20Shot%202019-12-26%20at%2012.43.17.png)\n\nIn the figure given below,,first figure is for L1 and the second one is for L2 regularization. The blue shape refers the regularization term and other shape present refers to the least square error. The black point denotes that the least square error is minimized at that point and as we can see that it increases quadratically as we move away from it and the regularization term is minimized at the origin where all the parameters are zero . The cost function method determine coefficients by finding the first point where the elliptical contours hit the region of constraints.\n\nSince the shape formed by L2 regularizer is a circle, The L2 optimum(intersection point) can fall on the axis lines only when the minimum MSE (mean square error or the black point in the figure) is also exactly on the axis. But in case of L1, the L1 optimum can be on the axis line because its contour is sharp and therefore there are high chances of interaction point to fall on axis. Whenever the elliptical region hits such point, one of the features completely vanishes!","a3d09687":"### <u> Model Interpretation <\/u>\n\nThe Interpretation comes out as : y = 6.9748 + 0.05546x\n\nWhich means - A \"unit\" increase in TV ad spending is associated with a 0.05546 \"unit\" increase in Sales. Or, An additional $1,000 spent on TV ads is associated with an increase in sales of 55.46 items.","4b7deffb":"As we can see, as we increase the value of alpha, the magnitude of the coefficients decreases, where the values reaches to zero but not absolute zero.\n\nWe have to choose the value of alpha wisely by iterating it through a range of values and using the one which gives us the lowest error.","cb7379e9":"### Confidence in Model","8cc47daa":"### Multiple Linear Regression\n\nSimple linear regression can easily be extended to include multiple features. This is called multiple linear regression:\n\n$$y = a_0 + a_1 x_1 + a_2 x_2 + ... + a_n x_n$$","b346de86":"## <u> Linear Regression Assumption # 3 <\/u>\n\n### Homoscedasticity:\n\nOrdinary least squares (OLS) regression assumes that all residuals ($\\hat{y} - y$) are drawn from a population that has a constant variance (homoscedasticity). Homoscedasticity describes a situation in which the error term ($\\hat{y} - y$) is same across all values of the independent variables. There should be no clear pattern in the distribution and if there is a specific pattern,the data is heteroscedastic.\n\nThe presence of <b> Heteroskedasticity <\/b> indicates non linearity in the data.\n\nLinear regression doesn\u2019t assume anything about\nthe distributions of x and y. It only makes assumptions about the distribution of the error terms.\n\nIn order to capture the non-linear effects, we use polynomial regression (add quadratic and cubic polynomials to the data).\n\nA simple form of polynomial with a single independent variable $x$:\n\n$$y = a_0 + a_1x + a_2x^2 + a_3x^3$$\n\nA scatter plot of residual values vs predicted values is a goodway to check for homoscedasticity.","aa326eb0":"This heatmap gives us the correlation coefficients of each feature with respect to one another which are less than 0.4. Thus the features aren\u2019t highly correlated with each other.","4b75bbda":"## <u> Linear Regression Assumption # 2 <\/u>\n\n### Little or no Multicollinearity between the features:\n\nWe interprate a regression coefficient as the mean change in the target for each unit change in an feature when we hold all the other features constant. When features are correlated, changes in one feature in turn shifts another feature\/features. The stronger the correlation, the more difficult it is to change one feature without changing another. If multicollinearity exists between features, it becomes difficult for the model to estimate the relationship between each feature and the target independently.\n\n<b> How multicollinearity can be treated? <\/b>\n\nIf we have 2 features which are highly correlated we can drop one feature or combine the 2 features to form a new feature,which can further be used for prediction.\n\nPair plots and heatmaps(correlation matrix) can be used for identifying highly correlated features.","aaf1ba36":"If the p-value is less than 0.05, we reject the null hypothesis.\n\n<b> Interpreting p-values: <\/b>\n\n- p-value is less than 0.05.\n- Reject the null hypothesis.\n- There is a relationship between dependent and independent variable.\n\nWe generally ignore the p-value for the intercept.","a614f7b3":"### How Well Does the Model Fit the data?\n\nTo evaluate the overall fit of a linear model, we use the $R^2$ and $\\text{Adjusted }R^2$ values.\n\n$$R^2 = 1 - \\frac{(Y - \\hat{Y})^2}{(Y - \\bar{Y})^2}$$\n\n$R^2$ lets us know how good a regression model is when compared to the average. It explains the degree to which the input variables explain the variation of output \/ predicted variable. The value of R-square is always between 0 and 1, where 0 means that the model does not model explain any variability in the target variable (Y) and 1 meaning it explains full variability in the target variable.\n\nHowever, the problem with R-squared is that it will either stay the same or increase with addition of more variables, even if they do not have any relationship with the output variables. This is where $\\text{Adjusted }R^2$ comes to help. Adjusted R-square penalizes the model for adding variables which do not improve the existing model.\n\n$$\\text{Adjusted }R^2 = 1 - \\frac{(1 - R^2)(N - 1)}{N -P - 1}$$\n\nwhere $N$ is the number of instances and $P$ is the number of features.\n\nHence, if we are building Linear regression on multiple variable, it is always suggested that we use Adjusted R-squared to judge goodness of model. In case we only have one input variable, R-square and Adjusted R squared would be exactly same.\n\nTypically, the more non-significant variables you add into the model, the gap in R-squared and Adjusted R-squared increases.","7272861d":"### Plotting the Least Squares Line","a66786d0":"Thus, we predict Sales of 9,748 items in that market.","dd2ccc9c":"## <u> Linear Regression Assumption # 5 <\/u>\n### Little or No autocorrelation in the residuals:\n\nAutocorrelation occurs when the residual errors are dependent on each other.The presence of correlation in error terms drastically reduces model\u2019s accuracy.This usually occurs in time series models where the next instant is dependent on previous instant.\n\nThe Durbin Watson (DW) statistic is a test for autocorrelation in the residuals from a statistical regression analysis. The Durbin-Watson statistic will always have a value between 0 and 4. The closer to 0 the statistic, the more evidence for positive serial correlation. The closer to 4, the more evidence for negative serial correlation. A value of 2.0 means that there is no autocorrelation detected in the sample.\n\nDurbin-Watson:\t2.029 from the above ols summary shows that there is no autocorrelation detected in the sample.","73508e31":"### Hypothesis Testing and p-values\n\n<b> Steps for Hypothesis Testing <\/b>\n\n- Start with a null hypothesis and an alternative hypothesis (that is opposite the null)\n- Check whether the data supports rejecting the null hypothesis or failing to reject the null hypothesis\n\nIn inferential statistics, the null hypothesis is a general statement or default position that there is nothing significantly different happening, like there is no association among groups or variables, or that there is no relationship between two measured phenomena.\n\n<b> Hypothesis Testing <\/b>\n\nnull hypothesis:\n - There is no relationship between TV ads and Sales\n\nalternative hypothesis:\n- There is a relationship between TV ads and Sales\n","b67b393c":"### <u>Model Prediction <\/u>\nFor the TV advertising of $50,000. What is prediction for Sales?\n\nWe would use 50 instead of 50,000 because the original data consists of examples that are divided by 1000","c447d724":"## <u> Linear Regression Assumption # 1 <\/u>\n\n### <u>Linear Relationship between the features and target: <\/u>\n\nLinear regression captures only linear relationship. There should be a linear relationship between the features and the target.\n\nThis can be validated by plotting a scatter plot between the features and the target.","7960bce9":"## <u> Simple linear regression <\/u>\n\nOne independent(x) variable and one dependent(y) variable. Based on the given data points, we try to plot a line (best values for the coefficients a_0 and a_1) that fits the points (minimizes the sum of squared residuals (\"sum of squared errors\")) the best. The line is modelled based on the linear equation shown below.\n\n$$y = a_0 + a_1 * x$$\n\n<b> Let's estimate the model coefficients for the advertising data. <\/b>","48a56743":"In Elastic Net Regression, the cost function is altered by adding the penalties of both ridge and lasso regression.\n\n$$\\sum_{i = 1}^{n}(y_i - \\hat{y_i})^2  + \\lambda_1 \\sum_{j = 0}^{p}||w_j|| + \\lambda_2 \\sum_{j = 0}^{p}(w_j)^2$$","ef2c457f":"### Elastic Net Regression:","d000ebf6":"The above pair plot shows no significant relationship between the features.","350400a5":"# <u> Linear Regression <\/u>\n\nLinear Regression is a machine learning algorithm which performs regression to compute the regression coefficients.\n\n### <u> Regression <\/u> \nRegression is a statistical measurement that attempts to determine the strength of the relationship between one dependent variable (Y) and a series of independent variables (X). Regression is also used to understand which among the independent variables (X) are related to the dependent variable (Y), and to explore the forms of these relationships.\n\n$$y = a_0 + a_1 x_1 + a_2 x_2 + ... + a_n x_n$$\n\nLinear regression looks for optimizing the intersept and coefficients such that it minimizes the error (difference between the predicted value and the observed value) for every instance.\n\nThe error terms can be calculated as:\n\n- Sum of residuals $\\sum_{i = 1}^{n} (Y - \\hat{Y})$\n     - It might result in cancelling out of positive and negative errors.\n- Sum of the absolute value of residuals $\\sum_{i = 1}^{n} |Y - \\hat{Y}|$\n     - Absolute value would prevent cancellation of errors.\n     - More robust to outliers. \n     - Not easily differentiable.\n- Sum of square of residuals $\\sum_{i = 1}^{n} (Y - \\hat{Y})^2$\n     - Mostly used in practice.\n     - Heavily penalize higher error value.\n     - Easily differentiable.\n     - Not robust to outliers.\n\n<b> Importance of Intercept term <\/b>: The intercept is the expected mean value of Y when all X = 0. A regression without a constant means that the regression line should goes through the origin.\n### <u> Cost Function <\/u> \n\nCost Function is used to define and measure the error of the model.\n\n- Cost function for MSE: $J_{\\theta} = \\frac{1}{n} \\sum_{i = 1}^{n}(Y - \\hat{Y})^2$\n\nSo, in order to improve our prediction, we need to minimize the cost function. For this purpose we use the gradient descent algorithm.\n\n### <u> Gradient Descent <\/u>\n\nGradient Descent iteratively updates intercept and slope values(gradients) to find a point where the cost function would be minimum. It requires calculation of gradient by differentiation of cost function.","aab38950":"## <u> Linear Regression Assumption # 4 <\/u>\n### Normal distribution of error terms:\n\nError or residuals ($\\hat{y} - y$) should follow a normal distribution.\n\nNormal distribution of the residuals can be validated by plotting a q-q plot.\n\nUsing the q-q plot we can infer if the data comes from a normal distribution. If yes, the plot would show fairly straight line. Absence of normality in the errors can be seen with deviation in the straight line.","3ab78d50":"### Model Evaluation Using Train\/Test Split\n\nLet's use train\/test split with RMSE to see whether Newspaper should be kept in the model:","f41a2b29":"### Feature Selection\n\n- Keep features in the model if they have small p-values, check corresponding $\\text{Adjusted }R^2$ value.","6f80f7e0":"### Laaso Regression:\n\nIn lasso regression, the cost function is altered by adding a penalty equivalent to magnitude of the coefficients.\n\n$$\\sum_{i = 1}^{n}(y_i - \\hat{y_i})^2 + \\lambda \\sum_{j = 0}^{p}||w_j||$$\n\nThis is equivalent to saying minimizing the cost function $\\sum_{i = 1}^{n}(y_i - \\hat{y_i})^2$ under the condition:\n\n$$\\text{For some c > 0, }\\sum_{j = 0}^{p}||w_j|| \\text{ < c}$$\n\nL1 regularization can lead to zero coefficients i.e. some of the features are completely neglected for the evaluation of output. So <b> Lasso regression not only helps in reducing over-fitting but it can help us in feature selection. <\/b>\n\nRidge regression can be used when the number of predictor variables exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables)."}}