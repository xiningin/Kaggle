{"cell_type":{"74e8accb":"code","57e9b53f":"code","50d5f2c5":"code","c5119e83":"code","4a9deea6":"code","2637d5c1":"code","29105385":"code","a2d400d3":"code","0ace1417":"code","751e5dc3":"code","c042c415":"code","9e5b44b5":"code","1beb816a":"code","4902e7ac":"code","b73bcd0c":"code","1835194b":"code","ddeefc56":"code","f11a25df":"code","e6f8ba3f":"code","b1b62443":"code","e059cfbe":"code","e5fa1d10":"code","3ef109d7":"code","1ae5b0b6":"code","b910b459":"code","cc5eb688":"code","9917f464":"code","14cb89a4":"code","f543e5bb":"code","7e5bac87":"code","da43424f":"code","2694efd3":"code","34d94e7e":"code","522303a8":"code","7a60c59a":"code","1e58126b":"code","b6531627":"code","213f5210":"code","f8c8f5ae":"code","6e34d1b8":"code","9cc67d7b":"code","ff2bff37":"code","3b18f0bd":"code","8e24d5e0":"code","5f3ca863":"code","b7e27d17":"code","27beb4f2":"code","a06d54ac":"code","aa4ed2be":"code","7680564e":"code","2b620ed7":"code","601d45e6":"code","c2e57895":"code","7b719a24":"code","9de182ec":"code","e1c2dee3":"code","57d86772":"code","654e77ec":"code","bc0ab252":"code","6424e67d":"code","96ddc72a":"code","ea597446":"code","9c2848f9":"code","bb7ea875":"code","f1eb5eaf":"code","a8f20d14":"code","87c6469d":"code","d8dde424":"code","bd4591c5":"code","29eda2f8":"code","dfc93ee1":"code","30d9e177":"code","65fecb17":"code","103cc3a1":"code","593c029b":"code","6670f5a4":"code","0f4f890c":"code","5d6d8d86":"code","640b8dce":"code","72376a4e":"code","79da3866":"code","488b4bbb":"code","4b36c122":"code","a7228f4c":"code","6a5295ba":"code","263c4b2f":"code","3ab1a084":"code","ff42b913":"code","abbbbf98":"code","25d36ec7":"code","6c986b74":"code","1f9c17b7":"code","37a5e70e":"code","347dceab":"code","fbb56e60":"code","cc12b59d":"code","0bb15dbe":"code","9f209cc5":"code","fc4a0ebe":"code","e2d41652":"code","03d57f24":"code","4594a0d1":"code","659b0be5":"code","4cd6b1d4":"code","77d3beec":"code","51242dd7":"code","bff79963":"code","63ac83dd":"code","686c0178":"code","9f9948b4":"code","8b31d53d":"code","b09d0cc0":"code","50e5d53d":"code","724ff2c5":"code","540bf0bc":"code","b74ba127":"code","966184c1":"code","6d21945a":"code","da5dadf7":"code","d4097b37":"code","b9a67c9e":"code","52e79d87":"code","7f73a9b1":"code","47e0f049":"code","70b4611c":"code","7da8fc9a":"code","5add26ec":"code","c9e3da97":"code","c4b458f8":"code","4d28d8ce":"code","8e56711e":"code","632c4fa5":"code","8c9c8a47":"code","0a369755":"code","5c7ad441":"code","10c9ba5c":"code","551abf1e":"code","38b3bd7a":"code","7b509da5":"code","3bca5eff":"code","ad4250fa":"code","278d189d":"code","aafa0ee5":"code","7011ae8b":"code","4fcf8dd6":"code","969321ee":"code","1d38c198":"code","6b5b2bf8":"code","a331fdcf":"code","9e56d4d5":"code","3779ebca":"code","bc69f5d4":"code","1c9dc377":"code","2d999d4e":"code","d4a68821":"code","20f72c78":"code","6196a489":"code","370ab232":"code","df4e3d27":"code","a6841c9f":"code","ec46c468":"code","32aed794":"code","34c347bc":"code","6e3ad86b":"code","45ba6073":"code","88b96ef0":"code","75099922":"code","2309f499":"code","e331920a":"code","c37badd7":"code","96790850":"code","84a25619":"code","b495c5e4":"code","21757961":"code","3e3a4cd9":"code","6635b652":"code","59831693":"code","09daa17f":"code","83b2bfee":"code","344a697b":"code","de4747b1":"code","08bbf8fd":"code","c40aaf87":"code","b2efa4c8":"code","c985fcf4":"code","77540d86":"code","45e7c89f":"code","0f750fc2":"code","c0200038":"code","5a33caae":"code","844ad098":"code","5c5c1064":"code","cb5f6e78":"code","064c731c":"code","6bb81561":"code","8aa122c4":"code","68315369":"code","673ec38d":"code","d8a6679a":"code","dc80f907":"code","83138324":"code","40bd5e95":"code","985fcb01":"code","eec55d8f":"code","b566b4b4":"code","69d6573e":"code","00348277":"code","493ddd94":"code","40d5bfa4":"code","3c35aaf8":"code","78e96a58":"code","d4a719b2":"markdown","738887a3":"markdown","3048a7fc":"markdown","c3a01707":"markdown","6ca8de8a":"markdown","ec6ca98c":"markdown","e5c51c8a":"markdown","fb3f2093":"markdown","e177cb31":"markdown","07ac22f9":"markdown","43279ee0":"markdown","88738b33":"markdown","ab76fb4e":"markdown","a2f7147c":"markdown","d5fe41fb":"markdown","abb55851":"markdown","a1b60685":"markdown","a3346049":"markdown","971a62d7":"markdown","f3da3615":"markdown","3ab380c0":"markdown","944e814a":"markdown","2cdef456":"markdown","2c4ba6e5":"markdown","b73c946d":"markdown","1324e062":"markdown","a4ad1858":"markdown","4f6b89b0":"markdown","1cfe073c":"markdown","e274546e":"markdown","0ac87c4d":"markdown","327b9e11":"markdown","bf4ec772":"markdown","f3dba395":"markdown","dcbb52d1":"markdown","653ee469":"markdown","3a7199fc":"markdown","3bd23c96":"markdown","46e1b342":"markdown","d6c62cb9":"markdown","64b63642":"markdown","dccb470e":"markdown","541f5106":"markdown","7fe55dae":"markdown","e61db4d3":"markdown","c160137c":"markdown","ab1a2620":"markdown","afcc5324":"markdown"},"source":{"74e8accb":"# Gerekli k\u00fct\u00fcphaneler\n# Necessary Libraries\n\nimport pandas as pd \nimport seaborn as sns\nimport tensorflow as tf                       \nimport numpy as np                           \nimport matplotlib.pyplot as plt               \n%matplotlib inline","57e9b53f":"from tensorflow.keras.datasets.mnist import load_data    # To load the MNIST digit dataset (handwriting numbers)\n                                                         # MNIST rakamlar datasetinin y\u00fckleyelim (el yaz\u0131s\u0131 numaralar)\n    \n(X_train, y_train) , (X_test, y_test) = load_data()      # Loading data (tuple unpacking ile ay\u0131r\u0131rz)\n                                                         # Verileri tuple olarak a\u00e7arak y\u00fckleriz.","50d5f2c5":"print(\"Toplamda \", len(X_train), \"adet e\u011fitim veri seti mevcuttur.\")     \nprint(\"Toplamda \", len(X_test), \"adet test veri seti mevcuttur.\",\"\\n\")\n\nprint(\"There are \", len(X_train), \"images in the training dataset\")     \nprint(\"There are \", len(X_test), \"images in the test dataset\")   ","c5119e83":"# Checking the shape of one image\n# G\u00f6r\u00fcnt\u00fcn\u00fcn \u015feklini kontrol etme\n\nX_train[5].shape","4a9deea6":"X_train[5]","2637d5c1":"# Viewing the image\n# Resmi g\u00f6r\u00fcnt\u00fcleme\n\nplt.imshow(X_train[5]);\n# plt.matshow(X_train[5])","29105385":"plt.matshow(X_train[3]);","a2d400d3":"y_train[3]","0ace1417":"# Data setindeki resimlerin for d\u00fcng\u00fcs\u00fc ile sample al\u0131narak g\u00f6sterimi. --> (c * 2) + r ya da bu denklem de olur (r * 5 + c)\n# Representation of the pictures in the data set by sampling with the for loop. --> (c * 2) + r or you can use (r * 5 + c)\n\nrows, columns = 2, 5\nfig, ax = plt.subplots(rows, columns, gridspec_kw={'hspace':0.3}, figsize=(12,5), squeeze=True)\n\nfor r in range(rows):\n    for c in range(columns):   \n        image_index = (c * 2) + r # random not in exact rule\n        ax[r,c].axis(\"off\")\n        ax[r,c].matshow(X_train[image_index], cmap='Blues')\n        ax[r,c].set_title('No. %d' % y_train[image_index])\nplt.show()\nplt.close()","751e5dc3":"# En d\u00fc\u015f\u00fck pixel de\u011feri [0-255]\n# min pixel value\n\nX_train[5].min()","c042c415":"# En y\u00fcksek pixel de\u011feri [0-255]\n# max pixel value\n\nX_train[5].max()","9e5b44b5":"# \u0130leride i\u015flem yapabilmek i\u00e7in float'a \u00e7evirelim. Ve scale edelim. 255 --> 1 ve 0 --> 0\n# Let's convert it to float for further processing. And scale them. \n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train \/= 255\nX_test \/= 255","1beb816a":"X_train[5].min()","4902e7ac":"X_train[5].max()","b73bcd0c":"plt.imshow(X_train[5])\nplt.axis(\"off\");","1835194b":"X_train.shape, X_test.shape","ddeefc56":"# Reshape: G\u00f6r\u00fcnt\u00fcm\u00fcz\u00fcn boyut ayarlar\u0131n\u0131 de\u011fi\u015ftirir. Yukar\u0131da orjinal boyutlar\u0131 bak\u0131n ve a\u015fa\u011f\u0131da i\u015flem sonucu ne hale geldi\u011fini g\u00f6r\u00fcn. \n# Reshape: Changes the sizes (width,length etc.) of our image. See the original dimensions above and see what happens as a result of the process below.\n\nX_train = X_train.reshape(X_train.shape[0],28, 28, 1) # --> [0] sat\u0131ra kar\u015f\u0131l\u0131k gelir. 1 --> siyah beyaz. 3 --> RGB renkler\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)   # --> [0] corresponds to the row. 1 --> black and white. 3 --> RGB colors","f11a25df":"X_train.shape, X_test.shape","e6f8ba3f":"len(X_train)","b1b62443":"len(y_train)","e059cfbe":"y_train # 0 dan 9 a kadar olan rakamlar. Bu rakamlar\u0131 kategorik yani class a d\u00f6n\u00fc\u015ft\u00fcrmem laz\u0131m ve bunu \"keras\" yapar.\n        # Numbers 0 to 9, I need to convert them to categorical aka. class. And \"keras\" does it.","e5fa1d10":"from tensorflow.keras.utils import to_categorical","3ef109d7":"Y_train = to_categorical(y_train, 10)  # --> y_traini al\u0131r ve kategorik hale getirir. Class say\u0131s\u0131n\u0131 10 olarak ayarlad\u0131m.\nY_test = to_categorical(y_test, 10)    # --> takes y_test like above and makes it categorical. Number of classes is 10.\nY_test.shape","1ae5b0b6":"from tensorflow.keras.models import Sequential  # Sequential model.\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n\n# 1 --> Model\n# 2 --> conv. layer, \n# 3 --> maxpool, \n# 4 --> padding\n# 5 --> flatten layer\n# 6 --> Hidden layer and it's activation function\n# 7 --> Output layer and it's activation function \/ dense layer","b910b459":"# 1) Model se\u00e7imi \/ Model selection\nmodel = Sequential()\n\n# 2) FILTER - KERNEL\n# 32 times --> 3X3 kernel(filter), activation --> relu ve input --> 1X28X28\nmodel.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=(28, 28, 1), activation='relu',))  \n\n# 3) POOLING\n# 28 ve 32 lik boyutlar i\u00e7in 2x2 lik pooling yeterli. (For sizes 28 and 32, 2x2 pooling is fairly enough.) \nmodel.add(MaxPool2D(pool_size=(2, 2)))  \n\n# 4) PADDING\n# Padding kurmad\u0131k \u00e7\u00fcnk\u00fc k\u00f6\u015felerde veri yok, olsayd\u0131 parametreyi same ayarlar\u0131k. (No Padding --> no data at the edges) \n# stride --> 2 (default)\n\n# 5) FLATTEN \n# matris --> vekt\u00f6re d\u00f6n\u00fc\u015f\u00fcm\u00fc \/ matris --> vector transformation\nmodel.add(Flatten())\n\n# 6) Activation Function\n# n\u00f6ron say\u0131s\u0131(quantity) --> 128  ve activation --> relu\nmodel.add(Dense(128, activation='relu'))\n\n# 7) ANN deki gibi 2. hidden layer iste\u011fe ba\u011fl\u0131 kuruldu.\n# As in ANN, the 2nd hidden layer was optionally setted.\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',  # OPTIMIZER !!!\n              metrics=['accuracy'])","cc5eb688":"# Yukar\u0131da kurmu\u015f oldu\u011fumuz Deep Learning modelinin \u00f6zeti. (Padding yok, \u00e7\u00fcnk\u00fc resimlerdeki veriler resimlerin merkezinde)\n# Summary of the Deep Learning model we have established above. (No padding, because the data in the images is in the center of the images)\n\nmodel.summary()","9917f464":"from tensorflow.keras.callbacks import EarlyStopping","14cb89a4":"# D\u00fc\u015f\u00fc\u015f\u00fcn\u00fc yani \u00f6\u011frenmesini takip etmemiz gereken de\u011feri monitor de\u011fi\u015fkenine atar\u0131z\n# We assign the value we need to follow to the variable \"monitor\".\n\nearly_stop = EarlyStopping(monitor='val_loss',patience=2)","f543e5bb":"model.fit(X_train, Y_train, batch_size=32, epochs=10, validation_data=(X_test,Y_test),callbacks=[early_stop])","7e5bac87":"model.metrics_names","da43424f":"model.history.history","2694efd3":"summary = pd.DataFrame(model.history.history)\nsummary.head()","34d94e7e":"plt.figure(figsize=(10,6))\nplt.plot(summary.loss)\nplt.plot(summary.val_loss)\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.gca().legend(('loss',\"val_loss\"));","522303a8":"plt.figure(figsize=(10,6))\nplt.plot(summary.accuracy, label=\"accuracy\")\nplt.plot(summary.val_accuracy, label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\");","7a60c59a":"print(model.evaluate(X_test,Y_test,verbose=1))  # soldaki loss sa\u011fdaki accuracy \/ left one is loss and the other accuracy","1e58126b":"from sklearn.metrics import classification_report,confusion_matrix","b6531627":"preds = model.predict(X_test)\n# preds --> ihtimalleri al\u0131r\u0131z. Ama predict classes art\u0131k kald\u0131r\u0131ld\u0131. Bu y\u00fczden art\u0131k a\u015fa\u011f\u0131daki y\u00f6ntem uygulan\u0131r.\n# # preds --> we get probabilities. But predict classes are now removed. So now the following method is applied.","213f5210":"preds # Kullan\u0131\u015fs\u0131z \/ Useless","f8c8f5ae":"# Do\u011fru olan yol \/ Correct Way\npredictions= np.argmax(preds, axis=1)","6e34d1b8":"predictions","9cc67d7b":"print(classification_report(y_test, predictions))","ff2bff37":"print(confusion_matrix(y_test, predictions))","3b18f0bd":"score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","8e24d5e0":"score  # index 0 --> loss \/ index 1 --> accuracy","5f3ca863":"# Boyutlara g\u00f6re reshape edilmi\u015f \u00f6rneklemin her bir pixel'inin renk kodlar\u0131 ile g\u00f6sterimi.\n# Color codes of each pixel of the reshaped sample according to dimensions.\n\nX_test[20].reshape(28,28)","b7e27d17":"my_number = X_test[20]","27beb4f2":"plt.imshow(my_number.reshape(28,28));","a06d54ac":"plt.matshow(my_number.reshape(28,28));","aa4ed2be":"# Modele sokabilmek i\u00e7in resim say\u0131s\u0131 eklenmesi ve resmin siyah beyaz olarak de\u011ferlendirilmesi.\n# Adding the number of pictures and evaluating the picture as black and white in order to be able to put it into the model.\n\nmodel.predict(my_number.reshape(1,28,28,1))","7680564e":"# SHAPE --> (num_images,width,height,color_channels)\nnp.argmax(model.predict(my_number.reshape(1,28,28,1)), axis=1)","2b620ed7":"# Modeli daha sonra haz\u0131r halde kullanabilmek i\u00e7in kaydedelim ve istersek denemelerimizi haz\u0131r model \u00fczerinden yapal\u0131m.\n# Let's save the model so that we can use it later. And if we want, we can make our predictions on the ready model.\n\nmodel.save(\"cnn-1.h5\")","601d45e6":"import pandas as pd\nimport seaborn as sns\nimport tensorflow as tf                       \nimport numpy as np                           \nimport matplotlib.pyplot as plt               \n%matplotlib inline","c2e57895":"from tensorflow.keras.datasets import cifar10\n\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()","7b719a24":"print(\"Toplamda \", len(X_train), \"adet e\u011fitim veri seti mevcuttur.\")     \nprint(\"Toplamda \", len(X_test), \"adet test veri seti mevcuttur.\",\"\\n\")\n\nprint(\"There are \", len(X_train), \"images in the training dataset\")     \nprint(\"There are \", len(X_test), \"images in the test dataset\")   ","9de182ec":"# Resimlerin boyutlar\u0131n\u0131 kontrol edelim\n# Checking the shape of one image\n\nX_train[5].shape","e1c2dee3":"X_train[5]","57d86772":"plt.imshow(X_train[5])\nplt.axis('off');\n\n# plt.matshow(X_train[5]);","654e77ec":"y_train[5]  # index 1 --> autonobile","bc0ab252":"plt.matshow(X_train[28])\nplt.axis('off');","6424e67d":"y_train[28]  # index 4 --> deer","96ddc72a":"classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]","ea597446":"# Datasetinden for d\u00fcng\u00fcs\u00fc ile 15x15=225 adet resim. (Data i\u00e7ini g\u00f6rme ama\u00e7l\u0131)\n# # 15x15=225 images from the Dataset with the for loop. (For seeing inside data)\n\nplt.figure(figsize= (16,16))\nfor i in range(225):\n  plt.subplot(15,15,i+1)\n  plt.axis('off')\n  plt.imshow(X_train[i])","9c2848f9":"classes","bb7ea875":"X_train[5].min()","f1eb5eaf":"X_train[5].max()","a8f20d14":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train \/= 255\nX_test \/= 255","87c6469d":"X_train[5].min()","d8dde424":"X_train[5].max()","bd4591c5":"plt.imshow(X_train[5])\nplt.axis('off');","29eda2f8":"plt.imshow(X_train[28])\nplt.axis('off');","dfc93ee1":"X_train.shape, X_test.shape","30d9e177":"y_train.shape, y_test.shape","65fecb17":"from tensorflow.keras.utils import to_categorical","103cc3a1":"Y_train = to_categorical(y_train, 10) \nY_test = to_categorical(y_test, 10)\nY_test.shape","593c029b":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout","6670f5a4":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=32, kernel_size=(4,4),input_shape=(32, 32, 3), activation='relu',))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","0f4f890c":"model.summary()","5d6d8d86":"from tensorflow.keras.callbacks import EarlyStopping","640b8dce":"early_stop = EarlyStopping(monitor='val_loss',patience=5)","72376a4e":"model.fit(X_train, Y_train, batch_size=32, epochs=20, validation_data=(X_test,Y_test),callbacks=[early_stop])","79da3866":"model.metrics_names","488b4bbb":"model.history.history","4b36c122":"summary = pd.DataFrame(model.history.history)\nsummary.head()","a7228f4c":"plt.figure(figsize=(10,6))\nplt.plot(summary.loss, label=\"loss\")\nplt.plot(summary.val_loss, label=\"val_loss\")\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.show();","6a5295ba":"plt.figure(figsize=(10,6))\nplt.plot(summary.accuracy, label=\"accuracy\")\nplt.plot(summary.val_accuracy, label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.show()","263c4b2f":"print(model.evaluate(X_test,Y_test,verbose=1))","3ab1a084":"from sklearn.metrics import classification_report,confusion_matrix, plot_confusion_matrix","ff42b913":"preds = model.predict(X_test)\n","abbbbf98":"predictions= np.argmax(preds, axis=1)","25d36ec7":"predictions","6c986b74":"print(classification_report(y_test, predictions))","1f9c17b7":"print(confusion_matrix(y_test, predictions))","37a5e70e":"score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","347dceab":"my_image = X_test[17]","fbb56e60":"plt.imshow(my_image)\nplt.axis(\"off\");","cc12b59d":"image_prediction=model.predict(my_image.reshape(1,32,32,3))","0bb15dbe":"# SHAPE --> (num_images,width,height,color_channels)\nnp.argmax(image_prediction, axis=1)","9f209cc5":"result=np.argmax(image_prediction, axis=1)\nclasses[result[0]]","fc4a0ebe":"model.save(\"cnn-2.h5\")","e2d41652":"early_stop = EarlyStopping(monitor='val_loss',patience=15)","03d57f24":"model= Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(32,32,3)))\nmodel.add(MaxPool2D((2,2)))\n        \nmodel.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\nmodel.add(MaxPool2D((2,2)))\nmodel.add(Dropout(0.3))\n    \nmodel.add(Conv2D(filters=128,kernel_size=(3,3),activation='relu'))\nmodel.add(MaxPool2D((2,2)))\nmodel.add(Dropout(0.3))\n        \nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","4594a0d1":"model.summary()","659b0be5":"model.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_test,Y_test), callbacks=[early_stop])","4cd6b1d4":"score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","77d3beec":"preds = model.predict(X_test)\npredictions= np.argmax(preds, axis=1)","51242dd7":"print(classification_report(y_test, predictions))","bff79963":"print(confusion_matrix(y_test, predictions))","63ac83dd":"plt.figure(figsize=(10,6))\nsns.heatmap(confusion_matrix(y_test, predictions), annot=True);","686c0178":"from tensorflow.keras.layers import BatchNormalization, MaxPooling2D","9f9948b4":"early_stop = EarlyStopping(monitor='val_loss',patience=5)","8b31d53d":"model = Sequential()\n\nmodel.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(128, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(256, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, (3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))    # num_classes = 10\n\n# Checking the model summary\nmodel.summary()","b09d0cc0":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","50e5d53d":"model.fit(X_train, Y_train, batch_size=32, epochs=20, validation_data=(X_test,Y_test), callbacks=[early_stop])","724ff2c5":"score = model.evaluate(X_test, Y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","540bf0bc":"preds = model.predict(X_test)\npredictions= np.argmax(preds, axis=1)","b74ba127":"print(classification_report(y_test, predictions))","966184c1":"model.save(\"cnn-3.h5\")","6d21945a":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport warnings\nwarnings.filterwarnings('ignore')","da5dadf7":"pwd  # Mevcut dosya lokasyonumuz \/ Our current location","d4097b37":"# \u00d6ncelikle ham verileri (786M ZIP ar\u015fivi)'ni indirelim:\n# First, let's download the 786M ZIP archive of the raw data:\n!curl -O https:\/\/download.microsoft.com\/download\/3\/E\/1\/3E1C3F21-ECDB-4869-8368-6DEBA77B919F\/kagglecatsanddogs_3367a.zip","b9a67c9e":"# \u015eimdi indirdi\u011fimiz dosyay\u0131 zip'ten \u00e7\u0131karal\u0131m. Ve PetImages klas\u00f6r\u00fcne eri\u015felim.\n# Now let's unzip the downloaded file. And let's access the PetImages folder.","52e79d87":"import zipfile\nwith zipfile.ZipFile(\".\/kagglecatsanddogs_3367a.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\".\/\")","7f73a9b1":"# Gereksiz dosyalar\u0131 silelim\n# Let's delete unnecessary folders","47e0f049":"import os\nos.remove('.\/kagglecatsanddogs_3367a.zip')","70b4611c":"os.remove('.\/MSR-LA - 3467.docx')","7da8fc9a":"os.remove('.\/readme[1].txt')","5add26ec":"# g\u00fcncel klas\u00f6r\u00fcm\n# updated current folder\nos.listdir('.\/')","c9e3da97":"my_data_dir = '.\/PetImages\/'","c4b458f8":"os.listdir(my_data_dir)","4d28d8ce":"num_skipped = 0\nfor folder_name in (\"Cat\", \"Dog\"):\n    folder_path = os.path.join(\"PetImages\", folder_name)\n    for fname in os.listdir(folder_path):\n        fpath = os.path.join(folder_path, fname)\n        try:\n            fobj = open(fpath, \"rb\")\n            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n        finally:\n            fobj.close()\n\n        if not is_jfif:\n            num_skipped += 1\n            # Delete corrupted image\n            os.remove(fpath)\n\nprint(\"Deleted %d images\" % num_skipped)","8e56711e":"len(os.listdir(my_data_dir + '\/Cat'))","632c4fa5":"len(os.listdir(my_data_dir + '\/Dog'))","8c9c8a47":"# ImageDataGenerator'\u00fc kullanal\u0131m.\n# Let's use ImageDataGenerator.\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","0a369755":"# help(ImageDataGenerator)","5c7ad441":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","10c9ba5c":"image_gen = ImageDataGenerator(\n              rotation_range=20,       # Resmi 20 derece d\u00f6nd\u00fcr\u00fcr \/ rotate the image 20 degrees\n              width_shift_range=0.10,  # Resim eninin maksimum %5 kayd\u0131r\u0131r \/ Shift the pic width by a max of 5%\n              height_shift_range=0.10, # Resim boyunu maksimum %5 kayd\u0131r\u0131r \/ Shift the pic height by a max of 5%\n              rescale=1\/255,           # Resmi normalle\u015ftirerek yeniden \u00f6l\u00e7eklendirir \/ Rescale the image by normalzing it.\n              shear_range=0.1,         # G\u00f6r\u00fcnt\u00fcn\u00fcn bir k\u0131sm\u0131n\u0131 keser max 0.1 \/ Shear means cutting away part of the image (max 10%)\n              zoom_range=0.1,          # Maksimum %10 yak\u0131nla\u015ft\u0131r \/ Zoom in by 10% max\n              horizontal_flip=True,    # Yatay \u00e7evirir \/ Allow horizontal flipping\n              fill_mode='nearest'      # Eksik pikselleri en yak\u0131ndaki ile doldurur \/ Fill in missing pixels with the nearest filled value\n              )","551abf1e":"pwd","38b3bd7a":"my_data_dir","7b509da5":"os.listdir(my_data_dir)","3bca5eff":"os.listdir(my_data_dir + 'Cat')[0]","ad4250fa":"os.listdir(my_data_dir + 'Dog')[0]","278d189d":"cat_image_path = my_data_dir + '\/Cat' + '\/0.jpg'\ndog_image_path = my_data_dir + '\/Dog' + '\/0.jpg'","aafa0ee5":"cat_img=imread(cat_image_path)\ndog_img=imread(dog_image_path)","7011ae8b":"plt.imshow(cat_img)\nplt.axis('off');","4fcf8dd6":"plt.imshow(dog_img)\nplt.axis('off');","969321ee":"plt.imshow(image_gen.random_transform(cat_img));","1d38c198":"plt.imshow(image_gen.random_transform(dog_img));","6b5b2bf8":"import tensorflow_datasets as tfds\n(train_ds, val_ds, test_ds), metadata = tfds.load(\n    'CatsVsDogs',\n    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n    with_info=True,\n    as_supervised=True,\n)","a331fdcf":"# labels  --> cats and dogs --> 2\nnum_classes = metadata.features['label'].num_classes\nprint(num_classes)","9e56d4d5":"get_label_name = metadata.features['label'].int2str\n\nimage, label = next(iter(train_ds))\n_ = plt.imshow(image)\n_ = plt.title(get_label_name(label))","3779ebca":"from tensorflow import keras\nfrom tensorflow.keras import layers","bc69f5d4":"data_augmentation = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\"),\n  layers.RandomRotation(0.2),\n])","1c9dc377":"image = tf.expand_dims(image, 0)","2d999d4e":"plt.figure(figsize=(10, 10))\nfor i in range(9):\n    augmented_image = data_augmentation(image)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_image[0])\n    plt.axis(\"off\")","d4a68821":"!pip install split-folders","20f72c78":"import splitfolders","6196a489":"os.makedirs('.\/PetImages2')","370ab232":"pwd","df4e3d27":"input_folder = '.\/PetImages\/'\noutput_folder = '.\/PetImages2\/'","a6841c9f":"# SpLit with a ratio.\n# To onLy spLit into training and vaLidation set, set a tupLe to \u2018ratio\u2018, i.e,\n# Train, vaL, test\n# If you don't want to use ratio, I mean if you want to split with fixed number of images:\n# use \"fixed\" instead of \"ratio\"\nsplitfolders.ratio(input_folder, output= output_folder, seed = 42, ratio = (.7, .2, .1), group_prefix = None)","ec46c468":"# PetImages klas\u00f6r\u00fcn\u00fc silebiliriz\n# we can delete PetImages folder\nimport shutil\nshutil.rmtree('.\/PetImages')","32aed794":"my_data_dir = '.\/PetImages2'\nmy_data_dir","34c347bc":"os.listdir(my_data_dir)","6e3ad86b":"val_path = my_data_dir + '\/val\/'\ntrain_path = my_data_dir + '\/train\/'\ntest_path = my_data_dir + '\/test\/'","45ba6073":"os.listdir(train_path)","88b96ef0":"# Train\nlen(os.listdir(train_path+'\/Cat')), len(os.listdir(train_path+'\/Dog'))","75099922":"# Test\nlen(os.listdir(test_path+'\/Cat')), len(os.listdir(test_path+'\/Dog'))","2309f499":"# Val\nlen(os.listdir(val_path+'\/Cat')), len(os.listdir(val_path+'\/Dog'))","e331920a":"# image generator'\u00fc hat\u0131rlayal\u0131m\n# let's remember image generator\nimage_gen","c37badd7":"image_gen.flow_from_directory(train_path)","96790850":"image_gen.flow_from_directory(val_path)","84a25619":"image_gen.flow_from_directory(test_path)","b495c5e4":"image_shape = (128,128,3)","21757961":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D","3e3a4cd9":"\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Flatten())\n\n\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\n\n\nmodel.add(Dropout(0.5))\n\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","6635b652":"model.summary()","59831693":"from tensorflow.keras.callbacks import EarlyStopping","09daa17f":"early_stop = EarlyStopping(monitor='val_loss',patience=2)","83b2bfee":"batch_size = 16","344a697b":"train_image_gen = image_gen.flow_from_directory(train_path,\n                                               target_size=image_shape[:2],\n                                                color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary')","de4747b1":"val_image_gen = image_gen.flow_from_directory(val_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',shuffle=False)","08bbf8fd":"test_image_gen = image_gen.flow_from_directory(test_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',shuffle=False)","c40aaf87":"train_image_gen.class_indices","b2efa4c8":"test_image_gen.class_indices","c985fcf4":"import warnings\nwarnings.filterwarnings('ignore')","77540d86":"results = model.fit_generator(train_image_gen,epochs=10,\n                              validation_data=val_image_gen,\n                              callbacks=[early_stop])","45e7c89f":"summary = pd.DataFrame(model.history.history)\nsummary.head()","0f750fc2":"plt.figure(figsize=(10,6))\nplt.plot(summary.loss, label=\"loss\")\nplt.plot(summary.val_loss, label=\"val_loss\")\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.show()","c0200038":"plt.figure(figsize=(10,6))\nplt.plot(summary.accuracy, label=\"accuracy\")\nplt.plot(summary.val_accuracy, label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.show()","5a33caae":"model.metrics_names","844ad098":"model.evaluate_generator(val_image_gen)","5c5c1064":"pred_probabilities = model.predict_generator(val_image_gen)","cb5f6e78":"pred_probabilities","064c731c":"val_image_gen.classes","6bb81561":"predictions = pred_probabilities > 0.5","8aa122c4":"predictions","68315369":"from sklearn.metrics import classification_report,confusion_matrix","673ec38d":"print(classification_report(val_image_gen.classes,predictions))","d8a6679a":"confusion_matrix(val_image_gen.classes,predictions)","dc80f907":"from tensorflow.keras.models import load_model\nmodel.save('cat_dog_detector.h5')","83138324":"# PetImages klas\u00f6r\u00fcn\u00fc silebiliriz\n# we can delete PetImages folder\nimport shutil\nshutil.rmtree('.\/PetImages2')","40bd5e95":"# Modelimin do\u011fru \u00e7al\u0131\u015f\u0131p \u00e7al\u0131\u015fmad\u0131\u011f\u0131n\u0131 \u00f6\u011frenmek i\u00e7in modeli kullanabilir ve resminizin kedi mi k\u00f6pek mi oldu\u011funu tahmin edebilirsiniz.\n# To find out if my model is working correctly, you can use the model and guess whether your picture is a cat or a dog.","985fcb01":"from tensorflow.keras.preprocessing import image","eec55d8f":"model=load_model('cat_dog_detector.h5')","b566b4b4":"from PIL import Image","69d6573e":"path_pred = '..\/input\/image-to-predict-cnn'","00348277":"img=image.load_img(path_pred+'\/newimage1.jpg')\nimg","493ddd94":"resized_img = img.resize((128, 128))\nresized_img","40d5bfa4":"resized_img1=np.array(resized_img)\nresized_img1.shape","3c35aaf8":"resized_img1=np.expand_dims(resized_img1, axis=0)\nresized_img1.shape","78e96a58":"# BRAVO!!!!\nmodel.predict(resized_img1)  # --> class 1 is dog","d4a719b2":"**data_augmentation veya ImageDataGenerator'\u0131 Kullanal\u0131m**<br>\nB\u00fcy\u00fck bir g\u00f6r\u00fcnt\u00fc veri k\u00fcmeniz olmad\u0131\u011f\u0131nda, e\u011fitim g\u00f6r\u00fcnt\u00fclerine rastgele yatay \u00e7evirme veya k\u00fc\u00e7\u00fck rastgele d\u00f6nd\u00fcrmeler gibi rastgele ancak ger\u00e7ek\u00e7i d\u00f6n\u00fc\u015f\u00fcmler uygulayarak \u00f6rnek \u00e7e\u015fitlili\u011fini yapay olarak tan\u0131tmak iyi bir uygulamad\u0131r. Bu, overfitting'i azalat\u0131rken, modelin, e\u011fitim verilerinden generate edilmi\u015f farkl\u0131 resimlere maruz kalmas\u0131na yard\u0131mc\u0131 olur.<br>\n<span style=\"color:#1565C0; opacity:1; font-weight:bold\">Using image data_augmentation or ImageDataGenerator<\/span><br>\n<span style=\"color:#1565C0; opacity:1\">When you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting.<\/span><br>","738887a3":"**A sample example showing the conversion of 3D data to 2D**<br>\n![3Dto2D](https:\/\/dphi-courses.s3.ap-south-1.amazonaws.com\/Deep+Learning+Bootcamp\/3D+to++2D.png)","3048a7fc":"## Early Stopping","c3a01707":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">ImageDataGenerator<\/font><\/span> <a id=\"4.3.1\"><\/a>","6ca8de8a":"## Model Training","ec6ca98c":"Bu \u00f6rnek, \u00f6nceden e\u011fitilmi\u015f ya da \u00f6nceden yap\u0131lm\u0131\u015f bir Keras Uygulama modelinden yararlanmadan, lokaldeki JPEG g\u00f6r\u00fcnt\u00fc dosyalar\u0131ndan ba\u015flayarak s\u0131f\u0131rdan g\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131n\u0131n nas\u0131l yap\u0131ld\u0131\u011f\u0131n\u0131 g\u00f6sterir. Kaggle Cats vs Dogs ikili s\u0131n\u0131fland\u0131rma veri k\u00fcmesindeki i\u015fleyi\u015fi g\u00f6rselle\u015ftirece\u011fiz.<br>\nVeri k\u00fcmelerini olu\u015fturmak i\u00e7in image_dataset_from_directory yard\u0131mc\u0131 program\u0131n\u0131 kullan\u0131yoruz ve g\u00f6r\u00fcnt\u00fc standardizasyonu ve veri geli\u015ftirme(augmentation) i\u00e7in Keras g\u00f6r\u00fcnt\u00fc \u00f6n i\u015fleme katmanlar\u0131n\u0131 kullan\u0131yoruz.<br>\n\n<span style=\"color:#1565C0; opacity:1\">This example shows how to do image classification from scratch, starting from JPEG image files on disk, without leveraging pre-trained weights or a pre-made Keras Application model. We demonstrate the workflow on the Kaggle Cats vs Dogs binary classification dataset.<br>\nWe use the image_dataset_from_directory utility to generate the datasets, and we use Keras image preprocessing layers for image standardization and data augmentation.<\/span><br> ","e5c51c8a":"![image.png](attachment:32baab3d-82d9-40c8-aa17-f9da5144123d.png)","fb3f2093":"# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">Image Classification \/ Data Augmentation<\/font><\/span> <a id=\"4.3\"><\/a><br>\n**Dictionary of Data** --> [Kaggle](https:\/\/www.kaggle.com\/c\/dogs-vs-cats\/data)<br>\n\n\u00c7ok az veri kullanarak g\u00fc\u00e7l\u00fc g\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rma modelleri olu\u015fturmak i\u00e7in:<br>\n<span style=\"color:#1565C0; opacity:1\">To build powerful image classification models using very little data:<\/span><br>\n\n**Explanatory Video** --> [Youtube](https:\/\/www.youtube.com\/watch?v=hxLU32zhze0)<br>\n**Explanatory Blog** --> [Blog Keras](https:\/\/blog.keras.io\/building-powerful-image-classification-models-using-very-little-data.html)","e177cb31":"# <center><span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">PROJECTS<\/font><\/span><\/center> <a id=\"4\"><\/a>","07ac22f9":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Data_Augmentation<\/font><\/span> <a id=\"4.3.2\"><\/a>","43279ee0":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Veri Setlerini Olu\u015ftural\u0131m \/ Generating the DataSets <\/font><\/span> <a id=\"4.3.3\"><\/a>","88738b33":"# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">Sonu\u00e7 \/ Conclusion<\/font><\/span> <a id=\"5\"><\/a>","ab76fb4e":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Hadi Deneyelim \/ Let's Predict<\/font><\/span> <a id=\"4.1.5\"><\/a>","a2f7147c":"___","d5fe41fb":"![image.png](attachment:52f7b0ff-96f8-4ad0-94dc-80fea13e40fb.png)","abb55851":"<center><span style=\"color:#1565C0; opacity:1\"><font size=\"6\">Do\u011fru Tahmin Bravo!!! \/ Correct Prediction Congrats!!!<\/font><\/span><\/center><br>\n<img src=\"http:\/\/www.femoticons.net\/images\/posts\/cool_sunglasses_emoticon.jpg\" alt=\"acorrect_pred\">","a1b60685":"___","a3346049":"# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">CNN - MNIST<\/font><\/span> <a id=\"4.1\"><\/a>","971a62d7":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Confusion Matrix \/ Classification Report<\/font><\/span> <a id=\"4.1.4\"><\/a>\n\nMakine \u00f6\u011freniminde klasifikasyon, supervised(denetimli) \u00f6\u011frenmenin bir par\u00e7as\u0131d\u0131r ve bu; modeli e\u011fitmek i\u00e7in kullan\u0131lan verilerin her kategoriyi tan\u0131mlayan etiketlere(label) sahip oldu\u011fu anlam\u0131na gelir.<br>\nBir makine \u00f6\u011frenimi modelinin ya\u015fam d\u00f6ng\u00fcs\u00fcndeki kritik bir ad\u0131m, performans\u0131n\u0131n de\u011ferlendirilmesidir.<br>\nBir klasifikayson modelini de\u011ferlendirmek i\u00e7in kullan\u0131lan iki teknik, confusion matrisi ve klasifikasyon raporudur.<br>\nDaha fazlas\u0131 i\u00e7in [makale](https:\/\/medium.com\/swlh\/confusion-matrix-and-classification-report-88105288d48f) ve [video](https:\/\/www.youtube.com\/watch?v=O-eH80oi2P4).<br>\n<span style=\"color:#1565C0; opacity:1\">In machine learning, classification is part of supervised learning, which means that the data used to train the model have labels that identify each category.<br>\nA critical step in the life cycle of a machine learning model is the evaluation of its performance.<br>\nTwo techniques used to evaluate a classification model are the confusion matrix and the classification report.<br>\nTo learn more here is the <\/span>[article](https:\/\/medium.com\/swlh\/confusion-matrix-and-classification-report-88105288d48f) <span style=\"color:#1565C0; opacity:1\">and<\/span> [video](https:\/\/www.youtube.com\/watch?v=O-eH80oi2P4)<\/span>","f3da3615":"# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">Merhaba Millet!!!<\/font><\/span> <a id=\"0\"><\/a>\n\nBu projemde sizlere CNN'nin ne oldu\u011funu, ne i\u015fe yarad\u0131\u011f\u0131n\u0131 ve bir CNN derin \u00f6\u011frenme algoritmas\u0131n\u0131 a\u015fama a\u015fama nas\u0131l in\u015fa etti\u011fimizi anlatmaya \u00e7al\u0131\u015faca\u011f\u0131m. **Sak\u0131n panik yapmay\u0131n!** Bu notebookta ihtiyac\u0131n\u0131z olacak b\u00fct\u00fcn a\u00e7\u0131klamalar\u0131 bulacaks\u0131n\u0131z ve hatta CNN hakk\u0131nda elinizin alt\u0131nda bir kitap olarak bile bulundurabileceksiniz.<br>\n\nKaggle platformu teorik k\u0131s\u0131m i\u00e7in, notebookumda \u00e7ok fazla g\u00f6m\u00fcl\u00fc resim oldu\u011fundan dolay\u0131 kaydetmeye m\u00fcsaade etmiyor. O y\u00fczden teorik k\u0131s\u0131m i\u00e7in --> [l\u00fctfen buraya t\u0131klay\u0131n\u0131z.](https:\/\/github.com\/emir1031\/03_PROJECTS\/blob\/main\/05_CNN_Guide_From-Start-to-Finish_All-in-One\/CNN_Guide_From-Start-to-Finish_All-in-One.ipynb)<br>\n\nDilerseniz b\u00fct\u00fcn bir notebooku, bir \u00fcst sat\u0131rda linkini verdi\u011fim GitHub sayfamdan indirebilirsiniz.\n\n--> [L\u00fctfen GitHub Sayfama G\u00f6z At\u0131n](https:\/\/github.com\/emir1031)\n\n# <span style=\"color:#1565C0; background-color:#F5EEF8\"><font size=\"4\">Hi Everyone!<\/font><\/span> <a id=\"0\"><\/a>\n\n<span style=\"color:#1565C0; opacity:1\">In this project, I will try to explain to you what CNN is, what it does, and how we built a CNN deep learning algorithm step by step.<\/span> <span style=\"color:#1565C0; opacity:1; font-weight:bold\" >**Don't panic!** <\/span> <span style=\"color:#1565C0; opacity:1\">In this notebook you will find all the explanations you will need and you will even have a book about CNN.<br><\/span>\n\n<span style=\"color:#1565C0; opacity:1\">For the theoretical part, kaggle platform does not allow saving because there are too many embedded images in my notebook. So please<\/span> --> [click here for the theoretical part.](https:\/\/github.com\/emir1031\/03_PROJECTS\/blob\/main\/05_CNN_Guide_From-Start-to-Finish_All-in-One\/CNN_Guide_From-Start-to-Finish_All-in-One.ipynb)\n\n<span style=\"color:#1565C0; opacity:1\">If you wish, you can download a whole notebook from my GitHub page, which I have given the link above.<br><\/span> \n\n--> [Please Browse My Github Profile](https:\/\/github.com\/emir1031)","3ab380c0":"![image.png](attachment:cc376ccc-458b-441f-a808-0672fc3071f8.png)","944e814a":"___","2cdef456":"# <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"6\">CNN - CIFAR-10 Data<\/font><\/span> <a id=\"4.2\"><\/a><br>\n**Dictionary of Data** --> [Kaggle](https:\/\/www.kaggle.com\/c\/cifar-10\/data)","2c4ba6e5":"<center><span style=\"color:#1565C0; opacity:1\"><font size=\"6\">Do\u011fru Tahmin Bravo!!! \/ Correct Prediction Congrats!!!<\/font><\/span><\/center><br>\n<img src=\"http:\/\/www.femoticons.net\/images\/posts\/cool_sunglasses_emoticon.jpg\" alt=\"acorrect_pred\">","b73c946d":"<h1 style=\"color:MidnightBlue; opacity: 0.8; font-size:250%; text-align:center; border-radius:7px;\">Convolutional Neural Network<\/h1>\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">Selamlar! \/ Greetings!<\/font><\/span>](#0)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">CNN Nedir? \/ What is CNN?<\/font><\/span>](#1)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">Genel Anlamda CNN \/ CNN in General<\/font><\/span>](#2)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">Convolution Nedir? \/ What is Convolution?<\/font><\/span>](#2.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">CNN ve ANN (D\u00fczenli) kar\u015f\u0131la\u015ft\u0131rmas\u0131! \/ Comparison of CNN and ANN (Regular)!<\/font><\/span>](#2.2)\n    \n\n* [<span style=\"color:LightCoral\"><font size=\"5\">CNN \u0130\u015fleyi\u015fi \/ The Process of CNN<\/font><\/span>](#3)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">Noktalar\u0131n Seviyeleri \/ Feature Levels<\/font><\/span>](#3.1)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">Ka\u00e7 Katmana \u0130htiya\u00e7 Var? \/ How Many Layers Do You Need?<\/font><\/span>](#3.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">Convolution \u0130\u015flemi \/ Convolution Process<\/font><\/span>](#3.3)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">PADDING Nedir? \/ What is PADDING?<\/font><\/span>](#3.4)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">POOLING Katman\u0131 \/ POOLING Layer<\/font><\/span>](#3.5)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">MAX POOLING<\/font><\/span>](#3.6)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">FLATTEN \u0130\u015flemi \/ FLATTEN Process<\/font><\/span>](#3.7)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">Fully-Connected Katman \/ Fully-Connected Layer<\/font><\/span>](#3.8)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">B\u00fct\u00fcn S\u00fcrecin G\u00f6rseli \/ \u0130llustration of the Whole Process<\/font><\/span>](#3.9)\n\n\n* [<span style=\"color:LightCoral\"><font size=\"5\">Projeler \/ Projects<\/font><\/span>](#4)\n\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">MNIST Veri Seti ile CNN \/ CNN with MNIST Data<\/font><\/span>](#4.1)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">KERAS \"to categorical\"<\/font><\/span>](#4.1.1)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Earlystopping<\/font><\/span>](#4.1.2)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Batch Size \/ Epochs \/ Learning Rate<\/font><\/span>](#4.1.3)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Confusion Matrix \/ Classification Report<\/font><\/span>](#4.1.4)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Hadi Deneyelim \/ Let's Predict<\/font><\/span>](#4.1.5)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">CIFAR-10 Veri Seti ile CNN \/ CNN with CIFAR-10 Data<\/font><\/span>](#4.2)\n    - [<span style=\"color:#9C27B0; opacity:1\"><font size=\"4\">Image Classification \/ Data Augmentation<\/font><\/span>](#4.3)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">ImageDataGenerator<\/font><\/span>](#4.3.1)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Data Augmentation<\/font><\/span>](#4.3.2)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Veri setlerini Olu\u015fturma \/ Generating Datasets *pip install split-folders*<\/font><\/span>](#4.3.3)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">flow_from_directory<\/font><\/span>](#4.3.4)\n        - [<span style=\"color:#19A4E6; opacity:1\"><font size=\"3\">Hadi Deneyelim \/ Let's Predict<\/font><\/span>](#4.3.5)\n \n \n* [<span style=\"color:LightCoral\"><font size=\"5\">Sonu\u00e7 \/ Conclusion<\/font><\/span>](#5)","1324e062":"\u015eimdi PetImages2 ad\u0131nda bir bo\u015f klas\u00f6r olu\u015ftural\u0131m ve split edilmi\u015f datasetlerini oraya atal\u0131m.<br>\n\n<span style=\"color:#1565C0; opacity:1\">Now let's create an empty folder named PetImages2 and put the split datasets there.<\/span><br>\n![image.png](attachment:f3e6bc51-9051-4c5b-a226-ad5843bf1903.png)","a4ad1858":"**Hadi Klas\u00f6r Uzant\u0131lar\u0131n\u0131 Ayarlayal\u0131m**<br>\n<span style=\"color:#1565C0; opacity:1;font-weight:bold\">Let's Specify the Folder Paths<\/span><br>","4f6b89b0":"**G\u00f6r\u00fcnt\u00fc Manip\u00fclasyonu**\n\nG\u00f6r\u00fcnt\u00fcleri d\u00f6nd\u00fcrme, yeniden boyutland\u0131rma ve \u00f6l\u00e7ekleme ile i\u015flemek i\u00e7in ImageDataGenerator'\u0131 kullanabiliriz, b\u00f6ylece model, veri setimizin sahip olmad\u0131\u011f\u0131 farkl\u0131 g\u00f6r\u00fcnt\u00fclere kar\u015f\u0131 daha sa\u011flam hale gelir. ImageDataGenerator a\u015fa\u011f\u0131dakileri yapar.\n\n\n* E\u011fitim i\u00e7in kullan\u0131lan batch g\u00f6r\u00fcnt\u00fcleri kabul eder.\n* Batch\/grup i\u00e7indeki her g\u00f6r\u00fcnt\u00fcye bir dizi rastgele d\u00f6n\u00fc\u015f\u00fcm uygular.\n* Orijinal batch\/grup ile rastgele d\u00f6n\u00fc\u015ft\u00fcr\u00fclm\u00fc\u015f grup resmi de\u011fi\u015ftirir.\n* CNN'yi bu rastgele d\u00f6n\u00fc\u015ft\u00fcr\u00fclm\u00fc\u015f batchler ile e\u011fitir.\n\n\nVeri b\u00fcy\u00fctmeyi uygulaman\u0131n amac\u0131, daha genelle\u015ftirilmi\u015f bir modele sahip olmakt\u0131r.\n\n<span style=\"color:#1565C0; opacity:1;font-weight:bold\">Image Manipulation<\/span>\n\n<span style=\"color:#1565C0; opacity:1\"><br>\nWe can use the ImageDataGenerator to manipulate the images with rotation, resizing, and scaling so the model becomes more robust to different images that our data set doesn't have. ImageDataGenerator does the followings.<\/span>\n\n<span style=\"color:#1565C0; opacity:1\"><br>\n     *Accepts a batch of images used for training.<br>\n     *Applies a series of random transformations to each image in the batch.<br>\n     *Replaces the original batch with randomly transformed batch.<br>\n     *Training the CNN on this randomly transformed batch.<\/span>\n\n<span style=\"color:#1565C0; opacity:1\"><br>\nThe goal of applying data augmentation is to have a more generalized model.<\/span>","1cfe073c":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Batch Size \/ Epochs \/ Learning Rate<\/font><\/span> <a id=\"4.1.3\"><\/a>\n\n\u0130terasyonlar yani tekrarlar, bir epoch'u tamamlamak i\u00e7in gereken batch say\u0131s\u0131d\u0131r. Batch say\u0131s\u0131, bir epoch i\u00e7indeki iterasyon say\u0131s\u0131na e\u015fittir.<br>\nK\u0131saca: Bizim datam\u0131z 60000 sat\u0131r idi. A\u015fa\u011f\u0131da her bir batch i\u00e7inde 1875 sat\u0131r var ve batch say\u0131m\u0131z 32'dir. 1875x32=60000<br>\nVe her bir epoch'ta hata kay\u0131b\u0131m\u0131z global point'e d\u00fc\u015fmesi i\u00e7in e\u011fitimler tekrarlan\u0131r.<br>\nHatan\u0131n en d\u00fc\u015f\u00fck olaca\u011f\u0131 global point'i yakalayabilmek i\u00e7in p\u00fcf noktalar (learning rate etc.) [buradad\u0131r](https:\/\/towardsdatascience.com\/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0).<br>\n<span style=\"color:#1565C0; opacity:1\">Iterations is the number of batches needed to complete one epoch.The number of batches is equal to number of iterations for one epoch.<br>\nIn short: Our data was 60000 rows. Below are 1875 rows in each batch and our batch number is 32. 1875x32=60000.<br>\nAnd in each epoch, the trainings are repeated so that our error loss falls to the global point.<br>\nTricks to catch the global point where the error will be lowest (learning rate etc.)<\/span> [here](https:\/\/towardsdatascience.com\/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)","e274546e":"## \u015eimdi Modelimizi Geli\u015ftirelim \/ Let's improve our model","0ac87c4d":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Earlystopping<\/font><\/span> <a id=\"4.1.2\"><\/a>\n\n\u0130zlenen bir metrik iyile\u015ftirmeyi durdurdu\u011funda e\u011fitimi durdurur. Daha fazlas\u0131n\u0131 \u00f6\u011frenmek istiyorsan\u0131z l\u00fctfen [t\u0131klay\u0131n](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping).<br>\nE\u011fer g\u00f6rsel ve uygulamal\u0131 olarak izleyerek \u00f6\u011frenmek isterseniz l\u00fctfen videoya [buradan](https:\/\/www.youtube.com\/watch?v=2UHCjhyNLKw) eri\u015fin.<br>\n<span style=\"color:#1565C0; opacity:1\">It stops training when a monitored metric has stopped improving. If you wanna learn more please<\/span> [click](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping)<br>\n<span style=\"color:#1565C0; opacity:1\">If you want to learn by watching visually and practically, please access the video <\/span>[here](https:\/\/www.youtube.com\/watch?v=2UHCjhyNLKw)","327b9e11":"![image.png](attachment:41a0b355-e416-4f3d-9944-bd9b5e94184c.png)","bf4ec772":"**ImageDataGenerator ile D\u00f6n\u00fc\u015ft\u00fcrelim**<br>\n<span style=\"color:#1565C0; opacity:1;font-weight:bold\">Let's Transform the images with ImageDataGenerator<\/span><br>","f3dba395":"## Modelin De\u011ferlendirilmesi \/ Evaluating the Model","dcbb52d1":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">Hadi Deneyelim \/ Let's Predict<\/font><\/span> <a id=\"4.3.5\"><\/a>","653ee469":"![image.png](attachment:4c822d63-cc7e-4f04-8007-8d69ece10754.png)","3a7199fc":"## Data Preprocessing","3bd23c96":"Datam\u0131z\u0131 train, test ve validation olarak b\u00f6lelim.<br>\nDiskinizdeki verileri e\u011fitme, test etme ve do\u011frulama olarak ay\u0131rman\u0131n kolay yolu.<br>\n\n<span style=\"color:#1565C0; opacity:1\">Let's split our data into train, test and validation.<br>\nEasy way to split data on your disk into train, test, and validation.<span>\n    \nSource = [Youtube](https:\/\/www.youtube.com\/watch?v=C6wbr1jJvVs)","46e1b342":"## Data Preprocessing","d6c62cb9":"# Modeli Olu\u015ftural\u0131m \/ Creating the Model","64b63642":"![image.png](attachment:28c06f68-c1f8-43d2-9525-b053de4cb9c2.png)","dccb470e":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">flow_from_directory <\/font><\/span> <a id=\"4.3.4\"><\/a>\n### Bir klas\u00f6rden \u00e7ok say\u0131da manip\u00fcle edilmi\u015f g\u00f6r\u00fcnt\u00fc olu\u015fturma\nflow_from_directory i\u015flevi, alt dizinlerde d\u00fczenlenen resimlerle \u00e7al\u0131\u015f\u0131r. Dizinleriniz yaln\u0131zca bir g\u00f6r\u00fcnt\u00fc s\u0131n\u0131f\u0131 i\u00e7ermelidir, bu nedenle g\u00f6r\u00fcnt\u00fc s\u0131n\u0131f\u0131 ba\u015f\u0131na bir klas\u00f6r.\n\n### <span style=\"color:#1565C0; opacity:1\">Generating many manipulated images from a directory<\/span>\n<span style=\"color:#1565C0; opacity:1\">flow_from_directory function works with images organized in sub-directories. Your directories should include only one class of images, so one folder per class of images.<\/span>","541f5106":"**Data_Augmentation ile D\u00f6n\u00fc\u015ft\u00fcrelim**<br>\n\nVeri k\u00fcmesindeki ilk g\u00f6r\u00fcnt\u00fcye art arda data_augmentation uygulayarak art\u0131r\u0131lm\u0131\u015f \u00f6rneklerin nas\u0131l g\u00f6r\u00fcnd\u00fc\u011f\u00fcn\u00fc g\u00f6rselle\u015ftirelim:\n\n<span style=\"color:#1565C0; opacity:1;font-weight:bold\">Let's Transform the images with Data_Augmentation<\/span><br>\n\n<span style=\"color:#1565C0; opacity:1\">Let's visualize what the augmented samples look like, by applying data_augmentation repeatedly to the first image in the dataset:<\/span>","7fe55dae":"<center><span style=\"color:#1565C0; opacity:1\"><font size=\"6\">Do\u011fru Tahmin Bravo!!! \/ Correct Prediction Congrats!!!<\/font><\/span><\/center><br>\n<img src=\"http:\/\/www.femoticons.net\/images\/posts\/cool_sunglasses_emoticon.jpg\" alt=\"acorrect_pred\">","e61db4d3":"**Bozuk resimleri filtreleme**<br>\n\u00c7ok say\u0131da ger\u00e7ek d\u00fcnya g\u00f6r\u00fcnt\u00fc verisi ile \u00e7al\u0131\u015f\u0131rken, bozuk g\u00f6r\u00fcnt\u00fcler yayg\u0131n bir durumdur.<br> \nBa\u015fl\u0131klar\u0131nda \"JFIF\" dizesini i\u00e7ermeyen k\u00f6t\u00fc kodlanm\u0131\u015f g\u00f6r\u00fcnt\u00fcleri filtreleyelim.<br>\n\n<span style=\"color:#1565C0; opacity:1\">**Filter out corrupted images**<br>\nWhen working with lots of real-world image data, corrupted images are a common occurrence.<br>\nLet's filter out badly-encoded images that do not feature the string \"JFIF\" in their header.<\/span>","c160137c":"K\u00f6pekler ve Kediler veri seti, resimlerin k\u00f6pek veya bir kedi mi oldu\u011funun s\u0131n\u0131fland\u0131r\u0131ld\u0131\u011f\u0131 yayg\u0131n kullan\u0131lan bir g\u00f6rsel veri setidir.<br>\nVeri seti ile iyi bir \u015fekilde \u00e7al\u0131\u015f\u0131lmas\u0131, g\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 i\u00e7in evri\u015fimli sinir a\u011flar\u0131n\u0131n (CNN) nas\u0131l tasarlanaca\u011f\u0131n\u0131, de\u011ferlendirilece\u011fini ve uygulanaca\u011f\u0131n\u0131 anlamak ve uygulamak i\u00e7in faydal\u0131 olacakt\u0131r.<br>\nG\u00f6r\u00fcnt\u00fclerle bir s\u0131n\u0131fland\u0131rma olu\u015fturup ve CNN kullanarak kedileri ve k\u00f6pekleri tespit etmeye \u00e7al\u0131\u015faca\u011f\u0131z.<br>\n\n<span style=\"color:#1565C0; opacity:1\">This example shows how to do image classification from scratch, starting from JPEG image files on disk, without leveraging pre-trained weights or a pre-made Keras Application model. We demonstrate the workflow on the Kaggle Cats vs Dogs binary classification dataset.<\/span><br> ","ab1a2620":"## <span style=\"color:LightCoral; background-color:#F5EEF8\"><font size=\"4\">KERAS to_categorical<\/font><\/span> <a id=\"4.1.1\"><\/a>\n\nBir s\u0131n\u0131f(class) vekt\u00f6r\u00fcn\u00fc (yani tam say\u0131lar\u0131) ikili s\u0131n\u0131f matrisine d\u00f6n\u00fc\u015ft\u00fcr\u00fcr. Daha fazlas\u0131n\u0131 \u00f6\u011frenmek istiyorsan\u0131z l\u00fctfen [t\u0131klay\u0131n](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/to_categorical).<br>\n<span style=\"color:#1565C0; opacity:1\">Converts a class vector (integers) to binary class matrix. If you wanna learn more please <\/span>[click](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/to_categorical)<br>","afcc5324":"![image-2.png](attachment:image-2.png)"}}