{"cell_type":{"825e1f76":"code","554a0b78":"code","2d85ec41":"code","aac3bbf6":"code","42e78c01":"code","da215ad4":"code","c1682383":"code","0eba6123":"code","12fa12d0":"code","5ccb8dba":"code","d83070ee":"code","bca369a6":"code","075476c5":"code","515b708e":"code","d8bc7820":"code","4d7924c4":"code","28d431cd":"code","3ffc75c5":"code","69eebdb6":"code","33077a77":"code","ffc39824":"code","9268eab2":"code","1581f2fc":"code","b9dd3fd0":"code","43560dca":"code","fad7377c":"code","a7a919dc":"code","5994f14d":"code","3e2916c0":"code","34933c89":"code","b9babd99":"code","fc55fce2":"code","e96f9166":"code","c3e4e976":"code","f95b2f5a":"code","3eaa7896":"code","31ec5d01":"code","6e2ac78c":"code","ac3785de":"code","571469a8":"code","422bd01f":"code","665e95f1":"code","6cb1a9a5":"code","bd2d945d":"code","ca91cc93":"code","1521486a":"code","3f14a433":"code","d7abceed":"code","7ffd8e41":"code","a1d9725f":"code","a03190f9":"code","857829a1":"code","77bed55c":"code","99a07a87":"code","3422c819":"code","401462be":"code","a7c8df59":"markdown"},"source":{"825e1f76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","554a0b78":"import numpy as np\nimport pandas as pd \nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport random\nimport os\nprint(os.listdir(\"\/kaggle\/input\/nnfl-lab-1\"))","2d85ec41":"filenames = os.listdir(\"\/kaggle\/input\/nnfl-lab-1\/training\/training\")\ncategories = []\nfor filename in filenames:\n    category = filename.split('_')[0]\n    if category == 'chair':\n        categories.append(0)\n    elif category == 'kitchen':\n        categories.append(1)\n    elif category == 'knife':\n        categories.append(2)\n    elif category == 'saucepan':\n        categories.append(3)\n\n\ndf = pd.DataFrame({\n    'filename': filenames,\n    'category': categories\n})","aac3bbf6":"df","42e78c01":"df['category'].value_counts().plot.bar()","da215ad4":"sample = random.choice(filenames)\nimage = load_img(\"\/kaggle\/input\/nnfl-lab-1\/training\/training\/\"+sample)\nplt.imshow(image)","c1682383":"FAST_RUN = False\nIMAGE_WIDTH=200\nIMAGE_HEIGHT=200\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nIMAGE_CHANNELS=3","0eba6123":"# from keras.models import Sequential\n# from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n\n# model = Sequential()\n\n# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(30, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(30, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Flatten())\n# model.add(Dense(30, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(4, activation='softmax')) # 2 because we have cat and dog classes\n\n# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n# model.summary()","12fa12d0":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau","5ccb8dba":"earlystop = EarlyStopping(patience=10)","d83070ee":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","bca369a6":"callbacks = [earlystop, learning_rate_reduction]","075476c5":"df['category'].head()","515b708e":"df[\"category\"] = df[\"category\"].replace({0: 'chair', 1: 'kitchen', 2: 'knife', 3: 'saucepan'}) ","d8bc7820":"df.head()","4d7924c4":"train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)","28d431cd":"train_df['category'].value_counts().plot.bar()","3ffc75c5":"validate_df['category'].value_counts().plot.bar()","69eebdb6":"total_train = train_df.shape[0]\ntotal_validate = validate_df.shape[0]\nbatch_size=15","33077a77":"print(total_train)\nprint(total_validate)","ffc39824":"import cv2\nfrom sklearn.model_selection import train_test_split","9268eab2":"train_images = []       \ntrain_labels = []\nshape = (200,200)  \ntrain_path = '\/kaggle\/input\/nnfl-lab-1\/training\/training\/'\n\nfor filename in os.listdir('\/kaggle\/input\/nnfl-lab-1\/training\/training\/'):\n    if filename.split('.')[1] == 'jpg':\n        img = cv2.imread(os.path.join(train_path,filename))\n        \n        # Spliting file names and storing the labels for image in list\n        name=filename.split('_')[0]\n        if name=='chair':\n            train_labels.append(0)\n        elif name=='kitchen':\n            train_labels.append(1)\n        elif name=='knife':\n            train_labels.append(2)\n        elif name=='saucepan':\n            train_labels.append(3)\n        \n        # Resize all images to a specific shape\n        img = cv2.resize(img,shape)\n        \n        train_images.append(img)\n\n# Converting labels into One Hot encoded sparse matrix\n#train_labels = pd.DataFrame(train_labels).values\ntrain_labels = pd.get_dummies(train_labels).values\n# Converting train_images to array\ntrain_images = np.array(train_images)\n\n# Splitting Training data into train and validation dataset\nx_train,x_val,y_train,y_val = train_test_split(train_images,train_labels,random_state=1)","1581f2fc":"train_labels.shape","b9dd3fd0":"x_train.shape","43560dca":"x_val.shape","fad7377c":"test_images = []\ntest_labels = []\nshape = (200,200)\ntest_path = '\/kaggle\/input\/nnfl-lab-1\/testing\/testing'\n\nfor filename in os.listdir('\/kaggle\/input\/nnfl-lab-1\/testing\/testing'):\n    if filename.split('.')[1] == 'jpg':\n        img = cv2.imread(os.path.join(test_path,filename))\n        \n        # Spliting file names and storing the labels for image in list\n        test_labels.append(filename.split('_')[0])\n        \n        # Resize all images to a specific shape\n        img = cv2.resize(img,shape)\n        \n        test_images.append(img)\n        \n# Converting test_images to array\ntest_images = np.array(test_images)","a7a919dc":"len(test_labels)","5994f14d":"print(train_labels[3])\nplt.imshow(train_images[3])","3e2916c0":"print(train_labels[217])\nplt.imshow(train_images[217])","34933c89":"# from keras.models import Sequential\n# from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n\n# model = Sequential()\n\n# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(64, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Conv2D(64, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n\n# model.add(Conv2D(128, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(128, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(256, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n# model.add(Conv2D(256, (3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Flatten())\n# model.add(Dense(512, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n\n# model.add(Dense(128))\n# model.add(BatchNormalization())\n# model.add(Activation('relu'))\n# #Add Dropout\n# model.add(Dropout(0.4))\n\n# model.add(Dense(4, activation='softmax')) \n\n# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n# model.summary()","b9babd99":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization, Input\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n#model.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n# model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# #model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D((2, 2)))\n# model.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n#model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n#model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n#model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\n# model.add(Dense(128, activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dense(128, activation='relu'))\n# model.add(BatchNormalization())\n\nmodel.add(Dropout(0.5))\n\n# model.add(Dense(4096, activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Activation('relu'))\n# model.add(Dropout(0.4))\n\nmodel.add(Dense(4, activation='softmax')) \n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\nmodel.summary()","fc55fce2":"# AlexNet = Sequential()\n\n# #1st Convolutional Layer\n# AlexNet.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n# AlexNet.add(Conv2D(filters=96, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS), kernel_size=(11,11), strides=(4,4), padding='same'))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n# AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n# #2nd Convolutional Layer\n# AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n# AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n# #3rd Convolutional Layer\n# AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n\n# #4th Convolutional Layer\n# AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n\n# #5th Convolutional Layer\n# AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n# AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n# #Passing it to a Fully Connected layer\n# AlexNet.add(Flatten())\n# # 1st Fully Connected Layer\n# AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n# # Add Dropout to prevent overfitting\n# AlexNet.add(Dropout(0.4))\n\n# #2nd Fully Connected Layer\n# AlexNet.add(Dense(4096))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n# #Add Dropout\n# AlexNet.add(Dropout(0.4))\n\n# #3rd Fully Connected Layer\n# AlexNet.add(Dense(1000))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('relu'))\n# #Add Dropout\n# AlexNet.add(Dropout(0.4))\n\n# #Output Layer\n# AlexNet.add(Dense(10))\n# AlexNet.add(BatchNormalization())\n# AlexNet.add(Activation('softmax'))\n\n# AlexNet.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\n# #Model Summary\n# AlexNet.summary() ","e96f9166":"dataAugmentaion = ImageDataGenerator(rotation_range = 15, zoom_range = 0.20, shear_range = 0.1, horizontal_flip = True, \nwidth_shift_range = 0.1, height_shift_range = 0.1, rescale=1.\/255)","c3e4e976":"# rotation_range=15,\n#     rescale=1.\/255,\n#     shear_range=0.1,\n#     zoom_range=0.2,\n#     horizontal_flip=True,\n#     width_shift_range=0.1,\n#     height_shift_range=0.1","f95b2f5a":"print(x_train.shape, y_train.shape)","3eaa7896":"# AlexNet.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","31ec5d01":"# Training the model\nhistory = model.fit(x_train,y_train,epochs=74,batch_size=16,validation_data=(x_val,y_val))\n# epochs=3 if FAST_RUN else 10\n#history=model.fit_generator(dataAugmentaion.flow(x_train, y_train, batch_size = 32),validation_data = (x_val, y_val), steps_per_epoch = len(x_train) \/\/ 32,epochs = 50)","6e2ac78c":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ac3785de":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","571469a8":"evaluate = model.evaluate(x_val,y_val)\nprint(evaluate)\n","422bd01f":"model.save_weights(\"checkv10.h5\")","665e95f1":"predict = model.predict(test_images)","6cb1a9a5":"(np.argmax(predict[4]))","bd2d945d":"outputs=[]\nfor i in range(len(predict)) :\n    temp=[]\n    temp.append(test_labels[i])\n    temp.append(np.argmax(predict[i]))\n    outputs.append(temp)\noutput=pd.DataFrame(outputs)\n#columns=columns={\"0\": \"id\", \"1\": \"label\"}\n#output.rename(str.lower, axis='columns')","ca91cc93":"output=output.rename(columns={0: \"id\", 1: \"label\"})","1521486a":"#output","3f14a433":"print(test_labels[42])\nprint(np.argmax(predict[42]))\nplt.imshow(train_images[42])","d7abceed":"output.to_csv('sv10final.csv', index=False)","7ffd8e41":"plt.figure(figsize=(20,100))\nfor n , i in enumerate(list(np.random.randint(0,len(predict),100))) : \n    plt.subplot(20,5,n+1)\n    plt.imshow(test_images[i])    \n    plt.axis('off')\n    classes = {'chair':0 ,'kitchen':1,'knife':2,'saucepan':3}\n    def get_img_class(n):\n        for x , y in classes.items():\n            if n == y :\n                return x\n    plt.title(get_img_class(np.argmax(predict[i])))","a1d9725f":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\nax1.plot(history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nax1.set_xticks(np.arange(1, 50, 1))\nax1.set_yticks(np.arange(0, 1, 0.1))\n\nax2.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax2.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, 50, 1))\n\nlegend = plt.legend(loc='best', shadow=True)\nplt.tight_layout()\nplt.show()","a03190f9":"output","857829a1":"# test_filenames = os.listdir(\"\/kaggle\/input\/nnfl-lab-1\/testing\/testing\")\n# test_df = pd.DataFrame({\n#     'filename': test_filenames\n# })\n# nb_samples = test_df.shape[0]","77bed55c":"# test_df['category'] = np.argmax(predict, axis=-1)","99a07a87":"# sample_test = test_df.head(10)\n# sample_test.head()\n# plt.figure(figsize=(12, 24))\n# for index, row in sample_test.iterrows():\n#     filename = row['filename']\n#     category = row['category']\n#     img = load_img(\"\/kaggle\/input\/nnfl-lab-1\/testing\/testing\/\"+filename, target_size=IMAGE_SIZE)\n#     plt.subplot(6, 3, index+1)\n#     plt.imshow(img)\n#     plt.xlabel(filename + '(' + \"{}\".format(category) + ')' )\n# plt.tight_layout()\n# plt.show()","3422c819":"# submission_df = test_df.copy()\n# submission_df['id'] = submission_df['filename'].str.split('.').str[0]\n# submission_df['label'] = submission_df['category']\n# submission_df.drop(['filename', 'category'], axis=1, inplace=True)\n# submission_df.to_csv('s1.csv', index=False)","401462be":"from IPython.display import HTML \nimport pandas as pd \nimport numpy as np\nimport base64 \ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"): \n    csv = df.to_csv(index=False) \n    b64 = base64.b64encode(csv.encode()) \n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(output)","a7c8df59":"referred to the following notebook for data processing https:\/\/www.kaggle.com\/prateek0x\/multiclass-image-classification-using-keras"}}