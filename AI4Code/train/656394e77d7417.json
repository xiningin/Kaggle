{"cell_type":{"4112d063":"code","38544294":"code","57a26241":"code","19596ae9":"code","45c070e7":"code","93de4eed":"code","4823a172":"code","0b93de0a":"code","de935b21":"code","86da8145":"code","d287a20f":"code","2605c210":"code","7334f2e0":"code","3a610f12":"code","8f5cf560":"code","4bf86177":"markdown","b98bba03":"markdown","8d087c0b":"markdown","1c962f1f":"markdown","4294168c":"markdown","fe27b1d5":"markdown","22af478f":"markdown","d59dad17":"markdown","b8edb487":"markdown","6d686482":"markdown","efc29746":"markdown","9eee4458":"markdown"},"source":{"4112d063":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","38544294":"input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\ninput_str = input_str.lower()\nprint(input_str)","57a26241":"import re\ninput_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\nresult = re.sub(r'\\d+', '', input_str)\nprint(result)","19596ae9":"# The following code removes this set of symbols [!\u201d#$%&\u2019()*+,-.\/:;<=>?@[\\]^_`{|}~]\nimport string\ninput_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\nprint(string.punctuation)\nresult = input_str.translate(str.maketrans('', '', string.punctuation))\nprint(result)","45c070e7":"input_str = \" \\t a string example\\t. \"\ninput_str = input_str.strip()\nprint(\" \\t a string example \\t .\")\ninput_str","93de4eed":"from spacy.lang.en.stop_words import STOP_WORDS\ninput_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n\nstop_words = set(STOP_WORDS)\nfrom nltk.tokenize import word_tokenize\ntokens = word_tokenize(input_str)\nresult = [i for i in tokens if not i in stop_words]\noutput_str = \" \".join(result)\nprint (input_str)\nprint (output_str)","4823a172":"result","0b93de0a":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nstemmer= PorterStemmer()\ninput_str= \"There are several types of stemming algorithms. The city mice recommends NLTK.\"\n# input_str=word_tokenize(input_str)\nl = []\nfor word in word_tokenize(input_str):\n    l.append(stemmer.stem(word))\nprint(input_str)\nprint(\" \".join(l))","de935b21":"from nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nlemmatizer=WordNetLemmatizer()\ninput_str=\"There are several types of stemming algorithms. The city mice recommends NLTK.\"\n\nl2 = []\nfor word in word_tokenize(input_str):\n    l2.append(lemmatizer.lemmatize(word))\nprint(input_str)\nprint(\" \".join(l2))","86da8145":"input_str=\"The quick brown fox jumps over the lazy little dog.\"\nfrom textblob import TextBlob\nresult = TextBlob(input_str)\nprint(result.tags)","d287a20f":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\ndoc1 = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')\ndoc2 = nlp('Chad ate an apple from Apple.')\ndoc3 = nlp('Bill works for Apple so he went to Boston for a conference.')\n\nprint([(ent.text, ent.label_) for ent in doc1.ents])\nprint([(ent.text, ent.label_) for ent in doc2.ents])\nprint([(ent.text, ent.label_) for ent in doc3.ents])","2605c210":"text1 = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power \\\n        in the mobile phone market and ordered the company to alter its practices'\ntext2 = 'Chad ate an apple from Apple.'\ntext3 = 'Bill works for Apple so he went to Boston for a conference.'\n\ndisplacy.render(nlp(text1), jupyter=True, style='ent')\ndisplacy.render(nlp(text2), jupyter=True, style='ent')\ndisplacy.render(nlp(text3), jupyter=True, style='ent')","7334f2e0":"from gensim.models import Word2Vec\n# define training data\nsentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n\t\t\t['yet', 'another', 'sentence'],\n\t\t\t['one', 'more', 'sentence'],\n\t\t\t['and', 'the', 'final', 'sentence']]\n# train model\nmodel = Word2Vec(sentences, min_count=1, size=100, window=5, workers=4)\n# summarize the loaded model\n#print(model)\n# summarize vocabulary\nwords = list(model.wv.vocab)\nprint(words)\n# access vector for one word\n#print(model['sentence'])\n# save model\n#model.save('model.bin')\n# load model\n#new_model = Word2Vec.load('model.bin')\n#print(new_model)\nprint(model['final'])","3a610f12":"from sklearn.decomposition import PCA\nfrom matplotlib import pyplot\nX = model[model.wv.vocab]\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n    ","8f5cf560":"from gensim.models import KeyedVectors\n# load the google word2vec model\nfilename = '\/kaggle\/input\/GoogleNews-vectors-negative300.bin.gz'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)\n# calculate: (king - man) + woman = ?\nresult = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\nprint(result)","4bf86177":"## Remove punctuation","b98bba03":"## Importing Google's Word2Vec embeddings","8d087c0b":"## Lemmatization\n","1c962f1f":"## Stemming\nStemming is a process of reducing words to their word stem, base or root form (for example, books \u2014 book, looked \u2014 look). ","4294168c":"## Named entity recognition\nNamed-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.).","fe27b1d5":"## Word Embeddings in Gensim","22af478f":"## Part of speech tagging (POS)\nPart-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context.\n\n\t1.\tCC\tCoordinating conjunction\n\t2.\tCD\tCardinal number\n\t3.\tDT\tDeterminer\n\t4.\tEX\tExistential there\n\t5.\tFW\tForeign word\n\t6.\tIN\tPreposition or subordinating conjunction\n\t7.\tJJ\tAdjective\n\t8.\tJJR\tAdjective, comparative\n\t9.\tJJS\tAdjective, superlative\n\t10.\tLS\tList item marker\n\t11.\tMD\tModal\n\t12.\tNN\tNoun, singular or mass\n\t13.\tNNS\tNoun, plural\n\t14.\tNNP\tProper noun, singular\n\t15.\tNNPS\tProper noun, plural\n\t16.\tPDT\tPredeterminer\n\t17.\tPOS\tPossessive ending\n\t18.\tPRP\tPersonal pronoun\n\t19.\tPRP\tPossessive pronoun\n\t20.\tRB\tAdverb\n\t21.\tRBR\tAdverb, comparative\n\t22.\tRBS\tAdverb, superlative\n\t23.\tRP\tParticle\n\t24.\tSYM\tSymbol\n\t25.\tTO\tto\n\t26.\tUH\tInterjection\n\t27.\tVB\tVerb, base form\n\t28.\tVBD\tVerb, past tense\n\t29.\tVBG\tVerb, gerund or present participle\n\t30.\tVBN\tVerb, past participle\n\t31.\tVBP\tVerb, non-3rd person singular present\n\t32.\tVBZ\tVerb, 3rd person singular present\n\t33.\tWDT\tWh-determiner\n\t34.\tWP\tWh-pronoun\n\t35.\tWP\tPossessive wh-pronoun\n\t36.\tWRB\tWh-adverb","d59dad17":"## Remove whitespace","b8edb487":"## Convert text to lower case","6d686482":"## Visualize word embeddings","efc29746":"## Remove numbers","9eee4458":"## Tokenization and Stop Words\nTokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. \n\n\u201cStop words\u201d are the most common words in a language like \u201cthe\u201d, \u201ca\u201d, \u201con\u201d, \u201cis\u201d, \u201call\u201d. These words do not carry important meaning and are usually removed from texts. "}}