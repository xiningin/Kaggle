{"cell_type":{"c7e45ad4":"code","99508dc8":"code","e05e8ddf":"code","44bc7f63":"code","ab06393e":"code","a888ee41":"code","edc13cff":"code","a0cd24d3":"code","2eca8132":"code","811770c4":"code","5eedea05":"code","acf93885":"code","847f19f7":"code","d5c54ce1":"code","4a3f0fe0":"code","14099f08":"code","c96abed3":"code","5a9e33f6":"code","cc963abf":"code","da3d2de3":"code","6419b47b":"code","536891e6":"code","5810a7ff":"code","43d21fa9":"code","8530568f":"code","f4c7f016":"code","cd05782b":"code","73fd61ae":"code","b1d57335":"code","8e9b3e73":"code","c8058844":"code","21c3feb7":"code","b0b0b1ab":"code","4373461b":"code","a38bf0ae":"code","045344d4":"code","f6808337":"code","33174c6f":"code","f4b6d0cd":"code","2b8653fb":"code","ed7efc39":"code","94e47ebf":"code","a6c9c847":"code","72613dc8":"code","90995f66":"code","6c44c799":"code","6e551fe6":"code","08b76a68":"code","1e24e9a7":"code","c2b10fcc":"code","8c4f298b":"code","dcb060ce":"code","d2766dd4":"code","cd38d7fa":"code","1ff34936":"code","761dd65a":"code","9b8e36d2":"code","fb75ba8f":"code","0e78922c":"markdown","3811c047":"markdown","b0f0ce2b":"markdown","359e0ecd":"markdown","c6e6bdc3":"markdown","60a26c70":"markdown","c1584e65":"markdown","4edde86f":"markdown","3a0abaaa":"markdown","6bbb09b2":"markdown","6c18d6ca":"markdown","b5ef636b":"markdown","e1ecdf01":"markdown","b8ebd7bf":"markdown","bcad0d56":"markdown","5593d4b6":"markdown","621a1753":"markdown","ab7bb0de":"markdown","108b6df5":"markdown","fed5e652":"markdown","8be097cf":"markdown","47bed7d8":"markdown","ac111190":"markdown","962f7f6f":"markdown","5cf0cc1a":"markdown","96058148":"markdown","462ffbad":"markdown","637e3d6a":"markdown","dd79cd9a":"markdown","f24199f5":"markdown"},"source":{"c7e45ad4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport os","99508dc8":"dataset = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","e05e8ddf":"dataset.head()","44bc7f63":"dataset.isnull().sum()","ab06393e":"dataset.shape","a888ee41":"dataset.describe()","edc13cff":"def prevalance(dataset, column):\n    '''Check prevalance of heart diease in the dataset'''\n    positives = dataset[dataset[column] == 1]\n    prevalance = positives.size \/ dataset.size\n    return prevalance","a0cd24d3":"prevalance(dataset, 'DEATH_EVENT')","2eca8132":"def columns_histogram(dataset):\n    '''Plot histograms of all columns in dataset'''\n    columns = dataset.columns\n    plt.figure(figsize = (5,5))\n    for index in columns:\n        plt.hist(dataset[index], bins='auto')\n        plt.title(index)\n        plt.show()","811770c4":"columns_histogram(dataset)","5eedea05":"# select which features to standardize for the model\nstandardize_features = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium']","acf93885":"X = dataset.drop(['DEATH_EVENT', 'time'], axis=1)\ny = dataset['DEATH_EVENT']\nprint(f'Shape of X: {X.shape} and Y: {y.shape}')","847f19f7":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, stratify=y)\nprint('Train data')\nprint(f'Shape of X_train: {X_train.shape} and Y_train: {y_train.shape}')\nprint('Test data')\nprint(f'Shape of X_test: {X_test.shape} and Y_test: {y_test.shape}')","d5c54ce1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_std = X_train.copy()\nX_test_std = X_test.copy()\nX_train_std[standardize_features] = sc.fit_transform(X_train[standardize_features])\nX_test_std[standardize_features] = sc.transform(X_test[standardize_features])","4a3f0fe0":"columns_histogram(X_train_std)\n#X_train_std[standardize_features].hist(figsize=(14,14))\n#plt.show()","14099f08":"def logistic_regression(X_train, y_train):\n    '''Fit a logistic regression model to predict death event of patients'''\n    from sklearn.linear_model import LogisticRegression\n    \n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    return model","c96abed3":"logistic_regression_model = logistic_regression(X_train_std, y_train)","5a9e33f6":"# Test model on a training example\nexample = 45\nprint(f'Actual death event of patient with featues {X_train_std.iloc[example,:]} is {y_train.iloc[example,]}')\nprint()\nprediction = logistic_regression_model.predict(X_train_std[example:example+1])\nprint(f'Predicted death event of patient = {prediction}')","cc963abf":"y_pred = logistic_regression_model.predict(X_test_std)","da3d2de3":"y_pred.shape","6419b47b":"from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\n\nconfusion_matrix = confusion_matrix(y_test.values, y_pred)\nlogistic_accuracy = accuracy_score(y_test.values, y_pred)\n\nprint(f'Confusion matrix values: {confusion_matrix}')\nplot_confusion_matrix(logistic_regression_model, X_test_std, y_test)  \nplt.show()  \nprint(f'Accuracy of logistic regression model: {logistic_accuracy}')","536891e6":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test.values, y_pred))","5810a7ff":"def cindex(y_true, scores):\n    '''\n    Input:\n    y_true (np.array): a 1-D array of true binary outcomes (values of zero or one)\n        0: patient does not have a death event\n        1: patient has a death event\n    scores (np.array): a 1-D array of corresponding death scores output by the model\n\n    Output:\n    c_index (float): (concordant pairs + 0.5*ties) \/ number of permissible pairs\n    '''\n    n = len(y_true)\n    assert len(scores) == n\n\n    concordant = 0\n    permissible = 0\n    ties = 0\n    \n    for i in range(n):\n        for j in range(i+1, n): \n            \n            # Check if the pair is permissible (the patient outcomes are different)\n            if y_true[i] != y_true[j]:\n                # Count the pair if it's permissible\n                permissible += 1\n\n                # For permissible pairs, check if they are concordant or are ties\n                # check for ties in the score\n                if scores[i] == scores[j]:\n                    # count the tie\n                    ties += 1\n                    # if it's a tie, we don't need to check patient outcomes, continue to the top of the for loop.\n                    continue\n\n                # case 1: patient i doesn't get the disease, patient j does\n                if y_true[i] == 0 and y_true[j] == 1:\n                    # Check if patient i has a lower risk score than patient j\n                    if scores[i] < scores[j]:\n                        # count the concordant pair\n                        concordant += 1\n                    # Otherwise if patient i has a higher risk score, it's not a concordant pair.\n                    # Already checked for ties earlier\n\n                # case 2: patient i gets the disease, patient j does not\n                if y_true[i] == 1 and y_true[j] == 0:\n                    # Check if patient i has a higher risk score than patient j\n                    if scores[i] > scores[j]:\n                        #count the concordant pair\n                        concordant += 1\n                    # Otherwise if patient i has a lower risk score, it's not a concordant pair.\n                    # We already checked for ties earlier\n\n    # calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs.\n    c_index = (concordant + 0.5 * ties) \/ permissible    \n    return c_index","43d21fa9":"scores = logistic_regression_model.predict_proba(X_test_std)[:, 1]\nc_index_X_test = cindex(y_test.values, scores)\nprint(f\"c-index on test set is {c_index_X_test:.4f}\")","8530568f":"coeffs = pd.DataFrame(data = logistic_regression_model.coef_, columns = X_train_std.columns)\ncoeffs.T.plot.bar(legend=None)","f4c7f016":"def add_interactions(X):\n    \"\"\"Add interaction terms between columns to dataframe.\"\"\"\n    features = X.columns\n    m = len(features)\n    X_int = X.copy(deep=True)\n\n    for i in range(m):\n        feature_i_name = features[i]\n        feature_i_data = X_int[feature_i_name]\n        \n        # choose the index of column 'j' to be greater than column i\n        for j in range(i+1, m):\n            \n            feature_j_name = features[j]\n            feature_j_data = X_int[feature_j_name]\n            \n            feature_i_j_name = f\"{feature_i_name}_x_{feature_j_name}\"\n            X_int[feature_i_j_name] = feature_i_data * feature_j_data\n            \n    return X_int","cd05782b":"X_train_int = add_interactions(X_train_std)\nX_test_int = add_interactions(X_test_std)","73fd61ae":"X_train_int.head()","b1d57335":"logistic_reg_model_interaction = logistic_regression(X_train_int, y_train)","8e9b3e73":"y_pred_int = logistic_reg_model_interaction.predict(X_test_int)","c8058844":"plot_confusion_matrix(logistic_reg_model_interaction, X_test_int, y_test)  \nplt.show()  \nlogistic_accuracy_int = accuracy_score(y_test.values, y_pred_int)\nprint(f'Accuracy of logistic regression model: {logistic_accuracy_int}')","21c3feb7":"print(classification_report(y_test.values, y_pred_int))","b0b0b1ab":"scores_int = logistic_reg_model_interaction.predict_proba(X_test_int)[:, 1]\nc_index_X_test_int = cindex(y_test.values, scores_int)\nprint(f\"c-index on test set is {c_index_X_test_int:.4f}\")","4373461b":"int_coeffs = pd.DataFrame(data = logistic_reg_model_interaction.coef_, columns = X_train_int.columns)\nint_coeffs.T.plot.bar();","a38bf0ae":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, stratify=y)\nsc = StandardScaler()\nX_train_rf = X_train.copy()\nX_test_rf = X_test.copy()\nX_train_rf[standardize_features] = sc.fit_transform(X_train_rf[standardize_features])\nX_test_rf[standardize_features] = sc.transform(X_test_rf[standardize_features])","045344d4":"def holdout_grid_search(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams={}):\n    '''\n    Conduct hyperparameter grid search on hold out validation set. Use holdout validation.\n    Hyperparameters are input as a dictionary mapping each hyperparameter name to the\n    range of values they should iterate over. Use the cindex function as your evaluation\n    function.\n    '''\n    best_estimator = None\n    best_hyperparams = {}\n    best_score = 0.0\n\n    lists = hyperparams.values()\n    \n    # get all param combinations\n    param_combinations = list(itertools.product(*lists))\n    total_param_combinations = len(param_combinations)\n\n    for i, params in enumerate(param_combinations, 1):\n        param_dict = {}\n        for param_index, param_name in enumerate(hyperparams):\n            param_dict[param_name] = params[param_index]\n            \n        # create estimator with specified params\n        estimator = clf(**param_dict, **fixed_hyperparams)\n\n        # fit estimator\n        estimator.fit(X_train_hp, y_train_hp)\n        \n        # get predictions on validation set\n        preds = estimator.predict_proba(X_val_hp)\n        \n        # compute cindex for predictions\n        estimator_score = cindex(y_val_hp, preds[:,1])\n\n        print(f'[{i}\/{total_param_combinations}] {param_dict}')\n        print(f'Val C-Index: {estimator_score}\\n')\n\n        # if new high score, update high score, best estimator\n        # and best params \n        if estimator_score >= best_score:\n                best_score = estimator_score\n                best_estimator = estimator\n                best_hyperparams = param_dict\n\n    # add fixed hyperparamters to best combination of variable hyperparameters\n    best_hyperparams.update(fixed_hyperparams)\n    \n    return best_estimator, best_hyperparams","f6808337":"def random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped):\n\n    hyperparams = { 'n_estimators':[100,1000,5000], 'max_depth': [4,5,6], 'min_samples_leaf': [0.1, 0.15, 0.2]}\n    fixed_hyperparams = {'random_state': 10,} \n    rf = RandomForestClassifier\n\n    best_rf, best_hyperparams = holdout_grid_search(rf, X_train_dropped, y_train_dropped,\n                                                    X_val_dropped, y_val_dropped, hyperparams,\n                                                    fixed_hyperparams)\n\n    print(f\"Best hyperparameters:\\n{best_hyperparams}\")\n\n    \n    y_train_best = best_rf.predict_proba(X_train_dropped)[:, 1]\n    print(f\"Train C-Index: {cindex(y_train_dropped, y_train_best)}\")\n\n    y_val_best = best_rf.predict_proba(X_val_dropped)[:, 1]\n    print(f\"Val C-Index: {cindex(y_val_dropped, y_val_best)}\")\n    \n    # add fixed hyperparamters to best combination of variable hyperparameters\n    best_hyperparams.update(fixed_hyperparams)\n    \n    return best_rf, best_hyperparams","33174c6f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state=10)\nrandom_forest.fit(X_train_rf, y_train)\n\ngscv = GridSearchCV(estimator=random_forest, param_grid={\"n_estimators\":[100,500,1000,5000],\n                                                   'max_depth': [4,6,8],\n                                                   \"criterion\":[\"gini\",\"entropy\"]},\n                    cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_rf,y_train)\nrandom_forest = gscv.best_estimator_","f4b6d0cd":"y_pred_rf = random_forest.predict(X_test_rf)\nprint(classification_report(y_test, y_pred_rf))","2b8653fb":"random_forest","ed7efc39":"y_pred_rf_train = random_forest.predict_proba(X_train_rf)[:, 1]\nprint(f\"Train C-Index: {cindex(y_train.values, y_pred_rf_train)}\")\n\ny_pred_rf_prob = random_forest.predict_proba(X_test_rf)[:, 1]\nprint(f\"Test C-Index: {cindex(y_test.values, y_pred_rf_prob)}\")","94e47ebf":"from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n\nplot_roc_curve(random_forest, X_test_rf, y_test)\nplt.show()","a6c9c847":"plot_precision_recall_curve(random_forest, X_test_rf, y_test)\nplt.show()","72613dc8":"plot_confusion_matrix(random_forest, X_test_rf, y_test)  \nplt.ylabel(\"Actual\")\nplt.xlabel(\"Prediction\");","90995f66":"dataset.head()","6c44c799":"dataset.time.hist();\nplt.xlabel(\"Observation time before death or censorship (days)\");\nplt.ylabel(\"Frequency (number of patients)\");","6e551fe6":"df_censored = dataset[dataset.DEATH_EVENT == 0]\ndf_uncensored = dataset[dataset.DEATH_EVENT == 1]\n\ndf_censored.time.hist()\nplt.title(\"Censored\")\nplt.xlabel(\"Time (days)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\ndf_uncensored.time.hist()\nplt.title(\"Uncensored\")\nplt.xlabel(\"Time (days)\")\nplt.ylabel(\"Frequency\")\nplt.show()","08b76a68":"df_dev, df_test = train_test_split(dataset, test_size = 0.2)\ndf_train, df_val = train_test_split(df_dev, test_size = 0.25) ","1e24e9a7":"df_train.head()","c2b10fcc":"!pip install lifelines\n\nfrom lifelines import CoxPHFitter\nfrom lifelines.utils import concordance_index as cindex","8c4f298b":"standardize_features = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium']","dcb060ce":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndf_train[standardize_features] = sc.fit_transform(df_train[standardize_features])\ndf_val[standardize_features] = sc.transform(df_val[standardize_features])\ndf_test[standardize_features] = sc.transform(df_test[standardize_features])","d2766dd4":"df_train.describe()","cd38d7fa":"cph = CoxPHFitter()\ncph.fit(df_train, duration_col = 'time', event_col = 'DEATH_EVENT', step_size=0.01, show_progress = True)","1ff34936":"cph.print_summary()","761dd65a":"def harrell_c(y_true, scores, event):\n    '''Compute Harrel C-index given true event\/censoring times,\n    model output, and event indicators.'''\n    \n    n = len(y_true)\n    assert (len(scores) == n and len(event) == n)\n    \n    concordant = 0.0\n    permissible = 0.0\n    ties = 0.0\n    result = 0.0\n    \n    for i in range(n):\n        for j in range(i+1, n):\n            \n            # check if at most one is censored\n            if event[i] == 1 or event[j] == 1:\n            \n                # check if neither are censored\n                if event[i] == 1 and event[j] == 1:\n                    permissible += 1\n                    \n                    # check if scores are tied\n                    if (scores[i] == scores[j]):\n                        ties += 1\n                    \n                    # check for concordant\n                    elif (y_true[i] < y_true[j]) and (scores[i] > scores[j]):\n                        concordant += 1\n                    elif (y_true[i] > y_true[j]) and (scores[i] < scores[j]):\n                        concordant += 1\n                \n                # check if one is censored\n                elif event[i] != event[j]:\n                    \n                    # get censored index\n                    censored = j\n                    uncensored = i\n                    \n                    if event[i] == 0:\n                        censored = i\n                        uncensored = j\n                        \n                    # check if permissible\n                    if y_true[uncensored] <= y_true[censored]:\n                        permissible += 1\n                        \n                        # check if scores are tied\n                        if (scores[censored] == scores[uncensored]):\n                            # update ties \n                            ties += 1\n                            \n                        # check if scores are concordant \n                        if scores[uncensored] > scores[censored]:\n                            concordant += 1\n   \n    result = (concordant + (0.5 * ties)) \/ permissible    \n    return result   ","9b8e36d2":"# Train\nscores = cph.predict_partial_hazard(df_train)\ncox_train_scores = harrell_c(df_train['time'].values, scores.values, df_train['DEATH_EVENT'].values)\n# Validation\nscores = cph.predict_partial_hazard(df_val)\ncox_val_scores = harrell_c(df_val['time'].values, scores.values, df_val['DEATH_EVENT'].values)\n# Test\nscores = cph.predict_partial_hazard(df_test)\ncox_test_scores = harrell_c(df_test['time'].values, scores.values, df_test['DEATH_EVENT'].values)\n\nprint(\"Train:\", cox_train_scores)\nprint(\"Val:\", cox_val_scores)\nprint(\"Test:\", cox_test_scores)","fb75ba8f":"cph.plot_partial_effects_on_outcome('smoking', values=[0, 1]);","0e78922c":"To evaluate how good our model is performing, we will use a C-index in the survival context of the probability that, given a randomly selected pair of individuals, the one who died sooner has a higher risk score. \n\nHowever, we need to take into account censoring. Imagine a pair of patients, $A$ and $B$. \n\n#### Scenario 1\n- A was censored at time $t_A$ \n- B died at $t_B$\n- $t_A < t_B$. \n\nBecause of censoring, we can't say whether $A$ or $B$ should have a higher risk score. \n\n#### Scenario 2\nNow imagine that $t_A > t_B$.\n\n- A was censored at time $t_A$ \n- B died at $t_B$\n- $t_A > t_B$\n\nNow we can definitively say that $B$ should have a higher risk score than $A$, since we know for a fact that $A$ lived longer. \n\nTherefore, when we compute our C-index\n- We should only consider pairs where at most one person is censored\n- If they are censored, then their censored time should occur *after* the other person's time of death. \n\nThe metric we get if we use this rule is called **Harrel's C-index**.\n\nNote that in this case, being censored at time $t$ means that the true death time was some time AFTER time $t$ and not at $t$. \n- Therefore if $t_A = t_B$ and A was censored:\n    - Then $A$ actually lived longer than $B$. ","3811c047":"The c-index measures the discriminatory power of a risk score.\nIntuitively, a higher c-index indicates that the model's prediction is in agreement with the actual outcomes of a pair of patients.\nThe formula for the c-index is\n\n$$ \\mbox{cindex} = \\frac{\\mbox{concordant} + 0.5 \\times \\mbox{ties}}{\\mbox{permissible}} $$\n \nA permissible pair is a pair of patients who have different outcomes.\nA concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome.\nA tie is a permissible pair where the patients have the same risk score.","b0f0ce2b":"# Evaluate using Harrell's C-Index","359e0ecd":"# Evaluate using C-Index","c6e6bdc3":"Since for this part, we're only estimating the likelihood of death due to a heart diesease, we will not look at the time column. We drop the time column since this is a time-to-event data and time to death is unknown in real-world. ","60a26c70":"Evaluate model","c1584e65":"We see that the group with smoking has a lower survival rate at all times (the x-axis is time) compared to the treatment group.","4edde86f":"# Add interaction terms","3a0abaaa":"Earlier we saw that the classes were imbalanced and the prevalance of disease was 32%. F1 score would be an appropriate metric instead of accuracy","6bbb09b2":"# Import Packages","6c18d6ca":"# Evaluate logistic model with feature interactions","b5ef636b":"* An interaction term is the product of two variables. \n    * For example, if we have data \n    $$ x = [x_1, x_2]$$\n    * We could add the product so that:\n    $$ \\hat{x} = [x_1, x_2, x_1*x_2]$$\n    ","e1ecdf01":"The column `time` states how long the patient lived before they died or were censored.\n\nThe column `DEATH_EVENT` says whether a death was observed or not. `DEATH_EVENT` is 1 if the event is observed (i.e. the patient died) and 0 if data was censored.\n\nCensorship here means that the observation has ended without any observed event.\nFor example, let a patient be in a hospital for 100 days at most. If a patient dies after only 44 days, their event will be recorded as `time = 44` and `DEATH_EVENT = 1`. If a patient walks out after 100 days and dies 3 days later (103 days total), this event is not observed in our process and the corresponding row has `time = 100` and `DEATH_EVENT = 0`. If a patient survives for 25 years after being admitted, their data for are still `time = 100` and `DEATH_EVENT = 0`.","b8ebd7bf":"Check which features are having the most effect","bcad0d56":"# Standardize data features","5593d4b6":"# Risk Models that vary with time","621a1753":"# Censored Data\n\nWe can plot a histogram of the survival times to see in general how long cases survived before censorship or events.","ab7bb0de":"Distribution for censored and uncensored patients","108b6df5":"We can compare the predicted risk curves for any variable such as smoking. ","fed5e652":"Previously, we dropped the time column to predict the risk of death.\n\nLet's look into probability of survival past any time t for an individual patient. \nThis is a time-to-event data. In order to be able to model death risk from heart disease, we need to be able to represent the data in a form which we can process. The primary challenge is censored observations, which is a particular form of missing data. \n\nWhen we're dealing with time-to-event data, we want to answer a different question. We want to answer the question, what is the probability of survival? Not just past a fixed amount of years, but past any time t for an individual patient based on his\/her features.","8be097cf":"# Exploratory data analysis","47bed7d8":"# Cox Proportional Hazards\n\nOur goal is to build a risk score using the survival data that we have. We'll fit a Cox Proportional Hazards model to the data.\n\nCox Proportional Hazards model describes the hazard for an individual $i$ at time $t$ as \n\n$$\n\\lambda(t, x) = \\lambda_0(t)e^{\\theta^T X_i}\n$$\n\nThe $\\lambda_0$ term is a baseline hazard and incorporates the risk over time, and the other term incorporates the risk due to the individual's covariates. After fitting the model, we can rank individuals using the person-dependent risk term $e^{\\theta^T X_i}$. ","ac111190":"Evaluate model using c-index","962f7f6f":"# Train\/Test split","5cf0cc1a":"# Building Linear Risk Model","96058148":"# Evaluate model ","462ffbad":"* `y_true` is the array of actual patient outcomes, 0 if the patient does not eventually get the disease, and 1 if the patient eventually gets the disease.\n* `scores` is the risk score of each patient.  These provide relative measures of risk, so they can be any real numbers. By convention, they are always non-negative.\n* Here is an example of input data and how to interpret it:\n```Python\ny_true = [0,1]\nscores = [0.45, 1.25]\n```\n    * There are two patients. Index 0 of each array is associated with patient 0.  Index 1 is associated with patient 1.\n    * Patient 0 does not have the disease in the future (`y_true` is 0), and based on past information, has a risk score of 0.45.\n    * Patient 1 has the disease at some point in the future (`y_true` is 1), and based on past information, has a risk score of 1.25.","637e3d6a":"# Random Forest Classifier","dd79cd9a":"Since the dataset is imbalanced, we use stratification to split labels equally","f24199f5":"Import Cox proportional hazard model from lifelines package"}}