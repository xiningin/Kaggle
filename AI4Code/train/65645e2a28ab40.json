{"cell_type":{"d30327b7":"code","fca57ba6":"code","3ad2679f":"code","3cae378e":"code","9f6b8bf2":"code","69c0c6c9":"code","39114037":"code","ceb647e1":"code","4ad7471d":"markdown","11c226d2":"markdown","06c73f55":"markdown","40262840":"markdown","bb817879":"markdown","edf5aed6":"markdown","941ac809":"markdown"},"source":{"d30327b7":"\n\"\"\"Basic word2vec example.\"\"\"\n\n# \u5bfc\u5165\u4e00\u4e9b\u9700\u8981\u7684\u5e93\n\nimport collections\nimport math\nimport os\nimport random\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport PIL","fca57ba6":"\nurl = 'http:\/\/mattmahoney.net\/dc\/'\n\n\ndef maybe_download(filename, expected_bytes):\n    \"\"\"\n    \u8fd9\u4e2a\u51fd\u6570\u7684\u529f\u80fd\u662f\uff1a\n        \u5982\u679cfilename\u4e0d\u5b58\u5728\uff0c\u5c31\u5728\u4e0a\u9762\u7684\u5730\u5740\u4e0b\u8f7d\u5b83\u3002\n        \u5982\u679cfilename\u5b58\u5728\uff0c\u5c31\u8df3\u8fc7\u4e0b\u8f7d\u3002\n        \u6700\u7ec8\u4f1a\u68c0\u67e5\u6587\u5b57\u7684\u5b57\u8282\u6570\u662f\u5426\u548cexpected_bytes\u76f8\u540c\u3002\n    \"\"\"\n    if not os.path.exists(filename):\n        print('start downloading...')\n        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print('Found and verified', filename)\n    else:\n        print(statinfo.st_size)\n        raise Exception(\n            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n    return filename\n\n\n# \u4e0b\u8f7d\u8bed\u6599\u5e93text8.zip\u5e76\u9a8c\u8bc1\u4e0b\u8f7d\nfilename = maybe_download('text8.zip', 31344016)\n\n\n# \u5c06\u8bed\u6599\u5e93\u89e3\u538b\uff0c\u5e76\u8f6c\u6362\u6210\u4e00\u4e2aword\u7684list\ndef read_data(filename):\n    \"\"\"\n    \u8fd9\u4e2a\u51fd\u6570\u7684\u529f\u80fd\u662f\uff1a\n        \u5c06\u4e0b\u8f7d\u597d\u7684zip\u6587\u4ef6\u89e3\u538b\u5e76\u8bfb\u53d6\u4e3aword\u7684list\n    \"\"\"\n    with zipfile.ZipFile(filename) as f:\n        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n\nvocabulary = read_data(filename)\nprint('Data size', len(vocabulary))  # \u603b\u957f\u5ea6\u4e3a1700\u4e07\u5de6\u53f3\n# \u8f93\u51fa\u524d100\u4e2a\u8bcd\u3002\nprint(vocabulary[0:100])","3ad2679f":"# \u8bcd\u8868\u7684\u5927\u5c0f\u4e3a10\u4e07\uff08\u5373\u6211\u4eec\u53ea\u8003\u8651\u6700\u5e38\u51fa\u73b0\u768410\u4e07\u4e2a\u8bcd\uff09\n\nvocabulary_size = 100000\n\n\ndef build_dataset(words, n_words):\n    \"\"\"\n    \u51fd\u6570\u529f\u80fd\uff1a\u5c06\u539f\u59cb\u7684\u5355\u8bcd\u8868\u793a\u53d8\u6210index\n    \"\"\"\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(words).most_common(n_words - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # UNK\u7684index\u4e3a0\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary\n\n\ndata, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n                                                            vocabulary_size)\ndel vocabulary  # \u5220\u9664\u5df2\u8282\u7701\u5185\u5b58\n# \u8f93\u51fa\u6700\u5e38\u51fa\u73b0\u76845\u4e2a\u5355\u8bcd\nprint('Most common words (+UNK)', count[:5])\n# \u8f93\u51fa\u8f6c\u6362\u540e\u7684\u6570\u636e\u5e93data\uff0c\u548c\u539f\u6765\u7684\u5355\u8bcd\uff08\u524d10\u4e2a\uff09\nprint('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n# \u6211\u4eec\u4e0b\u9762\u5c31\u4f7f\u7528data\u6765\u5236\u4f5c\u8bad\u7ec3\u96c6\nprint(\"+++++++++++++++++\")\ndata_index = 0","3cae378e":"\ndef generate_batch(batch_size, cbow_window):\n    global data_index\n    assert cbow_window % 2 == 1\n    span = 2 * cbow_window + 1\n    # \u53bb\u9664\u4e2d\u5fc3word: span - 1\n    batch  = np.ndarray(shape=(batch_size, span - 1), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n\n    buffer = collections.deque(maxlen=span)\n    for _ in range(span):\n        buffer.append(data[data_index])\n        # \u5faa\u73af\u9009\u53d6 data\u4e2d\u6570\u636e\uff0c\u5230\u5c3e\u90e8\u5219\u4ece\u5934\u5f00\u59cb\n        data_index = (data_index + 1) % len(data)\n\n    for i in range(batch_size):\n        # target at the center of span\n        target = cbow_window\n        # \u4ec5\u4ec5\u9700\u8981\u77e5\u9053context(word)\u800c\u4e0d\u9700\u8981word\n        target_to_avoid = [cbow_window]\n\n        col_idx = 0\n        for j in range(span):\n            # \u7565\u8fc7\u4e2d\u5fc3\u5143\u7d20 word\n            if j == span \/\/ 2:\n                continue\n            batch[i, col_idx] = buffer[j]\n            col_idx += 1\n        labels[i, 0] = buffer[target]\n        # \u66f4\u65b0 buffer\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n\n    return batch, labels\n\n\nnum_steps = 230001\n\nbatch_size = 128\nembedding_size = 128  # Dimension of the embedding vector.\ncbow_window = 1  # How many words to consider left and right.\nnum_skips = 2  # How many times to reuse an input to generate a label.\n# We pick a random validation set to sample nearest neighbors. here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\nvalid_size = 16  # Random set of words to evaluate similarity on.\n# Only pick dev samples in the head of the distribution.\nvalid_window = 100\n# pick 16 samples from 100\nvalid_examples = np.array(random.sample(range(valid_window), valid_size \/\/ 2))\nvalid_examples = np.append(valid_examples, random.sample(range(1000, 1000+valid_window), valid_size \/\/ 2))\nnum_sampled = 64  # Number of negative examples to sample.","9f6b8bf2":"\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # Input data.\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, 2 * cbow_window])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # Variables.\n    # embedding, vector for each word in the vocabulary\n    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                                  stddev=1.0 \/ math.sqrt(embedding_size)))\n    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # Model.\n    # Look up embeddings for inputs.\n    # this might efficiently find the embeddings for given ids (traind dataset)\n    # manually doing this might not be efficient given there are 50000 entries in embeddings\n    embeds = None\n    for i in range(2 * cbow_window):\n        embedding_i = tf.nn.embedding_lookup(embeddings, train_dataset[:, i])\n        print('embedding %d shape: %s' % (i, embedding_i.get_shape().as_list()))\n        emb_x, emb_y = embedding_i.get_shape().as_list()\n        if embeds is None:\n            embeds = tf.reshape(embedding_i, [emb_x, emb_y, 1])\n        else:\n            embeds = tf.concat([embeds, tf.reshape(embedding_i, [emb_x, emb_y, 1])], 2)\n\n    assert embeds.get_shape().as_list()[2] == 2 * cbow_window\n    print(\"Concat embedding size: %s\" % embeds.get_shape().as_list())\n    avg_embed = tf.reduce_mean(embeds, 2, keep_dims=False)\n    print(\"Avg embedding size: %s\" % avg_embed.get_shape().as_list())\n\n    loss = tf.reduce_mean(tf.nn.nce_loss(nce_weights, nce_biases,\n                                         labels=train_labels,\n                                         inputs=avg_embed,\n                                         num_sampled=num_sampled,\n                                         num_classes=vocabulary_size))\n\n    # Optimizer.\n    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n    # This is because the embeddings are defined as a variable quantity and the\n    # optimizer's `minimize` method will by default modify all variable quantities\n    # that contribute to the tensor it is passed.\n    # See docs on `tf.train.Optimizer.minimize()` for more details.\n    # Adagrad is required because there are too many things to optimize\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n    # Compute the similarity between minibatch examples and all embeddings.\n    # We use the cosine distance:\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings \/ norm\n    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))","69c0c6c9":"\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    average_loss = 0\n    for step in range(num_steps):\n        batch_data, batch_labels = generate_batch(batch_size, cbow_window)\n        feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += l\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss = average_loss \/ 2000\n                # The average loss is an estimate of the loss over the last 2000 batches.\n            print('Average loss at step %d: %f' % (step, average_loss))\n            average_loss = 0\n        # note that this is expensive (~20% slowdown if computed every 500 steps)\n        if step % 10000 == 0:\n            sim = similarity.eval()\n            for i in range(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # number of nearest neighbors\n                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n                log = 'Nearest to %s:' % valid_word\n                for k in range(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log = '%s %s,' % (log, close_word)\n                print(log)\n    final_embeddings = normalized_embeddings.eval()","39114037":"\n# \u53ef\u89c6\u5316\u7684\u56fe\u7247\u4f1a\u4fdd\u5b58\u4e3a\u201ctsne1.png\u201d\n\ndef plot_with_labels(low_dim_embs, labels, filename='tsne1.png'):\n    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(label,\n                     xy=(x, y),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n\n    plt.savefig(filename)\n\n\ntry:\n    # pylint: disable=g-import-not-at-top\n    from sklearn.manifold import TSNE\n    import matplotlib\n    matplotlib.use('agg')\n    import matplotlib.pyplot as plt\n    # \u56e0\u4e3a\u6211\u4eec\u7684embedding\u7684\u5927\u5c0f\u4e3a128\u7ef4\uff0c\u6ca1\u6709\u529e\u6cd5\u76f4\u63a5\u53ef\u89c6\u5316\n    # \u6240\u4ee5\u6211\u4eec\u7528t-SNE\u65b9\u6cd5\u8fdb\u884c\u964d\u7ef4\n    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=8000)\n    # \u53ea\u753b\u51fa500\u4e2a\u8bcd\u7684\u4f4d\u7f6e\n    plot_only = 500\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n    plot_with_labels(low_dim_embs, labels)\n\nexcept ImportError:\n    print('Please install sklearn, matplotlib, and scipy to show embeddings.')","ceb647e1":"PIL.Image.open(\".\/tsne1.png\")","4ad7471d":"# \u7b2c\u4e8c\u6b65: \u5236\u4f5c\u4e00\u4e2a\u8bcd\u8868\uff0c\u5c06\u4e0d\u5e38\u89c1\u7684\u8bcd\u53d8\u6210\u4e00\u4e2aUNK\u6807\u8bc6\u7b26","11c226d2":"# \u7b2c\u56db\u6b65\uff0c\u5b9a\u4e49\u597d\u8f93\u5165\uff0c\u53d8\u91cf\uff0c\u6a21\u578b\u548c\u4f18\u5316\u5668","06c73f55":"\u8be6\u7ec6\u7b14\u8bb0\u8bf7\u89c1: [\u8fd9\u91cc](https:\/\/coggle.it\/diagram\/XXIxd7Ml_i_-r37F\/t\/word2vec%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0)","40262840":"# \u7b2c\u4e00\u6b65: \u4e0b\u8f7d\u8bed\u6599\u5e93","bb817879":"# \u7b2c\u4e09\u6b65\uff1a\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\uff0c\u7528\u4e8e\u751f\u6210cbow\u6a21\u578b\u7528\u7684batch\u3002\u8bbe\u7f6e\u53c2\u6570","edf5aed6":"# Step 6: \u53ef\u89c6\u5316","941ac809":"# \u7b2c\u4e94\u6b65\uff0c\u5f00\u59cb\u8bad\u7ec3CBOW\u6a21\u578b"}}