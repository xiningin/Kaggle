{"cell_type":{"04b17519":"code","fb71d12a":"code","89f269c2":"code","c11db9e5":"code","4f4db1db":"code","31f717da":"code","9c739b41":"code","2f14c29c":"code","edcbea47":"code","936198ef":"code","fbd95866":"code","ac5b0b08":"code","755fcf52":"code","a67b14a2":"code","b30e3f51":"code","279ce276":"code","f4aabf73":"markdown","a3011b50":"markdown","ca077d2f":"markdown","f6bf2da5":"markdown","2f6ab58e":"markdown","117a2678":"markdown","9e2ec384":"markdown","0078ed47":"markdown","d5576b0d":"markdown","7c61ee25":"markdown","e86fccb6":"markdown","27e9dfa9":"markdown","977468b4":"markdown","9c816a89":"markdown","a62eabe1":"markdown"},"source":{"04b17519":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nseed=1234\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n%config IPCompleter.use_jedi = False","fb71d12a":"# A zero rank tensor. A zero rank tensor is nothing but a single value\nx = tf.constant(5.0)\nprint(x)","89f269c2":"# We can convert any tensor object to `ndarray` by calling the `numpy()` method\ny = tf.constant([1, 2, 3], dtype=tf.int8).numpy()\nprint(f\"`y` is now a {type(y)} object and have a value == {y}\")","c11db9e5":"# Immutability check\n\n# Rank-1 tensor\nx = tf.constant([1, 2], dtype=tf.int8)\n\n# Try to modify the values\ntry:\n    x[1] = 3\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","4f4db1db":"# tf.constant(..) is no special. Let's create a tensor using a diff method\nx = tf.ones(2, dtype=tf.int8)\nprint(x)\n\ntry:\n    x[0] = 3\nexcept Exception as ex:\n    print(\"\\n\", type(ex).__name__, ex)","31f717da":"# Check all the properties of a tensor object\nprint(f\"Shape of x : {x.shape}\")\nprint(f\"Another method to obtain the shape using `tf.shape(..)`: {tf.shape(x)}\")\n\nprint(f\"\\nRank of the tensor: {x.ndim}\")\nprint(f\"dtype of the tensor: {x.dtype}\")\nprint(f\"Total size of the tensor: {tf.size(x)}\")\nprint(f\"Values of the tensor: {x.numpy()}\")","9c739b41":"# Create a tensor first. Here is another way\nx = tf.cast([1, 2, 3, 4, 5], dtype=tf.float32)\nprint(\"Original tensor: \", x)\n\nmask = x%2 == 0\nprint(\"Original mask: \", mask)\n\nmask = tf.cast(mask, dtype=x.dtype)\nprint(\"Mask casted to original tensor type: \", mask)\n\n# Some kind of operation on an tensor that is of same size \n# or broadcastable to the original tensor. Here we will simply\n# use the range object to create that tensor\ntemp = tf.cast(tf.range(1, 6) * 100, dtype=x.dtype)\n\n# Output tensor\n# Input tensor -> [1, 2, 3, 4, 5]\n# Mask -> [0, 1, 0, 1, 0]\nout = x * (1-mask) + mask * temp\nprint(\"Output tensor: \", out)","2f14c29c":"# Another way to achieve the same thing\nindices_to_update = tf.where(x % 2 == 0)\nprint(\"Indices to update: \", indices_to_update)\n\n# Update the tensor values\nupdates = [200., 400.]\nout = tf.tensor_scatter_nd_update(x, indices_to_update, updates)\nprint(\"\\nOutput tensor\")\nprint(out)","edcbea47":"# This works!\narr = np.random.randint(5, size=(5,), dtype=np.int32)\nprint(\"Numpy array: \", arr)\n\nprint(\"Accessing numpy array elements based on a  condition with irregular strides\", arr[[1, 4]])","936198ef":"# This doesn't work\ntry:\n    print(\"Accessing tensor elements based on a  condition with irregular strides\", x[[1, 4]])\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","fbd95866":"print(\"Original tensor: \", x.numpy())\n\n# Using the indices that we used for mask\nprint(\"\\nIndices to update: \", indices_to_update.numpy())\n\n# This works!\nprint(\"\\n Accesing tensor elements using gather\")\nprint(\"\\n\", tf.gather(x, indices_to_update).numpy())","ac5b0b08":"#  An example with a python list\ny = tf.convert_to_tensor([1, 2, 3])\nprint(\"Tensor from python list: \", y)\n\n#  An example with a ndarray\ny = tf.convert_to_tensor(np.array([1, 2, 3]))\nprint(\"Tensor from ndarray: \", y)\n\n#  An example with symbolic tensors\nwith tf.compat.v1.Graph().as_default():\n    y = tf.convert_to_tensor(tf.compat.v1.placeholder(shape=[None, None, None], dtype=tf.int32))\nprint(\"Tensor from python list: \", y)","755fcf52":"# String as a tensor object with dtype==tf.string\nstring = tf.constant(\"abc\", dtype=tf.string)\nprint(\"String tensor: \", string)\n\n# String tensors are atomic and non-indexable. \n# This doen't work as expected!\nprint(\"\\nAccessing second element of the string\")\ntry:\n    print(string[1])\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","a67b14a2":"# This works!\ny = [[1, 2, 3],\n     [4, 5],\n     [6]\n    ]\n\nragged = tf.ragged.constant(y)\nprint(\"Creating ragged tensor from python sequence: \", ragged)","b30e3f51":"# This won't work\nprint(\"Trying to create tensor from above python sequence\\n\")\ntry:\n    z = tf.constant(y)\nexcept Exception as ex:\n    print(type(ex).__name__, ex)","279ce276":"# Let's say you have a an array like this one\n# [[1 0 0]\n#  [0 2 0]\n#  [0 0 3]]\n# If there are too many zeros in your `huge` tensor, then it is wise to use `sparse`\n# tensors instead of `dense` one. Let's say how to create this one. We need to specify:\n# 1. Indices where our values are\n# 2. The values \n# 3. The actual shape\n\nsparse_tensor = tf.SparseTensor(indices=[[0, 0], [1, 1], [2, 2]],\n                                values=[1, 2, 3],\n                                dense_shape=[3, 3]\n                               )\nprint(sparse_tensor)\n\n# You can convert sparse tensors to dense as well\nprint(\"\\n\", tf.sparse.to_dense(sparse_tensor))","f4aabf73":"`tf.constant(..)`: This is the simplest way yet with some `gotchas` to create a tensor object. First, let's try to create a tensor with it, and then we will look at the gotchas later on.","a3011b50":"As you can see above, that the tensor object has a `shape` and a `dtype`. There are other attributes\/properties as well that are associated with a tensor object. \n\n\n1. Shape: The length (number of elements) of each of the axes of a tensor.\n2. Rank: Number of axes. For example, a matrix is a tensor of rank 2.\n3. Axis or Dimension: A particular dimension of a tensor.\n4. Size: The total number of items in the tensor.","ca077d2f":"#### Sparse tensors","f6bf2da5":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n\n---\n\n\n\n<img src=\"https:\/\/i.ytimg.com\/vi\/yjprpOoH5c8\/maxresdefault.jpg\" width=\"300\" height=\"300\" align=\"center\"\/>\n\nWelcome to the TensorFlow\/JAX tutorials. These tutorials are meant for everyone (from novice to advanced). We will start from the very basics. No rush!\nThese tutorials are meant to improve your understanding of the two frameworks TensorFlow and JAX. We will start with TensorFlow and will cover JAX side-by-side. This isn't typically a documentation-type tutorial, and these aren't meant to be the replacement. These are meant to give you insights with minimal effort. Sit tight and let's start!\n\n**Note** The tutorials will be following this format:\n1. TF Fundamentals (2-3 notebooks)\n2. JAX Fundamentals (2-3 notebooks)\n3. Advanced TF (2-3 notebooks)\n4. Advanced JAX (2-3 notebooks)","2f6ab58e":"**A few important things along with some gotchas**<br>\n1. People confuse `tf.constant(..)` with an operation that creates a `constant` tensor. There is no such relation. This is related to how we embed a node in a `tf.Graph`\n2. Any tensor in TensorFlow is **immutable** by default i.e. you cannot change the values of a tensor once created. You always create a new one. This is different from `numpy` and `pytorch` where you can actually modify the values. We will see an example on this in a bit\n3. One of the closest member to `tf.constant` is the `tf.convert_to_tensor()` method with a few difference which we will see later on\n4. `tf.constant(..)` is just one of the many ways to create a tensor. There are many other methods as well","117a2678":"## Tensors\n\nWhat is a `Tensor` anyway?<br>\nAlthough the meaning of `Tensor` is much diverse than what we typically use in ML, whenever we say `tensor` in ML, we mean that it is a **`multi-dimensional array`** where all the values have a uniform `dtype`. There are many ways to create a TF tensor. We will take a look at a few of them, a few important ones.","9e2ec384":"Not able to do assignment in Tensor objects is a bit (more than bit TBH) frustrating. What's the solution then?<br>\nThe best way that I have figured out, that has always worked for my use case is to create a mask or to use [tf.tensor_scatter_nd_update](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/tensor_scatter_nd_update). Let's look at an example.<br>\n\nOriginal tensor -> `[1, 2, 3, 4, 5]` <br>\nOutput tensor we want -> `[1, 200, 3, 400, 5]`<br>","0078ed47":"### Other kind of Tensor objects available","d5576b0d":"#### Ragged tensors\nIn short, a tensor with variable numbers of elements along some axis.","7c61ee25":"**Exercise for readers**:\n1. Create a random 10x10 sparse tensor\n2. Gather the elements that are > 5\n3. Update these elements with a value of 500","e86fccb6":"What now? If you want to extract multiple elements from a tensor with irregular strides, or not so well defined strides, then [tf.gather](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/gather) and [tf.gather_nd](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/gather_nd) are your friend. Let;s try it again! ","27e9dfa9":"Let's take a look at another interesting thing now.","977468b4":"#### String tensors","9c816a89":"That's it for part 1! We will be looking at other things in the next tutorial!<br>\n\n\n**References**:\n1. https:\/\/www.tensorflow.org\/guide\/tensor\n2. https:\/\/keras.io\/getting_started\/intro_to_keras_for_researchers\/","a62eabe1":"There is another method `tf.convert_to_tensor(..)` to create a tensor. This is very similar to `tf.constant(..)` but with a few subtle differences:<br>\n1. Whenever you pass a non tf.Tensor object like a Python list or a ndarray to an op, `convert_to_tensor(..)` is always called automaically\n2. It doesn't take `shape` as an input argument.\n3. It allows to pass even `symbolic tensors`. We will take a look at it in a bit.\n\nWhen to use `tf.convert_to_tensor(..)`? It's up to your mental model! "}}