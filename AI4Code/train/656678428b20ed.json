{"cell_type":{"26e5c6f1":"code","a6b73d9e":"code","f07d8b50":"code","60ef9e77":"code","6939f0d1":"code","6f09a83d":"code","d4b2eb96":"code","8a793d5d":"code","15ffdc18":"code","be0ba8ed":"code","fb123d83":"code","e29c31ac":"code","091d47b4":"code","f6bb3e0f":"code","82601d4e":"code","bd3557e9":"markdown","f267b0d3":"markdown","09f2f2fb":"markdown","d304770f":"markdown","c94860c5":"markdown","8724f07d":"markdown","6871e483":"markdown","ad9af92f":"markdown","699d68b4":"markdown","657c9c9d":"markdown"},"source":{"26e5c6f1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","a6b73d9e":"# pandas - data analysis library\nimport pandas as pd\n\n#scientific computing library\nimport numpy as np\n\n# data visualization library\nimport matplotlib.pyplot as plt\n\n# line required for inline charts\/plots\n%matplotlib inline\n\n# for high-level interface for drawing attractive and informative statistical graphics\nimport seaborn as sns","f07d8b50":"#read dataframe\ndf_iris = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\n\n# check first five row of dataframe\ndf_iris.head()","60ef9e77":"# get summary\ndf_iris.info()","6939f0d1":"# drop id column - data cleaning\ndf_iris = df_iris.drop(['Id'], axis = 1)","6f09a83d":"#findout no of rows for each Species. to check whether dataset is balanced or not\nprint(df_iris.groupby('Species').size())","d4b2eb96":"#Plot a pairwise relationships\nsns.pairplot(df_iris,x_vars=['SepalLengthCm','SepalWidthCm'], \n             y_vars=['PetalLengthCm','PetalWidthCm'],hue='Species')","8a793d5d":"#Defining data and label\nX = df_iris.iloc[:, :-1]\ny = df_iris.iloc[:, -1]\n\n#Split data into training and test datasets (training will be based on 70% of data)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify =y)\n\n# transform data so its distribution will have a mean value 0 and standard deviation of 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n#test_size: if integer, number of examples into test dataset; if between 0.0 and 1.0, means proportion\nprint('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))","15ffdc18":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n#create report function\ndef generateClassificationReport(y_test,y_pred):\n    print(classification_report(y_test,y_pred))\n    print(confusion_matrix(y_test,y_pred))    \n    print('accuracy is ',accuracy_score(y_test,y_pred))\n    df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n    df['result'] = np.where(df['Actual'] == df['Predicted'], 'correct', 'wrong')\n    print(df)","be0ba8ed":"#K-NEAREST NEIGHBOUR\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)","fb123d83":"from sklearn import metrics\n\n# empty variable for storing the KNN metrics\nscores=[]\n\n# We try different values of k for the KNN (from k=1 up to k=26)\nlrange=list(range(1,26,2))\n\n# loop the KNN process\nfor k in lrange:\n    # input the k value and 'distance' measure\n    knn=KNeighborsClassifier(n_neighbors=k)\n    # input the train data to train KNN\n    knn.fit(X_train,y_train)\n    # see KNN prediction by inputting the test data\n    y_pred=knn.predict(X_test)\n    # append the performance metric (accuracy)\n    scores.append(metrics.accuracy_score(y_test,y_pred))\n\noptimal_k = lrange[scores.index(max(scores))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)\nprint(\"The optimal score is %.2f\" % max(scores))\n\nplt.figure(2,figsize=(15,5))\n    \n# plot the results\nplt.plot(lrange, scores,ls='dashed')\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.xticks(lrange)\nplt.yticks(scores)\n\nplt.grid()\nplt.show()","e29c31ac":"#using 10 fold cross validation\n# empty variable for storing the KNN metrics\nscores=[]\n\n# We try different values of k for the KNN (from k=1 up to k=26)\nlrange=list(range(1,26, 2))\n\n# loop the KNN process\nfor k in lrange:\n    # input the k value and 'distance' measure\n    knn=KNeighborsClassifier(n_neighbors=k, weights='distance', algorithm='auto')\n    # get score for the 10 fold cross validation\n    score = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    scores.append(score.mean())\n\noptimal_k = lrange[scores.index(max(scores))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)\nprint(\"The optimal score is %.2f\" % max(scores))\n\nplt.figure(2,figsize=(15,5))\n    \n# plot the results\nplt.plot(lrange, scores,ls='dashed')\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.xticks(lrange)\n\nplt.grid()\nplt.show()","091d47b4":"#using hyperparameter\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'n_neighbors' : [5, 25],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}\ngrid_kn = GridSearchCV(estimator = knn,\n                        param_grid = params,\n                        scoring = 'accuracy', \n                        cv = 5, \n                        verbose = 1,\n                        n_jobs = -1)\ngrid_kn.fit(X_train, y_train)","f6bb3e0f":"# extract best estimator\nprint(grid_kn.best_estimator_)","82601d4e":"# to test the bestfit\nprint(grid_kn.score(X_test, y_test))","bd3557e9":"I use head() function to get initial overview of the dataframe.","f267b0d3":"Iris dataset is one of the basic simple and small dataset for those who want to start learning data science which including myself. This is one of my early code and it's very easy to follow.\n\nIn beginning I learned about supervised and unsupervised learning. For supervised learning, kNN is one of the famous algorithm to use and hence for this I decided to use kNN.\n\nI added hyperparameter GridSearchCV after I've learn more about Machine Learning and having much better understanding of kNN","09f2f2fb":"#### **1.2 Assign dataframe**","d304770f":"## Super beginner","c94860c5":"### **1. Exploratory Data Analysis**\n####  **1.1 Import Libraries**","8724f07d":"the visualisation helps me to understand the correlation of each attributes to the class","6871e483":"### **Split DataFrame into Train & Test**","ad9af92f":"I use info() function to get more information of the number of rows and columns, data type and missing value","699d68b4":"class is balance","657c9c9d":"# **KNN**"}}