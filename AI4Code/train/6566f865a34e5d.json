{"cell_type":{"a781bb08":"code","7cca4d5d":"code","7b39e6cb":"code","dd1d3e58":"code","851890cc":"code","aefcf412":"code","967e27fe":"code","8a6e7281":"code","eea6a5a4":"code","d952cb6a":"code","3a218291":"code","23cd9a5f":"code","27d48cc8":"code","0ac73581":"code","f9b5da47":"code","85283dcc":"code","17770f79":"code","4d9262f0":"code","f99bc721":"code","eb88a760":"code","a66e88fd":"code","fcb08b0f":"code","28d2c7c7":"code","8ce9c50a":"code","d0729307":"code","85276450":"code","6b0ebc8f":"code","95591864":"code","4b04c9cb":"code","b736f765":"code","3b0ff570":"code","6570c62f":"code","e6e0da8c":"code","a73ce060":"code","f658af54":"code","377de0f3":"code","41380d8c":"code","fcd470a9":"code","9dec5016":"code","9b320279":"code","f2288266":"code","ec596a5f":"code","e6e55910":"markdown","da36b8ac":"markdown","d61bf250":"markdown","b95376f0":"markdown","9929a42c":"markdown","9fce18d1":"markdown","3cf881e2":"markdown","cc8e326f":"markdown","12d15633":"markdown","082969d9":"markdown","54e764b3":"markdown","7116e5d4":"markdown","028c152c":"markdown","417dc029":"markdown","fe3d7f90":"markdown","078fe17c":"markdown","b169a4aa":"markdown","4af8fff2":"markdown","f9053e1c":"markdown","464499f2":"markdown","ba061942":"markdown","06b3be5a":"markdown","e6fd4799":"markdown","1a926d4c":"markdown","672b8817":"markdown","21220b44":"markdown","3a4e601c":"markdown","a8db4333":"markdown","5caeec64":"markdown","7f768378":"markdown","589f5dca":"markdown","3c416fa5":"markdown"},"source":{"a781bb08":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization advanced","7cca4d5d":"df_train = pd.read_csv('\/kaggle\/input\/dataanalyticscoaching\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/dataanalyticscoaching\/test.csv')","7b39e6cb":"df_train.head()","dd1d3e58":"df_test.head()","851890cc":"df_train.describe()","aefcf412":"df_train.isnull().sum().sum()","967e27fe":"df_test.isnull().sum().sum()","8a6e7281":"df_train['target'].value_counts()","eea6a5a4":"cat_feats = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']","d952cb6a":"fig, ax = plt.subplots(2, 4, figsize=(20,8))\nfor n, feat in enumerate(cat_feats):\n    sns.countplot(x=feat, hue='target', data=df_train, ax=ax[n\/\/4][n%4])","3a218291":"num_feats = list(set(df_train.columns) - set(cat_feats) - {'id', 'target'})\nprint(num_feats)","23cd9a5f":"fig, ax = plt.subplots(1, 5, figsize=(20,4))\nfor n, feat in enumerate(num_feats):\n    sns.distplot(df_train[df_train['target']==0][feat], ax=ax[n], label='0')\n    sns.distplot(df_train[df_train['target']==1][feat], ax=ax[n], label='1')\n    ax[n].legend()","27d48cc8":"fig, ax = plt.subplots(1,1, figsize=(10,10))\nsns.heatmap(df_train.corr(), \n            annot=True,\n            fmt='.2f',\n            cmap=sns.diverging_palette(240, 10, n=25), \n            cbar=False,\n            square=True, ax=ax)","0ac73581":"X_train = df_train[cat_feats + num_feats]\ny_train = df_train['target']","f9b5da47":"X_train.shape, y_train.shape","85283dcc":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler","17770f79":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        return X[self.attribute_names].values","4d9262f0":"num_pipeline = Pipeline([\n                            ('selector', DataFrameSelector(num_feats)),\n                            ('std_scaler', StandardScaler())\n                        ])","f99bc721":"cat_pipeline = Pipeline([\n                            ('selector', DataFrameSelector(cat_feats)),\n                        ])","eb88a760":"num_pipeline.fit_transform(X_train).shape","a66e88fd":"cat_pipeline.fit_transform(X_train).shape","fcb08b0f":"full_pipeline = FeatureUnion(transformer_list=[\n                                ('num_pipeline', num_pipeline),\n                                ('cat_pipeline', cat_pipeline)\n                            ])","28d2c7c7":"full_pipeline.fit_transform(X_train).shape","8ce9c50a":"X_prep_train = full_pipeline.fit_transform(X_train)","d0729307":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","85276450":"from sklearn.metrics import accuracy_score, classification_report","6b0ebc8f":"LR_clf = LogisticRegression()\nLR_clf.fit(X_prep_train, y_train)\n\ny_pred = LR_clf.predict(X_prep_train)\nprint(classification_report(y_train, y_pred))","95591864":"from sklearn.model_selection import cross_val_score, KFold","4b04c9cb":"kfold = KFold(n_splits=5)","b736f765":"scores = cross_val_score(LR_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\nprint('Scores:',scores)\nprint('Mean:',np.mean(scores))\nprint('Std:',np.std(scores))","3b0ff570":"from sklearn.model_selection import GridSearchCV","6570c62f":"param_grid = {\n                'C':[0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n                'max_iter':[50, 100, 150],\n             }\nLR_clf = LogisticRegression()\ngrid_search = GridSearchCV(LR_clf, param_grid, cv=kfold, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nLR_clf = grid_search.best_estimator_\nscores = cross_val_score(LR_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nLR_clf.fit(X_prep_train, y_train)\ny_pred = LR_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ndf_comp = pd.DataFrame([['Logistic Regression', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp","e6e0da8c":"param_grid = {\n                'n_neighbors': range(1, 25, 5),\n                'weights':['uniform', 'distance'],\n                'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n             }\nKNN_clf = KNeighborsClassifier()\ngrid_search = GridSearchCV(KNN_clf, param_grid, cv=kfold, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nKNN_clf = grid_search.best_estimator_\nscores = cross_val_score(KNN_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nKNN_clf.fit(X_prep_train, y_train)\ny_pred = KNN_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ntemp = pd.DataFrame([['KNN', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp = df_comp.append(temp,ignore_index=True)\ndf_comp","a73ce060":"param_grid = {\n             }\nNB_clf = GaussianNB()\ngrid_search = GridSearchCV(NB_clf, param_grid, cv=kfold, scoring='accuracy')\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nNB_clf = grid_search.best_estimator_\nscores = cross_val_score(NB_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nNB_clf.fit(X_prep_train, y_train)\ny_pred = NB_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ntemp = pd.DataFrame([['Naive Bayes', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp = df_comp.append(temp,ignore_index=True)\ndf_comp","f658af54":"param_grid = {\n                'criterion':['gini', 'entropy'],\n                'splitter':['best','random'],\n                'max_depth':[None] + list(range(1,20, 2)),\n                'max_features':[None, 'auto', 'sqrt', 'log2'],\n                'ccp_alpha':[0.0, 0.01, 0.03, 0.1]\n             }\nDT_clf = DecisionTreeClassifier()\ngrid_search = GridSearchCV(DT_clf, param_grid, cv=kfold, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nDT_clf = grid_search.best_estimator_\nscores = cross_val_score(DT_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nDT_clf.fit(X_prep_train, y_train)\ny_pred = DT_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ntemp = pd.DataFrame([['Decision Tree', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp = df_comp.append(temp,ignore_index=True)\ndf_comp","377de0f3":"param_grid = {\n                'C':[0.01, 0.03, 0.1, 0.3, 1, 3, 10],\n                'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n                'degree':list(range(1,5)),\n             }\nSVM_clf = SVC()\ngrid_search = GridSearchCV(SVM_clf, param_grid, cv=kfold, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nSVM_clf = grid_search.best_estimator_\nscores = cross_val_score(SVM_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nSVM_clf.fit(X_prep_train, y_train)\ny_pred = SVM_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ntemp = pd.DataFrame([['SVM', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp = df_comp.append(temp,ignore_index=True)\ndf_comp","41380d8c":"param_grid = {\n                'n_estimators':range(50,200,50),\n                'criterion':['gini', 'entropy'],\n                'bootstrap':[True, False],\n                'max_features':['auto', 'sqrt', 'log2'],\n                'ccp_alpha':[0.01, 0.03, 0.1]\n             }\nRF_clf = RandomForestClassifier()\ngrid_search = GridSearchCV(RF_clf, param_grid, cv=kfold, scoring='accuracy', verbose=1, n_jobs=-1)\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nRF_clf = grid_search.best_estimator_\nscores = cross_val_score(RF_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nRF_clf.fit(X_prep_train, y_train)\ny_pred = RF_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ntemp = pd.DataFrame([['Random Forest', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp = df_comp.append(temp,ignore_index=True)\ndf_comp","fcd470a9":"param_grid = {\n                'loss':['deviance', 'exponential'],\n                'learning_rate':[0.01, 0.03, 0.1, 0.3, 1],\n                'n_estimators':list(range(50,200,50)),\n                'criterion':['friedman_mse', 'mse', 'mae'],\n                'ccp_alpha':[0.01, 0.03, 0.1]\n             }\nGB_clf = GradientBoostingClassifier()\ngrid_search = GridSearchCV(GB_clf, param_grid, cv=kfold, scoring='accuracy', verbose=1, n_jobs=-1)\n\ngrid_search.fit(X_prep_train, y_train)\n\nprint(grid_search.best_params_)\n\nGB_clf = grid_search.best_estimator_\nscores = cross_val_score(GB_clf, X_prep_train, y_train, scoring='accuracy', cv=kfold)\n\nGB_clf.fit(X_prep_train, y_train)\ny_pred = GB_clf.predict(X_prep_train)\nacc = accuracy_score(y_train, y_pred)\n\ntemp = pd.DataFrame([['Grad. Boost.', np.round(acc,2), np.round(np.mean(scores),2)]], \n                       columns=['Algo', 'Train_acc', 'Val_acc'])\ndf_comp = df_comp.append(temp,ignore_index=True)\ndf_comp","9dec5016":"df_test","9b320279":"X_test = df_test[cat_feats + num_feats]\nX_prep_test = full_pipeline.fit_transform(X_test)","f2288266":"y_pred_test = SVM_clf.predict(X_prep_test)","ec596a5f":"submission = pd.DataFrame({\n        \"id\": df_test['id'],\n        \"target\": y_pred_test\n    })\n\nsubmission.to_csv('submission.csv', index=False)","e6e55910":"**5. How are the features coorelated?**","da36b8ac":"That seems great! 87% accuracy on the first attempt.\\\nLet's test though if cross-validation reveals our optimism is right or not.","d61bf250":"Let's look at some basic statistic on the data","b95376f0":"That's more on less balanced.","9929a42c":"**4. What are the numerical features?**","9fce18d1":"### Fine-tuning\nLet's fine-tune different ml algorithms and see which gives the best results","3cf881e2":"# Heart Disease Prediction - Starter Model\n\nThe purpose of this notebook is to get you started to solve this problem.\n\nThe main steps of any Data Science Project can be roughly jotted as follows:\n1. Data Exploration\n2. Data Visualization\n3. Data Preprocessing\n4. Feature Engineering\n5. Modelling\n6. Validation, Evaluation, Testing\n\nThese steps overlap in multiple places and serve only as a guide to loosely follow.","cc8e326f":"**3. What are the categorical features?**","12d15633":"The different data field are as follows:\n\n1. id - unique id\n2. age - age in years\n3. sex - (1 = male; 0 = female)\n4. cp - chest pain type (0,1,2,3)\n5. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n6. chol - serum cholestoral in mg\/dl\n7. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n8. restecg - resting electrocardiographic results (0,1,2)\n9. thalach - maximum heart rate achieved\n10. exang - exercise induced angina (1 = yes; 0 = no)\n11. oldpeak - ST depression induced by exercise relative to rest\n12. slope - the slope of the peak exercise ST segment (0,1,2)\n13. ca - number of major vessels (0-3) colored by flourosopy\n14. thal - 0 = normal; 1 = fixed defect; 2 = reversable defect\n15. condition - 0 = disease, 1 = no disease","082969d9":"The train data has different features about a patient and the target represents the presence of heart disease (0) or not (1).\nThe task is to train a machine learning model that will be able to generalize well on the training data so it can predict for unseen data i.e. test data","54e764b3":"Let's start by importing common data science packages","7116e5d4":"Great!! No missing values.","028c152c":"### SVM","417dc029":"Let's get more professional by using pipelines. \\\nThe first task is to create a class for selecting features from the Dataframe.\\\nThis is so that we can do separate handling for numerical and categorical features.","fe3d7f90":"### Logistic Regression","078fe17c":"## Data Exploration & Data Visualization","b169a4aa":"## Modelling","4af8fff2":"# Deployment","f9053e1c":"Let's also see the countplot to get a visual idea of how the values are linked with heart disease","464499f2":"### Random Forest","ba061942":"The description of the data fields helps to identify these as\n\nsex, cp, fbs, restecg, exang, slope, ca and thal\n\nLet's create a list of these features","06b3be5a":"### Preparing Data for Training","e6fd4799":"**1. Are there any missing values?**","1a926d4c":"Well, cross-validation reveals that we get a mean score of 83.4% \\\nSo our training accuracy of 87% was optimistic.","672b8817":"Let's try some models now","21220b44":"### KNN","3a4e601c":"**2. How is the target class? Balanced or not?**","a8db4333":"### Decision Tree","5caeec64":"### Naive Bayes","7f768378":"Give the present work on preprocessing, feature engineering and modelling; it seems the best model we have so far is the SVM.\\\nLet's use it to predict for the test dataset.","589f5dca":"### Gradient Boosting","3c416fa5":"## Loading the Data"}}