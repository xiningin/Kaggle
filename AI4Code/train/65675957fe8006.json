{"cell_type":{"c8436360":"code","353a271f":"code","53e022ad":"code","49c25bbc":"code","3c155446":"code","ce80afad":"code","3b2a44b3":"code","34a92c97":"code","3c4b6e25":"code","d68531c8":"code","08abcb9d":"code","8b3c255a":"code","580d7ddd":"code","c8c64268":"code","c5d86156":"code","2faf1258":"code","ad39fa9d":"code","be642298":"code","d5485d13":"code","44545d4f":"code","8faed618":"code","426dff67":"code","6aff571d":"code","14d760f1":"code","474f37ab":"code","1175c862":"code","7bf7b6c1":"code","ebc24b20":"code","a81c010d":"code","e31a5fd4":"code","05a5e6c7":"code","00d1164d":"code","426cd3f7":"code","309659cb":"code","b749964c":"code","d4c2d661":"code","025672d0":"code","9b06a28d":"code","470f457d":"code","44fb813e":"code","0dbdd5e5":"code","d459ecbf":"code","b1938dfa":"code","58216e75":"code","128f3984":"code","fd9b3678":"code","c1bc2eea":"code","f742318f":"code","130acc4f":"code","a5ab943b":"code","21c5e324":"code","9c63fc62":"code","0d5d6792":"code","5626a78d":"code","ebcf2567":"code","8a3df1da":"code","4b14c9de":"code","29e3f0f2":"code","0955c170":"code","3114fc6f":"code","920d3e71":"code","814102da":"code","35c51e2d":"code","9cf5107b":"code","82b62f8b":"code","2ec32e48":"code","9a0ea4b8":"code","ccb0dd42":"code","1b6d8d13":"code","4bbf8db2":"code","7611fed8":"code","3148a80c":"code","d5d48f7b":"code","efe2901d":"code","3c4322b0":"code","3694632c":"markdown","d8928094":"markdown","004968b4":"markdown","8bcfc773":"markdown","08c0cb0f":"markdown","1bdb56ac":"markdown","c77d8bc2":"markdown","c55f62e5":"markdown","15dcc52b":"markdown","44041f05":"markdown","a1df440e":"markdown","840f32b0":"markdown","db7ebce1":"markdown","5491ff87":"markdown","2f71c442":"markdown","3ee196f7":"markdown","eb61fa96":"markdown","125dfa33":"markdown","74d9e7b5":"markdown","f900a98f":"markdown","4cfb2bb3":"markdown","6d645447":"markdown","c8be7046":"markdown"},"source":{"c8436360":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","353a271f":"train = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/train.csv\")\nlabel = train['income']\n\ndel train['income']\n\ntest = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/test.csv\")","53e022ad":"label.head()","49c25bbc":"# \ub77c\ubca8 \uac12 \uc778\ucf54\ub529\nlabel = label.map(lambda x: 1 if x == '>50K' else 0)","3c155446":"label.head()","ce80afad":"del train['id']\ndel test['id']","3b2a44b3":"tmp_train = train.copy()\ntmp_test  = test.copy()","34a92c97":"tmp_train","3c4b6e25":"tmp_train.head()","d68531c8":"tmp_train.info()","08abcb9d":"tmp_train.describe()","8b3c255a":"tmp_test.head()","580d7ddd":"has_na_columns = ['workclass', 'occupation', 'native_country']","c8c64268":"(train[has_na_columns] == '?').sum()","c5d86156":"for c in has_na_columns:\n    tmp_train.loc[train[c] == '?', c] = train[c].mode()[0]\n    tmp_test.loc[test[c]   == '?', c] = test[c].mode()[0]","2faf1258":"(tmp_train[has_na_columns] == '?').sum()","ad39fa9d":"tmp_train['capital_gain'].plot.hist()","be642298":"tmp_train['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_gain']  = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_gain'].plot.hist()","d5485d13":"train['capital_loss'].plot.hist()","44545d4f":"tmp_train['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\ntmp_test['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntmp_train['log_capital_loss'].plot.hist()","8faed618":"tmp_train = tmp_train.drop(columns=['capital_loss', 'capital_gain'])\ntmp_test  = tmp_test.drop(columns=['capital_loss', 'capital_gain'])","426dff67":"tmp_train","6aff571d":"from sklearn.model_selection import train_test_split\n\ntmp_train, tmp_valid, y_train, y_valid = train_test_split(tmp_train, label, \n                                                          test_size=0.3,\n                                                          random_state=2020,\n                                                          shuffle=True,\n                                                          stratify=label)","14d760f1":"tmp_train.head()","474f37ab":"# \uc778\ub371\uc2a4 \ucd08\uae30\ud654\ntmp_train = tmp_train.reset_index(drop=True)\ntmp_valid = tmp_valid.reset_index(drop=True)\ntmp_test  = tmp_test.reset_index(drop=True)","1175c862":"tmp_train.head()","7bf7b6c1":"tmp_train.columns","ebc24b20":"tmp_train.dtypes","a81c010d":"tmp_train.dtypes.index, tmp_train.dtypes","e31a5fd4":"cat_columns = [c for c, t in zip(tmp_train.dtypes.index, tmp_train.dtypes) if t == 'O'] \nnum_columns = [c for c in tmp_train.columns if c not in cat_columns]\n\nprint('\ubc94\uc8fc\ud615 \ubcc0\uc218: \\n{}\\n\\n \uc218\uce58\ud615 \ubcc0\uc218: \\n{}\\n'.format(cat_columns, num_columns))","05a5e6c7":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntmp_train[num_columns] = scaler.fit_transform(tmp_train[num_columns])\ntmp_valid[num_columns] = scaler.transform(tmp_valid[num_columns])\ntmp_test[num_columns]  = scaler.transform(tmp_test[num_columns])","00d1164d":"tmp_train.describe()","426cd3f7":"tmp_valid.describe()","309659cb":"tmp_test.describe()","b749964c":"from sklearn.preprocessing import OneHotEncoder #, LabelEncoder\n\ntmp_all = pd.concat([tmp_train, tmp_valid, tmp_test])\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(tmp_all[cat_columns])","d4c2d661":"tmp_all[cat_columns]","025672d0":"ohe_columns = list()\nfor lst in ohe.categories_:\n    ohe_columns += lst.tolist()","9b06a28d":"ohe.categories_","470f457d":"ohe_columns","44fb813e":"new_train_cat = pd.DataFrame(ohe.transform(tmp_train[cat_columns]), columns=ohe_columns)\nnew_valid_cat = pd.DataFrame(ohe.transform(tmp_valid[cat_columns]), columns=ohe_columns)\nnew_test_cat  = pd.DataFrame(ohe.transform(tmp_test[cat_columns]), columns=ohe_columns)","0dbdd5e5":"new_train_cat.head()","d459ecbf":"cat_columns","b1938dfa":"tmp_train.shape","58216e75":"tmp_train = pd.concat([tmp_train, new_train_cat], axis=1)\ntmp_valid = pd.concat([tmp_valid, new_valid_cat], axis=1)\ntmp_test = pd.concat([tmp_test, new_test_cat], axis=1)\n\n# \uae30\uc874 \ubc94\uc8fc\ud615 \ubcc0\uc218 \uc81c\uac70\ntmp_train = tmp_train.drop(columns=cat_columns)\ntmp_valid = tmp_valid.drop(columns=cat_columns)\ntmp_test = tmp_test.drop(columns=cat_columns)","128f3984":"tmp_train.head()","fd9b3678":"tmp_y_train = y_train\ntmp_y_valid = y_valid","c1bc2eea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import f1_score","f742318f":"lr = LogisticRegression()\n\nlr.fit(tmp_train, tmp_y_train)\n\ny_pred = lr.predict(tmp_valid)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","130acc4f":"svc = SVC()\n\nsvc.fit(tmp_train, tmp_y_train)\n\ny_pred = svc.predict(tmp_valid)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","a5ab943b":"rf = RandomForestClassifier()\n\nrf.fit(tmp_train, tmp_y_train)\n\ny_pred = rf.predict(tmp_valid)\n\nprint(f\"RandomForest F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","21c5e324":"xgb = XGBClassifier(tree_method='gpu_hist')\n\nxgb.fit(tmp_train, tmp_y_train)\n\ny_pred = xgb.predict(tmp_valid)\n\nprint(f\"XGBoost F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","9c63fc62":"lgb = LGBMClassifier(tree_method='gpu_hist')\n\nlgb.fit(tmp_train, tmp_y_train)\n\ny_pred = lgb.predict(tmp_valid)\n\nprint(f\"LightGBM F1 Score: {f1_score(tmp_y_valid, y_pred, average='micro')}\")","0d5d6792":"def preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    for c in has_na_columns:\n        tmp_x_train.loc[x_train[c] == '?', c] = x_train[c].mode()[0]\n        tmp_x_valid.loc[x_valid[c] == '?', c] = x_valid[c].mode()[0]\n        tmp_x_test.loc[x_test[c]   == '?', c] = x_test[c].mode()[0]\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n    tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n    tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n    \n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    \n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","5626a78d":"def xgb_f1(y, t, threshold=0.5):\n    t = t.get_label()\n    y_bin = (y > threshold).astype(int) \n    return 'f1',f1_score(t, y_bin, average='micro')","ebcf2567":"from sklearn.model_selection import StratifiedKFold\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)","8a3df1da":"skf.split?","4b14c9de":"label","29e3f0f2":"val_scores = list()\noof_pred = np.zeros((test.shape[0],))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 Log Loss \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","0955c170":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","3114fc6f":"oof_pred.shape","920d3e71":"(oof_pred > 0.5).astype(int)","814102da":"lr.fit?","35c51e2d":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    lr = LogisticRegression()\n    #clf = XGBClassifier(tree_method='gpu_hist')\n\n    lr.fit(x_train, y_train)\n           \n    # \ubaa8\ub378 \ud559\uc2b5\n#    clf.fit(x_train, y_train,\n#            eval_set = [[x_valid, y_valid]], \n#            eval_metric = xgb_f1,        \n#            early_stopping_rounds = 100,\n#            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n#    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n#    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    trn_f1_score = f1_score(y_train, lr.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, lr.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n#    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    oof_pred += lr.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","9cf5107b":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    lgb = LGBMClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    lgb.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = 'logloss',        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, lgb.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, lgb.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += lgb.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","82b62f8b":"val_scores = list()\n\nnew_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(4)]\nnew_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(4)]\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    print(f\"Fold {i} Start\")\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clfs = [LogisticRegression(), \n            RandomForestClassifier(), \n            XGBClassifier(tree_method='gpu_hist'), \n            LGBMClassifier(tree_method='gpu_hist')]\n    \n    for model_idx, clf in enumerate(clfs):\n        clf.fit(x_train, y_train)\n        \n        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) \/ n_splits","2ec32e48":"new_x_train_list","9a0ea4b8":"new_x_test_list","ccb0dd42":"new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None)\n#new_label = np.concatenate([tmp_y_train, tmp_y_valid])\nnew_label = label\n\nnew_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n\nnew_train.shape, new_label.shape, new_test.shape","1b6d8d13":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n#     oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","4bbf8db2":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n#     oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","7611fed8":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clf = LGBMClassifier(tree_method='gpu_hist')\n    \n    # \ubaa8\ub378 \ud559\uc2b5\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = 'logloss',        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # \ud6c8\ub828, \uac80\uc99d \ub370\uc774\ud130 F1 Score \ud655\uc778\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# \uad50\ucc28 \uac80\uc99d F1 Score \ud3c9\uade0 \uacc4\uc0b0\ud558\uae30\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","3148a80c":"submit = pd.read_csv(\"\/kaggle\/input\/kakr-4th-competition\/sample_submission.csv\")","d5d48f7b":"submit.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)","efe2901d":"submit.head()","3c4322b0":"submit.to_csv('submission.csv', index=False)","3694632c":"#### 1) \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0 \ubaa8\ub378","d8928094":"### 4. OOF(Out-Of-Fold) \uc559\uc0c1\ube14\nk-Fold\ub97c \ud65c\uc6a9\ud574\uc11c \ubaa8\ub378 \uac80\uc99d \ubc0f \uac01 \ud3f4\ub4dc\uc758 \uacb0\uacfc\ub97c \uc559\uc0c1\ube14\ud558\ub294 OOF \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4.","004968b4":"#### 6) \uc2a4\ucf00\uc77c\ub9c1\nScikit-learn \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0 \uc788\ub294 Standard Scaler\ub97c \uc0ac\uc6a9\ud574\uc11c \uc218\uce58\ud615 \ubcc0\uc218\ub4e4\uc758 \ud45c\uc900\ud654\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.","8bcfc773":"#### 3) \uacb0\uce21\uce58 \ucc98\ub9ac\n\uc774\uc804 \ud0dc\uc9c4\ub2d8 \uac15\uc758\uc5d0\uc11c 'workclass', 'occupation', 'native_country' \uceec\ub7fc\uc5d0 \uacb0\uce21\uce58\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. <br>\n\uc77c\ubc18\uc801\uc778 \uacb0\uce21\uce58\uc640 \ub2e4\ub974\uac8c '?'\ub85c \ud45c\ud604\ub418\uc5b4\uc788\ub294 \uac12\ub4e4\uc740 \ud574\ub2f9 \uceec\ub7fc\uc758 \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\n##### \ubc94\uc8fc\ud615 \ubcc0\uc218\uc758 \uacbd\uc6b0 \uac00\uc7a5 \uac04\ub2e8\ud558\uac8c \ucd5c\ube48\uac12\uc73c\ub85c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc9c0\ub9cc, \ub2e4\ub978 \uceec\ub7fc\uc744 \ud544\ud130\ub9c1\ud574\uc11c \uacb0\uce21\uce58 \ucc98\ub9ac\ub97c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. ex) education_num \ub4f1","08c0cb0f":"### 6. \uacb0\uacfc \ub9cc\ub4e4\uae30","1bdb56ac":"#### 2) \uc11c\ud3ec\ud2b8 \ubca1\ud130 \uba38\uc2e0(rbf \ucee4\ub110)","c77d8bc2":"ID \uceec\ub7fc\uc740 \ud589\uc758 \uc2dd\ubcc4\uc790\ub85c \ud544\uc694 \uc5c6\ub294 \uceec\ub7fc\uc774\ubbc0\ub85c \uc0ad\uc81c\ud558\uaca0\uc2b5\ub2c8\ub2e4. ","c55f62e5":"### 4_1. OOF \uc559\uc0c1\ube14 \uc2e4\uc2b5 (Logic Regression, LightGBM, 20\ubd84\uac04)","15dcc52b":"#### 3) \ub79c\ub364 \ud3ec\ub808\uc2a4\ud2b8","44041f05":"\uc313\uc740 \ubaa8\ub378\uc774 \uc801\uc5b4\uc11c \uc131\ub2a5\uc774 \uc88b\uc9c0 \uc54a\uc73c\ub2c8 OOF \uc559\uc0c1\ube14\ub85c \uc608\uce21\ud55c \uac12\uc744 \uacb0\uacfc\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. ","a1df440e":"### 3. k-Fold Cross Validation\n\uba3c\uc800 1. \uc5d0\uc11c \uc815\ub9ac\ud55c \uc804\ucc98\ub9ac \ud504\ub85c\uc138\uc2a4\ub97c \ud558\ub098\uc758 \ud568\uc218\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.","840f32b0":"### 2. Scikit-Learn \ubd84\ub958 \ubaa8\ub378 \uc0ac\uc6a9\ud574\ubcf4\uae30\nScikit-Learn\uc758 \uae30\ubcf8 \ubd84\ub958 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\uac01 \ubaa8\ub378\uc758 \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc740 \ub300\ud68c \ud3c9\uac00 \uba54\ud2b8\ub9ad\uc778 f1_score\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","db7ebce1":"##### \ub370\uc774\ud130 \ucabc\uac1c\uae30, Train -> (Train, Valid)\n- train_test_split \ud30c\ub77c\ubbf8\ud130 \n    - test_size  (float): Valid(test)\uc758 \ud06c\uae30\uc758 \ube44\uc728\uc744 \uc9c0\uc815\n    - random_state (int): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \ub0b4\ubd80\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \ub09c\uc218 \uac12 (\ud574\ub2f9 \uac12\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ub9e4\ubc88 \ub2ec\ub77c\uc9d1\ub2c8\ub2e4.)\n    - shuffle     (bool): \ub370\uc774\ud130\ub97c \ucabc\uac24 \ub54c \uc11e\uc744\uc9c0 \uc720\ubb34\n    - stratify   (array): Stratify\ub780, \ucabc\uac1c\uae30 \uc774\uc804\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc744 \ucabc\uac1c\uace0 \ub098\uc11c\ub3c4 \uc720\uc9c0\ud558\uae30 \uc704\ud574 \uc124\uc815\ud574\uc57c\ud558\ub294 \uac12\uc785\ub2c8\ub2e4. \ud074\ub798\uc2a4 \ub77c\ubca8\uc744 \ub123\uc5b4\uc8fc\uba74 \ub429\ub2c8\ub2e4.\n        - ex) \uc6d0\ubcf8 Train \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\uc774 (7:3) \uc774\uc5c8\ub2e4\uba74, \ucabc\uac1c\uc5b4\uc9c4 Train, Valid(test) \ub370\uc774\ud130\uc758 \ud074\ub798\uc2a4 \ube44\uc728\ub3c4 (7:3)\uc774 \ub429\ub2c8\ub2e4. \ub2f9\uc5f0\ud788 \ubd84\ub958 \ub370\uc774\ud130\uc5d0\uc11c\ub9cc \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","5491ff87":"### 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac","2f71c442":"#### 2) \ub370\uc774\ud130 \ud655\uc778\n.head(), .describe(), .info() \ub4f1\uc758 \ud568\uc218\ub85c \ub370\uc774\ud130\ub97c \ud655\uc778\ud569\ub2c8\ub2e4. ","3ee196f7":"#### 2) 2 Stage Meta Model \ud559\uc2b5\nnew_train, new_test\uc5d0 \ub4e4\uc5b4\uc788\ub294 \ubcc0\uc218\ub294 \ubaa8\ub450 \uc218\uce58\ud615 \ubcc0\uc218\uc774\ubbc0\ub85c Standard Scaling\ub9cc \uc9c4\ud589\ud558\uaca0\uc2b5\ub2c8\ub2e4.<br>\n\uc0c8\ub85c \uc0dd\uc131\ud55c \ub370\uc774\ud130 new_train, new_test \ub370\uc774\ud130\ub97c \uac00\uc9c0\uace0 2 Stage Meta Model\uc744 \ud559\uc2b5\ud558\uace0 \uacb0\uacfc\ub97c \ub9cc\ub4ed\ub2c8\ub2e4.","eb61fa96":"#### 5) LightGBM","125dfa33":"#### 4) Log \ubcc0\ud658\ncapital_gain \ubcc0\uc218\uc640 capital_loss \ubcc0\uc218\uc758 \ubd84\ud3ec\uac00 \ud55c\ucabd\uc73c\ub85c \uce58\uc6b0\uce5c \ud615\ud0dc\uc774\ubbc0\ub85c Log \ubcc0\ud658\uc744 \ud1b5\ud574 \ubd84\ud3ec\uc758 \ud615\ud0dc\ub97c \uc870\uc815\ud574\uc8fc\uaca0\uc2b5\ub2c8\ub2e4.","74d9e7b5":"#### 1) CSV \ud30c\uc77c \ubd88\ub7ec\uc624\uae30","f900a98f":"#### 6) \uc778\ucf54\ub529\n\ubc94\uc8fc\ud615 \ubcc0\uc218\ub97c \uc218\uce58\ud615 \ubcc0\uc218\ub85c \uc778\ucf54\ub529 \ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubc94\uc8fc\ud615 \ubcc0\uc218\uc5d0\ub294 Onehot Encoding\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.","4cfb2bb3":"### 5. Stacking \uc559\uc0c1\ube14\n2 stage \uc559\uc0c1\ube14\uc778 Stacking \uc559\uc0c1\ube14 \uc785\ub2c8\ub2e4. Stacking \uc559\uc0c1\ube14\uc740 \uc218\uc2ed\uac1c\uc758 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc\ub97c \ubaa8\uc544 2 stage \ubaa8\ub378\ub85c \ud559\uc2b5 \ud6c4 \uacb0\uacfc\ub97c \ub0b4\ub294 \uc559\uc0c1\ube14 \ubc29\uc2dd\uc785\ub2c8\ub2e4.\n\n#### 1) 1 stage \uacb0\uacfc \ubaa8\uc73c\uae30\nStacking \uc559\uc0c1\ube14\uc744 \uc9c4\ud589\ud560 1 stage \ubaa8\ub378\uc758 \uacb0\uacfc(train, test)\ub97c \ubaa8\uc74d\ub2c8\ub2e4. ","6d645447":"#### 5) \ub370\uc774\ud130 \ucabc\uac1c\uae30\n##### 1. Train, Valid, Test Set\n* Train Data : \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\ub294\ub370 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \uc54c\uace0 \uc788\ub294 \ud559\uc2b5\ud560 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Valid Data : \ud559\uc2b5\ud55c \ubaa8\ub378\uc758 \uc131\ub2a5\uc744 \uac80\uc99d\ud558\ub294 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \ud559\uc2b5\ud558\uc9c0 \uc54a\uc744 \ub370\uc774\ud130, \ubaa8\ub378 \uac80\uc99d\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ub370\uc774\ud130, \uacfc\uac70 \ub370\uc774\ud130)\n* Test Data : \ud559\uc2b5\ud55c \ubaa8\ub378\ub85c \uc608\uce21\ud560 \ub370\uc774\ud130 (\ubaa8\ub378\uc774 \ubaa8\ub974\ub294 \uc608\uce21\ud560 \ub370\uc774\ud130, \ubbf8\ub798 \ub370\uc774\ud130)","c8be7046":"#### 4) XGBoost"}}