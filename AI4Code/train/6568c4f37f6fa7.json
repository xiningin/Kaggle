{"cell_type":{"6c160459":"code","f81d52a5":"code","5fbbf62b":"code","eb63434d":"code","6e624af9":"code","52dd6bde":"code","aaaa8807":"code","df23f4e0":"code","2ead9b16":"code","509fd399":"code","fb68f5bf":"code","2e73a21f":"code","5dcb01e3":"code","183d41e3":"code","81eafb36":"code","25e0e7c6":"code","84cd9462":"code","d52298bf":"code","66834d18":"code","9135af18":"code","d856d68d":"code","5ebcd17a":"code","8a29583c":"code","0ddbac68":"code","1d042835":"code","9eaf6845":"code","5ce0962e":"code","7c133c3e":"code","7c7a2a00":"code","f3fb0c68":"code","39c8e784":"code","ffcc6419":"code","163e8cc1":"code","13aaa078":"code","dec89eca":"code","ce9c9dd7":"code","3ad2a6ed":"code","27d60beb":"code","936ee9f7":"code","1a609814":"code","8f7896a7":"code","b6aa619c":"code","b1abe681":"code","a178b951":"markdown","62d6cf57":"markdown","30f899ad":"markdown","be9665d7":"markdown","a2b015ff":"markdown","c82439a9":"markdown","c961bd25":"markdown","3a442795":"markdown","0b6ff6c1":"markdown","2ca4d5e6":"markdown","72a6ce7a":"markdown","3390fd34":"markdown","9a1218f2":"markdown","40f08a39":"markdown","b60be4e3":"markdown","fd451a44":"markdown","4cd36eec":"markdown","2791cb1d":"markdown","5c3aaf1c":"markdown","a89c5a79":"markdown","4cc31604":"markdown","a45e39b8":"markdown"},"source":{"6c160459":"from __future__ import print_function\nimport os\nos.environ['TF_C_API_GRAPH_CONSTRUCTION']='0'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#general packages\nimport time\nimport datetime\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\n# plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\n%matplotlib inline\n\n#metrics\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\n\nfrom scipy.stats import norm, rankdata\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n#modeling\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\nimport tensorflow as tf\n\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D, Conv1D, MaxPool1D, GlobalMaxPooling1D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n\n# module loading settings\n%load_ext autoreload\n%autoreload 2","f81d52a5":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","5fbbf62b":"print(os.listdir('..\/input\/ieee-fraud-detection\/'))\nWDR = '..\/input\/ieee-fraud-detection\/'","eb63434d":"train_transaction = reduce_mem_usage(pd.read_csv(WDR +'train_transaction.csv'))\nprint(train_transaction.shape)\ntest_transaction = reduce_mem_usage(pd.read_csv(WDR +'test_transaction.csv'))\nprint(test_transaction.shape)\ntrain_identity = reduce_mem_usage(pd.read_csv(WDR +'train_identity.csv'))\nprint(train_identity.shape)\ntest_identity = reduce_mem_usage(pd.read_csv(WDR +'test_identity.csv'))\nprint(test_identity.shape)","6e624af9":"train = train_transaction.merge(train_identity, how='left', on='TransactionID')\ntest = test_transaction.merge(test_identity, how='left', on='TransactionID')\nprint(\"shape of train is .....\"+str(train.shape))\nprint(\"shape of test is .....\"+str(test.shape))","52dd6bde":"df_original = pd.concat([train, test],axis=0,sort=False)\ndf_original = df_original.rename(columns={\"isFraud\": \"target\"})\nprint(\"shape of df is .....\"+str(df_original.shape))","aaaa8807":"def minify_identity_df(df):\n\n    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n\n    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n\n    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n\n    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n\n    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n\n    df['id_34'] = df['id_34'].fillna(':0')\n    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n    \n    df['id_33'] = df['id_33'].fillna('0x0')\n    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n\n    df['id_33'] = df['id_33'].fillna('unseen_before_label')  \n    df['DeviceType'].map({'desktop':1, 'mobile':0})\n    \n    return df","df23f4e0":"#minify\nminify_identity_df(df_original)\n\n#change missing\n#id_var = [col for col in df_original if col.startswith('id')]\n#for col in id_var:\n#    df_original[col] = df_original[col].fillna(df_original[col].mode()[0])\n#    df_original[col] = df_original[col].astype('category')","2ead9b16":"#Filter out columns\ntarget = df_original['target']\ndf_original = df_original.loc[:, df_original.isnull().mean() < .05]\ndf_original['target'] = target.values\nprint(\"shape of filtered df is .....\"+str(df_original.shape))","509fd399":"def corret_card_id(x):\n    x=x.replace('.0','')\n    x=x.replace('-999','nan')\n    return x\n\ndef define_indices(df):\n    # create date column\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['date'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['dow'] = df['date'].dt.dayofweek\n    df['day'] = df['date'].dt.day\n    df['hour'] = df['date'].dt.hour\n    # create card ID \n    cards_cols= ['card1', 'card2', 'card3', 'card5']\n    df['Card_ID'] = df[cards_cols].astype(str).sum(axis=1)\n    \n    # sort train data by Card_ID and then by transaction date \n    df= df.sort_values(['Card_ID', 'date'], ascending=[True, True])\n    \n    # small correction of the Card_ID\n    df['Card_ID']=df['Card_ID'].apply(corret_card_id)\n    \n    # set indexes\n    #df = df.reset_index()\n    return df","fb68f5bf":"#create Card_ID\ndf_original = define_indices(df_original)","2e73a21f":"def encoder(df, numerics_only, categorical_only):\n    \n    #Numeric Encoding\n    for col in numerics_only:\n        df[col] = (df[col].fillna(-99))\n        df[col] = df[col].astype(float)\n        df[col] = df[col].fillna(df[col].mean())\n        \n    # Label Encoding\n    for col in categorical_only:\n        df[col] = df[col].fillna(df[col].mode()[0])\n        df[col] = df[col].astype('category')\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[col].values))\n        df[col] = lbl.transform(list(df[col].values))\n        \n    return df","5dcb01e3":"#subset variables\n#all usuable features\nfeatures = [f for f in df_original if f not in ['target','TransactionID','TransactionDT','date',\n                                                'Card_ID','year','month','dow','day','hour']]\n#numeric\ndf_numerics_only = [col for col in df_original[features].select_dtypes(include=[np.number]) \n                    if not col.startswith('id')]\n#cat\ndf_categorical_only = [col for col in df_original[features].columns if col not in df_numerics_only]\n\n#encode df\ndf_original[features] = encoder(df_original[features], df_numerics_only, df_categorical_only)\ndf = df_original","183d41e3":"del df_original, train, test, test_identity, train_identity, train_transaction, test_transaction\ngc.collect()\n%whos DataFrame","81eafb36":"df_categorical_only.append('Card_ID')\nfor var in (df[df_categorical_only]):\n    print(var)\n    df['weight_' + var] = df.groupby([var])[var].transform('count')\n    df['binary_' + var] = df['weight_' + var].apply(lambda x: 1 if x > 1 else 0) * df[var]","25e0e7c6":"#Select features\ndf_numerics_only = [col for col in df[features].select_dtypes(include=[np.number]) if not col.startswith('id')]","84cd9462":"df_numerics_only","d52298bf":"#Guass transformation\nfrom scipy.special import erfinv\nfor col in df_numerics_only:\n    values = sorted(set(df[col]))\n    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n    f = np.sqrt(2) * erfinv(f)\n    f -= f.mean()\n    df[col] = df[col].map(f)\n    df[col] = df[col].round(4)","66834d18":"print(df.head(10))","9135af18":"#define uniform batch size\nfrom fractions import gcd\ngcb = []\n\nfor i in range(32,128):\n    gcb.append(gcd(df[df_numerics_only].shape[0],i))\n    \ngcb.sort(reverse=True)\ngcb = [nom for nom in gcb if nom != df[df_numerics_only].shape[0]]\nbatch = gcb[0]\nprint(batch)","d856d68d":"#set params\nlen_input_columns, len_data = df[df_numerics_only].shape[1], df[df_numerics_only].shape[0]\nkernel_initializer=keras.initializers.lecun_normal()\nNUM_GPUS=1\nlearning_rate = 0.001\nnb_epoch = int(10)\ndcy = learning_rate \/ nb_epoch","5ebcd17a":"#create probability function\ndef Hbeta(D, beta):\n    P = np.exp(-D * beta)\n    sumP = np.sum(P)\n    H = np.log(sumP) + beta * np.sum(np.multiply(D, P)) \/ sumP\n    P = P \/ sumP\n    return H, P\n\ndef x2p(X, u=15, tol=1e-4, print_iter=500, max_tries=50, verbose=0):\n    # Initialize some variables\n    n = X.shape[0]                     # number of instances\n    P = np.zeros((n, n))               # empty probability matrix\n    beta = np.ones(n)                  # empty precision vector\n    logU = np.log(u)                   # log of perplexity (= entropy)\n    \n    # Compute pairwise distances\n    if verbose > 0: print('Computing pairwise distances...')\n    sum_X = np.sum(np.square(X), axis=1)\n    # note: translating sum_X' from matlab to numpy means using reshape to add a dimension\n    D = sum_X + sum_X[:,None] + -2 * X.dot(X.T)\n\n    # Run over all datapoints\n    if verbose > 0: print('Computing P-values...')\n    for i in range(n):\n        \n        if verbose > 1 and print_iter and i % print_iter == 0:\n            print('Computed P-values {} of {} datapoints...'.format(i, n))\n        \n        # Set minimum and maximum values for precision\n        betamin = float('-inf')\n        betamax = float('+inf')\n        \n        # Compute the Gaussian kernel and entropy for the current precision\n        indices = np.concatenate((np.arange(0, i), np.arange(i + 1, n)))\n        Di = D[i, indices]\n        H, thisP = Hbeta(Di, beta[i])\n        \n        # Evaluate whether the perplexity is within tolerance\n        Hdiff = H - logU\n        tries = 0\n        while abs(Hdiff) > tol and tries < max_tries:\n            \n            # If not, increase or decrease precision\n            if Hdiff > 0:\n                betamin = beta[i]\n                if np.isinf(betamax):\n                    beta[i] *= 2\n                else:\n                    beta[i] = (beta[i] + betamax) \/ 2\n            else:\n                betamax = beta[i]\n                if np.isinf(betamin):\n                    beta[i] \/= 2\n                else:\n                    beta[i] = (beta[i] + betamin) \/ 2\n            \n            # Recompute the values\n            H, thisP = Hbeta(Di, beta[i])\n            Hdiff = H - logU\n            tries += 1\n        \n        # Set the final row of P\n        P[i, indices] = thisP\n        \n    if verbose > 0: \n        print('Mean value of sigma: {}'.format(np.mean(np.sqrt(1 \/ beta))))\n        print('Minimum value of sigma: {}'.format(np.min(np.sqrt(1 \/ beta))))\n        print('Maximum value of sigma: {}'.format(np.max(np.sqrt(1 \/ beta))))\n    \n    return P, beta\n\ndef compute_joint_probabilities(samples, batch_size=5000, d=2, perplexity=30, tol=1e-5, verbose=0):\n    v = d - 1\n    \n    # Initialize some variables\n    n = samples.shape[0]\n    batch_size = min(batch_size, n)\n    \n    # Precompute joint probabilities for all batches\n    if verbose > 0: print('Precomputing P-values...')\n    batch_count = int(n \/ batch_size)\n    P = np.zeros((batch_count, batch_size, batch_size))\n    for i, start in enumerate(range(0, n - batch_size + 1, batch_size)):   \n        curX = samples[start:start+batch_size]                   # select batch\n        P[i], beta = x2p(curX, perplexity, tol, verbose=verbose) # compute affinities using fixed perplexity\n        P[i][np.isnan(P[i])] = 0                                 # make sure we don't have NaN's\n        P[i] = (P[i] + P[i].T) # \/ 2                             # make symmetric\n        P[i] = P[i] \/ P[i].sum()                                 # obtain estimation of joint probabilities\n        P[i] = np.maximum(P[i], np.finfo(P[i].dtype).eps)\n\n    return P","8a29583c":"# P is the joint probabilities for this batch (Keras loss functions call this y_true)\n# activations is the low-dimensional output (Keras loss functions call this y_pred)\ndef tsne(P, activations):\n    d = int(model.get_layer('Output').get_output_at(0).get_shape().as_list()[1])#need to find a way to automate\n    d = 5 \n    n = batch \n    v = d - 1.\n    eps = K.variable(10e-15) # needs to be at least 10e-8 to get anything after Q \/= K.sum(Q)\n    sum_act = K.sum(K.square(activations), axis=1)\n    Q = K.reshape(sum_act, [-1, 1]) + -2 * K.dot(activations, K.transpose(activations))\n    Q = (sum_act + Q) \/ v\n    Q = K.pow(1 + Q, -(v + 1) \/ 2)\n    Q *= K.variable(1 - np.eye(n))\n    Q \/= K.sum(Q)\n    Q = K.maximum(Q, eps)\n    C = K.log((P + eps) \/ (Q + eps))\n    C = K.sum(P * C)\n    return C","0ddbac68":"from keras.callbacks import LearningRateScheduler\nimport math\nfrom math import exp\n\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(drop,  \n           math.floor((1+epoch)\/epochs_drop))\n    return lrate\n#lrate = LearningRateScheduler(step_decay)\n        \n#def exp_decay(epoch):\n#    initial_lrate = 0.1\n#    k = 0.1\n#    t = epoch\n#    lrate = initial_lrate * exp(-k*t)\n#    return lrate\n#lrate = LearningRateScheduler(exp_decay)\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.lr = []\n \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n#        self.lr.append(exp_decay(len(self.losses)))\n        self.lr.append(step_decay(len(self.losses)))","1d042835":"#build model\nmodel = Sequential()\nmodel.add(Dense(units=len_input_columns, dtype='float32', name='Input', input_shape=(len_input_columns,)))\nmodel.add(Dense(units=500, activation='relu', dtype='float32', name='Hidden1', kernel_initializer=kernel_initializer))\nmodel.add(Dense(units=500, activation='relu', dtype='float32', name='Hidden2', kernel_initializer=kernel_initializer))\nmodel.add(Dense(units=2000, activation='relu', dtype='float32', name='Hidden3', kernel_initializer=kernel_initializer))\nmodel.add(Dense(units=5, activation='linear', dtype='float32', name='Output', kernel_initializer=kernel_initializer))\n\nsgd = SGD(lr=0.1)\nmodel.compile(metrics=['accuracy'], loss=tsne, optimizer=sgd)\n\nprint(\"GPU MODEL\")\nprint(model.summary())","9eaf6845":"#define callbacks\ncp = ModelCheckpoint(filepath=\"tsne_0.h5\",save_best_only=True,verbose=0)\ncpnn = ModelCheckpoint(filepath=\"ae_0.h5\",save_best_only=True,verbose=0)\ntb = TensorBoard(log_dir='.\/logs',histogram_freq=0,write_graph=True,write_images=True)\nrl = tf.keras.callbacks.ReduceLROnPlateau(monitor='acc', patience=1, verbose=1)\nes = tf.keras.callbacks.EarlyStopping(monitor='acc', patience=5, verbose=1)\nloss_history = LossHistory()\nlrate = LearningRateScheduler(step_decay)","5ce0962e":"#compute probability\nP = compute_joint_probabilities(df[df_numerics_only].values, batch_size=batch, d = int(model.get_layer('Output').get_output_at(0).get_shape().as_list()[1]), verbose=0)\nY_train = P.reshape(df[df_numerics_only].shape[0], -1)\nY_train.shape","7c133c3e":"#train model\nmodel.fit(df[df_numerics_only].values, Y_train, batch_size=batch, shuffle=False, epochs=nb_epoch,\nverbose=1, callbacks=[cpnn,tb,es,loss_history,lrate])\n\n#plot weights\nplt.hist(model.get_weights(), bins = 100)\nplt.show()","7c7a2a00":"#make prediction\nreconstruction = pd.DataFrame(model.predict(df[df_numerics_only].values))\nreconstruction = reconstruction.reset_index(drop=True)\nprint(\"reconstruction values .....\")\nprint(reconstruction.head(10))\n\n#join data\ntarget = df['target']\ndf = df.reset_index(drop=True)\ndf = pd.concat([df[features], reconstruction], axis=1)\ndf = df.rename(index=str, columns={0: \"tsne_0\", 1: \"tsne_1\", 2: \"tsne_2\", 3: \"tsne_3\", 4: \"tsne_4\",})\ndf['target'] = target.values\nprint(\"df shape .....\"+str(df.shape))\n\n#del recon\ndel reconstruction\ngc.collect()","f3fb0c68":"for feature in df.filter(like='tsne').columns:\n    df['mean_'+feature] = (df[feature].mean()-df[feature])\n    df['zscore_'+feature] = (df[feature] - df[feature].mean())\/df[feature].std(ddof=0)\n    df['sq_'+feature] = (df[feature])**2\n    df['sqrt_'+feature] = np.abs(df[feature])**(1\/2)\n    \n    #these need work\n    #df['cp_'+feature] = pd.DataFrame(rankdata(df[feature]))\n    #df['cnp_'+feature] = pd.DataFrame((norm.cdf(df[feature])))\n    #df['zscorecountarctan'+feature]=(np.arctan(df['weight_'+features])2\/np.pi)df['zscore_'+feature]\n    #df['zscorecount' +feature] = df['zscore_'+feature] * ((df['weight_'+feature]-df['weight_'+feature].min())\/((df['weight_'+feature].max()-df['weight_'+feature].min())*8+1))","39c8e784":"#Select features\ntraining_features = [f for f in df if f not in ['target', 'TransactionID', 'TransactionDT', 'date', 'Card_ID']]","ffcc6419":"#split data\ntrain = df[df['target'].notnull()]\ntrain['target'] = train['target'].astype(int)\ntest = df[df['target'].isnull()]\n\n#check data\nprint(\"df shape .....\"+str(df.shape))\nprint(\"train shape .....\"+str(train.shape))\nprint(\"test shape .....\"+str(test.shape))\nprint(\"df .....\")\nprint(df.head(5))\nprint(\"train .....\")\nprint(train.head(5))\n\ndel df\ngc.collect()","163e8cc1":"def plot_model(embedding, labels):\n    fig = plt.figure(figsize=(8,8))\n    plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=labels)\n    fig.tight_layout()\n    \nfrom matplotlib.lines import Line2D\ndef plot_differences(embedding, actual, lim=1000):\n    fig = plt.figure(figsize=(12,12))\n    ax = fig.gca()\n    for a, b in zip(embedding, actual)[:lim]:\n        ax.add_line(Line2D((a[0], b[0]), (a[1], b[1]), linewidth=1))\n    ax.autoscale_view()\n    plt.show()","13aaa078":"#make prediction on train\ntrain_reconstruction = pd.DataFrame(model.predict(train[df_numerics_only].values))\ntrain_reconstruction = train_reconstruction.reset_index(drop=True)\nprint(train_reconstruction.head(10))\n\n#plot reconstruction\nplot_model(train_reconstruction.values, train['target'].values)\n\ndel train_reconstruction\ngc.collect()","dec89eca":"#set archtype\nnb_folds = 2\nruns_per_fold = 2 #bagging on each fold\nnb_epoch = int(100)\nkfolds = StratifiedKFold(n_splits=nb_folds, shuffle=True, random_state=42)\n#kfolds = TimeSeriesSplit(n_splits=nb_folds)\n#kfolds = BlockingTimeSeriesSplit(n_splits=nb_folds)\n\n#create empty dataframes\nfeatures_importance = pd.DataFrame({'Feature':[], 'Importance':[], 'Fold':[], 'Aug':[]})\noof = np.zeros(shape=(np.shape(train)[0], 1))\npredict = np.zeros(shape=(np.shape(test)[0], 1))\npredictions = np.zeros(shape=(np.shape(test)[0], nb_folds))\noof_lgbm = np.zeros(len(train))\navg_roc = np.zeros(1)\ncv_roc = []","ce9c9dd7":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (2, 300), \n    'min_data_in_leaf': (2, 300),\n    'bagging_fraction' : (0.1,0.9),\n    'feature_fraction' : (0.1,0.9),\n    'learning_rate': (0.003, 0.01),\n    'min_child_weight': (0.00001, 0.01),   \n    'min_child_samples':(100, 500), \n    'subsample': (0.2, 0.8),\n    'colsample_bytree': (0.4, 0.6), \n    'reg_alpha': (0, 2), \n    'reg_lambda': (0, 2),\n    'max_depth':(1,100)\n    }","3ad2a6ed":"# define roc_callback, inspired by https:\/\/github.com\/keras-team\/keras\/issues\/6050#issuecomment-329996505\ndef auc_roc(y_true, y_pred):\n    # any tensorflow metric\n    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n\n    # find all variables created for this metric\n    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('\/')[1]]\n\n    # Add metric variables to GLOBAL_VARIABLES collection.\n    # They will be initialized for new session.\n    for v in metric_vars:\n        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n\n    # force to update metric values\n    with tf.control_dependencies([update_op]):\n        value = tf.identity(value)\n        return value\n    \ndef gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() \/ totalLosses\n\n    giniSum -= (len(actual) + 1) \/ 2.\n    return giniSum \/ len(actual)\n\ndef gini_normalizedc(actual, pred):\n    return gini(actual, pred) \/ gini(actual, actual)","27d60beb":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","936ee9f7":"def LGB_Grid(trn_data, dev_data, X_dev,  y_dev):\n    #base LGBM \n    \"\"\"Apply Bayesian Optimization to LGBM parameters.\"\"\"\n    \n    init_points = 3\n    n_iter = 3\n    score = np.zeros(1)\n    num_round = 100\n    oof_lgb = np.zeros(len(X_dev))\n    \n    def LGB_bayesian(\n        learning_rate,\n        num_leaves, \n        bagging_fraction,\n        feature_fraction,\n        min_child_samples, \n        min_child_weight,\n        subsample, \n        min_data_in_leaf,\n        max_depth,\n        colsample_bytree,\n        reg_alpha,\n        reg_lambda\n         ):\n\n        # LightGBM expects next three parameters need to be integer. \n        num_leaves = int(num_leaves)\n        min_data_in_leaf = int(min_data_in_leaf)\n        max_depth = int(max_depth)\n\n        assert type(num_leaves) == int\n        assert type(min_data_in_leaf) == int\n        assert type(max_depth) == int\n\n        param = {\n                  'num_leaves': num_leaves, \n                  'min_child_samples': min_child_samples, \n                  'min_data_in_leaf': min_data_in_leaf,\n                  'min_child_weight': min_child_weight,\n                  'bagging_fraction' : bagging_fraction,\n                  'feature_fraction' : feature_fraction,\n                  'learning_rate' : learning_rate,\n                  'subsample': subsample, \n                  'max_depth': max_depth,\n                  'colsample_bytree': colsample_bytree,\n                  'reg_alpha': reg_alpha,\n                  'reg_lambda': reg_lambda,\n                  'objective': 'binary',\n                  'save_binary': True,\n                  'seed': 420,\n                  'feature_fraction_seed': 420,\n                  'bagging_seed': 420,\n                  'drop_seed': 420,\n                  'data_random_seed': 420,\n                  'boosting_type': 'gbdt',\n                  'verbose': 1,\n                  'is_unbalance': True,\n                  'boost_from_average': False,\n                  'metric':'auc'\n                  }   \n\n        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, dev_data], verbose_eval=False, early_stopping_rounds=200)\n        oof_lgb = clf.predict(X_dev, num_iteration=clf.best_iteration)\n        score = gini_normalizedc(y_dev, oof_lgb)\n        #score = auc_roc(y_dev, oof_lgb)\n        #print(score)\n        auc_score = score\n        return auc_score\n    \n    LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB)\n    print(LGB_BO.space.keys)\n    \n    print('-' * 130)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n        \n    print(LGB_BO.max['target'])\n    return(LGB_BO.max['params'])","1a609814":"#run grid\nfor fold_, (trn_idx, dev_idx) in enumerate(kfolds.split(train[training_features], train['target'])):  \n    \n    #manipulate data\n    #train_df = pd.concat([aggregate_transactions(time_train.iloc[trn_idx]), aggregate_transactions(time_train.iloc[dev_idx])])\n    trn = train[training_features].iloc[trn_idx]\n    dev = train[training_features].iloc[dev_idx]\n    \n    #grab targets\n    trn_target = train['target'].iloc[trn_idx]\n    dev_target = train['target'].iloc[dev_idx]\n    \n    #encode data    \n    #trn[encoder_features] = encoder(trn)\n    #dev[encoder_features] = encoder(dev)\n    \n    #create train\/dev splits\n    #trn_data = lgb.Dataset(trn, label=trn_target)\n    #dev_data = lgb.Dataset(dev, label=dev_target)\n    X_train, y_train = trn, trn_target\n    X_dev, y_dev = dev, dev_target\n    \n    #upsampling adapted from kernel: \n    #https:\/\/www.kaggle.com\/ogrellier\/xgb-classifier-upsampling-lb-0-283\n    pos = (pd.Series(y_train == 0))\n    \n    # Add positive examples\n    X_train = pd.concat([X_train, X_train.loc[pos]], axis=0)\n    y_train = pd.concat([y_train, y_train.loc[pos]], axis=0)\n    \n    # Shuffle data\n    idx = np.arange(len(X_train))\n    np.random.shuffle(idx)\n    X_train = X_train.iloc[idx]\n    y_train = y_train.iloc[idx]\n \n    #run augmentation\n    predictions_lgb = np.zeros(len(test))\n    roc = np.zeros(1)\n    p_valid,yp = 0,0\n    \n    for i in range(runs_per_fold):\n        print('-')\n        print(\"Fold {}\".format(fold_ + 1))\n        print(\"Augment {}\".format(i + 1))\n        X_t, y_t = augment(X_train.values, y_train.values)\n        X_t = pd.DataFrame(X_t)\n        X_t = X_t.add_prefix('var_')\n    \n        #run probability grid\n        trn_data = lgb.Dataset(X_t, label=y_t)\n        dev_data = lgb.Dataset(X_dev, label=y_dev)\n        best_param =  LGB_Grid(trn_data, dev_data, X_dev,  y_dev)\n\n        param_lgb = {\n            'min_data_in_leaf': int(best_param['min_data_in_leaf']), \n            'num_leaves': int(best_param['num_leaves']), \n            'learning_rate': best_param['learning_rate'],\n            'min_child_weight': best_param['min_child_weight'],\n            'colsample_bytree' : best_param['colsample_bytree'],\n            'bagging_fraction': best_param['bagging_fraction'], \n            'min_child_samples': best_param['min_child_samples'],\n            'subsample': best_param['subsample'],\n            'reg_lambda': best_param['reg_lambda'],\n            'reg_alpha': best_param['reg_alpha'],\n            'max_depth': int(best_param['max_depth']), \n            'objective': 'binary',\n            'save_binary': True,\n            'seed': 420,\n            'feature_fraction_seed': 420,\n            'bagging_seed': 420,\n            'drop_seed': 420,\n            'data_random_seed': 420,\n            'boosting_type': 'gbdt',\n            'verbose': 1,\n            'is_unbalance': True,\n            'boost_from_average': False,\n            'metric':'auc'\n        }\n        #train best model and predict\n        clf = lgb.train(param_lgb, trn_data, nb_epoch, valid_sets = [trn_data, dev_data], verbose_eval=100, early_stopping_rounds=200)\n        oof_lgbm[dev_idx] += clf.predict(X_dev, num_iteration=clf.best_iteration) \/ runs_per_fold\n        predictions[:, fold_] += clf.predict(test[training_features ], num_iteration=clf.best_iteration) \/ runs_per_fold\n        #roc += roc_auc_score(dev_target.values, oof_lgbm[dev_idx]) \/ runs_per_fold\n        roc += gini_normalizedc(dev_target.values, oof_lgbm[dev_idx]) \/ runs_per_fold\n\n        #pull best features\n        fold_importance = pd.DataFrame({'Feature':[], 'Importance':[]})\n        fold_importance['Feature']= training_features\n        fold_importance['Importance']= clf.feature_importance()\n        fold_importance[\"Fold\"] = fold_ + 1\n        fold_importance[\"Aug\"] = i + 1\n        features_importance = pd.concat([features_importance, fold_importance], axis=0)\n    \n    #aggregate all folds into final scores\n    print('Round AUC score %.6f' % roc)\n    avg_roc += roc \/ nb_folds\n\n#average out features\nfeatures_importance = features_importance.groupby('Feature').mean().sort_values(by = ['Importance'], ascending=False).drop(columns=['Fold', 'Aug'])\nfeatures_importance = features_importance.reset_index()\nfeatures_importance = features_importance[features_importance.Importance >= np.percentile(features_importance.Importance, 50)]\n\n#save predictions\ntest_predictions = np.mean(predictions, axis=1)\nnp.save('predictions', test_predictions)\nprint('Full AUC score %.6f' % avg_roc)","8f7896a7":"import seaborn as sns\nfrom matplotlib import pyplot as plt\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"Feature\", \"Importance\"]]\n    best_features = feature_importance_df_[[\"Feature\", \"Importance\"]][:100]\n    best_features.reset_index(inplace=True)\n    print(best_features.dtypes)\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"Importance\", y=\"Feature\", data=best_features)\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()","b6aa619c":"display_importances(features_importance)","b1abe681":"sample_submission = pd.DataFrame()\nsample_submission['TransactionID'] = test['TransactionID']\nsample_submission['isFraud'] = test_predictions\nsample_submission.to_csv('submission.csv', index=False)","a178b951":"Clean up workspace","62d6cf57":"Build model","30f899ad":"Plot T-SNE","be9665d7":"Let's Create the functions we will use for the grid","a2b015ff":"Filter out columns","c82439a9":"Generate more features","c961bd25":"Define grid architecture","3a442795":"Set working directory","0b6ff6c1":"Join Data","2ca4d5e6":"Run the grid!","72a6ce7a":"Define Card_ID","3390fd34":"Scale data","9a1218f2":"Plot the features","40f08a39":"Define T-SNE","b60be4e3":"Generate counts","fd451a44":"Submit predictions","4cd36eec":"I wanted to mix it up a little bit and create my own T-SNE using Keras.","2791cb1d":"Encode","5c3aaf1c":"Split into train\/test","a89c5a79":"Clean ID columns","4cc31604":"Load data","a45e39b8":"Train T-SNE"}}