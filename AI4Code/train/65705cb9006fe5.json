{"cell_type":{"8bd87c89":"code","be897293":"code","857fa669":"code","fc994e14":"code","6d29a442":"code","695843a1":"code","a148b822":"code","57a5acd5":"code","332576d3":"code","5a183e5d":"code","37d8d357":"code","77eb6be3":"code","518ed876":"code","42290ae2":"code","f1c8fea9":"code","7b9024eb":"code","6f15d2dc":"code","dcd8d789":"code","1bbb0bb7":"code","29423a76":"code","84735327":"code","ae929f7e":"code","9d64d17f":"markdown","20de4676":"markdown","1e4e42cd":"markdown","e84437d5":"markdown","d320752d":"markdown","d5cf2941":"markdown","86b98284":"markdown","1f64104b":"markdown","8b543e0f":"markdown","340b39d8":"markdown","0abdbc3d":"markdown","f92445ea":"markdown","0ef34200":"markdown","eef00482":"markdown","0ade8e5b":"markdown","2a4662f6":"markdown"},"source":{"8bd87c89":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\nsns.set_style(\"white\")","be897293":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, classification_report","857fa669":"dataset = load_breast_cancer()\ndf = pd.DataFrame(dataset.data, columns=dataset.feature_names)\ndf['target'] = dataset.target\ndf.head()","fc994e14":"# check for null values\ndf.isna().sum()","6d29a442":"X = df.drop('target', axis='columns')\ny = df.target","695843a1":"print(X.shape)\nprint(y.shape)","a148b822":"X.info()","57a5acd5":"X.describe()","332576d3":"#Check distribution of classes in target\nsns.countplot(df['target'],label='count')","5a183e5d":"y.value_counts()","37d8d357":"# distribution of every feature \nfor column in X:\n    plt.figure()\n    column_data = X[column]\n    sns.kdeplot(column_data[y == 0], label='B')\n    sns.kdeplot(column_data[y == 1], label='M')\n    plt.xlabel(column)\n    plt.ylabel('Frequency')\n    plt.title('Distribution of {}'.format(column))","77eb6be3":"# Calculate and visualise correlations between features\nplt.figure(figsize=(20, 16))\nsns.heatmap(X.corr(), annot=True, cmap=\"YlGnBu\")","518ed876":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)","42290ae2":"pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=2))\n])\nX_pca = pipe.fit_transform(X_train, y_train)\nsns.scatterplot(X_pca[:,0], X_pca[:,1], hue=y_train.map({0:'M', 1:'B'}))\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.title('First two principal components of dataset')","f1c8fea9":"knn_pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA()),\n    ('knn', KNeighborsClassifier())\n])\nparam_grid = {\n    'pca__n_components': np.arange(1, X_train.shape[1]+1),\n    'knn__n_neighbors': np.arange(1, X_train.shape[1], 2)\n}\nknn_model = GridSearchCV(knn_pipe, param_grid=param_grid, verbose=1, n_jobs=-1)\nknn_model.fit(X_train, y_train)\n\nprint('Best params: {}'.format(knn_model.best_params_))\nprint('Training Score: {}'.format(knn_model.score(X_train, y_train)))\nprint('CV Score: {}'.format(knn_model.best_score_))\nprint('Test Score: {}'.format(knn_model.score(X_test, y_test)))","7b9024eb":"gnb_pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA()),\n    ('gnb', GaussianNB())\n])\nparam_grid = {\n    'pca__n_components': np.arange(1, X_train.shape[1]+1)\n}\ngnb_model = GridSearchCV(gnb_pipe, param_grid=param_grid, verbose=1, n_jobs=-1)\ngnb_model.fit(X_train, y_train)\nprint('Best params: {}'.format(gnb_model.best_params_))\nprint('Training Score: {}'.format(gnb_model.score(X_train, y_train)))\nprint('CV Score: {}'.format(gnb_model.best_score_))\nprint('Test Score: {}'.format(gnb_model.score(X_test, y_test)))","6f15d2dc":"lgr_pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA()),\n    ('lgr', LogisticRegression())\n])\nparam_grid = {\n    'pca__n_components': np.arange(1, X_train.shape[1]\/\/3),\n    'lgr__C': np.logspace(0, 1, 10)\n}\nlgr_model = GridSearchCV(lgr_pipe, param_grid=param_grid, verbose=1, n_jobs=-1)\nlgr_model.fit(X_train, y_train)\nprint('Best params: {}'.format(lgr_model.best_params_))\nprint('Training Score: {}'.format(lgr_model.score(X_train, y_train)))\nprint('CV Score: {}'.format(lgr_model.best_score_))\nprint('Test Score: {}'.format(lgr_model.score(X_test, y_test)))","dcd8d789":"rdf_pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('rdf', RandomForestClassifier())\n])\nparam_grid = {\n    'rdf__n_estimators': np.arange(200, 1001, 200),\n    'rdf__max_depth': np.arange(1,4),\n}\nrdf_model = GridSearchCV(rdf_pipe, param_grid=param_grid, verbose=1, n_jobs=-1)\nrdf_model.fit(X_train, y_train)\nprint('Best params: {}'.format(rdf_model.best_params_))\nprint('Training Score: {}'.format(rdf_model.score(X_train, y_train)))\nprint('CV Score: {}'.format(rdf_model.best_score_))\nprint('Test Score: {}'.format(rdf_model.score(X_test, y_test)))","1bbb0bb7":"svc_pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('pca', PCA()),\n    ('svc', SVC())\n])\nparam_grid = {\n    'pca__n_components': np.arange(1, X_train.shape[1]\/\/3),\n    'svc__C': np.logspace(0, 3, 10),\n    'svc__kernel': ['rbf'],\n    'svc__gamma': np.logspace(-4, -3, 10)\n}\nsvc_model = GridSearchCV(svc_pipe, param_grid=param_grid, verbose=1, n_jobs=-1)\nsvc_model.fit(X_train, y_train)\nprint('Best params: {}'.format(svc_model.best_params_))\nprint('Training Score: {}'.format(svc_model.score(X_train, y_train)))\nprint('CV Score: {}'.format(svc_model.best_score_))\nprint('Test Score: {}'.format(svc_model.score(X_test, y_test)))","29423a76":"xgb_pipe = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n#     ('pca', PCA()),\n    ('xgb', XGBClassifier())\n])\nparam_grid = {\n#     'pca__n_components': np.arange(1, X_train.shape[1]\/\/3),\n    'xgb__n_estimators': [100],\n    'xgb__learning_rate': np.logspace(-3, 0, 10),\n    'xgb__max_depth': np.arange(1, 6),\n    'xgb__gamma': np.arange(0, 1.0, 0.1),\n    'xgb__reg_lambda': np.logspace(-3, 3, 10)\n}\nxgb_model = GridSearchCV(xgb_pipe, param_grid=param_grid, verbose=1, n_jobs=-1)\nxgb_model.fit(X_train, y_train)\nprint('Best params: {}'.format(xgb_model.best_params_))\nprint('Training Score: {}'.format(xgb_model.score(X_train, y_train)))\nprint('CV Score: {}'.format(xgb_model.best_score_))\nprint('Test Score: {}'.format(xgb_model.score(X_test, y_test)))","84735327":"\n%%time\nmodels = {\n    'KNN': knn_model,\n    'GaussianNB': gnb_model,\n    'LogisticRegression': lgr_model,\n    'RandomForests': rdf_model,\n    'SVC': svc_model,\n    'XGBoost': xgb_model\n}\ny_stacked = pd.DataFrame({model_name: model.predict(X_train) for model_name, model in models.items()})\ny_stacked_train, y_stacked_test, y_train_train, y_train_test = train_test_split(y_stacked, y_train, \n                                                                              random_state=0, stratify=y_train)\nparam_grid = {\n    'C': np.logspace(0, 3, 10),\n    'kernel': ['rbf'],\n    'gamma': np.logspace(-3, 3, 10)\n}\nstacked_model = GridSearchCV(SVC(), param_grid=param_grid, verbose=1, n_jobs=-1)\nstacked_model.fit(y_stacked_train, y_train_train)\nprint('Best params: {}'.format(stacked_model.best_params_))\nprint('Training Score: {}'.format(stacked_model.score(y_stacked_train, y_train_train)))\nprint('CV Score: {}'.format(stacked_model.best_score_))\nprint('Test Score: {}'.format(stacked_model.score(y_stacked_test, y_train_test)))","ae929f7e":"y_stacked = pd.DataFrame({model_name: model.predict(X_test) for model_name, model in models.items()})\ny_pred = stacked_model.predict(y_stacked)\nprint('Overall Accuracy Score: {:.2%}'.format(accuracy_score(y_test, y_pred)))\nprint('Classification report:')\nprint(classification_report(y_test, y_pred))","9d64d17f":"# INRODUCTION\n> Breast cancer is a malignant cell growth in the breast. If left untreated, the cancer spreads to other areas of the body. Excluding skin cancer, breast cancer is the most common type of cancer in women in the United States, accounting for one of every three cancer diagnoses. Breast cancer ranks second among cancer deaths in women. \n\n# GOAL \n> The goal of this notebook is the application of several machine learning techniques to classify whether the tumor mass is benign or malignant in women residing in the state of Wisconsin, USA. This will help in understanding the important underlaying importance of attributes thereby helping in predicting the stage of breast cancer depending on the values of these attributes. ","20de4676":"# XGBoost\n> XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured\/tabular data, decision tree based algorithms are considered best-in-class right now.                                                                       \nXGBoost and Gradient Boosting Machines (GBMs) are both ensemble tree methods that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture.","1e4e42cd":"# Stacking\n> Stacking (sometimes called Stacked Generalization) is a different paradigm. The point of stacking is to explore a space of different models for the same problem. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem. So, you can build multiple different learners and you use them to build an intermediate prediction, one prediction for each learned model.\n\n> The best estimators for each are used to make uncorrelated predictions which in turn are concatenated and fed into a secondary Support Vector Machine estimator by stacking. ","e84437d5":"# Evaluation","d320752d":"# Principal Component Analysis (PCA)\n> Principal Component Analysis (PCA) is a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space. It tries to preserve the essential parts that have more variation of the data and remove the non-essential parts with fewer variation.           \nOne important thing to note about PCA is that it is an Unsupervised dimensionality reduction technique, you can cluster the similar data points based on the feature correlation between them without any supervision","d5cf2941":"# Support Vector Classifier\n> SVM depends on supervised learning models and trained by learning algorithms. A SVM generates parallel partitions by generating two parallel lines. For each category of data in a high-dimensional space and uses almost all attributes. It separates the space in a single pass to generate flat and linear partitions. Divide the 2 categories by a clear gap that should be as wide as possible. Do this partitioning by a plane called hyperplane.\nAn SVM creates hyperplanes that have the largest margin in a high-dimensional space to separate given data into classes. The margin between the 2 classes represents the longest distance between closest data points of those classes.","86b98284":"# KNN\n> knn is essentially classification by finding the most similar data points in the training data, and making an educated guess based on their classifications. K is number of nearest neighbors that the classifier will use to make its prediction. KNN makes predictions based on the outcome of the K neighbors closest to that point. One of the most popular choices to measure this distance is known as Euclidean.","1f64104b":"# *Please upvote the kernel if you find it insightful!*","8b543e0f":"# Gaussian Naive Bayes\n> Gaussian Naive Bayes is a variant of Naive Bayes that follows Gaussian normal distribution and supports continuous data. We have explored the idea behind Gaussian Naive Bayes along with an example.                                        Gaussian Naive Bayes supports continuous valued features and models each as conforming to a Gaussian (normal) distribution.                                                                                                           \nAn approach to create a simple model is to assume that the data is described by a Gaussian distribution with no co-variance (independent dimensions) between dimensions. This model can be fit by simply finding the mean and standard deviation of the points within each label, which is all what is needed to define such a distribution.\n\n","340b39d8":"# Load the dataset","0abdbc3d":"# Create Train and Test Dataset","f92445ea":"# Data Preparation\nThis stage would include identifying if the data set contains any missing values or bad data.","0ef34200":"# Random Forest\n> Random Forests, also known as random decision forests, are a popular ensemble method that can be used to build predictive models for both classification and regression problems. Ensemble methods use multiple learning models to gain better predictive results - in the case of a random Forest, the model creates an entire forest of random uncorrelated decision trees to arrive at the best possible answer.\nThe random Forest starts with a standard machine learning technique called a \u201cdecision tree\u201d which, in ensemble terms, corresponds to our weak learner. In a decision tree, an input is entered at the top and as it traverses down the tree the data gets bucketed into smaller and smaller sets. The random Forest takes this notion to the next level by combining trees with the notion of an ensemble. Thus, in ensemble terms, the trees are weak learners and the random Forest is a strong learner.","eef00482":"# Import Libraries","0ade8e5b":"# EDA\nThe EDA process will help in understanding the nature of the dataset and help in identification of potential outliers or correlated variables.","2a4662f6":"# Logistic Regression\n> Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail which is represented by an indicator variable, where the two values are labeled \"0\" and \"1\". "}}