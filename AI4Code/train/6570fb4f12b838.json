{"cell_type":{"9a3fcd2a":"code","94764084":"code","5c7eaa41":"code","ed1826bd":"code","f861bd4c":"code","d2521cd9":"code","34640921":"code","c980cf09":"code","7ba6dd95":"code","42490b87":"code","e3ed5fe9":"code","95062bd5":"code","75b23d64":"code","027a767b":"code","76a2127e":"code","338bc297":"code","76a64ec3":"code","48c6fd26":"code","6fc66a49":"code","4f066856":"code","0ad60481":"code","00746cf4":"code","e114054c":"code","e7b27350":"code","83a6816a":"code","ac75d747":"code","e6b1df3f":"code","7dfdbcf6":"code","d197342f":"code","cd65bd30":"code","a362396d":"code","271f74a9":"code","da98b72e":"code","a5276dc5":"code","6eb4ed80":"code","fcd0425e":"code","baba7596":"code","da6f6d03":"code","b4fee73a":"code","c265fae9":"code","1e055a64":"code","6d2baac8":"code","ec6343fe":"code","bc88f783":"code","00ca3a5b":"code","dd35a1ba":"code","81755f9b":"code","de88b670":"code","6a145adc":"code","d0d936f5":"code","14dc7f85":"code","6d8a6810":"code","75642447":"code","871c2d26":"code","757e5809":"code","e364af0a":"code","648b7956":"code","fbd4a978":"code","18e02743":"code","a8f534e8":"code","ad2b3c90":"code","c3ebe9f0":"code","6b35b9fb":"code","da1da153":"code","cafc22f3":"code","513dcc44":"code","372cc4a7":"code","1285581f":"code","5446605a":"code","ec627826":"code","205307f0":"code","5aa799b8":"code","0deeac69":"code","9ecebb20":"code","febd3fee":"code","928227b0":"code","c0351030":"code","f8295395":"code","1c3fae80":"code","63e378e3":"code","2dbee556":"code","fa1df825":"code","e023c720":"code","6fb57abc":"code","e4add6e2":"code","8a2ca7ba":"code","0d6848a6":"code","c2daa5c2":"code","5ca512ca":"code","d7b3621d":"code","cecd253a":"code","8e4a9ae8":"code","8f4f9c4e":"code","d6e2b36a":"code","311f1cae":"code","4c3a8063":"code","b119db01":"code","98ae8b98":"code","0516fcbe":"code","718526b4":"code","3329a47d":"code","1f4d9080":"code","7abd865f":"code","b895cf9f":"markdown","64050748":"markdown","16aa24c5":"markdown","adc6c2ee":"markdown","23c571c1":"markdown","66daf7d9":"markdown","62081f06":"markdown","d01e8765":"markdown","2eca2466":"markdown","e9e0d0ed":"markdown","f70db572":"markdown","619730fb":"markdown","e85aa07c":"markdown","5dd3151b":"markdown","844b35f3":"markdown","9376fc43":"markdown","d7ed6cf2":"markdown","5a0aa2cd":"markdown","9c8b1b2d":"markdown","fff074dc":"markdown","2ad5b44f":"markdown","1eb51892":"markdown","49ade1a0":"markdown","8246a158":"markdown","eb48b362":"markdown","2d67f2a2":"markdown","e266002d":"markdown","9c8947a7":"markdown","237e0ae9":"markdown","5135c63f":"markdown","5f49483f":"markdown","64118b59":"markdown","7597d5d9":"markdown","0936efa2":"markdown","99250f95":"markdown","2a93793f":"markdown","18626402":"markdown","b1d08bf2":"markdown","3b0a8841":"markdown","6ef2c5eb":"markdown","9e8f0cdc":"markdown","35d67ef2":"markdown","30070f6b":"markdown","0a9ed593":"markdown","84b2c256":"markdown","2b7b09eb":"markdown","79413bb5":"markdown","a674b341":"markdown","1c009dae":"markdown","150b9745":"markdown","d6851f63":"markdown","36131c6e":"markdown"},"source":{"9a3fcd2a":"%matplotlib inline\n# data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\nfrom time import time\n\n# Graphs libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nplt.style.use('seaborn-white')\nimport seaborn as sns\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom plotly import tools\n\n# Libraries to study\nfrom aif360.datasets import StandardDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\nfrom aif360.algorithms.preprocessing import LFR, Reweighing\nfrom aif360.algorithms.inprocessing import AdversarialDebiasing, PrejudiceRemover\nfrom aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, EqOddsPostprocessing, RejectOptionClassification\n\n# ML libraries\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_curve, auc\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\n\n# Design libraries\nfrom IPython.display import Markdown, display\nimport warnings\nwarnings.filterwarnings(\"ignore\")","94764084":"data = pd.read_csv('..\/input\/database.csv', na_values=['Unknown', ' '])","5c7eaa41":"data.shape","ed1826bd":"data.head().T","f861bd4c":"data.columns","d2521cd9":"cols_to_drop = ['Record ID', 'Agency Code', 'Perpetrator Ethnicity']\ndata_orig = data.copy()\ndata.drop(columns=cols_to_drop, inplace=True)\ncols_to_drop = []","34640921":"def print_missing_values(data):\n    data_null = pd.DataFrame(len(data) - data.notnull().sum(), columns = ['Count'])\n    data_null = data_null[data_null['Count'] > 0].sort_values(by='Count', ascending=False)\n    data_null = data_null\/len(data)*100\n\n    trace = go.Bar(x=data_null.index, y=data_null['Count'], marker=dict(color='#c0392b'),\n              name = 'At least one missing value', opacity=0.9)\n    layout = go.Layout(barmode='group', title='Column with missing values in the dataset', showlegend=True,\n                   legend=dict(orientation=\"h\"), yaxis=dict(title='Percentage of the dataset'))\n    fig = go.Figure([trace], layout=layout)\n    py.iplot(fig)","c980cf09":"print('Number total of rows : '+str(data.shape[0]))\nprint_missing_values(data)","7ba6dd95":"data['Crime Solved'].value_counts()","42490b87":"data_orig = data.copy()\n# data = data_orig\n\ndata = data[data['Crime Solved'] == 'Yes']\ncols_to_drop += ['Crime Solved']","e3ed5fe9":"data['Perpetrator Age category'] = np.where(data['Perpetrator Age'] > 64, 'Elder', np.where(data['Perpetrator Age'] < 25, 'Young', 'Adult'))\n# data['Victim Age category'] = np.where(data['Victim Age'] > 64, 'Elder', np.where(data['Victim Age'] < 25, 'Young', 'Adult'))","95062bd5":"Y_columns = ['Perpetrator Sex', 'Perpetrator Race', 'Perpetrator Age category']\nignore_columns = ['Crime Solved']\ncat_columns = []\nnum_columns = []\n\nfor col in data.columns.values:\n    if col in Y_columns+ignore_columns:\n        continue\n    elif data[col].dtypes == 'int64':\n        num_columns += [col]\n    else:\n        cat_columns += [col]\n","75b23d64":"median_val = pd.Series()\nfor col in num_columns:\n    if col not in cols_to_drop:\n        median_val[col] = data[col].median()","027a767b":"median_val","76a2127e":"def handle_missing_values(data, median_val):\n    df = data.copy()\n    for col in df:\n        if col in median_val.index.values:\n            df[col] = df[col].fillna(median_val[col])\n        else:\n            df[col] = df[col].fillna(\"Missing value\")\n    \n    return df","338bc297":"data = handle_missing_values(data, median_val)","76a64ec3":"def target_distribution(y_var, data):\n    val = data[y_var]\n\n    plt.style.use('seaborn-whitegrid')\n    plt.rcParams.update({'font.size': 13})\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n\n    cnt = val.value_counts().sort_values(ascending=True)\n    labels = cnt.index.values\n\n    sizes = cnt.values\n    colors = sns.color_palette(\"PuBu\", len(labels))\n\n    #------------COUNT-----------------------\n    ax1.barh(cnt.index.values, cnt.values, color=colors)\n    ax1.set_title('Count plot of '+y_var)\n\n    #------------PERCENTAGE-------------------\n    ax2.pie(sizes, labels=labels, colors=colors,autopct='%1.0f%%', shadow=True, startangle=130)\n    ax2.axis('equal')\n    ax2.set_title('Distribution of '+y_var)\n    plt.show()","48c6fd26":"var = 'Perpetrator Race'\ntarget_distribution(y_var=var, data=data)","6fc66a49":"var = 'Perpetrator Sex'\ntarget_distribution(y_var=var, data=data)","4f066856":"var = 'Perpetrator Age category'\ntarget_distribution(y_var=var, data=data)","0ad60481":"data['Frequency'] = 1\nfreq_target = data[['Perpetrator Sex', 'Perpetrator Race', 'Perpetrator Age category', 'Frequency']]\ndel data['Frequency']\nfreq_target = freq_target.groupby(by=['Perpetrator Sex', 'Perpetrator Race', 'Perpetrator Age category']).count() \/ len(data)\nprint(freq_target.sort_values(by='Frequency', ascending=False))","00746cf4":"def plot_histo(data, col, Y_columns):\n    df = data.copy()\n    fig, axs = plt.subplots(1,2,figsize=(20,6))\n    \n    for i in range(0,2):\n        cnt = []; y_col = Y_columns[i]\n        Y_values = df[y_col].dropna().drop_duplicates().values\n        for val in Y_values:\n            cnt += [df[df[y_col] == val][col].values]\n        bins = df[col].nunique()\n\n        axs[i].hist(cnt, bins=bins, stacked=True)\n        axs[i].legend(Y_values,loc='upper right')\n        axs[i].set_title(\"Histogram of the \"+col+\" column by \"+y_col)\n\n    plt.show()","e114054c":"plot_histo(data, col='Year',Y_columns=Y_columns)","e7b27350":"plot_histo(data, col='Incident',Y_columns=Y_columns)","83a6816a":"cols_to_drop += ['Incident']","ac75d747":"plot_histo(data, col='Victim Age',Y_columns=Y_columns)","e6b1df3f":"data[data['Victim Age'] > 100]['Victim Age'].value_counts()","7dfdbcf6":"data['Victim Age'] = np.where(data['Victim Age'] == 998, np.median(data[data['Victim Age'] <= 100]['Victim Age']), data['Victim Age'])","d197342f":"plot_histo(data, col='Victim Age',Y_columns=Y_columns)","cd65bd30":"plot_histo(data, col='Victim Count',Y_columns=Y_columns)","a362396d":"plot_histo(data, col='Perpetrator Count',Y_columns=Y_columns)","271f74a9":"cat_columns","da98b72e":"def plot_bar(data, col, Y_columns, max_cat=10):\n    df = data.copy()\n    \n    fig, axs = plt.subplots(1,2,figsize=(20,6))\n    cat_val = df[col].value_counts()[0:max_cat].index.values\n    df = df[df[col].isin(cat_val)]\n\n    for i in range(0,2):\n        y_col = Y_columns[i]\n        Y_values = df[y_col].dropna().drop_duplicates().values\n        for val in Y_values:\n            cnt = df[df[y_col] == val][col].value_counts().sort_index()\n            axs[i].barh(cnt.index.values, cnt.values)\n        axs[i].legend(Y_values,loc='upper right')\n        axs[i].set_title(\"Bar plot of the \"+col+\" column by \"+y_col)\n\n    plt.show()","a5276dc5":"plot_bar(data, col='Agency Name',Y_columns=Y_columns)","6eb4ed80":"plot_bar(data, col='Agency Type',Y_columns=Y_columns)","fcd0425e":"plot_bar(data, col='City',Y_columns=Y_columns)","baba7596":"plot_bar(data, col='State',Y_columns=Y_columns)","da6f6d03":"plot_bar(data, col='Month',Y_columns=Y_columns, max_cat=12)","b4fee73a":"plot_bar(data, col='Crime Type',Y_columns=Y_columns)","c265fae9":"plot_bar(data, col='Victim Sex',Y_columns=Y_columns)","1e055a64":"plot_bar(data, col='Victim Race',Y_columns=Y_columns)","6d2baac8":"plot_bar(data, col='Victim Ethnicity',Y_columns=Y_columns)","ec6343fe":"plot_bar(data, col='Relationship', Y_columns=Y_columns)","bc88f783":"plot_bar(data, col='Weapon', Y_columns=Y_columns)","00ca3a5b":"plot_bar(data, col='Record Source', Y_columns=Y_columns)","dd35a1ba":"data.drop(cols_to_drop, axis=1, inplace=True)","81755f9b":"categorical_features = cat_columns + ['Perpetrator Sex', 'Perpetrator Race', 'Perpetrator Age category']\n# categorical_features = categorical_features \ncategorical_features_idx = [np.where(data.columns.values == col)[0][0] for col in categorical_features]\n\ndel cat_columns","de88b670":"data_encoded = data.copy()\n\ncategorical_names = {}\nencoders = {}\n\n# Use Label Encoder for categorical columns (including target column)\nfor feature in categorical_features:\n    le = LabelEncoder()\n    le.fit(data_encoded[feature])\n    \n    data_encoded[feature] = le.transform(data_encoded[feature])\n    \n    categorical_names[feature] = le.classes_\n    encoders[feature] = le","6a145adc":"numerical_features = [c for c in data.columns.values if c not in categorical_features]\n\nfor feature in numerical_features:\n    val = data_encoded[feature].values[:, np.newaxis]\n    mms = MinMaxScaler().fit(val)\n    data_encoded[feature] = mms.transform(val)\n    encoders[feature] = mms\n    \ndata_encoded = data_encoded.astype(float)\n\ndel num_columns","d0d936f5":"data_encoded.head()","14dc7f85":"def decode_dataset(data, encoders, numerical_features, categorical_features):\n    df = data.copy()\n    for feat in df.columns.values:\n        if feat in numerical_features:\n            df[feat] = encoders[feat].inverse_transform(np.array(df[feat]).reshape(-1, 1))\n    for feat in categorical_features:\n        df[feat] = encoders[feat].inverse_transform(df[feat].astype(int))\n    return df","6d8a6810":"decode_dataset(data_encoded, encoders=encoders, numerical_features=numerical_features, categorical_features=categorical_features).head()","75642447":"data_perp_sex = data_encoded.drop(['Perpetrator Race','Perpetrator Age category','Perpetrator Age'], axis=1)","871c2d26":"privileged_sex = np.where(categorical_names['Victim Sex'] == 'Male')[0]\nprivileged_race = np.where(categorical_names['Victim Race'] == 'White')[0]","757e5809":"data_orig_sex = StandardDataset(data_perp_sex, \n                               label_name='Perpetrator Sex', \n                               favorable_classes=[1], \n                               protected_attribute_names=['Victim Sex', 'Victim Race'], \n                               privileged_classes=[privileged_sex, privileged_race])","e364af0a":"def meta_data(dataset):\n    # print out some labels, names, etc.\n    display(Markdown(\"#### Dataset shape\"))\n    print(dataset.features.shape)\n    display(Markdown(\"#### Favorable and unfavorable labels\"))\n    print(dataset.favorable_label, dataset.unfavorable_label)\n    display(Markdown(\"#### Protected attribute names\"))\n    print(dataset.protected_attribute_names)\n    display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n    print(dataset.privileged_protected_attributes, dataset.unprivileged_protected_attributes)\n    display(Markdown(\"#### Dataset feature names\"))\n    print(dataset.feature_names)","648b7956":"meta_data(data_orig_sex)","fbd4a978":"np.random.seed(42)\n\ndata_orig_sex_train, data_orig_sex_test = data_orig_sex.split([0.7], shuffle=True)\n\ndisplay(Markdown(\"#### Train Dataset shape\"))\nprint(\"Perpetrator Sex :\",data_orig_sex_train.features.shape)\ndisplay(Markdown(\"#### Test Dataset shape\"))\nprint(\"Perpetrator Sex :\",data_orig_sex_test.features.shape)","18e02743":"# Train and save the models\nrf_orig_sex = RandomForestClassifier().fit(data_orig_sex_train.features, \n                     data_orig_sex_train.labels.ravel(), \n                     sample_weight=data_orig_sex_train.instance_weights)","a8f534e8":"X_test_sex = data_orig_sex_test.features\ny_test_sex = data_orig_sex_test.labels.ravel()","ad2b3c90":"def get_model_performance(X_test, y_true, y_pred, probs):\n    accuracy = accuracy_score(y_true, y_pred)\n    matrix = confusion_matrix(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n\n    return accuracy, matrix, f1, fpr, tpr, roc_auc\n\ndef plot_model_performance(model, X_test, y_true):\n    y_pred = model.predict(X_test)\n    probs = model.predict_proba(X_test)\n    accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n    display(Markdown('#### Accuracy of the model :'))\n    print(accuracy)\n    display(Markdown('#### F1 score of the model :'))\n    print(f1)\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(1, 2, 1)\n    sns.heatmap(matrix, annot=True, cmap='Blues', fmt='g')\n    plt.title('Confusion Matrix')\n\n    ax = fig.add_subplot(1, 2, 2)\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic curve')\n    plt.legend(loc=\"lower right\")","c3ebe9f0":"plot_model_performance(rf_orig_sex, data_orig_sex_test.features, y_test_sex)","6b35b9fb":"# This DataFrame is created to stock differents models and fair metrics that we produce in this notebook\nalgo_metrics = pd.DataFrame(columns=['model', 'fair_metrics', 'prediction', 'probs'])\n\ndef add_to_df_algo_metrics(algo_metrics, model, fair_metrics, preds, probs, name):\n    return algo_metrics.append(pd.DataFrame(data=[[model, fair_metrics, preds, probs]], columns=['model', 'fair_metrics', 'prediction', 'probs'], index=[name]))","da1da153":"def fair_metrics(dataset, pred, pred_is_dataset=False):\n    if pred_is_dataset:\n        dataset_pred = pred\n    else:\n        dataset_pred = dataset.copy()\n        dataset_pred.labels = pred\n    \n    cols = ['statistical_parity_difference', 'equal_opportunity_difference', 'average_abs_odds_difference',  'disparate_impact', 'theil_index']\n    obj_fairness = [[0,0,0,1,0]]\n    \n    fair_metrics = pd.DataFrame(data=obj_fairness, index=['objective'], columns=cols)\n    \n    for attr in dataset_pred.protected_attribute_names:\n        idx = dataset_pred.protected_attribute_names.index(attr)\n        privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n        unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n        \n        classified_metric = ClassificationMetric(dataset, \n                                                     dataset_pred,\n                                                     unprivileged_groups=unprivileged_groups,\n                                                     privileged_groups=privileged_groups)\n\n        metric_pred = BinaryLabelDatasetMetric(dataset_pred,\n                                                     unprivileged_groups=unprivileged_groups,\n                                                     privileged_groups=privileged_groups)\n\n        acc = classified_metric.accuracy()\n\n        row = pd.DataFrame([[metric_pred.mean_difference(),\n                                classified_metric.equal_opportunity_difference(),\n                                classified_metric.average_abs_odds_difference(),\n                                metric_pred.disparate_impact(),\n                                classified_metric.theil_index()]],\n                           columns  = cols,\n                           index = [attr]\n                          )\n        fair_metrics = fair_metrics.append(row)    \n    \n    fair_metrics = fair_metrics.replace([-np.inf, np.inf], 2)\n        \n    return fair_metrics\n\ndef plot_fair_metrics(fair_metrics):\n    fig, ax = plt.subplots(figsize=(20,4), ncols=5, nrows=1)\n\n    plt.subplots_adjust(\n        left    =  0.125, \n        bottom  =  0.1, \n        right   =  0.9, \n        top     =  0.9, \n        wspace  =  .5, \n        hspace  =  1.1\n    )\n\n    y_title_margin = 1.2\n\n    plt.suptitle(\"Fairness metrics\", y = 1.09, fontsize=20)\n    sns.set(style=\"dark\")\n\n    cols = fair_metrics.columns.values\n    obj = fair_metrics.loc['objective']\n    size_rect = [0.2,0.2,0.2,0.4,0.25]\n    rect = [-0.1,-0.1,-0.1,0.8,0]\n    bottom = [-1,-1,-1,0,0]\n    top = [1,1,1,2,1]\n    bound = [[-0.1,0.1],[-0.1,0.1],[-0.1,0.1],[0.8,1.2],[0,0.25]]\n\n    display(Markdown(\"### Check bias metrics :\"))\n    display(Markdown(\"A model can be considered bias if just one of these five metrics show that this model is biased.\"))\n    for attr in fair_metrics.index[1:len(fair_metrics)].values:\n        display(Markdown(\"#### For the %s attribute :\"%attr))\n        check = [bound[i][0] < fair_metrics.loc[attr][i] < bound[i][1] for i in range(0,5)]\n        display(Markdown(\"With default thresholds, bias against unprivileged group detected in **%d** out of 5 metrics\"%(5 - sum(check))))\n\n    for i in range(0,5):\n        plt.subplot(1, 5, i+1)\n        ax = sns.barplot(x=fair_metrics.index[1:len(fair_metrics)], y=fair_metrics.iloc[1:len(fair_metrics)][cols[i]])\n        \n        for j in range(0,len(fair_metrics)-1):\n            a, val = ax.patches[j], fair_metrics.iloc[j+1][cols[i]]\n            marg = -0.2 if val < 0 else 0.1\n            ax.text(a.get_x()+a.get_width()\/5, a.get_y()+a.get_height()+marg, round(val, 3), fontsize=15,color='black')\n\n        plt.ylim(bottom[i], top[i])\n        plt.setp(ax.patches, linewidth=0)\n        ax.add_patch(patches.Rectangle((-5,rect[i]), 10, size_rect[i], alpha=0.3, facecolor=\"green\", linewidth=1, linestyle='solid'))\n        plt.axhline(obj[i], color='black', alpha=0.3)\n        plt.title(cols[i])\n        ax.set_ylabel('')    \n        ax.set_xlabel('')","cafc22f3":"def get_fair_metrics_and_plot(data, model, plot=True, model_aif=False):\n    pred = model.predict(data).labels if model_aif else model.predict(data.features)\n    # fair_metrics function available in the metrics.py file\n    fair = fair_metrics(data, pred)\n\n    if plot:\n        # plot_fair_metrics function available in the visualisations.py file\n        # The visualisation of this function is inspired by the dashboard on the demo of IBM aif360 \n        plot_fair_metrics(fair)\n        display(fair)\n    \n    return fair","513dcc44":"display(Markdown('### Bias metrics for the Sex model'))\nfair = get_fair_metrics_and_plot(data_orig_sex_test, rf_orig_sex)","372cc4a7":"data_orig_test = data_orig_sex_test\ndata_orig_train = data_orig_sex_train\nrf = rf_orig_sex\n\nprobs = rf.predict_proba(data_orig_test.features)\npreds = rf.predict(data_orig_test.features)\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, rf, fair, preds, probs, 'Origin')","1285581f":"def get_attributes(data, selected_attr=None):\n    unprivileged_groups = []\n    privileged_groups = []\n    if selected_attr == None:\n        selected_attr = data.protected_attribute_names\n    \n    for attr in selected_attr:\n            idx = data.protected_attribute_names.index(attr)\n            privileged_groups.append({attr:data.privileged_protected_attributes[idx]}) \n            unprivileged_groups.append({attr:data.unprivileged_protected_attributes[idx]}) \n\n    return privileged_groups, unprivileged_groups","5446605a":"privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\nLFR_model = LFR(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, k=1, verbose=0)\n# LFR.fit(data_orig_train)\ndata_transf_train = LFR_model.fit_transform(data_orig_train)\n\n# Train and save the model\nrf_transf = RandomForestClassifier().fit(data_transf_train.features, \n                     data_transf_train.labels.ravel(), \n                     sample_weight=data_transf_train.instance_weights)\n\ndata_transf_test = LFR_model.transform(data_orig_test)\nfair = get_fair_metrics_and_plot(data_transf_test, rf_transf, plot=False)\nprobs = rf_transf.predict_proba(data_orig_test.features)\npreds = rf_transf.predict(data_orig_test.features)\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'LFR')\nprint('time elapsed : %.2fs'%(time()-t0))","ec627826":"privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\nRW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n# RW.fit(data_orig_train)\ndata_transf_train = RW.fit_transform(data_orig_train)\n\n# Train and save the model\nrf_transf = RandomForestClassifier().fit(data_transf_train.features, \n                     data_transf_train.labels.ravel(), \n                     sample_weight=data_transf_train.instance_weights)\n\ndata_transf_test = RW.transform(data_orig_test)\nfair = get_fair_metrics_and_plot(data_orig_test, rf_transf, plot=False)\nprobs = rf_transf.predict_proba(data_orig_test.features)\npreds = rf_transf.predict(data_orig_test.features)\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, rf_transf, fair, preds, probs, 'Reweighing')\nprint('time elapsed : %.2fs'%(time()-t0))","205307f0":"privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\n# sess.close()\n# tf.reset_default_graph()\nsess = tf.Session()\n\ndebiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n                          unprivileged_groups = unprivileged_groups,\n                          scope_name='debiased_classifier',\n                          num_epochs=10,\n                          debias=True,\n                          sess=sess)\n\ndebiased_model.fit(data_orig_train)\n\nfair = get_fair_metrics_and_plot(data_orig_test, debiased_model, plot=False, model_aif=True)\ndata_pred = debiased_model.predict(data_orig_test)\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, debiased_model, fair, data_pred.labels, data_pred.scores, 'AdvDebiasing')\nprint('time elapsed : %.2fs'%(time()-t0))","5aa799b8":"t0 = time()\ndebiased_model = PrejudiceRemover(sensitive_attr=\"Victim Race\", eta = 25.0)\ndebiased_model.fit(data_orig_train)\n\nfair = get_fair_metrics_and_plot(data_orig_test, debiased_model, plot=False, model_aif=True)\ndata_pred = debiased_model.predict(data_orig_test)\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, debiased_model, fair, data_pred.labels, data_pred.scores, 'PrejudiceRemover')\nprint('time elapsed : %.2fs'%(time()-t0))","0deeac69":"data_orig_test_pred = data_orig_test.copy(deepcopy=True)\n\n# Prediction with the original RandomForest model\nscores = np.zeros_like(data_orig_test.labels)\nscores = rf.predict_proba(data_orig_test.features)[:,1].reshape(-1,1)\ndata_orig_test_pred.scores = scores\n\npreds = np.zeros_like(data_orig_test.labels)\npreds = rf.predict(data_orig_test.features).reshape(-1,1)\ndata_orig_test_pred.labels = preds\n\ndef format_probs(probs1):\n    probs1 = np.array(probs1)\n    probs0 = np.array(1-probs1)\n    return np.concatenate((probs0, probs1), axis=1)","9ecebb20":"privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\ncost_constraint = \"fnr\" # \"fnr\", \"fpr\", \"weighted\"\n\nCPP = CalibratedEqOddsPostprocessing(privileged_groups = privileged_groups,\n                                     unprivileged_groups = unprivileged_groups,\n                                     cost_constraint=cost_constraint,\n                                     seed=42)\n\nCPP = CPP.fit(data_orig_test, data_orig_test_pred)\ndata_transf_test_pred = CPP.predict(data_orig_test_pred)\n\nfair = fair_metrics(data_orig_test, data_orig_test_pred, pred_is_dataset=True)\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, \n                                      CPP, \n                                      fair, \n                                      data_transf_test_pred.labels, \n                                      format_probs(data_transf_test_pred.scores), \n                                      'CalibratedEqOdds')\nprint('time elapsed : %.2fs'%(time()-t0))","febd3fee":"privileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nt0 = time()\n\nROC = RejectOptionClassification(privileged_groups = privileged_groups,\n                             unprivileged_groups = unprivileged_groups)\n\nROC = ROC.fit(data_orig_test, data_orig_test_pred)\ndata_transf_test_pred = ROC.predict(data_orig_test_pred)\n\nfair = fair_metrics(data_orig_test, data_transf_test_pred, pred_is_dataset=True)\n\nalgo_metrics = add_to_df_algo_metrics(algo_metrics, \n                                      ROC, \n                                      fair, \n                                      data_transf_test_pred.labels, \n                                      format_probs(data_transf_test_pred.scores), \n                                      'RejectOption')\nprint('time elapsed : %.2fs'%(time()-t0))","928227b0":"def plot_fair_metrics_plotly(fair_metrics):\n    bottom = [-1, -1, -1, 0, 0]\n    max_valid = [0.1, 0.1, 0.1, 1.2, 0.25]\n    min_valid = [-0.1, -0.1, -0.1, 0.8, 0]\n    cols = fair_metrics.columns.values\n\n    for i in range(0, 5):\n        col = cols[i]\n\n        x, y = (fair_metrics[col].values, fair_metrics.index)\n        colors = []\n        for v in x:\n            color = '#e74c3c' if v < min_valid[i] or v > max_valid[i] else '#2ecc71'\n            colors.append(color)\n\n        trace = go.Bar(x=x, y=y, marker=dict(color=colors)\n                       , opacity=0.9, orientation='h')\n\n        layout = go.Layout(barmode='group',\n                           title=col,\n                           xaxis=dict(range=[bottom[i], bottom[i] + 2]),\n                           yaxis=go.layout.YAxis(automargin=True),\n                           shapes=[\n                               {\n                                   'type': 'line',\n                                   'x0': min_valid[i],\n                                   'y0': -1,\n                                   'x1': min_valid[i],\n                                   'y1': len(y),\n                                   'line': {\n                                       'color': 'rgb(0, 0, 0)',\n                                       'width': 2,\n                                   },\n                               }, {\n                                   'type': 'line',\n                                   'x0': max_valid[i],\n                                   'y0': -1,\n                                   'x1': max_valid[i],\n                                   'y1': len(y),\n                                   'line': {\n                                       'color': 'rgb(0, 0, 0)',\n                                       'width': 2,\n                                   },\n                               }])\n        fig = go.Figure([trace], layout=layout)\n        py.iplot(fig)\n\n\ndef plot_score_fair_metrics(score):\n    display(score.sort_values(['nb_valid', 'score'], ascending=[0, 1]))\n    score.sort_values(['nb_valid', 'score'], ascending=[1, 0], inplace=True)\n\n    gold, silver, bronze, other = ('#FFA400', '#bdc3c7', '#cd7f32', '#3498db')\n    colors = [gold if i == 0 else silver if i == 1 else bronze if i == 2 else other for i in range(0, len(score))]\n    colors = [c for c in reversed(colors)]\n\n    x, y = (score['score'].values, score.index)\n\n    trace = go.Bar(x=x, y=y, marker=dict(color=colors)\n                   , opacity=0.9, orientation='h')\n    layout = go.Layout(barmode='group',\n                       title='Fairest algorithm',\n                       yaxis=go.layout.YAxis(automargin=True))\n    fig = go.Figure([trace], layout=layout)\n    py.iplot(fig)\n    \n\ndef score_fair_metrics(fair):\n    objective = [0, 0, 0, 1, 0]\n    max_valid = [0.1, 0.1, 0.1, 1.2, 0.25]\n    min_valid = [-0.1, -0.1, -0.1, 0.8, 0]\n\n    nb_valid = np.sum(((fair.values > min_valid) * (fair.values < max_valid)), axis=1)\n    score = np.sum(np.abs(fair.values - objective), axis=1)\n    score = np.array([score, nb_valid])\n\n    score = pd.DataFrame(data=score.transpose(), columns=['score', 'nb_valid'], index=fair.index)\n    return score\n\n\ndef score_all_attr(algo_metrics):\n    attributes = algo_metrics.loc['Origin', 'fair_metrics'].index.values[1:]\n\n    all_scores = np.zeros((len(algo_metrics), 2))\n    for attr in attributes:\n        df_metrics = pd.DataFrame(columns=algo_metrics.loc['Origin', 'fair_metrics'].columns.values)\n        for fair in algo_metrics.loc[:, 'fair_metrics']:\n            df_metrics = df_metrics.append(fair.loc[attr], ignore_index=True)\n        all_scores = all_scores + score_fair_metrics(df_metrics).values\n\n    final = pd.DataFrame(data=all_scores, columns=['score', 'nb_valid'], index=algo_metrics.index)\n    return final\n","c0351030":"def compare_fair_metrics(algo_metrics, attr='Victim Race'):\n    \n    df_metrics = pd.DataFrame(columns=algo_metrics.loc['Origin','fair_metrics'].columns.values)\n    for fair in algo_metrics.loc[:,'fair_metrics']:\n        df_metrics = df_metrics.append(fair.loc[attr], ignore_index=True)\n\n    df_metrics.index = algo_metrics.index.values\n    df_metrics = df_metrics.replace([np.inf, -np.inf], np.NaN)\n    \n    display(df_metrics)\n    plot_fair_metrics_plotly(df_metrics)\n    score = score_fair_metrics(df_metrics)\n    plot_score_fair_metrics(score.dropna())","f8295395":"compare_fair_metrics(algo_metrics)","1c3fae80":"def plot_compare_model_performance(algo_metrics, dataset):\n    X_test = dataset.features\n    y_true = dataset.labels\n    perf_metrics = pd.DataFrame()\n\n    models_name = algo_metrics.index.values\n\n    fig = plt.figure(figsize=(7, 7))\n    plt.title('ROC curve for differents models')\n    lw = 2\n    palette = sns.color_palette(\"Paired\")\n\n    for model_name, i in zip(models_name, range(0, len(models_name))):\n        model = algo_metrics.loc[model_name, 'model']\n\n        if model_name != 'AdvDebiasing':\n            probs = algo_metrics.loc[model_name, 'probs']\n            y_pred = algo_metrics.loc[model_name, 'prediction']\n            accuracy, matrix, f1, fpr, tpr, roc_auc = get_model_performance(X_test, y_true, y_pred, probs)\n\n            perf_metrics = perf_metrics.append(\n                pd.DataFrame([[accuracy, f1]], columns=['Accuracy', 'F1 Score'], index=[model_name]))\n            plt.plot(fpr, tpr, color=palette[i], lw=lw, label=str(model_name) + ' (area = %0.2f)' % roc_auc)\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic curve')\n    plt.legend(loc=\"lower right\")\n    display(perf_metrics.sort_values(by=['Accuracy', 'F1 Score'], ascending=[False, False]))\n    plt.show()","63e378e3":"plot_compare_model_performance(algo_metrics, data_orig_test)","2dbee556":"# I remove others perpetrator column\ncolumns_to_drop = ['Perpetrator Sex','Perpetrator Age category','Perpetrator Age']\ndataset_race = data_encoded.drop(columns_to_drop, axis=1)","fa1df825":"dataset_race.head()","e023c720":"categorical_names['Victim Sex'], categorical_names['Victim Race'], categorical_names['Perpetrator Race'] ","6fb57abc":"dataset = dataset_race\nlabel_name = 'Perpetrator Race'\nprotected_attribute_names=['Victim Sex', 'Victim Race']\n\nfavorable_classes = np.where(categorical_names['Perpetrator Race'] == 'White')[0]\nprivileged_sex = np.where(categorical_names['Victim Sex'] == 'Male')[0]\nprivileged_race = np.where(categorical_names['Victim Race'] == 'White')[0]","e4add6e2":"privileged_classes = [privileged_sex, privileged_race]","8a2ca7ba":"data_orig_rac = StandardDataset(dataset, \n                               label_name=label_name, \n                               favorable_classes=favorable_classes, \n                               protected_attribute_names=protected_attribute_names, \n                               privileged_classes=[privileged_sex, privileged_race])","0d6848a6":"meta_data(data_orig_rac)","c2daa5c2":"np.random.seed(42)\ndata_orig_rac_train, data_orig_rac_test = data_orig_rac.split([0.7], shuffle=True)\n\ndisplay(Markdown(\"#### Train Dataset shape\"))\nprint(\"Perpetrator Race : \",data_orig_rac_train.features.shape)\ndisplay(Markdown(\"#### Test Dataset shape\"))\nprint(\"Perpetrator Race : \",data_orig_rac_test.features.shape)","5ca512ca":"# if you want to change to model be my guest \nmodel = RandomForestClassifier()\nmodel = model.fit(data_orig_rac_train.features, data_orig_rac_train.labels.ravel())","d7b3621d":"X_test_rac = data_orig_rac_test.features\ny_test_rac = data_orig_rac_test.labels.ravel()\nplot_model_performance(model, data_orig_rac_test.features, y_test_rac)","cecd253a":"# we need to create a new dataset with predictions from the model into labels attribute\ndataset = data_orig_rac_test\ndataset_pred = dataset.copy()\ndataset_pred.labels = model.predict(data_orig_rac_test.features)","8e4a9ae8":"dataset_pred.protected_attribute_names","8f4f9c4e":"# I get the index of Male and Female classes\nprivileged_race   = np.where(categorical_names['Victim Race'] == 'White')[0]\nunprivileged_race = np.where(categorical_names['Victim Race'] == 'Black')[0]\n\n# I format variable like in the documentation of ClassificationMetric and BinaryLabelDatasetMetric\nprivileged_groups   = [{'Victim Race' : privileged_race}] \nunprivileged_groups = [{'Victim Race' : unprivileged_race}] \n\n# I create both classes\nclassified_metric_race = ClassificationMetric(dataset, \n                                         dataset_pred,\n                                         unprivileged_groups=unprivileged_groups,\n                                         privileged_groups=privileged_groups)\n\nmetric_pred_race = BinaryLabelDatasetMetric(dataset_pred,\n                                         unprivileged_groups=unprivileged_groups,\n                                         privileged_groups=privileged_groups)","d6e2b36a":"# TODO : do the same as above but with Victim Sex attribute\nprivileged_sex   = None\nunprivileged_sex = None \n\nprivileged_groups   = [{'' : privileged_sex}] \nunprivileged_groups = [{'' : unprivileged_sex}] \n\n# Remove comments below\n# classified_metric_sex = ClassificationMetric(dataset, \n#                                          dataset_pred,\n#                                          unprivileged_groups=unprivileged_groups,\n#                                          privileged_groups=privileged_groups)\n\n# metric_pred_sex = BinaryLabelDatasetMetric(dataset_pred,\n#                                          unprivileged_groups=unprivileged_groups,\n#                                          privileged_groups=privileged_groups)","311f1cae":"def is_fair(metric, objective=0, threshold=0.2):\n    return abs(metric - objective) <= threshold","4c3a8063":"# This is an example for mean_difference metric and race attribute:\nmy_fair_metric = metric_pred_race.mean_difference()\nprint(my_fair_metric)\n\nif is_fair(my_fair_metric):\n    print('My metric is fair')\nelse:\n    print('My metric is not fair')","b119db01":"# This is all the code for 4 others fair metrics and victim race attribute\nclassified_metric_race.equal_opportunity_difference()\nclassified_metric_race.average_abs_odds_difference()\nmetric_pred_race.disparate_impact()\nclassified_metric_race.theil_index()\n\n# TODO : find out which metrics said that this is fair or not. \n#        remember that disparate_impact fair is at 1 and the others at 0\nyour_metric = 0","98ae8b98":"# TODO : do the same for the victim sex attribute \n#        metric_pred_sex and classified_metric_sex class\n\n\n","0516fcbe":"# This is a function you can use to get privileged_groups and unprivileged_groups informations\n# that are used for processors \n\ndef get_attributes(data, selected_attr=None):\n    unprivileged_groups = []\n    privileged_groups = []\n    if selected_attr == None:\n        selected_attr = data.protected_attribute_names\n    \n    for attr in selected_attr:\n            idx = data.protected_attribute_names.index(attr)\n            privileged_groups.append({attr:data.privileged_protected_attributes[idx]}) \n            unprivileged_groups.append({attr:data.unprivileged_protected_attributes[idx]}) \n\n    return privileged_groups, unprivileged_groups","718526b4":"# an example\nget_attributes(dataset, selected_attr=['Victim Sex'])","3329a47d":"# also to remember the dataset used for training is this one\ndataset = data_orig_rac_train\n# and the test one :\ndataset_test = data_orig_rac_test","1f4d9080":"privileged_groups, unprivileged_groups = get_attributes(dataset, selected_attr=['Victim Race'])\nt0 = time()\n\n# Creation of the algorithm class\nRW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n# I transform my dataset\ndata_transf_train = RW.fit_transform(dataset)\n\n# Train and save the model\nrf_transf = RandomForestClassifier()\nrf_transf = rf_transf.fit(data_transf_train.features, \n                          data_transf_train.labels.ravel(), \n                         sample_weight=data_transf_train.instance_weights) # sample_weight arguments is used thanks to Reweighing class\n\nprint('time elapsed : %.2fs'%(time()-t0))","7abd865f":"# Now I can predict on the test dataset\ndata_transf_test = RW.transform(dataset_test)\n\nrf_transf.predict(dataset_test.features)","b895cf9f":"With the function that I created above, we will be abble to look out bias present into the data or the model.","64050748":"So with all this information we see that the most unbiased algorithm is the less performant..","16aa24c5":"As we can see, this first model is biased. The next step is to answer the question : How to fix it ?\n\n### <a id='4.2'>4.2 How to fix it ?<\/a>\n\nAIF360 use 3 types of algorithms :\n* Pre-processing algorithms : they are used before training the model\n* In-processing algorithms : they are fair classifiers so it's during the training\n* Post-processing algorithms : they are used after training the model\n\n![Fairness pipeline](http:\/\/image.noelshack.com\/fichiers\/2018\/50\/1\/1544437769-fairness-pipeline.png)\n\nThis is the **fairness pipeline**. An example instantiation of this generic pipeline consists of loading data into a dataset object, transforming\nit into a fairer dataset using a fair pre-processing algorithm, learning a classifier from this transformed dataset, and obtaining\npredictions from this classifier. Metrics can be calculated on the original, transformed, and predicted datasets as well as between the\ntransformed and predicted datasets. Many other instantiations are also possible (more information on [the aif360 paper](https:\/\/arxiv.org\/pdf\/1810.01943.pdf)).\n\n#### <a id='4.2.1'>4.2.1 Pre-processing algorithms <\/a>\n\nThere are 4 pre-processing algorithms but for 3 of them **there is a problem** : it will work only if the dataset have one protected attribute but here we have 2 : *Sex* and *Race*. But let's have a quick intro for all of them and at the end I will use the one that work for my case.\n\nBecause this notebook is an exploration of fairness into models I will only use 1 protected attribute : Victim Race, it will allow me to use almost all the algorithms (but for your information the *Reweighing* algo can works with all protected so that's why I will create a function that gives me the protected and unprotected attributes.\n\nYou can find all the code information on [the documentation](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/preprocessing.html#)\n\n","adc6c2ee":"With this information that there are 998 years old people in the dataset, we can conclude this is an error, so let's handle these values by replacing them with the median value.","23c571c1":"If we based our model on the most frequents values we found that by default there is 26% of chance that the perpetrator is a white adult man.","66daf7d9":"### Predict on test set","62081f06":"This dataset is hard to understand for a human so let's create a function that allows us to get back the original values.","d01e8765":"### <a id='4.3'>4.3 Compare all the algorithms for one attribute<\/a>","2eca2466":"Well done ! You succeed to know if our model needs to be fairest or no. As you can see, it needs to be more fair !\n\n**So let's use some processors to mitigate bias !**\n\nI will do an example for one algorithm and then you will have carte blanche to explore by yourself others algorithms.","e9e0d0ed":"## <a id=\"2\">2. EDA and data preparation<\/a>\n\nLet's dive into the data and understand what this dataset is about !\n\n### <a id=\"2.1\">2.1 Prepare the data<\/a>","f70db572":"So here I want to use a pre-processor named `Reweighing`, this algorithm set weight for each row during the model training, this is how it's used :","619730fb":"<span style=\"color:red;\">You turn<\/span>","e85aa07c":"#### <a id=\"2.4.2\">2.4.2 Scale numerical columns<\/a>\n\nI choose to scale these columns because later in this notebook I will use a neural network so it's necessary to use it.\n","5dd3151b":"To construct your aif360 dataset you need 5 informations :\n* the dataset : good you have it ==> `dataset_race`\n* the name of your label \/ target column ==> `'Perpetrator Race'`\n* favorable classes of your target column : here it's white but you'll need to get the class number\n* columns that contains protected attribute ==> `['Victim Sex', 'Victim Race']` \n* privileged classes for protected attributes : so it's Male and White ","844b35f3":"To conclude, this situation is perfect to illustrate a problem that Data Scientists will have to face at least once : **Should I prefer the performance or the fairness of my model ?**\n\n*Spoiler alert : the fairness* \ud83d\ude09\n\nThanks a lot for reading, sorry for my english I'm not really good at it ! \nPlease tell me if there is any improvements that I can make for this kernel.\n\n$Nathan$","9376fc43":"Now let's create the dataset","d7ed6cf2":"Now let's drop column that are useless","5a0aa2cd":"All you need to play with processors is in this part of the notebook : <a href='#4.2'>4.2 How to fix it ?<\/a>\n\nIf you have any questions, do not hesitate.\n\n<span style=\"color:red;\">You turn<\/span>","9c8b1b2d":"This column is hard to use, so I decided to removed it.","fff074dc":"If we want to predict the sex and skin color of the perpretator it's necessary to drop the cases wich are not resolved","2ad5b44f":"*****\n##### <a id='4.2.3.2'>4.2.3.2 Equality of Odds<\/a>\nSource : [Hardt et al., 2016](https:\/\/papers.nips.cc\/paper\/6374-equality-of-opportunity-in-supervised-learning)\n\nEqualized odds postprocessing is a post-processing technique that solves a linear program to find probabilities with which to change output labels to optimize equalized odds.\n\n*I tried to used it, but an error keep happening so this is the code that I wanted to use*\n\nSome code example :\n```\nprivileged_groups, unprivileged_groups = get_attributes(data_orig_train, selected_attr=['Victim Race'])\nEOPP = EqOddsPostprocessing(privileged_groups = privileged_groups,\n                             unprivileged_groups = unprivileged_groups,\n                             seed=42)\nEOPP = EOPP.fit(data_orig_test, data_orig_test_pred)\ndata_transf_test_pred = EOPP.predict(data_orig_test_pred)\n```","1eb51892":"## <a id=\"3\">3. AIF360 Introduction<\/a>\n### <a id=\"3.1\">3.1 Create dataset using aif360<\/a>","49ade1a0":"Now you can use `mean_difference()` and `disparate_impact()` with the `BinaryLabelDatasetMetric` class and `equal_opportunity_difference()`, `average_abs_odds_difference()`, `theil_index()` with `ClassificationMetric` class.\n\n**Remember : all of metrics fair objective is 0 except for `disparate_impact()` which is 1 !**\n\nNow you can use the created classes to check bias into data and model ","8246a158":"We have to be carefull about bias for this column, specialy for the skin color value, we can see that some city have more white perpetrator.","eb48b362":"So now we see that it's the Origin model (Random Forest) is not the fairest one ! \ud83d\udc4d\n\nNow let's compare our algorithm for the performance.\n\n### <a id='4.4'>4.4 Compare algorithms performance<\/a>","2d67f2a2":"<span style=\"color:red;\">You turn<\/span>","e266002d":"Next part is the creation of the model, I just use a classic Random Forest, but I encourage you to choose the model of your choice ! :) ","9c8947a7":"With this histogram, it's obvious that there are some outliers.","237e0ae9":"#### <a id='4.2.2'>4.2.2 In-processing algorithms<\/a>\n\nIt exists 3 in-processing algorithms with aif360. But **I will only use 2** : *Adversarial Debiasing* wich is using TensorFlow and *Prejudice Remover Regularizer*. I'm not using the last one : *ART Classifier*, because I didn't found any documentation that show how it works (maybe I didn't search enough \ud83d\ude44).\n\nYou can find all the code information on [the documentation](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/inprocessing.html)\n*****\n#####  <a id='4.2.2.1'>4.2.2.1 Adversarial Debiasing<\/a>\nSource : [Zhang et al., 2018](http:\/\/www.aies-conference.com\/wp-content\/papers\/main\/AIES_2018_paper_162.pdf)\n\nAdversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary\u2019s ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.","5135c63f":"*****\n##### <a id='4.2.3.1'>4.2.3.1 Calibrated Equality of Odds<\/a>\nSource : [Pleiss et al., 2017](https:\/\/papers.nips.cc\/paper\/7151-on-fairness-and-calibration)\n\nCalibrated equalized odds postprocessing is a post-processing technique that optimizes over calibrated classifier score outputs to find probabilities with which to change output labels with an equalized odds objective.","5f49483f":"#### <a id=\"2.3.2\">2.3.2 Categorical columns<\/a>","64118b59":"As we can see this model is not really biased, but it can be better and for this example we will look how to mitigate those bias.","7597d5d9":"*****\n##### <a id='4.2.1.3'>4.2.1.3 Optimized preprocessing<\/a>\nSource : [Calmon et al., 2017](http:\/\/papers.nips.cc\/paper\/6988-optimized-pre-processing-for-discrimination-prevention)\n\nOptimized preprocessing is a preprocessing technique that learns a probabilistic transformation that edits the features and labels in the data with group fairness, individual distortion, and data fidelity constraints and objectives.\nThere is also [a demo notebook on the aif360 GitHub](https:\/\/github.com\/IBM\/AIF360\/blob\/master\/examples\/demo_optim_data_preproc.ipynb).\n\n*To be honest I tried to work with this one but it's more complicated : it uses options that you have to configure yourself and I don't really find how to choose it. Also it use an Optimizer and I didn't find how to build this class. (I didn't read the paper about this algorithm)*\n*****\n##### <a id='4.2.1.4'>4.2.1.4 Reweighing<\/a>\nSource : [Kamiran and Calders, 2012](https:\/\/link.springer.com\/article\/10.1007%2Fs10115-011-0463-8)\n\nReweighing is a preprocessing technique that Weights the examples in each (group, label) combination differently to ensure fairness before classification.","0936efa2":"# Ethics and AI : how to prevent bias on ML ? \u2696\n*Nathan Lauga*\n\nHi everyone,\nWith all the performance that AI can get, it's important to ask ourselves a question : can we understand the algorithm ? Is it fair ?\nTo answer these questions, I will construct a model based on the US Homicide Reports 1980-2014 that will predict either the sex of the perpetrator or this skin color. \n\n**The analyse will consist in looking if one of this model is biased on the victim description (sex, skin color) and if so, how to mitigate the bias.**\n\nI'm currently working in general on the aspect of ethics in AI. After this Kernel I will produce an other one concerning the interpretability of AI.\n\n**This notebook is a work version prepared for [IBM event : Watson & Cloud Academy III](https:\/\/www-01.ibm.com\/events\/wwe\/grp\/grp404.nsf\/EventLanding.xsp?openform&seminar=8QZG6KES&locale=fr_FR) in Paris, the 25th september 2019.**\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*EuQmEYhdjV6xnwn-hFFicQ.jpeg)\n\n*****\n\n## Table of content\n* <a href=\"#1\">1. Load and prepare data<\/a>\n    * <a href=\"#1.1\"> 1.1 Load libraries<\/a>\n    * <a href=\"#1.2\"> 1.2 Load the data<\/a>\n* <a href=\"#2\">2. EDA and data preparation<\/a>\n    * <a href=\"#2.1\">2.1 Prepare the data<\/a>\n    * <a href=\"#2.2\">2.2 Handle missing values<\/a>\n    * <a href=\"#2.3\">2.3 Visualization<\/a>\n        * <a href=\"#2.3.1\">2.3.1 Numerical columns<\/a>\n        * <a href=\"#2.3.2\">2.3.2 Categorical columns<\/a>\n    * <a href=\"#2.4\">2.4 Work with feature<\/a>\n        * <a href=\"#2.4.1\">2.4.1 Encoding categorical columns<\/a>\n        * <a href=\"#2.4.2\">2.4.2 Scale numerical columns<\/a>\n* <a href=\"#3\">3. AIF360 Introduction<\/a>\n    * <a href=\"#3.1\">3.1 Create dataset using aif360<\/a>\n    * <a href=\"#3.2\">3.2 First models<\/a>\n* <a href='#4'>4. Bias and Fairness<\/a>\n    * <a href='#4.1'>4.1 Metrics<\/a>\n        * <a href='#4.1.1'>4.1.1 Statistical Parity Difference<\/a>\n        * <a href='#4.1.2'>4.1.2 Equal Opportunity Difference<\/a>\n        * <a href='#4.1.3'>4.1.3 Average Absolute Odds Difference<\/a>\n        * <a href='#4.1.4'>4.1.4 Disparate Impact<\/a>\n        * <a href='#4.1.5'>4.1.5 Theil Index<\/a>\n    * <a href='#4.2'>4.2 How to fix it ?<\/a>\n        * <a href='#4.2.1'>4.2.1 Pre-processing algorithms <\/a>\n            * <a href='#4.2.1.1'>4.2.1.1 Disparate impact remover<\/a>\n            * <a href='#4.2.1.2'>4.2.1.2 Learning fair representations<\/a>\n            * <a href='#4.2.1.3'>4.2.1.3 Optimized preprocessing<\/a>\n            * <a href='#4.2.1.4'>4.2.1.4 Reweighing<\/a>\n        * <a href='#4.2.2'>4.2.2 In-processing algorithms<\/a>\n            * <a href='#4.2.2.1'>4.2.2.1 Adversarial Debiasing<\/a>\n            * <a href='#4.2.2.2'>4.2.2.2 Prejudice Remover Regularizer<\/a>\n        * <a href='#4.2.3'>4.2.3 Post-processing algorithms<\/a>\n            * <a href='#4.2.3.1'>4.2.3.1 Calibrated Equality of Odds<\/a>\n            * <a href='#4.2.3.2'>4.2.3.2 Equality of Odds<\/a>\n            * <a href='#4.2.3.3'>4.2.3.3 Reject Option Classification<\/a>\n    * <a href='#4.3'>4.3 Compare all the algorithms for one attribute<\/a>\n    * <a href='#4.4'>4.4 Compare algorithms performance<\/a>\n\n*****\n\n## <a id='1'>1. Load and prepare data<\/a>\n### <a id=1.1>1.1 Load libraries<\/a>","99250f95":"Let's see the model's performance","2a93793f":"*****\n##### <a id='4.2.1.1'>4.2.1.1 Disparate impact remover<\/a>\nSource : [Feldman et al., 2015](https:\/\/dl.acm.org\/citation.cfm?doid=2783258.2783311)\n\nDisparate impact remover is a preprocessing technique that edits feature values increase group fairness while preserving rank-ordering within groups.\nIf you want to see how it works you can take a look on [an example Notebook from the GitHub of AIF360](https:\/\/github.com\/IBM\/AIF360\/blob\/master\/examples\/demo_disparate_impact_remover.ipynb).\n\nSome code example :\n```\nfrom aif360.algorithms.preprocessing import DisparateImpactRemover\nDIR = DisparateImpactRemover()\ndata_transf_train = DIR.fit_transform(data_orig_train)\n```\n*****\n##### <a id='4.2.1.2'>4.2.1.2 Learning fair representations<\/a>\nSource : [Zemel et al., 2013](http:\/\/proceedings.mlr.press\/v28\/zemel13.html)\n\nLearning fair representations is a pre-processing technique that finds a latent representation which encodes the data well but obfuscates information about protected attributes.\nYou can find more information on [the demo notebook](https:\/\/github.com\/IBM\/AIF360\/blob\/master\/examples\/demo_lfr.ipynb).","18626402":"*****\n##### <a id='4.2.3.3'>4.2.3.3 Reject Option Classification<\/a>\nSource : [Kamishima et al., 2012](https:\/\/ieeexplore.ieee.org\/document\/6413831\/)\n\nReject option classification is a postprocessing technique that gives favorable outcomes to unpriviliged groups and unfavorable outcomes to priviliged groups in a confidence band around the decision boundary with the highest uncertainty.","b1d08bf2":"## <a id=\"1.2\">1.2 Load the data<\/a>","3b0a8841":"### <a id=\"2.4\">2.4 Work with feature<\/a>\n#### <a id=\"2.4.1\">2.4.1 Encoding categorical columns<\/a>\n","6ef2c5eb":"We need to drop the following column : \n* Record ID : it's used to be an id so unique for each record\n* Agency Code : can be relevant but it's exactly the same as Agency\n* Perpetrator Ethnicity & Age : it corresponds to the target columns","9e8f0cdc":"#### Training the model : classic Random Forest","35d67ef2":"#### <a id='4.2.3'>4.2.3 Post-processing algorithms<\/a>\n\nIt exists 3 post-processing algorithms with aif360. \n\nYou can find all the code information on [the documentation](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/postprocessing.html)\n\nI prepare the predict dataset for all the algorithms that require a dataset with predict scores and labels.","30070f6b":"### <a id=\"2.3\">2.3 Visualization<\/a>\n#### <a id=\"2.3.1\">2.3.1 Numerical columns<\/a>","0a9ed593":"### <a id=\"2.2\">2.2 Handle missing values<\/a>\nNow let's handle missing values, there are two cases to handle, numerical and categorical values.\n* Numerical values : put the median value of the column instead\n* Categorical values : put a value called \"Missing value\" instead\n\nSo the first step is to collect each median value","84b2c256":"Both the models have some good metrics (not excellent), so I will use these models for origin models.\n\n## <a id='4'>4. Bias and Fairness<\/a>\n\nToday, a problem of the model that can be produce by Machine Learning is bias that data can have. So a question is how to measure those bias and how to avoid them. In python there is a package produced by IBM called [aif360](https:\/\/github.com\/IBM\/AIF360) that can gives us some metrics and algorithms to know if our data \/ model are bias and to get a fair model.\n\n### <a id='4.1'>4.1 Metrics<\/a>\n\nSo with aif360 we have some metrics that indicate if our data or model are bias. I will use 5 metrics : \n* Statistical Parity Difference\n* Equal Opportunity Difference\n* Average Absolute Odds Difference\n* Disparate Impact\n* Theil Index\n\n#### <a id='4.1.1'>4.1.1 Statistical Parity Difference<\/a>\n\nThis measure is based on the following formula : \n\n$$ Pr(Y=1|D=unprivileged) - Pr(Y=1|D=privileged) $$\n\nHere the bias or *statistical imparity* is the difference between the probability that a random individual drawn from unprivileged is labeled 1 (so here that he has more than 50K for income) and the probability that a random individual from privileged is labeled 1.\n\nSo it has to be close to **0** so it will be fair.\n\nAlso you can find more details about that here : [One definition of algorithmic fairness: statistical parity](https:\/\/jeremykun.com\/2015\/10\/19\/one-definition-of-algorithmic-fairness-statistical-parity\/)\n\n\n#### <a id='4.1.2'>4.1.2 Equal Opportunity Difference<\/a>\n\nThis metric is just a difference between the true positive rate of unprivileged group and the true positive rate of privileged group so it follows this formula :\n\n$$ TPR_{D=unprivileged} - TPR_{D=privileged} $$ \n\nSame as the previous metric we need it to be close to **0**.\n\n#### <a id='4.1.3'>4.1.3 Average Absolute Odds Difference<\/a>\n\nThis measure is using both false positive rate and true positive rate to calculate the bias. It's calculating the equality of odds with the next formula :\n\n$$ \\frac{1}{2}[|FPR_{D=unprivileged} - FPR_{D=privileged} | + | TPR_{D=unprivileged} - TPR_{D=privileged}|]$$\n\nIt needs to be equal to **0** to be fair.\n\n#### <a id='4.1.4'>4.1.4 Disparate Impact<\/a>\n\nFor this metric we use the following formula :\n\n$$ \\frac{Pr(Y=1|D=unprivileged)}{Pr(Y=1|D=privileged)} $$\n\nLike the first metric we use both probabities of a random individual drawn from unprivileged or privileged with a label of 1 but here it's a ratio. \n\nIt changes the objective, for the disparate impact it's **1** that we need.\n\n#### <a id='4.1.5'>4.1.5 Theil Index<\/a>\n\nThis measure is also known as the generalized entropy index but with $\\alpha$ equals to 1 (more informations on [the Wikipedia page](https:\/\/en.wikipedia.org\/wiki\/Generalized_entropy_index)). So we can calculate it with this formula :\n\n$$ \\frac{1}{n} \\sum_{i=0}^{n} \\frac{b_i}{\\mu} ln \\frac{b_i}{\\mu} $$ \n\nWhere $b_i = \\hat{y}_i - y_i + 1 $\n\nSo it needs to be close to **0** to be fair.\n\n\nWith these differents metrics we can see that for some we need the prediction and for others just the original dataset. This is why we will use 2 classes of the aif360 package : `ClassificationMetric` and `BinaryLabelDatasetMetric`. \n\nFor the first one we need the prediction so we can have the following metrics : \n* Equal Opportunity Difference : [documentation link](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/metrics.html#aif360.metrics.ClassificationMetric.equal_opportunity_difference)\n   \n`equal_opportunity_difference()`\n* Average Absolute Odds Difference : [documentation link](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/metrics.html#aif360.metrics.ClassificationMetric.average_abs_odds_difference)\n\n`average_abs_odds_difference()`\n* Theil Index : [documentation link](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/metrics.html#aif360.metrics.ClassificationMetric.theil_index)\n\n`theil_index()`\n\nThen for the metrics that don't require prediction (the second class) we can use them with the following functions : \n* Statistical Parity Difference : [documentation link](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/metrics.html#aif360.metrics.BinaryLabelDatasetMetric.statistical_parity_difference)\n\n`statistical_parity_difference()`\n* Disparate Impact : [documentation link](https:\/\/aif360.readthedocs.io\/en\/latest\/modules\/metrics.html#aif360.metrics.ClassificationMetric.disparate_impact)\n\n`disparate_impact()`\n\n\nNow I construct a `DataFrame` that will keep the values of the differents metrics I talked just above with a function. ","2b7b09eb":"Now you need to find out if there are bias in the dataset or the model with metrics.\n\nRemember you can see details about metrics here : <a href='#4.1'>4.1 Metrics<\/a>\n\nYou need to construct both `ClassificationMetric` and `BinaryLabelDatasetMetric` classes because with that you'll be able to get the metrics.\n\nFirst I will do an example to construct both classes for `'Victim Race'` feature then it's you turn for the `'Victim Sex'`","79413bb5":"## <a id='5'>5. Practice<\/a>\n\nThis part is construct to exercice yourself with the aif360 package, how to construct a dataset, use some fair metrics and how to mitigate the bias.\n\nYou will write code below comments with *TODO* like this :\n```\nsome_code_already_written()\n\n# TODO : print the first 5 rows of the dataset\ndata.head()\n```\n\nThe first exercice is to create a `StandardDataset` from the original dataset which we encoded : `data_encoded` \n\nIn the previous part we work with the model that predict the perpetrator sex, now we'll work with the ethnic named **Perpetrator Race**.\n\n","a674b341":"### <a id=\"3.2\">3.2 First models<\/a>\n\nI'd like to remember that the goal of this Kernel is not to get a performant model, but the main goal is to find out how we can prevent bias on our model. So I will just construct a simple Random Forest model.\n\n#### Split into train and test set","1c009dae":"### Performance of the model","150b9745":"You can look at the information with the `meta_data()` function that I created early","d6851f63":"Great ! Now we have a `StandardDataset` and a model, let's see if bias are presents.","36131c6e":"*****\n##### <a id='4.2.2.2'>4.2.2.2 Prejudice Remover Regularizer<\/a>\nSource : [Kamishima et al., 2012](https:\/\/rd.springer.com\/chapter\/10.1007\/978-3-642-33486-3_3)\n\nPrejudice remover is an in-processing technique that adds a discrimination-aware regularization term to the learning objective."}}