{"cell_type":{"6965314d":"code","1a7efdd1":"code","ee465582":"code","0018ba96":"code","ecd6526a":"code","40b7e18a":"code","58365b90":"code","5bc1f500":"code","f2118155":"code","620c86d9":"code","dbdcda1c":"code","e5f23bb0":"code","ef5640d7":"code","8c9752cd":"code","4bc69ac8":"code","d753e1ff":"code","c8800eed":"code","ab5d55c8":"code","fdc22ff0":"code","a4dfa276":"code","82f871d8":"code","3c138a72":"code","a179e173":"code","28d85030":"code","c1c83f3f":"code","4408cabf":"code","27aefb10":"code","4242d779":"code","443a98bd":"code","c9193021":"code","47150c1d":"code","3e8b6df9":"code","dd3a31a0":"code","26232da0":"code","fdb0ba6c":"code","a9d3c7a2":"code","d6a48a06":"code","f1d6bed9":"code","6a408616":"code","e61ba1aa":"code","79e51527":"code","742ccd16":"code","0433d848":"code","aed68381":"code","7d522944":"code","109510f3":"code","47d288f7":"code","b4893551":"code","b545d849":"code","727ef290":"code","7e549668":"code","06730761":"code","4bbbac4e":"code","29db9393":"code","6d7e4cf8":"code","8ea5b553":"code","03f4c46b":"code","3ac6084e":"markdown","3ae4e391":"markdown","cb1fc880":"markdown","61dc10fb":"markdown","f5846628":"markdown","89a0c363":"markdown","023b2762":"markdown","a0a65cca":"markdown","9b8e50b2":"markdown","f77137bd":"markdown","dda1135b":"markdown","a49c1ec3":"markdown","e5c0111d":"markdown","a6f0466d":"markdown","f1cff937":"markdown","8847e058":"markdown","33058f88":"markdown","3ffb3870":"markdown","73f0a40d":"markdown","390e71fc":"markdown","e96bfad8":"markdown","c4b238d7":"markdown","71e3a5d9":"markdown","11f9513d":"markdown","f4c30696":"markdown","b6e5ef0e":"markdown","72dfd9b7":"markdown","6e399778":"markdown","1f78a909":"markdown"},"source":{"6965314d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a7efdd1":"# Check python system version\nimport sys\nsys.version","ee465582":"# load the training and test data\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')","0018ba96":"# inspect the dataframe\ntrain.head()","ecd6526a":"# inspect the dataframe for entries, columns, missing values, and data types\n\ntrain.info()\nprint('')\ntest.info()","40b7e18a":"# Statistical values for all the numerical categories\n\ntrain.describe()","58365b90":"# list of columns for segmentation\n\ntrain.columns","5bc1f500":"# split categorical and numerical dataframes for analysis\n\ndf_num = train[['Age', 'SibSp', 'Parch', 'Fare']]\ndf_cat = train.drop(['Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)","f2118155":"# shapes of quantitative features\ntrain.hist(column=df_num.columns, figsize=(20,4), layout=(1,4))","620c86d9":"# plot hist with kernal density edstimation to get an average line\nfig, ax = plt.subplots(2, 2, figsize=(10, 8))\nsns.histplot(data=train, x='Age', kde=True, ax=ax[0,0])\nsns.histplot(data=train, x='SibSp', kde=True, ax=ax[0,1])\nsns.histplot(data=train, x='Parch', kde=True, ax=ax[1,0])\nsns.histplot(data=train, x='Fare', kde=True, ax=ax[1,1])","dbdcda1c":"# Gender breakdown\nprint(train['Sex'].value_counts())\nprint()\nprint(train['Sex'].value_counts(normalize=True)) #Percentage breakdown","e5f23bb0":"fig, ax = plt.subplots(2, 3, figsize=(15,10))\nsns.barplot(x='Sex', y='Survived', data=train, ax=ax[0,0])\nsns.barplot(x='Embarked', y='Survived', data=train, ax=ax[0,1])\nsns.barplot(x='Pclass', y='Survived', data=train, ax=ax[0,2])\nsns.barplot(x='SibSp', y='Survived', data=train, ax=ax[1,0])\nsns.barplot(x='Parch', y='Survived', data=train, ax=ax[1,1])\n# sns.barplot(x='SibSp', y='Survived', data=train, ax=ax[1,2])","ef5640d7":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\nsns.countplot(x='Sex', data=train, ax=ax[0])\nsns.countplot(x='Pclass', data=train, ax=ax[1])","8c9752cd":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\nsns.countplot(x='Sex', data=train, hue='Survived', ax=ax[0])\nsns.countplot(x='Pclass', data=train, hue='Survived', ax=ax[1])","4bc69ac8":"columns = ['Sex', 'Pclass', 'Embarked', 'SibSp', 'Parch']\nfor column in columns:\n    print(pd.pivot_table(train, index='Survived', columns=column, values='Name', aggfunc='count'))\n    print()","d753e1ff":"# Analysis of SES - Pclass survival rates, counts, and averages\n\nprint('Socio Economic Class - Count')\nprint(train.groupby(['Pclass']).count())\nprint()\nprint('Socio Economic Class - Mean')\nprint(train.groupby(['Pclass']).mean())","c8800eed":"print('Value Counts')\nprint(pd.pivot_table(train, index='Pclass', columns='Embarked', values='Name', aggfunc='count'))\nprint('')\nprint('Average Fare Amount')\nprint(pd.pivot_table(train, index='Pclass', columns='Embarked', values='Fare', aggfunc='median'))","ab5d55c8":"fig, ax = plt.subplots(1, 2, figsize=(12, 4))\nsns.boxplot(x='Embarked', y='Fare', data=train, ax=ax[0])\nsns.boxplot(x='Embarked', y='Age', data=train, ax=ax[1])\n# sns.boxplot(x='Embarked', y='Age', hue='Sex', data=train, ax=ax[2])\n\nax[0].set_title('Fare Boxplot By Port of Embarkment')\nax[1].set_title('Age Distribution Boxplot By Embarkement')","fdc22ff0":"cut_labels = ['>10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80']\n# cut_bins = [0, 70000, 100000, 130000, 200000]\ncut_df = train\ncut_df['cut_age'] = pd.cut(cut_df['Age'], bins=8, labels=cut_labels)\nage_bin_df = cut_df.groupby(['cut_age']).sum()\nprint(type(age_bin_df))\nage_bin_df","a4dfa276":"train = train.drop(columns=['cut_age'])","82f871d8":"g = sns.countplot(data=cut_df, x='cut_age', hue='Survived')\ng.set_title('Surviors by Age')","3c138a72":"def boxplt(column):\n    sns.set_theme(style='whitegrid')\n    g = sns.boxplot(x=train[column])\n    g.set_title(f\"{column} Outlier Boxplot\")\n    return g ","a179e173":"boxplt('Fare')","28d85030":"train.loc[train['Fare'] > 500]","c1c83f3f":"# The outliers had similar ticket number structures with 'PC'. We find that all tickets with 'PC' had a 65% of survival. \n\nticket_pc_df = train[train['Ticket'].str.match(\"PC\")]\nprint(ticket_pc_df.describe())","4408cabf":"# Inspect 1st class ticket structure\n\npclass_df = train.loc[train['Pclass'] == 1]\npclass_df.groupby(['Ticket']).mean()","27aefb10":"# Inspect patterns for tickets with Letters in 1st class who survived - 'PC' in all but one\n\ntickets_df = train.loc[train.Ticket.str.contains('[A-Za-z]')]\ntickets_df2 = train.copy()\ntickets_df.loc[train.Pclass == 1].loc[train.Survived == 1]\ntickets_df2['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntickets_df2\npd.pivot_table(tickets_df2, index='Survived', columns='numeric_ticket', values='Name', aggfunc='count')","4242d779":"# Simply just having any letter in the 'Ticket' would not help your chance of survival.\n\ntickets_df = train.loc[train.Ticket.str.contains('[A-Za-z]')]\ntickets_df.describe()","443a98bd":"# Analyze tickets with the letters 'P', 'C', or 'A'\n# Again, we find that 'P' has the highest rate in these numbers for survival but lower than 'PC'\n\nletters = ['P', 'C', 'A']\nfor letter in letters:\n    letter_df = train.loc[train.Ticket.str.contains(letter)]\n    percent = letter_df.Survived.mean()\n    count = letter_df.Survived.count()\n    print(f'Letter {letter}: {round(percent*100, 1)}% survival rate out of {count} tickets')","c9193021":"boxplt('SibSp')","47150c1d":"# We see that the outliers came from 2 families\n\nfamily_df = train.loc[train['SibSp'] > 4]\nfamily_df.sort_values('Name')","3e8b6df9":"# The Goodwin family of 6\n\ntrain.loc[train['Ticket'] == 'CA 2144']","dd3a31a0":"# The Sage family of 7\n\ntrain.loc[train['Ticket'] == 'CA. 2343']","26232da0":"# Survival rates for group size.\n\ntrain.groupby(['SibSp'])['Survived'].mean()","fdb0ba6c":"boxplt('Age')","a9d3c7a2":"# Inspect passengers Ages 65 and up\n\nage_df = train.loc[train['Age'] > 64]\nage_df.sort_values('Age', ascending=False)","d6a48a06":"title_df = train.copy()\ntitle_df['title'] = train.Name.apply(lambda x: x.split(',')[1].split(' ')[1].strip())\ntitle_df['title'].value_counts()\npd.pivot_table(title_df, index='Survived', columns='title', values='Name', aggfunc='count')","f1d6bed9":"title_df2 = title_df.groupby(['title']).mean().reset_index(inplace=False)\ntitle_df2 = title_df2.sort_values('Survived', ascending=False)\ng = sns.barplot(x='title', y='Survived', data=title_df2)\ng.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')","6a408616":"g = sns.barplot(x='title', y='Survived', data=title_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')","e61ba1aa":"# inspect the column shape and NaN values\nprint('Test Data', test.info())\nprint()\nprint('Train Data', train.info())","79e51527":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = MinMaxScaler()\n\n# cabin - will we do this feature??\n\n# name titles\ntrain['title'] = train.Name.apply(lambda x: x.split(',')[1].split(' ')[1].strip())\ntest['title'] = test.Name.apply(lambda x: x.split(',')[1].split(' ')[1].strip())\n\n# age and fare imputation w\/ median to acccount for outliers\n# scale\ntrain.Age = train.Age.fillna(train.Age.median())\ntest.Age = test.Age.fillna(test.Age.median())\ntest.Fare = test.Fare.fillna(test.Fare.mean())\n\n# created dummy variables from categories (also can use OneHotEncoder)\n# sex, embarked, pclass, title get dummies\ntrain = pd.get_dummies(train, columns=['Sex', 'Pclass', 'Embarked', 'title', 'SibSp', 'Parch'])\ntest = pd.get_dummies(test, columns=['Sex', 'Pclass', 'Embarked', 'title', 'SibSp', 'Parch'])\n\n# numeric tickets\n# tickets with 'PC'\ntrain['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain['pc_ticket'] = train['Ticket'].str.match(\"PC\").apply(lambda x: 1 if True else 0)\ntest['numeric_ticket'] = test.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntest['pc_ticket'] = test['Ticket'].str.match(\"PC\").apply(lambda x: 1 if True else 0)\n\n# age normalized\ntrain['age_norm'] = scaler.fit_transform(train[['Age']])\ntest['age_norm'] = scaler.fit_transform(test[['Age']])\n\n# fare normalized\ntrain['fare_norm'] = scaler.fit_transform(train[['Fare']])\ntest['fare_norm'] = scaler.fit_transform(test[['Fare']])","742ccd16":"# Create the training and test datasets\nX_train = train.drop(columns=['Survived', 'Fare', 'Name', 'Age', 'Ticket', 'Cabin', 'title_Mme.', 'title_the', 'title_Sir.', 'title_Mlle.', 'title_Major.', 'title_Lady.', 'title_Jonkheer.', 'title_Don.', 'title_Capt.'])\nX_test = test.drop(columns=['Fare', 'Name', 'Age', 'Ticket', 'Cabin', 'title_Dona.', 'Parch_9'])\ny_train = train['Survived']\nprint(f'y_train.shape: {y_train.shape}')\nprint(f'X_train.shape: {X_train.shape}')\nprint(f'X_test.shape: {X_test.shape}')\nprint('X_train Columns:', X_train.columns)\nprint('X_test Columns:', X_test.columns)","0433d848":"# inspect the X_train dataset\nX_train.head(3)","aed68381":"# Import machine learning libraries\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier \nimport time","7d522944":"# create function to output scores for each model\n\ndef clf_scoring(clf):\n    clf = clf.fit(X_train, y_train)\n    cv_score = cross_val_score(clf, X_train, y_train, cv=5)\n    clf_id = clf.__class__.__name__\n    print(f'Model: {clf}')\n    print('single pass score:', round(clf.score(X_train, y_train)*100, 2))\n    print('k-fold scores:', cv_score)\n    print(f'cross-validated score: {round(cv_score.mean()*100, 2)} std+\/- {cv_score.std()}')\n    print()\n    return dict({\n        'id': clf_id, 'score roc_auc': cv_score.mean(), 'singe pass score': clf.score(X_train, y_train), \n        'std': cv_score.std(), 'model': clf, 'params': clf.get_params()\n    })","109510f3":"clfs = [DecisionTreeClassifier(), RandomForestClassifier(), \n        LogisticRegression(), KNeighborsClassifier(), \n        GaussianNB()] # XGBClassifier() funny error, will wait to do it later\nclf_outputs = []\nfor model in clfs:\n    clf_outputs.append(clf_scoring(model))","47d288f7":"# Sorted ML models based on ROC_AUC or cross-validated score\n\nclf_df = pd.DataFrame(clf_outputs)\nclf_df.sort_values('score roc_auc', ascending=False)","b4893551":"clf_df.loc[1, 'model']","b545d849":"# Best baseline model for submission\nparams = clf_df.loc[2].params\nclf = LogisticRegression(**params).fit(X_train, y_train)","727ef290":"# Confusion Matrix\n\nfrom sklearn.model_selection import train_test_split\n\ncm_X_train, cm_X_test, cm_y_train, cm_y_test = train_test_split(\n    X_train, y_train, train_size=.7, random_state=0)\n\nclf_cm = LogisticRegression(**params).fit(cm_X_train, cm_y_train)\ncm_y_hat = clf.predict(cm_X_test)\n\nprecision = round(precision_score(cm_y_test, cm_y_hat, average='binary')*100, 2)\nrecall = round(recall_score(cm_y_test, cm_y_hat, average='binary')*100, 2)\nf1 = round(f1_score(cm_y_test, cm_y_hat, average='binary')*100, 2)\n\nprint('LogisticRegression Model')\nprint(f'Precision score: %{precision}')\nprint(f'Recall score: %{recall}')\nprint(f'F1 score: %{f1}')\nprint('')\nprint(f'Confusion Matrix {plot_confusion_matrix(clf_cm, cm_X_test, cm_y_test)}')  ","7e549668":"rf = RandomForestClassifier()\nrf = rf.fit(X_train, y_train)\n# .best_estimator_.fit(X_train,y_train)\nfeat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\n\n# clf = clf.fit(X_train, y_train)\n# cv_score = cross_val_score(clf, X_train, y_train, cv=5)","06730761":"dt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\nlr = LogisticRegression()\nknn = KNeighborsClassifier()\ngnb = GaussianNB()","4bbbac4e":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n    \"max_depth\": range(1,20,2)}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","29db9393":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    t1 = time.time()\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    t2 = time.time()\n    print(classifier[i])\n    print(f'Time elapsed: {round(t2-t1, 2)} seconds')\n    print(f'Score: {round(cv_result[i]*100, 2)}%')\n    print('')","6d7e4cf8":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(f'Ensemble Socre: {round(accuracy_score(votingC.predict(cm_X_test),cm_y_test)*100,2)}%')","8ea5b553":"# Predict the outcomes for the test dataset\ny_hat = votingC.predict(X_test)","03f4c46b":"# prepare submission\nsubmission_tree = pd.DataFrame({'PassengerID': X_test.index, 'Survived': y_hat})\nsubmission_tree.to_csv('voticC_submission.csv', index=False)\nsubmission_tree","3ac6084e":"<a id=\"preprocessing\"><\/a>\n# <div align='center'>4. Preprocessing Data\n    \nThe training and test datasets don't have the same attributes after feature engineering. Therefore, I had to be careful about choosing with features to leave in the datasets.\n    \n## Feature engineer:\n* **Ticket lettering** - with 'P' have a 58% chance of survival -> **refer to above code**\n* **Pclass** Include 1st and 2nd class. Drop 3rd class?\n* **Age immputation** for continuous data - use mean\/median `fillna()`\n* **Fare and Age scaling\/normalization** for ML optimization. Gaussian calculations do not like large variance in values: [article1](https:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/), [article2](https:\/\/stackoverflow.com\/questions\/51237635\/difference-between-standard-scaler-and-minmaxscaler)\n* **Family size SibSp** binning two groups of 1-3, >3\n* **Name title segregation** - split\/strip titles out names: [source](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.str.split.html)\n* **One Hot Encoding** - categorial data must me turned into numerical with [One Hot Encoding or Get Dummies](https:\/\/towardsdatascience.com\/what-is-one-hot-encoding-and-how-to-use-pandas-get-dummies-function-922eb9bd4970)","3ae4e391":"### Observations:\n* People were more chivalrous back in then? Or women were better able to negotiate their way on to life boats? Maybe they were with their children?\n* ","cb1fc880":"### Observations\n* Fare has a wide distribution. Let's dive deeper:\n    * Viz - histogram, normalize\n    * Pivot table by various categories: Age, Sex, Survived, Pclass\n* Create histograms for `['Age', 'SibSp', 'Parch', 'Fare']`to see spread of instances","61dc10fb":"# <div align='center'>Outliers Analysis\n* Fare\n* Sibsp\n* Ticket numbering\n* Age","f5846628":"### Observations:\n* Many more females survived\n* People who embarked from Cherbourg had higher survival rate. Let's explore this further?\n* First and second class passangers survived better. Due to location of cabins?\n* Smaller families had higher survival rates,best if you had 1 or 2 other people in your family.","89a0c363":"<a id=\"setup\"><\/a>\n# <div align='center'>2. Setup System Environment","023b2762":"## Findings:\nTickets with the letter `'PC'` had a **65% survival rate**. Because of this, we should add this as a feature to our model.","a0a65cca":"## Title Findings:\n* Titles were a fairly good predictor if you survived the Titanic\n* Female titles faired very well","9b8e50b2":"<a id=\"overview\"><\/a>\n# <div align='center'>1. Overview\n## The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n## Goal\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each in the test set, you must predict a 0 or 1 value for the variable.\n\nMetric\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\nSubmission File Format\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\nPassengerId (sorted in any order)\n\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\n\n```\nPassengerId,Survived\n892,0\n893,1\n894,0\nEtc.\n```","f77137bd":"## Sibling or Spouse Outlier Analysis","dda1135b":"<a id=\"training\"><\/a>\n# <div align='center'>5. Train Model\n\n\n**The highest out of the box, cross-validated model: LogisticRegression %81.59 Accuracy** \n\nUsing 'Cross Validation' to find the average scores for each alogrithm. This will help to prevent scores from seeming to overfit. \n* [sklearn cross-validation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)\n* [Plotting the confusion matrix](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix)\n* [Precision and Recall](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html)\n\nI will test 5 different ML models to baseline prediction accuracy:\n* [Decision Tree](https:\/\/scikit-learn.org\/stable\/modules\/tree.html)\n* [Random Forest](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html)\n* [Logistic Regression](https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/) - I use a logistic versus a linear regression because our problem is a binary classification. A sigmoid line is preferencial to that of a straight one.\n* [K Nearest Neighbor](https:\/\/scikit-learn.org\/stable\/modules\/neighbors.html)\n* [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)\n* [XGBoost](https:\/\/machinelearningmastery.com\/develop-first-xgboost-model-python-scikit-learn\/) - I decided to try out XGBoost instead of the ScikitLearn version of gradient boosting because: A. I haven't tried either and B. The reviews say this is efficient. \n\n``` \nimport graphviz \ndot_data = tree.export_graphviz(clf, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"iris\") \n```","a49c1ec3":"<a id=\"conclusion\"><\/a>\n# <div align='center'>7. Conclusion\n* Write up\n* Submissions with different names","e5c0111d":"## Age Outlier Analysis","a6f0466d":"### Observations:\n* People from Cherbourg, France are able to pay more money for fare, and support the hypothesis that wealther passengers had a higher chance of survival. \n* Major outliers for Cherbourg dataset \n* Wealth matters. Please from 1st class had higher rates of survival, and more people survived rather than drowned compared to the other classes. \n* You have a higher chance of surviving if you're solo or have 1 family member with you compared to a large family. Maybe it was difficult to choose which family member would survive? Or maybe it was more expensive to have a 1st class cabin and better location to boats?\n\n","f1cff937":"* [feature importances](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html)","8847e058":"### Observations:\n* Survival based on age followed a somewhat normal distribution with a tail to the right with older people.\n* Young children and infants survived more than died. Probably the mentality of women and children to be saved first. \n* High rates of survival for people in their 30s, 40s and 50s. \n","33058f88":"<a id=\"eda\"><\/a>\n# <div align='center'>3. Exploratory Data Analysis\nCredits: [Ken Jee](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example), [Emanuele Panizio](https:\/\/www.kaggle.com\/emanuelepanizio\/pytanicnb\/notebook), [Ju Liu](https:\/\/www.youtube.com\/watch?v=fS70iptz-XU&t=2569s]), [Sharlto Cope](https:\/\/www.kaggle.com\/dwin183287\/titanic-machine-learning-from-disaster-eda)\n\nReferences: [Data](https:\/\/www.kaggle.com\/c\/titanic\/data)\n\nFunctions to try out:\n* create subplots \n* seaborn and plt plots\n* sns.barplot\n* sns.countplot\n* [sns.histplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.histplot.html)\n* sns.catplot\n* sns.displot\n* pivot_table\n* plot fares as line and bins","3ffb3870":"### Observations:\n* Only 'Age' has a somewhat normal distribution. The other values are skewed to the left with longtails to the right. \n    * Does age group affect the survival rate? Bin by age group\n    * Most of the population is in their late teens to late 30s.\n* Most paid a low fare, many are single with no family\n* Should we normalize these values using a logarithmic method?","73f0a40d":"## Age Outlier Findings:\nWhile it doesn't bode well to be elderly on the Titanic, the eldest person actually survived! He actually was able to buoy himself on his briefcase and a fur coat until he found an overturned lifeboat. To learn more visit the biography of [Mr. Algernon Barkworth](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/algernon-barkworth.html).","390e71fc":"## Title Analysis\nHere I will look if the title of the passenger had any correlation to surving the Titantic - e.g. Mr., Mrs., etc.","e96bfad8":"### Observations:\n* ML models must have numerical values to compute, there fore categorical datatype `objects` must be converted to numerical values.\n* Nan values to address with imputation: `['Age', 'Fare', 'Cabin', 'Embark']`\n* Nan values to drop: `['Name']`\n* Test dataset has 1 less column: `['Survived']` which is the target or label","c4b238d7":"# Table of Contents\n<a id='table-of-contents'><\/a>\n1. [Overview](#overview)\n1. [System Setup](#setup)\n1. [Exploratory Data Analysis](#eda)\n1. [Preprocessing](#preprocessing)\n1. [Training Models](#training)\n1. [Evaluate -> Tune -> Ensemble](#evaluate)\n1. [Conclusion](#conclusion)\n\n**TLDR: I am able to predict surival with 82% accuracy.**","71e3a5d9":"<a id=\"evaluate\"><\/a>\n# <div align='center'>6. Evaluate -> Tune -> Ensemble\n* [gridsearch](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html)\n* [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html)\n* ensembles - hard and soft voting: [sklearn ensemble overview](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html), [VotingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html)","11f9513d":"### Findings:\nThese outliers paid the princely sum of 512 pounds. In today's pound it would be worth a staggering **58,864 pounds**! (Note: each group had both the same ticket number and fare paid). This was by far the most out of any of the guests. Part of the same group lead by the wealthy Mr. Cardeza - Miss. Ward the maid, and Mr. Lesurer the manservant. They were on their way back to the USA by way of Cherbourg, France. \n* All similar age\n* Multiple cabins but interestly all on the 'B' deck. **Could this be the best deck on the ship? We should analyze deck levels for our model.**\n* **Feature: Cherbourg was the best place to leave if you wanted to survive. We will add this to our list of features for the ML models.**\n* **Feature: Maybe is there a correlations between 'PC' tickets and survival rate?**\n\nTo learn more about this group visit this biography of [Miss. Anna Ward](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/annie-moore-ward.html)","f4c30696":"## Create the X_train, X_test, and y_train Datasets","b6e5ef0e":"## Fare Outlier Analysis","72dfd9b7":"![titanic](https:\/\/www.history.com\/.image\/ar_233:100%2Cc_fill%2Ccs_srgb%2Cg_faces:center%2Cq_auto:good%2Cw_3840\/MTc2NTQ1ODM1NDQwNDgyMDU4\/sinking-of-the-titanic-gettyimages-542907919-1.webp)","6e399778":"### Ticket Number Analysis","1f78a909":"## Family Size Outlier Findings:\nUnforetunately, the Goodwin and Sage families - who had 5 or more members - did not fare well. They all perished. Maybe it's because of their socio economic status. Maybe it's because there was just too many of them to quickly escape to a life boat. Or maybe it's because of their cabin placement. Having a large family was tragic in the Titanic.\n\n**Can we bin people by family or not? Maybe people had a higher survival rate if they had fewer family members?**"}}