{"cell_type":{"ad3f25dd":"code","924023f5":"code","e769cf05":"code","b86de415":"code","ff420853":"code","797061f1":"code","dceb4204":"code","0a6cf36f":"code","f660ac38":"code","ddaf2cdb":"code","664c14a3":"code","98902335":"code","fb0b8183":"code","dabe2d98":"code","c6f0cbce":"code","93bab775":"code","eb2a5e4f":"code","250fde25":"code","774ca8db":"code","c115761b":"code","c81198eb":"code","96a49dbc":"code","816c4b65":"markdown","8de4e044":"markdown","6846aa94":"markdown","46aa23ea":"markdown","5695c788":"markdown","ae95bc9a":"markdown","b60e7fbc":"markdown","20d7b0f4":"markdown","0ddca551":"markdown","ef265a09":"markdown","bbfc94f1":"markdown","6c190c45":"markdown","1285f9e0":"markdown","d89411e4":"markdown","be14bec0":"markdown","87e9e2a0":"markdown","ead04a18":"markdown","bbaefc8b":"markdown","99cdc4f1":"markdown"},"source":{"ad3f25dd":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = FutureWarning)\n%matplotlib inline\nimport matplotlib.pyplot as plt","924023f5":"sdss_pd = pd.read_csv(\"..\/input\/Skyserver_SQL2_27_2018 6_51_39 PM.csv\")\nsdss_pd.head()","e769cf05":"sdss_pd.info()","b86de415":"sdss_pd[\"class\"].value_counts().sort_index()","ff420853":"sdss_pd.columns.values","797061f1":"sdss_pd.drop([\"objid\",\"specobjid\",\"run\",\"rerun\",\"camcol\",\"field\"], axis = 1, inplace = True)\nsdss_pd.head()","dceb4204":"print(\"Mapping: \", dict(enumerate([\"GALAXY\",\"QSO\",\"STAR\"])))\nsdss_pd[\"class\"] = sdss_pd[\"class\"].astype(\"category\")\nsdss_pd[\"class\"] = sdss_pd[\"class\"].cat.codes\nprint(sdss_pd[\"class\"].value_counts().sort_index())","0a6cf36f":"corr_matrix = sdss_pd.corr()\ncorr_matrix[\"class\"].sort_values(ascending = False)","f660ac38":"sdss_feat = sdss_pd.drop(\"class\", axis = 1)\nsdss_labels = sdss_pd[\"class\"].copy()","ddaf2cdb":"X_train, X_test, y_train, y_test = train_test_split(sdss_feat, sdss_labels, test_size=0.2, random_state=42, stratify=sdss_labels)","664c14a3":"default_forest = RandomForestClassifier(random_state = 42)\ndefault_forest.fit(X_train, y_train)\ndefault_forest.get_params()","98902335":"print(\"Test accuracy for default forest:\", default_forest.score(X_test, y_test))","fb0b8183":"y_pred = default_forest.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix_pd = pd.DataFrame(data = conf_matrix, \n                              index = [\"GALAXY\",\"QSO\",\"STAR\"],\n                              columns = [\"GALAXY\",\"QSO\",\"STAR\"])\nconf_matrix_pd","dabe2d98":"feat_imp_pd = pd.DataFrame(data = default_forest.feature_importances_,\n                          index = sdss_feat.columns,\n                          columns = [\"Importance\"])\nfeat_imp_pd = feat_imp_pd.sort_values(by = 'Importance', ascending = False)\nfeat_imp_pd","c6f0cbce":"feat_imp_pd.plot(kind = \"bar\", figsize = (10,5), grid = True)\nplt.show()","93bab775":"n_estimators = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\nmax_features = ['auto', 'log2']\nmax_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               scoring = 'accuracy', \n                               n_iter = 25, \n                               cv = 4, \n                               verbose = 2, \n                               random_state = 42,\n                               n_jobs = -1)\nrf_random.fit(X_train, y_train)","eb2a5e4f":"rf_random.best_score_","250fde25":"best_forest = rf_random.best_estimator_\nbest_forest","774ca8db":"print(\"Test accuracy for best forest:\", best_forest.score(X_test, y_test))","c115761b":"y_pred = best_forest.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\nconf_matrix_pd = pd.DataFrame(data = conf_matrix,\n                              index = [\"GALAXY\",\"QSO\",\"STAR\"],\n                              columns = [\"GALAXY\",\"QSO\",\"STAR\"])\nconf_matrix_pd","c81198eb":"feat_imp_pd = pd.DataFrame(data = best_forest.feature_importances_,\n                           index = sdss_feat.columns,\n                           columns = [\"Importance\"])\nfeat_imp_pd = feat_imp_pd.sort_values(by = 'Importance', ascending = False)\nfeat_imp_pd","96a49dbc":"feat_imp_pd.plot(kind = \"bar\", figsize = (10,5), grid = True)\nplt.show()","816c4b65":"We need to transform the class column in a more convenient way than a list of string. Here, we will transform the 3 strings into 3 integers:","8de4e044":"## Introduction\n\nIn this notebook, a random forest will perform a task of classification (galaxy, quasar or star) based on space observations. The data come from  the **Sloan Digital Sky Survey** (release 14). For more information on this dataset, [here is the link to its \"Overview\" section on kaggle](https:\/\/www.kaggle.com\/lucidlenn\/sloan-digital-sky-survey\/home).  \nYou can also visit the SDSS website for even more information: https:\/\/www.sdss.org\/.\n\nSo, let's get started by importing libraries:","6846aa94":"## Deeper training\n\nTo improve the model, we perform at the same time a random search for hyperparemeters optimization and a 5-fold cross validation. The hyperparameters that we try to optimize here are:\n* **n_estimators**: Number of trees in the forest\n* **max_features**: Number of features the tree can use at every split\n* **max_depth**: Maximum level of a tree\n* **min_samples_split**: Minimum number of samples required to split a node\n* **min_samples_leaf**: Minimum number of samples in a leaf node\n* **bootstrap**: Method to choose the samples for training each tree  \n\nWe are going to to train 25 models with different hyperparemeters. With the 4-fold CV, the total is 100 models to train.","46aa23ea":"As predicted before, **mjd** and **plate** are two important features but the most important one is the **redshift**. The correlation matrix did not point out this relation.\nMoreover, 3 features have less than 1%: **dec**, **fiberid** and **ra**. We can consider the possibility to drop these features.\nFinally, with the relative importance of **i**, **g**, **z**, **u** and **r**, dimension reduction can also be an option.","5695c788":"Finally, we can plot the importance of each feature:","ae95bc9a":"## Training the model and evaluate it\n\nLet's see what a random forest with the default values of scikit can achieve. It is trained on the whole training set and evaluated on the test set.","b60e7fbc":"## Conclusion\n\nDuring this notebook, I tried to present the workflow of a machine learning problem, from data analysis to testing the model in a simple way. We explored the SDSS RD14 dataset, selected the revelant features at first sight by analyzing them. Then we created a simple random forest classifier which we tried to improve and took some insights from. Finally, we gave some paths to explore in order to enhance the model.  \nAs I am not an expert and as I am still learning new concepts constantly on machine learning and trying to apply them to real world problems, I found this dataset interesting to apply some of the techniques I have learned.","20d7b0f4":"There are already a few features that we can guess are not useful for the classification task. Both **objid** and **specobjid** are just identifers in the original dataset. Moreover, features related to the camera (**run**, **rerun**, **camcol**, **field**) can also be dropped:","0ddca551":"Ok, so 17 feature columns with various types (**int64**, **float64**) and 1 target column. Fortunately, no missing values. Let's see how many examples we have for each category:","ef265a09":"Now, we can make some computations, for example with the correlation matrix.","bbfc94f1":"We can evaluate some metrics with the test set:","6c190c45":"The model obtained with a random search performs better than the default one from scikit-learn: **98.875%** against **99.1%**  which is more than **0.2%** of improvement. The confusion matrix has the same structure as the previous one (e.g. a recall of 100% for the \"GALAXY\" class). The bar chart for the feature importance confirms what has been stated for the previous one and suggests more processing on the features.","1285f9e0":"Almost **50%** of the examples are **GALAXY**, whereas **QUASAR** is less than **10%** of the examples. This inequality in the repartition of the labels may be a problem later.","d89411e4":"# Classification of Stars, Galaxies and Quasars with a simple random forest\n## Dataset from the Sloan Digital Sky Survey RD14\n---","be14bec0":"## Further developments\n\nSome ways of improvement have been stated before, let's summarize the ones which are available here.\n* **Scaling** the values of each feature. Generally, scaling helps improving the model performances. *Normalization* or *standardization* can be performed.\n*  **Dropping** more features like *dec*, *fiberid* and *ra*.\n* **Feature engineering** and **Feature extraction** e.g. *PCA* (Principal Component Analysis).\n* **Using different metrics**. Here, we evaluated the performances of our model mostly on the accuray and a bit on the confusion matrix. However, different metrics exist and can be revelant according to what you want to achieve: *recall*, *precision*, *F1 score*, *ROC AUC*.","87e9e2a0":"For now,  the values will not be scaled, as scaling is not mandatory for the random forest. But it can still give room for possible improvements.","ead04a18":"The **correlation coefficient** is a rough representation of how much a variable is **related** to another: it ranges from -1 to 1, extremum mean complete correlation and 0 means no relation.  \nFrom the output, 2 features seem very correlated: **mjd** and **plate**. With the random forest, we will see if these relations are meaningful.","bbaefc8b":"## Data exploration\n\nLet's import the data in a Dataframe in order to have a rough idea of what the dataset looks like.","99cdc4f1":"We are going to separate the target column from the others and split the dataset into a training set and a test set with a stratified method where the repartition of each label in both sets is the same as the original one to limit bias."}}