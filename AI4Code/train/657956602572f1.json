{"cell_type":{"685602f8":"code","25576a0c":"code","1dc15897":"code","16c13a1f":"code","76d5e00f":"code","74ee28ee":"code","bf5c34f8":"code","8bca61d9":"code","f1dbbb59":"code","d869c785":"code","9c248eb3":"code","f3066108":"code","7d6671a9":"code","b0ed33d6":"code","4b6095f7":"code","c3e19289":"code","e27221e5":"code","e5adb2a9":"code","3b2150b6":"code","1949ed27":"code","9e1a44b7":"code","5097c62d":"code","8f672084":"code","b30c1794":"code","35583d3b":"code","0a5aeb2c":"code","527fe9a9":"code","13fc7561":"code","ac1076b0":"code","9f547254":"code","80cf7b08":"code","5b3d9929":"code","8581ba95":"code","ef080048":"code","b4cf08f8":"markdown","ef64c449":"markdown","0b123b80":"markdown","2c736cce":"markdown","1ffd0c48":"markdown","990a3966":"markdown","eb6b6147":"markdown","b143a8ec":"markdown","5f792a17":"markdown","bf5f47af":"markdown","7189ac25":"markdown","05d3df94":"markdown","a5a8341e":"markdown","c892e93c":"markdown","4d35bd54":"markdown","e211b7c8":"markdown","0c6b6b36":"markdown","3616d7fe":"markdown","e96aed89":"markdown","14354b52":"markdown","e0859b64":"markdown","65888af8":"markdown","cb431a70":"markdown","73e0fc3b":"markdown","e620b823":"markdown","795d67d4":"markdown","675d2821":"markdown","3947858b":"markdown","62286fe7":"markdown","a5bcba62":"markdown","bc8fbb3d":"markdown","d5068373":"markdown","75c6de5f":"markdown","123e3a60":"markdown","dd67045e":"markdown","36636d05":"markdown","83c9e2fa":"markdown","968fbf7c":"markdown","fd949f3c":"markdown","7778de34":"markdown","566d9d8a":"markdown","ecf8c744":"markdown","bcb70e7d":"markdown","ad8b766f":"markdown","03454a00":"markdown","1d24877f":"markdown","dd64f6b6":"markdown","58b55234":"markdown","5263c6b3":"markdown","5f5b6b83":"markdown","19d425b3":"markdown","8b3b6081":"markdown","3bfffa47":"markdown","83654b6a":"markdown","798f3340":"markdown","e95ac29d":"markdown","97efc1f8":"markdown","6934e015":"markdown","dd0d21cf":"markdown","93fd28df":"markdown","9e138333":"markdown"},"source":{"685602f8":"import numpy as np \nimport pandas as pd \nfrom scipy import stats\nimport seaborn as sns\nimport pylab \nimport matplotlib.pyplot as plt\n\ndf_heart = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf_heart.columns\ntrestbps = df_heart['trestbps']\nchol = df_heart['chol']\ntarget_heart = df_heart['target']\ndf_health = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\nage = df_health['Age']\ntarget_health = df_health['Response']\n","25576a0c":"years = [1, 3, 8]\nsalary = [8, 11, 14]\nvalues = list(zip(years, salary))\nnames = ['years', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf","1dc15897":"df.plot.line('years', 'salary', color='black')\nplt.show()","16c13a1f":"years = [1, 3, 8, 10, 11]\nsalary = [8, 11, 14, 14, 15]\nvalues = list(zip(years, salary))\nnames = ['years', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf.plot.line('years', 'salary', color='black')\nplt.show()","76d5e00f":"years = [1, 3, 4, 8, 10, 11]\nsalary = [8, 11, 9, 14, 14, 15]\nvalues = list(zip(years, salary))\nnames = ['years', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf.plot.line('years', 'salary', color='black')\nplt.show()","74ee28ee":"def monotony(feature, target):\n    '''\n    A simple function for determining the monotony of the feature-target relationship.\n    '''\n    num_samples = len(target)\n    feature_name = feature.name\n    target_name = target.name\n    df = pd.concat([feature, target], axis=1)\n    # sorts with priority 1: feature, priority 2: target\n    df_sorted = df.sort_values([feature_name, target_name], ascending=[True, True])\n    # first target value after sorting:\n    first_target_val = df_sorted.loc[0, [target_name]].values[0] \n    \n    \n    \n    # monotoniccally increasing ? \n    def mon_inc(target_val):\n        nonlocal last_target_val\n        nonlocal violations_inc\n        if (target_val < last_target_val):\n            violations_inc = violations_inc + 1\n        last_target_val = target_val\n            \n    last_target_val = first_target_val\n    violations_inc = 0\n    df_sorted[target_name].apply(mon_inc)\n    \n     # monotoniccally decreasing ? \n    def mon_dec(target_val):\n        nonlocal last_target_val\n        nonlocal violations_dec\n        if (target_val > last_target_val):\n            violations_dec = violations_dec + 1\n        last_target_val = target_val\n            \n    last_target_val = first_target_val\n    violations_dec = 0\n    df_sorted[target_name].apply(mon_dec)\n    \n    \n    \n    # scores:\n    score_inc = 1 - round(violations_inc \/ num_samples,2)\n    score_dec = 1 - round(violations_dec \/ num_samples,2)\n    return [score_inc, score_dec, violations_inc, violations_dec]\n        \n    \n    \n    \nmonotony_metrics = monotony(df['years'], df['salary'])\nprint(f'monotonically increasing? violations: {monotony_metrics[2]}, monotony score: {monotony_metrics[0]}\\n' +\n     f'monotonically decreasing? violations: {monotony_metrics[3]}, monotony score: {monotony_metrics[1]}')","bf5c34f8":"years = [1, 3, 4, 8, 10, 11]\nmotivation = [0.1, 0.4, 0.6, 0.4, 0.9, 1]\nsalary = [8, 11, 9, 14, 14, 15]\nvalues = list(zip(years, motivation, salary))\nnames = ['years', 'motivation', 'salary']\ndf = pd.DataFrame(values, columns=names)\ndf","8bca61d9":"ax = plt.axes()\nax.set_title('Absolute Pearson correlation')\nsns.heatmap(np.abs(df.corr()), annot=True)\nplt.savefig('correlation')\nplt.show()","f1dbbb59":"df = df.drop('motivation', axis=1)\nax = plt.axes()\nax.set_title('Absolute Pearson correlation')\nsns.heatmap(np.abs(df.corr()), annot=True);","d869c785":"age = [3, 4, 2, 7, 8, 104,5]\nhouse_trained = [1, 1, 0, 1, 0, 0, 1]\nvalues = list(zip(age, house_trained))\nnames = ['age', 'house_trained']\ndf = pd.DataFrame(values, columns=names)\ndf","9c248eb3":"from sklearn.preprocessing import StandardScaler\n# centering = subtract the mean\ncenter = StandardScaler(with_std=False)\ndf['centered'] = center.fit_transform(df['age'].values.reshape((-1,1)))\n\n# standardization = divide a centered feature by its' std\nstd = StandardScaler()\ndf['standardized'] = std.fit_transform(df['age'].values.reshape((-1,1)))\ndf","f3066108":"from sklearn.preprocessing import MinMaxScaler\n# self defined interval after transformation: [-1,1]\nscaler = MinMaxScaler(feature_range=(-1, 1)) \ndf['minmax']  = scaler.fit_transform(df['age'].values.reshape((-1,1)))\ndf","7d6671a9":"from sklearn.preprocessing import RobustScaler\n\ndf['median_centered'] = df['age'] - np.median(df['age'].values.reshape((-1,1)))\ndf['robust'] = RobustScaler().fit_transform(df['age'].values.reshape((-1,1)))\ndf","b0ed33d6":"fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\n\nskewness = stats.skew(chol)\ntitle = f'original, skewness = {round(skewness, 2)}'\nchol.plot(kind='hist', ax=ax1, color='red', alpha=0.5, title=title)\nchol_t = chol.apply(np.log)\nchol_t = pd.Series(chol_t)\nskewness_t = stats.skew(chol_t)\ntitle_t = f'transformed, skewness = {round(skewness_t, 2)}'\nchol_t.plot(kind='hist', ax=ax2, color='cyan', alpha=0.8, title=title_t)\n\nplt.tight_layout()\nplt.show()","4b6095f7":"from statsmodels.graphics.gofplots import qqplot\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, figsize=(5, 20))\n\n# original\nskewness = stats.skew(trestbps)\ntitle = f'original, skewness = {round(skewness, 2)}'\ntrestbps.plot(kind='hist', ax=ax1, color='red', alpha=0.5, title=title)\n\n##qqplot\nqqplot(data=trestbps, dist=\"norm\", ax=ax2, line='s')\n\n# transformation\ntrestbps_t, lmbda_best = stats.boxcox(trestbps)\ntrestbps_t = pd.Series(trestbps_t)\nskewness_t = stats.skew(trestbps_t)\ntitle_t = f'transformed, skewness = {round(skewness_t, 2)}, lambda = {round(lmbda_best, 2)}'\ntrestbps_t.plot(kind='hist', ax=ax3, color='cyan', alpha=0.8, title=title_t)\n\n##qqplot\nqqplot(data=trestbps_t, dist=\"norm\", ax=ax4, line='s')\n\n\nplt.tight_layout()\nplt.savefig('.png')\nplt.show()","c3e19289":"from sklearn.preprocessing import MinMaxScaler\nfrom scipy.special import logit\nmms = MinMaxScaler()\ntrestbps_mms = pd.Series(mms.fit_transform(trestbps.values.reshape(-1, 1)).flatten())\n\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(8, 10))\ntitle = f'original'\ntrestbps_mms.plot(kind='hist', ax=ax1, color='red', alpha=0.5, title=title)\n\ntrestbps_t = pd.Series(logit(trestbps_mms))\ntrestbps_t = trestbps_t.replace(np.Inf, 4) # for the plot\ntrestbps_t = trestbps_t.replace(np.NINF, -4) # for the plot\ntrestbps_t = pd.Series(trestbps_t)\ntitle_t = f'transformed'\ntrestbps_t.plot(kind='hist', ax=ax2, color='cyan', alpha=0.8, title=title_t)\nplt.tight_layout()\nplt.show()","e27221e5":"df_heart_failure = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nc_p = df_heart_failure['creatinine_phosphokinase']\ntarget_heart_failure = df_heart_failure['DEATH_EVENT']\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nx = c_p.values.reshape(-1,1)\n\nhyperparameter = {'max_depth' : [1,2,4, 6, 8]}\nval = GridSearchCV(DecisionTreeClassifier(), \n                         hyperparameter, cv=5, \n                         scoring='roc_auc')\n\nval.fit(x, target_heart_failure)\ndisc_tree = val.best_estimator_\n# do this on bith, train and test set:\nx_binned = pd.Series(disc_tree.predict_proba(x)[:,1], name='x_binned')","e5adb2a9":"x_binned.value_counts().plot.bar(title='resulting feature categories', alpha=0.5)\nplt.show()","3b2150b6":"from sklearn.tree import export_graphviz\nimport cv2\nexport_graphviz(disc_tree, 'tree.dot', feature_names = ['c_p'])\n! dot -Tpng tree.dot -o tree.png\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (18, 18))\nplt.imshow(img)\nplt.axis('off')\nplt.show()","1949ed27":"cor = np.corrcoef(target_heart_failure, x.flatten())[0][1]\ncor_transformed = np.corrcoef(target_heart_failure, x_binned)[0][1]\nprint(f'The Pearson Correaltion between the Target and the numerical feature: {round(cor, 2)}')\nprint(f'The Pearson Correaltion between the Target and the binned feature: {round(cor_transformed, 2)}')","9e1a44b7":"house_nr = [1, 3, 3, 2, 1, 1, 3, 2, 2, 2, 1, 1]\npet = ['dog', 'cat', 'dog', 'dog', 'rabbit', 'mouse', 'cat', 'rabbit', 'dog', 'cat', 'rat', 'rat']\nhouse_trained = [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\nvalues = list(zip(house_nr, pet, house_trained))\nnames = ['house_nr', 'pet', 'house_trained']\ndf = pd.DataFrame(values, columns=names)\ndf","5097c62d":"# import a label encoder from sklearn\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ndf_label = df.copy()\n\n# fit -> create parameters for the encoding (which category will be encoded as which integer?)\n# transform -> encode the feature using the parameters\n# fit_transform -> performs both, fit and transform\n# fit_transform on training data, transform on test data\ndf_label['pet'] = le.fit_transform(df_label['pet'])\ndf_label['house_nr'] = le.fit_transform(df_label['house_nr'])\ndf_label","8f672084":"grouped_by_pet = df_label.groupby('pet')['house_trained'].mean()\n\nplt.bar(x=grouped_by_pet.index, height=grouped_by_pet.values, alpha=0.5)\ngrouped_by_pet.plot.line(color='black', label='non-monotonic function')\nplt.xlabel('encoded pet')\nplt.ylabel('chance of being house trained')\nplt.legend()\nplt.show()","b30c1794":"# import a one-hot encoder from sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\n\n# we will focus on the feature 'pet'\ndf_oh = df.copy().drop(['house_nr'], axis=1)\n\n# creates the new features\ndummies = pd.get_dummies(df_oh['pet'])\n\n# adds the new features to our dataframe\ndf_oh = pd.concat([df_oh, dummies], axis=1)\ndf_oh","35583d3b":"grouped_by_rabbit = df_oh.groupby('rabbit')['house_trained'].mean()\n\n#fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(11,7))\nplt.bar(x=grouped_by_rabbit.index, height=grouped_by_rabbit.values, alpha=0.5)\ngrouped_by_rabbit.plot.line(color='black', label='monotonic function')\nplt.ylabel('chance of being house trained')\nplt.xticks([0, 1], ['is a rabbit', 'is not a rabbit'])\nplt.legend()\nplt.show()","0a5aeb2c":"pet = ['dog', 'cat', 'dog', 'dog', 'rabbit', 'mouse', 'cat', 'rabbit', 'dog', 'cat', 'rat', 'rat']\nhouse_trained = [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\nvalues = list(zip(pet, house_trained))\nnames = ['pet', 'house_trained']\ndf = pd.DataFrame(values, columns=names)\ndf","527fe9a9":"def target_mean_encode(feature, target):\n    encoded = feature\n    for val in feature.unique():\n        ser_pure = feature[feature==val]\n        target_pure = target[ser_pure.index].sum()\n        encoded = encoded.replace(val, target_pure\/len(ser_pure))\n    return encoded\n\n\n\n\n\npet_encoded = target_mean_encode(feature=df['pet'], target=df['house_trained'])\ndf_with_encoding = pd.concat([df, pet_encoded.rename('pet_encoded')], axis=1)\ndf_with_encoding","13fc7561":"married = [0, 1, 0, 1, 0, 1, 0]\nempl = [2, 5, 4, 15, 2, 6, 1]\nage = [25, 27, 41, 43, 28, 29, 22]\nfemale = [0, 1,  1, 1, 1, 0, 1]\nvalues = list(zip(married, empl, age, female))\nnames = ['married', 'empl', 'age', 'female']\ndf = pd.DataFrame(values, columns=names)\ndf","ac1076b0":"c = 10_000_000 # a constant for better readability\n\ndef parent(row):\n    return  c * (1 + 0.5*row['married'])*(row['empl']\/(row['age']**5) + 0.5*row['female']*row['empl']\/(row['age']**5))\n    \n\n    \nparent = df.apply(parent, axis=1) # axis=1 for row-wise operation\ndf_with_parent = pd.concat([df, parent.rename('parent')], axis=1)\ndf_with_parent","9f547254":"salary = [40, 42, 30, 32, 45, 44, 31, 44, 29, 33, 46, 50, 33, 39]\ndep =[1, 1, 0, 0, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0]\nvalues = list(zip(salary, dep))\nnames = ['salary', 'dep']\ndf = pd.DataFrame(values, columns=names)\ndf","80cf7b08":"mean_per_dep = df.groupby('dep')['salary'].mean()\n\nmean_per_dep_rows = df['dep'].replace(mean_per_dep.index, mean_per_dep.values)\nsalary_per_dep = df['salary'] - mean_per_dep_rows\ndf_with_salary_per_dep = pd.concat([df, salary_per_dep.rename('salary_per_dep')], axis=1)\ndf_with_salary_per_dep","5b3d9929":"age = [23, 24, 51, 41, 24, 72, 35, 21, 64, 29, 27]\nproducts_bought =[1, 4, 3, 2, 3, 1, 5, 1, 2, 7, 4]\nvalues = list(zip(age, products_bought))\nnames = ['age', 'products_bought']\ndf = pd.DataFrame(values, columns=names)\ndf","8581ba95":"def is_young_fan(df) : \n    if df['age'] < 30 and (df['products_bought'] > 1):\n        return 1\n    else : \n        return 0\n    \ndf['young_fan'] = df.apply(is_young_fan, axis=1)\ndf","ef080048":"def is_young(age):\n    return int(age < 30)\n\nyoung = df['age'].apply(is_young)\nyoung_count = young.sum()\nolder_count = len(young) - young_count\n\n\n\ndef is_fan(products_bought):\n    return int(products_bought > 1)\n\nfan = df['products_bought'].apply(is_fan)\nyoung_fan_count = df['young_fan'].sum()\nyoung_non_fan_count = young_count - young_fan_count\nolder_fan_count = fan.sum() - young_fan_count\nolder_non_fan_count = older_count - older_fan_count\n\n\n\n# colors by colormap\ncolor_f, color_m = [plt.cm.Reds, plt.cm.Blues]\n\n# outer\nouter_names = ['young', 'older'] # female male\nouter_size = [young_count, older_count]\nfig, ax = plt.subplots()\npie_outer, _ = ax.pie(outer_size, radius=1.5, labels=outer_names, colors=[color_f(0.3), color_m(0.3)] )\nplt.setp( pie_outer, width=1.2, edgecolor='white')\n \n# inner\ninner_names = ['fan', 'non-fan', 'fan', 'non-fan']\ninner_size = [young_fan_count, young_non_fan_count, older_fan_count, older_non_fan_count]\npie_inner, _ = ax.pie(inner_size, radius=1, labels=inner_names, labeldistance=0.55, colors=[color_f(0.7), color_f(0.5), color_m(0.7), color_m(0.5)])\nplt.setp(pie_inner, width=0.5, edgecolor='white')\nplt.margins(0,0)\nplt.savefig('fan.png')\nplt.show()","b4cf08f8":"As we can see, young people who are employed for several years are very likely to go on parental leave. Moreover, the likelihood increases a lot if the employee is female and even more if the employee is married. Note, that this approach benefits from some implicit constraints. Since we are taking a look at employee data, there will be no Kid in this dataset. According to this equation, kids would be extremely likely to become parents within the next year. Moreover, this approach needs some domain knowledge as well. We cant train any (supervised) predictor to obtain the best possible equation\/function to create this new feature. We have to wrap our minds around the topic and we might obtain very unpredictable features, but time-consuming creative approaches might provide valuable new features.","ef64c449":"As we can see, each *pet* has a new integer representation. Note that *house_nr* contains now values from a consecutive sequence of integers starting at 0.\n\nLet's focus on the feature *pet* and the target *house_trained*. The relationship between these features is not monotonic, as we can see in the following plot:","0b123b80":"<a id=\"sec51\"><\/a>\n<a id=\"tail\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 5.1. Distribution Tails<\/h1>\n\n* **tail:** The part on the left side of the modes of the distribution is called the left tail and vice versa.\n* **heavy-tailed distribution:** A Distribution with a bigger area under the curve in the tails than a normal distribution\n* **long-tailed distribution:** A distribution with a long tail has some values which are far away from the mean of the distribution on the respective side of the mean(most long tails are also **\"thin\"** for obvious reasons). long-tailed distributions contain many outliers; vice versa:**short and fat** \n* **skewness:** describes the asymmetry of a distribution\n* **negative skew:** distribution tends to have a long tail on the left side\n* **positive skew:** distribution tends to have a long tail on the right side\n* **zero skewness:** both sides of the modes balance out overall. (e.g. symmetry or one tail is long and thin and the other is short but fat\n* [**kurtosis:**](https:\/\/corporatefinanceinstitute.com\/resources\/knowledge\/other\/kurtosis\/) measures the conformity of the tails of a distribution with the tails of a normal distribution","2c736cce":"<a href=\"#top\" class=\"btn btn-info btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to Topics<\/a>","1ffd0c48":"The result is a **non-monotonic** function, since the *salary* at *years* = 4 is lower than the *salary* at *years* = 3, **and** lower than the *salary* at *years* = 8. \n\nIn reality, most relationships between your features and your target will not be monotonic, and we will most likely not achieve perfectly monotonic relationships by performing the feature transformations, which we will take a look at within the next sections. \n\nHowever, we should still make them **as monotonic as possible**, and therefore, I suggest using a simple metric for **monotony**, in order to compare the monotony of the original features and our transformed features. \n\nMy very simple approach counts all monotony violations as seen in the last graphical example, and it returns $montony = 1 - \\frac{|\\text{monotony violations}|}{|\\text{samples}|}$.","990a3966":"<a href=\"#top\" class=\"btn btn-info btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to Topics<\/a>","eb6b6147":"<a id=\"sec25\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.5. Binning with Decision Trees <\/h1>\n\nBinning Transforms numerical features into categorical features, which we can treat like any other categorical feature. There are several approaches like taking Quantiles as bin limits, or any arbitrary numbers. For example, if your job is to find out, whether patients which are older than 60 have a higher chance to have a specific illness, it might be interesting to bin the numerical age feature using the intervals $(0, 60)$ and $[60,\\text{inf})$. One of the less self-explaining methods of binning is **Binning with Decision Trees:**\n* The bins will not necessarily contain equal numbers of cases, but we might end up being lucky, which might improve the model performance even more\n* Each predicted probability will form one category\n* Since Predictions are made in the leaf nodes, and multiple leaves could make the same predictions, we end up having as many categories as leaf nodes or fewer\n* Usually improves the **correlation with the target**, due to having a [monotonical relation with the target](https:\/\/www.statisticshowto.com\/monotonic-relationship\/)\n* handles outliers, since they are assigned to one of the bins\n* Since Deep Decision Trees have a High [Variance](https:\/\/machinelearningmastery.com\/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning\/), this procedure might lead to overfitting","b143a8ec":"<a id=\"sec1\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 1. Motivation and General Advices<\/h1>\n\nReworking features to uncover **key relationships** between features and outcome is called **Feature Engineering**. It might be helpful to have some domain knowledge in order to understand the data best.\n\nFeature Engineering relies on the resulting insights of [EDA](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis).\nThe combination of Feature Engineering and EDA occurs in different phases of the whole modeling process, e.g. during post-modeling, based on Residual Analysis. **Residual Analysis** is the process of analyzing which feature values lead to false predictions.\n\n\n### Key relationships may be between the outcome and\n* a transformation of a feature \/ different representation of a feature\n* a functional relationship between features e.g. product or ratio of multiple features \n\n\n### Feature Engineering helps us to obtain a good trade-off between:\n* accuracy\n* simplicity\n* robustness\n\n### A good Mindset for Feature Engineering leads to:\n* Simplifying relationships with the target to either **binary flags** or **monotonic functions, linear if possible**.\n* Treating each transformation as one element in your ensembled machine learning model (just like in  [Pipelining](https:\/\/www.kaggle.com\/milankalkenings\/no-pipelines-you-are-probably-doing-it-wrong))","5f792a17":"https:\/\/docs.scipy.org\/doc\/scipy-0.13.0\/reference\/generated\/scipy.special.logit.html\n\nhttp:\/\/strata.uga.edu\/8370\/rtips\/proportions.html\n\nhttps:\/\/www.statsdirect.com\/help\/data_preparation\/transform_logit.htm","bf5f47af":"<a id=\"sec21\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.1. Scaling, Centering, and Standardization <\/h1>\n\nWhen talking about this topic, people tend to mix the following terms:\n* [Centering](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) refers to subtracting the mean of a column\n* [Standardization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) refers to dividing a centered feature by the standard deviation and leads to a standard deviation of one\n* [Range Scaling](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) refers to using the Minimum and the Maximum value of a feature to rescale the data on a different scale (e.g. between 0 and 1)","7189ac25":"<a id='top'><\/a>","05d3df94":"As we can see in the [qqplots](qqplot), the data used to be [right skewed](#tail) and matches the normal distribution way better now.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Power_transform\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html\n\nhttps:\/\/www.statisticshowto.com\/box-cox-transformation\/\n\nhttps:\/\/en.wikipedia.org\/wiki\/Variance-stabilizing_transformation","a5a8341e":"Let's take a look at an example for visualizing the (absolute) correlation coefficients of three variables, the *years* which an employee used to work in your company, his *motivation*, and his *salary*. The salary is the target variable. ","c892e93c":"<a id=\"sec23\"><\/a>\n<a id=\"box-cox\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.3. Box-Cox Power Transformation <\/h1>\n\n$\\Large\n     x_{transformed}=\\left\\{\\begin{array}{ll} \\frac{x^\\lambda}{\\lambda}, & x\\neq 0 \\\\\n         ln(x), & x = 0\\end{array}\\right. \n$\n  \n  \n  \n* transforms the feature into normal shape\n* the parameter $\\lambda$ might be set explicitly or might be estimated to obtain **as normally distributed data as possible**\n* different $\\lambda$ cover the Identity Transformation, the [Log Transformation](#log), the Square Root Transformation, the Inverse Transformation, and no-name transformations in between\n* requires the data to be positive\n* is a [variance stabilizing transformation](https:\/\/en.wikipedia.org\/wiki\/Variance-stabilizing_transformation) \n* improves the **validity** of Pearson **Correlation**, and thus **multicollinearity** between features\n* the [scipy implementation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.boxcox.html) allows us to store the best lambda. We can apply a Box-Cox transformation with that lambda value when predicting outcomes for our test data\/ validation data\n\nAnother Power Transformation that might be interesting is the [Yeo Johnson transformation](https:\/\/www.stat.umn.edu\/arc\/yjpower.pdf). It allows the feature to contain negative values","4d35bd54":"And here we go, we now have the salary of each clerk relative to the salary of his coworkers in the same department. ","e211b7c8":"<a id=\"sec-21\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4.1. Combine Features using Equations<\/h1>\n\n\nThere are multiple ways of combining features as introduced in [this video from Jeff Heaton](https:\/\/www.youtube.com\/watch?v=X4pWmkxEikM). Combining features demands you to wrap your mind around the given data and to think of new features you could create by combining the given ones. A combination can be seen as a formula\/equation for a new feature.\n\nProbably the most common, and most interpretable **building blocks** of these equations are:\n\n* Products of numerical features (E.g. daily cigarettes \u22c5 days or area = width \u22c5 length)\n* Ratios of numerical features (E.g. $\\frac{price}{gram}$)\n* Sums of numerical features (E.g. weight of passengers + weight of the transported goods)\n* Differences of numerical values (E.g. workdays - sick days). One often uses differences to subtract means\n\nThe whole equation for a new feature parent\nthat describes the likelihood of an employee going on parental leave (*parent*), given the numerical features age, and the years of employment (*empl*), as well as the binary categorical features *sex* (female), and the marital status (*married*) could look like this:\n\n$\\large\n     \\text{parent} =C (1+0.7married)(\\frac{empl}{age^5}+0.5\\frac{empl}{age^5}) \n$\n\nTake a moment to think about the way the binary categorical features affect this equation.\n\nLet's apply this equation to some data to see if it works:","0c6b6b36":"As we can see, higher feature values don't necessarily relate to higher chances of having a higher target value, so we don't have a *monotonic relationship* with the target. This example covers a *binary classification* task. Thus, our target variable can either have the value $1$ or $0$, whereas $1$ indicates the case to belong to the so-called *positive class*. \n\nWe could simply define a custom Label Encoding, which enforces our encoding to choose *higher values* for categories, which lead to a *higher chance* of belonging to the *positive class*. Unfortunately, we would have to create one encoding for each class in a *multiclass classification task*, and it might be very complex for regression tasks.\n\nHowever, we would still struggle with the other drawbacks as listed above, and I don't recommend you to rely on this encoding method.","3616d7fe":"<a id=\"sec-22\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4.2. Combine Features using Groupby<\/h1>\n\nIn some cases, it might be helpful to create a new feature based on one feature grouped by another one. Why should this be helpful?\nImagine having data about your employees' salary and their department. The salary on its own might already be an important feature, but it might be helpful to compare the salary of your employees with the salary of the other employees of the same department when it comes to finding out why some of your employees seem to be less motivated than others, even though they already have high salaries in comparison with employees from other departments. ","e96aed89":"<a id=\"sec33\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3.3. Target-Mean Encoding <\/h1>\n\n$\\Large\n     x_{transformed}=\\frac{|y=1_{X=x}|}{|X=x|} \n$\n\nThis **advanced encoding** method for classification problems with **binary targets** (i.e. two classes) is an elaborate alternative to the commonly used ones. One should always take a look at this representation of the categorical feature and its *predictive abilities*. \n\n* **Encoding:**  $\n    \\frac{\\text{observations of the  positive class with the respective feature value}}{\\text{observations with the respective feature value}}\n$\n* **Result:** Probability of the target value given each feature value\n* Provides a monotonic relationship between the feature and the target\n* Encodes the feature within **one column** and thus doesn't lead to huge amounts of new columns in contrast to [One-Hot Encoding](#sec31), which might be beneficial for models who can't handle huge amounts of features\n* Might decrease the [cardinality](#sec3) of the categorical feature (e.g. 2 values might be encoded as 0.5 and thus would merge into one category)\n* **Alternative for non-binary classification tasks:** create one Target-Mean encoded column for each target value and treat the respective target value as positive, and all other target values as negative\n* **Regularization:** Instead of using the whole training data to determine the encoding, use K folds and use the average encoding in the final feature representation","14354b52":"\nWe can now plot the data to take a look at the relationship between the feature *years* and the target variable *salary*:","e0859b64":"<a id=\"sec32\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3.2. One-Hot Encoder <\/h1>\n\nAs I already mentioned, label Encodings have some huge drawbacks. Probably the biggest drawback is, that numerical relationships like *distances* and *hierarchies* will be assumed between the categories because these are solely encoded as discrete numbers inside the same column. Categories, on the other hand, don't have any meaningful numerical relationships.\n\nOne-Hot Encoders evade this problem and the encoded feature will have the following properties:\n* Each *category* is stored in a separate, new column.\n* Each of these new columns contains solely zeroes and ones.\n* ones indicate, that the case is of the respective category.\n* Each of these new columns has a *monotonic relationship with the target*.\n* There will be no numerical relationships assumed between the categories\n\nNevertheless, there are still some downsides of using this method:\n* Huge drawback: multiple new columns, which might lead to a worse model due to the [Curse of Dimensionaility](https:\/\/en.wikipedia.org\/wiki\/Curse_of_dimensionality)\n* In many cases, it makes sense to merge very uncommon categories into one category called *' other'* in order to evade having many columns containing very few ones. For example, you could merge all categories together, which occur in less than 5% of your *observations*.","65888af8":"Note: I didn't finetune the other parameters of the decision tree, and I used the *roc_auc* score. One should always use an appropriate score and feel free to finetune the other hyperparameters in your models, to obtain the best possible features.\n\n\nfurther sources: \n\nhttps:\/\/www.youtube.com\/watch?v=vsKNxbP8R_8?t=1388\n\nhttps:\/\/towardsdatascience.com\/discretisation-using-decision-trees-21910483fa4b\n\n","cb431a70":"<a id=\"sec212\"><\/a>\n## Range Scaling\nThe transformed numerical feature will...\n* have a similarly formed distribution as the original feature\n* still contain outliers\n* contain values between $0$ and $1$ by default, which enables further transformations like the [Logit Transformation](#sec24)\n* be especially beneficial for models, which assume the data to be on the same scale (distance-based methods like KNN), if applied to all numerical features","73e0fc3b":"As we can see, rats and rabbits end up having the same encoding. Thus, the encoding has a *cardinality* of 4, whereas the original feature had a *cardinality* of 5.","e620b823":"<a id=\"sec22\"><\/a>\n<a id=\"log\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.2. Log Transformation <\/h1>\n\n$\\Large\n     x_{transformed}=ln(x)\n$\n* commonly used\n* suitable for data which approximately follows a [log-normal distribution](https:\/\/en.wikipedia.org\/wiki\/Log-normal_distribution)\n* is a special case of the [Box-Cox Transformation](#box-cox), so take a look at that section if you are interested in this kind of transformations\n","795d67d4":"As suggested by [@anashamoutni](https:\/\/www.kaggle.com\/anashamoutni), [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) and other [Dimensionality Reduction](https:\/\/en.wikipedia.org\/wiki\/Dimensionality_reduction) methods can be seen as methods for combining features as well, since they merge multiple features in a more or less meaningful way. I will probably make a separate notebook about that topic as well...","675d2821":"<a id=\"sec213\"><\/a>\n## Robust Scaling\n\n$\\Large\n     x_{transformed}=\\frac{x - median(x)}{q_{75}(x) - q_{25}(x)} \\text{ ,with } q_{n} \\text{ being the nth quantile}\n$\n\nThis transformation...\n* is very robust to outliers. You should consider using it, whenever you don't want the outliers to have too much impact on the transformation\n* is performed on every feature individually, and thus doesn't ensure the features to be on the same scale, which might have a negative impact on your model perfromance.","3947858b":"<a href=\"#top\" class=\"btn btn-info btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to Topics<\/a>","62286fe7":"Some initial imports..","a5bcba62":"As we can see, the relation is an almost **monotonically increasing function**. However, this approach is pretty naive, since it makes too many assumptions about the data. These are some of the reasons, why simply checking for monotony this way might be **problematically**:\n\n* Feature values might be **non-unique** which rises multiple opportunities (E.g. should we use the mean\/median target value in these cases?)\n* Functions might have many local minima and maxima, but they could still follow a monotonic trend when smoothed.\n\nA better approach might include checking the **integral** values for varying areas of the feature space. \n\n\nHowever, we can also use another, yet conceptionally pretty similar indicator for approximate (linear) monotony, the so-called **Pearson Correlation Coefficient**.","bc8fbb3d":"As you can see, being a rabbit provides a higher chance of being house trained than not being a rabbit. This relationship very easy to interpret by the model and thus can be very beneficial. In a regression task, binary columns like this could indicate either higher or lower target values.","d5068373":"<a href=\"#top\" class=\"btn btn-info btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to Topics<\/a>","75c6de5f":"This relationship is called **(strictly) monotonic**, because the higher the value of *years*, the higher is the value of *salary*. Note: this function of the input variable *years* and the output *salary*  is said to be (strictly) monotonically increasing, but (strictly) monotonically decreasing functions are also considered **(strictly) monotonic**.\n\nSo far so good. Let's consider you collected some more data and the relating relationship looks like this:","123e3a60":"Note: I will focus on weak heredity. Besides that, strong heredity demands both features to be predictive","dd67045e":"<a id=\"sec31\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3.1. Label Encoding <\/h1>\n\nLabel Encoders are probably the most simple way to encode a categorical feature. The Resulting Encoding has the following properties:\n* Encodes the feature into one column (so we don't struggle with having too many features)\n* Consecutive integers, starting at 0. \n* Each Category shares the same integer\n* Indicates meaningful numerical *hierarchies* and *distances* between the categories ($ 1 < 2$ and $1 = 0.5 \\cdot 2$)\n* Some models like *Linear Regression* might assign more meaning to categories with higher integer representation.\n* In most cases, it  **violates** the **key idea** of forcing features to have a monotonic relationship with the target.\n\n\nLet's apply our encodings to some data about *pets*, the *houses* in which they live and whether they are *house trained* or not.","36636d05":"<a id=\"sec11\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 1.1. Feature-Target Relations and Monotony<\/h1>\n\nAs mentioned above, it is best practice to create features which have a **monotonic relationship** with the target. This is due to:\n\n* The relationship is easy to interpret for Data Analysts\n* Machine Learning algorithms might converge faster\n* Machine Learning algorithms in most cases provide better predictions with features like these\n\n\n\n\nBut what exactly are these monotonic relationships? Let me give you an example:\nImagine having data about some employees of your department, and you want to find out the relationship between years of deployment (*years*) and the *salary* of the employees. You want to predict your income over the next years (which means, that your target variable is the salary).","83c9e2fa":"<a id=\"sec-2\"><\/a>\n***\n\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4. Combine Features <\/h1>\n\nMany machine learning algorithms utilize feature interactions and combinations implicitly. However, experience has shown that it still might be a good idea to combine features manually, because we can't rely on our model 'doing all the work'.\n\n\nFinding and combining features could be important for standing out in Kaggle competitions, but finding useful combinations might be a non-trivial problem. Suggestions from **domain experts** are oftentimes the best entry point to detecting valuable combinations.\n\nBesides relying on domain experts, we could try every possible combination of features to identify *predictive* (i.e. model improving) ones. This approach would take too much time, but at least we can already find many of the most important combinations if we follow these guidelines: \n* **effect sparsity:** the fewer features are part of the combination, the higher the chance for the combination to be predictive (including singletons, i.e. uncombined features). We should focus on combinations between 2 or 3 features.\n* **heredity:** the combination $(feat_1, feat_2)$ should only be considered to be predictive, if at least one of the features, $feat_1$ or $feat_2$, is already known to be predictive.[$^{1}$](#note)\n* **priority:** in most cases, the interpretability and the predictivity of a combination is better when the original features aren't transformed ([scaled](#sec21), encoded, [log-transformed](#log)...). Thus we should create the combinations *prior* to any transformations.","968fbf7c":"<a href=\"#top\" class=\"btn btn-info btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to Topics<\/a>","fd949f3c":"<a id=\"sec3\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 3. Encode Categorical Data <\/h1>\n\nCategorical features contain discrete, oftentimes even string values. The number of unique values a categorical feature contains is called **cardinality**. \n\nMost common machine learning models can't handle this kind of data, since they assume data to be numerical. Thus we have to use encoding methods to transform the categorical features into a suitable representation in order to utilize their *predictive abilities*.","7778de34":"This relationship is still called **monotonic**, despite the fact that the *salary* is the same for *years* = 8 **and** *years* = 10. The relationship is just not called **strictly monotonic** anymore. The same holds for monotonically decreasing functions.","566d9d8a":"<a id=\"sec6\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 6. Further Readings & Helpful Videos<\/h1>\n\nI hope that you noticed, that I tried to add some resources for further readings. Maybe I already linked you to some of them, but I want to emphasize the importance of these sources for this notebook:\n\nhttps:\/\/www.youtube.com\/watch?v=lUg0dRrlsoA\n\nhttps:\/\/www.youtube.com\/watch?v=vsKNxbP8R_8\n\nhttps:\/\/www.youtube.com\/watch?v=X4pWmkxEikM\n\nhttps:\/\/www.goodreads.com\/book\/show\/45832399-feature-engineering-and-selection","ecf8c744":"Assume we have a dataset containing several pets from your friends and whether they are house trained or not. We want to Target-Mean Encode the categorical feature *pet* with respect to the target *house_trained*.","bcb70e7d":"<a id=\"sec12\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 1.2. Pearson Correlation and Collinearity<\/h1>\n\n\nThe Pearson Correlation Coefficient indicates the degree to which two variables have a linear relationship. A perfect linear relationship is by definition a [monotonic relationship](#sec11).\n\n\nOur goal is to reach:\n\n* as **strong correlations between every single feature and the target** as possible\n\n* as **weak correlations between multiple features** as possible.\n\nWhenever two features are correlated to each other, and one of them is way more correlated to the target, one should consider dropping the feature, which is less correlated to the target. Correlation between features is also known as **(multi) collinearity**.\n","ad8b766f":"\n<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment<\/span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote<\/span> if you like this project.<\/h3>\n<\/div>","03454a00":"<h1 style=\"background-color:DodgerBlue; color:white\" >-> Topics:<\/h1>\n\n## 1. [Motivation and General Advices](#sec1)\n#### 1.1. [Feature-Target Relations and Monotony](#sec11)\n#### 1.2. [Pearson Correlation and Collinearity](#sec12)\n\n## 2. [Univariate Transformations on Numerical Data](#sec2)\n#### 2.1. [Scaling, Centering, and Standardization](#sec21)\n* [Centering and Standardization](#sec211)\n* [Range Scaling](#sec212)\n* [Robust Scaling](#sec213)\n\n#### 2.2. [Log Transformation](#sec22)\n#### 2.3. [Box-Cox Power Transformation](#sec23)\n#### 2.4. [Logit Transformation](#sec24)\n#### 2.5. [Binning with Decision Trees](#sec25) \n\n## 3. [Encode Categorical Data](#sec3)\n#### 3.1. [Label Encoding](#sec31)\n#### 3.2. [One-Hot Encoding](#sec32)\n#### 3.3. [Target-Mean Encoding](#sec33)\n\n\n## 4. [Combine interacting Features](#sec-2)\n#### 4.1. [Combine Features using Equations](#sec-21)\n#### 4.2. [Combine Features using Groupby](#sec-22)\n#### 4.3. [Combine Features using Conditions](#sec-23)\n\n## 5. [Statistics Vocabulary and Plots](#sec-1)\n#### 5.1. [Distribution Tails](#sec51)\n#### 5.2. [The Quantile-Quantile Plot (qqplot)](#sec52)\n\n## 6. [Further Readings & Helpful Videos](#sec6)","1d24877f":"Let's consider you collected even more data and this is the resulting relationship:","dd64f6b6":"<a id=\"sec24\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2.4. Logit Transformation <\/h1>\n\n\n$\\Large\n     x_{transformed}=ln(\\frac{x}{1-x})\n$\n* useful on continuous data between 0 and 1, e.g. proportions, with a **sigmoid distribution** (many values with either very high or very low values)\n* transformed data provides better distinction between the data with either very high or very low values\n* provides the log-odds\n* maps the data to continuous values between **-inf** and **inf**\n* the ends of the scale have a larger difference on the logit-transformed scale\n* is a [variance stabilizing transformation](https:\/\/en.wikipedia.org\/wiki\/Variance-stabilizing_transformation) \n* the [scipy implementation](https:\/\/docs.scipy.org\/doc\/scipy-0.13.0\/reference\/generated\/scipy.special.logit.html)\n\n\nThe [Arcsine Transformation](http:\/\/strata.uga.edu\/8370\/rtips\/proportions.html) works pretty similar and might be better in some cases, but in general, the Logit Transformation is the better choice.","58b55234":"As mentioned in the Section about [Encodings](#sec3), these binary columns have some huge benefits. Nevertheless, we should only use this method for very common relationships, in order to evade columns mostly containing zeros.","5263c6b3":"Let's take a look at the Resulting categories, the tree, and the correlation improvement.","5f5b6b83":"<a id=\"sec211\"><\/a>\n## Centering and Standardization\nThe transformed numerical feature will...\n* have a *mean* of $0$\n* have a standard deviation of $1$ \n\nMost Deep Learning methods demand these properties.\n\nTherefore, the standard scaler simply subtracts the mean of the feature and divides each value by the standard deviation of the feature.\n\nLet's take a look at some data containing the age of some *pets* and whether they are *house trained* or not:","19d425b3":"As you can see, each row contains just a single $1$ within the new columns, since each case still belongs solely to one of the categories. \n\nMoreover, the common machine learning models can handle this representation of the feature very well, since a category will be either recognized to be absent or not. Last but not least, every new column has either a positive or a negative *monotonic and linear * relationship with the target (for obvious reasons, since there are only two discrete values per column).\n\nLet's for example take a look at the relationship between the column *rabbit* and the target:","8b3b6081":"We end up having no multicollinearity, which might improve the performance of our machine learning model.","3bfffa47":"# About this Notebook\nHey all,\nmy goal is to write a **compact guide** on feature engineering.\n\n\n<div class=\"alert alert-danger\" role=\"alert\">\n    <h3>Feel free to <span style=\"color:red\">comment<\/span> if you have any suggestions   |   motivate me with an <span style=\"color:red\">upvote<\/span> if you like this project.<\/h3>\n<\/div>\n","83654b6a":"As we can see, both features (*years, motivation*) are highly correlated to the target (*salary*). Unfortunately, the features are highly correlated to each other as well. We could either try to merge these two features e.g. by using [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis), or discard one feature from the dataset in order to **evade multicollinearity**.\n\nI suggest discarding the multicollinear feature, which is less correlated to the target: ","798f3340":"<a id=\"sec52\"><\/a>\n<a id=\"qqplot\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 5.2. The Quantile-Quantile Plot (qqplot)<\/h1>\n\n* plots the [quantiles](https:\/\/en.wikipedia.org\/wiki\/Quantile) (basically just the data sorted in ascending order) of two variables against each other\n* each axis represents one of these variables\n* the more similar the distributions of the variables are, the more looks the plot like the line formed by $x=y$\n* quantile plots underneath the line have lower $y$-variable values than $x$-variable values and vice versa\n* is oftentimes used to determine graphically, whether the data follows any known distribution like the normal distribution (by plotting these known distributions against the data)\n* take a look at these [typical qqplot results](https:\/\/stats.stackexchange.com\/questions\/101274\/how-to-interpret-a-qq-plot) and the respective interpretations regarding [skewness and kurtosis](#tail).","e95ac29d":"<a id=\"sec2\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 2. Univariate Transformations on Numerical Data <\/h1>\n\n\nNumerical Data may...\n* be on different scales\n* follow a [long-tailed distribution](#tail). Long tails might dominate the underlying calculations in models, which rely on polynomial calculations on the features (most linear models, SVMs, and neural networks)\n* have a complex relationship with the outcome\n* be represented inefficiently, sometimes simply **normally distributed representations may already improve the performance**\n\n### One often wants data to be **normally distributed**, but why?\n* The whole distribution is defined by the mean(= mode = median) and the variance, which might be of importance\n* the normal distribution is [symmetric](#tail), which has some significant impact on the performance of many models \n* due to the [central limit theorem](https:\/\/sphweb.bumc.bu.edu\/otlt\/mph-modules\/bs\/bs704_probability\/BS704_Probability12.html), many machine learning models [assume the feature values to be drawn from a normally distributed population](https:\/\/stackoverflow.com\/questions\/54071893\/a-feature-distribution-is-nearly-normal-what-does-that-imply-for-my-ml-model) like linear regression, logistic regression, LDA, QDA, and Gaussian Naive Bayes","97efc1f8":"The (Pearson) correlation coefficient can be between -1 and 1. Since both, high negative and high positive correlations indicate linear relationships, either with a negative or a positive slope, I prefer using the **absolute correlation coefficients** in order to improve the interpretability. \n\n**Per definition:**\n\n* **low absolute correlation:** 0.3 to 0.5\n* **moderate absolute correlation:** 0.5 to 0.7\n* **high absolute correlation:** 0.7 to 0.9\n* **very high absolute correlation:** 0.9 to 0.1\n\n\nTake a look at the [this visualization](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient#\/media\/File:Correlation_examples2.svg) of different pearson correlation values to get some feeling for the way positive and negative correlation works.\n\n","6934e015":"<a id=\"sec-23\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 4.3. Combine Features using Conditions<\/h1>\n\nOne more very common method of creating a new feature by combining the original features is using conditions. This method is especially important if your project demands you to focus on a particular subgroup of our observations, or when you already figured out any frequent sets in your Dataset.\n\n* Use this method for commonly fulfilled conditions \n* The conditions should contain multiple features, to evade *collinearity*\n* Construct the condition based on your project goals or frequent patterns in your data\n* You can find frequent patterns in your data using the [Apriori Algorithm](https:\/\/www.youtube.com\/watch?v=guVvtZ7ZClw)\n\n\nImagine your Data Exploration reveals the fact that a particular combination of features occurs frequently with a particular outcome. In such a case, it might be interesting to create a *binary flag*, indicating the particular combination. Of course, the algorithm could find out this relationship automatically, but we can never be sure about it. \n\nFor example, we could have found out, that all young customers, who already bought multiple products from our company are very interested in our new product. Let's call these young people *young fans*. Creating such a feature could look like this:","dd0d21cf":"<a id=\"sec-1\"><\/a>\n<a id=\"stats\"><\/a>\n***\n<h1 style=\"background-color:DodgerBlue; color:white\" >-> 5. Statistics Vocabulary and Plots<\/h1>\n\n* **population:** \nthe true data one could achieve with immense effort\n* **sample:** \nthe part of the data which is available for the modeling process \/ the training data \n* **Population Parameter:** \nan aspect of a population (e.g. the ground truth mean of a feature)\n* **statistic:** \nan aspect of a sample (e.g. the mean of a feature in our training data) \n* **parametric statistical test:** \nmakes an assumption about the population parameters(e.g. stdent's T test, ANOVA)\n* **nonparametric statistical test:** \ndoesn't assume anything about the population parameters (e.g. chi-square)\n* **parametric models:**\nmachine learning models that make strong assumptions\/have a high bias about the sample on which they are applied (e.g. they assume the data to follow a specific distribution).","93fd28df":"When we compare *centered* and *median_centered*, we can observe that subtracting the median instead of the mean provides one huge advantage: Whenever the dataset contains huge outliers (like this pet, that is 104 years old, probably a turtle), the median is far less affected by the outliers than the mean. Thus, centering by subtracting the mean shifts all values of the feature into the direction of the huge outliers. However, this leads to another issue: the outliers will stay outliers, and we will have to deal with them later on.","9e138333":"<a href=\"#top\" class=\"btn btn-info btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to Topics<\/a>"}}