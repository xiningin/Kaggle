{"cell_type":{"8042ac7f":"code","2bbc3fb2":"code","ab1e7d65":"code","43f07714":"code","84eccdc2":"code","ef9f1f42":"code","20969a39":"code","fc589f41":"code","58582c34":"code","ab256ab1":"code","39da1782":"markdown"},"source":{"8042ac7f":"import warnings\nwarnings.filterwarnings(\"ignore\")","2bbc3fb2":"# Install dependencies \n# A dependency of the preprocessing for BERT inputs\n!pip install -q -U tensorflow-text\n# A dependency for using the AdamW optimizer\n!pip install -q tf-models-official \n\n# Load necessary modules \nfrom sklearn.model_selection import train_test_split \nimport shutil \nimport os\nimport tensorflow as tf \nimport numpy as np \nimport pandas as pd \nimport tensorflow_hub as hub \nimport tensorflow_text as text \nfrom official.nlp import optimization \nimport matplotlib.pyplot as plt \n\ntf.get_logger().setLevel('ERROR')","ab1e7d65":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n# Select only the columns of interest\ntrain_df = train_df[['text','target']]\n# Split the training data so we get validation data as well  \nX_train, X_test, y_train, y_test = train_test_split(train_df['text'],train_df['target'],test_size = 0.12, random_state = 42, shuffle = True)","43f07714":"# Build the model using Keras functional API \ndef build_model():\n  # Get the shape of the input text\n  input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n  # Load preprocessing layer tensorflow hub \n  preprocessing_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\", name='preprocessing_layer')\n  # Apply preprocessing \n  processed_text = preprocessing_layer(input_text)\n  # Load encoder from tensorflow hub \n  encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/4\", trainable=True, name='encoder')\n  # Apply encoding \n  outputs = encoder(processed_text)\n  # The BERT model returns a map with 3 keys: pooled_output, sequence_output, encoder_outputs\n  # For the fine-tuning we are going to use the pooled_output array which creates an embedding \n  # entire dataset\n  x = outputs['pooled_output']\n  # Apply Dropout to avoid overfitting \n  x = tf.keras.layers.Dropout(0.1)(x)\n  # Apply the classifier layer and use sigmoid activation function for fine-tuning \n  x = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(x)\n  return tf.keras.Model(input_text, x)","84eccdc2":"# Build the model \nmodel = build_model()\n# Use Binary Cross Entropy as loss function \nloss = tf.keras.losses.BinaryCrossentropy()\n# Use Binary Accuracy to assess fitness accuracy \nmetrics = tf.metrics.BinaryAccuracy()\n# Plot the model \ntf.keras.utils.plot_model(model)","ef9f1f42":"epochs = 2\n\nsteps_per_epoch = tf.data.experimental.cardinality(tf.data.Dataset.range(len(train_df))).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\n\n#For fine-tuning, we use the same optimizer that BERT was originally trained. \n#This optimizer minimizes the prediction loss and does regularization by weight decay (aka AdamW).\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)","20969a39":"history = model.fit(x = X_train,\n                    y = y_train, \n                    validation_data = (X_test,y_test),\n                    batch_size = 16, \n                    epochs=epochs)","fc589f41":"# Save the model \nmodel.save('\/kaggle\/working\/classifier_model', include_optimizer = True)\n# Load the test data \ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n# Predict the target values \npredictions = model.predict(test_df['text'])\n# Squeeze them to a list \npredictions = tf.squeeze(predictions, axis = 1)\n# Apply rounding so we get values between 0 and 1\npredictions = np.rint(predictions)","58582c34":"def plot_loss_curves(history):\n    '''\n    Returns loss curves for training and validation metrics (if available)\n    '''\n    if \"val_loss\" in history.history:\n        loss = history.history[\"loss\"]\n        val_loss = history.history[\"val_loss\"]\n        accuracy = history.history[\"binary_accuracy\"]\n        val_accuracy = history.history[\"val_binary_accuracy\"]\n\n        epochs = range(len(history.history[\"loss\"])) #number of epochs \n\n        # Plot losses \n        plt.figsize=(10,7)\n        plt.plot(epochs, loss, label = 'training_loss')\n        plt.plot(epochs, val_loss, label = 'val_loss')\n        plt.title('loss')\n        plt.xlabel('epochs')\n        plt.legend()\n\n        # Plot accuracy \n        plt.figure()\n        plt.plot(epochs, accuracy, label = 'training_accuracy')\n        plt.plot(epochs, val_accuracy, label = 'val_accuracy')\n        plt.title('accuracy')\n        plt.xlabel('epochs')\n        plt.legend()\n    \n    else:\n        # Plot training loss and accuracy together \n        loss = history.history[\"loss\"]\n        accuracy = history.history[\"accuracy\"]\n\n        epochs = range(len(history.history[\"loss\"])) #number of epochs \n\n        fig, ax1 = plt.subplots(figsize=(11, 9))\n        ax1.plot(epochs, accuracy, label = 'training_accuracy')\n        plt.xlabel('epochs')\n        ax1.set_ylabel('Training Accuracy')\n        \n        ax2 = ax1.twinx()\n        ax2.plot(epochs, loss, label = 'training_loss', color = 'tab:red')\n        ax2.set_ylabel('Training Loss')\n        \nplot_loss_curves(history)","ab256ab1":"# Create submission data \ntest_df['target'] = predictions \ntest_df['target'] = test_df['target'].astype(int)\nsubmission = test_df[['id','target']]\nsubmission.to_csv('submission.csv', index = False)\nsubmission","39da1782":"## **Basic information about the model and preprocessing:**\n* The weights of this model are those released by the original BERT authors. \n* This model has been pre-trained for English on the Wikipedia and BooksCorpus. \n\n* Text inputs has been lower-cased before tokenization into word pieces, and any accent markers have been stripped.\n* For training, random input masking has been applied independently to word pieces (as in the original BERT paper)."}}