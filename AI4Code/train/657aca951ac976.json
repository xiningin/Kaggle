{"cell_type":{"dc11d118":"code","97ae3617":"code","5c7e46d2":"code","e328b40f":"code","7777568c":"code","924de7f7":"code","226aca2c":"code","affe628c":"code","54294f2a":"code","7c43764b":"code","189997cf":"code","2189791b":"code","665af2e4":"code","60abda15":"code","a2e1188d":"code","ff0e93d6":"code","b9aef76d":"code","53412270":"code","39b83b49":"code","18190646":"markdown","7fc68d45":"markdown","a3f4673e":"markdown","a21c2888":"markdown","4ee78e7e":"markdown","a1fe40a7":"markdown","12577b01":"markdown","d39bd1ea":"markdown","40e5868d":"markdown","e4a0be37":"markdown","34b53448":"markdown","a50cd97a":"markdown","42e312e1":"markdown","5de23331":"markdown","5a8c83b4":"markdown","07ea8388":"markdown","eec75c58":"markdown"},"source":{"dc11d118":"!python -m pip install --upgrade pip\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","97ae3617":"import random\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score\nimport IPython\nimport sys\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import BertForSequenceClassification, BertPreTrainedModel, BertConfig, BertModel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm","5c7e46d2":"class config:\n    EPOCHS = 1\n    BATCH_SIZE = 32\n    VAL_BATCH_SIZE = 128\n    TEST_BATCH_SIZE = 128\n    LR = 3e-5","e328b40f":"valid = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation-processed-seqlen128.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train-processed-seqlen128.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test-processed-seqlen128.csv\")\nsubmit = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv\")\ntrain = train[['id', 'comment_text', 'input_word_ids', 'input_mask','all_segment_id', 'toxic']].iloc[:20000]","7777568c":"class TweetDataset(Dataset):\n    def __init__(self, mode, df):\n        self.mode = mode\n        self.df = df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        token, segment, mask = self.df.loc[idx, [\"input_word_ids\", \"all_segment_id\", \"input_mask\"]].values\n        if self.mode==\"train\" or self.mode == \"valid\":\n            label_tensor = torch.tensor(self.df.loc[idx, \"toxic\"])\n        else:\n            label_tensor = torch.tensor(-1)\n        tokens_tensor = torch.tensor([int(i) for i in token[1:-1].split(\",\")])\n        segments_tensor = torch.tensor([int(i) for i in segment[1:-1].split(\",\")])\n        masks_tensor = torch.tensor([int(i) for i in mask[1:-1].split(\",\")])\n           \n        return tokens_tensor, segments_tensor, masks_tensor, label_tensor","924de7f7":"lang = {'Spanish': 'es', 'Italian': 'it', 'Turkish': 'tr'}\n\nvalidsets = {}\nfor i, k in lang.items():\n    validsets[i] = TweetDataset(\"valid\", valid[valid[\"lang\"] == k].reset_index(drop=True))\ntrainset = TweetDataset(\"train\", train)\nvalidset = TweetDataset(\"valid\", valid)\ntestset = TweetDataset(\"test\", test)\n\nvalidloaders = {}\nfor i, k in validsets.items():\n    validloaders[i] = DataLoader(k, batch_size=config.VAL_BATCH_SIZE, num_workers=4, shuffle=False)\ntrainloader = DataLoader(trainset, batch_size=config.BATCH_SIZE, num_workers=4, shuffle=False)\nvalidloader = DataLoader(validset, batch_size=config.VAL_BATCH_SIZE, num_workers=4, shuffle=False)\ntestloader = DataLoader(testset, batch_size=config.TEST_BATCH_SIZE, num_workers=4, shuffle=False)","226aca2c":"class Model(nn.Module):\n    \n    def __init__(self, labels=1):\n        \n        super().__init__()\n        \n        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n        self.num_features = self.bert.pooler.dense.out_features\n        self.labels = labels\n        \n        self.drop = nn.Dropout(0.3)\n        self.fc1 = nn.Linear(self.num_features * 2, self.num_features)\n        self.logit = nn.Linear(self.num_features, self.labels)\n        \n    def forward(self, tokens_tensors, segments_tensors, masks_tensors):\n        \n        hidden_states, cls = self.bert(input_ids=tokens_tensors, token_type_ids=segments_tensors, attention_mask=masks_tensors)\n        avgpool = torch.mean(hidden_states, 1)\n        maxpool, _ = torch.max(hidden_states, 1)\n        cat = torch.cat((avgpool, maxpool), 1)\n        x = self.drop(cat)\n        x = torch.tanh(self.fc1(x))\n        output = self.logit(x)\n\n        return output","affe628c":"model = Model()","54294f2a":"model","7c43764b":"device = xm.xla_device()\nmodel.to(device)\nprint(f\"Now we use {device}\\n\")","189997cf":"def training(model, warmup_prop=0.1):\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LR)\n    num_warmup_steps = int(warmup_prop * config.EPOCHS * len(trainloader))\n    num_training_steps = config.EPOCHS * len(trainloader)\n    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n    loss_fun = torch.nn.BCEWithLogitsLoss(reduction='mean').to(device)    \n\n    for epoch in range(config.EPOCHS):\n        model.train()\n        \n        optimizer.zero_grad()\n        avg_loss = 0\n        \n        for data in tqdm(trainloader):             \n            tokens_tensor, segments_tensor, masks_tensor, labels_tensor = [k.to(device) for k in data if k is not None]\n            output = model(tokens_tensor, segments_tensor, masks_tensor)\n            loss = loss_fun(output.view(-1).float(), labels_tensor.float().to(device))\n            loss.backward()\n            avg_loss += loss.item() \/ len(trainloader)\n\n            xm.optimizer_step(optimizer, barrier=True)\n            scheduler.step()\n            model.zero_grad()\n            optimizer.zero_grad()\n                \n        model.eval()\n        preds = []\n        truths = []\n        avg_val_loss = 0.\n\n        with torch.no_grad():\n            for data in validloader:\n                tokens_tensor, segments_tensor, masks_tensor, labels_tensor = [k.to(device) for k in data if k is not None]\n                output = model(tokens_tensor, segments_tensor, masks_tensor)\n                loss = loss_fun(output.detach().view(-1).float(), labels_tensor.float().to(device))\n                avg_val_loss += loss.item() \/ len(validloader)\n                \n                probs = torch.sigmoid(output).detach().cpu().numpy()\n                preds += list(probs.flatten())\n                truths += list(labels_tensor.detach().cpu().numpy().flatten())\n            score = roc_auc_score(truths, preds)\n        \n        lr = scheduler.get_last_lr()[0]\n        print(f'[Epoch {epoch + 1}] lr={lr:.1e} loss={avg_loss:.4f} val_loss={avg_val_loss:.4f} val_auc={score:.4f}')","2189791b":"threshold = lambda x: 1 if x>=0.5 else 0\n\ndef predict(model, dataloader, df, isAccuracy=True):\n \n    model.eval().to(device)\n    preds = np.empty((0, 1))\n    accuracy = None\n\n    with torch.no_grad():\n        for data in tqdm(dataloader):\n            tokens_tensor, segments_tensor, masks_tensor, labels_tensor = [k.to(device) for k in data if k is not None]\n            probs = torch.sigmoid(model(tokens_tensor, segments_tensor, masks_tensor)).detach().cpu().numpy()\n            preds = np.concatenate([preds, probs])\n            \n    preds = preds.reshape(len(preds))        \n    predicts = np.array([threshold(i) for i in preds])\n    if isAccuracy:\n        accuracy = (df[\"toxic\"].values == predicts).sum() \/ len(df)\n\n    return preds, predicts, accuracy ","665af2e4":"# before training model accuracy\npre, pre_class, accuracy = predict(model, trainloader, train)\nauc = roc_auc_score(train[\"toxic\"].values, pre_class)\nprint(\"Train: \")\nprint(f\"Model before fine-tune accuracy: {accuracy * 100:.3f}%\\nModel before fine-tune AUC: {auc:.3f}\")\n\nfor key, value in validloaders.items():\n    pre, pre_class, accuracy = predict(model, value, valid[valid[\"lang\"] == lang[key]].reset_index(drop=True))\n    auc = roc_auc_score(valid[valid[\"lang\"] == lang[key]].reset_index(drop=True)[\"toxic\"].values, pre_class)\n    print(f\"{key} Valid: \")\n    print(f\"Model before fine-tune accuracy: {accuracy * 100:.2f}%\\nModel before fine-tune AUC: {auc:.3f}\")\n\npre, pre_class, accuracy = predict(model, validloader, valid)\nauc = roc_auc_score(valid[\"toxic\"].values, pre_class)\nprint(f\"Combined Valid: \")\nprint(f\"Model before fine-tune accuracy: {accuracy * 100:.2f}%\\nModel before fine-tune AUC: {auc:.3f}\")","60abda15":"%%time \n\ntraining(model)","a2e1188d":"# After training model accuracy\npre, pre_class, accuracy = predict(model, trainloader, train)\nauc = roc_auc_score(train[\"toxic\"].values, pre)\nprint(\"Train: \")\nprint(f\"Model before fine-tune accuracy: {accuracy * 100:.3f}%\\nModel before fine-tune AUC: {auc:.3f}\")\n\nfor key, value in validloaders.items():\n    pre, pre_class, accuracy = predict(model, value, valid[valid[\"lang\"] == lang[key]].reset_index(drop=True))\n    auc = roc_auc_score(valid[valid[\"lang\"] == lang[key]].reset_index(drop=True)[\"toxic\"].values, pre)\n    print(f\"{key} Valid: \")\n    print(f\"Model before fine-tune accuracy: {accuracy * 100:.2f}%\\nModel before fine-tune AUC: {auc:.3f}\")\n\npre, pre_class, accuracy = predict(model, validloader, valid)\nauc = roc_auc_score(valid[\"toxic\"].values, pre)\nprint(f\"Combined Valid: \")\nprint(f\"Model before fine-tune accuracy: {accuracy * 100:.2f}%\\nModel before fine-tune AUC: {auc:.3f}\")","ff0e93d6":"torch.save(model.state_dict(), \".\/model.bin\")","b9aef76d":"pre, pre_class, accuracy = predict(model, testloader, test, False)\nsubmit['toxic'] = pre\nsubmit.to_csv('submission.csv', index=False)\nsubmit.head()","53412270":"!test -d bertviz_repo || git clone https:\/\/github.com\/jessevig\/bertviz bertviz_repo\nif not 'bertviz_repo' in sys.path:\n    sys.path += ['bertviz_repo']\n\nfrom transformers import BertTokenizer, BertModel\nfrom bertviz import head_view\n\ndef call_html():\n    display(IPython.core.display.HTML('''\n        <script src=\"\/static\/components\/requirejs\/require.js\"><\/script>\n        <script>\n          requirejs.config({\n            paths: {\n              base: '\/static\/base',\n              \"d3\": \"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/d3\/3.5.8\/d3.min\",\n              jquery: '\/\/ajax.googleapis.com\/ajax\/libs\/jquery\/2.0.0\/jquery.min',\n            },\n          });\n        <\/script>\n        '''))\n","39b83b49":"model_version = 'bert-base-chinese'\nmodel = BertModel.from_pretrained(model_version, output_attentions=True)\ntokenizer = BertTokenizer.from_pretrained(model_version)\n\nsentence_a = \"\u963f\u660e\u53bb\u8cb7\u6771\u897f\uff0c\"\nsentence_b = \"\u56de\u4f86\u7684\u6642\u5019\u8981\u7d66\u4ed6\u9322\u3002\"\n\ninputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\ntoken_type_ids = inputs['token_type_ids']\ninput_ids = inputs['input_ids']\nattention = model(input_ids, token_type_ids=token_type_ids)[-1]\ninput_id_list = input_ids[0].tolist() # Batch index 0\ntokens = tokenizer.convert_ids_to_tokens(input_id_list)\ncall_html()\n\nhead_view(attention, tokens)","18190646":"# Dataset\n1. \u56e0\u70ba\u653e\u9032 Bert \u7684\u8cc7\u6599\u9700\u8981\u4e09\u6a23\u6771\u897f\uff0c\u5206\u5225\u662f (token_tensor, segment_tensor, mask_tensor)\n    - token tensor \u5c31\u662f\u6bcf\u4e00\u500b\u53e5\u5b50\u8f49\u63db\u70ba id \u4e4b\u5f8c\u7684\u53e5\u5b50\n    - segment tensor \u662f\u56e0\u70ba Bert \u7684\u61c9\u7528\u4e2d\u67d0\u4e00\u4e9b\u60c5\u6cc1\u662f\u5169\u500b\u53e5\u5b50\u7684\u8f38\u5165\uff0c\u5fc5\u9808\u544a\u8a34\u6a21\u578b\u54ea\u4e00\u4e9b\u662f\u7b2c\u4e00\u53e5\uff0c\u54ea\u4e00\u4e9b\u662f\u7b2c\u4e8c\u53e5\n    - mask tensor \u4e3b\u8981\u8ddf Bert \u5169\u500b\u4e3b\u8981\u7684\u4efb\u52d9\u6709\u95dc\uff0cBert \u6703\u628a\u55ae\u5b57\u906e\u7f69\u8d77\u4f86\u7576\u4f5c\u514b\u6f0f\u5b57\u4f86\u9810\u6e2c (\u53e6\u4e00\u500b\u4efb\u52d9\u662f Next Sentence Prediction)\n2. \u5c07\u90a3\u4e09\u6a23\u6771\u897f\u5f9e train \u62ff\u51fa\u4f86\uff0c\u9084\u6709\u6b63\u78ba\u7b54\u6848 label_tensor","7fc68d45":"# Predict\n1. \u5c07\u8d85\u904e 0.5 \u5b9a\u70ba\u6709\u60e1\u610f\u7684\u8a55\u8ad6\n2. \u8f38\u5165\u6a21\u578b\u3001\u5207\u5272\u597d batch \u7684 dataloader\u3001\u8ddf\u539f\u4f86\u7684 dataframe \u7b97\u51fa\u9810\u6e2c\u7684\u7b54\u6848","a3f4673e":"# \u8996\u89ba\u5316 Attention\n- \u5f9e\u9019\u500b\u8996\u89ba\u8a71\u5de5\u5177\u4e2d\u6211\u5011\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u67d0\u4e00\u4e9b Bert \u7684\u67d0\u4e9b\u5c64\u662f\u80fd\u77e5\u9053\u300e\u4ed6 - \u963f\u660e\u300f\u4e4b\u9593\u7684\u95dc\u4fc2\uff0c\u6216\u662f\u300e\u7d66 - \u963f\u660e\u300f\u9019\u500b\u52d5\u4f5c\u7684\u95dc\u4fc2\n- \u4e0d\u540c\u7684 head \u80fd\u76e3\u63a7\u4e0d\u4e00\u6a23\u7684\u7279\u5fb5","a21c2888":"# Device\n- \u5c07\u6574\u500b\u6a21\u578b\u653e\u5165 TPU \u7576\u4e2d","4ee78e7e":"# \u8b80\u53d6\u8cc7\u6599\n1. \u5f9e\u7af6\u8cfd\u4e2d\u53d6\u51fa\u7d93\u9810\u8655\u7406\u7684\u8a13\u7df4\u8cc7\u6599 `jigsaw-toxic-comment-train-processed-seqlen128.csv` --> \u53e5\u5b50\u7684\u6700\u5927\u9577\u5ea6\u662f 128\n2. \u4e5f\u53d6\u51fa\u8655\u7406\u904e\u7684\u9a57\u8b49\u8cc7\u6599 `validation-processed-seqlen128.csv` \u548c\u6e2c\u8a66\u8cc7\u6599 `test-processed-seqlen128.csv`\n3. \u4fdd\u7559 `[id, comment_text, input_word_ids, input_mask, all_segment_id, toxic]`\uff0ccomment_text \u4ee3\u8868 twitter \u7684\u7559\u8a00\uff0ctoxic \u662f 1 \u8868\u793a\u60e1\u610f\uff0c0 \u8868\u793a\u5b89\u5168\n4. \u8207\u524d\u9762\u5169\u500b\u6a21\u578b\u76f8\u540c\u4f7f\u7528 20000 \u7b46\u8cc7\u6599\u7576\u4f5c\u6211\u5011\u6700\u5f8c\u7684\u8a13\u7df4\u8cc7\u6599","a1fe40a7":"# Jigsaw Multilingual Toxic Comment Classification - Bert \n#### Members: \u8cc7\u79d1\u56db \u5289\u4e0a\u9293 105703030 \u8cc7\u79d1\u56db \u90b1\u986f\u5b89 105703012 \u8cc7\u79d1\u56db \u6797\u701a\u8ed2 105703004","12577b01":"# Bert \u67b6\u69cb\n1. \u8b80\u53d6 bert-base-multilingual-cased \u7684\u9810\u8a13\u7df4\u6a21\u578b\n2. \u53d6\u51fa\u6700\u5f8c\u4e00\u5c64\u7684 hidden states \u800c\u4e0d\u662f CLS (\u81ea\u9996\u5206\u985e\u7684\u7279\u6b8a\u5b57\u5143) \u7684\u8cc7\u8a0a --> \u7d93\u904e\u5617\u8a66\u4f7f\u7528 CLS \u7684\u6210\u7e3e\u6bd4\u8f03\u4f4e\u843d\n3. \u5c07\u53d6\u51fa\u4f86\u7684 hidden states \u505a\u4e00\u6b21 average pooling\uff0c\u63a5\u8457\u505a max pooling\uff0c\u5c07\u5169\u500b\u7684\u7d50\u679c\u4e32\u63a5\u8d77\u4f86\uff0c\u56e0\u6b64\u7dad\u5ea6\u8b8a\u70ba\u539f\u4f86\u7684\u5169\u500d ($2\\times768$)\n4. \u63a5\u8457 dropout(0.3) --> linear(tanh) --> output\n","d39bd1ea":"# Bert \u5728\u8a13\u7df4\u4e4b\u524d\u7684\u6210\u7e3e\n- \u5206\u5225\u662f\u67e5\u770b train\u3001\u5404\u500b\u8a9e\u8a00\u7684 valid \u5728\u8a13\u7df4\u4e4b\u524d\u7684\u6210\u679c\n- \u5f9e\u9019\u88e1\u6211\u5011\u53ef\u4ee5\u770b\u51fa\uff0c\u4e0d\u7ba1\u662f train \u9084\u662f valid \u7684\u5404\u7a2e\u8a9e\u8a00\u5728\u6c92\u6709\u8a13\u7df4\u4e4b\u524d\u7684 auc \u8ddf accuracy \u7684\u8868\u73fe\u90fd\u662f\u975e\u5e38\u4e0d\u597d\u7684","40e5868d":"# Bert \u57fa\u790e\u53c3\u6578\u8a2d\u5b9a","e4a0be37":"\u78ba\u8a8d\u6a21\u578b\u7684\u8f38\u51fa\u8ddf\u6240\u8a2d\u8a08\u7684\u67b6\u69cb\u662f\u5426\u76f8\u540c","34b53448":"# \u9810\u6e2c\u6e2c\u8a66\u8cc7\u6599\u4e26\u8f38\u51fa","a50cd97a":"# Bert \u5728\u8a13\u7df4\u4e4b\u5f8c\u7684\u6210\u7e3e\n1. \u7d93\u904e\u53c3\u6578\u7684\u5fae\u8abf\u8b93 Bert \u66f4\u7b26\u5408\u73fe\u5728\u9019\u500b\u4efb\u52d9\n2. \u7d50\u679c\u53ef\u4ee5\u770b\u51fa\u9032\u6b65\u975e\u5e38\u5730\u986f\u8457\uff0cAUC \u8ddf accuracy \u5728\u6240\u6709\u7684\u8cc7\u6599\u96c6\u4e0a\u5e7e\u4e4e\u90fd\u6709\u9054\u5230 0.8 \u4ee5\u4e0a\n3. \u900f\u904e fine-tune \u80fd\u8b93 transfer learning \u7522\u751f\u6700\u5927\u7684\u6548\u679c\n","42e312e1":"# Train\n1. \u8a13\u7df4\u968e\u6bb5\u53ea\u8a13\u7df4\u4e86\u4e00\u500b epoch\uff0c\u56e0\u70ba\u4e4b\u524d\u5617\u8a66\u4f7f\u7528\u66f4\u591a epoch \u6642\uff0closs \u4e0d\u65b7\u7684\u4e0a\u5347\n2. \u5728 5 \u5206\u9418\u4ee5\u5167\u80fd\u5b8c\u6210\u4e86 20000 \u7b46\u8cc7\u6599\u7684\u8a13\u7df4 (\u66fe\u5617\u8a66\u4f7f\u7528\u8f03\u5927\u91cf\u7684\u8cc7\u6599\u4f46 performance \u6c92\u6709\u592a\u5927\u7684\u9032\u6b65)","5de23331":"# TPU\n1. \u5b89\u88dd\u65b0\u7248\u7684 pip\uff0c\u53bb\u9664 warning\n2. \u5b89\u88dd\u70ba\u4e86\u4f7f\u7528 TPU \u7684\u76f8\u4f9d\u5957\u4ef6","5a8c83b4":"# Train - function\n1. optimizer: Adam, scheduler: lr \u96a8\u8457\u6b65\u9a5f\u4e0b\u964d, loss function: BCEWithLogitsLoss\n2. \u5c07\u6bcf\u4e00\u500b input \u5206\u5225\u653e\u9032 tensor \u4ee5\u5229\u8a08\u7b97\n3. loss --> backward --> optimize --> schedule\n4. \u8a08\u7b97\u4e00\u500b epoch \u7d50\u675f\u5f8c valid \u7684\u6210\u7e3e","07ea8388":"# Dataloader\n1. \u5c07\u5f9e csv \u8b80\u51fa\u4f86\u7684\u8cc7\u6599\u7d93\u904e\u525b\u525b\u7684 Dataset \u9032\u884c\u8655\u7406\n2. \u5c07\u4e0d\u540c\u7684\u8a9e\u8a00\u5f9e valid \u5206\u96e2\u51fa\u4f86\uff0c\u5206\u5225\u5b58\u653e\uff0c\u70ba\u4e86\u89c0\u5bdf\u5404\u8a9e\u8a00\u7684\u60c5\u6cc1\n3. \u4e4b\u5f8c\u5207\u5272\u51fa\u5c0d\u61c9\u7684 batch size ","eec75c58":"\u5c07 model \u7684\u6b0a\u91cd\u5132\u5b58\u8d77\u4f86"}}