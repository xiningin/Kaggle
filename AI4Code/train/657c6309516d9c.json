{"cell_type":{"5d3d095d":"code","e752a852":"code","b8ff1794":"code","14b01af5":"code","49ca70fb":"code","5817b103":"code","92455cbb":"code","df8fea4f":"code","8dc58558":"code","f63c6976":"code","6f3e8227":"code","eab7ac97":"code","06b27c31":"code","c7cbd8de":"code","6592d0fb":"code","47daca9b":"code","db362ed9":"code","3b3c324d":"code","8a7ec148":"code","9929570c":"code","896d4a03":"code","e3279cca":"code","c1ceb4dc":"code","3a27aee4":"code","a2dbbccc":"code","481d8560":"code","a77db962":"code","ae7c63c5":"code","551e1857":"code","35746b9b":"code","a228a1f6":"code","c81291e5":"code","b845f3a8":"code","9fe9eef4":"code","548e3988":"code","8ffd619c":"code","7759cb66":"code","576a90d5":"code","0a588dcb":"code","a1308591":"code","af8d7fbd":"code","25e33a45":"code","a1da1560":"code","ef1882fc":"code","1e72ea93":"code","9df4605a":"code","1d464245":"code","5422d730":"code","432295a9":"code","708b1844":"code","b49937ee":"code","a8e8bd4e":"code","24d0032c":"code","726e5932":"code","6f8bec53":"code","a2c0ca56":"code","c62a9617":"code","58414eb0":"code","181d6466":"code","0f1713b4":"code","b02df986":"code","d036dd3a":"code","a13b2511":"code","1b6833e7":"code","9449c58a":"code","8c56c603":"code","9db8594a":"code","89e86186":"code","a08d3756":"code","17ab718f":"code","003477e8":"code","d8a35eb6":"code","1bac8fef":"code","c81dd013":"code","a09ff654":"code","be7977fb":"code","eb1b820c":"code","c8fa6c6b":"code","f6505027":"markdown","52e108e4":"markdown","dfff262e":"markdown","21eaaf4e":"markdown","d1bbd3bb":"markdown","4362d6be":"markdown","7487dab1":"markdown","43b768a5":"markdown","929b68bd":"markdown","adc69165":"markdown","de3a2418":"markdown","5a99d834":"markdown","f52b1171":"markdown","5bacac0f":"markdown","189a02f1":"markdown","2d1fe025":"markdown","09bd731a":"markdown"},"source":{"5d3d095d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e752a852":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\nprint(df.shape)","b8ff1794":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, plot_roc_curve, roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom category_encoders import OrdinalEncoder\nfrom numpy.random import permutation\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.linear_model import LogisticRegression\nfrom category_encoders import OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import make_pipeline\n\nfrom statsmodels.formula.api import ols\n\nfrom skopt import BayesSearchCV\n!pip install scikit-optimize\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom numpy.random import permutation\nfrom xgboost import XGBRegressor\nfrom pdpbox.pdp import pdp_isolate, pdp_plot\nfrom pdpbox.pdp import pdp_interact, pdp_interact_plot\n","14b01af5":"#Let's do this using a different subset of our data (the Clinical Attributes) so that we can look at DNA 'Mutation_count' more closely.","49ca70fb":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\n\ndef wrangle_clinical(df):\n    \n    Clinical31_df = df.iloc[:,0:31].copy() #The clinical features\n\n    drop_cols = ['patient_id','cancer_type','er_status_measured_by_ihc','her2_status_measured_by_snp6'] #reduncancy and high cardinality\n\n\n    Clinical31_df.drop(columns = drop_cols, inplace = True)\n\n\n    # Drop nulls\n    Clinical31_df.dropna(inplace = True)\n\n    C26_df = Clinical31_df.copy()\n\n    # #Feature Engineering\n\n    # #I will divide those into months lived, into quartiles, then into whether they died or not, yeilding 8 categories\n\n    # #Alive Quartile Masks\n\n    mask1 = (C26_df['overall_survival'] == 1) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[3]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[4]) \n\n    mask2 = (C26_df['overall_survival'] == 1) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[4]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[5]) \n\n    mask3 = (C26_df['overall_survival'] == 1) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[5]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[6]) \n\n    mask4 = (C26_df['overall_survival'] == 1) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[6]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[7]) \n\n    #Dead Quartile Masks\n\n    mask5 = (C26_df['overall_survival'] == 0) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[3]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[4]) \n\n    mask6 = (C26_df['overall_survival'] == 0) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[4]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[5]) \n\n    mask7 = (C26_df['overall_survival'] == 0) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[5]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[6]) \n\n    mask8 = (C26_df['overall_survival'] == 0) & \\\n    (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[6]) & \\\n    (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[7]) \n\n    C26_df['mo_Q\/Alive\/Deceased'] = 0\n\n    C26_df['mo_Q\/Alive\/Deceased'][mask1] = 'Alive_Q1'\n    C26_df['mo_Q\/Alive\/Deceased'][mask2] = 'Alive_Q2'\n    C26_df['mo_Q\/Alive\/Deceased'][mask3] = 'Alive_Q3'\n    C26_df['mo_Q\/Alive\/Deceased'][mask4] = 'Alive_Q4'\n\n    C26_df['mo_Q\/Alive\/Deceased'][mask5] = 'Dead_Q1'\n    C26_df['mo_Q\/Alive\/Deceased'][mask6] = 'Dead_Q2'\n    C26_df['mo_Q\/Alive\/Deceased'][mask7] = 'Dead_Q3'\n    C26_df['mo_Q\/Alive\/Deceased'][mask8] = 'Dead_Q4'\n\n    # Patient ID is a unique identifier, high cardinality\n    #Drop 'cancer_type'because of redundancy with 'cancer_type_detailed'\n    #Drop '-er_status_measured_by_ihc'=, redundancy with 'er_status'  (immunohistocompatibility)\n    # 'her2_status_measured_by_snp6', redundancy with'her2_status'\n    \n    return C26_df\n\n","5817b103":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\nC26_df = wrangle_clinical(df)","92455cbb":"print(C26_df.shape)\nprint(C26_df.columns)","df8fea4f":"\nplt.bar(x = ['dead','alive'], height = df['overall_survival'].value_counts())\n\n#The alive\/dead categories are farily balanced, so accuracy could be a good evaluative metric for models","8dc58558":"\n#Get boxplot graphs for overall surival months and some interesting categories \n \nplt.figure(figsize=(20,8))\nsns.boxplot(x='lymph_nodes_examined_positive', y='overall_survival_months', hue='overall_survival', data=df) #ax=axes[0, 0]\nplt.xlabel(\"Lymph nodes examined positive\", fontsize=16)\nplt.xticks(fontsize=12)\nplt.ylabel(\"Overall survival (months)\", fontsize=16)\nplt.yticks(fontsize=12)\nplt.title(\"Overall survival vs. positive lymph nodes\", fontsize=16)","f63c6976":"#My main goal is to illuminate the effects of mutations on survivall\n#Let's look at a plot on 'mutation_count'\n\nplt.figure(figsize = (20,8))\nsns.boxplot(x = 'mutation_count', y = 'overall_survival_months', hue = 'overall_survival', data = df)\nplt.xlabel('mutation_count')\nplt.ylabel('overall_survival_months')\nplt.title('survival time vs mutation count')","6f3e8227":"plt.figure(figsize = (22,10))\nsns.boxplot(x = 'mutation_count', y = 'overall_survival_months', hue = 'death_from_cancer', data = df)\nplt.xlabel('mutation_count')\nplt.ylabel('overall_survival_months')\nplt.title('survival time vs mutation count')","eab7ac97":"plt.figure(figsize = (42,18))\nsns.boxplot(x = 'mutation_count', y = 'overall_survival_months' , hue = 'mo_Q\/Alive\/Deceased',\n            hue_order = ['Alive_Q1','Alive_Q2','Alive_Q3','Alive_Q4','Dead_Q1','Dead_Q2','Dead_Q3','Dead_Q4'],data = C26_df)\nplt.xlabel('mutation_count', fontsize = 20)\nplt.xticks(fontsize = 20)\nplt.xlim([0,18])\nplt.ylabel('overall_survival_months', fontsize = 20)\nplt.yticks(fontsize = 20)\nplt.title('survival time vs mutation count', fontsize = 20)\nplt.legend(fontsize = 20)","06b27c31":"C26_df.columns","c7cbd8de":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\nC26_df = wrangle_clinical(df)\n\nC26_df.drop(columns = ['mo_Q\/Alive\/Deceased','death_from_cancer','overall_survival'], inplace = True) #to prevent leakage\n\ntarget = 'overall_survival_months'\n\nc26_y = C26_df[target]\nc26_X = C26_df.drop(columns = target)\n\n\nc26X_train, c26X_test, c26y_train, c26y_test = train_test_split(c26_X, c26_y, test_size=0.2, random_state=42)","6592d0fb":"print('Mean Survival(months):', c26_y.mean())\nc26y_pred = [c26_y.mean()] * len(c26_y)\nBaseline  = mean_absolute_error(c26_y, c26y_pred)\nprint('Baseline MAE:', Baseline)","47daca9b":"# We will investigate a range of Linear models and select the best\n\nk_range = range(1,66,2)\n\ntrain_mae = []\ntest_mae = []\n\nfor k in k_range:\n  model_lr = make_pipeline(\n    OneHotEncoder(use_cat_names=True),\n    SimpleImputer(),\n    SelectKBest(k=k),\n    LinearRegression()\n  )\n\n  model_lr.fit(c26X_train, c26y_train);\n\n#   print(f'trained model, k={k}')\n\n  train_mae.append(mean_absolute_error(c26y_train, model_lr.predict(c26X_train)));\n  test_mae.append(mean_absolute_error(c26y_test, model_lr.predict(c26X_test)));","db362ed9":"#Let's see what we got\n\nplt.plot(k_range, test_mae, color='blue', label='Test')\nplt.plot(k_range, train_mae, color='orange', label='Train')\nplt.plot([Baseline]*k, color = 'black', label = 'Baseline')\nplt.title('Linear Model Hyperparameter Tuning')\nplt.xlabel('k features selected')\nplt.ylabel('MAE (months_lived)')\nplt.legend()","3b3c324d":"# Define the Best Linear Model for comparison against XG Boost\n\nc26_best25_model_lr1 = make_pipeline(\n    OneHotEncoder(use_cat_names=True),\n    SimpleImputer(),\n    StandardScaler(), #Assume features are normal distributed\n    SelectKBest(k = 65),\n    LinearRegression()\n)\n\nc26_best25_model_lr1.fit(c26X_train, c26y_train);\n","8a7ec148":"features = c26_best25_model_lr1.named_steps['onehotencoder'].get_feature_names()\nprint(len(features))\nfeatures","9929570c":"dir(c26_best25_model_lr1.named_steps)","896d4a03":"mask = c26_best25_model_lr1.named_steps['selectkbest'].get_support(indices = True)\nmask\n\nmask = mask.tolist()\nfeatures_selected = [features[i] for i in mask]\n","e3279cca":"c26_best25_model_lr1.named_steps['onehotencoder']","c1ceb4dc":"dir(c26_best25_model_lr1.named_steps['linearregression'])","3a27aee4":"r2_score(c26y_test, c26_best25_model_lr1.predict(c26X_test))","a2dbbccc":"plt.figure(figsize=(20,24))\nfeatures = features_selected\ncoefficients = c26_best25_model_lr1.named_steps['linearregression'].coef_\nfeat_importances = pd.Series(coefficients, index=features).sort_values(key=abs)\nfeat_importances.tail(65).plot(kind='barh')\nplt.title(\"Linear Model (Feature Importances)\")","481d8560":"#Seems to be that what I set out to do, determine how 'mutation_count' affects survival and survival time, has been a false errand.\n#I figured that with greater 'mutation_count', the less functional the cells become, leading to higher mortality and severity\n#But as can be seen with the feature importances, 'mutation_count' is pretty much at the bottom, irrelevant to mortality and severity \n#perhaps therer is a relationship but the model is getting distracted, or it is parabolic (hard for a linear model to interpret)\n\n#The next models will be to continue to confirm or deny the first","a77db962":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\nC26_df = wrangle_clinical(df)\n\n\nC26_df.drop(columns = ['mo_Q\/Alive\/Deceased','overall_survival_months','overall_survival'], inplace = True) #to prevent leakage\n\ntarget = 'death_from_cancer'\n\nc26_y = C26_df[target]\nc26_X = C26_df.drop(columns = target)\n\nc26X_train, c26X_test, c26y_train, c26y_test = train_test_split(c26_X, c26_y, test_size=0.2, random_state=42)","ae7c63c5":"#For Logistic regression, I need to get the feature matrix to be numerical features\n\n#X_train\n\nnon_floats = []\nfor col in c26X_train:\n    if c26X_train[col].dtypes != \"float64\":\n        non_floats.append(col)\nc26X_train = c26X_train.drop(columns=non_floats)\n\n#X_test\n\nnon_floats = []\nfor col in c26X_test:\n    if c26X_test[col].dtypes != \"float64\":\n        non_floats.append(col)\nc26X_test = c26X_test.drop(columns=non_floats)\n\n","551e1857":"c26X_train.info()","35746b9b":"print('Baseline Accuracy Score(Categorical)')\nprint(c26y_train.value_counts(normalize = True))","a228a1f6":"#classification report (after fit the model)","c81291e5":"model_logistic = make_pipeline(\n    OneHotEncoder(),\n    SimpleImputer(),\n    LogisticRegression(multi_class = 'multinomial'))\n    \nmodel_logistic.fit(c26X_train, c26y_train);","b845f3a8":"\nprint('Accuracy Train', accuracy_score(c26y_train, model_logistic.predict(c26X_train)));\nprint('Accuracy Test', accuracy_score(c26y_test, model_logistic.predict(c26X_test)));","9fe9eef4":"print('Prediction Train-----------')\nprint(pd.DataFrame(model_logistic.predict(c26X_train)).value_counts(normalize = True))\nprint('Actual Train-------------')\nprint(c26y_train.value_counts(normalize = True))","548e3988":"print('Prediction Test-----------')\nprint(pd.DataFrame(model_logistic.predict(c26X_test)).value_counts(normalize = True))\nprint('Actual Test-------------')\nprint(c26y_test.value_counts(normalize = True))","8ffd619c":"print(classification_report(c26y_test, model_logistic.predict(c26X_test)))","7759cb66":"# roc curve for classes\nfpr = []\ntpr = []\nthresh =[]\n\nn_class = 3\nfor i in range(n_class):\n    fpr[i], tpr[i], thresh[i] = roc_curve(c26y_test, model_logistic.predict_proba[:,i])\n# plotting\nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\nplt.title('Multiclass ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300);\n\n\n# plot_roc_curve(model_logistic, c26X_test, c26y_test)\n# plt.plot([(0,0), (1,1)], color='grey', linestyle='--')\n# plt.legend();","576a90d5":"dir(model_logistic)","0a588dcb":"C26_df = wrangle_clinical(df)\n\nmodel_ols = ols('overall_survival ~ mutation_count', data=C26_df).fit()\n\n#Print the model summary\nprint(model_ols.summary())","a1308591":"C26_df = wrangle_clinical(df)\n\nmodel_ols = ols('overall_survival ~ overall_survival_months + mutation_count', data=C26_df).fit()\n\n#Print the model summary\nprint(model_ols.summary())","af8d7fbd":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\nC26_df = wrangle_clinical(df)\n\nmodel_ols = ols('overall_survival ~ overall_survival_months', data=C26_df).fit()\n\n#Print the model summary\nprint(model_ols.summary())","25e33a45":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\nC26_df = wrangle_clinical(df)\n\nC26_df.drop(columns = ['mo_Q\/Alive\/Deceased','death_from_cancer','overall_survival'], inplace = True) #to prevent leakage\n\ntarget = 'overall_survival_months'\n\nc26_y = C26_df[target]\nc26_X = C26_df.drop(columns = target)\n\n\nc26X_train, c26X_test, c26y_train, c26y_test = train_test_split(c26_X, c26_y, test_size=0.2, random_state=42)","a1da1560":"c26X_train.info()","ef1882fc":"model_forest = make_pipeline(\n    OneHotEncoder(),\n    SimpleImputer(strategy = 'most_frequent'),\n    RandomForestRegressor(random_state=42)\n)","1e72ea93":"paras = {'simpleimputer__strategy': ['mean','median','most_frequent'],\n         'randomforestregressor__n_estimators': np.arange(50, 101, 10), \n         'randomforestregressor__max_depth': np.arange(10, 41, 5),\n         'randomforestregressor__max_samples': np.arange(0.3, 0.71, 0.2),}\n\n# oe = OneHotEncoder() #Tuple vs variable\n# c26X_train_ = oe.fit_transform(X = c26X_train)\n# c26X_test_ = oe.transform(X = c26X_test)\n\nmodel_forest_bs = BayesSearchCV(\n    model_forest,\n    random_state = 42,\n    search_spaces=paras,\n    n_iter=5,\n    cv=5,\n    n_jobs=-1,\n    verbose=1\n)\nmodel_forest_bs.fit(c26X_train, c26y_train);","9df4605a":"print(mean_absolute_error(c26y_train,model_forest_bs.predict(c26X_train)));\nprint(mean_absolute_error(c26y_test, model_forest_bs.predict(c26X_test)));\nprint((r2_score(c26y_test,model_forest_bs.predict(c26X_test)))) #Better than linear Model","1d464245":"model_forest_bs.best_params_","5422d730":"model_forest_best = make_pipeline(\n    OneHotEncoder(),\n    SimpleImputer(strategy = 'median'),\n    RandomForestRegressor(max_depth = 20, max_samples = 0.5,random_state=42)\n)\n\nmodel_forest_best.fit(c26X_train, c26y_train);","432295a9":"oe = model_forest_best.named_steps['onehotencoder']\noe2 = model_forest_best.named_steps['simpleimputer']\nc26X_test_t = oe.transform(c26X_test)\nc26X_test_ = oe2.transform(c26X_test_t)","708b1844":"perm_imp = permutation_importance(\n    model_forest_best.named_steps['randomforestregressor'],\n    c26X_test_,  #This might not be right, need to use val data? But since I did CV I don't have a val set?\n    c26y_test, \n    n_jobs=-1,\n    random_state=42\n)\n\n# Put results into DataFrame\ndata = {'importances_mean' : perm_imp['importances_mean'],\n        'importances_std' : perm_imp['importances_std']}\n\ndf = pd.DataFrame(data, index=c26X_test_t.columns) #!!!! oops, I set it equal to Df, change in future\ndf.sort_values(by='importances_mean', inplace=True)","b49937ee":"data","a8e8bd4e":"model_forest_best.named_steps['onehotencoder'].get_feature_names()","24d0032c":"plt.figure(figsize = (20,22))\ndf['importances_mean'].tail(65).plot(kind='barh')\nplt.xlabel('Importance (drop in accuracy)')\nplt.ylabel('Feature')\nplt.title('Permutation importance for model_forest');","726e5932":"#The BayesSearchCV takes a very long time for some reason.  I will comment it out to save time on saving versions of this notebook","6f8bec53":"# We will investigate a range of XG Boost models and select the best\n\n# c26_model_xgb = make_pipeline(OrdinalEncoder(),\n#                    SimpleImputer(),\n#                    XGBRegressor(n_estimators=20, random_state = 42))\n\n\n# paras = {'xgbregressor__n_estimators': np.arange(50, 111, 10),\n#          'xgbregressor__eta': np.arange(0.04,0.25,0.05),\n#          'xgbregressor__max_depth': np.arange(5, 12, 1)}\n\n# xgb_bs1 =  BayesSearchCV(\n#     c26_model_xgb,\n#     random_state = 42,\n#     search_spaces=paras,\n#     n_iter=13,\n#     cv=5,\n#     n_jobs=-1,\n#     verbose=1\n# )","a2c0ca56":"# xgb_bs1.fit(c26X_train, c26y_train);","c62a9617":"# print(xgb_bs1.best_score_)\n# xgb_bs1.best_params_","58414eb0":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\nC26_df = wrangle_clinical(df)\n\nC26_df.drop(columns = ['mo_Q\/Alive\/Deceased','death_from_cancer','overall_survival'], inplace = True) #to prevent leakage\n\ntarget = 'overall_survival_months'\n\nc26_y = C26_df[target]\nc26_X = C26_df.drop(columns = target)\n\n\nc26X_train, c26X_test, c26y_train, c26y_test = train_test_split(c26_X, c26_y, test_size=0.2, random_state=42)","181d6466":"c26_model_xgb_best = make_pipeline(OrdinalEncoder(),\n                   SimpleImputer(),\n                   XGBRegressor(n_estimators=300, eta = 0.01, max_depth = 5, random_state = 42, n_jobs = -1))\n\nc26_model_xgb_best.fit(c26X_train, c26y_train)","0f1713b4":"print(mean_absolute_error(c26y_train, c26_model_xgb_best.predict(c26X_train)));\nprint(mean_absolute_error(c26y_test, c26_model_xgb_best.predict(c26X_test)));\nprint((r2_score(c26y_test,c26_model_xgb_best.predict(c26X_test)))) #Better than linear Model","b02df986":"# %%time\n# xgb_gs1.fit(c26X_train, c26y_train)","d036dd3a":"# c26_model_xgb.get_params().keys()","a13b2511":"perm_imp = permutation_importance(\n    c26_model_xgb_best,\n    c26X_train,  #This might not be right, need to use val data? But since I did CV I don't have a val set?\n    c26y_train, \n    n_jobs=-1,\n    random_state=42\n)\n\n# Put results into DataFrame\ndata = {'importances_mean' : perm_imp['importances_mean'],\n        'importances_std' : perm_imp['importances_std']}\n\ndf = pd.DataFrame(data, index=c26X_train.columns) #!!!! oops, I set it equal to Df, change in future\ndf.sort_values(by='importances_mean', inplace=True)","1b6833e7":"plt.figure(figsize = (20,10))\ndf['importances_mean'].tail(30).plot(kind='barh')\nplt.xlabel('Importance (drop in accuracy)')\nplt.ylabel('Feature')\nplt.title('Permutation importance for model_xgb');","9449c58a":"importances = c26_model_xgb_best.named_steps['xgbregressor'].feature_importances_\nfeatures = c26X_train.columns\n\nfeat_imp = pd.Series(importances, index=features).sort_values()\nfeat_imp.tail(28).plot(kind='barh')\nplt.xlabel('Gini Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances for model_xgbregressor (target, \"months_lived\")');","8c56c603":"\n#Mutation count seems to be much more important to the tree based model and\/or permutation importance metric\/Gini Importance than the Linear model\n","9db8594a":"\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nplt.rcParams['figure.dpi'] = 72\n\n# Build your `pdp_isolate`\nisolate = pdp_isolate(\n    model=c26_model_xgb_best,\n    dataset=c26X_test,\n    model_features=c26X_test.columns,\n    feature='mutation_count',\n    n_jobs = -1\n)\n\n# Build your plot\npdp_plot(isolate, feature_name='mutation_count');\nplt.ylabel('Change number of months lived')\nplt.title('PDP for XGBRegressor')","89e86186":"features = ['mutation_count','age_at_diagnosis']\n\ninteract = pdp_interact(\n    c26_model_xgb_best,\n    dataset=c26X_test,\n    model_features=c26X_test.columns,\n    features=features\n)\n\npdp_interact_plot(interact, plot_type='grid', feature_names=features);","a08d3756":"features = ['mutation_count','lymph_nodes_examined_positive']\n\ninteract = pdp_interact(\n    c26_model_xgb_best,\n    dataset=c26X_test,\n    model_features=c26X_test.columns,\n    features=features\n)\n\npdp_interact_plot(interact, plot_type='grid', feature_names=features);","17ab718f":"#Looks like for the XGBRressor, mutation count seems to matter after there is a very high count","003477e8":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-gene-expression-profiles-metabric\/METABRIC_RNA_Mutation.csv')\n\nC26_df = wrangle_clinical(df)\n\nC26_df.drop(columns = ['mo_Q\/Alive\/Deceased','death_from_cancer','overall_survival'], inplace = True) #to prevent leakage\n\ntarget = 'overall_survival_months'\n\nc26_y = C26_df[target]\nc26_X = C26_df.drop(columns = target)\n\n\nc26X_train, c26X_test, c26y_train, c26y_test = train_test_split(c26_X, c26_y, test_size=0.2, random_state=42)","d8a35eb6":"oe = OneHotEncoder() #Tuple vs variable\nc26X_train_ = oe.fit_transform(X = c26X_train)\nc26X_test_ = oe.transform(X = c26X_test)\n\nc26_model_xgb_bestOHE = make_pipeline(\n                   SimpleImputer(),\n                   XGBRegressor(n_estimators=300, eta = 0.01, max_depth = 5, random_state = 42, n_jobs = -1))\n\nc26_model_xgb_bestOHE.fit(c26X_train_, c26y_train)","1bac8fef":"c26_model_xgb_bestOHE.named_steps","c81dd013":"print(mean_absolute_error(c26y_train, c26_model_xgb_bestOHE.predict(c26X_train_)));\nprint(mean_absolute_error(c26y_test, c26_model_xgb_bestOHE.predict(c26X_test_)));\nprint((r2_score(c26y_test,c26_model_xgb_bestOHE.predict(c26X_test_)))) #Better than linear Model","a09ff654":"print(c26X_train_.columns)\nprint(len(c26X_train_.columns))","be7977fb":"perm_imp = permutation_importance(\n    c26_model_xgb_bestOHE,\n    c26X_train_,  #This might not be right, need to use val data? But since I did CV I don't have a val set?\n    c26y_train, \n    n_jobs=-1,\n    random_state=42\n)\n\n# Put results into DataFrame\ndata = {'importances_mean' : perm_imp['importances_mean'],\n        'importances_std' : perm_imp['importances_std']}\n\ndf = pd.DataFrame(data, index=c26X_train_.columns) #!!!! oops, I set it equal to Df, change in future\ndf.sort_values(by='importances_mean', inplace=True)","eb1b820c":"plt.figure(figsize = (20,25))\ndf['importances_mean'].tail(65).plot(kind='barh')\nplt.xlabel('Importance (drop in accuracy)')\nplt.ylabel('Feature')\nplt.title('Permutation importance for model_xgb');","c8fa6c6b":"\nClinical31_df = df.iloc[:,0:31].copy()\n\n# Patient ID is a unique identifier, high cardinality\n#Drop 'cancer_type'because of redundancy with 'cancer_type_detailed'\n#Drop '-er_status_measured_by_ihc'=, redundancy with 'er_status'  (immunohistocompatibility)\n# 'her2_status_measured_by_snp6', redundancy with'her2_status'\n#'death_from_cancer' redundancy\n# We want to target 'overall_survival_months', but we will leave 'overall_survival', because we will feature engineer with them\n\n\ndrop_cols = ['patient_id','cancer_type','er_status_measured_by_ihc','her2_status_measured_by_snp6']\n\nClinical31_df.drop(columns = drop_cols, inplace = True)\n\n\n# Drop nulls\nClinical31_df.dropna(inplace = True)\n\nC26_df = Clinical31_df.copy()\n\n# #Feature Engineering\n\n# #I will divide those into months lived, into quartiles, then into whether they died or not, yeilding 8 categories\n\n# #Alive Quartile Masks\n\n# mask1 = (C26_df['overall_survival'] == 1) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[3]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[4]) \n\n# mask2 = (C26_df['overall_survival'] == 1) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[4]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[5]) \n\n# mask3 = (C26_df['overall_survival'] == 1) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[5]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[6]) \n\n# mask4 = (C26_df['overall_survival'] == 1) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[6]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[7]) \n\n# #Dead Quartile Masks\n\n# mask5 = (C26_df['overall_survival'] == 0) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[3]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[4]) \n\n# mask6 = (C26_df['overall_survival'] == 0) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[4]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[5]) \n\n# mask7 = (C26_df['overall_survival'] == 0) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[5]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[6]) \n\n# mask8 = (C26_df['overall_survival'] == 0) & \\\n# (C26_df['overall_survival_months'] >= C26_df['overall_survival_months'].describe()[6]) & \\\n# (C26_df['overall_survival_months'] <= C26_df['overall_survival_months'].describe()[7]) \n\n# C26_df['mo_Q\/Alive\/Deceased'] = 0\n\n# C26_df['mo_Q\/Alive\/Deceased'][mask1] = 'Alive_Q1'\n# C26_df['mo_Q\/Alive\/Deceased'][mask2] = 'Alive_Q2'\n# C26_df['mo_Q\/Alive\/Deceased'][mask3] = 'Alive_Q3'\n# C26_df['mo_Q\/Alive\/Deceased'][mask4] = 'Alive_Q4'\n\n# C26_df['mo_Q\/Alive\/Deceased'][mask5] = 'Dead_Q1'\n# C26_df['mo_Q\/Alive\/Deceased'][mask6] = 'Dead_Q2'\n# C26_df['mo_Q\/Alive\/Deceased'][mask7] = 'Dead_Q3'\n# C26_df['mo_Q\/Alive\/Deceased'][mask8] = 'Dead_Q4'","f6505027":"# Investigation #1","52e108e4":"# XG Boost Model (with OHE, Target = 'overall_survival_months')","dfff262e":"# Potential Target Variables","21eaaf4e":"# What is important to the Linear Model?","d1bbd3bb":"# Investigation of genetic data without expanded genetic data","4362d6be":"# Split C26_df into Training and Test sets (Target = 'overall_survival_months')","7487dab1":"# Best Boost Model (Target 'overall_survival_months')","43b768a5":"# XG Boost Model","929b68bd":"# 'death_from_cancer' Baseline","adc69165":"# --------------------------------","de3a2418":"# Random Forest Model (Target 'overall_survival_months')","5a99d834":"# Misc Workspace","f52b1171":"# Linear Model","5bacac0f":"# Ordinary Least Squares of 'mutation_count' and Targets","189a02f1":"# Logistic Regression Model (Target = 'death_from_cancer')","2d1fe025":"# Establish Baseline","09bd731a":"# Wrangling for a Target =  'overall_survial_months'"}}