{"cell_type":{"a298a609":"code","3db610d4":"code","30291b28":"code","87fc59b3":"code","19c6f59c":"code","97194e85":"code","0f1c7010":"code","b3ff69a9":"code","72223662":"code","b92b983a":"code","0d47d32a":"code","34c91b63":"code","94f19884":"code","05d6b2cb":"code","ddc1642d":"code","7f127dae":"code","4092245c":"code","6e4dac78":"code","25eb4277":"code","8bc3269b":"code","07d0b2ba":"code","19a89861":"code","aded66ee":"code","a0264a48":"code","d1e8ab35":"code","fef30c98":"code","71fe01f2":"code","b33c3d49":"code","3dd7d87e":"code","b1effd4f":"code","d21c0cd3":"code","0e2fbb8f":"code","9014c79c":"code","d3e8da09":"code","6055f80c":"code","4a837fb5":"code","47fb771e":"code","d755527a":"code","26f7ed37":"code","6b0e525f":"code","6f7698b6":"code","b9b8a0cf":"code","a5a17d4a":"code","166915f4":"code","c5608a23":"code","d9a881e9":"code","9c76c74b":"code","370f3469":"code","63e513e8":"code","19887b40":"markdown","3f189a65":"markdown","444ebcfa":"markdown","65ef2bfe":"markdown","6e5f3fea":"markdown","c9d4344e":"markdown","bd675620":"markdown","ea95094a":"markdown","696e615b":"markdown","d2c70f08":"markdown","43c50ce2":"markdown","46cb951a":"markdown","ae105e48":"markdown","7c307d61":"markdown","2f4894c9":"markdown","7cf95066":"markdown","d1da9845":"markdown","574440d6":"markdown","b2991eb5":"markdown","6add8704":"markdown","5677e626":"markdown","5148df38":"markdown","9a10a99f":"markdown","bc8532c7":"markdown","e3adf5b8":"markdown","03fb1256":"markdown","333b3b2c":"markdown","4b3b56ae":"markdown","13343090":"markdown","eb275a09":"markdown","cb01e3d5":"markdown","fc4c7973":"markdown","872cc5b7":"markdown","109c132b":"markdown","30a41341":"markdown","fcfb6233":"markdown","5e747952":"markdown","4694f479":"markdown","bdc466ea":"markdown"},"source":{"a298a609":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,plot_confusion_matrix,precision_score,recall_score,f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom scipy import ndimage\nfrom sklearn import  metrics\nfrom sklearn.metrics import roc_curve,roc_auc_score,plot_roc_curve\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.signal import savgol_filter\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n%matplotlib inline","3db610d4":"train = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\ntest = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTest.csv')","30291b28":"train","87fc59b3":"test","19c6f59c":"fig = plt.figure(figsize=(15,40))\nfor i in range(12):\n    ax = fig.add_subplot(14,4,i+1)\n    ax.scatter(np.arange(3197),train[train['LABEL'] == 2].iloc[i,1:],s=1)","97194e85":"fig = plt.figure(figsize=(15,40))\nfor i in range(12):\n    ax = fig.add_subplot(14,4,i+1)\n    ax.scatter(np.arange(3197),train[train['LABEL']==1].iloc[i,1:],s=1)","0f1c7010":"fig = plt.figure(figsize=(15,40))\nfor i in range(12):\n    ax = fig.add_subplot(14,4,i+1)\n    train[train['LABEL']==2].iloc[i,1:].hist(bins=40)","b3ff69a9":"fig = plt.figure(figsize=(15,40))\nfor i in range(12):\n    ax = fig.add_subplot(14,4,i+1)\n    train[train['LABEL']==1].iloc[i,1:].hist(bins=40)","72223662":"maxval = train.iloc[:,1:].max(axis=1)\nminval = train.iloc[:,1:].min(axis=1)","b92b983a":"plt.figure(figsize=(20,5))\nplt.title('Maximum flux value of each stars')\nplt.xlabel('Index number of stars')\nplt.ylabel('Max flux values')\nplt.plot(np.arange(len(maxval)),maxval)","0d47d32a":"plt.figure(figsize=(20,5))\nplt.title('Minimum flux value of each stars')\nplt.xlabel('Index number of stars')\nplt.ylabel('Min flux values')\nplt.plot(np.arange(len(minval)),minval)","34c91b63":"def reset(train,test):\n    train_X = train.drop('LABEL', axis=1)\n    train_y = train['LABEL'].values\n    test_X = test.drop('LABEL', axis=1)\n    test_y = test['LABEL'].values\n    return train_X,train_y,test_X,test_y\n\ntrain_X,train_y,test_X,test_y = reset(train,test)","94f19884":"def robust(df1,df2):\n    scaler = RobustScaler()\n    train_X = scaler.fit_transform(df1)\n    test_X = scaler.transform(df2)\n    return train_X,test_X","05d6b2cb":"def std_scaler(df1,df2):\n    std_scaler = StandardScaler()\n    train_X = std_scaler.fit_transform(df1)\n    test_X = std_scaler.fit_transform(df2)\n    return train_X,test_X","ddc1642d":"def norm(df1,df2):\n    train_X = normalize(df1)\n    test_X = normalize(df2)\n    return train_X,test_X","7f127dae":"# def gaussian(df1,df2):\n#     train_X = ndimage.filters.gaussian_filter(df1, sigma=10)\n#     test_X = ndimage.filters.gaussian_filter(df2, sigma=10)\n#     return train_X,test_X","4092245c":"def fourier(df1,df2):\n    train_X = np.abs(np.fft.fft(df1, axis=1))\n    test_X = np.abs(np.fft.fft(df2, axis=1))\n    return train_X,test_X","6e4dac78":"def savgol(df1,df2):\n    x = savgol_filter(df1,21,4,deriv=0)\n    y = savgol_filter(df2,21,4,deriv=0)\n    return x,y","25eb4277":"def smote(a,b):\n    model = SMOTE()\n    X,y = model.fit_sample(a, b)\n    return X,y","8bc3269b":"def logistic(train_X,train_y,test_X,test_y):\n    lgr = LogisticRegression(max_iter=1000)\n    lgr.fit(train_X,train_y)\n    prediction_lgr=lgr.predict(test_X)\n    print(\"-------------------------------------------\")\n    print(\"Logistic Regression\")\n    print(\"\")\n    print(classification_report(test_y,prediction_lgr))\n    fig = plt.figure(figsize=(22,7))\n    ax = fig.add_subplot(1,3,1)\n    plot_confusion_matrix(lgr,test_X,test_y,ax=ax)\n    ax = fig.add_subplot(1,3,2)\n    metrics.plot_roc_curve(lgr,test_X, test_y,ax=ax)\n    plt.plot([0, 1], [0, 1], 'k--')\n    ax = fig.add_subplot(1,3,3)\n    metrics.plot_precision_recall_curve(lgr, test_X, test_y,ax=ax)\n    f1=metrics.f1_score(test_y, prediction_lgr,pos_label=2)\n    print(\"F1 score of minority class:\",f1)\n    plt.show()\n    return f1\n#logistic(train_X,train_y,test_X,test_y)","07d0b2ba":"def decisionTree(train_X,train_y,test_X,test_y):\n    clf = tree.DecisionTreeClassifier()\n    clf = clf.fit(train_X, train_y)\n    y_pred_clf = clf.predict(test_X)\n    print(\"-------------------------------------------\")\n    print(\"DecisionTree Classifier\")\n    print(\"\")\n    print(classification_report(test_y,y_pred_clf))\n    fig = plt.figure(figsize=(22,7))\n    ax = fig.add_subplot(1,3,1)\n    plot_confusion_matrix(clf,test_X,test_y,ax=ax)\n    ax = fig.add_subplot(1,3,2)\n    metrics.plot_roc_curve(clf,test_X, test_y,ax=ax)\n    plt.plot([0, 1], [0, 1], 'k--')\n    ax = fig.add_subplot(1,3,3)\n    metrics.plot_precision_recall_curve(clf, test_X, test_y,ax=ax)\n    f1=metrics.f1_score(test_y, y_pred_clf,pos_label=2)\n    print(\"F1 score of minority class:\",f1)\n    plt.show()\n    return f1\n\n#decisionTree(train_X,train_y,test_X,test_y)","19a89861":"def linearSVC(train_X,train_y,test_X,test_y):\n    svm=LinearSVC()\n    svm.fit(train_X,train_y)\n    prediction_svm=svm.predict(test_X)\n    print(\"-------------------------------------------\")\n    print(\"Linear SVClassifier\")\n    print(\"\")\n    print(classification_report(test_y,prediction_svm))\n    fig = plt.figure(figsize=(22,7))\n    ax = fig.add_subplot(1,3,1)\n    plot_confusion_matrix(svm,test_X,test_y,ax=ax)\n    ax = fig.add_subplot(1,3,2)\n    metrics.plot_roc_curve(svm,test_X, test_y,ax=ax)\n    plt.plot([0, 1], [0, 1], 'k--')\n    ax = fig.add_subplot(1,3,3)\n    metrics.plot_precision_recall_curve(svm, test_X, test_y,ax=ax)\n    f1=metrics.f1_score(test_y, prediction_svm,pos_label=2)\n    print(\"F1 score of minority class:\",f1)\n    plt.show()\n    return f1\n#linearSVC(train_X,train_y,test_X,test_y)","aded66ee":"def naiveBayes(train_X,train_y,test_X,test_y):\n    gnb = GaussianNB()\n    gnb.fit(train_X, train_y)\n    y_pred=gnb.predict(test_X)\n    print(\"-------------------------------------------\")\n    print(\"Gaussian NaiveBayes Classifier\")\n    print(\"\")\n    print(classification_report(test_y,y_pred))\n    fig = plt.figure(figsize=(22,7))\n    ax = fig.add_subplot(1,3,1)\n    plot_confusion_matrix(gnb,test_X,test_y,ax=ax)\n    ax = fig.add_subplot(1,3,2)\n    metrics.plot_roc_curve(gnb,test_X, test_y,ax=ax)\n    plt.plot([0, 1], [0, 1], 'k--')\n    ax = fig.add_subplot(1,3,3)\n    metrics.plot_precision_recall_curve(gnb, test_X, test_y,ax=ax)\n    f1 = metrics.f1_score(test_y, y_pred,pos_label=2)\n    print(\"F1 score of minority class:\",f1)\n    plt.show()\n    return f1\n#naiveBayes(train_X,train_y,test_X,test_y)","a0264a48":"def knn(train_X,train_y,test_X,test_y):\n    neigh = KNeighborsClassifier(n_neighbors=3)\n    neigh.fit(train_X, train_y)\n    y_pred_neigh = neigh.predict(test_X)\n    print(\"-------------------------------------------\")\n    print(\"k-Nearest Neighbour Classifier\")\n    print(\"\")\n    print(classification_report(test_y,y_pred_neigh))\n    fig = plt.figure(figsize=(22,7))\n    ax = fig.add_subplot(1,3,1)\n    plot_confusion_matrix(neigh,test_X,test_y,ax=ax)\n    ax = fig.add_subplot(1,3,2)\n    metrics.plot_roc_curve(neigh,test_X, test_y,ax=ax)\n    plt.plot([0, 1], [0, 1], 'k--')\n    ax = fig.add_subplot(1,3,3)\n    metrics.plot_precision_recall_curve(neigh, test_X, test_y,ax=ax)\n    plt.show()\n    f1 = metrics.f1_score(test_y, y_pred_neigh,pos_label=2)\n    print(\"F1 score of minority class:\",f1)\n    return f1\n#knn(train_X,train_y,test_X,test_y) ","d1e8ab35":"def randomForest(train_X,train_y,test_X,test_y):\n    rnd = RandomForestClassifier()\n    rnd.fit(train_X, train_y)\n    y_pred_rnd = rnd.predict(test_X)\n    print(\"-------------------------------------------\")\n    print(\"Random Forest Classifier\")\n    print(\"\")\n    print(classification_report(test_y,y_pred_rnd))\n    fig = plt.figure(figsize=(22,7))\n    ax = fig.add_subplot(1,3,1)\n    plot_confusion_matrix(rnd,test_X,test_y,ax=ax)\n    ax = fig.add_subplot(1,3,2)\n    metrics.plot_roc_curve(rnd,test_X,test_y,ax=ax)\n    plt.plot([0, 1], [0, 1], 'k--') \n    ax = fig.add_subplot(1,3,3)\n    metrics.plot_precision_recall_curve(rnd, test_X, test_y,ax=ax)\n    plt.show()\n    f1 = metrics.f1_score(test_y, y_pred_rnd,pos_label=2)\n    print(\"F1 score of minority class:\",f1)\n    return f1\n#randomForest(train_X,train_y,test_X,test_y)","fef30c98":"train_X,train_y,test_X,test_y = reset(train,test)\nf1_original =  []\nf1_original.append(logistic(train_X,train_y,test_X,test_y))\nf1_original.append(decisionTree(train_X,train_y,test_X,test_y))\nf1_original.append(linearSVC(train_X,train_y,test_X,test_y))\nf1_original.append(naiveBayes(train_X,train_y,test_X,test_y))\nf1_original.append(knn(train_X,train_y,test_X,test_y))\nf1_original.append(randomForest(train_X,train_y,test_X,test_y))\nf1_original","71fe01f2":"# train_X,train_y,test_X,test_y = reset(train,test)\n\n# c=[0.01,0.1,1,10,100]\n# penalty = ['l1','l2']\n# loss = ['hinge','squared_hinge']\n# fit_intercept = [True,False]\n# max_iter = [1000,2000]\n# reports_lsvc={}            \n# for i in c:\n#     for j in penalty:\n#         for k in loss:\n#             for m in fit_intercept:\n#                 for n in max_iter:\n#                     if (j=='l1' and (k in ['hinge','squared_hinge'])):\n#                         continue\n#                     print(\"---------------------------------------------\")\n#                     print(\"C:\",i,\"Penalty:\",j,\"Loss:\",k,\"fit_intercept:\",m,\"max_iter:\",n)\n#                     svm=LinearSVC(penalty=j, loss=k, C=i, fit_intercept=m,max_iter=n,class_weight={1:100,2:1})\n#                     svm.fit(train_X,train_y)\n#                     prediction_svm=svm.predict(test_X)\n#                     report=classification_report(test_y,prediction_svm)\n#                     f1 = metrics.f1_score(test_y, prediction_svm,pos_label=2)\n#                     reports_lsvc[(i,j,k,m,n)]=[report,f1]\n#                     print(report)","b33c3d49":"# train_X,train_y,test_X,test_y = reset(train,test)\n\n# solvers = ['newton-cg', 'lbfgs', 'sag', 'saga']\n# penalty = ['l2']\n# c_values = [100, 10, 1.0, 0.1, 0.01]\n# reports_lg={}\n# for i in c_values:\n#     for j in solvers:\n#         print(\"C:\",i,\" Solver:\",j,\" Penalty: l2\")\n#         lgr = LogisticRegression(penalty='l2',C=i,solver=j,max_iter=1000)\n#         lgr.fit(train_X,train_y)\n#         prediction_lgr=lgr.predict(test_X)\n#         report_lg=classification_report(test_y,prediction_lgr)\n#         f1 = metrics.f1_score(test_y, prediction_lgr,pos_label=2)\n#         reports_lg[(\"l2\",i,j)]=[report_lg,f1]\n#         print(report_lg)","3dd7d87e":"train_X,train_y,test_X,test_y = reset(train,test)\na,b = fourier(train_X,test_X)\nc,d = savgol(a,b)\ne,f = norm(c,d)\ng,h = robust(e,f)","b1effd4f":"#index 0-37 contains stars with exoplanets and the rest are stars with no exoplanets\nplanets = [0,1,2,3,4,5,6,44,56,67,78,91,92,97]","d21c0cd3":"fig = plt.figure(figsize=(24,500))\nx = np.array(range(3197))\nfor i in range(0,len(planets)):\n    ax = fig.add_subplot(100,5,5*i+1)\n    ax.set_title('Original '+\"I:\"+str(planets[i])+\" Label:\"+str(train_y[planets[i]]))\n    ax.scatter(x,train_X.iloc[planets[i],:],s=1)\n    \n    ax = fig.add_subplot(100,5,5*i+2)\n    ax.set_title('Fourier '+\"I:\"+str(i)+\" Label:\"+str(train_y[planets[i]]))\n    ax.scatter(x,a[planets[i],:],s=1)\n    \n    ax = fig.add_subplot(100,5,5*i+3)\n    ax.set_title('Savgol '+\"I:\"+str(planets[i])+\" Label:\"+str(train_y[planets[i]]))\n    ax.scatter(x,c[planets[i],:],s=1)\n    \n    ax = fig.add_subplot(100,5,5*i+4)\n    ax.set_title('Normalized '+\"I:\"+str(planets[i])+\" Label:\"+str(train_y[planets[i]]))\n    ax.scatter(x,e[planets[i],:],s=1)\n    \n    ax = fig.add_subplot(100,5,5*i+5)\n    ax.set_title('Robust '+\"I:\"+str(planets[i])+\" Label:\"+str(train_y[planets[i]]))\n    ax.scatter(x,g[planets[i],:],s=1)    ","0e2fbb8f":"train_X,train_y,test_X,test_y = reset(train,test)\ntrain_X,test_X = fourier(train_X,test_X)\ntrain_X,test_X = savgol(train_X,test_X)\ntrain_X,test_X = norm(train_X,test_X)\ntrain_X,test_X = robust(train_X,test_X)\n\nf1_processed =  []\nf1_processed.append(logistic(train_X,train_y,test_X,test_y))\nf1_processed.append(decisionTree(train_X,train_y,test_X,test_y))\nf1_processed.append(linearSVC(train_X,train_y,test_X,test_y))\nf1_processed.append(naiveBayes(train_X,train_y,test_X,test_y))\nf1_processed.append(knn(train_X,train_y,test_X,test_y))\nf1_processed.append(randomForest(train_X,train_y,test_X,test_y))\nf1_processed","9014c79c":"train_X,train_y,test_X,test_y = reset(train,test)\n\ntrain_X,test_X = fourier(train_X,test_X)\ntrain_X,test_X = savgol(train_X,test_X)\ntrain_X,test_X = norm(train_X,test_X)\ntrain_X,test_X = robust(train_X,test_X)\n\ntrain_X,train_y = smote(train_X,train_y)\ntrain_X, X, train_y, y = train_test_split(train_X, train_y, test_size=0.3)\ntest_X = np.concatenate((test_X, X), axis=0)\ntest_y = np.concatenate((test_y, y), axis=0)\n\nf1_smote =  []\nf1_smote.append(logistic(train_X,train_y,test_X,test_y))\nf1_smote.append(decisionTree(train_X,train_y,test_X,test_y))\nf1_smote.append(linearSVC(train_X,train_y,test_X,test_y))\nf1_smote.append(naiveBayes(train_X,train_y,test_X,test_y))\nf1_smote.append(knn(train_X,train_y,test_X,test_y))\nf1_smote.append(randomForest(train_X,train_y,test_X,test_y))\nf1_smote","d3e8da09":"train_X,train_y,test_X,test_y = reset(train,test)\n\ntrain_X,test_X = fourier(train_X,test_X)\ntrain_X,test_X = savgol(train_X,test_X)\ntrain_X,test_X = norm(train_X,test_X)\ntrain_X,test_X = robust(train_X,test_X)\n\ntrain_X,train_y = smote(train_X,train_y)\ntrain_X, X, train_y, y = train_test_split(train_X, train_y, test_size=0.3)\ntest_X = np.concatenate((test_X, X), axis=0)\ntest_y = np.concatenate((test_y, y), axis=0)","6055f80c":"pca = PCA(n_components=0.90)\npca.fit(train_X)\nprint(pca.explained_variance_ratio_)\n#print(pca.singular_values_)\npca_train_X = pca.transform(train_X)\npca.fit(test_X)\nprint(pca.explained_variance_ratio_)\n#print(pca.singular_values_)\npca_test_X = pca.transform(test_X)\ntrain_X = pca_train_X[:,:6]\ntest_X = pca_test_X[:,:6]","4a837fb5":"f1_pca =  []\nf1_pca.append(logistic(train_X,train_y,test_X,test_y))\nf1_pca.append(decisionTree(train_X,train_y,test_X,test_y))\nf1_pca.append(linearSVC(train_X,train_y,test_X,test_y))\nf1_pca.append(naiveBayes(train_X,train_y,test_X,test_y))\nf1_pca.append(knn(train_X,train_y,test_X,test_y))\nf1_pca.append(randomForest(train_X,train_y,test_X,test_y))\nf1_pca","47fb771e":"pca_test_X = pd.DataFrame(pca_test_X)\npca_test_X['label'] = test_y\npd.plotting.parallel_coordinates(pca_test_X, 'label')","d755527a":"train_X,train_y,test_X,test_y = reset(train,test)\n\ntrain_X,test_X = fourier(train_X,test_X)\ntrain_X,test_X = savgol(train_X,test_X)\ntrain_X,test_X = norm(train_X,test_X)\ntrain_X,test_X = robust(train_X,test_X)","26f7ed37":"pca = PCA(n_components=3)\npca.fit(test_X)\nprint(pca.explained_variance_ratio_)\npca_test_X = pca.transform(test_X)","6b0e525f":"km = KMeans(n_clusters=2)\ny_predicted = km.fit_predict(test_X)","6f7698b6":"print(y_predicted[:40])\nprint(test_y[:40])","b9b8a0cf":"y_predicted\n#y_predicted = np.where(y_predicted == 1, 2, y_predicted)\ny_predicted = np.where(y_predicted == 0, 2, y_predicted)\n#y_predicted = np.where(y_predicted == 0, 2, y_predicted)","a5a17d4a":"print(classification_report(test_y,y_predicted))\nprint(confusion_matrix(test_y,y_predicted))","166915f4":"pca_test_X = pd.DataFrame(pca_test_X)\na = pd.Series(test_y)\nb = pd.Series(y_predicted)\npca_test_X = pd.concat([pca_test_X,a.rename('original'),b.rename('cluster')], axis=1)","c5608a23":"pca_test_X.tail()","d9a881e9":"x2 = pca_test_X[pca_test_X[\"original\"]==2]\nx1 = pca_test_X[pca_test_X[\"original\"]==1]\ny2 = pca_test_X[pca_test_X[\"cluster\"]==2]\ny1 = pca_test_X[pca_test_X[\"cluster\"]==1]","9c76c74b":"fig = plt.figure(figsize=(15,7))\n\nax = fig.add_subplot(1,2,1, projection='3d')\nax.set_title('Original')\nax.scatter(x1.iloc[:,0],x1.iloc[:,1],x1.iloc[:,2],c='yellow',s=5)\nax.scatter(x2.iloc[:,0],x2.iloc[:,1],x2.iloc[:,2],c='red',s=5)\nax = fig.add_subplot(1,2,2, projection='3d')\nax.set_title('K Means')\nax.scatter(y1.iloc[:,0],y1.iloc[:,1],y1.iloc[:,2],c='yellow',s=5)\nax.scatter(y2.iloc[:,0],y2.iloc[:,1],y2.iloc[:,2],c='red',s=5)","370f3469":"cols = ['Logistic','DecisionTree','LinearSVC','NaiveBayes','kNN','RandomForest']\nimport pandas as pd\ndf = pd.DataFrame(list(zip(f1_original,f1_processed,f1_smote,f1_pca)),columns = \n                  ['Original Data','Processed','Processed with SMOTE','Processed-SMOTE and PCA'],index=cols) \ndf","63e513e8":"f,ax = plt.subplots(figsize=(12,7))\ndf.plot.barh(ax=ax,width=0.9,linewidth=1)","19887b40":"# k Means Clustering","3f189a65":"Lets try to correlate our classification problem with clusteing. Here we are giving input as 2 clusters and see whether those 2 clusters resembles our class labels.","444ebcfa":"Linear SVC with Hyperparameter Tuning","65ef2bfe":"Below table shows F1 scores of testing data with different combinations of scalers and normalizations","6e5f3fea":"### Histogram of Stars without Exoplanets","c9d4344e":"# Model Comparison with feature engineering","bd675620":"### Stars without Exoplanets","ea95094a":"- Without feature engineering and with original testing dataset, all the models gave low scores.\n- Hyperparameter tuning on original data gave unsatisfactory results.\n- With feature engineering and Original dataset, the models have improved their scores considerably.\n- With SMOTE, we got a perfect classification scores for almost all scores.\n- PCA with 5 columns also gave a good result.The score for Gaussian NB and kNN also improved with PCA.","696e615b":"From this we can conclude that the feature engineering with Fast Fourier Transform, Savitzky-Golay filter, Normalization and then Robust Scaler yields best results. **Decision Tree** gave best result for the original testing data.","d2c70f08":"We can see that with the feature engineering, the results have improved drastically even with our small imbalanced testing dataset. Here the **Decision Tree** model gave the best result.","43c50ce2":"![image-3.png](attachment:image-3.png)","46cb951a":"With SMOTE, the results have improved significantly. Only Gaussian Naive Bayes gave comparitively low score.","ae105e48":"## Sample Visualization","7c307d61":"# Comparison of Machine Learning Models","2f4894c9":"## Visualization with feature engineering","7cf95066":"## Range of maximum and minimum values.","d1da9845":"### Histogram of Stars with Exoplanets","574440d6":"From the above figure we can observe that both the clusters are different although all of our stars with exoplanets are in one cluster (Right side figure - red cluster).","b2991eb5":"# Model comparison with Original Data","6add8704":" #### I will not be running the parameter tuning code here due to taking too much time for getting results. ","5677e626":"# Identifying Stars with Exoplanets","5148df38":"#### Stars with Exoplanets","9a10a99f":"# Conclusion","bc8532c7":"# Hyperparameter tuning with Logistic Regression and Linear SVC","e3adf5b8":"Here, I am oversampling my training dataset. Then 30% of the data is extracted and merged with testing data. Now the resulting sample contains both artificial samples as well as the original sample.","03fb1256":"# From the above results, we can conclude that Logistic Regression and Random Forest with SMOTE gave the best F1-score and overall scores.","333b3b2c":"Defining ML models and scalers as function so that they can be reused","4b3b56ae":"Even with PCA, the models gave good result.","13343090":"### Reference:\n- Signal Processing: https:\/\/www.kaggle.com\/muonneutrino\/exoplanet-data-visualization-and-exploration\n\n#### Let me know about any suggestions or mistakes I made.","eb275a09":"First 5 columns contains 74% of my data. Therefore let's train the models using these components.","cb01e3d5":"# Feature Engineering","fc4c7973":"I have tried different combinations of filering, scaling and normalizing, the results for which can be found in the CONCLUSION section. The best one among them is listed below.\n\nWe are converting the original time domain signal to frequency domain.Then we are smooting it and the outliers at beginning and the end of the can be dealt while applying robust scaler.\n\nSteps involved in feature engineering\n- For reducing noise and filtering, we can apply fast fourier transform\n- For signal smooting, Savitzky-Golay  filter can be applied.\n- Then the signal can be normalized and then a robust scaler can be applied\n\nThe Robust scaler is ideal here because of the presence of the outliers.","872cc5b7":"From the above results, decision tree classifier gave a better F1 score although it missed a few stars with exoplanets. The poor perfromance of the other models is because of non-processed data.","109c132b":"Even with the Hyperparameter tuning, most of the models gave a maximum F1 score of 0.04 which is unsatisfactory. Therefore we need to do feature engineering.","30a41341":"Since the data is highly imbalanced, the evaluation metrics that we will be using is **F1 scores and Precision-Recall curves**. Our main aim is to get high recall as possible without sacrificing much precision. In a worst case scenerio, it is okay for model to classify a star as non-exoplanet star as exoplanet-star as long as it doesn't classify an exoplanet star as non-exoplanet star ie missing stars with exoplanets.","fcfb6233":"We are looking for U-shaped curves in the signal. The dataset doesn't seems to have any null or missing values.","5e747952":"# Principal Component Analysis","4694f479":"## Observations so far\n- Dataset is highly imbalanced i.e. 100:1\n- Even with a few plotted samples, we can see that, there are a lot of outliers.\n- May contains noise as U-shaped curve is not easily visible\n- So we may require signal filters and scalers","bdc466ea":"# Model Comparison with SMOTE"}}