{"cell_type":{"0e79bf05":"code","d0f86c6f":"code","270b25b6":"code","32b8b988":"code","ee015a8c":"code","32e86b59":"code","eecd882e":"code","69ad8dda":"code","c4580530":"code","90f64c73":"code","239772fd":"code","7a853dc1":"code","4faf9bd6":"code","4acbfb12":"code","0844d473":"code","e81e4c48":"code","acca7ef2":"code","8ce45b78":"code","fe086a08":"markdown","6a3d725f":"markdown","45f9ca46":"markdown","d78e380b":"markdown","b06ec61b":"markdown"},"source":{"0e79bf05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0f86c6f":"import pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport random\nimport unidecode\nimport torch","270b25b6":"train_on_gpu = torch.cuda.is_available()\nif(train_on_gpu): print(\"Training on GPU\")\nelse: print(\"Training on CPU, make number of epochs small\")","32b8b988":"text = \"\"\"The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation.\n\nWe hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a candid world.\n\nHe has refused his Assent to Laws, the most wholesome and necessary for the public good.\n\nHe has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.\n\nHe has refused to pass other Laws for the accommodation of large districts of people, unless those people would relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.\n\nHe has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of their public Records, for the sole purpose of fatiguing them into compliance with his measures.\n\nHe has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights of the people.\n\nHe has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining in the mean time exposed to all the dangers of invasion from without, and convulsions within.\n\nHe has endeavoured to prevent the population of these States; for that purpose obstructing the Laws for Naturalization of Foreigners; refusing to pass others to encourage their migrations hither, and raising the conditions of new Appropriations of Lands.\n\nHe has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary powers.\n\nHe has made Judges dependent on his Will alone, for the tenure of their offices, and the amount and payment of their salaries.\n\nHe has erected a multitude of New Offices, and sent hither swarms of Officers to harrass our people, and eat out their substance.\n\nHe has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.\n\nHe has affected to render the Military independent of and superior to the Civil power.\n\nHe has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged by our laws; giving his Assent to their Acts of pretended Legislation:\n\nFor Quartering large bodies of armed troops among us:\n\nFor protecting them, by a mock Trial, from punishment for any Murders which they should commit on the Inhabitants of these States:\n\nFor cutting off our Trade with all parts of the world:\n\nFor imposing Taxes on us without our Consent:\n\nFor depriving us in many cases, of the benefits of Trial by Jury:\n\nFor transporting us beyond Seas to be tried for pretended offences\n\nFor abolishing the free System of English Laws in a neighbouring Province, establishing therein an Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit instrument for introducing the same absolute rule into these Colonies:\n\nFor taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms of our Governments:\n\nFor suspending our own Legislatures, and declaring themselves invested with power to legislate for us in all cases whatsoever.\n\nHe has abdicated Government here, by declaring us out of his Protection and waging War against us.\n\nHe has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.\n\nHe is at this time transporting large Armies of foreign Mercenaries to compleat the works of death, desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled in the most barbarous ages, and totally unworthy the Head of a civilized nation.\n\nHe has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.\n\nHe has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction of all ages, sexes and conditions.\n\nIn every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury. A Prince whose character is thus marked by every act which may define a Tyrant, is unfit to be the ruler of a free people.\n\nNor have We been wanting in attentions to our Brittish brethren. We have warned them from time to time of attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, and we have conjured them by the ties of our common kindred to disavow these usurpations, which, would inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.\n\nWe, therefore, the Representatives of the united States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all Allegiance to the British Crown, and that all political connection between them and the State of Great Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and Things which Independent States may of right do. And for the support of this Declaration, with a firm reliance on the protection of divine Providence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor.\"\"\"","ee015a8c":"len(text.split())\n#number of sentences","32e86b59":"StopWords = set(nltk.corpus.stopwords.words('english'))\nexclude = set(string.punctuation)\nlemma = nltk.stem.wordnet.WordNetLemmatizer()\n\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.split() if i not in StopWords])\n    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\ntest_sentence = clean(text).lower().split()","eecd882e":"test_sentence","69ad8dda":"trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence)-2)]\nchunk_len = len(trigrams)\nprint(trigrams[:3])","c4580530":"vocab = set(test_sentence)\nvocab_len = len(vocab)\nword_to_idx = {word : i for (i, word) in enumerate(vocab)}","90f64c73":"inp = []\ntar= []\n#context here has two words\nfor context, target in trigrams:\n    context_idxs = torch.tensor([word_to_idx[w] for w in context], dtype = torch.long)\n    inp.append(context_idxs)\n    targ = torch.tensor([word_to_idx[target]], dtype = torch.long)\n    tar.append(targ)","239772fd":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable #this package has predefined gradient and derivative functions\n\nclass customGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers = 1):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        \n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers, batch_first = True)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, input, hidden):\n        input = self.encoder(input.view(1, -1))\n        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n        output = self.decoder(output.view(1, -1))\n        return output, hidden\n    \n    def init_hidden(self):\n        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))","7a853dc1":"def train(inp, target):\n    hidden = decoder.init_hidden().cuda()\n    decoder.zero_grad()\n    loss = 0\n    \n    for c in range(chunk_len):\n        output, hidden = decoder(inp[c].cuda(), hidden)\n        loss += criterion(output, target[c].cuda())\n    \n    loss.backward()\n    decoder_optimizer.step()\n    \n    return loss.data.item() \/ chunk_len","4faf9bd6":"import time, math\n\ndef time_since(since):\n    s = time.time()-since\n    m = math.floor(s\/60)\n    s -= m*60\n    return ('%dm %ds' %(m, s))","4acbfb12":"n_epochs = 200\nprint_every = 10\nplot_every = 10\nhidden_size = 100\nn_layers = 1\nlr = 0.01\n\ndecoder = customGRU(vocab_len, hidden_size, vocab_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr = lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif(train_on_gpu):\n    decoder.cuda()\nfor epoch in range(1, n_epochs + 1):\n    loss = train(inp,tar)       \n    loss_avg += loss\n\n    if epoch % print_every == 0:\n        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch \/ n_epochs * 50, loss))\n#         print(evaluate('ge', 200), '\\n')\n\n    if epoch % plot_every == 0:\n        all_losses.append(loss_avg \/ plot_every)\n        loss_avg = 0","0844d473":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline","e81e4c48":"plt.figure()\nplt.plot(all_losses)","acca7ef2":"def evaluate(prime_str = \"This process\", predict_len = 50, temperature = 0.8):\n    hidden = decoder.init_hidden().cuda()\n    \n    for p in range(predict_len):\n        \n        prime_input = torch.tensor([word_to_idx[w] for w in prime_str.split()], dtype = torch.long).cuda()\n        inp = prime_input[-2:] #last two words\n        output, hidden = decoder(inp, hidden)\n        \n        #sampling from the network as a multinomial distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        \n        #add predicted word to string and use as next_input\n        predicted_word = list(word_to_idx.keys())[list(word_to_idx.values()).index(top_i)]\n        prime_str += \" \" + predicted_word\n    return prime_str","8ce45b78":"print(evaluate('united states', 50, 1))","fe086a08":"> **GRU Model for Text Generation**","6a3d725f":"> Generating text","45f9ca46":">  Dataset Cleaning","d78e380b":"> **N-gram Language Modelling**","b06ec61b":"Here we will be using n = 3 for the language modelling that is given words wi-1 & wi-2, we want to compute P(wi | wi-1, wi-2)"}}