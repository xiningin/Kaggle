{"cell_type":{"55fb00a6":"code","b40d870e":"code","dc51a521":"code","f1e87b8c":"code","e4cab3e3":"code","8c737f5a":"code","f34f9674":"code","96e73b67":"code","5ab98189":"code","c32f1fb3":"code","b8511917":"code","c7ef7c1a":"code","43fa0687":"code","cf1ccd87":"code","2ed5d9f7":"markdown","6b07b485":"markdown","49a8a259":"markdown","95e405ef":"markdown","0e183246":"markdown","cee5e756":"markdown","d7c473c3":"markdown","0f6864ff":"markdown","501e34c9":"markdown","f6986d83":"markdown","f348f96d":"markdown","e1f78fbb":"markdown"},"source":{"55fb00a6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nimport requests, json, os\nprint(os.listdir(\"..\/input\"))\n\ndef load_data():\n    try:\n        #with open('tweet_master.json') as json_file:  \n        with open('..\/input\/twitter-data-tweets\/final_tweet_master.json') as json_file:  \n            tweet_json = json.load(json_file)\n        #with open('tweet_master.json') as json_file:  \n        with open('..\/input\/twitter-data-users\/final_user_master.json') as json_file:  \n            usr_json = json.load(json_file)\n\n    except Exception as e:\n        print(\"Loading Data from Azure - data files do not exist locally\")\n        print(\"Warning - these files were since removed by the host. Cheers!\")\n        '''\n        final_url = \"https:\/\/eumarharvardfiles.blob.core.windows.net\/cscis109\/final_tweets_master_withNLP.json\"\n        fina_content = requests.get(final_url).content\n\n        more_url = \"https:\/\/eumarharvardfiles.blob.core.windows.net\/cscis109\/more_final_tweets_master_withNLP.json\"\n        more_content = requests.get(more_url).content\n\n        tweet_json = json.load(io.StringIO(fina_content.decode('utf-8')))\n        tweet_json_more =json.load(io.StringIO(more_content.decode('utf-8')))\n        '''\n\n    return(pd.read_json(tweet_json), pd.read_json(usr_json))\n\n\ntweet_df, user_df = load_data()\nprint(tweet_df.shape, user_df.shape)","b40d870e":"user_df.set_index('screen_name',drop=False, inplace=True)\n\nbots = user_df[user_df['known_bot'] == True].copy()\nverifieds = user_df[user_df['verified'] == True].copy()\nunknowns = user_df[np.logical_and(user_df['known_bot'] != True, user_df['verified'] != True)].copy()\n\ntweets_bot_users = tweet_df.join(bots,'user_screen_name',rsuffix='user',how='right')\ntweets_verified_users = tweet_df.join(verifieds,'user_screen_name',rsuffix='user',how='right')\ntweets_unknown_users = tweet_df.join(unknowns,'user_screen_name',rsuffix='user',how='right')\n\ntweets_unknown_users[\"TypeOfUser\"] = \"Unknown\"\ntweets_bot_users[\"TypeOfUser\"] = \"Bot\"\ntweets_verified_users[\"TypeOfUser\"] = \"Verified\"\n\ntweets_unknown_users[\"TypeOfUser\"] = \"Unknown\"\ntweets_bot_users[\"TypeOfUser\"] = \"Bot\"\ntweets_verified_users[\"TypeOfUser\"] = \"Verified\"\n\nnew_tweet_df = pd.concat([tweets_unknown_users,tweets_bot_users, tweets_verified_users], ignore_index=True)","dc51a521":"# Prepare DF\nnew_tweet_df[\"nlp_key_phrases\"] = \"\"\nnew_tweet_df[\"nlp_count_key_phrases\"] = 0\nnew_tweet_df[\"nlp_sentiment_score\"] = 0.0\n\n# Select English\/UK twitter users\nmask = np.logical_or(new_tweet_df[\"lang\"] == \"en\", new_tweet_df[\"lang\"] == \"uk\")\n\nnew_tweet_df_english = new_tweet_df[mask].copy()\nnew_tweet_df_english.reset_index(drop=True, inplace=True)","f1e87b8c":"## THE CODE below uses Microsoft Azure APIs to detect sentiment and key phrases from each Tweet\n#https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/text-analytics\/quickstarts\/python#extract-key-phrases\ntext_analytics_base_url = \"https:\/\/eastus.api.cognitive.microsoft.com\/text\/analytics\/v2.0\/\"\nsentiment_api_url = text_analytics_base_url + \"sentiment\"\nkey_phrase_api_url = text_analytics_base_url + \"keyPhrases\"\n\nsubscription_key = \"SORRY - CAN'T GIVE THIS OUT! Get your own and stick it in here :)\"\n\nstart_pos = 0 \ntotal = len(new_tweet_df_english)\n'''\nwhile start_pos <= total:\n\n    try:\n        index1 = start_pos\n        index2 = start_pos + 1\n        index3 = start_pos + 2\n        index4 = start_pos + 3\n        index5 = start_pos + 4\n        index6 = start_pos + 5\n        index7 = start_pos + 6\n        index8 = start_pos + 7\n        index9 = start_pos + 8\n        index10 = start_pos + 9        \n\n        print(\"Started Processing: Range({0}, {1})\".format(index1, index10))\n            \n        documents = {'documents' : [\n          {'id': index1, 'language': 'en', 'text': new_tweet_df_english.loc[index1].text},\n          {'id': index2, 'language': 'en', 'text': new_tweet_df_english.loc[index2].text},\n          {'id': index3, 'language': 'en', 'text': new_tweet_df_english.loc[index3].text},\n          {'id': index4, 'language': 'en', 'text': new_tweet_df_english.loc[index4].text},\n          {'id': index5, 'language': 'en', 'text': new_tweet_df_english.loc[index5].text},\n          {'id': index6, 'language': 'en', 'text': new_tweet_df_english.loc[index6].text},\n          {'id': index7, 'language': 'en', 'text': new_tweet_df_english.loc[index7].text},\n          {'id': index8, 'language': 'en', 'text': new_tweet_df_english.loc[index8].text},\n          {'id': index9, 'language': 'en', 'text': new_tweet_df_english.loc[index9].text},\n          {'id': index10, 'language': 'en', 'text': new_tweet_df_english.loc[index10].text}\n        ]}\n\n\n        headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n        response_key_phrase  = requests.post(key_phrase_api_url, headers=headers, json=documents)\n        key_phrases = response_key_phrase.json()\n\n        response_sentiment  = requests.post(sentiment_api_url, headers=headers, json=documents)\n        sentiment = response_sentiment.json()\n\n\n        for document in key_phrases[\"documents\"]:\n            id_doc = int(document[\"id\"])\n            num_phrases = len(document[\"keyPhrases\"])\n            phrases_doc = \",\".join(document[\"keyPhrases\"])\n\n            new_tweet_df_english.loc[id_doc, \"nlp_key_phrases\"] = phrases_doc\n            new_tweet_df_english.loc[id_doc, \"nlp_count_key_phrases\"] = num_phrases    \n\n            #print(id_doc,new_tweet_df.loc[id_doc, \"nlp_key_phrases\"],  new_tweet_df.loc[id_doc, \"nlp_count_key_phrases\"])\n\n        for document in sentiment[\"documents\"]:\n            id_doc = int(document[\"id\"])\n            score = document[\"score\"]\n\n            new_tweet_df_english.loc[id_doc, \"nlp_sentiment_score\"] = float(score)\n\n            #print (id_doc, new_tweet_df.loc[id_doc, \"nlp_sentiment_score\"])\n\n        print(\"Finished Processing: Range({0}, {1})\".format(index1, index10))\n        \n        start_pos = start_pos + 10\n    except Exception as e:\n        print(f'SOME ERROR OCCURRED...PASSING!!!')\n        print(e)\n        start_pos = start_pos + 10\n        pass\n'''","e4cab3e3":"# Errors aren't pretty; commenting this out since above wasn't run.\n'''\nfilter_df = new_tweet_df_english[\"nlp_key_phrases\"] != \"\"\ntweets_with_nlp = new_tweet_df_english[filter_df]\nwith open('more_final_tweets_master_withNLP.json', 'w') as outfile:  \n    json.dump(new_tweet_json , outfile)\n'''","8c737f5a":"import nltk\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nimport string\n\nprint(\"Let's examine how many users are bots.\")\nprint(\"\\nRaw Counts:\")\nprint(new_tweet_df.groupby(\"TypeOfUser\").count()['known_bot'])\nprint(\"\\nPercents:\")\nusr_sum = sum(new_tweet_df.groupby(\"TypeOfUser\").count()['known_bot'])\nprint(new_tweet_df.groupby(\"TypeOfUser\").count()['known_bot']\/usr_sum*100)\n\nprint(\"\\nHmmm.\")","f34f9674":"tt = TweetTokenizer()\n\nprint(\"Original, non-retweeted count of bot tweets: {}.\".format(len(new_tweet_df)))\nnew_tweet_df.dropna(subset=['text'], inplace=True)\nprint(\"Count after removing empty tweets: {}.\\n\".format(len(new_tweet_df)))\n\n\nuseful_cols = ['followers_count','known_bot','text','tokens','name',\n               'is_tweet', 'tweet_length'] # Unavailable values: 'nlp_key_phrases','nlp_sentiment_score','nlp_count_key_phrases',\nnew_tweet_df['tokens'] = new_tweet_df['text'].apply(tt.tokenize)\n\nnew_tweet_df['tweet_length'] = new_tweet_df['tokens'].str.len()\n\nbot_texts = new_tweet_df.loc[new_tweet_df.known_bot == True][useful_cols]\nreal_texts= new_tweet_df.loc[new_tweet_df.known_bot == False][useful_cols]","96e73b67":"print(\"\\nLet's look at the distribution of tweet lengths by bot vs tweet lengths by real people. \\\nWe achieve this by tokenizing the tweet sentences with the NLTK package (natural language processing library).\\\nNext we group by name and find the mean tweet length by user name.\\n\")\ntweet_len_by_bot = bot_texts.groupby(['name']).tweet_length.mean().sort_values(ascending=False)\ntweet_len_by_usr = real_texts.groupby(['name']).tweet_length.mean().sort_values(ascending=False)\n\n\nfig, ax = plt.subplots(1,2, figsize= (16,8))\nax[0].hist([tweet_len_by_bot,tweet_len_by_usr], bins = int(max(tweet_len_by_bot)), \n        label=[\"Bot Tweets\",\"People Tweets\"], alpha=0.3)\nax[0].set_title(\"Histogram of Mean Tweet Word Length per User(Bot)\")\nax[0].set_ylabel(\"Tweets\")\nax[0].set_xlabel(\"Count\")\nax[0].legend()\n\nax[1].boxplot([bot_texts.groupby(['name']).tweet_length.mean(), real_texts.groupby(['name']).tweet_length.mean()], \n              labels = ['Bot', 'Person'])\nax[1].set_title(\"Barplot of Tweet Length by User\")\nax[1].set_ylabel(\"Counts\")\nax[1].set_xlabel(\"User Type\")\nax[1].legend()\n\nprint(\"The histogram isn't very illuminating; the barplot indicates real people's tweets have a considerably wider word length span, where 75% of tweets by non-bots\\\nrange from 6-30 words per tweet. Bots typically keep their conversations between 10-25 words in length. They both average to around 16-18 words.\") ","5ab98189":"bot_words = bot_texts.groupby(['name']).tokens.agg(sum)\nusr_words = real_texts.groupby(['name']).tokens.agg(sum)\nbot_words = pd.DataFrame(bot_words)\nusr_words = pd.DataFrame(usr_words)\n\nbot_words.columns = ['words']\nusr_words.columns = ['words']\n\nstop_words  = stopwords.words('english') + list(string.punctuation) + [' ','rt',\"\\'\", \"...\", \"..\",\"`\",'\\\"', '\u2013', '\u2019', \"I'm\", '\u2026','\"\"','\u201c','\u201d']\n\n# Construct list of cleaned words\nusr_words['cleaned_words'] = [[word for word in words if word.lower() not in stop_words] \n                                for words in usr_words['words']]\nbot_words['cleaned_words'] = [[word for word in words if word.lower() not in stop_words] \n                              for words in bot_words['words']]","c32f1fb3":"freq_per_usr = FreqDist(list([a for b in usr_words.cleaned_words.tolist() for a in b]))\nfreq_per_bot = FreqDist(list([a for b in bot_words.cleaned_words.tolist() for a in b]))\n\n# Most common words\ncommon_words_bot = pd.DataFrame(freq_per_bot.most_common())\ncommon_words_usr = pd.DataFrame(freq_per_usr.most_common())\n\ncols = [\"Words\", \"Count\"]\ncommon_words_bot.columns = cols\ncommon_words_usr.columns = cols\n\ncommon_words_usr['Frequency'] = common_words_usr['Count']\/len(common_words_usr)\ncommon_words_bot['Frequency'] = common_words_bot['Count']\/len(common_words_bot)","b8511917":"print(\"The following calculation was conducted on words greater than two letters. This removes silly 1-emoji \\\ntweets and the such.\\n\")\n\nfilter1 = (common_words_usr['Words'].str.len()>=3)\nfilter2 = (common_words_bot['Words'].str.len()>=3)\n\n\nfiltered_usr = common_words_usr.loc[filter1]\nfiltered_bot = common_words_bot.loc[filter2]\n\nprint(\"\\nThe top 15 words used in real tweets (out of {} unique words)::\\n\".format(len(filtered_usr)))\nprint(filtered_usr[:15])\nprint(\"\\nThe top 15 word used by all bots (out of {} unique words):\\n\".format(len(filtered_bot)))\nprint(filtered_bot[:15])","c7ef7c1a":"naughty_words = filtered_bot[:10]\n\n# Set these to 0\nfor word in naughty_words['Words']:\n    new_tweet_df[word] = 0 # Set to 0\n    new_tweet_df[word] = new_tweet_df.apply(lambda row: row['tokens'].count(word), axis=1) # Fill if counted\n    \ntext_by_names = new_tweet_df.groupby(['name']).sum()[naughty_words['Words']]\nto_join = new_tweet_df[['name','known_bot']].drop_duplicates().set_index('name')\ntext_by_names=text_by_names.join(to_join, how='inner').drop_duplicates()\nprint(text_by_names.head())","43fa0687":"bot_texts2 = text_by_names.loc[text_by_names.known_bot == True].join(tweet_len_by_bot, how='inner')\nusr_texts2= text_by_names.loc[text_by_names.known_bot == False].join(tweet_len_by_usr, how='inner')\n\ntweet_len_by_bot = pd.DataFrame(tweet_len_by_bot)\ntweet_len_by_usr = pd.DataFrame(tweet_len_by_usr)\n\ntweet_len_by_bot.columns = ['mean_tweet_length']\ntweet_len_by_usr.columns = ['mean_tweet_length']\n\n\nfig, ax = plt.subplots(2,5, figsize= (20,8))\nax = ax.ravel()\n\nfor i, word in enumerate(naughty_words['Words']):\n    ax[i].hist([bot_texts2[word],\n                usr_texts2[word]], \n               label=[\"Bot\",\"Real\"])\n    ax[i].set_title(word)\n    ax[i].set_ylabel('# of Times Word Used')\n    ax[i].set_xlabel(\"# of Accounts\")\n\nfig.legend()\nplt.tight_layout()","cf1ccd87":"usr_texts2 = usr_texts2.join(tweet_len_by_usr)\nbot_texts2 = bot_texts2.join(tweet_len_by_bot)\n\nfor word in naughty_words['Words']:\n    usr_texts2[word+\"_freq\"] = usr_texts2[word]\/usr_texts2['mean_tweet_length']\n    bot_texts2[word+\"_freq\"] = bot_texts2[word]\/bot_texts2['mean_tweet_length']\n\n\nfig, ax = plt.subplots(2,5, figsize= (20,8))\nax = ax.ravel()\n\nfor i, word in enumerate(naughty_words['Words']):\n    ax[i].boxplot([bot_texts2[word+\"_freq\"],\n                usr_texts2[word+\"_freq\"]], labels = ['Bot', 'Person'])\n    ax[i].set_title(word)\n    ax[i].set_ylabel('Count')\n    ax[i].set_xlabel(\"Accounts\")\nplt.tight_layout()\n","2ed5d9f7":"## Problem: \n\nWe have a 1:4 ratio of bots\/non-bots in our data. The good news is that we have excellent granularity amongst bot posts. The bad news is we can \"develop\" a classification algorithm to detect bot which prints \"Yep - definitely a bot\" and it'll have a minimum error rate of 81.6% on our data (if we declare all unkowns as verified!). To overcome this problem, we'll have to use stratified sampling in our training, but this comes much later.","6b07b485":"## Load Data and Packages\n\nThe helper exception doesn't work anymore; the host (partner in this project) has since removed it. The files are available in this journal!","49a8a259":"## General Remarks\n\nIt appears bots were over 10x more likely to be mentioning Trump - and making any references to politics. For instance, real people aren't talking about clinton, obama, and throwing http links.\n\nThe plan now is to score users based on the top 10 words from the known bots. We'll assign one point to the user in the bot-word column every time the bot-word is used. \n1. First we'll declare the column names and set them to 0 as default. This is sensibly valid; it should stay 0 if these topics aren't mentioned.\n2.  Aggregate all of the tweets with a groupby statement, then count the total number of instances of these words in each tokenenized tweet collection.\n3. Make sure the tweet names are unique and the user-status remain. (verification check)","95e405ef":"### Feature Engineering with Azure!\n\nNotes:\n1. Feature engineering will only be performed on Tweets that have valid users, where the join with user table has succeeded.\n2. NLP will be conducted in English only.\n3. The code below uses Microsoft Azure NLP APIs to extract sentiment and key phrases from each Tweet. View more at https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/text-analytics\/quickstarts\/python#extract-key-phrases\n\n## **SKIP THIS SECTION!** Run this locally with your own key if you are inclined - the 3rd notebook provided contains this data.\n\nUncomment the big comment chunk in the second code block below to run the Azure NLP.","0e183246":"## The code should've worked if your subscription key is valid. \n\nCleaning and saving the progress. **Skip this section if you didn't do any NLP with Azure!**","cee5e756":"Above we can see that there's more variability in real people's sentences; this means people are having short and long conversations with huge variability in the length of their tweets. Bots however are having much more consice conversations of which 75% are 16-20 word length. Real people have 13-23 words in their tweets,  on average.\n\nThis is an excellent place to begin classification but it's only one parameter. Let's construct a much richer series of parameters on which to perform classification based on vocabulary. \nNext we shall investigate all of their tweets as one big bundle, and look at the vocabulary choices across several tweets. To begin, \n1. We shall aggregate by name and add the tokenized tweets to one big list. \n2. Remove stop words.","d7c473c3":"### Clean and polish the DF before passing it through Azure\n\nThe goal here is to clean the dataframe and label the groups. ","0f6864ff":"We're observing that our bots have a very narrow vocabulary range per tweet, compared to our real people counterparts. The real people mention these key words much less frequently than the bots. This looks like a good place to start applying classification methodologies. Later we'll begin by using PCA, and chasing it up with logistic regression after scaling the botty words with sklearn.\n\n## That's all for now!\n\n### The third notebook containing NLP data shall be uploaded in the upcoming days.","501e34c9":"# Twitter Bot Classification Project (Cont.)\n\n\n## Feature Engineering \n\nIn this notebook I outline the steps we took for our feature engineering. This project uses an API from Microsoft\u2019s AI Text Analytics service on Azure\u2019s cloud computing platform (https:\/\/azure.microsoft.com\/en-us\/services\/cognitive-services\/text-analytics\/). This feature is not readily accessible to the casual data scientist; to overcome this hurdle the dataset enclosed in the modeling notebook comes pre-equipped with the features we aim to engineer. This will be highlighted in the journal. \n\n\n### Project Overview\n\nThis project spans three notebooks. This notebook is the second of three.\n\n**1. Notebook 1: Preparation** [Found Here](https:\/\/www.kaggle.com\/housemusic\/twitter-bot-detection-data-collection)\n1.  Data Collection\n2. Storage\n3. EDA\n\n**2. Notebook 2: Feature Engineering** [This one]\n4. NLP with Microsoft Azure\n5. NLP with NLTK\n6. EDA\n\n**3. Notebook 3: Modeling**\n7. Modeling\n8. Results\/Conclusion\n\n\n### Notes\n\n***1***. If you're running this file locally, following the data collection notebook, continue to use those files. \n\n***2***. Again, another data sheet is attached to the 3rd notebook (modeling) containing the Azure NLP data. Don't worry about being able to process the nlp!","f6986d83":"### Hmm...\n\nThe figures above aim to highlight the total number of times any account would mention any of these known botty words. \nWhat we relly want to know are the frequencies on a scale relative to their respective to the sample sizes of the bot-non bot groups, to account for the difference in total sample count across the two groups.\n\nThis figure is difficult to read; most of the posters never mention these key words. Let's consider another approach!\n\nLet's look at their decision to use these words as a ratio to all of the words they typically use. In other words, we shall take a ratio of the key-word count (ie Trump) to mean tweet length for these particular users. We are expecting bots to use these words way more frequently than the average person.","f348f96d":"## **Resume here!**\n\n# Feature Engineering with NLTK\n\nThere are a couple of key text-features I found potentially important: \n\n1. Average Tweet Length.\n2. Top words used by known bot account.\n3. Number of unique words used.\n\n","e1f78fbb":"### Note for production:\nAfter cleaning stop words, get rid of the uncleaned vectors. This reduces space complexity by 50%. \n\nNext we'll get the counts and the frequencies of the words used. We'll also pull out the most used words by both groups to investigate what is going on in their respective tweets."}}