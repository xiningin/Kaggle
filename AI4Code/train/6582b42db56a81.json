{"cell_type":{"1b3fb6b3":"code","b73836d0":"code","561bf26f":"code","2a7eee3c":"code","3131c564":"code","51c37b89":"code","cd016167":"code","b5272c23":"code","1a46185f":"code","4b96763e":"code","ad8891dd":"code","f6029f63":"code","7b010ae3":"code","f796dd33":"code","67360b0c":"code","7dde7cef":"code","d5669f52":"code","5d2dea26":"code","b7d6d20c":"code","1cd582d9":"markdown","4dfe3e7d":"markdown","4e2796c8":"markdown","89265ba5":"markdown","642651a7":"markdown","cf3dccc6":"markdown","ec75a4a8":"markdown","55c40c41":"markdown","33ed6e29":"markdown","65e7179f":"markdown","3c821e21":"markdown","d8b4d7d0":"markdown","e8d02245":"markdown","627c3772":"markdown","9a51ff7b":"markdown","ca284db3":"markdown","80a24436":"markdown","c4366a77":"markdown"},"source":{"1b3fb6b3":"import numpy as np # We'll be storing our data as numpy arrays\nimport os # For handling directories\nfrom PIL import Image # For handling the images\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg # Plotting\nfrom matplotlib.pyplot import imshow","b73836d0":"sorted(os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/'))","561bf26f":"sorted(os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/00\/'))","2a7eee3c":"print(len(os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/00\/01_palm')))\nsorted(os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/00\/01_palm'))[:10]","3131c564":"# show the first sample image\nsample_img_1 = Image.open('..\/input\/leapgestrecog\/leapGestRecog\/00\/01_palm\/frame_00_01_0001.png')\nsample_img_1","51c37b89":"# show original image size\nsample_img_2 = Image.open('..\/input\/leapgestrecog\/leapGestRecog\/00\/01_palm\/frame_00_01_0002.png')\nprint('original image size:', sample_img_2.size)\n# resize size of a image\nsample_img_2 = sample_img_2.resize((320, 120))\nprint('resized image size:', sample_img_2.size)","cd016167":"lookup = dict()\nreverselookup = dict()\ncount = 0\nfor j in os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/00\/'):\n    if not j.startswith('.'): # If running this code locally, this is to \n                              # ensure you aren't reading in hidden folders\n        lookup[j] = count\n        reverselookup[count] = j\n        count = count + 1\nlookup","b5272c23":"x_data = []\ny_data = []\ndatacount = 0 # We'll use this to tally how many images are in our dataset\nfor i in range(0, 10): # Loop over the ten top-level folders\n    for j in os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/0' + str(i) + '\/'):\n        if not j.startswith('.'): # Again avoid hidden folders\n            count = 0 # To tally images of a given gesture\n            for k in os.listdir('..\/input\/leapgestrecog\/leapGestRecog\/0' + \n                                str(i) + '\/' + j + '\/'):\n                                # Loop over the images\n                img = Image.open('..\/input\/leapgestrecog\/leapGestRecog\/0' + \n                                 str(i) + '\/' + j + '\/' + k).convert('L')\n                                # Read in and convert to greyscale\n                img = img.resize((320, 120))\n                arr = np.array(img)\n                x_data.append(arr) \n                count = count + 1\n            y_values = np.full((count, 1), lookup[j]) \n            y_data.append(y_values)\n            datacount = datacount + count\nx_data = np.array(x_data, dtype = 'float32')\ny_data = np.array(y_data)\ny_data = y_data.reshape(datacount, 1) # Reshape to be the correct size","1a46185f":"from random import randint\nfor i in range(0, 10):\n    plt.imshow(x_data[i*200, :, :])\n    plt.title(reverselookup[y_data[i*200 ,0]])\n    plt.show()","4b96763e":"y_data.shape","ad8891dd":"y_data[199:210]","f6029f63":"import keras\nfrom keras.utils import to_categorical\ny_data = to_categorical(y_data)","7b010ae3":"y_data.shape","f796dd33":"# x_data = x_data.reshape((-1, 120, 320, 1))\nx_data = x_data.reshape((datacount, 120, 320, 1))\nx_data \/= 255","67360b0c":"from sklearn.model_selection import train_test_split\nx_train,x_further,y_train,y_further = train_test_split(x_data,y_data,test_size = 0.2)\nx_validate,x_test,y_validate,y_test = train_test_split(x_further,y_further,test_size = 0.5)","7dde7cef":"from keras import layers\nfrom keras import models","d5669f52":"model=models.Sequential()\nmodel.add(layers.Conv2D(32, (5, 5), strides=(2, 2), activation='relu', input_shape=(120, 320,1))) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu')) \nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))","5d2dea26":"model.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_validate, y_validate))","b7d6d20c":"[loss, acc] = model.evaluate(x_test,y_test,verbose=1)\nprint(\"Accuracy:\" + str(acc))","1cd582d9":"Now, y_data is categorized to (sample_count, class_num).","4dfe3e7d":"Since our images are big (we chose not to do any cropping) and the classification problem looks quite easy, we're going to downsample fairly aggressively, beginning with a 5 x 5 filter with a stride of 2. Note we have to specify the correct input shape at this initial layer, and keras will figure it out from then on. We won't worry about padding since it's clear that all the useful features are well inside the image. We'll continue with a sequence of convolutional layers followed by max-pooling until we arrive at a small enough image that we can add a fully-connected layer. Since we need to classify between 10 possibilities, we finish with a softmax layer with 10 neurons. ","4e2796c8":"At this point we would typically graph the accuracy of our model on the validation set, and choose a suitable number of epochs to train for to avoid overfitting. We might also consider introducing dropout and regularization. However, we can see we're getting perfect accuracy on the validation set after just one or two epochs, so we're pretty much done. Let's quickly confirm that this is carrying through to the test set:","89265ba5":"Each folder has 10 subfolders, which represent 10 calsses of the hand gestures.","642651a7":"Finally, we fit the model.","cf3dccc6":"Now, let's show the first image of calss \"01_palm\".","ec75a4a8":"You'll get slightly different numbers each time you run it but you should be getting between 99.9 and 100% accuracy. Great!","55c40c41":"Now it's time to build our network. We'll use keras.","33ed6e29":"As described in the Data Overview, there are 10 folders labelled 00 to 09, each containing images from a given subject. In each folder there are subfolders for each gesture. We'll build a dictionary `lookup` storing the names of the gestures we need to identify, and giving each gesture a numerical identifier. We'll also build a dictionary `reverselookup` that tells us what gesture is associated to a given identifier.","65e7179f":"We need a cross-validation set and a test set, and we'll use the `sklearn` package to construct these. In order to get an 80-10-10 split, we call `train_test_split` twice, first to split 80-20, then to split the smaller chunk 50-50. Note that we do this after the rescaling step above, to ensure that our train and test sets are coming from the same distribution.","3c821e21":"# Intro\nThe Hand Gesture Recognition Database is a collection of near-infra-red images of ten distinct hand gestures. In this notebook we use end-to-end deep learning to build a classifier for these images.\n\nThis kernal is forked from: https:\/\/www.kaggle.com\/lamine16\/hand-gesture-recognition-database-with-cnn\n\nWe'll first load some packages required for reading in and plotting the images. ","d8b4d7d0":"Let's find the file structure of the dataset.\nFist, the full dataset is divided into 10 folders.","e8d02245":"Next we read in the images, storing them in `x_data`. We store the numerical classifier for each image in `y_data`. Since the images are quite large and are coming from an infra-red sensor, there's nothing really lost in converting them to greyscale and resizing to speed up the computations.","627c3772":"Let's take a look at some of the pictures. Since each of the subfolders in `00` contained 200 images, we'll use the following piece of code to load one image of each gesture.","9a51ff7b":"In each \"Class Folder\", there are 200 samples that has the same label.","ca284db3":"The first thing to note is that this is not a difficult classification problem. The gestures are quite distinct, the images are clear, and there's no background whatsoever to worry about. If you weren't comfortable with deep learning, you could do quite well with some straight-forward feature detection -- for example the '07_ok' class could easily be detected with binary thresholding followed by circle detection. \n\nMoreover, the gestures consistently occupy only about 25% of the image, and all would fit snugly inside a square bounding box. Again if you're looking to do basic feature detection, an easy first step would be to write a short script cropping everything to the relevant 120 x 120 square. \n\nBut the point of this notebook is to show how effective it is to just throw a neural network at a problem like this without having to worry about any of the above, so that's what we're going to do. \n\nAt the moment our vector `y_data` has shape `(datacount, 1)`, with `y_data[i,0] = j` if the `i`th image in our dataset is of gesture `reverselookup[j]`. In order to convert it to one-hot format, we use the keras function to_categorical:","80a24436":"Then, we can get the original image size. And try to resize the shape to half of its original size.","c4366a77":"Our set of images has shape `(datacount, 120, 320)`. Keras will be expecting another slot to tell it the number of channels, so we reshape `x_data` accordingly. We also rescale the values in `x_data` to lie between 0 and 1."}}