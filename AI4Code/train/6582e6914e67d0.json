{"cell_type":{"4e589632":"code","14033cb2":"code","929f0723":"code","ce0c344a":"code","e695391a":"code","2c5ff691":"code","5a9a5eac":"code","85021d67":"code","82060ea7":"code","4f7bbb30":"code","91ae59ed":"code","26cdf2a2":"code","4ea7eb26":"code","5a0f5bba":"code","253c1945":"code","f10b03a7":"code","1b0ba06a":"code","b5865c37":"code","cb174fdc":"code","cec8c922":"code","79b7145b":"code","86157639":"code","52d7bde0":"code","82dfc8ed":"code","68b131ae":"code","6173743b":"markdown","917eafd3":"markdown","f0993e70":"markdown","0aaa4a30":"markdown","6a781360":"markdown","1b17ca68":"markdown","4c6cf79c":"markdown","5bd8e626":"markdown","39fa641f":"markdown","6f4596be":"markdown","a2140001":"markdown"},"source":{"4e589632":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom glob import glob\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom joblib import Parallel, delayed\nimport time","14033cb2":"start_time = time.time()\nis_debug = True\nshow_feat_imp = True\ndevice='cpu'\n    \ninput_dir = '..\/input\/optiver-realized-volatility-prediction\/'\nagg_feat_list = ['trade_price_mean',\n                 'trade_seconds_in_bucket_count_unique']","929f0723":"def down_cast(df):\n    f_cols = df.select_dtypes('float').columns\n    i_cols = df.select_dtypes('int').columns\n    df[f_cols] = df[f_cols].apply(pd.to_numeric, downcast='float')\n    df[i_cols] = df[i_cols].apply(pd.to_numeric, downcast='integer')\n    \n    return df","ce0c344a":"# for book data\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef bid_size_sum(df):\n    return df['bid_size1'] + df['bid_size2']\n    \ndef ask_size_sum(df):\n    return df['ask_size1'] + df['ask_size2']\n\ndef diff_bid_ask_price(df):\n    return df['bid_price1'] + df['bid_price2'] - df['ask_price1'] - df['ask_price2']\n\ndef diff_bid_ask_size(df):\n    return df['bid_size1'] + df['bid_size2'] - df['ask_size1'] - df['ask_size2']\n\ndef diff_bid_price(df):\n    return df['bid_price1'] - df['bid_price2']\n\ndef diff_ask_price(df):\n    return df['ask_price1'] - df['ask_price2']\n\ndef diff_bid_size(df):\n    return df['bid_size1'] - df['bid_size2']\n\ndef diff_ask_size(df):\n    return df['ask_size1'] - df['ask_size2']\n\n# for trade data\ndef amount(df):\n    return df['price'] * df['size']\n\ndef count_unique(series):\n    return len(np.unique(series))\n\n# common \ndef log_return(series):\n    return np.log(series).diff()\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(log_return(series)**2))\n\n\ndef get_stats_window(df, fe_dict, seconds_in_bucket, add_suffix=False):\n    # Group by the window\n    df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n    # Rename columns joining suffix\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n    # Add a suffix to differentiate windows\n    if add_suffix:\n        df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n    return df_feature\n\ndef agg_feat_groupby_stock_id(df, agg_feat_list):\n    df_feature = df.groupby(['stock_id'])[agg_feat_list].agg(['mean', 'std']).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.add_suffix('_' + 'stock')\n    return df_feature.rename(columns={'stock_id__stock': 'stock_id'})\n\ndef agg_feat_groupby_time_id(df, agg_feat_list):\n    df_feature = df.groupby(['time_id'])[agg_feat_list].agg(['mean',  'std']).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    df_feature = df_feature.add_suffix('_' + 'time')\n    return df_feature.rename(columns={'time_id__time': 'time_id'})\n\ndef calc_tau(df):\n    df['size_tau'] = np.sqrt(1 \/ df['trade_seconds_in_bucket_count_unique'])\n    df['size_tau_400'] = np.sqrt(1 \/ df['trade_seconds_in_bucket_count_unique_400'])\n    df['size_tau_200'] = np.sqrt(1 \/ df['trade_seconds_in_bucket_count_unique_200'])\n\n    # delta tau\n    df['size_tau_d'] = df['size_tau_400'] - df['size_tau']\n\n    df['size_tau2'] = np.sqrt(1 \/ df['trade_order_count_sum'])\n    df['size_tau2_400'] = np.sqrt(0.33 \/ df['trade_order_count_sum'])\n    df['size_tau2_200'] = np.sqrt(0.66 \/ df['trade_order_count_sum'])\n\n    # delta tau\n    df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n    \n    return df","e695391a":"def preprocess_book(file_path):\n    df = pd.read_parquet(file_path)\n    # make new columns\n    df['wap1'] = calc_wap1(df)\n    df['bid_size_sum'] = bid_size_sum(df)\n    df['ask_size_sum'] = ask_size_sum(df)\n    df['diff_bid_ask_price'] = diff_bid_ask_price(df)\n    df['diff_bid_ask_size'] = diff_bid_ask_size(df)\n    df['diff_bid_price'] = diff_bid_price(df)\n    df['diff_ask_price'] = diff_ask_price(df)\n    df['diff_bid_size'] = diff_bid_size(df)\n    df['diff_ask_size'] = diff_ask_size(df)\n    \n    # log returns\n    lr_cols = ['wap1',\n               'bid_price1',\n               'ask_price2',\n               'bid_size1',\n               'ask_size2',\n               'bid_size_sum',\n               'ask_size_sum',\n               'diff_bid_ask_price',\n               'diff_bid_price',\n               'diff_ask_price'\n              ]\n    for col in lr_cols:\n        df[col + '_lr'] = df.groupby(['time_id'])[col].apply(log_return)\n    \n    create_feature_dict = {\n        'wap1': [realized_volatility],\n        'wap1_lr': [np.std, np.max, np.min],\n        'bid_price1': [np.std],\n        'bid_size1': [np.mean],\n        'bid_price2': [np.std],\n        'bid_size2': [np.mean],\n        'ask_price1': [np.std],\n        'ask_price2': [np.std],\n        'diff_bid_ask_price': [realized_volatility, np.mean, np.max, np.min],\n        'diff_bid_price': [np.mean, np.max, np.min],\n        'diff_ask_price': [np.max, np.min],\n        'bid_price1_lr': [np.std],\n        'ask_price2_lr': [np.std],\n        'bid_size_sum_lr': [np.mean],\n        'ask_size_sum_lr': [np.mean],\n        'diff_bid_price_lr': [np.mean, np.std, np.max, np.min],\n    }\n    # Get the stats for different windows\n    df_feature = get_stats_window(df, create_feature_dict,seconds_in_bucket=0, add_suffix=False)\n    df_feature_200 = get_stats_window(df, create_feature_dict, seconds_in_bucket=200, add_suffix=True)\n    df_feature_400 = get_stats_window(df, create_feature_dict, seconds_in_bucket=400, add_suffix=True)\n    \n    df_feature = df_feature.merge(df_feature_200, how='left', left_on='time_id', right_on='time_id_200')\n    df_feature = df_feature.merge(df_feature_400, how='left', left_on='time_id', right_on='time_id_400')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id_200'], axis=1, inplace=True)\n    df_feature.drop(['time_id_400'], axis=1, inplace=True)\n    \n    # row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id'], axis = 1, inplace = True)\n    \n    return df_feature","2c5ff691":"def preprocess_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['amount'] = amount(df)\n    \n    lr_cols = ['price',\n               'amount',\n               'order_count'\n              ]\n    for col in lr_cols:\n        df[col + '_lr'] = df.groupby(['time_id'])[col].apply(log_return)\n    \n    create_feature_dict = {\n        'price': [realized_volatility, np.mean, np.std],\n        'price_lr': [np.max, np.min, np.std],\n        'seconds_in_bucket': [count_unique],\n        'amount_lr': [np.mean, np.std],\n        'order_count': [np.sum]\n    }\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(df, create_feature_dict,seconds_in_bucket=0, add_suffix=False)\n    df_feature_200 = get_stats_window(df, create_feature_dict, seconds_in_bucket=200, add_suffix=True)\n    df_feature_400 = get_stats_window(df, create_feature_dict, seconds_in_bucket=400, add_suffix=True)\n    \n    df_feature = df_feature.merge(df_feature_200, how='left', left_on='time_id', right_on='time_id_200')\n    df_feature = df_feature.merge(df_feature_400, how='left', left_on='time_id', right_on='time_id_400')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id_200'], axis=1, inplace=True)\n    df_feature.drop(['time_id_400'], axis=1, inplace=True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id'], axis=1, inplace=True)\n    \n    return df_feature","5a9a5eac":"# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train=True, is_parallel=True):\n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = input_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = input_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = input_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = input_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n        \n        # Return the merged dataframe\n        return df_tmp\n\n    if is_parallel:\n        # Use parallel api to call paralle for loop\n        df = Parallel(n_jobs=-1, verbose=1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n        # Concatenate all the dataframes that return from Parallel\n        df = pd.concat(df, ignore_index = True)\n    else:\n        for stock_id in list_stock_ids:\n            if is_train:\n                file_path_book = input_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = input_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n            else:\n                file_path_book = input_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = input_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n        df = pd.merge(preprocess_book(file_path_book), preprocess_trade(file_path_trade), on='row_id', how='left')\n    df['stock_id'] = df['row_id'].apply(lambda x: int(str(x).split('-')[0]))\n    df['time_id'] = df['row_id'].apply(lambda x: int(str(x).split('-')[1]))\n\n    return df.drop('row_id', axis=1)","85021d67":"def extract_list_stock_ids(book_file_exp, trade_file_exp):\n    book_file_list = glob(book_file_exp)\n    trade_file_list = glob(trade_file_exp)\n    book_stock_ids = {int(file_path.split('stock_id=')[1]) for file_path in book_file_list}\n    trade_stock_ids = {int(file_path.split('stock_id=')[1]) for file_path in trade_file_list}\n\n    return list(book_stock_ids and trade_stock_ids)","82060ea7":"book_train_file_exp = input_dir + 'book_train.parquet\/stock_id=*'\ntrade_train_file_exp = input_dir + 'trade_train.parquet\/stock_id=*'\n\nlist_train_stock_ids = extract_list_stock_ids(book_train_file_exp, trade_train_file_exp)","4f7bbb30":"if is_debug:\n    list_train_stock_ids = [0]\ndf_train_org = pd.read_csv(input_dir + 'train.csv') \ndf_train_feature = preprocessor(list_train_stock_ids, is_train=True)\ndf_train = df_train_org.merge(df_train_feature, on=['stock_id', 'time_id'], how='left')\n\n# aggregate features group by stock_id\ndf_stock_id_feat_train = agg_feat_groupby_stock_id(df_train, agg_feat_list)\ndf_train = df_train.merge(df_stock_id_feat_train, on='stock_id', how='left')\n\n# aggregate features group by time_id\ndf_time_id_feat_train = agg_feat_groupby_time_id(df_train, agg_feat_list)\ndf_train = df_train.merge(df_time_id_feat_train, on='time_id', how='left')","91ae59ed":"df_train = calc_tau(df_train)","26cdf2a2":"# find too much corr combinations of feature\nif is_debug:\n    df_corr = df_train.drop(['stock_id', 'time_id', 'target'], axis=1).corr()\n    too_match_corr_list = []\n    threshold = 0.98\n    for col in df_corr:\n        for idx in df_corr.index:\n            if (df_corr.loc[idx, col] > threshold) and (col != idx) and ((idx, col) not in too_match_corr_list):\n                too_match_corr_list.append((col, idx))\n    print(too_match_corr_list)","4ea7eb26":"# Train data\ncb_encoder = CatBoostEncoder(cols=['stock_id'])\ncb_encoder.fit(df_train.drop('target', axis=1), df_train['target'])\ndf_train['stock_id_cbenc'] = cb_encoder.transform(df_train.drop('target', axis=1))['stock_id']\ndf_train = down_cast(df_train)","5a0f5bba":"from sklearn.model_selection import KFold\nimport lightgbm as lgbm\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","253c1945":"if is_debug:\n    d_tmp = (df_train.describe() > 1000000).any().reset_index()\n    d_tmp.columns = ['colname', 'too_large']\n    for i, row in d_tmp.iterrows():\n        if row['too_large']:\n            print(row['colname'])","f10b03a7":"# train\nscaler_dict = {}\nX = df_train.drop(['stock_id', 'time_id', 'target'], axis=1)\nX.fillna(0)\nfor col in X.columns:\n    if col != 'stock_id':\n        scaler = StandardScaler()\n        x = X[col].values.reshape(-1, 1)\n        scaler.fit(x)\n        X[col] = scaler.transform(x)\n        scaler_dict[col] = scaler\ny = df_train['target']","1b0ba06a":"# loss functions\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds), 5), False","b5865c37":"# feature importance\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\n\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(16, 32)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()\n    \n    \ndef calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    return mean_df","cb174fdc":"# lgbm parms\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 50,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'feature_fraction_bynode': 0.8,\n      'device': device\n  }\n# list of models\nmodels = []\n# validation score\nscores = 0.0\ngain_importance_list = []\nsplit_importance_list = []","cec8c922":"n_splits = 5\nif is_debug:\n    num_boost_round = 100\nelse:\n    num_boost_round = 5000\n    \nkf = KFold(n_splits=n_splits, random_state=20210101, shuffle=True)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(f\"Fold : {fold + 1}\")\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    # RMSPE weight\n    weights = 1 \/ np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train, y_train, weight=weights)\n\n    weights = 1 \/ np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid, y_valid, reference=lgbm_train, weight=weights)\n    \n    # train model\n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=num_boost_round,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100             \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true=y_valid, y_pred=y_pred), 3)\n    print(f'RMSPE: {RMSPE}')\n\n    scores += RMSPE \/ n_splits\n    models.append(model)\n    print(\"*\" * 100)\n    if show_feat_imp:    \n        feature_names = X_train.columns.values.tolist()\n        gain_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='gain')\n        gain_importance_list.append(gain_importance_df)","79b7145b":"if show_feat_imp:\n    mean_gain_df = calc_mean_importance(gain_importance_list)\n    plot_importance(mean_gain_df, title='Model feature importance by gain')\n    mean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\n    mean_gain_df.to_csv('gain_importance_mean.csv', index=False)","86157639":"book_test_file_exp  = input_dir + 'book_test.parquet\/stock_id=*'\ntrade_test_file_exp = input_dir + 'trade_test.parquet\/stock_id=*'\nlist_test_stock_ids = extract_list_stock_ids(book_test_file_exp, trade_test_file_exp)","52d7bde0":"df_test_org = pd.read_csv(input_dir + 'test.csv') \ndf_test_feature = preprocessor(list_test_stock_ids, is_train=False)\n\ndf_test = df_test_org.merge(df_test_feature, on=['stock_id', 'time_id'], how='left')\n\n# aggregate groupby stock_id\ndf_stock_id_feat_test = agg_feat_groupby_stock_id(df_test, agg_feat_list)\ndf_test = df_test.merge(df_stock_id_feat_test, on='stock_id', how='left')\n\n# aggregate groupby time_id\ndf_time_id_feat_test = agg_feat_groupby_time_id(df_test, agg_feat_list)\ndf_test = df_test.merge(df_time_id_feat_test, on='time_id', how='left')\n\ndf_test = calc_tau(df_test)","82dfc8ed":"df_test = df_test.drop('row_id', axis=1)\n# target encoding\ndf_test['stock_id_cbenc'] = cb_encoder.transform(df_test)['stock_id']\n\n# standard scaler\nX_test = df_test.drop(['stock_id', 'time_id'], axis=1)\nfor col in X_test.columns:\n    if col != 'stock_id':\n        x = X_test[col].values.reshape(-1, 1)\n        X_test[col] = scaler_dict[col].transform(x)\n\ndf_test = down_cast(df_test)","68b131ae":"# prediction with light gbm models\ntarget = np.zeros(len(X_test))\nfor model in models:\n    pred = model.predict(X_test[X.columns], num_iteration=model.best_iteration)\n    target += pred \/ len(models)\n    \ndf_test['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\ndf_submission = df_test[['row_id']].assign(target=target)\ndf_submission.to_csv('submission.csv', index=False)\nend_time = time.time()\nprint(f'total time: {round(end_time - start_time)} seconds')","6173743b":"## Preprocess of Book Data","917eafd3":"# Target Encoding","f0993e70":"# Standard Scaler","0aaa4a30":"# Test Set","6a781360":"# Prediction","1b17ca68":"## Feature Engineering Functions","4c6cf79c":"# Preprocess","5bd8e626":"## Preprocess of All","39fa641f":"# References\n+ https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\n+ https:\/\/www.kaggle.com\/monolith0456\/2xlgbm-fnn-ensemble","6f4596be":"# Train Model","a2140001":"## Preprocess of Trade Data"}}