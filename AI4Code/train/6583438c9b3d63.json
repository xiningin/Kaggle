{"cell_type":{"e80ea04e":"code","21fc20c0":"code","802058d9":"code","114d0923":"code","f750cd65":"code","3dde48a0":"code","1d079313":"code","6526d7d9":"code","4d623967":"code","1323619e":"code","438cc082":"code","9aa763c4":"code","1d619cf0":"code","18bf2fe5":"code","ee1cd59f":"code","fed9dc01":"code","070daa37":"code","ff53801a":"code","1c490da8":"code","c3ef1656":"code","ee7cf1bb":"code","2e4ac566":"code","a53a201a":"code","a96c8400":"code","9652097a":"code","26611ac2":"code","8ef49e85":"code","bf17f06e":"code","592f9717":"code","2fc31062":"code","ab5db30a":"code","0ba1ac4e":"code","086e53db":"code","5e715ec5":"code","1b932523":"code","ccfaa307":"code","900181f6":"code","2b7f379e":"code","21238ee0":"code","f44de89f":"code","3a1c6e0f":"code","7ca841bb":"code","197fbe69":"code","be5676bb":"code","10306715":"code","e7765161":"code","88320dc4":"code","098e7872":"code","700c1a5c":"markdown","7f3153ed":"markdown","274031cd":"markdown","d4ae6966":"markdown","14b48386":"markdown","957b785b":"markdown","4f2248b5":"markdown","a3d6729e":"markdown","2292d308":"markdown","719b5039":"markdown"},"source":{"e80ea04e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas_profiling import ProfileReport\nimport plotly.express as px\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","21fc20c0":"\n# import joypy\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns; sns.set()\n%matplotlib inline\n\nfrom sklearn.datasets import load_breast_cancer\n\n# garbage\nimport gc; gc.enable()\n\n# warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# modeling\nfrom sklearn.naive_bayes import GaussianNB\n# from sklego.mixture import GMMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV\n# from sklego.meta import Thresholder\nfrom sklearn.pipeline import make_pipeline","802058d9":"instruments = pd.read_json('\/kaggle\/input\/amazon-music-reviews\/Musical_Instruments_5.json', lines = True)","114d0923":"reviews = pd.read_csv('\/kaggle\/input\/amazon-music-reviews\/Musical_instruments_reviews.csv')","f750cd65":"assert reviews.shape == instruments.shape","3dde48a0":"# train_profile = ProfileReport(reviews, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n# train_profile","1d079313":"reviews","6526d7d9":"reviews.overall.value_counts(normalize=True)","4d623967":"\nfig = px.histogram(reviews, x=\"overall\")\nfig.show()","1323619e":"reviews['log_overall'] = np.log1p(reviews['overall']) ","438cc082":"fig = px.histogram(reviews, x=\"log_overall\")\nfig.show()","9aa763c4":"#check amount of reviewers\nreviews.reviewerID.nunique()","1d619cf0":"reviews.reviewerID.value_counts(ascending=False)","18bf2fe5":"reviews[reviews.reviewerID == 'ADH0O8UVJOT10']","ee1cd59f":"reviews.groupby('reviewerID')['overall'].agg(['mean','count']).sort_values(by='mean')","fed9dc01":"reviews[reviews.reviewerID == 'A1B3CNORXB1USI']","070daa37":"reviews.info()","ff53801a":"target = reviews['overall']","1c490da8":"from nltk.corpus import wordnet\nimport string\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer","c3ef1656":"def get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","ee7cf1bb":"def clean_text(text):\n\n    # lower text\n    text = text.lower()\n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n\n    # remove stop words\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    # remove empty tokens\n    text = [t for t in text if len(t) > 0]\n    # pos tag text\n    pos_tags = pos_tag(text)\n    # lemmatize text\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    # remove words with only one letter\n    text = [t for t in text if len(t) > 1]\n    # join all\n    text = \" \".join(text)\n    return(text)","2e4ac566":"reviews[\"reviewText\"] = reviews[\"reviewText\"].astype(str)\nreviews['clean_review'] = reviews[\"reviewText\"].apply(lambda x: clean_text(x))\nreviews","a53a201a":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\nreviews[\"sentiments\"] = reviews['clean_review'].apply(lambda x: sid.polarity_scores(x))\nreviews_df = pd.concat([reviews.drop(['sentiments'], axis=1), reviews['sentiments'].apply(pd.Series)], axis=1)","a96c8400":"reviews_df","9652097a":"\n# add number of characters column\nreviews[\"nb_chars\"] = reviews[\"reviewText\"].apply(lambda x: len(x))\n\n# add number of words column\nreviews[\"nb_words\"] = reviews[\"reviewText\"].apply(lambda x: len(x.split(\" \")))","26611ac2":"# !pip install gensim\n","8ef49e85":"\n# create doc2vec vector columns\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(reviews_df[\"clean_review\"].apply(lambda x: x.split(\" \")))]\n\n# train a Doc2Vec model with our text data\nmodel = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n\n# transform each document into a vector data\ndoc2vec_df = reviews_df[\"clean_review\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\ndoc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\nreviews_df = pd.concat([reviews_df, doc2vec_df], axis=1)","bf17f06e":"# t_profile = ProfileReport(reviews_df, title='Pandas Profiling Report', html={'style':{'full_width':True}})\n# t_profile","592f9717":"\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df = 10)\ntfidf_result = tfidf.fit_transform(reviews_df[\"clean_review\"]).toarray()\ntfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\ntfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\ntfidf_df.index = reviews_df.index\nreviews_df = pd.concat([reviews_df, tfidf_df], axis=1)","2fc31062":"reviews_df.columns[:11]","ab5db30a":"ignore_cols = reviews_df.columns[:11].tolist()\nused_cols = [c for c in reviews_df.columns.tolist() if c not in ignore_cols]","0ba1ac4e":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom xgboost import XGBRFRegressor\nfrom sklearn.linear_model import LinearRegression, MultiTaskElasticNet\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn\n","086e53db":"xgb = XGBRFRegressor()\nlr = LinearRegression()\nforest = RandomForestClassifier()","5e715ec5":"used_cols","1b932523":"sorted(sklearn.metrics.SCORERS.keys())","ccfaa307":"scores_xgb = cross_val_score(xgb,reviews_df[used_cols], reviews_df['overall'], cv=5, scoring='neg_median_absolute_error')\nscores_forest = cross_val_score(forest,reviews_df[used_cols], reviews_df['overall'], cv=5)\n","900181f6":"print(scores_xgb)\nprint(scores_forest)","2b7f379e":"scores_lr = cross_val_score(lr,reviews_df[used_cols], reviews_df['overall'], cv=5, scoring= 'neg_median_absolute_error')","21238ee0":"scores_lr","f44de89f":"X_train, X_test, y_train, y_test = train_test_split(reviews_df[used_cols], reviews_df['overall'])","3a1c6e0f":"xgb.fit(X_train,y_train)\nforest.fit(X_train,y_train)","7ca841bb":"xgb_preds = xgb.predict(X_test)\nforest_preds = forest.predict(X_test)","197fbe69":"for i in [xgb_preds,forest_preds]:\n    print(i.mean(),\n          i.std(),\n          i.min(),\n          i.max())\n   \n","be5676bb":"print(y_test.mean(),\n        y_test.std())","10306715":"(xgb_preds - y_test).sum()\/len(y_test)","e7765161":"(forest_preds - y_test).sum()\/len(y_test)","88320dc4":"from sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt","098e7872":"titles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nclass_names = y_test.unique()\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(forest, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()\n","700c1a5c":"##### above i take the difference of the predicted values and the true values sum them and divide by the length of the test to get an average error ","7f3153ed":"fairly imbalanced data with approx. 87% being 4 and 5 stars","274031cd":"Ya the imbalanced dataset really hurt the random forest \nnext set is to try SMOTE or another way of compensating with the imbalanced data","d4ae6966":"a lot of repeat reviewer IDs\n\nlets check the most prolific","14b48386":"The xgbreggressor seems to do better as it isn't constrained to discrete values \nLets try a confusion matrix to see how that looks","957b785b":"Well that didnt work","4f2248b5":"Looks like loging the target may normalize the data","a3d6729e":"Also interesting to note that the lowest amount of reviews is 5","2292d308":"#### With this imbalanced dataset both algorithms favor 5 star reviews with little variation ","719b5039":"Nltk is natral language tool.\n*TODO* adapt code to use spacy package\n"}}