{"cell_type":{"8c206324":"code","3e215dad":"code","d4468d50":"code","ffcd2883":"code","b4dec09b":"code","95c07f22":"code","5c59d7e7":"code","4c8ebf59":"code","89c5e653":"code","504d8d49":"code","bc18a5f5":"code","83b125b0":"code","e822a83c":"code","6ca93bef":"markdown","1d8fb0ff":"markdown","26c7d9ec":"markdown","edcd2613":"markdown","92fbdc29":"markdown","61bb8694":"markdown","d2d51d9a":"markdown","55476f9e":"markdown","239e1fd7":"markdown","cde836e4":"markdown","dd6f0a6d":"markdown","49792ee9":"markdown","fbe733f7":"markdown","b373f111":"markdown","06c50e3e":"markdown","821ccc56":"markdown","ad14ebfe":"markdown"},"source":{"8c206324":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport glob\nimport json\nimport re\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","3e215dad":"\npath = '\/kaggle\/input\/'\nall_json = glob.glob(f'{path}\/**\/*.json', recursive=True)\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\ncovid_df = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\ncovid_df.drop_duplicates(['abstract'], inplace=True)\ncovid_df.head()","d4468d50":"covid_df['body_text'] = covid_df['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ncovid_df['abstract'] = covid_df['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\ncovid_df['body_text'] = covid_df['body_text'].apply(lambda x: lower_case(x))\ncovid_df['abstract'] = covid_df['abstract'].apply(lambda x: lower_case(x))\ncovid_df.head()","ffcd2883":"text = covid_df.drop([\"paper_id\", \"abstract\"], axis=1)\ntext.head()\ntext.to_csv('.\/clean_text.csv')","b4dec09b":"import spacy\nspacy.load('en')\nfrom spacy.lang.en import English\nparser = English()","95c07f22":"def tokenize(text):\n    lda_tokens = []\n    tokens = parser(text)\n    for token in tokens:\n        if token.orth_.isspace():\n            continue\n        elif token.like_url:\n            lda_tokens.append('URL')\n        elif token.orth_.startswith('@'):\n            lda_tokens.append('SCREEN_NAME')\n        else:\n            lda_tokens.append(token.lower_)\n    return lda_tokens","5c59d7e7":"import nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\ndef get_lemma(word):\n    lemma = wn.morphy(word)\n    if lemma is None:\n        return word\n    else:\n        return lemma\ndef get_lemma2(word):\n    return WordNetLemmatizer().lemmatize(word)","4c8ebf59":"en_stop = set(nltk.corpus.stopwords.words('english'))","89c5e653":"def prepare_text_for_lda(text):\n    tokens = tokenize(text)\n    tokens = [token for token in tokens if len(token) > 4]\n    tokens = [token for token in tokens if token not in en_stop]\n    tokens = [get_lemma(token) for token in tokens]\n    return tokens","504d8d49":"import random\nfrom random import randint\n\ntext_data = []\nwith open('.\/clean_text.csv') as f:\n    for line in f:\n        tokens = prepare_text_for_lda(line)\n        value = randint(0, 100)\n        if value==99:\n            text_data.append(tokens)","bc18a5f5":"from gensim import corpora\ndictionary = corpora.Dictionary(text_data)\ncorpus = [dictionary.doc2bow(text) for text in text_data]\nimport pickle\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","83b125b0":"import gensim\nNUM_TOPICS = 10\nldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\nldamodel.save('model10.gensim')\ntopics = ldamodel.print_topics(num_words=4)\nfor topic in topics:\n    print(topic)","e822a83c":"dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = gensim.models.ldamodel.LdaModel.load('model10.gensim')\nimport pyLDAvis.gensim\nlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","6ca93bef":"We have to clean-up the text by\u00a0\n- Remove punctuation\n- Convert each text to lower case","1d8fb0ff":"- Because we only need body_text of the article so we will drop paper_id and abstract then save clean file, we will use it later","26c7d9ec":"All topic related to virus mechanism but research on difference way","edcd2613":"- We can define a function to prepare the text for topic modelling","92fbdc29":"- We will use following function to clean our text and return list of tokens:","61bb8694":"# pyLDAvis\n- pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n- Visualizing 20 topics:","d2d51d9a":"- Next we will import spacy. If you never installed spacy before then you have to install before import\n- If you are using anaconda then implement\n    - *conda install -c conda-forge spacy*\n- If you are not using anaconda and you want to install via pip then implement:\n    - *pip install -U spacy*\n- If you want to install from source then implement:\n    - *git clone https:\/\/github.com\/explosion\/spaCy\n    - *cd spaCy*\n    - *pip install -r requirements.txt*\n    - *python setup.py build_ext\u200a-\u200ainplace*\n- You can refer to this page for more option: https:\/\/spacy.io\/usage\n- **Then what is spaCy\u00a0?**\n    - spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n    - If you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n    - spaCy is designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning ([source](https:\/\/spacy.io\/usage\/spacy-101))\n- ok let's import spacy","55476f9e":"- Open up our data, read line by line, for each line, prepare text for LDA, then add to a list.\n","239e1fd7":"We use NLTK\u2019s Wordnet to find the meanings of words, synonyms, antonyms, and more. In addition, we use WordNetLemmatizer to get the root word.","cde836e4":"**Latent Dirichlet Allocation (LDA) with\u00a0Gensim**\n- What is Gensim\u00a0?\n    - Gensim = \"Generate Similar\".\u00a0\n    - Gensim started off as a collection of various Python scripts for the Czech Digital Mathematics Library dml.cz in 2008, where it served to generate a short list of the most similar articles to a given article (source)\n- Install Gensim via anaconda\n    - conda install -c anaconda gensim\n- Install Gensim via pip\n    - pip install\u200a-\u200aupgrade gensim\n    \n**Then what is\u00a0LDA**\n- In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics. LDA is an example of a topic model and belongs to the machine learning toolbox and in wider sense to the artificial intelligence toolbox (source)\n- Ok, we will create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use","dd6f0a6d":"- We use NLTK Wordnet and WordNetLemmatizer to find the meaning of words such as synonyms, antonyms, etc. and also get the root word\n- Before that feel free to install nltk and download wordnet together with stopword\n    - *pip install\u200a-\u200auser -U nltk*\n    - *nltk.download('wordnet')*\n    - *nltk.download('stopwords')*","49792ee9":"- So we are trying to ask LDA to find 20 topics in the data","fbe733f7":"- Filter out stop words:","b373f111":"> When we have 10 or more topics, we can see certain topics are clustered together, this indicates the similarity between topics","06c50e3e":"*ok Lets go*\n- Because kaggle provided us lot of json file so we will load all json data to dataframe and drop abstract duplicate to make sure unique articles","821ccc56":"- Saliency: a measure of how much the term tells you about the topic.\n- Relevance: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\n- The size of the bubble measures the importance of the topics, relative to the data.\n- First, we got the most salient terms, means terms mostly tell us about what\u2019s going on relative to the topics. We can also look at individual topic.","ad14ebfe":"**Goal**\n- With a large amount of literature and fast spreading of COVID-19. It's difficult for health care professionals figure out relevant research. \n- In this post, we will try to identify which topic is discussed in research. It also reduce number of articles which scientist has go through. \n- Research paper topic modelling is an unsupervised machine learning method which allow us to learn topic of articles in corpus"}}