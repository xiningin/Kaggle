{"cell_type":{"c6691232":"code","7d2d2a83":"code","9ed6ec27":"code","1e630397":"code","094150d5":"code","03f76be7":"code","f128a92c":"code","414e24bb":"code","a68f7c33":"code","47e6ca91":"code","45ad60a6":"code","8829920c":"code","5f1975b6":"code","70b4e5eb":"code","541ffa97":"code","aacbda2a":"code","7f914776":"code","d3a6f5ab":"code","d73d63d9":"code","ff8c9db8":"code","5ae16771":"code","74284752":"markdown"},"source":{"c6691232":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport glob\nimport shutil\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\ncwd = os.getcwd()\nfrom random import sample\nfrom PIL import Image\nfrom IPython.display import HTML","7d2d2a83":"HTML('<iframe width=\"840\" height=\"560\" src=\"https:\/\/www.youtube.com\/embed\/jSGOzjmN8q0?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe>')","9ed6ec27":"!git clone https:\/\/github.com\/sumanyumuku98\/contrastive-unpaired-translation.git CUT\n# !pip install -r requirements.txt\n# os.chdir(cwd)","1e630397":"os.chdir(\"CUT\/\")\n!pip install -r requirements.txt\nos.chdir(cwd)","094150d5":"os.chdir(\"\/kaggle\/working\/CUT\/datasets\/\")\nos.mkdir(\"images\")\nos.chdir(\"images\")\nos.mkdir(\"trainA\")\nos.mkdir(\"trainB\")\n# os.chdir(cwd)","03f76be7":"trainA = glob.glob(\"\/kaggle\/input\/gan-getting-started\/photo_jpg\/*\")\ntrainB = glob.glob(\"\/kaggle\/input\/gan-getting-started\/monet_jpg\/*\")","f128a92c":"# os.chdir(\"\/kaggle\/working\/CUT\/datasets\/images\/\")\n# !ls","414e24bb":"# Sample Few images from photos_jpg as required. Here I have sampled 300 out of 7038 images.\ntrainA_sample = sample(trainA,300)\ntrainB_sample = trainB\nfor file in trainA_sample:\n    imgName = file.split(\"\/\")[-1]\n    newPath = os.path.join(\".\/trainA\/\",imgName)\n    shutil.copyfile(file,newPath)\nfor file in trainB_sample:\n    imgName = file.split(\"\/\")[-1]\n    newPath = os.path.join(\".\/trainB\/\",imgName)\n    shutil.copyfile(file,newPath)","a68f7c33":"os.chdir(cwd)\nos.chdir(\"\/kaggle\/working\/CUT\/\")","47e6ca91":"# Change the no. of epochs and decay accordingly\n!python train.py --dataroot .\/datasets\/images --name monet_CUT --CUT_mode FastCUT --n_epochs 1 --n_epochs_decay 1 --save_epoch_freq 1","45ad60a6":"os.mkdir(\".\/checkpoints\/monet_test\")","8829920c":"!cp .\/checkpoints\/monet_CUT\/latest_net_G.pth .\/checkpoints\/monet_test\/","5f1975b6":"!ln -s \/kaggle\/input\/gan-getting-started\/photo_jpg .\/datasets\/images\/testA","70b4e5eb":"!ln -s \/kaggle\/input\/gan-getting-started\/monet_jpg .\/datasets\/images\/testB","541ffa97":"!python test.py --dataroot \/kaggle\/working\/CUT\/datasets\/images --name monet_test --CUT_mode FastCUT --num_test 7037","aacbda2a":"# lister = glob.glob(\".\/results\/monet_test\/test_latest\/images\/fake_B\/*\")","7f914776":"shutil.make_archive(\"\/kaggle\/working\/images\",\"zip\",\".\/results\/monet_test\/test_latest\/images\/fake_B\/\")","d3a6f5ab":"!du -sh \/kaggle\/working\/images.zip","d73d63d9":"os.chdir(cwd)","ff8c9db8":"!rm -r CUT","5ae16771":"# !ls","74284752":"### Refer to this paper for more details regarding the implementation [Link](https:\/\/arxiv.org\/pdf\/2007.15651.pdf). CUT is a lighter and faster alternative to CycleGAN. Have a look at the video below where the authors describe their approach:"}}