{"cell_type":{"5bebe27f":"code","0c5c76f6":"code","6a35ce20":"code","3d057c9d":"code","3e634436":"code","80e2b2f4":"code","781f17f1":"code","aa7bd5fe":"code","ba8aeea3":"code","91ddd138":"code","0c70b7a9":"code","cf2030d8":"code","1222b26b":"code","f9fdb204":"code","f9986964":"code","2cbc27f5":"code","88d13359":"code","7d48c365":"code","c44ea46a":"code","6a1ee822":"code","88bb27ee":"code","8a60a4b1":"code","e88b160d":"code","38aad800":"code","684e4828":"code","64855bd4":"code","5b6ca9d5":"code","bd9bb6cb":"code","c3c8e871":"code","e5e18e04":"code","d50b4410":"code","6145d1f3":"code","5583e4b7":"code","8690f32b":"code","2f81dc28":"code","f4378184":"code","2c92239d":"code","766c81ab":"code","c4fb6ddc":"code","7cc5e00c":"code","99e877fc":"code","a76bbd8d":"code","a6f207fa":"code","86cd092d":"code","a1a5ed0c":"code","2fc36074":"code","13941c1a":"code","c23d417b":"code","342f0ec7":"code","e5304781":"code","696699a4":"code","619d02fb":"code","07fda8d3":"code","244ce85a":"code","87234c2f":"code","69155f4d":"code","3b0c87ca":"code","1be5c79a":"code","be7d460d":"code","11f8585b":"code","c4bad1b2":"code","dd8473f1":"code","47f45b53":"code","26e7a8e8":"code","7c214358":"code","6666b285":"code","e0d4dfb7":"code","57a387ad":"code","75edc893":"code","9c8f8553":"code","3dfef014":"code","3e5f5049":"code","c8c8b251":"code","cec5f955":"code","ac56a5a6":"code","e0f427b4":"code","352b6ee6":"code","b58d4589":"code","53411d1d":"code","c151eff0":"code","2dea93ce":"code","e2198244":"code","4657c8d4":"code","23e513bc":"code","0688fed8":"code","3677cec2":"code","250c77e3":"code","836d1592":"code","f0a9a225":"code","ec0696dc":"code","753ce882":"code","db4f99a3":"code","540ee2a4":"code","e391ca2f":"code","e5f2509b":"code","2da94d07":"code","090b39aa":"code","7588b593":"code","8d49bf01":"code","a71ec02d":"code","f45de00b":"code","fdfde035":"code","0f889149":"code","57421651":"code","c623523b":"code","7cbd436f":"code","f572b297":"code","b79eb254":"code","56cf23b3":"code","37416f23":"code","10909006":"code","d333318b":"code","a068789b":"code","1e546664":"code","177a7c9c":"code","7bb6d29f":"code","6054fa3c":"code","3ae18636":"code","741ee08d":"code","a8c3bc69":"code","dcbb37d8":"code","f96948bb":"code","34d131e3":"code","97ec1956":"code","0f691e6d":"code","7d05aecb":"code","caa38e6b":"code","8bb201cc":"code","b35469a9":"code","f7fd97f0":"code","4ddf49be":"code","9fab21a7":"code","f62c3991":"code","b11304b0":"code","77b64cf2":"code","ee39e17c":"code","19a333de":"code","35e4e419":"code","80928316":"code","78c781c9":"code","f0ba91ff":"code","498401b1":"code","460b66a1":"code","beb575b4":"code","81ba5fee":"code","68f7d14f":"code","83a5df55":"code","f9b4b460":"code","09452ac7":"code","0cfbd7b6":"code","7df96cdf":"code","37a1e876":"code","4952cede":"code","0077417e":"code","6de505d2":"code","7666c94b":"code","b58683e9":"code","a8967125":"code","0ca49d47":"code","ad9d5ec6":"code","d5970b1c":"code","f911019e":"code","65870340":"code","9b49e2b6":"code","16a8acbf":"code","fbc613e4":"code","8bf37f80":"code","9f8289cb":"code","bc916757":"code","3633cb6a":"code","939b55b3":"code","0f68068f":"code","50f053e1":"code","fefae96b":"code","0cba7ce4":"code","3c4a24ec":"code","6e3d8b6b":"code","4f20f9b4":"code","91fecdc5":"code","435d3a4b":"code","88335f81":"code","fb46b137":"code","f1e32853":"code","b0851ecc":"code","5f2d5c03":"code","062d2cbc":"code","816ed5cf":"code","0a20cffd":"code","b997897e":"code","6d234414":"code","2834b5e8":"code","65e5cec7":"code","aee92a37":"code","3564429c":"code","44a094ea":"code","b9a076b3":"code","3ce49035":"code","42ff17ea":"code","137325a7":"code","799d5312":"code","a7bc4aca":"code","826eaa5d":"code","63c58b21":"code","f4eb7389":"code","bcb54041":"code","6e42af23":"code","a269ae4f":"code","118f1029":"code","0ed15e03":"code","f47df14d":"code","6e72e955":"code","00e5b41f":"code","7ca74c3f":"code","702c5de8":"code","f8e96d19":"code","4df5a735":"code","ab84c6a5":"code","2f6d5f34":"code","632bfe81":"code","66e0152a":"code","5856855d":"code","5660cc18":"code","39aeda59":"code","45cdcd46":"code","b663c1e9":"code","d4ef875e":"code","9cf507da":"code","1b95cf94":"code","1704d5d5":"code","a4d21a03":"code","3e2b0cc4":"code","bb96603a":"code","c7165e7c":"code","73024a58":"code","1be1d010":"code","46f20dc1":"code","613a84e0":"code","746ca83b":"code","1dd05532":"code","e81756d0":"code","335f4c0e":"code","514639b0":"code","ee693c3e":"code","c5e26bf7":"code","c6052ef4":"code","a47244d1":"code","92be0fcf":"code","f927cb77":"code","2f862456":"code","d7561111":"code","ef62d65b":"code","ae9ed229":"code","9ef57df8":"code","aa730c7f":"code","56312e5f":"code","f113bea1":"code","96836cb6":"code","8020e575":"code","0fdc640e":"code","fa6b16d2":"code","2fb13e89":"code","e74e2fd6":"code","6de30107":"code","c988cdf8":"code","bf2b00c7":"code","960986e0":"code","16bc7a38":"code","5920e356":"code","8ddc2cdd":"code","8f2e4e2e":"code","2a60a373":"code","eb5fa8eb":"markdown","a87f06d3":"markdown","9a1d45a7":"markdown","d6ab3b92":"markdown","4f51e75e":"markdown","a16c6429":"markdown","37bad174":"markdown","07194cd3":"markdown","3ed3401f":"markdown","9995c93d":"markdown","795ea01e":"markdown","304113af":"markdown","7ba32eec":"markdown","b7023018":"markdown","43516346":"markdown","559000c5":"markdown","1e21dcc6":"markdown","4296e6b3":"markdown","d80fc43b":"markdown","2f41124e":"markdown","9a479f3b":"markdown","d8644a6f":"markdown","17d24894":"markdown","1b2a8f0f":"markdown","06b11e4c":"markdown","117d5994":"markdown","9134ca4b":"markdown"},"source":{"5bebe27f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder,RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nimport math","0c5c76f6":"# importing statsmodels package\n# !python -m pip install statsmodels","6a35ce20":"df_org = pd.read_csv(\"..\/input\/dataset\/house_data_eda.csv\")","3d057c9d":"df_org.head()","3e634436":"df_org.drop(\"Unnamed: 0\", inplace=True,axis=1)","80e2b2f4":"# garageyrblt colm has inf values\n# so its replaced with one of the values in dataset\ndf_org['garageyrblt'].replace(np.inf,2010.0,inplace = True)","781f17f1":"df = df_org.copy()","aa7bd5fe":"df.head()","ba8aeea3":"df.head()","91ddd138":"df.info()","0c70b7a9":"list(set(df.dtypes.tolist()))","cf2030d8":"df_num = df.select_dtypes(include = ['float64','int64'])","1222b26b":"df_num.head()","f9fdb204":"df_num.describe()","f9986964":"for i in range(0, len(df_num.columns), 5):\n    sns.pairplot(data=df_num,\n                x_vars=df_num.columns[i:i+5],\n                y_vars=['saleprice'])","2cbc27f5":"df_num.columns","88d13359":"df_cat = df.select_dtypes(\"O\")","7d48c365":"df_cat.head()","c44ea46a":"df_cat_colms = df_cat.columns\ndf_cat_colms","6a1ee822":"le = LabelEncoder()","88bb27ee":"for i in range(len(df_cat.columns)):\n    df[df_cat.columns[i]] = le.fit_transform( df[df_cat.columns[i]])","8a60a4b1":"df.head()","e88b160d":"df.info()","38aad800":"df_num.columns","684e4828":"# scaling Numerical Data\n# robust Scaler\n# its not affected by outliers because it takes Median values to scale.\n# robust scaling formula \n\n# xi = xi - Q1(x)\/ Q3(x) - Q1(x)\n\n# like MinMaxScaler it takes the percentile values","64855bd4":"df['garageyrblt'].groupby(df['garageyrblt']).count()","5b6ca9d5":"rs = RobustScaler()\ndf[list(df_num.columns)] = rs.fit_transform(df[list(df_num.columns)])\ndf[list(df_num.columns)].head()","bd9bb6cb":"df.head(10)","c3c8e871":"# train test splitting\ny = df[[\"saleprice\"]]\nX = df.drop([\"saleprice\"],axis=1)\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.20,random_state = 42)","e5e18e04":"y.head()","d50b4410":"X.head(10)","6145d1f3":"X_train.shape","5583e4b7":"X_test.shape","8690f32b":"X_train_sm = sm.add_constant(X_train)\nlr = sm.OLS(y_train,X_train_sm)\nlr_model = lr.fit()\nlr_model.params\nlr_model.summary()","2f81dc28":"y_train_pred = lr_model.predict(X_train_sm)\ny_train_pred.head()","f4378184":"y_train.head()","2c92239d":"y_train[\"saleprice_pred_ols\"] = y_train_pred","766c81ab":"y_train.head()","c4fb6ddc":"# Residual Analysis\ny_train['res_ols'] = y_train['saleprice'] - y_train['saleprice_pred_ols']","7cc5e00c":"y_train.head()","99e877fc":"plt.figure(figsize = (15,8))\nsns.distplot(y_train.res_ols)","a76bbd8d":"# residuals are normally distributed","a6f207fa":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = lr_model.predict(X_test_sm)","86cd092d":"r2_score(y_test,y_test_pred)","a1a5ed0c":"y_test['saleprice_pred_ols'] = y_test_pred","2fc36074":"y_test['res_ols'] = y_test.saleprice - y_test.saleprice_pred_ols","13941c1a":"y_test.head()","c23d417b":"plt.figure(figsize = (15,8))\nsns.distplot(y_test.res_ols)","342f0ec7":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test.saleprice_pred_ols)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test.saleprice_pred_ols)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","e5304781":"lr = LinearRegression()","696699a4":"print(X_train.shape,y_train.shape)","619d02fb":"print(X_test.shape,y_test.shape)","07fda8d3":"lr.fit(X_train,y_train.saleprice)","244ce85a":"y_train_pred = lr.predict(X_train)","87234c2f":"print('{}\\n'.format(repr(y_train_pred)))","69155f4d":"print('Coefficients: {}\\n'.format(repr(lr.coef_)))","3b0c87ca":"print('Intercept: {}\\n'.format(lr.intercept_))","1be5c79a":"y_train[\"saleprice_pred_lr\"] = y_train_pred","be7d460d":"y_train.head()","11f8585b":"r2_score(y_train.saleprice,y_train.saleprice_pred_lr)","c4bad1b2":"y_train['res_lr'] = y_train.saleprice - y_train.saleprice_pred_lr","dd8473f1":"y_train.head()","47f45b53":"plt.figure(figsize = (15,8))\nsns.distplot(y_train.res_lr)","26e7a8e8":"# the residuals are normally distributed","7c214358":"y_test_pred = lr.predict(X_test)","6666b285":"y_test['saleprice_pred_lr'] = y_test_pred","e0d4dfb7":"y_test['res_lr'] = y_test.saleprice - y_test.saleprice_pred_lr","57a387ad":"y_test.head()","75edc893":"r2_score(y_test.saleprice,y_test.saleprice_pred_lr)","9c8f8553":"plt.figure(figsize = (15,8))\nsns.distplot(y_test.res_lr)","3dfef014":"# the residuals in test data also has a normal distribution.\n# so most of the data points are has residuals zero\n# so the model is better","3e5f5049":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test.saleprice_pred_lr)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test.saleprice_pred_lr)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)\n","c8c8b251":"dcr = DecisionTreeRegressor(max_depth=5)","cec5f955":"dcr.fit(X_train,y_train.saleprice)","ac56a5a6":"y_train_pred = dcr.predict(X_train)","e0f427b4":"y_train_pred","352b6ee6":"y_train['saleprice_pred_dcr'] = y_train_pred","b58d4589":"y_train.head()","53411d1d":"y_train['res_dcr'] = y_train.saleprice - y_train.saleprice_pred_dcr","c151eff0":"y_train.head()","2dea93ce":"plt.figure(figsize = (15,8))\nsns.distplot(y_train.res_dcr)","e2198244":"# r2 value\nr2_score(y_train.saleprice,y_train.saleprice_pred_dcr)","4657c8d4":"# this graph shows that there is 86 score","23e513bc":"y_test_pred = dcr.predict(X_test)","0688fed8":"y_test['saleprice_pred_dcr'] = y_test_pred","3677cec2":"y_test.head()","250c77e3":"y_test['res_dcr'] = y_test.saleprice - y_test.saleprice_pred_dcr","836d1592":"y_test.head()","f0a9a225":"plt.figure(figsize = (15,8))\nsns.distplot(y_test.res_dcr)","ec0696dc":"# this residuals are also normally distributed\n# this says that the model is performing well","753ce882":"# r2 value\nr2_score(y_test.saleprice,y_test.saleprice_pred_dcr)","db4f99a3":"# The above models give about 80 and 79 r2 value\n# the above models are pretty good than DCR","540ee2a4":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test.saleprice_pred_dcr)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test.saleprice_pred_dcr)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","e391ca2f":"# In this DCR the error terms or values are higher than above 2 models\n# so this model","e5f2509b":"rfr = RandomForestRegressor(n_estimators = 100)","2da94d07":"rfr.fit(X_train,y_train.saleprice)","090b39aa":"y_train_pred = rfr.predict(X_train)","7588b593":"y_train['saleprice_pred_rfr'] = y_train_pred","8d49bf01":"y_train.head()","a71ec02d":"y_train['res_rfr'] = y_train.saleprice - y_train.saleprice_pred_rfr","f45de00b":"y_train.head()","fdfde035":"r2_score(y_train.saleprice,y_train.saleprice_pred_rfr)","0f889149":"# the random forest regressor is giving training score of 98","57421651":"plt.figure(figsize = (15,8))\nsns.distplot(y_train.res_rfr)","c623523b":"# this graph shows its in normal distribution\n# so most of the data points residuals are around 0","7cbd436f":"y_test_pred = rfr.predict(X_test)","f572b297":"y_test['saleprice_pred_rfr'] = y_test_pred","b79eb254":"y_test.head()","56cf23b3":"r2_score(y_test.saleprice,y_test.saleprice_pred_rfr)","37416f23":"# the testing accuracy score is about 86","10909006":"y_test['res_rfr'] = y_test.saleprice - y_test.saleprice_pred_rfr","d333318b":"y_test.head()","a068789b":"plt.figure(figsize = (15,8))\nsns.distplot(y_test.res_rfr)","1e546664":"# this testing data is also normally distributed so most of the data points are nearer to 0","177a7c9c":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test.saleprice_pred_rfr)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test.saleprice_pred_rfr)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","7bb6d29f":"# to above all the models this random forest regressor gives the best score and low error values","6054fa3c":"df = df_org.copy()","3ae18636":"df.head()","741ee08d":"# Robust Scaling\ndf[list(df_num.columns)] = rs.fit_transform(df[list(df_num.columns)])\ndf[list(df_num.columns)].head()","a8c3bc69":"# one hot encoding\ndum = pd.get_dummies(df[list(df_cat.columns)],drop_first = True)","dcbb37d8":"dum.head()","f96948bb":"dum.shape","34d131e3":"len(df_cat.columns)","97ec1956":"78-41","0f691e6d":"df.drop(list(df_cat.columns),axis = 1,inplace = True)","7d05aecb":"df.shape","caa38e6b":"df1 = pd.concat([df,dum], axis =1)","8bb201cc":"df1.shape","b35469a9":"df1.head(10)","f7fd97f0":"df1.shape","4ddf49be":"y1 = df1[[\"saleprice\"]]\nX1 = df1.drop([\"saleprice\"],axis=1)\nX_train1,X_test1,y_train1,y_test1=train_test_split(X1,y1,test_size = 0.20,random_state = 42)","9fab21a7":"X_train1.shape","f62c3991":"X_test1.shape","b11304b0":"# creating OLS model (Ordinary Least Squares)\nX_train_sm1 = sm.add_constant(X_train1)\nlr = sm.OLS(y_train1,X_train_sm1)\nlr_model = lr.fit()\nlr_model.params\nlr_model.summary()","77b64cf2":"# this model gives 94% variation among Independant variables\n# the Adj R2 value is nearer to r2\n# so the model is pretty doing well\n# the prob(F-stat)=0 says that confidence interval is high","ee39e17c":"y_train_pred1 = lr_model.predict(X_train_sm1)\ny_train_pred1.head()","19a333de":"y_train1[\"saleprice_pred_ols\"] = y_train_pred1","35e4e419":"# Residual Analysis\ny_train1['res_ols'] = y_train1['saleprice'] - y_train1['saleprice_pred_ols']","80928316":"y_train1.head()","78c781c9":"plt.figure(figsize = (15,8))\nsns.distplot(y_train1.res_ols)\n","f0ba91ff":"# the residuals are perfectly normally distributed","498401b1":"X_test_sm1 = sm.add_constant(X_test1)\ny_test_pred1 = lr_model.predict(X_test_sm1)","460b66a1":"r2_score(y_test1,y_test_pred1)","beb575b4":"y_test1['saleprice_pred_ols'] = y_test_pred1","81ba5fee":"y_test1['res_ols'] = y_test1.saleprice - y_test1.saleprice_pred_ols","68f7d14f":"plt.figure(figsize = (15,8))\nsns.distplot(y_test1.res_ols)\n","83a5df55":"# the test data residuals are also normally distributed","f9b4b460":"# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test1.saleprice_pred_ols)\nprint(mse)\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test1.saleprice_pred_ols)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","09452ac7":"lr = LinearRegression()","0cfbd7b6":"lr.fit(X_train1,y_train1.saleprice)","7df96cdf":"y_train1_pred = lr.predict(X_train1)","37a1e876":"y_train1['saleprice_pred_lr'] = y_train1_pred","4952cede":"y_train1.head()","0077417e":"y_train1['res_lr'] = y_train1.saleprice - y_train1.saleprice_pred_lr","6de505d2":"y_train1.head()","7666c94b":"r2_score(y_train1.saleprice,y_train1.saleprice_pred_lr)","b58683e9":"# training score is about 94","a8967125":"plt.figure(figsize = (15,8))\nsns.distplot(y_train1.res_lr)","0ca49d47":"y_test1_pred = lr.predict(X_test1)","ad9d5ec6":"#lr.summary()","d5970b1c":"y_test1.head()","f911019e":"y_test1['saleprice_pred_lr'] = y_test1_pred","65870340":"y_test1['res_lr'] = y_test1.saleprice - y_test1.saleprice_pred_lr","9b49e2b6":"y_test1.head()","16a8acbf":"r2_score(y_test1.saleprice,y_test1.saleprice_pred_lr)","fbc613e4":"# the test score is all about 80","8bf37f80":"plt.figure(figsize = (15,8))\nsns.distplot(y_test1.res_lr)","9f8289cb":"# Regression metrics\n#explained_variance=metrics.explained_variance_score(y_test1.saleprice, y_test1.saleprice_pred_dcr)\n#mean_absolute_error=metrics.mean_absolute_error(y_test1.saleprice, y_test1.saleprice_pred_dcr) \n#mse=metrics.mean_squared_error(y_test1.saleprice, y_test1.saleprice_pred_dcr) \n#mean_squared_log_error=metrics.mean_squared_log_error(y_test1.saleprice, y_test1.saleprice_pred_dcr)\n#median_absolute_error=metrics.median_absolute_error(y_test1.saleprice, y_test1.saleprice_pred_dcr)\n#r2=metrics.r2_score(y_test1.saleprice, y_test1.saleprice_pred_dcr)\n#\n#print('explained_variance: ', round(explained_variance,4))    \n#print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n#print('r2: ', round(r2,4))\n#print('MAE: ', round(mean_absolute_error,4))\n#print('MSE: ', round(mse,4))\n#print('RMSE: ', round(np.sqrt(mse),4))","bc916757":"dcr = DecisionTreeRegressor(max_depth=5)\ndcr.fit(X_train1,y_train1.saleprice)","3633cb6a":"y_train1_pred = dcr.predict(X_train1)","939b55b3":"r2_score(y_train1.saleprice,y_train1_pred)","0f68068f":"y_train1['saleprice_pred_dcr'] = y_train1_pred","50f053e1":"y_train1['res_dcr'] = y_train1.saleprice - y_train1.saleprice_pred_dcr","fefae96b":"plt.figure(figsize = (15,8))\nsns.distplot(y_train1.res_dcr)","0cba7ce4":"# this training set score is 86","3c4a24ec":"y_test1_pred = dcr.predict(X_test1)","6e3d8b6b":"y_test1['saleprice_pred_dcr'] = y_test1_pred","4f20f9b4":"y_test1['res_dcr'] = y_test1.saleprice - y_test1.saleprice_pred_dcr","91fecdc5":"y_test1.head()","435d3a4b":"r2_score(y_test1.saleprice,y_test1.saleprice_pred_dcr)","88335f81":"# this test score is  low\n# training score is 86\n# testing  score is 0.70","fb46b137":"plt.figure(figsize = (15,8))\nsns.distplot(y_test1.res_dcr)","f1e32853":"# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test1.saleprice_pred_dcr)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test1.saleprice_pred_dcr)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","b0851ecc":"rfr = RandomForestRegressor(n_estimators = 500)","5f2d5c03":"rfr.fit(X_train1,y_train1.saleprice)","062d2cbc":"y_train1_pred = rfr.predict(X_train1)","816ed5cf":"y_train1['saleprice_pred_rfr'] = y_train1_pred","0a20cffd":"y_train1['res_rfr'] = y_train1.saleprice - y_train1.saleprice_pred_rfr","b997897e":"r2_score(y_train1.saleprice,y_train1.saleprice_pred_rfr)","6d234414":"# its score is too good as 98","2834b5e8":"plt.figure(figsize = (15,8))\nsns.distplot(y_train1.res_rfr)","65e5cec7":"# the data distribution too good for training data","aee92a37":"y_test1_pred = rfr.predict(X_test1)","3564429c":"y_test1['saleprice_pred_rfr'] = y_test1_pred\n\n","44a094ea":"y_test1['res_rfr'] = y_test1.saleprice - y_test1.saleprice_pred_rfr\n","b9a076b3":"r2_score(y_test1.saleprice,y_test1.saleprice_pred_rfr)","3ce49035":"#  this score is pretty good for trainning model","42ff17ea":"plt.figure(figsize = (15,8))\nsns.distplot(y_test1.res_rfr)","137325a7":"# this data has some values distributed around 0 in residuals","799d5312":"# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test1.saleprice_pred_rfr)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test1.saleprice_pred_rfr)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","a7bc4aca":"# the feature selection can be done in many ways\n# forward selection\n# backward elimination\n# Recursive feature elimination\n# some methods like Boruta, SHAP, Null importance","826eaa5d":"df.head()","63c58b21":"df1.head()","f4eb7389":"X_train.head()","bcb54041":"X_train1.head()","6e42af23":"vif_df = pd.DataFrame()\nvif_df[\"features\"] = X_train.columns\nvif_df[\"VIF\"] = [vif(X_train.values,i) for i in range(X_train.shape[1])]\nvif_df[\"VIF\"] = round(vif_df[\"VIF\"],3)\nvif_df = vif_df.sort_values(by = \"VIF\",ascending = False)\nvif_df","a269ae4f":"vif_less_ten = vif_df[vif_df.VIF < 10.0]","118f1029":"vif_less_ten","0ed15e03":"vif_less_ten.shape","f47df14d":"# this has selected 69 columns form 77 columns (label encod & robust scaling)","6e72e955":"vif_less_five = vif_df[vif_df.VIF < 5.0]","00e5b41f":"vif_less_five","7ca74c3f":"vif_less_two = vif_df[vif_df.VIF < 2.0]","702c5de8":"vif_less_two","f8e96d19":"vif_less_two.shape","4df5a735":"# the vif <2 has selected 40 features","ab84c6a5":"# OLS\n","2f6d5f34":"# vif < 10\nX_train_sm = sm.add_constant(X_train[list(vif_less_ten.features)])\nlr = sm.OLS(y_train.saleprice,X_train_sm)\nlr_model = lr.fit()\nlr_model.params\nlr_model.summary()","632bfe81":"y_train_pred = lr_model.predict(X_train_sm)\ny_train_pred.head()","66e0152a":"y_train[\"saleprice_pred_vif10_ols\"] = y_train_pred","5856855d":"# Residual Analysis\ny_train['res_vif10_ols'] = y_train['saleprice'] - y_train['saleprice_pred_vif10_ols']","5660cc18":"plt.figure(figsize = (15,8))\nsns.distplot(y_train.res_vif10_ols)","39aeda59":"# see the errors are normally distributed","45cdcd46":"X_test_sm = sm.add_constant(X_test[list(vif_less_ten.features)])\ny_test_pred = lr_model.predict(X_test_sm)","b663c1e9":"r2_score(y_test.saleprice,y_test_pred)","d4ef875e":"# the model score is 85 and test score is 75\n# pretty good model","9cf507da":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","1b95cf94":"# the values of errors are high","1704d5d5":"# vif < 5\nX_train_sm = sm.add_constant(X_train[list(vif_less_five.features)])\nlr = sm.OLS(y_train.saleprice,X_train_sm)\nlr_model = lr.fit()\nlr_model.params\nlr_model.summary()","a4d21a03":"X_test_sm = sm.add_constant(X_test[list(vif_less_five.features)])","3e2b0cc4":"y_test_pred = lr_model.predict(X_test_sm)\ny_test_pred.head()","bb96603a":"r2_score(y_test.saleprice,y_test_pred)","c7165e7c":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","73024a58":"# vif < 2\nX_train_sm = sm.add_constant(X_train[list(vif_less_two.features)])\nlr = sm.OLS(y_train.saleprice,X_train_sm)\nlr_model = lr.fit()\nlr_model.params\nlr_model.summary()","1be1d010":"X_test_sm = sm.add_constant(X_test[list(vif_less_two.features)])","46f20dc1":"y_test_pred = lr_model.predict(X_test_sm)\ny_test_pred.head()","613a84e0":"r2_score(y_test.saleprice,y_test_pred)","746ca83b":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","1dd05532":"# Linear Regression","e81756d0":"lr = LinearRegression()","335f4c0e":"lr.fit(X_train[list(vif_less_ten.features)],y_train.saleprice)","514639b0":"lr.score(X_train[list(vif_less_ten.features)],y_train.saleprice)","ee693c3e":"# the score of training is 85 for vif < 10","c5e26bf7":"y_test_pred = lr.predict(X_test[list(vif_less_ten.features)])","c6052ef4":"r2_score(y_test.saleprice,y_test_pred)","a47244d1":"# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)\n","92be0fcf":"lr = LinearRegression()","f927cb77":"lr.fit(X_train[list(vif_less_five.features)],y_train.saleprice)\n\ntrain_score = lr.score(X_train[list(vif_less_five.features)],y_train.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = lr.predict(X_test[list(vif_less_five.features)])\n\ntest_score = r2_score(y_test.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","2f862456":"# this is also the best model\n# as train and test score is same","d7561111":"rfr = RandomForestRegressor(n_estimators = 100)","ef62d65b":"rfr.fit(X_train[list(vif_less_ten.features)],y_train.saleprice)\n\ntrain_score = rfr.score(X_train[list(vif_less_ten.features)],y_train.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = rfr.predict(X_test[list(vif_less_ten.features)])\n\ntest_score = r2_score(y_test.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","ae9ed229":"# the training score is 97\n# the testing score is 83\n# moderate model","9ef57df8":"rfr = RandomForestRegressor(n_estimators = 500)\nrfr.fit(X_train[list(vif_less_five.features)],y_train.saleprice)\n\ntrain_score = rfr.score(X_train[list(vif_less_five.features)],y_train.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = rfr.predict(X_test[list(vif_less_five.features)])\n\ntest_score = r2_score(y_test.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","aa730c7f":"rfr = RandomForestRegressor(n_estimators = 100)\nrfr.fit(X_train[list(vif_less_two.features)],y_train.saleprice)\n\ntrain_score = rfr.score(X_train[list(vif_less_two.features)],y_train.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = rfr.predict(X_test[list(vif_less_two.features)])\n\ntest_score = r2_score(y_test.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","56312e5f":"vif_df = pd.DataFrame()\nvif_df[\"features\"] = X_train1.columns\nvif_df[\"VIF\"] = [vif(X_train1.values,i) for i in range(X_train1.shape[1])]\nvif_df[\"VIF\"] = round(vif_df[\"VIF\"],3)\nvif_df = vif_df.sort_values(by = \"VIF\",ascending = False)\nvif_df","f113bea1":"vif_df[vif_df.VIF.isnull()]","96836cb6":"vif_nan = list(vif_df[vif_df.VIF.isnull()].features)","8020e575":"X_train1[vif_nan].describe()","0fdc640e":"# these variables has all the values are 0\n# so we can eliminate the values","fa6b16d2":"vif_less_ten = vif_df[vif_df.VIF < 10.0]\nvif_less_ten","2fb13e89":"# vif < 10\nX_train_sm = sm.add_constant(X_train1[list(vif_less_ten.features)])\nlr = sm.OLS(y_train.saleprice,X_train_sm)\nlr_model = lr.fit()\n#lr_model.params\nprint(lr_model.summary())\n\n\n\nX_test_sm = sm.add_constant(X_test1[list(vif_less_ten.features)])\ny_test_pred = lr_model.predict(X_test_sm)\n\ntest_score = r2_score(y_test1.saleprice,y_test_pred)\nprint(\"\\ntest score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","e74e2fd6":"# here the training score is 88 , 76\n# its a moderate model","6de30107":"lr = LinearRegression()\nlr.fit(X_train1[list(vif_less_ten.features)],y_train1.saleprice)\n\ntrain_score = lr.score(X_train1[list(vif_less_ten.features)],y_train1.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = lr.predict(X_test1[list(vif_less_ten.features)])\n\ntest_score = r2_score(y_test1.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","c988cdf8":"# the score for linear Regression model is 88 and 76","bf2b00c7":"rfr = RandomForestRegressor(n_estimators = 100)\nrfr.fit(X_train1[list(vif_less_ten.features)],y_train1.saleprice)\n\ntrain_score = rfr.score(X_train1[list(vif_less_ten.features)],y_train1.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = rfr.predict(X_test1[list(vif_less_ten.features)])\n\ntest_score = r2_score(y_test1.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","960986e0":"# this RFR has a scores of 97 and 82","16bc7a38":"vif_less_five = vif_df[vif_df.VIF < 5.0]\nvif_less_five","5920e356":"# vif < 5\nX_train_sm = sm.add_constant(X_train1[list(vif_less_five.features)])\nlr = sm.OLS(y_train1.saleprice,X_train_sm)\nlr_model = lr.fit()\n#lr_model.params\nprint(lr_model.summary())\n\n\nX_test_sm = sm.add_constant(X_test1[list(vif_less_five.features)])\ny_test_pred = lr_model.predict(X_test_sm)\n\ntest_score = r2_score(y_test1.saleprice,y_test_pred)\nprint(\"\\ntest score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mae)\nprint(rmse)","8ddc2cdd":"# the model has score of 74 and 65\n# its a moderate model","8f2e4e2e":"rfr = RandomForestRegressor(n_estimators = 100)\nrfr.fit(X_train1[list(vif_less_five.features)],y_train1.saleprice)\n\ntrain_score = rfr.score(X_train1[list(vif_less_five.features)],y_train1.saleprice)\nprint(\"train score : \",train_score)\n\ny_test_pred = rfr.predict(X_test1[list(vif_less_five.features)])\n\ntest_score = r2_score(y_test1.saleprice,y_test_pred)\nprint(\"test score: \",test_score)\n\n# mean squared error\nmse = mean_squared_error(y_test1.saleprice,y_test_pred)\nprint(mse)\n\n# mean absolute error\nmae = mean_absolute_error(y_test1.saleprice,y_test_pred)\nprint(mae)\n\n# root mean squared error\nrmse = math.sqrt(mse)\nprint(rmse)","2a60a373":"# its score is 95 and 71 \n# its a pretty bad model\n# the gap bw the scores is high","eb5fa8eb":"### VIF for Features","a87f06d3":"# Label Encoding","9a1d45a7":"# Linear Regression Model","d6ab3b92":"# Numerical Data","4f51e75e":"# Robust Scaling","a16c6429":"# Robust scaler & One Hot Encoding","37bad174":"# VIF(Variance Inflation Factor)","07194cd3":"# Robust Scaling and One hot Encoding","3ed3401f":"# Decision Tree Regressor","9995c93d":"# Random Forest Regressor","795ea01e":"# creating a base line model","304113af":"# Linear Regression","7ba32eec":"# VIF <10","b7023018":"# Feature Selection","43516346":"## Finally with VIF\n### The VIF<5 features gives the best model with best features in Robust Scaling and Label Encoding \n### OLS and LR gives the best Scores as 82 for Training and 82 for testing\n### The errors reduced a bit","559000c5":"### VIF<10","1e21dcc6":"# There are four assumptions associated with a linear regression model:\n\n    Linearity: The relationship between X and the mean of Y is linear.\n    Homoscedasticity: The variance of residual is the same for any value of X.\n    Independence: Observations are independent of each other.\n    Normality: For any fixed value of X, Y is normally distributed.","4296e6b3":"# this model the score are pretty good as 64 for training and 64 for testing\n# but the error value increased\n# so the model of VIF <5 is best model which has score of 82 and 82","d80fc43b":"# this is a best model as it shows training score and testing score as same 82","2f41124e":"# Decision Tree Regressor","9a479f3b":"# from all these base line models Random Forest Regressor has high Training Score and Testing Score","d8644a6f":"# creating OLS model (Ordinary Least Squares)","17d24894":"# Random Forest Classifier ","1b2a8f0f":"# creating models using various VIF","06b11e4c":"### dropping variables\n    high p, high vif - compulsory drop\n    low p, low vif - compulsory keep\n### critical conditions\n    high p, low vif - drop first\n    low p, high vif - drop second","117d5994":"# Data Standardization vs Normalization vs Robust Scaler practically.\n\n    In theory, the guidelines are:\n\n    Advantages:\n\n    Standardization: scales features such that the distribution is centered around 0, with a standard deviation of 1. \n    \n    x-mean \/ sd\n    \n    Normalization: shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\n    \n    x-min \/ max - min\n    \n    Robust Scaler: similar to normalization but it instead uses the interquartile range, so that it is robust to outliers.\n    \n    x - q1 \/ q3 - q1\n\n    Disadvantages:\n\n    Standardization: not good if the data is not normally distributed (i.e. no Gaussian Distribution).\n    Normalization: get influenced heavily by outliers (i.e. extreme values).\n    Robust Scaler: doesn't take the median into account and only focuses on the parts where the bulk data is.","9134ca4b":"# Random Forest Regressor"}}