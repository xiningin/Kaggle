{"cell_type":{"2aac6963":"code","c65e86e4":"code","ac801892":"code","324f3574":"code","e57bf9d1":"code","023fecd6":"code","6443563b":"code","3794c703":"code","129f3e95":"code","b7d205a9":"code","885baa8a":"code","b2e26967":"code","0b5741ca":"code","95ecd227":"code","ca2f3235":"code","10b9402b":"code","6ce09962":"code","4a252f64":"code","4c145e1c":"code","3f706dcf":"code","2b668c35":"code","3505e9df":"code","9372f6ab":"code","b9d00a2c":"code","c2d9653f":"code","c61135f9":"code","9c372557":"code","0a216238":"code","17cf8de7":"code","f03776f9":"code","1b288f55":"code","30f03678":"code","bbf37e1c":"code","74c3a602":"code","7445319e":"code","a746696d":"code","1e706d66":"code","a5318d87":"code","7a9b5241":"code","e3f36baf":"code","aaf21ebf":"code","d1625d1f":"code","9c8de139":"code","b42a4960":"code","02c14988":"code","99e22e43":"code","ad6e97b5":"code","5932b8cc":"code","2199702a":"code","a1d83659":"markdown","36c658ff":"markdown","81059a27":"markdown","1b1d3df8":"markdown","d345cdec":"markdown","bb290f73":"markdown","263413d5":"markdown","ce0adcc6":"markdown","3deb5ede":"markdown","953f821c":"markdown","d5483f7a":"markdown","adee076a":"markdown","fae14e68":"markdown","ea3f0473":"markdown"},"source":{"2aac6963":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n# Import necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport pydicom as dicom\nimport seaborn as sns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.densenet import DenseNet121\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\n\nfrom keras.models import load_model\nsns.set()","c65e86e4":"# l=[]\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     if dirname=='\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/PNEUMONIA':\n#         for filename in filenames:\n#             if 'chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/'+filename not in l:\n#                 l.append('chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/'+filename)\n\n\n# dict1={'image_id':l,'Atelectasis':[0]*len(l),'Cardiomegaly':[0]*len(l),'Consolidation':[0]*len(l),'Infiltration':[0]*len(l),'Nodule\/Mass':[0]*len(l),'Pleural effusion':[0]*len(l),'Pleural thickening':[0]*len(l),'Pneumothorax':[0]*len(l),'Pulmonary fibrosis':[0]*len(l),'Pneumonia':[1]*len(l)}\n# train_df1=pd.DataFrame(dict1)\n# train_df1.head()\n# # Extract numpy values from Image column in data frame\n# images = train_df1['image_id'].values\n\n# # Extract 9 random images from it\n# random_images = [np.random.choice(images) for i in range(9)]\n\n# # Location of the image dir\n# img_dir = '\/kaggle\/input\/'\n\n# print('Display Random Images')\n\n# # Adjust the size of your images\n# plt.figure(figsize=(20,10))\n\n# # Iterate and plot random images\n# for i in range(9):\n#     plt.subplot(3, 3, i + 1)\n#     img = plt.imread(img_dir+random_images[i])\n#     plt.imshow(img, cmap='gray')\n#     plt.axis('off')\n    \n# # Adjust subplot parameters to give specified padding\n# plt.tight_layout()\n# train_df2=pd.read_csv(\"..\/input\/nih-df-changed\/mycsvfile (3).csv\")\n# train_df2.head()\n# for i in range(len(train_df2['Image Index'])):\n#     train_df2['Image Index'][i]='data\/'+train_df2['Image Index'][i]\n# train_df2 = train_df2.rename(columns={'Image Index': 'image_id'})\n# # Extract numpy values from Image column in data frame\n# images = train_df2['image_id'].values\n\n# # Extract 9 random images from it\n# random_images = [np.random.choice(images) for i in range(9)]\n\n# # Location of the image dir\n# img_dir = '\/kaggle\/input\/'\n\n# print('Display Random Images')\n\n# # Adjust the size of your images\n# plt.figure(figsize=(20,10))\n\n# # Iterate and plot random images\n# for i in range(9):\n#     plt.subplot(3, 3, i + 1)\n#     img = plt.imread(img_dir+random_images[i])\n#     plt.imshow(img, cmap='gray')\n#     plt.axis('off')\n    \n# # Adjust subplot parameters to give specified padding\n# plt.tight_layout()\n\n# train_df3 = pd.read_csv(\"..\/input\/vinbigdata-original-image-dataset\/vinbigdata\/train.csv\")\n# # Print first 5 rows\n# print(f'There are {train_df3.shape[0]} rows and {train_df3.shape[1]} columns in this data frame')\n# train_df3.head()\n# c='vinbigdata-original-image-dataset\/vinbigdata\/train\/'\n# for i in range(len(train_df3['image_id'])):\n#     train_df3['image_id'][i]=c+train_df3['image_id'][i]+'.jpg'\n# ata1=[];cardi3=[];conso4=[];infi6=[];nod8=[];peff10=[];pth11=[];pntho12=[];pfib13=[];pne14=[]\n\n# for i in train_df3['class_id']:\n#     if i==1:\n#         ata1.append(1)\n#     else:\n#         ata1.append(0)\n        \n#     if i==3:\n#         cardi3.append(1)\n#     else:\n#         cardi3.append(0)\n        \n#     if i==4:\n#         conso4.append(1)\n#     else:\n#         conso4.append(0)\n    \n#     if i==6:\n#         infi6.append(1)\n#     else:\n#         infi6.append(0)\n        \n#     if i==8:\n#         nod8.append(1)\n#     else:\n#         nod8.append(0)\n        \n#     if i==10:\n#         peff10.append(1)\n#     else:\n#         peff10.append(0)\n        \n#     if i==11:\n#         pth11.append(1)\n#     else:\n#         pth11.append(0)\n        \n#     if i==12:\n#         pntho12.append(1)\n#     else:\n#         pntho12.append(0)\n        \n#     if i==13:\n#         pfib13.append(1)\n#     else:\n#         pfib13.append(0)\n        \n#     if i==15:\n#         pne14.append(1)\n#     else:\n#         pne14.append(0)\n# train_df3['Atelectasis']=ata1\n# train_df3['Cardiomegaly']=cardi3\n# train_df3['Consolidation']=conso4\n# train_df3['Infiltration']=infi6\n# train_df3['Nodule\/Mass']=nod8\n# train_df3['Pleural effusion']=peff10\n# train_df3['Pleural thickening']=pth11\n# train_df3['Pneumothorax']=pntho12\n# train_df3['Pulmonary fibrosis']=pfib13\n# train_df3['Pneumonia']=pne14\n# # train_df['PatientId']=[i for i in range(len(train_df['class_id']))]\n# train_df3=train_df3.drop(['class_name','class_id','rad_id','x_min','y_min','x_max','y_max','width', 'height'], axis = 1)\n# train_df3.head()\n# columns = train_df3.keys()\n# columns = list(columns)\n# # print(columns)\n# columns.remove('image_id')\n# # Print out the number of positive labels for each class\n# for column in columns:\n#     print(f\"The class {column} has {train_df3[column].sum()} samples\")\n# # Extract numpy values from Image column in data frame\n# images = train_df3['image_id'].values\n\n# # Extract 9 random images from it\n# random_images = [np.random.choice(images) for i in range(9)]\n\n# # Location of the image dir\n# img_dir = '\/kaggle\/input\/'\n\n# print('Display Random Images')\n\n# # Adjust the size of your images\n# plt.figure(figsize=(20,10))\n\n# # Iterate and plot random images\n# for i in range(9):\n#     plt.subplot(3, 3, i + 1)\n#     img = plt.imread(img_dir+random_images[i])\n#     plt.imshow(img, cmap='gray')\n#     plt.axis('off')\n    \n# # Adjust subplot parameters to give specified padding\n# plt.tight_layout()\n# base=max(train_df2['Patient ID'])\n# f_base=base+len(train_df1['image_id'])+1\n# df1pid=[i for i in range(base+1,f_base)]\n# df3pid=[i for i in range(f_base,f_base+len(train_df3['image_id']))]\n# print(len(df1pid)==len(train_df1['image_id']),len(df3pid)==len(train_df3['image_id']))\n\n# train_df1['Patient ID']=df1pid\n# train_df3['Patient ID']=df3pid\n# train_df=train_df2.append(train_df1,ignore_index=True)\n# train_df=train_df.append(train_df3,ignore_index=True)\n# train_df = train_df.sample(frac = 1)\n# train_df.info()\n# train_df.to_csv('final_merged_df.csv',index=False)","ac801892":"train_df=pd.read_csv('..\/input\/final-merged-df\/final_merged_df.csv')","324f3574":"train_df.head()","e57bf9d1":"columns = train_df.keys()\ncolumns = list(columns)\n# print(columns)\ncolumns.remove('image_id')\ncolumns.remove('Patient ID')\n# Print out the number of positive labels for each class\nfor column in columns:\n#     print(f\"The class {column} has {train_df1[column].sum()} samples\")\n#     print(f\"The class {column} has {train_df2[column].sum()} samples\")\n#     print(f\"The class {column} has {train_df3[column].sum()} samples\")\n    print(f\"The class {column} has {train_df[column].sum()} samples\")","023fecd6":"labels = ['Atelectasis','Cardiomegaly','Consolidation','Infiltration','Nodule\/Mass','Pleural effusion','Pleural thickening','Pneumothorax','Pulmonary fibrosis','Pneumonia']","6443563b":"def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=16, seed=1, target_w = 320, target_h = 320):\n    \"\"\"\n    Return generator for training set, normalizing using batch\n    statistics.\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size\n    # and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator","3794c703":"def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=16, seed=1, target_w = 320, target_h = 320):\n    \"\"\"\n    Return generator for validation set and test test set using \n    normalization statistics from training set.\n\n    Args:\n      valid_df (dataframe): dataframe specifying validation data.\n      test_df (dataframe): dataframe specifying test data.\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n    \"\"\"\n    print(\"getting train and valid generators...\")\n    # get generator to sample dataset\n    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n        dataframe=train_df, \n        directory=IMAGE_DIR, \n        x_col=x_col, \n        y_col=labels, \n        class_mode=\"raw\", \n        batch_size=sample_size, \n        shuffle=True, \n        target_size=(target_w, target_h))\n    # get data sample\n    batch = raw_train_generator.next()\n    data_sample = batch[0]\n\n    # use sample to fit mean and std for test set generator\n    image_generator = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization= True)\n    \n    # fit generator to sample from training data\n    image_generator.fit(data_sample)\n\n    # get test generator\n    valid_generator = image_generator.flow_from_dataframe(\n            dataframe=valid_df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed,\n            target_size=(target_w,target_h))\n\n    test_generator = image_generator.flow_from_dataframe(\n            dataframe=test_df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed,\n            target_size=(target_w,target_h))\n    return valid_generator, test_generator","129f3e95":"train_df = train_df.sample(frac = 1)\nvalid_df=pd.DataFrame()\n# valid_df.append(train_df.iloc[0],ignore_index = True)\n# valid_df.head()\nadd_pd=[]\ndropidx=[]\ni=0\nfor j in range(len(train_df['Patient ID'])):\n    if i>2000:\n        break\n    if train_df['Patient ID'][j] not in add_pd:\n        valid_df=valid_df.append(train_df.iloc[j],ignore_index = True)\n        add_pd.append(train_df['Patient ID'][j])\n        dropidx.append(j)\n        i+=1\ntrain_df.drop(dropidx,inplace=True)\ntrain_df = train_df.sample(frac = 1)\n\n\nfrom sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(train_df, test_size=0.005)","b7d205a9":"print(len(train_df),len(test_df),len(valid_df))\nvalid_df = valid_df.sample(frac = 1)","885baa8a":"IMAGE_DIR = '\/kaggle\/input\/'\ntrain_generator = get_train_generator(train_df, IMAGE_DIR, \"image_id\", labels)\nvalid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"image_id\", labels)","b2e26967":"x, y = train_generator.__getitem__(10)\nplt.imshow(x[0]);","0b5741ca":"plt.xticks(rotation=90)\nplt.bar(x=labels, height=np.mean(train_generator.labels, axis=0))\nplt.title(\"Frequency of Each Class\")\nplt.show()","95ecd227":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef compute_class_freqs(labels):\n    \"\"\"\n    Compute positive and negative frequences for each class.\n\n    Args:\n        labels (np.array): matrix of labels, size (num_examples, num_classes)\n    Returns:\n        positive_frequencies (np.array): array of positive frequences for each\n                                         class, size (num_classes)\n        negative_frequencies (np.array): array of negative frequences for each\n                                         class, size (num_classes)\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # total number of patients (rows)\n    N = labels.shape[0]\n    \n    positive_frequencies = np.sum(labels, axis = 0) \/ N\n    negative_frequencies = 1 - positive_frequencies\n\n    ### END CODE HERE ###\n    return positive_frequencies, negative_frequencies","ca2f3235":"# Test\nlabels_matrix = np.array(\n    [[1, 0, 0],\n     [0, 1, 1],\n     [1, 0, 1],\n     [1, 1, 1],\n     [1, 0, 1]]\n)\nprint(\"labels:\")\nprint(labels_matrix)\n\ntest_pos_freqs, test_neg_freqs = compute_class_freqs(labels_matrix)\n\nprint(f\"pos freqs: {test_pos_freqs}\")\n\nprint(f\"neg freqs: {test_neg_freqs}\")","10b9402b":"freq_pos, freq_neg = compute_class_freqs(test_generator.labels)\nfreq_pos","6ce09962":"data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\ndata = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\nplt.xticks(rotation=90)\nf = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)","4a252f64":"pos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights \nneg_contribution = freq_neg * neg_weights","4c145e1c":"data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\ndata = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n                        for l,v in enumerate(neg_contribution)], ignore_index=True)\nplt.xticks(rotation=90)\nsns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);","3f706dcf":"# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n    \"\"\"\n    Return weighted loss function given negative weights and positive weights.\n\n    Args:\n      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n    \n    Returns:\n      weighted_loss (function): weighted loss function\n    \"\"\"\n    def weighted_loss(y_true, y_pred):\n        \"\"\"\n        Return weighted loss value. \n\n        Args:\n            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n        Returns:\n            loss (Float): overall scalar loss summed across all classes\n        \"\"\"\n        # initialize loss to zero\n        loss = 0.0\n        \n        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n        for i in range(len(pos_weights)):\n            # for each class, add average weighted loss for that class \n            loss += K.mean(-(pos_weights[i] *y_true[:,i] * K.log(y_pred[:,i] + epsilon) \n                             + neg_weights[i]* (1 - y_true[:,i]) * K.log( 1 - y_pred[:,i] + epsilon))) #complete this line\n        return loss\n    \n        ### END CODE HERE ###\n    return weighted_loss","2b668c35":"def weighted_loss(y_true, y_pred):\n    \"\"\"\n    Return weighted loss value. \n\n    Args:\n        y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n        y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n    Returns:\n        loss (Float): overall scalar loss summed across all classes\n    \"\"\"\n    # initialize loss to zero\n    loss = 0.0\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    for i in range(len(pos_weights)):\n        # for each class, add average weighted loss for that class \n        loss += K.mean(-(pos_weights[i] *y_true[:,i] * K.log(y_pred[:,i] + epsilon) \n                         + neg_weights[i]* (1 - y_true[:,i]) * K.log( 1 - y_pred[:,i] + epsilon))) #complete this line\n    return loss","3505e9df":"# Test\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\nsess = tf.compat.v1.keras.backend.get_session()\n# sess = K.get_session()\nwith sess.as_default() as sess:\n    print(\"Test example:\\n\")\n    y_true = K.constant(np.array(\n        [[1, 1, 1],\n         [1, 1, 0],\n         [0, 1, 0],\n         [1, 0, 1]]\n    ))\n    print(\"y_true:\\n\")\n    print(y_true.eval())\n\n    w_p = np.array([0.25, 0.25, 0.5])\n    w_n = np.array([0.75, 0.75, 0.5])\n    print(\"\\nw_p:\\n\")\n    print(w_p)\n\n    print(\"\\nw_n:\\n\")\n    print(w_n)\n\n    y_pred_1 = K.constant(0.7*np.ones(y_true.shape))\n    print(\"\\ny_pred_1:\\n\")\n    print(y_pred_1.eval())\n\n    y_pred_2 = K.constant(0.3*np.ones(y_true.shape))\n    print(\"\\ny_pred_2:\\n\")\n    print(y_pred_2.eval())\n\n    # test with a large epsilon in order to catch errors\n    L = get_weighted_loss(w_p, w_n, epsilon=1)\n\n    print(\"\\nIf we weighted them correctly, we expect the two losses to be the same.\")\n    L1 = L(y_true, y_pred_1).eval()\n    L2 = L(y_true, y_pred_2).eval()\n    print(f\"\\nL(y_pred_1)= {L1:.4f}, L(y_pred_2)= {L2:.4f}\")\n    print(f\"Difference is L1 - L2 = {L1 - L2:.4f}\")","9372f6ab":"from keras.utils.generic_utils import get_custom_objects\n# import SSD_Loss\n\nloss = get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7)\nget_custom_objects().update({\"weighted_loss\": loss})","b9d00a2c":"################################################\n# import tensorflow as tf\n# # from keras import backend as K\n# import keras\n# import numpy as np\n\nmodel = tf.keras.models.load_model(\"..\/input\/final-weights-densenet-epoch-4\/final_1_ep4.h5\")\n","c2d9653f":"from keras.utils import plot_model\nplot_model(model, show_shapes=True, to_file='model_1_densenet_graph.png')","c61135f9":"# sess = tf.compat.v1.keras.backend.get_session()\n# # sess = K.get_session()\n# with sess.as_default() as sess:\n#     print(\"Learning rate before first fit:\", model.optimizer.learning_rate.eval())\n# K.set_value(model.optimizer.learning_rate, 0.00001)","9c372557":"from keras import callbacks\nearlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                        mode =\"min\", verbose=2, patience = 5, \n                                        restore_best_weights = True)","0a216238":"# history = model.fit_generator(train_generator, \n#                               validation_data=valid_generator,callbacks =[earlystopping],\n#                               steps_per_epoch=11250, \n#                               validation_steps=125, \n#                               epochs = 2)\n# model.save(\"final_1_ep4.h5\")\n# print(history.history.keys())\n# #  \"Accuracy\"\n# # plt.plot(history.history['acc'])\n# # plt.plot(history.history['val_acc'])\n# # plt.title('model accuracy')\n# # plt.ylabel('accuracy')\n# # plt.xlabel('epoch')\n# # plt.legend(['train', 'validation'], loc='upper left')\n# # plt.show()\n# # \"Loss\"\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title('model loss')\n# plt.ylabel('loss')\n# plt.xlabel('epoch')\n# plt.legend(['train', 'validation'], loc='upper left')\n# plt.show()","17cf8de7":"predicted_vals = model.predict_generator(test_generator, steps = len(test_generator))","f03776f9":"predicted_vals.shape","1b288f55":"true_val=test_generator.labels\ntrue_val.shape","30f03678":"y=true_val\npred=predicted_vals","bbf37e1c":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\n    average_precision_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\n\n\ndef get_true_pos(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == True) & (y == 1))\n\n\ndef get_true_neg(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == False) & (y == 0))\n\n\ndef get_false_neg(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == False) & (y == 1))\n\n\ndef get_false_pos(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == True) & (y == 0))\n\n\ndef get_performance_metrics(y, pred, class_labels, tp=get_true_pos,\n                            tn=get_true_neg, fp=get_false_pos,\n                            fn=get_false_neg,\n                            acc=None, prevalence=None, spec=None,\n                            sens=None, ppv=None, npv=None, auc=None, f1=None,\n                            thresholds=[]):\n    if len(thresholds) != len(class_labels):\n        thresholds = [.5] * len(class_labels)\n\n    columns = [\"\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n               \"Sensitivity\",\n               \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\", \"Threshold\"]\n    df = pd.DataFrame(columns=columns)\n    for i in range(len(class_labels)):\n        df.loc[i] = [\"\"] + [0] * (len(columns) - 1)\n        df.loc[i][0] = class_labels[i]\n        df.loc[i][1] = round(tp(y[:, i], pred[:, i]),\n                             3) if tp != None else \"Not Defined\"\n        df.loc[i][2] = round(tn(y[:, i], pred[:, i]),\n                             3) if tn != None else \"Not Defined\"\n        df.loc[i][3] = round(fp(y[:, i], pred[:, i]),\n                             3) if fp != None else \"Not Defined\"\n        df.loc[i][4] = round(fn(y[:, i], pred[:, i]),\n                             3) if fn != None else \"Not Defined\"\n        df.loc[i][5] = round(acc(y[:, i], pred[:, i], thresholds[i]),\n                             3) if acc != None else \"Not Defined\"\n        df.loc[i][6] = round(prevalence(y[:, i]),\n                             3) if prevalence != None else \"Not Defined\"\n        df.loc[i][7] = round(sens(y[:, i], pred[:, i], thresholds[i]),\n                             3) if sens != None else \"Not Defined\"\n        df.loc[i][8] = round(spec(y[:, i], pred[:, i], thresholds[i]),\n                             3) if spec != None else \"Not Defined\"\n        df.loc[i][9] = round(ppv(y[:, i], pred[:, i], thresholds[i]),\n                             3) if ppv != None else \"Not Defined\"\n        df.loc[i][10] = round(npv(y[:, i], pred[:, i], thresholds[i]),\n                              3) if npv != None else \"Not Defined\"\n        df.loc[i][11] = round(auc(y[:, i], pred[:, i]),\n                              3) if auc != None else \"Not Defined\"\n        df.loc[i][12] = round(f1(y[:, i], pred[:, i] > thresholds[i]),\n                              3) if f1 != None else \"Not Defined\"\n        df.loc[i][13] = round(thresholds[i], 3)\n\n    df = df.set_index(\"\")\n    return df\n\n\ndef print_confidence_intervals(class_labels, statistics):\n    df = pd.DataFrame(columns=[\"Mean AUC (CI 5%-95%)\"])\n    for i in range(len(class_labels)):\n        mean = statistics.mean(axis=1)[i]\n        max_ = np.quantile(statistics, .95, axis=1)[i]\n        min_ = np.quantile(statistics, .05, axis=1)[i]\n        df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n    return df\n\n\ndef get_curve(gt, pred, target_names, curve='roc'):\n    for i in range(len(target_names)):\n        if curve == 'roc':\n            curve_function = roc_curve\n            auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n            label = target_names[i] + \" AUC: %.3f \" % auc_roc\n            xlabel = \"False positive rate\"\n            ylabel = \"True positive rate\"\n            a, b, _ = curve_function(gt[:, i], pred[:, i])\n            plt.figure(1, figsize=(7, 7))\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.plot(a, b, label=label)\n            plt.xlabel(xlabel)\n            plt.ylabel(ylabel)\n\n            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n                       fancybox=True, ncol=1)\n        elif curve == 'prc':\n            precision, recall, _ = precision_recall_curve(gt[:, i], pred[:, i])\n            average_precision = average_precision_score(gt[:, i], pred[:, i])\n            label = target_names[i] + \" Avg.: %.3f \" % average_precision\n            plt.figure(1, figsize=(7, 7))\n            plt.step(recall, precision, where='post', label=label)\n            plt.xlabel('Recall')\n            plt.ylabel('Precision')\n            plt.ylim([0.0, 1.05])\n            plt.xlim([0.0, 1.0])\n            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n                       fancybox=True, ncol=1)\n        ","74c3a602":"def get_roc_curve(labels, predicted_vals, generator):\n    auc_roc_vals = []\n    for i in range(len(labels)):\n        try:\n            gt = generator.labels[:, i]\n            pred = predicted_vals[:, i]\n            auc_roc = roc_auc_score(gt, pred)\n            auc_roc_vals.append(auc_roc)\n            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n            plt.figure(1, figsize=(10, 10))\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.plot(fpr_rf, tpr_rf,\n                     label=labels[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n            plt.xlabel('False positive rate')\n            plt.ylabel('True positive rate')\n            plt.title('ROC curve')\n            plt.legend(loc='best')\n        except:\n            print(\n                f\"Error in generating ROC curve for {labels[i]}. \"\n                f\"Dataset lacks enough examples.\"\n            )\n    plt.show()\n    return auc_roc_vals","7445319e":"def get_accuracy(y, pred, th=0.5):\n    \"\"\"\n    Compute accuracy of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        accuracy (float): accuracy of predictions at threshold\n    \"\"\"\n    accuracy = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TP, FP, TN, FN using our previously defined functions\n    TP = get_true_pos(y, pred, th)   \n    FP = get_false_pos(y, pred, th)\n    TN = get_true_neg(y, pred, th)\n    FN = get_false_neg(y,pred, th)\n\n    # Compute accuracy using TP, FP, TN, FN\n    accuracy = (TP + TN) \/ ( TP + FP + TN + FN)\n    \n    ### END CODE HERE ###\n    \n    return accuracy\n\n\n\ndef get_prevalence(y):\n    \"\"\"\n    Compute prevalence.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n    Returns:\n        prevalence (float): prevalence of positive cases\n    \"\"\"\n    prevalence = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    prevalence = np.mean(y)\n    \n    ### END CODE HERE ###\n    \n    return prevalence\n\n\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_sensitivity(y, pred, th=0.5):\n    \"\"\"\n    Compute sensitivity of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        sensitivity (float): probability that our test outputs positive given that the case is actually positive\n    \"\"\"\n    sensitivity = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TP and FN using our previously defined functions\n    TP = get_true_pos(y,pred, th)\n    FN = get_false_neg(y, pred, th)\n\n    # use TP and FN to compute sensitivity\n    sensitivity = TP \/ (TP + FN)\n    \n    ### END CODE HERE ###\n    \n    return sensitivity\n\ndef get_specificity(y, pred, th=0.5):\n    \"\"\"\n    Compute specificity of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        specificity (float): probability that the test outputs negative given that the case is actually negative\n    \"\"\"\n    specificity = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TN and FP using our previously defined functions\n    TN = get_true_neg(y,pred, th)\n    FP = get_false_pos(y, pred, th)\n    \n    # use TN and FP to compute specificity \n    specificity = TN \/ (TN + FP)\n    \n    ### END CODE HERE ###\n    \n    return specificity\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_ppv(y, pred, th=0.5):\n    \"\"\"\n    Compute PPV of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        PPV (float): positive predictive value of predictions at threshold\n    \"\"\"\n    PPV = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TP and FP using our previously defined functions\n    TP = get_true_pos(y,pred,th)\n    FP = get_false_pos(y,pred,th)\n\n    # use TP and FP to compute PPV\n    PPV = TP \/ (TP + FP)\n    \n    ### END CODE HERE ###\n    \n    return PPV\n\ndef get_npv(y, pred, th=0.5):\n    \"\"\"\n    Compute NPV of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        NPV (float): negative predictive value of predictions at threshold\n    \"\"\"\n    NPV = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TN and FN using our previously defined functions\n    TN = get_true_neg(y,pred,th)\n    FN = get_false_neg(y,pred,th)\n\n    # use TN and FN to compute NPV\n    NPV = TN \/ (TN + FN)\n    \n    ### END CODE HERE ###\n    \n    return NPV","a746696d":"class_labels=labels","1e706d66":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\ndef bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000):\n    statistics = np.zeros((len(classes), bootstraps))\n\n    for c in range(len(classes)):\n        df = pd.DataFrame(columns=['y', 'pred'])\n        df.loc[:, 'y'] = y[:, c]\n        df.loc[:, 'pred'] = pred[:, c]\n        # get positive examples for stratified sampling\n        df_pos = df[df.y == 1]\n        df_neg = df[df.y == 0]\n        prevalence = len(df_pos) \/ len(df)\n        for i in range(bootstraps):\n            # stratified sampling of positive and negative examples\n            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n\n            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n            score = roc_auc_score(y_sample, pred_sample)\n            statistics[c][i] = score\n    return statistics\n\nstatistics = bootstrap_auc(y, pred, class_labels)","a5318d87":"auc_roc=get_roc_curve(labels, predicted_vals, test_generator)","7a9b5241":"import random\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom tensorflow.compat.v1.logging import INFO, set_verbosity\n\nrandom.seed(a=None, version=2)\n\nset_verbosity(INFO)","e3f36baf":"get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)","aaf21ebf":"print_confidence_intervals(class_labels, statistics)","d1625d1f":"get_curve(y, pred, class_labels, curve='prc')","9c8de139":"def grad_cam(input_model, image, cls, layer_name, H=320, W=320):\n    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n    y_c = input_model.output[0, cls]\n    conv_output = input_model.get_layer(layer_name).output\n    grads = K.gradients(y_c, conv_output)[0]\n\n    gradient_function = K.function([input_model.input], [conv_output, grads])\n\n    output, grads_val = gradient_function([image])\n    output, grads_val = output[0, :], grads_val[0, :, :, :]\n\n    weights = np.mean(grads_val, axis=(0, 1))\n    cam = np.dot(output, weights)\n\n    # Process CAM\n    cam = cv2.resize(cam, (W, H), cv2.INTER_LINEAR)\n    cam = np.maximum(cam, 0)\n    cam = cam \/ cam.max()\n    return cam\n\ndef compute_gradcam(model, img, image_dir, df, labels, selected_labels,\n                    layer_name='bn'):\n    preprocessed_input = load_image(img, image_dir, df)\n    predictions = model.predict(preprocessed_input)\n\n    print(\"Loading original image\")\n    plt.figure(figsize=(15, 10))\n    plt.subplot(151)\n    plt.title(\"Original\")\n    plt.axis('off')\n    plt.imshow(load_image(img, image_dir, df, preprocess=False), cmap='gray')\n\n    j = 1\n    for i in range(len(labels)):\n        if labels[i] in selected_labels:\n            print(f\"Generating gradcam for class {labels[i]}\")\n            gradcam = grad_cam(model, preprocessed_input, i, layer_name)\n            plt.subplot(151 + j)\n            plt.title(f\"{labels[i]}: p={predictions[0][i]:.3f}\")\n            plt.axis('off')\n            plt.imshow(load_image(img, image_dir, df, preprocess=False),\n                       cmap='gray')\n            plt.imshow(gradcam, cmap='jet', alpha=min(0.5, predictions[0][i]))\n            j += 1\n            \ndef load_image(img, image_dir, df, preprocess=True, H=320, W=320):\n    \"\"\"Load and preprocess image.\"\"\"\n    img_path = image_dir + img\n    mean, std = get_mean_std_per_batch(img_path, df, H=H, W=W)\n    x = image.load_img(img_path, target_size=(H, W))\n    if preprocess:\n        x -= mean\n        x \/= std\n        x = np.expand_dims(x, axis=0)\n    return x\n\ndef get_mean_std_per_batch(image_path, df, H=320, W=320):\n    sample_data = []\n    for idx, img in enumerate(df.sample(100)[\"image_id\"].values):\n        # path = image_dir + img\n        sample_data.append(\n            np.array(image.load_img(image_path, target_size=(H, W))))\n\n    mean = np.mean(sample_data[0])\n    std = np.std(sample_data[0])\n    return mean, std","b42a4960":"labels_to_show = np.take(labels, np.argsort(auc_roc)[::-1])[:4]","02c14988":"compute_gradcam(model, 'chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/person1004_bacteria_2935.jpeg','\/kaggle\/input\/', test_df, labels, labels_to_show)","99e22e43":"compute_gradcam(model, 'data\/images_006\/images\/00011559_000.png','\/kaggle\/input\/', test_df, labels, labels_to_show)","ad6e97b5":"compute_gradcam(model, 'data\/images_008\/images\/00016052_014.png','\/kaggle\/input\/', test_df, labels, labels_to_show)","5932b8cc":"compute_gradcam(model, '\/data\/images_008\/images\/00016051_023.png','\/kaggle\/input\/', test_df, labels, labels_to_show)","2199702a":"compute_gradcam(model, 'vinbigdata-original-image-dataset\/vinbigdata\/test\/02d04b6b6883fd92c12a3dde5d2ff6c0.jpg','\/kaggle\/input\/', test_df, labels, labels_to_show)","a1d83659":"the graph of model is included in model pictures folder****","36c658ff":"# LOADING THE CSV FILE WHICH CONTAIN ALL THE INFORMATION ABOUT OUR ALL THREE DATASETS IN MERGED FORM","81059a27":"# diffrent diseases classes****","1b1d3df8":"# FUNCTION FOR TRAIN GENRATOR and data augmentation","d345cdec":"# *THE BELOW KERNEL IS USED FOR DATA PREPROCESSING WHICH INCLUDES MERGING IMAGES FROM DIFFRENT DATASETS TO A SINGLE CSV FILE AND WE ALSO NEED TO CHANGE THE DIRECTORY OF SOME IMAGES TO GERNALIZE OUR CODE.HENCE ALL THE INFORMATION ABOUT RESULTS OF THIS KERNEL IS IN FILE  **final_merged_df.csv** .WHICH IS TAKEN AS INPUT AND MAKE PUBLIC.*\n\n# SO ALL THE CODE OF BELOW KERNEL IS COMMENTED OUT FOR SAVING TIME.","bb290f73":"# SPLITTING DATASET INTO TRAIN TEST VALID BY TAKING CARE OF DATA LEAKING","263413d5":"# FUNCTION FOR VALID AND TEST GENRATOR AND DATA AUGMENTATION","ce0adcc6":"# we have loaded the finally trained model weight and commented out the training process.\n# weights are also made public","3deb5ede":"****we will look at the 4 classes with the highest performing AUC measures.","953f821c":"# Visualizing Learning with GradCAM\nIn this section we will use a GradCAM's technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition.\n\nThis is done by extracting the gradients of each predicted class, flowing into our model's final convolutional layer. \nIt is worth mentioning that GradCAM does not provide a full explanation of the reasoning for each classification probability.\n\nHowever, it is still a useful tool for \"debugging\" our model and augmenting our prediction so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image","d5483f7a":"# GETTING TRAINING\/VALID\/TEST GENERATOR","adee076a":"# confidence_intervals","fae14e68":"> # ROC CURVE","ea3f0473":"# Precision-Recall Curve"}}