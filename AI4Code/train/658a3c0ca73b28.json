{"cell_type":{"b02b037b":"code","52099e63":"code","0443fe2d":"code","a6cc3bb2":"code","fbfe68ed":"code","904c8188":"code","264c6551":"code","f73c138c":"code","771b3653":"code","7a5527eb":"code","b74bd476":"code","de961c6b":"code","1ddbe414":"code","3f6a553f":"code","c1ec4ed2":"code","ba16ee66":"code","259eda97":"code","32aece16":"code","4c0733ea":"code","f1f9b74a":"code","e8d1c0d1":"code","38ef4ea0":"code","21037122":"code","2de136db":"code","88589777":"code","1bfb5041":"code","b518a85d":"code","f1801ffe":"code","da141bdf":"code","6accc805":"code","24d7ab43":"code","a281b910":"code","ae84238d":"code","19255a18":"code","bdaa42dc":"code","99bf8e07":"code","c6051753":"code","a2e33263":"code","4fd0c4f7":"code","c3852141":"code","6f1d348e":"code","eba7853a":"code","fa264914":"code","b6a71ba5":"code","c654cb4e":"code","27f6bffd":"code","fbdfe651":"code","484be9c5":"code","392274a0":"code","4dcdb96a":"code","75d940da":"code","1ae85fa1":"code","bd785ac7":"code","4cffbaa2":"code","530ecdb1":"code","b308419d":"code","e503ddc3":"code","aeb1b89d":"code","c42232bd":"code","ddc9a251":"code","f62edd24":"code","37c6aead":"code","cb5f23af":"code","10a25ab5":"code","56f7c967":"code","ba023a00":"code","9c1d320b":"code","09753b30":"code","fb2fad46":"code","238e5075":"code","8dbf36cf":"code","b6417a90":"code","e0406c3e":"code","2224d900":"code","d298318c":"code","1b064a13":"code","919b7092":"code","c2d71edb":"code","83b51353":"code","f8f1cf7a":"code","2d858c75":"code","444b9cce":"code","bb67b4dd":"code","fa35b73d":"code","94ce8427":"code","ded75574":"code","ea5d59e9":"code","83977a5a":"code","016d515a":"code","7708ac31":"code","d37c42db":"code","3d0dc91b":"code","2041896e":"code","65102834":"code","e52f4848":"code","9c52afd9":"code","0422e095":"code","5f6a95e1":"code","53375196":"code","7ff80f31":"code","573fedf1":"markdown","40b8d034":"markdown","e7a993e4":"markdown","bcfad69b":"markdown","dc5fc51d":"markdown","e79efc55":"markdown","ff0e72f3":"markdown","995e4cbf":"markdown","1d7e8764":"markdown","cad0701e":"markdown","f2328707":"markdown","1bbefa9e":"markdown","4eb3dbea":"markdown","39e951f1":"markdown","58bedfb3":"markdown","9c5301fa":"markdown","dc54365e":"markdown","90df33cb":"markdown","fdef7632":"markdown","4e684aef":"markdown","fb6af3b5":"markdown","e43a813e":"markdown","d74c053a":"markdown","0d9cb175":"markdown","e9196947":"markdown","5b69319c":"markdown","a0f952f1":"markdown","042075c7":"markdown","4f68861c":"markdown","625164bf":"markdown","7808a596":"markdown","7124fb1b":"markdown","e457ce14":"markdown","8693970c":"markdown","791033fd":"markdown","8219f0e2":"markdown","8a9f07d1":"markdown","078ecbff":"markdown","e7fcfd30":"markdown","e645af81":"markdown","2c86f892":"markdown","3a1d955d":"markdown","d5cbe70a":"markdown","abc1be15":"markdown","352d5a83":"markdown","4a37e4ae":"markdown","4858eaf2":"markdown","4ea4883c":"markdown","8ea805f2":"markdown","a9d749c6":"markdown"},"source":{"b02b037b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nplt.rc('axes', edgecolor='black')\nsns.set(rc={'axes.facecolor':'whitesmoke'})\nplt.rcParams['figure.dpi'] = 120","52099e63":"data = pd.read_csv(\"\/kaggle\/input\/hr-data\/HR_comma_sep.csv\")\nprint(f'Dimension of the DataFrame: {data.shape}')\nfor column in data.columns:\n    print(f'Column : {column:25} | Type : {str(data[column].dtype):10}\\\n    | Missing Column : {data[column].isnull().sum():3d}')","0443fe2d":"data.rename(columns={'average_montly_hours': 'average_monthly_hours',\n                     'Work_accident': 'work_accident',\n                     'sales':'department',\n                     'promotion_last_5years':'promotion_last_5_years'},\n            inplace=True)\ndata.head()","a6cc3bb2":"data['number_project'].value_counts()","fbfe68ed":"plt.figure(figsize=(9,5))\ndata['number_project'].value_counts()\\\n            .plot(kind='barh', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='horizontal')\nplt.show()","904c8188":"data['time_spend_company'].value_counts()","264c6551":"plt.figure(figsize=(9,5))\ndata['time_spend_company'].value_counts()\\\n            .plot(kind='barh', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='horizontal')\nplt.show()","f73c138c":"data['left'].value_counts()","771b3653":"left_encoded = {0: 'no', 1:'yes'}\ndata['left'] = data['left'].apply(lambda x: left_encoded[x])","7a5527eb":"plt.figure(figsize=(9,5))\ndata['left'].value_counts()\\\n            .plot(kind='bar', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='horizontal')\nplt.show()","b74bd476":"data['work_accident'].value_counts()","de961c6b":"accident_encoded = {0: 'no', 1:'yes'}\ndata['work_accident'] = data['work_accident']\\\n.apply(lambda x: accident_encoded[x])","1ddbe414":"plt.figure(figsize=(9,5))\ndata['work_accident'].value_counts()\\\n            .plot(kind='bar', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='horizontal')\nplt.show()","3f6a553f":"data['promotion_last_5_years'].value_counts()","c1ec4ed2":"promotion_encoded = {0:'no', 1:'yes'}\ndata['promotion_last_5_years'] = data['promotion_last_5_years']\\\n.apply(lambda x: promotion_encoded[x])","ba16ee66":"plt.figure(figsize=(9,5))\ndata['promotion_last_5_years'].value_counts()\\\n            .plot(kind='bar', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='horizontal')\nprint('Percentage of Promotion given:\\n', \n    data['promotion_last_5_years']\\\n        .value_counts()['yes'] \/ len(data) )\nplt.show()","259eda97":"data['department'].value_counts()","32aece16":"plt.figure(figsize=(9,5))\ndata['department'].value_counts()\\\n            .plot(kind='bar', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='vertical')\nplt.show()","4c0733ea":"data['salary'].value_counts()","f1f9b74a":"plt.figure(figsize=(9,5))\ndata['salary'].value_counts()\\\n            .plot(kind='bar', grid=False, \n                  color='#28282b', edgecolor='#28282b')\nplt.xticks(rotation='vertical')\nplt.show()","e8d1c0d1":"data.describe()","38ef4ea0":"import plotly.express as px","21037122":"fig = px.scatter(data, x='average_monthly_hours',\n        y='satisfaction_level', color='left',\n            color_discrete_sequence=['#28282b','#8c939c'],\n            trendline='ols', facet_col='left',\n            title='Relationship Trend:Satisfaction level vs Average Monthly Hour')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.show()","2de136db":"fig = px.scatter(data, x='satisfaction_level',\n                 y='last_evaluation',opacity=0.4,\n                 color_discrete_sequence=['#28282b'],\n                 marginal_x='box',marginal_y='violin',\n                 title='Relationship Density:Evaluation vs Satisfaction')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_traces(marker={'size':9})\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.show()","88589777":"fig = px.scatter(data, x='satisfaction_level',\n                 y='average_monthly_hours',opacity=0.4,\n                 color_discrete_sequence=['#28282b'],\n                 marginal_x='box',marginal_y='violin',\n                 title='Relationship Density:Average Monthly Hours vs Satisfaction')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_traces(marker={'size':9})\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.show()","1bfb5041":"fig = px.scatter(data, x='average_monthly_hours',\n                 y='last_evaluation',opacity=0.4,\n                 color_discrete_sequence=['#28282b'],\n                 marginal_x='box',marginal_y='violin',\n                 title='Relationship Density:Evaluation vs Average Monthly Hours')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_traces(marker={'size':9})\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False)\nfig.show()","b518a85d":"'''\nDefining the percentiles required for understanding the distribution \nmore in depth. Emperical Cumulative Distribution function plot\nfor satisfaction levels.\n'''\npercentiles = np.array([25,50,75])\nranges = np.percentile(data['satisfaction_level'], percentiles)\nx = np.sort(data['satisfaction_level'])\ny = np.arange(1, len(x) + 1) \/ len(x)\n_ = plt.plot(x,y,marker='_', linestyle='none', markersize=5)\n_ = plt.plot(ranges, percentiles \/ 100, marker='o', \n             color='red', linestyle='none')  \nplt.xlabel('Satisfaction Levels', fontsize=10)\nplt.ylabel('Emperical Cumulative Distribution Functions',\n           fontsize=10)\nplt.grid(False)\nplt.title('ECDF Plot of Satisfaction Levels', fontsize=12)\nplt.show()","f1801ffe":"'''\nECDF plot for last evaluation scores\n'''\nranges = np.percentile(data['last_evaluation'], percentiles)\nx = np.sort(data['last_evaluation'])\ny = np.arange(1, len(x) + 1) \/ len(x)\n_ = plt.plot(x, y, marker='_', linestyle='none', markersize=5)\n_ = plt.plot(ranges, percentiles \/ 100, marker='o',\n             color='red', linestyle='none')\nplt.grid(False)\nplt.xlabel('Last evaluation scores', fontsize=10)\nplt.ylabel('Emperical Cumulative Distribution Function',\n           fontsize=10)\nplt.title('ECDF Plot of Last evaluation scores',fontsize=12)\nplt.show()","da141bdf":"'''\nECDF plot for average monthly working hours.\n'''\nranges = np.percentile(data['average_monthly_hours'], percentiles)\nx = np.sort(data['average_monthly_hours'])\ny = np.arange(1, len(x) + 1) \/ len(x)\n_ = plt.plot(x, y, marker='_', linestyle='none',markersize=4)\n_ = plt.plot(ranges, percentiles \/ 100, marker='o',\n             color='red', linestyle='none')\nplt.grid(False)\nplt.xlabel('Average Monthly Working Hours', fontsize=10)\nplt.ylabel('Emperical Cumulative Distribution Function',\n           fontsize=10)\nplt.title('ECDF Plot of Average monthly working hours',\n          fontsize=12)\nplt.show()","6accc805":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data=data,x='salary',\n               y='satisfaction_level',\n            palette=['#28282b','#686c73','#c2c8cf',])\nplt.show()","24d7ab43":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data=data,x='salary',\n               y='number_project',\n            palette=['#28282b','#686c73','#c2c8cf',])\nplt.show()","a281b910":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data=data,x='salary',\n               y='last_evaluation',\n            palette=['#28282b','#686c73','#c2c8cf',])\nplt.show()","ae84238d":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data=data,x='salary',\n               y='average_monthly_hours',\n            palette=['#28282b','#686c73','#c2c8cf',])\nplt.show()","19255a18":"fig = px.histogram(data, x='department', color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","bdaa42dc":"fig = px.histogram(data, x='work_accident', color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","99bf8e07":"fig = px.histogram(data, x='promotion_last_5_years',\n                   color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","c6051753":"fig = px.histogram(data, x='average_monthly_hours', color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],opacity=0.9,\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","a2e33263":"fig = px.histogram(data, x='time_spend_company', color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","4fd0c4f7":"fig = px.histogram(data, x='salary', color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","c3852141":"fig = px.histogram(data, x='number_project', color='left',\n                color_discrete_sequence=['#28282b','#8c939c'],\n                title='Distribution of Employees Leaving in Each Department')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","6f1d348e":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data= data, x='department', \n               y='average_monthly_hours', \n               palette=['#676b73'])\nplt.xticks(fontsize=12, rotation='vertical')\nplt.show()","eba7853a":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data= data, x='department', \n               y='satisfaction_level', \n               palette=['#676b73'])\nplt.xticks(fontsize=12, rotation='vertical')\nplt.show()","fa264914":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data= data, x='department', \n               y='last_evaluation', \n               palette=['#676b73'])\nplt.xticks(fontsize=12, rotation='vertical')\nplt.show()","b6a71ba5":"plt.figure(figsize=(9,5))\nplt.grid(False)\nsns.violinplot(data= data, x='department', \n               y='time_spend_company', \n               palette=['#676b73'])\nplt.xticks(fontsize=12, rotation='vertical')\nplt.show()","c654cb4e":"fig = px.box(data, x='left', y='satisfaction_level',\n             color_discrete_sequence=['#28282b'], \n             title='Satisfaction of Employees Leaving vs Staying')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","27f6bffd":"fig = px.box(data, x='left', y='last_evaluation',\n             color_discrete_sequence=['#28282b'], \n             title='Evaluation of Employees Leaving vs Staying')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","fbdfe651":"fig = px.box(data, x='left', y='average_monthly_hours',\n             color_discrete_sequence=['#28282b'], \n             title='Evaluation of Employees Leaving vs Staying')\nfig.update_layout(plot_bgcolor='whitesmoke')\nfig.update_yaxes(showgrid=False)\nfig.show()","484be9c5":"'''\nDefining a function to compute the ECDF.\n'''\ndef ecdf(value):\n    n= len(value)\n    x= np.sort(value)\n    y= np.arange(1, n+1) \/ n\n    return x,y\n","392274a0":"''' \nComparing theoretical and actual distribution of \nsatisfaction levels.\n'''\nmean = np.mean(data['satisfaction_level'])\nstd = np.std(data['satisfaction_level'])\nsamples = np.random.normal(mean, std, size=10000)\nx, y = ecdf(data['satisfaction_level'])\nx_theor, y_theor = ecdf(samples)\nplt.figure(figsize=(9,5))\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x,y, marker='_', linestyle='none', markersize=2, \n             color='#28282b')\nplt.grid(False)\nplt.xlabel('Satisfaction Levels', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()","4dcdb96a":"mean = np.mean(data['satisfaction_level'])\nstd = np.std(data['satisfaction_level'])\nsamples = np.random.exponential(mean,size=10000)\nx, y = ecdf(data['satisfaction_level'])\nx_theor, y_theor = ecdf(samples)\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x,y, marker='_', linestyle='none', markersize=2, \n             color='#28282b')\nplt.grid(False)\nplt.xlabel('Satisfaction Levels', fontsize=10)\nplt.ylabel('ECDF', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()","75d940da":"mean = np.mean(data['average_monthly_hours'])\nstd = np.std(data['average_monthly_hours'])\nsamples = np.random.normal(mean,std,size=10000)\nx, y = ecdf(data['average_monthly_hours'])\nx_theor, y_theor = ecdf(samples)\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x,y, marker='_', linestyle='none', markersize=2, \n             color='#28282b')\nplt.grid(False)\nplt.xlabel('Average Monthly Hours', fontsize=10)\nplt.ylabel('ECDF', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n","1ae85fa1":"samples = np.random.exponential(mean,size=10000)\nx, y = ecdf(data['average_monthly_hours'])\nx_theor, y_theor = ecdf(samples)\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x,y, marker='_', linestyle='none', markersize=2, \n             color='#28282b')\nplt.grid(False)\nplt.xlabel('Average Monthly Hours', fontsize=10)\nplt.ylabel('ECDF', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n","bd785ac7":"mean = np.mean(data['last_evaluation'])\nstd = np.std(data['last_evaluation'])\nsamples = np.random.normal(mean,std,size=10000)\nx, y = ecdf(data['last_evaluation'])\nx_theor, y_theor = ecdf(samples)\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x,y, marker='_', linestyle='none', markersize=2, \n             color='#28282b')\nplt.grid(False)\nplt.xlabel('Last Evaluation', fontsize=10)\nplt.ylabel('ECDF', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n","4cffbaa2":"samples = np.random.exponential(mean,size=10000)\nx, y = ecdf(data['last_evaluation'])\nx_theor, y_theor = ecdf(samples)\n_ = plt.plot(x_theor, y_theor)\n_ = plt.plot(x,y, marker='_', linestyle='none', markersize=2, \n             color='#28282b')\nplt.grid(False)\nplt.xlabel('Last Evaluation', fontsize=10)\nplt.ylabel('ECDF', fontsize=10)\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=8)\nplt.show()\n","530ecdb1":"from scipy.stats import ttest_ind\nfrom scipy.stats import ks_2samp\nhours_col = 'average_monthly_hours'\nleft_mask = data['left'] =='yes'\nhours_left = data.loc[left_mask, hours_col]\nhours_staying = data.loc[~ left_mask, hours_col]\nleft_test = ttest_ind(hours_left, hours_staying)\nprint('Hypothesis Test')\nprint('-'*40)\nprint(f'Statistic Value: {left_test[0]:.5f}, \\\n\\nP-Value: {left_test[1]:.5f}')\nprint('-'* 40)\nks_left = ks_2samp(hours_left, hours_staying)\nprint('Kolmogorov-Smirnov Test')\nprint('-'*40)\nprint(f'Statistic Value: {ks_left[0]:.5f}\\\n      \\nP-Value: {ks_left[1]:.5f}')","b308419d":"project_col = 'number_project'\nleft_mask = data['left'] =='yes'\nproject_left = data.loc[left_mask, project_col]\nproject_staying = data.loc[~ left_mask, project_col]\nleft_test = ttest_ind(project_left, project_staying)\nprint('Hypothesis Test')\nprint('-'*40)\nprint(f'Statistic Value: {left_test[0]:.5f}, \\\n\\nP-Value: {left_test[1]:.5f}')\nprint('-'* 40)\nks_left = ks_2samp(project_left, project_staying)\nprint('Kolmogorov-Smirnov Test')\nprint('-'*40)\nprint(f'Statistic Value: {ks_left[0]:.5f}\\\n      \\nP-Value: {ks_left[1]:.5f}')","e503ddc3":"time_col = 'time_spend_company'\nleft_mask = data['left'] =='yes'\ntime_left = data.loc[left_mask, time_col]\ntime_staying = data.loc[~ left_mask, time_col]\nleft_test = ttest_ind(time_left, time_staying)\nprint('Hypothesis Test')\nprint('-'*40)\nprint(f'Statistic Value: {left_test[0]:.5f}, \\\n\\nP-Value: {left_test[1]:.5f}')\nprint('-'* 40)\nks_left = ks_2samp(time_left, time_staying)\nprint('Kolmogorov-Smirnov Test')\nprint('-'*40)\nprint(f'Statistic Value: {ks_left[0]:.5f}\\\n      \\nP-Value: {ks_left[1]:.5f}')","aeb1b89d":"satisfaction_col = 'satisfaction_level'\nleft_mask = data['left'] =='yes'\nsatisfaction_left = data.loc[left_mask, satisfaction_col]\nsatisfaction_staying = data.loc[~ left_mask, satisfaction_col]\nleft_test = ttest_ind(satisfaction_left, satisfaction_staying)\nprint('Hypothesis Test')\nprint('-'*40)\nprint(f'Statistic Value: {left_test[0]:.5f}, \\\n\\nP-Value: {left_test[1]:.5f}')\nprint('-'* 40)\nks_left = ks_2samp(satisfaction_left, satisfaction_staying)\nprint('Kolmogorov-Smirnov Test')\nprint('-'*40)\nprint(f'Statistic Value: {ks_left[0]:.5f}\\\n      \\nP-Value: {ks_left[1]:.5f}')","c42232bd":"eval_col = 'last_evaluation'\nleft_mask = data['left'] =='yes'\neval_left = data.loc[left_mask, eval_col]\neval_staying = data.loc[~ left_mask, eval_col]\nleft_test = ttest_ind(eval_left, eval_staying)\nprint('Hypothesis Test')\nprint('-'*40)\nprint(f'Statistic Value: {left_test[0]:.5f}, \\\n\\nP-Value: {left_test[1]:.5f}')\nprint('-'* 40)\nks_left = ks_2samp(eval_left, eval_staying)\nprint('Kolmogorov-Smirnov Test')\nprint('-'*40)\nprint(f'Statistic Value: {ks_left[0]:.5f}\\\n      \\nP-Value: {ks_left[1]:.5f}')","ddc9a251":"'''\ncomputing the pearson coefficient correlation.\n'''\ndef pearson_r(x,y):\n    corr = np.corrcoef(x,y)\n    return corr[0,1]\n\n'''\nComputing  the pearso correaltion coefficient between \nsatisfaction levels and last evaluation.\n'''\nr = pearson_r(data['satisfaction_level'],data['last_evaluation'])\nprint('Correaltion Coeffifcient of \\nSatisfaction Level and Last Evaluation score:', r)","f62edd24":"'''\nComputing the pearson correaltion coefficient between \nSatisfaction Level and Average Monthly Hours.\n'''\nr = pearson_r(data['satisfaction_level'], data['average_monthly_hours'])\nprint('Correlation Coefficient of \\nSatisfaction Levels and Average Monthly Hours:', r)","37c6aead":"'''\nComputing the pearson correlation coefficient between \nLast Evaluation and Average Monthly Hours.\n'''\nr = pearson_r(data['last_evaluation'], data['average_monthly_hours'])\nprint('Correlation Coefficient of \\nAverage Monthly Hours and Last Evaluation:',r)\n","cb5f23af":"data['salary'] = data['salary'].astype('category').cat.codes\ndata['left'] = data['left'].astype('category').cat.codes\ndata['department'] = data['department'].astype('category').cat.codes\ndata['left'] = data['left'].astype('category').cat.codes\ndata['promotion_last_5_years'] = data['promotion_last_5_years'].astype('category').cat.codes\ndata['work_accident'] = data['work_accident'].astype('category').cat.codes\ndata = data.drop_duplicates()","10a25ab5":"from sklearn.model_selection import train_test_split\nfeatures = ['satisfaction_level', 'last_evaluation']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.3, random_state=1)","56f7c967":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","ba023a00":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_decision_regions\n\nsvm = SVC(kernel='linear', random_state=1, C=1, gamma='scale')\nsvm.fit(scaled_X_train, y_train)\npredictions = svm.predict(scaled_X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)\nplt.figure(figsize=(9,4))\nsamples = 200\nplt.xticks(fontsize=5)\nplt.yticks(fontsize=5)\nX,y = scaled_X_train[:samples], y_train[:samples]\nplot_decision_regions(X,y, clf=svm)\nplt.show()","9c1d320b":"svm = SVC(kernel='rbf', C=1, gamma='scale', random_state=1)\nsvm.fit(scaled_X_train,y_train)\npredictions = svm.predict(scaled_X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)\nplt.figure(figsize=(9,4))\nsamples = 200\nplt.xticks(fontsize=5)\nplt.yticks(fontsize=5)\nX,y = scaled_X_train[:samples], y_train[:samples]\nplot_decision_regions(X,y, clf=svm)\nplt.show()","09753b30":"features = ['satisfaction_level', 'last_evaluation',\n            'number_project','average_monthly_hours',\n            'time_spend_company', 'work_accident',\n            'promotion_last_5_years','department','salary']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.3, random_state=1)\n\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)\n","fb2fad46":"svm = SVC(kernel='rbf', C=50, gamma='scale', random_state=1)\nsvm.fit(scaled_X_train,y_train)\npredictions = svm.predict(scaled_X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)","238e5075":"features = ['satisfaction_level', 'last_evaluation']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.3, random_state=1)\n\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)\n","8dbf36cf":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(scaled_X_train, y_train)\npredictions = knn.predict(scaled_X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)\nplt.figure(figsize=(9,6))\nsamples = 200\nplt.xticks(fontsize=5)\nplt.yticks(fontsize=5)\nX,y = scaled_X_train[:samples], y_train[:samples]\nplot_decision_regions(X,y, clf=knn)\nplt.show()","b6417a90":"knn = KNeighborsClassifier(n_neighbors=25)\nknn.fit(scaled_X_train, y_train)\npredictions = knn.predict(scaled_X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)\nplt.figure(figsize=(9,6))\nsamples = 200\nplt.xticks(fontsize=5)\nplt.yticks(fontsize=5)\nX,y = scaled_X_train[:samples], y_train[:samples]\nplot_decision_regions(X,y, clf=knn)\nplt.show()","e0406c3e":"features = ['satisfaction_level', 'last_evaluation',\n            'number_project','average_monthly_hours',\n            'time_spend_company', 'work_accident',\n            'promotion_last_5_years','department','salary']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.3, random_state=1)\n\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)\n","2224d900":"from sklearn.model_selection import GridSearchCV\nknn = KNeighborsClassifier(n_neighbors=25)\nparam_grid = {'n_neighbors':np.arange(3,30)}\nknn_cv = GridSearchCV(knn, param_grid, cv=5)\nknn_cv.fit(scaled_X_train,y_train)\nknn_cv.best_params_","d298318c":"\nknn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(scaled_X_train, y_train)\npredictions = knn.predict(scaled_X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)","1b064a13":"features = ['satisfaction_level', 'last_evaluation']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.3, random_state=1)\n\nscaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)\n","919b7092":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=50,\n                        max_depth=5, random_state=1)\nforest.fit(X_train,y_train)\npredictions = forest.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)\nplt.figure(figsize=(9,6))\nsamples = 200\nplt.xticks(fontsize=5)\nplt.yticks(fontsize=5)\nX,y = scaled_X_train[:samples], y_train[:samples]\nplot_decision_regions(X,y, clf=forest)\nplt.show()","c2d71edb":"from sklearn.tree import export_graphviz\nimport graphviz\n\ndot_data = export_graphviz(forest.estimators_[0],\n                           out_file=None,\n                           feature_names=features,\n                           class_names=['no','yes'],\n                           filled=True, rounded=True,\n                           special_characters=True)\ngraph= graphviz.Source(dot_data)\ngraph","83b51353":"features = ['satisfaction_level', 'last_evaluation',\n            'number_project','average_monthly_hours',\n            'time_spend_company', 'work_accident',\n            'promotion_last_5_years','department','salary']\nX_train, X_test, y_train, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.3, random_state=1)","f8f1cf7a":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=120,criterion='entropy',\n                        min_samples_split=15,max_depth=5,\n                        min_samples_leaf=10,random_state=1)\nforest.fit(X_train,y_train)\npredictions = forest.predict(X_test)\nscore = accuracy_score(y_test, predictions)\nprint(f'Accuracy Score: {score*100:.1f}%')\nprint('-'*40)\ncon_mat = confusion_matrix(y_test, predictions)\nprint(f'Confusion Matrix:\\n{con_mat}')\nprint('-'*40)\nprint('Percent Of Accuracy Score per Class')\ncon_mat = confusion_matrix(y_test,predictions)\nscores = con_mat.diagonal() \/ con_mat.sum(axis=1) * 100\nprint(f'Left = 0 : {scores[0]:.2f}%')\nprint(f'Left = 1 : {scores[1]:.2f}%')\nprint('-'*40)","2d858c75":"dot_data = export_graphviz(forest.estimators_[0],\n                           out_file=None,\n                           feature_names=features,\n                           class_names=['no','yes'],\n                           filled=True, rounded=True,\n                           special_characters=True)\ngraph= graphviz.Source(dot_data)\ngraph","444b9cce":"features = ['satisfaction_level', 'last_evaluation']\nX, X_test, y, y_test = train_test_split(\n    data[features].values, data['left'].values,\n    test_size=0.15, random_state=1)","bb67b4dd":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = DecisionTreeClassifier(max_depth=5)\nnp.random.seed(1)\nscores = cross_val_score(estimator=clf, X=X_train,\n                         y=y_train, cv=10)\nprint('Accuracy = {:.3f} +\/- {:.3f}'.format(scores.mean(),\n                                            scores.std()))\nprint(scores)","fa35b73d":"from sklearn.model_selection import StratifiedKFold\n\ndef cross_val_score(clf, X, y, cv=10):\n    kfold = (StratifiedKFold(n_splits=cv).split(X,y))\n    class_accuracy = []\n    for k, (train, test) in enumerate(kfold):\n        clf.fit(X[train], y[train])\n        y_test= y[test]\n        predictions= clf.predict(X[test])\n        cmat = confusion_matrix(y_test, predictions)\n        class_acc = cmat.diagonal()\/ cmat.sum(axis=1)\n        class_accuracy.append(class_acc)\n        print('Fold: {:d} Accuracy: {:s}'.format(k+1,\n                                                 str(class_acc)))\n    return np.array(class_accuracy)","94ce8427":"np.random.seed(1)\nscores = cross_val_score(clf,X,y)\nprint('accuracy = {} +\/- {}'. format(scores.mean(axis=0),\n                                     scores.std(axis=0)))","ded75574":"df = data.copy()\ndata.info()","ea5d59e9":"df = pd.get_dummies(data_preprocessed, drop_first=True)\ndf.columns","83977a5a":"df.head()","016d515a":"features = ['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_monthly_hours', 'time_spend_company', 'work_accident',\n       'promotion_last_5_years', 'department_RandD', 'department_accounting',\n       'department_hr', 'department_management', 'department_marketing',\n       'department_product_mng', 'department_sales', 'department_support',\n       'department_technical', 'salary_low', 'salary_medium']\nX = df[features].values\ny = df['left'].values\ntree = DecisionTreeClassifier(max_depth=10)\ntree.fit(X,y)\n\n(pd.Series(tree.feature_importances_, name='Feature Importance',\n           index= df[features].columns).sort_values().plot(kind='barh', grid=False,\n                                                figsize=(9,5),color='#28282b'))\nplt.xlabel('Feature Importance')\nplt.yticks(fontsize=9)\nplt.show()\n","7708ac31":"importances = list(pd.Series(tree.feature_importances_,\n            index=df[features].columns.sort_values(ascending=False)))\nlow_important_features = importances[5:]\nhgih_important_features = importances[:5]\n","d37c42db":"np.array(low_important_features)","3d0dc91b":"np.array(hgih_important_features)","2041896e":"from sklearn.decomposition import PCA\n\npca_features = ['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_monthly_hours', 'time_spend_company', 'work_accident', 'left',\n       'promotion_last_5_years', 'department_RandD', 'department_accounting',\n       'department_hr', 'department_management', 'department_marketing',\n       'department_product_mng', 'department_sales', 'department_support',\n       'department_technical', 'salary_low', 'salary_medium']\n\nX_reduce = df[pca_features] \npca = PCA(n_components=3)\npca.fit(X_reduce)\nX_pca = pca.transform(X_reduce)\nprint('Shape of PCA - X:',X_pca.shape)","65102834":"df_final = df.copy()","e52f4848":"df_final['first_principle_component'] = X_pca.T[0]\ndf_final['second_principle_components'] = X_pca.T[1]\ndf_final['third_principle_components'] = X_pca.T[2]","9c52afd9":"df_final.columns","0422e095":"features = ['satisfaction_level', 'last_evaluation', \n             'time_spend_company','number_project',\n             'average_monthly_hours', 'first_principle_component',\n             'second_principle_components','third_principle_components',]\nX, X_test, y, y_test= train_test_split(\n    df_final[features].values, df_final['left'].values,\n    test_size=0.15, random_state=1)    ","5f6a95e1":"tree = DecisionTreeClassifier(max_depth=8)\nnp.random.seed(1)\nscores = cross_val_score(tree, X,y)\nprint('Accuracy = {} +\/- {}'.format(scores.mean(axis=0),\n                                    scores.std(axis=0)))","53375196":"fig = plt.figure(figsize=(9,7))\nplt.grid(False)\nsns.boxplot(data=pd.DataFrame(scores, columns=[0,1]),\n            palette=['#28282b', '#ccd1d9'])\nplt.xlabel('Left(0= \"no\", 1=\"yes\"')\nplt.ylabel=('Accuracy')\nplt.show()","7ff80f31":"tree = DecisionTreeClassifier(max_depth=8)\ntree.fit(X,y)\npredictions = tree.predict(X_test)\ncon_mat = confusion_matrix(y_test, predictions)\ncon_mat.diagonal()\/ con_mat.sum(axis=1) *100","573fedf1":"## 5.2: Dimensionality Reductuion - Principal Component Analysis(PCA):","40b8d034":"## 1.1: Data Exploration:\n***","e7a993e4":"## 3.1: Emperical Cummulative Distribution Functions:\n","bcfad69b":"In the histogram above, for the employees who left the firm we can observe:\n- The maximum number of employees who left the firm were those who got a mid level salary\n- The least number of employees who left the firm were those who got a low level salary.\n***","dc5fc51d":"In the plot above we have compared the theoretical distiribution (blue) of the satisfaction levels with the actual distribution (green) obtained from the data to check if it follows a exponential distribution.\n\nFrom the plot above we can say:\n- Satisfaction levels are not exponentially distributed at all.\n***","e79efc55":"# 4: Machine Learning:\n","ff0e72f3":"|Tags| Purpose|\n|------| -----------| \n| Deliverables |Analysis, Insights, Data Distribution, Machine Learning Model|\n|Libraries| Pandas, Matplotlib, Seaborn, Plotly, Scipy, Support Vector Machine, K-Nearest Neighbor, Random Forest Classifier, K-Fold Cross validation, Principal Component Analysis, Decision Tree Classifier.|\n|Topics| Data Analysis, Data Visualisation, Null Hypothesis, Clustering, Classification, Dimensionality Reduction.","995e4cbf":"In the plot above we have compared the theoretical distiribution (blue) of the average monthly hours with the actual distribution (green) obtained from the data to check if it follows a exponential distribution.\n\nFrom the plot above we can say:\n- Satisfaction levels are not exponentially distributed at all.\n***","1d7e8764":"Here, you can see an increase in overall accuracy, which now scores over 90%, and a significant improvement for class 1 in particular, which scores around 96%. However, the decision region plot indicates that we are overfitting the data. This is evident by the hard,  choppy  decision boundary, and small pockets of class 1 prediction ranges (the orange contours) scattered throughout the feature space. You can soften the decision boundary and decrease overfitting by increasing the number of nearest neighbors used to make classifications.\n***","cad0701e":"## 5.1: Tuning Hyperparameter with K-Fold Cross Validation:","f2328707":"As shown in the preceding bar plot, there are a handful of features that are of significant importance when it comes to making predictions, and the rest appear to have near-zero importance. Keep in mind, however, that this chart does not represent the true feature importance, but simply that of the quick and dirty decision tree model, clf. In other words, the features with near-zero importance in the preceding chart may be more important for other models. In any case, the information here is sufficient for us to make a selection on which features to reduce with PCA.\n***","1bbefa9e":"In the histogram above for the employees who left the firm we can observe:\n- The maximum number of employees who left the firm only did 2 projects for the firm.\n- The least number of employees who left the firm did 3 projects for the firm.\n***","4eb3dbea":"In the boxplot we can observe:\n- The median last evaluation scores of the employees who left the firm were indeed greater than the employees who stayed suggesting that the good employees did indeed leave the firm.\n***","39e951f1":"From the plot above we can infer that most of the employees who between 130 to 150 hours amonth have correspondingly low last evaluation scores between 0.45 to 0.55. This may tell us the that the organisation values the time put in by the employee over the quality of work produced which might be why the good employees have left the firm.\n\n***","58bedfb3":"In the violin plot above we can observe how the distribution of average monthly working hours is distributed equally throughout all the departments indicating that all departments worked almost the same number of hours every month.\n***","9c5301fa":"# 3: Statistical Modelling:\nThe aim of statistical modelling is to verify the authencity of the quality of the data that we have got from the HR department.","dc54365e":"# 1: Human Resource Analytics - Case Study:","90df33cb":"These test accuracies should fall within or very close to the range of the k-fold cross validation accuracies we calculated previously. For class 0, you can see 99.3%, which falls within the k-fold range of 99.2% \u2013 99.6%, and for class 1, you can see 94.4%, which falls just above the k-fold range of 91.0% \u2013 93.8%. These are good results, which give you confidence that your model will perform well  in production.\n***","fdef7632":"In the histogram above, for the employees who left the firm we can observe:\n- The maximum number of employees that left the firm were those who had really low workinghours (Lesser than 160) and those who had higher working hours (Between 230 to 270).\n- Employees stayed with the firm when the working hours were between 160 to 220 every month.\n***","4e684aef":"The scatter density plot above shows us that the bulk of the employees either had low satisfaction levels (0.35 to 0.45) and correspondly low working hours (125 to 160 hours). They also have high satisfaction levels (0.7 to 0.9) and correspondly high average working hours (220 to 260) hours. This shows us the employees in the company do not lead very balanced lives. They either work too much or too little and have really high or really low satisfaction levels.\n***","fb6af3b5":"# Data Visualisation Key Takeaways:\n\nFrom the visualizations carried above we have come to understand the following about the employees who left the firm:\n- The employees who left the firm did indeed have a better performance than the employees who stayed with the firm.\n- The employees who left the firm had lower satisfaction levels\nThe employees who left the firm were made to work longer hours monthly.\n- The management department stayed the longest number of years with the firm suggesting that the employees in the other departments could have had issues with the management.\n- Employees who were given lesser projects were more likely to leave the firm.\n-Employees having a mid level salary were more likely to leave the firm.\n- Employees who worked really less number of hours or worked a high number of hours were more likely to leave the firm.\n- Employee were more likely to leave the firm after working for 3 years.\n***","e43a813e":"in the boxplot above we can notice:\n- That the median number of hours worked by the employees who left the firm were higher than the median hours worked by the employees who stayed with the firm suggesting that they could be overworked.\n***","d74c053a":"The scatter density plot above shows us a lot of employees in the firm have low satisfaction levels in the range of 0.35 to 0.45. These employees also have lower scores in their last evaluation. This shows us that the people who are not satisfied do not perform well on their jobs in the firm.\n***","0d9cb175":"In the plot above we have compared the theoretical distiribution (blue) of the Last evaluation scores with the actual distribution (green) obtained from the data to check if it follows a exponential distribution.\n\nFrom the plot above we can say:\n- Last evaluation scores are not exponentially distributed at all.\n***","e9196947":"From the ECDF plot for satisfaction levels we can understand that the mean satisfaction levels in the firm are slightly above average(0.67). We can also observe that the lower 25th percentile of satisfaction levels are not that low (0.45). Employees seem pretty satisfied in the firm.\n\nThe last evaluation scores in the firm are also quite high and all above average for the 25th, 50th and 75th percentiles.\n\nThe employees in the firm worked 200 hours every month on average with the ECDF plot following a normal distribution.\n***","5b69319c":"# 5: Model Optimization - Hyperparameter Tuning:","a0f952f1":"We can also notice how the Satisfaction levels and the last evaluation scores are distributed equally across all departments in the firm.\n***","042075c7":"## 3.2: Hypothesis Testing & Kolmogorov- Smirnov Testing:","4f68861c":"While looking at the decision boundary chart, notice the distinctive axes-parallel decision boundaries produced by decision tree machine learning algorithms. We can access any of the individual decision trees used to build the Random Forest. These trees are stored in the estimators_ attribute of the model. Let's draw one of these decision trees to get a feel for what's going on.\n***","625164bf":"In the plot above we have compared the theoretical distiribution (blue) of the satisdaction levels with the actual distribution (green) obtained from the data to check if it follows a normal distribution.\n\nFrom the plot above we can say:\n- The satisfaction levels are not EXACTLY normally distributed but is close to being normally distributed.\n***","7808a596":"## 1.2: Understadning The Data Distribution:\n***","7124fb1b":"This result is much better. We can see an overall accuracy of nearly 90%, where the class 1 accuracy is now 64%, compared to 0% with the linear SVM! We were able to capture the non-linear patterns in the data and correctly classify the majority of the employees who have left.","e457ce14":"In the plot above we have compared the theoretical distiribution (blue) of the average monthly hours with the actual distribution (green) obtained from the data to check if it follows a normal distribution.\nFrom the plot above we can say:\nThe satisfaction levels are not EXACTLY normally distributed but is close to being normally distributed.\n***","8693970c":"# 6: Training Production Ready Model :","791033fd":"## 4.1: Support Vector Machine:","8219f0e2":"Using  cross_val_score  is a convenient way to accomplish k-fold cross validation, but it doesn't specify about the accuracies within each class. Since the problem at hand is sensitive to each class' accuracy, it will be needed to manually implement k-fold cross validation so that this information is available. In particular, we are interested in the accuracy of class 1, which represents the employees who   have left. \n***","8a9f07d1":"## 4.3: Random Forest Classifier:","078ecbff":"In the histogram above, for the employees who left the firm we can observe:\n- The maximum number of employees left the firm after working for 3 years.\n- The number of employees leaving the firm is minimum at (0 to 2) years and 6 years but is consistently high between 4 to 5 years with the firm.\n***","e7fcfd30":"# 1: Introduction - Business Understanding:\n\nA Company wants to know, understand and find insights about their employees leaving the company. The company has compiled a set of data, they think will be helpful in this case study. It includes employees **satisfaction levels** , **evaluations**, **time spent at work**, **department** they work in and their **salary**.\n\nThe aim of this case study is apply data science, machine learning technique to provide :\n- Determing a plan for using data modelling tp provide impactful business insights.\n- Create a machine learning pipeline for classifying future results.\n***","e645af81":"you can see, the decision boundaries are significantly less choppy compared to the plot for the  n_neighbors=3  model, and there are far fewer pockets of class 1 prediction ranges (the orange contours). Looking at the metrics, the accuracy for class 1 is slightly less, but we would need to use a more comprehensive method (such as k-fold cross validation) to decide whether there's a significant difference between this model and the  n_ neighbors=3 model that we trained previously. Note that increasing  n_neighbors  has no effect on training time as the model is simply memorizing the data. The prediction time, however, becomes longer as n_neighbors is increased.\n***","2c86f892":"What is different is the time spend in the firm. Notice how the employees in the HR and R and D departments spend lesser time in the firm compared to the rest of firm.We can also notice how the management department spends the longest time in the firm. *This might indicate that the management is to blame for employees leaving the firm.\n***","3a1d955d":"# Index\n|Serial | Contents|\n|-------|---------|\n|1| Introduction.|\n|2| Data Exploration.|\n|3| Statistical Modelling.|\n|4| Machine Learning. |\n|5| Production Ready Model.| ","d5cbe70a":"The preceding function plots decision regions, along with a set of records that are passed as arguments. In order to see the decision regions properly without too many records obstructing our view, we only pass a 200-record sample of the test data to the  plot_decision_regions  function. In this case, of course, it does not matter. We see the result is entirely red, indicating that every point in the feature space will be classified as 0. It shouldn't be surprising that a linear model can't do a good job of describing these nonlinear patterns. If the dataset was linearly separable, we would be able to separate the majority of records from each class using a straight line. Recall the kernel trick for using SVMs to classify nonlinear problems. Let's see whether doing this can improve the result.\n***","abc1be15":"As can be seen, this model is performing much better than previous models for class 1, with an average accuracy of 93.6% +\/- 1.5%. This can be attributed to the additional features we are using here, compared to earlier models that relied on only two features.\n***","352d5a83":"In the boxplot above we can notice:\n- The median value of the satisfaction levels of the employees who left are lower than the employees who stayed.\n- We can also observe that the satisfaction levels of the meployees who left are indeed lower than the employees who stayed.\n***","4a37e4ae":"From the 4 plots above we can observe that the distributions for the satisfaction levels,number of projects, last evaluation scores and the average monthly working hours of the employees are more or less the same across all three levels of salaries - Low (0), Medium (1) and High(2).\n\nNext we will take a look at some histograms that shows us the comparison between the employees who stayed and the emloyees who left the firm.\n\n- The first set of histograms shows us the factors that have little to NO influence on the employees who left and the employees who stayed because both the histograms under comparison are similar in distribution.\n- The second set of histograms shows us the factors that have some to significant influence on the employees who left and the employees who stayed becuase the two histograms under comparison have different distributions.\n\n***","4858eaf2":"# 2: Data Visualisation:\nThis section deals with the key insights, the main aim here is to obtain information regarding the employees who left and the employees who stayed with with the firm.\n***","4ea4883c":"These outputs show the class accuracies, where the first value corresponds to class 0 and the second corresponds to class 1. ","8ea805f2":"The plot show linear relationship between **satisfaction_level** and **average_monthly_hours** on scatter plot with a regression line. Employees who have left the firm are shown in black and employees staying at the firm are shown in grey.\n\nWe can observe that the bulk of the employees who have left the firm have worked between 225 to 275 hours and have satisfaction levels between 0.7 to 0.9.\nThe next chunk of employees that have left the firm have worked between 130 to 160 hours and have lower satisfaction levels of 0.3 to 0.5.\n\nThe final set of employees who have left the firm have worked between 245 to 320 hours have the lowest satisfaction levels in the range of 0 to 0.15.\nThis clearly shows us that employees who work more hours for the firm and have high satisfaction levels have indeed left the firm.\n***","a9d749c6":"## 4.2: K-Nearest Neighbors:"}}