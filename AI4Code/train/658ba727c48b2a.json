{"cell_type":{"133e9072":"code","be68f790":"code","b7bf6cab":"code","24cca0d2":"code","a5d9519c":"code","a29c0fb7":"code","d2f53566":"code","8ba81d50":"code","24ff2b28":"code","fe1cf61e":"code","f345f325":"code","b27a5df5":"code","e83d8780":"code","e731de23":"code","80f64a61":"code","1d39dc2a":"code","6efb5106":"code","8d29c420":"code","e2314512":"code","9ff14bf7":"code","7f1bcbe3":"code","6ea541b5":"code","db476ba3":"code","c188e94d":"code","1894f243":"code","b3ca3c34":"code","6675a1d8":"code","9bf0e17e":"code","134b2bcc":"code","ac5b2336":"code","cc1421a1":"code","0f8f78be":"code","c8ab9627":"code","4573015f":"code","66c0e09d":"code","2550b43c":"code","ee080774":"markdown","1000bf06":"markdown","0426405a":"markdown","448f033f":"markdown","92a13836":"markdown","5729f100":"markdown","0c70a894":"markdown","f62a0a76":"markdown","89e3c66b":"markdown","bbf05545":"markdown","a1c5c2db":"markdown","d581fead":"markdown","c657ccf3":"markdown","3093fb95":"markdown","00171451":"markdown","1ff16d37":"markdown","28642eb5":"markdown","dc035d32":"markdown","8b9d5c96":"markdown","f9384959":"markdown","0d413033":"markdown","cb9bbf3a":"markdown","cc6336e2":"markdown","2ae7c482":"markdown","74561b17":"markdown","7b01e714":"markdown","22aef734":"markdown"},"source":{"133e9072":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\n# turn off warnings for final notebook\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\n%matplotlib inline\nsns.set_context('notebook')\nsns.set_palette('Set2')\nsns.set_style('darkgrid')\n\nclass color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n","be68f790":"df=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head(10)","b7bf6cab":"df.rename(columns={'DiabetesPedigreeFunction': 'PedigreeFunction'},inplace=True)","24cca0d2":"# general information about the dataset\ndf.info()\ndf.isnull().sum().sort_values(ascending=False)","a5d9519c":"def highlight_columns(df, rows=20, color='lightgreen', columns_to_shadow=[], columns_to_show=[]):\n    highlight = lambda slice_of_df: 'background-color: %s' % color\n    sample_df = df.head(rows)\n    if len(columns_to_show) != 0:\n        sample_df = sample_df[columns_to_show]\n    highlighted_df = sample_df.style.applymap(highlight, subset=pd.IndexSlice[:, columns_to_shadow])\n    return highlighted_df\n\nhighlight_columns(df.describe().T,columns_to_shadow=['min'])","a29c0fb7":"df_clean1=df.copy()\ndf_clean1[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI']]=df_clean1[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI']].replace(0,np.NaN)\n\n## showing the count of Nans\nprint(color.BOLD+\"NUMBER OF ABSENT VALUES\\n\"+color.END)\nprint(df_clean1.isnull().sum())","d2f53566":"sns.heatmap(df_clean1.isnull(),cbar=True).set_title(\"Visualizing Missing Values\\n\", fontsize=25)\nplt.gcf().set_size_inches(25,6)\nplt.xticks(rotation=0, fontsize=15)\nplt.show()\n","8ba81d50":"fig,ax = plt.subplots(figsize=(20,8))\n\ndata=df_clean1.drop(columns=['Insulin','Outcome','PedigreeFunction'])\n\ndata.boxplot(patch_artist=True)\n\n#Just some coloring\ndef random_color():\n    r = lambda: random.randint(0,255)\n    hex= ('#%02X%02X%02X' % (r(),r(),r()))\n    return hex\n\nfor i in range (0,data.shape[1]):\n    ax.findobj(matplotlib.patches.Patch)[i].set_facecolor(random_color())\n\n#Editing the final plot\nax.set_title(\"Numerical Variables BoxPlot\",size=20,fontweight='bold')\nax.set_ylabel('Score', fontsize=16)\nax.tick_params(axis='x', labelrotation=0, labelsize=15)","24ff2b28":"fig,ax = plt.subplots(figsize=(5,8))\n\ndata=df_clean1[['Insulin']]\n\ndata.boxplot(patch_artist=True)\n\n#Just some coloring\ndef random_color():\n    r = lambda: random.randint(0,255)\n    hex= ('#%02X%02X%02X' % (r(),r(),r()))\n    return hex\n\nfor i in range (0,data.shape[1]):\n    ax.findobj(matplotlib.patches.Patch)[i].set_facecolor(random_color())\n\n#Editing the final plot\nax.set_title(\"Numerical Variables BoxPlot\",size=20,fontweight='bold')\nax.set_ylabel('Score', fontsize=16)\nax.tick_params(axis='x', labelrotation=0, labelsize=15)","fe1cf61e":"fig,ax = plt.subplots(figsize=(5,8))\n\ndata=df_clean1[['PedigreeFunction']]\n\ndata.boxplot(patch_artist=True)\n\n#Just some coloring\ndef random_color():\n    r = lambda: random.randint(0,255)\n    hex= ('#%02X%02X%02X' % (r(),r(),r()))\n    return hex\n\nfor i in range (0,data.shape[1]):\n    ax.findobj(matplotlib.patches.Patch)[i].set_facecolor(random_color())\n\n#Editing the final plot\nax.set_title(\"Numerical Variables BoxPlot\",size=20,fontweight='bold')\nax.set_ylabel('Score', fontsize=16)\nax.tick_params(axis='x', labelrotation=0, labelsize=15)","f345f325":"# Frequency distribution\nfig, axs = plt.subplots(nrows=3,ncols=3, figsize=(18,18*3\/4), sharey=False)\n\nbin_num=30\ncolors=['blue','red','green','blue','red','green','blue','red','green']\nprops = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n\nnum= df_clean1\n\nfor i in range(0,9):\n    x_matrix= int((i-i%3)\/3)\n    y_matrix= i%3\n    \n    n, bins, patches = axs[x_matrix,y_matrix].hist(num[num.columns[i]],bins=bin_num,color=colors[i])\n    axs[x_matrix,y_matrix].set_title(num.columns[i], size=20,fontweight='bold')\n    axs[x_matrix,y_matrix].axvline(x=num[num.columns[i]].mean())\n    mu = num[num.columns[i]].mean()\n    sigma = num[num.columns[i]].std()\n    textstr = '\\n'.join((\n        r'$\\mu=%.2f$' % (mu, ),\n        r'$\\sigma=%.2f$' % (sigma, ),\n        r'$CV=%.2f$' % (mu\/sigma, )))\n    axs[x_matrix,y_matrix].annotate(textstr,xy=(0.75, 0.95), xycoords=\"axes fraction\", fontsize=14, verticalalignment='top', bbox=props)\n     \n\nfig.suptitle('Frequency Distribution', fontsize=20,fontweight='bold')\n","b27a5df5":"fig, ax = plt.subplots(figsize=(10, 8))\n\nax.set_title(\"Correlation Matrix\\n\", size=20,fontweight='bold')\nsns.heatmap(num.corr(), annot=True,ax=ax, annot_kws={'size':15}, cmap=\"YlOrRd\")\nax.tick_params(axis='x', labelrotation=75, labelsize=15)\nax.tick_params(axis='y', labelrotation=0, labelsize=15)","e83d8780":"from scipy.stats import ttest_ind\n\ncount = 0\narray_1 = np.array([])\nprops = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n\ndef ttest_p(x,**kws):\n    global count,array_1\n    if count==0:\n        array_1=x\n        count=1\n    else:\n        stat, p = ttest_ind(array_1,x)\n        ax = plt.gca()\n        ax.annotate(\n             'p = {:.2f}'.format(p),\n             #'p = {:.2f}, stat={:.2f}'.format(p,stat), # for sanity check\n              xy=(0.05, 0.9), xycoords=ax.transAxes, bbox=props,fontsize=12)\n        count=0\n\ndf_clean1['Outcome_read']=df_clean1['Outcome'].map({0:\"Non Diabetic\",1:\"Diabetic\"})      \n\ng = sns.PairGrid(df_clean1.drop(columns=['Outcome']), diag_sharey=False, height=4,hue='Outcome_read',corner=False, dropna= True)\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, fill=True)\ng.map_diag(ttest_p)\ng.map_lower(sns.kdeplot,  levels=40,hue=None,thresh=0,cmap=\"rocket\",fill=True)\nsns.set(font_scale=1.1)\n\ng.add_legend(title='', fontsize=20,fancybox=True)\ng.fig.suptitle('Pairplot of features (Hue = Diabetes Diagnosis)', fontsize=25,y=1.03) # y= some height>1\n\ndf_clean1.drop(columns=['Outcome_read'],inplace=True)\n","e731de23":"from scipy.stats import ttest_ind\n'''\nFail to Reject H0: Sample distributions are equal.\nReject H0: Sample distributions are not equal.\n'''\ndf_ttest = df_clean1\nfeature_hue = 'Outcome'\n\nprint(color.BOLD+\"Statitical Test for the distributions along\",feature_hue,\"\\n\"+color.END)\nfor i in df_ttest.columns[:-1]:\n    feature=i\n    df_ttest_1= df_ttest[feature][df_ttest[feature_hue]==1]\n    df_ttest_2= df_ttest[feature][df_ttest[feature_hue]==0]\n    \n    stat, p = ttest_ind(df_ttest_1.dropna(),df_ttest_2.dropna())\n    print('(Statistics=%.3f, p=%.3f)' % (stat, p))\n    # interpret\n    alpha = 0.05\n    print('Distribution Comparison of' +color.BOLD,feature,color.END+' for different ',feature_hue)\n    if p > alpha:\n        print('Same distributions (fail to reject H0)')\n    else:\n        print('Different distributions'+color.BOLD+'(reject H0)\\n'+color.END)","80f64a61":"!pip install missingpy\nfrom missingpy import MissForest\nimputer = MissForest(random_state=2324)\n\nX = df_clean1\nX_imputed = imputer.fit_transform(X)\n\ndf_imputed = pd.DataFrame(X_imputed,columns=df_clean1.columns)","1d39dc2a":"print(df_imputed.isnull().sum())","6efb5106":"highlight_columns(df_clean1[df_clean1.isna().any(axis=1)].head(10),columns_to_shadow=['Insulin'])","8d29c420":"highlight_columns(df_imputed.iloc[[0,1,2,5,7,9,10,11,12,15]],columns_to_shadow=['Insulin'])","e2314512":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\n\nX=df_imputed.drop(columns=['Outcome'])\ny=df_imputed['Outcome']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=8991)\nX_train_scaled_array = scale(X_train)\nX_test_scaled_array = scale(X_test)\n\nX_train_scaled = pd.DataFrame(X_train_scaled_array, columns = X_train.columns) #after being scaled the results lose the column headers, so this puts it back\nX_test_scaled = pd.DataFrame(X_test_scaled_array, columns = X_train.columns)","9ff14bf7":"#modelling using Support Vector Classifier\nfrom sklearn.svm import SVC\n\nclf_svm=SVC(random_state=2324)\nclf_svm.fit(X_train_scaled, y_train)","7f1bcbe3":"from sklearn.metrics import plot_confusion_matrix\n\n\nwith sns.axes_style(\"white\"):\n    titles_options = [(\"Confusion matrix\\nwithout normalization\", None),\n                      (\"Confusion matrix\\nnormalized\", 'true')]\n    class_names = ['Non Diabetic','Diabetic']\n    for title, normalize in titles_options:\n        fig, ax = plt.subplots(figsize=(7, 7))\n        disp = plot_confusion_matrix(clf_svm,\n                                     X_test_scaled,\n                                     y_test,\n                                     display_labels=class_names,\n                                     cmap='rocket',\n                                     normalize=normalize,\n                                    ax=ax)\n        disp.ax_.set_title(title)\n\n        print(title)\n        print(disp.confusion_matrix)\n    \nplt.show()","6ea541b5":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'C':[0.5,1,10,100],\n              'gamma':['scale', 1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel':['rbf']}]\n\noptimal_params = GridSearchCV(SVC(), param_grid,cv=5,scoring='accuracy',verbose=0)\n\noptimal_params.fit(X_train_scaled, y_train)\nprint(\"Optimal parameters:\"+str(optimal_params.best_params_))","db476ba3":"clf_svm=SVC(random_state=8991, C=0.5, gamma=0.01)\nclf_svm.fit(X_train_scaled, y_train)","c188e94d":"with sns.axes_style(\"white\"):\n    titles_options = [(\"Confusion matrix, without normalization\", None),\n                      (\"Normalized confusion matrix\", 'true')]\n    class_names = ['Non Diabetic','Diabetic']\n    for title, normalize in titles_options:\n        fig, ax = plt.subplots(figsize=(7, 7))\n        disp = plot_confusion_matrix(clf_svm,\n                                     X_test_scaled,\n                                     y_test,\n                                     display_labels=class_names,\n                                     cmap='rocket',\n                                     normalize=normalize,\n                                    ax=ax)\n        disp.ax_.set_title(title)\n\n        print(title)\n        print(disp.confusion_matrix)\n    \nplt.show()","1894f243":"# Bayes factor for positive: Sensitivity\/False positive rate = 6.34\n# Bayes factor for negative: False Negative rate\/Specificity = 0.47","b3ca3c34":"!pip install lazypredict","6675a1d8":"from lazypredict.Supervised import LazyClassifier\n\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(X_train_scaled, X_test_scaled, y_train, y_test)\n\nmodels","9bf0e17e":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clf_svm, random_state=8991).fit(X_test_scaled, y_test)\n\nprint(color.BOLD+\"   PERMUTATION IMPORTANCE\\n\"+color.END)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(), top=7)","134b2bcc":"import shap\n\nexplainer = shap.Explainer(clf_svm.predict,X_test_scaled)\nshap_values = explainer(X_test_scaled)\n","ac5b2336":"plt.title('SHAP summary for Diabetes prediction', size=20,fontweight='bold')\n          \nshap.summary_plot(shap_values, X_test_scaled, show=False)\n\nfig = plt.gcf()\nfig.set_figheight(7)\nfig.set_figwidth(12)\nax = plt.gca()\nax.set_xlabel(r'Average SHAP values', fontsize=16)\nax.set_ylabel('Features', fontsize=16)\nleg = ax.legend()\nt = ax.text(\n    0.05, -0.6, \"Predict diabetic\", ha=\"left\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"rarrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))\nt = ax.text(\n    -0.05, -0.6, \"Predict non-diabetic\", ha=\"right\", va=\"center\", rotation=0, size=13,\n    bbox=dict(boxstyle=\"larrow,pad=0.3\", fc=\"#CBC3E3\", ec=\"#301934\", lw=1))\nplt.show()","cc1421a1":"shap.plots.bar(shap_values,\n               show=False)\n\nfig = plt.gcf()\nfig.set_figheight(7)\nfig.set_figwidth(12)\nplt.title('Mean SHAP value per feature\\n Diabetes Prediction',size=20,fontweight='bold')\nplt.show()","0f8f78be":"selected=42\n\nplt.title('Diabetes Prediction Explanation for subject n\u00ba '+str(selected), size=20,fontweight='bold')\n\nprops = dict(boxstyle='round', facecolor='#CBC3E3', alpha=0.5)\n\n\nshap.plots.waterfall(shap_values[selected],\n               show=False)\n\nfig = plt.gcf()\nfig.set_figheight(7)\nfig.set_figwidth(12)\nax = plt.gca()\n\ntext='f(x)=0, predict non-diabetic\\nf(x)=1, predict diabetic\\n\\nTrue outcome: '+str(y_test.iloc[selected])+ ('\\nCorrect prediction'if (round(shap_values[selected].values.sum()+shap_values[selected].base_values)==y_test.iloc[selected]) else '\\nIncorrect prediction')\n    \nt = ax.annotate(text,xy=(1, 1), xycoords=\"axes fraction\", fontsize=14, verticalalignment='top', bbox=props)\nplt.show()","c8ab9627":"from sklearn.decomposition import PCA\n\npca = PCA()\nX_train_pca = pca.fit_transform(X_train_scaled)\n\nper_var = np.round(pca.explained_variance_ratio_* 100,decimals=1)\nlabels = [str(x) for x in range(1, len(per_var)+1)]\n\nfig,ax = plt.subplots()\n\nplt.bar(x=range(1,len(per_var)+1), height=per_var)\nplt.tick_params(\n    axis='x',\n    which='both',\n    bottom=False,\n    top=False,\n    labelbottom=False)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Components')\nplt.title('Scree Plot')\n\nfor i, v in enumerate(per_var):\n    ax.text( i + .6, 0.5, str(v), color='white', fontweight='bold')\nplt.show()\n","4573015f":"train_pc1_coords = X_train_pca[:,0]\ntrain_pc2_coords = X_train_pca[:,1]\n\n#centering and scaling the PCs\npca_train_scaled = scale(np.column_stack((train_pc1_coords, train_pc2_coords)))\n\n#Optimize the SVM fit to the x and y-axis coordinates of the data after PCA dimension reduction\n\nparam_grid = [{'C':[0.5,1,10,100],\n              'gamma':['scale', 1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel':['rbf']}]\n\noptimal_params = GridSearchCV(SVC(), param_grid,cv=5,scoring='accuracy',verbose=0)\n\noptimal_params.fit(pca_train_scaled, y_train)\nprint(optimal_params.best_params_)","66c0e09d":"import matplotlib.colors as colors\n\nclf_svm = SVC(random_state=2324, C=0.5,gamma=0.1)\nclf_svm.fit(pca_train_scaled, y_train)\n\n#Transform the test dataset with the PCA\nX_test_pca = pca.transform(X_train_scaled)\n\ntest_pc1_coords = X_test_pca[:,0]\ntest_pc2_coords = X_test_pca[:,1]\n\n\nx_min= test_pc1_coords.min() - 1\nx_max= test_pc1_coords.max() + 1\n\ny_min= test_pc2_coords.min() - 1\ny_max= test_pc2_coords.max() + 1\n\nxx,yy = np.meshgrid(np.arange(start=x_min, stop=x_max, step=0.1),\n                    np.arange(start=y_min, stop=y_max, step=0.1))\n\nZ= clf_svm.predict(np.column_stack((xx.ravel(), yy.ravel())))\n\n\nZ=Z.reshape(xx.shape)\n\n","2550b43c":"fig, ax = plt.subplots(figsize=(10,10))\n\nax.contourf(xx,yy,Z,alpha=0.1)\n\ncmap= colors.ListedColormap(['#e41a1c','#4daf4a'])\n\nscatter = ax.scatter(test_pc1_coords, test_pc2_coords, c=y_train,\n                    cmap=cmap,\n                    s=100,\n                    edgecolors='k',\n                    alpha= 0.7 )\nlegend = ax.legend(scatter.legend_elements()[0],\n                   scatter.legend_elements()[1],\n                   loc=\"upper right\")\nlegend.get_texts()[0].set_text(\"Non Diabetic\")\nlegend.get_texts()[1].set_text(\"Diabetic\")\n\nax.set_ylabel('PC2',fontsize=15)\nax.set_xlabel('PC1',fontsize=15)\nax.set_title('Decision surface using the PCA transformed\/projected features', fontsize= 20)\nplt.show()","ee080774":"**Conclusions\/Insights:**  \n* No apparent absent data. (Possible absent data not classified as Null)  \n* The datatypes seem to be correct. (Words weren't used to sign absent data)","1000bf06":"Special thanks to [Josh Starmer from StatQuest](https:\/\/www.youtube.com\/watch?v=8A7L0GsBiLQ)","0426405a":"**Conclusions\/Insights:**   \nWhen separated by outcome, all distributions present themselves as statistically different. \nThe pairwise scatter seems to indicate in most cases that there is a distincion between the positive and negative diagnosis.Nevertheless, the border between the two groups is not clear. This might indicate that the ML model will have difficulties predicting when most feature values are in this border region and consequently affect the precision of the model.","448f033f":"# 0. Stablishing the goal \n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83c\udfc1 Goal <\/h1><a id=0><\/a>","92a13836":"**Let's see how other models would perform with this data:**","5729f100":"**Conclusions\/Insights:**   \nWe can conclude that **Glucose,BMI and Insulin** were pointed as the three most important features in concluding over the Diabetes diagnosis.","0c70a894":"First of all, we are going to trim the size of one column, namely DiabetesPedigreeFunction to PedigreeFunction, for simplicity and for avoiding ploting conflicts.","f62a0a76":"# 5. Most Relevant Features\n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83e\udd47 Most Relevant Features <\/h1><a id=5><\/a>","89e3c66b":"Let's take a look at the \"Before and After\" picture, focusing at the Insulin feature.","bbf05545":"**Conclusions\/Insights:**  \n1. It is unreasonable to have 0 for the values of the following features. The most sensible assumption is that \"0\" was used as a placeholder to represent missing values:  \n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI\n2. The others features have no apparent discrepancy.","a1c5c2db":"The goal of this notebook is to explore a dataset about patients with and without the diagnosis of Diabetes(https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database). Machine Learning models will be derived from the data, through which the most important variables will be ordered. Understanding through visualizations will be prioritized.","d581fead":"<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83d\ude09 Upvote if you liked the content \ud83d\udc4d<\/h1><a id=5><\/a>","c657ccf3":"**Conclusions\/insights:**  \nThe prediction has a low Sensitivity (40\/70=57.1%), but a high Specificity (111\/121=91.7%), which gives us a Bayes factor of 6.34 for a positive result or 0.47 in case of a negative result.  \nThis would mean for a person with a positive result an update in ther prior odds from 23:77 to (23:77 * 6.34) =~ 146:77; or  \nfor a person with negative result an update in their prior odds from 23:77 to (23:77 * 0.47) =~ 11:77.    \n  \nIn probability terms, the result of the test means an updated to the prior risk from 23% to 65% or 12.5%, depending on the result.\u00b9    \n\u00b9For this specific population","3093fb95":"**Understanding the features**\n\n**Pregnancies**: Number of times pregnant  \n**Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test  \n**BloodPressure**: Diastolic blood pressure (mm Hg)  \n**SkinThickness**: Triceps skin fold thickness (mm)  \n**Insulin**: 2-Hour serum insulin (mu U\/ml)  \n**BMI**: Body mass index (weight in kg\/(height in m)^2)  \n**DiabetesPedigreeFunction**: Diabetes pedigree function  \n**Age**: Age (years)  \n**Outcome**: Class variable (0 or 1)  ","00171451":"# 1. Importing \n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83d\udcda Importing Libraries and Data <\/h1><a id=1><\/a>","1ff16d37":"**Conclusions\/Insights:**   \nBlood Pressure, BMI and Blood Glucose are the features with the highest Coeficient of Variance (CV), i.e. have a spread out range of values.","28642eb5":"# 6. Bonus\n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83d\udc31\u200d\ud83d\udc64 Bonus: Depicting the SVM decision surface using PCA<\/h1><a id=6><\/a>","dc035d32":"# 2. Exploring and Preparing \n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83d\udd0e Exploring and Preparing <\/h1><a id=2><\/a>","8b9d5c96":"# 4. Modelling\n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83d\udd2e Modelling <\/h1><a id=4><\/a>","f9384959":"**Conclusions\/Insigths:**\nThere is no sensible difference between the model applied and other models.","0d413033":"After this inicial attempt we will try to optimize the parameters for the SVM model.","cb9bbf3a":"## IMPUTATION\n\nAs we saw previously, 2 important rows have a high percentage of missing values. In order to fully exploit value out of those rows, we are goint to perform an imputation of values through the Random Forest Model.","cc6336e2":"**Conclusions\/Insights:**   \nLow correlation among the features. Highest correlations are understandable (Age x Pregnancies & BMI x Skin Thickness & Glucose x Insulin).","2ae7c482":"For getting a sense of the most relevant variables we are goint to apply 2 different methods, namely the \"Permutation Importance\"\u00b9 and the \"Shapley values\".  \n  \n\u00b9A.k.a. Mean Decrease Accuracy(MDA).","74561b17":"Despite the nice visuals and complex procedures, here we have roughtly an 80% accurate model depicted at a cost of ~50% of variance due to PCA, which ultimately means that this graph has around 40% accuracy. So it's unfitting for the current case.  \nBut it is still cool.","7b01e714":"**Conclusions\/Insights:**  \n\nThe most lacking informations are insulin(48,6%) and SkinThickness(29,5%).\n\nHere we reach a critical decision for modelling a ML model.  \nMany ML models do not accept missing values, SVM included.So, depending on the solution adopted it might restrict the alternative for Models.  \nPossible ways to deal with this are:\n* Delete the incomplete rows.\n* Delete the incomplete columns.\n* Imputation of the missing data.   \n\n**Considering the high percentage of incomplete rows, and the relevance of the features, in this notebook we will execute the imputation of the missing values in the processing step**.","22aef734":"# 3. Processing \n<h1 style=\"background:#FFDE91;\n           font-family:newtimeroman;\n           font-size:350%;\n           text-align:center;\n           border-radius: 50px 50px;\n           border:5px solid black;\n           ; padding:15px\"> \ud83d\udee0 Processing <\/h1><a id=3><\/a>"}}