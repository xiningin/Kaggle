{"cell_type":{"742bc09d":"code","9f8180c8":"code","869eb294":"code","455990af":"code","50839f09":"code","bcb79c4b":"code","73793b16":"code","bc8dccfd":"code","cad5a5f1":"code","c4f063f2":"code","1eaf5397":"code","68edb81c":"code","a7104a86":"code","a60fa245":"code","d3dc759f":"code","9bbcf439":"code","412f4871":"code","5f2d7c6e":"code","02a44898":"code","4287adb0":"code","333419d4":"code","01148dca":"code","141db4c9":"code","f52f0a35":"code","9edad0e6":"code","02898a48":"code","850442f1":"code","c55ebb72":"code","b2f14a7e":"code","0ec0398b":"code","e6ad4661":"code","050d1d49":"code","b6ce460e":"code","563175f1":"code","f7c09901":"code","755a772f":"code","90325275":"code","ae0a5159":"code","d49999e4":"code","0a115ca9":"code","4274dfc7":"code","0ecb09c8":"code","6560090e":"code","4f2c14ec":"code","90600a96":"code","cfea8e0a":"code","17286c48":"code","aea78fcf":"code","0f027027":"code","a2244130":"markdown","c5b0c3c4":"markdown","ca36566a":"markdown","65cef1fe":"markdown","7799169e":"markdown","a7fe23e6":"markdown","86d84310":"markdown","16ec9983":"markdown","55f59ce7":"markdown","4dc866af":"markdown","eed175ce":"markdown","a6a884e1":"markdown","7f0f8af5":"markdown","836ce777":"markdown","e646aa84":"markdown","36cd763d":"markdown","b1ebb1a2":"markdown","b5823db8":"markdown","c4652f0d":"markdown","0c34530d":"markdown","39e5f442":"markdown","a85f7ea6":"markdown","8e4ae226":"markdown","700b18a9":"markdown","2e63b7df":"markdown","8c04ab6b":"markdown","cbbc7ace":"markdown","e59bb7fa":"markdown","9699d540":"markdown"},"source":{"742bc09d":"import numpy as np\nimport pandas as pd\nfrom pprint import pprint\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport pylab\nimport scipy\n#import stemgraphic \nimport seaborn as sns\nimport math\nfrom scipy.stats import kurtosis, skew\nimport warnings\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom scipy.stats import wilcoxon\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import chi2\n\nfrom catboost import CatBoostClassifier, cv\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import train_test_split","9f8180c8":"import os\n%matplotlib notebook","869eb294":"warnings.simplefilter(action='ignore', category=FutureWarning)","455990af":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","50839f09":"df_train = pd.read_csv('..\/input\/cardiovascular-study-dataset-predict-heart-disea\/train.csv',index_col=0, delimiter=',')\ndf_train = df_train.drop('is_smoking', axis=1)\nenc = LabelEncoder()\nlabel_encoder = enc.fit(df_train['sex'])\ny = label_encoder.transform(df_train['sex'])\ndf_train['sex'] = y\ndf_train=df_train.drop('education',axis=1)\ndf_train","bcb79c4b":"all(map(lambda x: x == 0, df_train.isnull().sum())) # no missed values","73793b16":"filt = filter(lambda x: df_train[x].isnull().sum() !=0, df_train)\ndf_null = pd.DataFrame(df_train[filt].isnull().sum()\/len(df_train)*100).T\ndf_null.iloc[:,np.where(df_null > 20)[1]] ","bc8dccfd":"df_train = df_train.fillna(df_train.mean())\ndf_train","cad5a5f1":"def plotPerColumnDistribution(df):\n    # \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u0430\u0441\u0441\u0438\u043c\u0435\u0442\u0440\u0438\u044e \u0438 \u044d\u043a\u0441\u0446\u0435\u0441\u0441\n    colors = {0:'powderblue',1:'lightsalmon',2:'darkcyan',3:'mediumorchid',\n              4:'plum', 5:'black', 6: 'blue', 7: 'orange', 8: 'cyan', 9: 'pink',\n             10: 'red', 11 : 'yellow', 12: 'violet', 13: 'salmon'}\n    plt.figure(figsize=(20,20))\n    plt.tight_layout()\n    plt.subplots_adjust(\n#         left  = 0.125,  # the left side of the subplots of the figure\n#         right = 0.9,    # the right side of the subplots of the figure\n#         bottom = 0.4,   # the bottom of the subplots of the figure\n#         top = 0.9,      # the top of the subplots of the figure\n#         wspace = 0.2,   # the amount of width reserved for blank space between subplots\n         hspace = 0.5    # the amount of height reserved for white space between subplots\n    )\n \n    for i,x in enumerate(df.columns):\n        label=f'''Mean: {round(df[x].mean(),2)} Std: {round(df[x].std(),2)}\nMin: {round(df[x].min(),2)} Max:{round(df[x].max(),2)}\nQ1: {round(df[x].quantile(0.25))}\nQ2:{round(df[x].quantile(0.5),2)} \nQ3:{round(df[x].quantile(0.75),2)} \nKurtosis: {round(kurtosis(df[x]),2)} Simmetry: {round(skew(df[x]),2)}\n        '''\n        plt.subplot(5,3,i+1)\n        #plt.figure(figsize=(10,7))\n        plt.title(x.upper(),fontsize=15)\n        sns.distplot(df[x], color=colors[i],label=label)\n        plt.legend(fontsize=10, loc='best')\n        ","c4f063f2":"%matplotlib inline\nvarContin = ['age','cigsPerDay','totChol', 'sysBP','diaBP','BMI','heartRate','glucose']\nplotPerColumnDistribution(df_train[varContin])","1eaf5397":"plt.figure(figsize=(8,8))\nplt.title('Pair correlation')\nsns.heatmap(df_train[varContin].corr(), vmax=1, square=True,annot=True,cmap='BuPu')\n","68edb81c":"plotPerColumnDistribution(df_train.drop(varContin, axis=1))","a7104a86":"num_0 = len(df_train[df_train['TenYearCHD']==0])\nnum_1 = len(df_train[df_train['TenYearCHD']==1])\n\n\u0441ounter_1 = Counter(df_train['TenYearCHD'])\nprint(\u0441ounter_1)\nprint('random undersampling')\nbalanced_data = pd.concat([df_train[df_train['TenYearCHD']==0].sample(num_1, replace=True), #decrease majority\n                               df_train[df_train['TenYearCHD']==1]])\n\ncounter_2 = Counter(balanced_data['TenYearCHD'])\nprint(counter_2)\n","a60fa245":"# LDA\nX_train_lda = balanced_data.iloc[:,:-1].drop('diaBP',axis=1).values\nX_train_lda= StandardScaler().fit_transform(X_train_lda) # to one dimension\ny_train = balanced_data['TenYearCHD'].values\nX_train_lda.shape","d3dc759f":"# Trees\nX = df_train.iloc[:,:-1].values\nX= StandardScaler().fit_transform(X) # to one demensiion\ny = df_train['TenYearCHD'].values","9bbcf439":"# LDA\nX_train_lda = StandardScaler().fit_transform(X_train_lda)\nX_train_lda = X_train_lda.astype(np.float)","412f4871":"\nX = StandardScaler().fit_transform(X) \nX = X.round(5)\nX.shape\ny.shape","5f2d7c6e":"X_train_lda = StandardScaler().fit_transform(balanced_data.iloc[:,:-1].drop('diaBP',axis=1).values)\ny_train_lda = balanced_data['TenYearCHD'].values\nX_train_lda = X_train_lda.astype(np.float)\nlda = LinearDiscriminantAnalysis()\nlda.fit_transform(X_train_lda, y_train_lda)\nprint('model trained')","02a44898":"lda.coef_","4287adb0":"# Weights\npd.DataFrame({'feature': df_train.drop('diaBP',axis=1).columns[:-1], 'weight': lda.coef_.tolist()[0]})","333419d4":"pd.DataFrame([lda.decision_function(X_train_lda[y_train_lda==0]).mean(),lda.decision_function(X_train_lda[y_train_lda==1]).mean()], columns=['func'], index=['class 0','class 1'])","01148dca":"lda.decision_function(X_train_lda).mean()","141db4c9":"f_test = pd.DataFrame({'feature': df_train.drop('diaBP',axis=1).columns[:-1],'F': f_classif(X_train_lda,y_train_lda)[:][0], 'p-value': f_classif(X_train_lda,y_train_lda)[:][1]})\nf_test['result'] = f_test['p-value'] < 0.05 \nf_test['result'].replace({True: ' Significant'}, inplace=True)\nf_test['result'].replace({False: ' Indsignificant'},inplace=True)\nf_test","f52f0a35":"# The Lambda-Wilkes coefficient for each variable\nd = []\nfor x in X_train_lda.T:\n    d.append({'Lambda Wilcx': wilcoxon(x[y_train_lda==0], x[y_train_lda==1])[0], 'p-value': wilcoxon(x[y_train_lda==0], x[y_train_lda==1])[1]})\n\nwilc_test = pd.DataFrame(d)\nwilc_test['result'] = wilc_test['p-value'] < 0.05 \nwilc_test['result'].replace({True: ' Significant'}, inplace=True)\nwilc_test['result'].replace({False: ' Indsignificant'},inplace=True)\nwilc_test['feature'] = df_train.drop('diaBP',axis=1).columns[:-1]\nwilc_test","9edad0e6":"# Significance of differences in the average values of the discriminant function in the two groups.\nw, p = wilcoxon(lda.decision_function(X_train_lda[y_train_lda==0]),lda.decision_function(X_train_lda[y_train_lda==1]))\nw, p\nif p < 0.05:\n    print(w,p)\n    print('The function is fit well and significant')","02898a48":"disc_func = pd.DataFrame({'func' : lda.decision_function(X_train_lda), \n              'probability class 1': lda.predict_proba(X_train_lda)[:,0],  \n              'probability class 2': lda.predict_proba(X_train_lda)[:,1],\n              'pred class' : lda.predict(X_train_lda),\n              'actual class': y_train_lda})\ndisc_func['dif?'] = disc_func['pred class'] != disc_func['actual class']\ndisc_func[10:20] #\u0441\u0440\u0435\u0437 \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432","850442f1":"var1 = disc_func[disc_func['actual class'] == 0]['func'].var()\nvar2 = disc_func[disc_func['actual class'] == 1]['func'].var()\n\n# intra-group variance\nvar3 = (var1*len(X_train_lda[y_train_lda==0])+var2*len(X_train_lda[y_train_lda==1]))\/len(X_train_lda)\n\nmean_total = disc_func['func'].mean()\nmean1 = disc_func[disc_func['actual class'] == 0]['func'].mean()\nmean2 = disc_func[disc_func['actual class'] == 1]['func'].mean()\n\n# intergroup variance\nvar = ((mean1 - mean_total)**2 + (mean2-mean_total)**2)\/len(X_train_lda)\n\neighen_val = (var\/var3)\neighen_val","c55ebb72":"# Quality control for cross validation\nfrom sklearn.metrics import f1_score\nf1_score(y_train_lda, lda.predict(X_train_lda), average='macro') ","b2f14a7e":"for i, x in enumerate(lda.means_):\n    print(f'Class center {i} : {x}')","0ec0398b":"distances_0 = pd.DataFrame(columns=['Dist from center 0', 'x'])\ndistances_1 = pd.DataFrame(columns=['Dist from center 1', 'x'])\nfor x in X_train_lda[y_train_lda==0]:\n    distances_0=distances_0.append({'Dist from center 0': (np.linalg.norm(x - lda.means_[0])), 'x':x},ignore_index=True)\nfor x in X_train_lda[y_train_lda==1]:\n    distances_1 = distances_1.append({'Dist from center 1': (np.linalg.norm(x - lda.means_[1])), 'x':x},ignore_index=True)\n#sorted(distances_0, reverse=True)","e6ad4661":"distances_0.sort_values('Dist from center 0')","050d1d49":"for x in X_train_lda[[129,168,452]]:\n    print(x)\n# X_train_lda[452], y_train_lda[452]","b6ce460e":"distances_1.sort_values('Dist from center 1')","563175f1":"for x in X_train_lda[[327,154,224]]:\n    print(x)","f7c09901":"df_test = pd.read_csv('..\/input\/cardiovascular-study-dataset-predict-heart-disea\/test.csv',index_col=0, delimiter=',')\nenc = LabelEncoder()\nlabel_encoder = enc.fit(df_test['sex'])\ny = label_encoder.transform(df_test['sex'])\ndf_test['sex'] = y\ndf_test = df_test.drop('is_smoking', axis=1)\ndf_test = df_test.fillna(df_test.mean().round(5)).drop(['education','diaBP'],axis=1)\n\n\nX_test = df_test.values\nmask = np.isnan(X_test)\nidx = np.where(~mask,np.arange(mask.shape[1]),0)\nnp.maximum.accumulate(idx,axis=1, out=idx)\nout = X_test[np.arange(idx.shape[0])[:,None], idx]\nX_test = out","755a772f":"lda.predict(X_test)","90325275":"disc_func = pd.DataFrame({'func' : lda.decision_function(X_test), \n              'probability class 1': lda.predict_proba(X_test)[:,0],  \n              'probability class 2': lda.predict_proba(X_test)[:,1],\n              'pred class' : lda.predict(X_test),\n                         })\ndisc_func ","ae0a5159":"rascorer = make_scorer(f1_score, average='macro')\nmodel = GradientBoostingClassifier()\n#{'n_estimators': [300,400,500,700]}\nsearch = GridSearchCV(model, {'max_depth':[3,4,5,6,7,8,9,10]} , cv=KFold(n_splits=5, shuffle=True), scoring=rascorer, n_jobs=-1, verbose=True)\nsearch.fit(X, y)","d49999e4":"#plt.plot([3,4,5,6],search.cv_results_['mean_test_score'])\nplt.plot([3,4,5,6,7,8,9,10],search.cv_results_['mean_test_score'])\nsearch.best_score_ # settle with 7","0a115ca9":"model = GradientBoostingClassifier()\nsearch = GridSearchCV(model,{'n_estimators': [100,200,300,400,500]}, cv=KFold(n_splits=5, shuffle=True), scoring=rascorer, n_jobs=-1, verbose=True)\nsearch.fit(X, y)","4274dfc7":"plt.plot([100,200,300,400,500],search.cv_results_['mean_test_score'])\nsearch.best_score_ # settle with 300\n","0ecb09c8":"best_model = CatBoostClassifier(\n   custom_loss=['TotalF1'],\n   use_best_model=True,\n   max_depth=7,\n   n_estimators=300\n)","6560090e":"X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8, random_state=1234)\n\nbest_model.fit(\n   X_train, y_train,\n   eval_set=(X_validation, y_validation),\n   logging_level='Silent',\n   plot=True\n)","4f2c14ec":"# print('Resulting tree count:', best_model.tree_count_)\n# print('Best iteration:' , best_model.best_iteration_)\nprint('Best score')\nfor x in best_model.best_score_:\n    print(f'{x} : {best_model.best_score_[x]}')\n# print('Best score:' , best_model.best_score_)","90600a96":"pd.DataFrame({'feature': df_train.columns[:-1], 'weight': best_model.get_feature_importance()})","cfea8e0a":"df_test = pd.read_csv('..\/input\/cardiovascular-study-dataset-predict-heart-disea\/test.csv',index_col=0, delimiter=',')\ndf_test = df_test.drop('is_smoking', axis=1)\nenc = LabelEncoder()\nlabel_encoder = enc.fit(df_test['sex'])\ny = label_encoder.transform(df_test['sex'])\ndf_test['sex'] = y\ndf_test = df_test.fillna(df_test.mean().round(5)).drop(['education'],axis=1)\nX_test = df_test.values\nX_test.shape","17286c48":"labels = best_model.predict(X_test)\nlabels","aea78fcf":"disc_func = pd.DataFrame({ \n              'probability class 0': best_model.predict_proba(X_test)[:,0],  \n              'probability class 1': best_model.predict_proba(X_test)[:,1],\n              'pred class' : best_model.predict(X_test),\n                         })\ndisc_func","0f027027":"disc_func = pd.DataFrame({\n              'probability class 0': best_model.predict_proba(X_train)[:,0],  \n              'probability class 1': best_model.predict_proba(X_train)[:,1],\n              'actual class'  : y_train,\n              'pred class' : best_model.predict(X_train),\n                         })\ndisc_func","a2244130":"There is a strong linear dependency between systolic and diastolic blood pressure. This may affect the quality of the Linear Discriminant Analysis, so the diaBP attribute will be removed. The Catboost Decision Tree algorithm is strong enough so that the correlation does not confuse it. For catboost the attribute will remain in the training sample.","c5b0c3c4":"Significant proportions of omissions in the signs were not found. We will not delete the signs, we will replace the missing values with the average","ca36566a":"# Results on the test sample","65cef1fe":"# Correlation","7799169e":"# Decision Tree-Selection of hyperparameters\nWe use the GridSearch algorithm to try all combinations of parameters in parallel and compare the quality. To avoid over-training, we pass the result of cross-validation of the test sample to GridSearch (a sample divided into 5 subsamples, randomly mixed at each iteration).","a7fe23e6":"It can be noted that the model often makes mistakes on the test sample, but so far in cases where the predicted probability is small. Next, we will take a closer look at the quality of the model and the reasonableness of its application.","86d84310":"The most decisive risk factor was the patient's age, followed by the number of cigarettes per day and gender. Medical history indicators and current medical indicators contribute to the classification together with each other.","16ec9983":"The difference in the scale of the features is insignificant, but we will still bring them to the same dimension. In addition, reducing to one dimension will help the model to process binary features more correctly. And we have enough of them. The graphs also show that the distributions of features visually differ in peak and slope. The data should be normalized.","55f59ce7":"# Eliminating class imbalance for LDA","4dc866af":"## Continuous variables","eed175ce":"# Decision Tree - Catboost\nTo improve the quality of the model and reduce the risk of retraining, the gradient boosting algorithm over an ensemble of trees developed by Yandex (Catboost) will be used. Catboost is currently the best implementation of gradient boosting in terms of speed\/quality ratio.\n\nGradient boosting is a machine learning method that creates a crucial forecasting model in the form of an ensemble of weak forecasting models, usually decision trees. It builds the model in stages, allowing you to optimize an arbitrary differentiable loss function using a gradient (the direction of the fastest growth of the function).","a6a884e1":"\n#### The target feature is tagret-predisposition (1) or lack of predisposition (0) to diabetes mellitus.","7f0f8af5":"Obviously, the optimal maximum depth is 8.\nNext, we will find the optimal maximum number of\" trees \"in the\" random forest \" - n_estimators.","836ce777":"7 continuous variables, 6 binary variables (gender, smokes \/ does not smoke + 4). We will remove the is_smoking attribute, since the cigsPerDay attribute contains full information about smoking.","e646aa84":"# Context\n#### Introduction\nWorld Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This research intends to pinpoint the most relevant\/risk factors of heart disease as well as predict the overall risk using logistic regression\nData Preparation\n\n#### Task\nThe task is to predict whether patient have 10 year risk of coronary heart disease CHD or not. Additionally, participants also asked to create some data visualization about the data to gained actionable insight about the topic.\n\n#### Source\nThe dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,000 records and 15 attributes.\n#### Variables\nEach attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n\n#### Data Description\n##### Demographic:\n- Sex: male or female(\"M\" or \"F\")\n- Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n##### Behavioral\n- is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n- Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n##### Medical( history)\n- BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n- Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n- Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n- Diabetes: whether or not the patient had diabetes (Nominal)\n##### Medical(current)\n- Tot Chol: total cholesterol level (Continuous)\n- Sys BP: systolic blood pressure (Continuous)\n- Dia BP: diastolic blood pressure (Continuous)\n- BMI: Body Mass Index (Continuous)\n- Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n- Glucose: glucose level (Continuous)\n##### Predict variable (desired target)\n- 10 year risk of coronary heart disease CHD(binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)\n\n\nSource: https:\/\/www.kaggle.com\/christofel04\/cardiovascular-study-dataset-predict-heart-disea?select=test.csv","36cd763d":"## Binary and categorical features","b1ebb1a2":"The number of patients with a certain degree of education depends inversely on the degree of education. The sex attribute does not contain a significant imbalance. There are not much more men than women. Most of the patients in the training sample had no history of diseases and chronic conditions. Let's put the skewness towards zero for the signs related to the medical history, the costs associated with the pre-target area, and leave them as they are.\nThe histogram of the target attribute shows that class 0 significantly prevails. And this can greatly affect the quality of the model. Therefore, it is worth balancing the size of the classes. We will use random undersampling for class","b5823db8":"## Quality\nEigenvalue: 0.000362\nThe eigenvalue is suspiciously small. This indicates that the discriminant function is not well chosen.\n\nQuality control based on test data\nWe will use the F-measure.\nThe model showed: 0.6771 \n68% of the quality in the medical model is unacceptably low. Therefore, it is impractical to use LDA at this stage, besides, some parameters look suspicious. You should try to consider a nonlinear model or a decision tree.","c4652f0d":"# Scaling","0c34530d":"Strange, strange, everyone is sick","39e5f442":"## Quality\n\nWe also use the F-measure. The error is measured by the logarithm of the quadratic error function.\nThe maximum quality on validation is 0.80, on training-0.94, which is much better than LDA.\n","a85f7ea6":"The expression for the discriminant function has the following coefficients","8e4ae226":"The average value of the discriminant function is approximately 0.0027. This means that the class boundary runs almost near zero. Below, we can observe how objects with discriminant function values greater than 0.0027 belong to class 1, and less than 0 in the test sample.","700b18a9":"# Vectorizing features","2e63b7df":"With optimal parameters, an increase in the quality of the model is expected.\nWe will train the model on cross validation. At each iteration of the algorithm, the model will be trained and validated on different random subsamples.\nLet's consider the feature weights set by the algorithm. \nTrees already look more reasonable than LDA, judging by the weights. The weight of medical indicators is noticeably higher, but age remains the most significant factor. Next is the number of cigarettes per day, followed by glucose level and systolic blood pressure. The least significant effect of the prevalentStroke attribute.","8c04ab6b":"The decision trees showed the best result, the potential of linear models, such as LDA, is much lower. However, even the best algorithm of decision trees demonstrates an insufficient percentage of quality for medical research.","cbbc7ace":"# Primary data analysis","e59bb7fa":"# Results on test sample","9699d540":"# LDA"}}