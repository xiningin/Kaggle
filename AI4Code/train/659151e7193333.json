{"cell_type":{"41559623":"code","fd978b61":"code","2e81d4f0":"code","823f5cd5":"code","40b8d167":"code","cc5dc165":"code","2f93faef":"code","beada8f1":"code","6b26f3ce":"code","2178a0eb":"code","5d046eb6":"code","301b1b9d":"code","8c51059f":"code","4c14191a":"code","1d843914":"code","f6e148bc":"code","b3e19ba9":"code","dcae5d09":"code","ebdf2cc2":"code","bb89d8d9":"code","585aef6c":"code","801cb507":"code","66469d7b":"code","83d02b6b":"code","1e4f6c9c":"code","426413f6":"code","718b2a06":"code","a2a6d6e9":"code","13798b40":"code","0f070e4e":"code","e5553e31":"code","3c694782":"code","2023e0e1":"code","1e01fc8d":"code","04917870":"code","d2614ea4":"code","7abe2b49":"code","9c81e381":"code","af542547":"code","13c31afc":"code","1b8b3c9c":"code","68ed66f2":"code","1a8d7278":"code","454258c2":"code","509782ab":"code","4ca066e6":"code","e6eb45d7":"code","e9d14c91":"code","ae8b0797":"code","fe84abc9":"code","7b80b23e":"code","7185e134":"markdown","87c10b40":"markdown","7c25ca80":"markdown","bf15e9d2":"markdown","44ee7da2":"markdown","3816150e":"markdown","e0bdceae":"markdown","6fb333cd":"markdown","1c686fc8":"markdown","7c1c7dd5":"markdown","d3d150bc":"markdown","83af167e":"markdown","e27bb25d":"markdown","cfb23703":"markdown","cea396f5":"markdown","d96b775f":"markdown","2678f56e":"markdown","d33665ba":"markdown","4f976ce8":"markdown","564b261c":"markdown","851c00b8":"markdown","d1ff400a":"markdown","0d07b9e2":"markdown","3a274938":"markdown","1c42865a":"markdown","016d8860":"markdown","dca204e9":"markdown","3855784e":"markdown","97d351ab":"markdown","01e0e8b7":"markdown","8414204a":"markdown","fbfb6f58":"markdown","27c7ae0d":"markdown","1d06bdc8":"markdown","37258f75":"markdown"},"source":{"41559623":"\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom xgboost import XGBRegressor\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')\n","fd978b61":"def load_n_clean():\n    # Read data\n    data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    \n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    df = clean(df)\n    df = encode_dtype(df)\n    \n    df_train = df.loc[df_train.index,:]\n    df_test = df.loc[df_test.index,:]\n    \n    return df_train, df_test","2e81d4f0":"data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n\nprint(sorted(list(df.Exterior2nd.unique())))","823f5cd5":"df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\ndf_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n\ndf = pd.concat([df_train, df_test], axis=0)","40b8d167":"# Object datatypes\ndf_object = df.select_dtypes(include=object).fillna('zzzNaN') # just to be able to sort temporarily, for easier reading\n\nobject_nuniques = df_object.nunique()\nobject_uniquevals = dict(zip(object_nuniques.index, [sorted(list(df_object[ind].unique()))\n                                                     for ind in df_object.columns]))\nobject_uniquevals\n# This helps us figure out which labels need re-labelling (correcting typo, renaming, etc.)","cc5dc165":"df_num = df.select_dtypes(exclude=object)\ndf_num.nunique()","2f93faef":"df_ordinal = df_num.loc[:,df_num.nunique().values <= 12]\ndf_num.drop(columns=df_ordinal.columns, inplace=True)\n\ndf_ordinal.nunique()","beada8f1":"g = sns.histplot(df_num[['GarageYrBlt','YearBuilt']])\ng.set(xlim=(min(df_num.YearBuilt), max(df_num.GarageYrBlt)))","6b26f3ce":"df_num.GarageYrBlt[df_num.GarageYrBlt > 2000].sort_values()","2178a0eb":"df_num['GarageYrBlt'].iloc[2592] = df_num['YearBuilt'].iloc[2592]\n\ng = sns.histplot(df_num[['GarageYrBlt','YearBuilt']])\ng.set(xlim=(min(df_num.YearBuilt), max(df_num.GarageYrBlt)))","5d046eb6":"df_num['GarageYrBlt'].dtype","301b1b9d":"def clean(df):\n    # Replace typos or those that don't agree with data_description.txt\n    df[\"MSZoning\"] = df[\"MSZoning\"].replace({\"C (all)\":\"C\"})\n    df[\"BldgType\"] = df[\"BldgType\"].replace(\n        {\"2fmCon\":\"2FmCon\",\n         \"Duplex\":\"Duplx\",\n         \"Twnhs\":\"TwnhsI\"} # CK: I assumed \"TwnhsI\" since only \"TwnhsE\" is available in dataset\n    )\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace(\n        {\"Brk Cmn\": \"BrkComm\", \"WdShng\":\"WdShing\",}\n    )\n    \n    # Some values of GarageYrBlt are corrupt, so we'll replace them\n    # with the year the house was built\n    \n    # CK: I see that there's one year that's 2207, so I think more appropriate would be\n    # to replace GarageYrBlt GREATER THAN 2010 to be the same as YearBuilt, not LESS THAN\n    # OR EQUAL TO\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt > 2010, df.YearBuilt)\n    df['GarageYrBlt'] = df['GarageYrBlt'].astype('int64')\n    \n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"ThreeSeasonPorch\",\n    }, inplace=True,\n    )\n    return df\n","8c51059f":"\n# The numeric features are already encoded correctly (`float` for\n# continuous, `int` for discrete), but the categoricals we'll need to\n# do ourselves. Note in particular, that the `MSSubClass` feature is\n# read as an `int` type, but is actually a (nominative) categorical.\n\n# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\",\n                \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\",\n                \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\",\n                \"Foundation\", \"Heating\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\ndef encode_dtype(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories: # CK: .cat accesses categorical properties,\n                                                  #     and .categories calls the unique categories\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","4c14191a":"features_ord = list(ordered_levels.keys())","1d843914":"# Old code for imputing\n\n# def impute(df):\n#     for name in df.select_dtypes(\"number\"):\n#         df[name] = df[name].fillna(0)\n#     for name in df.select_dtypes(\"category\"):\n#         df[name] = df[name].fillna(\"None\")\n#     return df\n\n# My version: Could have had this step integrated into the columntransformer creation in scoring,\n#             but seeting as I will be needing to impute again (e.g. when computing MI scores),\n#             I'll just write a function\n\n## PROBLEMATIC; the code gives a dataframe whose dtypes are all OBJECTS, even for numericals\n# def impute(df):\n#     imputer = make_column_transformer(\n#         (SimpleImputer(strategy='mean'), make_column_selector(dtype_include='number')),\n#         (SimpleImputer(strategy='most_frequent'), make_column_selector(dtype_include=['object','category'])),\n#     )\n    \n#     df_new = pd.DataFrame(imputer.fit_transform(df),\n#                           columns= df.select_dtypes('number').columns.to_list() +\n#                                    df.select_dtypes(['object','category']).columns.to_list())\n#     return df_new\n\n# df = df_train.copy()","f6e148bc":"df_train, df_test = load_n_clean()","b3e19ba9":"# Peek at the values\n# display(df_train)\n# display(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","dcae5d09":"display(df_train.select_dtypes(['object','category']).nunique().max())\ndisplay(df_test.select_dtypes(['object','category']).nunique().max())","ebdf2cc2":"# Old code\n\n# def score_dataset(X, y, model=XGBRegressor()):\n#     # Label encoding for categoricals\n#     #\n#     # Label encoding is good for XGBoost and RandomForest, but one-hot\n#     # would be better for models like Lasso or Ridge. The `cat.codes`\n#     # attribute holds the category levels.\n#     for colname in X.select_dtypes([\"category\"]):\n#         X[colname] = X[colname].cat.codes # converts categorical levels to numerical lables; NaN gets -1\n#     # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n#     log_y = np.log(y)\n#     score = cross_val_score(\n#         model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n#     )\n#     score = -1 * score.mean()\n#     score = np.sqrt(score)\n#     return score\n\n\n## My version\n\ndef prep_n_score(X, y, model=XGBRegressor(), scoring_method='neg_mean_squared_error', cv=5):\n    \n    # design the pipeline; columntransformer to impute each dtype separately, then train model\n    num_trans = make_pipeline(\n        SimpleImputer(strategy='mean')\n    )\n    \n    max1 = df_train.select_dtypes(['number']).nunique().max()\n    max2 = df_train.select_dtypes(['object','category']).nunique().max()\n    \n    cat_trans = make_pipeline(\n        SimpleImputer(strategy='most_frequent'),\n        OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=np.array([max1,max2]).max()+2)\n    )\n    \n    preprocessor = make_column_transformer(\n        # numerical\n        (num_trans, make_column_selector(dtype_include = 'number')), \n        # categorical\n        (cat_trans, make_column_selector(dtype_include = ['object','category'])), \n    )\n    \n    my_pipeline = make_pipeline(preprocessor, model)\n    \n    # compute the score using cross validation (Root Mean Squared Error of logs in the case of competition)\n    log_y = np.log(y)\n    scores = cross_val_score(\n        my_pipeline, X, log_y, scoring=scoring_method, cv=cv\n    )\n    \n    score = np.sqrt(abs(scores.mean()))\n    return score","bb89d8d9":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = prep_n_score(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","585aef6c":"def make_mi_scores(X, y):\n    df = X.copy()\n    \n    # CK: added this section -- Imputes if necessary\n    if df.isnull().any().any():\n        num_imputer = SimpleImputer(strategy='mean')\n        cat_imputer = SimpleImputer(strategy='most_frequent')\n        \n        df_num = df.select_dtypes('number')\n        df_cat = df.select_dtypes(['object','category'])\n        \n        df_num = pd.DataFrame(num_imputer.fit_transform(df_num), columns=df_num.columns)\n        df_cat = pd.DataFrame(cat_imputer.fit_transform(df_cat), columns=df_cat.columns)\n        \n        df = pd.concat([df_num, df_cat], axis=1)\n        \n    # turn categoricals into integers\n    for colname in df.select_dtypes([\"object\", \"category\"]):\n        df[colname], _ = df[colname].factorize()\n\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in df.dtypes]\n    \n    mi_scores = mutual_info_regression(df, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=df.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","801cb507":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)\nmi_scores","66469d7b":"plot_mi_scores(mi_scores[-10:])","83d02b6b":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]","1e4f6c9c":"X = df_train.copy()\ny_new = X.pop(\"SalePrice\")\nX_new = drop_uninformative(X, make_mi_scores(X,y_new))\n# X_newer = X_new.iloc[:,:-10]\n# X.info()\n\nscore_lowMI_drop = prep_n_score(X_new, y_new)\n\nprint(\"New score with low-MI features dropped: {:.5f} RMSLE\".format(score_lowMI_drop))","426413f6":"def ordinal_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X\n","718b2a06":"mi_scores.hist()","a2a6d6e9":"mi_scores[features_nom].sort_values(ascending=False)","13798b40":"def sort_nominal(df, x, y, ascending=True):\n    '''\n    To be used on nominal categorical variables; reorders their categories\n    based on category-grouped mean of target values\n    '''\n    # set index, then reorder categories\n    idx = df[[x,y]].groupby(x).mean().sort_values(by=y).index.to_list()\n    df[x].cat.reorder_categories(idx, ordered=True, inplace=True)\n    df = df[[x,y]].copy().sort_values(by=x)\n    \n    return df\n    \ndef plot_nom_vs_target(data, x, y='SalePrice', reorder=False, order=1, ax=None):\n    \n    if reorder:\n        data = sort_nominal(data, x, y)\n    \n    data[x],_ = data[x].factorize()\n    \n    if order == 1:\n        f = sns.regplot(data=data, x=x, y=y, x_estimator=np.mean, ax=ax)\n    else:\n        f = sns.regplot(data=data, x=x, y=y, x_estimator=np.mean, order=order, ci=None, ax=ax)\n    \n    return f\n","0f070e4e":"fig, axs = plt.subplots(6,4,figsize=(25,30))\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfor i in range(len(features_nom)):\n    x = features_nom[i]\n    y = 'SalePrice'\n    \n    df_plot = sort_nominal(df_train,x,y)\n    \n    data_x = df_plot[x].replace(to_replace='None',value=np.nan)\n    data = pd.concat([data_x, df_plot[y]], axis=1)\n    \n    data.dropna( axis=0, inplace=True)\n    \n    data_x = data[x].factorize()[0]\n    data_y = data[y]\n    \n    z1 = np.polyfit(data_x,data_y,1)\n    p1 = np.poly1d(z1)\n    y_fit1 = p1(data_x)\n    r2_fit1 = r2_score(data_y, y_fit1)\n    \n    z2 = np.polyfit(data_x,data_y,2)\n    p2 = np.poly1d(z2)\n    y_fit2 = p2(data_x)\n    r2_fit2 = r2_score(data_y, y_fit2)\n    \n    q,r = divmod(i,4)    \n    \n    plot_nom_vs_target(x=x, y=y, data=data, reorder=True, ax=axs[q,r])\n    plot_nom_vs_target(x=x, y=y, data=data, reorder=True, order=2, ax=axs[q,r])\n    \n    axs[q,r].set_title(('{:.3f} vs. {:.3f}').format(r2_fit1,r2_fit2), fontsize=13)\n\n    \n#     if r2_score(data_y, y_fit1) >= r2_score(data_y, y_fit2):\n#         plot_cat_vs_target(x=x, y=y, df=df_hi, reorder=True)\n        \n#     elif r2_score(data_y, y_fit1) < r2_score(data_y, y_fit2):\n#         plot_cat_vs_target(x=x, y=y, df=df_hi, reorder=True, order=2)","e5553e31":"features_num = X_new.select_dtypes('number').columns.to_list()\nfeatures_num = mi_scores[features_num].sort_values(ascending=False).index.to_list()\n\nfig, axs = plt.subplots(6,5,figsize=(25,30))\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfor i in range(len(features_num)):\n    x = features_num[i]\n    y = 'SalePrice'\n        \n    data_x = X_new[x].replace(to_replace='None',value=np.nan)\n    data = pd.concat([data_x, y_new], axis=1)\n    \n    data.dropna( axis=0, inplace=True)\n    \n    data_x = data[x].factorize()[0]\n    data_y = data[y]\n    \n    z1 = np.polyfit(data_x,data_y,1)\n    p1 = np.poly1d(z1)\n    y_fit1 = p1(data_x)\n    r2_fit1 = r2_score(data_y, y_fit1)\n    \n    z2 = np.polyfit(data_x,data_y,2)\n    p2 = np.poly1d(z2)\n    y_fit2 = p2(data_x)\n    r2_fit2 = r2_score(data_y, y_fit2)\n    \n    q,r = divmod(i,5)    \n    \n    sns.regplot(x=x, y=y, data=data, ax=axs[q,r])\n    sns.regplot(x=x, y=y, data=data, order=2, ax=axs[q,r])\n    \n    axs[q,r].set_title(('{:.3f} vs. {:.3f}').format(r2_fit1,r2_fit2), fontsize=13)\n","3c694782":"mi_scores[features_num].sort_values(ascending=False)","2023e0e1":"features_ord = mi_scores[features_ord].sort_values(ascending=False).index.to_list()\n\nfig, axs = plt.subplots(6,4,figsize=(25,30))\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfor i in range(len(features_ord)):\n    x = features_ord[i]\n    y = 'SalePrice'\n    \n    data_x = pd.Series(X_new[x].cat.codes, name=x)\n    data_y = y_new\n    data = pd.concat([data_x, data_y], axis=1)\n    \n    # to be able to illustrate the relationship more clearly, get rid of NaNs\n    data = data.drop(index=data[data[x]==-1].index)\n    data_x = data[x]\n    data_y = data[y]\n\n    z1 = np.polyfit(data_x,data_y,1)\n    p1 = np.poly1d(z1)\n    y_fit1 = p1(data_x)\n    r2_fit1 = r2_score(data_y, y_fit1)\n    \n    z2 = np.polyfit(data_x,data_y,2)\n    p2 = np.poly1d(z2)\n    y_fit2 = p2(data_x)\n    r2_fit2 = r2_score(data_y, y_fit2)\n    \n    q,r = divmod(i,4)    \n    \n    sns.regplot(x=x, y=y, data=data, ax=axs[q,r])\n    sns.regplot(x=x, y=y, data=data, order=2, ax=axs[q,r])\n    \n    axs[q,r].set_title(('{:.3f} vs. {:.3f}').format(r2_fit1,r2_fit2), fontsize=13)","1e01fc8d":"mi_scores[features_ord].sort_values(ascending=False)","04917870":"def set_nominal_order(df, sorter=df_train, target='SalePrice'):\n    '''\n    Set an order in nominal categorical variables based on their grouped mean target variable values.\n    Sorting must always be based on the training dataset.\n    '''\n    for cat in features_nom:\n        sortidx = sorter[[cat, target]].groupby(cat).mean().sort_values(by=target).index.to_list()\n        df[cat] = df[cat].cat.set_categories(sortidx, ordered=True)\n    \n    return df","d2614ea4":"mi_scores.head(25)","7abe2b49":"[(name, mi_scores.loc[name]) for name in mi_scores.index if ('SF' in name) or ('Area' in name)]","9c81e381":"def mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X['SqrtMasVnrArea'] = np.sqrt(df.MasVnrArea)    \n    X['BsmtFin1Info'] = df.BsmtQual.cat.codes * df.BsmtFinSF1 * df.BsmtFinType1.cat.codes\n    X['Outdoor^2'] = (df.OpenPorchSF + df.WoodDeckSF + df.EnclosedPorch + df.ScreenPorch) ** 2\n    X['SqrtLivingAreas'] = np.sqrt(df.GrLivArea) + np.sqrt(df.TotalBsmtSF)\n    \n    area = df[['LotFrontage','LotArea']].replace({0:1})\n    X['LotInfo'] = np.log(area.LotFrontage) + np.log(area.LotArea)\n    \n    ## Some of my failed attempts\n#     X['Overall'] = df.OverallQual.cat.codes * df.OverallCond.cat.codes\n#     X['GarageInfo'] = df.GarageArea \/ df.GarageCars\n#     X['ElectricalExp'] = np.exp(df.Electrical.cat.codes)\n#     X['TotalLivArea'] = df.GrLivArea + df.TotalBsmtSF * (df.BsmtQual.cat.codes)\/5\n\n#     ratio = np.sqrt(df.TotalBsmtSF) + (np.sqrt(df.BsmtFinSF1) - np.sqrt(df.BsmtUnfSF))\n#     X['BasementFinRatio'] = ratio.replace(np.inf, np.nan).fillna(0)\n\n#     ratio = (df.GrLivArea \/ df.TotalBsmtSF).replace(np.inf, np.nan)\n#     X['AbvToBelowGrdRatio'] = ratio.fillna(0)\n    \n#     ratio = (df.TotRmsAbvGrd \/ (df.FullBath + df.HalfBath + df.BsmtFullBath)).replace(np.inf, np.nan)\n#     X['BedToBathRatio'] = ratio.fillna(0)\n    \n    ## Old features; they all made the score worse\n#     X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n#     X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n#     X[\"TotalOutsideSF\"] = \\\n#         df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n#         df.ThreeSeasonporch + df.ScreenPorch\n    return X\n\n\ndef interactions(df):\n#     X = pd.get_dummies(df.Exterior2nd, prefix=\"Ext2nd\")\n#     X = X.mul(df.SecondFlrSF, axis=0)\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n#         \"ThreeSeasonPorch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1) \n    return X\n\n\n# def break_down(df):\n#     X = pd.DataFrame()\n#     X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n#     return X\n\n\n# def group_transforms(df):\n#     X = pd.DataFrame()\n#     X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n#     return X\n\n\n# ## Used this section to score the changes in above transformations\n# df_train, df_test = load_n_clean()\n# X_train = create_features(df_train)\n# y_train = df_train.loc[:, \"SalePrice\"]\n\n# # X_train.columns\n# prep_n_score(X_train, y_train)","af542547":"mi_scores[features_num]","13c31afc":"\ncluster_features = [\n#     \"SecondFlrSF\",\n#     \"WoodDeckSF\",\n#     'GarageArea',\n#     \"EnclosedPorch\",\n    'ScreenPorch',\n#     \"BsmtFinType1\",\n#     'YearRemodAdd'\n]\n\ndef cluster_labels(df, features, n_clusters=2):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=2):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd\n\n# ## Used this section to score the changes in above transformations\n# df_train, df_test = load_n_clean()\n# X_train = create_features(df_train)\n# y_train = df_train.loc[:, \"SalePrice\"]\n\n# # X_train.columns\n# prep_n_score(X_train, y_train)","1b8b3c9c":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    \n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    \n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,\n        \n        # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","68ed66f2":"X_expt = df_train[mi_scores[features_num].index[:10]].copy()\n\nsimpImp = SimpleImputer(strategy='mean')\nX_expt = pd.DataFrame(simpImp.fit_transform(X_expt), columns=X_expt.columns)\n\npca_expt, X_pca_expt, loadings_expt = apply_pca(X_expt)\nplot_variance(pca_expt)","1a8d7278":"sns.clustermap(loadings_expt,\n               cmap='bwr',\n               vmin=-1.0,\n               vmax=1.0,)","454258c2":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"PCinsp1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"PCinsp2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    X['PCinsp3'] = df.FirstFlrSF * df.TotalBsmtSF\n#     X['PCinsp4'] = df.SecondFlrSF * df.BedroomAbvGr\n    #(df.GrLivArea * df.TotRmsAbvGrd)\/df.FirstFlrSF\n    # df.YearBuilt \/ (df.TotRmsAbvGrd * df.GrLivArea)\n    return X\n\n\n## This turned out to be unhelpful\n# def pca_components(df, features):\n#     X = df.loc[:, features]\n#     _, X_pca, _ = apply_pca(X)\n#     return X_pca\n\n\n# pca_features = list(mi_scores[features_num][:2].index)\n#     [\n#     \"GarageArea\",\n#     \"YearRemodAdd\",\n#     \"TotalBsmtSF\",\n#     \"GrLivArea\",\n# ]\n\n# ## Used this section to score the changes in above transformations\n# df_train, df_test = load_n_clean()\n# X_train = create_features(df_train)\n# y_train = df_train.loc[:, \"SalePrice\"]\n\n# # X_train.columns\n# prep_n_score(X_train, y_train)","509782ab":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","4ca066e6":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new\n\n# ## Used this section to score the changes in above transformations\n# df_train, df_test = load_n_clean()\n# X_train = create_features(df_train)\n# y_train = df_train.loc[:, \"SalePrice\"]\n\n# # X_train.columns\n# prep_n_score(X_train, y_train)","e6eb45d7":"\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n","e9d14c91":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    X = set_nominal_order(X)\n        \n    # Lesson 2 - Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n#     X = X.join(interactions(X))\n    X = X.join(counts(X))\n#     X = X.join(break_down(X))\n#     X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering\n    X = X.join(cluster_labels(X, cluster_features, n_clusters=2))\n#     X = X.join(cluster_distance(X, cluster_features, n_clusters=2))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n#     X = X.join(pca_components(X, pca_features))\n#     X = X.join(indicate_outliers(X))\n\n    X = ordinal_encode(X)\n    \n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf_train, df_test = load_n_clean()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nprep_n_score(X_train, y_train)","ae8b0797":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\n# cses = [0.35, 0.4, 0.45, 0.5]\n# scores = dict()\n\n# for cs in cses:\n    \nxgb_params = dict(\n    max_depth=4,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.05,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=5000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=2,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.4,  # f1ction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.0,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.75,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=2,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\n#     prep_n_score(X_train, y_train, xgb)\n    \n#     scores[cs] = prep_n_score(X_train, y_train, xgb)\n\n# scores","fe84abc9":"xgb","7b80b23e":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y_train))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","7185e134":"Use it like:\n\n```\nencoder = CrossFoldEncoder(MEstimateEncoder, m=1)\nX_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n```\n\nYou can turn any of the encoders from the [`category_encoders`](http:\/\/contrib.scikit-learn.org\/category_encoders\/) library into a cross-fold encoder. The [`CatBoostEncoder`](http:\/\/contrib.scikit-learn.org\/category_encoders\/catboost.html) would be worth trying. It's similar to `MEstimateEncoder` but uses some tricks to better prevent overfitting. Its smoothing parameter is called `a` instead of `m`.\n\n## Create Final Feature Set ##\n\nNow let's combine everything together. Putting the transformations into separate functions makes it easier to experiment with various combinations. The ones I left uncommented I found gave the best results. You should experiment with you own ideas though! Modify any of these transformations or come up with some of your own to add to the pipeline.","87c10b40":"### Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly.\n\nIn the tutorial, the imputing strategy used was replacing with `0` for missing numeric values and `\"None\"` for missing categorical values.\n\nIn my version, I will instead be creating a pipeline that will contain the imputation step, and this pipeline will be supplied to the scoring function. This way, imputation is applied separately and consistently to train, validation, and test data.\n\nLater on, I will also try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.\n","7c25ca80":"Later, we'll add the `drop_uninformative` function to our feature-creation pipeline.\n\n# Step 3 - Create Features #\n\nNow we'll start developing our feature set.\n\nTo make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set. It will look something like this:\n\n```\ndef create_features(df):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    X = X.join(create_features_1(X))\n    X = X.join(create_features_2(X))\n    X = X.join(create_features_3(X))\n    # ...\n    return X\n```\n\n*WILL THIS BE NECESSARY?* Here, I'll define the ordinal encoding step:","bf15e9d2":"Here are some ideas for other transforms you could explore:\n- Interactions between the quality `Qual` and condition `Cond` features. `OverallQual`, for instance, was a high-scoring feature. You could try combining it with `OverallCond` by converting both to integer type and taking a product.\n- Square roots of area features. This would convert units of square feet to just feet.\n- Logarithms of numeric features. If a feature has a skewed distribution, applying a logarithm can help normalize it.\n- Interactions between numeric and categorical features that describe the same thing. You could look at interactions between `BsmtQual` and `TotalBsmtSF`, for instance.\n- Other group statistics in `Neighboorhood`. We did the median of `GrLivArea`. Looking at `mean`, `std`, or `count` could be interesting. You could also try combining the group statistics with other features. Maybe the *difference* of `GrLivArea` and the median is important?\n\n## k-Means Clustering ##\n\nThe first unsupervised algorithm we used to create features was k-means clustering. We saw that you could either use the cluster labels as a feature (a column with `0, 1, 2, ...`) or you could use the *distance* of the observations to each cluster. We saw how these features can sometimes be effective at untangling complicated spatial relationships.","44ee7da2":"Just tuning these by hand can give you great results. However, you might like to try using one of scikit-learn's automatic [hyperparameter tuners](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html). Or you could explore more advanced tuning libraries like [Optuna](https:\/\/optuna.readthedocs.io\/en\/stable\/index.html) or [scikit-optimize](https:\/\/scikit-optimize.github.io\/stable\/).\n\nHere is how you can use Optuna with XGBoost:\n\n```\nimport optuna\n\ndef objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\nxgb_params = study.best_params\n```\n\nCopy this into a code cell if you'd like to use it, but be aware that it will take quite a while to run. After it's done, you might enjoy using some of [Optuna's visualizations](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/005_visualization.html).\n\n# Step 5 - Train Model and Create Submissions #\n\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n- create your feature set from the original data\n- train XGBoost on the training data\n- use the trained model to make predictions from the test set\n- save the predictions to a CSV file","3816150e":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/221677) to chat with other Learners.*","e0bdceae":"And here are transforms that produce the features from the Exercise 5. You might want to change these if you came up with a different answer.\n","6fb333cd":"# Step 4 - Hyperparameter Tuning #\n\nAt this stage, you might like to do some hyperparameter tuning with XGBoost before creating your final submission.","1c686fc8":"## Principal Component Analysis ##\n\nPCA was the second unsupervised model we used for feature creation. We saw how it could be used to decompose the variational structure in the data. The PCA algorithm gave us *loadings* which described each component of variation, and also the *components* which were the transformed datapoints. The loadings can suggest features to create and the components we can use as features directly.\n\nHere are the utility functions from the PCA lesson:","7c1c7dd5":"### Clean Data ###\n\nSome of the categorical features in this dataset have what are apparently typos in their categories:","d3d150bc":"Uncomment and run this cell if you'd like to see what they contain. Notice that `df_test` is missing values for `SalePrice`.","83af167e":"## Establish Baseline ##\n\nFinally, let's establish a baseline score to judge our feature engineering against.\n\nHere is the function we created in Lesson 1 that will compute the cross-validated RMSLE score for a feature set. We've used XGBoost for our model, but you might want to experiment with other models.\n","e27bb25d":"Adding the cluster labels based on clusters of `ScreenPorch`, which was chosen based on its distribution with respect to the target, further lowered the score to approximately **0.12510**.","cfb23703":"Recording some scores (these account for the subsequent steps already):\n- With no mathematical transforms: 0.13378549348075017\n- Set nominal order: 0.1334364060148393\n- Removed `interactions()`, `break_down()`, `group_transforms()` from `create_features()`: 0.13232179964070284\n- Added `SqrtMasVnrArea`: 0.1320468365263976\n- Added `BsmtFin1Info`: 0.13171299038109188\n- Added `Outdoor`: 0.13163751315214625\n- Added `SqrtLivingAreas`: 0.12860960026477253\n- Added `LotInfo`: 0.12555416070786582\n","cea396f5":"Let's look at our feature scores again:","d96b775f":"To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/notebooks) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","2678f56e":"You can see that we have a number of features that are highly informative and also some that don't seem to be informative at all (at least by themselves). As we talked about in Tutorial 2, the top scoring features will usually pay-off the most during feature development, so it could be a good idea to focus your efforts on those. On the other hand, training on uninformative features can lead to overfitting. So, the features with 0.0 scores we'll drop entirely:","d33665ba":"## Target Encoding ##\n\nNeeding a separate holdout set to create a target encoding is rather wasteful of data. In *Tutorial 6* we used 25% of our dataset just to encode a single feature, `Zipcode`. The data from the other features in that 25% we didn't get to use at all.\n\nThere is, however, a way you can use target encoding without having to use held-out encoding data. It's basically the same trick used in cross-validation:\n1. Split the data into folds, each fold having two splits of the dataset.\n2. Train the encoder on one split but transform the values of the other.\n3. Repeat for all the splits.\n\nThis way, training and transformation always take place on independent sets of data, just like when you use a holdout set but without any data going to waste.\n\nIn the next hidden cell is a wrapper you can use with any target encoder:","4f976ce8":"### Encode the Statistical Data Type ###\n\nPandas has Python types corresponding to the standard statistical types (numeric, categorical, etc.). Encoding each feature with its correct type helps ensure each feature is treated appropriately by whatever functions we use, and makes it easier for us to apply transformations consistently. This hidden cell defines the `encode` function:","564b261c":"## Load Data ##\n\nAnd now we can call the data loader and get the processed data splits:","851c00b8":"These are only a couple ways you could use the principal components. You could also try clustering using one or more components. One thing to note is that PCA doesn't change the distance between points -- it's just like a rotation. So clustering with the full set of components is the same as clustering with the original features. Instead, pick some subset of components, maybe those with the most variance or the highest MI scores.\n\nFor further analysis, you might want to look at a correlation matrix for the dataset:","d1ff400a":"Lowered the score to approximately **0.12509**.","0d07b9e2":"We can reuse this scoring function anytime we want to try out a new feature set. We'll run it now on the processed data with no additional features and get a baseline score:","3a274938":"Some notes:\n- From the scatterplots between each of the numerical features and the target variable, I think some of these can really benefit from clustering. Especially when there seems to be a more clear relationship when not accounting for the zero values (which probably were just non-applicable features). I see this in features like `SecondFlrSF`,`WoodDeckSF`, and `EnclodsedPrch`.\n- I thought about somehow not accounting for those, but we can't just remove datapoints because they're outliers. So, the best approach is probably to cluster them first, then fit different relationships to each group.","1c42865a":"Seemed like there was one corrupted value for `GarageYrBlt`, so we'll just change that one to be equal to the year that the house was built.","016d8860":"NOTE: My most recent submission was scored 0.12652\n\nNew scores:\n- Learning rate 0.05: 0.12353491164100372\n- Lambda 1.75: 0.12192367627451799","dca204e9":"Once again, there were some variables that could benefit from clustering\/imputing because outliers are ruining fits. Interestingly, for ordinal variables this was constrained to the quality descriptors, such as `OverallCond`,`BsmtCond`,`GarageCond`, `ExterCond`, and `GarageQual`. There was also `Electrical` which seemed to have an exponential relationship with the target. Also, maybe `BsmtFinType1` can benefit from clustering into two groups, since it kind of looks quadratic.","3855784e":"# Introduction #\n\nHere's my version of the feature engineering project for the [House Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition.\n\n# Step 1 - Preliminaries #\n## Imports and Configuration ##\n\nWe'll start by importing the packages we used in the exercises and setting some notebook defaults.","97d351ab":"In my version, when I used the `objective='reg:squarelogerror'` and `scoring=neg_mean_squared_log_error`, removing these features didn't lead to any gain in score. It was a little suspicious, because the score is identical all the way to its 10th decimal place. When I switched backed to the tutorial's way, `np.log`ing the `y` & using the default objective (square error) for `XGBRegressor` and scoring (MSE), it behaves more normally; scores **did improve** when low-MI score features were dropped. I'll stick with this method, especially since it seems that the scale of these scores match that of the competition.","01e0e8b7":"This baseline score helps us to know whether some set of features we've assembled has actually led to any improvement or not.\n\n# Step 2 - Feature Utility Scores #\n\nIn Lesson 2 we saw how to use mutual information to compute a *utility score* for a feature, giving you an indication of how much potential the feature has. This hidden cell defines the two utility functions we used, `make_mi_scores` and `plot_mi_scores`: ","8414204a":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The data we used in the course was a bit simpler than the competition data. For the *Ames* competition dataset, we'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nUnlike the tutorial, however, we'll be loading and cleaning data in a separate function that does not include the latter preprocessing steps (encoding and imputing). Instead, we'll have them incorporated into the scoring function that can take care of those. This is to ensure that train-test splitting takes place before any preprocessing has the opportunity to introduce data leakage.","fbfb6f58":"A label encoding is okay for any kind of categorical feature when you're using a tree-ensemble like XGBoost, even for unordered categories. If you wanted to try a linear regression model (also popular in this competition), you would instead want to use a one-hot encoding, especially for the features with unordered categories.\n\n## Create Features with Pandas ##\n\nHere, I want to explore the relationships between each feature and the target variable in a systematic manner.","27c7ae0d":"Comparing these to `data_description.txt` shows us what needs cleaning. We'll take care of a couple of issues here, but you might want to evaluate this data further.","1d06bdc8":"Groups of highly correlated features often yield interesting loadings.\n\n### PCA Application - Indicate Outliers ###\n\nIn Exercise 5, you applied PCA to determine houses that were **outliers**, that is, houses having values not well represented in the rest of the data. You saw that there was a group of houses in the `Edwards` neighborhood having a `SaleCondition` of `Partial` whose values were especially extreme.\n\nSome models can benefit from having these outliers indicated, which is what this next transform will do.","37258f75":"You could also consider applying some sort of robust scaler from scikit-learn's `sklearn.preprocessing` module to the outlying values, especially those in `GrLivArea`. [Here](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html) is a tutorial illustrating some of them. Another option could be to create a feature of \"outlier scores\" using one of scikit-learn's [outlier detectors](https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html)."}}