{"cell_type":{"c91e0f56":"code","16a496c9":"code","7e130980":"code","361c0c6c":"code","a760d934":"code","0e05a8fa":"code","0dfc41da":"code","d0940d22":"code","0749f0d5":"code","f7554ee4":"code","6963dbfd":"code","912852f1":"code","10b17445":"code","a85cd801":"code","86c1b522":"code","32789faa":"code","d4d763d6":"code","8be9462b":"code","7a088e7f":"code","f0184b26":"code","971bc8e9":"code","d9c99db9":"code","30c397fe":"code","8c84fbfb":"code","b28a3d31":"code","84708617":"code","804a7542":"code","83889330":"code","990d936a":"code","8ff13ca3":"code","96655e21":"code","39dbf24b":"code","ae61ac82":"code","b8eb5ef6":"code","3d49ddc6":"code","e82cd5ea":"code","cb6c3070":"code","5edb0258":"code","948ed41c":"code","eeb2e6a2":"code","a669eedc":"code","2076e845":"code","66a4e723":"code","1d2ff0c5":"code","3aabd5e5":"code","890a7fe9":"code","ec1de927":"code","12cec5cb":"code","35c50e33":"code","3aa430fb":"code","f50bef77":"code","ebe58de7":"code","5801c28f":"code","cf96e07f":"code","ab8db14f":"code","a75cb4fb":"code","3fb5ed25":"code","cb8fc05a":"code","4c69de5d":"code","339a201f":"code","9460286f":"code","28339726":"code","9e843082":"code","68a62f06":"code","fce01faa":"code","f2f0c3ab":"code","87d7c309":"code","48486d82":"code","d8d8aa52":"code","47cc148a":"code","68b31380":"code","b5dec2d0":"code","4292f28f":"code","1ae3e0b6":"code","a81e7a31":"code","e1420dbd":"code","7cd99cf4":"code","69fba2c4":"code","416f35c0":"code","93118665":"code","57810267":"code","d0485c85":"code","e5f257fd":"code","b98e145e":"code","73558a00":"code","f8f2a3c5":"code","7aba68a8":"code","e957eacc":"code","82263b5b":"code","81343be7":"code","c65ccfc0":"code","242e1ba6":"code","cb1df6cf":"code","f9c4a0ee":"code","ad09a99b":"code","acfa7e57":"code","ab7a7071":"code","0382ec73":"code","a6309ba4":"code","445ecfb3":"code","4f4f4c6b":"code","de534210":"code","882cfa3e":"code","7e19919b":"code","e8eb281a":"code","0a31a7ec":"code","5d6e8a52":"code","a60e9983":"code","2143b93f":"code","1952d9db":"code","d4300732":"code","51b4a303":"code","151df17d":"code","5310e902":"code","d518046a":"code","40e71407":"code","7f5e3fff":"code","e3ad7cc2":"code","c2676622":"code","9053aeb0":"code","d2380fda":"code","a86a44fd":"code","e35eaf0e":"code","18e61003":"code","ec974f1a":"code","81f03d76":"code","fa916d5c":"code","3b95c1cd":"code","e6fff539":"code","6ef10490":"code","3f9870df":"code","7a061ab5":"code","f940d66f":"code","346129b7":"code","5c58de13":"code","bbf665d1":"code","4bfe1293":"code","d3c510d0":"code","4ae2604e":"code","5cae2110":"code","40ead4a3":"code","11ef1fa9":"code","2cce1aa6":"code","9910ceaa":"code","dcbc36e2":"code","93d32ffe":"code","aaf0933b":"code","70050e7c":"code","9a3eb14e":"code","4793d030":"code","2790cfb9":"code","a9916ae8":"code","21357112":"code","a5e6b13d":"code","d094c488":"code","d3a4d6ff":"code","b5df048c":"code","e86ac168":"code","5ffcb83a":"code","c8f19add":"code","c5760451":"code","92c907dd":"code","e6c44fc9":"code","0ccdf624":"code","4c3b1b2c":"code","d77512ca":"code","40cb996c":"code","063d6d5d":"code","5d23868f":"code","9bb412db":"code","f05ec2c2":"code","7db4a59a":"code","cbd73281":"code","c7bc6cf3":"code","66ce0e3c":"code","f72d4382":"code","a5cd91e5":"code","786cc3a6":"code","498e9fed":"code","db1902b7":"code","02799df0":"code","8699ad34":"code","9f758f79":"code","bb072117":"code","71dad3d7":"code","70f0269d":"code","cb7ef103":"code","4d8a41cd":"code","a7aab6cc":"code","1def20f0":"code","082cbb02":"code","9801ff1f":"code","198f9a6f":"code","73d4f94b":"code","30adde09":"code","0a635ad2":"code","5ce25308":"code","933e8905":"code","6a7b6d97":"code","7d57ae51":"code","8774394e":"code","b4fdcbba":"code","14f67c2a":"markdown","5ab166b7":"markdown","6074461e":"markdown","8d470696":"markdown","3db5bfbc":"markdown","e9a43bec":"markdown","31295f12":"markdown","bee4cbea":"markdown","c5414374":"markdown","df7467d7":"markdown","1cb75e30":"markdown","aea6bc36":"markdown","538f3aba":"markdown","fa955a54":"markdown","0060a11a":"markdown","30123ec1":"markdown","054b4f00":"markdown","23359a0c":"markdown","2a425a24":"markdown","3373401f":"markdown","9e2518d2":"markdown","e9bd76ce":"markdown","c3f7c2f1":"markdown"},"source":{"c91e0f56":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from plotly.offline import iplot\n%matplotlib inline\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)\n","16a496c9":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","7e130980":"# # a = np.zeros((train.shape[0],))\n# # print(a)\n# train.shape[0]","361c0c6c":"# NFOLDS = 5\n# ntest = 4\n# oof_test_skf = np.empty((NFOLDS, ntest))\n# print(oof_test_skf)","a760d934":"train['SaleCondition'].value_counts()","0e05a8fa":"train['Electrical'].value_counts()","0dfc41da":"train['LowQualFinSF'].value_counts()","d0940d22":"train.isnull().sum()","0749f0d5":"# list of numerical variables\nnumerical_features = [feature for feature in train.columns if train[feature].dtypes != 'O']\n\nprint('Number of numerical variables: ', len(numerical_features))\n\n# visualise the numerical variables\ntrain[numerical_features].head()","f7554ee4":"# list of variables that contain year information\nyear_feature = [feature for feature in numerical_features if 'Yr' in feature or 'Year' in feature]\n\nyear_feature","6963dbfd":"## Numerical variables are usually of 2 type\n## 1. Continous variable and Discrete Variables\n\ndiscrete_feature=[feature for feature in numerical_features if len(train[feature].unique())<25 and feature not in year_feature+['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))\ndiscrete_feature","912852f1":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))\ncontinuous_feature","10b17445":"data = train.copy()\ncategorical_features=[feature for feature in train.columns if data[feature].dtypes=='O']\nlen(categorical_features)\ncategorical_features","a85cd801":"#NUMBER OF CAREGOURY IN A CATEGORICAL FEATURE\nfor feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(train[feature].unique())))","86c1b522":"# HOW THE continious neumerical VARIABLES ARE CHANGING WITH TERGATE VARIABLE\n# BAR COUNT OF CATEGORICAL VATIABLE\n# HISTOGRAM FOR CONTINIOUS NEUMERICAL VARIABLE TO FIND OUT THE DISTRIBUTION\n# BOX PLOT FOR FINDING OUTLIERS\n# CDFs","32789faa":"for i in continuous_feature:\n    \n    plt.scatter(train['SalePrice'],train[i],color = 'tomato')\n    plt.xlabel(\"i\",size = 15)\n    plt.ylabel('SalePrice',size = 15)\n    plt.title('SalePrice Vs '+ i,size = 15)\n    plt.show()","d4d763d6":"for i in continuous_feature:\n    train[i]\n","8be9462b":"train[continuous_feature[0]].unique()","7a088e7f":"#LOGNORMAL DISTRIBUTION\n\nfor feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()","f0184b26":"features_with_na=[features for features in train.columns if train[features].isnull().sum()>1]\nfeatures_with_na\n","971bc8e9":"tips = sns.load_dataset(\"tips\")\ntips.head()","d9c99db9":"for i in categorical_features:\n    sns.countplot(x=i,data=train)\n    plt.show()","30c397fe":"categorical_features[2]","8c84fbfb":"for feature in discrete_feature:\n    data=train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar(rot=0)\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","b28a3d31":"for i in continuous_feature:\n    sns.set_style('whitegrid')\n    plt.figure(figsize=(10,7))\n    sns.distplot(train[i].dropna(),bins=20,hist_kws={'edgecolor':'black'},kde_kws={'linewidth': 2},kde = False,norm_hist=False,color = 'darkblue',hist=True)\n    plt.xlabel(i,size = 15)\n    plt.ylabel('percentage of distribution',size = 15)\n    plt.title('histogram of '+ i,size = 15)\n#     plt.xticks(np.arange(0,10,1))\n#     plt.figure(figsize=(12,8))\n    plt.show()","84708617":"for feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","804a7542":"\nfrom string import ascii_letters\n\ncorr = train.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# plt.figure(figsize=(18,16))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Fig:1',size=15)\nplt.show()","83889330":"# train['MoSold'].plot(kind='bar',stacked = True)","990d936a":"categorical_features","8ff13ca3":"'''for feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.line(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()'''","96655e21":"plt.figure(figsize=(12,10))\nax = sns.lineplot(x=\"GarageArea\", y=\"SalePrice\", data=train,color=\"coral\", label=\"line\")\nplt.show()","39dbf24b":"'''from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles = len(train['GarageArea']),output_distribution = 'normal')\n\n## transforming above distributions to Normal distribution ##\nX = qt.fit_transform(train['GarageArea'])\n# Y = qt.fit_transform(Y)\nprint('distributions transformed')'''","ae61ac82":"'''GarageArea1 = train['GarageArea'].values.reshape(-1,1)\nGarageArea2 = train['GarageArea'].values.reshape(1,-1)'''","b8eb5ef6":"'''from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(n_quantiles = len(GarageArea1),output_distribution = 'normal')\n\n## transforming above distributions to Normal distribution ##\ntrain['GarageArea1'] = qt.fit_transform(GarageArea1)\n# Y = qt.fit_transform(Y)\nprint('distributions transformed')'''","3d49ddc6":"# from sklearn.preprocessing import QuantileTransformer\n# qt1 = QuantileTransformer(n_quantiles = len(GarageArea2),output_distribution = 'normal')\n\n# ## transforming above distributions to Normal distribution ##\n# Y = qt1.fit_transform(GarageArea2)\n# # Y = qt.fit_transform(Y)\n# print('distributions transformed')","e82cd5ea":"# plt.figure(figsize = (10,4), dpi = 120)\n\n# #Plotting transformed exponential\n# # plt.subplot(121)\n# plt.hist(Y, bins = 100)\n# plt.title(\"transformed 2\")","cb6c3070":"'''plt.figure(figsize = (10,4), dpi = 120)\n\n#Plotting transformed exponential\n# plt.subplot(121)\nplt.hist(train['GarageArea1'], bins = 100)\nplt.title(\"transformed exponential\")'''","5edb0258":"# plt.figure(figsize=(12,10))\n# ax = sns.lineplot(x=\"GarageArea1\", y=\"SalePrice\", data=train,color=\"coral\", label=\"line\")\n# plt.show()","948ed41c":"for feature in continuous_feature:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        ax1 = sns.lineplot(x=feature, y=\"SalePrice\", data=data,color=\"coral\", label=\"line\")\n        #     plt.line(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()","eeb2e6a2":"continuous_feature","a669eedc":"for feature in continuous_feature:\n#     data=train.copy()\n#     if 0 in data[feature].unique():\n#         pass\n#     else:\n#     data[feature]=np.log(data[feature])\n#     data['SalePrice']=np.log(data['SalePrice'])\n    ax1 = sns.lineplot(x=feature, y=\"SalePrice\", data=data,color=\"coral\", label=\"line\")\n    #     plt.line(data[feature],data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalesPrice')\n    plt.title(feature)\n    plt.show()","2076e845":"# def bar_chart(feature):\n#     survived = train[train['Survived']==1][feature].value_counts()\n#     dead = train[train['Survived']==0][feature].value_counts()\n#     df = pd.DataFrame([survived,dead])\n#     df.index = ['Survived','Dead']\n#     df.plot(kind='bar',stacked=True, figsize=(10,5))","66a4e723":"categorical_features","1d2ff0c5":"data = train.copy()\n\ndata['LotArea']=np.log(data['LotArea'])\ndata['SalePrice']=np.log(data['SalePrice'])\nax1 = sns.lineplot(x='LotArea', y=\"SalePrice\",hue= 'LotShape', data=data,color=\"coral\", label=\"LotArea\")\n#     plt.line(data[feature],data['SalePrice'])\nplt.xlabel('LotArea')\nplt.ylabel('SalesPrice')\nplt.title('LotArea')\nplt.legend()\nplt.show()\n\n\n\n# ax = sns.lineplot(x='LotArea',y='SalePrice',hue= 'LandContour',data = train,color=\"coral\", label=\"LotArea\")\n","3aabd5e5":"train['LandContour'].value_counts()","890a7fe9":"def bar_chart(feature):\n    Reg = train[train['LotShape']=='Reg'][feature].value_counts()\n    IR1 = train[train['LotShape']=='IR1'][feature].value_counts()\n    IR2 = train[train['LotShape']=='IR2'][feature].value_counts()\n    IR3 = train[train['LotShape']=='IR3'][feature].value_counts()\n    \n    df = pd.DataFrame([Reg,IR1,IR2,IR3])\n    df.index = ['Reg','IR1','IR2','IR3']\n    df.plot(kind='bar',stacked=True, figsize=(10,8))","ec1de927":"bar_chart('LandContour')\n","12cec5cb":"train['MSZoning'].value_counts()","35c50e33":"def bar_chart(feature):\n    RL = train[train['MSZoning']=='RL'][feature].value_counts()\n    RM = train[train['MSZoning']=='RM'][feature].value_counts()\n    FV = train[train['MSZoning']=='FV'][feature].value_counts()\n    RH = train[train['MSZoning']=='RH'][feature].value_counts()\n    C_all = train[train['MSZoning']=='C (all)'][feature].value_counts()\n    \n    df = pd.DataFrame([RL,RM,FV,RH,C_all])\n    df.index = ['RL','RM','FV','RH','C_all']\n    df.plot(kind='bar',stacked=True, figsize=(10,8))","3aa430fb":"bar_chart('Neighborhood')\n","f50bef77":"import scipy.stats as stats","ebe58de7":"data=train.copy()\ndataset_table=pd.crosstab(data['Neighborhood'],data['MSZoning'])\nprint(dataset_table)","5801c28f":"dataset_table.values\n","cf96e07f":"#Observed Values\nObserved_Values = dataset_table.values \nprint(\"Observed Values :-\\n\",Observed_Values)","ab8db14f":"val=stats.chi2_contingency(dataset_table)\n","a75cb4fb":"Expected_Values=val[3]\n","3fb5ed25":"no_of_rows=len(dataset_table.iloc[0:26,0])\nno_of_columns=len(dataset_table.iloc[0,0:6])\nddof=(no_of_rows-1)*(no_of_columns-1)\nprint(\"Degree of Freedom:-\",ddof)\nalpha = 0.05","cb8fc05a":"from scipy.stats import chi2\nchi_square=sum([(o-e)**2.\/e for o,e in zip(Observed_Values,Expected_Values)])\nchi_square_statistic=chi_square[0]+chi_square[1]","4c69de5d":"\ncritical_value=chi2.ppf(q=1-alpha,df=ddof)\nprint('critical_value:',critical_value)","339a201f":"#p-value\np_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)\nprint('p-value:',p_value)\nprint('Significance level: ',alpha)\nprint('Degree of Freedom: ',ddof)\nprint('p-value:',p_value)","9460286f":"if chi_square_statistic>=critical_value:\n    print(\"Reject H0,There is a relationship between Neighborhood and MSZoning\")\nelse:\n    print(\"Retain H0,There is no relationship between Neighborhood and MSZoning\")\n    \nif p_value<=alpha:\n    print(\"Reject H0,There is a relationship between Neighborhood and MSZoning\")\nelse:\n    print(\"Retain H0,There is no relationship between Neighborhood and MSZoning\")","28339726":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\n# data = train.copy\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","9e843082":"var = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","68a62f06":"var = 'MoSold'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","fce01faa":"var = 'YrSold'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","f2f0c3ab":"var = 'Neighborhood'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","87d7c309":"plt.figure(figsize=(10,8))\n\nfrom itertools import cycle, islice\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\n# missing.plot.bar()\n# sns.countplot(missing,color=\"salmon\")\nmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, 19))\n# my_colors = ['g', 'b']*5\n# my_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5\n# my_colors = [(x\/10.0, x\/20.0, 0.75) for x in range(0,1)]\n\nmissing.plot(kind='bar',color=my_colors)","48486d82":"corr_new_train=train.corr()\nplt.figure(figsize=(10,20))\nsns.heatmap(corr_new_train[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(60),vmin=-1, cmap='seismic', annot=True)","d8d8aa52":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","47cc148a":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=discrete_feature)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","68b31380":"# f = pd.melt(train, id_vars=['SalePrice'], value_vars=continuous_feature)\n# g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\n# g = g.map(sns.lineplot, \"value\", \"SalePrice\")","b5dec2d0":"train[continuous_feature].shape","4292f28f":"no_of_rows=len(dataset_table.iloc[0:26,0])\nno_of_rows","1ae3e0b6":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nimport numpy as np\nSEED = 42\n\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","a81e7a31":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\n# df_train = pd.read_csv('..\/input\/train.csv')\n# df_test = pd.read_csv('..\/input\/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\n# print('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","e1420dbd":"'''\n1.HANDEL MISSING VALUE\n2.LABEL ENCODING\n3.OUTLIER DETECTION AND HANDELING(performing log,sqrt,cbrt)\n4.SCALLING(using sklearn ScaleMinMax)\n\n\n'''","7cd99cf4":"## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(df_train[feature].isnull().mean(),4)))","69fba2c4":"\nfor feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=df_train[feature].median()\n    \n    ## create a new feature to capture nan values\n#     dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)\n    df_train[feature].fillna(median_value,inplace=True)\n    \ndf_train[numerical_with_nan].isnull().sum()","416f35c0":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n\nfor feature in num_features:\n    df_train[feature]=np.log(df_train[feature])\n    df_train[feature].hist(bins = 30)\n    plt.title('histogram of : {}'.format(feature))\n    plt.show()\n    \n    \n#########\n\n","93118665":"df_train.head()","57810267":"# with_zero_num = ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','2ndFlrSF','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','ScreenPorch']\n# for i in with_zero_num:\n#     df_train[i] = np.sqrt(df_train[i])\n#     df_train[i].hist(bins=30)\n#     plt.xlabel(i)\n#     plt.title(i)\n#     plt.show()","d0485c85":"features_nan=[feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes=='O']\n\n## Replace missing value with a new label\ndef replace_cat_feature(df_train,features_nan):\n    data=df_train.copy()\n    data[features_nan]=data[features_nan].fillna('Missing')\n    return data\n\ndf_train=replace_cat_feature(df_train,features_nan)\n\ndf_train[features_nan].isnull().sum()","e5f257fd":"for feature in categorical_features:\n    temp=df_train.groupby(feature)['SalePrice'].count()\/len(df_train)\n    temp_df=temp[temp>0.01].index\n    df_train[feature]=np.where(df_train[feature].isin(temp_df),df_train[feature],'rear_cat')","b98e145e":"df_train.head()","73558a00":"cat_fe = [i for i in df_train.columns if df_train[i].dtype=='O']\ncat_fe","f8f2a3c5":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor i in cat_fe:\n    df_train[i]=label_encoder.fit_transform(df_train[i])","7aba68a8":"df_train.head()","e957eacc":"feature_scale=[feature for feature in df_train.columns if feature not in ['Id','SalePrice']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(df_train[feature_scale])","82263b5b":"scaler.transform(df_train[feature_scale])","81343be7":"# transform the train and test set, and add on the Id and SalePrice variables\ndata = pd.concat([df_train[['Id', 'SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(df_train[feature_scale]), columns=feature_scale)],\n                    axis=1)","c65ccfc0":"data.head()","242e1ba6":"# data.to_csv('Data_After_FE.csv')","cb1df6cf":"data1 = data.copy() \n\nY_train = data1['SalePrice']\nX_train = data1.drop(['SalePrice'],axis=1)\n# Y_train = data1['SalePrice']","f9c4a0ee":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.33, random_state=SEED)","ad09a99b":"# from sklearn.matrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef validation(y_val,y_pred):\n    rmse_val = sqrt(mean_squared_error(y_val,y_pred)) \n    return rmse_val\n","acfa7e57":"from sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(max_depth=2, random_state=0)\nregr.fit(x_train, y_train)\ny_pred = regr.predict(x_val)","ab7a7071":"validation(y_val,y_pred)","0382ec73":"df_test.head()","a6309ba4":"df_test.isnull().sum()","445ecfb3":"numerical_with_nan_test=[feature for feature in df_test.columns if df_test[feature].isnull().sum()>1 and df_test[feature].dtypes!='O']\nfor feature in numerical_with_nan_test:\n    print(\"{}: {}% missing value\".format(feature,np.around(df_train[feature].isnull().mean(),4)))","4f4f4c6b":"for feature in numerical_with_nan_test:\n    ## We will replace by using median since there are outliers\n    median_value_test=df_test[feature].median()\n    \n    ## create a new feature to capture nan values\n#     dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)\n    df_test[feature].fillna(median_value_test,inplace=True)\n    \ndf_test[numerical_with_nan_test].isnull().sum()","de534210":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n\nfor feature in num_features:\n    df_test[feature]=np.log(df_test[feature])","882cfa3e":"features_nan_test=[feature for feature in df_test.columns if df_test[feature].isnull().sum()>1 and df_test[feature].dtypes=='O']\n\n## Replace missing value with a new label\ndef replace_cat_feature(df_test,features_nan_test):\n#     data_test=df_test.copy()\n    df_test[features_nan_test]=df_test[features_nan_test].fillna('Missing')\n    return df_test\n\ndf_test=replace_cat_feature(df_test,features_nan_test)\n\n# df_test[features_nan_test].isnull().sum()","7e19919b":"categorical_var_test = [i for i in df_test.columns if df_test[i].dtype == 'O' ]\nlen(categorical_var_test)","e8eb281a":"# for feature in categorical_var_test:\n#     temp=df_test.groupby(feature)['SalePrice'].count()\/len(df_train)\n#     temp_df=temp[temp>0.01].index\n#     df_train[feature]=np.where(df_train[feature].isin(temp_df),df_train[feature],'rear_cat')","0a31a7ec":"numerical_features_test = [feature for feature in df_test.columns if df_test[feature].dtypes != 'O']\nyear_feature_test = [feature for feature in numerical_features_test if 'Yr' in feature or 'Year' in feature]\ndiscrete_feature_test=[feature for feature in numerical_features_test if len(df_test[feature].unique())<25 and feature not in year_feature_test+['Id']]\ncontinuous_feature_test=[feature for feature in numerical_features_test if feature not in discrete_feature_test+year_feature_test+['Id']]\n","5d6e8a52":"len(numerical_features_test)","a60e9983":"year_feature_test","2143b93f":"len(discrete_feature_test)","1952d9db":"continuous_feature_test","d4300732":"df_test.KitchenQual.dtype","51b4a303":"df_test.info()","151df17d":"df_test.isnull().sum()","5310e902":"df_test['Exterior1st']=df_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0])\ndf_test['Exterior2nd']=df_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0])\ndf_test['KitchenQual']=df_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0])\ndf_test['SaleType']=df_test['SaleType'].fillna(df_test['SaleType'].mode()[0])\n\n\n","d518046a":"con_feature = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageArea','GarageCars']\n\n\n\n\n\nfor feature in con_feature:\n    ## We will replace by using median since there are outliers\n    median_value_con=df_test[feature].median()\n    \n    ## create a new feature to capture nan values\n#     dataset[feature+'nan']=np.where(dataset[feature].isnull(),1,0)\n    df_test[feature].fillna(median_value_con,inplace=True)\n    \n# df_test[numerical_with_nan_test].isnull().sum()","40e71407":"df_test.isnull().sum()","7f5e3fff":"# categorical_var_test=['Exterior1st',\n#  'Exterior2nd',\n#  'MasVnrType',\n#  'ExterQual',\n#  'ExterCond',\n#  'Foundation',\n#  'BsmtQual',\n#  'BsmtCond',\n#  'BsmtExposure',\n#  'BsmtFinType1',\n#  'BsmtFinType2',\n#  'Heating',\n#  'HeatingQC',\n#  'CentralAir',\n#  'Electrical',\n#  'KitchenQual',\n#  'Functional',\n#  'FireplaceQu',\n#  'GarageType',\n#  'GarageFinish',\n#  'GarageQual',\n#  'GarageCond',\n#  'PavedDrive',\n#  'PoolQC',\n#  'Fence',\n#  'MiscFeature',\n#  'SaleType',\n#  'SaleCondition',]","e3ad7cc2":"from sklearn.preprocessing import LabelEncoder\nlabel_encod = LabelEncoder()\n\nfor i in categorical_var_test:\n    df_test[i]=label_encod.fit_transform(df_test[i])","c2676622":"df_test.head()","9053aeb0":"feature_scale_test=[feature for feature in df_test.columns if feature not in ['Id']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler_test=MinMaxScaler()\nscaler_test.fit(df_test[feature_scale_test])","d2380fda":"scaler_test.transform(df_test[feature_scale_test])","a86a44fd":"# transform the train and test set, and add on the Id and SalePrice variables\ndata_test = pd.concat([df_test[['Id']].reset_index(drop=True),\n                    pd.DataFrame(scaler_test.transform(df_test[feature_scale_test]), columns=feature_scale_test)],\n                    axis=1)","e35eaf0e":"data_test.head()","18e61003":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso,LinearRegression\n\n# select_from_model = SelectFromModel(Lasso(alpha = 0.005,random_state = SEED))\nselect_from_model = SelectFromModel(LinearRegression())\n\nselect_from_model.fit(X_train,Y_train)\n","ec974f1a":"select_from_model.get_support().sum()","81f03d76":"selected_features = X_train.columns[(select_from_model.get_support())]\nselected_features","fa916d5c":"X_train_fs_linear_regression = X_train[selected_features]","3b95c1cd":"X_train_fs_linear_regression.head()","e6fff539":"x_train_fs_linear_regression, x_val_fs_linear_regression, y_train_fs_linear_regression, y_val_fs_linear_regression=train_test_split(X_train_fs_linear_regression,Y_train,test_size = 0.33,random_state = SEED)","6ef10490":"print(x_train_fs_linear_regression.shape, x_val_fs_linear_regression.shape, y_train_fs_linear_regression.shape, y_val_fs_linear_regression.shape)","3f9870df":"x_train_fs_linear_regression.head()","7a061ab5":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nridge=Ridge()\nparameters= {'alpha':[x for x in [0.1,0.2,0.4,0.5,0.7,0.8,1]]}\n\nridge_reg=GridSearchCV(ridge, param_grid=parameters)\nridge_reg.fit(x_train_fs_linear_regression,y_train_fs_linear_regression)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)","f940d66f":"ridge_mod=Ridge(alpha=1)\nridge_mod.fit(x_train_fs_linear_regression,y_train_fs_linear_regression)\ny_pred_train=ridge_mod.predict(x_train_fs_linear_regression)\ny_pred_val=ridge_mod.predict(x_val_fs_linear_regression)\n\ny_pred_train_series = pd.Series(np.log(y_pred_train))\ny_pred_train_series_median = y_pred_train_series.median()\ny_pred_train_series_1=y_pred_train_series.fillna(y_pred_train_series_median)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(np.log(y_train_fs_linear_regression), y_pred_train_series_1))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(np.log(y_val_fs_linear_regression), np.log(y_pred_val))))) ","346129b7":"# np.sqrt(mean_squared_error(np.log(y_val_fs_linear_regression), np.log(y_pred_val)))","5c58de13":"# np.log(y_train_fs_linear_regression)","bbf665d1":"# y_pred_train_series = pd.Series(np.log(y_pred_train))\n# y_pred_train_series.isnull().sum()","4bfe1293":"# y_pred_train_series_median = y_pred_train_series.median()\n# y_pred_train_series_1=y_pred_train_series.fillna(y_pred_train_series_median)","d3c510d0":"# np.log(y_pred_train_series_1)","4ae2604e":"# mean_squared_error(np.log(y_train_fs_linear_regression),y_pred_train_series_1)","5cae2110":"# y_val_fs_linear_regression","40ead4a3":"# y_pred_val","11ef1fa9":"# data_test_fs1 = data_test[selected_features]\n# data_test_fs1.head()","2cce1aa6":"# y_pred_fs1_test = ridge_mod.predict(data_test)\n","9910ceaa":"# y_pred_fs1_test = ridge_mod.predict(data_test_fs1)\n# pred_fs1 = pd.DataFrame(y_pred_fs1_test)\n# sub_df = pd.read_csv('sample_submission.csv')\n# datasets_fs1 = pd.concat([sub_df['Id'],pred_fs1],axis = 1)\n# datasets_fs1.columns=['Id','SalePrice']\n# datasets_fs1.to_csv('submission_fs1.csv',index=False)\n","dcbc36e2":"# datasets_fs1.head()","93d32ffe":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nparams = {\"max_depth\":[15,20,25], \"n_estimators\":[27,30,33]}\nrf_reg = GridSearchCV(rf, params, cv = 10, n_jobs =10)\nrf_reg.fit(x_train, y_train)\nprint(rf_reg.best_estimator_)\nbest_estimator=rf_reg.best_estimator_\ny_pred_train = best_estimator.predict(x_train)\ny_pred_val = best_estimator.predict(x_val)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(np.log(y_train), np.log(y_pred_train)))))\nprint('Root Mean Square Error val = ' + str(np.sqrt(mean_squared_error(np.log(y_val), np.log(y_pred_val)))))","aaf0933b":"y_pred_val","70050e7c":"y_val","9a3eb14e":"# y_pred_rf_test = best_estimator.predict(data_test)\n# pred_rf = pd.DataFrame(y_pred_rf_test)\n# sub_df = pd.read_csv('sample_submission.csv')\n# datasets_rf = pd.concat([sub_df['Id'],pred_rf],axis = 1)\n# datasets_rf.columns=['Id','SalePrice']\n# datasets_rf.to_csv('submission_rf.csv',index=False)\n","4793d030":"# datasets_rf.head()","2790cfb9":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport xgboost ","a9916ae8":"# params = {\n#     'learning_rate'   :[0.05,0.10,0.15,0.20,0.25,0.30],\n#     'max_depth'       :[3,5,6,7,8,11,12,15],\n#     'min_child_weight':[1,3,5,7],\n#     'gamma'           :[0.0,0.1,0.2,0.3,0.4],\n#     'colsample_bytree':[0.3,0.4,0.5,0.7]  \n# }\nparams={'min_child_weight':[4,5], 'gamma':[i\/10.0 for i in range(3,6)],  'subsample':[i\/10.0 for i in range(6,11)],\n'colsample_bytree':[i\/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}","21357112":"regressor = xgboost.XGBRegressor()","a5e6b13d":"# random_search = RandomizedSearchCV(regressor,param_distributions = params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)\nrandom_search = RandomizedSearchCV(regressor, params,n_iter=5, n_jobs=1, cv=5)","d094c488":"random_search.fit(x_train,y_train)\n","d3a4d6ff":"random_search.best_estimator_\n","b5df048c":"xgbo = xgboost.XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n             min_child_weight=5, monotone_constraints=None,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=0.7, tree_method=None,\n             validate_parameters=False, verbosity=None)","e86ac168":"xgbo.fit(x_train,y_train)\ny_pred = xgbo.predict(x_val)\nmse = mean_squared_error(np.log(y_val),np.log(y_pred))\nprint(np.sqrt(mse))","5ffcb83a":"y_pred","c8f19add":"y_val","c5760451":"y_pred_test = xgbo.predict(data_test)","92c907dd":"# pred = pd.DataFrame(y_pred_test)\n# sub_df = pd.read_csv('sample_submission.csv')\n# datasets = pd.concat([sub_df['Id'],pred],axis = 1)\n# datasets.columns=['Id','SalePrice']\n# datasets.to_csv('submission2.csv',index=False)\n\n","e6c44fc9":"# datasets.head()","0ccdf624":"# regressor=xgboost.XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n#              colsample_bynode=1, colsample_bytree=1.0, gamma=0.5, gpu_id=-1,\n#              importance_type='gain', interaction_constraints=None,\n#              learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n#              min_child_weight=5, monotone_constraints=None,\n#              n_estimators=100, num_parallel_tree=1,\n#              objective='reg:squarederror', random_state=0, reg_alpha=0,\n#              reg_lambda=1, scale_pos_weight=1, subsample=0.7, tree_method=None,\n#              validate_parameters=False, verbosity=None)\n# #min_child_weight=5, missing=nan,","4c3b1b2c":"# best_x = xgbr_reg.best_estimator_\n# y_train_pred_x = best_x.predict(X_train)\n# y_val_pred_x = best_x.predict(X_test)","d77512ca":"# pred = pd.DataFrame(preds)\n# sub_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n# datasets = pd.concat([sub_df['Id'],pred],axis = 1)\n# datasets.columns=['Id','SalePrice']\n# datasets.to_csv('submission.csv',index=False)\n\n","40cb996c":"# data_test.to_csv('test_data_after_FE.csv')","063d6d5d":"# !pip install xgboost","5d23868f":"# import xgboost as xgb\n# xgb_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n#                 max_depth = 5, alpha = 10, n_estimators = 10)","9bb412db":"# xgb_reg.fit(X_train,y_train)\n","f05ec2c2":"# y_predxgb = xgb_reg.predict(X_val)","7db4a59a":"# from sklearn.matrics import mean_squared_error\n# from sklearn.metrics import mean_squared_error\n# from math import sqrt\n\n# def validation(y_val,y_pred):\n#root_mean_squared_error = sqrt(mean_squared_error(y_val,y_pred)) \n#     return rmse_val\n#validation(y_val,y_predxgb)\n","cbd73281":"# preds = xgb_reg.predict(data_test)","c7bc6cf3":"# from keras import backend as K\n# def root_mean_squared_error(y_true, y_pred):\n#         return K.sqrt(K.mean(K.square(y_pred - y_true)))","66ce0e3c":"# y_train.shape","f72d4382":"# import keras\n# from keras.models import Sequential\n# from keras.layers import Dense\n# from keras.layers import LeakyReLU,PReLU,ELU\n# from keras.layers import Dropout\n\n\n# # Initialising the ANN\n# classifier = Sequential()\n\n# # Adding the input layer and the first hidden layer\n# classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu',input_dim = 80))\n\n# # Adding the second hidden layer\n# classifier.add(Dense(output_dim = 25, init = 'he_uniform',activation='relu'))\n\n# # Adding the third hidden layer\n# classifier.add(Dense(output_dim = 50, init = 'he_uniform',activation='relu'))\n# # Adding the output layer\n# classifier.add(Dense(output_dim = 1, init = 'he_uniform'))\n\n# # Compiling the ANN\n# classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')\n\n# # Fitting the ANN to the Training set\n# model_history=classifier.fit(x_train.values, y_train.values,validation_split=0.20, batch_size = 10, nb_epoch = 1000)","a5cd91e5":"# y_prednn = classifier.predict(X_val)","786cc3a6":"# validation(y_val,y_prednn)","498e9fed":"# main_preds = classifier.predict(data_test)","db1902b7":"from sklearn.model_selection import KFold\n\nntrain = train.shape[0]\nntest = df_test.shape[0]\nNFOLDS = 5\nkf = KFold(n_splits = NFOLDS, random_state = SEED)\n\n\n# # Some useful parameters which will come in handy later on\n# ntrain = train.shape[0]\n# ntest = test.shape[0]\n# SEED = 0 # for reproducibility\n# NFOLDS = 5 # set folds for out-of-fold prediction\n# kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer\n\n# class SklearnHelper(object):\n#     def __init__(self,clf,seed = SEED,params = None):\n#         params['random_state'] = seed\n# #         self.clf = clf(**params)\n#         self.clf = clf(**params)\n\n#     def train(self,x_train,y_train):\n#         self.clf.fit(x_train,y_train)\n        \n#     def predict(self,x):\n#         return self.clf.predict(x)\n    \n#     def fit(self,x,y):\n#         return self.clf.fit(self,x,y)\n    \n#     def feature_importance(self,x,y):\n#         print(self.clf.fit(x,y).feature_importances_)\n        \n    ","02799df0":"##ERROR BOOKMARK_1--> BECAUSE`x_test` is not used in `x_te = x_train[test_index]`\n\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS,ntest))\n    \n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        x_te = x_train[test_index]\n        y_tr = y_train[train_index]\n        \n        clf.train(x_tr,y_tr)\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i,:] = clf.predict(x_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1,1),oof_test.reshape(-1,1)\n        ","8699ad34":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n# from sklearn import svm\nfrom sklearn.svm import SVC\n","9f758f79":"rf_params= {\n    'n_jobs':-1,\n    'n_estimators':500,\n    'warm_start': True,\n    'max_depth':6,\n#     'min_sample_leaf':2,\n    'max_features':'sqrt',\n    'verbose':0\n}\n\n#Extra Tree\net_params = {\n    'n_jobs':-1,\n#     'min_sample_leaf':2,\n    'n_estimators':500,\n    'max_depth':8,\n    'verbose':0\n}\n\n#Adaboost\nada_params = {\n    'n_estimators':500,\n    'learning_rate':0.75\n}\n# Gradient Boosting\ngb_params = {\n    'n_estimators':500,\n    'max_depth':5,\n    'verbose':0,\n#     'min_sample_leaf':2\n}\n#SVM\nsvc_params = {\n    'kernel':'linear',\n    'C': 0.025\n}","bb072117":"#ERROR_BOOKMARK_2-->AT `SVM`\n# Create 5 objects that represent our 5 models\nrf = SklearnHelper(clf=RandomForestRegressor,seed = SEED,params = rf_params)\net = SklearnHelper(clf = ExtraTreesRegressor,seed = SEED,params = et_params)\nada = SklearnHelper(clf = AdaBoostRegressor,seed = SEED,params = ada_params)\ngb = SklearnHelper(clf = GradientBoostingRegressor,seed = SEED,params = gb_params)\n# svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n","71dad3d7":"# y_train --> y_Train\n#train -->Train\n#x_train-->x_Train\n# x_test --> x_Test\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_Train = data1['SalePrice'].ravel()\nTrain = data1.drop(['SalePrice'], axis=1)\nx_Train = Train.values # Creates an array of the train data\nx_Test = data_test.values # Creats an array of the test data","70f0269d":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_Train, y_Train, x_Test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_Train, y_Train, x_Test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_Train, y_Train, x_Test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_Train, y_Train, x_Test) # Gradient Boost\n# svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","cb7ef103":"rf_feature = rf.feature_importances(x_Train,y_Train)\net_feature = et.feature_importances(x_Train, y_Train)\nada_feature = ada.feature_importances(x_Train, y_Train)\ngb_feature = gb.feature_importances(x_Train,y_Train)","4d8a41cd":"rf_features = [2.30296094e-03, 3.71595859e-03, 2.23441229e-03, 1.18077881e-02,\n 2.14629766e-02, 5.79594518e-05 ,2.42966131e-04, 2.19595316e-03,\n 1.79370560e-03, 2.87098485e-06 ,9.39495555e-04, 1.10329735e-03,\n 6.92714279e-03, 5.82765289e-04 ,4.20023299e-04, 1.06039693e-03,\n 1.98479854e-03, 1.16456835e-01 ,3.29936804e-03, 5.08134961e-02,\n 1.92608039e-02, 2.44888510e-03 ,5.91500081e-04, 2.06369371e-03,\n 2.72229843e-03, 1.65819118e-03 ,1.57170228e-02, 6.71367635e-02,\n 5.19048976e-04, 7.47771702e-03 ,3.83320255e-02, 4.36231047e-04,\n 2.25067050e-03, 2.06893656e-03 ,3.35145896e-02, 5.28560135e-04,\n 7.33196408e-04, 4.66762851e-03 ,5.89901525e-02, 3.58310371e-04,\n 5.50510322e-03, 1.71525827e-03 ,4.80026349e-04, 6.18887565e-02,\n 2.81451320e-02, 6.98993146e-05 ,8.58004582e-02, 2.25542246e-03,\n 1.37579218e-03, 3.46473350e-02 ,4.47357199e-03, 4.78203278e-03,\n 9.72777607e-04, 3.23197710e-02 ,1.89111254e-02, 3.07640668e-04,\n 1.46939251e-02, 6.48567122e-03 ,1.79850578e-02, 2.94025676e-02,\n 9.53137990e-03,6.43523524e-02 ,4.89337906e-02, 1.28352792e-03,\n 1.87820052e-03, 4.25980769e-04 ,5.94989071e-03, 1.10899915e-02,\n 6.12271230e-04, 1.32804362e-04 ,1.27053449e-03, 2.14768461e-03,\n 8.02178720e-04, 1.02846874e-03 ,6.09719050e-05, 1.07850614e-04,\n 2.91628530e-03, 1.16519794e-03, 8.00933028e-04, 2.41098346e-03]\net_features = [1.45229888e-03, 1.67632511e-03 ,3.87459244e-03 ,2.24189006e-03,\n 7.95567000e-03, 6.93134743e-05, 4.90997518e-04, 1.38139084e-03,\n 2.49669545e-03 ,5.42242845e-07, 1.86639655e-03, 2.57556847e-03,\n 4.50714748e-03, 9.76043543e-04, 8.09376832e-04, 1.61731651e-03,\n 7.86374122e-04, 2.36690281e-01, 2.78587276e-03, 2.50454525e-02,\n 8.38576900e-03, 1.37067894e-03, 7.23863767e-04, 1.60126640e-03,\n 1.31707024e-03, 1.02127413e-03, 4.63077999e-03, 1.41927577e-01,\n 7.94185647e-04, 8.37447881e-04, 6.39535113e-02, 3.64713386e-04,\n 2.39558674e-03, 1.99223520e-03, 1.38778471e-02, 5.08384328e-04,\n 6.78202590e-04, 1.77890716e-03, 1.54030395e-02, 3.97982502e-04,\n 7.03740557e-04, 7.00866927e-03, 1.27363628e-04, 2.23378572e-02,\n 1.79792628e-02, 2.86997027e-04, 8.54433039e-02, 5.04129398e-03,\n 1.92705001e-03, 3.13904508e-02, 2.72465240e-03, 5.35973737e-03,\n 1.56699314e-03, 4.08769923e-02, 1.02208682e-02, 8.75570962e-04,\n 1.61054827e-02, 1.25893926e-03, 1.16915897e-02, 7.39197727e-03,\n 2.21129337e-03, 1.28382514e-01, 1.83760128e-02, 7.37727471e-04,\n 3.73880589e-04, 5.35761972e-04, 2.67221341e-03, 2.10482391e-03,\n 6.46268484e-04, 7.10688302e-04, 1.43294663e-03, 8.74988921e-04,\n 4.17676256e-04, 7.41264061e-04, 8.26651713e-05, 6.47875243e-05,\n 1.20801957e-03, 8.28163279e-04, 1.45533902e-03, 2.56430468e-03]\nada_features = [1.34955167e-02 ,3.57745815e-04 ,3.23452048e-05, 1.61118387e-02,\n 2.70689827e-02, 0.00000000e+00, 0.00000000e+00, 9.34217904e-05,\n 7.23903159e-03, 0.00000000e+00, 2.23152107e-03, 0.00000000e+00,\n 6.88125746e-02, 3.75473746e-07, 0.00000000e+00, 0.00000000e+00,\n 7.79578997e-05, 1.63406072e-01, 1.89622298e-04, 7.64543326e-03,\n 1.46269754e-02, 1.64907988e-05, 5.80140201e-05, 1.05471705e-03,\n 1.62060521e-03, 3.30966842e-03, 2.33188343e-03, 3.55039607e-03,\n 1.07216281e-04, 7.17935502e-06, 1.08345917e-02, 0.00000000e+00,\n 6.32647422e-03, 1.70416472e-03, 3.07856656e-02, 4.19102075e-06,\n 4.99386635e-05, 1.20047040e-03, 6.47835551e-02, 0.00000000e+00,\n 3.73133666e-04, 1.29402048e-05, 0.00000000e+00, 1.22848097e-02,\n 1.52721624e-01, 0.00000000e+00, 1.21573312e-01, 1.72231820e-03,\n 2.65856024e-07, 5.41076138e-03, 1.37941633e-03, 6.38597015e-03,\n 0.00000000e+00, 3.98298344e-02, 3.35637062e-02, 3.58173850e-05,\n 2.18966853e-02, 3.19785862e-03, 2.63871711e-03, 1.24629920e-02,\n 4.81050003e-04, 6.68867668e-02, 3.46780312e-03, 3.31265473e-06,\n 0.00000000e+00, 0.00000000e+00, 1.25874425e-02, 3.12158901e-02,\n 0.00000000e+00, 0.00000000e+00, 5.11559769e-03, 3.13467159e-07,\n 1.57088773e-05, 2.80802603e-05, 0.00000000e+00, 0.00000000e+00,\n 1.27408608e-02, 2.65764875e-03, 1.74253592e-04, 4.72657878e-07]\ngb_features = [1.92053033e-03 ,6.20128455e-04  ,4.56858878e-03 ,5.09113186e-03,\n 1.57791012e-02, 3.28417872e-08 ,3.27486727e-05 ,1.15699904e-03,\n 1.06671465e-03, 0.00000000e+00 ,4.55082956e-04 ,3.47793078e-04,\n 1.10634879e-02, 6.89237180e-04 ,7.25308612e-05 ,3.32459163e-05,\n 2.74023059e-04, 5.60148757e-01 ,7.26102679e-03 ,1.05363309e-02,\n 5.72780665e-03, 7.27924260e-05 ,2.60411345e-05 ,1.22973192e-03,\n 9.86032456e-04, 4.28647522e-04 ,3.33789328e-03 ,3.26937929e-02,\n 1.27676424e-04, 1.82465823e-04 ,1.18865067e-02 ,1.61277219e-04,\n 1.84338866e-03, 1.26554375e-03 ,3.16493663e-02 ,1.91008267e-04,\n 5.03276290e-04, 3.26587709e-03 ,3.99727170e-02 ,5.72462134e-06,\n 2.25228404e-04, 2.45829047e-03 ,8.95673426e-05 ,1.70123983e-02,\n 3.35628187e-02, 4.34042481e-05 ,1.11890972e-01 ,1.04414773e-03,\n 5.17931229e-05, 1.38639004e-03 ,3.74802495e-04 ,4.83005478e-04,\n 2.03040778e-03, 3.55519495e-03 ,4.45312124e-03 ,6.15758449e-04,\n 5.09871128e-03, 2.39691008e-04 ,4.77095762e-03 ,4.89349000e-03,\n 2.57854978e-04, 2.49060662e-02 ,1.01154402e-02 ,6.19381772e-04,\n 1.14740460e-04, 2.06420274e-04 ,1.87831623e-03 ,3.58140641e-03,\n 5.20430663e-04, 2.67712988e-04 ,7.14431312e-04 ,1.11839285e-04,\n 4.54101088e-05, 1.17486462e-04 ,5.13800919e-06 ,3.97132745e-05,\n 2.14676543e-03, 6.06264082e-04 ,2.33686424e-04 ,2.55628670e-03]","a7aab6cc":"cols = Train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","1def20f0":"feature_dataframe.head()","082cbb02":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","9801ff1f":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","198f9a6f":"x_Train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train), axis=1)\nx_Test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test), axis=1)","73d4f94b":"second_level_reg = xgboost.XGBRegressor()\nsecond_level_reg.fit(x_Train,y_Train)\nstack_predictions = second_level_reg.predict(x_Test)","30adde09":"# StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n#                             'Survived': predictions })","0a635ad2":"stack_pred = pd.DataFrame(stack_predictions)\nstack_sub_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nstack_datasets = pd.concat([stack_sub_df['Id'],stack_pred],axis = 1)\nstack_datasets.columns=['Id','SalePrice']\nstack_datasets.to_csv('stack_submission2.csv',index=False)\n\n","5ce25308":"stack_datasets.head()","933e8905":"# cols = train.columns.values\n# # Create a dataframe with features\n# feature_dataframe = pd.DataFrame( {'features': cols,\n#      'Random Forest feature importances': rf_features,\n#      'Extra Trees  feature importances': et_features,\n#       'AdaBoost feature importances': ada_features,\n#     'Gradient Boost feature importances': gb_features\n#     })","6a7b6d97":"33525.359153248755\/0.20745","7d57ae51":"33525.359153248755\/161606.93734995785","8774394e":"32129.766156650927\/161606.93734995785","b4fdcbba":"39984.82004253466\/161606.93734995785","14f67c2a":"## Bivariate Analysis:","5ab166b7":"## Feature Engineering:","6074461e":"## Neural Network:","8d470696":"# Test DataSet:","3db5bfbc":"### Ridge:","e9a43bec":"### Feature Scalling:","31295f12":"## Random Forest: Best One(Without Feature Engineering)","bee4cbea":"## train test split:","c5414374":"### Categorical Data:","df7467d7":"## Test Categorical Data:","1cb75e30":"# Feature Selection:","aea6bc36":"## Encoding:","538f3aba":"## Stacking:","fa955a54":"## Evaluation:","0060a11a":"## Model Building:","30123ec1":"## Data fields\n###### Here's a brief version of what you'll find in the data description file.\n\n*    SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n*    MSSubClass: The building class---------------------------------------------------S\/N(STRUCTURAL\/DON'T KNOW)\n*    MSZoning: The general zoning classification--------------------------------------L(LOCATIONAL)\n*    LotFrontage: Linear feet of street connected to property-------------------------F(FACILITIES)\n*    LotArea: Lot size in square feet-------------------------------------------------L\/F\n*    Street: Type of road access------------------------------------------------------F\n*    Alley: Type of alley access------------------------------------------------------F\n*    LotShape: General shape of property----------------------------------------------S\n*    LandContour: Flatness of the property--------------------------------------------S\n*    Utilities: Type of utilities available-------------------------------------------F\n*    LotConfig: Lot configuration-----------------------------------------------------S\n*    LandSlope: Slope of property-----------------------------------------------------S\n*    Neighborhood: Physical locations within Ames city limits-------------------------L\n*    Condition1: Proximity to main road or railroad-----------------------------------L\n*    Condition2: Proximity to main road or railroad (if a second is present)----------L\n*    BldgType: Type of dwelling(\u09ac\u09b8\u09ac\u09be\u09b8\u09c7\u09b0 \u09a7\u09b0\u09a3)------------------------------------------F\n*    HouseStyle: Style of dwelling----------------------------------------------------F\n*    OverallQual: Overall material and finish quality---------------------------------Q(QUALITATIVE)\n*    OverallCond: Overall condition rating--------------------------------------------Q\n*    YearBuilt: Original construction date--------------------------------------------T(TIME RELATED)\n*    YearRemodAdd: Remodel date-------------------------------------------------------T\n*    RoofStyle: Type of roof----------------------------------------------------------S\n*    RoofMatl: Roof material----------------------------------------------------------S\n*    Exterior1st: Exterior covering on house------------------------------------------S\n*    Exterior2nd: Exterior covering on house (if more than one material)--------------S\n*    MasVnrType: Masonry veneer type--------------------------------------------------F\/Q\n*    MasVnrArea: Masonry veneer area in square feet-----------------------------------S\n*    ExterQual: Exterior material quality---------------------------------------------Q\n*    ExterCond: Present condition of the material on the exterior---------------------Q\n*    Foundation: Type of foundation---------------------------------------------------S\n*    BsmtQual: Height of the basement-------------------------------------------------S\n*    BsmtCond: General condition of the basement--------------------------------------Q\n*    BsmtExposure: Walkout or garden level basement walls-----------------------------S\n*    BsmtFinType1: Quality of basement finished area----------------------------------Q\n*    BsmtFinSF1: Type 1 finished square feet------------------------------------------S\n*    BsmtFinType2: Quality of second finished area (if present)-----------------------Q\n*    BsmtFinSF2: Type 2 finished square feet------------------------------------------Q\n*    BsmtUnfSF: Unfinished square feet of basement area-------------------------------Q\n*    TotalBsmtSF: Total square feet of basement area-----------------------------------S\n*    Heating: Type of heating---------------------------------------------------------F\n*    HeatingQC: Heating quality and condition-----------------------------------------Q\n*    CentralAir: Central air conditioning---------------------------------------------S(BASED ON AVAILABLE OR NOT)\n*    Electrical: Electrical system----------------------------------------------------Q\n*    1stFlrSF: First Floor square feet------------------------------------------------S\n*    2ndFlrSF: Second floor square feet---------------------------------------------------S\n*    LowQualFinSF: Low quality finished square feet (all floors)-----------------------Q\n*    GrLivArea: Above grade (ground) living area square feet---------------------S\n*    BsmtFullBath: Basement full bathrooms----------------------------------------S\n*    BsmtHalfBath: Basement half bathrooms-----------------------------------------S\n*    FullBath: Full bathrooms above grade---------------------------------------------S\n*    HalfBath: Half baths above grade--------------------------------------------S\n*    Bedroom: Number of bedrooms above basement level-----------------------------------S\n*    Kitchen: Number of kitchens-----------------------------------------------------S\n*    KitchenQual: Kitchen quality--------------------------------------------------Q\n*    TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)-----------------S\n*    Functional: Home functionality rating------------------------------------------Q\n*    Fireplaces: Number of fireplaces---------------------------------------------S\n*    FireplaceQu: Fireplace quality--------------------------------------------------Q\n*    GarageType: Garage location------------------------------------------------L\n*    GarageYrBlt: Year garage was built------------------------------------------------------T\n*    GarageFinish: Interior finish of the garage---------------------------------------------Q\n*    GarageCars: Size of garage in car capacity-------------------------------S\n*    GarageArea: Size of garage in square feet----------------------------------------S\n*    GarageQual: Garage quality-------------------------------------------------------------Q\n*    GarageCond: Garage condition----------------------------------------------Q\n*    PavedDrive: Paved driveway------------------------------------------------S\n*    WoodDeckSF: Wood deck area in square feet---------------------------------S\n*    OpenPorchSF: Open porch area in square feet---------------------------------S\n*    EnclosedPorch: Enclosed porch area in square feet-------------------------------S\n*    3SsnPorch: Three season porch area in square feet-----------------------------S\n*    ScreenPorch: Screen porch area in square feet-----------------------------------S\n*    PoolArea: Pool area in square feet---------------------------------------------S\n*    PoolQC: Pool quality----------------------------------------------------Q\n*    Fence: Fence quality------------------------------------------------------------Q\n*    MiscFeature: Miscellaneous feature not covered in other categories-------------F\n*    MiscFeature: Miscellaneous feature not covered in other categories--------------F\n*    MiscVal: Value of miscellaneous feature-------------------------------------------Q\n*    MoSold: Month Sold----------------------------------------------------------------T\n*    YrSold: Year Sold----------------------------------------------------------T\n*    SaleType: Type of sale------------------------------------------------SELLING\n*    SaleCondition: Condition of sale------------------------------------------------SELLING\n: Condition of sale\n    \n38-50\n66-75\n\n### Most Important features According to me:\n- area of the house(LotArea)\n- shape of the area(LotShape,LandContour)\n- number of floors(not mentioned)\n- no. of rooms(not mentioned)\n- no. of fireplaces & it's quality(Fireplaces,FireplaceQu)\n- size of room(GrLivArea,1stFlrSF,2ndFlrSF,LowQualFinSF)\n- basment size (TotalBsmtSF)\n- Masonry veneer area size(MasVnrArea)\n- porch size(OpenPorchSF,EnclosedPorch,3SsnPorch,ScreenPorch)\n- presance of garage and it's size(GarageType, GarageYrBlt, GarageFinish, GarageCars,GarageArea,GarageQual,GarageCond)\n- presence of suiatable lane(LotFrontage,LotArea,Street)\n- neighbours(Neighborhood)\n- selling time(MoSold,YrSold)\n- year made(YearBuilt)\n- elactrical system(Electrical)\n- number of bed room(Bedroom)\n- quality of kitchen(Kitchen,KitchenQual)\n- remodeling date(YearRemodAdd)\n- foundation type(Foundation)\n- bathroom (FullBath,HalfBath)\n- no. of rooms(TotRmsAbvGrd)\n- rural or not\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n- area of the house(LotArea)[CON]\n- shape of the area(LotShape[CAT],LandContour[CAT])\n- number of floors(not mentioned)\n- no. of fireplaces & it's quality(Fireplaces[DIS],FireplaceQu[CAT])\n- size of room(GrLivArea[CON],1stFlrSF[CON],2ndFlrSF[CON],LowQualFinSF[DIS])\n- basment size (TotalBsmtSF[CON])\n- Masonry veneer area size(MasVnrArea[CON])\n- porch size(OpenPorchSF[CON],EnclosedPorch,3SsnPorch,ScreenPorch)\n- presance of garage and it's size(GarageType, GarageYrBlt, GarageFinish, GarageCars,GarageArea,GarageQual,GarageCond)\n- presence of suiatable lane(LotFrontage,LotArea,Street)\n- neighbours(Neighborhood)\n- selling time(MoSold,YrSold)\n- year made(YearBuilt)\n- elactrical system(Electrical)\n- number of bed room(Bedroom)\n- quality of kitchen(Kitchen,KitchenQual)\n- remodeling date(YearRemodAdd)\n- foundation type(Foundation)\n- bathroom (FullBath,HalfBath)\n- no. of rooms(TotRmsAbvGrd)\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n\n\n'MSSubClass',--1\n 'OverallQual',--1\n 'OverallCond',\n 'LowQualFinSF',\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',--1\n 'HalfBath',--1\n 'BedroomAbvGr',--1\n 'KitchenAbvGr',--1\n 'TotRmsAbvGrd',--1\n 'Fireplaces',--1\n 'GarageCars',--1\n '3SsnPorch',--2\n 'PoolArea',--2\n 'MiscVal',--1\n 'MoSold'--1\n\n\n\n\n\n\n\n\n\n###### QUALITY######\n\nOverallQua\n\nOverallCond\n\nMasVnrArea\n\nExterCond\n\nFoundation\n\nBsmtCond\n\nBsmtFinType1\n\nBsmtFinType2\n\nBsmtFinSF2\n\nBsmtUnfSF\n\nHeatingQC\n\nElectrical\n\nLowQualFinSF\n\nKitchenQual\n\nFunctional: Home functionality rating\n\nFireplaceQu: Fireplace quality-\n\nGarageFinish\n\nGarageQual\n\nFence\n\nMiscVal\n\nMasVnrType\n\n\n##### STRUCTURE ######\n\n\nMSSubClass\n\nLotShape\n\nLandContour\n\nLotConfig\n\nLandSlope\n\nRoofStyle\n\nRoofMatl\n\nExterior1st\n\nExterior2nd\n\nMasVnrArea\n\nBsmtQual\n\nBsmtExposure\n\nBsmtFinSF1\n\nTotalBsmtSF\n\nCentralAir\n\n1stFlrSF\n\n2ndFlrSF\n\nGrLivArea\n\nBsmtFullBath\n\nBsmtHalfBath\n\nFullBath: Full bathrooms above grade\n\nHalfBath: Half baths above grade\n\nBedroom: Number of bedrooms above basement level\n\nKitchen\n\nTotRmsAbvGrd\n\nFireplaces\n\nGarageCars\n\nPavedDrive: Paved driveway\n\nWoodDeckSF: Wood deck area in square feet\n\nOpenPorchSF: Open porch area in square feet\n\nEnclosedPorch: Enclosed porch area in square feet\n\n3SsnPorch: Three season porch area in square feet\n\nScreenPorch: Screen porch area in square feet\n\nPoolArea\n\nGarageType\n\n\n##### LOCATION ######\nMSSubClass\n\nLotArea(L\/F)\n\nNeighborhood: Physical locations within Ames city limits\n\nCondition1: Proximity to main road or railroad\n\nCondition2\n\n\n\n###### FACILITES#######\n\nLotFrontage\n\nLotArea\n\nStreet\n\nAlley\n\nUtilities\n\nBldgType: Type of dwelling(\u09ac\u09b8\u09ac\u09be\u09b8\u09c7\u09b0 \u09a7\u09b0\u09a3)\n\nHouseStyle\n\nMasVnrType\n\nHeating\n\nMiscFeature: Miscellaneous feature not covered in other categories\n\nMiscFeature\n\n##### TIME#####\n\nYearBuilt\n\nYearRemodAdd\n\nGarageYrBlt\n\nMoSold: Month Sold\n\nYrSold\n\n##### SEAL#####\n\nSaleType: Type of sale\n\nSaleCondition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","054b4f00":"'MSSubClass',--1\n 'OverallQual',--1\n 'OverallCond',\n 'LowQualFinSF',\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',--1\n 'HalfBath',--1\n 'BedroomAbvGr',--1\n 'KitchenAbvGr',--1\n 'TotRmsAbvGrd',--1\n 'Fireplaces',--1\n 'GarageCars',--1\n '3SsnPorch',--2\n 'PoolArea',--2\n 'MiscVal',--1\n 'MoSold'--1","23359a0c":"## Model Building:","2a425a24":"### Most Important features According to me:\n- area of the house\n- number of floors\n- no. of rooms\n- size of room\n- is there a garage\n- presence of suiatable lane\n- neighbours\n- year sold\n- year made\n- elactrical system\n- number of beadroom\n- quality of kitchen","3373401f":"so feature engineering is done","9e2518d2":"### 1. Feature Selection Using Linear Regression","e9bd76ce":"## Xgboost:","c3f7c2f1":"## Univariate Analysis:"}}