{"cell_type":{"d0d1d0f1":"code","531d16b1":"code","b9c789ce":"code","bdce6941":"code","7ddaac58":"code","aba9ca2a":"code","e2f15309":"code","2a82cd9a":"code","1eecf8d1":"code","e4b5cf3e":"code","8dbd9b0e":"code","1ecd4830":"code","b61b11e1":"code","6a487fe8":"code","22881285":"code","85370ccf":"code","9be3142f":"code","89eaa99f":"code","dddae014":"code","11a1cc75":"code","6cc4788c":"code","c0fc0e5b":"code","b012c5ed":"code","887a1eed":"code","d5c8f1de":"code","2e1b56cc":"code","80ab87be":"code","92b32a55":"code","3b3678f7":"code","881bdc58":"code","91a1f05c":"code","dcd75a2b":"code","ae5f8d29":"code","02ace120":"code","aaa8ee52":"code","33a6ab1a":"code","bd3049d1":"code","8bd78e05":"code","97c13e02":"code","e7df96d8":"code","6b62208c":"markdown","902b2621":"markdown","c36174c4":"markdown","c90fe855":"markdown","c4d55f05":"markdown","b46cdfc1":"markdown","1106b4b3":"markdown","7171542e":"markdown","1dbda6e8":"markdown","d05a57c7":"markdown","aeeb2114":"markdown","bc8241d0":"markdown","711c6530":"markdown","cdaa0ad5":"markdown","bafeed81":"markdown","663bacff":"markdown","834cfc7d":"markdown","6bffd961":"markdown","8363c2dc":"markdown","cf8de471":"markdown","ed5ba822":"markdown","079abc79":"markdown","d4698a44":"markdown","14e32b63":"markdown","1e9a71b3":"markdown","402f7f27":"markdown","f9f4eff3":"markdown","32b4a5c3":"markdown","150759a8":"markdown","b076a2f9":"markdown","36973216":"markdown","c2ea014d":"markdown","33e9565f":"markdown","5645f21b":"markdown","7afd1502":"markdown","21821fbe":"markdown","f47fde6f":"markdown","97669d9a":"markdown","e40a1f9d":"markdown"},"source":{"d0d1d0f1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom IPython.display import Markdown as md\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\npd.set_option('float_format', '{:f}'.format)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","531d16b1":"\ndf = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.describe()","b9c789ce":"features = ['Pregnancies','Glucose', 'BloodPressure','SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']","bdce6941":"# number of null values in each column\ndf.isnull().sum()","7ddaac58":"for feature in features:\n    number = np.random.normal(df[feature].mean(), df[feature].std()\/2)\n    df[feature].fillna(value=number, inplace=True)","aba9ca2a":"# number of null values in each column\ndf.isnull().sum()","e2f15309":"df.where( df < 0).count()","2a82cd9a":"for feature in features:\n    df.loc[df[feature] < 0, feature] = 0","1eecf8d1":"df.where( df < 0).count()","e4b5cf3e":"df.loc[df['Insulin'] > 300].Insulin.count()","8dbd9b0e":"df.loc[df.Insulin > 300, 'Insulin'] = 300","1ecd4830":"df.loc[df['Insulin'] > 300].Insulin.count()","b61b11e1":"df.describe()","6a487fe8":"plot = scatter_matrix(df, alpha=0.2, figsize=(20, 20))","22881285":"x = df.loc[:, features].values\ny = df.loc[:,['Outcome']].values\nx = StandardScaler().fit_transform(x)\n\npca = PCA(n_components=7)\nprincipal_components = pca.fit_transform(x)\nprincipal_df = pd.DataFrame(data = principal_components, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5', '  principal component 6', 'principal component 7'])\n\npca_df = pd.concat([principal_df, df[['Outcome']]], axis = 1)","85370ccf":"pca_df.describe()\nplot = scatter_matrix(pca_df, alpha=0.2, figsize=(20, 20))","9be3142f":"def PCA_split_dataset(pca_df):\n    pca_df = pca_df.sample(frac=1)\n    pca_X = pca_df[pca_df.columns[0:7]]\n    pca_y = pca_df[pca_df.columns[7]] \n    return train_test_split(pca_X, pca_y, test_size = 0.20)","89eaa99f":"lr_accuracy = []\nfor i in range(500):\n    train_X, val_X, train_y, val_y = PCA_split_dataset(pca_df)\n    model = LogisticRegression(max_iter=2000, solver='lbfgs',random_state=0)\n    model.fit(train_X, train_y)\n    accuracy_percent = model.score(val_X, val_y)*100\n    lr_accuracy.append(accuracy_percent)","dddae014":"lr_average_accuracy = np.mean(lr_accuracy)\nprint(\"Accuracy for Logistic Regression\\nAverage:\",lr_average_accuracy,\"\\nMaximum:\", max(lr_accuracy),\"\\nMinimum:\", min(lr_accuracy))","11a1cc75":"svm_accuracy = []\nfor i in range(500):\n    train_X, val_X, train_y, val_y = PCA_split_dataset(pca_df)\n    model = SVC(kernel='linear',random_state=0)\n    model.fit(train_X, train_y)\n    accuracy_percent = model.score(val_X, val_y)*100\n    svm_accuracy.append(accuracy_percent)","6cc4788c":"svm_average_accuracy = np.mean(svm_accuracy)\nprint(\"Accuracy using SVM\\nAverage:\",svm_average_accuracy,\"\\nMaximum:\", max(svm_accuracy),\"\\nMinimum:\", min(svm_accuracy))","c0fc0e5b":"nb_accuracy = []\nfor i in range(500):\n    train_X, val_X, train_y, val_y = PCA_split_dataset(pca_df)\n    model = GaussianNB()\n    model.fit(train_X, train_y)\n    accuracy_percent = model.score(val_X, val_y)*100\n    nb_accuracy.append(accuracy_percent)","b012c5ed":"nb_average_accuracy = np.mean(nb_accuracy)\nprint(\"Accuracy using SVM\\nAverage:\",nb_average_accuracy,\"\\nMaximum:\", max(nb_accuracy),\"\\nMinimum:\", min(nb_accuracy))","887a1eed":"print(\"Model\\t\\t\\t Accuracy\")\nprint(\"Logistic Regression\\t\",lr_average_accuracy)\nprint(\"SVM\\t\\t\\t\",svm_average_accuracy)\nprint(\"Naive Bayes\\t\\t\",nb_average_accuracy)","d5c8f1de":"def regress_zero_values(df, feature, target):\n    zero_target_data = df[ df[target] == 0 ]\n    non_zero_target_data = df[ df[target] != 0]\n\n    train_X = non_zero_target_data[feature].values.reshape(-1,1)\n    train_y = non_zero_target_data[target].values.reshape(-1,1)\n    val_X = zero_target_data[feature].values.reshape(-1,1)\n\n    model = LinearRegression()\n    model.fit(train_X, train_y)\n    predicted_y = model.predict(val_X)\n\n    j = 0\n    for i in df.index:\n        if df.at[i, target] == 0:\n            df.at[i, target] = predicted_y[j][0]\n            j+=1","2e1b56cc":"plot = df.plot(x='SkinThickness', y='BMI', style='.')\ny_label = plot.set_ylabel('BMI')","80ab87be":"regress_zero_values(df, 'BMI', 'SkinThickness')","92b32a55":"plot = df.plot(x='SkinThickness', y='BMI', style='.')\ny_label = plot.set_ylabel('BMI')","3b3678f7":"plot = df.plot(x='Insulin', y='Glucose', style='.')\ny_label = plot.set_ylabel('Glucose')","881bdc58":"regress_zero_values(df, 'Glucose', 'Insulin')","91a1f05c":"plot = df.plot(x='Insulin', y='Glucose', style='.')\ny_label = plot.set_ylabel('Glucose')","dcd75a2b":"for feature in features:\n    df[feature] = (df[feature] - df[feature].mean())\/(df[feature].std())\ndf.describe()","ae5f8d29":"def split_dataset(df):\n    df = df.sample(frac=1)\n    X = df[df.columns[0:8]]\n    y = df[df.columns[8]] \n    return train_test_split(X, y, test_size = 0.20)","02ace120":"lr_accuracy = []\nfor i in range(500):\n    train_X, val_X, train_y, val_y = split_dataset(df)\n    model = LogisticRegression(max_iter=2000, solver='lbfgs',random_state=0)\n    model.fit(train_X, train_y)\n    accuracy_percent = model.score(val_X, val_y)*100\n    lr_accuracy.append(accuracy_percent)","aaa8ee52":"lr_average_accuracy = np.mean(lr_accuracy)\nprint(\"Accuracy for Logistic Regression\\nAverage:\",lr_average_accuracy,\"\\nMaximum:\", max(lr_accuracy),\"\\nMinimum:\", min(lr_accuracy))","33a6ab1a":"svm_accuracy = []\nfor i in range(500):\n    train_X, val_X, train_y, val_y = split_dataset(df)\n    model = SVC(kernel='linear',random_state=0)\n    model.fit(train_X, train_y)\n    accuracy_percent = model.score(val_X, val_y)*100\n    svm_accuracy.append(accuracy_percent)","bd3049d1":"svm_average_accuracy = np.mean(svm_accuracy)\nprint(\"Accuracy using SVM\\nAverage:\",svm_average_accuracy,\"\\nMaximum:\", max(svm_accuracy),\"\\nMinimum:\", min(svm_accuracy))","8bd78e05":"nb_accuracy = []\nfor i in range(500):\n    train_X, val_X, train_y, val_y = split_dataset(df)\n    model = GaussianNB()\n    model.fit(train_X, train_y)\n    accuracy_percent = model.score(val_X, val_y)*100\n    nb_accuracy.append(accuracy_percent)","97c13e02":"nb_average_accuracy = np.mean(nb_accuracy)\nprint(\"Accuracy using SVM\\nAverage:\",nb_average_accuracy,\"\\nMaximum:\", max(nb_accuracy),\"\\nMinimum:\", min(nb_accuracy))","e7df96d8":"print(\"Model\\t\\t\\t Accuracy\")\nprint(\"Logistic Regression\\t\",lr_average_accuracy)\nprint(\"SVM\\t\\t\\t\",svm_average_accuracy)\nprint(\"Naive Bayes\\t\\t\",nb_average_accuracy)","6b62208c":"**1. Logistic Regression**","902b2621":"**Helper Function -** The function `regress_zero_values` implements Deterministic Regression Imputation for two linearly dependent features, where the `target` contains the values we're imputing. This is done using linear regression on the `feature` and `target` variables.","c36174c4":"The helper function `PCA_split_dataset` is used to shuffle the dataset and split it into training and validation data.","c90fe855":"**Dealing with outlier values:**\n<br> Insulin range is between:  \n* 16-166 mIU\/L for non-diabetic people\n* 166 - 260 mIU\/L for high-risk diabetic people\n* 260 - 300 mIU\/L for diabetic people\nInsulin values greater than 300 mIU\/L are highly unlikely.<br>Hence all insulin values in the dataset are capped to 300.<br>\n[Reference](https:\/\/www.mayoclinic.org\/diseases-conditions\/diabetes\/diagnosis-treatment\/drc-20371451) ","c4d55f05":"Verify that there are no null values in any of the columns.","b46cdfc1":"<a id='1_model'><\/a>\n## Model Building","1106b4b3":"From the plot above, we can infer two things: \n    1. There is a linear relationship between Insulin and Glucose. \n    2. A lot of insulin values are zero but it is not possible for a person's insulin level to fall below 16 mIU\/L. \n<br>Hence we use the `regress_zero_values` helper function to impute the zero values of Insulin.","7171542e":"**3. Naive Bayes**","1dbda6e8":"### Comparison across different models","d05a57c7":"**Dealing with negative values:** <br>Print number of negative values in each column <br> Since none of these features can be negative, they must be invalid and must be replaced. Since the number of negative values is low, we replace them with 0.","aeeb2114":"Hence zero values are imputed for Insulin","bc8241d0":"Verify that there are no more negative values in the columns","711c6530":"<a id='preprocessing'><\/a>\n## Data Preprocessing\nAll outliers were were replaced with appropriate values as shown below.","cdaa0ad5":"## Comparison across different models","bafeed81":"The new scatter matrix verifies our hypothesis of a linear relationship between some of the features and now all the components have no relation between each other and are independent.","663bacff":"# Results","834cfc7d":"<a id='feature'><\/a>\n## Feature extraction \nSince all features are relevent and valid, all columns were used for data preprocessing and considered for feature selection.","6bffd961":"<a id='1'><\/a>\n# Approach 1 (Using PCA)","8363c2dc":"<a id='2_model'><\/a>\n## Model Building","cf8de471":"Hence zero values are imputed for SkinThickness","ed5ba822":"<a id='2'><\/a>\n# Approach 2 (using Linear regression for imputation)","079abc79":"**3. Naive Bayes**","d4698a44":"**1. Logistic Regression**","14e32b63":"**2. SVM**","1e9a71b3":"From the plot above, we can infer two things: \n    1. There is a linear relationship between SkinThickness and BMI. \n    2. A lot of SkinThickness values are zero but it is not possible for a person's skin thickness to be zero. \n<br>Hence we use the `regress_zero_values` helper function to impute the zero values of Insulin.","402f7f27":"Features with larger scales(i.e range of each feature) skew the model during training, making it suseptible to small changes in those features, hence data normalization was applied to regularize the scales of each feature.","f9f4eff3":"<a id='i_eda'><\/a>\n## Exploratory Data Analysis\nFrom the scatter matrix, we can infer that there is a linear relationship between Insulin and Glucose as well as BMI and SkinThickness. To remove redundant information from the dataset, we applied Principal Component Analysisand limited the number of features to 7","32b4a5c3":"As it can be seen from the graphs below, none of the 7 principal components have relations. All principal components maximize the spread of data in their dimension, hence maximizing the information in the datapoints.","150759a8":"We train the data using multiple algorithm to find the model which gives the best results on this dataset.","b076a2f9":"# Diabetes Prediction\nA machine learning model to accurately classify whether or not the patients in the dataset have diabetes or not.<br><br>\n* Ayush Yadav ( IMT2017009 )\n* Kaustubh Nair ( IMT2017025 )\n* Sarthak Khoche ( IMT2017038 )\n\n## Overview: \n1. [**Feature Extraction**](#feature)\n2. [**Missing Data Handling**](#missing_data)\n3. [**Data Preprocessing**](#preprocessing)\n4. [**Approach 1 (Using PCA)**](#1)\n  1. [**Exploratory Data Analysis**](#1_eda)\n  2. [**PCA**](#1_pca)\n  3. [**Model building**](#1_model)\n5. [**Approach 2 (Using Regression Imputation)**](#2)\n  1. [**Exploratory Data Analysis**](#2_eda)\n  2. [**Model building**](#2_model)\n","36973216":"<a id='2_eda'><\/a>\n## Exploratory Data Analysis","c2ea014d":"Print number of null values in the columns","33e9565f":"**2. SVM**","5645f21b":"We train the data using multiple algorithm to find the model which gives the best results on this dataset.","7afd1502":"<a id='missing_data'><\/a>\n## Missing Data Handling\nSince the given data set has many missing values (i.e NaN) and many 0 values for columns which should not be 0, this data had to either be removed or imputed with relevant and useful values. Imputing values rather than deletion of rows was considered because of the sheer amoount of rows lost if these columns were removed (~340 rows containing NaN and 0 values)","21821fbe":"Assuming the distribution to be a gaussian, imputed data was sampled from \"mean +- std.dev\" for each feature. ","f47fde6f":"We used the following two preprocessing techniques on our data:\n- PCA : After finding relations between two features in our given data set, we attempted to reduce the number of features to 7, and then applied different models to calculate the average accuracy after randomly spliting the training and testing data in 500 iterations:<br>\n   \n    - Logistic Regression: 77.22727272727273\n    - SVM:\t\t\t 77.02597402597404\n    - Naive Bayes:\t\t 75.18051948051948\n    \n<br>    \n- Imputation using linear regression: After finding relations between two pairs of features in the given data set, we filled the missing values in these features using a linear regression model. The linear regression model was trained on all other non-missin values of the feature, and was used to fit the values of the missing and 0 valued data. The average accuracy after randomly splitting the training and testing data in 500 iterations was:<br>\n    - Logistic Regression\t 76.15584415584415\n    - SVM\t\t\t 76.34545454545454\n    - Naive Bayes\t\t 75.82467532467534","97669d9a":"This helper function is used to shuffle the dataset and split it into training and validation data.","e40a1f9d":"<a id='1_pca'><\/a>\n## Principal Component Analysis (PCA)"}}