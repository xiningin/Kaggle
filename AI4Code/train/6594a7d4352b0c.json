{"cell_type":{"3148dad6":"code","ed7108de":"code","298f2798":"code","bd531d08":"code","251728ba":"code","d035d3ea":"code","852ad79b":"code","1a58e877":"code","e77ec79e":"code","31f12c61":"code","8d6eecbf":"code","b8563f3f":"code","4c351b67":"code","b840610d":"code","5c88b436":"code","d5ddafde":"code","6b1d0e34":"code","88f40f10":"code","46799f27":"code","2ac32e67":"code","3f8ca590":"code","d3ab576e":"code","59c7af0d":"code","58da7461":"code","16e14ed7":"code","9b35f1f5":"code","3933034d":"code","0507ad2b":"code","f5deb23f":"code","c82f6b57":"code","fb3adf3b":"code","45683630":"code","95ec3036":"code","39450e89":"code","bb65adf0":"code","a83fabed":"code","22c34ebe":"code","2e2b89af":"code","b0818249":"code","d7405872":"code","3d09ef51":"code","5cb03946":"code","50292b66":"code","9cdebdae":"code","1f015016":"code","29636cf9":"code","f88c04d2":"code","2f94b187":"code","3bd7b346":"code","13f557a0":"code","fd4fac92":"code","65a13b4c":"code","551f45dc":"code","46dc6e7f":"code","f9f77093":"code","9afa586f":"code","9f63feea":"code","8ad237cf":"code","d373d197":"code","4bd4b329":"code","d91d1a1a":"code","72db080c":"code","18a85d12":"code","dd6251c8":"code","a81d19f3":"code","028afa0e":"code","0b27d21e":"code","5db5012a":"code","73872937":"code","897ea0f3":"code","fce53479":"code","6a4c82a5":"code","33eaf834":"code","5937fbca":"code","76a0b9fc":"code","6cae7116":"code","9ba9ade7":"code","e62fb8d7":"code","facf737d":"code","2cb298d5":"code","a7fc9580":"code","c828b997":"code","0f222938":"code","0fee9371":"code","41177f81":"code","8a4b90fd":"code","1730ba9b":"code","404d19bf":"code","c552f950":"markdown","ef42d4dd":"markdown","00ce9c20":"markdown","dfefc58d":"markdown","c37356e3":"markdown","27268f90":"markdown","386eb570":"markdown","3a27ddf5":"markdown","4d27ebd3":"markdown","1c829aa1":"markdown","0bfdec93":"markdown","67a2f0bf":"markdown","1c75e3e5":"markdown","768fb253":"markdown","4c27af3a":"markdown","9b4e63fc":"markdown","7be614d7":"markdown","8f8e8e94":"markdown","52af6a9b":"markdown","25c05f4a":"markdown","b80e8e52":"markdown","702645eb":"markdown","3e9b2c48":"markdown","031b4dad":"markdown","0877ecb2":"markdown","2be8de2c":"markdown","a718b92e":"markdown","b9e74b9f":"markdown","ea683b39":"markdown","ca4f1240":"markdown"},"source":{"3148dad6":"import pandas as pd \nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport string\nimport re\n\nfrom IPython import display\n\n#Feature Engineering\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.impute import SimpleImputer\n\n#feature extraction \nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#Feature Selection\nfrom sklearn.feature_selection import RFECV\n\n##Models\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor, VotingClassifier, VotingRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression , Lasso, Ridge , LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import  SVR, SVC\n\nfrom sklearn.metrics import r2_score, mean_squared_error,accuracy_score,f1_score , classification_report,confusion_matrix","ed7108de":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col=\"PassengerId\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\",index_col=\"PassengerId\")","298f2798":"train.head()","bd531d08":"test.shape","251728ba":"df = pd.concat([train,test],axis = 0)","d035d3ea":"df.head()","852ad79b":"df.describe()","1a58e877":"df.info()","e77ec79e":"plt.figure(figsize=(9,6))\nsns.heatmap(df.corr(), vmax= 1, vmin= -1, cmap=\"Spectral\")","31f12c61":"sns.pairplot(df, hue = \"Survived\", palette=\"Set1\")","8d6eecbf":"sns.displot(data = df, x= \"Age\", hue = \"Sex\", col= \"Survived\")","b8563f3f":"# Get names in brackets and actual name \ndf[\"Name_purchaser\"] = df[\"Name\"].apply(lambda x : \"\".join(re.findall(r\"\\((.*?)\\)+\", x)))\ndf[\"Name_passenger\"] =  df[\"Name\"].apply(lambda x : x.split(\"(\")[0] )","4c351b67":"#get title\ndf[\"Title\"] = df[\"Name\"].apply( lambda x : x.split(\",\")[1].split(\".\")[0].strip())","b840610d":"#remove punctuation and title from Name\ndf[\"Name\"] = df[\"Name\"].apply(lambda s : ''.join(x for x in s if x not in string.punctuation))\ndf[\"Name\"] = df[[\"Name\",\"Title\"]].apply(lambda x : x[0].replace(\" \"+x[1],\"\"), axis =1)","5c88b436":"df.head()","d5ddafde":"df[\"Title\"].value_counts()","6b1d0e34":"## merge similar titles  \nmap_dict = {\"Ms\":\"Miss\", \"Mme\":\"Mrs\", \"Sir\": \"Mr\", \"Jonkheer\":\"Mr\", \"Lady\":\"Mrs\", \"Don\":\"Mr\", \"Dona\":\"Mrs\",\"Mlle\":\"Miss\", \"Col\":\"Soldier\", \"Capt\":\"Soldier\",\"Major\":\"Soldier\"}\ndf[\"Title\"] = df[\"Title\"].apply(lambda x : map_dict[x] if x in map_dict else x)","88f40f10":"# Get length of name \ndf[\"Name_length\"] = df[\"Name\"].apply(len)\n#df[\"Name_pas_len\"] = df[\"Name_passenger\"].apply(len)\n#df[\"Name_pur_len\"] = df[\"Name_purchaser\"].apply(len)","46799f27":"#Identify if the there is a purchaser of the ticket.\n#i.e a sponsor\ndf[\"purchaser\"] = df[\"Name_purchaser\"] = df[\"Name_purchaser\"].apply(lambda x: 1 if len(x)>0 else 0)","2ac32e67":"## Get surname\ndf[\"Surname\"] = df[\"Name_passenger\"].apply(lambda x : x.split(\",\")[0])","3f8ca590":"df.drop([\"Name\",\"Name_passenger\",\"Name_purchaser\"],axis =1,inplace=True)","d3ab576e":"df.head()","59c7af0d":"## Fill empty cabins with unknown (\"U\")\ndf[\"Cabin\"] = df[\"Cabin\"].fillna(\"U\")","58da7461":"df[\"Cabin\"].value_counts().head()","16e14ed7":"def get_cabin_letter(string):\n    try:\n        x= re.search(r\"\\D\",string).group()\n    except:\n        x= \"U\"\n    return x","9b35f1f5":"## Get room number of cabin, I chose to get the max room number but could get min \ndef get_cabin_max_num(string):\n    try:\n        x= max(re.findall(r\"\\d+\",string))\n    except ValueError:\n        x= 0\n    return x","3933034d":"df[\"Cabin_letter\"] = df[\"Cabin\"].apply(get_cabin_letter)\ndf[\"Cabin_max_num\"] = df[\"Cabin\"].apply(get_cabin_max_num).astype(int)","0507ad2b":"## Dont need Cabin anymore \ndf.drop(\"Cabin\",axis =1, inplace = True)","f5deb23f":"# unsure if to make U = min or max \ncabin_map = {\"A\":0, \"B\":1, \"C\":2,\"D\":3, \"E\":4, \"F\":5,\"G\":6,\"T\":8, \"U\":8}\n\ndf[\"Cabin_letter\"] = df[\"Cabin_letter\"].map(cabin_map).astype(int)\n#only seems like one Cabin called = T, setting this to unknown\ndf[\"Cabin_letter\"].head()","c82f6b57":"df.corr()","fb3adf3b":"df['FamilySize'] = df ['SibSp'] + df['Parch'] + 1","45683630":"df['Lone'] = 1 \ndf.loc[df['FamilySize'] > 1,'Lone' ] = 0","95ec3036":"def split_ticket_start(string):\n    if \" \" in string:\n        list_s = string.split(\" \")\n        if len(list_s) >2:\n            x = \" \".join(list_s[:2])\n        else:\n            x = list_s[0]\n    else: \n        x = \"None\"\n    return x ","39450e89":"df[\"Ticket_prefix\"] = df[\"Ticket\"].apply(split_ticket_start)\ndf[\"Ticket_num\"] = df[\"Ticket\"].apply(lambda x : \"\".join( re.findall(r\"[\\d]+$\", x)))\n\ntest[\"Ticket_prefix\"] = test[\"Ticket\"].apply(split_ticket_start)\ntest[\"Ticket_num\"] = test[\"Ticket\"].apply(lambda x : \"\".join( re.findall(r\"[\\d]+$\", x)))","bb65adf0":"# replace empty values with \"\"\ndf[\"Ticket_num\"] = df[\"Ticket_num\"].apply(lambda x : \"0\" if x ==\"\" else x)\n\ntest[\"Ticket_num\"] = test[\"Ticket_num\"].apply(lambda x : \"0\" if x ==\"\" else x)","a83fabed":"## Remove punctuation\ndf[\"Ticket_prefix\"] = df[\"Ticket_prefix\"].apply(lambda s : ''.join(x.strip() for x in s if x not in string.punctuation))","22c34ebe":"df[[\"Ticket\",\"Ticket_num\",\"Ticket_prefix\"]].head()","2e2b89af":"df.drop(\"Ticket\",axis =1 , inplace = True)","b0818249":"# done above","d7405872":"imputer_Fare = SimpleImputer()\ndf[\"Fare\"]= imputer_Fare.fit_transform(df[[\"Fare\"]])","3d09ef51":"imputer_Embarked = SimpleImputer(strategy=\"most_frequent\")\ndf[\"Embarked\"]= imputer_Embarked.fit_transform(df[[\"Embarked\"]])\n\ndf['Embarked'] = df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","5cb03946":"# Populate Age\ndf['Age'] = df['Age'].groupby([df['Pclass'], df['Sex']]).apply(lambda x: x.fillna(x.mean()))","50292b66":"df.isnull().sum()","9cdebdae":"df.info()","1f015016":"df.head()","29636cf9":"## columns to encode excluding Sex column (see below)\n\ncat_columns = [\"Title\",\"Ticket_prefix\"]","f88c04d2":"# Sex column is quick to encode ,we can do this with map\n\ndf[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\":1})\n\ndummies = pd.get_dummies(df[cat_columns],drop_first=True)\ndummies.head()","2f94b187":"dummies.shape","3bd7b346":"df = pd.concat([df, dummies],axis = 1)\ndf.drop(cat_columns, axis = 1, inplace=True)","13f557a0":"df.head()","fd4fac92":"df = df.reset_index(drop = True)\n\ncount_i = CountVectorizer()\nname_v = count_i.fit_transform(df[\"Surname\"])\nname_vector_df = pd.DataFrame(data = name_v.todense(), columns = count_i.get_feature_names())\ndf = pd.concat([df,name_vector_df], axis =1 )","65a13b4c":"df.drop([\"Surname\"],axis = 1, inplace=True)","551f45dc":"df.head()","46dc6e7f":"df[\"Age_bin\"] = pd.cut(df[\"Age\"], bins = [-1, 10, 20, 40, 50,60,70,80], labels=[0, 1, 2, 3,4,5,6]).astype(int)","f9f77093":"df[\"Age_bin\"].value_counts(dropna=False)","9afa586f":"df.head()","9f63feea":"scaler_all = StandardScaler()","8ad237cf":"scaler_all.fit_transform(df)","d373d197":"#not including Age as we will run linear regression on this column\nscalable_columns = [\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Name_length\",\"Cabin_letter\",\"Cabin_max_num\",\"Ticket_num\"]","4bd4b329":"scaled_df = pd.DataFrame( data = scaler_all.fit_transform(df[scalable_columns]) , columns=scalable_columns)\n\n# are applying leakage here but its not too bad as we wanted to retrain on a full data set","d91d1a1a":"scaled_df.head()","72db080c":"df = df.drop(scalable_columns, axis = 1)\ndf = df.reset_index(drop = True)\ndf = pd.concat([df,scaled_df], axis =1)\ndf.head()","18a85d12":"df.info()","dd6251c8":"imputer_age = SimpleImputer(strategy=\"mean\")","a81d19f3":"quick_df = df.copy()\nquick_df= quick_df[~quick_df[\"Survived\"].isnull()]","028afa0e":"X = quick_df.drop([\"Survived\"],axis =1)\ny = quick_df[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.33, random_state=42,)","0b27d21e":"rf = RandomForestClassifier(n_estimators=len(X_train.columns))\nlogr_q = LogisticRegression(solver='liblinear')","5db5012a":"def score_model_class(model):\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    print(\"accuracy_score test: \",accuracy_score(y_test,y_pred))\n    print(\"f1_score test: \",f1_score(y_test,y_pred))\n    \n    print(confusion_matrix(y_test,y_pred))\n    print(classification_report(y_test,y_pred))","73872937":"score_model_class(rf)\nscore_model_class(logr_q)","897ea0f3":"rf = RandomForestClassifier()\nlogr = LogisticRegression()\nxgb = XGBClassifier(eval_metric='mlogloss')\ndtree= DecisionTreeClassifier(random_state = 0)\n\nknn = KNeighborsClassifier()\nsvc = SVC(probability=True)\nridge_c = RidgeClassifier()","fce53479":"from sklearn.model_selection import StratifiedKFold\nkfold =StratifiedKFold(n_splits=5,shuffle=True,random_state=42)","6a4c82a5":"X = df[~df[\"Survived\"].isnull()].drop([\"Survived\"],axis =1)\ny = df[~df[\"Survived\"].isnull()][\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.3, random_state=42, stratify=y)","33eaf834":"\"\"\"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state = 42)\nX_train, y_train = smote.fit_resample(X_train, y_train)\"\"\"","5937fbca":"sns.countplot(y_train)","76a0b9fc":"def score_model_class(model, params ,scoring = \"f1_macro\"):\n    \n    model = GridSearchCV(model, param_grid= params, scoring =scoring, cv= kfold)\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    print (\"Model and params: \", model.best_estimator_, model.best_params_) \n    print(\"\\n\")\n    print(\"Train f1 score: \", model.best_score_)\n    print(\"test f1_score: \",f1_score(y_test,y_pred))\n    print(\"\\n\")\n    print(\"Test Report:\")\n    print(classification_report(y_test,y_pred))\n    return model","6cae7116":"ridge_params = {\"max_iter\": [120, 200,300], \"alpha\":[0.1,0.8, 1,1.5, 2,3]}\nlogr_params = {\"solver\":[\"liblinear\",\"saga\", \"lbfgs\", \"newton-cg\"],\n    \"penalty\": [\"l2\",\"l1\", \"elasticnet\", \"none\"],\n    \"C\": [0.01,0.5,1,3,4],\n    \"max_iter\": [4000]\n}\nxgb_params={'learning_rate': [0.05,0.01,0.1, 1], \n           # \"subsample\": [0.5, 0.7, 1.0],\n            'max_depth': [2, 3, 5, 7],\n            #\"gamma\" : [3,4,5,6] ,             \n            'n_estimators': [150, 200, 300, 500]\n           }\nforest_params = {     \n    \"max_depth\" : [10,20,30,40],       \n    \"n_estimators\" : [300,350,400,410,420,440],\n    \"max_features\" :[\"auto\", \"log2\", None]\n}\ntree_params = { \"max_depth\" : [8,10,20,30],       \n    \"max_features\" :[\"auto\", \"log2\", None]}\n\nknn_params = {\"n_neighbors\" : [2,3,5] , \"metric\" :[\"euclidean\", \"manhattan\", \"minkowski\"], \"weights\" :[\"uniform\", \"distance\"]}\nsvc_params = {\"kernel\" : [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], \"C\" : [ 0.001, 0.1, 1, 2, 3], \"gamma\":[\"scale\", \"auto\"]}","9ba9ade7":"ridge_m = score_model_class (ridge_c, {'alpha': [1.2, 1.5, 2], 'max_iter': [2]}\n                            )","e62fb8d7":"logr_m = score_model_class(logr, {'C': [3], 'max_iter': [5000], 'penalty': ['l2'], 'solver': ['liblinear']})","facf737d":"rf_m =random_forest = score_model_class(rf,{'max_depth': [40], 'max_features': ['sqrt'], 'n_estimators': [500]})","2cb298d5":"dtree_m = score_model_class(dtree,{'max_depth': [50], 'max_features': [60]})","a7fc9580":"xgb_m = score_model_class(xgb, {'learning_rate': [0.01], 'max_depth': [3,5,8], 'n_estimators': [200]} )","c828b997":"knn_m = score_model_class ( knn, {'n_neighbors': [5,6,7], 'weights': [\"uniform\", \"distance\"]})","0f222938":"svc_m = score_model_class (svc, {'C': [0.8], 'kernel': ['linear']}\n)","0fee9371":"vc = VotingClassifier([(\"knn_m\",knn_m), \n                       (\"svc_m\",svc_m), (\"xgb_m\",xgb_m), \n                       (\"ridge_m\",ridge_m),\n                       (\"dtree_m\",dtree_m),\n                       (\"logr_m\",logr_m),\n                       (\"rf_m\",rf_m)],\n                      voting = \"hard\")\n\nvc.fit(X_train,y_train)","41177f81":"print(\"training: \", vc.score(X_train,y_train))\nprint(classification_report(y_test,vc.predict(X_test)))","8a4b90fd":"X_sub = df[df[\"Survived\"].isnull()].drop([\"Survived\",],axis =1)","1730ba9b":"y_sub = vc.predict(X_sub)\nsub = pd.DataFrame({'PassengerId': test.index, 'Survived': y_sub})\nsub[\"Survived\"] = sub[\"Survived\"].astype(int)","404d19bf":"sub.to_csv(\"submission.csv\",index=False)","c552f950":"# Submission","ef42d4dd":"### Merge similar Titles:\nMapping Title with low frequency back to its majority counterpart i.e. \"Mlle\" is an abreviation in french for \"Miss\" as well as group titles i.e. Capt and Major are both military title (i.e. soldiers)","00ce9c20":"# Voting Classification\nCreate ensemble model of the above models and use hard voting i.e. highest probability ","dfefc58d":"# Fix null values\n**comments**\n* Ignore Survived null values as these are only in Test data\n* Age - previously used Regression. Now imputed with mean grouped by Plcass & Sex\n* Fare -  only 1 missing - use mean  \n* Embarked  - only 2 missing , use mode  ","c37356e3":"#### Age - mean by Pclass, Sex ","27268f90":"## Standard Scaler\n\nWe are doing this before **train test split** so there defintely will be some leakage here\n\nHowever Im not too worried as I would have refit the data on the full dataset **after** prediction was done anyway ","386eb570":"# Notes on Kernel  \nFirst competition, lets see what we can do \nInitial Ideas -> 0.78 score:\n1. Name, title extraction\n1. Name - Countvectorizer\n1. Name - Surname extraction\n1. Name - extract Sponsor (purchaser of ticket )\n1. Age - imputation with supervised learning (Regression)\n1. Ticket - extract intial letters and numbers\n1. Cabin - extract letter and max number of cabin\n \n\nIdeas from other Notebooks \n1. Feature creation: Family size and Alone \n1. Age imputation by mean of Pclass,Sex\n1. Bin fare (low, medium, high)  - Havent implemented\n1. Bin age (child, teenager, adult, elderly) \n1. SMOTE to add create a balanced dataset - removed as the prediction % didnt increase\n","3a27ddf5":"The surname can identify families as well as some cultural differences. \n\nAnother interesting option would be to group surnames into buckets such as asian, western, european etc etc ","4d27ebd3":"### Fare - median","1c829aa1":"Looks like there are some groupings 0-10, 10-25, 25-35 \n\nWe can try group with 10 year intervals (note that other noteboooks bin the ages i.e. <16 = Child ,16-45 Adult ,>45 elderly )","0bfdec93":"### Ticket\n* cleanup ticket (miss-spellings etc) \n* get start and end of ticket\n\nThere are so many issues and misspellings with Tickets that we did the bare basics here ","67a2f0bf":"### Age Binning \nBetter binning has been done in other notebooks but I wanted to try my own take and not reuse the same code found elsewhere","1c75e3e5":"# Instantiate Models","768fb253":"# Feature Engineering \/ Feature Extraction \n\nThere is a lot that can be done here, I just went through a few off the top of my head: \n1. Name  - length \n1. Name  - Title of person (Mr, Miss etc)\n1. Name  - Seperate name in brackets (sponsor) from actual passenger name \n1. Ticket  - seperate number from text values \n1. Family size\n1. binning \n\nOther potential options:\n* Name - CountVectorizer \n* Name - Topic modelling\n* Name - Jaccard distance","4c27af3a":"### Cabin","9b4e63fc":"### Family Size and lone traveller","7be614d7":"# Quick Survival prediction \nWe can use this as a baseline","8f8e8e94":"### Name \/ Title processing\n\n* Get Title of person \n* merge similar titles\n* Get Surname\n* Get length of name\n* Get Sponsor (purchaser of ticket ) 0\/1","52af6a9b":"#### Option: Can analyse the Ticket column futher \n* cleanup potential spelling issues\n* Seperate Ticket prefix futher on forward slash","25c05f4a":"## CountVectorizer - Name\n* run on Surname and create a dense matrix and concat to df ","b80e8e52":"**Cabin imputation** for simplicity lets replace null values with \"None\" i.e no cabin","702645eb":"# EDA \nMerging train and test data for better visualisation and preprocessing \n\n**Note** seperate before fitting ","3e9b2c48":"#### Embarked - Mode ","031b4dad":"# Cabin \nSuspected that your cabin letter or the cabin number will be at a certain location on the ship therefore you could be closer to the boats\n\nA quick look on [wikipedia](https:\/\/en.wikipedia.org\/wiki\/First-class_facilities_of_the_Titanic) shows this is true","0877ecb2":"# Create Dummies ","2be8de2c":"# Full Survival prediction \ni.e. with hyperparamater tuning ","a718b92e":"#### Option: to do some further analysis on Name_passenger & Name_purchaser \n\n**Note** that we will use countvectorizer later, so some of this info will be in the sparse matrix output of \"Surname\"","b9e74b9f":"Cound do more with the Ticket but lets such as split the prefix on the forward slash as well as cleanup some spelling or punctuation differences \\\nBut to save time lets remove the punctuation and use this instead","ea683b39":"## Trying Oversampling\nremoved as this didnt improve the score","ca4f1240":"#### Length of name"}}