{"cell_type":{"6e2ae8a9":"code","c1304efd":"code","cb06188c":"code","3b73b4c4":"code","c9232de9":"code","d53cb336":"code","07abda18":"code","60df959d":"code","90b5fdec":"code","78d2293f":"code","1f2f0bfa":"code","36d8d5a6":"code","86100ec9":"code","cc611f1c":"code","55d02750":"code","7904a2c1":"code","18804af2":"code","09d86c31":"code","adebfb49":"code","6dcfa6d5":"code","f9a3ef88":"code","e6762dfe":"code","3dd39fac":"code","4384cd0d":"code","961f8980":"code","45e6a644":"code","cb595a59":"code","547da9bd":"code","3a1c5e92":"code","0b2c3d4a":"code","bc8cae02":"code","a8bf88ea":"code","e59d1157":"code","bcf76155":"code","e8ec6437":"code","4e9ce97c":"code","cfd89a67":"code","8313785d":"code","71c87941":"code","1546ca0b":"code","48b5e818":"code","2f1da1a6":"code","1b767cdb":"code","2c58b27c":"code","d6a3479f":"code","08a4aad7":"code","85f29254":"code","244d2f19":"markdown","526b8c08":"markdown","ab9eb7e3":"markdown","d81085b5":"markdown","327575a6":"markdown","5c78b7a4":"markdown","f0d1dbfd":"markdown","901daaff":"markdown","672281af":"markdown","0b1f1ab3":"markdown","0cb02fed":"markdown","6e2f5a13":"markdown","00e24cd9":"markdown","056b0ad1":"markdown","d1225a0d":"markdown","1972a782":"markdown","9fa857d3":"markdown","c514cb86":"markdown","6a232dad":"markdown","6645934f":"markdown","9a78ff7c":"markdown","d8c444c7":"markdown","97d3b88c":"markdown","0fda2e1b":"markdown","8d562a34":"markdown","231d72d5":"markdown"},"source":{"6e2ae8a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c1304efd":"import numpy as np    # For array operations\nimport pandas as pd   # For DataFrames\nimport matplotlib.pyplot as plt    # For plotting\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,mean_squared_error,accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","cb06188c":"unibank = pd.read_csv('..\/input\/bank-loan-classification\/UniversalBank.csv')\nunibank.head()","3b73b4c4":"## Check the datatype of each variable\nunibank.dtypes","c9232de9":"## Drop columns which are not significant\nunibank.drop([\"ID\",\"ZIP Code\"],axis=1,inplace=True)","d53cb336":"## Convert Categorical Columns to Dummies\ncat_cols = [\"Family\",\"Education\",\"Personal Loan\",\"Securities Account\",\"CD Account\",\"Online\",\"CreditCard\"]\nunibank = pd.get_dummies(unibank,columns=cat_cols,drop_first=True,)","07abda18":"## Split the data into X and y\nX = unibank.copy().drop(\"Personal Loan_1\",axis=1)\ny = unibank[\"Personal Loan_1\"]\n\n## Split the data into trainx, testx, trainy, testy with test_size = 0.20 using sklearn\ntrainx, testx, trainy, testy = train_test_split(X, y, test_size=0.20)\n\n## Print the shape of X_train, X_test, y_train, y_test\nprint(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","60df959d":"from sklearn.preprocessing import StandardScaler\n\n## Scale the numeric attributes\nscaler = StandardScaler()\nscaler.fit(trainx.iloc[:,:5])\n\ntrainx.iloc[:,:5] = scaler.transform(trainx.iloc[:,:5])\ntestx.iloc[:,:5] = scaler.transform(testx.iloc[:,:5])","90b5fdec":"# Logistic Regression\nX = trainx\ny = trainy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nmodel = LogisticRegression()\nmodel.fit(X , y)\npredicted_classes = model.predict(X)\naccuracy = accuracy_score(y,predicted_classes)\nparameters = model.coef_","78d2293f":"print(accuracy)\nprint(parameters)\nprint(model)","1f2f0bfa":"model.fit(testx , testy)\npredicted_classes_test = model.predict(testx)\naccuracy = accuracy_score(testy,predicted_classes_test)\nprint(accuracy)","36d8d5a6":"from sklearn.naive_bayes import GaussianNB\n\nNB = GaussianNB()\n\nNB.fit(X , y)\n\nNB_train_pred = NB.predict(X)\nprint(accuracy_score(y,NB_train_pred))\n\nNB_test_pred = NB.predict(testx)\nprint(accuracy_score(testy,NB_test_pred))","86100ec9":"knn_classifier = KNeighborsClassifier(algorithm='brute',weights='distance')\nparams = {'n_neighbors':[1,11,25],'metric':[\"euclidean\",'cityblock']}\ngrid = GridSearchCV(knn_classifier,param_grid=params,scoring='accuracy',cv=10)","cc611f1c":"grid.fit(trainx,trainy)\nprint(grid.best_score_)\nprint(grid.best_params_)","55d02750":"best_knn = grid.best_estimator_\npred_train = best_knn.predict(trainx) \npred_test = best_knn.predict(testx)\nprint(\"Accuracy on train is:\",accuracy_score(trainy,pred_train))\nprint(\"Accuracy on test is:\",accuracy_score(testy,pred_test))","7904a2c1":"from sklearn import tree\nfrom sklearn.metrics import accuracy_score, mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\n# Defining the model\n# Fit \/ train the model\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(trainx,trainy)","18804af2":"# Get the prediction for both train and test\npred_train = dtc.predict(trainx)\npred_test = dtc.predict(testx)\n# Measure the accuracy of the model for both train and test sets\nprint(\"Accuracy on train is:\",accuracy_score(trainy,pred_train))\nprint(\"Accuracy on test is:\",accuracy_score(testy,pred_test))","09d86c31":"# Max_depth = 3\n\ndtc_2 = tree.DecisionTreeClassifier(max_depth=3)\ndtc_2.fit(trainx,trainy)\n\npred_train2 = dtc_2.predict(trainx)\npred_test2 = dtc_2.predict(testx)\n\nprint(\"Accuracy on train is:\",accuracy_score(trainy,pred_train2))\nprint(\"Accuracy on test is:\",accuracy_score(testy,pred_test2))","adebfb49":"from sklearn.svm import SVC\n\n## Create an SVC object and print it to see the default arguments\nsvc = SVC()\nprint(svc)","6dcfa6d5":"## Fit\nsvc.fit(trainx,trainy)\n\n## Predict\ntrain_predictions = svc.predict(trainx)\ntest_predictions = svc.predict(testx)\n\n","f9a3ef88":"### Train data accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(trainy,train_predictions))\n      \n### Test data accuracy\nprint(accuracy_score(testy,test_predictions))","e6762dfe":"from sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC()\n \n\nparam_grid = {\n\n'C': [0.001, 0.01, 0.1, 1, 10],\n'gamma': [0.001, 0.01, 0.1, 1], \n'kernel':['linear','rbf']}\n\n \nsvc_cv_grid = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 10)","3dd39fac":"svc_cv_grid.fit(X = trainx, y = trainy)\nprint(svc_cv_grid.best_score_,svc_cv_grid.best_params_)","4384cd0d":"best_svc = svc_cv_grid.best_estimator_\npred_train = best_svc.predict(trainx) \npred_test = best_svc.predict(testx)\nprint(\"Accuracy on train is:\",accuracy_score(trainy,pred_train))\nprint(\"Accuracy on test is:\",accuracy_score(testy,pred_test))","961f8980":"print(roc_auc_score(trainy, pred_train))\nprint(roc_auc_score(testy, pred_test))","45e6a644":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nprint(rfc)","cb595a59":"rfc.fit(trainx,trainy)\n## Predict\nrfc_train_predictions = rfc.predict(trainx)\nrfc_test_predictions = rfc.predict(testx)\n\n### Train data accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(trainy,rfc_train_predictions))\n      \n### Test data accuracy\nprint(accuracy_score(testy,rfc_test_predictions))","547da9bd":"from sklearn.model_selection import RandomizedSearchCV\n\n## n_jobs = -1 uses all cores of processor\n## max_features is the maximum number of attributes to select for each tree\nrfc_grid = RandomForestClassifier(n_jobs=-1, max_features='sqrt', class_weight='balanced_subsample')\n \n# Use a grid over parameters of interest\n## n_estimators is the number of trees in the forest\n## max_depth is how deep each tree can be\n## min_sample_leaf is the minimum samples required in each leaf node for the root node to split\n## \"A node will only be split if in each of it's leaf nodes there should be min_sample_leaf\"\n \nparam_grid = {\"n_estimators\" : [10, 25, 50, 75, 100],\n           \"max_depth\" : [10, 12, 14, 16, 18, 20],\n           \"min_samples_leaf\" : [5, 10, 15, 20]}\n \nrfc_cv_grid = RandomizedSearchCV(estimator = rfc_grid, param_distributions = param_grid, cv = 3, n_iter=10)\nrfc_cv_grid.fit(trainx, trainy)\nrfc_cv_grid.best_estimator_","3a1c5e92":"## Predict\nrfc2_train_predictions = rfc_cv_grid.predict(trainx)\nrfc2_test_predictions = rfc_cv_grid.predict(testx)\n\nprint(accuracy_score(trainy,rfc2_train_predictions))\n      \n### Test data accuracy\nprint(accuracy_score(testy,rfc2_test_predictions))","0b2c3d4a":"# import modules as necessary\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier","bc8cae02":"# Create adaboost-decision tree classifer object\nAdaboost_model = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators = 600,\n    learning_rate = 1)","a8bf88ea":"# Train model\n%time Adaboost_model.fit(trainx, trainy)","e59d1157":"# Predict on Train \ntrain_preds = Adaboost_model.predict(trainx)\n# Predict on Test \ntest_preds = Adaboost_model.predict(testx)","bcf76155":"print(accuracy_score(trainy, train_preds))\nprint(accuracy_score(testy, test_preds))\n","e8ec6437":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_estimators' : [100, 150, 200],\n              'learning_rate' : [0.1, 0.5, 0.9]}\n\nAdaboost_model_clf = GridSearchCV(AdaBoostClassifier(\n            DecisionTreeClassifier(max_depth=2)), param_grid, n_jobs=-1)","4e9ce97c":"# Train model\n%time Adaboost_model_clf.fit(trainx, trainy)","cfd89a67":"# Find best model\nbest_ada_model = Adaboost_model_clf.best_estimator_\nprint (Adaboost_model_clf.best_score_, Adaboost_model_clf.best_params_) ","8313785d":"# Predict on Train \ntrain_preds2 = best_ada_model.predict(trainx)\n# Predict on Test \ntest_preds2 = best_ada_model.predict(testx)\n\nprint(accuracy_score(trainy, train_preds2))\nprint(accuracy_score(testy, test_preds2))","71c87941":"from sklearn.ensemble import GradientBoostingClassifier\nGBM_model = GradientBoostingClassifier(n_estimators=50,\n                                       learning_rate=0.3,\n                                       subsample=0.8)\n\n%time GBM_model.fit(X=trainx, y=trainy)","1546ca0b":"gtrain_pred = GBM_model.predict(trainx)\ngtest_pred = GBM_model.predict(testx)\n\nprint(accuracy_score(trainy, gtrain_pred))\nprint(accuracy_score(testy, gtest_pred))","48b5e818":"# Model in use\nGBM = GradientBoostingClassifier() \n \n# Use a grid over parameters of interest\nparam_grid = { \n           \"n_estimators\" : [100,150,200,250],\n           \"max_depth\" : [5, 10],\n           \"learning_rate\" : [0.1,0.5,0.9]}\n \nCV_GBM = GridSearchCV(estimator=GBM, param_grid=param_grid, cv= 10)","2f1da1a6":"%time CV_GBM.fit(X=trainx, y=trainy)","1b767cdb":"# Find best model\nbest_gbm_model = CV_GBM.best_estimator_\nprint (CV_GBM.best_score_, CV_GBM.best_params_)","2c58b27c":"g2train_pred = GBM_model.predict(trainx)\ng2test_pred = GBM_model.predict(testx)\n\nprint(accuracy_score(trainy, g2train_pred))\nprint(accuracy_score(testy, g2test_pred))","d6a3479f":"\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(trainy, g2train_pred))\nprint(roc_auc_score(testy, g2test_pred))","08a4aad7":"#importing library and building model\nfrom catboost import CatBoostClassifier\ncatmodel=CatBoostClassifier(iterations=50, depth=12, learning_rate=0.1,)\ncatmodel.fit(X,y,plot=True)","85f29254":"cat_train_pred = catmodel.predict(X)\ncat_test_pred = catmodel.predict(testx)\n\nprint(accuracy_score(trainy, cat_train_pred))\nprint(accuracy_score(testy, cat_test_pred))","244d2f19":"**Read the Dataset**","526b8c08":"Converting Categorical Columns to Dummies","ab9eb7e3":"Search Grid for Hyper paramenter tuning","d81085b5":"A quick Comparision of Accuracy metrics for each of the models","327575a6":"\n![image.png](attachment:image.png)","5c78b7a4":"Train-Test Split","f0d1dbfd":"Search Grid for Hyper paramenter tuning","901daaff":"Search Grid for Hyper paramenter tuning","672281af":"# Random Forest","0b1f1ab3":"# SVM","0cb02fed":"Dropping Unneccesary columns ","6e2f5a13":"# AdaBoost Classifier","00e24cd9":"# Naive Bayes ","056b0ad1":"A Quick comparision of different Classifiers applied on the popular Bank_loan_Classification dataset","d1225a0d":"# **Import Libraries**","1972a782":"Search Grid for Hyper paramenter tuning","9fa857d3":"# EDA","c514cb86":"AUC Scores","6a232dad":"Data types","6645934f":"# Scaling Numeric features","9a78ff7c":"# Cat Boost","d8c444c7":"# Comparison","97d3b88c":"# Decision Tree","0fda2e1b":"# Gradient Boosting","8d562a34":"# Logistic Regression","231d72d5":"# KNN Classifier"}}