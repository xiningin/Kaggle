{"cell_type":{"ee2fd58d":"code","ff3cebb7":"code","3cea163e":"code","ab5ededb":"code","caeed1f2":"code","798fccfa":"code","8830e5fd":"code","5b9f8d64":"code","ef8e2e99":"code","861dd015":"code","8bc00f4e":"code","91c5f268":"code","3b6fe0c9":"code","d0c4979b":"code","be614465":"code","65dbf660":"code","019f024e":"code","7b910f91":"code","61cadd6e":"code","5417b6ea":"code","40033eaa":"code","d8189bc5":"code","38cf9036":"code","e4c147dc":"code","ebc6627c":"code","3f7fa8ae":"code","ef1921df":"code","d1ca802b":"code","c8ee619e":"code","611f72ff":"code","1079101d":"code","621ba94f":"code","c8c1b0a2":"code","30b4635b":"code","47de4ed1":"code","319d87b0":"code","659221d9":"code","1deae157":"code","6535f287":"code","57283713":"markdown","2ef47478":"markdown","55483104":"markdown","a5b25ef1":"markdown","eb542503":"markdown","277bdd00":"markdown","5e4c6186":"markdown","a51f9dfc":"markdown","62f105d3":"markdown","30777846":"markdown","ef2d769f":"markdown","ea4e6378":"markdown","ac16a509":"markdown","b29454b9":"markdown","ead49bdd":"markdown","b30c8cf6":"markdown","717c053a":"markdown","4beaba77":"markdown","78331e4e":"markdown","33487914":"markdown","e0074bff":"markdown","e09d6066":"markdown","ac360bae":"markdown","e9957fce":"markdown","0f58d79f":"markdown","621a1d41":"markdown","21cd37d6":"markdown","e16d294b":"markdown","abdfb873":"markdown","5f03ab67":"markdown"},"source":{"ee2fd58d":"import numpy as np\nimport pandas as pd\nimport os\nimport pickle\nimport time\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.impute import SimpleImputer\n\n# Dimension compression\nfrom sklearn.feature_selection import RFE\n\n# Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# widget\nfrom ipywidgets import interact,interactive,fixed,interact_manual\nfrom IPython.display import display\nimport ipywidgets as widgets","ff3cebb7":"# Change view options\npd.options.display.max_columns = 50","3cea163e":"# file path(train data and test data)\ntrain_file = '..\/input\/tabular-playground-series-may-2021\/train.csv'\ntest_file = '..\/input\/tabular-playground-series-may-2021\/test.csv'","ab5ededb":"# column name for submission file\nID_column = 'id'\n\n# column name of the target data\ntarget = 'target'\nsubmission_cols = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\n\n# create list to reject columns from dataset for train dataset preparation\nunuse_columns_for_train = [ID_column,]\ntrain_reject_items = unuse_columns_for_train + [target]","caeed1f2":"# one-hot-encoding columns (categorical data)\nohe_columns = []\n\n# create dic to set data type for each column\nmy_dtype = {k: object  for k in ohe_columns}\nprint(my_dtype)","798fccfa":"# read csv file\ndataset = pd.read_csv(train_file, header=0)\n\n# To drop reject items columns from train data set\n# pandas.core.frame.DataFrame\nX = pd.DataFrame(dataset).drop(columns=train_reject_items, axis=1)\n\n# pandas.core.series.Series\ny = pd.Series(dataset[target])\n\n# replace target value as below\ndic={\"Class_1\":0, \"Class_2\":1, \"Class_3\":2, \"Class_4\":3}\ny.replace(dic, inplace=True)\n\n# check the shape\nprint('----------------------------------------------------------------')\nprint('X shape: (%i,%i)' %X.shape)\nprint('----------------------------------------------------------------')\nprint('y shape: (%i,)' %y.shape)\nprint('----------------------------------------------------------------')\nprint(y.value_counts())\nprint('----------------------------------------------------------------')\nprint()\nprint('dataset:csv file raw data')\ndisplay(dataset.head())\nprint('X: Train dataset')\nX.join(y).head()","8830e5fd":"# Check missing value in each column of dataset\ndataset.isnull().sum()","5b9f8d64":"def one_hot_encoding(data, ohe_columns):\n    X_ohe = pd.get_dummies(data,\n                       dummy_na=True,    # True:including missing value\n                       columns=ohe_columns)\n    print('X_ohe shape:(%i,%i)' % X_ohe.shape)\n    display(X_ohe.head())\n    return X_ohe\n    \nX_ohe = one_hot_encoding(X, ohe_columns)\nX_ohe_columns = X_ohe.colimns.values","ef8e2e99":"X_ohe_columns = X_ohe.columns.values\nX_ohe_columns","861dd015":"# Parameters of RFE dimension compression\nn_estimators=100\nn_features_to_select=50\nstep=.05","8bc00f4e":"def dimension_compression(X_ohe, y):\n    start = time.time()\n    selector = RFE(RandomForestClassifier(n_estimators=n_estimators, random_state=1),\n               n_features_to_select=n_features_to_select, # number of column number after compression\n               step=step)\n    selector.fit(X_ohe,y)\n    X_ohe_columns =  X_ohe.columns.values\n    \n    # selector.support_ list of True or False\n    X_fin = X_ohe.loc[:, X_ohe_columns[selector.support_]]\n    \n    # Duration time\n    duration = time.time() - start\n    print(duration,'s')\n    \n    print('Duration Time:', 'X_fin shape:(%i,%i)' % X_fin.shape)\n    display(X_fin.head())\n    return X_fin, selector\n    \nX_fin, selector = dimension_compression(X_ohe, y)","91c5f268":" selector","3b6fe0c9":"print('-----------------------------------')\nprint('X_fin shape: (%i,%i)' %X_fin.shape)\nprint('-----------------------------------')\nprint(y.value_counts())\nprint('--------------------------------------------------')\nprint('--------------------------------------------------')\nprint('y shape: (%i,)' %y.shape)\nprint('--------------------------------------------------')","d0c4979b":"# data split for train and test\nX_train, X_test, y_train, y_test = train_test_split(X_fin, y, test_size=0.2, random_state=46, stratify=y)","be614465":"X_train.shape","65dbf660":"# set pipelines for different algorithms\npipelines = {\n    'knn':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',KNeighborsClassifier())]),\n    'tree':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',DecisionTreeClassifier(random_state=1))]),\n    'rf':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',RandomForestClassifier(random_state=1))]),\n\n    'lgbm':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',lgb.LGBMClassifier())]),\n}","019f024e":"params = {\n    'knn' : {'est__n_neighbors':[5, ],\n             'est__weights':['uniform','distance'],},\n    \n    'tree': {'est__max_depth': list(range(10, 20)),\n            'est__criterion': ['gini', 'entropy'],},\n    \n    'rf': {'est__n_estimators':[100, 300],\n            'est__max_depth': [8, 10,],\n            'est__random_state': [0],},\n    \n    'lgbm': {'est__max_depth':[10, 50,],\n            'est__learning_rate':[0.01,0.1],\n            'est__num_leaves':[31, 64],\n            'est__n_estimators':[100, 500,],},\n}","7b910f91":"#  evaluation parameter\nevaluation_scoring = 'accuracy'\n\n# instance initialization for score data durring grid search\nscores = {}\nbest_params ={}\nbest_scores ={}\n\n# training pipeline with grid search\nfor pipe_name, pipeline in pipelines.items():\n    print(pipe_name)\n    print(params[pipe_name])\n    start = time.time()\n    gs = GridSearchCV(estimator=pipeline,\n                     param_grid = params[pipe_name],\n                     scoring=evaluation_scoring,\n                     cv=5,\n                     return_train_score=False)\n    # train\n    gs.fit(X_train, y_train)\n    \n    print('time', time.time()-start)\n    scores[(pipe_name,'train')] = accuracy_score(y_train, gs.predict(X_train))\n    scores[(pipe_name,'test')] = accuracy_score(y_test, gs.predict(X_test))\n    best_params[pipe_name] = gs.best_params_\n    best_scores[pipe_name] = gs.best_score_\n    \n    # create directories for each models\n    os.makedirs('\/kaggle\/working\/models\/pipeline_models', exist_ok=True)\n     # save created model \n    file_name = '\/kaggle\/working\/models\/pipeline_models\/'+pipe_name+str(time.time())+'.pkl'\n    pickle.dump(pipeline, open(file_name, 'wb'))\n\nprint('---accuracy---')\npd.Series(scores).unstack()","61cadd6e":"def get_answer(x):\n    return x","5417b6ea":"# select model by using radio button\nmodel_selection = get_answer(widgets.RadioButtons(options=pipelines.keys()))\ndisplay(model_selection)","40033eaa":"selected_model_name = 'lgbm'","d8189bc5":"# check selected model name\n# selected_model_name = model_selection.value\nprint(selected_model_name)","38cf9036":"X_fin.shape","e4c147dc":"def train_on_selected_model(pipe_name, pipelines, params, evaluation_scoring, tag):\n    start = time.time()\n    gs = GridSearchCV(estimator=pipelines[pipe_name],\n                     param_grid = params,\n                     scoring=evaluation_scoring,\n                     cv=5,\n                     return_train_score=False)\n    # Train with all data\n    # model = gs.fit(X_train, y_train)\n    model = gs.fit(X_fin, y)\n    \n    print('time', time.time()-start)\n    scores[(pipe_name,'train')] = accuracy_score(y_train, gs.predict(X_train))\n    scores[(pipe_name,'test')] = accuracy_score(y_test, gs.predict(X_test))\n    \n    # create dirs to save pkl model\n    os.makedirs('\/kaggle\/working\/models\/', exist_ok=True)\n    # save pkl\n    file_name = '\/kaggle\/working\/models\/' + pipe_name + '_' + tag + str(time.time())+'.pkl'\n    pickle.dump(model, open(file_name, 'wb'))\n    \n    return model","ebc6627c":"# to cast type of best_params[seleced_model_name] value to list type\ndef params_parser(best_params, selected_model_name):\n    return {k:[v] for k,v in best_params[selected_model_name].items()}\n\nbest_param_parsed = params_parser(best_params, selected_model_name)\nprint(best_param_parsed)","3f7fa8ae":"# Re-train with all data on selected model\nselected_model = train_on_selected_model(selected_model_name, pipelines, best_param_parsed, evaluation_scoring, 'selected')","ef1921df":"# read test data\ndataset_s = pd.read_csv(test_file,header=0)\n\n# To drop reject items columns from train data set\n# pandas.core.frame.DataFrame\nID_s = dataset_s.iloc[:,[0]]\nX_s = dataset_s.drop(unuse_columns_for_train, axis=1)\n\n# # check the shape\nprint('-----------------------------------')\nprint('Raw Shape: (%i, %i)' %dataset_s.shape)\nprint('X_s Shape: (%i, %i)' %X_s.shape)\nprint('-----------------------------------')\nprint(X_s.dtypes)\nID_s.join(X_s).head()","d1ca802b":"X_ohe_s = pd.get_dummies(X_s,\n                         dummy_na=True,\n                         columns=ohe_columns)\nprint('X_ohe_s shape:(%i,%i)' % X_ohe_s.shape)\nX_ohe_s.head(3)","c8ee619e":"cols_model= set(X_ohe.columns.values)\ncols_score = set(X_ohe_s.columns.values)\n\ndiff1 = cols_model - cols_score\nprint('Columns of existing in training data:: %s' %diff1)\n\ndiff2 = cols_score - cols_model\nprint('Columns of existing in test data for submission: %s' %diff2)","611f72ff":"# DataFrame X_ohe\n# Case1: Train data have, Test data does not have column -> Fill \"0\"\n# Case2: Train data does not have, Test data have column -> Drop\n\ndataset_cols_m = pd.DataFrame(None,\n                         columns=X_ohe.columns.values,\n                         dtype=float)\ndisplay(dataset_cols_m)\n\nX_ohe_s = pd.concat([dataset_cols_m, X_ohe_s])\nprint(X_ohe_s.shape)\ndisplay(X_ohe_s.head(3))","1079101d":"# Concat Empty DataFrame and test data(X_s)\nX_ohe_s = pd.concat([dataset_cols_m, X_ohe_s])\nprint(X_ohe_s.shape)\ndisplay(X_ohe_s.head(3))","621ba94f":"set_Xm = set(X_ohe.columns.values)\nset_Xs = set(X_ohe_s.columns.values)\nprint(set_Xs-set_Xm)\nX_ohe_s = X_ohe_s.drop(list(set_Xs-set_Xm),axis=1)\n\nprint(X_ohe_s.shape)\ndisplay(X_ohe_s.head(3))","c8c1b0a2":"print(set_Xm-set_Xs)\nX_ohe_s.loc[:,list(set_Xm-set_Xs)] = X_ohe_s.loc[:,list(set_Xm-set_Xs)].fillna(0,axis=1)\nprint(X_ohe_s.shape)\nX_ohe_s.head(3)","30b4635b":"X_ohe_s = X_ohe_s.reindex(X_ohe_columns, axis=1)\nX_ohe_s.head(3)\nprint(X_ohe_s.shape)","47de4ed1":"selector.support_","319d87b0":"X_fin_s = X_ohe_s.loc[:, X_ohe_columns[selector.support_]]\nprint(X_fin_s.shape)\nX_fin_s.head(3)","659221d9":"# prediction\ny_pred = selected_model.predict_proba(X_fin_s)\nprint(len(y_pred))","1deae157":"id_df=pd.DataFrame(data=ID_s,columns=[ID_column])\n\nsubmission_column_list =  ['Class_1','Class_2', 'Class_3', 'Class_4']\nresult_df = pd.DataFrame(data=y_pred,columns=submission_column_list)\nsubmit_df = pd.concat([id_df, result_df], axis=1)\nsubmit_df.head()","6535f287":"# Setting for submission file\nsubmit_file_dir = '\/kaggle\/working\/'\nsubmit_file_name = 'submission.csv'\n\n# make directories for submittion file\nos.makedirs(submit_file_dir, exist_ok=True)\n\n# To generate csv file for submittion\nsubmit_df.to_csv(submit_file_dir + submit_file_name , index=False)","57283713":"### 1-1. import libraries","2ef47478":"### 3-2. Dimension compression with RFE","55483104":"## 4. Pipeline Definition  \n### 4-1. Define algorithm in pipeline\ndescribe ML algorithm as a dictionary type","a5b25ef1":"### 5-3. Re-training wiht all data on selected model","eb542503":"## 3. Data Preprocessing","277bdd00":"### 1-2. Path setting","5e4c6186":"### 2-2. Check missing value","a51f9dfc":"### 7-2. Generate csv file","62f105d3":"## 6. Prediction  \nprediction with test data","30777846":"![Screenshot 2021-05-30 23.25.35.png](attachment:9331ab4d-226a-43db-88e8-1abaf36d91af.png)","ef2d769f":"### 3-1. One Hot Encoding\nOne hot encoding process for tha data of categorical data along with ohe_columns list","ea4e6378":"### 4-2. Parameter for Gridsearch Setting  \nMany parameters needs long time for training.","ac16a509":"### 6-2. One-Hot-Encodeing for test data","b29454b9":"Case1: Train data have, Test data does not have column -> Fill \"0\"","ead49bdd":"# Algorithm pipeline & GridSearch, Simple my AutoML\n\nThis notebook includes training pipeline building with grid search.Several algorithm and parameters can be evaluated at one notebook.Too many parameters variation needs long time for training.  \nThis notebook does not include detail EDA process, but show data process flow, like a simple Auto ML.\n\n1. Preparation  \n    1-1. import libraries  \n    1-2. Path setting  \n    1-3. Setting for Training, Prediction and Submission  \n2. Load Dataset  \n    2-1. Read csv file  \n    2-2. Check missing value  \n3. Data Preprocessing  \n    3-1. One Hot Encoding  \n    3-2. Dimension compression with RFE  \n4. Pipeline Definition  \n    4-1. Define algorithm in pipeline  \n    4-2. Parameter for Gridsearch Setting  \n5. Training  \n    5-1. Training  \n    5-2. Selecting models for test data prediction  \n    5-3. Re-training wiht all data on selected model  \n6. Prediction  \n    6-1. Load test data  \n    6-2. One-Hot-Encoding  \n    6-3. Difference at columns between training data and test data  \n    6-4. Prediction  \n7. Submission  \n    7-1. provide predicted data for submission  \n    7-2. Generate csv file  ","b30c8cf6":"Select columns after dimension compressing","717c053a":"## 7. Submission  \n### 7-1. provide predicted data for submission","4beaba77":"## 5. Training  \n### 5-1. Training\ntraining on pipeline and grid search","78331e4e":"To choose \"selected model\" from pipeline , GridSearchCV is used with signal algorithm and parameter.","33487914":"(Added explanation)\nIn 18 code is for notebook save & run. The radio button selection can not be used in the save & run.If you copy & edit this notebook, radio button can be used for algorithm selection ","e0074bff":"### 1-3. Setting for Training, Prediction and Submission","e09d6066":"Display radio button and select model","ac360bae":"### 6-1. Load Test Data","e9957fce":"## 2. Load Dataset  \n### 2-1. Read csv file\nload data to provide training dataset","0f58d79f":"### 5-2. Selecting models for test data prediction  \nselect algorithm model for submission from pipeline  ","621a1d41":"The above radio button can be used for algorithm selection.  \nBut the top algorithm is selected when the save & run.  ","21cd37d6":"### 6-4. Prediction","e16d294b":"Case2: Train data does not have, Test data have column -> Drop","abdfb873":"## 1. Preparation","5f03ab67":"### 6-3. Difference at columns between training data and test data  \nCompare column between X_ohe and X_ohe_s after One-Hot-Encoding  "}}