{"cell_type":{"682608b7":"code","a2c7cae6":"code","9b3a6e90":"code","1f0b65ca":"code","db4f99b2":"code","25fb7013":"code","2b7f85ee":"code","36ff3bb0":"code","256fc267":"code","0fc87833":"code","23327257":"code","1a2b1257":"code","d2069e0f":"markdown","8cfa7f93":"markdown","e22036fb":"markdown","0b7e8dac":"markdown","df8b1920":"markdown","ed513ff5":"markdown","35d73ae8":"markdown","bf284c69":"markdown","274cf5d4":"markdown"},"source":{"682608b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2c7cae6":"from sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\nimport random\nimport pickle\nimport matplotlib.pyplot as plt","9b3a6e90":"def getKmers(sequence, size=8):\n    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]\nhuman = pd.read_table('\/kaggle\/input\/dna-sequence-dataset\/human.txt')\nchimp = pd.read_table('\/kaggle\/input\/dna-sequence-dataset\/chimpanzee.txt')\ndog = pd.read_table('\/kaggle\/input\/dna-sequence-dataset\/dog.txt')\ndef generate_dataset(dfs,kmer_size,max_features,split=5):\n    kmer_dfs=[]\n    for cur_df in dfs:\n        cur_df['words']=cur_df.apply(lambda x: getKmers(x['sequence'],size=kmer_size), axis=1)\n        cur_df=cur_df.drop('sequence',axis=1)\n        kmer_dfs.append(cur_df)\n    all_data=pd.concat(kmer_dfs).reset_index(drop=True)\n    perm=np.random.permutation(len(all_data)) #shuffle the data\n    test_data=all_data[:len(all_data)\/\/split]\n    train_data=all_data[len(all_data)\/\/split:]\n    train_kmers=[]\n    for cur_kmer_list in train_data.words.values:\n        train_kmers.extend(cur_kmer_list)\n    vectorizer = CountVectorizer(max_features=max_features).fit(train_kmers) \n    \n    print(train_data[\"class\"].value_counts())\n    print(test_data[\"class\"].value_counts())\n    \n    X_train=[]\n    Y_train=[]\n    X_test=[]\n    Y_test=[]\n    for cur_data, label in zip(train_data['words'],train_data['class']):\n        cur_transformed=vectorizer.transform(cur_data)\n        X_train.append(cur_transformed.toarray().sum(axis=0))\n        Y_train.append(label)\n    for cur_data, label in zip(test_data['words'],test_data['class']):\n        cur_transformed=vectorizer.transform(cur_data)\n        X_test.append(cur_transformed.toarray().sum(axis=0))\n        Y_test.append(label)  \n    return X_train, Y_train, X_test, Y_test\nX_train, Y_train, X_test, Y_test=generate_dataset([human,chimp,dog],kmer_size=9,max_features=1500)\n\n","1f0b65ca":"import torch.utils.data as data_utils\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader","db4f99b2":"class NN(nn.Module):\n    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n        super(NN, self).__init__()\n        self.in_dim=in_dim\n        self.n_hidden_1=n_hidden_1\n        self.n_hidden_2=n_hidden_2\n        self.out_dim=out_dim\n        self.layer1 = nn.Sequential(\n            nn.Linear(in_dim, n_hidden_1),\n            nn.ReLU(True))\n        self.layer2 = nn.Sequential(\n            nn.Linear(n_hidden_1, n_hidden_2),\n            nn.ReLU(True))\n        self.layer3 = nn.Sequential(\n            nn.Linear(n_hidden_2, out_dim),\n            nn.ReLU(True))\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x","25fb7013":"train_data = data_utils.TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))\ntrain_loader = data_utils.DataLoader(train_data, batch_size=4, shuffle=True)\ntest_data=data_utils.TensorDataset(torch.tensor(X_test), torch.tensor(Y_test))\ntest_loader=data_utils.DataLoader(test_data, batch_size=4, shuffle=True)\nn_features=len(train_data[0][0])\nn_labels=7","2b7f85ee":"learning_rate=5e-3\nnum_epochs=10\ntorch.manual_seed(2021)\nmodel = NN(n_features, n_features\/\/5, n_features\/\/10, n_labels)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","36ff3bb0":"def train_model(model,num_epochs,train_loader,criterion,optimizer,verbose=False,learning_rate=5e-3):\n    for epoch in range(num_epochs):\n        if(verbose):\n            print('*' * 10)\n            print(f'epoch {epoch+1}')\n        running_loss = 0.0\n        running_acc = 0.0\n        for i, data in enumerate(train_loader, 1):\n            cur_seq, label = data\n            cur_seq=cur_seq.float()\n            label=label.squeeze_()\n            optimizer.zero_grad()\n            out = model(cur_seq)\n            loss = criterion(out, label)\n            running_loss += loss.item()\n            _, pred = torch.max(out, 1)\n            running_acc += (pred == label).float().mean()\n            loss.backward()\n            optimizer.step()\n            if (verbose):\n                if i % 400 == 0:\n                    print(f'[{epoch+1}\/{num_epochs}] Loss: {running_loss\/i:.6f}, Acc: {running_acc\/i:.6f}')\n        if(verbose):\n            print(f'Finish {epoch+1} epoch, Loss: {running_loss\/i:.6f}, Acc: {running_acc\/i:.6f}')\n    return running_acc\/i, model\ndef validate_model(model,test_loader):\n        # eval\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in test_loader:\n            cur_seq, labels = data\n            cur_seq=cur_seq.float()\n            labels=labels.squeeze_()\n            # calculate outputs by running images through the network\n            out = model(cur_seq)\n            _, predicted = torch.max(out, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct\/total","256fc267":"acc,model=train_model(model,20,train_loader,criterion,optimizer,verbose=False,learning_rate=5e-3)\nprint(\"Model training accuracy is %.4f\"%acc)","0fc87833":"validate_model(model,test_loader)","23327257":"learning_rate=5e-3\nnum_epochs=10\ntorch.manual_seed(2021)\nmodel = NN(n_features, n_features\/\/5, n_features\/\/10, n_labels)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","1a2b1257":"acc,model=train_model(model,num_epochs,train_loader,criterion,optimizer,verbose=True,learning_rate=5e-3)\nvalidate_model(model,test_loader)","d2069e0f":"## 00. Background\nThe basic background of the data used here is exceptionally covered in the kernel provided by the uploader for this dataset [1]. In this notebook, we will base the analysis on kmer-counting and expand on this strategy using various models. ","8cfa7f93":"### 02.3 Model Sanity Test\nIt's usally good practice to try overfitting the model on the traning data first. If the model can overfit the data, that means this dataset is learnable by our naive implementation of neural network. Clearly, overfitting will hurt the model performance on unseen data as shown by the diagram below. However, avoiding overfitting would be a later job once we confirm the model is learning from the dataset. Red is the error on test dataset while blue is the error on training dataset. \n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fc\/Overfitting.png\" alt=\"Overfitting Schematic\">","e22036fb":"**References**\n1. https:\/\/www.kaggle.com\/nageshsingh\/demystify-dna-sequencing-with-machine-learning#Demystify-DNA-Sequencing-with-Machine-Learning\n2. https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c2\/MultiLayerNeuralNetworkBigger_english.png\n3. https:\/\/medium.com\/thenoobengineer\/numpy-arrays-vs-tensors-c58ea54f0e59\n4. https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fc\/Overfitting.png\n5. https:\/\/github.com\/L1aoXingyu\/pytorch-beginner\/tree\/master\/03-Neural%20Network","0b7e8dac":"## 03 Conclusion\nChanging only the number of epochs doesn't offer improvements. But the model does show potential in learning from the data. So we can try tunning both the feature engineering part and the model training part to improve its performace in the future. ","df8b1920":"### 02.4 Training\nSeems like our naive neural network could learn this dataset. Now we will reduce the number of epoch we train the model and see if it prevents the model from overfitting. ","ed513ff5":"## 02. Model\nFor this kernel, we will leverage the power of deep learning using Pytorch. The most simple model is a feedforward neural network composed of an input layer, several hidden layers and a final output layer. The number of nodes in input layer represents the number of features and the number of nodes in output layer represents number of classes. Here is a general schematic of what a feedforward neural network looks like:\n\n<img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c2\/MultiLayerNeuralNetworkBigger_english.png\" alt =\"FNN Schematic\">\n\nThe specific configuration of our experimental model has two hidden layers. We can fine tune model structure after we have shown the prototye works fine on our data (i.e. the model is learning). \n","35d73ae8":"## 01. Preprocess\nMost of the preprocessing done here is based on [1]. The main difference is how kmer is represented as a matrix. The original method used a 4-gramers for 6-mers. This should be equivalent to a 9-mer theoratically. For instance, [\"**ACCAT**\",\"**CCACTA**\",\"**CACTAA**\",\"**ACTAAG**\"] is equivalent to \"**ACCACTAG**\" in terms of the information it contains. Therefore, the preprocessing here does kmer counting for all the train sequences. It then selects the top X number of most frequent kmer as feature to transform the each sequence. For example, if a sequence is \"ACGTAGACGT\" and the top most frequented kmer feature is [\"**ACGT**\",\"**GTAG**\",\"**GGCC**\"], the transfromed matrix for this sequence is [**2,1,0**]. \n\nSince human, chimpanzee and dog are all mammals, we will consider these DNA data as a single dataset. The train and validation split is done on this combined dataset as well. 80% of the sequences are used for training and the rest is used for test. ","bf284c69":"### 02.2 Hyperparameters\nDeep learning also involves controlling parameters that are not part of the model itself. These parameters are termed **hyperparameters** as they control how the model learns from training data. For experimentation, we used emprically reasonable values for these parameters.","274cf5d4":"### 02.1 Numpy to Tensor\nThis might be an over-simplification, but deep learning at its core is really just a lot of matrix operations. These matrices are termed **tensor**. Despite its different nomenclature, it does support most of the functions we can find in an numpy array. If interested, you may google the difference between numpy arrays and tensors. But basically, tensor is a multi-dimensional array that can operate on both cpu and gpu. When on GPU, the matrix operation will be significantly faster than numpy array on CPU as shwon in the diagram below thanks to this [benchmark](http:\/\/medium.com\/thenoobengineer\/numpy-arrays-vs-tensors-c58ea54f0e59)[3]. \n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*S6fg58538jD1Ittq1wE5aw.png\" alt=\"diagram of runtime\">\nThis is why many deep learning projects use GPU to do training instead of CPU. But since this DNA sequence dataset isn't that much and our model is fairly simple, CPU works just fine. We will use some built-in functions of Pytorch to convert numpy arrays into tensors and dataloaders. "}}