{"cell_type":{"5067a037":"code","c9cf41b7":"code","154097b1":"code","4da5cab7":"code","9a0feb09":"code","3b8bb84b":"code","a5a7518a":"code","7ec7da58":"code","220ba78c":"code","c3d98b56":"code","bd7f5802":"code","9811cfb8":"code","158fd8a2":"code","971a9a9e":"code","0bc8b1ea":"code","672d22ab":"code","09fb5ae1":"code","29837c6d":"code","454b352b":"code","6da04206":"code","e37c7876":"code","d8a4279a":"code","1b44d052":"code","3464192b":"code","a82e633e":"code","92e8595b":"code","6c9799ec":"code","657fc51b":"code","fa337675":"code","da4c5791":"code","71e042c5":"code","151c90b9":"code","04729660":"code","2aa8395c":"code","e6ef50f8":"code","584d47f6":"code","5bf4cf0c":"code","e143a2ce":"code","b8a915ca":"code","fa9b934b":"code","42e68b68":"code","7971e69a":"code","45fb2aaf":"code","82986721":"code","247ac60a":"code","09d1c0c0":"markdown","7550fda2":"markdown","dd4eabcb":"markdown","5f340076":"markdown","e4ef1e8f":"markdown","10b1679a":"markdown"},"source":{"5067a037":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns","c9cf41b7":"df = pd.read_csv('..\/input\/room-occupancy\/file.csv')\ndf.head().style.background_gradient(axis = 0)","154097b1":"df.info()","4da5cab7":"df.describe().style.background_gradient(axis = 0)","9a0feb09":"# No balance issue D:\nsns.countplot(data = df , x = 'Occupancy')","3b8bb84b":"df['Occupancy'].value_counts()","a5a7518a":"plt.figure(figsize=(10,6),dpi = 150)\nsns.scatterplot(data = df , x = 'Humidity' , y ='HumidityRatio' , hue = 'Occupancy')","7ec7da58":"plt.figure(figsize=(10,6),dpi = 150)\nsns.scatterplot(data = df , x = 'HumidityRatio' , y ='CO2' , hue = 'Occupancy')","220ba78c":"plt.figure(figsize = (10,6))\nsns.boxplot(data = df , x = 'Occupancy' , y ='Humidity')","c3d98b56":"Oc_count = df['Occupancy'].value_counts().tolist()\nOc_label = df['Occupancy'].value_counts().index\nplt.figure(figsize = (8,4),dpi = 150)\nplt.pie(Oc_count , labels = Oc_label , autopct = '%1.2f%%',textprops={'fontweight':'bold','size' :13},startangle= 90,\n       pctdistance = 0.8 , radius = 2)\nplt.show()","bd7f5802":"plt.figure(figsize=(8,6),dpi = 150)\nsns.heatmap(df.corr(),annot = True)","9811cfb8":"sns.pairplot(data = df , hue = 'Occupancy')","158fd8a2":"# Now we are going to divid the data to X and y : \n\nX = df.drop('Occupancy', axis = 1)\ny = df['Occupancy']","971a9a9e":"# Now we are going to make train test split : \nfrom sklearn.model_selection import train_test_split","0bc8b1ea":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","672d22ab":"# Now we are going to scale the data  : \n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_x_train = scaler.fit_transform(X_train)\nscaled_x_test = scaler.transform(X_test)","09fb5ae1":"# Create logisticRegression model and fit it on scaled X train \nfrom sklearn.linear_model import LogisticRegression\nlg_model = LogisticRegression()\nlg_model.fit(scaled_x_train,y_train)","29837c6d":"# Now we are going to define function contain all needed metrixes , that will need it to know the model accuracy \n\nfrom sklearn.metrics import accuracy_score , plot_confusion_matrix , plot_roc_curve , plot_precision_recall_curve , classification_report\n\ndef metrix(model):\n    y_pred = model.predict(scaled_x_test)\n    print(classification_report(y_test,y_pred))\n    plot_confusion_matrix(model , scaled_x_test , y_test)\n    plot_roc_curve(model , scaled_x_test , y_test)\n    plot_precision_recall_curve(model , scaled_x_test , y_test)","454b352b":"metrix(lg_model)","6da04206":"from sklearn.neighbors import KNeighborsClassifier","e37c7876":"# Now we need to know the best K value \"Lowest error rate for created loop\"\n\ntest_error_rate = []\n\nfor k in range(1,30):\n    knn_model = KNeighborsClassifier(n_neighbors = k )\n    knn_model.fit(scaled_x_train , y_train)\n    y_pred = knn_model.predict(scaled_x_test)\n    error = 1 - accuracy_score(y_test , y_pred)\n    \n    test_error_rate.append(error)","d8a4279a":"# Lowest error rate is 1 , so this is the k value :\nplt.figure(figsize=(8,4),dpi = 150)\nplt.plot(range(1,30),test_error_rate)","1b44d052":"knn_model = KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_x_train , y_train)","3464192b":"# Knn_model Accuracy : \nmetrix(knn_model)","a82e633e":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C':[0.001,0.01,1]}\nsvc = SVC()","92e8595b":"grid_model = GridSearchCV(svc,param_grid)","6c9799ec":"grid_model.fit(scaled_x_train , y_train)","657fc51b":"# Check Grid_model accuracy : \n\nmetrix(grid_model)","fa337675":"from sklearn.tree import DecisionTreeClassifier","da4c5791":"tree_model = DecisionTreeClassifier()","71e042c5":"tree_model.fit(scaled_x_train , y_train)","151c90b9":"metrix(tree_model)","04729660":"from sklearn.ensemble import RandomForestClassifier","2aa8395c":"random_model = RandomForestClassifier()\nrandom_model.fit(scaled_x_train , y_train)","e6ef50f8":"metrix(random_model)","584d47f6":"from sklearn.ensemble import AdaBoostClassifier\n# Now we want to get the best N_estimators ","5bf4cf0c":"error_rate = []\n\nfor n in range(1,30):\n    Ad_model = AdaBoostClassifier(n_estimators= n )\n    Ad_model.fit(scaled_x_train , y_train)\n    predections = Ad_model.predict(scaled_x_test)\n    \n    error = 1-accuracy_score(y_test , predections)\n    \n    error_rate.append(error)","e143a2ce":"plt.plot(range(1,30),error_rate)","b8a915ca":"Ad_model = AdaBoostClassifier(n_estimators=29)","fa9b934b":"Ad_model.fit(scaled_x_train , y_train)","42e68b68":"metrix(Ad_model)","7971e69a":"Ad_model.feature_importances_","45fb2aaf":"features = pd.DataFrame(index = X.columns , data = Ad_model.feature_importances_ , columns = ['Importance'])\nfeatures ","82986721":"Important_features = features.sort_values('Importance')\nImportant_features.style.background_gradient(axis=0)","247ac60a":"plt.figure(figsize=(10,6),dpi = 200)\nsns.barplot(data = Important_features.sort_values('Importance') , x = Important_features.sort_values('Importance').index , y = 'Importance')\nplt.xticks(rotation = 90);","09d1c0c0":"Random_Forest_classifier_Algorithm","7550fda2":"****Logistic_Regression Algorithm","dd4eabcb":"Decision_Tree Algorithm","5f340076":"SVM Algorithm","e4ef1e8f":"**KNN Algorithm **","10b1679a":"**** AdaboostClassifier Algorithm : "}}