{"cell_type":{"a1bdae6a":"code","f4e40fd9":"code","a19a68a3":"code","e202c166":"code","125f1df7":"code","a2263fba":"code","2cc4ef12":"code","52488b9a":"code","b2ab9deb":"code","e864878b":"code","d96d92b9":"code","1a42462c":"code","6b0edaa4":"code","83a39c14":"code","5cf38fb6":"code","3c498507":"code","8e5be5a9":"code","8a09cb1d":"code","0ba55133":"code","4dd56195":"code","1e420bc3":"code","b7997a93":"code","59072e0a":"code","db069620":"code","fbee453e":"code","97a55e5e":"code","8286347d":"code","c579127c":"code","0e7f0e94":"code","51aff0c2":"code","5f67d20a":"code","144a81a7":"code","24913b80":"code","aee611af":"code","d5b1c75c":"code","4bef245f":"code","6dd6f676":"code","8d5f38bf":"code","34d6b108":"code","6090b3d6":"code","e34bb2ed":"code","48faf6f8":"code","e8f0d545":"code","a414891b":"code","47814824":"code","f059e286":"code","6162f91b":"code","d8c9e401":"code","8f7022fd":"code","e9ca12cc":"code","575a5033":"code","7eef1e3b":"code","584318b8":"code","672bf4af":"code","ef35e118":"code","120547ec":"code","53f6fd4e":"code","0553f80c":"markdown","41566118":"markdown","ed8c17b0":"markdown","c0ed1ce8":"markdown","7f290bdb":"markdown","6580eb08":"markdown","c40cf57e":"markdown","acf01622":"markdown","d0153a03":"markdown","8a5edba0":"markdown","06422348":"markdown","3783e607":"markdown","8c6d7e27":"markdown","37bc04ff":"markdown","2c4504e5":"markdown","5e094d79":"markdown","02b04cde":"markdown","f7481ea9":"markdown","38747d7f":"markdown","302c14d1":"markdown","da81c90b":"markdown","867da529":"markdown","64e95d31":"markdown","f8754ed2":"markdown","43786139":"markdown","9245850a":"markdown","6fc5a8dc":"markdown","dd004688":"markdown","7f053895":"markdown"},"source":{"a1bdae6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import linear_model\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold\nimport math\nfrom sklearn.svm import SVR\nimport lightgbm\nfrom xgboost.sklearn import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom lightgbm import LGBMRegressor\nfrom sklearn import neighbors\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.ensemble import AdaBoostRegressor\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f4e40fd9":"#Cr\u00e9er un Pandas DataFrame \u00e0 partir des fichiers CSV:\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#enregistrer la colonne  'Id'\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#puis la supprimer puisqu\"elle inutile dans le processus de la prediction \ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#mettre train et test dans une seule dataset\ndata = train.append(test,sort=False)\n#taille des datasets:\nprint(train.shape)\nprint(test.shape)# test ne contient pas la colonne 'SalePrice'\nprint(data.shape)\n\ndata.head()","a19a68a3":"#quelques statisques sur la 'target variable' 'SalePrice'\ndata.SalePrice.describe() #on constate que le prix de vente moyen est ~ 180921$","e202c166":"#distribution de la variable 'SalePrice' \nplt.hist(train['SalePrice'], color= 'r')\nplt.title('Distribution de sales price des maisons', fontsize = 24)\nplt.ylabel('observation', fontsize = 20)\nplt.xlabel('sales price', fontsize = 20)\n\nplt.show()","125f1df7":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","a2263fba":"#prendre juste les valeurs num\u00e9riques\nnumeric_features = data.select_dtypes(include = [np.number])\n#features with the most correlation with the predictor variable\ncorr = numeric_features.corr()\nprint(corr['SalePrice'].sort_values(ascending = False)[:5], '\\n')\nprint(corr['SalePrice'].sort_values(ascending = False)[-5:])","2cc4ef12":"#log transforming transformer sale price en une distribution gaussienne\ntarget = np.log(data.SalePrice)\n#definir\nplt.scatter(x=data['GarageArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nf1=plt.show()\nf1\n#definir\nplt.scatter(x=data['OverallQual'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('OverallQual')\nf2=plt.show()\nf2\n#definir\nplt.scatter(x=data['GrLivArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('GrLivArea')\nf3=plt.show()\nf3\n#definir\nplt.scatter(x=data['GarageCars'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('GarageCars')\nf4=plt.show()\nf4","52488b9a":"#v\u00e9rifier l'existence des valeurs nulles:\ndata[data.isnull().any(axis=1)]","b2ab9deb":"#maintenant on va afficher le nombre des valeurs manquantes pour chaque collone 'feature':\na=data.isnull().sum().sort_values(ascending=False)[:25]\nb=a\/2919*100\nnulls = pd.DataFrame({'Nb_val_nulles': a,'pourcentage': b})\nnulls.index.name = 'Features'\nnulls","e864878b":"#les colonnes'features' qui ont plus que 1000 valeur manquantes doivent etre supprim\u00e9es:\ndata = data.dropna(axis=1, how='any', thresh = 1000)\ndata.shape","d96d92b9":"#rempacer les valeurs manquantes par la veleur mean:\ndata = data.fillna(data.mean())\n#train = train.select_dtypes(include= [np.number]).interpolate().dropna()\n#v\u00e9rifier l'existence de valeurs nulles\ndata[data.isnull().any(axis=1)]","1a42462c":"#remplacer les valeurs de type object par des integers\ndata = pd.get_dummies(data)\ndata.shape","6b0edaa4":"#Verifying missing values\ndata[data.isnull().any(axis=1)]\n#sum(data.isnull().sum() != 0)","83a39c14":"#Supprimer les variables corr\u00e9l\u00e9es  entre eux: car ils donnent tous les memes informations:\ncovariance = data.corr()\nallFeatures = [i for i in covariance]\nsetOfDroppedFeatures = set() \nfor i in range(len(allFeatures)) :\n    for j in range(i+1,len(allFeatures)): \n        feature1=allFeatures[i]\n        feature2=allFeatures[j]\n        if abs(covariance[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n            setOfDroppedFeatures.add(feature1)\ndata = data.drop(setOfDroppedFeatures, axis=1)\ndata.shape","5cf38fb6":"#supprimer les features qui ont la minimale correlation avec notre target variable'SalePrice':\nnonCorrelated = [column for column in data if abs(data[column].corr(data[\"SalePrice\"])) < 0.045]\ndata = data.drop(nonCorrelated, axis=1)\ndata.shape","3c498507":"#on separe les datastes (Because removing outliers \u21d4 removing rows, and we don't want to remove rows from test set)\n\nnewTrain = data.iloc[:1460]\nnewTest = data.iloc[1460:]\n\n\n#Second, definir une fonction quie retourne les valeurs des outliers via la methode  percentile()\n\ndef outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75]) #Get 1st and 3rd quartiles (25% -> 75% of data will be kept)\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5) #Get lower bound\n    upper_bound = quartile_3 + (iqr * 1.5) #Get upper bound\n    return np.where((ys > upper_bound) | (ys < lower_bound)) #Get outlier values\n\n#Third, supprimer les ouliers juste de train dataset \n\ntrainWithoutOutliers = newTrain #We can't change train while running through it\n\nfor column in newTrain:\n    outlierValuesList = np.ndarray.tolist(outliers_iqr(newTrain[column])[0]) #outliers_iqr() returns an array\n    trainWithoutOutliers = newTrain.drop(outlierValuesList) #Drop outlier rows\n    \ntrainWithoutOutliers = newTrain\nprint(outlierValuesList)\nprint(trainWithoutOutliers.shape)","8e5be5a9":"X = trainWithoutOutliers.drop(\"SalePrice\", axis=1) #supprimer la colonne SalePrice \nY = np.log1p(newTrain[\"SalePrice\"])","8a09cb1d":"#splitting the data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 42, test_size = .33)","0ba55133":"#\u00e7a va me servire pour faire la prediction\nnewTest = newTest.drop(\"SalePrice\", axis=1) ","4dd56195":"rmse_val = [] #to store rmse values for different k\nfor K in range(20):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train, y_train) \n    \n    predictions=model.predict(X_test) #make prediction on test set\n    error = mean_squared_error(y_test, predictions) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)","1e420bc3":"# on choisi K=5 puisque cette valeur donne RMSE minimal\n#cr\u00e9er le mod\u00e8le\nknn = neighbors.KNeighborsRegressor(n_neighbors = 4)\n#fitting linear regression on the data\nknn_fit = knn.fit(X_train, y_train)","b7997a93":"#R square value\nprint('R square is: {}'.format(knn_fit.score(X, Y)))\n#predicting on the test set\nprediction1 = knn_fit.predict(X_test)\n#evaluating the model on mean square error\nprint('RMSE is {}'.format(mean_squared_error(y_test, prediction1)))","59072e0a":"lab_enc = preprocessing.LabelEncoder()\ny_train_encoded = lab_enc.fit_transform(y_train)\nprint(y_train_encoded)\nprint(utils.multiclass.type_of_target(y_train))\nprint(utils.multiclass.type_of_target(y_train.astype('int')))\nprint(utils.multiclass.type_of_target(y_train_encoded))","db069620":"#cr\u00e9er le mod\u00e8le\nlogir = LogisticRegression(random_state = 0)\n#fitting linear regression on the data\nlogir_fit = logir.fit(X_train, y_train_encoded)","fbee453e":"#R square value\n#print('R square is: {}'.format(logir_fit.score(X_test, y_test)))\n#predicting on the test set\nprediction2 = logir.predict(X_test)\n#evaluating the model on mean square error\nprint('RMSE is {}'.format(mean_squared_error(y_test, prediction2)))","97a55e5e":"#cr\u00e9er le mod\u00e8le\nlr = linear_model.LinearRegression()\n#fitting linear regression on the data\nlr_fit = lr.fit(X_train, y_train)","8286347d":"#R square value\nprint('R square is: {}'.format(lr_fit.score(X, Y)))\n#predicting on the test set\npredictions = lr_fit.predict(X_test)\n#evaluating the model on mean square error\nprint('RMSE is {}'.format(mean_squared_error(y_test, predictions)))","c579127c":"pred1=np.expm1(lr_fit.predict(newTest))\nsub1 = pd.DataFrame() #Create a new DataFrame for submission\nsub1['Id'] = test_ID\nsub1['SalePrice'] = pred1\nsub1.to_csv(\"submissionlin.csv\", index=False) #Convert DataFrame to .csv file\nsub1","0e7f0e94":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\nalphas_Rd = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphasL = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","51aff0c2":"#cr\u00e9er mod\u00e8le\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphasL, random_state=42, cv=kfolds))","5f67d20a":"#fitting\nlasso_fit=lasso.fit(X_train, y_train)","144a81a7":"#prediction on train set\ny_train_predict = lasso_fit.predict(X_train)\n#prediction on test set\ny_test_predict = lasso_fit.predict(X_test)\n#MSE trainset\nlasso_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(lasso_train))\n#MSE test set\nlasso_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(lasso_test))\n","24913b80":"#c\u00e9re modele\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_Rd, cv=kfolds))","aee611af":"#fiting\nridge_fit=ridge.fit(X_train, y_train)","d5b1c75c":"#prediction on train set\ny_train_predict = ridge_fit.predict(X_train)\n#prediction on test set\ny_test_predict = ridge_fit.predict(X_test)\n#MSE trainset\nridge_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(ridge_train))\n#MSE test set\nridge_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(ridge_test))","4bef245f":"#cr\u00e9er mod\u00e8le\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","6dd6f676":"#fiting\nsvr_fit=svr.fit(X_train, y_train)","8d5f38bf":"#prediction on train set\ny_train_predict = svr_fit.predict(X_train)\n#prediction on test set\ny_test_predict = svr_fit.predict(X_test)\n#MSE trainset\nsvr_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(svr_train))\n#MSE test set\nsvr_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(svr_test))","34d6b108":"#Gradient boosting regressor model\ngbr = GradientBoostingRegressor(n_estimators= 1000, max_depth= 2, learning_rate= .01)\n#fiting\ngbr_fit=gbr.fit(X_train, y_train)","6090b3d6":"#prediction on train set\ny_train_predict = gbr_fit.predict(X_train)\n#prediction on test set\ny_test_predict = gbr_fit.predict(X_test)\n#MSE trainset\nest_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(est_train))\n#MSE test set\nest_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(est_test))","e34bb2ed":"pred4=np.expm1(gbr_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissiongbr.csv\", index=False) #Convert DataFrame to .csv file\nsub4","48faf6f8":"# cr\u00e9er mod\u00e9le\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","e8f0d545":"# fiting\nxgboost_fit=xgboost.fit(X_train, y_train)","a414891b":"#prediction on train set\ny_train_predict = xgboost_fit.predict(X_train)\n#prediction on test set\ny_test_predict = xgboost_fit.predict(X_test)\n#MSE trainset\nxgboost_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(xgboost_train))\n#MSE test set\nxgboost_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(xgboost_test))","47814824":"pred4=np.expm1(xgboost_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionxgb2.csv\", index=False) #Convert DataFrame to .csv file\nsub4","f059e286":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","6162f91b":"# fiting\nlightgbm_fit=lightgbm.fit(X_train, y_train)","d8c9e401":"#prediction on train set\ny_train_predict = lightgbm_fit.predict(X_train)\n#prediction on test set\ny_test_predict = lightgbm_fit.predict(X_test)\n#MSE trainset\nlightgbm_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(lightgbm_train))\n#MSE test set\nlightgbm_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(lightgbm_test))","8f7022fd":"pred3=np.expm1(lightgbm_fit.predict(newTest))\nsub3 = pd.DataFrame() #Create a new DataFrame for submission\nsub3['Id'] = test_ID\nsub3['SalePrice'] = pred3\nsub3.to_csv(\"submissionlight3.csv\", index=False) #Convert DataFrame to .csv file\nsub3","e9ca12cc":"#hyperparameter tuning\n#ada=AdaBoostRegressor()\n#search_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1],'random_state':[1]}\n#search=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=5)\n#search.fit()","575a5033":"#cr\u00e9er le mod\u00e8le\nadab=AdaBoostRegressor(n_estimators=500,learning_rate=0.001,random_state=1)\nscore=np.mean(cross_val_score(adab,X_train, y_train,scoring='neg_mean_squared_error',cv=7,n_jobs=1))\nscore","7eef1e3b":"# fiting\nadab_fit=adab.fit(X, Y)","584318b8":"#prediction on train set\ny_train_predict = adab_fit.predict(X_train)\n#prediction on test set\ny_test_predict = adab_fit.predict(X_test)\n#MSE trainset\nadab_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(adab_train))\n#MSE test set\nadab_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(adab_test))","672bf4af":"pred4=np.expm1(xgboost_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionadab.csv\", index=False) #Convert DataFrame to .csv file\nsub4","ef35e118":"from sklearn.ensemble import RandomForestRegressor\n#Cr\u00e9er model\nforest = RandomForestRegressor(n_estimators=100)\n#fitting\nforest.fit(X, Y)\ny_test_predict = forest.predict(X_test)","120547ec":"#MSE test set\nforest_test = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(forest_test))","53f6fd4e":"pred4=np.expm1(forest.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionforest3.csv\", index=False) #Convert DataFrame to .csv file\nsub4","0553f80c":" ## 1-Exploration et pr\u00e9paration des donn\u00e9es","41566118":"## submission ","ed8c17b0":"## 4. R\u00e9duction de dimensionnalit\u00e9","c0ed1ce8":"## *1.3 Data cleaning:*","7f290bdb":"### avant reduction","6580eb08":"### xgboost","c40cf57e":"### 1.4 supprimer les features selon la corr\u00e9lation","acf01622":"### KNN","d0153a03":"## *1.1-Data ingestion*","8a5edba0":"### LASSO","06422348":"* ***distribution de la variable 'SalePrice'***","3783e607":"### La r\u00e9gression logistique","8c6d7e27":"### Light Gradient Boosting Machine","37bc04ff":"### 1.3.1 les valeurs nulles","2c4504e5":"### Random Forest","5e094d79":"## 2-agr\u00e9gation","02b04cde":"### La r\u00e9gression d\u2019ar\u00eates(Ridge)","f7481ea9":"### Data processing:","38747d7f":"###  AdaBoost","302c14d1":"## *1.2-stastical analysis*","da81c90b":"### SVR","867da529":"* ***La cor\u00e9lation avec 'Sale Price*** ","64e95d31":"### gradiant boosting","f8754ed2":"### 1.3.2 transformer les string en int","43786139":"### 1.3.3 Les ouliers","9245850a":"* ***visualiser les ouliers*** ","6fc5a8dc":"### linear regression:","dd004688":"## Reduction de dimentionnalit\u00e9","7f053895":"## 2-Apprentissage du mod\u00e8le"}}