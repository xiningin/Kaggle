{"cell_type":{"4e8da16a":"code","6af631b9":"code","2d609307":"code","41d8c93b":"code","3dd5e81f":"code","21adbe2e":"code","69c96ffd":"code","ef1e7b9a":"code","349ffd49":"code","77562d4e":"code","584efb9c":"code","ffeb1c9a":"code","c63d2964":"markdown","b06608f9":"markdown","4b9b1e81":"markdown","e917347e":"markdown","66da7e21":"markdown","cfc98d78":"markdown","63e998b3":"markdown","a52f5123":"markdown","03143be8":"markdown","5f9e045d":"markdown","e20dc80b":"markdown","2806fa98":"markdown","080d60fc":"markdown","240ebf9f":"markdown","d76c4a32":"markdown","c5891f85":"markdown","55ab335b":"markdown","6b465f01":"markdown","f5f33e7e":"markdown","020285c3":"markdown","18bb90fd":"markdown","b23757ae":"markdown","16c4a99a":"markdown","7f1cd386":"markdown","10ff9109":"markdown","f5beae89":"markdown","6888feb7":"markdown","ad23d0f0":"markdown","49fc5c4e":"markdown","9d917480":"markdown","5da0284f":"markdown","6b57724b":"markdown","44a2bfcc":"markdown","b988023b":"markdown"},"source":{"4e8da16a":"# import\nfrom sklearn.metrics import \"metric_name\"\n\n#create instance\n\"metric_name\"(y_test, model.predict(X_test)) # y_test => real target, X_test => predicted target","6af631b9":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3) ","2d609307":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters = number_of_clusters)\nmodel.fit(X)","41d8c93b":"# Datam\u0131zdaki her bir verinin etiketlerine(label) eri\u015fmek a\u015fa\u011f\u0131daki gibidir.\nmodel.labels_\n\n# Her verinin etiketi, olu\u015fturdu\u011fumuz verilerin bir s\u00fctununda saklanabilir.\ndata['Label'] = model.labels_\n\n# Yeni verilerin k\u00fcme etiketlerine eri\u015fmek i\u00e7in a\u015fa\u011f\u0131daki kodu kullanabiliriz. Yeni datam\u0131z bir dizi, bir liste veya bir DataFrame \u015feklinde olmal\u0131d\u0131r.\ndata.predict(new_data)\n\n# Her k\u00fcmenin k\u00fcme merkezlerine a\u015fa\u011f\u0131daki gibidir. \u0130ki boyutlu bir dizi bi\u00e7iminde d\u00f6nd\u00fcr\u00fcl\u00fcr.\ndata.cluster_centers_\n","3dd5e81f":"from sklearn.metrics import silhouette_score\nscores = []\nfor cluster_num in range(lower_bound, upper_bound):\n     model = KMeans(n_clusters=cluster_num)\n     model.fit(data)\n     score = silhouette_score(data, model.predict(data))","21adbe2e":"# fit_transform => Modeli otomatik olarak verilere fit eder ve daha az say\u0131da boyuta d\u00f6n\u00fc\u015ft\u00fcr\u00fcr. \n# \"number\" de\u011fi\u015fkeni, indirgenmi\u015f verilerin sahip olaca\u011f\u0131 boyut say\u0131s\u0131n\u0131 temsil eder. \u00d6rne\u011fin, g\u00f6rselle\u015ftirme s\u00f6z konusu oldu\u011funda, iki boyut olacakt\u0131r.\n\n\nfrom sklearn.decomposition import PCA\n\nmodel = PCA(n_components=number)\ndata = model.fit_transform(data)","69c96ffd":"model.explained_variance_ratio_","ef1e7b9a":"model.components_","349ffd49":"from sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_components = number)\ntransformed = lda.fit_transform(X, y)","77562d4e":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nmodel = PermutationImportance(model)\nmodel.fit(X,y)\neli5.show_weights(model, feature_names = X.columns.tolist())","584efb9c":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(data)\ntransformed_data = scaler.transform(data)","ffeb1c9a":"from sklearn.preprocessing import Normalizer\n\nnormalize = Normalizer()\ntransformed_data = normalize.fit_transform(data)","c63d2964":"# Regresyon Y\u00f6ntemi (Regression Method)\n\nRegresyon problemleri, \u00fcretilen \u00e7\u0131kt\u0131n\u0131n s\u00fcrekli say\u0131lardan olu\u015ftu\u011fu durumlar i\u00e7in kullan\u0131l\u0131yor.\n\n\u00d6rne\u011fin: bir \u00e7al\u0131\u015fan\u0131n i\u015fe geldi\u011fi g\u00fcn say\u0131s\u0131, g\u00fcn i\u00e7inde \u00fcretti\u011fi \u00fcr\u00fcn adedine g\u00f6re ona say\u0131sal bir verimlilik puan\u0131 olu\u015fturmak isterseniz regresyon algoritmalar\u0131n\u0131 kullanabilirsiniz.","b06608f9":"> Modelinin \u00e7al\u0131\u015ft\u0131\u011f\u0131 verilerde, ***lat*** s\u00fctunu hedef de\u011fi\u015fken \u00fczerinde en b\u00fcy\u00fck etkiye sahiptir. En iyi tahmin performans\u0131 i\u00e7in modellerde hangisinin kald\u0131r\u0131laca\u011f\u0131na (i\u015faretlenmi\u015f) karar verir. D\u00fc\u015f\u00fck perm\u00fctasyon de\u011ferine sahip olanlar gereksiz ve de\u011fi\u015fikli\u011fe sebep vermeyecek de\u011ferlerdir.\n![](https:\/\/miro.medium.com\/max\/1400\/1*O2SM0n2j60rqpry--B6bSw.png)\n\n","4b9b1e81":"# Sklearn Regressors\n>Sklearn k\u00fct\u00fcphanesinin regresyon modelleri a\u015fa\u011f\u0131daki gibidir.\n * **branch** olarak bahsedilen b\u00f6l\u00fcm ***mavi***,\n * **model_name** ise ***k\u0131rm\u0131z\u0131*** renktedir.\n![image.png](attachment:image.png)\n","e917347e":"# Train-Test Split\nA\u015fa\u011f\u0131da belirtilen modelimizin test boyutu %30 olarak belirtilmi\u015ftir. \u00d6rne\u011fin: 100 verilik bir datam\u0131z\u0131n 70'ini train 30'u test olarak kullanabiliriz.\n\n![image.png](attachment:image.png)","66da7e21":"# **S\u0131n\u0131fland\u0131rma y\u00f6nteminde en \u00e7ok kullan\u0131lan algoritmalar:**\n\n1. **Naive Bayes :** \nVerileri olas\u0131l\u0131k ilkeleri ile hasaplayarak s\u0131n\u0131fland\u0131ran bir s\u0131n\u0131fland\u0131rma algoritmas\u0131d\u0131r. Basit bir ifadeyle, bir Naive Bayes s\u0131n\u0131fland\u0131r\u0131c\u0131, bir s\u0131n\u0131ftaki belirli bir \u00f6zelli\u011fin varl\u0131\u011f\u0131n\u0131n ba\u015fka herhangi bir \u00f6zelli\u011fin varl\u0131\u011f\u0131na ba\u011fl\u0131 olmad\u0131\u011f\u0131n\u0131 varsayar. \u00d6rne\u011fin, bir meyve k\u0131rm\u0131z\u0131, yuvarlak ve \u00e7ap\u0131 yakla\u015f\u0131k 3 in\u00e7 ise bir elma olarak d\u00fc\u015f\u00fcn\u00fclebilir. Bu \u00f6zellikler birbirlerine veya di\u011fer \u00f6zelliklerin varl\u0131\u011f\u0131na ba\u011fl\u0131 olsa bile, bu \u00f6zelliklerin t\u00fcm\u00fc, bu meyvenin bir elma olmas\u0131 olas\u0131l\u0131\u011f\u0131na ba\u011f\u0131ms\u0131z olarak katk\u0131da bulunur ve bu y\u00fczden \u201cNaif\u201d olarak bilinir.\n2. **K-Nearest Neighbours (En Yak\u0131n Kom\u015fu) : **\nBu tip s\u0131n\u0131fland\u0131rma, her bir noktan\u0131n en yak\u0131n kom\u015fular\u0131n\u0131n basit \u00e7o\u011funluk oyu ile hesaplanmas\u0131 ile elde edilen s\u0131n\u0131fland\u0131rmad\u0131r. Veri hangi veriye en \u00e7ok yak\u0131nd\u0131r? mant\u0131\u011f\u0131 ile dallan\u0131r. \nBu algoritman\u0131n uygulanmas\u0131 kolayd\u0131r, g\u00fcr\u00fclt\u00fcl\u00fc e\u011fitim verisine (noisy training data) dayan\u0131kl\u0131d\u0131r ve e\u011fitim verileri b\u00fcy\u00fckse olduk\u00e7a etkilidir.\n3. **Decision Tree (Karar A\u011fac\u0131) :**\nVeriler, s\u0131n\u0131flar\u0131 ile birlikte bu algoritmaya verildi\u011finde, algoritma verileri s\u0131n\u0131fland\u0131rmak i\u00e7in kullan\u0131labilecek bir dizi kural \u00fcretir. Karar d\u00fc\u011f\u00fcmleri(decision node) ve yaprak d\u00fc\u011f\u00fcmleri(leaf node) olan bir a\u011fa\u00e7 yap\u0131s\u0131na sahiptir. Hem s\u0131n\u0131fland\u0131rma hem de regresyon y\u00f6nteminde kullan\u0131labilir.\n4. **Random Forest :**\nS\u0131n\u0131fland\u0131rma i\u015flemi s\u0131ras\u0131nda birden fazla decision-tree kullan\u0131larak s\u0131n\u0131fland\u0131rma de\u011ferinin y\u00fckseltilmesi hedefleyen, s\u0131n\u0131flama veya regresyon y\u00f6nteminde kullan\u0131labilen algoritmad\u0131r.\n5. **Support Vector Machine (Destek\u00e7i Vekt\u00f6r Makinesi) : **\nVeri setinde birbirine benzeyen gruplar aras\u0131na birbirinden en uzak olan noktalardan s\u0131n\u0131rlar \u00e7izmeye yarayan algoritmad\u0131r.","cfc98d78":"# 4. \u00d6nemli \u00f6zelliklerin bulunmas\u0131 (Feature Importance)\n\nBir hedef i\u00e7in en \u00f6nemli \u00f6zelli\u011fi bulma i\u015flemidir. PCA arac\u0131l\u0131\u011f\u0131yla, en fazla bilgiyi i\u00e7eren \u00f6zellik bulunabilir, ancak bir \u00f6zelli\u011fin hedef \u00fczerindeki etkisini bulmak bizim i\u00e7in daha \u00f6nemlidir. \"\u00d6nemli\" bir \u00f6zellikteki de\u011fi\u015fikli\u011fin y de\u011fi\u015fkeni \u00fczerinde b\u00fcy\u00fck bir etkisi olurken, \"\u00f6nemsiz\" \u00f6zellikteki de\u011fi\u015fikli\u011fin y de\u011fi\u015fkeni \u00fczerinde \u00e7ok az etkisi olur veya hi\u00e7 olmaz. Bunlar\u0131n kontrol edilip bulunmas\u0131 gerekmektedir.","63e998b3":"# S\u0131n\u0131fland\u0131rma Y\u00f6ntemi (Classification Method)\n\nE\u011fer sistem, hangi verinin, hangi ko\u015fullarda, hangi s\u0131n\u0131fa ait olaca\u011f\u0131 bilgisi ile s\u0131n\u0131fland\u0131r\u0131larak e\u011fitilirse, yeni veri setindeki veriyi de \u00f6\u011frendiklerine benzer bi\u00e7imde s\u0131n\u0131fland\u0131rabilir.\n\n\u00d6rne\u011fin:\n0-17 ya\u015f aral\u0131\u011f\u0131ndaki ki\u015fileri \u00e7ocuk,\n18-25 ya\u015f aral\u0131\u011f\u0131ndaki ki\u015fileri gen\u00e7,\n26 ya\u015f ve \u00fcst\u00fcndeki ki\u015fileri yeti\u015fkin s\u0131n\u0131f\u0131yla s\u0131n\u0131fland\u0131rmak,\n\n\n**S\u0131n\u0131fland\u0131rma Y\u00f6ntemleri de a\u015fa\u011f\u0131daki gibi kategorize edilir:**\n1. **Binary Classification (\u0130kili S\u0131n\u0131fland\u0131rma):** \u0130ki olas\u0131 sonu\u00e7 ile s\u0131n\u0131fland\u0131rma. \u00d6rn: Cinsiyet s\u0131n\u0131fland\u0131rmas\u0131 (Erkek \/ Kad\u0131n)\n\n2. **Multi Class Classification (\u00c7oklu S\u0131n\u0131f S\u0131n\u0131fland\u0131rma):** \u0130kiden fazla s\u0131n\u0131f\u0131 s\u0131n\u0131fland\u0131rma. Bir s\u0131n\u0131fa ait birden fazla farkl\u0131 veri varsa bu farkl\u0131 veriler tespit edilir ve her biri tek bir etikete atan\u0131r. \u00d6rn: Bir hayvan s\u0131n\u0131f\u0131nda kedi ya da k\u00f6pek olabilir ancak ikisi birlikte bir s\u0131n\u0131fta olamaz kendi i\u00e7inde s\u0131n\u0131flara b\u00f6l\u00fcnmelidir.\n\n3. **Multi Label Classification (\u00c7oklu Etiket S\u0131n\u0131fland\u0131rma):** Bir veri birden fazla s\u0131n\u0131fla ili\u015fkilendirilebilir. \u00d6rn: Bir makale hem sa\u011fl\u0131k hem spor hem de insan ile ilgili olabilir.","a52f5123":"# 1. Tahminsel Modelleme (Predictive Modelling)\n\n**Train(e\u011fitim,\u00f6\u011fretme) ve Test Split Sistemi:** Bir modelin, belirlenen train verileri \u00fczerinde e\u011fiterek ve test verileri \u00fczerinde test edilerek ne kadar iyi performans g\u00f6sterdi\u011finin \u00f6nemli bir par\u00e7as\u0131d\u0131r. Bu \u015fekilde, modelin yeni verileri e\u011fitme(train) yetene\u011fi \u00f6l\u00e7\u00fclebilir.","03143be8":"# Sklearn Classifiers\n> Sklearn k\u00fct\u00fcphanesinin s\u0131n\u0131fland\u0131r\u0131c\u0131 modelleri a\u015fa\u011f\u0131daki gibidir.\n * **branch** olarak bahsedilen b\u00f6l\u00fcm ***mavi***,\n * **model_name** ise ***k\u0131rm\u0131z\u0131*** renktedir.\n![image.png](attachment:image.png)","5f9e045d":"Optimum k\u00fcme say\u0131s\u0131n\u0131 bulmak i\u00e7in, belirli say\u0131da k\u00fcmenin verilere ne kadar iyi uydu\u011funun bir \u00f6l\u00e7\u00fcs\u00fc olan silhouette_score metric'ini kullanabiliriz. \n\n\u00d6nceden tan\u0131mlanm\u0131\u015f bir aral\u0131ktaki (lower_bound and upper_bound) her k\u00fcme say\u0131s\u0131(n_cluster)i\u00e7in, bir K-Means k\u00fcmeleme algoritmas\u0131 e\u011fitilir ve skorumuz bir listeye kaydedilir(scores). Sonras\u0131nda bu skorlar\u0131 grafi\u011fe d\u00f6k\u00fcp en y\u00fcksek skoru hangi k\u00fcme say\u0131s\u0131nda bulmu\u015f diye kontrol sa\u011flayabiliriz.","e20dc80b":"# 5. Veri d\u00f6n\u00fc\u015f\u00fcm\u00fc (Data Transformation)","2806fa98":"# Normalizing \nVerileri normalle\u015ftirmek, 0'a 1 \u00f6l\u00e7e\u011fine getirir. Standartla\u015ft\u0131r\u0131lm\u0131\u015f verilere benzer \u015fekilde, verilerin model i\u00e7in matematiksel olarak kullan\u0131m\u0131n\u0131 kolayla\u015ft\u0131r\u0131r.","080d60fc":"#### Bu notebook'un \u00e7o\u011fu b\u00f6l\u00fcm\u00fc \u0130ngilizce i\u00e7eriklerden T\u00fcrk\u00e7e'ye \u00e7evirilen b\u00f6l\u00fcmlerden ve al\u0131nt\u0131lardan olu\u015fmaktad\u0131r.","240ebf9f":"# Principal Component Analysis (PCA)\n\n> Temel Bile\u015fen Analizi(Principal Component Analysis) veya PCA, azalt\u0131lm\u0131\u015f boyut say\u0131s\u0131n\u0131 temsil etmek i\u00e7in birka\u00e7 dikey vekt\u00f6r \u00e7izerek verilerin boyutu azaltmak i\u00e7in kullan\u0131lan pop\u00fcler bir y\u00f6ntemdir. \n![](https:\/\/miro.medium.com\/max\/1400\/0*2VjanmuMIrpFPUqd.png)\n\n","d76c4a32":"# Introduction\n\n#### Bu notebook'ta Makine \u00f6\u011frenmesi(Machine learning) ile ilgili bilgilendirmeler yap\u0131lacakt\u0131r. Sayfan\u0131n sa\u011f taraf\u0131ndan b\u00f6l\u00fcmlere direkt olarak ge\u00e7i\u015f sa\u011flayabilirsiniz.\n\n\nMakine \u00f6\u011frenmesi, makinelerin veri analizi yaparak insanlar gibi \u00f6\u011frenmelerini sa\u011flayan bir bilimdir.\n\nMakine \u00f6\u011frenmesi algoritmalar\u0131 genellikle \u00fc\u00e7 \u015fekilde s\u0131n\u0131fland\u0131r\u0131l\u0131r: \n\n    1. G\u00f6zetimli \u00d6\u011frenme(Supervised Learning): \n    \nBu s\u0131n\u0131ftaki algoritmalar, \u00f6\u011frendiklerinden yola \u00e7\u0131karak tahminleme yapmak i\u00e7in etiketli (labeled) verileri kullan\u0131r.Yani e\u011fitimde kullan\u0131lacak veri ve veriye ait s\u0131n\u0131flar (kategoriler\/etiketler) \u00f6nceden bilinir. Bu bilgi ile sistem \u00f6\u011frenir ve yeni gelen datay\u0131 bu \u00f6\u011frendikleriyle yorumlar.\n\n    2. G\u00f6zetimsiz \u00d6\u011frenme (Unsupervised Learning):\n    \nG\u00f6zetimli \u00f6\u011frenme y\u00f6nteminin aksine herhangi bir kategorize edilmi\u015f, etiketlenmi\u015f e\u011fitim verisi kullan\u0131larak e\u011fitilmez. G\u00f6zetimsiz \u00f6\u011frenme y\u00f6ntemi, \u00f6nceden e\u011fitilmemi\u015f veriler \u00fczerinde \u00e7al\u0131\u015farak veriler aras\u0131nda ba\u011f\u0131nt\u0131lar bulup birbirine yak\u0131n anlamda\/i\u00e7erikte\/de\u011ferde olan verilerin kendi i\u00e7inde k\u00fcmelenmesi mant\u0131\u011f\u0131yla \u00e7al\u0131\u015f\u0131r.\n       \n    3. Takviyeli \u00d6\u011frenme (Reinforcement Learning):\nTemelinde canl\u0131lar\u0131n davran\u0131\u015f psikolojisine dayand\u0131r\u0131l\u0131r. Bu y\u00f6ntem \u00f6\u011frenme i\u015flemini \u00e7evreden ald\u0131\u011f\u0131 geri bildirim (feed-back) ile ger\u00e7ekle\u015ftirmektedir. Bu y\u00f6ntemle olas\u0131 durumlar\u0131n, hedef olup olmad\u0131\u011f\u0131n\u0131n kontrol edilir. Denemelerin sonucunda hedefe ula\u015f\u0131lamad\u0131\u011f\u0131nda ceza (penalty), ula\u015ft\u0131\u011f\u0131nda ise \u00f6d\u00fcl (reward) sinyali al\u0131n\u0131r ve sistem ceza sinyali ald\u0131\u011f\u0131 hamleyi bir daha tekrarlamaz. \u00d6d\u00fcl sinyali ald\u0131\u011f\u0131 deneyimden faydalanarak \u00f6\u011frenmeye devam eder ve hep maksimum \u00f6d\u00fcl\u00fc ama\u00e7layarak s\u00fcrekli \u00f6\u011frenmeye i\u015flevini s\u00fcrd\u00fcr\u00fcr.\n\n\n![image.png](attachment:image.png)\n","c5891f85":"# SHapley Addative ExPlanations (SHAP)\n\nPermutation Importance'dan farkl\u0131 olarak, SHAP feature'\u0131n \u00f6nemini de\u011ferlendirmek i\u00e7in daha \u00e7ok form\u00fcl ve hesaplamaya dayal\u0131 bir y\u00f6ntem kullan\u0131r. A\u011fa\u00e7 tabanl\u0131 (tree-based) bir model (Decision Tree, Random Forest) gerektirir ve i\u00e7inde hem regresyon hem de s\u0131n\u0131fland\u0131rmay\u0131 bar\u0131nd\u0131r\u0131r.\n\n![](https:\/\/miro.medium.com\/max\/1182\/1*Crkd8FDknGOLnPaobjTT9A.png)","55ab335b":"# \u0130\u015flemler\n\n1. **Tahminsel Modelleme (Predictive Modelling) i\u015flemi:** Bu b\u00f6l\u00fcm Denetimli \u00f6\u011frenme (Supervised Learning) i\u00e7in regresyon(Regression) ve s\u0131n\u0131fland\u0131rma(Classification) algoritmalar\u0131, model performans\u0131n\u0131n de\u011ferlendirilmesi i\u00e7in \u00f6l\u00e7\u00fctler(metrics) i\u00e7erir.\n\n\n2. **K\u00fcmeleme (Clustering) i\u015flemi:** Bu b\u00f6l\u00fcm etiketi(label) olmayan verileri k\u00fcmeler(cluster) halinde K-Means gibi grupland\u0131rma y\u00f6ntemleri(algoritmalar\u0131) ile k\u00fcme say\u0131lar\u0131na dayal\u0131 objektif \u00f6l\u00e7\u00fctlerin\/metriklerin se\u00e7ilmesini i\u00e7erir.\n\n\n3. **Boyut Azaltma (Dimensionality Reduction) i\u015flemi:** Bu b\u00f6l\u00fcm verilerin boyutsall\u0131\u011f\u0131n\u0131(dimensionality) azaltma(reducing) y\u00f6ntemleri ve bu y\u00f6ntemlerin \u00f6zelliklerini i\u00e7erir.\n\n    * Principal Component Analysis (PCA),\n    * Latent Dirichlet Allocation (LDA)\n\n\n4. **\u00d6nemli \u00f6zelliklerin bulunmas\u0131 (Feature Importance)i\u015flemi:** Bu b\u00f6l\u00fcm Dataset'imizin \u00f6nemli \u00f6zelliklerini bulma, perm\u00fctasyonunu \u00f6nemi ve grafiksel y\u00f6ntemler i\u00e7erir. \n\n\n5. **Veri d\u00f6n\u00fc\u015f\u00fcm\u00fc (Data Transformation):** Bu b\u00f6l\u00fcm verileri daha y\u00fcksek tahmin g\u00fcc\u00fc(prediction) ve daha kolay analiz i\u00e7in standardizasyon (standardization), normalle\u015ftirme (normalization) gibi y\u00f6ntemler i\u00e7erir.","6b465f01":"# 3. Boyut Azaltma (Dimensionality Reduction)\n\nBoyutsal k\u00fc\u00e7\u00fcltme, her biri en fazla miktarda bilgi i\u00e7erecek \u015fekilde, verilerin azalt\u0131lmas\u0131 i\u015flemidir. Y\u00fcksek boyutlu verilerin g\u00f6r\u00fcnt\u00fclenip azalt\u0131larak veya az ili\u015fkili (low-correlation) \u00f6zelliklerin kald\u0131r\u0131lmas\u0131yla makine \u00f6\u011frenme modellerinin h\u0131zland\u0131r\u0131lmas\u0131 i\u00e7in kullan\u0131l\u0131r.\n","f5f33e7e":"**model.explained_variance_ratio_**\n\nA\u015fa\u011f\u0131daki kod ile her bir bilginin, o boyutun variance_ratio'suna kar\u015f\u0131l\u0131k geldi\u011fi bir liste olu\u015fturulur. Bu varyans oran\u0131 datada tutulan bilgilerin toplam y\u00fczdesidir","020285c3":"> Normalle\u015ftirme, verilerin \u015feklini standartla\u015ft\u0131rman\u0131n yapt\u0131\u011f\u0131 gibi d\u00f6n\u00fc\u015ft\u00fcrmese de, verilerin s\u0131n\u0131rlar\u0131n\u0131 k\u0131s\u0131tlar. Verileri normalle\u015ftirmek veya standartla\u015ft\u0131rmak algoritmaya ba\u011fl\u0131d\u0131r.","18bb90fd":"## K-Means k\u00fcmeleme modelinin e\u011fitilmesi ve olu\u015fturulmas\u0131","b23757ae":"# G\u00f6zetimli \u00d6\u011frenme (Supervised \u00d6\u011frenme) \n\nBir sistem tasarlayaca\u011f\u0131n\u0131z\u0131 varsayal\u0131m. Bu sistem sizin g\u00f6sterdi\u011finiz resimde kedi veya k\u00f6pek olup olmad\u0131\u011f\u0131n\u0131 size s\u00f6yleyecek. B\u00f6yle bir sistem i\u00e7in g\u00f6zetimli \u00f6\u011frenme kategorisindeki makine \u00f6\u011frenmesi algorima(lar\u0131na)s\u0131na ihtiya\u00e7 duyabilirsiniz.\n\nG\u00f6zetimli \u00d6\u011frenme y\u00f6ntemleri iki grupta incelenir;\n* S\u0131n\u0131fland\u0131rma Y\u00f6ntemi (Classification Method)\n* Regresyon Y\u00f6ntemi (Regression Method)\n","16c4a99a":"# Sklearn Metrics\n> S\u0131n\u0131fland\u0131rma (Classification) ve regresyon (Regression) i\u00e7in sklearn \u00f6l\u00e7\u00fcmleri(metrics) a\u015fa\u011f\u0131daki gibidir.\n* En s\u0131k kullan\u0131lan metrik ***ye\u015fil*** ile i\u015faretlenmi\u015ftir. \n* ***Gri*** metriklerin \u00e7o\u011fu, belirli ba\u011flamlarda ye\u015fil i\u015faretli metriklerden daha uygundur. Her birinin kendi avantajlar\u0131 ve dezavantajlar\u0131 vard\u0131r.\n![image.png](attachment:image.png)\n","7f1cd386":"# Standardizing\nStandartla\u015ft\u0131rma(Standardizing), verileri ayn\u0131 bilgileri i\u00e7erecek, ancak ortalama 0 ve 1 varyansa sahip olacak \u015fekilde \"yeniden \u015fekillendirme\" (reshaping) s\u00fcrecidir. Verileri \u00f6l\u00e7eklendirerek algoritmalar\u0131n matematiksel yap\u0131s\u0131 genellikle verileri daha iyi i\u015fleyebilir.","10ff9109":"# Model Performance\n> Model performans\u0131n\u0131n de\u011ferlendirilmesi a\u015fa\u011f\u0131daki gibi yap\u0131l\u0131r.\n![image.png](attachment:image.png)\n","f5beae89":"# G\u00f6zetimsiz \u00d6\u011frenme(Unsupervised Learning)\n\nGirdi verisinin hangi s\u0131n\u0131fa ait oldu\u011fu \u00f6nceden bilinmez. Bu s\u0131n\u0131fland\u0131rma i\u015flemleri veriye bak\u0131larak algoritmalar taraf\u0131ndan \u00f6\u011frenilir. Yeni gelen veriler de algoritman\u0131n olu\u015fturdu\u011fu gruplara uygun olarak en yak\u0131n gruba atan\u0131r.\n\n\nG\u00f6zetimsiz \u00d6\u011frenme y\u00f6ntemleri \u00fc\u00e7 grupta incelenir:\n* K\u00fcmeleme (Clustering)\n* Boyut Azaltma (Dimensionality Reduction)\n\n\nBu b\u00f6l\u00fcmlere a\u015fa\u011f\u0131daki K\u00fcmeleme ve Boyut Azaltma k\u0131s\u0131mlar\u0131ndan bilgi alabilirsiniz.","6888feb7":"> Support Vector Machine ve K-Nearest Neighbors gibi bir\u00e7ok mesafe tabanl\u0131 algoritma i\u00e7in kullan\u0131labilir. Standartla\u015ft\u0131r\u0131lm\u0131\u015f veri kullanan algoritmalar\u0131n sonu\u00e7lar\u0131n\u0131n \"standartla\u015ft\u0131r\u0131lmamas\u0131\" gerekir, b\u00f6ylece do\u011fru yorumlanabilirler. ","ad23d0f0":"# Import, Create and Fit\n\n![](http:\/\/)![image.png](attachment:image.png)","49fc5c4e":"# Kullan\u0131labilir K\u00fct\u00fcphaneler\n\n**NumPy:** NumPy, bilimsel\/matematiksel\/mant\u0131ksal\/istatistiksel hesaplama i\u00e7in olu\u015fturulmu\u015f bir python k\u00fct\u00fcphanesidir. NumPy kullan\u0131larak istatistik i\u015flemleri ve sim\u00fclasyonlarda yap\u0131labilir.\n\n**SciPy:** S\u0131k kullan\u0131lan matematiksel ve fiziksel problemlerinin bilgisayar ortam\u0131nda ifade edilmesine y\u00f6nelik fonksiyonlar\u0131 bar\u0131nd\u0131rmaktad\u0131r.\n\n**Pandas:** \u201cetiketli\u201d ve \u201cili\u015fkisel\u201d verilerle \u00e7al\u0131\u015fmak \u00fczere tasarlanm\u0131\u015f, yap\u0131sal olmayan verinizi yap\u0131sal veritabanlar\u0131ndaki gibi \u00e7al\u0131\u015ft\u0131rman\u0131z\u0131 sa\u011flayan k\u00fct\u00fcphanedir. Pandas, h\u0131zl\u0131 ve kolay veri i\u015fleme, toplama ve g\u00f6rselle\u015ftirme i\u00e7in tasarlanm\u0131\u015ft\u0131r. \u0130ki temel veri yap\u0131s\u0131na sahiptir;\n\n**Matplotlib:** Grafik \u00e7izimi i\u00e7in kullan\u0131l\u0131r. Bilimsel programlaman\u0131n en \u00f6nemli ara\u00e7lar\u0131ndan birisidir. Verilerin etkile\u015fimini g\u00f6rselle\u015ftirebilir ve g\u00f6rsel raporlar olu\u015fturulabilinir. \u0130ki boyutlu ya da \u00fc\u00e7 boyutlu grafikler \u00fcretilebilir. Grafik \u00e7e\u015fitleri a\u015fa\u011f\u0131daki gibidir.\nLine plots,\nScatter plots,\nBar charts and Histograms,\nPie charts,\nStem plots,\nContour plots,\nQuiver plots,\nSpectrograms\n\n**Seaborn:** Daha \u00e7ok istatistiksel modellerin g\u00f6rselle\u015ftirilmesine odaklanm\u0131\u015ft\u0131r; Bu t\u00fcr g\u00f6rselle\u015ftirmeler, \u0131s\u0131 haritalar\u0131n\u0131, verileri \u00f6zetleyen ama yine de genel da\u011f\u0131l\u0131mlar\u0131 tasvir eden grafikler i\u00e7in kullan\u0131l\u0131r. Seaborn, temelinde Matplotlib\u2019e ba\u011f\u0131ml\u0131d\u0131r.\n\n**Scikit-Learn: **Scikits, g\u00f6r\u00fcnt\u00fc i\u015fleme ve makine \u00f6\u011frenimi kolayla\u015ft\u0131rma gibi belirli i\u015flevler i\u00e7in tasarlanm\u0131\u015f bir Python k\u00fct\u00fcphanesidir. Do\u011frusal regresyon, lojistik regresyon, karar a\u011fa\u00e7lar\u0131, rastgele orman gibi bir\u00e7ok temel y\u00f6ntemi i\u00e7erir. \n\n**TensorFlow:** Google\u2019\u0131n \u00e7\u0131kartt\u0131\u011f\u0131 bir makine \u00f6\u011frenmesi k\u00fct\u00fcphanesidir. A\u00e7\u0131k kaynak kodludur. Python ile geli\u015ftirilebilinir. Derin \u00f6\u011frenme i\u00e7in kullan\u0131lan bir k\u00fct\u00fcphanedir.\n\n**Keras:** Modeller\u0131 tan\u0131mlamay\u0131 ve e\u011fitmeyi kolayla\u015ft\u0131rmay\u0131 sa\u011flayan, Theano veya Tensorflow\u2019u backend olarak kullanan ve python dilini kullanan bir wrapper.\n\n**NLTK (Natural Language Toolkit):** Do\u011fal Dil \u0130\u015fleme i\u00e7in kullan\u0131l\u0131r. NLTK, dilbilim, bili\u015fsel bilim, yapay zeka, vb. gibi konular\u0131n \u00f6\u011fretimini ve ara\u015ft\u0131rmas\u0131n\u0131 kolayla\u015ft\u0131rmay\u0131 ama\u00e7lam\u0131\u015ft\u0131r ve bug\u00fcn bu konuya odaklanarak kullan\u0131lmaktad\u0131r.","9d917480":"# Permutation Importance\n\nBir feature'\u0131n ne kadar \u00f6nemli olup olmad\u0131\u011f\u0131n\u0131 de\u011ferlendiren bir y\u00f6ntemdir. Her biri bir s\u00fctun eksik olan birka\u00e7 model e\u011fitilmi\u015ftir. Veri eksikli\u011finin bir sonucu olarak model do\u011frulu\u011fundaki buna kar\u015f\u0131l\u0131k gelen d\u00fc\u015f\u00fc\u015f, s\u00fctunun bir modelin do\u011fruluk oran\u0131n\u0131n (accuracy) ne kadar \u00f6nemli oldu\u011funu g\u00f6sterir. ***Eli5*** k\u00fct\u00fcphanesi kullan\u0131labilir.","5da0284f":"# 2. K\u00fcmeleme (Clustering)\nClusteringden \u00f6nce \u00f6nce verilerin standart hale getirilmesi gerekir(bununla ilgili bilgiler Veri D\u00f6n\u00fc\u015f\u00fcm\u00fc(Data Transformation) b\u00f6l\u00fcm\u00fcnde bulunmaktad\u0131r). K\u00fcmeleme, bir nevi nokta mesafelerine g\u00f6re k\u00fcmeler olu\u015fturma i\u015flemidir.\n![](https:\/\/miro.medium.com\/max\/1400\/0*ibd-a_gS3Mg_6-PT.jpg)","6b57724b":"**model.components_**\n\nA\u015fa\u011f\u0131daki kod ile her yeni olu\u015fturulan \u00f6zellik \u00f6nceki verilerin \u00f6zelliklerinin do\u011frusal bir birle\u015fimidir. Bu do\u011frusal a\u011f\u0131rl\u0131klara(weight) model.components_ ile eri\u015filebilir ve Feature importance a\u00e7\u0131s\u0131ndan iyi bir g\u00f6stergedir (daha y\u00fcksek weight, bu \u00f6zellikte temsil edilen daha fazla bilgiyi g\u00f6sterir).","44a2bfcc":"# **Regresyon y\u00f6nteminde en \u00e7ok kullan\u0131lan algoritmalar:**\n\n1. **Linear Regression: ** \nSay\u0131sal girdi ve \u00e7\u0131kt\u0131lar aras\u0131ndaki do\u011frusal ili\u015fkiyi tespit etmeyi sa\u011flar. D\u00fczlemde yay\u0131lm\u0131\u015f verinin modelini en iyi bi\u00e7imde do\u011frusal olarak \u00e7\u0131kartmaya \u00e7al\u0131\u015fan y\u00f6ntemdir.\n2. **Logistic Regression:  **\nBir sonucu belirleyen bir veya daha fazla ba\u011f\u0131ms\u0131z de\u011fi\u015fken bulunan veri k\u00fcmesini analiz etmek i\u00e7in d\u00fczlemde en iyi e\u011friyi yakalamaya \u00e7al\u0131\u015fan istatistiksel bir y\u00f6ntemdir. Sonu\u00e7, ikiye b\u00f6l\u00fcnm\u00fc\u015f bir de\u011fi\u015fkenle \u00f6l\u00e7\u00fcl\u00fcr (sadece iki olas\u0131 sonu\u00e7 vard\u0131r).\nBu algoritman\u0131n uygulanmas\u0131 kolayd\u0131r, g\u00fcr\u00fclt\u00fcl\u00fc e\u011fitim verisine (noisy training data) dayan\u0131kl\u0131d\u0131r ve e\u011fitim verileri b\u00fcy\u00fckse olduk\u00e7a etkilidir.\n3. **Multiple Linear Regression:**\nBirden fazla tahminleyici (predictor) de\u011fi\u015fken kullanarak tahminlemeye \u00e7al\u0131\u015f\u0131lan do\u011frusal regresyonun ad\u0131d\u0131r.\n4. **Polynomial Regression: **\nVeriler aras\u0131 ili\u015fki her zaman do\u011frusal olmayabilir. Optimum ili\u015fkiyi bulmak i\u00e7in bir e\u011fri gerekebilir. T\u0131pk\u0131 polinom fonksiyonlar\u0131nda oldu\u011fu gibi bu y\u00f6ntemde de bir terimin karesi veya k\u00fcp\u00fc(veya terimin \u00fcss\u00fc herhangi bir say\u0131 olabilir) al\u0131narak do\u011frusal olmayan bir regresyon modeli olu\u015fturulmak istenebilir. Bu gibi durumlarda kullan\u0131labilen bir algoritmad\u0131r.\n5. **Support Vector Regression:  **\nAlgoritmay\u0131 karakterize eden t\u00fcm ana \u00f6zellikleri (maksimal marj\u0131) koruyan bir regresyon y\u00f6ntemi olarak da kullan\u0131labilir. Support Vector Machine ile ayn\u0131 ilkeleri kullan\u0131r. Ana fikir, hatan\u0131n en \u00fcst d\u00fczeye \u00e7\u0131kar\u0131ld\u0131\u011f\u0131 hiper d\u00fczlemi bireyselle\u015ftirerek hatay\u0131 en aza indirgemek, hatan\u0131n bir k\u0131sm\u0131n\u0131n tolere edildi\u011fini g\u00f6z \u00f6n\u00fcnde bulundurmak. \u00d6rn: Bir personelin e\u011fitim seviyesine g\u00f6re maa\u015f\u0131n\u0131 tahmin eden model geli\u015ftirmek.\n6. **Decision Tree: **\nS\u0131n\u0131fland\u0131rma y\u00f6nteminde de kullan\u0131lan decision tree, regresyon y\u00f6nteminde de ayn\u0131 \u015fekilde kullan\u0131labilir. Bu algoritma, k\u00f6k d\u00fc\u011f\u00fcmden ba\u015flayarak, yukar\u0131dan a\u015fa\u011f\u0131ya in\u015fa edilen node\u2019lar (d\u00fc\u011f\u00fcm) ile verilerin, kendi i\u00e7lerinde benzer de\u011ferlere (homojen) sahip olanlar\u0131n\u0131n alt k\u00fcmelere ayr\u0131lmas\u0131n\u0131 sa\u011flayan algoritmad\u0131r.","b988023b":"# Latent Dirichlet Allocation (LDA)\n\nLDA (Lineer Diskriminant Analizi) ve PCA aras\u0131ndaki temel fark, LDA'n\u0131n denetimli bir algoritma olmas\u0131d\u0131r, yani hem x hem de y'yi dikkate al\u0131r. PCA yaln\u0131zca x de\u011ferini dikkate al\u0131r ve dolay\u0131s\u0131yla denetlenmeyen bir algoritmad\u0131r.\n\nPCA, verilerin yap\u0131s\u0131n\u0131 (varyans\u0131n\u0131) yaln\u0131zca noktalar aras\u0131ndaki mesafelere g\u00f6re korumaya \u00e7al\u0131\u015f\u0131rken, LDA s\u0131n\u0131flar\u0131n temiz bir \u015fekilde ayr\u0131lmas\u0131na \u00f6ncelik verir."}}