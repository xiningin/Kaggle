{"cell_type":{"f5ddfd2c":"code","97779aa6":"code","1327355d":"code","bf84d402":"code","fc5fb397":"code","66cd3cd7":"code","9ddb3c18":"code","24efa316":"code","89bdac34":"code","6ab948df":"code","36c90a4d":"code","ab2c4801":"code","70bf8b99":"code","00059a8f":"code","707a7c29":"markdown","156cdaa3":"markdown","71cfacd0":"markdown","139c2932":"markdown"},"source":{"f5ddfd2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport statsmodels.api as sm\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97779aa6":"# Loading the data\nvw_raw_data = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\nvw_raw_data.head()","1327355d":"# Describe method\nvw_raw_data.describe()","bf84d402":"# Year and Price distribution\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\nax1.hist(vw_raw_data['year'], bins = 12, color = 'slateblue')\nax1.set_title('Year distribution', fontsize = 16, weight = 'bold')\nax1.set_xlabel('Year', fontsize = 14)\nax1.set_ylabel('Count', fontsize = 14)\n\nax2.hist(vw_raw_data['price'], bins = 12, color = 'midnightblue')\nax2.set_title('Price distribution', fontsize = 16, weight = 'bold')\nax2.set_xlabel('Price', fontsize = 14)\nax2.set_ylabel('Count', fontsize = 14)\n\nplt.show()","fc5fb397":"# Seaborn PairGrid of scatter plots\ng = sns.PairGrid(vw_raw_data)\ng.map(plt.scatter)","66cd3cd7":"# Seaborn PairGrid of scatter plots\nsns.heatmap(vw_raw_data.corr(), annot=True)","9ddb3c18":"# Categorical columns unique values\nprint(vw_raw_data['model'].unique(), vw_raw_data['transmission'].unique(), vw_raw_data['fuelType'].unique())","24efa316":"# Creating dummies for those values\ndf = pd.get_dummies(vw_raw_data)\nprint(df.columns)\ndf.head()","89bdac34":"# Separating inputs and targets\ntargets = df['price']\ninputs = df.drop(['price'], axis=1)\n\n# Normalizing the data\nscaler = StandardScaler()\nscaler.fit(inputs)\n\ninputs_scaled = scaler.transform(inputs)\n\n# Splitting the data into training and test sets\nx_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size = 0.2)","6ab948df":"# Multiple Linear Regression with StatsModels\ninputs_sm = sm.add_constant(inputs)\nsm_model = sm.OLS(targets, inputs_sm)\n\n# Summary table\nprint(sm_model.fit().summary())","36c90a4d":"# Multiple Linear Regression with sklearn\nreg = LinearRegression()\nreg.fit(x_train, y_train)\n\n# Cross validation\nnp.mean(cross_val_score(reg, x_train, y_train, cv=10, scoring='neg_mean_absolute_error'))","ab2c4801":"alpha_ridge = []\nerror_ridge = []\n\nfor i in range(1500, 2000):\n    alpha_ridge.append(i\/10)\n    ridge_cv = Ridge(alpha=(i\/10.0))\n    error_ridge.append(np.mean(cross_val_score(ridge_cv, x_train, y_train, cv=4, scoring='neg_mean_absolute_error')))\n\nplt.plot(alpha_ridge, error_ridge)\n\ndf_err_ridge = pd.DataFrame(data = {'alpha': alpha_ridge, 'error': error_ridge})\nridge_alpha = df_err_ridge['alpha'][df_err_ridge['error'].idxmax()]\n\n# Ridge Regression\nridge = Ridge(alpha=ridge_alpha)\nridge.fit(x_train, y_train)","70bf8b99":"alpha_lasso = []\nerror_lasso = []\n\nfor i in range(50, 250):\n    alpha_lasso.append(i\/10)\n    lasso_cv = Lasso(alpha=(i\/10.0))\n    error_lasso.append(np.mean(cross_val_score(lasso_cv, x_train, y_train, cv=4, scoring='neg_mean_absolute_error')))\n\nplt.plot(alpha_lasso, error_lasso)\n\ndf_err_lasso = pd.DataFrame(data = {'alpha': alpha_lasso, 'error': error_lasso})\nlasso_alpha = df_err_lasso['alpha'][df_err_lasso['error'].idxmax()]\n\n# Lasso Regression\nlasso = Lasso(alpha=lasso_alpha)\nlasso.fit(x_train, y_train)","00059a8f":"# Testing the models\n\n# OLS\ntpred_reg = reg.predict(x_test)\n# Ridge\ntpred_ridge = ridge.predict(x_test)\n# Lasso\ntpred_lasso = lasso.predict(x_test)\n\nprint('OLS mae: ', mean_absolute_error(y_test, tpred_reg))\nprint('Ridge mae: ', mean_absolute_error(y_test, tpred_ridge))\nprint('Lasso mae: ', mean_absolute_error(y_test, tpred_lasso))","707a7c29":"# About the correlation matrix\n\nThe correlation matrix shows us that the price is correlated with all the variables. The less correlated variable is tax, and is still 0.48. Let's try and build a model then.\n\n# Data preprocessing\n\nWe need to deal with the categorical columns creating dummies, let's check how many unique values they have\n","156cdaa3":"# About the VW dataset\n\n* Has over 10 thousand observations, no NaN values, which is pretty good\n* I Would like to see more features, but it keeps the model simple\n* Let's visualize the data and build our regression models\n\n# Data Visualization","71cfacd0":"# The dataset\n\nThis dataset contains separated files for each car manufacturer. Let's try to work one at a time. Starting with the VW dataset.","139c2932":"# Building the models"}}