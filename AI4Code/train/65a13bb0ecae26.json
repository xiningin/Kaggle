{"cell_type":{"2f2deda3":"code","1d7455c6":"code","83990fe2":"code","a1e1bbf7":"code","3e9082a1":"code","f1d21c19":"code","52cf67a6":"code","2342c300":"code","eabb32bd":"code","488229f1":"code","0a960f96":"code","50e416da":"code","f7f79560":"code","a7d049a9":"code","17c0c933":"markdown","fe752d8d":"markdown","701eaff4":"markdown","1efc3459":"markdown"},"source":{"2f2deda3":"#IMPORTS\n\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tqdm import tqdm\nimport cv2","1d7455c6":"#LOADING THE DATA INTO A NUMPY ARRAY\n\nx_data = []\nfor png in tqdm(os.listdir('..\/input\/data\/data')):\n    path = '..\/input\/data\/data\/{}'.format(png)\n    \n    image = plt.imread(path)\n    image = image.astype('float32')\n    x_data.append(image)\n    \nx_data = np.array(x_data)    ","83990fe2":"# KERAS IMPORTS\n\nimport keras\n\nfrom keras.models import Sequential, Model, Input\n\nfrom keras.layers import Dense\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import MaxPool2D, AvgPool2D\nfrom keras.layers import UpSampling2D\nfrom keras.layers.advanced_activations import LeakyReLU\n\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Lambda\n\nfrom keras.layers import Flatten\nfrom keras.layers import Reshape\n\nfrom keras.layers import Add, Multiply\n\nfrom keras.losses import mse, binary_crossentropy\n\nimport keras.backend as K","a1e1bbf7":"#SET A SEED FOR REPRODUCABILITY\nnp.random.seed(20)\n\n#NUMBER OF DIMENSIONS IN THE ENCODED LAYER\nlatent_dims = 512","3e9082a1":"#ENCODER\n#BUILT WITH FUNCTIONAL MODEL DUE TO THE MULTIPLE INPUTS AND OUTPUTS\n\nencoder_in = Input(shape=(64,64,3))   ##INPUT FOR THE IMAGE\n\nencoder_l1 = Conv2D(filters=32, kernel_size=5, strides=1, padding='same', input_shape=(64,64,3))(encoder_in)\nencoder_l1 = BatchNormalization()(encoder_l1)\nencoder_l1 = Activation(LeakyReLU(0.2))(encoder_l1)\n\nencoder_l1 = Conv2D(filters=64, kernel_size=5, strides=2, padding='same')(encoder_l1)\nencoder_l1 = BatchNormalization()(encoder_l1)\nencoder_l1 = Activation(LeakyReLU(0.2))(encoder_l1)\n\n\nencoder_l2 = Conv2D(filters=128, kernel_size=5, strides=2, padding='same')(encoder_l1)\nencoder_l2 = BatchNormalization()(encoder_l2)\nencoder_l2 = Activation(LeakyReLU(0.2))(encoder_l2)\n\nencoder_l3 = Conv2D(filters=256, kernel_size=5, strides=2, padding='same')(encoder_l2)\nencoder_l3 = BatchNormalization()(encoder_l3)\nencoder_l3 = Activation(LeakyReLU(0.2))(encoder_l3)\n\n\nencoder_l4 = Conv2D(filters=512, kernel_size=5, strides=2, padding='same')(encoder_l3)\nencoder_l4 = BatchNormalization()(encoder_l4)\nencoder_l4 = Activation(LeakyReLU(0.2))(encoder_l4)\n\nflatten = Flatten()(encoder_l4)\n\nencoder_dense = Dense(1024)(flatten)\nencoder_dense = BatchNormalization()(encoder_dense)\nencoder_out = Activation(LeakyReLU(0.2))(encoder_dense)\n\n\nmu = Dense(latent_dims)(encoder_out)\nlog_var = Dense(latent_dims)(encoder_out)\n\n\nepsilon = Input(tensor=K.random_normal(shape=(K.shape(mu)[0], latent_dims)))  ##INPUT EPSILON FOR RANDOM SAMPLING\n\nsigma = Lambda(lambda x: K.exp(0.5 * x))(log_var) # CHANGE log_var INTO STANDARD DEVIATION(sigma)\nz_eps = Multiply()([sigma, epsilon])\n\nz = Add()([mu, z_eps])\n\nencoder=Model([encoder_in,epsilon], z)\nencoder.summary()\n","f1d21c19":"# DECODER\n# BUILT WITH SEQUENTIAL MODEL AS NO BRANCHING IS REQUIRED\n\ndecoder = Sequential()\ndecoder.add(Dense(1024, input_shape=(latent_dims,)))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Dense(8192))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Reshape(target_shape=(4,4,512)))\n\ndecoder.add(Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\n\ndecoder.add(Conv2DTranspose(filters=32, kernel_size=5, strides=2, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation(LeakyReLU(0.2)))\n\ndecoder.add(Conv2DTranspose(filters=3, kernel_size=5, strides=1, padding='same'))\ndecoder.add(BatchNormalization())\ndecoder.add(Activation('sigmoid'))\n\ndecoder.summary()\n","52cf67a6":"# COMBINE ENCODER AND DECODER THE COMPLETE THE VARIATIONAL AUTO ENCODER\n\nvae_preds = decoder(z)\nvae = Model([encoder_in, epsilon], vae_preds)\n\nvae.summary()\n","2342c300":"# MY LOSS FUNCTIONS\n\ndef reconstruction_loss(y_true, y_pred):\n    return K.mean(K.square(y_true - y_pred))\n\ndef kl_loss(y_true, y_pred):\n    kl_loss = - 0.5 * K.mean(1 + log_var - K.square(mu) - K.exp(log_var), axis=-1)\n    return kl_loss\n\ndef vae_loss(y_true, y_pred):\n    return reconstruction_loss(y_true, y_pred) + 0.03 * kl_loss(y_true, y_pred)   #scaling kl_loss by 0.03 seem to help\n","eabb32bd":"vae.compile(optimizer='adam', loss=vae_loss , metrics=[reconstruction_loss, kl_loss])","488229f1":"vae.fit(x_data,x_data, epochs=50, batch_size=64)","0a960f96":"def plot_images(rows, cols, images, title):\n    grid = np.zeros(shape=(rows*64, cols*64, 3))\n    for row in range(rows):\n        for col in range(cols):\n            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n\n    plt.figure(figsize=(20,20))       \n    plt.imshow(grid)\n    plt.title(title)\n    plt.show()","50e416da":"# ORIGINAL IMAGES\n\npredictions = x_data[:100]\nplot_images(10,10,predictions,\"ORIGINAL FACES\")","f7f79560":"# RECONSTRUCTION OF ORIGINAL IMAGES\n\npredictions  = vae.predict(x_data[:100])\nplot_images(10,10,predictions, \"RECONSTRUCTED FACES\")","a7d049a9":"#NEW FACES GENERATED FROM RANDOM NOISE\n\npredictions= decoder.predict(np.random.randn(100, latent_dims))\nplot_images(10,10,predictions, \"RANDOM IMAGINED FACES\")","17c0c933":"## Building a variational autoencoder with the aim of generating new anime faces derived from the anime faces dataset\n\n### The Theory\n\nAn autoencoder is made up of 2 parts, encoder and decoder. \n\nThe encoder takes an image and maps in onto a latent vector space, usually of smaller dimensions to that of the original input.\n\nThe decoder then takes that encoded vector and trys to reconstruct the original image. \n\nIf the encoder and decoder do a good job of mapping the original image into a smaller vector space and then reconstructing the image, then we have a data compression algorithm.\n\nAn variational autoencoder has an encoder and decoder but also has a regulizer which is trying restrict the values of the encodings to within a certain range, usually mean zero and standard deviation of one.\n\nThis means that by sampling random vectors from the encoded vector space, we have a good chance of reconstructing a face.\n\nIt is very unlikely that we will randomly choose a vector which corresponds exactly with one of the images the VAE was trained on. \n\nBut the VAE should have the ability to interpolate between close images there by generating new faces, arbeit ones based on other images.\n","fe752d8d":"## Improvements\n\n* Play around with the general architecture of the encoder and decoder. More layers? bigger layers?   \n* Increase the number of latent dimensions. My intuition says more dimension will help the reconstruction of original images, but may result in a sparser latent space resulting in poorer newly generated images. \n* Investigate the best way to implement kl_loss and reconstruction_loss, so as not have to add a scalling factor. \n","701eaff4":"## Loss Functions\n\nReconstruction loss \n* Measures how accurately the images are reconstructed.\n* Simply Mean squared error, but if I use the built in MSE function gives a dimension miss match error. \n\nkl_loss\n* kullback leibler loss\n* applied to the latent dimensions\n* The formula is a bit beyond my understanding, but this tries pull the mean and standard deviation of (z) to zero and one. \n\nThe reconstruction_loss and the kl_loss need to work well together to variational auto encoder to work as we wish\n* If the kl_loss is to weak and the reconstruction_loss too strong, then the VAE will work well in reconstructing the original images, but there maybe lots of space between encodings resulting in vectors which generate noise or lousy faces.\n* If kl_loss is too strong and reconstruction loss is too weak then all the encodings will occupy the same space. meaning all encoded vectors will try to output similar faces.\n\nvae_loss is a summation of reconstruction loss and kl_loss.\nWith some experimentation I found scaling the kl_loss by 0.03 seemed to give the best results. \nOther research has suggested that reconstruction_loss should be the sum of the errors rather than the mean. Negating the need to scale the kl_loss. \nShould the kl_loss be a summation also? I need to do more research. ","1efc3459":"## Building the encoder\n\nThe encoder compresses the image via a series of convolutions into a smaller dimensional space (z).\n\nThe output is split into 2 blocks, the mean (mu) and log_variance (log_var), which are then recombined to predict a distribution rather than a value.\n* The mean to predict the mean of the distribution.\n* log_variance, because we cant have negative variance so by exponentiating the log_var we will always get a positive number. \n* A second input(epsilon) for inputing random numbers to simulate sampling randomly from latend distribution. \n* having the random numbers input as a seperate Input allows back propagation proceed normally.\n* z is the output of the encoderer, z = mean + sigma * epsilon"}}