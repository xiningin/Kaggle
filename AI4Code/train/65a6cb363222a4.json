{"cell_type":{"16af6c52":"code","a1517af2":"code","acae6b43":"code","548774bc":"code","61b00570":"code","18a131f7":"code","ccbe3fee":"code","3c202b2d":"code","2b170835":"code","107bb57c":"code","d3154c26":"code","afa4c5cb":"code","443c775b":"code","61d418f8":"code","187054fe":"code","517c7e20":"code","b37d6cd1":"code","4ca12a5b":"code","0885a24a":"code","94eacc19":"markdown","f489d653":"markdown","e17201f3":"markdown","c1615a7b":"markdown","191beb7c":"markdown","6bd2bd07":"markdown","459ed070":"markdown","e315b0b9":"markdown","09922f65":"markdown","95965692":"markdown","7978c9a3":"markdown","4b176570":"markdown","55f9850f":"markdown","38d2e62f":"markdown","cf67f89c":"markdown","02c9aff4":"markdown","bf2dac1b":"markdown","f7703a22":"markdown","1a7e242e":"markdown","078c0868":"markdown","c1e7dbc6":"markdown"},"source":{"16af6c52":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport tensorflow as tf","a1517af2":"df = pd.read_csv('\/kaggle\/input\/bike-share-daily-data\/bike_sharing_daily.csv', index_col ='dteday')\ndf.drop('instant', axis=1, inplace=True)\ndf['temp'] = df['temp']*41\ndf['hum'] = df['hum']*100\ndf['windspeed'] = df['windspeed']*100\ndf\n","acae6b43":"df.describe().T","548774bc":"df.isnull().values.any()","61b00570":"df[['temp', 'hum', 'windspeed']].plot(figsize=(10, 5))\ndf[['casual', 'registered']].plot(figsize=(10, 5))","18a131f7":"plt.subplots(figsize=(8, 6))\nsns.heatmap(df.corr())","ccbe3fee":"n = len(df)\ntrain_df = df[:int(0.8*n)].copy()\ntest_df = df[int(0.8*n):].copy()","3c202b2d":"non_cat = ['temp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\ntrain_mean = train_df[non_cat].mean()\ntrain_std = train_df[non_cat].std()\n\ntrain_df.loc[:, non_cat] = (train_df.loc[:, non_cat] - train_mean) \/ train_std\ntest_df.loc[:, non_cat] = (test_df.loc[:,non_cat] - train_mean) \/ train_std\n","2b170835":"df_std = (df[non_cat] - train_mean) \/ train_std\ndf_melt = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_melt)\n_ = ax.set_xticklabels(df[non_cat].keys(), rotation=90)","107bb57c":"# separate input and output in each dataset\ninput_columns = ['season', 'yr', 'mnth', 'weekday', 'workingday','temp', 'hum', 'windspeed']\noutput_columns = ['casual', 'registered']\n\n\n\ntrain_in = train_df[input_columns].to_numpy()\ntest_in = test_df[input_columns].to_numpy()\n\n\ntrain_out = train_df[output_columns].to_numpy()\ntest_out = test_df[output_columns].to_numpy()\n\nprint(f'train_in shape: {train_in.shape}')\nprint(f'test_in shape: {test_in.shape}')\nprint(f'train_out shape: {train_out.shape}')\nprint(f'test_out shape: {test_out.shape}')","d3154c26":"# mean and std to be used for inverting the normalizatioin\n_mean = train_mean[output_columns].values\n_std = train_std[output_columns].values\n\n# Denormalize the data\ndef denorm(data, mean, std):\n    data_denorm = data * std + mean\n    return data_denorm\n","afa4c5cb":"def evaluate_model(model, train_in, train_out, test_in, test_out):\n    print('\\n***** Training performance: *****') \n    train_pred = model.predict(train_in)\n    train_pred_denorm = denorm(train_pred, _mean, _std) \n    train_out_denorm = denorm(train_out, _mean, _std) \n    train_mae = round(mean_absolute_error(train_pred_denorm, train_out_denorm), 2)\n    print('MAE = ', train_mae)\n\n\n    print('\\n***** Testing performance: *****') \n    test_pred = model.predict(test_in)\n    test_pred_denorm = denorm(test_pred, _mean, _std) \n    test_out_denorm = denorm(test_out, _mean, _std) \n    test_mae = round(mean_absolute_error(test_pred_denorm, test_out_denorm), 2)\n    print('MAE = ', test_mae)\n\n    for i, col in enumerate(output_columns):   \n        plt.figure(figsize=(10,5))\n        plt.plot(test_pred_denorm[:, i])\n        plt.plot(test_out_denorm[:, i])\n        plt.legend(['predict', 'true'])\n        plt.title(f'Predict vs. ground truth for {col}')\n    return train_mae, test_mae","443c775b":"# Creating a MultiOutputRegressor object with GradientBoostingRegressor estimtor \n# This assumes that the outputs are independent of each other, which might not be a correct assumption. \ngbr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))\n\nprint('\\n====== GradientBoostingRegressor =====')\ngbr.fit(train_in, train_out) \ngbr_train_mae, gbr_test_mae = evaluate_model(gbr, train_in, train_out, test_in, test_out)","61d418f8":"rfr = RandomForestRegressor(n_estimators=100, criterion='mae', random_state=0)\nprint('\\n====== RandomForestRegressor =====')\nrfr.fit(train_in, train_out) \nrfr_train_mae, rfr_test_mae = evaluate_model(rfr, train_in, train_out, test_in, test_out)","187054fe":"nn = tf.keras.Sequential([\n    tf.keras.layers.Dense(32, input_shape=(len(input_columns),)),\n    tf.keras.layers.Dense(len(output_columns))\n])\n\nprint(nn.summary())\n","517c7e20":"MAX_EPOCHS = 100\n\ndef compile_and_fit(model, train_in, train_out, test_in, test_out, patience=5):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n    model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n    history = model.fit(train_in, train_out, epochs=MAX_EPOCHS,\n                      validation_data=(test_in, test_out),\n                      callbacks=[early_stopping],\n                      batch_size = 32, verbose=0)\n    return history","b37d6cd1":"tf.keras.backend.clear_session()\nhistory = compile_and_fit(nn, train_in, train_out, test_in, test_out)\nplt.figure()\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.figure()\nplt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.legend(['mean_absolute_error', 'val_mean_absolute_error'])","4ca12a5b":"nn_train_mae, nn_test_mae = evaluate_model(nn, train_in, train_out, test_in, test_out)","0885a24a":"print('Model: Train MAE - Test MAE')\nprint(f'Gradient Boosting: {gbr_train_mae} - {gbr_test_mae }')\nprint(f'Random Forest: {rfr_train_mae} - {rfr_test_mae }')\nprint(f'DNN: {nn_train_mae} - {nn_test_mae }')\n","94eacc19":"## Split the data\nLet's make a simple manual split for now.","f489d653":"## Visualization of time series","e17201f3":"## Feature selection\n\nThere are two types of input data:\n- Seasonal data\n- Weather data\n\nand the output of the model is the bike user count which can be found in three columns of:\n\n- casual \n- registered\n- cnt (sum of the causual and registered)\n\nWe can drop 'cnt' as it's just the total count of users. I experimented with seasonal data only or weather data only as the input to the model. But, the performance was better when all input features are used.","c1615a7b":"## Loading the data","191beb7c":"## Cross Correlation Matrix","6bd2bd07":"![pascal-muller-ZG9pFLCKkXA-unsplash.jpg](attachment:pascal-muller-ZG9pFLCKkXA-unsplash.jpg)\n<span>Photo by <a href=\"https:\/\/unsplash.com\/@millerthachiller?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Pascal M\u00fcller<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/bike-sharing?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/span>\n\n","459ed070":"According to the description of the dataset, the enviromental features (4 columns of temp, atemp, hum and windspeed) are normalized. I think the temp and atemp (feels like temperature) are too correlated, so I discard atemp here. I need the data in its original scale, so I'll reverse the normalization according to the description below from the dataset owner:\n\n- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n- hum: Normalized humidity. The values are divided to 100 (max)\n- windspeed: Normalized wind speed. The values are divided to 67 (max)","e315b0b9":"## Any missing data? \n\n\nGood, there is no missing data. ","09922f65":"## Random Forest","95965692":"It looks like the best results based on test MAE is driven from Gradient Boosting method. Perhaps the other models specially DNN can be tuned to achieve at least the same level of performance. Please let me know if you have any suggestion to improve the models. Thanks for your feedback.","7978c9a3":"## Comparing the models","4b176570":"Let's plot the probability density of the data using violin data to make sure our normalization looks good.","55f9850f":"# Bike Sharing Count Prediction\nIn this notebook, I intend to make a model that predicts the count of casual and registered users given the environmental and seasonal settings. After a short EDA and normalization, I'll be using Gradient Boosting, Random Forest, and a simple DNN. ","38d2e62f":"## Denormalizing function","cf67f89c":"Let's define a function to evaluate the models.","02c9aff4":"## Normalize the data\nWe just need to normalize the non-categorical data. So, first we define them and then normalize those columns only.","bf2dac1b":"## DNN","f7703a22":"## Models","1a7e242e":"## Gradient Boosting","078c0868":"![Screen%20Shot%202020-09-26%20at%207.48.56%20AM.png](attachment:Screen%20Shot%202020-09-26%20at%207.48.56%20AM.png)","c1e7dbc6":"Before evaluating the models, we invert the normalization and compare the predictions and true values in their actual scale. We use the following function to invert the normalization."}}