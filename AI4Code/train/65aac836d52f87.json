{"cell_type":{"a53b5621":"code","10fe3818":"code","1701dee5":"code","ed0bf6e7":"code","ada60601":"code","5e71f6a8":"code","074c7788":"code","d38ad739":"code","537c7681":"code","28e965cd":"code","8d2adfd7":"code","2db8a258":"code","a922a90d":"code","fdc5446c":"code","720a7dc2":"code","4dc559d9":"code","b390358a":"code","bb03c1d7":"code","ab9567f8":"code","775d98b3":"code","da5654c5":"code","b870f37c":"code","e8196abe":"code","eaa0e41b":"code","dd031996":"code","5365a33e":"code","2a17e454":"code","c33c7dc8":"code","818ad719":"code","57756c07":"code","b573bfa7":"code","5fc5e6bf":"code","8c94566c":"code","54841bd0":"code","b73449c7":"code","0710c382":"code","7de75c1f":"code","27ba6741":"code","fd9ca9e5":"code","52fbafc5":"code","a03216a6":"code","6512c720":"code","89854493":"code","ba94f4a8":"code","9fcad25a":"code","69ab82b2":"code","8df572b3":"code","3fa85a56":"code","c77855e8":"code","c9f9be3d":"code","987a0fe5":"markdown","e6a7507f":"markdown","1ccaceba":"markdown","8d8009e1":"markdown","95fd7357":"markdown","cfde99b3":"markdown","e5c27268":"markdown","bc6c1054":"markdown","2cea8400":"markdown","61546ad2":"markdown","3ecc8dd2":"markdown","3bc9e222":"markdown","96473dac":"markdown","b7f79c48":"markdown","f5dc75c8":"markdown","94928616":"markdown","164a62ba":"markdown","978f5728":"markdown","1967a5af":"markdown","4012e184":"markdown","8d73fcf2":"markdown","2999e057":"markdown","bafb4090":"markdown","42258c99":"markdown","a4e1513e":"markdown","55525122":"markdown","6bdb1085":"markdown","efb381de":"markdown","4b0d776a":"markdown","d1487bfc":"markdown","40af7ee2":"markdown","7d37292a":"markdown","f8d2ed97":"markdown","6c64724f":"markdown","bffa4ba1":"markdown"},"source":{"a53b5621":"#Import Relevant Libraries\nimport re, cv2, os, json\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom PIL import Image, ImageOps\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport decimal\nimport shutil","10fe3818":"%%writefile skip_kernel_extension.py\ndef skip(line, cell=None):\n    '''Skips execution of the current line\/cell if line evaluates to True.'''\n    if eval(line):\n        return\n\n    get_ipython().ex(cell)\n\ndef load_ipython_extension(shell):\n    '''Registers the skip magic when the extension loads.'''\n    shell.register_magic_function(skip, 'line_cell')\n\ndef unload_ipython_extension(shell):\n    '''Unregisters the skip magic when the extension unloads.'''\n    del shell.magics_manager.magics['cell']['skip']","1701dee5":"%load_ext skip_kernel_extension","ed0bf6e7":"#Load data and paths\ndata = pd.read_csv('\/kaggle\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/data.csv')\nimages_folder = \"\/kaggle\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/images\"\nmasks_folder = \"\/kaggle\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/masks\"\ncoll_folder = \"\/kaggle\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/collage\"","ada60601":"#Obtain a count of images, masks, and observations.\nprint(f'Total number of images: {len(os.listdir(images_folder))}')\nprint(f'Total number of image masks: {len(os.listdir(masks_folder))}')\nprint(f'Length of dataset: {len(data)}')","5e71f6a8":"#Create figure and empty list for axes\naxes=[]\nfig=plt.figure(figsize=(15, 15))\n\n#Show first 4 images in dataset with corresponding shape.\nfor a in range(4):\n    #Obtain file name and create path.\n    file = os.listdir(\"\/kaggle\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/images\/\")[a]\n    image_path = os.path.join(images_folder, file) \n\n    #Read the file image and resize it for show.\n    img = cv2.imread(image_path)\n    resized_image = cv2.resize(img, (1300, 1500), interpolation = cv2.INTER_AREA)\n    \n    #Print the resized image and dislpay the shape.\n    axes.append(fig.add_subplot(1, 4, a+1) )\n    subplot_title=(f\"Original Size: {img.shape}\")\n    axes[-1].set_title(subplot_title)  \n    plt.imshow(resized_image)\n\n#Remove ticks from each image.\nfor ax in axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n#Plot the image.\nfig.tight_layout()    \nplt.show()","074c7788":"#Create figure and empty list for axes\naxes=[]\nfig=plt.figure(figsize=(15, 15))\n\n#Show first 4 images in dataset with corresponding shape.\nfor a in range(4):\n    #Obtain file name and create path.\n    file = os.listdir(\"\/kaggle\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/masks\/\")[a]\n    image_path = os.path.join(masks_folder, file) \n    \n    #Read the file image and resize it for show.\n    img = cv2.imread(image_path)\n    resized_image = cv2.resize(img, (1300, 1500), interpolation = cv2.INTER_AREA)\n    \n    #Print the resized image and dislpay the shape.\n    axes.append(fig.add_subplot(1, 4, a+1) )\n    subplot_title=(f\"Original Size: {img.shape}\")\n    axes[-1].set_title(subplot_title)  \n    plt.imshow(resized_image)\n\n#Remove ticks from each image.\nfor ax in axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n#Plot the image.\nfig.tight_layout()    \nplt.show()","d38ad739":"#Examine the head of the 'data' DataFrame\npd.set_option('display.max_colwidth', 70)\ndisplay(data.head())","537c7681":"#Create function to extract polygon locations from 'location' string.\ndef location_vals(obvs, x_or_y):\n    '''\n    Function uses regular expressions to parse the \"location\" string for each observation.\n    Inputs are \"obvs\" and \"x_or_y\".\n    \n    obvs: This simply serves as the string being passed into the function.\n    x_or_y: If \"x\" is entered, then the function extracts all \"x\" location values. If anything else, then it extracts \"y\" location values.\n    '''\n    if x_or_y == 'x':\n        x = re.findall(r\"\\'x\\': ([0-9.]*),\", obvs)\n        return x\n    else:\n        y = re.findall(r\"\\'y\\': ([0-9.]*)}\", obvs)\n        return y","28e965cd":"#Create new column with x and y location values.\ndata['x_loc_perc'] = data['location'].apply(lambda obvs: location_vals(obvs, 'x'))\ndata['y_loc_perc'] = data['location'].apply(lambda obvs: location_vals(obvs, 'y'))\ndisplay(data.head())","8d2adfd7":"#Creat function to return image size.\ndef image_size(img_name):\n    '''\n    The image name from each observation serves as the input.\n    The image is then read using cv2, and its shape is returned.\n    '''\n    image_path = os.path.join(images_folder, img_name)\n    img = cv2.imread(image_path)\n    return img.shape\n\n#Apply function to each row of DataFrame.\ndata['shape'] = data['photo_name'].apply(image_size)\ndisplay(data.head())","2db8a258":"#Save height and weight data as separate features.\ndata['height'] = data['shape'].apply(lambda x: x[0])\ndata['width'] = data['shape'].apply(lambda x: x[1])\n\n#Display stats for height and width of images.\ndata[['height', 'width']].describe()","a922a90d":"#Make sure that similar all files in each folder have the same location.\nfor i, j, k in zip(os.listdir(masks_folder), \\\n                   os.listdir(images_folder), \\\n                   os.listdir(coll_folder)):\n    if (i == j) & (j == k):\n        pass\n    else:\n        print(f'File {i} in one folder does not match name in others.')","fdc5446c":"#Create arrays \ny = np.zeros((1244, 224, 224), dtype='float32')\nX = np.zeros((1244, 224, 224, 3), dtype='float32')\n\nfor n, image, mask in tqdm(zip(range(1244), os.listdir(images_folder), os.listdir(masks_folder))):\n    dir_img = os.path.join(images_folder, image)\n    dir_mask = os.path.join(masks_folder, mask)\n    \n    #Open image, resize it.\n    img = cv2.imread(dir_img)\n    img = cv2.resize(img, (224, 224))\n    #img = ImageOps.exif_transpose(img)\n    X[n] = img \n    \n    #Open mask image, resize and normalize it.\n    msk = cv2.imread(dir_mask)\n    msk = cv2.resize(msk, (224, 224))\n    \n    #Normalize mask values.\n    msk = 1.0 * (msk[:, :, 0] > .1)\n    \n    #Save mask array to y array.\n    y[n] = msk   ","720a7dc2":"#Create function to plot images used with segmentation. \ndef plot_seg_imgs(array_or_collage, name):\n    '''\n    This function can be called to print 4 images used with the segmentation model.\n    array_or_collage - Accepts any values for arrays, from training arrays to predicted outputs.\n        Also accepts 'collage': If this is input, then 4 images from the collages folder will be printed.\n    name - What name would you like printed with the number of each image plotted.\n    '''\n    axes=[]\n    fig=plt.figure(figsize=(15, 15))\n\n    for a in range(4):\n        \n        #Print the resized image and dislpay the shape.\n        axes.append(fig.add_subplot(1, 4, a+1))\n        subplot_title=(f\"{name} Image #{a} Resized\")\n        axes[-1].set_title(subplot_title)  \n        \n        if str(array_or_collage) == 'collage':\n            img = cv2.imread(os.path.join(coll_folder, os.listdir(coll_folder)[a]))\n            img = cv2.resize(img, (224, 224))\n            plt.imshow(img)\n        else:\n            plt.imshow(array_or_collage[a].astype('uint8'))\n            \n    #Remove ticks from each image.\n    for ax in axes:\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    #Plot the image.\n    fig.tight_layout()    \n    plt.show()","4dc559d9":"#Print first 4 resized meter, resized mask, and collage images.\nplot_seg_imgs(X,'Meter Image')\nplot_seg_imgs(y, 'Mask')\nplot_seg_imgs('collage', 'Collage Image')","b390358a":"#Split data into train and test set.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","bb03c1d7":"%%capture\n\n#Download segmentation model from github.\n!pip install git+https:\/\/github.com\/qubvel\/segmentation_models\n    \nimport segmentation_models as sm","ab9567f8":"%%capture\n\n#Download presaved weights\n!wget https:\/\/github.com\/K-Merrick\/Water-Meters\/blob\/main\/weights-file.hdf5","775d98b3":"import segmentation_models as sm\n            \nsm.set_framework('tf.keras')\nsm.framework()\n\nBACKBONE = 'resnet34'\npreprocess_input = sm.get_preprocessing(BACKBONE)\n\n# preprocess input\nX_train = preprocess_input(X_train)\nX_test = preprocess_input(X_test)\n\n#Optimizer\nopt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\n# define model\nmodel = sm.Unet(BACKBONE, encoder_weights='imagenet')\nmodel.compile(\n    opt,\n    loss=sm.losses.bce_jaccard_loss,\n    metrics=[sm.metrics.iou_score],\n)\n\ntry: \n    #fname = os.path.sep.join([os.getcwd(), \"weights-file.hdf5\"])\n    fname = '..\/input\/weights\/weights-file.hdf5'\n    print('Weights file loaded.')\nexcept:\n    pass\n\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath='new_weights_output.hdf5',\n    save_weights_only=False,\n    monitor='val_iou_score',\n    mode='max',\n    save_best_only=True,\n    verbose=1)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=15)\n\nif os.path.isfile(fname) == True:\n    model.load_weights(fname)\n\n# fit model\nmodel.fit(\n   x=X_train,\n   y=y_train,\n   batch_size=16,\n   epochs=10, #Only 10, since we've pretrained the model.\n   validation_data=(X_test, y_test),\n   callbacks=[model_checkpoint_callback, early_stopping]\n)\n\nmodel.save('final_segmentation_model')\n\n# The model weights (that are considered the best) are loaded into the model.\n#model.load_weights(fname)","da5654c5":"# Examine predictions for training images.\nplot_seg_imgs(X_train, 'Train Images')\nplot_seg_imgs(model.predict(X_train), 'Train Predictions')\nplot_seg_imgs(y_train, 'Train Ground Truths')","b870f37c":"#Examine predictions for testing images.\nplot_seg_imgs(X_test, 'Test Images')\nplot_seg_imgs(model.predict(X_test), 'Test Predictions')\nplot_seg_imgs(y_test, 'Test Ground Truths')","e8196abe":"#Create function to crop images.\ndef crop(img, bg, mask) -> np.array:\n    '''\n    Function takes image, background, and mask, and crops the image.\n    The cropped image should correspond only with the positive portion of the mask.\n    '''\n    fg = cv2.bitwise_or(img, img, mask=mask) \n    fg_back_inv = cv2.bitwise_or(bg, bg, mask=cv2.bitwise_not(mask))\n    New_image = cv2.bitwise_or(fg, fg_back_inv)\n    return New_image","eaa0e41b":"#%%skip True\n\n#Crop training images using supplied masks.\nif os.path.exists('.\/cropped_for_ocr') == False:\n    os.mkdir('cropped_for_ocr')\nelse:\n    pass\n\nocr_path = '.\/cropped_for_ocr'\n\nfor n, image, mask in tqdm(zip(range(len(os.listdir(images_folder))), os.listdir(images_folder), os.listdir(masks_folder))):\n    dir_img = os.path.join(images_folder, image)\n    dir_mask = os.path.join(masks_folder, mask)\n    \n    #Read images and masks.\n    img = cv2.imread(dir_img).astype('uint8')\n    mask = cv2.imread(dir_mask).astype('uint8')\n    \n    #Get dimensions of image.\n    h, w, _ = img.shape\n    \n    #Ensure mask is binary, and create black background in shape of image.\n    mask = cv2.resize(cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY), (w, h)) # Resize image\n    bg = np.zeros_like(img, 'uint8') # Black background\n\n    #Crop image based on mask and make it RBG.\n    New_image = crop(img,bg,mask)\n    New_image = cv2.cvtColor(New_image, cv2.COLOR_BGR2RGB)\n\n    #Extract portion of image where meter reading is.\n    #Use min and max x and y coordinates to obtain final image.\n    where = np.array(np.where(New_image))\n    x1, y1, z1 = np.amin(where, axis=1)\n    x2, y2, z2 = np.amax(where, axis=1)\n    sub_image = New_image.astype('uint8')[x1:x2, y1:y2]\n\n    #Write image to file\n    cv2.imwrite(os.path.join(ocr_path , image), sub_image)","dd031996":"%%capture\n\nfrom skimage import io\nfrom skimage.transform import rotate\nfrom skimage.color import rgb2gray\ntry:\n    from deskew import determine_skew\nexcept:\n    !pip install deskew\n    from deskew import determine_skew\nfrom typing import Tuple, Union\nimport math","5365a33e":"def rotate(image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]) -> np.ndarray:\n    '''\n    This function attempts to rotate meter reading images to make them horizontal.\n    Its arguments are as follows:\n    \n    image - The image to be deskewed (in numpy array format).\n    angle - The current angle of the image, found with the determine_skew function of the deskew library.\n    background - The pixel values of the boarder, either int (default 0) or a tuple.\n    \n    The function returns a numpy array.\n    '''\n    old_width, old_height = image.shape[:2]\n    angle_radian = math.radians(angle)\n    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)\n    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)\n    \n    image_center = tuple(np.array(image.shape[1::-1]) \/ 2)\n    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n    rot_mat[1, 2] += (width - old_width) \/ 2\n    rot_mat[0, 2] += (height - old_height) \/ 2\n    \n    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)","2a17e454":"def resize_aspect_fit(path, final_size: int, write_to, save=True):\n    '''\n    Function resizes the image to specified size.\n    \n    path - The path to the directory with images.\n    final_size - The size you want the final images to be. Should be in int (will be used for w and h).\n    write_to - The file you wish to write the images to. \n    save - Whether to save the files (True) or return them.\n    '''   \n    for item in tqdm(os.listdir(path)):\n        im = Image.open(path+item)\n        f, e = os.path.splitext(path+item)\n        size = im.size\n        ratio = float(final_size) \/ max(size)\n        new_image_size = tuple([int(x*ratio) for x in size])\n        im = im.resize(new_image_size, Image.ANTIALIAS)\n        new_im = Image.new(\"RGB\", (final_size, final_size))\n        new_im.paste(im, ((final_size-new_image_size[0])\/\/2, (final_size-new_image_size[1])\/\/2))\n        if save==True:\n            cv2.imwrite(os.path.join(resize_for_rcnn, item), np.array(new_im))\n        else:\n            return np.array(new_im)","c33c7dc8":"#%%skip True\n\n#Create new directory for images if they haven't been saved yet.\nif os.path.exists('.\/croped_components') == False:\n    os.mkdir('croped_components')\nelse:\n    pass\n\ncomponents_path = '.\/croped_components'\n\n#Create new image depicting rotated meter reading.\nfor n, image in tqdm(zip(range(1244), os.listdir('.\/cropped_for_ocr'))):\n    \n    #Rotate all images and write to file.\n    dir_img = os.path.join('.\/cropped_for_ocr', image)\n    photo = cv2.imread(os.path.join(dir_img))\n    grayscale = cv2.cvtColor(photo, cv2.COLOR_BGR2GRAY)\n    angle = determine_skew(grayscale)\n    try:\n        rotated = rotate(photo, angle, (0, 0, 0))\n    except:\n        rotated = photo\n    cv2.imwrite(os.path.join(components_path , image), rotated)","818ad719":"#%%skip True\n\n#Reshape all images to 224x224x3 size, while retaining aspect. \n#IMPORTANT FOR PREPROCESSING IMAGES\n\nfrom PIL import Image\nimport os, sys\n\nif os.path.exists('.\/resized_for_rcnn') == False:\n    os.mkdir('resized_for_rcnn')\nelse:\n    pass\n\n#Specify argument values for resize function.\nresize_for_rcnn = '.\/resized_for_rcnn'\npath = '.\/croped_components\/'\nfinal_size = 224\n\nresize_aspect_fit(path, final_size, resize_for_rcnn)","57756c07":"%%capture\n\n#Note, it takes a while to run this cell.\n!apt-get update && apt-get install -y python3-opencv\n!pip install opencv-python\n!pip install opendatasets\n!pip install -U torch==1.5 torchvision==0.6 -f https:\/\/download.pytorch.org\/whl\/cu101\/torch_stable.html \n!pip install cython pyyaml==5.1\n!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n!gcc --version\n!pip install detectron2==0.1.3 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu101\/torch1.5\/index.html\n\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.data.catalog import DatasetCatalog\n\n#Import Relevant Libraries\nimport random\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom PIL import Image, ImageOps\nfrom tqdm import tqdm\nimport decimal\nimport shutil\nimport opendatasets as od","b573bfa7":"#Create new directory and add model images to it.\nos.mkdir('content')\nos.chdir('content')","5fc5e6bf":"%%capture\n\n#Download processed data from Roboflow.\n!curl -L \"{https:\/\/app.roboflow.com\/ds\/jGCiAQzrvI?key=ZmR7CmNT98}\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip","8c94566c":"#Register directories for training, testing, and validation datasets.\nfrom detectron2.data.datasets import register_coco_instances\nregister_coco_instances(\"my_dataset_train\", {}, \"train\/_annotations.coco.json\", \"train\")\nregister_coco_instances(\"my_dataset_val\", {}, \"valid\/_annotations.coco.json\", \"valid\")\nregister_coco_instances(\"my_dataset_test\", {}, \"test\/_annotations.coco.json\", \"test\")","54841bd0":"#Visualize training data\nmy_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\ndataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n\naxes=[]\nfig=plt.figure(figsize=(10, 10))\n\n#View three sample training images.\nfor i, d in enumerate(random.sample(dataset_dicts, 3)):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n    vis = visualizer.draw_dataset_dict(d)\n        \n    #Print the resized image and dislpay the shape.\n    axes.append(fig.add_subplot(1, 3, i+1))\n    plt.imshow(vis.get_image()[:, :, ::-1])\n\n#Remove ticks from each image.\nfor ax in axes:\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n#Plot the image.\nfig.tight_layout()    \nplt.show()","b73449c7":"from detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator\n\n#Create trainer. \nclass CocoTrainer(DefaultTrainer):\n\n  @classmethod\n  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n\n    if output_folder is None:\n        os.makedirs(\"coco_eval\", exist_ok=True)\n        output_folder = \"coco_eval\"\n\n    return COCOEvaluator(dataset_name, cfg, False, output_folder)","0710c382":"#Configure model parameters.\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"my_dataset_train\",)\ncfg.DATASETS.TEST = (\"my_dataset_val\",)\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")  #Initialize training from model zoo.\ncfg.SOLVER.IMS_PER_BATCH = 4\ncfg.SOLVER.BASE_LR = 0.001\ncfg.SOLVER.WARMUP_ITERS = 1000\ncfg.SOLVER.MAX_ITER = 1500 #Adjusted up as mAP was still increasing after 1500.\ncfg.SOLVER.STEPS = (1000, 1500)\ncfg.SOLVER.GAMMA = 0.05\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 11 #classes + 1 | We are trying to detect 10 different digits (0-9). \ncfg.TEST.EVAL_PERIOD = 500","7de75c1f":"#Need to clear GPU memory, as Kaggle's limits sometimes cause memory error.\nfrom numba import cuda\ncuda.select_device(0)\ncuda.close()","27ba6741":"#Train model.\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = CocoTrainer(cfg)\ntrainer.resume_or_load(resume=True)\ntrainer.train()","fd9ca9e5":"#%%skip True\n\n#Get prediction metrics for test dataset.\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\n\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.85\n\npredictor = DefaultPredictor(cfg)\nevaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=\".\/output\/\")\nval_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\ninference_on_dataset(trainer.model, val_loader, evaluator)","52fbafc5":"#Obtain model and parameters for obtaining predictions from test images.\n#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\") <-- Changed path to saved file. \ncfg.MODEL.WEIGHTS = '..\/..\/input\/water-meter-ocr-images\/output\/model_final.pth'\ncfg.DATASETS.TEST = (\"my_dataset_test\", )\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\npredictor = DefaultPredictor(cfg)\ntest_metadata = MetadataCatalog.get(\"my_dataset_test\")\n#metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0]) <-- Can't be used when changed saved dir.\nmetadata = MetadataCatalog.get(\"my_dataset_train\")\nclass_catalog = metadata.thing_classes\n#Import necessary libraries.\nfrom detectron2.utils.visualizer import ColorMode\nimport glob\n\n#List for storing meter readings.\nlist_of_img_reading = []\n\n#Obtain test predictions.\n#for i, imageName in enumerate(glob.glob('test\/*jpg')): <-- Once reading from saved file, have to change.\nfor i, imageName in enumerate(os.listdir('..\/..\/input\/water-meter-ocr-images\/content\/test')):\n    #Read image and get output information.\n    if 'coco' in imageName:\n        pass\n    else:\n        im = cv2.imread(os.path.join('..\/..\/input\/water-meter-ocr-images\/content\/test', imageName))\n        outputs = predictor(im)\n    \n        #Find predicted boxes and labels.\n        instances = outputs['instances']\n        coordinates = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n        pred_classes = outputs['instances'].pred_classes.cpu().tolist()\n    \n        #Obtain list of all predictions and the leftmost x-coordinate for bounding box.\n        pred_list = []\n        for pred, coord in zip(pred_classes, coordinates):\n            pred_list.append((pred, coord[0]))\n    \n        #Sort the list based on x-coordinate in order to get proper order or meter reading.\n        pred_list = sorted(pred_list, key=lambda x: x[1])\n    \n        #Get final order of identified classes, and map them to class value.\n        final_predictions = [x[0] for x in pred_list]\n        pred_class_names = list(map(lambda x: class_catalog[x], final_predictions))\n    \n        #Add decimal point to list of digits depending on number of bounding boxes.\n        if len(pred_class_names) == 5:\n            pass\n        else:\n            pred_class_names.insert(5, '.')\n    \n        #Combine digits and convert them into a float.\n        combine_for_float = \"\".join(pred_class_names)\n        meter_reading = float(combine_for_float)\n    \n        #Visualize prediction.  \n        metadata_model = MetadataCatalog.get(\"mydataset\")\n        v = Visualizer(im[:, :, ::-1], metadata=test_metadata, scale=0.8)\n    \n        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n        parsed_name = re.findall(r\"([a-zA-Z0-9]*_?[a-zA-Z0-9]*_?[a-zA-Z0-9]*_?[a-zA-Z0-9]*_?[a-zA-Z0-9]*)\", imageName)\n        list_of_img_reading.append((parsed_name[0], meter_reading))\n    \n        #Plot only subset of images.\n        if i % 5 == 0:\n            plt.figure()\n            plt.imshow(out.get_image()[:, :, ::-1])\n            plt.xticks([])\n            plt.yticks([])\n        else:\n            pass","a03216a6":"#Create DataFrame from predictions, and format photo name to the same style as in provided DataFrame.\ndf_predicted = pd.DataFrame(list_of_img_reading, columns=['photo_name', 'reading'])\ndf_predicted.photo_name = df_predicted['photo_name'].apply(lambda x: x + '.jpg')\n\n#Upload provided data to DataFrame.\ndf_provided = data.copy()\n\n#Create list to get info from both DataFrames where photo names overlap.\nprovided_and_predicted = []\npred_list = list(df_predicted['photo_name'].values)\n\nfor image_name, predicted_value in df_predicted.values:\n    provided_row = df_provided[df_provided['photo_name'] == image_name]\n    provided_value = float(provided_row['value'].values)\n    provided_and_predicted.append((image_name, provided_value, predicted_value))\n    \ncompiled_df = pd.DataFrame(provided_and_predicted, columns=['image_name', 'ground_truth', 'predicted_value'])\ndisplay(compiled_df.head())","6512c720":"#Round each value to 3 decimal places and calculate difference between predicted value and ground truth.\ncompiled_df['ground_truth'] = compiled_df['ground_truth'].apply(lambda x: round(x, 3))\ncompiled_df['predicted_value'] = compiled_df['predicted_value'].apply(lambda x: round(x, 3))\ncompiled_df['difference'] = abs((compiled_df['ground_truth']) - (compiled_df['predicted_value']))\n\n#Calculate percentage difference between ground truth and predicted value.\ncompiled_df['percent_diff'] = (compiled_df['difference'] \/ ((compiled_df['ground_truth'] + compiled_df['predicted_value']) \/ 2) * 100)\ndisplay(compiled_df.head())","89854493":"#Obtain list of percentages over 5% error and with no error.\nover_5_error = [x for x in compiled_df['percent_diff'] if x >= .05]\nno_error = [x for x in compiled_df['percent_diff'] if x == 0]\n\nprint(f'Total number of accurate predictions: {len(no_error)}. Percentage of total: {round(len(no_error) \/ len(compiled_df), 3)}')\nprint(f'Total number of predictions with less than 5% error: {len(compiled_df) - len(over_5_error)}. Percentage of Total: {round((len(compiled_df) - len(over_5_error)) \/ len(compiled_df), 3)}')\nprint(f'Total number of predictions with over 5% error: {len(over_5_error)}. Percentage of total: {round(len(over_5_error) \/ len(compiled_df), 3)}')","ba94f4a8":"\n%%capture\n\n!apt-get update && apt-get install -y python3-opencv\n!pip install opencv-python\n!pip install opendatasets\n!pip install -U torch==1.5 torchvision==0.6 -f https:\/\/download.pytorch.org\/whl\/cu101\/torch_stable.html \n!pip install cython pyyaml==5.1\n!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n!gcc --version\n!pip install detectron2==0.1.3 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu101\/torch1.5\/index.html\n#Download segmentation model from github.\n!pip install git+https:\/\/github.com\/qubvel\/segmentation_models\n!pip install pandas\n!pip install numpy\n\n#General libraries\nimport re, cv2, os, json, sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageOps\nimport random\nimport decimal\nimport shutil\nimport opendatasets as od\nimport keras\nimport math\nimport scipy\n\n#Image deskew libraries.\nfrom skimage import io\nfrom skimage.transform import rotate\nfrom skimage.color import rgb2gray\ntry:\n    from deskew import determine_skew\nexcept:\n    !pip install deskew\n    from deskew import determine_skew\nfrom typing import Tuple, Union\n\nimport segmentation_models as sm\nimport torch, torchvision\nimport detectron2\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n# import some common detectron2 utilities\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\nfrom detectron2.data.catalog import DatasetCatalog\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9fcad25a":"os.listdir('..\/input\/water-meter-ocr-images\/output\/')","69ab82b2":"#Chose random image.\nrandom_img = '..\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/images\/id_1022_value_95_735.jpg'\n\n#Specify files for model weights.\nsegmentation_model_file = '..\/input\/water-meter-ocr-images\/final_segmentation_model'\nfaster_rcnn_path = '..\/input\/water-meter-ocr-images\/output\/model_final.pth' #<-- for cfg.MODEL.WEIGHTS","8df572b3":"#Function to resize image.\ndef prod_resize_input(img_link):\n    '''\n    Function takes an image and resizes it.\n    '''\n    img = cv2.imread(img_link)\n    img = cv2.resize(img, (224, 224))\n    return img.astype('uint8')\n\n#Create function to crop images.\ndef crop_for_seg(img, bg, mask):\n    '''\n    Function extracts an image where it overlaps with its binary mask.\n    img - Image to be cropped.\n    bg - The background on which to cast the image.\n    mask - The binary mask generated from the segmentation model.\n    '''\n    #mask = mask.astype('uint8')\n    fg = cv2.bitwise_or(img, img, mask=mask) \n    fg_back_inv = cv2.bitwise_or(bg, bg, mask=cv2.bitwise_not(mask))\n    New_image = cv2.bitwise_or(fg, fg_back_inv)\n    return New_image\n\ndef extract_meter(image_to_be_cropped):\n    '''\n    Function further extracts image such that the meter reading takes up the majority of the image.\n    The function finds the edges of the ROI and extracts the portion of the image that contains the entire ROI.\n    '''\n    where = np.array(np.where(image_to_be_cropped))\n    x1, y1, z1 = np.amin(where, axis=1)\n    x2, y2, z2 = np.amax(where, axis=1)\n    sub_image = image_to_be_cropped.astype('uint8')[x1:x2, y1:y2]\n    return sub_image\n\ndef rotate(image: np.ndarray, angle: float, background: Union[int, Tuple[int, int, int]]) -> np.ndarray:\n    '''\n    This function attempts to rotate meter reading images to make them horizontal.\n    Its arguments are as follows:\n    \n    image - The image to be deskewed (in numpy array format).\n    angle - The current angle of the image, found with the determine_skew function of the deskew library.\n    background - The pixel values of the boarder, either int (default 0) or a tuple.\n    \n    The function returns a numpy array.\n    '''\n    old_width, old_height = image.shape[:2]\n    angle_radian = math.radians(angle)\n    width = abs(np.sin(angle_radian) * old_height) + abs(np.cos(angle_radian) * old_width)\n    height = abs(np.sin(angle_radian) * old_width) + abs(np.cos(angle_radian) * old_height)\n    \n    image_center = tuple(np.array(image.shape[1::-1]) \/ 2)\n    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n    rot_mat[1, 2] += (width - old_width) \/ 2\n    rot_mat[0, 2] += (height - old_height) \/ 2\n    return cv2.warpAffine(image, rot_mat, (int(round(height)), int(round(width))), borderValue=background)\n\ndef resize_aspect_fit(img, final_size: int):\n    '''\n    Function resizes the image to specified size.\n    \n    path - The path to the directory with images.\n    final_size - The size you want the final images to be. Should be in int (will be used for w and h).\n    write_to - The file you wish to write the images to. \n    save - Whether to save the files (True) or return them.\n    '''   \n    im_pil = Image.fromarray(img)\n    size = im_pil.size\n    ratio = float(final_size) \/ max(size)\n    new_image_size = tuple([int(x*ratio) for x in size])\n    im_pil = im_pil.resize(new_image_size, Image.ANTIALIAS)\n    new_im = Image.new(\"RGB\", (final_size, final_size))\n    new_im.paste(im_pil, ((final_size-new_image_size[0])\/\/2, (final_size-new_image_size[1])\/\/2))\n    new_im = np.asarray(new_im)\n    return np.array(new_im)\n\ndef prep_for_ocr(img):\n    img = resize_aspect_fit(img, 224)\n    output_name = 'test_img_for_ocr.jpg'\n    cv2.imwrite(output_name, img)\n    return output_name\n\n#Segment input image.\ndef segment_input_img(img):\n        \n    #Resize image.\n    img_small = prod_resize_input(img)\n    \n    #Open image and get dimensions.\n    input_img = cv2.imread(img, cv2.IMREAD_UNCHANGED)\n    input_w = int(input_img.shape[1])\n    input_h = int(input_img.shape[0])\n    dim = (input_w, input_h)\n    \n    #Load model, preprocess input, and obtain prediction.\n    BACKBONE = 'resnet34'\n    preprocess_input = sm.get_preprocessing(BACKBONE)\n    img_small = preprocess_input(img_small)\n    img_small = img_small.reshape(-1, 224, 224, 3).astype('uint8')\n    model = tf.keras.models.load_model(segmentation_model_file, custom_objects={'binary_crossentropy_plus_jaccard_loss': sm.losses.bce_jaccard_loss, 'iou_score' : sm.metrics.iou_score})\n    mask = model.predict(img_small)\n    \n    #Change type to uint8 and fill in holes.\n    mask = mask.astype('uint8')\n    mask = scipy.ndimage.morphology.binary_fill_holes(mask[0, :, :, 0]).astype('uint8')\n    \n    #Resize mask to equal input image size.\n    mask = cv2.resize(mask, dsize=dim, interpolation=cv2.INTER_AREA)\n   \n    # Taking a matrix of size 5 as the kernel\n    kernel = np.ones((10,10), np.uint8)\n    \n    mask = cv2.dilate(mask, kernel, iterations=3)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN,cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15)))\n    \n    #Create background array.\n    bg = np.zeros_like(input_img, 'uint8')\n    \n    #Get new cropped image and make RGB.\n    New_image = crop_for_seg(input_img, bg, mask)\n    New_image = cv2.cvtColor(New_image, cv2.COLOR_BGR2RGB)\n\n    #Extract meter portion.\n    extracted = extract_meter(New_image)\n    \n    grayscale = cv2.cvtColor(extracted, cv2.COLOR_BGR2GRAY)\n    angle = determine_skew(grayscale)\n    \n    if angle == None:\n        angle = 1\n    \n    rotated = rotate(extracted, angle, (0, 0, 0))\n    \n    return rotated","3fa85a56":"def get_reading(image_path):\n    '''\n    This is the main function for the pipeline. \n    It takes an input image path as its only argument.\n    It then carries out all the necessary steps to extract a meter reading.\n    The output is the reading.\n    \n    NOTE: Due to having to load and generate predictions from two models, \n    this script may take a while to run.\n    '''\n    \n    #Segment image.\n    segmented = segment_input_img(image_path)\n    \n    #Prep image and save path.\n    prepped_path = prep_for_ocr(segmented)\n        \n    #Class labels.\n    labels = ['number', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n    \n    #List for storing meter readings.\n    list_of_img_reading = []\n    \n    #Configure model parameters.\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n    cfg.MODEL.WEIGHTS = '..\/input\/water-meter-ocr-images\/output\/model_final.pth'\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6\n    cfg.MODEL.DEVICE='cpu'\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 11\n    predictor = DefaultPredictor(cfg)\n\n    #Read prepped image and obtain prediction.\n    im = cv2.imread(prepped_path)\n    outputs = predictor(im)\n    \n    #Find predicted boxes and labels.\n    instances = outputs['instances']\n    coordinates = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n    pred_classes = outputs['instances'].pred_classes.cpu().tolist()\n    \n    #Obtain list of all predictions and the leftmost x-coordinate for bounding box.\n    pred_list = []\n    for pred, coord in zip(pred_classes, coordinates):\n        pred_list.append((pred, coord[0]))\n    \n    #Sort the list based on x-coordinate in order to get proper order or meter reading.\n    pred_list = sorted(pred_list, key=lambda x: x[1])\n    \n    #Get final order of identified classes, and map them to class value.\n    final_predictions = [x[0] for x in pred_list]\n    pred_class_names = list(map(lambda x: labels[x], final_predictions))\n    \n    #Add decimal point to list of digits depending on number of bounding boxes.\n    if len(pred_class_names) == 5:\n        pass\n    else:\n        pred_class_names.insert(5, '.')\n        \n    #Combine digits and convert them into a float.\n    combine_for_float = \"\".join(pred_class_names)\n    meter_reading = float(combine_for_float)\n    \n    return meter_reading","c77855e8":"print(f'Meter reading: {get_reading(random_img)}')","c9f9be3d":"plt.imshow(segment_input_img(random_img))","987a0fe5":"## Resize for Faster RCNN Model","e6a7507f":"### Install Libraries and Obtain Data","1ccaceba":"Finally, before moving to the next step, it's important to ensure that the images in all folders have the same relative location. This is because we will be iterating through the images in at least two folders when building a mask segmentation model. If an image segmentation model is trained with mismatched imgaes and masks, then we the algorithm will perform very poorly.","8d8009e1":"### Build and Train Model","95fd7357":"# Build Segmentation Model","cfde99b3":"# Build Faster RCNN Model ","e5c27268":"From the above output, we can see that there are 1244 images in this dataset, which corresponds to the number of masks and observations.","bc6c1054":"# Water Meter OCR Apiary Project","2cea8400":"From the above description of the range in image dimensions, we can see that most images have a height of 1300 pixels and width of 1000 pixels. Given the standard deviation and how closely bound the quantile values are, it appears that we do not have normally distributed dimensions, but rather many photos of the same dimension and a few outliers. This should not be a problem for us, as we intend on resizing images when creating out masking model. ","61546ad2":"As we can see from the above, our pipeline works. We go from inputing an initial image, through various preprocessing stages, through two deep learning models, and finally a prediction. Using the above code we developed an application for providing meter readings from photos. Using flast, we created a simple web framework that allows a user to upload a photo; the photo goes through all the above steps, and then the user is taken to a different page displaying the final reading. We dockerized this application, ensuring that all necessary libraries were installed. The application works well locally - see video demonstration in github repo. Unfortunately, the application crashed when pushed to Heroku after submitting an uploaded image. The error indicates that the program requires more than the 512MB alloted memory, which is understandable given the size of the models being used. This error could be circumvented by purchasing better Heroku subscriptions or deploying the application on a better server. As this project was meant as a demonstration, we will not attempt to deploy the appliction on another platform. If you have any questions, please feel free to leave a comment.","3ecc8dd2":"In this section of the project we will attempt to build an image segmentation model. Based on our research, it appears that UNet models allow for training using images and their associated masks. Since we have all the information necessary to train such a model, we will attempt to use UNet for the segmentation portion of the project.","3bc9e222":"# Create OCR Model","96473dac":"From the above images, we can see that the resized masks are appropriately positioned with regard to the location of the meter reading region for each meter pictured. We can see, however, that unlike the original masks, these are quite pixelated. This is due to our downsizing the mask images to a small fraction of their original size. This should not be problematic for our later UNet model; however, if it does prove problematic, we will adjust the mask sizes accordingly.","b7f79c48":"As we can see, each observation in the DataFrame now contains the associated image's shape, along with the locations of each point of the image's ROI. To get a better feeling for our data, let us examine the statistics for the dimensions of images contained in our dataset.","f5dc75c8":"# Segmentation Mask","94928616":"Having created new coordinate features for each image, we would also like to conduct a quick examination of the range of image sizes for those images in our dataset. To facilitate this, let us add the dimensions of each image to the 'data' DataFrame. Note, the shape of images are returned in height, width, channels format.","164a62ba":"We will be building and training a Detectron2 model according to a guide created by Jacob Solawetz from Roboflow.\nThe article can be found here: https:\/\/towardsdatascience.com\/how-to-train-detectron2-on-custom-object-detection-data-be9d1c233e4","978f5728":"The aim of this project is to develope an OCR model that is capable of accurately reading water meters. To achieve our goal, we will break the project down into three steps. First, we will develop a model that accurately detects the region of interest for each photo - that is, the region where the meter reading is heald. Next, we will develop a model to segment each digit in a meter. Finally, we will develop a model which can return the digits from each meter.","1967a5af":"Note: It appears that our DataFrame contains the image name, ground truth meter reading, and polygonal mask locations for each image. While the 'location' values are stored as strings, they appear to be in JSON format, where each x and y coordinate corresponds with a point on a polygonal mask. These points seem to be represented as percentages, as they appear to be scaled between 0 and 1; this may be useful in case of image rescaling, as the points could always accurately represent coordinates of a ROI when plotted according to the relevant image's scale. We will leave the 'location' column untouched and deal with any sort of rescaling\/reformatting we may need when building our segmentation mode. That being said, in case we are required to submit the polygonal coordinates in a different format than what was provided, let us quickly save all the x and y coordinates for each image as different features. ","4012e184":"In this section we will be building out Faster RCNN Model. To do this, we manually labeled the digits in over 200 meters. In order to obtain the data in appropriate COCO format, we uploaded the images and labels to Roboflow and will download the images and reformatted data in this section.\n\nNote: Since we've trained our model and saved it already, we will skip the model training steps and simply evaluate the model.","8d73fcf2":"It appears that the masks accurately convey the region of interest for our meters - that is, they accurately convey the location of the portion of the meter that contains the current reading. This is great news, as we can use these masks to train a UNET model to produce masks of other water meters. Let us now examine the type of informaiton contained in the data provided.","2999e057":"We can see from the above lack of output that all the files are similarly ordered in the three folders examined. That means, for example, the photo in spot 10 in the masks folder is also in spot 10 in the images and collage folders.","bafb4090":"As we can see from the above, our masks generally line up with the ground truth masks. While there are some spots inside of the binary masks that don't line up perfectly, our overall iou score for the test dataset is quite high! In the following section, in order to prepare images to be processed using OCR, we will extract the meter reading portion of the images using the bonudaries of the binary masks.","42258c99":"## Create Prediction Program","a4e1513e":"The above cell creates a multidimensional numpy array that contains all relevant values for the resized meter and mask images. When attempting to resize and scale the images, we noticed that some of the images opened with Pillow were rotated by 90 degrees. After some research, we found that this was because the images contained some EXIF data. In order to ensure that all the images are similarly oriented to their masks, we removed the exif data from each image. We also reduced the size of the masks to be used with UNet, ensuring that each image size is a multiple of its relevant mask size.\n\nTo visualize whether our resized images and masks are properly oriented, we've created the following function that will allow us to print a certain number of images from each relevant folder.","55525122":"### Crop Images","6bdb1085":"To start, let us see how many observations we will be working with and examine a few representative images of water meters.","efb381de":"Note: As we already cropped, deskewed, resized, and saved our images, we will skip the following cells that save data to our output folder.","4b0d776a":"### Faster RCNN Model Summary","d1487bfc":"## Exploratory Data Analyis","40af7ee2":"### Deskew Images","7d37292a":"From the above images, we can see that there are different types of water meters contained within the dataset. We can also see that the angles of the photos differ between images. This should not be a problem for our purposes, so long as the annotation was carried out correctly. To see whether this is the case, let us show the corresponding mask images.\n\nNote: Later on in the project, we built a Faster RCNN model in order to predict the digits displayed on a meter. In doing so, we manually labeled the digits in over 200 meters, which allowed us to visualize the different types of meters present. We found that most meters have digit displays, though there are some with 7 digits and few with 5. For the 7 and 8 digit meters, the final three digits represent milliliters; however, milliliters are not present on the 5 digit displays. This is important to keep in mind, as we will need to infer where the decimal is to be placed once we obtain predicted readings from our Faster RCNN model. ","f8d2ed97":"As we can see above, out images are segmented and annotated with the proper label for each digit. We will not train a custom Detectron2 Faster RCNN model using our dataset, which is split into training(70%), validation(20%), and testing(10%) datasets.","6c64724f":"From the above output, we can see that our Faster RCNN model is quite good at predicting the reading on a meter, consider how few images we trained the model on. In total, there were around 200 images manually segmented, which is quite low for computer vision standards. Even with this low training amount, our model was able to provide a prediction with 95% of the ground truth on around 84% of the test images. \n\nUnfortunately, around 16% of the predictions were above the 5% error rate; however, we believe that this error could be reduced by taking a few measures. First and foremost, it would be beneficial to train our model on more images; with more images, the model may be able to better learn to read digits on dimly lighted, smudged, or otherwise partially obscured meters. Second, for anyone putting such a system into production, it would be beneficial to have specific guidelines concerning image standards; for example, those photographing meters should always try to take photograph the meter from the front and ensure that the meter is adequately lit. Additionally, one should always ensure that the portion of the meter housing the reading is not damaged or smudged in such a way that makes a digit hard to read. Finally, one might be able to improve upon this model by introducing reinforcement learning. We believe that before implementing a computer vision meter reading system, companies should have an adequate history of customer usages, such that they are able to estimate average monthly water consumption. With this in mind, when a meter reading is predicted for a customer via photograph, a program could automatically check whether the reading is within a certain range from the estimages usage; if the prediction is off by a certain percentage, the customer could be prompted to manually enter the reading, and the reinforcement learning algorithm could take that into account. Another option would be to have employees manually view said images and input the displayed value.\n\nSince I endeavored to demonstrate that automatic meter reading is possible, rather than building a fully functional system, I am happy with the results. As such, I will not manually segment more images and attempt to increase the mAP of our Faster RCNN model. As a personal learning experience, I will, however, attempt to productize this model. To that end, I will, in the following section, create a program that takes an imput image, appropriately formats it, obtains a segmentation mask, crops the image, and outputs a meter reading prediction. I will then dockerize the program and host it on Heroku.","bffa4ba1":"For our OCR model, our goal is to input cropped images using the masks produced by our segmentation model and their associated image. We will then manually label each digit in around 200 meters and use the labeled images to train a Faster RCNN model. Our goal is to build a Faster RCNN model that will accurately idenfity digits in meters and predict their values. Using the ouput information from such a model on test images, we will parse the data and reformat the predictions such that the predictions appear in order from left to right. We will then combine the digits appropriately to obtain a final meter reading."}}