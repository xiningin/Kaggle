{"cell_type":{"d61fe2af":"code","4aea6da1":"code","dcc2e3c4":"code","672fe1b6":"code","03301df8":"code","69c3873b":"code","17e536df":"code","eddef968":"code","a935b9f2":"code","7792bc94":"code","538fd204":"code","4144b040":"code","bb87b647":"code","876c08d5":"code","1e7375f9":"code","6c235aae":"code","d1accb95":"code","f9ac31ea":"code","4a4bbc17":"code","5475dd50":"code","d2c707cd":"code","c82a4c43":"code","7e2753f5":"code","a8c29a30":"code","1f6d761c":"code","14a7826d":"code","f8084b9e":"code","5716bb25":"code","82af43cf":"code","5d6d43b4":"code","9637471f":"code","f49807f3":"markdown","c96ab6b7":"markdown","9470cb9f":"markdown","05f11c44":"markdown","1d967279":"markdown","31f7c80c":"markdown","941e7b43":"markdown","34e1acde":"markdown","34eab8a9":"markdown","ef7795c6":"markdown","9342fce9":"markdown","0ce08dd2":"markdown","2d1e7338":"markdown","5b00a36c":"markdown","2fbd8bcd":"markdown","f58b7fbe":"markdown","ebf2fb55":"markdown","4c937468":"markdown","6e4b0b55":"markdown"},"source":{"d61fe2af":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4aea6da1":"df = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\ndisplay(df.head())\ndisplay(df.isna().sum())","dcc2e3c4":"cat_col = [cname for cname in df.columns if df[cname].dtype in ['object']]\ncat_col","672fe1b6":"df1 = df.copy() # make a copy of the dataset \n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\n\ndf1[cat_col] = ordinal_encoder.fit_transform(df1[cat_col])\n\ndf1.head()","03301df8":"df['sex_code'] = df1.sex\ndf['smoker_code'] = df1.smoker\ndf['region_code'] = df1.region\ndf.head()","69c3873b":"sns.set(rc={'figure.figsize':(10,7)}) # Defines the size of seaborn graphs \nsns.heatmap(df.corr(), annot=True)","17e536df":"# let's find out the percentage of males and females in the dataset\n\nlabels = list(df.sex.unique())\nsizes = list(df.sex.value_counts())\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.2f%%', startangle=90)\nplt.show()","eddef968":"# Let's compare male and females in respect to age vs charges \n\nsns.FacetGrid(data=df, col='sex', hue='smoker', height=5).map(sns.scatterplot, 'age', 'charges').add_legend()","a935b9f2":"sns.barplot(data=df, x='sex', y='charges', hue='smoker')","7792bc94":"# The number of smokers between males and females\nsns.catplot(x=\"smoker\", kind=\"count\", hue = 'sex', palette=\"pink\", data=df)","538fd204":"sns.histplot(data=df[df.smoker == 'no'], x='charges', color='y')\nsns.histplot(data=df[df.smoker == 'yes'], x='charges')","4144b040":"sns.lmplot(data=df, x='bmi', y='charges', hue='smoker')","bb87b647":"df['bmiagre'] = pd.cut(df.bmi, [10, 20, 30, 40, 50])\nsns.boxplot(data=df, x='bmiagre', y='charges', hue='smoker').set(title='outliers')","876c08d5":"# Distribution of smoker and non-smoker according to the age \n\nsns.histplot(data=df, x='age', hue='smoker')","1e7375f9":"sns.lmplot(data=df, x='age', y='charges', hue='smoker')","6c235aae":"# For the Random forests algorithm we are going to use the df1 dataframe\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\ny = df1.charges\nfeatures = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']\nX = df1[features]\n\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size = 0.2, train_size=0.8, random_state = 4)\n\nforest_model = RandomForestRegressor(n_estimators=100, random_state=0)\nforest_model.fit(train_X, train_y)\npred = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, pred))","d1accb95":"forest_model.score(val_X, val_y)","f9ac31ea":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(val_y, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(pred, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price')\n\nplt.show()\nplt.close()","4a4bbc17":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nquad = PolynomialFeatures(degree = 2)\nx_quad = quad.fit_transform(X)\nY = df1.charges\n\nX_train,X_test,Y_train,Y_test = train_test_split(x_quad,Y, random_state = 0)\nplr = LinearRegression().fit(X_train,Y_train)\npred_logis = plr.predict(X_test)\nprint(mean_absolute_error(Y_test, pred_logis))","5475dd50":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(val_y, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(pred_logis, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price')\n\nplt.show()\nplt.close()","d2c707cd":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\nmy_pipeline = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n    ('model', RandomForestRegressor(n_estimators=50, random_state=0))\n])\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"Average MAE score:\", scores.mean())","c82a4c43":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=3, \n             eval_set=[(val_X, val_y)],\n             verbose=False)\n\npredictions = my_model.predict(val_X)\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, val_y)))","7e2753f5":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\n\nax1 = sns.distplot(val_y, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(predictions, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n\n\nplt.title('Actual vs Fitted Values for Price')\nplt.xlabel('Price')\n\nplt.show()\nplt.close()","a8c29a30":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\ndf = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\n\nX = pd.DataFrame(df.drop('charges', axis=1))\ny = pd.DataFrame(df.pop('charges'))","1f6d761c":"# Divide the datset into train and testset and standardize the data \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nnum_cols = ['age', 'bmi', 'children']\ncat_cols = ['sex', 'smoker', 'region']\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), num_cols),\n    (OneHotEncoder(), cat_cols),\n)\n\nX_train = preprocessor.fit_transform(X_train)\nX_test = preprocessor.transform(X_test)\ny_train = y_train \/ 1000\ny_test = y_test \/ 1000\n\ninput_shape = [X_train.shape[1]]","14a7826d":"## Our first NN model\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=input_shape),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.3),\n    layers.BatchNormalization(),\n    layers.Dense(1),\n    \n])\n\nmodel.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=512,\n    epochs=500,\n    verbose=0,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))","f8084b9e":"# Make and plot the predicted values \n\ny_pred = model.predict(X_test).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(y_test, y_pred)\nplt.xlabel('True values')\nplt.ylabel('Predicted values')\nplt.title('A plot that shows the true and predicted values')\nplt.xlim([0, 60])\nplt.ylim([0, 60])\nplt.plot([0, 60], [0, 60])","5716bb25":"# Evaluate the model\n\nmodel.evaluate(X_test, y_test, batch_size=128)","82af43cf":"model = keras.Sequential([\n    layers.Dense(512, activation='relu', input_shape=input_shape),\n    layers.Dense(300, activation='relu'),    \n    layers.Dense(150, activation='relu'),\n    layers.Dense(1),\n])\n\nmodel.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae'])\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=64,\n    epochs=100,\n    callbacks=[early_stopping],\n    verbose=0\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))","5d6d43b4":"y_pred = model.predict(X_test).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(y_test, y_pred)\nplt.xlabel('True values')\nplt.ylabel('Predicted values')\nplt.title('A plot that shows the true and predicted values')\nplt.xlim([0, 60])\nplt.ylim([0, 60])\nplt.plot([0, 60], [0, 60])","9637471f":"model.evaluate(X_test, y_test, batch_size=128)","f49807f3":"The dataset does not have null values.","c96ab6b7":"## Second NN example","9470cb9f":"Smokers spend more on treatment","05f11c44":"We can see that from 20 to 40% there's a lot of outliers in the non-smoker category","1d967279":"The percentage is almost the same for the two categories","31f7c80c":"From the heatmap we can see that 'charges' is highly correlated with the fact of a person being a smoker or not ","941e7b43":"# Neural Network","34e1acde":"Males and females are charged almost the same. Gender does not influences the charges, what really influences the charges is if a person smokes","34eab8a9":"### Random Forest with pipeline ","ef7795c6":"Does BMI influences on the charges? ","9342fce9":"In orer to better understand the data contained in the variable BMI, we will divide the data into an BMI strata and construct side-by-side boxplots of the distribution ","0ce08dd2":"### Logistic Regression","2d1e7338":"There are 3 columns that contain categorical data, let's transform the features into ordinal integers","5b00a36c":"Now in order to see the correlation between all the vaariable, we will add all the encoded catergorical variables from 'df1' to 'df'","2fbd8bcd":"Now let's see the distribution of the data for smokers and non-smokers.","f58b7fbe":"BMI does not have a great influence on the price that is paid","ebf2fb55":"### Random forests","4c937468":"#### Categorical variables","6e4b0b55":"### XGB Regressor "}}