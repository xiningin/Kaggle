{"cell_type":{"a422a3d7":"code","2d9881af":"code","7fdd1f26":"code","3bf49335":"code","56b4476f":"code","0f54287b":"code","baca125f":"code","86521217":"code","cdd549e9":"code","6a6c7924":"code","11692474":"code","2daa5fa5":"code","f61269dc":"code","cd48cbf1":"code","d653fc62":"code","b16d2ef0":"code","c6bb903e":"code","c0efdf58":"code","73f4d23f":"code","c444388c":"code","acd36aff":"code","3a3452e7":"code","5c57b9d3":"code","5ecce92a":"code","7b4fad59":"code","c445a808":"code","22502b08":"code","77a83df6":"code","362fb052":"code","bcad9e52":"markdown","acee43f9":"markdown","4ebc1d32":"markdown","f6fe9364":"markdown","dc7c4f8b":"markdown","a35e2e7a":"markdown","30730023":"markdown","09d35d88":"markdown","e74194d1":"markdown","1dc6bfa5":"markdown","d4ff24ed":"markdown","4d20652a":"markdown","34b15920":"markdown","f83b7c49":"markdown","9f6efd96":"markdown","4ba49021":"markdown","244716e9":"markdown","a74f155f":"markdown","21c22640":"markdown","c1924688":"markdown"},"source":{"a422a3d7":"!pip install -q --upgrade wandb ","2d9881af":"!git clone https:\/\/github.com\/idstcv\/GPU-Efficient-Networks.git","7fdd1f26":"cd .\/GPU-Efficient-Networks","3bf49335":"import GENet","56b4476f":"cd ..\/","0f54287b":"# Python library to interact with the file system.\nimport os\n\n#Visualization\nimport plotly.express as px\n\n\n# Software library written for data manipulation and analysis.\nimport pandas as pd\n\n# fastai library for computer vision tasks\nfrom fastai.vision.all import *\nfrom fastai.callback.wandb import *\n\n# Developing and training neural network based deep learning models.\nimport torch\nfrom torch import nn\n\n# Python library for image augmentation\nimport albumentations as A\n\n\nimport wandb\nwandb.login()","baca125f":"def set_seed(dls, x=42): \n    random.seed(x)\n    dls.rng.seed(x) \n    np.random.seed(x)\n    torch.manual_seed(x)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed_all(x)\n    ","86521217":"path = Path('..\/input\/g2net-constant-q-transformed')","cdd549e9":"train_df = pd.read_csv(path\/'training_labels.csv')\ntrain_df","6a6c7924":"train_df['id'] = train_df['id'].map(lambda x : f'{path}\/train_cqt\/train_cqt\/{x}.png' )\ntrain_df.head()","11692474":"\ntrain_df = train_df.sample(frac=0.1, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nlen(train_df)","2daa5fa5":"dist = train_df.target.map({0:'Target 0', 1:'Target 1'})\ndist = dist.value_counts()\nfig = px.pie(dist,\n             values='target',\n             names=dist.index,\n             hole=.4,title=\"Target Distribution\")\nfig.update_traces(textinfo='percent+label', pull=0.05)\nfig.show()","f61269dc":"# obtain the input images.\ndef get_x(r):\n    return r['id']\n\n# obtain the targets.\ndef get_y(r):\n    return r['target']","cd48cbf1":"'''AlbumentationsTransform will perform different transforms over both\n   the training and validation datasets ''' \nclass AlbumentationsTransform(RandTransform):\n    \n    '''split_idx is None, which allows for us to say when we're setting our split_idx.\n       We set an order to 2 which means any resize operations are done first before our new transform. '''\n    split_idx, order = None, 2\n    \n    def __init__(self, train_aug, valid_aug): store_attr()\n    \n    # Inherit from RandTransform, allows for us to set that split_idx in our before_call.\n    def before_call(self, b, split_idx):\n        self.idx = split_idx\n    \n    # If split_idx is 0, run the trainining augmentation, otherwise run the validation augmentation. \n    def encodes(self, img: PILImage):\n        if self.idx == 0:\n            aug_img = self.train_aug(image=np.array(img))['image']\n        else:\n            aug_img = self.valid_aug(image=np.array(img))['image']\n        return PILImage.create(aug_img)","d653fc62":"def get_train_aug(size): \n    \n    return A.Compose([\n            # allows to combine RandomCrop and RandomScale\n            A.RandomResizedCrop(size,size),\n        \n            # Randomly apply affine transforms: translate, scale and rotate the input.\n            A.ShiftScaleRotate(p=0.5),\n        \n            # Randomly change hue, saturation and value of the input image.\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n        \n            # Randomly change brightness and contrast of the input image.\n            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n        \n            # CoarseDropout of the rectangular regions in the image.\n            A.CoarseDropout(p=0.5),\n        \n            # Cutout of the square regions in the image.\n            A.Cutout(p=0.5) ])\n\ndef get_valid_aug(size): \n    \n    return A.Compose([\n    # Crop the central part of the input.   \n    A.CenterCrop(size, size, p=1.),\n    \n    # Resize the input to the given height and width.    \n    A.Resize(size,size)], p=1.)","b16d2ef0":"'''The first step item_tfms resizes all the images to the same size (this happens on the CPU) \n   and then batch_tfms happens on the GPU for the entire batch of images. '''\n# Transforms we need to do for each image in the dataset\nitem_tfms = [Resize(224), AlbumentationsTransform(get_train_aug(224), get_valid_aug(224))]\n\n# Transforms that can take place on a batch of images\nbatch_tfms = [Normalize.from_stats(*imagenet_stats)]","c6bb903e":"def get_data(bs=32, data_df=train_df):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                       splitter=RandomSplitter(seed=42), # split data into training and validation subsets.\n                       get_x=get_x, # obtain the input images.\n                       get_y=get_y, # obtain the targets.\n                       item_tfms = item_tfms,\n                       batch_tfms = batch_tfms)\n    return dblock.dataloaders(data_df,bs=bs)\n\ndls = get_data()\nset_seed(dls, 42)","c0efdf58":"# We can call show_batch() to see what a sample of a batch looks like.\ndls.show_batch()","73f4d23f":"model = GENet.genet_large(pretrained=True, root='..\/input\/genetparam\/')","c444388c":"wandb.init(project='G2Net-Fastai', job_type='train', name = 'GPU Efficient Network Large', config = {'competetion': 'G2Net','_wandb_kernel':'tang'})","acd36aff":"# Group together some dls, a model, and metrics to handle training\nlearn = Learner(dls, model, metrics = RocAucBinary(), cbs=[WandbCallback(log='all'), SaveModelCallback()]) ","3a3452e7":"# Choosing a good learning rate\nlearn.lr_find()","5c57b9d3":"# We can use the fine_tune function to train a model with this given learning rate\nlearn.fine_tune(4, base_lr=0.0010000000474974513)","5ecce92a":"test_path = Path('..\/input\/g2net-gravitational-wave-detection')\nsample = pd.read_csv(test_path\/'sample_submission.csv')\nsample","7b4fad59":"_sample = sample.copy()\n_sample['id'] = _sample['id'].map(lambda x: f'{path}\/test_cqt\/test_cqt\/{x}.png')\ntest_dl = dls.test_dl(_sample)","c445a808":"_sample.head()","22502b08":"test_dl.show_batch()","77a83df6":"a, _ = learn.tta(dl=test_dl, n=1)\npred = a.argmax(dim=1).numpy()\nsample['target'] = pred","362fb052":"sample.to_csv('submission.csv',index=False)","bcad9e52":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">GPU Efficient Networks<\/p>\n\n![](https:\/\/raw.githubusercontent.com\/idstcv\/GPU-Efficient-Networks\/master\/misc\/genet_acc_speed_curve.jpg)\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> The proposed design space is optimized for fast GPU inference. In this space, it uses a semi-automatic NAS to help us design GPU-Efficient Networks. GENets use full convolutions in low-level stages and depth-wise\nconvolution and\/or bottleneck structure in high-level stages.<br>This design is inspired by the observation that convolutional kernels in the high-level stages are more likely to have low intrinsic rank and different types of convolutions have different kinds of efficiency on GPU.<br>","acee43f9":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">What are we discussing today? <\/p>\n <p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\"> Constant Q Transform<br>\n GPU Efficient Network <br>\n FastAI <br>\n Test Time Augmentation <br>\n Weights and Biases for Experiment Tracking","4ebc1d32":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Final Metrics<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/HBzvpxZ.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","f6fe9364":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Prediction Samples<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/qoFuEyk.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","dc7c4f8b":"<center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br>We'll be using this to train our K Fold Cross Validation and gain better insights about our training. <br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","a35e2e7a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Submission<\/p><br>","30730023":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Preprocess and Prepare Dataloader<\/p>","09d35d88":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Load Train and Test<\/p>","e74194d1":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Evaluation Metrics<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/wy8lpLe.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","1dc6bfa5":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\"><a href = 'https:\/\/wandb.ai\/tanishqgautam\/G2Net-Fastai'>Check out the Weights and Biases Dashboard here $\\rightarrow$ <\/a><\/p>","d4ff24ed":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Minimal EDA<\/p>","4d20652a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import GPU Efficient Network from Github<\/p>","34b15920":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Create Dataloaders<\/p>","f83b7c49":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">G2Net Gravitational Waves<\/p>\n\n![](https:\/\/www.g2net.eu\/wp-content\/uploads\/2021\/03\/Acoustic_sound_wave_quantum_research-1170x600.jpg)\n","9f6efd96":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Visualize Stacked CQT Images<\/p>","4ba49021":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Seed for Reproducibility<\/p>","244716e9":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import Libraries<\/p>","a74f155f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Albumentations with FastAI<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> The Albumentation code has been borrowed from Fastai docs. It's very common to use different transforms on the training dataset versus the validation dataset. Lets see how!<br>\nAlbumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.<br><br>\n\ud83d\udccd Albumentations supports all common computer vision tasks such as classification, semantic segmentation, instance segmentation, object detection, and pose estimation.<br>\n\ud83d\udccd The library provides a simple unified API to work with all data types: images (RBG-images, grayscale images, multispectral images), segmentation masks, bounding boxes, and keypoints.<br>\n\ud83d\udccd The library contains more than 70 different augmentations to generate new training samples from the existing data.<br>\n\ud83d\udccd Albumentations is fast.<br>\n","21c22640":"<p p style = \"font-family: garamond; font-size:35px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>","c1924688":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Test Time Augmentation (TTA)<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\"> Similar to what Data Augmentation is doing to the training set, the purpose of Test Time Augmentation is to perform random modifications to the test images. Thus, instead of showing the regular, \u201cclean\u201d images, only once to the trained model, we will show it the augmented images several times. We will then average the predictions of each corresponding image and take that as our final guess. <br><br>\nThe reason why it works is that, by averaging our predictions, on randomly modified images, we are also averaging the errors. The error can be big in a single vector, leading to a wrong answer, but when averaged, only the correct answer stand out.<\/p>"}}