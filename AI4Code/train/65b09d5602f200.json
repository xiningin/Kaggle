{"cell_type":{"039f0cf4":"code","f18fa996":"code","1656bdd8":"code","f46c079c":"code","ab4aaebf":"code","68df5eef":"code","544a0d69":"code","d3d3a6b2":"code","2a21a56d":"code","3ee422e7":"code","8ed1e20a":"code","3e713d58":"code","271283c7":"code","2f72dd65":"code","36ff49ce":"code","2545ca7c":"code","ba35ec32":"code","6cb77687":"code","94cc5780":"code","7eb1372c":"code","5f8569ea":"code","9023239d":"code","c196c8e4":"markdown","deffef7d":"markdown","d591f6dc":"markdown","a2962360":"markdown","0a842bbd":"markdown","8efabf74":"markdown"},"source":{"039f0cf4":"import pandas as pd \nimport numpy as np \nfrom tqdm import tqdm\nimport re\nimport pickle\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LSTM,RNN,Conv2D,Dense,Flatten,GlobalAveragePooling2D,Embedding,Bidirectional,Input,Dropout,Conv1D,MaxPooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam,SGD\n\n\nSEED=42","f18fa996":"paths=[\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\",\n       '\/kaggle\/input\/commonlitreadabilityprize\/train.csv',\n       '\/kaggle\/input\/commonlitreadabilityprize\/test.csv']\ndf_train=pd.read_csv(paths[1])\ndf_test=pd.read_csv(paths[2])\ndf_ss=pd.read_csv(paths[0])","1656bdd8":"df_train","f46c079c":"df_test","ab4aaebf":"df_train['excerpt'][0]","68df5eef":"def clean(string):\n    pattern='\\n'\n    pattern1= '\\''\n    test= re.sub(pattern,'',string)\n    string= re.sub(pattern1,'',string)\n\n    return string","544a0d69":"#remove stopwords \nnltk.download('stopwords')\nstop=stopwords.words('english')\n\n\ndef remove_stopwords(df):\n    new_text=[]\n    for i in range(len(df)):\n        test=[j for j in df['excerpt'][i].split() if j not in stop]\n        new_text.append(' '.join(test))\n    new_text=pd.Series(new_text,name='cleaned_text')\n    df=pd.concat([df,new_text],axis='columns',copy=False)\n    return df","d3d3a6b2":"df_train['excerpt']=df_train['excerpt'].map(clean)\ndf_test['excerpt']=df_test['excerpt'].map(clean)\n","2a21a56d":"df_train= remove_stopwords(df_train)\ndf_test= remove_stopwords(df_test)\n","3ee422e7":"length=[]\nfor i in df_train['cleaned_text']:\n    length.append(len(i))\n    \nlength= np.array(length)\nprint(length.mean(),length.min(),length.max())","8ed1e20a":"X=df_train['cleaned_text']\ny=df_train['target']\ntest=df_test['cleaned_text']\n\n","3e713d58":"df_train['cleaned_text'][0]","271283c7":"VOCAB= 25000\nmax_len=681\noov_token='<OOV_TOKEN>'\ntruncate_type='post'\npadding_type='post'\nembedding_dim=16\n","2f72dd65":"#tokenize\ntokenizer=Tokenizer(oov_token=oov_token,num_words=VOCAB)\ntokenizer.fit_on_texts(X)\nword_index = tokenizer.word_index\nprint(len(word_index))\n\n#\n\ntrain_sequences= tokenizer.texts_to_sequences(X)\ntest_sequences= tokenizer.texts_to_sequences(test)\n\ntrain_padding = pad_sequences(train_sequences, maxlen=max_len, padding= padding_type, truncating= truncate_type)\ntest_padding = pad_sequences(test_sequences, maxlen=max_len, padding= padding_type , truncating= truncate_type)","36ff49ce":"import pickle\nfrom time import time\n\nt = time()\nwith open('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb') as fp:\n    embeddings_index  = pickle.load(fp)","2545ca7c":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nprint(f'Shape of Embedding: {embedding_matrix.shape}')","ba35ec32":"\"\"\"embeddings_index = {}\nwith open('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembeddings_matrix = np.zeros(((len(word_index)+1),200))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector \"\"\"","6cb77687":"input_layer = Input(shape=(681,))\nx= embedding_layer= Embedding(len(word_index)+1,300,trainable=False,weights=[embedding_matrix])(input_layer)\nx= Conv1D(32,3,activation='relu')(x)\nx=MaxPooling1D(pool_size=2)(x)\nx= Bidirectional(LSTM(150))(x)\nx=Dense(128,activation='relu')(x)\nx=Dense(64,activation='relu')(x)\nx=Dense(32,activation='relu')(x)\nx=Dense(16,activation='relu')(x)\npredictions = Dense(1,activation='linear')(x)\n\nmodel1=Model(inputs=input_layer, outputs= predictions)\n\nprint(model1.summary())\n\n\nmodel1.compile(\n    optimizer= Adam(learning_rate=1e-5),\n    loss='mse',\n    metrics='mae'\n)\n\nmodel_checkpoint=ModelCheckpoint('golve_840b300d.h5',monitor='loss',save_best_only=True,mode='min')\nearly_stopping=EarlyStopping(monitor=\"loss\",min_delta=0,patience=10,verbose=0,mode=\"min\",restore_best_weights=True)\nreduce_lr=ReduceLROnPlateau(monitor=\"loss\",factor=0.2,patience=10,min_lr=0.00001)\n","94cc5780":"model1.fit(train_padding,y,epochs=200,batch_size=256, callbacks = [model_checkpoint,reduce_lr,early_stopping])","7eb1372c":"y_pred = model1.predict(test_padding)","5f8569ea":"sub_scores=[]\nfor i in y_pred:\n    sub_scores.append(i)\nsub_scores","9023239d":"sub=pd.DataFrame({'id':df_ss['id'],'target':y_pred})\nsub.to_csv('submission.csv',index=False)\nsub.head()","c196c8e4":"# Model Construction","deffef7d":"# Cleaning","d591f6dc":"# Submission code","a2962360":"# Versions\n1. Version 4- Used Glove 6B 200D\n2. Version 5- Used Glove 840B 300d ","0a842bbd":"# Tokenizing","8efabf74":"# Glove Embedding Prep"}}