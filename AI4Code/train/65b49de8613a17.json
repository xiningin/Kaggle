{"cell_type":{"55d5829a":"code","5001d894":"code","12eaca6b":"code","37874d00":"code","e388e2ff":"code","81888e67":"code","330eec24":"code","2f2c3df7":"code","4bb1b05d":"code","c61d3a50":"code","9c54c126":"code","32f3a368":"code","e0096554":"code","f44abcff":"code","7b0e5768":"code","552b304b":"code","ad9f5ad7":"markdown","28686391":"markdown","8b315df2":"markdown","b6b89e22":"markdown","106b7d6c":"markdown","79f02f37":"markdown"},"source":{"55d5829a":"import numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport glob\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.datasets as dset\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nimport pandas as pd\nfrom IPython.display import HTML\nfrom tqdm.auto import tqdm\nfrom torchvision.models import inception_v3, vgg16\nfrom torch.cuda.amp import GradScaler, autocast\n\ndef is_cuda():\n    if torch.cuda.is_available():\n        print(\"CUDA available\")\n        return \"cuda\"\n    else:\n        print(\"No CUDA. Working on CPU.\")\n        return \"cpu\"\n        \ndevice = is_cuda()","5001d894":"root1 = \"..\/input\/lionking2frame\/LION2_warp\/\"\nroot2 = \"..\/input\/lionkingscreenshots\/\"\n\nbatch_size = 8\nimage_size = 256\nlr = 2e-4","12eaca6b":"def show_tensor_images(image_tensor, num_images=8, size=(3, 64, 64), nrow=4, figsize=8):\n\n    image_tensor = (image_tensor + 1) \/ 2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n    plt.figure(figsize=(figsize, figsize))\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n    \ndef to_rgb(img):\n    rgb_img = Image.new(\"RGB\", img.size)\n    rgb_img.paste(img)\n    return rgb_img\n\nclass ImageSet(Dataset):\n    def __init__(self, transform):\n        self.transform = transform\n        self.imgs = sorted(glob.glob(os.path.join(root1, \"*.*\"))) # LionKing2\n        \n    def __getitem__(self, index):\n        img = Image.open(self.imgs[index % len(self.imgs)])\n        img = to_rgb(img)\n        img = self.transform(img)\n        return img\n    \n    def __len__(self):\n        return len(self.imgs)\n    \nclass ImageSet_test(Dataset):\n    def __init__(self, transform):\n        self.transform = transform\n        self.imgs = sorted(glob.glob(os.path.join(root2, \"*.*\"))) # LionKing1\n        \n    def __getitem__(self, index):\n        img = Image.open(self.imgs[index % len(self.imgs)])\n        img = to_rgb(img)\n        img = self.transform(img)\n        return img\n    \n    def __len__(self):\n        return len(self.imgs)\n\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n\ndataset = ImageSet(transform=transform)\ndataloader_lerp = DataLoader(dataset, batch_size=batch_size, shuffle=False)\ndataset_test = ImageSet_test(transform=transform)","37874d00":"real_batch = next(iter(dataloader_lerp))\nshow_tensor_images(real_batch)","e388e2ff":"real_batch.shape","81888e67":"class Generator(nn.Module):\n    def __init__(self, z_dim=64, out_res=256):\n        super().__init__()\n        assert out_res == 256, \"Only Output Resolution of 256x256 Implemented, got {}\".format(out_res)\n        \n        self.block1 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.Conv2d(z_dim, z_dim\/\/2, 3, 1, 1),\n            nn.BatchNorm2d(z_dim\/\/2),\n            nn.ReLU()\n        )\n        \n        self.block2 = self.make_block(z_dim\/\/2, z_dim\/\/4)\n        self.out = nn.Sequential(\n            nn.Conv2d(z_dim\/\/4, 3, 3, 1, 1),\n            nn.Tanh()\n        )\n        self.skip1 = self.upsample(z_dim, z_dim\/\/2)\n        self.skip2 = self.upsample(z_dim\/\/2, z_dim\/\/4)\n        \n\n    def make_block(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 3, 1, 1)),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU()\n        )\n        return block\n    \n    def upsample(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 1, 1, 0)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1)\n        )\n        return block\n\n    def forward(self, x):      \n        y1 = self.block1(x)\n        y2 = self.skip1(x)\n        x = y1 + y2\n        y1 = self.block2(x)\n        y2 = self.skip2(x)\n        x = y1 + y2\n        x = self.out(x)\n        return x\n    \nclass Discriminator(nn.Module):\n    def __init__(self, hidden_dim=64, in_res=256):\n        super().__init__()\n        assert in_res == 256, \"Only Output Resolution of 256x256 Implemented, got {}\".format(in_res)\n        self.block1 = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(3, hidden_dim\/\/2, 4, 2, 1)),\n            nn.LeakyReLU(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(hidden_dim\/\/2, hidden_dim, 3, 1, 1)),\n            nn.BatchNorm2d(hidden_dim),\n            nn.LeakyReLU(0.1)\n        )\n        self.block2 = self.make_block(hidden_dim, hidden_dim)\n        self.skip2 = self.down_sample(hidden_dim, hidden_dim)\n\n\n    def forward(self, x):\n        y = self.block1(x)  # 32 x 128 x 128\n        y1 = self.block2(y) # 64 x 64 x 64\n        y2 = self.skip2(y)  # 64 x 64 x 64\n        y = y1 + y2         # 64 x 64 x 64\n        return y\n\n    def make_block(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 4, 2, 1)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1)\n        )\n        return block\n\n    def down_sample(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.AvgPool2d(2, 2),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 1, 1, 0)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1)\n        )\n        return block\n    \nclass AutoEncoder(nn.Module):\n    def __init__(self, res=256, z_dim=64):\n        super().__init__()\n        self.decoder = Discriminator(in_res=res, hidden_dim=z_dim)\n        self.encoder = Generator(out_res=res, z_dim=z_dim)\n        \n    def forward(self, x):\n        x = self.decoder(x) # 64 x 64 x 64\n        x = self.encoder(x) # 3 x 256 x 256\n        return x\n    \n    def encode(self, x):\n        return self.decoder(x)\n    \n    def decode(self, x):\n        return self.encoder(x)","330eec24":"model = AutoEncoder()\nmodel.load_state_dict(torch.load(\"..\/input\/project-ae\/model.pt\"))\nmodel.to(device)","2f2c3df7":"# Import pretrained vgg16 model\nvgg = vgg16(pretrained=True)\nvgg.classifier = nn.Identity()\nvgg.to(device)\nvgg","4bb1b05d":"def not_sequence(img1, img2, check=False):\n    temp = torch.cat([img1.unsqueeze(0), img2.unsqueeze(0)], dim=0)\n    temp = temp.data.float()\n    temp = temp.to(device)\n    out = vgg(temp)\n    if check:\n        print(abs(out[0].mean().detach().item() - out[1].mean().detach().item()))\n    if abs(out[0].mean().detach().item() - out[1].mean().detach().item()) > 0.01:\n        return True\n    else:\n        return False","c61d3a50":"dataloader_c1 = DataLoader(dataset, batch_size=batch_size, shuffle=False)\ndataloader_c2 = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nn = np.random.randint(500)\n\ncheck1 = next(iter(dataloader_c1))\ncheck2 = next(iter(dataloader_c2))\n\nprint(not_sequence(check1[0],check1[-1],check=True))\nshow_tensor_images(check1)\nprint(not_sequence(check2[0],check2[-1],check=True))\nshow_tensor_images(check2)","9c54c126":"alpha_lerp = 1\nalpha_latent = 1e1\n\nrecon_loss = nn.L1Loss()\n\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\nlosses, imgs_list = [], []","32f3a368":"datalen = len(os.listdir(root1))\n\ndef train_lerp(num_iters_lerp, show_iter=1000):\n    \n    cur_iter = 0\n    \n    while cur_iter < num_iters_lerp:\n        # Starts from random point, select 8 sequential data\n        start = np.random.randint(datalen-8)        \n        data = torch.empty(8,3,256,256)\n        for i in range(8):\n            data[i] = dataset[start+i]\n            \n        # Skip if 8 layers are not in sequence\n        if not_sequence(data[0], data[-1], check=(cur_iter<4)):\n            continue\n        \n        # Tansfer data to GPU\n        data = data.to(device)\n        optimizer.zero_grad()\n        \n        # New loss function : L1 loss from each image pair and latent space\n        z_in = model.encode(data)\n        z_out = torch.cat([torch.lerp(z_in[0].unsqueeze(0), z_in[-1].unsqueeze(0), v) for v in np.arange(0, 1.1, 0.142857)]).cuda()\n        zs = model.decode(z_out)\n        \n        # alpha_lerp : \uccab\uc7a5\uacfc \ub05d\uc7a5\uc740 \uc880 \ub354 \uac15\uc870\ud574\uc11c \ubcf5\uc6d0\ud574\uc57c \ud560 \uac83\uac19\uc544\uc11c \uc77c\ub2e8 hyperparameter \ub9cc\ub4e4\uc5b4\ubd04\n        # alpha_latent : latent space\uc758 linearity\ub97c \ube44\uad50\ud558\uae30 \uc704\ud55c hyperparameter\n        loss = alpha_lerp * (recon_loss(data[0],zs[0]) + recon_loss(data[-1],zs[-1]))\n        for i in range(1,7):\n            loss += recon_loss(data[i],zs[i]) + alpha_latent * recon_loss(z_in[i],z_out[i])\n        \n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        cur_iter += 1\n        \n        # Show first 4 iteration\n        if cur_iter < 5:\n            print(\"Difference at latent space\")\n            print((z_in - z_out).mean((1,2,3)))\n            print(\"Difference at Image\")\n            print((data - zs).mean((1,2,3)))\n            print(\"{} \/ {}, loss: {:.4f}\".format(cur_iter, num_iters_lerp, loss.item()))\n            print(\"Original Images\")\n            show_tensor_images(data.float())\n            print(\"Generated Images\")\n            show_tensor_images(zs.float())\n            #imgs_list.append(model(fixed).detach().cpu())\n            \n            torch.save(model.state_dict(), \"model-lerp-demo.pt\")\n\n\n        if (cur_iter) % show_iter == 0:\n            print(\"Difference at latent space\")\n            print((z_in - z_out).mean((1,2,3)))\n            print(\"Difference at Image\")\n            print((data - zs).mean((1,2,3)))\n            print(\"{} \/ {}, loss: {:.4f}\".format(cur_iter, num_iters_lerp, loss.item()))\n            print(\"Original Images\")\n            show_tensor_images(data.float())\n            print(\"Generated Images\")\n            show_tensor_images(zs.float())\n            #imgs_list.append(model(fixed).detach().cpu())\n            \n            torch.save(model.state_dict(), \"model-lerp-demo.pt\")\n\n        del loss\n        torch.cuda.empty_cache()","e0096554":"num_iters_lerp = 20000\nshow_iters_lerp = 100\ntrain_lerp(num_iters_lerp=num_iters_lerp, show_iter=show_iters_lerp)","f44abcff":"#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\nmodel.eval()\n\nimgs = []\nimgs_original = []\n\ncheckpoint = 0\nnext_frame = 1\n\nt=0\nframe = checkpoint\n\nwhile t < 50:\n    data = torch.empty(8,3,256,256)\n    for i in range(8):\n        data[i] = dataset_test[frame]\n        frame += next_frame\n    \n    if not_sequence(data[0], data[-1]):\n        continue\n    \n    z1 = data[0].unsqueeze(0).cuda()\n    z2 = data[-1].unsqueeze(0).cuda()\n    z1 = model.encode(z1)\n    z2 = model.encode(z2)\n    zs = torch.cat([torch.lerp(z1, z2, v) for v in np.arange(0.1, 1, 0.1)]).cuda()\n    zs = model.decode(zs)\n    for i in range(len(zs)):\n        imgs.append(zs[i].detach().cpu())\n        \n    for i in range(len(data)):\n        imgs_original.append(data[i].detach().cpu())\n    \n    t += 1\n        \nlen(imgs)","7b0e5768":"fig = plt.figure(figsize=(6,6))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in imgs]\nani = animation.ArtistAnimation(fig, ims, interval=200, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","552b304b":"fig_original = plt.figure(figsize=(6,6))\nplt.axis(\"off\")\nims_original = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in imgs_original]\nani_original = animation.ArtistAnimation(fig_original, ims_original, interval=200, repeat_delay=1000, blit=True)\n\nHTML(ani_original.to_jshtml())","ad9f5ad7":"\ub77c\uc774\uc5b8\ud0b92\ub85c \ud559\uc2b5, \ub77c\uc774\uc5b8\ud0b91\uc73c\ub85c \ubaa8\ub378\uc0dd\uc131","28686391":"# 8\uc7a5\uc774 \uc5f0\uc18d\ub41c\uc9c0 \uc544\ub2cc\uc9c0 \ud310\ub2e8\ud558\ub294 \ud568\uc218 \uc124\uc815","8b315df2":"# \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \uae30\ubc18\uc73c\ub85c \uc601\uc0c1 \uc0dd\uc131\ud558\uae30","b6b89e22":"# Sequence\ub97c \ud559\uc2b5\ud558\ub294 \uc0c8\ub85c\uc6b4 \ud559\uc2b5\ubaa8\ub378 \uc0dd\uc131","106b7d6c":"# AutoEncoder \uc120\uc5b8\ud558\uace0 \uae30\uc874 \ubaa8\ub378 \ubd88\ub7ec\uc624\uae30","79f02f37":"# \uae30\ubcf8 \uc138\ud305"}}