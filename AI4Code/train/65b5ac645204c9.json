{"cell_type":{"bed2c9bc":"code","454fb33b":"code","53f50066":"code","94e0bf47":"code","a982583b":"code","1a5214f6":"code","dd87ef4c":"code","7e98266a":"code","017f9ecf":"code","d5788df2":"code","3ee78f48":"code","861310a6":"code","77888f83":"code","fa4c290c":"code","8b50de8e":"code","dc184733":"code","597851b5":"code","a9c6a0e8":"code","b665afba":"code","11595145":"code","c1112dbb":"code","15708c56":"code","8b17f9a4":"code","95fba0dd":"code","4b675818":"code","ddf004c1":"code","1f2f9b66":"code","e5f8817d":"code","bfaf5917":"code","f6776eb0":"code","6d0f5f8d":"code","6b3e75ad":"code","8af726ee":"code","78773301":"code","29b5e243":"code","1c65f824":"code","449026f2":"code","49e29596":"code","f46fb688":"code","6083f9be":"code","68ba6623":"code","4b68ea3b":"code","79159e78":"code","3aee67d4":"code","c140a50e":"code","a867c427":"code","96bb82e3":"code","08804bea":"code","568bf2b1":"code","3f14d622":"code","2f556895":"code","843fac0e":"code","02885aa1":"code","d690b8b1":"code","83e4e1de":"code","bcc91bef":"code","bc0864da":"code","21d1ce24":"code","fc5bba9e":"code","4c3e9a16":"code","d0dc4e3c":"markdown","6d59b5b3":"markdown","f7c6ae26":"markdown","3a60b01d":"markdown","b1b82637":"markdown","ed801b23":"markdown","e2a23844":"markdown","474a46ba":"markdown","fbfa7b40":"markdown","1102c477":"markdown","eb1acf4a":"markdown","7faab91c":"markdown","abcf545b":"markdown","ad64be31":"markdown","ad39cfbe":"markdown","d98cc3cd":"markdown","83a59d06":"markdown","33800fcc":"markdown","26cd194e":"markdown","4aa08d4a":"markdown","af1f445b":"markdown","f7225f53":"markdown","38c2fa6c":"markdown","7e438c1c":"markdown","6ee8cb71":"markdown","d8fb55ea":"markdown","d4955945":"markdown","4f13f89d":"markdown","a6783531":"markdown","ec309d12":"markdown","ed105e25":"markdown","d849d078":"markdown","2792d2c8":"markdown","2fedb0d7":"markdown","d8e9e2e0":"markdown","9a87fa14":"markdown","4ff0ea3a":"markdown","de13c6b0":"markdown","61123633":"markdown","953cf1e8":"markdown","4c916e48":"markdown","c316ad7c":"markdown","9ecd10ad":"markdown","829d89a7":"markdown","fcbb9ebe":"markdown","ed8044f9":"markdown","8c5ef67d":"markdown","59267be1":"markdown","19aec6f2":"markdown","e5f4a5cf":"markdown","1983cc54":"markdown","442e4bad":"markdown","bc92c402":"markdown","312184bf":"markdown","5a2b2501":"markdown","3d3cbb87":"markdown","e91f8aee":"markdown","82bf6f5b":"markdown","a4ff5ed3":"markdown","57d34b39":"markdown","66b746a2":"markdown"},"source":{"bed2c9bc":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","454fb33b":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/Santander\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","53f50066":"%%time\ntrain_df = pd.read_csv(PATH+\"train.csv\")\ntest_df = pd.read_csv(PATH+\"test.csv\")","94e0bf47":"train_df.shape, test_df.shape","a982583b":"train_df.head()","1a5214f6":"test_df.head()","dd87ef4c":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","7e98266a":"%%time\nmissing_data(train_df)","017f9ecf":"%%time\nmissing_data(test_df)","d5788df2":"%%time\ntrain_df.describe()","3ee78f48":"%time\ntest_df.describe()","861310a6":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature], df2[feature], marker='+')\n        plt.xlabel(feature, fontsize=9)\n    plt.show();","77888f83":"features = ['var_0', 'var_1','var_2','var_3', 'var_4', 'var_5', 'var_6', 'var_7', \n           'var_8', 'var_9', 'var_10','var_11','var_12', 'var_13', 'var_14', 'var_15', \n           ]\nplot_feature_scatter(train_df[::20],test_df[::20], features)","fa4c290c":"sns.countplot(train_df['target'])","8b50de8e":"print(\"There are {}% target values with 1\".format(100 * train_df[\"target\"].value_counts()[1]\/train_df.shape[0]))","dc184733":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","597851b5":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","a9c6a0e8":"features = train_df.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","b665afba":"features = train_df.columns.values[2:102]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","11595145":"features = train_df.columns.values[102:202]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","c1112dbb":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train_df[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","15708c56":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train_df[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","8b17f9a4":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train_df[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","95fba0dd":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train_df[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test_df[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","4b675818":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","ddf004c1":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","1f2f9b66":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(train_df[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","e5f8817d":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of min values per column in the train and test set\")\nsns.distplot(train_df[features].min(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","bfaf5917":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of max values per row in the train and test set\")\nsns.distplot(train_df[features].max(axis=1),color=\"brown\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","f6776eb0":"plt.figure(figsize=(16,6))\nfeatures = train_df.columns.values[2:202]\nplt.title(\"Distribution of max values per column in the train and test set\")\nsns.distplot(train_df[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].max(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","6d0f5f8d":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","6b3e75ad":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","8af726ee":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"gold\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","78773301":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","29b5e243":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per row in the train and test set\")\nsns.distplot(train_df[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","1c65f824":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per column in the train and test set\")\nsns.distplot(train_df[features].skew(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].skew(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","449026f2":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per row in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=1),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","49e29596":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train_df[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test_df[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","f46fb688":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","6083f9be":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","68ba6623":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","4b68ea3b":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","79159e78":"%%time\ncorrelations = train_df[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","3aee67d4":"correlations.tail(10)","c140a50e":"correlations.head(10)","a867c427":"%%time\nfeatures = train_df.columns.values[2:202]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train_df[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = test_df[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])","96bb82e3":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","08804bea":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","568bf2b1":"%%time\nidx = features = train_df.columns.values[2:202]\nfor df in [test_df, train_df]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","3f14d622":"train_df[train_df.columns[202:]].head()","2f556895":"test_df[test_df.columns[201:]].head()","843fac0e":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","02885aa1":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[202:]\nplot_new_feature_distribution(t0, t1, 'target: 0', 'target: 1', features)","d690b8b1":"features = train_df.columns.values[202:]\nplot_new_feature_distribution(train_df, test_df, 'train', 'test', features)","83e4e1de":"print('Train and test columns: {} {}'.format(len(train_df.columns), len(test_df.columns)))","bcc91bef":"features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget = train_df['target']","bc0864da":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}","21d1ce24":"folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","fc5bba9e":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","4c3e9a16":"sub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)","d0dc4e3c":"Here we check test dataset.","6d59b5b3":"# <a id='3'>Data exploration<\/a>  \n\n## <a id='31'>Check the data<\/a>  \n\nLet's check the train and test set.","f7c6ae26":"Let's show also the distribution of max values per columns in the train set.","3a60b01d":"Let's load the train and test data files.","b1b82637":"## Load data   \n\nLet's check what data files are available.","ed801b23":"## <a id='35'>Distribution of skew and kurtosis<\/a>  \n\nLet's see now what is the distribution of skew values per rows and columns.\n\nLet's see first the distribution of skewness calculated per rows in train and test sets.","e2a23844":"We will show just 5% of the data. On x axis we show train values and on the y axis we show the test values.","474a46ba":"We show here the distribution of min values per columns in train set.","fbfa7b40":"We define the hyperparameters for the model.","1102c477":"Let's see now the distribution of skewness on rows in train separated for values of target 0 and 1.","eb1acf4a":"Let's see now the distribution of skewness on columns in train separated for values of target 0 and 1.","7faab91c":"\n## <a id='32'>Density plots of features<\/a>  \n\nLet's show now the density plot of variables in train dataset. \n\nWe represent with different colors the distribution for values with **target** value **0** and **1**.","abcf545b":"The first 100 values are displayed in the following cell. Press <font color='red'>**Output**<\/font> to display the plots.","ad64be31":"<code>\nfeatures = [c for c in train_df.columns if c not in ['ID_code', 'target']]\nfor feature in features:\n    train_df['r2_'+feature] = np.round(train_df[feature], 2)\n    test_df['r2_'+feature] = np.round(test_df[feature], 2)\n    train_df['r1_'+feature] = np.round(train_df[feature], 1)\n    test_df['r1_'+feature] = np.round(test_df[feature], 1)\n<\/code>","ad39cfbe":"Let's check how many features we have now.","d98cc3cd":"The next 100 values are displayed in the following cell. Press <font color='red'>**Output**<\/font> to display the plots.","83a59d06":"Let's see first the distribution of kurtosis calculated per columns in train and test sets.","33800fcc":"Let's check the distribution of **target** value in train dataset.","26cd194e":"Let's check now the distribution of the mean value per column in the train dataset, grouped by value of target.","4aa08d4a":"Let's show the top 15 max of duplicate values per train set.","af1f445b":"Let's see now what is the distribution of kurtosis values per rows and columns.\n\nLet's see first the distribution of kurtosis calculated per rows in train and test sets.","f7225f53":"We add rounded features.  \n**Note**: this is a work in progress, some of the features added here will be later dropped.","38c2fa6c":"Let's show now the max distribution on columns for train and test set.","7e438c1c":"We can observe that there is a considerable number of features with significant different distribution for the two target values.  \nFor example, **var_0**, **var_1**, **var_2**, **var_5**, **var_9**, **var_13**, **var_106**, **var_109**, **var_139** and many others.\n\nAlso some features, like **var_2**, **var_13**, **var_26**, **var_55**, **var_175**, **var_184**, **var_196** shows a distribution that resambles to a bivariate distribution.\n\nWe will take this into consideration in the future for the selection of the features for our prediction model.  \n\nLe't s now look to the distribution of the same features in parallel in train and test datasets. \n\nThe first 100 values are displayed in the following cell. Press <font color='red'>**Output**<\/font> to display the plots.","6ee8cb71":"# <a id='1'>Introduction<\/a>  \n\nIn this challenge, Santander invites Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.  \n\nThe data is anonimyzed, each row containing 200 numerical values identified just with a number.  \n\nIn the following we will explore the data, prepare it for a model, train a model and predict the target value for the test set, then prepare a submission.\n\nStay tuned, I will frequently update this Kernel in the next days.\n\n","d8fb55ea":"Let's show now the distributions of min values per row in train set, separated on the values of target (0 and 1).","d4955945":"Let's see also the least correlated features.","4f13f89d":"Let's check the distribution of the standard deviation of values per columns in the train and test datasets.","a6783531":"<h1><center><font size=\"6\">Santander EDA and Prediction<\/font><\/center><\/h1>\n\n<h2><center><font size=\"4\">Dataset used: Santander Customer Transaction Prediction<\/font><\/center><\/h2>\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4a\/Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\/640px-Another_new_Santander_bank_-_geograph.org.uk_-_1710962.jpg\" width=\"500\"><\/img>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n- <a href='#3'>Data exploration<\/a>   \n - <a href='#31'>Check the data<\/a>   \n - <a href='#32'>Density plots of features<\/a>   \n - <a href='#33'>Distribution of mean and std<\/a>   \n - <a href='#34'>Distribution of min and max<\/a>   \n  - <a href='#35'>Distribution of skew and kurtosis<\/a>   \n - <a href='#36'>Features correlations<\/a>   \n - <a href='#37'>Duplicate values<\/a>   \n- <a href='#4'>Feature engineering<\/a>\n- <a href='#5'>Model<\/a>\n- <a href='#6'>Submission<\/a>  \n- <a href='#7'>References<\/a>","ec309d12":"# <a id='4'>Feature engineering<\/a>  \n\nThis section is under construction.  \n\nLet's calculate for starting few aggregated values for the existing features.","ed105e25":"Let's see now the distribution of kurtosis on rows in train separated for values of target 0 and 1.","d849d078":"We run the model.","2792d2c8":"Let's show now the distribution of max values per rown in the train set.","2fedb0d7":"## <a id='36'>Features correlation<\/a>  \n\nWe calculate now the correlations between the features in train set.  \nThe following table shows the first 10 the least correlated features.","d8e9e2e0":"Let's show the distribution of new features values for train and test.","9a87fa14":"Let's check the feature importance.","4ff0ea3a":"The data is unbalanced with respect with **target** value.   ","de13c6b0":"Both train and test data have 200,000 entries and 202, respectivelly 201 columns. \n\nLet's glimpse train and test dataset.","61123633":"Let's look to the top most correlated features, besides the same feature pairs.","953cf1e8":"The train and test seems to be well ballanced with respect with distribution of the numeric variables.  \n\n## <a id='33'>Distribution of mean and std<\/a>  \n\nLet's check the distribution of the mean values per row in the train and test set.","4c916e48":"Train contains:  \n\n* **ID_code** (string);  \n* **target**;  \n* **200** numerical variables, named from **var_0** to **var_199**;\n\nTest contains:  \n\n* **ID_code** (string);  \n* **200** numerical variables, named from **var_0** to **var_199**;\n\n\nLet's check if there are any missing data. We will also check the type of data.\n\nWe check first train.","c316ad7c":"A long queue to the lower values for both, extended as long as to -80 for test set, is observed.\n\nLet's now show the distribution of min per column in the train and test set.","9ecd10ad":"The next 100 values are displayed in the following cell. Press <font color='red'>**Output**<\/font> to display the plots.","829d89a7":"We can make few observations here:   \n\n* standard deviation is relatively large for both train and test variable data;  \n* min, max, mean, sdt values for train and test data looks quite close;  \n* mean values are distributed over a large range.\n\nThe number of values in train and test set is the same. Let's plot the scatter plot for train and test set for few of the features.\n","fcbb9ebe":"Let's check the new created features.","ed8044f9":"Same columns in train and test set have the same or very close number of duplicates of same or very close values. This is an interesting pattern that we might be able to use in the future.","8c5ef67d":"Let's see also the top 15 number of duplicates values per test set.","59267be1":"The correlation between the features is very small. \n\n## <a id='37'>Duplicate values<\/a>  \n\nLet's now check how many duplicate values exists per columns.","19aec6f2":"Let's show the distribution of standard deviation of values per row for train and test datasets.","e5f4a5cf":"# <a id='6'>Submission<\/a>  \n\nWe submit the solution.","1983cc54":"Let's check now the distribution of max values per rows for train and test set.","442e4bad":"Let's check the distribution of these new, engineered features.  \n\nWe plot first the distribution of new features, grouped by value of corresponding `target` values.","bc92c402":"Let's see first the distribution of skewness calculated per columns in train and test set.","312184bf":"There are no missing data in train and test datasets. Let's check the numerical values in train and test dataset.","5a2b2501":"## <a id='34'>Distribution of min and max<\/a>  \n\nLet's check the distribution of min per row in the train and test set.","3d3cbb87":"Let's check the distribution of the mean values per columns in the train and test set.","e91f8aee":"Let's see now the distribution of kurtosis on columns in train separated for values of target 0 and 1.","82bf6f5b":"# <a id='5'>Model<\/a>  \n\nFrom the train columns list, we drop the ID and target to form the features list.","a4ff5ed3":"# <a id='2'>Prepare for data analysis<\/a>  \n\n\n## Load packages\n","57d34b39":"Let's check now the distribution of the mean value per row in the train dataset, grouped by value of target.","66b746a2":"# <a id='7'>References<\/a>    \n\n[1] https:\/\/www.kaggle.com\/gpreda\/elo-world-high-score-without-blending  \n[2] https:\/\/www.kaggle.com\/chocozzz\/santander-lightgbm-baseline-lb-0-897  \n[3] https:\/\/www.kaggle.com\/brandenkmurray\/nothing-works\n\n"}}