{"cell_type":{"ab8ec977":"code","4a60a805":"code","f1887f36":"code","b880b726":"code","e8ddeb64":"code","4d3c7c2a":"code","1cac9b39":"code","4aaaa8a9":"code","1556228d":"code","ff605353":"code","fd71d6b7":"code","f3041589":"code","99a78375":"code","9468e52d":"code","19094395":"code","ff1fd0e1":"code","59faa146":"code","f6a1a34f":"code","812b686a":"code","f734688d":"code","fe54fde9":"code","be2e2ac7":"code","c9d5491d":"code","b60255ea":"code","4e580e09":"code","d51502c8":"code","49361564":"code","a97f1a57":"code","e2a2bddb":"code","26152b2e":"code","8e0102fe":"code","fa42835f":"code","5dbef33f":"code","6017cf54":"code","06e4f18d":"code","5b41e17c":"code","5c79c0eb":"code","e8fd65fd":"code","2fa260cc":"code","58d6e9f4":"code","51aeaa8b":"code","9d5f7386":"code","7d24d33e":"code","5b24b824":"code","dbb6983a":"code","d95473f7":"code","92cdf736":"code","7b2a42c5":"code","647ea819":"code","a5a34df5":"code","5b71fe58":"code","3b78feb5":"code","c0783a1d":"code","207de094":"code","df1c6384":"code","88fb47f1":"code","e46ba7ae":"code","e1d5e03d":"code","a2c819a2":"code","2500b565":"code","9f42667b":"code","d4658578":"code","48806754":"code","e92bd21c":"code","c98a20c2":"code","16ac17c2":"code","5b844579":"code","8dd23897":"code","de0b57e9":"code","70b631f1":"code","22b65c36":"code","603cc22b":"code","8e87ef6a":"code","22ed338e":"code","7d24eb45":"code","94576861":"code","7d16aacd":"code","065a9041":"code","f23a1240":"code","6d3c308a":"code","7e5d4a43":"code","1df7efa8":"code","473f89ec":"code","a762570c":"markdown","acfe2341":"markdown","2c85567c":"markdown","65994146":"markdown","d066254a":"markdown","3e73dae9":"markdown","be98faf6":"markdown","9ffed133":"markdown","71d9b768":"markdown","77e402bc":"markdown","9421a1cf":"markdown","4111b75e":"markdown","c3598ea5":"markdown","182c42d0":"markdown","c603efd8":"markdown","f051ee89":"markdown","6a1d4c85":"markdown","809c3e3a":"markdown","6e5cabf4":"markdown","bc195e4e":"markdown","9376984e":"markdown","46ffb6e3":"markdown","7e47561c":"markdown","c5c526b1":"markdown","d9e64f31":"markdown","7a466d6e":"markdown","37de1b47":"markdown","b23b62a7":"markdown","44067f61":"markdown","9a549cc8":"markdown","55654ab9":"markdown","92e00a91":"markdown","885b7714":"markdown"},"source":{"ab8ec977":"import pandas as pd","4a60a805":"# train_df = pd.read_csv('\/kaggle\/input\/siimcovid19-train-data-that-opacitycount-added\/train_df.csv')\n# local\ntrain_df = pd.read_csv('\/kaggle\/input\/siimcovid19-train-data-that-opacitycount-added\/train_df.csv')","f1887f36":"train_df.head()","b880b726":"train_df.drop(columns='Path', axis=1,inplace=True)","e8ddeb64":"train_df.head()","4d3c7c2a":"train_df['Opacity'] = train_df.apply(lambda row : 1 if row.label.split(' ')[0]=='opacity' else 0, axis=1)\ntrain_df","1cac9b39":"train_df.drop(columns=['Unnamed: 0'], inplace=True)\ntrain_df","4aaaa8a9":"meta_df = pd.read_csv('\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/meta.csv')","1556228d":"meta_df.head()","ff605353":"meta_df.info()","fd71d6b7":"meta_df.split.unique()","f3041589":"import warnings\nwarnings.filterwarnings(action='ignore')","99a78375":"train_meta_df = meta_df.loc[meta_df.split=='train']\ntrain_meta_df.drop('split',axis=1,inplace=True)\ntrain_meta_df.columns = ['id', 'origin_img_height','origin_img_width']\ntrain_meta_df.info()","9468e52d":"train_meta_df","19094395":"train_df.head()","ff1fd0e1":"# test lambda\ntrain_df['id'].apply(lambda x : x.split('_')[0])\n","59faa146":"train_df['id'] = train_df['id'].apply(lambda x : x.split('_')[0])","f6a1a34f":"train_df.head()","812b686a":"train_df = pd.merge(train_df, train_meta_df, on='id')","f734688d":"train_df.head()","fe54fde9":"path = '\/kaggle\/input\/siim-covid19-resized-to-256px-jpg\/train\/'\ntrain_imgs_path = list(train_df['id'].apply(lambda x : path + x + '.jpg').values)\ntrain_imgs_path[:10]","be2e2ac7":"import matplotlib.pyplot as plt","c9d5491d":"img = plt.imread(train_imgs_path[0])","b60255ea":"img.shape","4e580e09":"plt.imshow(img, cmap='gray');","d51502c8":"import numpy as np","49361564":"i = 0\ntrain_imgs = []\nfor img_path in train_imgs_path:\n    img = plt.imread(img_path)\n    train_imgs.append(img)\n    i += 1\n    if i % 1000 == 0:\n        print('{} \/ {}'.format(i, len(train_imgs_path)))\n    elif i == 6334:\n        print('6334 \/ 6334 (End)')","a97f1a57":"type(train_imgs)","e2a2bddb":"train_imgs = np.array(train_imgs)","26152b2e":"train_imgs.shape","8e0102fe":"train_imgs_path[0]","fa42835f":"train_imgs[:,:,:,np.newaxis].shape","5dbef33f":"train_imgs_4dim = train_imgs[:,:,:,np.newaxis]\ntrain_imgs_4dim.shape","6017cf54":"len(train_imgs)","06e4f18d":"min(train_imgs[0].reshape(-1)), max(train_imgs[0].reshape(-1))","5b41e17c":"min(train_imgs[13].reshape(-1)), max(train_imgs[13].reshape(-1))","5c79c0eb":"train_df['origin_img_height']","e8fd65fd":"train_df['height_ratio'] = train_df['origin_img_height'].apply(lambda x : 255\/x)\ntrain_df['height_ratio']","2fa260cc":"train_df['origin_img_width']","58d6e9f4":"train_df['width_ratio'] = train_df['origin_img_width'].apply(lambda x : 255\/x)\ntrain_df['width_ratio']","51aeaa8b":"train_df","9d5f7386":"types = list(train_df.columns[5:9])\ntypes","7d24d33e":"path","5b24b824":"train_imgs.shape","dbb6983a":"!mkdir .\/genData\n!mkdir .\/genData\/Negative\n!mkdir .\/genData\/Typical\n!mkdir .\/genData\/Indeterminate\n!mkdir .\/genData\/Atypical","d95473f7":"# Negative for Pneumonia\nimgs_Negative = list(train_df[train_df[types[0]]==1].index)\nfor idx in imgs_Negative:\n    plt.imsave('.\/genData\/Negative\/{}.jpg'.format(train_df.loc[idx,'id']), train_imgs[idx], cmap='gray')","92cdf736":"# Typical Apperance\nimgs_Typical = list(train_df[train_df[types[1]]==1].index)\nfor idx in imgs_Typical:\n    plt.imsave('.\/genData\/Typical\/{}.jpg'.format(train_df.loc[idx,'id']), train_imgs[idx], cmap='gray')","7b2a42c5":"# Indeterminate Apearance\nimgs_Indeterminate = list(train_df[train_df[types[2]]==1].index)\nfor idx in imgs_Indeterminate:\n    plt.imsave('.\/genData\/Indeterminate\/{}.jpg'.format(train_df.loc[idx,'id']), train_imgs[idx], cmap='gray')","647ea819":"# Atypical Apearance\nimgs_Atypical = list(train_df[train_df[types[3]]==1].index)\nfor idx in imgs_Atypical:\n    plt.imsave('.\/genData\/Atypical\/{}.jpg'.format(train_df.loc[idx,'id']), train_imgs[idx], cmap='gray')","a5a34df5":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","5b71fe58":"idg = ImageDataGenerator(\n    rescale=1. \/ 255,\n    rotation_range=3,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    zoom_range=0.05,\n    horizontal_flip=False,\n    fill_mode='reflect',\n    validation_split=0.2\n)","3b78feb5":"data_path = '.\/genData'\nbatch_size = 64\ntarget_size = (256, 256)\nclass_mode = 'categorical'\ncolor_mode = 'grayscale'","c0783a1d":"train_gen = idg.flow_from_directory(\n    data_path,\n    batch_size=batch_size,\n    target_size=target_size,\n    class_mode=class_mode,\n    color_mode=color_mode,\n    subset = 'training'\n)\n\nvalid_gen = idg.flow_from_directory(\n    data_path,\n    batch_size = batch_size,\n    target_size = target_size,\n    class_mode = class_mode,\n    color_mode=color_mode,\n    subset = 'validation'\n)","207de094":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint","df1c6384":"model = Sequential([\n    Conv2D(64, (3,3), activation='relu', input_shape=(256, 256,1)),\n    MaxPooling2D(2,2),\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    Conv2D(128, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    Conv2D(128, (3,3), activation='relu'),\n    MaxPooling2D(2,2),\n    Flatten(),\n    Dropout(0.5),\n    Dense(128, activation='relu'),\n    Dense(4, activation='softmax')\n])\nmodel.summary() ","88fb47f1":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","e46ba7ae":"filepath = 'my_checkpoint.ckpt'\ncp = ModelCheckpoint(\n    filepath = filepath,\n    save_weights_only = True,\n    save_best_only = True,\n    monitor = 'val_loss',\n    verbose=1\n)","e1d5e03d":"epochs = 1 # just for test\nmodel.fit(\n    train_gen,\n    validation_data = (valid_gen),\n    epochs = epochs,\n    callbacks=[cp]\n)","a2c819a2":"model.load_weights(filepath)","2500b565":"model.evaluate(valid_gen)","9f42667b":"model.save('.\/model\/basic_cnn.h5')","d4658578":"import tensorflow as tf","48806754":"mymodel = tf.keras.models.load_model('.\/model\/basic_cnn.h5')","e92bd21c":"mymodel.summary()","c98a20c2":"from tensorflow.keras.applications import EfficientNetB0","16ac17c2":"efc = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(256,256,3))\nefc.trainable=False","5b844579":"model = Sequential([\n    efc,\n    Flatten(),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dense(4, activation='softmax')\n])","8dd23897":"model.summary()","de0b57e9":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","70b631f1":"filepath = 'my_checkpoint_efc.ckpt'\ncp = ModelCheckpoint(\n    filepath = filepath,\n    save_weights_only = True,\n    save_best_only = True,\n    monitor = 'val_loss',\n    verbose=1\n)","22b65c36":"epochs=1\nmodel.fit(\n    train_gen,\n    validation_data=(valid_gen),\n    epochs=epochs,\n    callbacks=[cp]\n)","603cc22b":"model.load_weights(filepath)","8e87ef6a":"model.evaluate(valid_gen)","22ed338e":"idg = ImageDataGenerator(\n    # rescale False\n    rotation_range=3,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    zoom_range=0.05,\n    horizontal_flip=False,\n    fill_mode='reflect',\n    validation_split=0.2\n)","7d24eb45":"data_path = '.\/genData'\nbatch_size = 64\ntarget_size = (224, 224)\nclass_mode = 'categorical'\ncolor_mode = 'grayscale'","94576861":"train_gen = idg.flow_from_directory(\n    data_path,\n    batch_size=batch_size,\n    target_size=target_size,\n    class_mode=class_mode,\n    color_mode=color_mode,\n    subset = 'training'\n)\n\nvalid_gen = idg.flow_from_directory(\n    data_path,\n    batch_size = batch_size,\n    target_size = target_size,\n    class_mode = class_mode,\n    color_mode=color_mode,\n    subset = 'validation'\n)","7d16aacd":"efc = EfficientNetB0(weights='imagenet',\n                     include_top=False, \n                     input_shape=(224,224,3),\n                     drop_connect_rate=0.4)\nefc.trainable=False","065a9041":"model = Sequential([\n    efc,\n    Flatten(),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dense(4, activation='softmax')\n])","f23a1240":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","6d3c308a":"filepath = 'my_checkpoint_efc_224.ckpt'\ncp = ModelCheckpoint(\n    filepath = filepath,\n    save_weights_only = True,\n    save_best_only = True,\n    monitor = 'val_loss',\n    verbose=1\n)","7e5d4a43":"epochs=1\nmodel.fit(\n    train_gen,\n    validation_data=(valid_gen),\n    epochs=epochs,\n    callbacks=[cp]\n)","1df7efa8":"model.load_weights(filepath)","473f89ec":"model.evaluate(valid_gen)","a762570c":"### 1-a. load train-dataframe","acfe2341":"### 4-b.  Improving performance with an appropriate form","2c85567c":"### 3-c. model compile","65994146":"## Step 4. Modeling II - Multiclass classifier using EfficientNet(Transfer Learning)","d066254a":"### 2-b. sort image files into each type's folder","3e73dae9":"### 2-c. data generation, split train\/valid set","be98faf6":"And add 'Opacity' Column. The Value is 1 If Opacity detected, else 0","9ffed133":"### 3-f. model evaluate & save","71d9b768":"And simply EDA","77e402bc":"### 3-e. model fit","9421a1cf":"### 3-g. reload model & model summary","4111b75e":"## Step 2. Image Pre-Classification with Data generator","c3598ea5":"### Thanks for nice reference :\n\n`load dataset(original image size info-)`\n- [Resized to 256px JPG](https:\/\/www.kaggle.com\/xhlulu\/siim-covid19-resized-to-256px-jpg)","182c42d0":"## Step 3. Modeling I - Basic Multiclass classifier","c603efd8":"# SIIM: Step-by-Step Image Detection for Beginners \n## Part 2. Basic Modeling - Simplest Image Classification Models using Keras\n\n\ud83d\udc49 Part 1. [EDA to Preprocessing](https:\/\/www.kaggle.com\/songseungwon\/siim-covid-19-detection-10-step-tutorial-1)\n\n\ud83d\udc49 Mini Part. [Preprocessing for Multi-Output Regression that Detect Opacities](https:\/\/www.kaggle.com\/songseungwon\/siim-covid-19-detection-mini-part-preprocess)","f051ee89":"add Channel (3dim to 4dim, gray)","6a1d4c85":"Test sample image","809c3e3a":"The performance is not very different from the basic cnn model.\n\n\nIn fact, efficientnet (which is precisely efficientnetB0) is designed according to the image size (224,224), and the input data range should be 0~255. That is, pure data that has not been normalized must pass through the model. normalize is included in the model itself\n\ndocument : [Image classification via fine-tuning with EfficientNet](https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/)\n\nLet's use the model as recommended in the official documentation.","6e5cabf4":"> Index\n```\nStep 1. Load Data and Trim for use\n     1-a. load train-dataframe\n     1-b. load meta-dataframe\n     1-c. load image data array\n     1-d. calculate image resize ratio information\nStep 2. Image Pre-Classification with Data generator\n     2-a. classify image id by opacity types\n     2-b. sort image files into each type's folder\n     2-c. data generation, split train\/valid set\nStep 3. Modeling I - Basic Multiclass classifier\n     3-a. import libraries\n     3-b. basic modeling with keras api\n     3-c. model compile\n     3-d. save model checkpoint\n     3-e. model fit\n     3-f. model evaluate & save\n     3-g. reload model & model summary\nStep 4. Modeling II - Multiclass classifier using EfficientNet(Transfer Learning)\n     4-a. Load the EfficientNet and try it out\n     4-b. Improving performance with an appropriate form\n```","bc195e4e":"\nIn the first epoch, the accuracy increased noticeably (approximately 13%). If model learn iteratively, we can expect the difference in performance to become larger.\n\nIn this kernel, I made the simplest model with minimal coding. And now, Try to create model with better performance than this! with more complex models and more effective data!","9376984e":"### 1-b. load meta-dataframe","46ffb6e3":"### 3-d. save model checkpoint","7e47561c":"### 3-a. import libraries","c5c526b1":"### 2-a. classify image id by Opacity types","d9e64f31":"Create folders for each class **in advance**, and save images in each folder.","7a466d6e":"### 4-a. Load the EfficientNet and try it out","37de1b47":"We don't use dcm file. drop 'path' column","b23b62a7":"### 1-c. load image data array","44067f61":"We need the size of the individual images. This is necessary later to calculate the ratio and find the coordinates of the box border to detect the opacity.","9a549cc8":"- Y(height) : `dim0` \n- X(width) : `dim1`\n","55654ab9":"### 1-d. calculate image resize ratio information","92e00a91":"## Step 1. Load Data and Trim for use","885b7714":"### 3-b. basic modeling with keras api"}}