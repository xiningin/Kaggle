{"cell_type":{"3d407807":"code","548e20ea":"code","4c04199b":"code","bc99d7ed":"code","05b0f750":"code","68f06816":"code","13329153":"code","3f5f474b":"code","e87f34ec":"code","079a5891":"code","5cee6ca3":"code","de400285":"code","2a4f84fe":"code","ddb9d2fe":"code","467e58e0":"code","1d027251":"code","69aca162":"code","87b4abb8":"code","0e5a156e":"code","ee8a3778":"code","c29ae50e":"code","dce34de5":"code","59fb8272":"code","a5e42d5c":"code","6f19c140":"code","3956aa1e":"code","9d3bc7ca":"code","9870a6f4":"code","b0f39c36":"code","94af4583":"code","bd5049de":"code","d7e393e1":"code","9d3fdf37":"markdown","b6dd0d77":"markdown","616acf83":"markdown","034e6176":"markdown","d32511d5":"markdown","f680ddea":"markdown","7c5c26f9":"markdown","42d62ea2":"markdown"},"source":{"3d407807":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import normalize\npd.options.display.max_columns = 999\n%matplotlib inline","548e20ea":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\nfull_df = pd.concat([train,test],axis=0)","4c04199b":"train.head()","bc99d7ed":"full_df[\"ord_5a\"]=full_df[\"ord_5\"].str[0]\nfull_df[\"ord_5b\"]=full_df[\"ord_5\"].str[1]","05b0f750":"ordinal_features = ['ord_0','ord_3','ord_4','ord_5','ord_5a','ord_5b']","68f06816":"ord1_enc = OrdinalEncoder(categories=[np.array(['Novice','Contributor','Expert','Master','Grandmaster'])])","13329153":"full_df.ord_1 = ord1_enc.fit_transform(full_df.ord_1.values.reshape(-1,1)).astype(np.int16)","3f5f474b":"ord2_enc = OrdinalEncoder(categories=[np.array(['Freezing','Cold','Warm','Hot','Boiling Hot','Lava Hot'])])","e87f34ec":"full_df.ord_2 = ord2_enc.fit_transform(full_df.ord_2.values.reshape(-1,1)).astype(np.int16)","079a5891":"for feat in ordinal_features:\n    enc = OrdinalEncoder()\n    full_df[feat] = enc.fit_transform(full_df[feat].values.reshape(-1,1)).astype(np.int16)","5cee6ca3":"hex_df = full_df.loc[:,\"nom_5\":\"nom_9\"]","de400285":"hex_1 = lambda x: int(bin(int(x,16))[2:].zfill(36)[:9],2)\nhex_2 = lambda x: int(bin(int(x,16))[2:].zfill(36)[9:18],2)\nhex_3 = lambda x: int(bin(int(x,16))[2:].zfill(36)[18:27],2)\nhex_4 = lambda x: int(bin(int(x,16))[2:].zfill(36)[27:],2)","2a4f84fe":"new_ord_df = pd.DataFrame()\nfor col in hex_df:\n    new_ord_df['%s_1'%col] = hex_df[col].apply(hex_1)\n    new_ord_df['%s_2'%col] = hex_df[col].apply(hex_2)\n    new_ord_df['%s_3'%col] = hex_df[col].apply(hex_3)\n    new_ord_df['%s_4'%col] = hex_df[col].apply(hex_4)","ddb9d2fe":"full_df.drop(hex_df.columns,axis=1,inplace=True)","467e58e0":"full_df = pd.concat([full_df,new_ord_df],axis=1)","1d027251":"country_dict = {'Finland':[61.924110,25.748152,'europe',2], \n                'Russia':[61.524010,105.318756,'asia',4], \n                'Canada':[56.130367,-106.346771,'asia',3], \n                'Costa Rica':[9.748917,-83.753426,'sa',1], \n                'China':[35.861660,104.195396,'asia',6], \n                'India':[20.593683,78.962883,'na',5]}","69aca162":"country_df = pd.DataFrame()\ncountry_df['lat'] = full_df.nom_3.apply(lambda x: country_dict[x][0])\ncountry_df['lon'] = full_df.nom_3.apply(lambda x: country_dict[x][1])\ncountry_df['continent'] = full_df.nom_3.apply(lambda x: country_dict[x][2])","87b4abb8":"full_df = pd.concat([full_df,country_df],axis=1)","0e5a156e":"for feat in full_df.columns:\n    if full_df[feat].dtype == 'object':\n        print('Encoding ',feat)\n        le = LabelEncoder()\n        full_df[feat] = le.fit_transform(full_df[feat].values.reshape(-1,1))","ee8a3778":"cyclic_days = pd.DataFrame()\n\ncyclic_days['day_sin'] = np.sin(2 * np.pi * full_df['day']\/7)\ncyclic_days['day_cos'] = np.cos(2 * np.pi * full_df['day']\/7)\n\n# full_df['month_sin'] = np.sin(2 * np.pi * train['month']\/12)\n# full_df['month_cos'] = np.cos(2 * np.pi * train['month']\/12)","c29ae50e":"cyclic_months = pd.DataFrame()\n\ncyclic_months['month_sin'] = np.sin(2 * np.pi * full_df['month']\/12)\ncyclic_months['month_cos'] = np.cos(2 * np.pi * full_df['month']\/12)","dce34de5":"full_df = pd.concat([full_df,cyclic_days,cyclic_months],axis=1)","59fb8272":"drop_cols = ['target','id','bin_0','ord_5','day','month','nom_3']","a5e42d5c":"y = full_df.target[:len(train)]\nX = full_df.drop(drop_cols,axis=1)[:len(train)]","6f19c140":"lgb_train = lgbm.Dataset(X,label=y)","3956aa1e":"params = {\n        'max_depth':3,\n        'objective': 'binary',\n        'feature_fraction': 0.2,\n        'bagging_fraction': 1,\n        'verbose': -1,\n        'is_unbalance':False\n    }","9d3bc7ca":"# model = lgbm.cv(params,lgb_train,num_boost_round=4000,\n#                 early_stopping_rounds=30,metrics='auc',\n#                 eval_train_metric=True,verbose_eval=10)","9870a6f4":"model = lgbm.train(params,lgb_train,num_boost_round=3400)","b0f39c36":"lgbm.plot_importance(model)","94af4583":"test_X = full_df.drop(drop_cols,axis=1)[len(train):]","bd5049de":"sub_predictions = model.predict(test_X)","d7e393e1":"pd.DataFrame({\"id\": test[\"id\"], \"target\": sub_predictions}).to_csv(\"submission.csv\", index=False)","9d3fdf37":"### Encode Features","b6dd0d77":"- Here we do the trick. Each feature is a 36bit value encoded in hexadecimal. \n- Decode hex's into binary.\n- First time I wanted to make use of this discovery I thought they were 36 binary columns and they wanted us to discover that (Kaggle makes you a conspiracy theorist)\n- Treat as they are 4 9-bit integer values. (I found 4 as optimal by trying different values)\n- Encode newly formed 4 columns back to integer.\n- Now you have more columns with lower cardinality.\n","616acf83":"**Let's open the high cardinality hexadecimals and slice them into other low cardinality features!!**","034e6176":"#### Categorical","d32511d5":"#### Ordinal","f680ddea":"#### Countries","7c5c26f9":"#### Hex","42d62ea2":"#### Cyclical Encode"}}