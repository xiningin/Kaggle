{"cell_type":{"e22deb42":"code","1e056101":"code","3a962a59":"code","cbfd37e7":"code","f0c92925":"code","2206c41b":"code","8bb7b075":"code","15f8380a":"code","f6a29322":"code","01993fca":"code","4df48596":"markdown","7416b81e":"markdown","74bb35d6":"markdown","c763c843":"markdown","d144962d":"markdown","973e7ff7":"markdown","c39bd232":"markdown","d825b054":"markdown","855a2bf4":"markdown"},"source":{"e22deb42":"# imports\nimport numpy as np\nimport matplotlib.pyplot as plt","1e056101":"\"\"\"Generate fake data\"\"\"\ntrue_slope = 10.889\ntrue_intercept = 3.456\ninput_var = np.arange(0.0, 100.0)\noutput_var = true_slope * input_var + true_intercept + 500.0 * np.random.rand(len(input_var))","3a962a59":"plt.figure()\nplt.scatter(input_var, output_var)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","cbfd37e7":"\ndef compute_cost(input_var, output_var, params):\n    # get the length of the samples (m)\n    num_samples = len(input_var)\n\n    # init the cost of all the values\n    cost_sum = 0\n\n    # loop through all the values\n    for x, y in zip(input_var, output_var):\n        # calculate the hypnosis h(x)\n        y_pred = np.dot(params, np.array([1.0, x]))\n\n        # sum up all the costs\n        cost_sum += (y_pred - y) ** 2\n\n    cost = cost_sum \/ (num_samples * 2.0)\n\n    return cost","f0c92925":"def lin_reg_batch_gradient_descent(input_var, output_var, params, alpha, max_iter):\n    \"\"\"Compute the params for linear regression using batch gradient descent\"\"\"\n\n    # init the number of iterations\n    iteration = 0\n    # get the number of samples\n    num_samples = len(input_var)\n\n    # init the cost matrix\n    cost = np.zeros(max_iter)\n    params_store = np.zeros([2, max_iter])\n\n    # loop through the elements\n    while iteration < max_iter:\n        # store the cost so far\n        cost[iteration] = compute_cost(input_var, output_var, params)\n        # store the current value of the params\n        params_store[:, iteration] = params\n\n        print('--------------------------')\n        print(f'iteration: {iteration}')\n        print(f'cost: {cost[iteration]}')\n\n        for x, y in zip(input_var, output_var):\n            y_pred = np.dot(params, np.array([1.0, x]))\n            gradient = np.array([1.0, x]) * (y - y_pred)\n            params += alpha * gradient \/ num_samples\n        iteration += 1\n\n    return params, cost, params_store\n","2206c41b":"\"\"\"Train our model\"\"\"\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.20)\n\n# init the params (theta_0 , theta_1)\nparams_0 = np.array([20.0, 80.0])\n# init alpha (learning rate)\nalpha_batch = 1e-3\n# the number of max iterations that we want to make\nmax_iter = 500\n\n# now lets test our function\nparams_hat_batch, cost_batch, params_store_batch = lin_reg_batch_gradient_descent(x_train, y_train, params_0, alpha_batch, max_iter)","8bb7b075":"def lin_reg_stoch_gradient_descent(input_var, output_var, params, alpha):\n    \"\"\"Compute the params for linear regression using stochastic gradient descent\"\"\"\n    num_samples = len(input_var)\n    cost = np.zeros(num_samples)\n    params_store = np.zeros([2, num_samples])\n\n    i = 0\n    for x, y in zip(input_var, output_var):\n        cost[i] = compute_cost(input_var, output_var, params)\n        params_store[:, i] = params\n\n        print('--------------------------')\n        print(f'iteration: {i}')\n        print(f'cost: {cost[i]}')\n\n        y_hat = np.dot(params, np.array([1.0, x]))\n        gradient = np.array([1.0, x]) * (y - y_hat)\n        params += alpha * gradient \/ num_samples\n\n        i += 1\n\n    return params, cost, params_store","15f8380a":"alpha = 1e-3\nparams_0 = np.array([20.0, 80.0])\nparams_hat, cost, params_store = lin_reg_stoch_gradient_descent(x_train, y_train, params_0, alpha)","f6a29322":"plt.figure()\nplt.scatter(x_test, y_test)\nplt.plot(x_test, params_hat_batch[0] + params_hat_batch[1] * x_test, 'g', label='batch')\nplt.plot(x_test, params_hat[0] + params_hat[1] * x_test, '-r', label='stochastic')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\nprint(f'batch      T0, T1: {params_hat_batch[0]}, {params_hat_batch[1]}')\nprint(f'stochastic T0, T1: {params_hat[0]}, {params_hat[1]}')\nrms_batch = np.sqrt(np.mean(np.square(params_hat_batch[0] + params_hat_batch[1] * x_test - y_test)))\nrms_stochastic = np.sqrt(np.mean(np.square(params_hat[0] + params_hat[1] * x_test - y_test)))\nprint(f'batch rms(Error Rate):      {rms_batch}')\nprint(f'stochastic rms(Error Rate): {rms_stochastic}')","01993fca":"plt.figure()\nplt.plot(np.arange(max_iter), cost_batch, 'r', label='batch')\nplt.plot(np.arange(len(cost)), cost, 'g', label='stochastic')\nplt.xlabel('iteration')\nplt.ylabel('normalized cost')\nplt.legend()\nplt.show()\nprint(f'min cost with BGD: {np.min(cost_batch)}')\nprint(f'min cost with SGD: {np.min(cost)}')","4df48596":"# The End :)\n# If need help just contct <a href=\"https:\/\/www.linkedin.com\/in\/mahmoud-nasser-abdulhamed\/\">Me<\/a>","7416b81e":"### Batch gradient descent\n```\n   FOR j FROM 0 -> max_iteration:\n    FOR i FROM 0 -> m: \n        theta += (alpha \/ m) * (y[i] - h(x[i])) * x_bar\n    ENDLOOP\nENDLOOP\n```","74bb35d6":"## Compute linear regression cost\n## Note:\ninput_var ->  represent a list of the samples\n\nzip()  -> covert two list into a tuple of pairs like this ((x1, y1), (x2 , y2), .....(Xn, Yn))\n\nparams -> are the matrix of theta\n\n\\begin{equation}\nJ(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n\\end{equation}","c763c843":"## now lets do some plots!","d144962d":"## Batch Gradient descent\n\n\\begin{equation}\n\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n\\end{equation}\n","973e7ff7":"# useful links\n## <a href=\"https:\/\/www.youtube.com\/watch?v=IU5fuoYBTAM\">visit this link to know more about the difference between Batch GD and Stochastic GD<\/a>(English)\n## <a href=\"https:\/\/www.youtube.com\/watch?v=k_dnxLHXmDg\">What kind of problem that Stochastic GD solves<\/a>(Arabic)","c39bd232":"\n<center> <h1>Linear Regression with Gradient Descent<\/h1> <\/center>\n","d825b054":"### Stochastic gradient descent\n```\nshuffle(x, y)\nFOR i FROM 0 -> m:\n    theta += (alpha \/ m) * (y[i] - h(x[i])) * x_bar  \nENDLOOP\n```","855a2bf4":"## Gradient descent algorithm\n\n\n\\begin{equation}\n\\theta^{+} = \\theta^{-} + \\frac{\\alpha}{m} (y_{i} - h(x_{i}) )\\bar{x}\n\\end{equation}\n\nThis minimizes the following cost function\n\n\\begin{equation}\nJ(x, \\theta, y) = \\frac{1}{2m}\\sum_{i=1}^{m}(h(x_i) - y_i)^2\n\\end{equation}\n\nwhere\n\\begin{equation}\nh(x_i) = \\theta^T \\bar{x}\n\\end{equation}"}}