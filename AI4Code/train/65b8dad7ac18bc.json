{"cell_type":{"6e551b70":"code","5e3f9335":"code","a9206e78":"code","13efbf95":"code","4b0521f9":"code","95771b82":"code","33bcbf48":"code","212d2400":"code","a1a92315":"code","958801d7":"code","36e6659a":"code","abc7675e":"code","0cd45f3b":"code","74e1c03b":"code","b4503e95":"code","fc42a79d":"code","3109d29a":"code","b2973a89":"code","d1d6750a":"code","06aa5f9a":"code","2cc3d8d1":"code","c95eb174":"code","809d89e6":"code","e5049b50":"code","b8493407":"code","d802155a":"code","2f539f8f":"code","ab583c7a":"code","6afbce0b":"code","a43c282d":"code","b874767b":"code","d3ca9e92":"code","a1f1e7e5":"code","56df24b0":"code","a87efc15":"code","185d1615":"code","22294692":"code","5f797823":"code","e78769f3":"code","1983b43a":"code","142d76b0":"code","4bd0b826":"code","7aa2b8b1":"code","319c4438":"code","0b88795e":"code","6ab045f0":"code","b0b13e98":"code","f7ae61b5":"code","d820a876":"code","8e54bc1c":"code","130c7721":"code","5df11661":"code","7343a14b":"markdown","9946490d":"markdown","0fab1e0f":"markdown","8c4f89a3":"markdown","965b0f17":"markdown","8d08a1c3":"markdown","b5137e1f":"markdown","88845bb4":"markdown","80dccb0b":"markdown","d8c84e94":"markdown","7d256ccd":"markdown","6dbfded6":"markdown","48fcd23d":"markdown","fe02ddff":"markdown","330da60b":"markdown","53ed6ac4":"markdown"},"source":{"6e551b70":"import os, shutil\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","5e3f9335":"train_altar_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/train_dir\/train_altar_dir\"\ntrain_glass_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/train_dir\/train_glass_dir\"\ntest_altar_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/test_dir\/test_altar_dir\"\ntest_glass_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/test_dir\/test_glass_dir\"\nval_altar_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/val_dir\/val_altar_dir\"\nval_glass_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/val_dir\/val_glass_dir\"","a9206e78":"# Let us cross check\nprint('Training Altar images ={}'.format(len(os.listdir(train_altar_dir))),\n      '\\nTraining Glass images ={}'.format(len(os.listdir(train_glass_dir))),\n      '\\nValidation Altar images ={}'.format(len(os.listdir(val_altar_dir))),\n      '\\nValidation Glass images ={}'.format(len(os.listdir(val_glass_dir))),\n      '\\nTesting Altar images ={}'.format(len(os.listdir(test_altar_dir))),\n      '\\nTesting Glass images ={}'.format(len(os.listdir(test_glass_dir))),\n     )","13efbf95":"# View one image\nimport imageio\naltar_image_names = os.listdir(train_altar_dir)\naltar_img = imageio.imread(os.path.join(train_altar_dir, altar_image_names[5]))\nplt.imshow(altar_img)\n\nplt.figure()\n\nglass_image_names = os.listdir(train_glass_dir)\nglass_img = imageio.imread(os.path.join(train_glass_dir, glass_image_names[5]))\nplt.imshow(glass_img)","4b0521f9":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","95771b82":"# Compiling the model\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","33bcbf48":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/train_dir\"\nval_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/val_dir\"\n\ntrain_generator = train_datagen.flow_from_directory(\ntrain_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n\nval_generator = test_datagen.flow_from_directory(\nval_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n","212d2400":"# Let us look at these outputs\nfor data_batch, labels_batch in train_generator:\n    print('Data batch shape =', data_batch.shape)\n    print('Labels batch shape =', labels_batch.shape)\n    break","a1a92315":"history = model.fit(train_generator,\n                             epochs=20,\n                             validation_data=val_generator)","958801d7":"model.save('model1.h5')","36e6659a":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","abc7675e":"test_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/test_dir\"\ntest_generator = test_datagen.flow_from_directory(\ntest_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)","0cd45f3b":"model.evaluate(test_generator)","74e1c03b":"test_img = test_generator[12][0][9]\ntest_img.shape","b4503e95":"model.predict_classes(np.expand_dims(test_img, axis=0))\n# label 1 for glass and 0 for altar","fc42a79d":"plt.imshow(test_img)","3109d29a":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","b2973a89":"# Compiling the model\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","d1d6750a":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/train_dir\"\nval_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/val_dir\"\n\ntrain_generator = train_datagen.flow_from_directory(\ntrain_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n\nval_generator = test_datagen.flow_from_directory(\nval_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n","06aa5f9a":"# Let us look at these outputs\nfor data_batch, labels_batch in train_generator:\n    print('Data batch shape =', data_batch.shape)\n    print('Labels batch shape =', labels_batch.shape)\n    break","2cc3d8d1":"history = model.fit(train_generator,\n                             epochs=20,\n                             validation_data=val_generator)","c95eb174":"model.save('model2.h5')","809d89e6":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","e5049b50":"test_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/test_dir\"\ntest_generator = test_datagen.flow_from_directory(\ntest_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)","b8493407":"model.evaluate(test_generator)","d802155a":"test_img = test_generator[12][0][9]\ntest_img.shape","2f539f8f":"model.predict_classes(np.expand_dims(test_img, axis=0))\n# label 1 for glass and 0 for altar","ab583c7a":"plt.imshow(test_img)","6afbce0b":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","a43c282d":"# Compiling the model\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","b874767b":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/train_dir\"\nval_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/val_dir\"\n\ntrain_generator = train_datagen.flow_from_directory(\ntrain_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n\nval_generator = test_datagen.flow_from_directory(\nval_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n","d3ca9e92":"# Let us look at these outputs\nfor data_batch, labels_batch in train_generator:\n    print('Data batch shape =', data_batch.shape)\n    print('Labels batch shape =', labels_batch.shape)\n    break","a1f1e7e5":"history = model.fit(train_generator,\n                             epochs=20,\n                             validation_data=val_generator)","56df24b0":"model.save('model3.h5')","a87efc15":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","185d1615":"test_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/test_dir\"\ntest_generator = test_datagen.flow_from_directory(\ntest_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)","22294692":"model.evaluate(test_generator)","5f797823":"test_img = test_generator[12][0][9]\ntest_img.shape","e78769f3":"model.predict_classes(np.expand_dims(test_img, axis=0))\n# label 1 for glass and 0 for altar","1983b43a":"plt.imshow(test_img)","142d76b0":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n    tf.keras.layers.MaxPooling2D((2, 2)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","4bd0b826":"# Compiling the model\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-4),\n             loss='binary_crossentropy',\n             metrics=['accuracy'])","7aa2b8b1":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/train_dir\"\nval_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/val_dir\"\n\ntrain_generator = train_datagen.flow_from_directory(\ntrain_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n\nval_generator = test_datagen.flow_from_directory(\nval_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)\n","319c4438":"# Let us look at these outputs\nfor data_batch, labels_batch in train_generator:\n    print('Data batch shape =', data_batch.shape)\n    print('Labels batch shape =', labels_batch.shape)\n    break","0b88795e":"history = model.fit(train_generator,\n                             epochs=20,\n                             validation_data=val_generator)","6ab045f0":"model.save('model4.h5')","b0b13e98":"# Let us plot the loss and accuracy curves\nhistory_dict = history.history\nloss_value = history_dict['loss']\nval_loss_value = history_dict['val_loss']\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nepochs = range(1, len(loss_value) + 1)\nplt.plot(epochs, loss_value, 'b', label='Training Loss')\nplt.plot(epochs, val_loss_value, 'r', label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nplt.figure()\n\nplt.plot(epochs, acc, 'b', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","f7ae61b5":"test_dir = \"\/kaggle\/input\/dlproj-altarandglass\/DLProject\/test_dir\"\ntest_generator = test_datagen.flow_from_directory(\ntest_dir,\ntarget_size=(150, 150),\nbatch_size=20,\nclass_mode='binary'\n)","d820a876":"model.evaluate(test_generator)","8e54bc1c":"test_img = test_generator[12][0][9]\ntest_img.shape","130c7721":"model.predict_classes(np.expand_dims(test_img, axis=0))\n# label 1 for glass and 0 for altar","5df11661":"plt.imshow(test_img)","7343a14b":"*Read the images from the respective directories <br>\nWe shall use ImageDataGenerator method*","9946490d":"We utilized GPU Accelerator for building and testing our models.<br>\n\n1. Model 1, No advanced features\n    * Accuracy: 0.9650\n    * Loss: 0.1036\n    * Average Train Time: 2.3s; 45.15 ms\/step\n<br>\n2. Model 2, Only L2 weight regularization\n    * Accuracy: 0.9575\n    * Loss: 0.1704\n    * Average Train Time: 2.05s; 41.5ms\/step\n<br>\n3. Model 3, Only batch normalization\n    * Accuracy: 0.9500\n    * Loss: 0.2681\n    * Average Train Time: 2.05s; 43.3ms\/step\n<br>\n4. Model 4, both L2 weight regularization and batch normalization\n    * Accuracy: 0.9675\n    * Loss: 0.2098\n    * Average Train Time: 2.05s; 44.8ms\/step","0fab1e0f":"*Read the images from the respective directories <br>\nWe shall use ImageDataGenerator method*","8c4f89a3":"*Training the model*","965b0f17":"---\n\n# Model 4","8d08a1c3":"---\n\n# Model 3","b5137e1f":"*Read the images from the respective directories <br>\nWe shall use ImageDataGenerator method*","88845bb4":"---\n\n# Model 2","80dccb0b":"##### Reffering to a Kaggle Grandmaster Notebook for CNN Architecture.\nhttps:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n\nWe create 4 Models. Alternating between L2 weight regularization and Batch Normalization.<br>\n    \n    Each Model, has 3 2D Convolution Layers, with 32, 64, and 128 filters of size 3x3.\n    Alternating Max layers to obtain the best element features.\n    One flatten layer to convert the output into a 1D array.\n    First Dense layer of 128 neurons.\n    With a dropout rate of 0.4 to prevent overfitting.\n    Second Dense layer of 1 neuron for Binary Classification, with activation sigmoid.\n\n1. Model 1 has no weight regularization and batch normalization.\n2. Model 2 has L2 weight regularization but no batch normalization.\n3. Model 3 has no weight regularization but has batch normalization.\n4. Model 4 has both L2 weight regularization and batch normalization.<br>\n\nFrom the 4 models we will compare the accuracy, loss, and average train time. ","d8c84e94":"Understandings,\n* Accuracy improves as more advanced features are added.\n* But Loss increases due to advanced features. \n    * Improve using transfer learning.\n    * Increase dataset, data augmentation.\n    * Increase epochs.\n    * Increase dropout.\n* Advanced features also allow for faster training time.\n\nShortcomings,\n* Models are not relatively small to justify weight regularization.\n* Model is trained on a small dataset. No data augmentation used.\n* Due to small dataset the difference between model metrics is negligible.\n\n---","7d256ccd":"---\n\n# Model 1","6dbfded6":"---","48fcd23d":"*Training the model*","fe02ddff":"*Read the images from the respective directories <br>\nWe shall use ImageDataGenerator method*","330da60b":"*Training the model*","53ed6ac4":"*Training the model*"}}