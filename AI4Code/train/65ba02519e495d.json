{"cell_type":{"2bf8f028":"code","1d51f958":"code","d09eae56":"code","9b1ddfa4":"code","07885f03":"code","f43fa482":"code","f409eb8f":"code","fd1a02cb":"code","0c9ddc5e":"code","64584e13":"code","0c093577":"code","21f39190":"code","bfe79296":"code","27b1e411":"code","38ab2856":"code","00898817":"code","495e5696":"code","74f699db":"code","ce35f3dc":"code","760288b8":"code","a33e36fe":"code","03db71bb":"code","4563c402":"code","a0431d10":"code","70bef34b":"code","15aa0b01":"code","f071be3d":"code","6c0641e8":"code","81c66984":"code","dae7bd10":"code","dc6c9483":"code","354e4b09":"code","e5464a30":"code","2e120f6d":"code","7925f51b":"code","b5889751":"code","8210bd3a":"code","f204b3a7":"code","091cda0a":"code","a0d06be8":"code","7cf37589":"code","52097465":"markdown","c3409a8d":"markdown","4b0ce07d":"markdown","d99d850a":"markdown","80d701e6":"markdown","8eab966a":"markdown","d53637c3":"markdown","209495ef":"markdown","bdf6f14c":"markdown","8a50e77e":"markdown","7b209796":"markdown","d414480a":"markdown","92aee6b4":"markdown","0f7096ba":"markdown","642f186f":"markdown","5343a8fe":"markdown","ee8af410":"markdown","b5c446a2":"markdown","fa9b2c9e":"markdown","8e113103":"markdown","7789c947":"markdown","edcaacdb":"markdown","90ba2612":"markdown","ae208295":"markdown","20c61f8f":"markdown","f297c692":"markdown","aa4383fe":"markdown","42039b78":"markdown","8be513d9":"markdown","efd0a851":"markdown","62dc81b1":"markdown","bf1d4420":"markdown","8fa9b6d5":"markdown","da78433c":"markdown","caea9ff6":"markdown","8b8640b7":"markdown","a97d9d2c":"markdown","176a953c":"markdown","14cbae4b":"markdown","c7f27fea":"markdown","0c8d6497":"markdown","42c07475":"markdown","e9deb366":"markdown","364a9ce5":"markdown","b27810eb":"markdown","0d81558d":"markdown"},"source":{"2bf8f028":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d51f958":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 22}\nplt.rc('font', **font)\nplt.rcParams['figure.figsize'] = (15, 8)\n\nfrom scipy import stats","d09eae56":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","9b1ddfa4":"df.head()","07885f03":"df.shape","f43fa482":"df.info()","f409eb8f":"df.isna().sum().sum()","fd1a02cb":"df.drop('Unnamed: 32', 1,  inplace = True)","0c9ddc5e":"df['radius_mean'].hist(figsize = (15, 8))","64584e13":"m = df[df['diagnosis'] == 'M']\nb = df[df['diagnosis'] == 'B']","0c093577":"m['radius_mean'].hist(figsize = (15, 8), alpha = 0.4, label = 'Malignant')\nb['radius_mean'].hist(figsize = (15, 8), alpha = 0.4, label = 'Benign')\nplt.legend()","21f39190":"def Estimate(df, column, n = 7, m = 1000):\n    mu = df[column].mean()\n    \n    means = []\n    medians = []\n    \n    for _ in range(m):\n        sample = df[column].sample(n, replace = True)\n        mean = sample.mean()\n        median = sample.median()\n        \n        means.append(mean)\n        medians.append(median)\n        \n    print('RMSE of sample means: ', RMSE(means, mu))\n    print('RMSE of sample medians: ', RMSE(medians, mu))\n    \ndef RMSE(estimates, actual):\n    error2 = [(estimate - actual) ** 2 for estimate in estimates]\n    mse = np.mean(error2)\n    return np.sqrt(mse)","bfe79296":"Estimate(df, 'radius_mean')","27b1e411":"Estimate(m, 'radius_mean')\nprint('\\n')\nEstimate(df[df['diagnosis'] == 'B'], 'radius_mean')","38ab2856":"def SimulatedSample(df, column, n = 11, m = 1000):\n    mu = df[column].mean()\n    \n    means = []\n    \n    for _ in range(m):\n        sample = df[column].sample(n)\n        xbar = sample.mean()\n        \n        means.append(xbar)\n    \n    ci = [np.percentile(means, 5), np.percentile(means, 95)]\n    cdf = [Cdf(means, x) for x in sorted(means)]\n    stderr = RMSE(means, mu)\n    \n    return stderr, ci, cdf, means\n\ndef Cdf(sample, x):\n    count = 0\n    for i in sample:\n        if i <= x:\n            count += 1\n    return count \/ len(sample)","00898817":"def PlotSimulated(df, column):\n    stderr, ci, cdf, means = SimulatedSample(df, column)\n\n    print(\"Std Error: \", stderr)\n    print(\"90% Confidence Interval: \", ci)\n\n    plt.plot(sorted(means), cdf, ds = 'steps', label = 'CDF of sample means after simulation')\n    plt.axvline(ci[0], ls = ':', color = 'black', label = 'Confidence Interval')\n    plt.axvline(ci[1], ls = ':', color = 'black')\n    plt.axvline(df[column].mean(), ls = '-', color = 'maroon', label = 'population mean')\n    plt.legend()","495e5696":"PlotSimulated(m, 'radius_mean')","74f699db":"PlotSimulated(df[df['diagnosis'] == 'B'], 'radius_mean')","ce35f3dc":"rmm = df[df['diagnosis'] == 'M']['radius_mean'].values\nrmb = df[df['diagnosis'] == 'B']['radius_mean'].values\n\ntest_stat, pval = stats.ttest_ind(rmm, rmb, equal_var = False)\nprint(f'{pval : .4f}')\nprint(test_stat)","760288b8":"df['texture_mean'].hist(figsize = (15, 8))","a33e36fe":"m['texture_mean'].hist(figsize = (15, 8), alpha = 0.4, label = 'Malignant')\ndf[df['diagnosis'] == 'B']['texture_mean'].hist(figsize = (15, 8), alpha = 0.4, label = 'Benign')\nplt.legend()","03db71bb":"Estimate(df, 'texture_mean')","4563c402":"Estimate(m, 'texture_mean')\nprint('\\n')\nEstimate(df[df['diagnosis'] == 'B'], 'texture_mean')","a0431d10":"PlotSimulated(m, 'texture_mean')","70bef34b":"PlotSimulated(df[df['diagnosis'] == 'B'], 'texture_mean')","15aa0b01":"tmm = df[df['diagnosis'] == 'M']['texture_mean'].values\ntmb = df[df['diagnosis'] == 'B']['texture_mean'].values\n\ntest_stat, pval = stats.ttest_ind(tmm, tmb, equal_var = False)\nprint(f'{pval : .4f}')\nprint(test_stat)","f071be3d":"a, b = df['radius_mean'].values, df['texture_mean'].values \n\nsns.scatterplot(x = a, y = b)\nplt.xlabel('Radius Mean')\nplt.ylabel('Texture Mean')","6c0641e8":"def correlation(x, y):\n    return covariance(x, y) \/ (np.std(x) * np.std(y))\n\ndef covariance(x, y):\n    xbar = np.mean(x)\n    ybar = np.mean(y)\n    n = len(x)\n    \n    xs = [x_i - xbar for x_i in x]\n    ys = [y_i - ybar for y_i in y]\n    \n    return np.dot(xs, ys) \/ n","81c66984":"rm = df['radius_mean'].values\ntm = df['texture_mean'].values\n\nrm_scaled = [(rm_i - np.mean(rm)) \/ np.std(rm) for rm_i in rm]\ntm_scaled = [(tm_i - np.mean(tm)) \/ np.std(tm) for tm_i in tm]\n\nprint(correlation(rm_scaled, tm_scaled))\nprint(correlation(rm, tm))","dae7bd10":"def PearsonMedianSkewness(x):\n    median = np.median(x)\n    mean = np.mean(x)\n    std = np.std(x)\n    \n    gp = 3 * (mean - median) \/ std\n    \n    return gp","dc6c9483":"print(PearsonMedianSkewness(rm))\nprint(PearsonMedianSkewness(tm))","354e4b09":"def Percentile(sample, x):\n    count = 0\n    \n    for i in sample:\n        if i <= x:\n            count += 1\n    \n    return 100 * count \/ len(sample)","e5464a30":"rm_percentile = [Percentile(rm, x) for x in rm]\ntm_percentile = [Percentile(tm, x) for x in tm]\n\nprint(correlation(rm_percentile, tm_percentile))","2e120f6d":"test_stat, pval = stats.pearsonr(rm_percentile, tm_percentile)\n\nprint(test_stat)\nprint(f'{pval: .4f}')","7925f51b":"## KS test, Null Hypothesis: the two distributions are identical\n\nts_rm, pval_rm = stats.kstest(rm, 'norm')\nts_tm, pval_tm = stats.kstest(tm, 'norm')\n\nprint('KS for radius mean (p value): ' f'{pval_rm: .4f}')\nprint('KS for texture mean (p value): ' f'{pval_tm: .4f}')","b5889751":"## Anderson Darling test, Null Hypothesis: a sample is drawn from a population that follows a particular distribution.\n\nprint('KS for radius mean: ', stats.anderson(rm, 'norm'), '\\n')\nprint('KS for texture mean: ', stats.anderson(tm, 'norm'))","8210bd3a":"stats.probplot(rm, plot = plt)","f204b3a7":"stats.probplot(tm, plot = plt)","091cda0a":"from sklearn.linear_model import LinearRegression\n\nX = df[df['diagnosis'] == 'B']['radius_mean'].values\ny = df[df['diagnosis'] == 'B']['texture_mean'].values\n\nlr = LinearRegression().fit(X.reshape(-1, 1), \n                            y.reshape(-1, 1))\nprint(lr.coef_, lr.intercept_)","a0d06be8":"X = df[df['diagnosis'] == 'M']['radius_mean'].values\ny = df[df['diagnosis'] == 'M']['texture_mean'].values\n\nlr = LinearRegression().fit(X.reshape(-1, 1), \n                            y.reshape(-1, 1))\nprint(lr.coef_, lr.intercept_)","7cf37589":"plt.text(0.1, 0.8, 'RADIUS MEAN', color = 'maroon')\nplt.text(0.7, 0.8, 'TEXTURE MEAN', color = 'maroon')\nplt.text(0.015, 0.7, 'MALIGNANT', color = 'indigo')\nplt.text(0.3, 0.7, 'BENIGN', color = 'indigo')\nplt.text(0.6, 0.7, 'MALIGNANT', color = 'indigo')\nplt.text(0.9, 0.7, 'BENIGN', color = 'indigo')\n\nplt.text(0.0, 0.6, 'POPULATION MEAN', color = 'green', fontsize = 'x-small')\nplt.text(0.25, 0.6, 'POPULATION MEAN', color = 'green', fontsize = 'x-small')\nplt.text(0.58, 0.6, 'POPULATION MEAN', color = 'green', fontsize = 'x-small')\nplt.text(0.85, 0.6, 'POPULATION MEAN', color = 'green', fontsize = 'x-small')\n\nplt.text(0.1, 0.55, 17.46, color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.35, 0.55, 12.14, color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.68, 0.55, 21.60, color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.95, 0.55, 17.91, color = 'grey', fontsize = 'x-small', ha = 'center')\n\nplt.text(0.0, 0.47, 'STANDARD ERROR', color = 'green', fontsize = 'x-small')\nplt.text(0.25, 0.47, 'STANDARD ERROR', color = 'green', fontsize = 'x-small')\nplt.text(0.58, 0.47, 'STANDARD ERROR', color = 'green', fontsize = 'x-small')\nplt.text(0.85, 0.47, 'STANDARD ERROR', color = 'green', fontsize = 'x-small')\n\nplt.text(0.1, 0.42, 0.97, color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.35, 0.42, 0.54, color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.68, 0.42, 0.14, color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.95, 0.42, 1.19, color = 'grey', fontsize = 'x-small', ha = 'center')\n\n\nplt.text(0.0, 0.34, 'CONFIDENCE INTERVAL', color = 'green', fontsize = 'x-small')\nplt.text(0.25, 0.34, 'CONFIDENCE INTERVAL', color = 'green', fontsize = 'x-small')\nplt.text(0.58, 0.34, 'CONFIDENCE INTERVAL', color = 'green', fontsize = 'x-small')\nplt.text(0.85, 0.34, 'CONFIDENCE INTERVAL', color = 'green', fontsize = 'x-small')\n\nplt.text(0.1, 0.29, '15.88 - 19.01', color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.35, 0.29, '11.18 - 13.03', color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.68, 0.29, '19.74 - 23.51', color = 'grey', fontsize = 'x-small', ha = 'center')\nplt.text(0.95, 0.29, '16.05 - 20.06', color = 'grey', fontsize = 'x-small', ha = 'center')\n\n#plt.vlines(x = (0.25, 0.85), ymin = 0.2, ymax = 0.7)\n\nplt.title('BREAST CANCER FACT CHECK', family = 'monospace', fontsize = 'xx-large')\nplt.axis('off')","52097465":"The p value is 0, demonstrating that the correlation is neither greater than nor is it equal to the test statistic... Therefore, we can conclude that the correlation between both the variables is scarce.","c3409a8d":"## Sampling Distributions","4b0ce07d":"As is evident, there's not much difference between the correlation of the scaled and the unscaled data. It is because both have the same unit. But if we use different units for both, say, cm for radius mean and m fot texture mean, we must observe a drastic change.","d99d850a":"Another assumption for the 'Pearson's correlation' is that the variables are normally distributed. But this is not entirely true for both the variables. Let's see the magnitude of skewness in both the variables. For that I use 'Pearson's median skewness'.","80d701e6":"## Sampling Distributions","8eab966a":"Now I'll employ linear regression for model explaination.","d53637c3":"With the range of opportunities that the dataset bequeaths, it is important to take full advantage of it, in this regard, I present a very short but picturesque research using 2 variables\/columns; \"radius_mean\" and the \"texture_mean\", verbatim","209495ef":"# 6. Epilogue","bdf6f14c":"For both 'Malignant' and 'Benign' categories, sample mean is the MLE","8a50e77e":"## MLE","7b209796":"I'll not write the conclusions as, they must be understood by the reader as of now.","d414480a":"From the scatter plot, it occurs that there's a little or less correlation between both the variables","92aee6b4":"The same can be interpreted for the 'Malignant' cases.","0f7096ba":"# 5. Reporting Final Results","642f186f":"# 2. Analyzing the Texture Mean variable","5343a8fe":"Even though it is now established that neither the 'radius mean' and nor the 'texture means' are perfectly normally distributed, we can however perform some tests to verify this claim as well. I'll use the following tests to find out;\n\n1. KS test\n2. Anderson Darling test\n3. Normal probability plot","ee8af410":"# 1. Analyzing the Radius Mean variable","b5c446a2":"Generally, whenever we estimate some statistic, we report 2 other statistics with it, which are;\n\n1. ***Standard Error:*** If we run the experiment again again, or in this case, if we compute the mean of the radius mean, everytime surveying different patients (sample), how much do we expect the mean to deviate? This deviation is called the standard error\n\n2. ***Confidence Interval:*** If we run the experiment again and again, what is the expected range of the estimator? This range is known as the confidence interval","fa9b2c9e":"Even though I have tried to write the description of most of what I have done, nevertheless it is does not engulf the basic knowhow of what these concepts pertain. For some concepts, I didn't even write a description (which I might do in the future). Therefore, it would be wise for the young and inexperienced reader to search about all these concepts on the internet. Moreover, whatever I have done is subject to improvement (as more experienced people might say), however, I have made a balance between very basic and scarcely advanced analysis, this is in part because when you have been expounding on these topics for quite a while, you at some point get bored. \n\nI am also open for active criticism of my notebooks.\n\nI remain","8e113103":"# Disclaimer","7789c947":"Lastly, I articulate some of the most relevant facts (for a wider audience) using a text graph","edcaacdb":"The data that we have is sample taken from a larger population. Therefore, it is not wise to assume the sample mean as the population mean. This is in part, also because our distribution demonstrates outliers which might affect the sample mean. In that case, the median would be the true representation of the population mean. But how do we conclude whether the mean or the median is the best suited? For that I use the method of ***estimation***.","90ba2612":"There is a difference of almost 2 between the low and high ends of the CI for the 'Benign' cases, however, the difference between it's 'Malignant' counterpart is almost 4. Therefore, I have no hesitation in concluding that the sample mean of the radius mean for the 'Malignant' cases is a rough estimate as compared to 'Benign'. ","ae208295":"The distribution, though resembling the normal distribution is skewed to the right... We'll foster it again anon. But how does the distribution vary for the 'Malignant' and the 'Benign' categories? Let's have a look","20c61f8f":"The coefficient and intercept correspond to the slope and the intercept of the linear regression model. A coefficient of \"-0.0833\" means that with a unit change in 'radius mean' (feature) results in a decrease of 0.0833... units in the texture mean (target). In ML, a feature variable is generally denoted with 'X', whereas the target variable is denoted by 'y'.","f297c692":"In the beginning of the analysis, I concluded that the mean of the radius mean for the 'Malignant' cases is greater than that of its 'Benign' counterparts. However, there's a possibility of this effect occuring by chance. To investigate whether the effect is true for the population or has occured by chance, we use ***'Hypothesis Testing'.***","aa4383fe":"Briefly speaking, estimation is the process of running the experiment again and again, each time taking a sample from the data and estimating both its sample mean and median and remembering those values, thereafter taking the RMSE (root mean squared error) of both.","42039b78":"## Correlation","8be513d9":"The correlation is still almost the same... which is pretty less, which means that the 2 variables are scarcely correlated.\n\nBut is this effect representative of the population or is it merely by chance? I use scipy's 'pearsonr' function to test the hypothesis","efd0a851":"## Maximum Likelihood Estimator ","62dc81b1":"From the above analysis, it's clear that most of the dataset is free from null values except for the \"Unnamed:32\" variable, and that all of the values are floating point numbers (except of course, the diagnosis variable). However, we won't need the 'Unnamed: 32' variable, so I remove it","bf1d4420":"I use a method called 'T-test independent'. I employ the scipy's stats package for that.\n\nIt's documentation reads, **\"This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values\".**","8fa9b6d5":"## Hypothesis Testing","da78433c":"The pvalue turns out to be 0, which means that we can reject the null hypothesis, thus concluding that the means of the radius mean of both the distributions are not equal. The positive test statistic demonstrates that the mean of the 'Malignant' distribution is greater than that of its 'Benign' counterpart.","caea9ff6":"As we begin univariate analysis; the best way according to me is to plot its (variable's) histogram which helps us visualizing the distribution of the variable","8b8640b7":"# 3. Bivariate Analysis","a97d9d2c":"To visualize relationships, scatter plots are a good way begin with","176a953c":"Now that we have successfully analyzed both the variables individually, it's time for bivariate analysis, that is comparing the variables and finding relationships.","14cbae4b":"Another method to evaluate the relationship between the 2 variables is correlation. Since, there seems to a linear relationship, however small, between both the variables; I employ the 'Pearson's correlation'. Because it is duly affected by the scale of the data, it is wise to standardize both the variables","c7f27fea":"It is quite clear and quite obvious that the distribution for the 'Malignant' cases is skewed far more right than that for the 'Benign' cases. The mean of the radius mean of the 'Malignant' cases are thus, understandably so, outdoes its 'Benign' counterpart.","0c8d6497":"For radius mean, the RMSE for the sample mean (xbar) is lesser than that of the median. Therefore, we can say that the sample mean is the true representation of the population mean (mu). Hence, xbar is the ***'maximum likelihood estimator'*** (MLE) for the population mean.","42c07475":"## Tests for normality","e9deb366":"# Hypothesis Testing","364a9ce5":"Seems like both the variables are somewhat skewed to the right. In that case, we use 'percentiles' instead of scaled data for finding correlation.","b27810eb":"From the description of the data, I was acknowledged that the 'radius_mean' is the \"radius (mean of distances from center to points on the perimeter)\", whereas the 'texture_mean' is \"texture (standard deviation of gray-scale values)\" (gray scale values have pixel values within the range of 0 and 1).","0d81558d":"# 4. Linear Regression"}}