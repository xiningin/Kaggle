{"cell_type":{"c838d9ef":"code","0c247d1a":"code","c3d8b4ef":"code","996b316f":"code","25c0c327":"code","31964fa8":"code","645da40f":"code","4fe70e78":"code","01aa5ab7":"code","b27ceb3d":"code","2f435c62":"code","35ca62c7":"code","f720aeff":"code","ba615474":"code","0f74cc3f":"code","e53c0b1c":"code","daca14c7":"code","17583552":"code","bf59b5a7":"markdown","d53417d5":"markdown","9d13a74e":"markdown","fdd7ec30":"markdown","aba58f6f":"markdown","d37983f2":"markdown"},"source":{"c838d9ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c247d1a":"# df = pd.read_csv(\"..\/input\/unfiltered-raw-data-2021\/query_result.csv\",usecols = [\"community_id\",\"city_id\",\"bintix_uid\",\"pickup_date\",\"barcode\",\"quantity\",\"brand\",\"sub_brand\",\"variant\",\"attribute1\",\"attribute2\",\"attribute3\",\"price\",\"cat\",\"scat\",\"sscat\",\"std_unit_value\",\"std_unit\"])\n# df.loc[:,\"pickup_date\"]=pd.to_datetime(df.pickup_date)\n# df.loc[:,\"year\"] = df.pickup_date.dt.year\n# df = df.loc[df.year==2021]\n# df.loc[:,\"month\"] = df.pickup_date.dt.month\n# df.std_unit_value = pd.to_numeric(df['std_unit_value'],errors='coerce')\n# df.loc[:,\"volume\"] = df.std_unit_value*df.quantity\n# tghh_list=[552359,1092982,1246361,1082321,898693,619080,553601.6746,1096216,1249476.982,1083511.247,899322.0571,620317.9703,554847,1099459,1252601,1084703,899952,621559,556096,1102711.188,1255732.177,1085896.283,900581.5486,622801.7234,557347,1105973,1258872,1087091,901212,624047,558600.9306,1109245.213,1262018.686,1088286.569,901842.8041,625295.4215]\n# counter_for_tghh_set=0\n# for month in [7,8,9,10,11,12]:\n#     for city in [1,2,3,4,5,6]:\n#         df.loc[(df.city_id==city) & (df.month==month),\"tghh\"] = tghh_list[counter_for_tghh_set]\n#         counter_for_tghh_set+=1\n# df.loc[:,\"month_adj\"] = df.month\n","c3d8b4ef":"# df = pd.read_csv(\"..\/input\/allcities-jan5\/allcities_Jan5.csv\", encoding='latin-1')\ndf = pd.read_csv(\"..\/input\/allcities-jan17\/allcities_jan17_RTEC.csv\",encoding='latin-1')\n","996b316f":"# df.drop(columns = \"volume\",inplace=True)\ndf.rename(columns={\"binvalue\":\"volume\"},inplace=True)","25c0c327":"df.volume","31964fa8":"x =df.loc[(df.city_id==5) & (df.month_adj==26)]\nx.loc[:,\"month_adj\"] =25\ndf = pd.concat([df,x])","645da40f":"# df_butter = pd.read_csv(\"..\/input\/allcities-jan5-butter\/allcities_Jan5_butter.csv\")","4fe70e78":"# df = df.loc[df.sscat!=\"BUTTER\"]\n# df = pd.concat([df,df_butter])\n# df.shape\n# df.loc[:,\"sub_brand\"]=df.variant","01aa5ab7":"# df.loc[df.brand==\"PATANJALI\",\"cat\"]=\"BAKERY, CAKES & DAIRY\"# df.loc[(df.brand==\"NUTRALITE\") &( df.sscat ==\"CHOCOLATE SPREAD\"),\"sub_brand\"] = \"CHOCO SPREAD\"","b27ceb3d":"# df.loc[(df.brand==\"NUTRALITE\") &( df.sscat ==\"CHOCOLATE SPREAD\")]\n","2f435c62":"def EER_analysis(input_file,p1_start_month,p1_end_month,p2_start_month,p2_end_month,brand,sub_brand,city_id_list):\n    if len(city_id_list)==1:\n        p1 = list(range(p1_start_month,p1_end_month+1))\n        p2 = list(range(p2_start_month,p2_end_month+1))\n        months_to_include = p1+p2\n\n        only_city_filtered = input_file.loc[input_file.city_id.isin(city_id_list)]\n\n#         sample_bintix_uid = set(only_city_filtered.loc[only_city_filtered.month_adj.isin(p1[:2])].bintix_uid.unique()).intersection(set(only_city_filtered.loc[only_city_filtered.month_adj.isin(p2[-3:])].bintix_uid.unique()))\n\n#         sample_bintix_uid = set(only_city_filtered.loc[only_city_filtered.month_adj.isin(months_to_include[:1])].bintix_uid.unique())\n#         for a in months_to_include[1:]:\n#             sample_bintix_uid = sample_bintix_uid.intersection(set(only_city_filtered.loc[only_city_filtered.month_adj==a].bintix_uid.unique()))\n#use this \n        p1_month_presesnt_for_sample = only_city_filtered.loc[only_city_filtered.month_adj.isin(p1)].groupby(\"bintix_uid\").month_adj.nunique().reset_index()\n        p2_month_presesnt_for_sample = only_city_filtered.loc[only_city_filtered.month_adj.isin(p2)].groupby(\"bintix_uid\").month_adj.nunique().reset_index()\n        p1_month_present_filter = p1_month_presesnt_for_sample.loc[p1_month_presesnt_for_sample.month_adj>=1]\n        p2_month_present_filter =  p2_month_presesnt_for_sample.loc[p2_month_presesnt_for_sample.month_adj>=1]\n        sample_bintix_uid = set(p1_month_present_filter.bintix_uid.unique()).intersection(set(p2_month_present_filter.bintix_uid.unique()))\n        \n        target_household= only_city_filtered.loc[only_city_filtered.month_adj.isin(months_to_include)].groupby('month_adj').tghh.first().mean()\n        factor = target_household\/len(sample_bintix_uid)\n\n        \n        filtered = only_city_filtered.loc[(only_city_filtered.month_adj.isin(months_to_include)) & (only_city_filtered.brand == brand) & (only_city_filtered[\"sub_brand\"] == sub_brand) ,[\"bintix_uid\",\"month_adj\",\"volume\",\"tghh\"]]\n        filtered = filtered.loc[filtered.bintix_uid.isin(sample_bintix_uid)]\n        filtered['period'] = np.where(filtered['month_adj'].isin(p1), \"p1\", \"p2\")\n        grouped = filtered.groupby(['bintix_uid','period']).volume.sum().reset_index().sort_values(by = \"period\")\n        uid_p1 = list(grouped.loc[grouped.period=='p1']['bintix_uid'])\n        uid_p2 = list(grouped.loc[grouped.period=='p2']['bintix_uid'])\n        grouped['retained_lapse_new'] = np.where((grouped['period'] == 'p2') & (grouped['bintix_uid'].isin(uid_p1)), \"retained\", \"new\")\n        grouped.loc[grouped['period']=='p1',['retained_lapse_new']] = np.where(grouped.loc[grouped['period']=='p1',['bintix_uid']].isin(uid_p2), \"retained\", \"lapse\")\n        retained_count = grouped.loc[(grouped.period=='p1') & (grouped.retained_lapse_new==\"retained\")].shape[0]\n        new_count = grouped.loc[grouped.retained_lapse_new==\"new\"].shape[0]\n        lapse_count = grouped.loc[grouped.retained_lapse_new==\"lapse\"].shape[0]\n        if retained_count+lapse_count != 0:\n            percent_retained = round((retained_count*100)\/(retained_count+lapse_count))\n            percent_lapse = 100-percent_retained\n            percent_new=round((new_count*100)\/(retained_count+lapse_count))\n        else:#check this again\n            percent_retained = \"NA\"\n            percent_lapse = \"NA\"\n            percent_new = \"NA\"\n        avg_cons_retained_p1 = round(grouped.loc[(grouped.period=='p1')&(grouped.retained_lapse_new==\"retained\")].volume.mean()\/len(p1),2)\n        avg_cons_retained_p2 = round(grouped.loc[(grouped.period=='p2')&(grouped.retained_lapse_new==\"retained\")].volume.mean()\/len(p2),2)\n        avg_cons_lapse = round(grouped.loc[grouped.retained_lapse_new==\"lapse\"].volume.mean()\/len(p1),2)\n        avg_cons_new = round(grouped.loc[grouped.retained_lapse_new==\"new\"].volume.mean()\/len(p2),2)\n\n        projected_households_p1=round(factor*(retained_count+lapse_count))\n        projected_households_p2=round(factor*(retained_count+new_count))\n        brand_vol_change = round((grouped.loc[grouped.period == \"p2\"].volume.sum() - grouped.loc[grouped.period == \"p1\"].volume.sum())*factor\/1000000,2)\n        vol_change_retainer = round((grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"retained\")].volume.sum() - grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"retained\")].volume.sum())*factor\/1000000,2)\n        vol_change_new_lapse = round((grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"new\")].volume.sum() - grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"lapse\")].volume.sum())*factor\/1000000,2)\n\n        ref_brand = brand+\"-\"+sub_brand\n        month_num_dict = {1:\"JAN\",2:\"FEB\",3:\"MAR\",4:\"APR\",5:\"MAY\",6:\"JUN\",7:\"JUL\",8:\"AUG\",9:\"SEP\",10:\"OCT\",11:\"NOV\",0:\"DEC\"}\n        p1_start_string = month_num_dict[p1_start_month%12]+str(19+(p1_start_month-1)\/\/12)\n        p1_end_string = month_num_dict[p1_end_month%12]+str(19+(p1_end_month-1)\/\/12)\n        p2_start_string = month_num_dict[p2_start_month%12]+str(19+(p2_start_month-1)\/\/12)\n        p2_end_string = month_num_dict[p2_end_month%12]+str(19+(p2_end_month-1)\/\/12)\n        time_period_string = p1_start_string+\"-\"+p1_end_string+\"_vs_\"+p2_start_string+\"-\"+p2_end_string\n        \n        \n        \n        data = [ref_brand,time_period_string,target_household,projected_households_p1,projected_households_p2,percent_retained,avg_cons_retained_p1,avg_cons_retained_p2,percent_lapse,avg_cons_lapse,percent_new,avg_cons_new,brand_vol_change,vol_change_new_lapse,vol_change_retainer]\n        return pd.DataFrame(data,index = [\"Ref brand:\",\"Time_period:\",\"TG HH\",\n    \"Projected HH - In P1\",\n    \"Projected HH - In P2\",\n    \"Retained HH (% on P1)\",\n    \"Avg Cons  P1  (Gms\/HH\/month)\",\n    \"Avg Cons  P2  (Gms\/HH\/month)\",\n    \"Lapsers In P2 (% on P1)\",\n    \"Avg Cons  P1  (Gms\/HH\/month)\",\n    \"New Triers In P2 (% on P1)\",\n    \"Avg Cons  P2  (Gms\/HH\/month)\",\n    \"Changes In Brand Volumes(in 000Kgs)\",\n    \"Due to New Triers-Lapsers (in 000Kgs)\",\n    \"Due to Retained buyers (in 000Kgs)\"])\n    \n    else:\n        p1 = list(range(p1_start_month,p1_end_month+1))\n        p2 = list(range(p2_start_month,p2_end_month+1))\n        months_to_include = p1+p2\n\n        only_city_filtered = input_file.loc[input_file.city_id.isin(city_id_list)]\n\n#         sample_bintix_uid = set(only_city_filtered.loc[only_city_filtered.month_adj.isin(p1[:2])].bintix_uid.unique()).intersection(set(only_city_filtered.loc[only_city_filtered.month_adj.isin(p2[-3:])].fbintix_uid.unique()))\n#         target_household= filtered.loc[filtered.month_adj.isin(months_to_include)].groupby('month_adj').tghh.first().mean()\n#         factor = target_household\/len(sample_bintix_uid)\n        \n#         sample_bintix_uid = set(only_city_filtered.loc[only_city_filtered.month_adj.isin(months_to_include[:1])].bintix_uid.unique())\n#         for a in months_to_include[1:]:\n#             sample_bintix_uid = sample_bintix_uid.intersection(set(only_city_filtered.loc[only_city_filtered.month_adj==a].bintix_uid.unique()))\n#use this\n        p1_month_presesnt_for_sample = only_city_filtered.loc[only_city_filtered.month_adj.isin(p1)].groupby(\"bintix_uid\").month_adj.nunique().reset_index()\n        p2_month_presesnt_for_sample = only_city_filtered.loc[only_city_filtered.month_adj.isin(p2)].groupby(\"bintix_uid\").month_adj.nunique().reset_index()\n        p1_month_present_filter = p1_month_presesnt_for_sample.loc[p1_month_presesnt_for_sample.month_adj>=1]\n        p2_month_present_filter =  p2_month_presesnt_for_sample.loc[p2_month_presesnt_for_sample.month_adj>=1]\n        sample_bintix_uid = set(p1_month_present_filter.bintix_uid.unique()).intersection(set(p2_month_present_filter.bintix_uid.unique()))\n        \n        \n        filtered = only_city_filtered.loc[(only_city_filtered.month_adj.isin(months_to_include)) & (only_city_filtered.brand == brand) & (only_city_filtered[\"sub_brand\"] == sub_brand) ,[\"bintix_uid\",\"month_adj\",\"volume\",\"tghh\",\"city_id\"]]\n        filtered = filtered.loc[filtered.bintix_uid.isin(sample_bintix_uid)]\n        filtered['period'] = np.where(filtered['month_adj'].isin(p1), \"p1\", \"p2\")\n        grouped = filtered.groupby(['bintix_uid','period']).volume.sum().reset_index().sort_values(by = \"period\")\n        uid_p1 = list(grouped.loc[grouped.period=='p1']['bintix_uid'])\n        uid_p2 = list(grouped.loc[grouped.period=='p2']['bintix_uid'])\n        grouped['retained_lapse_new'] = np.where((grouped['period'] == 'p2') & (grouped['bintix_uid'].isin(uid_p1)), \"retained\", \"new\")\n        grouped.loc[grouped['period']=='p1',['retained_lapse_new']] = np.where(grouped.loc[grouped['period']=='p1',['bintix_uid']].isin(uid_p2), \"retained\", \"lapse\")\n#         retained_count = grouped.loc[(grouped.period=='p1') & (grouped.retained_lapse_new==\"retained\")].shape[0]\n#         new_count = grouped.loc[grouped.retained_lapse_new==\"new\"].shape[0]\n#         lapse_count = grouped.loc[grouped.retained_lapse_new==\"lapse\"].shape[0]\n#         if retained_count+lapse_count != 0:\n#             percent_retained = round((retained_count*100)\/(retained_count+lapse_count))\n#             percent_lapse = 100-percent_retained\n#             percent_new=round((new_count*100)\/(retained_count+lapse_count))\n#         else:#check this again\n#             percent_retained = \"NA\"\n#             percent_lapse = \"NA\"\n#             percent_new = \"NA\"\n#         avg_cons_retained_p1 = round(grouped.loc[(grouped.period=='p1')&(grouped.retained_lapse_new==\"retained\")].volume.mean()\/len(p1),2)\n#         avg_cons_retained_p2 = round(grouped.loc[(grouped.period=='p2')&(grouped.retained_lapse_new==\"retained\")].volume.mean()\/len(p2),2)\n#         avg_cons_lapse = round(grouped.loc[grouped.retained_lapse_new==\"lapse\"].volume.mean()\/len(p1),2)\n#         avg_cons_new = round(grouped.loc[grouped.retained_lapse_new==\"new\"].volume.mean()\/len(p2),2)\n\n#         projected_households_p1=round(factor*(retained_count+lapse_count))\n#         projected_households_p2=round(factor*(retained_count+new_count))\n#         brand_vol_change = round((grouped.loc[grouped.period == \"p2\"].volume.sum() - grouped.loc[grouped.period == \"p1\"].volume.sum())*factor\/1000000,2)\n#         vol_change_retainer = round((grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"retained\")].volume.sum() - grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"retained\")].volume.sum())*factor\/1000000,2)\n#         vol_change_new_lapse = round((grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"new\")].volume.sum() - grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"lapse\")].volume.sum())*factor\/1000000,2)\n        target_household = 0\n        projected_households_p1 = 0\n        projected_households_p2 = 0\n        brand_vol_change=0\n#         vol_change_retainer=0\n#         vol_change_new_lapse=0\n        vol_retainer_p2 = 0\n        vol_retainer_p1 = 0\n        vol_new_p2 = 0\n        vol_lapse_p1 =0\n        retained_count = 0\n        new_count = 0\n        lapse_count = 0\n        for c in city_id_list:\n            target_household_city= input_file.loc[(input_file.month_adj.isin(months_to_include)) & (input_file.city_id==c)].groupby('month_adj').tghh.first().mean()\n            sample_city = input_file.loc[(input_file.city_id==c) & (input_file.bintix_uid.isin(sample_bintix_uid)) & (input_file.month_adj.isin(months_to_include))].bintix_uid.unique()\n            sample_size_city = len(sample_city)\n            factor_city = target_household_city \/ sample_size_city\n            \n            target_household += target_household_city\n            projected_households_p1+= factor_city*len(filtered.loc[(filtered.city_id==c)&(filtered.period==\"p1\")].bintix_uid.unique())\n            projected_households_p2+= factor_city*len(filtered.loc[(filtered.city_id==c)&(filtered.period==\"p2\")].bintix_uid.unique())\n            brand_vol_change += (grouped.loc[(grouped.period == \"p2\") & (grouped.bintix_uid.isin(sample_city))].volume.sum() - grouped.loc[(grouped.period == \"p1\") & (grouped.bintix_uid.isin(sample_city)) ].volume.sum())*factor_city\/1000000\n            vol_retainer_p2+= (grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"retained\") & (grouped.bintix_uid.isin(sample_city))].volume.sum())*factor_city\/1000000\n            vol_retainer_p1 += (grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"retained\") & (grouped.bintix_uid.isin(sample_city))].volume.sum())*factor_city\/1000000      \n            vol_new_p2 += (grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"new\")& (grouped.bintix_uid.isin(sample_city))].volume.sum())  *factor_city\/1000000\n            vol_lapse_p1+= (grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"lapse\")& (grouped.bintix_uid.isin(sample_city))].volume.sum())*factor_city\/1000000\n            retained_count+=(grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"retained\") & (grouped.bintix_uid.isin(sample_city))].bintix_uid.nunique())*factor_city\n            new_count+=(grouped.loc[(grouped.period == \"p2\") & (grouped.retained_lapse_new == \"new\")& (grouped.bintix_uid.isin(sample_city))].bintix_uid.nunique())  *factor_city\n            lapse_count+=(grouped.loc[(grouped.period == \"p1\") & (grouped.retained_lapse_new == \"lapse\")& (grouped.bintix_uid.isin(sample_city))].bintix_uid.nunique())*factor_city\n        \n        vol_change_retainer = vol_retainer_p2-vol_retainer_p1\n        vol_change_new_lapse = vol_new_p2 - vol_lapse_p1\n        if retained_count+lapse_count != 0:\n            percent_retained = round((retained_count*100)\/(retained_count+lapse_count))\n            percent_lapse = 100-percent_retained\n            percent_new=round((new_count*100)\/(retained_count+lapse_count))\n        else:#check this again\n            percent_retained = \"NA\"\n            percent_lapse = \"NA\"\n            percent_new = \"NA\"\n        if retained_count!=0 :    \n            avg_cons_retained_p1 = (vol_retainer_p1\/retained_count)*1000000\/len(p1)\n            avg_cons_retained_p2 = (vol_retainer_p2\/retained_count)*1000000\/len(p2)\n        else:\n            avg_cons_retained_p1 = 0\n            avg_cons_retained_p2 = 0\n        if lapse_count!=0:     \n            avg_cons_lapse =  (vol_lapse_p1\/ lapse_count)*1000000\/len(p1)\n        else:\n            avg_cons_lapse=0\n        if new_count!=0:\n            avg_cons_new = (vol_new_p2\/new_count)*1000000\/len(p2)\n        else:\n            avg_cons_new=0\n   \n            \n        \n        ref_brand = brand+\"-\"+sub_brand\n        month_num_dict = {1:\"JAN\",2:\"FEB\",3:\"MAR\",4:\"APR\",5:\"MAY\",6:\"JUN\",7:\"JUL\",8:\"AUG\",9:\"SEP\",10:\"OCT\",11:\"NOV\",0:\"DEC\"}\n        p1_start_string = month_num_dict[p1_start_month%12]+str(19+(p1_start_month-1)\/\/12)\n        p1_end_string = month_num_dict[p1_end_month%12]+str(19+(p1_end_month-1)\/\/12)\n        p2_start_string = month_num_dict[p2_start_month%12]+str(19+(p2_start_month-1)\/\/12)\n        p2_end_string = month_num_dict[p2_end_month%12]+str(19+(p2_end_month-1)\/\/12)\n        time_period_string = p1_start_string+\"-\"+p1_end_string+\"_vs_\"+p2_start_string+\"-\"+p2_end_string\n        \n        \n        data = [ref_brand,time_period_string,target_household,projected_households_p1,projected_households_p2,percent_retained,avg_cons_retained_p1,avg_cons_retained_p2,percent_lapse,avg_cons_lapse,percent_new,avg_cons_new,brand_vol_change,vol_change_new_lapse,vol_change_retainer]\n        return pd.DataFrame(data,index = [\"Ref_brand:\",\"Time_period:\",\"TG HH\",\n    \"Projected HH - In P1\",\n    \"Projected HH - In P2\",\n    \"Retained HH (% on P1)\",\n    \"Avg Cons  P1  (Gms\/HH\/month)\",\n    \"Avg Cons  P2  (Gms\/HH\/month)\",\n    \"Lapsers In P2 (% on P1)\",\n    \"Avg Cons  P1  (Gms\/HH\/month)\",\n    \"New Triers In P2 (% on P1)\",\n    \"Avg Cons  P2  (Gms\/HH\/month)\",\n    \"Changes In Brand Volumes(in 000Kgs)\",\n    \"Due to New Triers-Lapsers (in 000Kgs)\",\n    \"Due to Retained buyers (in 000Kgs)\"])\n    \n    ","35ca62c7":"# brand = \"KELLOGG'S\"\n# sub_brand = \"CHOCOS\"\np1_start= 25\np1_end=30\np2_start=31\np2_end=36\ncomplete_data= df\ncity_name_list =[([1,2,3,4,5,6],\"6Metros\"),([1],\"Hyd\"),([2],\"Blr\"),([3],\"Del\"),([4],\"Mum\"),([5],\"Kol\"),([6],\"Che\")] #city\/city combination for sheet generation\n# ([1,2,3,4,5,6],\"6Metros\"),([1],\"Hyd\"),([2],\"Blr\"),([3],\"Del\"),([4],\"Mum\"),([5],\"Kol\"),([6],\"Che\")","f720aeff":"#new code for kellogg's \nkelloggs_template_gnl = [\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"BAR\"],[\"variant\",\"COOKIES'N'CREME\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"BAR\"],[\"variant\",\"CREAMY MILK\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"BAR\"],[\"variant\",\"WHOLE ALMONDS\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"variant\",\"ASSORTMENT\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"variant\",\"BLUEBERRY & ACAI\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"variant\",\"CALIFORNIAN ALMONDS WITH BLACKBERRY\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"variant\",\"CALIFORNIAN ALMONDS WITH GUAVA & MEXICAN CHILLI\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"variant\",\"POMEGRANATE\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"variant\",\"RASPBERRY & GOJI\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"KISSES\"],[\"variant\",\"ALMONDS\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"KISSES\"],[\"variant\",\"COOKIES'N'CREME\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"KISSES\"],[\"variant\",\"ASSORTED\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"KISSES\"],[\"variant\",\"ASSORTMENT\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n#     [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"KISSES\"],[\"variant\",\"MILK CHOCOLATE\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]]\n                         \n    \n    [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"KISSES\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"HERSHEY'S INDIA\"],[\"sub_brand\",\"EXOTIC DARK\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"DAIRY MILK SILK\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"DAIRY MILK\"],[\"sub_brand\",\"CRISPELLO\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"SNICKERS\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"KITKAT\"],[\"sub_brand\",\"DESSERT DELIGHT\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"BOURNVILLE\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"AMUL\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"FERRERO\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"DAIRY MILK\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]],\n    [[\"brand\",\"GALAXY\"],[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]]\n    \n# [[\"brand\",\"KELLOGG'S\"],[\"scat\",\"READY TO EAT CEREALS\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CORN FLAKES\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CORN FLAKES\"],[\"variant\",\"REAL ALMOND & HONEY\"]],\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CORN FLAKES\"],[\"variant\",\"ORIGINAL\"]],\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CHOCOS\",\"FROOT LOOPS INDIA\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CHOCOS\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"CHOCOS\"],[\"variant\",False,\"FILLS\"]], #for excluding use False\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"FROOT LOOPS INDIA\"]] ,  #too little data. \n# [[\"brand\",\"KELLOGG'S\"],[\"attribute1\",\"FRIENDS OF CHOCOS\"]], \n# [[\"brand\",\"KELLOGG'S\"],[\"sscat\",\"MUESLI & GRANOLA\"]] ,\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"MUESLI\"]] \n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"GRANOLA\"]] #, # use new data. older data has the name - CRUNCHY GRANOLA\n# [[\"sscat\",\"OATS & PORRIDGE\"],[\"sub_brand\",\"OATS\"]] ,#if no brand then keep first indicator which you want to use as a brand\n# [[\"brand\",\"KELLOGG'S\"],[\"sub_brand\",\"OATS\"]] ,\n# [[\"brand\",\"SOULFULL\"], [\"sub_brand\",\"MUESLI\"]] ,\n# [[\"brand\",\"SOULFULL\"],[\"sub_brand\",\"RAGI BITES\"]],\n# [[\"brand\",\"YOGA BAR\"],[\"sub_brand\",\"MUESLI\"]],\n# [[\"brand\",\"BAGRRY'S\"],[\"sub_brand\",\"MUESLI\"]] ,\n# [[\"brand\",\"BAGRRY'S\"],[\"sub_brand\",\"CORN FLAKES\"]]\n]\n\n# kelloggs_template_gnl =[ [[\"brand\",\"NUTRALITE\"],[\"sub_brand\",\"DOODH SHAKTI\"]] ]\ndef only_strings_list_from_1(any_list):\n    return [x for x in any_list[1:] if type(x) == str ]\n\ndef replace_false_with_ex(any_list):\n    return [\"excluding\" if x==False else x for x in any_list]\n\ndef adding_quotes_to_string(string):\n    return \"\\\"\" + string + \"\\\"\"\n\ndef new_sub_brand_name(indicator):\n    indicator_with_excl = [replace_false_with_ex(a) for a in indicator ]\n    return \"-\".join([\"_\".join([indicator_with_excl[j][i] for i in range(1,len(indicator_with_excl[j]))]) for j in range(1,len(indicator_with_excl)) ])\n\ndef new_brand_name(indicator):\n    return \"_\".join([indicator[0][i] for i in range(1,len(indicator[0]))])\n\ndef filters_for_loc(indicator):\n    fl=[]\n    for condition in indicator:\n        if condition[1]:\n            fl += [ \"(data.\" + condition[0] + \".isin([\" + \",\".join([adding_quotes_to_string(x) for x in only_strings_list_from_1(condition)]) +\"]))\" ]\n        else:\n            fl+= [\"~(data.\" + condition[0] + \".isin([\" + \",\".join([adding_quotes_to_string(x) for x in only_strings_list_from_1(condition)]) +\"]))\"]\n    return \"&\".join(fl)\n\ndef new_names_data_frame(indicator,data):\n    #don't change the name data\n    data.loc[eval(filters_for_loc(indicator)),\"brand\"] = new_brand_name(indicator)\n    data.loc[eval(filters_for_loc(indicator)),\"sub_brand\"] = new_sub_brand_name(indicator)\n    return data\n","ba615474":"def style_demo(kelloggs_template_gnl):\n    \n    last_col = len(kelloggs_template_gnl)\n    \n    align_format = workbook.add_format({'align':'center','valign':'vcenter','text_wrap':True})\n    worksheet.set_column(0,last_col,None,align_format)\n    \n    worksheet.set_default_row(22)\n    worksheet.set_row(0,35)\n    worksheet.set_row(1,35)\n    worksheet.set_column(0,0,40)\n    worksheet.set_column(1,last_col,35)\n    worksheet.freeze_panes(2,1) \n\n    \n    format_header = workbook.add_format({'bg_color':'#FDE9D9','bold':True,'border':2})\n    format_index_1 = workbook.add_format({'bg_color':'#EBF1DE','bold':True,'border':1})\n    format_index_2 = workbook.add_format({'bg_color':'#F2DCDB','bold':True,'border':1})\n    format_index_3 = workbook.add_format({'bg_color':'#DAEEF3','bold':True,'border':1})\n    format_entry_1 = workbook.add_format({'bg_color':'#EBF1DE','bold':False,'border':1,'num_format':'#,##0'})\n    format_entry_2 = workbook.add_format({'bg_color':'#F2DCDB','bold':False,'border':1,'num_format':'0'})\n    format_entry_3 = workbook.add_format({'bg_color':'#DAEEF3','bold':False,'border':1,'num_format':'0.00'})\n    \n    worksheet.conditional_format(0, 0, 1, last_col, {'type':'no_errors','format':format_header})\n    worksheet.conditional_format(2, 0, 4, 0, {'type':'no_errors','format':format_index_1})\n    worksheet.conditional_format(5, 0, 11, 0, {'type': 'no_errors','format':format_index_2})\n    worksheet.conditional_format(12, 0, 14, 0, {'type': 'no_errors','format':format_index_3})\n    worksheet.conditional_format(2, 1, 4, last_col, {'type': 'no_errors','format':format_entry_1})\n    worksheet.conditional_format(5, 1, 11, last_col, {'type': 'no_errors','format':format_entry_2})\n    worksheet.conditional_format(12, 1, 14, last_col, {'type': 'no_errors','format':format_entry_3})\n    \n#     bg_format_1 = workbook.add_format({'bg_color':'#FDE9D9'})\n#     bg_format_2 = workbook.add_format({'bg_color':'#EBF1DE'})\n#     bg_format_3 = workbook.add_format({'bg_color':'#F2DCDB'})\n#     bg_format_4 = workbook.add_format({'bg_color':'#DAEEF3'})\n    \n    ","0f74cc3f":"pip install openpyxl\n","e53c0b1c":"pip install xlsxwriter\n","daca14c7":"month_num_dict = {1:\"JAN\",2:\"FEB\",3:\"MAR\",4:\"APR\",5:\"MAY\",6:\"JUN\",7:\"JUL\",8:\"AUG\",9:\"SEP\",10:\"OCT\",11:\"NOV\",0:\"DEC\"}\np1_start_string = month_num_dict[p1_start%12]+str(19+(p1_start-1)\/\/12)\np1_end_string = month_num_dict[p1_end%12]+str(19+(p1_end-1)\/\/12)\np2_start_string = month_num_dict[p2_start%12]+str(19+(p2_start-1)\/\/12)\np2_end_string = month_num_dict[p2_end%12]+str(19+(p2_end-1)\/\/12)\ntime_period_string = p1_start_string+\"-\"+p1_end_string+\"_vs_\"+p2_start_string+\"-\"+p2_end_string\n\nwith pd.ExcelWriter(\"EER_\"+\"HERSHEY'S_variant\"+time_period_string+\".xlsx\") as writer: # remove kellogg's later                                                                                                   \n    for c in city_name_list:\n        df_final_city = pd.DataFrame()\n        for i in range(len(kelloggs_template_gnl)):\n            complete_data_copy = complete_data.copy()\n            changed_brand_sub_brand_df = new_names_data_frame(kelloggs_template_gnl[i],complete_data_copy)\n            if i==0:                                                  \n                df_final_city = EER_analysis(changed_brand_sub_brand_df,p1_start,p1_end,p2_start,p2_end,new_brand_name(kelloggs_template_gnl[i]),new_sub_brand_name(kelloggs_template_gnl[i]),c[0])\n            else:\n                df_final_city = pd.concat([df_final_city,EER_analysis(changed_brand_sub_brand_df,p1_start,p1_end,p2_start,p2_end,new_brand_name(kelloggs_template_gnl[i]),new_sub_brand_name(kelloggs_template_gnl[i]),c[0])],axis=1)\n        df_final_city.reset_index(level=0,inplace=True) #extra\n        df_final_city.to_excel(writer, sheet_name=c[1],header=False,index=False) #extra h,i false\n        \n        # Get workbook\n        workbook = writer.book #extra\n        # Get Sheet1\n        worksheet = writer.sheets[c[1]] #extra\n        style_demo(kelloggs_template_gnl) #extra\n        ","17583552":"[\"attribute3\",\"BintixFLAG2_PREMIUM\" ]","bf59b5a7":"500\n1\n1\n1\n1\n1\n1\n\n\n.\n.\n.\n.\n.\n","d53417d5":"For matching number with toplines","9d13a74e":"# EER function","fdd7ec30":"# Input","aba58f6f":"## Output","d37983f2":"## End"}}