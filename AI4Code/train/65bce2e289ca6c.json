{"cell_type":{"b0e15e44":"code","814d5773":"code","1b04f64c":"code","9ef19c45":"code","7a599638":"code","76ef7864":"code","cbaf82c5":"code","ae11cdf8":"code","b9059610":"code","188765a7":"code","7f50d8f3":"code","73d58dcd":"code","23dbc352":"code","bb68aa76":"code","23a9336c":"code","558b287e":"code","5a71b704":"code","d90a7f0b":"code","31a48b84":"code","0ef5c817":"code","cd5018c9":"markdown"},"source":{"b0e15e44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","814d5773":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve","1b04f64c":"df1=pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","9ef19c45":"df1","7a599638":"df1.fillna(method='ffill', inplace=True)","76ef7864":"df1.info()","cbaf82c5":"df1.describe()","ae11cdf8":"####Droping serial number as it is all unique value and wont contribute to the model.\ndf1.drop(['sl_no'], axis=1, inplace=True)","b9059610":"##Ploting the distribution of continious values\n###As we can see some the features are not normally distributed, we need to treat the skew.Power Transform in sklearn are created for this purpose.\n\nfor i in df1.columns:\n    if df1[i].dtype !='object':\n        sns.histplot(df1[i], kde=True)\n        plt.show()","188765a7":"from sklearn.preprocessing import PowerTransformer\nfor i in df1.columns:\n    if df1[i].dtype !='object' and i != 'salary':\n        pw=PowerTransformer()\n        df1[i]=pw.fit_transform(df1[i].values.reshape(-1,1))\n        sns.histplot(df1[i], kde=True)\n        plt.show()","7f50d8f3":"###As the salary column is highly skewed hence going for discretization to contain outliers and remove noise\nfrom sklearn.preprocessing import KBinsDiscretizer\nkb=KBinsDiscretizer(n_bins=10, encode='ordinal',strategy=\"quantile\")\ndf1['salary']=kb.fit_transform(df1.salary.values.reshape(-1,1))","73d58dcd":"####Categorical values encoding####\n###Checking the frequency distribution of the object dtype features\n\nfor i in df1.columns:\n    if df1[i].dtype =='object':\n        print(i)\n        print(df1[i].value_counts())","23dbc352":"####1)trying out frequency encoding for categorical feature transformation\n\n#for i in df1.columns:\n    #if df1[i].dtype =='object' and i !='status':\n    #    vl_1=df1.groupby(i).size()\/len(df1)\n    #    df1[i]=df1[i].map(vl_1)","bb68aa76":"#####2)trying out one hot encoding for categorical feature transformation\n\n#df1=pd.get_dummies(df1, columns=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'], drop_first=True, prefix=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'])","23a9336c":"######3)Trying mean encoding for categorical feature transformation\n#for i in df1.columns:\n#    if df1[i].dtype =='object' and i !='status':\n#        vl_1=df1.groupby(i).status.count()\/len(df1)\n#        df1[i]=df1[i].map(vl_1)","558b287e":"#df1","5a71b704":"def model_fit(df1, str1, enc_type):\n    df2=df1.copy(deep=True)\n    if enc_type =='frequency_encoding':\n        for i in df2.columns:\n            if df2[i].dtype =='object' and i !='status':\n                vl_1=df2.groupby(i).size()\/len(df2)\n                df2[i]=df2[i].map(vl_1)\n    elif enc_type =='mean_encoding':\n        for i in df2.columns:\n            if df2[i].dtype =='object' and i !='status':\n                vl_1=df2.groupby(i).status.count()\/len(df2)\n                df2[i]=df2[i].map(vl_1)\n    else:\n        df2=pd.get_dummies(df2, columns=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'], drop_first=True, prefix=['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation'])\n        \n    X=df2.drop('status', axis=1)\n    y=df2.status\n    ####Removing Multicolinearity \n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    vif=pd.DataFrame()\n    vif['columum_values']=X.columns.tolist()\n    lst_1=[]\n    for i in range(0,len(X.columns)):\n        vl_1=variance_inflation_factor(X.values, i)\n        lst_1.append(vl_1)\n    vif['VIF_value']=lst_1\n    lst_2=vif[vif.VIF_value >5].columum_values.tolist()\n    df2=df2.drop(lst_2, axis=1)\n    lg=LogisticRegression()\n    rf=RandomForestClassifier()\n    xg=XGBClassifier()\n    nb=GaussianNB()\n    lb=LabelEncoder()\n    target_names=df2.status.unique().tolist()\n    target=lb.fit_transform(df2.status)\n    df3=df2.drop('status', axis=1)\n    X_train,X_test,y_train,y_test=train_test_split(df3,target, test_size=0.2,stratify=target)\n    print(str1)\n    for i in [lg,rf,xg,nb]:\n        i.fit(X_train,y_train)\n        y_pred=i.predict(X_test)\n        print(i)\n        print(classification_report(y_test, y_pred, target_names=target_names))\n        print('=================================')\n        pred_prob1 = i.predict_proba(X_test)\n        fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\n        df3=pd.DataFrame({'Fale_Postive_Rate': fpr1, 'True_Positive_Rate': tpr1, 'Threshold': thresh1})\n        print(df3)\n        ","d90a7f0b":"str1='Creating models with frequency encoding'\nmodel_fit(df1, str1,enc_type='frequency_encoding')","31a48b84":"str1='Creating models with mean encoding'\nmodel_fit(df1, str1,enc_type='mean_encoding')","0ef5c817":"str1='Creating models with onehot encoding'\nmodel_fit(df1, str1,enc_type='onehot_encoding')","cd5018c9":"Trying different categorical encoding techniques"}}