{"cell_type":{"0ce2f940":"code","a9d4c70a":"code","c0eb466a":"code","d36c8a23":"code","9c802b9d":"code","50128cc0":"code","f0e73906":"code","84100242":"code","78c39cc7":"code","0fa13aed":"code","1fb77e77":"code","d65163b4":"code","cc43727d":"code","967918bc":"code","b713762e":"code","7d086d34":"code","133fb079":"code","d0877895":"code","be59c735":"code","551f889d":"code","68198a82":"code","0e4b2285":"code","3e1aeef2":"code","cdd3e1c9":"code","77014bba":"code","ae650f6f":"code","e2c1a68d":"code","90cbc2ed":"code","8189afef":"code","1ee64f23":"code","0fcf0141":"code","d70c004b":"code","fab33427":"code","ab036cdf":"code","dcd1c3d9":"code","502b015f":"code","e4ee4ffd":"code","d28ded65":"code","4b205c55":"code","c3cf1766":"code","89882eee":"code","a9863b20":"code","a94d90f9":"code","30440229":"code","aeeaad59":"code","60d4a045":"code","81f8833f":"code","e2a364ef":"code","17e38103":"markdown","ca51fdfd":"markdown","10f37eb6":"markdown","90d5251a":"markdown","4acf2112":"markdown","aec582b6":"markdown","a8ba19f4":"markdown","8a922941":"markdown","9080022c":"markdown"},"source":{"0ce2f940":"! pip install mglearn==0.1.9","a9d4c70a":"%matplotlib inline\nimport sys\nfrom scipy import sparse\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\nimport mglearn","c0eb466a":"from sklearn.datasets import load_iris\niris_dataset = load_iris()","d36c8a23":"print(\"Keys of iris_dataset: \\n{}\".format(iris_dataset.keys()))","9c802b9d":"print(iris_dataset['DESCR'][:193] + \"\\n...\")","50128cc0":"print(\"Target names: {}\".format(iris_dataset['target_names']))","f0e73906":"print(\"Feature names: \\n{}\".format(iris_dataset['feature_names']))","84100242":"print(\"Type of data: {}\".format(type(iris_dataset['data'])))","78c39cc7":"print(\"Shape of data: {}\".format(iris_dataset['data'].shape))","0fa13aed":"print(\"First five columns of data:\\n{}\".format(iris_dataset['data'][:5]))","1fb77e77":"print(\"Type of target: {}\".format(type(iris_dataset['target'])))","d65163b4":"print(\"Shape of target: {}\".format(iris_dataset['target'].shape))","cc43727d":"print(\"Target:\\n{}\".format(iris_dataset['target']))","967918bc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\niris_dataset['data'], iris_dataset['target'], random_state=0)","b713762e":"print(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))","7d086d34":"print(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))","133fb079":"# create dataframe from data in X_train\n# label the columns using the strings in iris_dataset.feature_names\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n# create a scatter matrix from the dataframe, color by y_train\ngrr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\nhist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)","d0877895":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)","be59c735":"knn.fit(X_train, y_train)","551f889d":"X_new = np.array([[5, 2.9, 1, 0.2]])\nprint(\"X_new.shape: {}\".format(X_new.shape))","68198a82":"prediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\niris_dataset['target_names'][prediction]))","0e4b2285":"y_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))","3e1aeef2":"print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))","cdd3e1c9":"print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))","77014bba":"import matplotlib.pyplot as plt\n# generate dataset\nX, y = mglearn.datasets.make_forge()\n# plot dataset\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\nplt.xlabel(\"First feature\")\nplt.ylabel(\"Second feature\")\nprint(\"X.shape: {}\".format(X.shape))","ae650f6f":"X, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Target\")","e2c1a68d":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(\"cancer.keys(): \\n{}\".format(cancer.keys()))","90cbc2ed":"print(\"Shape of cancer data: {}\".format(cancer.data.shape))","8189afef":"print(\"Sample counts per class:\\n{}\".format(\n{n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))","1ee64f23":"print(\"Feature names:\\n{}\".format(cancer.feature_names))","0fcf0141":"from sklearn.datasets import load_boston\nboston = load_boston()\nprint(\"Data shape: {}\".format(boston.data.shape))","d70c004b":"X, y = mglearn.datasets.load_extended_boston()\nprint(\"X.shape: {}\".format(X.shape))","fab33427":" from sklearn.datasets import make_blobs\nmglearn.plots.plot_knn_classification(n_neighbors=5)","ab036cdf":"mglearn.plots.plot_knn_classification(n_neighbors=10)","dcd1c3d9":"from sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","502b015f":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)","e4ee4ffd":"clf.fit(X_train, y_train)","d28ded65":"print(\"Test set predictions: {}\".format(clf.predict(X_test)))","4b205c55":"print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))","c3cf1766":"fig, axes = plt.subplots(1, 3, figsize=(10, 3))\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n# the fit method returns the object self, so we can instantiate\n# and fit in one line\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n    ax.set_xlabel(\"feature 0\")\n    ax.set_ylabel(\"feature 1\")\naxes[0].legend(loc=3)","89882eee":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\ncancer.data, cancer.target, stratify=cancer.target, random_state=66)\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n# build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n# record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","a9863b20":"mglearn.plots.plot_knn_regression(n_neighbors=3)","a94d90f9":"mglearn.plots.plot_knn_regression(n_neighbors=5)","30440229":"from sklearn.neighbors import KNeighborsRegressor\nX, y = mglearn.datasets.make_wave(n_samples=40)\n# split the wave dataset into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate the model and set the number of neighbors to consider to 3\nreg = KNeighborsRegressor(n_neighbors=3)\n# fit the model using the training data and training targets\nreg.fit(X_train, y_train)","aeeaad59":"print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))","60d4a045":"#coefficient of determination method\nprint(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))","81f8833f":"fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n# create 1,000 data points, evenly spaced between -3 and 3\nline = np.linspace(-3, 3, 1000).reshape(-1, 1)\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n# make predictions using 1, 3, or 9 neighbors\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n    reg.fit(X_train, y_train)\n    ax.plot(line, reg.predict(line))\n    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n    ax.set_title(\n\"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n    n_neighbors, reg.score(X_train, y_train),\n    reg.score(X_test, y_test)))\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data\/target\",\n\"Test data\/target\"], loc=\"best\")","e2a364ef":"mglearn.plots.plot_linear_regression_wave()","17e38103":"# Analyzing KNeighboursClassifier further","ca51fdfd":"# Regression using KNN","10f37eb6":"# Visualizing the algorithm with some sample datasets","90d5251a":"# Data Visualization","4acf2112":"# Building the model","aec582b6":"# Training and testing","a8ba19f4":"# Data Exploration","8a922941":"# Making Predictions ans Evaluating the model","9080022c":"# Final Training\n**Here, we are evaluating training and test set performance with different numbers of neighbors.**"}}