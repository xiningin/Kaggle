{"cell_type":{"96e03851":"code","aa86d887":"code","ab005ca3":"code","5352b0d2":"code","8f6c9327":"code","188bb587":"code","fcc7b8f4":"code","aec86463":"code","44e550cd":"code","5ec37502":"code","d16505c1":"code","6bd75819":"code","6959ca54":"code","4cf494a2":"code","9737f92c":"code","b7c56236":"code","ec4a6b7e":"code","5d22faed":"code","2c021794":"code","9e0d945e":"code","b6103f7d":"code","552b5eae":"code","888cd933":"code","d0c621eb":"code","1b221753":"code","04724b0a":"code","377e216a":"code","d95de973":"code","9149b9d3":"code","66caf52a":"code","7a28243c":"code","325e70eb":"code","db10d295":"code","634c9b6a":"code","c71f6ce2":"code","e5ec0c81":"code","dc86f079":"code","68f84c90":"code","5f4d990b":"code","e63de3f3":"code","24a4cfe5":"code","c6f6e79d":"code","f62d8441":"code","16ff5491":"code","d24fb9d4":"code","6dd4482e":"code","84f7d7fd":"code","cca5ebe6":"code","93424659":"markdown","574f4dd2":"markdown"},"source":{"96e03851":"# Importing packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_selection import RFECV,RFE\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","aa86d887":"# reading the dataset\ndata = pd.read_csv(r'..\/input\/h1n1-vaccination\/h1n1_vaccine_prediction.csv')\ndata","ab005ca3":"data.info()","5352b0d2":"pd.set_option('display.max_columns',100)\ndata.describe()","8f6c9327":"# Now lets look out for missing values","188bb587":"data.isna().sum().sort_values(ascending=False)","fcc7b8f4":"# Missing Percentage\nmiss_percent=((data.isna().sum()\/len(data))*100).sort_values(ascending=False)\nmiss_percent.plot.bar()\nplt.show()","aec86463":"# droping columns which has missing percentage greater than 10 percent\nmiss_cols=list(miss_percent[miss_percent>10].index)\ndata1=data.drop(data[miss_cols],axis=1)","44e550cd":"data1.isna().sum().sort_values(ascending=False)","5ec37502":"# Hence we can see that large proportion of the column have value zero in valuecounts\n# we are imputing the null value with the mode of the column have lesser that 1000 null values\nmiss_row = data1.isna().sum().sort_values(ascending=False)\nmiss_row = miss_row[(miss_row<1000) & (miss_row!=0)].index\nmiss_row","d16505c1":"data2= data1.copy()\ndata2[miss_row] = data2[miss_row].apply(lambda x: x.fillna(x.mode()[0]))","6bd75819":"# droping rest of the na values\ndata3 = data2.dropna()","6959ca54":"# changing some of categorical values into numbers to analyse it\nclean = {'age_bracket':{'18 - 34 Years':1,'35 - 44 Years':2,'45 - 54 Years':3,'55 - 64 Years':4,'65+ Years':5},\n        'qualification':{'< 12 Years':1,'12 Years':2,'College Graduate':3,'Some College':4}}\ndata3=data3.replace(clean)","4cf494a2":"# Lets analyse all feature with corresponding with the target variable by creating a function\ndef analysis(df,graph_per_row,max_graphs):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col]>1 and nunique[col]<50]]\n    nrow, ncol = df.shape\n    colname = list(df)\n    graph_row = (ncol+graph_per_row-1)\/graph_per_row\n    plt.figure(figsize=(12*graph_per_row,8*graph_row))\n    for i in range(min(ncol,max_graphs)):\n        plt.subplot(graph_row,graph_per_row,i+1)\n        coltype = df.iloc[:,i]\n        if (not np.issubdtype(type(coltype.iloc[0]),np.str)):\n            sns.countplot(colname[i], hue='h1n1_vaccine',data=df)\n        else:\n            coltype.hist()\n        plt.title(f'{colname[i]}')\n        plt.xticks(rotation=60)\n    plt.show()","9737f92c":"analysis(data3,5,25)","b7c56236":"data3.isna().sum().sort_values(ascending=False)","ec4a6b7e":"# Check for Multicollinearity\nobj = data3.select_dtypes(include='object').columns\n[print(i,'-->',data3[i].unique()) for i in obj]","5d22faed":"clean = {'sex':{'Female':0 ,'Male':1},\n        'employment':{'Not in Labor Force':1,'Employed':2,'Unemployed':3},\n        'census_msa':{'Non-MSA':1,'MSA, Not Principle  City':2,'MSA, Principle City':3},\n         'housing_status':{'Own':1,'Rent':0},\n        'marital_status':{'Not Married':0, 'Married':1}\n}\ndata4=data3.replace(clean)","2c021794":"plt.figure(figsize=(30,24))\ncorr = data4.corr()\nsns.heatmap(corr,annot=True)\nplt.show()","9e0d945e":"# we can more multicollinearity from the heat map \n# so we can check using Variance Influencing Factor\ndef vif_scores(df):\n    VIF_Scores = pd.DataFrame()\n    VIF_Scores[\"Independent Features\"] = df.columns\n    VIF_Scores[\"VIF Scores\"] = [variance_inflation_factor(df.values,i) for i in range(df.shape[1])]\n    return VIF_Scores","b6103f7d":"df1=data4.drop(data4[['unique_id','race','is_h1n1_vacc_effective','is_seas_vacc_effective','qualification']],axis=1)#'sex','marital_status','is_h1n1_vacc_effective','is_seas_vacc_effective'\ndf2 = df1.iloc[:,:-1]\nvif_scores(df2)","552b5eae":"# So, Except 'race','is_h1n1_vacc_effective','is_seas_vacc_effective','qualification' no other columns have Multicolinearity","888cd933":"# One Hot Encoding Race\none_hot = data4[['race']]\none_hot= pd.get_dummies(one_hot)\none_hot.columns","d0c621eb":"data5= data4.drop(data4[['unique_id','race']],axis=1)","1b221753":"data5 = pd.concat([data5,one_hot],axis = 1)","04724b0a":"data5.info()","377e216a":"# firt Creating model with all the features\nx = data5.drop(['h1n1_vaccine'],axis=1)\ny= data5['h1n1_vaccine']\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state=1)\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred= model.predict(x_test)","d95de973":"# Creating Function for viewing Result of the predicted\ndef res(y_valid):\n    cm1 = confusion_matrix(y_test,y_valid)\n    ConfusionMatrixDisplay(cm1).plot().ax_.set(ylabel = 'Actual value', xlabel ='Predicted value')\n    print('Accuracy',accuracy_score(y_test,y_valid))\n    print(classification_report(y_test,y_valid))\n    plt.show()","9149b9d3":"res(y_pred)","66caf52a":"plot_roc_curve(model,x_train,y_train,response_method='predict_proba')","7a28243c":"y_prob = model.predict_proba(x_test)\ny_prob = y_prob[:,1]","325e70eb":"#predict using custom thershold\nThersold = 0.2\ny_pred1 =  np.where(y_prob>Thersold,1,0)\nres(y_pred1)","db10d295":"# We can use Thershold based on our recuriment of the model","634c9b6a":"# DecisionTree Model\ndec = DecisionTreeClassifier()\ndec.fit(x_train,y_train)\ny_pred_dec = dec.predict(x_test)\nres(y_pred_dec)","c71f6ce2":"# Random Forest Model\nrand = RandomForestClassifier()\nrand.fit(x_train,y_train)\ny_pred_rand = rand.predict(x_test)\nres(y_pred)","e5ec0c81":"# KNN Model\nerror = []\nfor i in range(1,20,2):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    y_pred_knn = knn.predict(x_test)\n    error.append(np.mean(y_test!=y_pred_knn))\nplt.plot(range(1,20,2), error, marker='o')","dc86f079":"knn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train,y_train)\ny_pred_knn = knn.predict(x_test)\nres(y_pred_knn)","68f84c90":"# Getting best Features out of all of them Using\n#Recursive Feature Engineering\nrfe = RFE(rand)\nrfe.fit(x_train,y_train)","5f4d990b":"select = []\nfeatures = rfe.support_\ncols = x.columns\nfor i,j in enumerate(features):\n    if j==True:\n        select.append(cols[i])\nselect","e63de3f3":"# Now from the best Features, Creating logistice Model \nx = data5[['h1n1_worry',\n 'h1n1_awareness',\n 'dr_recc_h1n1_vacc',\n 'is_h1n1_vacc_effective',\n 'is_h1n1_risky',\n 'sick_from_h1n1_vacc',\n 'is_seas_vacc_effective',\n 'is_seas_risky',\n 'sick_from_seas_vacc',\n 'age_bracket',\n 'qualification',\n 'sex',\n 'employment',\n 'census_msa',\n 'no_of_adults',\n 'no_of_children']]\ny= data5['h1n1_vaccine']\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state=1)\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\ny_pred_features= model.predict(x_test)\nres(y_pred_features)","24a4cfe5":"# We can see from the Results that 16 features give the same accuracy as 32 features","c6f6e79d":"# Hyperparameter tunning\n# first geting best parameter to use in Random Forest model and predict with it\nparameters = {'n_estimators':[10,20,30,40,50],'max_depth':[3,4,5,6,7], 'criterion':('entropy', 'gini'),'max_leaf_nodes':[5,10,15,20]}\nclf = GridSearchCV(rand, parameters)\nclf.fit(x_train,y_train)\nclf.best_params_","f62d8441":"y_pred_GS = clf.predict(x_test)\nres(y_pred_GS)","16ff5491":"# !pip install xgboost\n# %pip install lightgbm\n# %pip install catboost","d24fb9d4":"# XGB \nxgb_model = xgb.XGBClassifier()\nxgb_model.fit(x_train,y_train)\ny_pred_xgb = xgb_model.predict(x_test)\nres(y_pred_xgb)","6dd4482e":"# Cat Boost\nmodel_cat = CatBoostClassifier()\nmodel_cat.fit(x_train,y_train,verbose=False)\ny_pred_cat = model_cat.predict(x_test)\nres(y_pred_cat)","84f7d7fd":"# LightGB\ntrain_data = lgb.Dataset(x_train,y_train)\nparams = {'learning_rate':0.001}\nmodel_lgb = lgb.train(params,train_data)\ny_pred_lgb=model.predict(x_test)\nres(y_pred_lgb)","cca5ebe6":"# So from the above model we can conclude that the XGB give some good recall and precision when compared to all\n# More over we cannot depend on accuracy on Classification Problem \n# We can change the True Positive Rate or False Positive Rate depending on our problem statement","93424659":"# Ensemble Models","574f4dd2":"# Creating Models"}}