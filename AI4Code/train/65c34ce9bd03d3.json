{"cell_type":{"a6d3576b":"code","c4c4dfe7":"code","f9afa107":"code","33670a73":"code","c3284c21":"code","a43a7246":"code","0aee2cf7":"code","766a415c":"code","6d7227c2":"code","d74b59e9":"code","4838cc04":"code","f583495d":"code","dd19148a":"code","ec8eeb45":"code","f39c0f9b":"code","7f9f95a8":"code","b57b0308":"code","354cb7c2":"code","1f209850":"code","2f501285":"markdown","906183da":"markdown","2c42f0fb":"markdown","6abe52ec":"markdown","ec4cc410":"markdown","3cd6f0d8":"markdown","2247d1d3":"markdown","8c9fc765":"markdown","71804aae":"markdown","cfc88b7c":"markdown","c77a0b76":"markdown","daa8afe4":"markdown","f52720db":"markdown","8445d04a":"markdown","af6ce651":"markdown","bab75141":"markdown","7ab56e7a":"markdown","dfd53f20":"markdown","a60402c8":"markdown","b6b70049":"markdown","0e2b022c":"markdown"},"source":{"a6d3576b":"# Standard library\nimport copy\nimport glob\nimport multiprocessing\nimport os\nimport time\nimport zipfile\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\n\n# Related third party\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm","c4c4dfe7":"base_dir = '..\/input\/dogs-vs-cats-redux-kernels-edition'\nwith zipfile.ZipFile(os.path.join(base_dir, 'train.zip')) as train_zip:\n    train_zip.extractall('..\/data')\nwith zipfile.ZipFile(os.path.join(base_dir, 'test.zip')) as test_zip:\n    test_zip.extractall('..\/data')\n\ntrain_dir = '..\/data\/train'\ntest_dir = '..\/data\/test'","f9afa107":"# with zipfile.ZipFile('.\/drive\/My Drive\/Data Set\/dogs-vs-cats-redux-kernels-edition.zip') as entire_zip:\n#     entire_zip.extractall('.')\n# with zipfile.ZipFile('.\/train.zip') as train_zip:\n#     train_zip.extractall('.')\n# with zipfile.ZipFile('.\/test.zip') as test_zip:\n#     test_zip.extractall('.')\n\n# train_dir = '.\/train'\n# test_dir = '.\/test'","33670a73":"input_size = 224\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Number of classes in the dataset\nnum_classes = 2 # dog, cat\n\n# Batch size for training (change depending on how much memory you have)\nbatch_size = 32\n\n# Number of epochs to train for\nnum_epochs = 2\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True\n\n# Switch to perform multi-process data loading\nnum_workers = multiprocessing.cpu_count()","c3284c21":"# train data file looks '.\/train\/dog.10435.jpg'\n# test data file looks '.\/test\/10435.jpg'\ndef extract_class_from(path):\n    file = path.split('\/')[-1]\n    return file.split('.')[0]","a43a7246":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    history = {'accuracy': [],\n               'val_accuracy': [],\n               'loss': [],\n               'val_loss': []}\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            if phase == 'train':\n                history['accuracy'].append(epoch_acc.item())\n                history['loss'].append(epoch_loss)\n            else:\n                history['val_accuracy'].append(epoch_acc.item())\n                history['val_loss'].append(epoch_loss) \n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","0aee2cf7":"all_train_files = glob.glob(os.path.join(train_dir, '*.jpg'))\ntrain_list, val_list = train_test_split(all_train_files, random_state=42)","766a415c":"print(len(train_list))\nprint(len(val_list))","6d7227c2":"fig, axes = plt.subplots(nrows=2,\n                         ncols=3,\n                         figsize=(18, 12))\nfor img_path, ax in zip(train_list, axes.ravel()):\n    ax.set_title(img_path)\n    ax.imshow(Image.open(img_path))","d74b59e9":"class DogVsCatDataset(Dataset):\n  \n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.file_list)\n  \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n       \n        img_name = self.file_list[idx]\n        image = Image.open(img_name)\n        if self.transform:\n            image = self.transform(image)\n    \n        label_category = extract_class_from(img_name)\n        label = 1 if label_category == 'dog' else 0\n    \n        return image, label","4838cc04":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(input_size, scale=(0.5, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n}","f583495d":"# Create training and validation datasets\nimage_datasets = {\n    'train': DogVsCatDataset(train_list,\n                             transform=data_transforms['train']),\n    'val': DogVsCatDataset(val_list,\n                           transform=data_transforms['val'])\n}\n\n# Create training and validation dataloaders\ndataloaders_dict = {x: DataLoader(image_datasets[x],\n                                  batch_size=batch_size,\n                                  shuffle=True,\n                                  num_workers=num_workers) for x in ['train', 'val']}\n\n# Detect if we have a GPU available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","dd19148a":"model_ft = models.mobilenet_v2(pretrained=True)\nmodel_ft.classifier[1] = nn.Linear(1280, num_classes)","ec8eeb45":"# Send the model to GPU\nmodel_ft = model_ft.to(device)\n\n# Gather the parameters to be optimized\/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","f39c0f9b":"# Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)","7f9f95a8":"acc = hist['accuracy']\nval_acc = hist['val_accuracy']\nloss = hist['loss']\nval_loss = hist['val_loss']\nepochs_range = range(num_epochs)\n\nplt.figure(figsize=(24, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","b57b0308":"test_list = glob.glob(os.path.join(test_dir, '*.jpg'))\ntest_data_transform = data_transforms['val']\n\nids = []\nlabels = []\n\nwith torch.no_grad():\n    for test_path in tqdm(test_list):\n        img = Image.open(test_path)\n        img = test_data_transform(img)\n        img = img.unsqueeze(0)\n        img = img.to(device)\n\n        model_ft.eval()\n        outputs = model_ft(img)\n        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n\n        test_id = extract_class_from(test_path)\n        ids.append(int(test_id))\n        labels.append(preds[0])","354cb7c2":"template = '\"{}\" with {:.2%} confidence'\ndef pred_result_message(pred):\n    if pred > 0.5:\n        return template.format('dog', pred)\n    else:\n        return template.format('cat', 1 - pred)\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=3,\n                         figsize=(18, 12))\nfor img_path, label, ax in zip(test_list, labels, axes.ravel()):\n    ax.set_title(pred_result_message(label))\n    ax.imshow(Image.open(img_path))","1f209850":"output = pd.DataFrame({'id': ids,\n                       'label': np.round(labels)})\n\noutput.sort_values(by='id', inplace=True)\noutput.reset_index(drop=True, inplace=True)\n\noutput.to_csv('submission.csv', index=False)","2f501285":"## Visualize training results\n\nThis procedure comes from https:\/\/www.tensorflow.org\/tutorials\/images\/classification#visualize_training_results.","906183da":"# Load Data","2c42f0fb":"# Global Declarations","6abe52ec":"# Run Training and Validation Step","ec4cc410":"## Check how well the prediction went","3cd6f0d8":"# Create the Optimizer\n\nSee https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#create-the-optimizer for details.","2247d1d3":"## Dataset class\n\nSee https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html#dataset-class for details.","8c9fc765":"### For Google Colab\n\nIt assumes that downloaded data will be located under `'My Drive\/Data Set\/'`","71804aae":"## Create dataloaders\n\nSee https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#load-data for details.","cfc88b7c":"### For Kaggle kernel","c77a0b76":"# Predict","daa8afe4":"## Constants\n\nRegarding `input_size`, `mean` and `std`, all pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. See [torchvision.models](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html) for details. ","f52720db":"# Pre process for each environment","8445d04a":"## Helper Functions","af6ce651":"# Import libraries","bab75141":"# Initialize and Reshape the Networks\n\nRegarding tuning VGG, see https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#vgg.\n\n","7ab56e7a":"## Check what train data looks like","dfd53f20":"This `train_model` function comes from https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code.","a60402c8":"# Generate submittion.csv","b6b70049":"Forked from https:\/\/www.kaggle.com\/alpaca0984\/dog-vs-cat-with-pytorch","0e2b022c":"# References\n\n- The competition is https:\/\/www.kaggle.com\/c\/dogs-vs-cats-redux-kernels-edition (it ended long time ago)\n- Pytorch official docs:\n    - [Finetuning Torchvision Models](https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html)\n    - [Writing Custom Datasets, DataLoaders and Transforms](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)\n    - Any other docs refered to train model or whatever, left comments on each section.\n- Refered kernels:\n    - [Dog_vs_Cat Transfer Learning - VGG16 by Pytorch](https:\/\/www.kaggle.com\/bootiu\/dog-vs-cat-transfer-learning-vgg16-by-pytorch)"}}