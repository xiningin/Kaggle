{"cell_type":{"4241f566":"code","f79536b5":"code","e055eccd":"code","857a31b8":"code","98171aa9":"code","311b4dd2":"code","74bb5e8a":"code","6771ad6f":"code","98b2ac52":"code","c8b6fb1a":"code","424931d4":"code","6b873cc7":"code","ae025bee":"code","cc298a97":"code","bed41000":"code","0e6e86b1":"code","150e2274":"code","30a2a387":"code","2e1be05f":"code","43f76251":"code","102fa000":"code","36259d94":"code","cbfb1b77":"code","a9d7f344":"code","bb289f39":"code","a3331db8":"code","a499c489":"code","f6b0316a":"code","74e6c367":"code","4ffd673c":"code","b4fa0423":"code","8c5a6451":"markdown","fcc5f77a":"markdown","c7c1c5b0":"markdown","f845a563":"markdown","70571071":"markdown","08ac4f67":"markdown","bfa9a263":"markdown","70421de2":"markdown","ab066100":"markdown","cde182e2":"markdown","470dd2f3":"markdown","1d05ce67":"markdown","813b4276":"markdown","d61f973c":"markdown","5cc15654":"markdown","c7a5d784":"markdown","7c83e77c":"markdown","c89a1802":"markdown","a79eb8ed":"markdown","5b815a92":"markdown","97975539":"markdown","21915445":"markdown","adec86a3":"markdown","2d7679ef":"markdown","9a9959a6":"markdown","48819c7c":"markdown","f546762e":"markdown","cb503970":"markdown","2740ee58":"markdown","ca66f705":"markdown","690b5adc":"markdown","b409bf27":"markdown","b22fdc73":"markdown","fa389f6b":"markdown","ec311c53":"markdown"},"source":{"4241f566":"# Installing the feature-engine library that is not installed by default in the Kaggle Noteboooks.\n!pip install feature-engine","f79536b5":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom feature_engine.encoding import RareLabelEncoder\nfrom feature_engine.encoding import MeanEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\nfrom sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)","e055eccd":"def importing_data(train_path, validation_path):\n\n    train_data = pd.read_csv(train_path)\n    validation_data = pd.read_csv(validation_path).drop(columns = ['PassengerId'])\n    validation_data_ids = pd.read_csv(validation_path)['PassengerId']\n\n    return train_data, validation_data, validation_data_ids\n\ntrain_titanic_path = '..\/input\/titanic\/train.csv'\nvalidation_titanic_path = '..\/input\/titanic\/test.csv'\n\ntrain_titanic, validation_titanic, validation_data_ids = importing_data(train_titanic_path, validation_titanic_path)","857a31b8":"def dropping_train_id(train_data, id_column):\n    \n    train_data = train_data.drop(columns = [id_column])\n\n    return train_data\n\ntrain_titanic = dropping_train_id(train_titanic, 'PassengerId')","98171aa9":"def renaming_pclass_values(dataframe, pclass_column):\n\n    dataframe[pclass_column] = dataframe[pclass_column].replace({1: 'first class', 2: 'second class', 3: 'third class'})\n\n    return dataframe\n\ntrain_titanic = renaming_pclass_values(train_titanic, 'Pclass')\nvalidation_titanic = renaming_pclass_values(validation_titanic, 'Pclass')","311b4dd2":"def extracting_social_title(dataframe, name_column):\n\n    social_titles = list()\n\n    for name in dataframe[name_column].to_list():\n        \n        start = name.find(', ') + len(', ')\n        end = name.find('. ')\n        title = name[start:end]\n        social_titles.append(str(title))\n\n    dataframe['SocialTitle'] = social_titles\n\n    return dataframe\n\ntrain_titanic = extracting_social_title(train_titanic, 'Name')\nvalidation_titanic = extracting_social_title(validation_titanic, 'Name')","74bb5e8a":"def extracting_name_type(dataframe, name_column):\n\n    name_type = list()\n\n    for name in dataframe[name_column].to_list():\n        \n        if '(' in name:\n            name_type.append('double name')\n        else:\n            name_type.append('single name')\n\n    dataframe['NameType'] = name_type\n\n    return dataframe\n\ntrain_titanic = extracting_name_type(train_titanic, 'Name')\nvalidation_titanic = extracting_name_type(validation_titanic, 'Name')","6771ad6f":"def extracting_surname(dataframe, name_column):\n\n    surnames = list()\n\n    for name in dataframe[name_column].to_list():\n        \n        start = name.find('') + len('')\n        end = name.find(', ')\n        surname = name[start:end]\n        surnames.append(str(surname))\n\n    dataframe['Surname'] = surnames\n    dataframe = dataframe.drop(columns = [name_column])\n\n    return dataframe\n\ntrain_titanic = extracting_surname(train_titanic, 'Name')\nvalidation_titanic = extracting_surname(validation_titanic, 'Name')","98b2ac52":"def extracting_total_companions(dataframe, parch_column, sibsp_column):\n\n    dataframe['TotalCompanions'] = dataframe[parch_column] + dataframe[sibsp_column] + 1\n\n    return dataframe\n\ntrain_titanic = extracting_total_companions(train_titanic, 'Parch', 'SibSp')\nvalidation_titanic = extracting_total_companions(validation_titanic, 'Parch', 'SibSp')","c8b6fb1a":"def dissecting_ticket(dataframe, ticket_column):\n\n    letters = list()\n    numbers = list()\n    symbols = list()\n\n    characters = list()\n\n    for ticket in dataframe[ticket_column].to_list():\n\n        letter_count = 0\n        number_count = 0\n        symbol_count = 0\n        \n        for character in ticket:\n\n            if character.isalpha():\n                letter_count += 1\n\n            elif character.isnumeric():\n                number_count += 1\n\n            elif character == \" \":\n                pass\n\n            else:\n                symbol_count += 1\n        \n        letters.append(letter_count)\n        numbers.append(number_count)\n        symbols.append(symbol_count)\n\n        total_characters = letter_count + number_count + symbol_count\n        characters.append(total_characters)\n\n    dataframe['TicketLetters'] = letters\n    dataframe['TicketNumbers'] = numbers\n    dataframe['TicketSymbols'] = symbols\n    dataframe['TicketCharacters'] = characters\n\n    dataframe = dataframe.drop(columns = [ticket_column])\n\n    return dataframe\n\ntrain_titanic = dissecting_ticket(train_titanic, 'Ticket')\nvalidation_titanic = dissecting_ticket(validation_titanic, 'Ticket')","424931d4":"def extracting_cabin_letter(dataframe, cabin_column):\n\n    letters = list()\n\n    dataframe[cabin_column] = dataframe[cabin_column].fillna('unknown')\n\n    for cabin in dataframe[cabin_column].to_list():\n\n        if cabin == 'unknown':\n            letters.append('unknown')\n\n        else:\n            letters.append(str(cabin[0]))\n\n\n    dataframe['CabinLetter'] = letters\n    dataframe = dataframe.drop(columns = [cabin_column])\n\n    return dataframe\n\ntrain_titanic = extracting_cabin_letter(train_titanic, 'Cabin')\nvalidation_titanic = extracting_cabin_letter(validation_titanic, 'Cabin')","6b873cc7":"def renaming__embarked_values(dataframe, embarked_column):\n\n    dataframe[embarked_column] = dataframe[embarked_column].replace({'S': 'southampton', 'C': 'cherbourg', 'Q': 'queenstown'})\n\n    return dataframe\n\ntrain_titanic = renaming__embarked_values(train_titanic, 'Embarked')\nvalidation_titanic = renaming__embarked_values(validation_titanic, 'Embarked')","ae025bee":"def imputing_age(dataframe_train, dataframe_test, age_column):\n\n    iterative_imputer = IterativeImputer()\n\n    dataframe_train[age_column] = iterative_imputer.fit_transform(dataframe_train[age_column].values.reshape(-1,1))\n    dataframe_train[age_column] = dataframe_train[age_column].astype('int64')\n\n    dataframe_test[age_column] = iterative_imputer.transform(dataframe_test[age_column].values.reshape(-1,1))\n    dataframe_test[age_column] = dataframe_test[age_column].astype('int64')\n\n    return dataframe_train, dataframe_test\n\ntrain_titanic, validation_titanic = imputing_age(train_titanic, validation_titanic, 'Age')","cc298a97":"def imputing_embarked(dataframe, embarked_column):\n\n    dataframe[embarked_column] = dataframe[embarked_column].fillna('cherbourg')\n\n    return dataframe\n\ntrain_titanic = imputing_embarked(train_titanic, 'Embarked')","bed41000":"def imputing_fare(dataframe_train, dataframe_validation, fare_column):\r\n\r\n    iterative_imputer = IterativeImputer()\r\n\r\n    iterative_imputer.fit(dataframe_train[fare_column].values.reshape(-1,1))\r\n\r\n    dataframe_validation[fare_column] = iterative_imputer.transform(dataframe_validation[fare_column].values.reshape(-1,1))\r\n\r\n    return dataframe_validation\r\n\r\nvalidation_titanic = imputing_fare(train_titanic, validation_titanic, 'Fare')","0e6e86b1":"def frequency_social_surnames_cabinletter(dataframe_train, dataframe_validation, socialtitle_column, surname_column, cabinletter_column, target):\n\n    y_variable = dataframe_train[target]\n    dataframe_train = dataframe_train.drop(columns = [target])\n\n    rare_label_encoder = RareLabelEncoder(tol = 0.002, n_categories = 2, variables = [socialtitle_column, surname_column, cabinletter_column], replace_with = 'rare')\n    \n    dataframe_train = rare_label_encoder.fit_transform(dataframe_train)\n    dataframe_validation = rare_label_encoder.transform(dataframe_validation)\n\n    frequency_encoder = MeanEncoder(variables = [socialtitle_column, surname_column, cabinletter_column])\n\n    dataframe_train = frequency_encoder.fit_transform(dataframe_train, y_variable)\n    dataframe_validation = frequency_encoder.transform(dataframe_validation)\n\n    dataframe_train[target] = y_variable\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = frequency_social_surnames_cabinletter(train_titanic, validation_titanic, 'SocialTitle', 'Surname', 'CabinLetter', 'Survived')","150e2274":"def binning_age_column(dataframe_train, dataframe_validation, age_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 6, encode = 'ordinal', strategy = 'quantile')\n\n    dataframe_train[age_column] = k_bins_discretizer.fit_transform(dataframe_train[age_column].values.reshape(-1,1))\n    dataframe_validation[age_column] = k_bins_discretizer.transform(dataframe_validation[age_column].values.reshape(-1,1))\n\n    dataframe_train[age_column] = dataframe_train[age_column].astype('int64')\n    dataframe_validation[age_column] = dataframe_validation[age_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_age_column(train_titanic, validation_titanic, 'Age')","30a2a387":"def binning_fare_column(dataframe_train, dataframe_validation, fare_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 6, encode = 'ordinal', strategy = 'quantile')\n\n    dataframe_train[fare_column] = k_bins_discretizer.fit_transform(dataframe_train[fare_column].values.reshape(-1,1))\n    dataframe_validation[fare_column] = k_bins_discretizer.transform(dataframe_validation[fare_column].values.reshape(-1,1))\n\n    dataframe_train[fare_column] = dataframe_train[fare_column].astype('int64')\n    dataframe_validation[fare_column] = dataframe_validation[fare_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_fare_column(train_titanic, validation_titanic, 'Fare')","2e1be05f":"def binning_sibsp_column(dataframe_train, dataframe_validation, sibsp_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[sibsp_column] = k_bins_discretizer.fit_transform(dataframe_train[sibsp_column].values.reshape(-1,1))\n    dataframe_validation[sibsp_column] = k_bins_discretizer.transform(dataframe_validation[sibsp_column].values.reshape(-1,1))\n\n    dataframe_train[sibsp_column] = dataframe_train[sibsp_column].astype('int64')\n    dataframe_validation[sibsp_column] = dataframe_validation[sibsp_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_sibsp_column(train_titanic, validation_titanic, 'SibSp')","43f76251":"def binning_parch_column(dataframe_train, dataframe_validation, parch_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[parch_column] = k_bins_discretizer.fit_transform(dataframe_train[parch_column].values.reshape(-1,1))\n    dataframe_validation[parch_column] = k_bins_discretizer.transform(dataframe_validation[parch_column].values.reshape(-1,1))\n\n    dataframe_train[parch_column] = dataframe_train[parch_column].astype('int64')\n    dataframe_validation[parch_column] = dataframe_validation[parch_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_parch_column(train_titanic, validation_titanic, 'Parch')","102fa000":"def binning_totaltompanions_column(dataframe_train, dataframe_validation, totalcompanions_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[totalcompanions_column] = k_bins_discretizer.fit_transform(dataframe_train[totalcompanions_column].values.reshape(-1,1))\n    dataframe_validation[totalcompanions_column] = k_bins_discretizer.transform(dataframe_validation[totalcompanions_column].values.reshape(-1,1))\n\n    dataframe_train[totalcompanions_column] = dataframe_train[totalcompanions_column].astype('int64')\n    dataframe_validation[totalcompanions_column] = dataframe_validation[totalcompanions_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_totaltompanions_column(train_titanic, validation_titanic, 'TotalCompanions')","36259d94":"def binning_ticketletters_column(dataframe_train, dataframe_validation, ticketletters_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[ticketletters_column] = k_bins_discretizer.fit_transform(dataframe_train[ticketletters_column].values.reshape(-1,1))\n    dataframe_validation[ticketletters_column] = k_bins_discretizer.transform(dataframe_validation[ticketletters_column].values.reshape(-1,1))\n\n    dataframe_train[ticketletters_column] = dataframe_train[ticketletters_column].astype('int64')\n    dataframe_validation[ticketletters_column] = dataframe_validation[ticketletters_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_ticketletters_column(train_titanic, validation_titanic, 'TicketLetters')","cbfb1b77":"def binning_ticketnumbers_column(dataframe_train, dataframe_validation, ticketnumbers_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 3, encode = 'ordinal', strategy = 'quantile')\n\n    dataframe_train[ticketnumbers_column] = k_bins_discretizer.fit_transform(dataframe_train[ticketnumbers_column].values.reshape(-1,1))\n    dataframe_validation[ticketnumbers_column] = k_bins_discretizer.transform(dataframe_validation[ticketnumbers_column].values.reshape(-1,1))\n\n    dataframe_train[ticketnumbers_column] = dataframe_train[ticketnumbers_column].astype('int64')\n    dataframe_validation[ticketnumbers_column] = dataframe_validation[ticketnumbers_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_ticketnumbers_column(train_titanic, validation_titanic, 'TicketNumbers')","a9d7f344":"def binning_ticketsymbols_column(dataframe_train, dataframe_validation, ticketsymbols_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'uniform')\n\n    dataframe_train[ticketsymbols_column] = k_bins_discretizer.fit_transform(dataframe_train[ticketsymbols_column].values.reshape(-1,1))\n    dataframe_validation[ticketsymbols_column] = k_bins_discretizer.transform(dataframe_validation[ticketsymbols_column].values.reshape(-1,1))\n\n    dataframe_train[ticketsymbols_column] = dataframe_train[ticketsymbols_column].astype('int64')\n    dataframe_validation[ticketsymbols_column] = dataframe_validation[ticketsymbols_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_ticketsymbols_column(train_titanic, validation_titanic, 'TicketSymbols')","bb289f39":"def binning_ticketcharacters_column(dataframe_train, dataframe_validation, ticketcharacters_column):\n\n    k_bins_discretizer = KBinsDiscretizer(n_bins = 2, encode = 'ordinal', strategy = 'quantile')\n\n    dataframe_train[ticketcharacters_column] = k_bins_discretizer.fit_transform(dataframe_train[ticketcharacters_column].values.reshape(-1,1))\n    dataframe_validation[ticketcharacters_column] = k_bins_discretizer.transform(dataframe_validation[ticketcharacters_column].values.reshape(-1,1))\n\n    dataframe_train[ticketcharacters_column] = dataframe_train[ticketcharacters_column].astype('int64')\n    dataframe_validation[ticketcharacters_column] = dataframe_validation[ticketcharacters_column].astype('int64')\n\n    return dataframe_train, dataframe_validation\n\ntrain_titanic, validation_titanic = binning_ticketcharacters_column(train_titanic, validation_titanic, 'TicketCharacters')","a3331db8":"def merging_training_validation(dataframe_training, dataframe_validation, target):\n\n    dataframe_validation['DataClass'] = 'validation'\n\n    y_variable = dataframe_training[target]\n    dataframe_training['DataClass'] = 'train'\n    dataframe_training = dataframe_training.drop(columns = [target])\n\n    dataframe = pd.concat([dataframe_training, dataframe_validation])\n\n    return dataframe, y_variable\n\ntitanic_dataframe, y_variable = merging_training_validation(train_titanic, validation_titanic, 'Survived')","a499c489":"def binarising_sex_nametype(dataframe, sex_column, nametype_column):\n\n    dataframe[sex_column] = dataframe[sex_column].replace({'male': 0, 'female': 1})\n    dataframe[nametype_column] = dataframe[nametype_column].replace({'single name': 0, 'double name': 1})\n\n    return dataframe\n\ntitanic_dataframe = binarising_sex_nametype(titanic_dataframe, 'Sex', 'NameType')","f6b0316a":"def one_hot_pclass_embarked(dataframe, pclass_column, embarked_column, datatype_column):\n\n    data_type = dataframe[datatype_column]\n\n    dataframe = dataframe.drop(columns = [datatype_column])\n    dataframe = pd.get_dummies(dataframe)\n    dataframe[datatype_column] = data_type\n\n    return dataframe\n\ntitanic_dataframe = one_hot_pclass_embarked(titanic_dataframe, 'Pclass', 'Embarked', 'DataClass')","74e6c367":"def train_validation_splitting(dataframe, identifier_column, target_values):\n\n    train_dataframe = dataframe[dataframe[identifier_column] == 'train']\n    train_dataframe = train_dataframe.drop(columns = [identifier_column])\n    train_dataframe['Survived'] = target_values\n\n    validation_dataframe = dataframe[dataframe[identifier_column] == 'validation']\n    validation_dataframe = validation_dataframe.drop(columns = [identifier_column])\n\n    return train_dataframe, validation_dataframe\n\ntrain_titanic, validation_titanic = train_validation_splitting(titanic_dataframe, 'DataClass', y_variable)","4ffd673c":"def splitting_x_y(dataframe, target):\n\n    y_variable = dataframe[target]\n    x_variables = dataframe.drop(columns = [target])\n\n    return x_variables, y_variable\n\nx_variables, y_variable = splitting_x_y(train_titanic, 'Survived')","b4fa0423":"def training_knclassifier_nca(independent_variables, dependent_variable, validation_data):\n\n    nca = NeighborhoodComponentsAnalysis(random_state = 42)\n    model = KNeighborsClassifier(n_neighbors = 50)\n\n    model_pipeline = Pipeline([('nca', nca), ('knn', model)])\n    model_pipeline.fit(independent_variables, dependent_variable)\n    scores = cross_val_score(model_pipeline, independent_variables, dependent_variable, cv = StratifiedKFold(n_splits = 5), n_jobs = -1)\n\n    predictions = model_pipeline.predict(validation_data)\n    submission = pd.DataFrame(data = zip(validation_data_ids, predictions), columns = ['PassengerId', 'Survived'])\n    submission.to_csv('.\/submission.csv', index = False)\n\n    return scores.mean(), scores.std()\n\nmodel_mean_score, model_mean_std = training_knclassifier_nca(x_variables, y_variable, validation_titanic)\n\nprint('\\nMODEL SUMMARY AND RESULTS')\nprint('-------------------------\\n')\n\nprint(f'CV MODEL MEAN ACCURACY: {round(model_mean_score * 100, 2)}%')\nprint(f'CV MODEL MEAN ACCURACY DEVIATION: {round(model_mean_std * 100, 2)}')\nprint('ACCURACY OF THE SUBMISSION IN THE KAGGLE LEADERBOARD: 80.382%\\n')","8c5a6451":"<h2>One Hot Encoding the \"Pclass\" and the \"Embarked\" columns.<\/h2>","fcc5f77a":"<h2>Importing the libraries.<\/h2>","c7c1c5b0":"<h2>Transforming the \"TicketSymbols\" column in categorical Bins.<\/h2>","f845a563":"<h1>- IMPORTING PHASE -<\/h1>","70571071":"<h2>Transforming the \"TicketCharacters\" column in categorical Bins.<\/h2>","08ac4f67":"<h2>Generating the \"NameType\" column using the name column, which distinguishes between individual and double names.<\/h2>","bfa9a263":"<h2>Transforming the \"TicketLetters\" column in categorical Bins.<\/h2>","70421de2":"<h2>Imputing the \"Fare\" column in the validation data using the Iterative Imputer function.<\/h2>","ab066100":"<h2>Imputing the \"Embarked\" column in the training data using the values from similar passengers to the missing ones.<\/h2>","cde182e2":"<h2>Dropping the \"PassengerId\" column from the training data.<\/h2>","470dd2f3":"<h2>Transforming the \"Parch\" column in categorical Bins.<\/h2>","1d05ce67":"<h2>Transforming the \"Age\" column in categorical Bins.<\/h2>","813b4276":"<h2>Renaming the values of the \"Pclass\" column for the training and validation data.<\/h2>","d61f973c":"<h2>Transforming the \"TotalCompanions\" column in categorical Bins.<\/h2>","5cc15654":"<h1>- FEATURE ENGINEERING -<\/h1>","c7a5d784":"<h2>Extracting all the posible information from the \"Ticket\" column. The letters, the numbers, the symbols and the total characters.<\/h2>","7c83e77c":"<h2>Transforming the \"TicketNumbers\" column in categorical Bins.<\/h2>","c89a1802":"<h2>Merging training and validation datasets to perform the encoding of all the categorical columns.<\/h2>","a79eb8ed":"<h2>Splitting the merged titanic dataframe in the train and validation dataframes.<\/h2>","5b815a92":"<h2>Training a KNeighborsClassifier applying a dimension reduction technique based on near centroids.<\/h2>","97975539":"<h2>Importing the Titanic train and validation data.<\/h2>","21915445":"<h2>Transforming the \"Fare\" column in categorical Bins.<\/h2>","adec86a3":"<h1>TITANIC \ud83d\udea2 - FEATURE ENGINEERING, PRE-PROCESSING AND MODEL TRAINING - 0.80382 LB \ud83d\udc40<\/h1>","2d7679ef":"<h2>Getting the letter of the cabin using the \"Cabin\" column. Generating a new column named \"CabinLetter\".<\/h2>","9a9959a6":"<h1>- PRE-PROCESSING -<\/h1>","48819c7c":"<h2>Extracting the surnames of the passengers using the \"Name\" column. Finally dropping the \"Name\" column.<\/h2>","f546762e":"<h2>Creating the \"SocialTitle\" column using the social abbreviations from the \"Name\" column.<\/h2>","cb503970":"<h2>Summing up the \"Parch\" and the \"SibSp\" columns to get the \"TotalCompanions\" column, taking into account the value of the passenger.<\/h2>","2740ee58":"<h2>Transforming the \"SibSp\" column in categorical Bins.<\/h2>","ca66f705":"<h2>Splitting the training dataframe in the independent variables and the target variable.<\/h2>","690b5adc":"<h1>- MODEL TRAINING -<\/h1>","b409bf27":"<h2>Encoding by frequency the variable \"SocialTitle\", the variable \"Surname\" and the variable \"CabinLetter\".<\/h2>","b22fdc73":"<h2>Imputing the \"Age\" column in the training and validation data using the Iterative Imputer function.<\/h2>","fa389f6b":"<h2>Renaming the values of the \"Embarked\" column for the training and validation data.<\/h2>","ec311c53":"<h2>Binarising the variable \"Sex\" and the variable \"NameType\".<\/h2>"}}