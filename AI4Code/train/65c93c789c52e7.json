{"cell_type":{"13c08223":"code","7486ba50":"code","bb2fabb3":"code","2e46a4e0":"code","f246923d":"code","5dbb39f3":"code","ea69935a":"code","e06c3e65":"code","4ed5f2e6":"code","3e0e1142":"code","bac87b76":"code","ff25ec62":"code","955e7b66":"markdown","b5c0e742":"markdown","8d239ec5":"markdown","d66dff0c":"markdown","9293cf90":"markdown","ee3981a2":"markdown","23417cd0":"markdown","a645d0d2":"markdown","be8ac458":"markdown","11efc249":"markdown","72f04aae":"markdown"},"source":{"13c08223":"!pip install tubesml==0.3.1","7486ba50":"import numpy as np \nimport pandas as pd\n\nimport tubesml as tml\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.manifold import TSNE\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bb2fabb3":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\ndf_train.head()","2e46a4e0":"print('Train Set')\nprint('\\n')\n_ = tml.list_missing(df_train)\nprint('_'*40)\nprint('Test Set')\nprint('\\n')\n_ = tml.list_missing(df_test)","f246923d":"def plot_frame(ax):\n    ax.set_facecolor('#292525')\n    ax.spines['bottom'].set_color('w')\n    ax.tick_params(axis='x', colors='w')\n    ax.xaxis.label.set_color('w')\n    ax.spines['left'].set_color('w')\n    ax.tick_params(axis='y', colors='w')\n    ax.yaxis.label.set_color('w')\n    return ax\n\n\nfig, ax = plt.subplots(5, 2, figsize=(14, 25), facecolor='#292525', sharey=True)\n\ni=0\n\nfor col in ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']:\n    df_train[col].value_counts(dropna=False, normalize=True).sort_index().plot(kind='bar', ax=ax[i][0], color='#C3C92E')\n    df_test[col].value_counts(dropna=False, normalize=True).sort_index().plot(kind='bar', ax=ax[i][1], color='#C93D2E')\n    ax[i][0] = plot_frame(ax[i][0])\n    ax[i][1] = plot_frame(ax[i][1])\n    ax[i][0].set_title(f'Train set - {col}', fontsize=14, color='w')\n    ax[i][1].set_title(f'Test set - {col}', fontsize=14, color='w')\n    ax[i][0].set_xticklabels(ax[i][0].get_xticklabels(), rotation=0)\n    ax[i][1].set_xticklabels(ax[i][1].get_xticklabels(), rotation=0)\n    i += 1","5dbb39f3":"fig, ax = plt.subplots(2, 2, figsize=(14, 10), facecolor='#292525', sharey=True)\n\ni=0\n\nfor col in ['Age', 'Fare']:\n    sns.kdeplot(df_train[col], ax=ax[i][0], shade=True, color='#C3C92E')\n    sns.kdeplot(df_test[col], ax=ax[i][1], shade=True, color='#C93D2E')\n    ax[i][0] = plot_frame(ax[i][0])\n    ax[i][1] = plot_frame(ax[i][1])\n    ax[i][0].set_title(f'Train set - {col}', fontsize=14, color='w')\n    ax[i][1].set_title(f'Test set - {col}', fontsize=14, color='w')\n    ax[i][0].set_xlabel('')\n    ax[i][1].set_xlabel('')\n    i += 1","ea69935a":"print(f'Unique ticket numbers in the Train set: {len(set(df_train.Ticket))}')\nprint(f'Unique ticket numbers in the Test set: {len(set(df_test.Ticket))}')\nprint(f'Ticket numbers that are present in both sets: {len(set(set(df_train.Ticket).intersection(set(df_test.Ticket))))}')","e06c3e65":"preds_none = [0]*len(df_train)\npreds_class = np.where(df_train['Pclass'].isin([1, 2]), 1, 0)\npreds_gender = np.where(df_train['Sex'] == 'female', 1, 0)\npreds_class_gender = np.where((df_train['Sex'] == 'female') & (df_train['Pclass'].isin([1, 2])), 1, 0)\n\nprint(f'Accuracy when predicting nobody survived: {accuracy_score(preds_none, df_train.Survived)}')\nprint(f'Accuracy when predicting only passengers in the first 2 classes survived: {accuracy_score(preds_class, df_train.Survived)}')\nprint(f'Accuracy when predicting only female passengers survived: {accuracy_score(preds_gender, df_train.Survived)}')\nprint(f'Accuracy when predicting only female passengers in the first 2 classes survived: {accuracy_score(preds_class_gender, df_train.Survived)}')","4ed5f2e6":"preds_none = [0]*len(df_test)\npreds_class = np.where(df_test['Pclass'].isin([1, 2]), 1, 0)\npreds_gender = np.where(df_test['Sex'] == 'female', 1, 0)\npreds_class_gender = np.where((df_test['Sex'] == 'female') & (df_test['Pclass'].isin([1, 2])), 1, 0)\n\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': preds_none})\noutput.to_csv('submission_none.csv', index=False)\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': preds_class})\noutput.to_csv('submission_class.csv', index=False)\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': preds_gender})\noutput.to_csv('submission_gender.csv', index=False)\noutput = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': preds_class_gender})\noutput.to_csv('submission_class_gender.csv', index=False)","3e0e1142":"train_feat = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\ndf_train = df_train[train_feat].copy()\ndf_test = df_test[train_feat].copy()\ndf_train['target'] = 1\ndf_test['target'] = 0\n\nadv = pd.concat([df_train, df_test], ignore_index=True)\ny_adv = adv['target']\nadv = adv.drop('target', axis=1)\nadv.head()","bac87b76":"# Full model pipeline to impute the missing values and prepare the data for the model\nnum_pipe = Pipeline([('fs', tml.DtypeSel('numeric')), \n                     ('imp', tml.DfImputer(strategy='median'))])\ncat_pipe = Pipeline([('fs', tml.DtypeSel('category')), \n                     ('imp', tml.DfImputer(strategy='most_frequent')), \n                     ('dum', tml.Dummify(drop_first=True))])\nproc_pipe = tml.FeatureUnionDf(transformer_list=[('num', num_pipe), ('cat', cat_pipe)])\nfull_pipe = Pipeline([('proc', proc_pipe), \n                      ('model', xgb.XGBClassifier(n_estimators=10000, subsample=0.7, random_state=10, n_jobs=-1))])\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=345)\n\n# generate an out of fold prediction for the full dataset\n# TubesML allows to do so with early stopping and it also returns the feature importance by fold\noof, imp = tml.cv_score(data=adv, target=y_adv,\n                             cv=kfolds, estimator=full_pipe, \n                             predict_proba=True, early_stopping=100, eval_metric='auc', imp_coef=True)\n\nprint(f'AUC score: {roc_auc_score(y_score=oof, y_true=y_adv)}')\nprint(f'Accuracy: {accuracy_score(y_pred=oof>0.5, y_true=y_adv)}')\n\n# plot the feature importance with some uncertainty bar\ntml.plot_feat_imp(imp)","ff25ec62":"df_train['target'] = 1\ndf_test['target'] = 0\ntrain_feat = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nadv = pd.concat([df_train, df_test], ignore_index=True).sample(100000)  # sampling because life is too short\n\ngreen = adv['target'] == 1\nred = adv['target'] == 0\n\nadv = proc_pipe.fit_transform(adv[train_feat])\ntsne = TSNE(n_components=2, init='pca', random_state=51, perplexity=100, learning_rate=500, n_jobs=-1)\n\ny_total = tsne.fit_transform(adv)             \n                           \nfig, ax = plt.subplots(1, figsize=(15,8), facecolor='#292525')\n\nax.scatter(y_total[red, 0], y_total[red, 1], c='#C93D2E', alpha=0.5, label='Test')\nax.scatter(y_total[green, 0], y_total[green, 1], c='#C3C92E', alpha=0.2, label='Train')\nax = plot_frame(ax)\nax.legend()\nplt.show()","955e7b66":"Where we also labeled with 1 all the entries from the train set and with 0 all the ones from the test set. Let's make use of the convenient methods of [**TubesML**](https:\/\/pypi.org\/project\/tubesml\/) to get quickly a model to see if we can correctly classify the 2 sets. An ideal situation is when we can't, hence we hope in low accuracy and and AUC around 0.5","b5c0e742":"This is a quick notebook to highlight how the provided train and test set present some differences that you might want to consider when building your model.","8d239ec5":"Which score on the public LB as follow:\n\n* No survivors: 65%\n* Only class: 67%\n* Only gender: 78%\n* Class and gender: 74%\n\nWhich is in line with what we expect given the different distribution showed above.\n\n# Adversarial validation\n\nLet's see if a model can tell the two dataset apart, we use the following dataset","d66dff0c":"If we then focus on the columns with descrete values (either categorical or ordinal), we see quite some differences between the train and the test set","9293cf90":"Which is an interesting thing as it might help identifying cluster of passengers traveling together\n\n# Why does it matter?\n\nIf we build naive models, we can create nice baselines for more sofisticated ones. Here some results one would then expect when validating the model against the training set","ee3981a2":"Thus we see some differences here too, especially in the Age distribution.\n\n**Something is not different, however, can be ticket number.** \n\nIf we count the unique ticket numbers in the two sets, we find that about 18000 of them are common to the 2 datasets.","23417cd0":"The data look as follows","a645d0d2":"That is, the test set has proportionally:\n\n* more males\n* more passengers from the third class\n* more passengers travelling alone\n\nAll three can be very good predictor of the survival of a passenger and training a model on data that have a different distribution might lead to not optimal solutions.\n\nSimilarly, we see for the continuous variables the following differences","be8ac458":"We can then make some submissions out of them","11efc249":"Both train and test sets present some missing values. The test set has a little more of them","72f04aae":"The model can reasonably find some signal to tell train and test sets apart, which complicates things when setting up a cross-validation strategy we can trust.\n\nOn the other hand, t-SNE does not show anything crazy"}}