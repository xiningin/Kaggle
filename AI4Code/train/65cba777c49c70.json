{"cell_type":{"8ac4e06d":"code","7619eb67":"code","76d9147f":"code","44f2994e":"code","9412f4b3":"code","3f80e4e2":"code","f4cd2dac":"code","eb355aba":"code","76fcddc4":"code","4a6d03ed":"code","6091e366":"code","91288d9f":"code","fd7bbd72":"code","bbeb3e72":"code","d359eb63":"code","282ed3b5":"code","d6990b7c":"code","fe7fc262":"code","6f1e341c":"code","ee89c3e8":"code","1f3182ed":"code","f415a69a":"markdown","b93bc97d":"markdown","efbbac28":"markdown","b750777f":"markdown","b22b8a8d":"markdown","0008ecd0":"markdown","93e9fb7f":"markdown","c69ad03c":"markdown","7bc2720f":"markdown","640a3f82":"markdown","f0408222":"markdown"},"source":{"8ac4e06d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7619eb67":"# Loading the dataset\ndf=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","76d9147f":"## Loading Libaries\nimport pandas as pd\nimport numpy as np ","44f2994e":"## Ignoring the Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9412f4b3":"# For independent features \nX=df.drop(\"target\",axis=1)\nX.head()","3f80e4e2":"# For Dependent features\ny=df.loc[:,[\"target\"]]\ny.head()","f4cd2dac":"# Splitting the dataset and taking test size as 35%\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.35,random_state=50)\nprint(X_train.shape)\nprint(X_test.shape)","eb355aba":"# Normal RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_nor=RandomForestClassifier(n_estimators=5)\nmodel_nor.fit(X_train,y_train)\ny_prednor=model_nor.predict(X_test)","76fcddc4":"# Checking the Confusion Matrix and Classification Report\nfrom sklearn import metrics \nprint(metrics.confusion_matrix(y_test,y_prednor))\nprint(metrics.classification_report(y_test,y_prednor))","4a6d03ed":"## Loading RandomSearchCV from sklearn library\nfrom sklearn.model_selection import RandomizedSearchCV","6091e366":"## Creating Space \nn_estimators=[int(x) for x in np.linspace(100,1200,10)]\ncriterion=[\"gini\",\"entropy\"]\nmax_depth=[50,100,150,200,300,500]\nmin_samples_split=[int(x) for x in np.linspace(5,50,10)]\nmin_samples_leaf=[int(x) for x in np.linspace(4,60,10)]\nmax_features=[\"auto\", \"sqrt\", \"log2\"]\nspace_random={\"n_estimators\":n_estimators,\n             \"criterion\":criterion,\n             \"max_depth\":max_depth,\n             \"min_samples_split\":min_samples_split,\n             \"min_samples_leaf\":min_samples_leaf,\n             \"max_features\":max_features}\nspace_random","91288d9f":"# Fitting RandomSearchCV\nmodel2=RandomForestClassifier()\nrandom=RandomizedSearchCV(estimator=model2,param_distributions=space_random,n_iter=100,cv=5,verbose=2,n_jobs=-1,\n                         random_state=100)\nrandom.fit(X_train,y_train)","fd7bbd72":"# Find the best parameter with the help of RandomSearchcv\nrandom.best_params_","bbeb3e72":"## Fitting the model (Random Forest Classifer) by taking the parameter of RandomSearchCv\nmodel_random=RandomForestClassifier(n_estimators=833,max_depth=150,max_features=\"auto\",\n                                    criterion=\"entropy\",min_samples_leaf=35,min_samples_split=30)\nmodel_random.fit(X_train,y_train)\ny_predrand=model_random.predict(X_test)","d359eb63":"# Finding accuracy and classification report \nprint(metrics.confusion_matrix(y_test,y_predrand))\nprint(metrics.classification_report(y_test,y_predrand))","282ed3b5":"random.best_params_","d6990b7c":"from sklearn.model_selection import GridSearchCV\n# Creating space\nn_estimators=[830,833,840]\nmin_samples_split=[30,35]\nmin_samples_leaf=[30,35,40]\nmax_features=[\"auto\"]\nmax_depth=[150,160,170]\ncriterion=[\"entropy\"]\nspace_grid={\"n_estimators\":n_estimators,\n           \"min_samples_split\":min_samples_split,\n           \"min_samples_leaf\":min_samples_leaf,\n           \"max_features\":max_features,\n           \"max_depth\":max_depth,\n           \"criterion\":criterion}\nspace_grid","fe7fc262":"# Fitting in GridSearchCv \nmodel3=RandomForestClassifier()\ngrid=GridSearchCV(estimator=model3,param_grid=space_grid,cv=5,verbose=2,n_jobs=-1)\ngrid.fit(X_train,y_train)","6f1e341c":"# Best parameters\ngrid.best_params_","ee89c3e8":"# ## Fitting the model (Random Forest Classifer) by taking the parameter of GridSearchCV\nmodel_grid=RandomForestClassifier(n_estimators=833,criterion=\"entropy\",max_depth=160,max_features=\"auto\",\n                                 min_samples_leaf=35,min_samples_split=35)\nmodel_grid.fit(X_train,y_train)\ny_predgrid=model_grid.predict(X_test)","1f3182ed":"# Checking the accuracy and Classification Report\nprint(metrics.confusion_matrix(y_test,y_predrand))\nprint(metrics.classification_report(y_test,y_predrand))","f415a69a":"#### As we have seen that the accuracy of the model is 76%. Let see by applying hyperparameter tuning how we will improve the accuracy of the model","b93bc97d":"## As we have seen that the model accuracy has incresed by 7% that is 83%. Now we will try GridSearchCV to improve the model","efbbac28":"## Grid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.","b750777f":"# Hyperparameter Technique","b22b8a8d":"# GridSearchCV","0008ecd0":"# Train Test Split","93e9fb7f":"# Model building by traditional normal Random Forest Classifier","c69ad03c":"# Feature Extracting ","7bc2720f":"## RandomSearchCV\n### Random search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. It is similar to grid search, and yet it has proven to yield better results comparatively","640a3f82":"## As we see the model accuracy is same as 83%. so we can conclude that with the help of tunning the parameters we can easily increase the accuracy of the model ","f0408222":"### As I am focusing on the hyperparameter tuning technique so I am skipping EDA and Feature Selection."}}