{"cell_type":{"746b7aba":"code","961037cc":"code","aab97b6d":"code","ff00dcdd":"code","d1d12127":"code","64533138":"code","cdce31ed":"code","9cb8fa72":"code","72795935":"code","475d0c51":"code","23cb56ab":"code","2f1e5cdf":"code","01f849c9":"code","3349fdff":"code","652ecdde":"code","981b1f17":"code","4280a020":"code","c903b1a1":"code","98ce24c4":"code","f1b1ba26":"code","f829d4aa":"code","da5fe562":"code","255b8b77":"code","10bc5441":"code","cb27e80c":"markdown","46684746":"markdown","eeccab9a":"markdown","6506ed84":"markdown","4dcce986":"markdown","0ff638be":"markdown","cb06f9fe":"markdown","7fcac4d0":"markdown","2db2b0fa":"markdown","f24ebe3e":"markdown","a76d4599":"markdown","2045d461":"markdown","ddb15b8b":"markdown","23afd60d":"markdown"},"source":{"746b7aba":"# model = Ridge()\n# model.fit(trn_x[col_subset],trn_y)","961037cc":"import gresearch_crypto\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split","aab97b6d":"df_train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')\ndf_test = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')","ff00dcdd":"df_train.info()","d1d12127":"df_train.describe()","64533138":"df_train.isnull().sum()","cdce31ed":"a1 = df_train[\"VWAP\"].isin([np.inf])\na2 = df_train[\"VWAP\"].isin([-np.inf])\na3 = df_train[\"VWAP\"].isin([np.nan])\nprint (\"inf:\", a1.sum(), \"  -inf:\", a2.sum(), \"  nan:\", a3.sum())","9cb8fa72":"df_train[df_train[\"Target\"].isna()].head()","72795935":"\n#df_train[\"VWAP\"]=df_train[\"VWAP\"].replace([np.inf, -np.inf], np.nan)\n\n#vwap_mean = df_train[[\"Asset_ID\", \"VWAP\"]].groupby(\"Asset_ID\").mean().reset_index()\n#vwap_mean.columns = [\"Asset_ID\", \"vwap_mean\"]\n\n#df_train = pd.merge(df_train, vwap_mean, on = \"Asset_ID\", how = \"left\")\n#df_train.loc[(df_train[\"VWAP\"].isnull()), \"VWAP\"] = df_train[\"vwap_mean\"]\n#df_train = df_train.drop(\"vwap_mean\", axis = 1)\n#df_train.describe()","475d0c51":"df_asset = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\ndf_asset","23cb56ab":"def feature_adjust(df) :\n    feature_cols = ['Asset_ID', 'Close', 'Count', 'High', 'Low', 'Open', 'VWAP','Volume', 'timestamp'] \n                #\"Upper_shadow\", \"Lower_shadow\"] #Bull_Bear\n    categories = [\"Asset_ID\", \"timestamp\"] #Bull_Bear\n    \n    x = df[feature_cols].copy()\n    #df = pd.merge(df, df_asset, on = \"Asset_ID\", how = \"left\")\n    #df = df.drop(\"Asset_Name\", axis = 1)\n    return x","2f1e5cdf":"df_train = df_train[~df_train.isin([np.nan, np.inf, -np.inf]).any(1)].reset_index(drop=True)\n#df_train = feature_adjust(df_train)","01f849c9":"df_train","3349fdff":"#feature_cols = ['Asset_ID', 'Close', 'Count', 'High', 'Low', 'Open', 'VWAP','Volume', 'timestamp'] # \"Weight\"\n#categories = [\"Asset_ID\", \"timestamp\"]","652ecdde":"def reduce_mem_usage(df):\n  \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n\n    df[\"Count\"] = df[\"Count\"].astype(np.int16)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n     \n    return df\n\ndf_train = reduce_mem_usage(df_train)","981b1f17":"df_test = reduce_mem_usage(df_test)","4280a020":"#x_train = df_train[feature_cols].copy()\n#y_train = df_train['Target'].copy()","c903b1a1":"#train_x = x_train.copy()\n#train_y = y_train.copy()\n#train_x, valid_x, train_y, valid_y = train_test_split(x_train, y_train, test_size = 0.4)","98ce24c4":"lgb_params = {\n    \"objective\": \"regression\",\n    \"n_estimators\" : 5000,     # <-- (9) change from 200 to 500\n    \"num_leaves\" : 300,       # <-- (10) Added parameter\n    \"learning_rate\" : 0.09,   # <-- (10) Added parameter\n    \"random_seed\" : 1234}\n    \n#model = lgb.LGBMRegressor(**lgb_params)\n    \n#model.fit(train_x, train_y)\n\n#model.fit(\n#    train_x, \n#    train_y,\n#    eval_set = (valid_x, valid_y),    \n#    early_stopping_rounds = 20,\n#    verbose = 10,\n#    categorical_feature = categories,\n#    )\n\n","f1b1ba26":"def get_model (asset_id) :\n    \n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    x = feature_adjust(df)\n    #x = df[feature_cols].copy()\n    y = df['Target'].copy()\n        \n    model = lgb.LGBMRegressor(**lgb_params)\n    model.fit(x, y)\n    \n    return model","f829d4aa":"asset_ids = list(df_asset[\"Asset_ID\"].unique())\nasset_ids.sort()\nmodels = {}\n\nfor asset_id in asset_ids :\n        \n    print(f\"Training model for  ID={asset_id}\")\n    \n    model = get_model(asset_id)\n    models[asset_id] = model","da5fe562":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\n#for (test_df, sample_prediction_df) in iter_test:\n    \n#    test_df = feature_adjust(test_df)\n#    test_sub = test_df[feature_cols]\n#    sample_prediction_df['Target'] = model.predict(test_sub)\n#    env.predict(sample_prediction_df)","255b8b77":"for i, (df_test, df_pred) in enumerate(iter_test) :\n    df_test = reduce_mem_usage(df_test)\n    \n    for j , row in df_test.iterrows():        \n        \n        model = models[row['Asset_ID']]\n        x_test = feature_adjust(row)\n        #x_test = x_test.drop([\"row_id\"])\n        y_pred = model.predict(pd.DataFrame([x_test]))[0]\n        df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n    \n    env.predict(df_pred)","10bc5441":"# FYI, for \"Upper_shadow\" and \"Lower_shadow\", following codes were used.\n# \u3054\u53c2\u8003\u307e\u3067\u3001\u4e0a\u30d2\u30b2\u30fb\u4e0b\u30d2\u30b2\u306b\u306f\u4ee5\u4e0b\u306e\u30b3\u30fc\u30c9\u3092\u4f7f\u3044\u307e\u3057\u305f\u3002\n    \n    #df[\"Upper_shadow\"] = df[\"High\"] - np.maximum(df[\"Open\"], df['Close'])\n    #df[\"Lower_shadow\"] = np.minimum(df[\"Open\"], df['Close']) - df[\"Low\"]","cb27e80c":"# (3) Try to replace Nan, Inf, -Inf by Mean  \nFrom a look at Training data, I can see NaN, Inf, -inf. They were dropped so far (code [12]), but come up with the idea that it may improved the score if replace them by the other value such as \"Mean\". Let's try :-)  \nThe result is, Failed... Public score became worse to **0.028**. OK, I'm convinced that they should be dropped rather than change.\n\nDescribe\u3067training\u30c7\u30fc\u30bf\u3092\u6982\u89b3\u3059\u308b\u3068\u3001NaN\u3001Inf\u3001-inf\u304c\u898b\u3048\u308b\u3002\u3053\u308c\u307e\u3067\u3001\u3053\u308c\u3089\u306e\u30ec\u30b3\u30fc\u30c9\u306fDrop\uff08\u30b3\u30fc\u30c9[12]\uff09\u3057\u3066\u305f\u3051\u3069\u3001\u5e73\u5747\u5024\u306b\u7f6e\u304d\u63db\u3048\u305f\u308a\u3057\u305f\u3089\u5c11\u3057\u306f\u30b9\u30b3\u30a2\u6539\u5584\u3059\u308b\u304b\u306a\uff1f\u3068\u601d\u3044\u3001\u8a66\u3057\u3066\u307f\u308b\u3002  \n\u304c\u3001\u898b\u4e8b\u306b\u60e8\u6557\u3002\u3002\u30b9\u30b3\u30a2\u306f**0.028**\u306b\u4f4e\u4e0b\u3002\u306a\u308b\u307b\u3069\u3001\u5909\u306b\u9069\u5f53\u306a\u6570\u5024\u306b\u5909\u3048\u308b\u3088\u308a\u3082\u843d\u3068\u305b\u3001\u3068\u3044\u3046\u3053\u3068\u3067\u3059\u306d\u3002\u7d0d\u5f97\u3002","46684746":"# (1) (2) Build simple model\n\n**(1) Build simple model using Ridge()**  \nAt first, anyway use the as-is data into the simple model, and let's see how good score I can get.  \nReferring to KONRAD BANACHEWICZ's code in below link, tried to feed the training data into Ridge model.  \nThe result is Public score **0.007**, which is a little bit better than original code since I added VWAP as a feature.  \nWell, this seems not so bad as a starting score.\n\n\u3068\u308a\u3042\u3048\u305a\u3001\u3042\u307e\u308a\u4f55\u3082\u8003\u3048\u305a\u3001\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\u3067\u3069\u306e\u7a0b\u5ea6\u306e\u30b9\u30b3\u30a2\u304c\u51fa\u308b\u304b\u898b\u3066\u307f\u3088\u3046\u3002  \nKONRAD BANACHEWICZ\u3055\u3093\u306e\u30b3\u30fc\u30c9\u3092\u53c2\u8003\u306b\u3001Model\u3068\u3057\u3066Ridge\u3092\u4f7f\u3044Training Data\u3092\u6295\u5165\u3057\u3066\u307f\u308b\u3002  \n\u7d50\u679c\u306f\u3001Public Score\uff1a**0.007**\u3002\u6295\u5165\u30c7\u30fc\u30bf\u3068\u3057\u3066VWAP\u3082\u8ffd\u52a0\u3057\u305f\u306e\u3067\u53c2\u8003\u30b3\u30fc\u30c9\uff080.005\uff09\u306b\u6bd4\u3079\u3066\u5c11\u3057\u826f\u304b\u3063\u305f\u3002  \n\u6700\u521d\u306f\u3001\u307e\u3001\u3053\u3093\u306a\u3082\u3093\u3067\u3057\u3087\u3002  \nhttps:\/\/www.kaggle.com\/konradb\/super-simple-baseline","eeccab9a":"Hmmm.. As mentioned in above (2), Training data is huge. It may be because of unnecessary Float64 of Dtypes. (e.g. \"Count\" doesn't need to be Float64.) Let's reduce the data memory later.\n\n\u3075\u3080\u3075\u3080\u3002\u4e0a\u8a18(2)\u3067\u66f8\u304d\u307e\u3057\u305f\u304c\u3001Training Data\u304c\u5de8\u5927\u306a\u306e\u306f\u3001\u3080\u3060\u306b\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u304cFloat64\u3067\u4fdd\u6301\u3055\u308c\u3066\u3044\u305f\u308a\u3059\u308b\u304b\u3089\u306a\u3093\u3067\u3057\u3087\u3046\u306d (Count\u3068\u304b\u4ef6\u6570\u306a\u3093\u3060\u304b\u3089Float64\u3067\u3042\u308b\u5fc5\u8981\u6027\u306a\u3044\uff09\u3002\u306e\u3061\u307b\u3069\u3001\u8efd\u304f\u3057\u307e\u3059\u3002","6506ed84":"# (9) Increase n_estimators from 200 to 500\n\nAbove (8) was less efficient than expected (rather got worse), thus tried more simple way which is to increase n_estimators from 200 to 500 (In[19]).  \nThe result is, Public score has got better to **0.2516**. This is the simple method but much more efficient than expected.\n\n\u4e0a\u8a18(8)\u304c\u601d\u3063\u305f\u307b\u3069\u52b9\u679c\u304c\u306a\u304b\u3063\u305f\uff08\u3080\u3057\u308d\u60aa\u5316\u3057\u305f\uff09\u306e\u3067\u3001\u30b7\u30f3\u30d7\u30eb\u306b\u5b66\u7fd2\u56de\u6570\u3092200\u2192500\u306b\u5897\u3084\u3057\u3066\u307f\u308b\uff08In[19]\uff09\u3002  \n\u7d50\u679c\u306f\u3001Public Score\u304c**0.2516**\u3078\u3068\u4e0a\u6607\u3002\u5358\u7d14\u3060\u3051\u3069\u601d\u3063\u305f\u4ee5\u4e0a\u306e\u52b9\u679c\u304c\u3042\u3063\u305f\u3002","4dcce986":"# (4) Try to add \"Weight\" as one of the features  \nI can see \"asset_details\" file as well as Training data. It is small file basically to show which Asset ID is which cryptocurrency, but I'm not sure what the \"Weight\" shows.. Let's add it to features, and see how Score becomes.  \nHowever, no, the score went slightly down to **0.036**. (Code [11] is just FYI, actually not used)\n\nTraining\u30c7\u30fc\u30bf\u4ee5\u5916\u306b\u3001asset_details\u3068\u3044\u3046\u30d5\u30a1\u30a4\u30eb\u3082\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3002\u3069\u306eAssetID\u304c\u5b9f\u969b\u306b\u306f\u4f55\u306e\u6697\u53f7\u901a\u8ca8\u304b\u3092\u793a\u3057\u305f\u308a\u3059\u308bSmall\u306a\u30d5\u30a1\u30a4\u30eb\u3060\u3051\u3069\u3001\u3053\u3053\u306b\u3042\u308bWeight\u3068\u3044\u3046\u306e\u304c\u4f55\u306b\u4f7f\u3046\u306e\u304b\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002\u3068\u308a\u3042\u3048\u305a\u7279\u5fb4\u91cf\u306e\u4e00\u3064\u306b\u52a0\u3048\u3066\u30b9\u30b3\u30a2\u304c\u826f\u304f\u306a\u308b\u304b\u898b\u3066\u307f\u308b\u3002  \n\u3002\u3002\u3002\u30c0\u30e1\u3067\u3059\u306d\u3002Score\u306f**0.036**\u3068\u50c5\u304b\u306b\u60aa\u5316\u3057\u307e\u3057\u305f\u3002\uff08\u30b3\u30fc\u30c9[11]\u306f\u305d\u306e\u540d\u6b8b\u3067\u3059\u3002\u3002\u7d50\u679c\u7684\u306b\u306f\u4f7f\u3063\u3066\u307e\u305b\u3093\u3002\uff09","0ff638be":"# (8) Add \"Upper_shadow\", \"Lower shadow\" as features  \n\nSo far, I used as-is fields in the existing Training data, but try to create new features.  \nFrom a look at some Web-sites to explain the price of Stock, many sites seem to be explaining Candle Stick (refer to below diagram), with regard to Open , High, Low and Close price.  \nFirst, try to create \"Upper shadow\", \"Lower shadow\" using the eixiting these existing 4 prices.  \nLet's see how Public Score shows... the result is **0.1761** ...Why? Score have got worse. I'm wondering if simply adding them doesn't work?   \n\n\u3053\u308c\u307e\u3067\u306f\u3001Training\u30c7\u30fc\u30bf\u306b\u3042\u308b\u9805\u76ee\u3092\u305d\u306e\u307e\u307e\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f7f\u3063\u3066\u304d\u305f\u304c\u3001\u3053\u308c\u3089\u306e\u9805\u76ee\u3092\u4f7f\u3063\u3066\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u3063\u3066\u307f\u308b\u3002  \n\u682a\u4fa1\u3092\u958b\u8a2d\u3057\u3066\u3044\u308bWeb\u30b5\u30a4\u30c8\u3092\u898b\u308b\u3068\u3001\u65e2\u5b58\u306e\u7279\u5fb4\u91cf\u3067\u3042\u308b\u59cb\u5024\u30fb\u9ad8\u5024\u30fb\u5b89\u5024\u30fb\u7d42\u5024\u306b\u95a2\u3057\u3066\u30ed\u30fc\u30bd\u30af\u8db3\uff08\u4e0b\u56f3\u53c2\u7167\uff09\u3092\u89e3\u8aac\u3057\u3066\u3044\u308b\u30b5\u30a4\u30c8\u304c\u591a\u3044\u3002  \n\u307e\u305a\u306f\u3001\u30ed\u30fc\u30bd\u30af\u8db3\u306e\u7279\u5fb4\u3067\u3042\u308b\u300c\u4e0a\u30d2\u30b2\u300d\u300c\u4e0b\u30d2\u30b2\u300d\u3092\u4f5c\u3063\u3066\u307f\u308b\u3002  \nPublic Score\u306e\u7d50\u679c\u306f\u3002\u3002\u3002**0.1761**\u3002\u3042\u308c\uff1f\u30b9\u30b3\u30a2\u304c\u60aa\u304f\u306a\u3063\u3066\u308b\u3002\u5358\u7d14\u306b\u52a0\u3048\u305f\u3060\u3051\u3060\u3068\u3060\u3081\u306a\u306e\u304b\uff1f\n\n![image.png](attachment:501b8a24-758c-4af5-8a9d-0eaaea089274.png)","cb06f9fe":"# (5) Try to use \"eval_set\" in LGBM  \nTried to improve the model in the latter half of code[19], adding \"eval_set\" to study with (train_x, train_y) and evaluate with (valid_x, valid_y). Then, always Memory-over error happens.  \nWhen make the test size bigger to 0.4 (= make dataset for fitting smaller to 0.6), somehow it worked without error. Let's see the Public score.  \nOh, miserable.. Public score went down to **0.018**. Once sit down and consider again, it may make sense because only 60% of entire training data is used.\n\n\u30b3\u30fc\u30c9[19]\u306e\u5f8c\u534a\u3067\u306f\u3001eval_set\u3082\u3064\u3051\u3066(train_x, train_y)\u3067\u5b66\u7fd2\u3001(valid_x, valid_y)\u3067\u8a55\u4fa1\u3057\u3066\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u5411\u4e0a\u3055\u305b\u3088\u3046\u3068\u3057\u3066\u307f\u305f\u3002\u3002\u3002\u983b\u7e41\u306b\u30e1\u30e2\u30ea\u30fc\u30aa\u30fc\u30d0\u30fc\u304c\u767a\u751f\u3002test_size\u30920.4\u3068\u5927\u304d\u3081\u306b\u3059\u308b\uff08\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u30920.6\u3068\u5c0f\u3055\u304f\u3059\u308b\uff09\u3068\u3001\u306a\u3093\u3068\u304b\u52d5\u3044\u305f\u306e\u3067\u3001\u3053\u308c\u3067\u30b9\u30b3\u30a2\u3092\u898b\u3066\u307f\u308b\u3002\u3002\u3002  \n\u60e8\u6557\u3067\u3059\u306d\u3001\u30b9\u30b3\u30a2\u306f**0.018**\u3078\u3068\u5927\u304d\u304f\u60aa\u5316\u3002\u305d\u308a\u3083\u305d\u3046\u304b\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306e6\u5272\u3057\u304b\u4f7f\u3063\u3066\u306a\u3044\u3093\u3060\u3082\u3093\u306d\u3002","7fcac4d0":"**(2) Use LightGBM**  \nNext, tried to use LGBMRegressor, simple model to fit with x\/y only without eval_set.  \nWhat I was confused here is, I encountered frequent Memory-over error which did not happen in Ridge model.  \nOf course, the original training data itself is very huge, and memory-over error makes sense...  \nReferring to the below Discussion, introduced \"reduce_mem_usage\" function.  \nThe result is, Public score was improved to **0.039**.  Well!, it works so good to simply change from Ridge to LGBM.  \nLet's start to see the actual code!\n\n\u6b21\u306b\u3001LGBM\u3092\u4f7f\u3063\u3066\u307f\u308b\uff08eval_set\u306f\u30bb\u30c3\u30c8\u305b\u305a\u3001x\u30fby\u3060\u3051\u6295\u5165\u3057\u3066fit\u3059\u308b\u30b7\u30f3\u30d7\u30eb\u306a\u30e2\u30c7\u30eb\uff09\u3002\u3053\u3053\u3067\u56f0\u3063\u305f\u306e\u304c\u3001Ridge\u30e2\u30c7\u30eb\u306e\u6642\u306f\u554f\u984c\u306a\u304b\u3063\u305f\u300c\u30e1\u30e2\u30ea\u30fc\u30aa\u30fc\u30d0\u30fc\u30a8\u30e9\u30fc\u300d\u3002  \n\u307e\u3041\u3001\u5143\u306e\u30c7\u30fc\u30bf\u304c\u5de8\u5927\u306a\u306e\u3067\u3001\u4ed5\u65b9\u306a\u3044\u3068\u601d\u3044\u307e\u3059\u304c\u3002\u3002  \n\u4ee5\u4e0b\u306eDiscussion\uff08\u306e\u30ea\u30f3\u30af\u5148\uff09\u3092\u53c2\u8003\u306b\u3001Training\u30c7\u30fc\u30bf\u306e\u5bb9\u91cf\u3092\u524a\u6e1b\u3059\u308b\u95a2\u6570\uff08reduce_mem_usage\uff09\u3092\u8ffd\u52a0\u3002  \nhttps:\/\/www.kaggle.com\/c\/g-research-crypto-forecasting\/discussion\/285719  \n\u7d50\u679c\u306f\u3001Public Score\uff1d**0.039**\u3002\u3078\u3047\u30fc\u3001LGBM\u306b\u3059\u308b\u3060\u3051\u3067\u3053\u3093\u306a\u306b\u30b9\u30b3\u30a2\u826f\u304f\u306a\u308b\u3093\u3060\u3002\n\n\u3068\u308a\u3042\u3048\u305a\u3053\u306e\u3042\u305f\u308a\u3067\u3001\u30b3\u30fc\u30c9\u3092\u3054\u7d39\u4ecb\u3057\u3066\u3044\u304d\u307e\u3059\u3002","2db2b0fa":"# (10) Adjust Hyperparameters\n\nSince the above(9) was much more improved than expected, tried to adjust other hyperparameters(num_leaves, learning_rate) as well, refering to the below link.  \nThe result is, Public score has been much improved to **0.4278**. It is surprising that just a slight parameter adjustment contributes to much improvement like this. (Wait, actually the biggest improvement so far...)  \n\n\u4e0a\u8a18(9)\u3067n_estimators\u3092\u5909\u3048\u308b\u3060\u3051\u3067\u4e88\u60f3\u4ee5\u4e0a\u306e\u6539\u5584\u3068\u306a\u3063\u305f\u306e\u3067\u3001\u4ed6\u306eHyperparameter(num_leaves\u3068learning_rate)\u3082\u5c11\u3057\u8abf\u6574\u3057\u3066\u307f\u308b\u3002  \n\u8abf\u6574\u9805\u76ee\u306b\u3064\u3044\u3066\u306f\u3001\u4ee5\u4e0b\u306e\u30ea\u30f3\u30af\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002  \nhttps:\/\/www.kaggle.com\/swaralipibose\/lgdm-model-with-new-features-better-generalization\n\n\u9a5a\u304f\u3053\u3068\u306b\u3001\u7d50\u679c\u306f**0.4278**\u3078\u3068\u5927\u304d\u304f\u6539\u5584\u3002Hyperparameter\u5909\u3048\u308b\u3060\u3051\u3067\u3053\u3093\u306a\u306b\u30b9\u30b3\u30a2\u304c\u6539\u5584\u3059\u308b\u3093\u3060\u3002  \n\uff08\u3068\u3044\u3046\u304b\u3001\u3053\u308c\u307e\u3067\u3067\u4e00\u756a\u5927\u304d\u306a\u6539\u5584\u5e45\u306a\u3093\u3067\u3059\u3051\u3069\u3002\u3002\uff09","f24ebe3e":"# (7) Modeling for each Asset ID\n\nSo far, in the above (1)-(6), I took an easy way to feed the entire Training data into the model at one shot.  \nHowever, the price trend is of course varied for each Asset, thus tried to fit per each Asset.  \nThe result of Public score is... **0.1938**. I see, only modeling change from \"for all data\" to \"for each asset\" can improve the score like this!\n\n\u4e0a\u8a18(1)\uff5e(6)\u3067\u306f\u3001\u7c21\u6613\u7684\u306bTraining\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3066\u30e2\u30c7\u30eb\u306b\u6295\u5165\u3057\u3066\u3044\u305f\u304c\u3001Asset\u6bce\u306b\u4fa1\u683c\u50be\u5411\u3082\u9055\u3046\u306e\u3067\u3001Asset\u6bce\u306b\u5206\u3051\u3066\u5b66\u7fd2\u3055\u305b\u3066\u307f\u308b\u3002  \n\u7d50\u679c\u306f\u3001**0.1938**\u3078\u3068\u5927\u304d\u304f\u4e0a\u6607\u3002\u3078\u3047\u30fc\u3001\u500b\u5225\u30a2\u30bb\u30c3\u30c8\u6bce\u306e\u30e2\u30c7\u30eb\u306b\u3059\u308b\u3060\u3051\u3067\u3001\u3053\u3093\u306a\u306b\u6539\u5584\u3059\u308b\u3093\u3060\u3002","a76d4599":"# (11) Gradual increasing n_estimator from 500 to 5000\n\nBased on the result of the above (9), I'll attempt to gradually increase n_estimator to see how the score is improved.  \nI had assumed the score will get worse due to over-fitting if increased the number too much, however, the more and more n_estimator is increased the higher and higher the score becomes, which is the interesting result.  \n(it takes 4-5 hours to see the score result after submission, hence this experiment required 1 week... got tired..)\n\n\u3053\u3053\u304b\u3089\u5c11\u3057\u5b9f\u9a13\u3057\u3001\u4e0a\u8a18(9)\u306e\u7d50\u679c\u3092\u8e0f\u307e\u3048\u3001n_estimator\u3092\u5f90\u3005\u306b\u5897\u3084\u3057\u3066\u3044\u304d\u3001\u3069\u3053\u307e\u3067Score\u304c\u4f38\u3073\u308b\u304b\u3092\u8a66\u3057\u3066\u307f\u308b\u3002  \n\u3069\u3053\u304b\u3067\u904e\u5b66\u7fd2\u306e\u5f71\u97ff\u304c\u51fa\u3066\u30b9\u30b3\u30a2\u306f\u60aa\u304f\u306a\u308b\u3068\u601d\u3063\u305f\u304c\u3001\u610f\u306b\u53cd\u3057\u3066n_estimator\u3092\u5897\u3084\u305b\u3070\u5897\u3084\u3059\u307b\u3069\u30b9\u30b3\u30a2\u306f\u6539\u5584\u3057\u3001\u306a\u304b\u306a\u304b\u9762\u767d\u3044\u7d50\u679c\u3068\u306a\u3063\u305f\u3002  \n\uff08\u63d0\u51fa\u5f8c\u30b9\u30b3\u30a2\u304c\u51fa\u308b\u306e\u306b4\u30fb5\u6642\u9593\u304b\u304b\u308b\u306e\u3067\u3001\u3053\u306e\u5b9f\u9a13\u3001\u4e00\u9031\u9593\u307b\u3069\u639b\u304b\u308a\u307e\u3057\u305f\u3002\u3002\u75b2\u308c\u305f\uff1b\uff1b\uff09\n\n![image.png](attachment:98d7a295-4a49-454e-a887-2c48968b10b0.png)","2045d461":"Above [15] is the function to reduce the data memory. Data type Float64 is changed, and then Training data was reduced from 1.8GB to 0.5GB. Data type change works well like this. I see.\n\n\u4e0a\u8a18\u306e[15]\u304c\u3001\u30c7\u30fc\u30bf\u30e1\u30e2\u30ea\u30fc\u3092\u524a\u6e1b\u3059\u308b\u95a2\u6570\u3067\u3001Float64\u306e\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u3092\u5909\u3048\u308b\u3053\u3068\u3067Training\u30c7\u30fc\u30bf\u304c1.8GB\u304b\u30890.5GB\u3078\u3068\u524a\u6e1b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u3092\u5909\u3048\u308b\u3060\u3051\u3067\u3053\u3093\u306a\u306b\u9055\u3046\u3093\u3060\u3002\u3078\u30fc\u3002","ddb15b8b":"# (6) LGBM (test_size = 0, N_estimators = 200)  \nAfter all, I changed the model back to simple fit(x,y).  \nIf so, don't need to split the training data for validation, hence no use for train_test_split.  \nIn the above (5) process, I confirmed that the epoch of training went over 100 without early stopping and arrived at almost 200. Thus increased the n_estimators from the default 100 to 200. Now, let's see.  \nThe result is... Public score shows **0.069**, much improved. Good! (^o^)\/\n\n\u3068\u3044\u3046\u3053\u3068\u3067\u3001model\u306f\u5358\u7d14\u306afit(x,y)\u306b\u623b\u3057\u305f\u3002\u305d\u3046\u306a\u308b\u3068\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306e\u4e00\u90e8\u3092validate\u7528\u306b\u53d6\u308a\u5206\u3051\u308b\u5fc5\u8981\u3082\u306a\u3044\u306e\u3067\u3001train_test_split\u3082\u4f7f\u308f\u305a\u3002\u4e0a\u8a18(5)\u3067\u3001\u5b66\u7fd2\u304c100\u56de\u3092\u8d85\u3048\u3066\u3082early stopping\u304c\u304b\u304b\u3089\u305a200\u56de\u8fd1\u304f\u307e\u3067\u5b66\u7fd2\u304c\u9032\u3093\u3060\u3053\u3068\u306f\u78ba\u8a8d\u3067\u304d\u305f\u306e\u3067\u3001\u5b66\u7fd2\u56de\u6570\u3092\u30c7\u30d5\u30a9\u30eb\u30c8\u306e100\u304b\u3089200\u306b\u5897\u3084\u3059\u3002  \n\u3059\u308b\u3068\u3001\u7d50\u679c\u306f\u3002\u3002\u3002Public Score\u306f**0.069**\u3078\u306e\u5927\u304d\u304f\u6539\u5584\u3002\u826f\u3057\u3088\u3057\u3002\u3000(^o^)\/","23afd60d":"# [G-Reaserch] Beginner's Try for simple LGBM\n\nThis is a Table Competition to predict future price of cryptocurrencies. A bit similar to the \"Optiver\" competition a couple of months ago.  \nStarted from Public Score **0.0077** and improved to **0.4278** so far, experiencing various difficulties...\n\n\u6697\u53f7\u8cc7\u7523\u306e\u4fa1\u683c\u3092\u4e88\u60f3\u3059\u308b\u4eca\u56de\u306e\u30c6\u30fc\u30d6\u30eb\u30b3\u30f3\u30da\u3002\u3064\u3044\u6570\u304b\u6708\u524d\u306b\u3042\u3063\u305f\u300cOptiver\u300d\uff08\u682a\u4fa1\u306e\u30dc\u30e9\u30c6\u30a3\u30ea\u30c6\u30a3\u3092\u4e88\u6e2c\u3059\u308b\uff09\u306b\u3061\u3087\u3063\u3068\u4f3c\u3066\u3044\u3066\u9762\u767d\u305d\u3046\u3001\u3068\u601d\u3044\u30c8\u30e9\u30a4\u3057\u3066\u307f\u307e\u3057\u305f\u3002Public Score\u3001**0.0077**\u304b\u3089\u30b9\u30bf\u30fc\u30c8\u3057\u3001\u56db\u82e6\u516b\u82e6\u3057\u306a\u304c\u3089\u3001\u3068\u308a\u3042\u3048\u305a**0.7005**\u307e\u3067\u4e0a\u3052\u3066\u304d\u307e\u3057\u305f\u3002\u3002\u3002\n\n![image.png](attachment:53dad4f7-f44f-4bc5-9bab-7710686467d9.png)"}}