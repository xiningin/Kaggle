{"cell_type":{"d60076cb":"code","8f4e1634":"code","de552efa":"code","d24b2dd1":"code","4fae8934":"code","79cc4a34":"code","a366dca9":"code","7d01be51":"code","948ff07c":"code","2efae2dd":"code","d43f4577":"code","4bb22416":"code","f8d808d9":"code","74f4b5a4":"code","6a49a43e":"code","818073c8":"code","37db7a4f":"code","f8679a1d":"markdown","94592b07":"markdown","d9021c09":"markdown","4a4ace97":"markdown","43a19c38":"markdown","c20ac9e3":"markdown","ec9a86be":"markdown","4cf97350":"markdown","446c2859":"markdown","9456b22d":"markdown","74290c83":"markdown","35094165":"markdown","e627de34":"markdown","c108eee4":"markdown","b1b32dc9":"markdown","62acc949":"markdown","d753e48b":"markdown"},"source":{"d60076cb":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub","8f4e1634":"train_file='..\/input\/aptos2019-blindness-detection\/train.csv'\ndf=pd.read_csv(train_file)\ncat_cnt=df['diagnosis'].value_counts().sort_index()\ntrain_image_count=np.sum(cat_cnt)\nprint(\"Train data set count: \", train_image_count)","de552efa":"print(os.listdir(\"..\/input\/aptos2019-blindness-detection\"))","d24b2dd1":"USE_INTERNET = False\n\nif USE_INTERNET:\n    %env TFHUB_CACHE_DIR=.\/tfhub_cache\/imagenet\/feature_vector\n    mod_link=\"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_large\/feature_vector\/3\"\n    mod_name=mod_link.split('imagenet\/')[1].split('\/')[0]\n    input_img_size = hub.get_expected_image_size(hub.Module(mod_link))\nelse:\n    mod_link=\"..\/input\/tfhub-cache\/nasnet_large\/nasnet_large\"\n    mod_name='nasnet'\n    input_img_size = [331, 331]\n\nprint(\"Expected input size for {} is:\".format(mod_name), input_img_size)","4fae8934":"def _preproc_image(file, img_size):\n    image = tf.read_file(file)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, img_size)\n    image \/= 255.0  # normalize to [0,1] range\n    return image","79cc4a34":"SPLIT=0.804 # train\/eval\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef imgs_input_fn(filename, batch_size=32, mode = tf.estimator.ModeKeys.TRAIN, total_images=1000, img_size=[200,200]):\n\n    def _agument_image(image):\n        image = tf.image.random_brightness(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n        image = tf.image.random_flip_left_right(image)        \n        return image\n    \n    def _proc_image(file, label=None):\n        image = _preproc_image(tf.strings.join([tf.strings.join([\"..\/input\/aptos2019-blindness-detection\/train_images\",file], \"\/\"), \"png\"], \".\"),\n                              img_size)\n        # Images is agumented in the TRAIN phase only\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            image = _agument_image(image)\n        return { 'image': image}, label\n     \n    dataset_raw = tf.data.experimental.CsvDataset(filename, [tf.string, tf.string], header=True)\n    dataset_raw = dataset_raw.shuffle(total_images, seed=1)\n    train_cnt = int(SPLIT*total_images)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        dataset_raw = dataset_raw.take(train_cnt)\n        d0 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '0')).repeat()\n        d1 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '1')).repeat()\n        d2 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '2')).repeat()\n        d3 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '3')).repeat()\n        d4 = dataset_raw.filter(lambda file, label: tf.math.equal(label, '4')).repeat()\n        dataset = tf.data.experimental.sample_from_datasets([d0, d1, d2, d3, d4])\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        dataset = dataset_raw.skip(train_cnt)\n        \n    dataset = dataset.map(_proc_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n    return dataset","a366dca9":"model_dir = os.path.join(os.getcwd(), \".\/models\/{}\").format(mod_name)\nos.makedirs(model_dir, exist_ok=True)\n#%load_ext tensorboard\n#%tensorboard --logdir {model_dir} # can access from http:\/\/localhost:6006\nprint(\"model_dir: \",model_dir)\nconfig = tf.estimator.RunConfig(\n    model_dir=model_dir,\n    save_summary_steps=100,\n    save_checkpoints_steps=100,\n    keep_checkpoint_max=2,\n    log_step_count_steps=10)","7d01be51":"NUM_CLASSES = 5\n\ndef model_fn(features, labels, mode, params):\n    module = hub.Module(mod_link, trainable=False, name=mod_name)\n    feature_vec = module(features['image'])\n    #dropout = tf.layers.dropout(inputs=feature_vec, rate=0.1, training=mode == tf.estimator.ModeKeys.TRAIN)\n    logits = tf.layers.Dense(NUM_CLASSES)(feature_vec)\n    head = tf.contrib.estimator.multi_class_head(n_classes=NUM_CLASSES, label_vocabulary=['0','1','2','3','4'])\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.003)\n    return head.create_estimator_spec(\n        features, mode, logits, labels, optimizer=optimizer\n    )","948ff07c":"BATCH_SIZE = 40\nMAX_STEPS = 1250\n\ntrain_spec = tf.estimator.TrainSpec(input_fn=lambda: imgs_input_fn(train_file, \n                                                                   batch_size=BATCH_SIZE,\n                                                                   mode = tf.estimator.ModeKeys.TRAIN, \n                                                                   total_images=train_image_count,\n                                                                   img_size=input_img_size),\n                                   max_steps=MAX_STEPS)\neval_spec = tf.estimator.EvalSpec(input_fn=lambda: imgs_input_fn(train_file, \n                                                                   batch_size=BATCH_SIZE,\n                                                                   mode = tf.estimator.ModeKeys.EVAL, \n                                                                   total_images=train_image_count,\n                                                                   img_size=input_img_size))\nclassifier = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=model_dir,\n        config=config,\n        params=[]\n    )\n\ntf.logging.set_verbosity(tf.logging.INFO)\ntf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)","2efae2dd":"import pathlib\nimport glob\ntest_set = [str(path) for path in list(pathlib.Path('..\/input\/aptos2019-blindness-detection\/test_images\/').glob('*.png'))]","d43f4577":"def pred_input_fn(filelist, batch_size=32, pred_images=100, img_size=[200,200]):\n    \n    def _proc_image(file):\n        image = _preproc_image(file, img_size)\n        return { 'image': image}\n     \n    dataset = tf.data.Dataset.from_tensor_slices(filelist)  \n    dataset = dataset.take(pred_images)\n    dataset = dataset.map(_proc_image, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n    return dataset","4bb22416":"predictions = classifier.predict(input_fn=lambda: pred_input_fn(test_set, \n                                                                batch_size=32,\n                                                                pred_images=len(test_set),\n                                                                img_size=input_img_size))","f8d808d9":"fl=test_set.copy()\ndf=pd.DataFrame([(fl.pop(0).split(\"test_images\/\")[1].split(\".\")[0], \n                p['class_ids'][0], p['probabilities'][p['class_ids'][0]]) for p in predictions], \n                columns=['id_code', 'diagnosis', 'probability'])","74f4b5a4":"df.head(5)","6a49a43e":"df.describe()","818073c8":"df.to_csv('submission.csv', columns=['id_code', 'diagnosis'], index=None)","37db7a4f":"import seaborn as sns\nsns.set()\ncp = sns.catplot(x=\"diagnosis\", y=\"probability\", kind=\"violin\", inner=None, data=df, palette=sns.color_palette(\"RdBu_r\", 5))\nsns.swarmplot(x=\"diagnosis\", y=\"probability\", color=\"k\", size=3, data=df, ax=cp.ax);","f8679a1d":"The input function below is used for TRAIN and EVAL. The AUTOTUNE option is used to let TensorFlow use available compute resources effectively. The images in this data set are generally large, and thus the preprocessing step could be the bottleneck rather than train step. The image files are shuffle and split into TRAIN and EVAL. Measures are taken here to ensure even distribution of the different categories in each mini-batch. ","94592b07":"## Final comments\nThis kernel scored ~0.57. Some serious feature engineering is needed to properly adapt and train this model. The large image size in the data set really puts the CPU resources under pressure. On a desktop (i7-3930K + RTX2070), the whole notebook runs in ~20min. On Kaggle, the runtime is ~75min. Let's take a look at the predictions with a graphical view:","d9021c09":"## Define model configuration\nModels are saved into separate `models` directories - useful when testing and comparing different pre-trained models.","4a4ace97":"## Define model function\nThe pre-trained model is used directly, with just a softmax layer with 5 classes on top. Additionally, the multi_class_head function is used that takes care of some details for us. The loss function it uses by default is `sparse_softmax_cross_entropy` loss. There are many useful discussions on using transfer learning for image classification, [for example this one](https:\/\/towardsdatascience.com\/transfer-learning-from-pre-trained-models-f2393f124751)","43a19c38":"# Image classification with Tensorflow Estimator API\nThis kernel uses the TensorFlow high-level Estimator API. The model is built using transfer learning with a model from the TensorFlow Hub.","c20ac9e3":"## Predict\nPredictions is then just a matter of calling the .predict method.","ec9a86be":"## Data set\nThe training data set is described in a .csv file with image IDs and labels. There are 5 classes named 0 to 4.","4cf97350":"## Input function\nFirst step is to make an input function that will serve the model with data (TensorFlow dataset). Preprocessing steps are collected in the `_preproc_image()` function that is common for TRAIN, EVAL and PREDICT steps.","446c2859":"## Pre-trained model\nThe base model can be picked from TensorFlow Hub, and can be changed in one line below. A pre-trained model is usually made for a specific input image size, and thankfully we can get that parameter from the hub. Here we pick the NASNet-A model:","9456b22d":"# Making predictions\nThe test images for prediction is located in the `test_images` directory, and we simply make a list of all the files.","74290c83":"## Setup the Estimator\nThe last thing remaining is to setup the estimator and then run training. The model is saved automatically in the model directory defined above.","35094165":"## Predict input function\nTo prevent cluttering up the input function used for training, a separate input function is defined for the predict step. ","e627de34":"Take a look at the first few predictions. The probability of each prediction is also included here for information.","c108eee4":"## Submit\nFinally, the result is submitted to a .csv file:","b1b32dc9":"The data set is small (and heavily skewed), and the pre-trained model has never seen retina photos before - what could possibly go wrong? Let's find out...","62acc949":"NOTE! Nothing happens after the above line - need to iterate through `predictions` to get it going.\nEach prediction will be a dict with this format:\n\n`{'logits': array([ 0.5168798,  1.1608064,  3.1475224, -5.6981955,  0.5543511], dtype=float32), \n  'probabilities': array([5.6095276e-02, 1.0680217e-01, 7.7875328e-01, 1.1213811e-04, 5.8237117e-02], dtype=float32), \n  'class_ids': array([2], dtype=int64), \n  'classes': array([b'2'], dtype=object), \n  'all_class_ids': array([0, 1, 2, 3, 4]), \n  'all_classes': array([b'0', b'1', b'2', b'3', b'4'], dtype=object)}`\n  \nThe payload we are interested in can be found under `class_ids`. Let's create a Pandas dataframe while iterating through the predictions:","d753e48b":"Also look at predictions stats:"}}