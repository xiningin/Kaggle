{"cell_type":{"d5c8329f":"code","d62d10b4":"code","ac4095b0":"code","13244352":"code","467634ac":"code","8c5bcbc8":"code","68ad2514":"code","0fdf4b93":"code","41ab0cc1":"code","10194004":"code","3a99b457":"code","e9642a1a":"code","1013c757":"code","bcc821fe":"code","6431ba09":"code","ab0b04f4":"code","9bc775a2":"code","8eaa7c5f":"code","e14f2132":"code","d3f412b8":"code","31229843":"code","e8bc8107":"code","b51bbad7":"code","f1924b61":"code","73a19b0d":"code","3804e71a":"code","5441badd":"code","0ca0a573":"code","fcb53c23":"code","923e9867":"code","bafd6520":"code","eaf54721":"code","ddca0ef0":"code","fde8864c":"code","c2e1c1d9":"code","38a94983":"code","344ac77b":"code","15e46636":"code","e84e6c8f":"code","07c734fe":"code","7c1cd254":"code","28dfc213":"code","15a58426":"code","f3bc95d8":"code","3f336d3a":"code","0b94861a":"code","ecc990aa":"code","d67e6b43":"markdown","d8c27151":"markdown","bcbe5378":"markdown","06461ac1":"markdown","ca0e5d57":"markdown","8bbff0b5":"markdown","040af942":"markdown","f17ccfd3":"markdown","50661e27":"markdown","c744643b":"markdown","e8062a98":"markdown","3beace21":"markdown","a4695bd2":"markdown","c802da1a":"markdown","864c0b47":"markdown","d5c0d7f1":"markdown","a8ba7422":"markdown","c0bf6e56":"markdown","a410c850":"markdown","37908d5a":"markdown","f47c193a":"markdown","e25ebfce":"markdown","bd43948f":"markdown","51f6db91":"markdown","3d3bae67":"markdown","960b0d03":"markdown","4f0a0a2b":"markdown","6c90d469":"markdown","768d433f":"markdown","9b70674f":"markdown","9a99cf34":"markdown","d07ceba0":"markdown","d99ad4f0":"markdown","9629f47a":"markdown","336dee09":"markdown","675bffb1":"markdown","491e590d":"markdown","08b46f2f":"markdown","6d010df4":"markdown","b3c9dc57":"markdown","1ff56251":"markdown","d5b4e24d":"markdown","374373db":"markdown","82f0080e":"markdown","c261e00b":"markdown","b5d4fe40":"markdown","b95ef651":"markdown"},"source":{"d5c8329f":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS , ImageColorGenerator","d62d10b4":"url='..\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv'\ndf=pd.read_csv(url)\ndf.head()","ac4095b0":"df.dropna(inplace=True)","13244352":"from datetime import date\ndf['date']=pd.to_datetime(df['date'])\n#====\nL = ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter']\ndf = df.join(pd.concat((getattr(df['date'].dt, i).rename(i) for i in L), axis=1))\ndf['year'].value_counts()","467634ac":"df['hash'] = df['tweet'].apply(lambda word:word.count('#'))","8c5bcbc8":"df['men'] = df['tweet'].apply(lambda word:word.count('@'))","68ad2514":"df['tweet_length_ch']=df['tweet'].apply(lambda x:len(x))\ndf=df.loc[df['tweet_length_ch']<=280]\n\n#=== \ndf['tweet_length']=df['tweet_length_ch'].apply(lambda x:'short' if x <=130 else 'long')","0fdf4b93":"df['med'] = df['tweet'].apply(lambda word:word.count('https:\/\/t.co\/'))\ndf['med'].unique()","41ab0cc1":"df_copy=df.copy()\ndf_copy2=df.copy()","10194004":"sns.boxplot(df['tweet_length_ch'])","3a99b457":"plt.figure(figsize=(14,5))\niris = df_copy['tweet_length_ch']\nsns.kdeplot(data=iris)","e9642a1a":"insult_tw=df_copy.groupby('tweet',as_index=False).agg({'insult':'count'})\ninsult_tw.describe()","1013c757":"insult_tw_75 = insult_tw.loc[insult_tw['insult']==16]\nprint('Most tweet have insulted Targets is : ',insult_tw_75.values)","bcc821fe":"df_media=df_copy.loc[df_copy['target']=='the-media']\nprint('Most insult word with The Media was : ',df_media['insult'].value_counts()[:1])\n#==============\ntweet_All = \" \".join(insul for insul in df_media.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100,colormap=\"inferno\", background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","6431ba09":"df_bide=df_copy.loc[df_copy['target']=='joe-biden']\nprint('Most insult word with joe biden was : ',df_bide['insult'].value_counts()[:1])\n#==============\ntweet_All = \" \".join(insul for insul in df_bide.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100,colormap='gray', background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","ab0b04f4":"\ndf_hc=df_copy.loc[df_copy['target']=='hillary-clinton']\nprint('Most insult word with hillary-clinton was : ',df_hc['insult'].value_counts()[:1])\n#==============\ntweet_All = \" \".join(insul for insul in df_hc.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100,colormap=\"Blues\", background_color=\"skyblue\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","9bc775a2":"\ndf_trump_russia =df_copy.loc[df_copy['target']=='trump-russia']\nprint('Most insult word with df_trump_russia was : ',df_trump_russia['insult'].value_counts()[:1])\n#==============\n\ntweet_All = \" \".join(insul for insul in df_trump_russia.insult)\n\nfig, ax = plt.subplots(1, 1, figsize  = (12,10))\nwordcloud_ALL = WordCloud(max_font_size=50,colormap=\"Reds\", max_words=100, background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off');","8eaa7c5f":"plt.figure(figsize=(10,7))\nlabels = 'Long', 'Short'\nsizes = [8748,1610]\nexplode = (0.1, 0)  \nplt.figure(figsize=(10,5))\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90);\nplt.axis('equal');  ","e14f2132":"plt.figure(figsize=(14,7))\nsns.countplot(data=df,x='tweet_length',hue='men').set_title('Mention \/ Tweet Length');","d3f412b8":"plt.figure(figsize=(14,7))\nNo_Media= len(df[df['med']==0])\nMedia = len(df[df['med']>0])\nPlatform = ['NoMedia','Media']\nCount = [No_Media,Media]\n#====\nfig = px.pie(names = Platform,\n             values = Count,\n             title='Media\/No Media',\n            color_discrete_sequence = px.colors.sequential.Agsunset)\nfig.update_traces(textposition='inside', textinfo='percent+label')","31229843":"d3 = df_copy[['tweet_length_ch','men','hash','tweet_length']]\nhashtag=df_copy['hash'].values\nmention=df_copy['men'].values\nlength=df_copy['tweet_length_ch'].values\nL= df_copy['tweet_length'].values\ntrace = go.Scatter3d(x=hashtag,y=mention,z=length,mode='markers',marker=dict(size=5,color=\"crimson\"))\nfig=go.Figure(data=[trace])\nfig.show()","e8bc8107":"r_op =df['target'].value_counts()\nr_op = r_op[:10]\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(20,6));\nr_op_vis = sns.barplot(r_op.index, r_op.values, alpha=0.8,palette=\"inferno\");\nplt.title('Trump Targets',fontsize=15);\nplt.ylabel('insults', fontsize=12);\nplt.xlabel('Target', fontsize=12);\nr_op_vis.set_xticklabels(rotation=30,labels=r_op.index,fontsize=15);\nplt.show();","b51bbad7":"from IPython.core.display import HTML\nHTML('''<div class=\"flourish-embed flourish-chart\" data-src=\"visualisation\/5060515\"><script src=\"https:\/\/public.flourish.studio\/resources\/embed.js\"><\/script><\/div>''')","f1924b61":"\ntweet_All = \" \".join(insul for insul in df.insult)\n\n\nfig, ax = plt.subplots(1, 1, figsize  = (10,10))\n\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\n\nax.imshow(wordcloud_ALL, interpolation='bilinear')\n\nax.axis('off')","73a19b0d":"dftime = df.groupby('year',as_index=False).agg({'insult':'count'}).reset_index()\npx.line(x=dftime['year'],y=dftime['insult'],title='insult by year')","3804e71a":"dftime_dw = df.groupby('dayofweek',as_index=False).agg({'insult':'count'}).reset_index()\npx.line(x=dftime_dw['dayofweek'],y=dftime_dw['insult'],title='insult by Daysofweek')","5441badd":"dftime_q = df.groupby('quarter',as_index=False).agg({'insult':'count'}).reset_index()\npx.line(x=dftime_q['quarter'],y=dftime_q['insult'],title='insult by quarter')","0ca0a573":"tweets = df['tweet'].drop_duplicates()","fcb53c23":"all_sentences = []\n\nfor word in tweets:\n    all_sentences.append(word)\n\nall_sentences\n\nlines = list()\nfor line in all_sentences:    \n    words = line.split()\n    for w in words: \n       lines.append(w)","923e9867":"import re\n\nlines = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in lines]\n\nlines\n\nlines2 = []\n\nfor word in lines:\n    if word != '':\n        lines2.append(word)","bafd6520":"from nltk.stem.snowball import SnowballStemmer\ns_stemmer = SnowballStemmer(language='english')\n\nstem = []\nfor word in lines2:\n    stem.append(s_stemmer.stem(word))","eaf54721":"import spacy\nnlp = spacy.load('en_core_web_lg')","ddca0ef0":"stem2 = []\n\nfor word in stem:\n    if word not in nlp.Defaults.stop_words:\n        stem2.append(word)","fde8864c":"df = pd.DataFrame(stem2)\ndf = df[0].value_counts()","c2e1c1d9":"df = df[:20,]\n#== \npx.bar(df, x=df.values,y= df.index, color=df.index, height=500)","38a94983":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n#====== \ndef show_ents(doc):\n    if doc.ents:\n        for ent in doc.ents:\n            print(ent.text + ' - ' + ent.label_ + ' - ' + str(spacy.explain(ent.label_)))\n#======\nnlp = spacy.load('en_core_web_sm') \nnlp.max_length = 2000000000000\n#=====\nstr1 = \" \" \nstem2 = str1.join(lines2)\n\nstem2 = nlp(stem2)\n\nlabel = [(X.text, X.label_) for X in stem2.ents]\n\ndf6 = pd.DataFrame(label, columns = ['Word','Entity'])\n\ndf7 = df6.where(df6['Entity'] == 'ORG')\n\ndf7 = df7['Word'].value_counts()","344ac77b":"df = df7[:20,]\nplt.figure(figsize=(10,5))\npx.bar(df, x=df.values,y= df.index, color=df.index, height=500)","15e46636":"nlp = spacy.load('en_core_web_sm') \nnlp.max_length = 2000000000000\n\nstr1 = \" \" \nstem2 = str1.join(lines2)\n\nstem2 = nlp(stem2)\n\nlabel = [(X.text, X.label_) for X in stem2.ents]\n\ndf10 = pd.DataFrame(label, columns = ['Word','Entity'])\n\ndf10 = df10.where(df10['Entity'] == 'PERSON')\n\ndf11 = df10['Word'].value_counts()","e84e6c8f":"df = df11[:20,]\n\nplt.figure(figsize=(10,5))\n\ndf = df11[:20,]\nplt.figure(figsize=(10,5))\npx.bar(df, x=df.values,y= df.index, color=df.index, height=500)","07c734fe":"features=tweets.values\n#=== \nprocessed_features = []\n\nfor sentence in range(0, len(features)):\n    # Remove all the Http: urls\n    processed_feature = re.sub('(https?:\/\/\\S+)', '', str(features[sentence]))\n    \n    # Remove all the special characters\n    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n\n    # Remove all single characters\n    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n\n    # Remove single characters from the start\n    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n\n    # Substituting multiple spaces with single space\n    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n\n    # Removing prefixed 'b'\n    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n\n    # Converting to Lowercase\n    processed_feature = processed_feature.lower()\n\n    processed_features.append(processed_feature)\n","7c1cd254":"df3=pd.DataFrame()\ndf3['Tweets']=processed_features\n#=======\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\n# Create a function to get the subjectivity\ndef getSubjectivity(text):\n   return TextBlob(text).sentiment.subjectivity\n\n# Create a function to get the polarity\ndef getPolarity(text):\n   return  TextBlob(text).sentiment.polarity\n\n\n# Create two new columns 'Subjectivity' & 'Polarity'\ndf3['Subjectivity'] = df3['Tweets'].apply(getSubjectivity)\ndf3['Polarity'] = df3['Tweets'].apply(getPolarity)\n","28dfc213":"#Create a function to compute negative (-1), neutral (0) and positive (+1) analysis\ndef getAnalysis(score):\n if score < 0:\n  return 'Negative'\n elif score == 0:\n  return 'Neutral'\n else:\n  return 'Positive'\ndf3['Analysis'] = df3['Polarity'].apply(getAnalysis)\ndf3","15a58426":"Neutral = len(df3[df3['Analysis']=='Neutral'])\nNegative = len(df3[df3['Analysis']=='Negative'])\nPositive = len(df3[df3['Analysis']=='Positive'])\nlabels = ['Negative','Positive','Neutral']\nvalues = [Negative,Positive,Neutral]\n#====\nimport plotly.graph_objects as go\ncolors = ['red','green', 'lightblue' ]\n\nfig = go.Figure(data=[go.Pie(labels=labels,\n                             values=values)])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,textposition='inside',\n                  marker=dict(colors=colors, line=dict(color='grey', width=1)))\nfig.show()","f3bc95d8":"df_copy['tweet']=df_copy['tweet'].drop_duplicates(inplace=True)\ndf3['year']=df_copy['year']\n#=== \ndf_copy[['sentiment']]=df3['Analysis']\ndf_tim_sen = df_copy[['year','sentiment']]\ndf_copy['year'].value_counts()\n#=== \ndf_time_sen =pd.get_dummies(df_tim_sen).groupby('year').sum().reset_index()\ndf_time_sen =df_time_sen.sort_values('year',ascending=True)\n#=======\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(14,7))\nplt.plot(df_time_sen['year'] ,df_time_sen['sentiment_Negative'],marker='o',label='Negative') \nplt.plot(df_time_sen['year'] , df_time_sen['sentiment_Neutral'],color='blue',marker='*',label='Neutral')  \nplt.plot(df_time_sen['year'] ,df_time_sen['sentiment_Positive'],color='green',marker='+',label='Positive') \n#=== \nplt.annotate('High Negative insult tweets', xy=(2018, 850),  xycoords='data',\n            xytext=(0.8, 0.95), textcoords='axes fraction',\n            arrowprops=dict(facecolor='black', shrink=0.10),\n            horizontalalignment='center', verticalalignment='top',\n            )","3f336d3a":"df_copy2.drop_duplicates(subset=['tweet'])\ndf3['insult']=df_copy2['insult']\ndf3['target']=df_copy2['target']\ndf3['med']=df_copy2['med']\ndf3['tweet_length']=df_copy2['tweet_length']\n#==== \nplt.figure(figsize=(14,5))\nsns.countplot(x='Analysis',data=df3,hue='tweet_length',palette=\"inferno\")","0b94861a":"plt.figure(figsize=(14,5))\nsns.countplot(x='Analysis',data=df3,hue='med',palette=\"Oranges\")","ecc990aa":"from IPython.core.display import HTML\nHTML('''<div class=\"flourish-embed flourish-cards\" data-src=\"visualisation\/5123150\"><script src=\"https:\/\/public.flourish.studio\/resources\/embed.js\"><\/script><\/div>''')","d67e6b43":"# Libraries ","d8c27151":"<a href=\"https:\/\/imgur.com\/mXAQKkV\"><img src=\"https:\/\/i.imgur.com\/mXAQKkV.jpg\" title=\"source: imgur.com\" \/><\/a>","bcbe5378":"<a href=\"https:\/\/imgur.com\/shNBFGq\"><img src=\"https:\/\/i.imgur.com\/shNBFGq.jpg\" title=\"source: imgur.com\" \/><\/a>","06461ac1":"# Data Manipulation","ca0e5d57":"## Most Targets in a Tweet ","8bbff0b5":"<img src=\"https:\/\/media.giphy.com\/media\/xTiTnHXbRoaZ1B1Mo8\/source.gif\">","040af942":"# Top mention People","f17ccfd3":"## Media in tweets ","50661e27":"### Gettig Words roots","c744643b":"<a href=\"https:\/\/imgur.com\/avUKB1f\"><img src=\"https:\/\/i.imgur.com\/avUKB1f.png\" title=\"source: imgur.com\" \/><\/a>","e8062a98":"# Top Mention Organizations\n","3beace21":"# EDA ","a4695bd2":"![](https:\/\/s3-eu-west-1.amazonaws.com\/tutor2u-media\/subjects\/politics\/Democrats.png?mtime=20150924080302)","c802da1a":"## Media in Sentiments ","864c0b47":"### Count Hashtags in Tweets ","d5c0d7f1":"![](https:\/\/www.pngitem.com\/pimgs\/m\/41-412092_as-seen-on-abc-cbs-fox-nbc-cnn.png)","a8ba7422":"### X = Hashtag                      ,            Y = Mention            ,  Z = Tweet Length","c0bf6e56":"### Drop Nan","a410c850":"## 3D Length-Hashtag-Mentions","37908d5a":"## The Media","f47c193a":"## Check Tweets Length ","e25ebfce":"## Joe Biden","bd43948f":"# Data Visualization( EDA ) ","51f6db91":"## Mentions in Tweet Length ","3d3bae67":"# Sentiment Analysis","960b0d03":"### Tweet Length Characters & Class","4f0a0a2b":"### Count Mentions in Tweets ","6c90d469":"#  Conclusion \n\n\n\n* Most of The Insult Tweets about Fake News and Democrats \n\n* Trump use  The  sarcastic expressions to insult The others or situations ( Crooked Hillary - Sleepy Joe - Witch Hunt )\n\n* 25% of trump insults tweets have more than 2 insults\n\n* Tweets have more insults when Trump talks about media and newspapers\n\n* Trump use insults as a style in defence of any person or institution\n\n* 85% of Trump insult tweets Length is long \n\n* only 10% of  Trump insult tweets have no media \n\n* Most target people Hillary Clinton, Joe Biden and Adam Schiff\n\n* More insults tweets started from 2017 \n\n* Trump typed more insults tweets in weekends \n\n* Trump typed more insults tweets in the last quarter of the year\n\n* Trump insults newspapers and democratic the most \n\n* 50% of tweets are Negative \n\n* Trump  write more insult negative tweets in 2018  ","768d433f":"###  Media ","9b70674f":"## Hillary-Clinton","9a99cf34":"<a href=\"https:\/\/imgur.com\/8kjeKny\"><img src=\"https:\/\/i.imgur.com\/8kjeKny.png\" title=\"source: imgur.com\" \/><\/a>","d07ceba0":"### Adding Subjectivity & Polarity","d99ad4f0":"## Tweet Length Distribution ","9629f47a":"# ADVANCED ANIMATED TARGET CARDS \n### Shows Top Targets in Trump insult Tweets and how Trump insult them ","336dee09":"## Tweets Length in Sentiments ","675bffb1":"### Removing characters","491e590d":"# Top Mention Keywords","08b46f2f":"<a href=\"https:\/\/imgur.com\/NzM9SSq\"><img src=\"https:\/\/i.imgur.com\/NzM9SSq.png\" title=\"source: imgur.com\" \/><\/a>","6d010df4":"# Sentiments By Time ","b3c9dc57":"## Tweet Length Class - Pie Chart","1ff56251":"## Most insults appears ","d5b4e24d":"### Removing Punctuation\n","374373db":"# Sentiment Analysis ","82f0080e":"## Top 10 Targets ","c261e00b":"## Time Series columns ","b5d4fe40":"## Russia-Trump","b95ef651":"## Most Target People by year  "}}