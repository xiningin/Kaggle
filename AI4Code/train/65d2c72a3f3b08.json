{"cell_type":{"dc45f1a5":"code","e1d6e9ca":"code","79e1a4e6":"code","44dd8a13":"code","b2106557":"code","7f3c50d9":"code","da08e4a2":"code","9c24655b":"code","b6a75461":"code","9301b2ae":"code","109a0aac":"code","03966f21":"code","5c7942ce":"code","3bd41813":"code","ed00bbff":"code","f57fe92b":"code","9bac0428":"markdown"},"source":{"dc45f1a5":"!pip uninstall -y lightgbm\n!apt-get install -y libboost-all-dev\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n\n","e1d6e9ca":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","79e1a4e6":"\n\n!cd LightGBM\/python-package\/;python setup.py install --precompile\n","44dd8a13":"\n!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","b2106557":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LinearRegression","7f3c50d9":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {\n        'random_state': 1, \n        'booster': 'gbtree',\n        'n_estimators': 10000,\n        'learning_rate': 0.03628302216953097,\n        'reg_lambda': 0.0008746338866473539,\n        'reg_alpha': 23.13181079976304,\n        'subsample': 0.7875490025178415,\n        'colsample_bytree': 0.11807135201147481,\n        'max_depth': 3\n    }\n    \n    model = XGBRegressor(\n        n_jobs=4,tree_method='gpu_hist',\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"test_pred_1.csv\", index=False)","da08e4a2":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    lgb_params = {'reg_alpha': 0.00016893787477692912,\n                 'reg_lambda': 7380.524142310997,\n                 'learning_rate': 0.05382799531839089,\n                 'max_depth': 4,\n                 'n_estimators': 9900,\n                 'min_child_weight': 10.617199131630898,\n                 'subsample': 0.2,\n                 'colsample_bytree': 0.2,'n_jobs':4}\n    \n    model = LGBMRegressor(\n        random_state=fold,\n        **lgb_params,device_type= 'gpu',metric = 'RMSE'\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"test_pred_2.csv\", index=False)","9c24655b":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    lgbm_parameters = {\n        'metric': 'RMSE',\n        'feature_pre_filter': False,\n        'lambda_l1': 0.45,\n        'lambda_l2': 4.8,\n        'learning_rate': 0.005,\n        'num_trees': 80000,\n        'num_leaves': 10, \n        'feature_fraction': 0.4, \n        'bagging_fraction': 1.0, \n        'bagging_freq': 0, \n        'min_child_samples': 100,\n        'num_threads': 4\n    }\n    \n    model = LGBMRegressor(\n        random_state=fold,\n        **lgbm_parameters,device= \"gpu\",\n        gpu_platform_id= 0,\n        gpu_device_id= 0\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"test_pred_3.csv\", index=False)","b6a75461":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {\n        'learning_rate': 0.07853392035787837,\n        'reg_lambda': 1.7549293092194938e-05,\n        'reg_alpha': 14.68267919457715, \n        'subsample': 0.8031450486786944, \n        'colsample_bytree': 0.170759104940733, \n        'max_depth': 3\n    }\n    \n    model = XGBRegressor(\n        random_state=fold,\n        n_jobs=4,\n        n_estimators=5000,tree_method='gpu_hist',\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_4\"]\nfinal_valid_predictions.to_csv(\"train_pred_4.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_4\"]\nsample_submission.to_csv(\"test_pred_4.csv\", index=False)","9301b2ae":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    lgbm_parameters_1 = {\n    'metric': 'RMSE',\n    'feature_pre_filter': False,\n    'reg_alpha': 0.4972562469417825, \n    'reg_lambda': 0.3273637203281044, \n    'num_leaves': 50, \n    'learning_rate': 0.032108486615557354,                      \n    'max_depth': 40,                     \n    'n_estimators': 4060, \n    'min_child_weight': 0.0173353329222102,\n    'subsample': 0.9493343850444064, \n    'colsample_bytree': 0.5328221263825876, \n    'min_child_samples': 80\n}\n    \n    model = LGBMRegressor(\n        random_state=fold,\n        n_jobs=4,\n        **lgbm_parameters_1,device= \"gpu\",\n        gpu_platform_id= 0,\n        gpu_device_id= 0\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_5\"]\nfinal_valid_predictions.to_csv(\"train_pred_5.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_5\"]\nsample_submission.to_csv(\"test_pred_5.csv\", index=False)","109a0aac":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    xgb_params = {'objective': 'reg:squarederror',\n              'n_estimators': 10000,\n              'learning_rate': 0.25,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'grow_policy':'lossguide',\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'n_jobs': 4}\n\n    \n    model = XGBRegressor(\n        **xgb_params,tree_method='gpu_hist'\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_6\"]\nfinal_valid_predictions.to_csv(\"train_pred_6.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_6\"]\nsample_submission.to_csv(\"test_pred_6.csv\", index=False)","03966f21":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    xgb_params = {'max_depth': 1,\n                 'learning_rate': 0.8616220572782383,\n                 'n_estimators': 12900,\n                 'eta': 0.011828729736491916,\n                 'subsample': 0.9,\n                 'colsample_bytree': 0.7,\n                 'colsample_bylevel': 0.30000000000000004,\n                 'min_child_weight': 1.5207698213399512,\n                 'reg_lambda': 0.0005791179588496165,\n                 'reg_alpha': 0.17560065566511068,\n                 'gamma': 0.1553307899331432,'n_jobs':4}\n\n    \n    model = XGBRegressor(\n        random_state=fold,tree_method='gpu_hist',\n        **xgb_params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_7\"]\nfinal_valid_predictions.to_csv(\"train_pred_7.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_7\"]\nsample_submission.to_csv(\"test_pred_7.csv\", index=False)","5c7942ce":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 10000,\n        \"early_stopping_round\": 300,\n        \"device\": \"gpu\",\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n    }\n    \n    param2 = {\n        'lambda_l1': 0.00472279780583036, \n        'lambda_l2': 2.9095205689488508e-05, \n        'num_leaves': 158, \n        'feature_fraction': 0.7386878356648194, \n        'bagging_fraction': 0.8459744550725283, \n        'bagging_freq': 2, \n        'max_depth': 2, \n        'max_bin': 249, \n        'learning_rate': 0.044738463593017294,\n        'min_child_samples': 13\n    }\n    param.update(param2)\n    \n    model = LGBMRegressor(\n        random_state=fold,\n        **param\n    )\n    model.fit(xtrain, ytrain, eval_set=[(xvalid, yvalid)])\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_8\"]\nfinal_valid_predictions.to_csv(\"train_pred_8.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_8\"]\nsample_submission.to_csv(\"test_pred_8.csv\", index=False)","3bd41813":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"train_pred_1.csv\")\ndf2 = pd.read_csv(\"train_pred_2.csv\")\ndf3 = pd.read_csv(\"train_pred_3.csv\")\ndf4 = pd.read_csv(\"train_pred_4.csv\")\ndf5 = pd.read_csv(\"train_pred_5.csv\")\ndf6 = pd.read_csv(\"train_pred_6.csv\")\ndf7 = pd.read_csv(\"train_pred_7.csv\")\ndf8 = pd.read_csv(\"train_pred_8.csv\")\n\ndf_test1 = pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"test_pred_3.csv\")\ndf_test4 = pd.read_csv(\"test_pred_4.csv\")\ndf_test5 = pd.read_csv(\"test_pred_5.csv\")\ndf_test6 = pd.read_csv(\"test_pred_6.csv\")\ndf_test7 = pd.read_csv(\"test_pred_7.csv\")\ndf_test8 = pd.read_csv(\"test_pred_8.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\ndf = df.merge(df6, on=\"id\", how=\"left\")\ndf = df.merge(df7, on=\"id\", how=\"left\")\ndf = df.merge(df8, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test6, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test7, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test8, on=\"id\", how=\"left\")\ndf.head()","ed00bbff":"useful_features = [\"pred_1\", \"pred_2\",'pred_3','pred_4','pred_5','pred_6','pred_7','pred_8']\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model = LinearRegression()\n    model.fit(xtrain, ytrain)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","f57fe92b":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","9bac0428":"## Notebook forked from [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek\/blending-blending-blending)"}}