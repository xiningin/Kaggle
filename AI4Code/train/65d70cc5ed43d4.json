{"cell_type":{"fab11db8":"code","0ec0ae05":"code","80ea3680":"code","c112473e":"code","ff03ee53":"code","d32d192d":"code","d1093f5e":"code","ba36e811":"code","f0e93053":"code","c9de8fdc":"code","9a69552b":"code","bf4ca9b5":"code","6b05bfb9":"code","5153ef09":"code","6d7e28e1":"code","85c13ab2":"code","aea541f5":"code","99cf0b5e":"code","f0aeb560":"code","e1669b08":"code","b519f1a1":"code","52843945":"code","ad53cbad":"code","93500825":"code","7089c04b":"code","8f4d4536":"code","27c1cd82":"code","36a2bf4c":"code","0d02925e":"code","5f668582":"code","56d984b5":"code","1b2981de":"code","83a8d9d7":"code","986b2583":"code","3a5a8a42":"code","2a521f80":"code","c74b1e83":"code","3e0c0d0d":"code","bad94c27":"code","a2f9a1cf":"code","8dea4b04":"code","3588ebfc":"code","ba2642ff":"code","6807772a":"code","3956256d":"code","f98e9257":"code","af276869":"code","a38db3f5":"code","6b358758":"code","26a65ae5":"code","451fd5f0":"code","9d15cfdc":"code","1fa8d14e":"code","0aafef23":"code","99ffd824":"code","009eb4fd":"code","32046084":"code","e170fcc6":"code","619fa881":"code","0536f476":"code","b74be8b7":"code","23f86d5a":"code","5799d105":"code","f2c4f7e2":"code","7a542326":"code","14a77ef0":"markdown","47b843fe":"markdown","0def5157":"markdown","9fec911a":"markdown","ffef4f0d":"markdown","dd90a76a":"markdown","e6a379b1":"markdown","9ed02f3e":"markdown","7be18252":"markdown","e4aed996":"markdown","e6b11fc1":"markdown","0d2ceaa6":"markdown","8e648158":"markdown","b38326b2":"markdown","e1b5cfc9":"markdown"},"source":{"fab11db8":"import pandas as pd\nimport numpy as np\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler , Normalizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","0ec0ae05":"df = pd.read_csv(\"..\/input\/covid19-symptoms-checker\/Cleaned-Data.csv\")\n\npd.pandas.set_option('display.max_columns',None)","80ea3680":"display(\"Peeking into Data\", df)","c112473e":"display(\"Shape of dataset\")\nprint(\"Rows:\",df.shape[0],\"\\nColumns:\",df.shape[1])","ff03ee53":"display(\"NULL Values\", df.isnull().sum())","d32d192d":"display(\"Description\",df.describe())","d1093f5e":"df.info()","ba36e811":"#df = df.drop('Country',axis=1)\nsns.distplot(df.drop('Country',axis=1))","f0e93053":"for i in df.columns:\n    print(\"\\nColumn Name:\",i,\"-->\",df[i].unique(),\"-->Unique Count\",len(df[i].unique()))","c9de8fdc":"severity_columns = df.filter(like='Severity_').columns","9a69552b":"df['Severity_None'].replace({1:'None',0:'No'},inplace =True)\ndf['Severity_Mild'].replace({1:'Mild',0:'No'},inplace =True)\ndf['Severity_Moderate'].replace({1:'Moderate',0:'No'},inplace =True)\ndf['Severity_Severe'].replace({1:'Severe',0:'No'},inplace =True)","bf4ca9b5":"df['Condition']=df[severity_columns].values.tolist()","6b05bfb9":"def removing(list1):\n    list1 = set(list1) \n    list1.discard(\"No\")\n    a = ''.join(list1)\n    return a","5153ef09":"df['Condition'] = df['Condition'].apply(removing)","6d7e28e1":"age_columns = df.filter(like='Age_').columns\ngender_columns = df.filter(like='Gender_').columns\ncontact_columns = df.filter(like='Contact_').columns","85c13ab2":"No_risk_age = df.groupby(['Severity_None'])[age_columns].sum()\nNo_risk_gender = df.groupby(['Severity_None'])[gender_columns].sum()\nNo_risk_contact = df.groupby(['Severity_None'])[contact_columns].sum()","aea541f5":"Low_risk_age = df.groupby(['Severity_Mild'])[age_columns].sum()\nLow_risk_gender = df.groupby(['Severity_Mild'])[gender_columns].sum()\nLow_risk_contact = df.groupby(['Severity_Mild'])[contact_columns].sum()","99cf0b5e":"Moderate_risk_age = df.groupby(['Severity_Moderate'])[age_columns].sum()\nModerate_risk_gender = df.groupby(['Severity_Moderate'])[gender_columns].sum()\nModerate_risk_contact = df.groupby(['Severity_Moderate'])[contact_columns].sum()","f0aeb560":"Severe_risk_age = df.groupby(['Severity_Severe'])[age_columns].sum()\nSevere_risk_gender = df.groupby(['Severity_Severe'])[gender_columns].sum()\nSevere_risk_contact = df.groupby(['Severity_Severe'])[contact_columns].sum()","e1669b08":"sns.countplot(df['Condition'])","b519f1a1":"df.drop(\"Country\",axis=1,inplace=True)","52843945":"df.drop(severity_columns,axis=1,inplace=True)","ad53cbad":"df['Symptoms_Score'] = df.iloc[:,:5].sum(axis=1) + df.iloc[:,6:10].sum(axis=1)","93500825":"df.shape","7089c04b":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['Condition'] = le.fit_transform(df['Condition'])","8f4d4536":"df","27c1cd82":"from pylab import rcParams\nrcParams['figure.figsize'] = 13, 18\ncorrmat = df.corr()\nk = 22\ncols = corrmat.nlargest(k, 'Condition')['Condition'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","36a2bf4c":"X= df.drop(['Condition'],axis=1)\ny= df['Condition']","0d02925e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","5f668582":"'''from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)'''","56d984b5":"'''# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)'''","1b2981de":"'''print('Best Parameters',grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\nprint('\\n Best Estimator',best_grid)'''","83a8d9d7":"\"\"\"Best Parameters {'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 100}\nBest Estimator RandomForestClassifier(max_depth=4, max_features='sqrt')\"\"\"","986b2583":"from sklearn.ensemble import RandomForestClassifier\nrfc1=RandomForestClassifier(criterion= 'gini', max_depth= 4, max_features= 'sqrt', n_estimators= 100)","3a5a8a42":"rfc1.fit(X_train, y_train)","2a521f80":"pred=rfc1.predict(X_test)","c74b1e83":"pred","3e0c0d0d":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy for Random Forest on CV data: \",accuracy_score(y_test,pred))","bad94c27":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,pred)","a2f9a1cf":"!pip install catboost","8dea4b04":"from catboost import CatBoostClassifier","3588ebfc":"model = CatBoostClassifier(iterations=200)","ba2642ff":"categorical_var = np.where(X_train.dtypes != np.float)[0]\nprint('\\nCategorical Variables indices : ',categorical_var)","6807772a":"model.fit(X_train,y_train,cat_features = categorical_var,plot=False)","3956256d":"predict_train = model.predict(X_train)\nprint('\\nTarget on train data',predict_train)","f98e9257":"accuracy_train = accuracy_score(y_train,predict_train)\nprint('\\naccuracy_score on train dataset : ', accuracy_train)","af276869":"predict_test = model.predict(X_test)\nprint('\\nTarget on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(y_test,predict_test)\nprint('\\naccuracy_score on test dataset : ', accuracy_test)","a38db3f5":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver = 'lbfgs')\nmodel.fit(X_train, y_train)","6b358758":"# use the model to make predictions with the test data\ny_pred = model.predict(X_test)","26a65ae5":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","451fd5f0":"'''from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=23)\nknn.fit(X_train, y_train)'''","9d15cfdc":"'''y_pred_knn = knn.predict(X_test)'''","1fa8d14e":"'''from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred_knn)'''","0aafef23":"'''from sklearn.svm import SVC\n\nsvm = SVC(kernel='linear',C=0.025, random_state=101)\n\nsvm.fit(X_train, y_train)'''","99ffd824":"'''y_pred_svc = svc.predict(X_test)'''","009eb4fd":"'''from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred_svc)'''","32046084":"from sklearn.naive_bayes import MultinomialNB\n\nmb = MultinomialNB()\n\nmb.fit(X_train, y_train)","e170fcc6":"y_pred_mb = mb.predict(X_test)","619fa881":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred_mb)","0536f476":"from keras.utils.np_utils import to_categorical\ny_train = to_categorical(y_train, num_classes = 4)\ny_train.shape","b74be8b7":"from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout\nfrom keras.models import Sequential,Model\nfrom keras.optimizers import SGD\nfrom keras.callbacks import ModelCheckpoint,LearningRateScheduler\nimport keras\nfrom keras import backend as K","23f86d5a":"model=keras.models.Sequential()\n#model.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(128,activation='relu'))\nmodel.add(keras.layers.Dense(128,activation='relu'))\nmodel.add(keras.layers.Dense(128,activation='relu'))\nmodel.add(keras.layers.Dense(128,activation='relu'))\nmodel.add(keras.layers.Dense(128,activation='relu'))\nmodel.add(keras.layers.Dense(4,activation='softmax'))","5799d105":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n                   \nmodel.fit(X_train, y_train,epochs=10, batch_size=32, verbose=1)","f2c4f7e2":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred,axis=1)","7a542326":"y_pred","14a77ef0":"- Catboost","47b843fe":"# EDA","0def5157":"## `Checking distribution of data`","9fec911a":"- Random Forest","ffef4f0d":"- Neural network","dd90a76a":"# Preprocessing","e6a379b1":"## `Grouping by severity`","9ed02f3e":"# Importing Libraries","7be18252":"- Logistic Regression","e4aed996":"# Feature Engineering","e6b11fc1":"# Model","0d2ceaa6":"# Loading Data","8e648158":"## `Getting to know data`","b38326b2":"## `Size of data`","e1b5cfc9":"## `NULL Values`"}}