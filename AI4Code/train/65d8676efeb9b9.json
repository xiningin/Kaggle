{"cell_type":{"2a215780":"code","d0009140":"code","eaeaa930":"code","eddedba2":"code","7c48421f":"code","ed1f3a83":"code","e4032247":"code","59e41483":"code","c0ef105b":"code","86598416":"code","58431134":"code","f2551752":"code","1e6e2978":"code","065c0241":"code","76b4c67d":"code","b44338a6":"code","a89b0455":"code","1da415b4":"code","7c9ecb03":"code","37cb57bf":"code","f738b52b":"code","f7c34af6":"code","e9077879":"code","6b14576c":"code","e560b5e0":"code","adb01d0b":"code","c01cb86b":"code","c5ed2405":"code","87f82720":"code","482effb6":"code","0a12fb62":"code","268d8e28":"markdown","0a524bbc":"markdown","beef837d":"markdown","f7872eda":"markdown","5fb1277c":"markdown","06b71085":"markdown","5d2a0eca":"markdown","961605e4":"markdown","d6456532":"markdown","fcdda927":"markdown","59db1e81":"markdown","4ad23321":"markdown","e177f730":"markdown","89d8210e":"markdown","5c1db68c":"markdown","12c4fce0":"markdown","6bee0c2c":"markdown","cc87b2cb":"markdown","3ec46ab6":"markdown","9075ff22":"markdown","ad6af30a":"markdown","c37b7c65":"markdown"},"source":{"2a215780":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\n#Statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\n\n#Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, PolynomialFeatures, Normalizer\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","d0009140":"#Look at 5 random dataframe samples\ndf=pd.read_csv('..\/input\/xAPI-Edu-Data.csv')\ndf.sample(5)","eaeaa930":"df.info()","eddedba2":"n_uniques=df.select_dtypes(include=object).nunique()\nn_uniques[n_uniques==2]","7c48421f":"df.Class.value_counts(normalize=True)","ed1f3a83":"continuous_variables=df.columns[df.dtypes==int]\nplt.figure(figsize=(10,7))\nfor i, column in enumerate(continuous_variables):\n    plt.subplot(2,2, i+1)\n    sns.distplot(df[column], label=column, bins=10, fit=norm)\n    plt.ylabel('Density');","e4032247":"plt.figure(figsize=(10,7))\nfor i, column in enumerate(continuous_variables):\n    plt.subplot(2,2, i+1)\n    df[column]=boxcox1p(df[column], 0.3)\n    sns.distplot(df[column], label=column, bins=10, fit=norm)\n    plt.ylabel('Density')","59e41483":"df['raisedhands_bin']=np.where(df.raisedhands>df.raisedhands.mean(),1,0)\ndf['VisITedResources_bin']=np.where(df.VisITedResources>df.VisITedResources.mean(),1,0)","c0ef105b":"plt.figure(figsize=(10,7))\nfor i, column in enumerate(continuous_variables):\n    plt.subplot(2,2,i+1)\n    sns.boxplot(x=df.Class, y=df[column]);","86598416":"plt.figure(figsize=(7,5))\nsns.heatmap(df.corr(), annot=True, fmt='.1g', cmap='RdBu');","58431134":"sns.pairplot(df);","f2551752":"categorical_variables=df.columns[df.dtypes==object]\nprint('Percent of students\\' nationality - Kuwait or Jordan: {}'.format(\n            round(100*df.NationalITy.isin(['KW','Jordan']).sum()\/df.shape[0],2)))\nprint('Percent of students, who was born in Kuwait or Jordan: {}'.format(\n            round(100*df.PlaceofBirth.isin(['KuwaIT','Jordan']).sum()\/df.shape[0],2)))\nprint('Percent of studets, who has same nationality and place of birth: {}'.format(\n            round(100*(df.NationalITy==df.PlaceofBirth).sum()\/df.shape[0])))","1e6e2978":"df['NationalITy'][df['NationalITy']=='KW']='KuwaIT'\npb_count=pd.DataFrame(df.PlaceofBirth.value_counts(normalize=True)*100)\npb_count.reset_index(inplace=True)\nnt_count=pd.DataFrame(df.NationalITy.value_counts(normalize=True)*100)\nnt_count.reset_index(inplace=True)\npb_nt_count=pd.merge(nt_count, pb_count, on='index')\npb_nt_count.rename(columns={'index':'Country'}, inplace=True)\npb_nt_count","065c0241":"plt.figure(figsize=(14,5))\nfor i, column in enumerate(df[['NationalITy','PlaceofBirth']]):\n    data=df[column].value_counts().sort_values(ascending=False)\n    plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index);","76b4c67d":"#Rename all coutries with percentage less that 4% to 'Other'\nsmall_countries=list(pb_nt_count['Country'][(pb_nt_count.PlaceofBirth<4)&(pb_nt_count.NationalITy<4)])\n\nfor column in ['PlaceofBirth', 'NationalITy']:\n    df[column][df[column].isin(small_countries)]='Other'\n    \nprint('After renaming unique values are {}'.format(df.PlaceofBirth.unique()))","b44338a6":"plt.figure(figsize=(14,5))\nfor i, column in enumerate(df[['GradeID','Topic']]):\n    data=df[column].value_counts().sort_values(ascending=False)\n    plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index);","a89b0455":"plt.figure(figsize=(15,8))\nfor i, column in enumerate(categorical_variables.drop(['NationalITy','PlaceofBirth','GradeID','Topic','Class'])):\n    plt.subplot(2,4,i+1)\n    sns.countplot(df[column]);","1da415b4":"plt.figure(figsize=(15,12))\nfor i, column in enumerate(categorical_variables.drop(['GradeID','Topic','Class'])):\n    plt.subplot(4,3,i+1)\n    sns.countplot(x=df.Class, hue=df[column]);","7c9ecb03":"#Cut of target variable from dataset\ntarget=df['Class']\ndf=df.drop('Class', axis=1)","37cb57bf":"#Create new feature - type of topic (technical, language, other)\nTopic_types={'Math':'technic', 'IT':'technic','Science':'technic','Biology':'technic',\n 'Chemistry':'technic', 'Geology':'technic', 'Arabic':'language', 'English':'language',\n 'Spanish':'language','French':'language', 'Quran':'other' ,'History':'other'}\ndf['Topic_type']=df.Topic.map(Topic_types)","f738b52b":"for column in continuous_variables:\n    SS=StandardScaler().fit(df[[column]])\n    df[[column]]=SS.transform(df[[column]])","f7c34af6":"categorical_variables=df.select_dtypes(include='object').columns\nfor column in categorical_variables:\n    #Binarize and LabelEncode\n    #\u041a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u0443 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 2 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0438 StageID, GradeID, \u0442\u0430\u043a \u043a\u0430\u043a \u0432 \u043d\u0438\u0445 \u0432\u0430\u0436\u0435\u043d \u043f\u043e\u0440\u044f\u0434\u043e\u043a\n    if (df[column].value_counts().shape[0]==2) | (column=='StageID') | (column=='GradeID'):\n        le=LabelEncoder().fit(df[column])\n        df[column]=le.transform(df[column])\n\n#One-hot encoding\ndf=pd.get_dummies(df)","e9077879":"X_train, X_test, y_train, y_test = train_test_split(df, target, test_size=0.20, random_state=42)\n\ndef modelling(model):\n    model.fit(X_train, y_train)\n    preds=model.predict(X_test)\n    print('Accuracy = {}'.format(100*round(accuracy_score(y_test,preds),2)))\n    print(classification_report(y_test, preds))\n    plt.figure(figsize=(7,5))\n    sns.heatmap(confusion_matrix(y_test,preds), annot=True, vmax=50)\n    plt.show()","6b14576c":"modelling(make_pipeline(PolynomialFeatures(2),LogisticRegression(random_state=42, C=0.1)))","e560b5e0":"modelling(XGBClassifier(n_estimators=100, n_jobs=-1, learning_rate=0.03));","adb01d0b":"modelling(KNeighborsClassifier(n_neighbors=25, n_jobs=-1))","c01cb86b":"modelling(DecisionTreeClassifier(random_state=42, max_depth=5))","c5ed2405":"modelling(RandomForestClassifier(n_estimators=2000, n_jobs=-1, max_depth=6, random_state=42))","87f82720":"modelling(SVC(random_state=42, C=10, kernel='rbf', degree=3, gamma=0.1))","482effb6":"modelling(LGBMClassifier(learning_rate=0.02,random_state=42, n_estimators=2000))","0a12fb62":"criterions=['gini','entropy']\nmax_depth=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15, None]\nmax_features=[1,2,3,4,5, None]\ncriterions=['gini','entropy']\nmin_samples_spilts=[2,3,4,5]\nmin_samples_leafs=[1,2,3,4]\nclass_weights=['balanced',None]\n\nmax_accuracy=0\nbest_params=None\nbest_model=None\nfor class_weight in class_weights:\n    for min_sample_leaf in min_samples_leafs:\n        for min_samples_spilt in min_samples_spilts:\n            for crit in criterions:\n                for splitter in splitters:\n                    for depth in max_depth:\n                        for feature in max_features:\n                            DT=DecisionTreeClassifier(class_weight=class_weight,min_samples_leaf=min_sample_leaf,\n                                max_depth=depth, min_samples_split=min_samples_spilt, criterion=crit, splitter=splitter,\n                                max_features=feature, random_state=42)\n                            DT.fit(X_train, y_train)\n                            acc=accuracy_score(y_test,DT.predict(X_test))\n                            if acc>max_accuracy:\n                                max_accuracy=acc\n                                best_model=DT\n                                best_params=DT.get_params()\nprint(\"Best accuracy at validation set is: {}%\".format(round(100*max_accuracy,2)))","268d8e28":"## Categorical variables","0a524bbc":"Then we can define relations of categorical variables with target variable  ","beef837d":"* Features doesn't have gaussian (normal) distribution.\n* As ML algorithms deal better with values, which are normally distributed, we need to transfrom them closer that view. BoxCox transformation will help us with it - http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html","f7872eda":"We have Multiclass classification task, as there are 3 targets (H,M,L).\nWe have almost half of samples of class M, while L and H take only 25% of dataset each. Dataset is imbalanced, but not badly. Therefore, it would be better to use F1-score metric, rather than just Accuracy. It will give more precise result","5fb1277c":"Due to that information, it would be better to combine countries with small percentage (<4%) of representation in dataset into one common category","06b71085":"## Exploratory data analysis\nIn this part we will analyze data and make hypothesises.","5d2a0eca":"Now we check boxplots for numerical features with target:\n* There is good division of classes for raisedhands. Students who get high marks raise hands much more than students with low marks. M-s are in the middle.\n* The same situation for VisitedResources. These two features will give high predictive power.\n* Last two features have similar pattern, but with less clear break.","961605e4":"Let's look at correlation between these features:\n* VisitedResources, RaisedHands \u0438 AnnouncementViews have medium correlation (0.5-0.7)\n* In other cases there is weak correlation\nSo, we don't need to worry about multicollinearity, which is important for linear algorithms","d6456532":"## Feature Engineering\nNow we can prepare data before using it in ML:\n* Encoding of categorical variables (Binarization\/LabelEncoding\/One-hot Encoding)\n* Standardization\/Normalization of numerical variables\n* Extraction of new variables","fcdda927":"* Amount of absence days highly affects students' perfomance\n* For most of good students (H) responsible person is mother, for bad students - father\n* Parents of H students regularly answer survey and they are mostly satisfied with school, while for L students situation is opposite\n* At the first semester number of L students was bigger than number of H students. It is possible, that all students start learning better (Ls move to M, Ms move to H)\n* According to gender, boys are prone to bad study, girls - to good\n* Variables StageId \u0438 GradeId are almost the same, in modeling we can drop one of them","59db1e81":"## Primary analysis of data\nLet's look at main information about dataset we have","4ad23321":"* There are 480 entries without missings, so we don't have to struggle with filling\/dropping NaNs\n* We have 16 features, where 5 of them are numerical and 11 - categorical","e177f730":"## Data loading\nFirstly we load dataset and look at 5 random entries","89d8210e":"## Modeling","5c1db68c":"Let's look at distribution of other variables","12c4fce0":"* More than 70% of students are from Kuweit and Jordan\n* More than half of students were born in their homelands","6bee0c2c":"### Continuous variables\n\nMake visual analysis of data and let's begin from numerical features:","cc87b2cb":"6 of categorical features have only 2 unique values. It means that at the section of feature engineering we will binarize them to 0 and 1. Now let's look at target variable","3ec46ab6":"However raisedhands and visitedresourses have double gaussian distribution. We can create new binary features for them, where 1 is when values more than its' average, and 0 - less. Then we can look how these features will improve model","9075ff22":"Encoding of categorical variables:\n* Binary variables translate into the form 0\/1 (gender, number of passes, etc.)\n* Order variables will be re-encoded by LabelEncoding (GradeID, StageID) to preserve their order\n* The remaining variables are encoded by the one-hot encoding method.","ad6af30a":"This notebook covers full pipeline of data science project in students perfomance dataset\n### Main steps:\n* Loading, cleaning, wrangling\n* Primary analysis\n* Exploratory data analysis\n    * looking at distribution of dataset\n    * making hypothesises about data, which can help in prediction\n* Feature engineering\n    * transformation of data to gaussian distribution\n    * standardization and normalization\n    * feature extraction\n    * selecting best features\n* Modeling\n    * using various ML algorithms\n    * try cross-validation\n    * improve models by tuning hyperparametes using grid-search\n* Evaluating \n","c37b7c65":"Standardization of continuous variables is necessary to eliminate the scalability problem.\nWe transfer continuous variables to the standard form by the formula $x=(x-mean)\/std$"}}