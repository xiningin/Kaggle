{"cell_type":{"95c92898":"code","41d9055d":"code","18fd6acc":"code","c6c102a8":"code","b67352ee":"code","ab8148f7":"code","51322b0b":"code","8d0e4f9b":"code","f304e2f6":"code","6df9d25f":"code","f23bb744":"code","1d87c084":"code","cf22f75c":"code","a9b71b19":"code","1737be0f":"code","ab478059":"code","e6d7a0f7":"code","aa1038b7":"code","3dae960c":"code","92bcf304":"code","28476a2e":"code","554d35ec":"code","42ca0cdd":"code","c43ecf14":"code","b1783162":"code","ba6a71c8":"code","b4b53afd":"code","be607aa4":"code","af3b5a8b":"code","b839340c":"code","e909c81b":"code","d234358d":"code","bab540de":"code","e3cf065f":"code","f6ca79e4":"code","3aa990c8":"code","62d51b83":"code","ad151e91":"code","5dbc3bed":"code","d6324c7c":"code","21b53059":"code","ef718029":"code","092f986b":"code","76f744bd":"code","432b715e":"code","f335097e":"code","d1094f26":"code","0354a586":"code","0f3296fc":"code","522829be":"code","a8c68fe5":"markdown","b0bf1a8e":"markdown","37ef42fa":"markdown","26065053":"markdown","a754ca3b":"markdown","c4cdf4d2":"markdown","ec224564":"markdown","2798d2a6":"markdown","bfaf5fbf":"markdown","e2248b4f":"markdown"},"source":{"95c92898":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")#\u5ffd\u7565\u6389\u666e\u901a\u7684warning\nprint(os.listdir(\"..\/input\"))\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n# Any results you write to the current directory are saved as output.","41d9055d":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","18fd6acc":"train=pd.read_csv(\"..\/input\/train.csv\",index_col=0)\ntest=pd.read_csv((\"..\/input\/test.csv\"),index_col=0)\ntest['SalePrice']=-99","c6c102a8":"train.head()","b67352ee":"sns.distplot(np.log1p(train['SalePrice']),color=\"r\")","ab8148f7":"var = 'OverallQual'\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=train)\nfig.axis(ymin=0, ymax=800000);","51322b0b":"na_des=train.isna().sum()\n# train['SalePrice']=np.log1p(train['SalePrice'])\nna_des[na_des>0].sort_values(ascending=False)","8d0e4f9b":"new_data=pd.concat([train,test],axis=0,sort=False)\nnew_data.head()","f304e2f6":"# list1=['MSSubClass']\n# for i in list1:\n#     new_data=new_data.drop(i,axis=1)","6df9d25f":"# cols1 = [\"PoolQC\",\"MiscFeature\",'SaleType', \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\ncols1=dict(new_data.dtypes[new_data.dtypes=='object'])\nfor col in cols1:\n    new_data[col].fillna(new_data[col][new_data[col].notna()].mode()[0],inplace=True)\n# cols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\ncols=dict(new_data.dtypes[train.dtypes=='int64'])\nfor col in cols:\n    new_data[col].fillna(new_data[col][new_data[col].notna()].median(),inplace=True)\nnew_data[\"LotFrontage\"] = new_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","f23bb744":"a=new_data.isna().sum()\na=a[a>0]\na=dict(a).keys()","1d87c084":"for col in a :\n    new_data[col].fillna(new_data[col][new_data[col].notna()].mode()[0],inplace=True)","cf22f75c":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","a9b71b19":"total= 'TotalBsmtSF'\nsns.scatterplot(x=total, y='SalePrice',data=train,style='Street',markers={'Pave':'^','Grvl':'o'});","1737be0f":"sns.scatterplot(x='GrLivArea', y='SalePrice',data=train,color='b',style='Street',markers={'Pave':'^','Grvl':'o'});","ab478059":"f, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=train)\nplt.xticks(rotation=90);\n\n","e6d7a0f7":"k = 10 \ncorrmat = train.corr()\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nax = sns.heatmap(cm, annot=True,annot_kws={'size': 10}, fmt=\".2f\",xticklabels=cols.values,yticklabels=cols.values)\nplt.show()","aa1038b7":"# sns.set()\n# cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt','Street']\n# sns.pairplot(train[cols], size = 2.5,hue=\"Street\", palette=\"husl\")\n# plt.show();","3dae960c":"sns.violinplot(x=\"SaleType\", y=\"SalePrice\", data=train,hue=\"Street\",palette=\"Set2\")","92bcf304":"f, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"GrLivArea\", data=train)\nplt.xticks(rotation=90);","28476a2e":"train.groupby(['YearBuilt']).SalePrice.aggregate(['mean','std','max']).plot()","554d35ec":"train.groupby(['YearBuilt']).GrLivArea.aggregate(['mean','std','max']).plot()","42ca0cdd":"train.sort_values(by = 'GrLivArea', ascending = False)['GrLivArea'][:1]\ntrain = train.drop(train[train.index == 1299].index)","c43ecf14":"fig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\n","b1783162":"sns.distplot(np.log1p(train['GrLivArea']));\nfig = plt.figure()\nres = stats.probplot(np.log1p(train['GrLivArea']), plot=plt)\n","ba6a71c8":"new_data['HasBsmt']=new_data['TotalBsmtSF'].apply(lambda x:1 if x!=0 else 0)","b4b53afd":"# new_data.loc[new_data['HasBsmt']==1,'TotalBsmtSF'] = np.log1p(new_data['TotalBsmtSF'])","be607aa4":"fig = plt.figure()\nres = stats.probplot(new_data[new_data['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","af3b5a8b":"# new_data['MSSubClass'] =new_data['MSSubClass'].apply(str)\n# new_data['YrSold'] = new_data['YrSold'].astype(str)\n# new_data['MoSold'] = new_data['MoSold'].astype(str)\n\nnew_data['GrLivArea'][new_data['GrLivArea']==0]=1\nnew_data['1stFlrSF'][new_data['1stFlrSF']==0]=1\nnew_data['2ndFlrSF'][new_data['2ndFlrSF']==0]=1\n\n# new_data['GrLivArea']=np.log1p(new_data['GrLivArea'])\n# new_data['1st_GrLivArea']=new_data['1stFlrSF']\/new_data['GrLivArea']\n# new_data['2st_GrLivArea']=new_data['2ndFlrSF']\/new_data['GrLivArea']\n# new_data['1st_2st']=new_data['1stFlrSF']\/new_data['2ndFlrSF']\nnew_data['TotalSF'] = new_data['TotalBsmtSF'] + new_data['1stFlrSF'] + new_data['2ndFlrSF']\n# list_del=['1stFlrSF','BsmtUnfSF','BsmtFinSF2','BsmtFinType1','2ndFlrSF','Fireplaces','GarageArea','GarageCond','HalfBath','MSSubClass','BsmtCond','Utilities', 'Street', 'PoolQC']\n# for i in list_del:\n#     new_data=new_data.drop(i,axis=1)","b839340c":"from scipy.stats import skew\nnumeric_feats = new_data.dtypes[new_data.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = new_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\n\n","e909c81b":"skewness.head(10)","d234358d":"skewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:   \n    new_data[feat] = np.log1p(new_data[feat])","bab540de":"new_data=pd.get_dummies(new_data)","e3cf065f":"train=new_data.loc[np.array(train.index)]\ntest=new_data.loc[np.array(test.index)]","f6ca79e4":"x=train.drop('SalePrice',axis=1)\ny=train['SalePrice']\nx_test=test.drop('SalePrice',axis=1)","3aa990c8":"x=np.array(x)\ny=np.array(y)\nx_test=np.array(x_test)\nx=x.reshape(x.shape[0],x.shape[1],1)\nx_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)","62d51b83":"from keras import backend as K\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping\n\nfrom keras.layers  import MaxPool1D,GRU,RNN\nfrom keras import Sequential\nfrom keras.layers import LSTM,Conv1D,Dense,Flatten,Conv2D,LeakyReLU, Activation,Bidirectional,BatchNormalization,Dropout,GlobalAveragePooling1D,Embedding\nfrom keras.activations import relu\nfrom keras import initializers\nfrom keras.optimizers import Adam,RMSprop\nfrom keras import backend as K\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Model\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=10, mode='auto')\n\ndef RMSE(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\near=EarlyStopping(monitor='val_loss',min_delta=0,patience=5,verbose=0, mode='auto',restore_best_weights=True)","ad151e91":"train1=x","5dbc3bed":"model=Sequential()\n\nmodel.add(Dense(units=train1.shape[1]))\nmodel.add(Dense(units=int(train1.shape[1]\/2)))\nmodel.add(Dense(units=int(train1.shape[1]\/2)))\nmodel.add(Dense(units=train1.shape[1]))\n# model.add(Conv1D(train1.shape[1],2))\n# model.add(Conv1D(train1.shape[1],2))\n# model.add(MaxPool1D(2,1))\n# model.add(Dropout(0.15))\n\nmodel.add(LSTM(32,return_sequences=True,input_shape=(None,1)))\nmodel.add(Activation(K.relu))\nmodel.add(Dense(units=train1.shape[1]))\nmodel.add(Dense(units=int(train1.shape[1]\/2)))\nmodel.add(Dense(units=int(train1.shape[1]\/2)))\nmodel.add(Dense(units=train1.shape[1]))\nmodel.add(Conv1D(train1.shape[1],2))\nmodel.add(Conv1D(train1.shape[1],2))\nmodel.add(MaxPool1D(2,1))\nmodel.add(Dropout(0.15))\nmodel.add(LSTM(32,return_sequences=True))\n\nmodel.add(LSTM(32,return_sequences=True,input_shape=(None,1)))\nmodel.add(Activation(K.relu))\nmodel.add(Dense(units=train1.shape[1]))\nmodel.add(Dense(units=int(train1.shape[1]\/2)))\nmodel.add(Dense(units=int(train1.shape[1]\/2)))\nmodel.add(Dense(units=train1.shape[1]))\nmodel.add(Conv1D(train1.shape[1],2))\nmodel.add(Conv1D(train1.shape[1],2))\nmodel.add(MaxPool1D(2,1))\nmodel.add(Dropout(0.15))\n\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(Flatten())\n\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=100))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units=10))\nmodel.add(Dense(units=5))\nmodel.add(Dense(units=1,activation='selu'))\n\nmodel.compile(optimizer=Adam(), loss=RMSE)","d6324c7c":"hist=model.fit(train1,y,epochs=100, batch_size=80,validation_split=0.1,verbose=1,callbacks=[reduce_lr,ear])\nmodel.summary()","21b53059":"y","ef718029":"# from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n# from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n# from catboost import CatBoostRegressor\n# from sklearn.kernel_ridge import KernelRidge\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import RobustScaler\n# from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n# from sklearn.model_selection import KFold, cross_val_score, train_test_split\n# from sklearn.metrics import mean_squared_error\n# import xgboost as xgb\n# import lightgbm as lgb\n# import time\n# #Validation function\n# n_folds = 5\n\n# def rmsle_cv(model):\n#     kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n#     rmse = np.sqrt(-cross_val_score(model,x,y, scoring=\"neg_mean_squared_error\", cv=kf))\n#     return(rmse)\n\n# def eval_model(model, name):\n#     start_time = time.time()\n#     score = rmsle_cv(model)\n#     print(\"{} score: {:.4f} ({:.4f}),     execution time: {:.1f}\".format(name, score.mean(), score.std(), time.time()-start_time))","092f986b":"# from sklearn.kernel_ridge import KernelRidge","76f744bd":"# # \n# mod_lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n# eval_model(mod_lasso, \"lasso\")\n# mod_enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n# eval_model(mod_enet, \"enet\")\n# KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n# eval_model(KRR, \"KRR\")\n# # mod_cat = CatBoostRegressor(iterations=10000, learning_rate=0.01,\n# #                             depth=5, eval_metric='RMSE',\n# #                             colsample_bylevel=0.7, random_seed = 17, silent=True,\n# #                             bagging_temperature = 0.2, early_stopping_rounds=200)\n# # eval_model(mod_cat, \"cat\")\n# mod_gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.06,\n#                                    max_depth=3, max_features='sqrt',\n#                                    min_samples_leaf=7, min_samples_split=10, \n#                                    loss='huber', random_state=5)\n# eval_model(mod_gboost, \"gboost\")\n# mod_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n#                              learning_rate=0.05, max_depth=3, \n#                              min_child_weight=1.7817, n_estimators=2200,\n#                              reg_alpha=0.4640, reg_lambda=0.8571,\n#                              subsample=0.5213, silent=1,\n#                              random_state=7, nthread=-1)\n# eval_model(mod_xgb, \"xgb\")\n# mod_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=6,\n#                               learning_rate=0.05, n_estimators=650,\n#                               max_bin=58, bagging_fraction=0.8,\n#                               bagging_freq=5, feature_fraction=0.2319,\n#                               feature_fraction_seed=9, bagging_seed=9,\n#                               min_data_in_leaf=7, min_sum_hessian_in_leaf=11)\n# eval_model(mod_lgb, \"lgb\")","432b715e":"# def valid(model):\n#     x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state=0)\n#     model.fit(x_train.values,y_train.values)\n#     train_pred = model.predict(test.values)\n#     print(rmsle(y, train_pred))\n#     return (pred)","f335097e":"# class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n#     def __init__(self, base_models, meta_model, n_folds=5):\n#         self.base_models = base_models\n#         self.meta_model = meta_model\n#         self.n_folds = n_folds\n   \n#     # We again fit the data on clones of the original models\n#     def fit(self, X, y):\n#         self.base_models_ = [list() for x in self.base_models]\n#         self.meta_model_ = clone(self.meta_model)\n#         kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n#         # Train cloned base models then create out-of-fold predictions\n#         # that are needed to train the cloned meta-model\n#         out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n#         for i, model in enumerate(self.base_models):\n#             for train_index, holdout_index in kfold.split(X, y):\n#                 instance = clone(model)\n#                 self.base_models_[i].append(instance)\n#                 instance.fit(X[train_index], y[train_index])\n#                 y_pred = instance.predict(X[holdout_index])\n#                 out_of_fold_predictions[holdout_index, i] = y_pred\n                \n#         # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n#         self.meta_model_.fit(out_of_fold_predictions, y)\n#         return self\n   \n#     #Do the predictions of all base models on the test data and use the averaged predictions as \n#     #meta-features for the final prediction which is done by the meta-model\n#     def predict(self, X):\n#         meta_features = np.column_stack([\n#             np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n#             for base_models in self.base_models_ ])\n#         return self.meta_model_.predict(meta_features)\n    \n# mod_stacked = StackingAveragedModels(base_models = (mod_enet,KRR,mod_gboost), meta_model =mod_lasso )\n# eval_model(mod_stacked, \"stacked\")","d1094f26":"# def rmsle(y, y_pred):\n#     return np.sqrt(mean_squared_error(y, y_pred))\n\n# def predict(model):\n#     model.fit(x,y)\n#     train_pred = model.predict(x)\n#     pred = np.expm1(model.predict(x_test))\n#     print(rmsle(y, train_pred))\n#     return (pred)","0354a586":"# pre_lasso = predict(mod_lasso)\n# pre_enet = predict(mod_enet)\n# pre_krr = predict(KRR)\n# pre_xgb = predict(mod_xgb)\n# pre_gboost = predict(mod_gboost)\n# pre_lgb = predict(mod_lgb)\n# pre_stack1 = predict(mod_stacked)","0f3296fc":"# test['id']=test.index\n# test['SalePrice']=0.7*pre_stack1+0.15*pre_lgb+0.15*pre_xgb\n# test[['id','SalePrice']].to_csv('submission_Dragon3.csv', index=False)\n# test[['id','SalePrice']].head()","522829be":"test['id']=test.index\ntest['SalePrice']=np.expm1(model.predict(x_test))\ntest[['id','SalePrice']].to_csv('submission_Dragon4.csv', index=False)","a8c68fe5":"\u6574\u4f53\u6750\u6599\u548c\u9970\u9762\u8d28\u91cf ","b0bf1a8e":"\u5730\u4e0b\u5ba4\u603b\u9762\u79ef","37ef42fa":"\u623f\u4ef7\u7684\u4fee\u5efa\u5e74\u4efd\u548c\u9500\u552e\u4ef7\u683c\u6709\u4e00\u4e2a\u8d8b\u52bf","26065053":"\u5728\u4e0d\u540c\u623f\u5c4b\u7684\u7c7b\u578b\u7684\u57fa\u7840\u4e0a\u505a\u4e86\u53ef\u89c6\u5316,\u53d1\u73b0\u4ef7\u683c\u4e0e\u8fd9\u51e0\u4e2a\u76f8\u5173\u6027\u5f88\u5f3a\u7684\u7279\u5f81\u8054\u7cfb\u5f88\u5927","a754ca3b":"\u4e0a\u9762\u4e00\u90e8\u5206\u6765\u81eakernel\u548c\u6211\u81ea\u5df1\u7684\u4e00\u4e9b\u6539\u8fdb,\u6211\u82f1\u8bed\u4e0d\u600e\u4e48\u597d,70\u4e2a\u7279\u5f81\u63cf\u8ff0\u5bf9\u4e8e\u6211\u6765\u8bf4\u592a\u56f0\u96be\u4e86","c4cdf4d2":"\u5730\u4e0a\u751f\u6d3b\u533a\u9762\u79ef\/\u5355\u4f4d\u662f\u5e73\u65b9\u82f1\u5c3a","ec224564":"\u6570\u636e\u5f88\u660e\u663e\u670d\u4ece\u4e00\u4e2a\u5206\u5e03","2798d2a6":"annot=True,annot_kws={'size': 10}, fmt=\".2f\" \u8fd9\u51e0\u4e2a\u53c2\u6570\u6bd4\u8f83\u91cd\u8981","bfaf5fbf":"\u770b\u8d77\u6765\u53ea\u6709WD\u5728GRVL\u5904\u9500\u552e,\u5e76\u4e14\u52a0\u4e2a\u6ca1\u6709PAVE\u8857\u9053\u7684\u597d,\u6570\u636e\u96c6\u5185\u5927\u90e8\u5206\u7684\u623f\u5b50\u90fd\u662f\u5728Pave\u5904\u9500\u552e","e2248b4f":"\u6bcf\u5e74\u7684\u4f4f\u5b85\u9762\u79ef\u6709\u7740\u5f88\u4e00\u4e2a\u8f83\u4e3a\u663e\u8457\u7684\u4e0b\u964d\u8d8b\u52bf,\u8bf4\u660e\u623f\u4ef7\u53ea\u4f1a\u8d8a\u6765\u8d8a\u9ad8"}}