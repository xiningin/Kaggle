{"cell_type":{"1ede56ba":"code","dc4e01fb":"code","2b74b6e6":"code","c02aa5f7":"code","d696d52d":"code","6dd34d9e":"code","c4813374":"code","e740f355":"code","db075d6f":"code","ee7b484a":"code","8ff1e296":"code","5b54e8d1":"code","051ca853":"code","9b40ed5b":"code","24c5bde0":"code","91053103":"code","251a6f15":"code","6ba83c73":"code","12a0d15d":"code","0883934a":"code","049d5b82":"code","b6f23114":"code","d37d839f":"code","d79ff5d4":"code","c8855ddc":"code","c249e2c3":"code","7552c05f":"code","5897f1cf":"code","f3fcb6ab":"code","2d645400":"code","63553486":"code","4dfbaf23":"code","2df19e73":"code","0c6aa456":"code","50d0e52d":"code","059a983d":"code","bd38fc92":"code","18416657":"code","27fbc747":"code","474a0633":"code","7eb870ec":"code","ada76fec":"code","755a6822":"code","8b81143e":"code","6f5f7236":"code","89e65fad":"code","c4f7409f":"code","015dd74b":"markdown","36931ebe":"markdown","2daadf38":"markdown","fbc9a40c":"markdown","2ad32f91":"markdown","bf574d52":"markdown","69ac014b":"markdown","3f531144":"markdown","1f3a8ef9":"markdown","49cf40fc":"markdown","caf99c2e":"markdown","6f956b53":"markdown","1a3adb1c":"markdown","8ef5606e":"markdown"},"source":{"1ede56ba":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nfrom tqdm.notebook import tqdm\nimport torchvision.transforms as T\nfrom sklearn.metrics import f1_score\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\nimport PIL\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nnp.random.seed(42)\ntorch.manual_seed(42)","dc4e01fb":"# Data augmentation\nimagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\ntrain_tfms = T.Compose([\n    T.RandomCrop(48, padding=8, padding_mode='reflect'),\n     #T.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    T.Resize((48, 48)),\n    T.RandomHorizontalFlip(), \n    T.RandomRotation(10),\n    T.ToTensor(), \n     T.Normalize(*imagenet_stats,inplace=True), \n    #T.RandomErasing(inplace=True)\n])\n\nvalid_tfms = T.Compose([\n     T.Resize((48, 48)), \n    T.ToTensor(), \n     T.Normalize(*imagenet_stats)\n])","2b74b6e6":"\ndataset = ImageFolder(root='\/kaggle\/input\/facial-expression-recognition\/train\/')\n\ndataset_size = len(dataset)\ndataset_size","c02aa5f7":"testdataset = ImageFolder(root='\/kaggle\/input\/facial-expression-recognition\/test\/', transform = valid_tfms)\n\ntestdataset_size = len(testdataset)\ntestdataset_size","d696d52d":"dict = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}","6dd34d9e":"random_image = np.random.randint(0, 28708)\nprint('Random image number ', random_image)\nprint('Class label', dict[dataset[random_image][1]])\ndataset[random_image][0]","c4813374":"random_image = np.random.randint(0, 28708)\nprint('Random image number ', random_image)\nprint('Class label', dict[dataset[random_image][1]])\ndataset[random_image][0]","e740f355":"random_image = np.random.randint(0, 28708)\nprint('Random image number ', random_image)\nprint('Class label', dict[dataset[random_image][1]])\ndataset[random_image][0]","db075d6f":"classes = dataset.classes\nclasses","ee7b484a":"num_classes = len(dataset.classes)\nnum_classes","8ff1e296":"val_size = 1000\ntrain_size = len(dataset) - val_size\n\ntrain_df, val_df = random_split(dataset, [train_size, val_size])\nlen(train_df), len(val_df)","5b54e8d1":"val_df.dataset.transform = valid_tfms\n\ntrain_df.dataset.transform = train_tfms","051ca853":"batch_size = 64\n\ntrain_dl = DataLoader(train_df, batch_size, shuffle=True, \n                      num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_df, batch_size*2, \n                    num_workers=2, pin_memory=True)\ntest_dl = DataLoader(testdataset, batch_size*2, \n                    num_workers=2, pin_memory=True)","9b40ed5b":"for images, _ in train_dl:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n    break","24c5bde0":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","91053103":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","251a6f15":"class CnnModel2(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.wide_resnet101_2(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 7)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))","6ba83c73":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","12a0d15d":"device = get_default_device()\ndevice","0883934a":"model = to_device(CnnModel2(), device)\ntrain_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\ntest_dl = DeviceDataLoader(test_dl, device)","049d5b82":"for images, labels in train_dl:\n    print('images.shape:', images.shape)\n    out = model(images)\n    print('out.shape:', out.shape)\n    print('out[0]:', out[0])\n    break","b6f23114":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","d37d839f":"# Set search to a larger number to test out more hyperparameters\nsearch = 1","d79ff5d4":"history = [evaluate(model, val_dl)]\nhistory","c8855ddc":"epochs = np.random.randint(2, 25)\nmax_lr = np.random.choice([5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6])\ngrad_clip = np.random.choice([0.5, 0.4, 0.3, 0.2, 0.1, 0.05])\nweight_decay = np.random.choice([1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5])\nopt_func = torch.optim.Adam\nprint('epoch = ', epochs, 'lr = ', max_lr, 'grad is ', grad_clip, 'weights = ', weight_decay)","c249e2c3":"torch.cuda.empty_cache()\n\n\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","7552c05f":"for j in range(search):\n    model = to_device(CnnModel2(), device)\n    history = [evaluate(model, val_dl)]\n    print(history)\n    epochs = np.random.randint(2, 25)\n    max_lr = np.random.choice([5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6])\n    grad_clip = np.random.choice([0.5, 0.4, 0.3, 0.2, 0.1, 0.05])\n    weight_decay = np.random.choice([1e-2, 5e-2, 1e-3, 5e-3, 1e-4, 5e-4, 1e-5])\n    opt_func = torch.optim.Adam\n    print('epoch = ', epochs, 'lr = ', max_lr, 'grad is ', grad_clip, 'weights = ', weight_decay)\n    torch.cuda.empty_cache()\n\n\n    history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                                 grad_clip=grad_clip, \n                                 weight_decay=weight_decay, \n                                 opt_func=opt_func)","5897f1cf":"for j in range(search):\n    model = to_device(CnnModel2(), device)\n    history = [evaluate(model, val_dl)]\n    print(history)\n    epochs = np.random.randint(12,24)\n    max_lr = np.random.choice([ 5e-3, 1e-4, 5e-4, 1e-5, 5e-5])\n    grad_clip = np.random.choice([0.2, 0.15, 0.1, 0.05])\n    weight_decay = np.random.choice([ 5e-3, 1e-4, 5e-4, 1e-5])\n    opt_func = torch.optim.Adam\n    print('epoch = ', epochs, 'lr = ', max_lr, 'grad is ', grad_clip, 'weights = ', weight_decay)\n    torch.cuda.empty_cache()\n\n\n    history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                                 grad_clip=grad_clip, \n                                 weight_decay=weight_decay, \n                                 opt_func=opt_func)","f3fcb6ab":"model = to_device(CnnModel2(), device)\nhistory = [evaluate(model, val_dl)]\nprint(history)\nepochs = 45\nmax_lr = 0.0001\ngrad_clip = 0.025\nweight_decay = 1e-5\nopt_func = torch.optim.Adam\nprint('epoch = ', epochs, 'lr = ', max_lr, 'grad is ', grad_clip, 'weights = ', weight_decay)\ntorch.cuda.empty_cache()\n\n\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                                 grad_clip=grad_clip, \n                                 weight_decay=weight_decay, \n                                 opt_func=opt_func)","2d645400":"torch.save(model.state_dict(), 'facial.pth')","63553486":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n\nplot_losses(history)","4dfbaf23":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\nplot_accuracies(history)","2df19e73":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');\n\nplot_lrs(history)","0c6aa456":"evaluate(model, val_dl)['val_acc']","50d0e52d":"evaluate(model, test_dl)['val_acc']","059a983d":"model = to_device(CnnModel2(), device)\nmodel.load_state_dict(torch.load('\/kaggle\/input\/facial\/facial.pth'))","bd38fc92":"def predict_image(img, model):\n    xb = to_device(img.unsqueeze(0), device)\n    yb = model(xb)\n    _, preds  = torch.max(yb, dim=1)\n    return preds[0].item()","18416657":"img, label = testdataset[0]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', testdataset.classes[label], ', Predicted:', testdataset.classes[predict_image(img, model)])","27fbc747":"random_image = np.random.randint(0, 7178)\nprint('Random image number ', random_image)\nimg, label = testdataset[random_image]\nplt.imshow(img[0], cmap='gray')\nprint('Label:', testdataset.classes[label], ', Predicted:', testdataset.classes[predict_image(img, model)])","474a0633":"my_ims = ImageFolder(root='\/kaggle\/input\/test-faces\/', transform = valid_tfms)\n\nmy_ims_size = len(my_ims)\nmy_ims_size","7eb870ec":"img, label = my_ims[0]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","ada76fec":"img, label = my_ims[1]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","755a6822":"img, label = my_ims[2]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","8b81143e":"img, label = my_ims[3]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","6f5f7236":"img, label = my_ims[4]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","89e65fad":"img, label = my_ims[5]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","c4f7409f":"img, label = my_ims[6]\nplt.imshow(img[0], cmap='gray')\nprint('Predicted:', dataset.classes[predict_image(img, model)])","015dd74b":"### Train the Model","36931ebe":"### My Images","2daadf38":"Those appear good. Let's try it out with some real world images!","fbc9a40c":"### Import Libraries","2ad32f91":"### Test it out!!","bf574d52":"### EDA","69ac014b":"Thank you for reading, and please upvote if you enjoyed! \n\n![](http:\/\/external-preview.redd.it\/Ag4b71XcvFY6yk2UZ244G6tLLfPSQxGQhpWQLjXW3Mo.jpg?auto=webp&s=cf9188290fdc98bd5cb4ec35ce7308db944bcf17)","3f531144":"### Final Results","1f3a8ef9":"### Load in the Data","49cf40fc":"### Define the Models","caf99c2e":"# Facial Expression Detection\n## By Sergei Issaev","6f956b53":"### Final Training with best hyperparameters","1a3adb1c":"### Introduction\nHello everyone! Today I will be using a dataset of human faces, provided by Sulthan Khan (https:\/\/www.kaggle.com\/sulthankhan\/facial-expression-recognition), to build a human expression classifier. Although the dataset is large, the images are relatively small - 48 by 48 pixels, which will make it more difficult to attain a high classification accuracy. Let's get started!","8ef5606e":"### Perform Train-Test Split"}}