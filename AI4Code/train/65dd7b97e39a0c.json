{"cell_type":{"47d8b8d6":"code","7cb0a05f":"code","e52bf0f0":"code","27a1fb95":"code","2f4db756":"code","993303cb":"markdown","13a3e541":"markdown","c5f5f6fb":"markdown","2de1449c":"markdown","3aa53d52":"markdown","da22a5f8":"markdown"},"source":{"47d8b8d6":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport seaborn as sns\n\nfrom IPython.display import display","7cb0a05f":"def lahman_year_team(inputdf,yr='2012',droplist=['yearID','G','Ghome','W','L','Rank']):\n    '''\n    function which obtains numeric-only data from Lahman Teams data set and selects year\n    inputdf    : Lahman Teams.csv, DataFrame\n    yr         : year to select, int\n    droplist   : static features to omit from returned data, list of str\n    returns    : numeric Teams features with team_id as index, DataFrame\n    '''\n    df = inputdf.copy()\n    # select numeric data only for PCA\n    numericdf = df.select_dtypes(exclude=['object'])\n    # assign team_ID as index\n    numericdf.set_index(df['teamID'].values,inplace=True)\n    # filter by year\n    numericdf = numericdf[numericdf.yearID==yr]\n    # drop constant features, where value is dependent or does not vary by player\/team performance\n    numericdf.drop(droplist,axis=1,inplace=True)\n    print('Lahman numeric feature team results {}:'.format(yr))\n    return numericdf \n\nteamsdf = pd.read_csv('..\/input\/Teams.csv')\ndroplist=['yearID','G','Ghome','W','L','Rank']\nteams12 = lahman_year_team(teamsdf,yr=2012,droplist=droplist)    \n# teams12.columns\nteams12.head()\n","e52bf0f0":"def center_scale(X):\n    '''\n    returns : X features centered by column mean and scaled by column std, df\n    '''\n    return (X-np.mean(X))\/np.std(X)\n\ndef pca(inputdf):\n    '''\n    function which computes largest variance directions (loading vectors) and principal components (score vectors)\n    inputdf    : features to compute variance explained\n    returns    : loading vectors, score vectors as PCs, variance explained as eigenvals    \n    '''\n    df = inputdf.copy()\n    # step 1: center\/scale the features\n    C = center_scale(df)\n    print('Shape of centered features matrix = {}'.format(C.shape))\n    # step 2: compute cov of tranpsose of centered features\n    cov = np.cov(C.T)\n    print('shape of covariance matrix = {}'.format(cov.shape))\n    # step 3: compute the PC loading vectors (direction of largest variacne in features space)\n    eigvals,eigvecs = np.linalg.eig(cov)\n    print('shape of eigenvalues, eigenvectors = {}, {}'.format(eigvals.shape,eigvecs.shape))\n    loadingheaders = ['L'+str(i) for i in range(1,len(df.columns)+1)]\n    # eigvecs are loadings \n    loadingdf = pd.DataFrame(eigvecs,columns=loadingheaders,index=df.columns).astype(float)\n    print('shape of loadings df = {}'.format(loadingdf.shape))\n    print('Top 5 PC loading vectors (direction of largest variation in feature-space):')\n    display(loadingdf.loc[:,:'L5'])\n    # step 4: compute score vectors as Principal Components (where scores are features C projected onto loading vectors)\n    scorematrix = loadingdf.values.T.dot(C.T)\n    scoreheaders = ['PC'+str(i) for i in range(1,len(C.columns)+1)]\n    scoredf = pd.DataFrame(scorematrix.T,index=C.index,columns=scoreheaders)\n    display(scoredf.head())\n    return loadingdf,scoredf,eigvals\n\n\nloadingdf,scoredf,eigvals = pca(teams12)\n","27a1fb95":"def pve(eigvals):\n    '''\n    function which computes percent variance explained (PVE), cumulative PVE of all PCs\n    inputdf     : numeric features X with named indices, DataFrame\n    eigvals     : eigenvalues resulting from principal components analyis, are the corresponding variance explained of ea. PC\n    '''\n    with plt.style.context('seaborn-white'):\n        fig,ax = plt.subplots(figsize=(14,8))\n        var_total = eigvals.sum()\n        # compute proportional variance explained per PC\n        pve = eigvals\/var_total\n        # compute cum. variance explained per PC\n        cumpve = np.cumsum(pve)\n        x = [i for i in range(1,len(eigvals)+1)]\n        ax.set_xticks(x)\n        ax.plot(x,pve,label='PVE')\n        ax.plot(x,cumpve,label='PVE_cumulative')\n        ax.set(title='Percent Variance Explained by Principal Components',\n              xlabel='PC',ylabel='Variance Explained')\n        # ref lines\n        hlinecolor='0.74'\n        ax.axhline(y=eigvals[0]\/eigvals.sum(),linestyle='dotted',color=hlinecolor)\n        ax.axhline(y=0,linestyle='dotted',color=hlinecolor)\n        ax.axhline(y=1,linestyle='dotted',color=hlinecolor)\n        ax.legend(loc='best')\npve(eigvals)\n\nnp.cumsum(eigvals\/eigvals.sum())\n(eigvals\/eigvals.sum())","2f4db756":"def lg_ranks(inputdf,year):\n    '''\n    function which displays team end of season results\n    inputdf    : Lahman database Teams.csv, DataFrame\n    year       : year to filter, int\n    '''\n    df = inputdf.copy()\n    algrp = df[(df.yearID==year)&(df.lgID=='AL')].groupby(['teamID','lgID','divID','W','L'],as_index=False).agg({'Rank':'last'}).sort_values(['Rank','lgID','divID'])\n    nlgrp = df[(df.yearID==year)&(df.lgID=='NL')].groupby(['teamID','lgID','divID','W','L'],as_index=False).agg({'Rank':'last'}).sort_values(['Rank','lgID','divID'])\n    print('{} Final MLB Team Standings:'.format(year))\n    return algrp,nlgrp\n\ndef biplot(loadingdf,scoredf,loading_color,score_color,score_axlim=7.5,load_axlim=7.5,load_arrows=4):\n    '''\n    function which computes biplot of PC scores, loadings\n    scoredf    : matrix of PC score vectors, used tp display how indices are projected onto PC loading vectors, DataFrame\n    loadingdf  : matrix of PC loading vectors from centered, std'd features, used to show actual direction of PC1 and PC2 2D vectors, DataFrame\n    _color     : matplotlib line colors for corresponding loading vectors, score projection points, str\n    '''\n    with plt.style.context('seaborn-white'):\n        f = plt.figure(figsize=(14,14))\n        ax0 = plt.subplot(111)\n        # plot the first two score vectors, as annotations, of teamID indices (PC1,PC2 are orhogonal to ea. other)\n        for teamid in scoredf.index:  \n            ax0.annotate(teamid,(scoredf['PC1'][teamid],-scoredf['PC2'][teamid]),ha='center',color=score_color)\n        score_axlim = score_axlim\n        ax0.set(xlim=(-score_axlim,score_axlim),ylim=(-score_axlim,score_axlim),\n               )\n        ax0.set_xlabel('Principal Component 1',color=score_color)\n        ax0.set_ylabel('Principal Component 2',color=score_color)\n        # add reference lines through origin\n        ax0.hlines(y=0,xmin=-score_axlim,xmax=score_axlim,linestyle='dotted',color='grey')\n        ax0.vlines(x=0,ymin=-score_axlim,ymax=score_axlim,linestyle='dotted',color='grey')\n        # plot PC1 and PC2 loadings (two directions in features space with largest variation) as reference vectors\n        ax1 = ax0.twinx().twiny()\n        ax1.set(xlim=(-load_axlim,load_axlim), ylim=(-load_axlim,load_axlim),\n               )\n        ax1.tick_params(axis='y',color='red')\n        ax1.set_xlabel('Principal Component Loading Weights',color=loading_color)\n        # plot first two PC loading vectors (as loadingdf.index annotations)\n        offset_scalar=1.175\n        for feature in loadingdf.index: \n            ax1.annotate(feature,(loadingdf['L1'].loc[feature]*offset_scalar,-loadingdf['L2'].loc[feature]*offset_scalar),color=loading_color)\n        # display first fourPCs as arrows\n        for i in range(0,load_arrows):\n            ax1.arrow(x=0,y=0,dx=loadingdf['L1'][i],dy=-loadingdf['L2'][i],head_width=0.0075,shape='full')\nbiplot(loadingdf,scoredf,loading_color='red',score_color='blue',score_axlim=8.5,load_axlim=.6,load_arrows=len(loadingdf.columns))        \nALrankdf,NLrankdf = lg_ranks(teamsdf,2012)\ndisplay(ALrankdf)\ndisplay(NLrankdf)\n    ","993303cb":"### Import Lahman Baseball Data and Select Features\n  - omit dependent features (e.g. earned runs, earned run average) and common features (e.g. games played)","13a3e541":"### Principal Component Analysis \n\nWhy do PCA?\n\n - Principal components analysis is an unsupervised learning method where we do not work with labels to study data, i.e., only explanatory features **X** are considered.\n  - Unlike supervised learning, e.g. regression analysis which has a response y to predict, the goal of unsupervised learning is to determine a relationship between features **X** and response y.   \n- Principal components are directions of vectors in which the data set vary the most in terms of the **X** features space.\n- PCA is useful as a method to reduce the dimension of features when there exist many features and we are unsure which features are important.\n  - In this sense, PCA can be viewed as a method for exploratory data analysis because we can compute PCs and visualize the vectors which have the highest variance in feature space. ","c5f5f6fb":"### Plot Percent Variance Explained\n\n- Each loading vector (eigenvector) and principal component (score vector) has a corresponding eigenvalue, which equals the sample variance of each directional score vector   \n- The cumulative PVE indicates that approximately 90% of the variation of the features is explained by the first 10 principal components ","2de1449c":"## Biplot to Visualize First Two PCs Explaining Largest Variation of Data \n - loading vectors reveal directions with largest variation in feature space\n - score vectors (features X projected onto loading direction vectors) reveal features in correspondence with vector directions with highest variance\n ","3aa53d52":"### PCA with numpy\n- step 1: center and scale the features matrix\n- step 2: compute covariance matrix of centered features\n- step 3: compute tuple eigenvalues, eigenvectors using np.linalg.eig(X)\n  - eigenvectors are the principal component loading vectors, which are the directions of highest variation within the feature space X\n- step 4: compute the Principal Components by projecting the centered X features onto the PC loading eigenvectors\n  - PC_Matrix = LoadingVector dotproduct centeredFeatures ","da22a5f8":"### Biplot Interpretation\n\n- Based on the year-end rankings and the X projections onto the biplot, the teams with better (worse) rankings were projected toward the right (left) sides of the biplot.\n\n- The first principal component (x-axis, blue text) for winning teams places the largest weights on: \n  - SV (relief pitcher game saves) \n  - SOA (strikeouts by pitchers),\n  - SHO (shutouts, i.e. no opponent hits),\n  - IPOuts (Outs pitched, innings pitched x 3) \n    - i.e. winning teams had good pitching statistics.\n- The first loading vector for losing teams places the largest weights on:\n  - ERA (earned run average)\n  - ER (earned runs allowed),\n  - RA (runs allowed),\n     - i.e., losing teams had poor pitching statistics.  \n- The second Principal component vector (y-axis, blue text) with the highest weights are:\n  - Runs (number of points scored),\n  - H (hits)\n  - 2B (doubles),\n   \n- The team indices (blue text) of the scores matrix reveal:\n  - The Cincinnatti Reds (CIN) and Washington Nationals did well SV (saves) and SOA (strikeouts by pitchers)\n  - Instead of PC1, the New York Yankees (NYA) did well with PC2 attributes HR (home runs) and R (runs) and that Steinbrenner did well with stadium attendance.   \n  - The Oakland Athletics' (OAK), San Francisco Giants' (SFN), and Atlanta Braves' (ATL) outderperformance coincided with SHO (pitcher shutouts) and BB (walks by batters).\n\nConclusion:\n  - There is evidence that the majority of first place teams in 2012 focused on good defense (pitching) over strong offense (hitting). \n"}}