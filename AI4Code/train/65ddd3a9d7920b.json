{"cell_type":{"c40117b7":"code","743f7257":"code","946160d2":"code","92af2f74":"code","efe3c27c":"code","fa6ce164":"code","5a34f505":"code","90b44570":"code","b500cd15":"code","8409e235":"code","66349a21":"code","ff9f1113":"code","786c53b9":"code","3236f8e0":"code","2621e27d":"code","46df5580":"code","e095e6bf":"code","3c335411":"code","759be6c7":"code","05f419c5":"code","06fe3411":"code","20d8122b":"code","6f82cbc1":"code","0937072c":"code","53faa20b":"code","00e1c041":"code","386f40aa":"code","d217ed80":"code","e654fab8":"code","22c9d8f3":"code","7262fd83":"code","6ca5292e":"code","ea16d705":"code","d7c50428":"code","38751cc4":"code","2c1d550d":"code","a2a40074":"code","ffc9fde4":"code","6eb02a8f":"code","8af355aa":"code","e2bb613b":"code","13211f19":"code","7f739399":"code","efd56461":"code","f667f447":"code","21746e3d":"code","6b65c3ba":"code","256d9fda":"code","2d128d84":"code","1c75ba93":"code","676f97fd":"code","869a0cda":"code","140bf135":"code","a952bccf":"code","cd0146bb":"code","a1965be2":"code","5783268c":"code","eac92a28":"code","17505c0f":"code","a0bc262f":"code","2ad36ea5":"code","d0c89765":"code","71d72a34":"code","51cc7f2d":"code","b6dd4392":"code","0d5ed13d":"code","5ac1c85a":"code","2df092ca":"code","6e50b68f":"code","6ef7bb46":"code","2294d315":"code","4aa6aae6":"code","a7cb7dfd":"code","a68295de":"code","35d64c15":"code","d4f1f561":"code","ed1c181b":"code","82841aac":"code","d2e3f30d":"code","bf5cec94":"code","8cf0c360":"code","5c1b689e":"code","23627443":"code","41696e0c":"code","8ced60b4":"code","dd4c6dd8":"code","ff9fb67a":"code","9b6cca7a":"code","7d649f14":"code","ed882d26":"code","6e5f2d7b":"code","4c3efd43":"code","6720f550":"code","4327144e":"code","9c136fa3":"code","4be33a8a":"code","b712f8b7":"code","543bc034":"code","674829fa":"code","f032b091":"code","c50b23f4":"code","a938494f":"code","192c2e66":"code","ab97ea12":"code","89e20d35":"code","aa63f8d0":"code","90b98b80":"markdown","3852d9d9":"markdown","dd95ce49":"markdown","0477f5b6":"markdown","edc220fe":"markdown","01ef7249":"markdown","e90ea773":"markdown","b20d70a7":"markdown","e5be5373":"markdown","3c8a80e5":"markdown","328130f5":"markdown","96da9007":"markdown","38468f9e":"markdown","7335a3c1":"markdown","c73c7fe1":"markdown","84724bf0":"markdown","a25b571d":"markdown","64c8033b":"markdown","7c86c66e":"markdown","6f239f8f":"markdown","5e4e9618":"markdown","22992ca6":"markdown","601c5a64":"markdown","4f36dd8d":"markdown","15b61412":"markdown","94b381af":"markdown","bd7b1a31":"markdown","d7e9aab6":"markdown","93d89451":"markdown","9a6fea6d":"markdown","19cc1981":"markdown","0fe71bcf":"markdown","d8b56154":"markdown","5d014e7b":"markdown","9bd740d6":"markdown","735fce63":"markdown","935cf833":"markdown","ef0f94de":"markdown","a127eeca":"markdown","97274d7c":"markdown","1757bcd5":"markdown","217e9a93":"markdown","c9ee069e":"markdown","aaafd2ea":"markdown","b518ea3f":"markdown","ef00138a":"markdown","61186a4c":"markdown","ef3713e8":"markdown","07dd83e2":"markdown","193f31b6":"markdown","234e5471":"markdown","f05d1e58":"markdown","db750b4c":"markdown"},"source":{"c40117b7":"%%writefile utils.py\n\n#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n#Common packages\nimport numpy as np\nimport pandas as pd\nimport warnings\n\n# ML\nimport scipy\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Keras\/Tensorflow\nimport tensorflow as tf\nimport keras\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n\n# Image processing\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n\nimg_folder='..\/input\/data\/imgs\/'\n\ncategories = {}\n\ndef setup_onehot(df):\n    categories['subspecies'] = np.unique(df['subspecies'])\n    categories['health'] = np.unique(df['health'])\n    \ndef read_data():\n    bees=pd.read_csv('..\/input\/data\/bees_train.csv', \n                index_col=False,\n                dtype={'subspecies':'category', 'health':'category','caste':'category'})\n    bees_test_for_evaluation=pd.read_csv('..\/input\/data\/bees_test.csv', \n                index_col=False,  \n                dtype={'caste':'category'})\n    \n    setup_onehot(bees)\n    \n    return bees, bees_test_for_evaluation\n\ndef read_img(file, img_folder, img_width, img_height, img_channels):\n    \"\"\"\n    Read and resize img, adjust channels. \n    @param file: file name without full path\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        img = skimage.io.imread(img_folder + file)\n        img = skimage.transform.resize(img, (img_width, img_height), mode='reflect', )\n    return img[:,:,:img_channels]\n\n\ndef split(bees):\n    \"\"\" \n    Split to train, test and validation. \n    \n    @param bees: Total Bees dataset to balance and split\n    @return:  train bees, validation bees, test bees\n    \"\"\"\n    # Split to train and test before balancing\n    train_bees, test_bees = train_test_split(bees, random_state=24)\n\n    # Split train to train and validation datasets\n    # Validation for use during learning\n    train_bees, val_bees = train_test_split(train_bees, test_size=0.1, random_state=24)\n\n    return(train_bees, val_bees, test_bees)\n\t\ndef load_images_and_target(train_bees, val_bees, test_bees, y_field_name, img_width, img_height, img_channels):\n    \"\"\"\n    Load images for features, drop other columns\n    One hot encode for label, drop other columns\n    @return: train images, validation images, test images, train labels, validation labels, test labels\n    \"\"\"\n    # Bees already splitted to train, validation and test\n    # Load and transform images to have equal width\/height\/channels. \n    # Use np.stack to get NumPy array for CNN input\n\n    # Train data\n    train_X = np.stack(train_bees['file'].apply(lambda x: read_img(x, img_folder, img_width, img_height, img_channels)))\n    #train_y = onehot_encoding(train_bees[y_field_name].values)\n    train_y  = pd.get_dummies(train_bees[y_field_name], drop_first=False)\n\n    # Validation during training data to calc val_loss metric\n    val_X = np.stack(val_bees['file'].apply(lambda x: read_img(x, img_folder, img_width, img_height, img_channels)))\n    #val_y = onehot_encoding(val_bees[y_field_name].values)\n    val_y = pd.get_dummies(val_bees[y_field_name], drop_first=False)\n\n    # Test data\n    test_X = np.stack(test_bees['file'].apply(lambda x: read_img(x, img_folder, img_width, img_height, img_channels)))\n    #test_y = onehot_encoding(test_bees[y_field_name].values)\n    test_y = pd.get_dummies(test_bees[y_field_name], drop_first=False)\n\n    return (train_X, val_X, test_X, train_y, val_y, test_y)\t\n\n\ndef class_weights(y) :\n    # Hint: usar\n    # http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.utils.class_weight.compute_class_weight.html\n    #return compute_class_weight(\"balanced\", np.unique(y), y)\n    #return np.ones(np.unique(y).shape[0])\n    return dict(enumerate(compute_class_weight(\"balanced\", np.unique(y), y)))\n\ndef train(      model,\n                train_X,\n                train_y, \n                batch_size,\n                epochs,\n                validation_data_X,\n\t\t\t\tvalidation_data_y,\n                steps_per_epoch,\n                rotation_range,  # randomly rotate images in the range (degrees, 0 to rotation_range)\n                zoom_range, # Randomly zoom image \n                width_shift_range,  # randomly shift images horizontally (fraction of total width)\n                height_shift_range,  # randomly shift images vertically (fraction of total height)\n                horizontal_flip,  # randomly flip images\n                vertical_flip,\n\t\t\t\tpatience,\n\t\t\t\tclass_weights):\n\t\t\t\t\n\tgenerator = ImageDataGenerator(\n                featurewise_center=False,  # set input mean to 0 over the dataset\n\t\t\t\tsamplewise_center=False,  # set each sample mean to 0\n\t\t\t\tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n\t\t\t\tsamplewise_std_normalization=False,  # divide each input by its std\n\t\t\t\tzca_whitening=False,  # apply ZCA whitening\n\t\t\t\trotation_range=rotation_range,  # randomly rotate images in the range (degrees, 0 to rotation_range)\n\t\t\t\tzoom_range = zoom_range, # Randomly zoom image \n\t\t\t\twidth_shift_range=width_shift_range,  # randomly shift images horizontally (fraction of total width)\n\t\t\t\theight_shift_range=height_shift_range,  # randomly shift images vertically (fraction of total height)\n\t\t\t\thorizontal_flip=horizontal_flip,  # randomly flip images\n\t\t\t\tvertical_flip=vertical_flip)\n\t\t\t\t\n\t\t\t\t\n\tgenerator.fit(train_X)\n\t#Train\n\t##Callbacks\n\tearlystopper = EarlyStopping(monitor='loss', patience=patience, verbose=1,restore_best_weights=True)\n    \n\ttraining = model.fit_generator(generator.flow(train_X,train_y, batch_size)\n                        ,epochs=epochs\n                        ,validation_data=[validation_data_X, validation_data_y]\n                        ,steps_per_epoch=steps_per_epoch\n                        ,callbacks=[earlystopper]\n                        ,class_weight = class_weights)\n\t\t\t\t\t\t\t\t\n\treturn training, model\n\ndef plot_images(data, attribute, samples) :\n    if len(samples) < 2 or len(samples) > 5 : \n        raise ValueError('len(samples) must be in [2, 5]') \n        \n    _, ax = plt.subplots(nrows = 1, ncols = len(samples), figsize = (20, 5))\n    for i, img_idx in enumerate(samples) :\n        attrname = data[attribute].iloc[img_idx]\n        filename = '..\/input\/data\/imgs\/' + data['file'].iloc[img_idx]\n        img = imageio.imread(filename)\n        ax[i].imshow(img)\n        ax[i].set_title(attrname, fontsize = 16)\n    plt.tight_layout()\n    plt.show()\n\ndef eval_model(training, model, test_X, test_y, field_name):\n    \"\"\"\n    Model evaluation: plots, classification report\n    @param training: model training history\n    @param model: trained model\n    @param test_X: features \n    @param test_y: labels\n    @param field_name: label name to display on plots\n    \"\"\"\n    ## Trained model analysis and evaluation\n    f, ax = plt.subplots(2,1, figsize=(5,5))\n    ax[0].plot(training.history['loss'], label=\"Loss\")\n    ax[0].plot(training.history['val_loss'], label=\"Validation loss\")\n    ax[0].set_title('%s: loss' % field_name)\n    ax[0].set_xlabel('Epoch')\n    ax[0].set_ylabel('Loss')\n    ax[0].legend()\n    \n    # Accuracy\n    ax[1].plot(training.history['acc'], label=\"Accuracy\")\n    ax[1].plot(training.history['val_acc'], label=\"Validation accuracy\")\n    ax[1].set_title('%s: accuracy' % field_name)\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Accuracy')\n    ax[1].legend()\n    plt.tight_layout()\n    plt.show()\n    \n    # Accuracy by subspecies\n    test_pred = model.predict(test_X)\n    \n    acc_by_subspecies = np.logical_and((test_pred > 0.5), test_y).sum()\/test_y.sum()\n    acc_by_subspecies.plot(kind='bar', title='Accuracy by %s' % field_name)\n    plt.ylabel('Accuracy')\n    plt.show()\n\n    # Print metrics\n    print(\"Classification report\")\n    test_pred = np.argmax(test_pred, axis=1)\n    test_truth = np.argmax(test_y.values, axis=1)\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        print(metrics.classification_report(test_truth, test_pred, target_names=test_y.columns))\n\n    # Loss function and accuracy\n    test_res = model.evaluate(test_X, test_y.values, verbose=0)\n    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])\n\ndef load_test(img_width, img_height, img_channels):\n\tX_test_partition=pd.read_csv('..\/input\/data\/bees_test.csv', \n\t\t\t\t\tindex_col=False,  \n\t\t\t\t\tdtype={'caste':'category'})\n\t\t\n\ttest_images = np.stack(X_test_partition['file'].apply(lambda x: read_img(x, img_folder, img_width, img_height, img_channels)))\n\t\n\treturn X_test_partition, test_images\n\ndef predict(model, data):\t\n\tpred = np.argmax(model.predict(data), axis = 1) \n\tpred = pred.reshape(-1,1)\n\treturn pred\n\t\ndef load_test_and_generate_prediction_file(model, class_weights, class_name, img_width, img_height, img_channels):\n\t\n\tX_test_partition, test_images = load_test(img_width, img_height, img_channels)\n\tpred = predict(model, test_images)\n\t    \n\ttest_ids = X_test_partition['id']\t\n\ttest_ids = np.array(test_ids).reshape(-1,1)\n\n\toutput = np.stack((test_ids, pred), axis=-1)\n\toutput = output.reshape([-1, 2])\n\n\tdf = pd.DataFrame(output)\n\tdf.columns = ['id','expected']\n    \n\tdf['expected'] = df['expected'].map(pd.Series(categories[class_name]))    \n\tdf.to_csv(\"test_\" + class_name + \".csv\",index = False,index_label = False)\t\n\treturn df","743f7257":"import pandas as pd\nimport numpy as np\n#import sys\n#import os\n#import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, Lambda, GlobalAveragePooling2D, Input, Activation, BatchNormalization, GlobalMaxPooling2D\nimport tensorflow as tf\n\nimport utils","946160d2":"from keras.applications import vgg16, inception_v3, resnet50, mobilenet, densenet\nfrom keras.preprocessing import image\nfrom keras import backend as K\nfrom keras.optimizers import SGD, Adam\nfrom keras.regularizers import l1, l2, l1_l2","92af2f74":"np.random.seed(28)\ntf.set_random_seed(28)","efe3c27c":"img_width = 100\nimg_height = 100\nimg_channels = 3","fa6ce164":"bees, bees_test_for_evaluation = utils.read_data()","5a34f505":"np.unique(['Italian']).reshape(-1,1)","90b44570":"np.unique(bees['subspecies'])","b500cd15":"bees.head()","8409e235":"bees_test_for_evaluation.head()","66349a21":"# bees es el archivo a tomar\nbees['subspecies'].head()","ff9f1113":"# Graficos embebidos.\n%matplotlib inline\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (5, 4)})\nbees.groupby('subspecies').size().plot(kind='bar', title='Cantidad de abejas x subespecie') \nplt.xlabel(\"Tipos de Subespecies\")\nplt.ylabel(\"Cantidad\")\nplt.show()","786c53b9":"# Gr\u00e1fico de tarta de subespecies de abejas\nbees['subspecies'].value_counts().plot(kind='pie', autopct='%.2f', \n                                            figsize=(6, 6),\n                                            title='Abejas x subespecie')\nplt.show()","3236f8e0":"# Tabla de contingencia health seg\u00fan subspecies\npd.crosstab(index=bees['health'],\n            columns=bees['subspecies'], margins=True)","2621e27d":"# tabla de contingencia en porcentajes relativos total\npd.crosstab(index=bees['health'], columns=bees['subspecies'],\n            margins=True).apply(lambda r: r\/len(bees) *100,\n                                axis=1)","46df5580":"# Gr\u00e1fico de barras de la cantidad de abejas por estado de salud segun subespecie\npd.crosstab(index=bees['subspecies'],\n            columns=bees['health']).plot(kind='bar', title = \"Estado de Salud por cantidad de abejas segun subespecie\", stacked=True)\n\n# Gr\u00e1fico de barras del porcentaje de abejas por estado de salud segun subespecie\npd.crosstab(index=bees['subspecies'],\n            columns=bees['health']).apply(lambda r: r\/r.sum() *100,\n                          axis=1).plot(kind='bar', stacked=True, title = \"% de enfermedades en subespecies\", legend=False)\nplt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), title=\"Health\")\nplt.show()\n","e095e6bf":"# plot_images(data, attribute, samples)\nutils.plot_images(bees,'subspecies',[1,3,6,7])\n# plot_images(data, attribute, samples)\nutils.plot_images(bees,'subspecies',[15,19,42])","3c335411":"# plot_images(data, attribute, samples)\nutils.plot_images(bees,'health',[2,6,8])\n# plot_images(data, attribute, samples)\nutils.plot_images(bees,'health',[10,19,43])","759be6c7":"train_bees, val_bees, test_bees = utils.split(bees)","05f419c5":"optimizer = Adam(lr=1e-5)\nloss = 'categorical_crossentropy'","06fe3411":"from enum import Enum\nclass Convolutional_Base(Enum):\n    SIMPLE = 1\n    COMPLEX = 2\n    MORE_COMPLEX = 3\nclass Classifier(Enum):\n    GLOBAL_MAX_POOLING = 1\n    GLOBAL_AVERAGE_POOLING = 2\n    FLATTEN = 3","20d8122b":"def add_simple_convolutional_base(model, batch_normalization=False):\n    model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', activation ='relu', input_shape = (img_height, img_width, img_channels), name=\"conv_layer_1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu', name=\"conv_layer_2\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n        \n    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu', name=\"conv_layer_3\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))","6f82cbc1":"def add_complex_convolutional_base(model, batch_normalization = False):\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='Same', activation ='relu', input_shape = (img_height, img_width, img_channels), name=\"conv_layer_1.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='Same', activation ='relu', name=\"conv_layer_1.2\"))    \n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_2.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_2.2\")) \n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_3.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_3.2\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    # agregado...\n    model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_4.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_4.2\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))","0937072c":"def add_more_complex_convolutional_base(model, batch_normalization = False):\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='Same', activation ='relu', input_shape = (img_height, img_width, img_channels), name=\"conv_layer_1.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='Same', activation ='relu', name=\"conv_layer_1.2\"))    \n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='Same', activation ='relu', name=\"conv_layer_1.3\"))    \n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_2.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_2.2\")) \n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_2.3\")) \n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    \n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_3.1\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_3.2\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding='Same', activation='relu', name=\"conv_layer_3.3\"))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))","53faa20b":"def add_classifier_with_global_pooling(model, global_pooling, batch_normalization = False, dropout = False, kernel_regularizer = None):\n    model.add(global_pooling)\n    model.add(Dense(1024, activation='relu', kernel_regularizer = kernel_regularizer, name='dense1'))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Dropout(0.5)) if (dropout) else False\n    model.add(Dense(1024, activation='relu', kernel_regularizer = kernel_regularizer, name='dense2'))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Dropout(0.5)) if (dropout) else False\n    model.add(Dense(train_y.columns.size, activation='softmax', name='predictions'))","00e1c041":"def add_classifier_with_flatten(model, batch_normalization = False, dropout = False, kernel_regularizer = None):\n    model.add(Flatten())\n    model.add(Dense(1024, activation = \"relu\", kernel_regularizer = kernel_regularizer, name='dense1'))\n    model.add(BatchNormalization()) if (batch_normalization) else False\n    model.add(Dropout(0.5)) if (dropout) else False\n    model.add(Dense(train_y.columns.size, activation = \"softmax\", name='predictions'))","386f40aa":"def create_model(base_model, classifier, batch_normalization, dropout, kernel_regularizer, summary):\n    model = Sequential()\n    if (base_model == Convolutional_Base.MORE_COMPLEX):\n        add_more_complex_convolutional_base(model, batch_normalization = True)\n    if (base_model == Convolutional_Base.COMPLEX):\n        add_complex_convolutional_base(model, batch_normalization = True)\n    if (base_model == Convolutional_Base.SIMPLE):\n        add_simple_convolutional_base(model, batch_normalization = True)\n    \n    if (classifier == Classifier.GLOBAL_MAX_POOLING):\n        add_classifier_with_global_pooling(model, global_pooling = GlobalMaxPooling2D(), batch_normalization = batch_normalization, dropout = dropout, kernel_regularizer = kernel_regularizer)\n    if (classifier == Classifier.GLOBAL_AVERAGE_POOLING):\n        add_classifier_with_global_pooling(model, global_pooling = GlobalAveragePooling2D(), batch_normalization = batch_normalization, dropout = dropout, kernel_regularizer = kernel_regularizer)\n    if (classifier == Classifier.FLATTEN):\n        add_classifier_with_flatten(model, batch_normalization, dropout, kernel_regularizer)\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy']) \n    model.summary() if (summary) else False\n    \n    return model","d217ed80":"def create_simple_model_with_flatten(summary = False):\n    return create_model(base_model = Convolutional_Base.SIMPLE, classifier = Classifier.FLATTEN, batch_normalization = False, dropout = False, kernel_regularizer = None, summary = False)","e654fab8":"def create_simple_model_with_average_pooling(summary = False):\n    return create_model(base_model = Convolutional_Base.SIMPLE, classifier = Classifier.GLOBAL_AVERAGE_POOLING, batch_normalization = False, dropout = False, kernel_regularizer = None, summary = False)","22c9d8f3":"def create_simple_model_with_max_pooling(summary = False):\n    return create_model(base_model = Convolutional_Base.SIMPLE, classifier = Classifier.GLOBAL_MAX_POOLING, batch_normalization = False, dropout = False, kernel_regularizer = None, summary = False)","7262fd83":"def create_complex_model_with_max_pooling():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.GLOBAL_MAX_POOLING, batch_normalization = False, dropout = False, kernel_regularizer = None, summary = False)","6ca5292e":"def create_complex_model_with_average_pooling():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.GLOBAL_AVERAGE_POOLING, batch_normalization = False, dropout = False, kernel_regularizer = None, summary = False)","ea16d705":"def create_complex_model_with_flatten():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.FLATTEN, batch_normalization = False, dropout = False, kernel_regularizer = None, summary = False)","d7c50428":"def create_best_simple_model_with_batch_normalization():\n    return create_model(base_model = Convolutional_Base.SIMPLE, classifier = Classifier.FLATTEN, batch_normalization = True, dropout = False, kernel_regularizer = None, summary = False)","38751cc4":"def create_best_complex_model_with_batch_normalization():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.FLATTEN, batch_normalization = True, dropout = False, kernel_regularizer = None, summary = False)","2c1d550d":"def create_model_with_dropout():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.FLATTEN, batch_normalization = True, dropout = True, kernel_regularizer = None, summary = False)","a2a40074":"def create_model_with_l1_regularization():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.FLATTEN, batch_normalization = True, dropout = False, kernel_regularizer = l1(0.01), summary = False)","ffc9fde4":"def create_model_with_l2_regularization():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.FLATTEN, batch_normalization = True, dropout = False, kernel_regularizer = l2(0.01), summary = False)","6eb02a8f":"def create_simple_model_fine_tuned():\n    return create_model(base_model = Convolutional_Base.SIMPLE, classifier = Classifier.GLOBAL_MAX_POOLING, batch_normalization = True, dropout = True, kernel_regularizer = l2(0.01), summary = False)","8af355aa":"def create_complex_model_fine_tuned():\n    return create_model(base_model = Convolutional_Base.COMPLEX, classifier = Classifier.GLOBAL_MAX_POOLING, batch_normalization = True, dropout = True, kernel_regularizer =l2(0.01), summary = False)","e2bb613b":"def create_classifier_global_max_pooling(base_model, kernel_regularizer = l2(0.01)):\n    model = Sequential()\n    model.add(base_model)\n    model.add(GlobalMaxPooling2D())\n    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense1\", activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(1024, kernel_regularizer = kernel_regularizer, name = \"dense2\", activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(train_y.columns.size, activation='softmax', name='predictions'))\n        \n    model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])  \n    model.summary()\n    \n    return model","13211f19":"def create_classifier_fully_connected(base_model):\n    model = Sequential()\n    model.add(base_model)\n    model.add(Flatten())\n    model.add(Dense(1024, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(1024, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(train_y.columns.size, activation='softmax', name='predictions'))\n    \n    model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])  \n    model.summary()\n    \n    return model","7f739399":"def add_preprocessing(pretrained_model, preprocess_input):\n    inputs = Input((img_height, img_width, img_channels))\n    x = Lambda(preprocess_input, name='preprocessing')(inputs)\n    outputs = pretrained_model(x)\n    base_model = Model(inputs, outputs)\n     \n    return base_model","efd56461":"def create_inceptionV3_model():\n    base_model = inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape = (img_height, img_width, img_channels))\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    return base_model","f667f447":"def create_vgg16_model():\n    base_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape = (img_height, img_width, img_channels))\n    for layer in base_model.layers:\n        layer.trainable = True\n     \n    return base_model","21746e3d":"def create_resnet50_model():\n    base_model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape = (img_height, img_width, img_channels))\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    return base_model","6b65c3ba":"def create_densenet201_model():\n    base_model = densenet.DenseNet201(weights='imagenet', include_top=False, input_shape = (img_height, img_width, img_channels))\n    for layer in base_model.layers:\n        layer.trainable = False\n \n    return base_model","256d9fda":"class VisualizeImageMaximizeFmap(object):\n    def __init__(self,pic_shape):\n        '''\n        pic_shape : a dimention of a single picture e.g., (96,96,1)\n        '''\n        self.pic_shape = pic_shape\n        \n    def find_n_feature_map(self,layer_name,max_nfmap):\n        '''\n        shows the number of feature maps for this layer\n        only works if the layer is CNN\n        '''\n        n_fmap = None\n        for layer in model.layers:\n            if layer.name == layer_name:\n                weights = layer.get_weights()\n                n_fmap=weights[1].shape[0]\n        if n_fmap is None:\n            print(layer_name + \" is not one of the layer names..\")\n            n_fmap = 1\n        n_fmap = np.min([max_nfmap,n_fmap])\n        return(int(n_fmap))\n\n    def find_image_maximizing_activation(self,iterate,input_img_data,\n                                         picorig=False,\n                                         n_iter = 30):\n        '''\n        The input image is scaled to range between 0 and 1\n        picorig  : True  if the picture image for input is original scale\n                         ranging between 0 and 225\n                   False if the picture image for input is ranging [0,1]\n        '''\n            \n        input_img_data = np.random.random((1, \n                                           self.pic_shape[0],\n                                           self.pic_shape[1],\n                                           self.pic_shape[2]))\n        if picorig:\n            ## if the original picture is unscaled and ranging between (0,225),\n            ## then the image values are centered around 123 with STD=25\n            input_img_data = input_img_data*25 + 123 \n        ## I played with this step value but the final image looks to be robust\n        step = 500 \n\n        \n        \n        # gradient ascent\n        loss_values = []\n        for i in range(n_iter):\n            loss_value, grads_value = iterate([input_img_data, 0])\n            input_img_data += grads_value * step\n            loss_values.append(loss_value) \n        return(input_img_data,loss_values)\n\n    def create_iterate(self,input_img, layer_output,filter_index):\n        '''\n        layer_output[:,:,:,0] is (Nsample, 94, 94) tensor contains:\n        W0^T [f(image)]_{i,j}], i = 1,..., 94, j = 1,..., 94\n        \n        layer_output[:,:,:,1] contains:\n        W1^T [f(image)]_{i,j}], i = 1,..., 94, j = 1,..., 94\n        \n        W0 and W1 are different kernel!\n        '''\n        ## loss is a scalar \n        if len(layer_output.shape) == 4:\n            ## conv layer \n            loss = K.mean(layer_output[:,  :, :, filter_index])\n        elif len(layer_output.shape) ==2:\n            ## fully connected layer\n            loss = K.mean(layer_output[:, filter_index])\n         \n        # calculate the gradient of the loss evaluated at the provided image\n        grads = K.gradients(loss, input_img)[0]\n        # normalize the gradients\n        grads \/= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n\n        # iterate is a function taking (input_img, scalar) and output [loss_value, gradient_value]\n        iterate = K.function([input_img, K.learning_phase()], [loss, grads])\n        return(iterate)\n\n    def deprocess_image(self,x):\n        # standardize to have a mean 0 and std  0.1 \n        x -= x.mean()\n        x \/= (x.std() + 1e-5)\n        x *= 0.1\n\n        # Shift x to have a mean 0.5 and std 0.1\n        # This means 95% of the x should be in between 0 and 1\n        # if x is normal\n        x += 0.5\n        x = np.clip(x, 0, 1)\n\n        # resclar the values to range between 0 and 255\n        x *= 255\n        x = np.clip(x, 0, 255).astype('uint8')\n\n        return x\n\n    def find_images(self,input_img,layer_names,layer_dict, max_nfmap,\n                    picorig=True,n_iter=30):\n        '''\n        Input :\n\n        input_img   : the alias of the input layer from the deep learning model\n        layer_names : list containing the name of the layers whose feature maps to be used\n        layer_dict  : symbolic outputs of each \"key\" layer (we gave them unique names).\n        max_nfmap   : the maximum number of feature map to be used for each layer.\n        pic_shape   : For example pic_shape = (96,96,1)\n\n        Output : \n        dictionary \n\n        key = layer name \n        value = a list containing the tuple of (images, list of loss_values) that maximize each feature map\n        '''\n        argimage = {}\n        ## Look for the image for each feature map of each layer one by one\n        for layer_name in layer_names: ## the layer to visualize\n            n_fmap = self.find_n_feature_map(layer_name,max_nfmap)\n            layer_output = layer_dict[layer_name].output\n            result = self.find_images_for_layer(input_img,\n                                                layer_output,\n                                                range(n_fmap),\n                                                picorig=picorig,\n                                                n_iter=n_iter)\n\n            argimage[layer_name] = result\n        return(argimage)\n\n    def find_images_for_layer(self,input_img,layer_output,indecies,\n                              picorig=False,n_iter=30):\n        '''\n        indecies : list containing index of \n                      --> filtermaps of CNN or \n                      --> nodes of fully-connected layer\n        Output\n\n        a list containing the tuple of (images, list of loss_values) \n        that maximize each feature map\n\n\n        '''\n        result_temp = []\n        for filter_index in indecies: # filtermap to visualize\n                iterate = self.create_iterate(input_img, layer_output,filter_index)\n                input_img_data, loss_values = self.find_image_maximizing_activation(\n                    iterate,input_img,\n                    picorig=picorig,\n                    n_iter=n_iter)\n                result_temp.append((input_img_data,loss_values))\n        return(result_temp)\n\n    def plot_images_wrapper(self,argimage,n_row = 8, scale = 1):\n        '''\n        scale : scale up or down the plot size\n        '''\n        pic_shape = self.pic_shape\n        if pic_shape[2] == 1:\n            pic_shape = self.pic_shape[:2]\n        layer_names = np.sort(list(argimage))\n\n\n        for layer_name in layer_names:\n            n_fmap = len(argimage[layer_name])\n            n_col = np.ceil(n_fmap\/float(n_row))\n            fig = plt.figure(figsize=(n_col*scale,\n                                      n_row*scale))\n            fig.subplots_adjust(hspace=0.001,wspace=0.001)\n            plt.title(layer_name + \" n_featuremap=\" + str(n_fmap))\n            count = 1\n            for value in argimage[layer_name]:\n                input_img_data = value[0][0]\n                img = self.deprocess_image(input_img_data)\n                ax = fig.add_subplot(n_row,n_col,count,\n                                    xticks=[],yticks=[])\n                ax.imshow(img.reshape(*pic_shape),cmap=\"gray\")\n                count += 1\n            plt.show()","2d128d84":"def visualize_filter_model(model, layer_names = ['conv_layer_1.1', 'conv_layer_1.2', 'conv_layer_2.1', 'conv_layer_2.2', 'conv_layer_3.1', 'conv_layer_3.2', 'dense1', 'dense2', 'predictions']):\n    input_img = model.layers[0].input\n    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n    visualizer = VisualizeImageMaximizeFmap(pic_shape = (img_height, img_width, img_channels))\n    max_nfmap = 3\n    argimage = visualizer.find_images(input_img,\n                                  layer_names,\n                                  layer_dict, \n                                  max_nfmap)\n    visualizer.plot_images_wrapper(argimage,n_row = 1, scale = 3)","1c75ba93":"rotation_range = 250      # rotaci\u00f3n aleatoria en grados entre 0 a rotation_range\nzoom_range = 0.5         # zoom aleatorio\nwidth_shift_range = 0.5  # desplazamiento horizontal aleatorio (fracci\u00f3n del total)\nheight_shift_range = 0.5 # desplazamiento vertical aleatorio (fracci\u00f3n del total)\nhorizontal_flip = True   # transposici\u00f3n horizontal\nvertical_flip = True     # transposici\u00f3n horizontal","676f97fd":"batch_size = 100\nepochs = 600\nsteps_per_epoch = 150\npatience = 7","869a0cda":"def training_model(model):\n    return utils.train(model,\n                train_X,\n                train_y, \n                batch_size = batch_size,\n                epochs = epochs,\n                validation_data_X = val_X, \n                validation_data_y = val_y,\n                steps_per_epoch = steps_per_epoch,\n                rotation_range = rotation_range,\n                zoom_range = zoom_range, \n                width_shift_range = width_shift_range,\n                height_shift_range = height_shift_range,\n                horizontal_flip = horizontal_flip,  \n                vertical_flip = vertical_flip,\n                patience = patience,\n                class_weights = class_weights\n            )","140bf135":"class_weights = utils.class_weights(bees['subspecies'])","a952bccf":"train_X, val_X, test_X, train_y, val_y, test_y = utils.load_images_and_target(train_bees, \n                                                                              val_bees, \n                                                                              test_bees,\n                                                                              'subspecies',\n                                                                              img_width, \n                                                                              img_height, \n                                                                              img_channels)","cd0146bb":"def eval_model(training, model):\n    utils.eval_model(training, model, test_X, test_y, 'subspecies')","a1965be2":"def prediction_file(model):\n    df = utils.load_test_and_generate_prediction_file(model, class_weights, 'subspecies', img_width, img_height, img_channels)\n    df.head(5)","5783268c":"train_base_model = False\ntrain_model_with_batch_normalization = False\ntrain_model_with_dropout = False\ntrain_model_with_l1_regularization = False\ntrain_model_with_l2_regularization = False\ntrain_model_fine_tuned = True","eac92a28":"if (train_base_model):\n    training, model = training_model(create_complex_model_with_average_pooling())\n    eval_model(training, model)","17505c0f":"if (train_base_model):\n    training, model = training_model(create_complex_model_with_max_pooling())\n    eval_model(training, model)","a0bc262f":"if (train_base_model):\n    training, model = training_model(create_complex_model_with_flatten())\n    eval_model(training, model)","2ad36ea5":"if (train_base_model):\n    training, model = training_model(create_simple_model_with_average_pooling())\n    eval_model(training, model)","d0c89765":"if (train_base_model):\n    training, model = training_model(create_simple_model_with_max_pooling())\n    eval_model(training, model)","71d72a34":"if (train_base_model):\n    training, model = training_model(create_simple_model_with_flatten())\n    eval_model(training, model)","51cc7f2d":"if (train_model_with_batch_normalization):\n    training, model = training_model(create_best_complex_model_with_batch_normalization())\n    eval_model(training, model)","b6dd4392":"if (train_model_with_batch_normalization):\n    training, model = training_model(create_best_simple_model_with_batch_normalization())\n    eval_model(training, model)","0d5ed13d":"if (train_model_with_dropout):\n    training, model = training_model(create_model_with_dropout())\n    eval_model(training, model)","5ac1c85a":"if (train_model_with_l1_regularization):\n    training, model = training_model(create_model_with_l1_regularization())\n    eval_model(training, model)","2df092ca":"if (train_model_with_l2_regularization):\n    training, model = training_model(create_model_with_l2_regularization())\n    eval_model(training, model)","6e50b68f":"if (train_model_fine_tuned):\n    training, model = training_model(create_complex_model_fine_tuned())","6ef7bb46":"if (train_model_fine_tuned):\n    eval_model(training, model)\n    visualize_filter_model(model)\n    prediction_file(model)","2294d315":"train_inception_v3 = False\ntrain_vgg16 = False\ntrain_resnet50 = False\ntrain_densenet201 = False","4aa6aae6":"if (train_inception_v3):\n    training, model = training_model(create_classifier_global_max_pooling(create_inceptionV3_model()))","a7cb7dfd":"if (train_inception_v3):\n    eval_model(training, model)","a68295de":"if (train_vgg16):\n    training, model = training_model(create_classifier_global_max_pooling(create_vgg16_model()))","35d64c15":"if (train_vgg16):\n    eval_model(training, model)\n    prediction_file(model)","d4f1f561":"if (train_resnet50):\n    training, model = training_model(create_classifier_global_max_pooling(create_resnet50_model()))","ed1c181b":"if (train_resnet50):\n    eval_model(training, model)","82841aac":"if (train_densenet201):\n    training, model = training_model(create_classifier_global_max_pooling(create_densenet201_model()))","d2e3f30d":"if (train_densenet201):\n    eval_model(training, model)","bf5cec94":"class_weights = utils.class_weights(bees['health'])","8cf0c360":"train_X, val_X, test_X, train_y, val_y, test_y = utils.load_images_and_target(train_bees, \n                                                                              val_bees, \n                                                                              test_bees,\n                                                                              'health',\n                                                                              img_width, \n                                                                              img_height, \n                                                                              img_channels)","5c1b689e":"def eval_model(training, model):\n    utils.eval_model(training, model, test_X, test_y, 'health')","23627443":"def prediction_file(model):\n    df = utils.load_test_and_generate_prediction_file(model, class_weights, 'health', img_width, img_height, img_channels)\n    df.head(5)","41696e0c":"train_base_model = False\ntrain_model_with_batch_normalization = False\ntrain_model_with_dropout = False\ntrain_model_with_l1_regularization = False\ntrain_model_with_l2_regularization = False\ntrain_model_fine_tuned = False","8ced60b4":"if (train_base_model):\n    training, model = training_model(create_complex_model_with_average_pooling())\n    eval_model(training, model)","dd4c6dd8":"if (train_base_model):\n    training, model = training_model(create_complex_model_with_max_pooling())\n    eval_model(training, model)","ff9fb67a":"if (train_base_model):\n    training, model = training_model(create_complex_model_with_flatten())\n    eval_model(training, model)","9b6cca7a":"if (train_base_model):\n    training, model = training_model(create_simple_model_with_average_pooling())\n    eval_model(training, model)","7d649f14":"if (train_base_model):\n    training, model = training_model(create_simple_model_with_max_pooling())\n    eval_model(training, model)","ed882d26":"if (train_base_model):\n    training, model = training_model(create_simple_model_with_flatten())\n    eval_model(training, model)","6e5f2d7b":"if (train_model_with_batch_normalization):\n    training, model = training_model(create_best_simple_model_with_batch_normalization())\n    eval_model(training, model)","4c3efd43":"if (train_model_with_batch_normalization):\n    training, model = training_model(create_best_complex_model_with_batch_normalization())\n    eval_model(training, model)","6720f550":"if (train_model_with_dropout):\n    training, model = training_model(create_model_with_dropout())\n    eval_model(training, model)","4327144e":"if (train_model_with_l1_regularization):\n    training, model = training_model(create_model_with_l1_regularization())\n    eval_model(training, model)","9c136fa3":"if (train_model_with_l2_regularization):\n    training, model = training_model(create_model_with_l2_regularization())\n    eval_model(training, model)","4be33a8a":"if (train_model_fine_tuned):\n    training, model = training_model(create_complex_model_fine_tuned())","b712f8b7":"if (train_model_fine_tuned):\n    eval_model(training, model)\n    visualize_filter_model(model)\n    prediction_file(model)","543bc034":"train_inception_v3 = False\ntrain_vgg16 = False\ntrain_resnet50 = False\ntrain_densenet201 = False","674829fa":"if (train_inception_v3):\n    training, model = training_model(create_classifier_global_max_pooling(create_inceptionV3_model()))","f032b091":"if (train_inception_v3):\n    eval_model(training, model)","c50b23f4":"if (train_vgg16):\n    training, model = training_model(create_classifier_global_max_pooling(create_vgg16_model()))","a938494f":"if (train_vgg16):\n    eval_model(training, model)\n    prediction_file(model)","192c2e66":"if (train_resnet50):\n    training, model = training_model(create_classifier_global_max_pooling(create_resnet50_model()))","ab97ea12":"if (train_resnet50):\n    eval_model(training, model)","89e20d35":"if (train_densenet201):\n    training, model = training_model(create_classifier_global_max_pooling(create_densenet201_model()))","aa63f8d0":"if (train_densenet201):\n    eval_model(training, model)","90b98b80":"## 1.3 Global variables","3852d9d9":"# 4. Clasificaci\u00f3n\n\n## 4.1. Data preprocessing\n### 4.1.1 Particionamiento","dd95ce49":"#### 4.4.1.3 Modelo con dropout","0477f5b6":"La carga se hace por cada tipo de entrenamiento (subespecies y estado de salud)","edc220fe":"## 3.1 An\u00e1lisis descriptivo: Distribuciones, Scatterplots, Barplots...","01ef7249":"#### 4.4.1.1.1 Modelo complejo","e90ea773":"# 3. An\u00e1lisis exploratorio de datos","b20d70a7":"#### 4.4.1.2 Modelo con batch normalization","e5be5373":"#### 4.4.2.1.2 Modelo simple","3c8a80e5":"## 3.2 Ver im\u00e1genes","328130f5":"#### 4.4.1.4 Modelo con regularizaci\u00f3n L1","96da9007":"## Transfer learning","38468f9e":"#### 4.4.1.6.2 Modelo VGG16","7335a3c1":"#### Modelos a correr","c73c7fe1":"#### 4.4.1.6.4 Modelo DenseNet201","84724bf0":"#### 4.4.1.1.2 Modelo simple","a25b571d":"### 4.3.1 Par\u00e1metros de transformaci\u00f3n de im\u00e1genes (data augmentation)","64c8033b":"## Regularizaci\u00f3n","7c86c66e":"#### 4.4.1.1 Modelo base","6f239f8f":"# 2. Carga de datos","5e4e9618":"## 1.1 Imports","22992ca6":"## 4.3 Entrenamiento","601c5a64":"## Modelo complejo con distintos clasificadores","4f36dd8d":"#### 4.4.2.4 Modelo con regularizaci\u00f3n L1","15b61412":"### 4.5.2 Modelo VGG16","94b381af":"## 4.5 Transfer learning\n\nLa siguiente secci\u00f3n presenta diferentes redes pre-entrenadas que se pueden utilizar para hacer transferencia de aprendizaje cambiando el clasificador para entrenarlo a nuestro problema particular de clasificaci\u00f3n.","bd7b1a31":"### 4.5.1 Modelo Inception V3 (GoogleNet)","d7e9aab6":"### 4.4.2 Evaluaci\u00f3n estado de salud","93d89451":"#### 4.4.1.5 Modelo con regularizaci\u00f3n L2","9a6fea6d":"## Modelo simple con distintos clasificadores","19cc1981":"#### 4.4.2.3 Modelo con dropout","0fe71bcf":"\n### 4.1.2 Carga de im\u00e1genes","d8b56154":"#### 4.4.1.6.1 Modelo Inception V3 (GoogleNet)","5d014e7b":"### 4.5.4 Modelo DenseNet201","9bd740d6":"#### 4.4.1.5 Modelo fine tuned","735fce63":"## 1.2 Set random seeds","935cf833":"## Fine tuning","ef0f94de":"#### 4.4.2.5 Modelo con regularizaci\u00f3n L2","a127eeca":"#### Modelos a correr","97274d7c":"#### 4.4.2.1 Modelo base","1757bcd5":"## Normalizaci\u00f3n","217e9a93":"#### 4.4.2.2 Modelo con batch normalization","c9ee069e":"## Visualizaci\u00f3n de filtros\n    \n El c\u00f3digo de visualizaci\u00f3n se extrae del siguiente blog: https:\/\/fairyonice.github.io\/Visualization%20of%20Filters%20with%20Keras.html","aaafd2ea":"### 4.4.1.6 Transfer learning\n\nLa siguiente secci\u00f3n presenta diferentes redes pre-entrenadas que se pueden utilizar para hacer transferencia de aprendizaje cambiando el clasificador para entrenarlo a nuestro problema particular de clasificaci\u00f3n.","b518ea3f":"#### 4.4.2.6 Modelo fine tuned","ef00138a":"#### 4.4.1.6.3 Modelo ResNet50","61186a4c":"# 1. Setup","ef3713e8":"### 4.5.3 Modelo ResNet50","07dd83e2":"#### 4.4.2.1.1 Modelo complejo","193f31b6":"## 4.2 CNN","234e5471":"## 4.4 Evaluaci\u00f3n del modelo","f05d1e58":"#### Modelos a correr","db750b4c":"### 4.4.1 Evaluaci\u00f3n sub especies"}}