{"cell_type":{"632e3463":"code","b5c07995":"code","edbbee38":"code","0f562ce9":"code","7ddebae2":"code","62c5ca58":"code","7c6dd956":"code","9a7f2a02":"code","e618dca5":"code","b917b4c8":"code","dd7a65dd":"code","3f4f341f":"code","13380b58":"code","19996a90":"code","a4cb6009":"code","61febd90":"code","2b708be4":"code","cd899ca2":"code","2cd0b22f":"code","c4679990":"code","12fb969d":"code","af54fa1e":"code","f7a5281d":"code","45b86094":"code","5ae526ac":"code","b3070bc9":"code","b3ee2855":"code","f2390175":"code","ffe79b80":"code","a39db69e":"code","75cddb9a":"code","ef01227b":"code","da849c34":"code","7b740d3d":"code","43390b97":"code","9d5b2e65":"code","a5276871":"code","5c5383fe":"code","5bfb4973":"code","34dbdd93":"code","a8234b74":"code","839afa39":"code","4c6eb358":"code","69105bab":"code","f2e9299a":"code","49f886a1":"code","fd5563b4":"code","bb7ae91f":"markdown","7cf78eca":"markdown","f46230df":"markdown","c6c99034":"markdown","e007bca4":"markdown","1c1b931b":"markdown","1f0b655c":"markdown","e76acb2b":"markdown","c9e9b784":"markdown","c76d430c":"markdown","87acb553":"markdown","43af60f5":"markdown","8540b987":"markdown","10676714":"markdown","706224b1":"markdown","ed4d980a":"markdown","d1720022":"markdown","fcb6e7c8":"markdown","41b2d79a":"markdown","de4a26c8":"markdown","52d5c352":"markdown","23281430":"markdown","4a1fe872":"markdown","0e7c86a8":"markdown","7dc52a7c":"markdown","e7f8498a":"markdown","16c22c2c":"markdown","7e712b19":"markdown","3f2b9753":"markdown","ae20b7ab":"markdown","1b9f9aa7":"markdown","395163e7":"markdown","375d07ad":"markdown","612ca6de":"markdown","268dce01":"markdown","a6075b3c":"markdown","f7079edd":"markdown","8cfa9235":"markdown","b761d2be":"markdown","184cb983":"markdown","0cb492af":"markdown","5fe2ac66":"markdown","4133a4a4":"markdown","842c8874":"markdown","4e2a4ee7":"markdown","41cd7a74":"markdown","834eb48e":"markdown","91323d9b":"markdown","74bd5907":"markdown","2e042779":"markdown","c01641fc":"markdown","98e7e6b9":"markdown","74b504d8":"markdown","48ae34c4":"markdown","36484903":"markdown","a8c99601":"markdown","1749e548":"markdown","a1963002":"markdown","27b97e92":"markdown","cd4b998b":"markdown","6970fe56":"markdown","62ab9cf7":"markdown","4a222392":"markdown","918df0f1":"markdown"},"source":{"632e3463":"import os\nimport re\nimport nltk\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nimport torch\nimport torchtext\nfrom torchtext import data\nfrom torch import nn\n# from torch.nn import functional as F\nfrom torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom tqdm.notebook import tqdm\nfrom IPython.core.display import display, HTML\ntqdm().pandas()","b5c07995":"DATA_PATH = '..\/input\/quora-insincere-questions-classification\/'\n\ntrain_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest_df  = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))","edbbee38":"train_df.head()","0f562ce9":"print(\"Number of data points in training data:\", train_df.shape[0])\nprint(\"Number of data points in test data:\", test_df.shape[0])","7ddebae2":"# Bar chart\nplt.subplot(1, 2, 1)\ntrain_df.groupby('target')['qid'].count().plot.bar()\nplt.grid(True)\nplt.title('Target Count')\nplt.subplots_adjust(right=1.9)\n\n# Pie Chart\nplt.subplot(1, 2, 2)\nvalues = [train_df[train_df['target']==0].shape[0], train_df[train_df['target']==1].shape[0]]\nlabels = ['Sincere questions', 'Insincere questions']\n\nplt.pie(values, labels=labels, autopct='%1.1f%%', shadow=True)\nplt.title('Target Distribution')\nplt.tight_layout()\nplt.subplots_adjust(right=1.9)\nplt.show()","62c5ca58":"#train_df['num_words'] = train_df['question_text'].apply(lambda x: len(str(x).split()))\n#print(\"Maximum length of a sincere question:\", max(train_df[train_df['target']==0]['num_words']))\n#print(\"Maximum length of a insincere question:\", max(train_df[train_df['target']==]['num_words']))","7c6dd956":"train_df['word_count']= train_df.question_text.progress_apply(lambda x: len(x.split()))\n#data_neg = train_df[train_df['target']==0]\n#data_pos = train_df[train_df['target']==1]\n\n#statistic = pd.merge(\n#    data_neg.describe(percentiles=[.8, .9999])\n#    data_pos[['word_count']].describe(percentiles=[.8, .9999]), \n#    left_index=True, right_index=True, suffixes=('_sincere', '_insincere')\n#)\ntrain_df.describe(percentiles=[.8, .9999]).apply(lambda s: s.apply('{0:.2f}'.format))","9a7f2a02":"train_df['word_count']= train_df.question_text.progress_apply(lambda x: len(x.split()))\ndata_neg = train_df[train_df['target']==0]\ndata_pos = train_df[train_df['target']==1]\n\nstatistic = pd.merge(\n    data_neg[['word_count']].describe(percentiles=[.8, .9999]), \n    data_pos[['word_count']].describe(percentiles=[.8, .9999]), \n    left_index=True, right_index=True, suffixes=('_sincere', '_insincere')\n)\n#statistic.describe(percentiles=[.8, .9999]).apply(lambda s: s.apply('{0:.2f}'.format))\n\ncolLabels = statistic.columns\ncellText = statistic.round(2).values\nrowLabels = statistic.index\n\nfig, axes = plt.subplots()\n#axes = fig.add_axes([0,0,1,1])\n#axes.bar(['sincere question', 'insincere question'], train_df.target.value_counts())\nfor p in axes.patches:\n    width = p.get_width()\n    height = p.get_height()\n    percent = height \/ len(train_df)\n    x, y = p.get_xy() \n    axes.annotate(f'{percent:.2%}', (x + width\/2, y + height + 0.01*len(train_df)), ha='center')\n    \naxes.axis('off')\nmpl_table = axes.table(cellText = cellText, colLabels=colLabels, rowLabels = rowLabels, bbox=[2, 0, 2, 1.5], )\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(14)","e618dca5":"# Number of capital_letters\ntrain_df['num_capital_let'] = train_df['question_text'].apply(lambda x: len([c for c in str(x) if c.isupper()]))\n\n# Number of special characters\ntrain_df['num_special_char'] = train_df['question_text'].str.findall(r'[^a-zA-Z0-9 ]').str.len()\n\n# Number of unique words\ntrain_df['num_unique_words'] = train_df['question_text'].apply(lambda x: len(set(str(x).split())))\n\n# Number of numerics\ntrain_df['num_numerics'] = train_df['question_text'].apply(lambda x: sum(c.isdigit() for c in x))\n\n# Number of characters\ntrain_df['num_char'] = train_df['question_text'].apply(lambda x: len(str(x)))\n\ntrain_df.head()","b917b4c8":"def display_boxplot(_x, _y, _data, _title):\n    sns.boxplot(x=_x, y=_y, data=_data)\n    plt.grid(True)\n    plt.title(_title)","dd7a65dd":"# Boxplot: Number of words\nplt.subplot(2, 3, 1)\ndisplay_boxplot('target', 'word_count', train_df, 'No. of words in each class')\n\n# Boxplot: Number of chars\nplt.subplot(2, 3, 2)\ndisplay_boxplot('target', 'num_char', train_df, 'Number of characters in each class')\n\n# Boxplot: Number of unique words\nplt.subplot(2, 3, 3)\ndisplay_boxplot('target', 'num_unique_words', train_df, 'Number of unique words in each class')\n\n# Boxplot: Number of special characters\nplt.subplot(2, 3, 4)\ndisplay_boxplot('target', 'num_special_char', train_df, 'No. of special characters in each class')\n\n# Boxplot: Number of capital letters\nplt.subplot(2, 3, 6)\ndisplay_boxplot('target', 'num_capital_let', train_df, 'No. of capital letters in each class')\n\n\nplt.subplots_adjust(right=3.0)\nplt.subplots_adjust(top=2.0)\nplt.show()","3f4f341f":"# Correlation matrix\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = train_df.corr()\nsns.heatmap(corr, ax=ax)\nplt.title(\"Correlation matrix\")\nplt.show()","13380b58":"contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\npunctuation = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n    '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`', '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', \n    '\u2588', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00ac', '\u2591', '\u00a1', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', \n    '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u22c5', '\u2018', '\u221e', \n    '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u2264', '\u2021', '\u221a', '\u25c4', '\u2501', \n    '\u21d2', '\u25b6', '\u2265', '\u255d', '\u2661', '\u25ca', '\u3002', '\u2708', '\u2261', '\u263a', '\u2714', '\u21b5', '\u2248', '\u2713', '\u2663', '\u260e', '\u2103', '\u25e6', '\u2514', '\u201f', '\uff5e', '\uff01', '\u25cb', \n    '\u25c6', '\u2116', '\u2660', '\u258c', '\u273f', '\u25b8', '\u2044', '\u25a1', '\u2756', '\u2726', '\uff0e', '\u00f7', '\uff5c', '\u2503', '\uff0f', '\uffe5', '\u2560', '\u21a9', '\u272d', '\u2590', '\u263c', '\u263b', '\u2510', \n    '\u251c', '\u00ab', '\u223c', '\u250c', '\u2109', '\u262e', '\u0e3f', '\u2266', '\u266c', '\u2727', '\u232a', '\uff0d', '\u2302', '\u2716', '\uff65', '\u25d5', '\u203b', '\u2016', '\u25c0', '\u2030', '\\x97', '\u21ba', \n    '\u2206', '\u2518', '\u252c', '\u256c', '\u060c', '\u2318', '\u2282', '\uff1e', '\u2329', '\u2399', '\uff1f', '\u2620', '\u21d0', '\u25ab', '\u2217', '\u2208', '\u2260', '\u2640', '\u2654', '\u02da', '\u2117', '\u2517', '\uff0a', \n    '\u253c', '\u2740', '\uff06', '\u2229', '\u2642', '\u203f', '\u2211', '\u2023', '\u279c', '\u251b', '\u21d3', '\u262f', '\u2296', '\u2600', '\u2533', '\uff1b', '\u2207', '\u21d1', '\u2730', '\u25c7', '\u266f', '\u261e', '\u00b4', \n    '\u2194', '\u250f', '\uff61', '\u25d8', '\u2202', '\u270c', '\u266d', '\u2523', '\u2534', '\u2513', '\u2728', '\\xa0', '\u02dc', '\u2765', '\u252b', '\u2120', '\u2712', '\uff3b', '\u222b', '\\x93', '\u2267', '\uff3d', \n    '\\x94', '\u2200', '\u265b', '\\x96', '\u2228', '\u25ce', '\u21bb', '\u21e9', '\uff1c', '\u226b', '\u2729', '\u272a', '\u2655', '\u061f', '\u20a4', '\u261b', '\u256e', '\u240a', '\uff0b', '\u2508', '\uff05', \n    '\u254b', '\u25bd', '\u21e8', '\u253b', '\u2297', '\uffe1', '\u0964', '\u2582', '\u272f', '\u2587', '\uff3f', '\u27a4', '\u271e', '\uff1d', '\u25b7', '\u25b3', '\u25d9', '\u2585', '\u271d', '\u2227', '\u2409', '\u262d', \n    '\u250a', '\u256f', '\u263e', '\u2794', '\u2234', '\\x92', '\u2583', '\u21b3', '\uff3e', '\u05f3', '\u27a2', '\u256d', '\u27a1', '\uff20', '\u2299', '\u2622', '\u02dd', '\u220f', '\u201e', '\u2225', '\u275d', '\u2610', \n    '\u2586', '\u2571', '\u22d9', '\u0e4f', '\u2601', '\u21d4', '\u2594', '\\x91', '\u279a', '\u25e1', '\u2570', '\\x85', '\u2662', '\u02d9', '\u06de', '\u2718', '\u272e', '\u2611', '\u22c6', '\u24d8', '\u2752', '\u2623', '\u2709', '\u230a', '\u27a0', '\u2223', '\u2751', '\u25e2', '\u24d2', '\\x80', '\u3012', '\u2215', '\u25ae', '\u29bf', '\u272b', '\u271a', '\u22ef', '\u2669', '\u2602', '\u275e', '\u2017', '\u0702', '\u261c', \n    '\u203e', '\u271c', '\u2572', '\u2218', '\u27e9', '\uff3c', '\u27e8', '\u0387', '\u2717', '\u265a', '\u2205', '\u24d4', '\u25e3', '\u0361', '\u201b', '\u2766', '\u25e0', '\u2704', '\u2744', '\u2203', '\u2423', '\u226a', '\uff62', \n    '\u2245', '\u25ef', '\u263d', '\u220e', '\uff63', '\u2767', '\u0305', '\u24d0', '\u2198', '\u2693', '\u25a3', '\u02d8', '\u222a', '\u21e2', '\u270d', '\u22a5', '\uff03', '\u23af', '\u21a0', '\u06e9', '\u2630', '\u25e5', \n    '\u2286', '\u273d', '\u26a1', '\u21aa', '\u2741', '\u2639', '\u25fc', '\u2603', '\u25e4', '\u274f', '\u24e2', '\u22b1', '\u279d', '\u0323', '\u2721', '\u2220', '\uff40', '\u25b4', '\u2524', '\u221d', '\u264f', '\u24d0', \n    '\u270e', '\u037e', '\u2424', '\uff07', '\u2763', '\u2702', '\u2724', '\u24de', '\u262a', '\u2734', '\u2312', '\u02db', '\u2652', '\uff04', '\u2736', '\u25bb', '\u24d4', '\u25cc', '\u25c8', '\u275a', '\u2742', '\uffe6', \n    '\u25c9', '\u255c', '\u0303', '\u2731', '\u2556', '\u2749', '\u24e1', '\u2197', '\u24e3', '\u267b', '\u27bd', '\u05c0', '\u2732', '\u272c', '\u2609', '\u2589', '\u2252', '\u2625', '\u2310', '\u2668', '\u2715', '\u24dd', \n    '\u22b0', '\u2758', '\uff02', '\u21e7', '\u0335', '\u27aa', '\u2581', '\u258f', '\u2283', '\u24db', '\u201a', '\u2670', '\u0301', '\u270f', '\u23d1', '\u0336', '\u24e2', '\u2a7e', '\uffe0', '\u274d', '\u2243', '\u22f0', '\u264b', \n    '\uff64', '\u0302', '\u274b', '\u2733', '\u24e4', '\u2564', '\u2595', '\u2323', '\u2738', '\u212e', '\u207a', '\u25a8', '\u2568', '\u24e5', '\u2648', '\u2743', '\u261d', '\u273b', '\u2287', '\u227b', '\u2658', '\u265e', \n    '\u25c2', '\u271f', '\u2320', '\u2720', '\u261a', '\u2725', '\u274a', '\u24d2', '\u2308', '\u2745', '\u24e1', '\u2667', '\u24de', '\u25ad', '\u2771', '\u24e3', '\u221f', '\u2615', '\u267a', '\u2235', '\u235d', '\u24d1', \n    '\u2735', '\u2723', '\u066d', '\u2646', '\u24d8', '\u2236', '\u269c', '\u25de', '\u0bcd', '\u2739', '\u27a5', '\u2195', '\u0333', '\u2237', '\u270b', '\u27a7', '\u220b', '\u033f', '\u0367', '\u2505', '\u2964', '\u2b06', '\u22f1', \n    '\u2604', '\u2196', '\u22ee', '\u06d4', '\u264c', '\u24db', '\u2555', '\u2653', '\u276f', '\u264d', '\u258b', '\u273a', '\u2b50', '\u273e', '\u264a', '\u27a3', '\u25bf', '\u24d1', '\u2649', '\u23e0', '\u25fe', '\u25b9', \n    '\u2a7d', '\u21a6', '\u2565', '\u2375', '\u230b', '\u0589', '\u27a8', '\u222e', '\u21e5', '\u24d7', '\u24d3', '\u207b', '\u239d', '\u2325', '\u2309', '\u25d4', '\u25d1', '\u273c', '\u264e', '\u2650', '\u256a', '\u229a', \n    '\u2612', '\u21e4', '\u24dc', '\u23a0', '\u25d0', '\u26a0', '\u255e', '\u25d7', '\u2395', '\u24e8', '\u261f', '\u24df', '\u265f', '\u2748', '\u21ac', '\u24d3', '\u25fb', '\u266e', '\u2759', '\u2664', '\u2209', '\u061b', \n    '\u2042', '\u24dd', '\u05be', '\u2651', '\u256b', '\u2553', '\u2573', '\u2b05', '\u2614', '\u2638', '\u2504', '\u2567', '\u05c3', '\u23a2', '\u2746', '\u22c4', '\u26ab', '\u030f', '\u260f', '\u279e', '\u0342', '\u2419', '\u24e4', '\u25df', '\u030a', '\u2690', '\u2719', '\u2199', '\u033e', '\u2118', '\u2737', '\u237a', '\u274c', '\u22a2', '\u25b5', '\u2705', '\u24d6', '\u2628', '\u25b0', '\u2561', '\u24dc', '\u2624', '\u223d', '\u2558', \n    '\u02f9', '\u21a8', '\u2659', '\u2b07', '\u2671', '\u2321', '\u2800', '\u255b', '\u2755', '\u2509', '\u24df', '\u0300', '\u2656', '\u24da', '\u2506', '\u239c', '\u25dc', '\u26be', '\u2934', '\u2707', '\u255f', '\u239b', \n    '\u2629', '\u27b2', '\u279f', '\u24e5', '\u24d7', '\u23dd', '\u25c3', '\u2562', '\u21af', '\u2706', '\u02c3', '\u2374', '\u2747', '\u26bd', '\u2552', '\u0338', '\u265c', '\u2613', '\u27b3', '\u21c4', '\u262c', '\u2691', \n    '\u2710', '\u2303', '\u25c5', '\u25a2', '\u2750', '\u220a', '\u2608', '\u0965', '\u23ae', '\u25a9', '\u0bc1', '\u22b9', '\u2035', '\u2414', '\u260a', '\u27b8', '\u030c', '\u263f', '\u21c9', '\u22b3', '\u2559', '\u24e6', \n    '\u21e3', '\uff5b', '\u0304', '\u219d', '\u239f', '\u258d', '\u2757', '\u05f4', '\u0384', '\u259e', '\u25c1', '\u26c4', '\u21dd', '\u23aa', '\u2641', '\u21e0', '\u2607', '\u270a', '\u0bbf', '\uff5d', '\u2b55', '\u2798', \n    '\u2040', '\u2619', '\u275b', '\u2753', '\u27f2', '\u21c0', '\u2272', '\u24d5', '\u23a5', '\\u06dd', '\u0364', '\u208b', '\u0331', '\u030e', '\u265d', '\u2273', '\u2599', '\u27ad', '\u0700', '\u24d6', '\u21db', '\u258a', \n    '\u21d7', '\u0337', '\u21f1', '\u2105', '\u24e7', '\u269b', '\u0310', '\u0315', '\u21cc', '\u2400', '\u224c', '\u24e6', '\u22a4', '\u0313', '\u2626', '\u24d5', '\u259c', '\u2799', '\u24e8', '\u2328', '\u25ee', '\u2637', \n    '\u25cd', '\u24da', '\u2254', '\u23e9', '\u2373', '\u211e', '\u250b', '\u02fb', '\u259a', '\u227a', '\u0652', '\u259f', '\u27bb', '\u032a', '\u23ea', '\u0309', '\u239e', '\u2507', '\u235f', '\u21ea', '\u258e', '\u21e6', '\u241d', \n    '\u2937', '\u2256', '\u27f6', '\u2657', '\u0334', '\u2644', '\u0368', '\u0308', '\u275c', '\u0321', '\u259b', '\u2701', '\u27a9', '\u0bbe', '\u02c2', '\u21a5', '\u23ce', '\u23b7', '\u0332', '\u2796', '\u21b2', '\u2a75', '\u0317', '\u2762', \n    '\u224e', '\u2694', '\u21c7', '\u0311', '\u22bf', '\u0316', '\u260d', '\u27b9', '\u294a', '\u2041', '\u2722']\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}","19996a90":"def clean_text(txt, contraction_dict=contraction_dict, punctuation=punctuation, mispell_dict=mispell_dict):\n    \"\"\"\"\"\n    cleans the input text in the following steps\n    1- replace contractions\n    2- removing punctuation\n    3- spliting into words\n    4- removing stopwords\n    \"\"\"\"\"\n\n    def _get_contraction(contraction_dict):\n        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n        return contraction_dict, contraction_re\n\n    # replace contractions\n    def remove_contraction(text, contraction_dict):\n        contractions, contractions_re = _get_contraction(contraction_dict)\n        def replace(match):\n            return contractions[match.group(0)]\n        return contractions_re.sub(replace, text)\n    \n    # remove punctuations\n    def remove_punctuation(text):\n        txt  = \"\".join([char for char in text if char not in punctuation])\n        return re.sub(\"[^a-zA-Z0-9]+\", ' ', txt)\n    \n    # remove stopword\n    def remove_stopword(words):\n        stop_words = set(stopwords.words('english'))\n        words = [w for w in words if not w in stop_words]\n        return words\n\n    # correct mispell \n    def correct_mispell(words):\n        for i in range(0, len(words)):\n            if mispell_dict.get(words[i]) is not None:\n                words[i] = mispell_dict.get(words[i])\n            elif mispell_dict.get(words[i].lower()) is not None:\n                words[i] = mispell_dict.get(words[i].lower())\n        return words    \n    \n    # to lower case\n    def to_lower(words):\n        return words.lower()\n\n    txt = remove_contraction(txt, contraction_dict)\n    txt = remove_punctuation(txt)\n    # split into words\n    words = word_tokenize(txt)\n    # words = remove_stopword(words)\n    \n    words = correct_mispell(words)\n    \n    cleaned_text = ' '.join(words)\n    \n    # to lower case\n    cleaned_text = to_lower(cleaned_text)\n    return cleaned_text","a4cb6009":"# Start preprocessing train dataset and test dataset\ntqdm.pandas()\ntrain_df['cleaned_question_text'] = train_df['question_text'].progress_apply(lambda txt: clean_text(txt))\ntest_df['cleaned_question_text']  = test_df['question_text'].progress_apply(lambda txt: clean_text(txt))","61febd90":"train_df.head()","2b708be4":"# plot and remove NaN value in train_set\nnan_rows = train_df[train_df['cleaned_question_text'].isnull(inplace=True)]\nnan_rows","cd899ca2":"train_df = train_df[train_df['cleaned_question_text'].notna()]","2cd0b22f":"# compression_opts = dict(method='zip', archive_name='out.csv')  \n# train_df.to_csv(\".\/train_processed.zip\", index=False, compression=compression_opts)\n\ntrain_df.to_csv('.\/train_preprocessed.csv', index=False)\ntest_df.to_csv('.\/test_preprocessed.csv', index=False)","c4679990":"train_df.head()","12fb969d":"TEXT  = data.Field(tokenize='spacy', batch_first=True, include_lengths=True)\nLABEL = data.LabelField(dtype = torch.int64, batch_first=True)","af54fa1e":"fields = [(None, None), (None,None), ('target', LABEL), ('text', TEXT)]\n\n# TabularDataset from torchtext only support to load from storage file\ndataset = data.TabularDataset('.\/train_preprocessed.csv', format = 'csv', fields = fields, skip_header = True)","f7a5281d":"# log data example\nprint(vars(dataset.examples[0]))","45b86094":"import random\n\nSEED = 42\ntorch.manual_seed(SEED)\ntraining_set, valid_set = dataset.split(split_ratio=0.8, random_state = random.seed(SEED))","5ae526ac":"!unzip ..\/input\/quora-insincere-questions-classification\/embeddings.zip -d .\/","b3070bc9":"#due to the space limitation of kaggle, we must clear non use embedding.\nNON_USE_DIR = ['.\/wiki-news-300d-1M\/', '.\/GoogleNews-vectors-negative300', '.\/paragram_300_sl999']\n\nimport os, shutil\ndef remove_dir(path):\n    for filename in os.listdir(path):\n        file_path = os.path.join(path, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print('Failed to delete %s. Reason: %s' % (file_path, e))\n    os.rmdir(path)\n            \nfor dir in NON_USE_DIR:\n    remove_dir(dir)","b3ee2855":"# Load embedding as storage file\nimport torchtext.vocab as vocab\n\ncustom_embeddings = vocab.Vectors(name = '.\/glove.840B.300d\/glove.840B.300d.txt')","f2390175":"TEXT.build_vocab(training_set, min_freq=3, vectors = custom_embeddings)\nLABEL.build_vocab(training_set)\n\n#No. of unique tokens in text\nprint(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n\n#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n\n#Commonly used words\nprint(TEXT.vocab.freqs.most_common(10)) ","ffe79b80":"from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence","a39db69e":"class BidirectionalLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layer, output_size, dropout_rate=0.1):\n        super(BidirectionalLSTM, self).__init__()\n        self.dimension = hidden_size\n        # Define layer\n        self.embedding   = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm        = nn.LSTM(embedding_dim, hidden_size, num_layer, batch_first=True, bidirectional=True)\n        self.dropout     = nn.Dropout(0.1)\n        self.fc          = nn.Linear(hidden_size*2, output_size)\n        self.sigmoid     = nn.Sigmoid()\n\n    def forward(self, input, input_len):\n        # embedding word\n        x = self.embedding(input)\n        \n        x = pack_padded_sequence(x, input_len.cpu(), batch_first=True, enforce_sorted=False)\n        \n        packed_output, (hidden, cell) = self.lstm(x)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n        out_forward = output[range(len(output)), input_len.cpu() - 1, :self.dimension]\n        out_reverse = output[:, 0, self.dimension:]\n        out_reduced = torch.cat((out_forward, out_reverse), 1)\n        text_feature = self.dropout(out_reduced)\n\n        text_feature = self.fc(text_feature).squeeze(1)\n        return self.sigmoid(text_feature)\n        \n        \n        # Notes!\n#         output = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n#         output = self.fc(output)\n#         return self.sigmoid(output)","75cddb9a":"class LSTM_GRU(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layer, output_size, dropout_rate=0.1):\n        super(LSTM_GRU, self).__init__()\n        # Define layer\n        self.embedding   = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm        = nn.LSTM(embedding_dim, hidden_size, num_layer, batch_first=True, bidirectional=True)\n        self.gru         = nn.GRU(hidden_size*2, hidden_size\/\/2, num_layer, batch_first=True, bidirectional=True)\n\n        self.fc          = nn.Linear(hidden_size*3, output_size)\n        \n        self.emb_dropout = nn.Dropout2d(dropout_rate)\n        self.sigmoid     = nn.Sigmoid()\n\n    def forward(self, input, input_len):\n        # layer 1: embedding\n        x = self.embedding(input)\n        \n        # layer 2: Spatial Dropout 1D\n        embed = x.unsqueeze(2) # (N, T, 1, K)\n        embed = embed.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        embed = self.emb_dropout(embed)  # (N, K, 1, T)\n        embed = embed.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = embed.squeeze(2)  # (N, T, K)\n        \n        # layer 3: Bidirectional LSTM\n        x = pack_padded_sequence(x, input_len.cpu(), batch_first=True, enforce_sorted=False) # (N, T, K)\n        packed_lstm, h_lstm = self.lstm(x)\n        \n        # layer 4: Bidirectional GRU\n        packed_gru, h_gru = self.gru(packed_lstm)\n        \n        packed_lstm = pad_packed_sequence(packed_lstm, batch_first=True)\n        packed_gru = pad_packed_sequence(packed_gru, batch_first=True)\n        \n        # layer 5: Concat\n        x = torch.cat((packed_gru[0], packed_lstm[0]), 2)\n        \n        # layer 6: Global Average Pool\n        avg_pool = torch.mean(x, 1)\n        \n        # layer 7: Fully connected\n        x = self.fc(avg_pool)\n        x = self.sigmoid(x)\n        return x","ef01227b":"def binary_accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.round(preds)\n    \n    correct = (rounded_preds == y).float() \n    acc = correct.sum() \/ len(correct)\n    return acc\n\ndef f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n    '''Calculate F1 score. Can work with gpu tensors\n    \n    The original implmentation is written by Michal Haltuf on Kaggle.\n    \n    Returns\n    -------\n    torch.Tensor\n        `ndim` == 1. 0 <= val <= 1\n    \n    Reference\n    ---------\n    - https:\/\/www.kaggle.com\/rejpalcz\/best-loss-function-for-f1-score-metric\n    - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n    - https:\/\/discuss.pytorch.org\/t\/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification\/28265\/6\n    \n    '''\n    assert y_true.ndim == 1\n    assert y_pred.ndim == 1 or y_pred.ndim == 2\n    \n    if y_pred.ndim == 2:\n        y_pred = y_pred.argmax(dim=1)\n        \n    \n    tp = (y_true * y_pred).sum().to(torch.float32)\n    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n    \n    epsilon = 1e-7\n    \n    precision = tp \/ (tp + fp + epsilon)\n    recall = tp \/ (tp + fn + epsilon)\n    \n    f1 = 2* (precision*recall) \/ (precision + recall + epsilon)\n#     f1.requires_grad = is_training\n    return f1","da849c34":"def train(model, device, train_iterator, optimizer, loss_function):\n    model.train()\n    running_loss = 0\n    accuracy     = 0\n    for input, target in train_iterator:\n        input, input_len, target = input[0].to(device), input[1].to(device), target.to(device, dtype=torch.float32)\n\n        # forward\n        predict = model(input, input_len).squeeze()\n        loss = loss_function(predict, target)\n\n        # metric\n        accuracy     += f1_loss(predict, target)\n        running_loss += loss.item()\n        \n        # zero the gradient \n        optimizer.zero_grad()\n\n        # backpropagation + step\n        loss.backward()\n        optimizer.step()\n        \n    epoch_loss = running_loss\/len(train_iterator)\n    epoch_acc  = accuracy\/len(train_iterator)\n\n    return epoch_loss, epoch_acc","7b740d3d":"def test(model, device, test_iterator, loss_function):\n    model.eval()\n    running_loss = 0\n    accuracy     = 0\n    \n    with torch.no_grad():\n        for input, target in test_iterator:\n            input, input_len, target = input[0].to(device), input[1].to(device), target.to(device, dtype=torch.float32)\n\n            predict = model(input, input_len).squeeze()\n            loss = loss_function(predict, target)\n\n            running_loss += loss.item()\n            accuracy     += f1_loss(predict, target)\n\n    epoch_loss = running_loss\/len(test_iterator)\n    epoch_acc  = accuracy\/len(test_iterator)\n    \n    return epoch_loss, epoch_acc","43390b97":"# using cuda\ndevice = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Training on:\", torch.cuda.get_device_name(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\")\n\n# config\nBATCH_SIZE       = 64\nVOCAB_SIZE       = len(TEXT.vocab)\nEMBEDDING_DIM    = 300\nHIDDEN_SIZE      = 128\nOUTPUT_SIZE      = 1\nNUM_LAYER        = 2\n\nN_EPOCH          = 10","9d5b2e65":"#Load an iterator\ntrain_iterator = data.BucketIterator(training_set, batch_size = BATCH_SIZE, \n                                     sort_key = lambda x: len(x.text), sort=True, sort_within_batch=True, device=device)\nvalid_iterator = data.BucketIterator(valid_set, batch_size = BATCH_SIZE, \n                                     sort_key = lambda x: len(x.text), sort=True, sort_within_batch=True, device=device)\n","a5276871":"# init model\nmodel = LSTM_GRU(vocab_size=VOCAB_SIZE, \n             embedding_dim=EMBEDDING_DIM, \n             hidden_size=HIDDEN_SIZE, \n             num_layer=NUM_LAYER, \n             output_size=OUTPUT_SIZE).to(device)\n\n# model.load_state_dict(torch.load('.\/weights.pt'))\n\n# loss function & optimizer\noptimizer = optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.07)\n\nciteration = nn.BCELoss()","5c5383fe":"#Initialize the pretrained embedding\npretrained_embeddings = TEXT.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embeddings)\n\nprint(pretrained_embeddings.shape)","5bfb4973":"# Start training\nbest_accuracy = 0\n\ntrain_losses, train_res = [], []\nvalid_losses, valid_res = [], []\n\nfor epoch in range(N_EPOCH):\n    # train\n    train_loss, train_accuracy = train(model, device, train_iterator, optimizer, citeration)\n    train_losses.append(train_loss)\n    train_res.append(train_accuracy)\n    \n    # evaluate\n    test_loss, test_accuracy = test(model, device, valid_iterator, citeration)\n    valid_losses.append(test_loss)\n    valid_res.append(test_accuracy)\n    \n    #save the best model\n    if test_accuracy > best_accuracy:\n        best_accuracy = test_accuracy\n        torch.save(model.state_dict(), '.\/weights.pt')\n    \n    print(f'Epoch {epoch+1} summary ===========================')\n    print(f'Train Loss: {train_loss:.3f} | Train F1 score: {train_accuracy*100:.2f}%')\n    print(f' Val. Loss: {test_loss:.3f} |  Val. F1 score: {test_accuracy*100:.2f}%')","34dbdd93":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix","a8234b74":"epoch = range(0, len(train_losses))\n\nfig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharex=True, figsize=(24, 6))\n\nax0.plot(epoch, train_losses, 'g', label='Training loss')\nax0.plot(epoch, valid_losses, 'b', label='validation loss')\nax0.set_title('Training and Validation loss')\nax0.set_xlabel('Epochs')\nax0.set_ylabel('Loss')\nax0.legend()\n\nax1.plot(epoch, train_res, 'g', label='Training F1 score')\nax1.plot(epoch, valid_res, 'b', label='validation F1 score')\nax1.set_title('Training and Validation F1 score')\nax1.set_xlabel('Epochs')\nax1.set_ylabel('F1 score')\nax1.legend()\n\nplt.show()","839afa39":"# plot confusion matrix\nnb_classes = 2\nconfusion_matrix = np.zeros((nb_classes, nb_classes))\n\nwith torch.no_grad():\n    for input, target in test_iterator:\n        input, input_len, target = input[0].to(device), input[1].to(device), target.to(device, dtype=torch.float32)\n\n        predict = model(input, input_len).squeeze()\n        loss = loss_function(predict, target)\n\n        \n#         _, preds = torch.max(outputs, 1)\n        for t, p in zip(target.view(-1), predict.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n\nplt.figure(figsize=(15,10))\n\nclass_names = list(label2class.values())\ndf_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","4c6eb358":"# test dataset\ntest_df = pd.read_csv('.\/test_preprocessed.csv')\ntest_df.head()","69105bab":"# helper function\nimport spacy\nnlp = spacy.load('en')\n\ndef predict(model, sentence):\n    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence # nlp.tokenizer(sentence)\n    indexed   = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n    length = [len(indexed)]                                    #compute no. of words\n    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n    length_tensor = torch.LongTensor(length)                   #convert to tensor\n    \n    try:\n        prediction = model(tensor, length_tensor)              #prediction \n    except:\n        # print(\"Empty sentence:\", sentence)\n        return 0\n    \n    return prediction.item()   ","f2e9299a":"raw_prediction, prediction = [], []\nTHRES_HOLD = 0.32\n\nmodel.load_state_dict(torch.load('.\/weights.pt'))\nmodel.eval()\n\nfor idx, row in test_df.iterrows():\n    pred = 0\n    raw_pred = predict(model, row['cleaned_question_text'])\n    if raw_pred >= THRES_HOLD:\n        pred = 1\n        \n    # save to list\n    prediction.append(pred)\n    raw_prediction.append(raw_pred)\n\nprint('# of prediction', len(prediction))","49f886a1":"# merge\ntest_df['raw_prediction'] = raw_prediction\ntest_df['prediction'] = prediction\n\ntest_df.head(10)","fd5563b4":"prediction_df = test_df.drop(['question_text', 'cleaned_question_text', 'raw_prediction'], axis=1)\nprediction_df.to_csv('.\/submission.csv', index=False)\n\nprediction_df.head(10)","bb7ae91f":"Sau khi c\u00f3 \u0111\u01b0\u1ee3c k\u1ebft qu\u1ea3 t\u1eeb LSTM \u0111\u00e3 s\u1eed d\u1ee5ng b\u00ean tr\u00ean, em th\u1ef1c hi\u1ec7n nghi\u00ean c\u1ee9u v\u00e0 th\u1eed nghi\u1ec7m m\u1ed9t c\u1ea5u tr\u00fac m\u1ea1ng nh\u01b0 \u1ea3nh d\u01b0\u1edbi \u0111\u00e2y.\n\n\u0110\u1eb7c \u0111i\u1ec3m c\u1ee7a n\u00f3 l\u00e0 **x\u1ebfp ch\u1ed3ng hai m\u1ea1ng n\u01a1 ron LSTM v\u00e0 GRU hai chi\u1ec1u** v\u1edbi nhau, k\u1ebft h\u1ee3p th\u00eam l\u1edbp **Dropout**, **Max Pooling** \u0111\u1ec3 t\u0103ng kh\u1ea3 n\u0103ng bi\u1ec3u di\u1ec5n ng\u00f4n ng\u1eef.","7cf78eca":"### c. Ph\u00e2n t\u00edch t\u1eeb","f46230df":"# 1. T\u00ecm hi\u1ec3u b\u00e0i to\u00e1n","c6c99034":"**Nh\u1eadn x\u00e9t**:\n\nV\u00ec s\u1ed1 l\u01b0\u1ee3ng c\u00e2u ch\u00e2n th\u00e0nh chi\u1ebfm \u0111a s\u1ed1, n\u00ean kh\u00f4ng ng\u1ea1c nhi\u00ean khi s\u1ed1 li\u1ec7u c\u1ee7a n\u00f3 kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng s\u1ed1 li\u1ec7u c\u1ee7a t\u1ed5ng b\u1ed9 d\u1eef li\u1ec7u.\n\nGi\u1eefa hai class d\u1eef li\u1ec7u, c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng **c\u00e2u kh\u00f4ng ch\u00e2n th\u00e0nh c\u00f3 xu h\u01b0\u1edbng d\u00e0i h\u01a1n c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh**.","e007bca4":"D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o cho th\u1ea5y 3 c\u1ed9t th\u00f4ng tin:\n* `qid`: ID c\u1ee7a c\u00e2u h\u1ecfi\n* `question_text`: ti\u00eau \u0111\u1ec1 c\u00e2u h\u1ecfi, d\u1eef li\u1ec7u ta c\u1ea7n ph\u00e2n lo\u1ea1i\n* `target`: k\u1ebft qu\u1ea3 cho th\u1ea5y c\u00e2u h\u1ecfi c\u00f3 thi\u1ebfu ch\u00e2n th\u00e0nh kh\u00f4ng (1 - thi\u1ebfu ch\u00e2n th\u00e0nh, 0 - ch\u00e2n th\u00e0nh)","1c1b931b":"Qu\u00e1 tr\u00ecnh train:\n\n* `load data`\n\n* `forward`: \u0110\u1ea9y input v\u00e0o model c\u0169ng nh\u01b0 t\u00ednh loss.\n\n* `zero_grad`: \u0110\u1ec3 grad tr\u1edf v\u1ec1 0 tr\u01b0\u1edbc khi ch\u1ea1y.\n\n* `backpropagation`: \u0110\u1ea1o h\u00e0m ng\u01b0\u1ee3c \u0111\u1ec3 ti\u1ebfn h\u00e0nh backpropagation.","1f0b655c":"Ph\u00e2n t\u00edch d\u1ef1a tr\u00ean t\u1ed5ng s\u1ed1 c\u00e2u h\u1ecfi \u1edf b\u1ed9 d\u1eef li\u1ec7u train:","e76acb2b":"## 3.2. L\u00e0m s\u1ea1ch d\u1eef li\u1ec7u","c9e9b784":"## 6.3. Ti\u1ebfn h\u00e0nh hu\u1ea5n luy\u1ec7n","c76d430c":"## 3.3. Load dataset","87acb553":"# 5. Kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n\n\n**V\u1ea5n \u0111\u1ec1**: X\u00e9t d\u1eef li\u1ec7u ta c\u1ea7n x\u1eed l\u00fd l\u00e0 d\u1ea1ng v\u0103n b\u1ea3n, hay n\u00f3i c\u00e1ch kh\u00e1c, \u0111\u00f3 l\u00e0 d\u1eef li\u1ec7u mang t\u00ednh li\u00ean t\u1ee5c, kh\u00e1c v\u1edbi d\u1eef li\u1ec7u r\u1eddi r\u1ea1c nh\u01b0 c\u00e1c t\u1ea5m \u1ea3nh.\n\n**Gi\u1ea3i ph\u00e1p**: \n\nTrong vi\u1ec7c l\u1ef1a ch\u1ecdn m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n th\u00edch h\u1ee3p, em \u0111\u00e3 ch\u1ecdn s\u1eed d\u1ee5ng RNN - Recurrent Neural Network.\n\nM\u1ea1ng n\u01a1 ron h\u1ed3i quy (RNN) l\u00e0 m\u1ed9t lo\u1ea1i M\u1ea1ng n\u01a1 ron trong \u0111\u00f3 \u0111\u1ea7u ra t\u1eeb c\u00e1c b\u01b0\u1edbc tr\u01b0\u1edbc \u0111\u01b0\u1ee3c cung c\u1ea5p l\u00e0m \u0111\u1ea7u v\u00e0o cho b\u01b0\u1edbc hi\u1ec7n t\u1ea1i, do \u0111\u00f3 ghi nh\u1edb m\u1ed9t s\u1ed1 th\u00f4ng tin v\u1ec1 tr\u00ecnh t\u1ef1. \nN\u00f3 c\u00f3 nh\u1eefng h\u1ea1n ch\u1ebf nh\u01b0 kh\u00f3 nh\u1edb c\u00e1c chu\u1ed7i d\u00e0i h\u01a1n.\n\nVi\u1ec7c s\u1eed d\u1ee5ng RNN gi\u00fap x\u1eed l\u00fd v\u1ea5n \u0111\u1ec1 v\u1ec1 d\u1eef li\u1ec7u li\u00ean t\u1ee5c, kh\u00f4ng gi\u1ed1ng v\u1edbi CNN d\u00e0nh cho d\u1ea1ng d\u1eef li\u1ec7u r\u1eddi r\u1ea1c.\n\n","43af60f5":"## 6.2. Kh\u1edfi t\u1ea1o model","8540b987":"## 6.1. C\u00e0i \u0111\u1eb7t thi\u1ebft l\u1eadp","10676714":"N\u1ed9p b\u00e0i:","706224b1":"**V\u1ea5n \u0111\u1ec1**: T\u1eadp d\u1eef li\u1ec7u c\u00e1c c\u00e2u thi\u1ebfu ch\u00e2n th\u00e0nh r\u1ea5t \u00edt so v\u1edbi b\u1ed9 d\u1eef li\u1ec7u.\n\n**Gi\u1ea3i ph\u00e1p**: L\u1ef1a ch\u1ecdn threshold b\u1eb1ng gi\u00e1 tr\u1ecb ph\u00f9 h\u1ee3p \u0111\u1ec3 kh\u00f4ng l\u00e0m sai l\u1ec7ch qu\u00e1 nhi\u1ec1u.\n\nTa xu\u1ea5t raw_prediction t\u1eeb t\u1eadp validation set \u0111\u1ec3 t\u1eeb m\u1ed7i gi\u00e1 tr\u1ecb threshold, sinh ra m\u1ed9t prediction. Tham chi\u1ebfu t\u1eeb \u0111\u00f3 \u0111\u1ec3 ra k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t.\n\n**K\u1ebft qu\u1ea3**: Em \u0111\u00e3 ch\u1ea1y threshold t\u1eeb 0.3 - 0.7 v\u00e0 th\u1ea5y 0.32 c\u00f3 th\u1ec3 l\u00e0 con s\u1ed1 t\u1ed1t nh\u1ea5t.\n\n","ed4d980a":"Load file embedding **GloVe** \u0111\u00e3 gi\u1ea3i n\u00e9n.","d1720022":"T\u1eeb d\u1eef li\u1ec7u c\u00e1c \u0111\u1eb7c tr\u01b0ng tr\u00ean, ta xu\u1ea5t ra **Correlation Matrix** \u0111\u1ec3 c\u00f3 th\u1ec3 r\u00fat ra th\u00eam \u00fd ngh\u0129a:","fcb6e7c8":"## 3.1. Thi\u1ebft k\u1ebf gi\u1ea3i ph\u00e1p l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u","41b2d79a":"## 5.1. Bidirectional LSTM Model\n","de4a26c8":"**V\u1ea5n \u0111\u1ec1**: C\u00f3 th\u1ec3 th\u1ea5y s\u1ed1 l\u01b0\u1ee3ng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh (target = 1) **th\u1ea5p h\u01a1n \u0111\u00e1ng k\u1ec3** so v\u1edbi c\u00e1c c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh.\n\n**Gi\u1ea3i ph\u00e1p**: \u0110\u00f3 l\u00e0 l\u00fd do t\u1ea1i sao c\u00e1c b\u00e0i d\u1ef1 thi \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 tr\u00ean **F1 Score** gi\u1eefa m\u1ee5c ti\u00eau d\u1ef1 \u0111o\u00e1n v\u00e0 m\u1ee5c ti\u00eau quan s\u00e1t. \nV\u00ec v\u1eady, t\u1eadp d\u1eef li\u1ec7u kh\u00f4ng c\u00e2n b\u1eb1ng s\u1ebd kh\u00f4ng \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn \u0111i\u1ec3m s\u1ed1. \n\n\n\n","52d5c352":"\u0110\u1ec3 b\u1eaft \u0111\u1ea7u, ta s\u1ebd t\u00ecm hi\u1ec3u v\u1ec1 **GloVe** - Global Vectors for Word Representation:\n\nDo s\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng (t\u1eeb trong t\u1eeb \u0111i\u1ec3n) l\u00e0 kh\u00e1 l\u1edbn (nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a one-hot vector, k\u1ef9 thu\u1eadt bi\u1ec3u di\u1ec5n t\u1eeb b\u1eb1ng vector c\u00f3 s\u1ed1 chi\u1ec1u b\u1eb1ng s\u1ed1 t\u1eeb v\u1ef1ng), n\u00ean Embedding \u0111\u01b0\u1ee3c t\u1ea1o ra \u0111\u1ec3 gi\u1ea3m s\u1ed1 chi\u1ec1u c\u1ee7a kh\u00f4ng gian \u0111\u1eb7c tr\u01b0ng. C\u1ee5 th\u1ec3 l\u00e0 m\u1ed7i t\u1eeb s\u1ebd \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng m\u1ed9t vector c\u00f3 s\u1ed1 chi\u1ec1u x\u00e1c \u0111\u1ecbnh.\n\n![alt text](https:\/\/miro.medium.com\/max\/2456\/1*gcC7b_v7OKWutYN1NAHyMQ.png \"Visualization c\u1ee7a Embedding Matrix\")\n\n![alt text](https:\/\/miro.medium.com\/max\/2892\/1*KdJGfpGf7eApnKuSZDv2ZQ.png)\n\n","23281430":"B\u1ed9 d\u1eef li\u1ec7u:\n* **\u0110\u1ed9 d\u00e0i trung b\u00ecnh**: 12.8\n* **S\u1ed1 l\u01b0\u1ee3ng t\u1eeb \u00edt nh\u1ea5t**: 1\n* **S\u1ed1 l\u01b0\u1ee3ng t\u1eeb cao nh\u1ea5t**: 134\n* **50% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 11\n* **80% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 17\n* **99.99% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 53\n\n\u0110\u1eb7c \u0111i\u1ec3m \u0111\u00e1ng ch\u00fa \u00fd c\u1ee7a c\u00e2u thi\u1ebfu ch\u00e2n th\u00e0nh:\n* **\u0110\u1ed9 d\u00e0i trung b\u00ecnh**: 17.28\n* **50% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 15\n* **80% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 25\n\n\u0110\u1eb7c \u0111i\u1ec3m \u0111\u00e1ng ch\u00fa \u00fd c\u1ee7a c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh:\n* **\u0110\u1ed9 d\u00e0i trung b\u00ecnh**: 12.51\n* **50% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 11\n* **80% s\u1ed1 c\u00e2u c\u00f3 \u00edt h\u01a1n**: 16","4a1fe872":"## 1.3. H\u01b0\u1edbng gi\u1ea3i quy\u1ebft\n\nS\u1eed d\u1ee5ng **GloVE** \u0111\u1ec3 nh\u00fang (embedding) ng\u00f4n ng\u1eef th\u00e0nh c\u00e1c vector, sau \u0111\u00f3 d\u00f9ng M\u1ea1ng n\u01a1-ron h\u1ed3i quy (RNN - Recurrent Neural Network) nh\u01b0 **LSTM** \u0111\u1ec3 x\u1eed l\u00fd.","0e7c86a8":"# 3. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u","7dc52a7c":"![LSTM model](https:\/\/i.ibb.co\/XLwSTTD\/download-1.png)\n\n* **LSTM**: L\u1ecdc b\u1ed9 nh\u1edb hi\u1ec7n t\u1ea1i, ch\u1eaft l\u1ecdc th\u00f4ng tin hi\u1ec7n t\u1ea1i \u0111\u1ec3 \u0111\u01b0a v\u00e0o b\u1ed9 nh\u1edb v\u00e0 k\u1ebft h\u1ee3p b\u1ed9 nh\u1edb v\u00e0 d\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i \u0111\u1ec3 \u0111\u01b0a ra \u0111\u1ea7u ra b\u1eb1ng 4 c\u1ed5ng.\n\n* **GRU** - Gated Recurrent Unit: Kh\u00e1c v\u1edbi LSTM, GRU ch\u1ec9 c\u00f3 2 c\u1ed5ng \u0111\u1ec3 ch\u1eaft l\u1ecdc th\u00f4ng tin, c\u1ed5ng reset v\u00e0 c\u1ed5ng update. N\u00f3 kh\u00f4ng c\u00f3 tr\u1ea1ng th\u00e1i cell m\u00e0 ch\u1ec9 c\u00f3 \u0111\u1ea7u ra v\u1eeba d\u00f9ng \u0111\u1ec3 \u0111\u01b0a ra quy\u1ebft \u0111\u1ecbnh v\u1eeba d\u00f9ng \u0111\u1ec3 th\u00f4ng tin cho c\u00e1c b\u01b0\u1edbc ti\u1ebfp theo.\n\n* **Dropout**: B\u1edfi v\u00ec l\u01b0\u1ee3ng d\u1eef li\u1ec7u \u0111\u01b0a v\u00e0o model b\u1ecb l\u1ec7ch r\u1ea5t nhi\u1ec1u, n\u00ean ta c\u1ea7n s\u1eed d\u1ee5ng l\u1edbp Dropout.\nB\u1ecf qua m\u1ed9t v\u00e0i unit trong su\u1ed1t qu\u00e1 tr\u00ecnh train trong m\u00f4 h\u00ecnh, nh\u1eefng unit b\u1ecb b\u1ecf qua \u0111\u01b0\u1ee3c l\u1ef1a ch\u1ecdn ng\u1eabu nhi\u00ean, \u0111\u1ec3 tr\u00e1ng overfit.\n\n* **Max Pooling**: M\u1ed9t layer n\u00e9n th\u00f4ng tin \u0111\u1ec3 \u0111\u01a1n gi\u1ea3n h\u00f3a th\u00f4ng tin \u0111\u1ea7u ra \u0111\u1ec3 gi\u1ea3m b\u1edbt s\u1ed1 l\u01b0\u1ee3ng neuron.","e7f8498a":"*Visualize training experiment*","16c22c2c":"Ti\u1ebfp theo ta s\u1ebd tr\u00edch xu\u1ea5t c\u00e1c lo\u1ea1i ch\u1eef v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t:\n* `capital_letters`: Ch\u1eef vi\u1ebft hoa \n* `special_char`: K\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t \n* `unique_words`: T\u1eeb \u0111\u1ed9c nh\u1ea5t, ch\u1ec9 xu\u1ea5t hi\u1ec7n duy nh\u1ea5t 1 l\u1ea7n\n* `numerics`: S\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 s\u1ed1\n* `char`: S\u1ed1 l\u01b0\u1ee3ng ch\u1eef","7e712b19":"## 1.2. D\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o v\u00e0 \u0111\u1ea7u ra\n* **Input**: Ti\u00eau \u0111\u1ec1 c\u00e2u h\u1ecfi. \u0110\u1ecbnh d\u1ea1ng v\u0103n b\u1ea3n (text).\n\n* **Output**: S\u1ed1 1\/0 t\u01b0\u01a1ng \u1ee9ng v\u1edbi kh\u00f4ng ch\u00e2n th\u00e0nh\/ch\u00e2n th\u00e0nh.","3f2b9753":"**F1 Score** \u0111\u01b0\u1ee3c t\u00ednh nh\u01b0 b\u00ean d\u01b0\u1edbi. Chi ti\u1ebft \u0111\u00e3 \u0111\u01b0\u1ee3c vi\u1ebft \u1edf ph\u1ea7n 1.4. \u0110\u00e1nh gi\u00e1.","ae20b7ab":"LSTM \/ GRU l\u00e0 phi\u00ean b\u1ea3n c\u1ea3i ti\u1ebfn c\u1ee7a RNN, chuy\u00ean ghi nh\u1edb th\u00f4ng tin trong m\u1ed9t th\u1eddi gian d\u00e0i b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng c\u01a1 ch\u1ebf ki\u1ec3m so\u00e1t m\u00e0 RNN kh\u00f4ng th\u1ef1c hi\u1ec7n \u0111\u01b0\u1ee3c. \n\n![](https:\/\/miro.medium.com\/max\/875\/0*Q0uzj4QO5OQADSSX.png)\n\nRNN \u0111\u01a1n h\u01b0\u1edbng ch\u1ec9 l\u01b0u gi\u1eef th\u00f4ng tin c\u1ee7a qu\u00e1 kh\u1ee9 v\u00ec c\u00e1c \u0111\u1ea7u v\u00e0o m\u00e0 n\u00f3 \u0111\u00e3 th\u1ea5y l\u00e0 t\u1eeb qu\u00e1 kh\u1ee9. S\u1eed d\u1ee5ng LSTM hai chi\u1ec1u s\u1ebd ch\u1ea1y c\u00e1c \u0111\u1ea7u v\u00e0o theo hai c\u00e1ch, m\u1ed9t t\u1eeb qu\u00e1 kh\u1ee9 \u0111\u1ebfn t\u01b0\u01a1ng lai v\u00e0 m\u1ed9t t\u1eeb t\u01b0\u01a1ng lai \u0111\u1ebfn qu\u00e1 kh\u1ee9 cho ph\u00e9p n\u00f3 l\u01b0u gi\u1eef th\u00f4ng tin theo ng\u1eef c\u1ea3nh t\u1eeb c\u1ea3 qu\u00e1 kh\u1ee9 v\u00e0 t\u01b0\u01a1ng lai v\u00e0o b\u1ea5t k\u1ef3 th\u1eddi \u0111i\u1ec3m n\u00e0o. ","1b9f9aa7":"# **K\u1ebeT QU\u1ea2**\n\nHi\u1ec7n t\u1ea1i k\u1ebft qu\u1ea3 cao nh\u1ea5t m\u00e0 em \u0111\u00e3 \u0111\u1ea1t \u0111\u01b0\u1ee3c l\u00e0 0.63 \u1edf private score v\u00e0 0.61 \u1edf public score.","395163e7":"## 1.4. \u0110\u00e1nh gi\u00e1\nCu\u1ed9c thi s\u1eed d\u1ee5ng F1 Score \u0111\u1ec3 \u0111\u00e1nh gi\u00e1. F1 Score \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng c\u00f3 th\u1ec3 l\u00e0 v\u00ec s\u1ef1 m\u1ea5t c\u00e2n b\u1eb1ng c\u1ee7a d\u1eef li\u1ec7u (xem th\u00eam \u1edf m\u1ee5c 2.3. Kh\u1ea3o s\u00e1t v\u00e0 ph\u00e2n t\u00edch), n\u00ean s\u1ebd c\u00f3 \u00fd ngh\u0129a h\u01a1n l\u00e0 s\u1eed d\u1ee5ng accuracy \u0111\u01a1n thu\u1ea7n.\n\n\nF1 score l\u00e0 trung b\u00ecnh \u0111i\u1ec1u ho\u00e0 (Harmonic Mean) c\u1ee7a Precision v\u00e0 Recall. \n\n![alt text](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/26\/Precisionrecall.svg\/525px-Precisionrecall.svg.png \"F1 Score\")","375d07ad":"Ti\u1ebfp theo ta s\u1ebd tr\u00edch xu\u1ea5t m\u1ed9t s\u1ed1 \u0111\u1eb7c tr\u01b0ng d\u1eef li\u1ec7u c\u01a1 b\u1ea3n:\n* `count`: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb \n* `mean`: Trung b\u00ecnh s\u1ed1 l\u01b0\u1ee3ng t\u1eeb \n* `std`: Sample standard deviation\n* `min`: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb nh\u1ecf nh\u1ea5t\n* `max`: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb l\u1edbn nh\u1ea5t\n* `50%\/80%\/99.99%`: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb theo th\u1ed1ng k\u00ea m\u00e0 50%\/80%\/99.99% c\u00e2u h\u1ecfi c\u00f3 s\u1ed1 l\u01b0\u1ee3ng th\u1ea5p h\u01a1n\n\nL\u1ea5y \u0111\u01b0\u1ee3c th\u00f4ng tin n\u00e0y tr\u1ef1c ti\u1ebfp t\u1eeb l\u1ec7nh `describe` c\u1ee7a pandas.","612ca6de":"`no_grad`: gi\u00fap file test kh\u00f4ng b\u1ecb m\u00f4 h\u00ecnh nh\u1edb\n\nSau \u0111\u00f3 m\u00f4 h\u00ecnh s\u1ebd ch\u1ea1y \u0111\u1ec3 t\u00ednh loss v\u00e0 accuracy.","268dce01":"# 6. Hu\u1ea5n luy\u1ec7n","a6075b3c":"## 2.1. Import th\u01b0 vi\u1ec7n\n\nTrong s\u1ed1 c\u00e1c th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng, c\u1ea7n k\u1ec3 \u0111\u1ebfn:\n* `nltk`: Natural Language Toolkit, m\u1ed9t b\u1ed9 th\u01b0 vi\u1ec7n v\u00e0 ch\u01b0\u01a1ng tr\u00ecnh d\u00e0nh cho x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean th\u1ed1ng k\u00ea v\u00e0 bi\u1ec3u t\u01b0\u1ee3ng ti\u1ebfng Anh.\n* `pandas`: Thao t\u00e1c v\u1edbi c\u00e1c d\u1eef li\u1ec7u d\u1ea1ng b\u1ea3ng (.csv).\n* `torch`: PyTorch, m\u1ed9t framework \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n ch\u1ee7 y\u1ebfu b\u1edfi ph\u00f2ng th\u00ed nghi\u1ec7m Nghi\u00ean c\u1ee9u AI c\u1ee7a Facebook.","f7079edd":"**Kh\u00f3 kh\u0103n v\u00e0 b\u00e0i h\u1ecdc r\u00fat ra**: C\u00f3 th\u1ec3 th\u1ea5y m\u00f4 h\u00ecnh \u0111ang b\u1ecb **underfit**. Nguy\u00ean nh\u00e2n c\u00f3 th\u1ec3 l\u00e0:\n\n* M\u00f4 h\u00ecnh ch\u01b0a \u0111\u1ee7 kh\u1ea3 n\u0103ng bi\u1ec3u di\u1ec5n\n\n* Qu\u00e1 tr\u00ecnh c\u00e0i \u0111\u1eb7t loss function c\u00f3 th\u1ec3 ch\u01b0a ch\u00ednh x\u00e1c","8cfa9235":"# 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","b761d2be":"C\u00f3 th\u1ec3 nh\u1eadn ra sau khi l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u c\u00f3 kh\u1ea3 n\u0103ng s\u1ebd \u0111\u1ec3 l\u1ea1i c\u00e1c c\u00e2u h\u1ecfi tr\u1ed1ng r\u1ed7ng, ho\u1eb7c v\u00f4 ngh\u0129a, g\u00e2y t\u00e1c \u0111\u1ed9ng ti\u00eau c\u1ef1c l\u00ean m\u00f4 h\u00ecnh.","184cb983":"Tham chi\u1ebfu t\u1eeb \u0111i\u1ec3n c\u1ee7a b\u1ed9 d\u1eef li\u1ec7u (training data) \u0111\u1ec3 t\u1ea1o ma tr\u1eadn nh\u00fang cho b\u1ed9 t\u1eeb \u0111i\u1ec3n.\n\nTokenization c\u0169ng \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n \u1edf \u0111\u00e2y.","0cb492af":"T\u00e1ch b\u1ea3ng th\u00e0nh 2 c\u1ed9t c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh v\u00e0 kh\u00f4ng ch\u00e2n th\u00e0nh \u0111\u1ec3 ph\u00e2n t\u00edch.","5fe2ac66":"Load iterator \u0111\u1ec3 tr\u1ecf v\u00e0o gi\u00e1 tr\u1ecb trong t\u1eadp d\u1eef li\u1ec7u.","4133a4a4":"## 3.4. Chia t\u1eadp d\u1eef li\u1ec7u Validation\n\nL\u1ea5y t\u1ec9 l\u1ec7 8-2 \u0111\u1ec3 chia b\u1ed9 d\u1eef li\u1ec7u ra 2 ph\u1ea7n `training_set` v\u00e0 `valid_set`.\n\n* **Training Set**: s\u1eed d\u1ee5ng 80% b\u1ed9 d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n luy\u1ec7n\n* **Validation Set**: s\u1eed d\u1ee5ng 20% b\u1ed9 d\u1eef li\u1ec7u \u0111\u1ec3 x\u00e1c th\u1ef1c","842c8874":"Em s\u1eed d\u1ee5ng **Adam** t\u1eeb th\u01b0 vi\u1ec7n torch l\u00e0m **optimizer** v\u1edbi m\u1ee5c \u0111\u00edch c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1.\n\n**Loss function** s\u1eed d\u1ee5ng **Binary Cross entropy**, \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1ec3 th\u00edch nghi v\u1edbi nh\u1eefng b\u00e0i to\u00e1n 2 class.\n\n\n\n","4e2a4ee7":"## 1.1. \u0110\u1eb7t v\u1ea5n \u0111\u1ec1\n**Quora** l\u00e0 m\u1ed9t n\u1ec1n t\u1ea3ng h\u1ecfi \u0111\u00e1p \u0111\u01b0\u1ee3c t\u1ea1o ra v\u1edbi m\u1ee5c \u0111\u00edch \u0111\u1ec3 m\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 h\u1ecdc h\u1ecfi v\u00e0 chia s\u1ebb ki\u1ebfn th\u1ee9c. M\u1ecdi ng\u01b0\u1eddi c\u00f3 th\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi tr\u00ean trang web n\u00e0y v\u00e0 k\u1ebft n\u1ed1i v\u1edbi nh\u1eefng ng\u01b0\u1eddi c\u00f3 th\u1ec3 c\u00f3 nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi ch\u1ea5t l\u01b0\u1ee3ng v\u00e0 \u0111\u00f3ng g\u00f3p nh\u1eefng th\u00f4ng tin chi ti\u1ebft, \u0111\u1ed9c \u0111\u00e1o.\n\nTuy v\u1eady, c\u0169ng v\u00ec Quora l\u00e0 m\u1ed9t n\u01a1i d\u00e0nh cho t\u1ea5t c\u1ea3 m\u1ecdi ng\u01b0\u1eddi m\u00e0 kh\u00f4ng th\u1ec3 tr\u00e1nh kh\u1ecfi m\u1ed9t th\u1ef1c tr\u1ea1ng \u0111\u00e1ng bu\u1ed3n: c\u00e1c **c\u00e2u h\u1ecfi mang t\u00ednh \u0111\u1ed9c h\u1ea1i** (toxic question), hay n\u00f3i c\u00e1ch kh\u00e1c, \u0111\u01b0\u1ee3c \u0111\u1eb7t ra m\u1ed9t c\u00e1ch **thi\u1ebfu ch\u00e2n th\u00e0nh** (insincere). Ch\u00fang \u0111\u01b0\u1ee3c \u0111\u1eb7t ra d\u1ef1a tr\u00ean nh\u1eefng \u0111\u1ecbnh ki\u1ebfn, ti\u1ec1n \u0111\u1ec1 sai l\u1ea7m ho\u1eb7c c\u00f3 \u00fd \u0111\u1ecbnh \u0111\u01b0a ra m\u1ed9t tuy\u00ean b\u1ed1 h\u01a1n l\u00e0 t\u00ecm ki\u1ebfm nh\u1eefng c\u00e2u tr\u1ea3 l\u1eddi h\u1eefu \u00edch. \u0110i\u1ec1u n\u00e0y \u0111i ng\u01b0\u1ee3c l\u1ea1i v\u1edbi t\u00f4n ch\u1ec9 c\u1ee7a Quora **\"Be Nice, Be Respectful\"** (d\u1ecbch th\u00f4: h\u00e3y l\u00e0 m\u1ed9t ng\u01b0\u1eddi t\u1ed1t v\u00e0 bi\u1ebft t\u00f4n tr\u1ecdng ng\u01b0\u1eddi kh\u00e1c).\n\nT\u1eeb \u0111\u00f3 m\u00e0 cu\u1ed9c thi **Quora Insincere Questions Classification** - Ph\u00e2n lo\u1ea1i nh\u1eefng c\u00e2u h\u1ecfi Quora thi\u1ebfu ch\u00e2n th\u00e0nh, \u0111\u01b0\u1ee3c ra \u0111\u1eddi.\n\nNhi\u1ec7m v\u1ee5 \u0111\u01b0\u1ee3c \u0111\u1eb7t ra l\u00e0 s\u1eed d\u1ee5ng t\u1eadp d\u1eef li\u1ec7u m\u00e0 Quora cung c\u1ea5p \u0111\u1ec3 ph\u00e2n lo\u1ea1i \u0111\u00e2u l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi mang h\u00e0m \u00fd kh\u00f4ng ch\u00e2n th\u00e0nh.","41cd7a74":"\u0110\u1ec3 load file embeddings sau n\u00e0y, ta th\u1ef1c hi\u1ec7n k\u00e9o v\u1ec1 gi\u1ea3i n\u00e9n.","834eb48e":"## 7.2. Tr\u1ea3 k\u1ebft qu\u1ea3","91323d9b":"## 5.2. C\u1ea3i ti\u1ebfn","74bd5907":"# 4. Tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng","2e042779":"# B\u00c1O C\u00c1O B\u00c0I T\u1eacP L\u1edaN H\u1eccC M\u00c1Y\n**Quora Insincere Question Classification**\n\n**M\u00e3 l\u1edbp**: INT3405 20\n\n**H\u1ecd t\u00ean**: Phan Quang H\u00f9ng\n\n**MSSV**: 18020582\n\n\n---","c01641fc":"**K\u1ebft qu\u1ea3**: C\u00f3 th\u1ec3 th\u1ea5y c\u00e1c c\u00e2u h\u1ecfi \u0111\u00e3 \u0111\u01b0\u1ee3c chu\u1ea9n ho\u00e1 l\u00e0m s\u1ea1ch, s\u1eb5n s\u00e0ng \u0111\u1ec3 tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng, vector ho\u00e1 b\u1ed9 d\u1eef li\u1ec7u.","98e7e6b9":"### b. Ph\u00e2n b\u1ed1 d\u1eef li\u1ec7u","74b504d8":"**Learning Rate** trong qu\u00e1 tr\u00ecnh tuning \u0111\u00e3 cho th\u1ea5y s\u1ebd \u0111\u1ea1t k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n n\u1ebfu > 0.0003 v\u00e0 < 0.001. V\u00ec v\u1eady em l\u1ef1a ch\u1ecdn lr=0.0004 cho version n\u00e0y.","48ae34c4":"## 7.1. Helper Function","36484903":"**Gi\u1ea3i ph\u00e1p**: \n\n* T\u0103ng s\u1ed1 l\u01b0\u1ee3ng parameter\n\n* T\u0103ng c\u01b0\u1eddng data (Data Augmentation)","a8c99601":"## 2.2. Nh\u1eadp d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o","1749e548":"### a. Tr\u01b0\u1eddng d\u1eef li\u1ec7u","a1963002":"Xo\u00e1 b\u1ecf c\u00e1c file kh\u00f4ng c\u1ea7n thi\u1ebft.","27b97e92":"**V\u1ea5n \u0111\u1ec1**: Nh\u01b0 \u0111\u00e3 n\u00f3i \u1edf ph\u1ea7n 2, c\u00f3 r\u1ea5t nhi\u1ec1u ch\u1eef, k\u00fd t\u1ef1, t\u1eeb \u0111\u1eb7c bi\u1ec7t m\u00e0 g\u00e2y lo\u00e3ng d\u1eef li\u1ec7u.\n\n**Gi\u1ea3i ph\u00e1p**:\n\n* Stemming: Chuy\u1ec3n \u0111\u1ed5i c\u00e1c t\u1eeb vi\u1ebft r\u00fat g\u1ecdn v\u1ec1 nguy\u00ean th\u1ec3 b\u1eb1ng c\u00e1ch xo\u00e1 contraction.\n* Punctuation: Xo\u00e1 c\u00e1c d\u1ea5u c\u00e2u\n* Tokenization: Ph\u00e1 c\u00e1c c\u00e2u th\u00e0nh c\u00e1c t\u1eeb \u0111\u01a1n l\u1ebb (tokenize)\n* Misspell: V\u00ec Quora l\u00e0 trang web c\u1ed9ng \u0111\u1ed3ng, c\u00e1c c\u00e2u h\u1ecfi kh\u00f4ng \u0111\u01b0\u1ee3c chu\u1ea9n ho\u00e1 ho\u00e0n to\u00e0n, n\u00ean vi\u1ec7c sai ch\u00ednh t\u1ea3 l\u00e0 r\u1ea5t c\u00f3 th\u1ec3 x\u1ea3y ra. B\u00ean c\u1ea1nh \u0111\u00f3 l\u00e0 c\u00e1c c\u00e1ch vi\u1ebft kh\u00e1c c\u1ee7a t\u1eeb do s\u1ef1 kh\u00e1c bi\u1ec7t v\u0103n ho\u00e1, v\u00f9ng mi\u1ec1n,... (*color\/colour*)\n\n* Lemmatization: Kh\u00f4ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng v\u00ec s\u1ebd l\u00e0m sai t\u1eeb g\u1ed1c ban \u0111\u1ea7u trong m\u1ed9t s\u1ed1 tr\u01b0\u1eddng h\u1ee3p. (*caring\/car\/care*)\n* Stopword: Trong h\u1ecdc m\u00e1y th\u1ed1ng k\u00ea, vi\u1ec7c lo\u1ea1i b\u1ecf c\u00e1c stopword nh\u01b0 *the* l\u00e0 c\u1ea7n thi\u1ebft. Tuy nhi\u00ean trong tr\u01b0\u1eddng h\u1ee3p s\u1eed d\u1ee5ng h\u01b0\u1edbng \u0111i c\u1ee7a em th\u00ec c\u1ea7n ph\u1ea3i gi\u1eef l\u1ea1i nguy\u00ean v\u1eb9n t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb \u0111\u1ec3 c\u00f3 th\u1ec3 vector ho\u00e1 t\u1ea5t c\u1ea3, \u0111\u1ec3 t\u1ea1o ra embedding matrix \u0111\u00fang nh\u1ea5t.","cd4b998b":"# 7. D\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3","6970fe56":"### d. Ph\u00e2n t\u00edch ch\u1eef v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t","62ab9cf7":"## 2.3. Kh\u1ea3o s\u00e1t v\u00e0 ph\u00e2n t\u00edch","4a222392":"**Nh\u1eadn x\u00e9t**: LSTM l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh t\u1ed1t v\u00e0 \u0111\u00e3 ch\u1ee9ng t\u1ecf \u0111\u01b0\u1ee3c hi\u1ec7u qu\u1ea3 c\u1ee7a n\u00f3. \u1ede ph\u1ea7n ti\u1ebfp theo, em s\u1ebd th\u1eed nghi\u1ec7m m\u1ed9t m\u00f4 h\u00ecnh ph\u1ee9c t\u1ea1p h\u01a1n khi ch\u1ed3ng 2 RNN v\u1edbi nhau, trong \u0111\u00f3 c\u00f3 m\u1ea1ng n\u01a1 ron LSTM hai chi\u1ec1u v\u1eeba \u0111\u1ec1 c\u1eadp.","918df0f1":"**Nh\u1eadn x\u00e9t**: \n\nC\u00f9ng v\u1edbi k\u1ebft lu\u1eadn t\u1eeb m\u1ee5c c, ta c\u0169ng th\u1ea5y r\u1eb1ng c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh xu\u1ea5t hi\u1ec7n nhi\u1ec1u ch\u1eef c\u00e1i v\u00e0 k\u00fd t\u1ef1 h\u01a1n.\n\nC\u00e1c c\u00e2u h\u1ecfi thi\u1ebfu ch\u00e2n th\u00e0nh c\u0169ng c\u00f3 v\u1ebb c\u00f3 nhi\u1ec1u t\u1eeb \u0111\u1ed9c nh\u1ea5t (unique word) h\u01a1n.\n\nC\u00f3 r\u1ea5t nhi\u1ec1u k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t, c\u0169ng nh\u01b0 c\u00e1c k\u00fd t\u1ef1 s\u1ed1 v\u00e0 ch\u1eef vi\u1ebft hoa m\u00e0 c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng \u0111\u00e3 l\u00e0m lo\u00e3ng d\u1eef li\u1ec7u, kh\u00f4ng gi\u00fap \u00edch trong vi\u1ec7c ph\u00e2n lo\u1ea1i.\n\n**Gi\u1ea3i ph\u00e1p**: Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u b\u1eb1ng c\u00e1c ph\u01b0\u01a1ng ph\u00e1p l\u00e0m s\u1ea1ch c\u01a1 b\u1ea3n.\n"}}