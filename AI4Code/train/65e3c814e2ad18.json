{"cell_type":{"c16126af":"code","8e578e49":"code","c1d97991":"code","ebfe7348":"code","5c9015c2":"code","ba689e1d":"code","e9d083a0":"code","d3937548":"code","d4fb18e3":"code","2000e091":"code","5c5ddecb":"code","b8c29459":"code","96de69ab":"code","e6c1a20d":"code","81f6d723":"code","a262405a":"code","b37370ec":"code","d31c168f":"code","296f80bd":"code","699b802e":"code","99a4b977":"code","4c1c8078":"code","4556dbfc":"code","cd4ed2b2":"code","a414eb0a":"code","8375ec61":"code","8eb3865e":"code","1b040e40":"code","b96a8898":"code","ba0da765":"code","deb39429":"code","03d9b95e":"code","7b4e9b16":"code","5b434e50":"code","c55db982":"code","5ccc5956":"code","bddfb8f3":"code","446cca00":"code","91481f19":"code","2b04fbad":"code","833c34cc":"code","feb55ee3":"code","10f9320d":"code","8e81e671":"code","2cda3bd9":"code","5b558c3d":"code","fd6865a4":"code","94695e1f":"code","d68c09cb":"code","88703687":"code","87e6df37":"code","8224e68e":"code","ce1cdb23":"markdown","243bc608":"markdown","a886da1b":"markdown","a3abbec3":"markdown","a1bfbd26":"markdown","2cc1cc4b":"markdown","9954eb9f":"markdown","b2854b5b":"markdown","302ab5e8":"markdown","1979ff8c":"markdown","76bc22d2":"markdown","5912ea74":"markdown","d857eb22":"markdown","0bd20c2e":"markdown","691bea64":"markdown"},"source":{"c16126af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e578e49":"# data analysis and wrangling\n# import pandas as pd\n# import numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nimport re\nimport sklearn\nimport xgboost as xgb\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\n\nfrom sklearn.model_selection import KFold","c1d97991":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ncombine = [train_data,test_data]","ebfe7348":"print(train_data.isnull().sum())\nprint('-'* 20)\nprint(test_data.isnull().sum())","5c9015c2":"print(train_data.info())\nprint('-'* 40)\nprint(test_data.info())","ba689e1d":"print(train_data.describe()) # describe the features of numerical attributes\nprint('-'* 40)\nprint(test_data.describe())","e9d083a0":"print(train_data.describe(include = ['O'])) # describe the features of non-numerical attributes\nprint('-'* 40)\nprint(test_data.describe(include = ['O']))","d3937548":"train_data.Embarked.unique()","d4fb18e3":"for dataset in combine:\n    dataset.Sex = dataset.Sex.map({'female': 0,'male': 1})\n    dataset.Embarked = dataset.Embarked.map({'S': 0,'C': 1,'Q': 2})","2000e091":"train_data.Sex.unique","5c5ddecb":"train_data.describe()","b8c29459":"drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin']\ntrain = train_data.drop(drop_elements, axis = 1)\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","96de69ab":"#sex\ntrain_data[['Sex','Survived']].groupby(['Sex']).mean().plot.bar()","e6c1a20d":"#age\ng = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","81f6d723":"train_data[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar()","a262405a":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","b37370ec":"train_data['HaveCabin'] = train_data['Cabin'].isnull().map({True: 0, False: 1})\ntest_data['HaveCabin'] = test_data['Cabin'].isnull().map({True: 0, False: 1})","d31c168f":"train_data.HaveCabin.describe()","296f80bd":"train_data[['HaveCabin','Survived']].groupby(['HaveCabin']).mean().plot.bar()","699b802e":"train_data.corr()","99a4b977":"train_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4c1c8078":"train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4556dbfc":"train_data.Name.head()","cd4ed2b2":"#We combine these datasets to run certain operations on both datasets together, for example, using the same algorithm to fill the missing values.\ncombine = [train_data, test_data]\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex']) #\u53ef\u4ee5\u7528\u8fd9\u4e2a\u9884\u6d4b\u5e74\u9f84","a414eb0a":"dataset['Title'].unique()","8375ec61":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","8eb3865e":"print(combine[0].Title.describe())\nprint('-'* 40)\nprint(combine[1].Title.describe())","1b040e40":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n\ntrain_data.head()","b96a8898":"dataset.groupby(['Title']).size()","ba0da765":"dataset.groupby(['Title']).Age.mean()","deb39429":"# Wrangle Data\ntrain_data.drop(['Cabin','Name','PassengerId','Ticket'], axis=1,inplace=True)\ntest_data.drop(['Name','Cabin','Ticket'], axis=1,inplace=True)\ncombine = [train_data, test_data]","03d9b95e":"train_data.describe()","7b4e9b16":"train_data.Embarked.fillna(0,inplace = True)","5b434e50":"test_data.Fare.fillna(test_data.Fare.median(),inplace = True)","c55db982":"train_data.info()","5ccc5956":"test_data.info()","bddfb8f3":"'''\nguess_ages = np.zeros((2,3))\nguess_ages\n\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()\n'''","446cca00":"guess_ages = np.zeros((5,3))\nfor dataset in combine:\n    for i in range(0, 5):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Title'] == i+1) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n            \n            # if there is no such guest of the target combination, then we use the 'Title' itself to make a prediction\n            if pd.isnull(age_guess):\n                age_guess = dataset[dataset['Title'] == i+1].Age.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 5):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Title == i+1) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","91481f19":"train_data.head()","2b04fbad":"train_data['AgeBand'] = pd.cut(train_data['Age'], 8)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","833c34cc":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 10, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 60) & (dataset['Age'] <= 70), 'Age'] = 6\n    dataset.loc[(dataset['Age'] > 70) & (dataset['Age'] <= 80), 'Age'] = 7\ntrain_data.head()","feb55ee3":"train_data.drop(['AgeBand'],axis = 1,inplace = True)\ncombine = [train_data, test_data]\ntrain_data.head()","10f9320d":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8e81e671":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","2cda3bd9":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()","5b558c3d":"train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1,inplace = True)\ntest_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1, inplace = True)\ncombine = [train_data, test_data]","fd6865a4":"train_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","94695e1f":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareBand'], axis=1)\ncombine = [train_data, test_data]","d68c09cb":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()\nX_train.info()","88703687":"predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"HaveCabin\",\"Title\",\"IsAlone\"]\n# \u5b58\u653e\u4e0d\u540c\u53c2\u6570\u53d6\u503c\uff0c\u4ee5\u53ca\u5bf9\u5e94\u7684\u7cbe\u5ea6\uff0c\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u662f\u4e00\u4e2a\u4e09\u5143\u7ec4(a, b, c)\nresults = []\n# \u6700\u5c0f\u53f6\u5b50\u7ed3\u70b9\u7684\u53c2\u6570\u53d6\u503c\nsample_leaf_options = list(range(1, 100, 3))\n# \u51b3\u7b56\u6811\u4e2a\u6570\u53c2\u6570\u53d6\u503c\nn_estimators_options = list(range(100,300, 5))\ngroud_truth = train_data['Survived'][601:]\n \n#\u627e\u6700\u597d\u7684\u90a3\u68f5\u6811\nfor leaf_size in sample_leaf_options:\n    for n_estimators_size in n_estimators_options:\n        alg = RandomForestClassifier(min_samples_leaf=leaf_size, n_estimators=n_estimators_size, random_state=50)\n        alg.fit(train_data[predictors][:600], train_data['Survived'][:600])\n        predict = alg.predict(train_data[predictors][601:])\n        # \u7528\u4e00\u4e2a\u4e09\u5143\u7ec4\uff0c\u5206\u522b\u8bb0\u5f55\u5f53\u524d\u7684 min_samples_leaf\uff0cn_estimators\uff0c \u548c\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u5ea6\n        results.append((leaf_size, n_estimators_size, (groud_truth == predict).mean()))\n        # \u771f\u5b9e\u7ed3\u679c\u548c\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u8ba1\u7b97\u51c6\u786e\u7387\n        print((groud_truth == predict).mean())\n \n# \u6253\u5370\u7cbe\u5ea6\u6700\u5927\u7684\u90a3\u4e00\u4e2a\u4e09\u5143\u7ec4\nprint(max(results, key=lambda x: x[2]))","87e6df37":"model = RandomForestClassifier(n_estimators=160, max_depth=5, min_samples_leaf=4, random_state=1)\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","8224e68e":"# from sklearn.ensemble import RandomForestClassifier\n\n# y = train_data[\"Survived\"]\n\n# X = pd.get_dummies(X_train)\n# X_test = pd.get_dummies(X_test)\n\n# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n# model.fit(X, y)\n# predictions = model.predict(X_test)\n\n# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","ce1cdb23":"We need to analyze the data type for further use:\n* categorical? Categorical: Survived, Sex(convert!), and Embarked(convert!). Ordinal: Pclass.\n* numerical? Continous: Age(belong to continuous!), Fare. Discrete: SibSp, Parch.\n* mixed data type? Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n\nblank:\n* Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n* Cabin > Age are incomplete in case of test dataset.","243bc608":"The distribution of age in test_data is similiar to the distribution in train_data, so we can use the same model to predict the missing values instead of just drop the null values.","a886da1b":"# which model to choose?\n\nA classification and regression problem.\n\nFrom Version one, we know that RF can lead to a best score, so we focus our attention on the parameter choosing.","a3abbec3":"There is no obvious relationship between 'SibSp' 'Parch' and 'Survived', so we can compress these two values into one 'IsAlone'. Because the dataset is too small, too many features choosen can lead to an overfitting.","a1bfbd26":"### Obeservations\n* infants (age < 4) has a high survival rate\n* Oldest passengers (Age = 80) survived.\n* Large number of 15-25 year olds did not survive.\n* Most passengers are in 15-35 age range.\n\n### Descisions\n* We should consider Age (our assumption classifying #2) in our model training.\n* Complete the Age feature for null values (completing #1).\n* We should band age groups (creating #3).","2cc1cc4b":"thanks to:\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/blog.csdn.net\/Koala_Tree\/article\/details\/78725881\n* https:\/\/analyticsindiamag.com\/5-ways-handle-missing-values-machine-learning-datasets\/\n* https:\/\/medium.com\/@praveen.orvakanti\/this-will-help-you-score-95-percentile-in-the-kaggle-titanic-ml-competition-aa2b3fd1b79b\n* https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic\n* https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n* https:\/\/mlwave.com\/kaggle-ensembling-guide\/","9954eb9f":"# my understandings\n\nAfter my 1st try to solve the problems of titanic, I have got the basic knowledge of how to get started for a data mining task. For a data mining process, I think there are mainly 4 processes:\n* Data Preprocessing: Most ML algorithms don't work well on a dataset with missing values so we have to do something with those missing values. \n* Feature Choosing: Some features couldn't be used directly, in other words, they do no good to a better result so we need either combine them into a new feature or just drop them away. This is usually done with matplotlib.\n* Model Training and Finding the Suitest Algorithm: We need to try different models to find which one suit our dataset best. And even just for one model, the values choosed will influence much to our final result.\n* Model Ensembing: In most times,a successful data mining work is a combination of multiple models.","b2854b5b":"# So...how to improve my score?\n\n* Learn more about the data\n* Experiment!\n     * Design\/create some new features \n     * Try different prepossessing \n     * Try different type of ML models\n     * Combine multiple models (ensemble)\n* Learn from other's code and ideas","302ab5e8":"# How to predict the values of age becomes one of the main problems\n\nThe combination of Title and Pclass is better than Sex and Pclass.","1979ff8c":"# Make some assumptions on data analysis\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n**Correlating.**\n\n* We want to know how well does *each feature correlate with Survival*. We want to do this early in our project and match these quick correlations with modelled correlations later in the project. (1)\n\n**Completing.**\n\n* We may want to *complete Age feature* as it is definitely correlated to survival. (1)\n* We may want to *complete the Embarked feature* as it may also correlate with survival or another important feature. (2)\n\n**Correcting.**\n\n* Ticket feature may be dropped from our analysis as it *contains high ratio of duplicates* (22%) and there may not be a correlation between Ticket and survival. (1)\n* Cabin feature may be dropped as it is *highly incomplete* or contains many null values both in training and test dataset. (2)\n* PassengerId may be dropped from training dataset as it does not contribute to survival. (3)\n* Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped. (4)\n\n**Creating.**\n\n* We may want to create a new feature called *Family based on Parch and SibSp* to get total count of family members on board. (1)\n* We may want to engineer the Name feature to extract Title as a new feature. (2)\n* We may want to create new feature for Age bands. This turns *a continous numerical feature into an ordinal categorical feature*. (3)\n* We may also want to *create a Fare range* feature if it helps our analysis. (4)\n\n**Classifying.**\n\n* We may also add to our assumptions based on the problem description noted earlier. (1)\n* Women (Sex=female) were more likely to have survived. (2)\n* Children (Age<?) were more likely to have survived. (3)\n* The upper-class passengers (Pclass=1) were more likely to have survived. (4)","76bc22d2":"It looks like only **Pclass\/Sex(\/Fare? Embarked?)** is high related to survival\n\nParch\/SibSp: These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).","5912ea74":"Obviously, 'Title' is a good attribute that can be used for predicting 'Age'.","d857eb22":"I choose to set the 'Age' to 8 sections to reach a more accurate result (but not overfitting)","0bd20c2e":"To draw the picture of correlation, we must do some type converting.","691bea64":"### Obeservations\n* Pclass=3 had most passengers, however most did not survive. Confirms our classifying assumption #2.\n* Infant passengers in Pclass=2 and Pclass=3 mostly survived. Further qualifies our classifying assumption #2.\n* Most passengers in Pclass=1 survived. Confirms our classifying assumption #3.\n* Pclass varies in terms of Age distribution of passengers.\n### Decisions\n* Consider Pclass for model training."}}