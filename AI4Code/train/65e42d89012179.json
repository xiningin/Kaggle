{"cell_type":{"7d0a8c93":"code","574e48f8":"code","e6269cf3":"code","2747f0fa":"code","3d6e3c20":"code","2b2175bf":"code","9bf26fd0":"code","7f3ef191":"code","4e75c3dc":"code","65305824":"code","b13adc0c":"code","6f33e612":"code","c5a624fc":"code","18d380db":"code","36952624":"code","8046e476":"code","2a93efe8":"code","898173f6":"code","ead52521":"code","76c10bf4":"code","1ae367a2":"code","f04f7f6e":"code","f25f9f4b":"code","6ef3e5d9":"markdown","aa9e6b31":"markdown","304eefc5":"markdown","634f9ba7":"markdown","9a326d1a":"markdown"},"source":{"7d0a8c93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","574e48f8":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt","e6269cf3":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","2747f0fa":"data.info()","3d6e3c20":"data.sample(3)","2b2175bf":"X, y = data.drop(['Survived'], axis = 1), data['Survived']","9bf26fd0":"num_cols = [x for x in X.columns if data[x].dtype in ['int64', 'float64']]\ncat_cols = [x for x in X.columns if data[x].dtype == 'object']","7f3ef191":"from sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","4e75c3dc":"num_transform = Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='constant')),\n                ('scale', MaxAbsScaler())\n])\n\ncat_transform = Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='most_frequent')),\n                ('onehot', OneHotEncoder(handle_unknown='ignore')),\n                ('scale', MaxAbsScaler())\n])","65305824":"preprocess = ColumnTransformer(transformers=[\n                               ('cat', cat_transform, cat_cols),\n                               ('num', num_transform, num_cols)\n])","b13adc0c":"X = preprocess.fit_transform(X)","6f33e612":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","c5a624fc":"X_train.shape, y_train.shape","18d380db":"X_train = X_train.toarray()\nX_test = X_test.toarray()\ny_train = y_train.values\ny_test = y_test.values","36952624":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\n\nclass LinearRegression(nn.Module):\n    def __init__(self,input_size,output_size):\n        super(LinearRegression,self).__init__()\n        self.f1 = nn.Linear(input_dim, 2000)\n        self.f2 = nn.Linear(2000, output_dim)\n\n\n    def forward(self,x):\n        x = self.f1(x)\n        x = F.leaky_relu(x)\n        x = F.dropout(x, p = 0.3)\n        x = self.f2(x)\n        return  F.sigmoid(x)","8046e476":"batch_size = 100\nbatch_no = len(X_train) \/\/ batch_size","2a93efe8":"X_train.shape","898173f6":"def generate_batches(X, y, batch_size):\n    assert len(X) == len(y)\n    np.random.seed(42)\n    X = np.array(X)\n    y = np.array(y)\n    perm = np.random.permutation(len(X))\n\n    for i in range(len(X)\/\/batch_size):\n        if i + batch_size >= len(X):\n            continue\n        ind = perm[i*batch_size : (i+1)*batch_size]\n        yield (X[ind], y[ind])","ead52521":"input_dim = 1730\noutput_dim = 2\nlearning_rate = 1\nmodel = LinearRegression(input_dim,output_dim)\nerror = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.5)\n\nloss_list = []\nacc_list = []\niteration_number = 300\n\nfor iteration in range(iteration_number):\n    batch_loss = 0\n    batch_accur = 0\n    temp = 0\n\n    for (x, y) in generate_batches(X_train, y_train, batch_size):\n        inputs = Variable(torch.from_numpy(x)).float()\n        labels = Variable(torch.from_numpy(y))\n            \n        optimizer.zero_grad() \n\n        results = model(inputs)\n        \n        loss = error(results, labels)\n\n        batch_loss += loss.data\n        \n        loss.backward()\n        \n        optimizer.step()\n\n        with torch.no_grad():\n            _, pred = torch.max(results, 1)\n            batch_accur += torch.sum(pred == labels)\n            temp += len(pred)\n    \n    loss_list.append(batch_loss\/batch_no)\n    acc_list.append(batch_accur\/temp)\n    \n    if(iteration % 50 == 0):\n        print('epoch {}: loss {}, accuracy {}'.format(iteration, batch_loss\/batch_no, batch_accur\/temp))\n\nplt.plot(range(iteration_number),loss_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Loss\")\nplt.show()\nplt.plot(range(iteration_number),acc_list)\nplt.xlabel(\"Number of Iterations\")\nplt.ylabel(\"Accuracy\")\nplt.show()","76c10bf4":"X_test_var = Variable(torch.FloatTensor(X_test), requires_grad=True) \nwith torch.no_grad():\n    test_result = model(X_test_var)\nvalues, labels = torch.max(test_result, 1)\nsurvived = labels.data.numpy()\nprint((survived == y_test).sum()\/len(survived))","1ae367a2":"X_test_origin = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nX_test_origin = preprocess.transform(X_test_origin)\nX_test_origin = X_test_origin.toarray()\nX_test_var = Variable(torch.FloatTensor(X_test_origin), requires_grad=True) \nwith torch.no_grad():\n    test_result = model(X_test_var)\nvalues, labels = torch.max(test_result, 1)\nsurvived = labels.data.numpy()\nX_test_1 = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","f04f7f6e":"import csv\n\nsubmission = [['PassengerId', 'Survived']]\nfor i in range(len(survived)):\n    submission.append([X_test_1.PassengerId.loc[i], survived[i]])","f25f9f4b":"with open('submission.csv', 'w') as submissionFile:\n    writer = csv.writer(submissionFile)\n    writer.writerows(submission)\n    \nprint('Writing Complete!')","6ef3e5d9":"**\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f**","aa9e6b31":"**\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438**","304eefc5":"**\u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445**","634f9ba7":"**\u041a\u043e\u0434 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f**","9a326d1a":"***Titanic prediction with LogisticRegression with leaky_relu and dropout on Pytorch***"}}