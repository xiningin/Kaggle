{"cell_type":{"661f7f93":"code","36e33998":"code","f011a00e":"code","199c6dff":"code","a0c6b779":"code","cdb8b9b1":"code","03cab870":"code","b81c68be":"code","14f0861d":"code","e84b010d":"code","f9ffbe40":"code","36a9e804":"code","823e28c3":"code","e88ae2f6":"code","0b503d6e":"markdown","14792afe":"markdown","a7982e0c":"markdown","1b72877c":"markdown","20f4c065":"markdown","2267c685":"markdown","14d90eff":"markdown","056cfbcd":"markdown","a3f40fc2":"markdown","d20f6186":"markdown","6c911d50":"markdown","279f6d31":"markdown","2473c66e":"markdown"},"source":{"661f7f93":"from part1_cleaning import get_clean_data\nfrom part4_vaderdata import get_vader_data\ndf1, df2, df3 = get_clean_data()\nvader_df1, vader_df2, vader_df3 = get_vader_data(df1, df2, df3)","36e33998":"from sklearn.cluster import KMeans\nX = vader_df1.iloc[:, -2:].values\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","f011a00e":"import matplotlib.pyplot as plt\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 20, c = 'green', label = 'C3')","199c6dff":"from sklearn.cluster import AgglomerativeClustering\nhc3 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny_hc3 = hc3.fit_predict(X)","a0c6b779":"plt.scatter(X[y_hc3 == 0, 0], X[y_hc3 == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_hc3 == 1, 0], X[y_hc3 == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_hc3 == 2, 0], X[y_hc3 == 2, 1], s = 20, c = 'green', label = 'C3')","cdb8b9b1":"c_sentiments = [2 if y == 0 else 1 if y == 1 else 0 for y in y_hc3]\nc_sentiments[0:10]","03cab870":"X = vader_df2.iloc[:, -2:].values\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 1)\ny_kmeans = kmeans.fit_predict(X)","b81c68be":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 20, c = 'green', label = 'C3')","14f0861d":"hc3 = AgglomerativeClustering(n_clusters = 3, affinity = 'euclidean', linkage = 'ward')\ny_hc3 = hc3.fit_predict(X)","e84b010d":"plt.scatter(X[y_hc3 == 0, 0], X[y_hc3 == 0, 1], s = 20, c = 'red', label = 'C1')\nplt.scatter(X[y_hc3 == 1, 0], X[y_hc3 == 1, 1], s = 20, c = 'blue', label = 'C2')\nplt.scatter(X[y_hc3 == 2, 0], X[y_hc3 == 2, 1], s = 20, c = 'green', label = 'C3')","f9ffbe40":"# reassign the values, which 2, 1, 0 being a positive sentiment, neutral sentiment, and negative sentiment, respectively\nr_sentiments = [2 if y == 1 else 1 if y == 2 else 0 for y in y_kmeans]\nr_sentiments[0:10]","36a9e804":"!pip install jenkspy\nimport jenkspy\nX = vader_df3.iloc[:, -1].values\nbreaks = jenkspy.jenks_breaks(X, nb_class=3)\nplt.hist(X, bins = 50)\nfor b in breaks:\n    plt.vlines(b, ymin=0, ymax=11000)","823e28c3":"breaks","e88ae2f6":"g_sentiments = [0 if x <= breaks[1] else 2 if x >= breaks[2] else 1 for x in X]\ng_sentiments[0:10]","0b503d6e":"From the scatter plot, I can conclude that data points colored in red (y=0) represent positive sentiment, data points colored in blue (y=1) represent negative sentiment, and data points colored in green (y=2) represent a mixed sentiment, in which I will assign as neutral.","14792afe":"# Compound Polarity Analysis with vaderSentiment - Financial News Sentiment - Clustering","a7982e0c":"For CNBC dataset, I use the 2 most popular clustering algorithms, K-Means Clustering and Hierarchical clustering, to categorized the sentiments of CNBC articles.","1b72877c":"Similar to part 3.2, this notebook utilizes clustering method, an unsupervised learning method of grouping data, to put each article into their respective group of sentiment.","20f4c065":"## CNBC data","2267c685":"Similar to part 3.2, instead of applying a clustering model, I apply a Natural Breaks Optimization to this dataset, specifically Jenks Natural Breaks.","14d90eff":"# Reuters data","056cfbcd":"## Import data","a3f40fc2":"# The Guardian data","d20f6186":"Assessing both K-Means and Hierarchical clusters, I can see 2 different ways these clustering algorithm group given data points. In order to minimize the amount of neutral sentiments, I decided to go with Hierarchical Clustering for this data since it creates significantly less neutral sentiments","6c911d50":"From the histogram, I can conclude that data points with sentiments below -0.2158 represent negative sentiment, data points with sentiments above 0.2584 represent positive sentiment, and data points with sentiments in between those 2 values represent neutral sentiment","279f6d31":"Again, for this dataset, both K-Means and Hierarchical clusters create a different way of grouping given data points. Knowing that the y-axis contains the preview of the article, it is possible that many articles have negative headlines, but carry positive information about the financial world nonetheless. Coupled with the effort to minimize the amount of neutral sentiments, I decided to go with K-Means Clustering for this data since it creates significantly less neutral sentiments","2473c66e":"From the scatter plot, I can conclude that data points colored in blue (y=1) represent positive sentiment, data points colored in red (y=0) represent negative sentiment, and data points colored in green (y=2) represent a mixed sentiment, in which I will assign as neutral."}}