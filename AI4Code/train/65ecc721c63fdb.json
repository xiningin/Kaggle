{"cell_type":{"a6ae78e6":"code","8dfa45ac":"code","35afd8d1":"code","51ad5fd4":"code","9e34a057":"code","0427f19a":"code","5908abfe":"code","67ae7086":"code","f9866601":"code","5b4a3fd4":"code","b60f171f":"code","c4ccbb5e":"code","d230ca52":"code","00e3db52":"code","494a2474":"code","ef17e1b7":"code","61c6c292":"code","6b8468a3":"code","fd4999cc":"code","8181fbf2":"code","a805dcda":"code","6509fa21":"code","95f43511":"code","33dab8a9":"code","3e89b1be":"code","cd5243a8":"code","88089a48":"code","27661e42":"code","7a271c4c":"code","9037dd31":"code","91a297dc":"code","e779536e":"code","9e6830c8":"code","0f9e34ce":"code","f8cf8cd0":"code","5fc081ba":"code","6f80be33":"code","890972f6":"code","d159dee5":"code","999e785a":"code","b66862a1":"code","94adbd1d":"code","5152702a":"code","2bc72de5":"markdown","19aa14bd":"markdown","deeb28b9":"markdown","77eb239f":"markdown","1f2da569":"markdown","d04ae653":"markdown","79aa4a21":"markdown","6ee95c15":"markdown","b98c6bdf":"markdown","9fcbe9b7":"markdown","f623aad3":"markdown","64e94a1d":"markdown","60262ff7":"markdown","74ccb74a":"markdown","5f44e344":"markdown","6e98c4e5":"markdown","3282a8ce":"markdown","fca9cc2f":"markdown","5e06a098":"markdown","927e60e7":"markdown","b138c3e2":"markdown","ef3e0e1a":"markdown","7bf4e1fb":"markdown","d130d4ff":"markdown","255572b2":"markdown","ffdb14f3":"markdown","8f19b0f0":"markdown","522b7c71":"markdown","b1a310cc":"markdown","41bea66a":"markdown","32bc3f8c":"markdown","5bd7f3b4":"markdown","7a3da4e1":"markdown","a9735dca":"markdown","3990dbb9":"markdown","df9c26c0":"markdown","0c4897d8":"markdown","e64877c4":"markdown","a17db274":"markdown","08cc5016":"markdown","00fd5df6":"markdown","01146bb6":"markdown","c708cd95":"markdown","b25b9af8":"markdown"},"source":{"a6ae78e6":"import pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n","8dfa45ac":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","35afd8d1":"df = pd.read_csv(\"..\/input\/lamudi.csv\")","51ad5fd4":"df.head() #top 5 rows in the dataset","9e34a057":"df.info() #data information","0427f19a":"df.describe()","5908abfe":"df.hist(bins=50, figsize=(20,15))\nplt.show()","67ae7086":"corr_matrix = df.corr()","f9866601":"corr_matrix[\"price\"].sort_values(ascending=False)","5b4a3fd4":"scatter_matrix(df,figsize=(12,8))","b60f171f":"\ndf.plot(kind=\"scatter\", x=\"price\",y=\"floor_area\", alpha=0.1)","c4ccbb5e":"df[\"rooms_per_floor_area\"] = df[\"Bedroom\"] \/ df[\"floor_area\"] # bedroom per floor area \ndf[\"floor_area_per_land_size\"] = df[\"floor_area\"] \/ df[\"land_size\"] #floor area per land size","d230ca52":"corr_matrix = df.corr()\ncorr_matrix[\"price\"].sort_values(ascending=False)","00e3db52":"# X = df[['Bedroom','floor_area','land_size']] #select feature\n# X = df[['Bedroom','land_size']] #select feature\nX = df[['land_size']] #select feature\ny = df[['price']].values   #select target var\ny = y.reshape(-1,1)","494a2474":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=42)","ef17e1b7":"# train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)","61c6c292":"lm = LinearRegression() \nlm.fit(X_train,y_train)","6b8468a3":"y_train_pred = lm.predict(X_train)","fd4999cc":"#make prediction using the training set first\ny_train_pred = lm.predict(X_train)","8181fbf2":"sTrain = mean_squared_error(y_train,y_train_pred)\nprint(\"Mean Squared error of training set: %2f\"%sTrain)","a805dcda":"np.sqrt(sTrain)","6509fa21":"dt = DecisionTreeRegressor()\ndt.fit(X_train,y_train)","95f43511":"dt_y_train_pred = dt.predict(X_train)","33dab8a9":"dtsTrain = mean_squared_error(y_train,dt_y_train_pred)\nprint(\"Mean squared error of testing set: %.2f\"%dtsTrain)","3e89b1be":"np.sqrt(dtsTrain)","cd5243a8":"rf = RandomForestRegressor() \nrf.fit(X_train,y_train)","88089a48":"rf_y_train_pred = rf.predict(X_train)","27661e42":"rfsTrain = mean_squared_error(y_train,rf_y_train_pred)\nprint(\"Mean squared error of testing set: %.2f\"%rfsTrain)","7a271c4c":"np.sqrt(rfsTrain)","9037dd31":"svr = SVR(kernel= 'linear')\nsvr.fit(X_train,y_train)","91a297dc":"svr_y_train_pred = svr.predict(X_train)","e779536e":"svrTrain = mean_squared_error(y_train,svr_y_train_pred)\nprint(\"Mean squared error of testing set: %.2f\"%svrTrain)","9e6830c8":"np.sqrt(svrTrain)","0f9e34ce":"def display_scores(scores):\n    print(\"Scores:\",scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n","f8cf8cd0":"scores = cross_val_score(dt, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)","5fc081ba":"tree_rmse_scores = np.sqrt(-scores)","6f80be33":"display_scores(tree_rmse_scores)","890972f6":"scores2 = cross_val_score(lm, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)","d159dee5":"lm_rmse_scores = np.sqrt(-scores2)\ndisplay_scores(lm_rmse_scores)","999e785a":"scores3 = cross_val_score(rf, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)","b66862a1":"rf_rmse_scores = np.sqrt(-scores3)\ndisplay_scores(rf_rmse_scores)","94adbd1d":"scores4 = cross_val_score(svr, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)","5152702a":"svr_rmse_scores = np.sqrt(-scores4)\ndisplay_scores(svr_rmse_scores)","2bc72de5":"# Feature Selection","19aa14bd":"The correlation coefficient ranges from -1 to 1. When it is colose to 1, it means that ther is a  strong positive corelation: for exampl, the price tends to go up when the floor_area goes up. WHen the coefficient is clost to -1, it means that there is a strong negative correlation. Coefficient close to zero means that there is no linear correlation.","deeb28b9":"Another way to check for correlation between attributes is to use Panda's scatter_matrix function, which plots every numerical attribute against every other numerical attribute.","77eb239f":"The info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute's type of and number of non-null values","1f2da569":"We have now a working Linear Regression model. Let's try it out on a few instances from the training set.","d04ae653":"Not a great score. This prediction error in training setis not very satisfying","79aa4a21":"Let's take a look at the top 5 rows using the dataframes head() method. Each row represent a house. There are 3 attributes : Bedroom,floor area and land size","6ee95c15":"## Linear Regression","b98c6bdf":"THe count mean and max are self explanatory. The std rows shows the standard deviation, which measures how dispersed ther values are. THe 25%, 50%, 75% rows show the corresponding percentiles: it indicates the value below which a given percentage of observation in a group of observation falls. They are often called the 25th percentile or 1st quartile, the median and the 75th percentile or the 3rd quartile.","9fcbe9b7":"## Support Vector Machine Model","f623aad3":"No error found in Random Foreset and Decision Tree? its more likely the model overfit the data. How do we know? Take note that we don't want to touch the test set until we are ready to launch a model we are confident about. Let's use a better model validation.","64e94a1d":"We framed the problem, we got the data and explored it, you sampled a traning set and test set. We are now ready to select and train a machine learning model.","60262ff7":"# Creating a Test Set","74ccb74a":"## Decision Tree Model","5f44e344":"## Support Vector Machine Model","6e98c4e5":"This is an example of of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make a good predictions or that the model is not powerful enough. The main ways to fix underfitting are:\n<ul>\n<li>Select a more powerful model<\/li>\n<li>Feed the training algorithm with better features<\/li>\n<li>Reduce constraints on the model<\/li>\n<\/ul>","3282a8ce":"## Random Forest","fca9cc2f":"# Experimenting with Attribute Combinations","5e06a098":"Let's try a more complex data. Let's train a decision tree regreesor. This is a powerful model, capable of finding complexnonlinear relationships in the data.","927e60e7":"This plot reveals few things. The correlation indeed very strong: we can see the upward trend and the points are not too dispersed. ","b138c3e2":"Tail heavy histogram may make it a bit harder for some Machine Learning algorithms to detect patterns. I will try transforming these attributes later on to have a more bell-shaped distributions.","ef3e0e1a":"Feature Selection\nHaving too many variables could potentially cause your model to become less accurate, especially if certain variables have no effect on the outcome or have a significant effect on other variables.","7bf4e1fb":"## Linear Regression Model","d130d4ff":"Since we don't have a null values to fill-in or categorical features to transform, we don't need further data sanitation.","255572b2":"Before we jump over to machine learning data preparation. Let's try out various attribute combination. ","ffdb14f3":"There are 6250 instances in the dataset, which means that it is fairly small by Machine Learning standards, but it's perfect to get started. There is no null values in our dataset. The describe() method shows a summary of the numerical attributes.","8f19b0f0":"- p-value = Smaller the better. Better to be <0.05\n- R squared = 0<R squared <1 . The Bigger the better\n- RMSE = Smaller the better","522b7c71":"K-fold validation randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores.","b1a310cc":"## CAVEAT: \nWhen one of our predictors is able to strongly predict another predictor or have weird relationships with each other , then your regression equation is going to be a mess.\n\nWhy is multicollinearity an issue with regression? Well, the regression equation is the best fit line to represent the effects of your predictors and the dependant variable, and does not include the effects of one predictor on another.\n\nHaving high collinearity (correlation of 1.00) between predictors will affect your coefficients and the accuracy, plus its ability to reduce the SSE (sum of squared errors \u2014 that thing you need to minimise with your regression).\n","41bea66a":"Since the dataset is not too large, we can easily compute the standard correlation coefficient (also called person's r) between every pair of attributes using the corr() method.","32bc3f8c":"Le'ts pick some instances randomly, typically 20% of the dataset, and set them aside. Scikit-learn provides a few functions to split dataset into multiple subsets in various ways. The simplest function is train_test_split. There is a random_state parameter that allows us to set the random generator seed.","5bd7f3b4":"let's look at the correlation matrix again.","7a3da4e1":"# Looking for Correlation","a9735dca":"# Take a Quick Look at the Data Structure","3990dbb9":"## Random Forest Model","df9c26c0":"## Decision Tree","0c4897d8":"# Better Evaluation Using Cross-Validation and other Metrics","e64877c4":"Another quick way to ge a feel of the type of data we are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances that have a given value range. We can either plot this one attribute at a time, or we can call hist() method on the whole dataset and it will plot a histogram for each numerical attribute.","a17db274":"The most promising attribute to predict the price value is the  floor_area, let's szoom in on their correlation scatterplot.","08cc5016":"# Select and Train a Model","00fd5df6":"# Prepare the Data for Machine Learning Algorithms","01146bb6":"## CAVEAT:\nOne other way to select features is to use the p-values.It tell you how statistically significant the variable is. Removing variables with high p-values can cause your accuracy\/R squared to increase, and even the p-values of the other variables to increase as well \u2014 and that\u2019s a good sign.\n\n\nThis action of omitting variables is part of stepwise regression. There are 3 ways to do this:\n<ul>\n    <li>Forward Selection<\/li>\n    <li>Backward Elimination<\/li>\n    <li>Bidirectional Elimination<\/li>\n<\/ul>","c708cd95":"We can see a interesting value of negative correlation. This exploration does not have to be absolutely thorough: the point is to start off on the right foot and quickly gain insights that will help us get a first reasonably good prototype. This is an iterative process: once we get a prototype up and running, we can analyze its output to gain more insights and come back to this exploration stage.","b25b9af8":"It works, although the predictions are not exactly accurate. Let's measure this regression model's RMSE on the whole training set using scikit-learn's mean_squared error function."}}