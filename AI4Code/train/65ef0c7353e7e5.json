{"cell_type":{"e6c77e05":"code","b3f9bbca":"code","64cbd761":"code","26f6f41d":"code","4e5645cb":"code","19ac58b3":"code","a9748acc":"code","65b4de37":"code","c26de82a":"code","c1a0aabe":"code","6c0ec9b8":"code","a57e3412":"code","03fad780":"code","d21ebe8d":"code","352d86dc":"code","93dff88d":"code","7bd00eda":"code","978a881e":"code","07d0beaf":"code","3bfa6489":"code","d49cd5e9":"code","fec3e14d":"code","8fd014a6":"code","0d36c28b":"code","4de7d9ff":"code","1a173b10":"code","38b59eff":"code","28176988":"code","c058f8fc":"code","e9cd601e":"markdown","270022dd":"markdown","bab5ab58":"markdown","2da9a841":"markdown","bd7369d4":"markdown","04ee35bf":"markdown","24ce3aad":"markdown","31c38366":"markdown","fe0f02cf":"markdown","b2ce5b86":"markdown","e550bcb9":"markdown","64969412":"markdown","8ab3d057":"markdown","f13df610":"markdown","578fb4a4":"markdown","f929b1b5":"markdown","cbd8e537":"markdown","9f09718f":"markdown","5779af57":"markdown","d8f31432":"markdown","a666b4ca":"markdown","ae10d9d8":"markdown","c319d03b":"markdown","0eeed710":"markdown","85cefb78":"markdown","5cef077b":"markdown","5d44ce48":"markdown","3507efd8":"markdown","44319d47":"markdown","8e029683":"markdown","b869bb9a":"markdown","0f5428f1":"markdown","e95fdf00":"markdown","5b57750f":"markdown","74c63bec":"markdown","8ab17df5":"markdown","39d302ca":"markdown","a21d2640":"markdown","33543a61":"markdown","e7f6b78e":"markdown","c0ef3ceb":"markdown","ba88ec62":"markdown","7f59aa93":"markdown","d67ed84a":"markdown"},"source":{"e6c77e05":"# Importing standard packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport copy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","b3f9bbca":"# Import dataset\ndf = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n# Display dataframe\ndf","64cbd761":"# Check for Nulls and dtype of dataframe\ndf.info()","26f6f41d":"# Check for NaNs\ndf.isna().sum()","4e5645cb":"#\u00a0Check for duplicates\ndf.duplicated().sum()","19ac58b3":"#\u00a0Drop NaNs and duplicates\ndf = df.dropna().drop_duplicates().reset_index(drop=True)","a9748acc":"#\u00a0Overall statistics\ndf.describe()","65b4de37":"#\u00a0Bar chart of class ratio \nquality_labels = pd.DataFrame(np.sort(df['quality'].unique()),columns= ['Quality'])\nquality_num = pd.DataFrame(np.bincount(df['quality'])[np.bincount(df['quality'])>0], columns= ['Quantity'])\nquality_percent = quality_num\/len(df)*100\nquality_percent.columns = ['Percentage']\ntarget_pd = pd.concat((quality_labels, quality_num, quality_percent), axis=1)\n# Plot barchart\nfig = plt.figure(figsize = (10, 5))\nplt.bar(list(target_pd['Quality']), target_pd['Quantity'], color= [\"Green\", \"Red\", \"Blue\", \"Maroon\", \"Purple\", \"Orange\"], width = 0.4)\nplt.xlabel(\"Wine Quality\")\nplt.ylabel(\"Number of quality wine\")\nplt.title(\"Distribution of wine qualities\");\n#\u00a0Print the dataframe\ntarget_pd","c26de82a":"# Histogram of features (check for skew)\nfig=plt.figure(figsize=(20,20))\nfor i, feature in enumerate(df.columns):\n    ax=fig.add_subplot(8,4,i+1)\n    df[feature].hist(bins=20,ax=ax,facecolor='black')\n    ax.set_title(feature+\" Distribution\",color='DarkRed')\n    ax.set_yscale('log')\nfig.tight_layout()  ","c1a0aabe":"# Check for correlation\nfig=plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot = True, cmap=\"tab20c\");\nfig.tight_layout()  ","6c0ec9b8":"#\u00a0Create X (features) and y (target) dataset\nX = df[df.columns[:-1]]\ny = df[df.columns[-1]]","a57e3412":"def feature_scaling(X: pd.DataFrame) -> pd.DataFrame:\n    \n    \"\"\" Normalises the features in X (dataframe) and returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1. \"\"\"\n    \n    # Return normalised data\n    return (X - np.mean(X, axis=0))\/np.std(X, axis=0, ddof=0)","03fad780":"# Create normalised data\nX = feature_scaling(X)\n# Split the dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/4, random_state=42, stratify=y)\n# Re-index\nX_train = X_train.reset_index(drop=True) \ny_train = y_train.reset_index(drop=True) \nX_test = X_test.reset_index(drop=True) \ny_test = y_test.reset_index(drop=True) ","d21ebe8d":"def euclidean_distance(p: np.array, q: np.array) -> float:\n\n    \"\"\" Return Euclidean Distance. \"\"\"\n        \n    return np.sqrt(np.sum((p-q)**2, axis=1))","352d86dc":"def kNN(X_train: pd.DataFrame, X_test: pd.DataFrame, k: int, return_distance: bool) -> np.array:\n    \n    \"\"\" Return k-neighbours. \"\"\"\n    \n    n_neighbours = k\n    dist = []\n    neigh_ind = []\n    # Compute distance from each point x_test in X_test to all points in X_train \n    point_dist = [euclidean_distance(X_test.loc[i], X_train) for i in range(len(X_test))]  \n    # Determine which k training points are closest to each test point\n    for row in point_dist:\n        enum_neigh = enumerate(row)\n        sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n        ind_list = [tup[0] for tup in sorted_neigh]\n        dist_list = [tup[1] for tup in sorted_neigh]\n        dist.append(dist_list)\n        neigh_ind.append(ind_list)\n    # Return distances together with indices of k nearest neighbouts\n    if return_distance:\n        return np.array(dist), np.array(neigh_ind)\n    return np.array(neigh_ind)","93dff88d":"def predict(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, k: int, return_false: bool) -> np.array:\n  \n    \"\"\" Return label predictions for test set. \"\"\"\n    \n    # Each of the k neighbours contributes equally to the classification of any data point in X_test  \n    neighbours = kNN(X_train, X_test, k, False)\n    # Count number of occurences of label \n    y_pred = np.array([np.argmax(np.bincount(y_train[neighbour])) for neighbour in neighbours]) \n    return y_pred","7bd00eda":"def score(X_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.Series, y_train: pd.Series, k: int) -> float:\n    \n    \"\"\" Return mean accuracy of test set. \"\"\"\n    \n    y_pred = predict(X_train, X_test, y_train, k=k) \n    return 100*np.float(sum(y_pred==y_test))\/ float(len(y_test))","978a881e":"class K_Nearest_Neighbours():\n    \n    def __init__(self):\n    \n        \"\"\" Initialise parameters. \"\"\"\n       \n        self.neighbours = None\n        \n    def fit(self, X_train: pd.DataFrame, X_test: pd.DataFrame, k: int, return_distance: bool) -> np.array:\n    \n        \"\"\" Fit kNN model. \"\"\"\n        \n        dist = []\n        neigh_ind = []\n        # Compute distance from each point x_test in X_test to all points in X_train \n        point_dist = [self.euclidean_distance(X_test.loc[i], X_train) for i in range(len(X_test))] \n        # Determine which k training points are closest to each test point\n        for row in point_dist:\n            enum_neigh = enumerate(row)\n            sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:k]\n            ind_list = [tup[0] for tup in sorted_neigh]\n            dist_list = [tup[1] for tup in sorted_neigh]\n            dist.append(dist_list)\n            neigh_ind.append(ind_list)\n        # Return distances together with indices of k nearest neighbours\n        if return_distance:\n            return np.array(dist), np.array(neigh_ind)\n        self.neighbours = np.array(neigh_ind)\n    \n    def predict(self, y_train: pd.Series) -> np.array:\n  \n        \"\"\" Return label predictions for test set. \"\"\"\n\n        # Count number of occurences of label \n        y_pred = np.array([np.argmax(np.bincount(y_train[neighbour])) for neighbour in self.neighbours]) \n        return y_pred\n    \n    def euclidean_distance(self, p: np.array, q: np.array) -> float:\n\n        \"\"\" Return Euclidean Distance. \"\"\"\n        \n        return np.sqrt(np.sum((p-q)**2, axis=1))\n\n    def score(self, y_true: pd.Series, y_pred: pd.Series) -> float:\n    \n        \"\"\" Return mean accuracy test set. \"\"\"\n    \n        return 100*np.float(sum(y_pred==y_true))\/ float(len(y_true))","07d0beaf":"# Create new y target labels which are binary\ny_bin_train = np.where(y_train > 6, 1, 0)\ny_bin_test = np.where(y_test > 6, 1, 0)","3bfa6489":"# Instantiate model\nwine_model_bin = K_Nearest_Neighbours()\n# Fit model to training and test dataset \nk_neighbours = np.arange(1,51,1)\naccuracy_score_train_bin = []\naccuracy_score_test_bin = []\nfor k in k_neighbours:\n    #\u00a0Train data\n    wine_model_bin.fit(X_train, X_train, k, return_distance=False)\n    y_pred_train = wine_model_bin.predict(y_train)\n    y_pred_train_bin = np.where(y_pred_train > 6, 1, 0)\n    accuracy_score_train_bin.append(wine_model_bin.score(y_bin_train, y_pred_train_bin))\n    #\u00a0Test data\n    wine_model_bin.fit(X_train, X_test, k, return_distance=False)\n    y_pred_test = wine_model_bin.predict(y_train)\n    y_pred_test_bin = np.where(y_pred_test > 6, 1, 0)\n    accuracy_score_test_bin.append(wine_model_bin.score(y_bin_test, y_pred_test_bin))","d49cd5e9":"# Plot accuracy scores for training set\nplt.plot(k_neighbours, accuracy_score_train_bin, marker = 'o',  mfc = 'r', mec = 'b')\nplt.xlabel(\"K-Neighbours\")\nplt.ylabel(\"Accuracy Score (%)\")\nplt.title(\"Accuracy Score for varying k-neighbours in training set\");","fec3e14d":"# Plot accuracy scores for test set\nplt.plot(k_neighbours, accuracy_score_test_bin, marker = 'o',  mfc = 'r', mec = 'b')\nplt.xlabel(\"K-Neighbours\")\nplt.ylabel(\"Accuracy Score (%)\")\nplt.title(\"Accuracy Score for varying k-neighbours in test set\");","8fd014a6":"print(f\"Optimal K-Neighbours is: {[i+1 for i, j in enumerate(accuracy_score_test_bin) if j == max(accuracy_score_test_bin)]}, with a mean accuracy of {round(accuracy_score_test_bin[np.argmax(accuracy_score_test_bin)],1)}%\")","0d36c28b":"#\u00a0Print classification report for optimal k-nearest-neighbours baseline model\nopt_baseline_wine_model_bin = K_Nearest_Neighbours()\nopt_baseline_wine_model_bin.fit(X_train, X_test, k=5, return_distance=False)\ny_pred_test = opt_baseline_wine_model_bin.predict(y_train)\ny_pred_test_bin = np.where(y_pred_test > 6, 1, 0)\npd.DataFrame(classification_report(y_bin_test, y_pred_test_bin, output_dict=True, zero_division=0))","4de7d9ff":"# Instantiate model\nwine_model_multi = K_Nearest_Neighbours()\n# Fit model to training and test dataset \nk_neighbours = np.arange(1,51,1)\naccuracy_score_train_multi = []\naccuracy_score_test_multi = []\nfor k in k_neighbours:\n    #\u00a0Train data\n    wine_model_multi.fit(X_train, X_train, k, return_distance=False)\n    y_pred_train = wine_model_multi.predict(y_train)\n    accuracy_score_train_multi.append(wine_model_multi.score(y_train, y_pred_train))\n    #\u00a0Test data\n    wine_model_multi.fit(X_train, X_test, k, return_distance=False)\n    y_pred_test = wine_model_multi.predict(y_train)\n    accuracy_score_test_multi.append(wine_model_multi.score(y_test, y_pred_test))","1a173b10":"# Plot accuracy scores for training set\nplt.plot(k_neighbours, accuracy_score_train_multi, marker = 'o',  mfc = 'r', mec = 'b')\nplt.xlabel(\"K-Neighbours\")\nplt.ylabel(\"Accuracy Score (%)\")\nplt.title(\"Accuracy Score for varying k-neighbours in training set\");","38b59eff":"# Plot accuracy scores for test set\nplt.plot(k_neighbours, accuracy_score_test_multi, marker = 'o',  mfc = 'r', mec = 'b')\nplt.xlabel(\"K-Neighbours\")\nplt.ylabel(\"Accuracy Score (%)\")\nplt.title(\"Accuracy Score for varying k-neighbours in test set\");","28176988":"print(f\"Optimal K-Neighbours is: {[i+1 for i, j in enumerate(accuracy_score_test_multi) if j == max(accuracy_score_test_multi)]}, with a mean accuracy of {round(accuracy_score_test_multi[np.argmax(accuracy_score_test_multi)],1)}%\")","c058f8fc":"#\u00a0Print classification report for optimal k-nearest-neighbours baseline model\nopt_baseline_wine_model_multi = K_Nearest_Neighbours()\nopt_baseline_wine_model_multi.fit(X_train, X_test, k=8, return_distance=False)\ny_pred_test_multi = opt_baseline_wine_model_multi.predict(y_train)\npd.DataFrame(classification_report(y_test, y_pred_test_multi, output_dict=True, zero_division=0))","e9cd601e":"**Note:** The inbuilt KNeighborsClassifier() from sklearn provides the exact same results (when using Euclidean distance)! ","270022dd":"## Splitting dataset","bab5ab58":"The kNN algorithm can be used both for classification and regression. \n\n1. Start with calculating the distance of a given point $x$ to all other points in the data set. \n2. Then, it finds the _k_ nearest points closest to $x$, and assigns the new point $x$ to the majority class of the _k_ nearest points _(classification)_. \n\ne.g. So, for example, if two of the _k_=3 closest points to $x$ were red while one is blue, $x$ would be classified as red.\n\nOn the other hand in _regression_, we see the labels as continuous variables and assign the label of a data point $x$ as the mean of the labels of its _k_ nearest neighbours. ","2da9a841":"## Extra","bd7369d4":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","04ee35bf":"It is clear that the target variable, which is multi-classed, has a skewed distribution between the quality of the wine. ","24ce3aad":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","31c38366":"<center> <img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSfDjqlgh8f1Py0JpOj7GKmGHLeawf4TKsBeQ&usqp=CAU\" width=\"400\" height=\"400\" \/> <\/center>","fe0f02cf":"<center> <h1>\ud83d\udc6a K-Nearest-Neighbours \ud83d\udc6a <\/h1> <\/center>","b2ce5b86":"## Import Modules","e550bcb9":"For binary, we will classify wine quality into two categories:\n1. Quality equal and below 6 is Bad.\n2. Quality above 6 is Good.","64969412":"For most machine learning models, we would like them to have low bias and low variance - that is, the model should perform well on the training set (low bias) and also the test set, alongside with other new random test sets (low variance). Therefore, to test for bias and variance of our model, we shall split the dataset into training and test set. We will not be tuning any hyperparameters (and thus do not need a validation set).  We will not be tuning any hyperparameters (and thus do not need a validation set). \n\nFor these functions, the $X$ dataset (of features) does not need to have a column 1's as the first column as there is no bias term. One check the order of magnitude of the features - if they differ hugely, one must apply feature scaling. Having looked at the data however, it is clear that the order of magnitude of some of the features are very different, so we must perform feature scaling. ","8ab3d057":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","f13df610":"We now create an accuracy function that calculates how many labels we have correctly classified.","578fb4a4":"We will take 5 as our answer since this will provide us with the quickest runtime. ","f929b1b5":"# Classification Prediction","cbd8e537":"# Full K-Nearest-Neighbours Model","9f09718f":"# Aim","5779af57":"# Background","d8f31432":"For multi, we will classify wine quality into their unique respective wine quality categories i.e. Quality 3,4,5,6,7 and 8. In order to find the optimal _k_ neighbours, we will return the _k_ which gives us the largest accuracy score. ","a666b4ca":"# Data Collection","ae10d9d8":"# Summary","c319d03b":"## Binary Classification","0eeed710":"Some comments about the code implementations:\n\n1. If dealing with arrays rather than dataframes, some of the functions may need altering to account for dimension\/shape issues e.g. the Euclidean distance is implemented with dataframes - the axis might need to be altered (or even removed) if working with arrays, the fit method will have to have .loc removed from X_test in the point_dist calculation etc... \n2. To debug this, it is important to print out the point_dist with neigh_ind so you can cross reference them against their respective target labels. ","85cefb78":"# Euclidean Distance","5cef077b":"# Data Processing","5d44ce48":"Once we know which _k_ neighbours are closest to our test points (from the training set), we can predict the labels of these test points.\nThe `predict` function determines how any point $x_\\text{test}$ in the test set is classified. Here, we only consider the case where each of the *k* neighbours contributes equally to the classification of $x_\\text{test}$.","3507efd8":"This graph makes a lot of sense. We compute the distances of each training set data point to every other training set data point - thus it is obvious why for _k_=1, we have 100% accuracy because every training data point is its own nearest neighbour. When we have _k_=2, then we are accounting for the first two closest neighbours (which will be the original data point + the next closest) etc...","44319d47":"We know that kNN algorithm is based on computing distances between data points. For simplicity, we will only work with **Euclidean distances** in this notebook, but other distances can be chosen interchangably, of course e.g. Hamming, Minkowski, Manhattan etc...\n\n**Note:** Choosing different distance metrics will give different accuracies. \n\nThe Euclidean distance $d$, is defined as\n$$\nd(\\boldsymbol p, \\boldsymbol q) = \\sqrt{\\sum_{i=1}^D{(q_i-p_i)^2}} \\, ,\n$$\nwhere $\\boldsymbol p$ and $\\boldsymbol q$ are the two points in our $D$-dimensional Euclidean space.","8e029683":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","b869bb9a":"Due to varying labels in the target variable, we need to decide wether we will treat the target variable via a binary classification or multi-class classification. \n\nWith binary, we decide on the threshold of wine quality e.g. any wine quality equal or above 6 is encoded as 1 (good quality) and anything below 6 is encoded as 0 (bad quality). With multi, the task is just to predict which wine qualities are \"close\" to each other in multi-dimensional space (and therefore perhaps share similar qualities). ","0f5428f1":"We try to find the _k_ nearest neighbours in our train set for every test data point. The majority of labels of the _k_ closest train points determines the label of the test point. ","e95fdf00":"<hr style=\"height:2px;border-width:0;color:gray;background-color:gray\">","5b57750f":"# Model Testing and Results","74c63bec":"# Multi-Class Classification","8ab17df5":"Thanks for reading this notebook. If there are any mistakes or things that need more clarity, feel free to respond in the comment section and I will be happy to reply.\n\nAs always, please leave an upvote - it would also be helpful if you cite this documentation if you are going to use any of the code. \ud83d\ude0a\n\n#CodeWithSid","39d302ca":"<center> <img src=\"https:\/\/machinelearningknowledge.ai\/wp-content\/uploads\/2018\/08\/KNN-Classification.gif\" width=\"600\" height=\"600\" \/> <\/center>","a21d2640":"- A big assumption kNN makes is that it automatically assumes the label of the data point depending on its neighbours which is not necessarily always accurate if some data points are anomalous or outliers. \n- It is clear in multi-class classification that when the test set has low values of certain types of classes, the kNN model does not perform well (also similar to the binary case as well). ","33543a61":"# Classification Score","e7f6b78e":"<p> <center> This notebook is in <span style=\"color: green\"> <b> Active <\/b> <\/span> state of development! <\/center> <\/p>  \n<p> <center> Be sure to checkout my other notebooks for <span style=\"color: blue\"> <b> knowledge, insight and laughter <\/b> <\/span>! \ud83e\udde0\ud83d\udca1\ud83d\ude02<\/center> <\/p> ","c0ef3ceb":"We are more concerned with the how the model does on the test set and so we aim to look for the optimal _k_ neighbours from these accuracies. We have not applied any hyperparameter tuning as mentioned before so this baseline model can be improved! ","ba88ec62":"# K-Nearest-Neighbours","7f59aa93":"Let us look at the information obtained **before** applying the pre-processing steps:","d67ed84a":"The aim is to provide, from scratch, code implementations for linear regression problems. This will involve both the main functions needed to solve a linear regression and some additional utility functions as well.\n\n**Note**: We will not be diving into in-depth exploratory data analysis, feature engineering etc... in these notebooks and so will not be commenting extensively on things such as skewness, kurtosis, homoscedasticity etc..."}}