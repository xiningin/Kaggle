{"cell_type":{"a7449280":"code","b2f29f3e":"code","6a2b78e7":"code","4cf54fa9":"code","f9de6ebd":"code","4b38eab3":"code","ec7f947b":"code","b6ca50dc":"code","ce5694cc":"code","311071ca":"code","bcf7141b":"code","c1c78992":"markdown"},"source":{"a7449280":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b2f29f3e":"\nimport lightgbm as lgbm\nfrom scipy import sparse as ssp\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n\n    # sort rows on prediction column\n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n\n    # get Lorenz curves\n    L_true = np.cumsum(true_order) * 1. \/ np.sum(true_order)\n    L_pred = np.cumsum(pred_order) * 1. \/ np.sum(pred_order)\n    L_ones = np.linspace(1 \/ n_samples, 1, n_samples)\n\n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n\n    # normalize to true Gini coefficient\n    return G_pred * 1. \/ G_true\n\ncv_only = True\nsave_cv = True\nfull_train = False\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', Gini(labels, preds), True\n\npath = \"..\/input\/\"\n\ntrain = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntrain_label = train['target']\ntrain_id = train['id']\ntest = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/test.csv')\ntest_id = test['id']\n\nNFOLDS = 5\nkfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\n\ny = train['target'].values\ndrop_feature = [\n    'id',\n    'target'\n]\n\nX = train.drop(drop_feature,axis=1)\nfeature_names = X.columns.tolist()\ncat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\nnum_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]\n\ntrain['missing'] = (train==-1).sum(axis=1).astype(float)\ntest['missing'] = (test==-1).sum(axis=1).astype(float)\nnum_features.append('missing')\n\nfor c in cat_features:\n    le = LabelEncoder()\n    le.fit(train[c])\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])\n\nenc = OneHotEncoder()\nenc.fit(train[cat_features])\nX_cat = enc.transform(train[cat_features])\nX_t_cat = enc.transform(test[cat_features])\n\nind_features = [c for c in feature_names if 'ind' in c]\ncount=0\nfor c in ind_features:\n    if count==0:\n        train['new_ind'] = train[c].astype(str)+'_'\n        test['new_ind'] = test[c].astype(str)+'_'\n        count+=1\n    else:\n        train['new_ind'] += train[c].astype(str)+'_'\n        test['new_ind'] += test[c].astype(str)+'_'\n\ncat_count_features = []\nfor c in cat_features+['new_ind']:\n    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n    cat_count_features.append('%s_count'%c)\n\ntrain_list = [train[num_features+cat_count_features].values,X_cat,]\ntest_list = [test[num_features+cat_count_features].values,X_t_cat,]\n\nX = ssp.hstack(train_list).tocsr()\nX_test = ssp.hstack(test_list).tocsr()\n\nlearning_rate = 0.1\nnum_leaves = 15\nmin_data_in_leaf = 2000\nfeature_fraction = 0.6\nnum_boost_round = 10000\nparams = {\"objective\": \"binary\",\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": learning_rate,\n          \"num_leaves\": num_leaves,\n           \"max_bin\": 256,\n          \"feature_fraction\": feature_fraction,\n          \"verbosity\": 0,\n          \"drop_rate\": 0.1,\n          \"is_unbalance\": False,\n          \"max_drop\": 50,\n          \"min_child_samples\": 10,\n          \"min_child_weight\": 150,\n          \"min_split_gain\": 0,\n          \"subsample\": 0.9\n          }\n\nx_score = []\nfinal_cv_train = np.zeros(len(train_label))\nfinal_cv_pred = np.zeros(len(test_id))\nfor s in range(16):\n    cv_train = np.zeros(len(train_label))\n    cv_pred = np.zeros(len(test_id))\n\n    params['seed'] = s\n\n    if cv_only:\n        kf = kfold.split(X, train_label)\n\n        best_trees = []\n        fold_scores = []\n\n        for i, (train_fold, validate) in enumerate(kf):\n            X_train, X_validate, label_train, label_validate = \\\n                X[train_fold, :], X[validate, :], train_label[train_fold], train_label[validate]\n            dtrain = lgbm.Dataset(X_train, label_train)\n            dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n            bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, verbose_eval=100,\n                            early_stopping_rounds=100)\n            best_trees.append(bst.best_iteration)\n            cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n            cv_train[validate] += bst.predict(X_validate)\n\n            score = Gini(label_validate, cv_train[validate])\n            print(score)\n            fold_scores.append(score)\n\n        cv_pred \/= NFOLDS\n        final_cv_train += cv_train\n        final_cv_pred += cv_pred\n\n        print(\"cv score:\")\n        print(Gini(train_label, cv_train))\n        print(\"current score:\", Gini(train_label, final_cv_train \/ (s + 1.)), s+1)\n        print(fold_scores)\n        print(best_trees, np.mean(best_trees))\n\n        x_score.append(Gini(train_label, cv_train))\n\nprint(x_score)\npd.DataFrame({'id': test_id, 'target': final_cv_pred \/ 16.}).to_csv('lgbm_test.csv', index=False)\npd.DataFrame({'id': train_id, 'target': final_cv_train \/ 16.}).to_csv('lgbm_cv.csv', index=False)\n","6a2b78e7":"from lightgbm import LGBMClassifier\nfrom scipy import sparse as ssp\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","4cf54fa9":"train = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntrain_label = train['target']\ntrain_id = train['id']\ntest = pd.read_csv('\/kaggle\/input\/porto-seguro-safe-driver-prediction\/test.csv')\ntest_id = test['id']","f9de6ebd":"train.head()","4b38eab3":"X = train.drop(['id','target'], axis=1)","ec7f947b":"feature_names = X.columns.tolist()\ncat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\nnum_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]","b6ca50dc":"train['missing'] = (train==-1).sum(axis=1)\ntest['missing'] = (test==-1).sum(axis=1)\nnum_features.append('missing')","ce5694cc":"\nfor c in ind_features:\n    if count==0:\n        train['new_ind'] = train[c].astype(str)+'_'\n        test['new_ind'] = test[c].astype(str)+'_'\n        count+=1\n    else:\n        train['new_ind'] += train[c].astype(str)+'_'\n        test['new_ind'] += test[c].astype(str)+'_'","311071ca":"ind_features = [c for c in feature_names if 'ind' in c]\nind_features","bcf7141b":"train['ps_ind_14'].astype(str)+'_'","c1c78992":"### Create 2 new features missing"}}