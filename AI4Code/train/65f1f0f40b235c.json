{"cell_type":{"9161238d":"code","e4212c52":"code","40162194":"code","89e73d9b":"code","120b37a3":"code","9698cc7b":"code","c3b1ec06":"code","b5d1695e":"code","9da51298":"code","5a72cfa9":"code","e82f4e09":"code","131b6cc9":"code","78a211b2":"code","e8794b46":"code","184fa356":"code","96b861d4":"code","905d152b":"code","ff192901":"code","654a07cb":"code","3da902d1":"code","cbe4f39b":"code","c471ff8d":"code","a3cc0c05":"code","9ecb9dbe":"code","10e1068e":"code","71d0f287":"code","e86743fc":"code","f4bfd286":"code","2dcc2de0":"code","3f9a8124":"code","988bad4f":"code","cdb2f006":"code","64bdb054":"code","0c6e569d":"code","3e9fb08a":"code","629b24a1":"code","a6da0f7a":"code","464b0fea":"code","4f05f505":"code","746daf9a":"code","bd8147f6":"code","6aa1d5c9":"code","6f588090":"code","5ec4766b":"code","716b90a6":"code","1eb1abf0":"code","038c7127":"code","a3b9b454":"code","3cde2df0":"code","53a23491":"code","de1f7fb3":"code","9a7c4f42":"code","14791bdb":"code","8cb38439":"code","9d7e60da":"code","941890ff":"code","84f56542":"code","6cc2ebea":"code","961bc40a":"code","439e540d":"code","a5f46129":"code","ec861351":"code","9b7dd7f6":"code","4e2f53c7":"code","d3e641ef":"code","74544c93":"code","4415f045":"markdown","ce0f82c8":"markdown"},"source":{"9161238d":"!pip install gTTS","e4212c52":"# Load libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport os\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model\nfrom keras.utils import np_utils\nimport random\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image, sequence\nimport matplotlib.pyplot as plt\n#from gtts import gTTS\nimport string","40162194":"from os import listdir\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom numpy import array\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom pickle import load\nfrom pickle import dump","89e73d9b":"# Load data\nimages_dir = os.listdir(\"..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\")\n\nimages_path = '..\/input\/flickr_data\/Flickr_Data\/Images\/'\ncaptions_path = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\ntrain_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt'\nval_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.devImages.txt'\ntest_path = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.testImages.txt'\n\ncaptions = open(captions_path, 'r').read().split(\"\\n\")\nx_train = open(train_path, 'r').read().split(\"\\n\")\nx_val = open(val_path, 'r').read().split(\"\\n\")\nx_test = open(test_path, 'r').read().split(\"\\n\")","120b37a3":"# extract features from each photo in the directory\ndef extract_features(directory):\n    # load the model\n    model = VGG16()\n\t# re-structure the model\n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n\t# summarize\n    print(model.summary())\n\t# extract features from each photo\n    features = dict()\n    for name in listdir(directory):\n        # load an image from file\n        filename = directory + '\/' + name\n        image = load_img(filename, target_size=(224, 224))\n        # convert the image pixels to a numpy array\n        image = img_to_array(image)\n        # reshape data for the model\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        # prepare the image for the VGG model\n        image = preprocess_input(image)\n        # get features\n        feature = model.predict(image, verbose=0)\n        # get image id\n        image_id = name.split('.')[0]\n        # store feature\n        features[image_id] = feature\n        print('>%s' % name)\n    return features\n\n# extract features from all images\ndirectory = '..\/input\/flickr8k\/flickr_data\/Flickr_Data\/Images\/'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\nwith open('features.pickle', 'wb') as f:\n    pickle.dump(features,f)\n","9698cc7b":"import string\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# extract descriptions for images\ndef load_descriptions(doc):\n\tmapping = dict()\n\t# process lines\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\tif len(line) < 2:\n\t\t\tcontinue\n\t\t# take the first token as the image id, the rest as the description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# remove filename from image id\n\t\timage_id = image_id.split('.')[0]\n\t\t# convert description tokens back to string\n\t\timage_desc = ' '.join(image_desc)\n\t\t# create the list if needed\n\t\tif image_id not in mapping:\n\t\t\tmapping[image_id] = list()\n\t\t# store description\n\t\tmapping[image_id].append(image_desc)\n\treturn mapping\n\ndef clean_descriptions(descriptions):\n\t# prepare translation table for removing punctuation\n    \n    table = str.maketrans('','',string.punctuation)\n    for key,desc_list in descriptions.items():\n        for i in range(len(desc_list)):\n            desc = desc_list[i]\n\t\t\t# tokenize\n            desc = desc.split()\n\t\t\t# convert to lower case\n            desc = [word.lower() for word in desc]\n\t\t\t# remove punctuation from each token\n            desc = [w.translate(table) for w in desc]\n\t\t\t# remove hanging 's' and 'a'\n            desc = [word for word in desc if len(word)>1]\n\t\t\t# remove tokens with numbers in them\n            desc = [word for word in desc if word.isalpha()]\n            # store as string\n            desc_list[i] =  ' '.join(desc)\n\n# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n\t# build a list of all description strings\n\tall_desc = set()\n\tfor key in descriptions.keys():\n\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n\treturn all_desc\n\n# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n\tlines = list()\n\tfor key, desc_list in descriptions.items():\n\t\tfor desc in desc_list:\n\t\t\tlines.append(key + ' ' + desc)\n\tdata = '\\n'.join(lines)\n\tfile = open(filename, 'w')\n\tfile.write(data)\n\tfile.close()\n\nfilename = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\n# load descriptions\ndoc = load_doc(filename)\n#parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))\n# clean descriptions\nclean_descriptions(descriptions)\n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))\n# save to file\nsave_descriptions(descriptions, 'descriptions.txt')","c3b1ec06":"from pickle import load\n \n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n \n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n'):\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n \n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions\n \n# load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n \n# load training dataset (6K)\nfilename = train_path\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('..\/working\/descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('..\/working\/features.pickle', train)\nprint('Photos: train=%d' % len(train_features))","b5d1695e":"# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)","9da51298":"# calculate the length of the description with the most words\ndef max_length(descriptions):\n\tlines = to_lines(descriptions)\n\treturn max(len(d.split()) for d in lines)","5a72cfa9":"# define the captioning model\ndef define_model(vocab_size, max_length):\n\t# feature extractor model\n\tinputs1 = Input(shape=(4096,))\n\tfe1 = Dropout(0.5)(inputs1)\n\tfe2 = Dense(256, activation='relu')(fe1)\n\t# sequence model\n\tinputs2 = Input(shape=(max_length,))\n\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n\tse2 = Dropout(0.5)(se1)\n\tse3 = LSTM(256)(se2)\n\t# decoder model\n\tdecoder1 = add([fe2, se3])\n\tdecoder2 = Dense(256, activation='relu')(decoder1)\n\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\t# tie it together [image, seq] [word]\n\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\t# summarize model\n\tprint(model.summary())\n\tplot_model(model, to_file='model.png', show_shapes=True)\n\treturn model","e82f4e09":"# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n\t# loop for ever over images\n\twhile 1:\n\t\tfor key, desc_list in descriptions.items():\n\t\t\t# retrieve the photo feature\n\t\t\tphoto = photos[key][0]\n\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n\t\t\tyield [[in_img, in_seq], out_word]","131b6cc9":"# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n\tX1, X2, y = list(), list(), list()\n\t# walk through each description for the image\n\tfor desc in desc_list:\n\t\t# encode the sequence\n\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n\t\t# split one sequence into multiple X,y pairs\n\t\tfor i in range(1, len(seq)):\n\t\t\t# split into input and output pair\n\t\t\tin_seq, out_seq = seq[:i], seq[i]\n\t\t\t# pad input sequence\n\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n\t\t\t# encode output sequence\n\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\t\t\t# store\n\t\t\tX1.append(photo)\n\t\t\tX2.append(in_seq)\n\t\t\ty.append(out_seq)\n\treturn array(X1), array(X2), array(y)","78a211b2":"filename = train_path\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('..\/working\/descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('..\/working\/features.pickle', train)\nprint('Photos: train=%d' % len(train_features))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n# prepare sequences\nX1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)","e8794b46":"# train the model, run epochs manually and save after each epoch\nmodel = define_model(vocab_size, max_length)\nepochs = 20\nsteps = len(train_descriptions)\nfor i in range(epochs):\n\t# create the data generator\n\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n\t# fit for one epoch\n\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n\t# save model\n\tmodel.save('model_' + str(i) + '.h5')","184fa356":"from numpy import argmax\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n'):\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n\tlines = to_lines(descriptions)\n\treturn max(len(d.split()) for d in lines)\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text\n\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n\tactual, predicted = list(), list()\n\t# step over the whole set\n\tfor key, desc_list in descriptions.items():\n\t\t# generate description\n\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n\t\t# store actual and predicted\n\t\treferences = [d.split() for d in desc_list]\n\t\tactual.append(references)\n\t\tpredicted.append(yhat.split())\n\t# calculate BLEU score\n\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\n# prepare tokenizer on train set\n\n# load training dataset (6K)\nfilename = train_path\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('..\/working\/descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n# prepare test set\n\n# load test set\nfilename = test_path\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n# descriptions\ntest_descriptions = load_clean_descriptions('..\/working\/descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('..\/working\/features.pickle', test)\nprint('Photos: test=%d' % len(test_features))\n\n# load the modelfilename = 'model-ep002-loss3.245-val_loss3.612.h5'\nfor i in range(0,20):\n    filename=f'model_{i}.h5'\n    #filename='model_2.h5'\n    model = load_model(filename)\n    # evaluate model\n    evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","96b861d4":"# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n \n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text","905d152b":"# load training dataset (6K)\nfilename = captions_path\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('..\/working\/descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\n# save the tokenizer\ndump(tokenizer, open('tokenizer.pkl', 'wb'))","ff192901":"from pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.models import load_model\n\n# extract features from each photo in the directory\ndef extract_features(filename):\n\t# load the model\n\tmodel = VGG16()\n\t# re-structure the model\n\tmodel.layers.pop()\n\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n\t# load the photo\n\timage = load_img(filename, target_size=(224, 224))\n\t# convert the image pixels to a numpy array\n\timage = img_to_array(image)\n\t# reshape data for the model\n\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\t# prepare the image for the VGG model\n\timage = preprocess_input(image)\n\t# get features\n\tfeature = model.predict(image, verbose=0)\n\treturn feature\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text\n\n# load the tokenizer\ntokenizer = load(open('..\/working\/tokenizer.pkl', 'rb'))\n# pre-define the max sequence length (from training)\nmax_length = 34\n# load the model\nmodel = load_model('..\/working\/model_3.h5')\n# load and prepare the photograph\nphoto = extract_features('..\/input\/test-images-caption\/t3.jpg')\n# generate description\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)","654a07cb":"# Loading captions as values and images as key in dictionary\ntokens = {}\n\nfor ix in range(len(captions)-1):\n    temp = captions[ix].split(\"#\")\n    if temp[0] in tokens:\n        tokens[temp[0]].append(temp[1][2:])\n    else:\n        tokens[temp[0]] = [temp[1][2:]]","3da902d1":"# displaying an image and captions given to it\ntemp = captions[78].split(\"#\")\nfrom IPython.display import Image, display\nz = Image(filename=images_path+temp[0])\ndisplay(z)\n\nfor ix in range(len(tokens[temp[0]])):\n    print(tokens[temp[0]][ix])","cbe4f39b":"# Creating train, test and validation dataset files with header as 'image_id' and 'captions'\ntrain_dataset = open('flickr_8k_train_dataset.txt','wb')\ntrain_dataset.write(b\"image_id\\tcaptions\\n\")\n\nval_dataset = open('flickr_8k_val_dataset.txt','wb')\nval_dataset.write(b\"image_id\\tcaptions\\n\")\n\ntest_dataset = open('flickr_8k_test_dataset.txt','wb')\ntest_dataset.write(b\"image_id\\tcaptions\\n\")","c471ff8d":"# Populating the above created files for train, test and validation dataset with image ids and captions for each of these images\nfor img in x_train:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        train_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        train_dataset.flush()\ntrain_dataset.close()\n\nfor img in x_test:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        test_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        test_dataset.flush()\ntest_dataset.close()\n\nfor img in x_val:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        val_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        val_dataset.flush()\nval_dataset.close()","a3cc0c05":"# Loading 50 layer Residual Network Model and getting the summary of the model\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"\"\"<a href=\"http:\/\/ethereon.github.io\/netscope\/#\/gist\/db945b393d40bfa26006\">ResNet50 Architecture<\/a>\"\"\"))\nmodel = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\nmodel.summary()\n# Note: For more details on ResNet50 architecture you can click on hyperlink given below","9ecb9dbe":"# Helper function to process images\ndef preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","10e1068e":"train_data = {}\nctr=0\nfor ix in x_train:\n    if ix == \"\":\n        continue\n    if ctr >= 3000:\n        break\n    ctr+=1\n    if ctr%1000==0:\n        print(ctr)\n    path = images_path + ix\n    img = preprocessing(path)\n    pred = model.predict(img).reshape(2048)\n    train_data[ix] = pred","71d0f287":"train_data['2513260012_03d33305cf.jpg'].shape","e86743fc":"# opening train_encoded_images.p file and dumping it's content\nwith open( \"train_encoded_images.p\", \"wb\" ) as pickle_f:\n    pickle.dump(train_data, pickle_f )  ","f4bfd286":"# Loading image and its corresponding caption into a dataframe and then storing values from dataframe into 'ds'\npd_dataset = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t')\nds = pd_dataset.values\nprint(ds.shape)","2dcc2de0":"pd_dataset.head()","3f9a8124":"# Storing all the captions from ds into a list\nsentences = []\nfor ix in range(ds.shape[0]):\n    sentences.append(ds[ix, 1])\n    \nprint(len(sentences))","988bad4f":"# First 5 captions stored in sentences\nsentences[:5]","cdb2f006":"# Splitting each captions stored in 'sentences' and storing them in 'words' as list of list\nwords = [i.split() for i in sentences]","64bdb054":"# Creating a list of all unique words\nunique = []\nfor i in words:\n    unique.extend(i)\nunique = list(set(unique))\n\nprint(len(unique))\n\nvocab_size = len(unique)","0c6e569d":"# Vectorization\nword_2_indices = {val:index for index, val in enumerate(unique)}\nindices_2_word = {index:val for index, val in enumerate(unique)}","3e9fb08a":"word_2_indices['UNK'] = 0\nword_2_indices['raining'] = 8253","629b24a1":"indices_2_word[0] = 'UNK'\nindices_2_word[8253] = 'raining'","a6da0f7a":"print(word_2_indices['<start>'])\nprint(indices_2_word[4011])\nprint(word_2_indices['<end>'])\nprint(indices_2_word[8051])","464b0fea":"vocab_size = len(word_2_indices.keys())\nprint(vocab_size)","4f05f505":"max_len = 0\n\nfor i in sentences:\n    i = i.split()\n    if len(i) > max_len:\n        max_len = len(i)\n\nprint(max_len)","746daf9a":"padded_sequences, subsequent_words = [], []\n\nfor ix in range(ds.shape[0]):\n    partial_seqs = []\n    next_words = []\n    text = ds[ix, 1].split()\n    text = [word_2_indices[i] for i in text]\n    for i in range(1, len(text)):\n        partial_seqs.append(text[:i])\n        next_words.append(text[i])\n    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n\n    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n    \n    #Vectorization\n    for i,next_word in enumerate(next_words):\n        next_words_1hot[i, next_word] = 1\n        \n    padded_sequences.append(padded_partial_seqs)\n    subsequent_words.append(next_words_1hot)\n    \npadded_sequences = np.asarray(padded_sequences)\nsubsequent_words = np.asarray(subsequent_words)\n\nprint(padded_sequences.shape)\nprint(subsequent_words.shape)","bd8147f6":"print(padded_sequences[0])","6aa1d5c9":"for ix in range(len(padded_sequences[0])):\n    for iy in range(max_len):\n        print(indices_2_word[padded_sequences[0][ix][iy]],)\n    print(\"\\n\")\n\nprint(len(padded_sequences[0]))","6f588090":"num_of_images = 2000","5ec4766b":"captions = np.zeros([0, max_len])\nnext_words = np.zeros([0, vocab_size])","716b90a6":"for ix in range(num_of_images):#img_to_padded_seqs.shape[0]):\n    captions = np.concatenate([captions, padded_sequences[ix]])\n    next_words = np.concatenate([next_words, subsequent_words[ix]])\n\nnp.save(\"captions.npy\", captions)\nnp.save(\"next_words.npy\", next_words)\n\nprint(captions.shape)\nprint(next_words.shape)","1eb1abf0":"with open('..\/input\/train_encoded_images.p', 'rb') as f:\n    encoded_images = pickle.load(f, encoding=\"bytes\")","038c7127":"imgs = []\n\nfor ix in range(ds.shape[0]):\n    if ds[ix, 0].encode() in encoded_images.keys():\n        \n        print(ix, encoded_images[ds[ix, 0].encode()])\n        imgs.append(list(encoded_images[ds[ix, 0].encode()]))\n\nimgs = np.asarray(imgs)\nprint(imgs.shape)","a3b9b454":"images = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        images.append(imgs[ix])\n        \nimages = np.asarray(images)\n\nnp.save(\"images.npy\", images)\n\nprint(images.shape)","3cde2df0":"image_names = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        image_names.append(ds[ix, 0])\n        \nimage_names = np.asarray(image_names)\n\nnp.save(\"image_names.npy\", image_names)\n\nprint(len(image_names))","53a23491":"#here we loaded the previously stored numpy array which contain the captions \ncaptions = np.load(\"captions.npy\")\nnext_words = np.load(\"next_words.npy\")\n\nprint(captions.shape)\nprint(next_words.shape)","de1f7fb3":"images = np.load(\"images.npy\")\n\nprint(images.shape)","9a7c4f42":"imag = np.load(\"image_names.npy\")\n        \nprint(imag.shape)","14791bdb":"embedding_size = 128\nmax_len = 40","8cb38439":"image_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()","9d7e60da":"language_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()","941890ff":"conca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"..\/input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","84f56542":"hist = model.fit([images, captions], next_words, batch_size=512, epochs=200)","6cc2ebea":"import json\n\nmodel_json = model.to_json()\nwith open(\"model_in_json.json\", \"w\") as json_file:\n    json.dump(model_json, json_file)","961bc40a":"model.save_weights(\"model_weights.h5\")","439e540d":"def preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","a5f46129":"def get_encoding(model, img):\n    image = preprocessing(img)\n    pred = model.predict(image).reshape(2048)\n    return pred","ec861351":"resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')","9b7dd7f6":"img = \"..\/input\/Flickr_Data\/Flickr_Data\/Images\/1045521051_108ebc19be.jpg\"\n\ntest_img = get_encoding(resnet, img)","4e2f53c7":"def predict_captions(image):\n    start_word = [\"<start>\"]\n    while True:\n        par_caps = [word_2_indices[i] for i in start_word]\n        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n        preds = model.predict([np.array([image]), np.array(par_caps)])\n        word_pred = indices_2_word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_len:\n            break\n            \n    return ' '.join(start_word[1:-1])\n\nArgmax_Search = predict_captions(test_img)","d3e641ef":"import os\nimport IPython\nlanguage='en-in'\nmytext=Argmax_Search\nmyobj = gTTS(text=mytext, lang=language, slow=False) \nmyobj.save(\"caption.mp3\") \n","74544c93":"z = Image(filename=img)\ndisplay(z)\n\n#print(\"Predicted caption is =\"+Argmax_Search)\nIPython.display.display(IPython.display.Audio('caption.mp3'))\nprint(mytext)","4415f045":"### Predictions","ce0f82c8":"###  **Model**"}}