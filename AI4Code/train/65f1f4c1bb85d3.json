{"cell_type":{"4efe7539":"code","a7ba21fb":"code","c4da84cc":"code","d4b24635":"code","932f6da7":"code","8758f1e1":"code","e2e21326":"code","35ca06a1":"code","a368600d":"code","ee79578f":"code","46ebef24":"code","f42be9aa":"code","1bbe1eaf":"code","e25f51ff":"code","1b27cc0d":"code","c6f1f804":"code","fd6bef72":"code","898a1da0":"code","0fe961a8":"code","e091390c":"code","36b4f2e9":"code","6253675f":"code","e3f48ead":"code","ed54da4a":"code","95725573":"code","4ba401fa":"code","a1816cc0":"code","4ba73fa3":"code","69a2d6e8":"code","f1926a6c":"code","a4a73334":"code","b999b967":"code","bec93974":"code","6b83bd3d":"code","08b2a16c":"code","a3bf97fc":"code","de051820":"code","bfc8102c":"code","32700428":"code","650859b2":"code","05497b91":"code","0097c8f6":"code","88c76b6e":"code","5df1c6e3":"code","6bcbd45c":"code","f29f8b2a":"code","9c83a2e3":"code","9fa540cf":"code","5f6467fb":"code","8947834d":"code","a9c3de97":"code","94ad3762":"code","bce6e12f":"code","18f49470":"code","685d6282":"code","a786d896":"code","ca6777b1":"code","dfb09b70":"code","330f4e29":"code","b9ee04ce":"code","a3b97553":"code","ade98636":"code","76d58169":"code","ae637e60":"code","25ee431a":"code","f919f226":"code","ce14c88f":"code","903a7d12":"code","0f8c6d43":"code","0d4f6086":"code","a960de5a":"code","1a00e1ea":"code","36ed27c5":"code","6991099e":"code","de58c2f8":"markdown","b4b0825b":"markdown","f5fae44a":"markdown","5d0d43aa":"markdown","f9f79023":"markdown","a800fd5d":"markdown","0dcc43c1":"markdown"},"source":{"4efe7539":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a7ba21fb":"#Import the libraries \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#preprocessing - lemmatizing, stemming\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport re\n\n#modelling - countvectorizing, confusion-matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\n#creating word cloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport warnings\nwarnings.filterwarnings('ignore')","c4da84cc":"#setting the display options\n\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)","d4b24635":"#read the train datset\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')","932f6da7":"#read the test dataset\nkaggle_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","8758f1e1":"#checking the train dataset\ntrain.tail()","e2e21326":"#checking the test dataset\nkaggle_test.head()","35ca06a1":"#checking the dimension of test dataset\nkaggle_test.shape","a368600d":"#checking the nulls in train dataset\ntrain.isnull().sum()","ee79578f":"#checking how balanced the two classes are\ntrain['target'].value_counts(normalize=True)","46ebef24":"#drop columns keyword and location \ntrain.drop(['keyword','location'], axis=1, inplace=True)","f42be9aa":"#checking the null\ntrain.isnull().sum()","1bbe1eaf":"#checking the datatype\ntrain.info()","e25f51ff":"#checking the text column\ntrain.text.head()","1b27cc0d":"# Printing the text lengths\nlengths = [len(text) for text in train['text']]\nlengths[:10]","c6f1f804":"#adding the length of the tweet to the train dataset\ntrain['tweet_length'] = lengths\ntrain.head()","fd6bef72":"# instantiate tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')","898a1da0":"#replacing certain words in the lemmatized text \ntrain['text']=train['text'].str.replace(r'amp','',regex=True)\ntrain['text']=train['text'].str.replace(r'\\d','',regex=True)\ntrain['text']=train['text'].str.replace(r'\u00db.*.*','',regex=True)\ntrain['text']=train['text'].str.lower()\ntrain['text']=train['text'].str.replace('\\#','',regex=True)","0fe961a8":"#tokenizing the title of working papers\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))","e091390c":"#Create stopword list\nstopwords = set(STOPWORDS)\nnew_words = [\"may\",\"aren\", \"couldn\", \"didn\", \"doesn\", \"don\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"let\", \n                  \"ll\", \"mustn\", \"re\", \"shan\", \"shouldn\", \"ve\", \"wasn\", \"weren\", \"won\", \"wouldn\", \"t\",\n            \"within\",\"upon\", \"greater\",\"effect\",\"new\", \"the\",\"will\",\"via\",\"still\",\"today\",\"day\",\"co\",\n            \"one\",\"now\",\"year\",\"time\",\"yr\",\"go\",\"want\",\"rt\",\"gt\",\"got\",\"know\",\"people\"]\nstopwords = stopwords.union(new_words)","36b4f2e9":"#instantiate lemmatizer\nlemmatizer = WordNetLemmatizer()","6253675f":"#function to lemmatize the title text\ndef word_lemmatizer(title):\n    lem_text = \" \".join([lemmatizer.lemmatize(i) for i in title if not i in stopwords])\n    return lem_text","e3f48ead":"#applying the lemmatizer and checking the title column\ntrain['text'] = train['text'].apply(lambda x: word_lemmatizer(x))\ntrain['text'].head()","ed54da4a":"#joining all words in text column\ntext = \" \".join(text for text in train['text'])","95725573":"#wordcloud for No Disaster\nno_disaster = \" \".join(text for text in train[train[\"target\"]==0]['text'])\n#Create and generate a word cloud image:\nwordcloud_no_disaster = WordCloud(stopwords = stopwords,collocations=False,background_color=\"white\", max_words=150).generate(no_disaster)\n\n# Display the generated image:\nplt.imshow(wordcloud_no_disaster, interpolation='bilinear',aspect=\"auto\")\nplt.axis(\"off\")\n# store to file\nplt.savefig(\"no_disaster_word_cloud.png\", format=\"png\")\nplt.show()","4ba401fa":"#wordcloud for Disaster\ndisaster = \" \".join(text for text in train[train[\"target\"]==1]['text'])\n#Create and generate a word cloud image:\nwordcloud_disaster = WordCloud(stopwords = stopwords,collocations=False,background_color=\"white\", max_words=150).generate(disaster)\n\n# Display the generated image:\nplt.imshow(wordcloud_disaster, interpolation='bilinear',aspect=\"auto\")\nplt.axis(\"off\")\n# store to file\nplt.savefig(\"disaster_word_cloud.png\", format=\"png\")\nplt.show()","a1816cc0":"#instantiating the count vectorizer\n#fitting and transforming the title \n\ncv = CountVectorizer(max_df=0.8,stop_words=stopwords, max_features=10000, ngram_range=(1,3))\nX = cv.fit_transform(train['text'])","4ba73fa3":"#Most frequently occuring words\ndef get_top_n_words(text, n=None):\n    vec = CountVectorizer(stop_words=stopwords).fit(train['text'])\n    bag_of_words = vec.transform(train['text'])\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(train['text'], n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\nprint(top_df)\n\n#Barplot of most freq words\n\nsns.set(rc={'figure.figsize':(13,8)});\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df);\ng.set_xticklabels(g.get_xticklabels(), rotation=90);","69a2d6e8":"#Most frequently occuring Bi-grams \ndef get_top_n2_words(text, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2),  \n            max_features=2000, stop_words=stopwords).fit(train['text'])\n    bag_of_words = vec1.transform(train['text'])\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\n\ntop2_words = get_top_n2_words(train['text'], n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n\n#Barplot of most freq Bi-grams\n\nsns.set(rc={'figure.figsize':(13,8)});\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df);\nh.set_xticklabels(h.get_xticklabels(), rotation=90);","f1926a6c":"#defining the X and y variables \nX = train[[\"text\"]]\ny = train[\"target\"]","a4a73334":"#train-test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.25,\n                                                    random_state=42,\n                                                    stratify=y)","b999b967":"# Instantiate our CountVectorizer.\ncvec = CountVectorizer(max_features=500, stop_words='english')","bec93974":"# Fit our CountVectorizer on the training data and transform training data.\nX_train_cvec = pd.DataFrame(cvec.fit_transform(X_train['text']).toarray(),\n                           columns = cvec.get_feature_names())\nX_train_cvec.head()","6b83bd3d":"# Transform our testing data with the already-fit CountVectorizer.\nX_test_cvec = pd.DataFrame(cvec.transform(X_test['text']).toarray(),\n                          columns = cvec.get_feature_names())\nX_test_cvec.head()","08b2a16c":"pipe = Pipeline([\n    ('cvec', CountVectorizer()),\n    ('lr', LogisticRegression())\n])","a3bf97fc":"pipe_params = {\n    'cvec__max_features': [500],\n    'cvec__ngram_range': [(1,1), (1,2),(2,2)],\n    'cvec__stop_words': [stopwords]\n}\ngs = GridSearchCV(pipe, param_grid = pipe_params, cv=5, scoring='roc_auc', verbose=1)\nmodel_log = gs.fit(X_train['text'], y_train)\nprint(gs.best_score_)\n# gs.best_params_","de051820":"#Training score\ngs.score(X_train['text'], y_train)","bfc8102c":"#Testing score\ngs.score(X_test['text'], y_test)","32700428":"#Predicting using X_train\ny_train_preds = gs.predict(X_train['text'])\ny_train_preds","650859b2":"#Predicting using X_test\ny_test_preds = gs.predict(X_test['text'])\ny_test_preds","05497b91":"# Generate a confusion matrix\nconfusion_matrix(y_test, y_test_preds)","0097c8f6":"#generating confusion matrix post logistic regression\ntn, fp, fn, tp = confusion_matrix(y_test, y_test_preds).ravel()","88c76b6e":"print(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","5df1c6e3":"# Instantiate our model\nnb = MultinomialNB()","6bcbd45c":"# Fit our model!\nmodel_nb = nb.fit(X_train_cvec, y_train)","f29f8b2a":"# Generate our predictions!\npredictions = model_nb.predict(X_test_cvec)\npredictions","9c83a2e3":"# Score our model on the training set.\nmodel_nb.score(X_train_cvec, y_train)","9fa540cf":"# Score our model on the testing set.\nmodel_nb.score(X_test_cvec, y_test)","5f6467fb":"# Generate a confusion matrix.\nconfusion_matrix(y_test, predictions)","8947834d":"tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()","a9c3de97":"print(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","94ad3762":"#checking the test dataset\nkaggle_test.head()","bce6e12f":"#checking for nulls\nkaggle_test.isnull().sum()","18f49470":"#removing the keyword and location column\nkaggle_test.drop(['keyword','location'],axis=1,inplace=True)","685d6282":"#checking for nulls and data types\nkaggle_test.info()","a786d896":"#checking the dimension of the dataset\nkaggle_test.shape","ca6777b1":"#replacing certain words in the lemmatized text \nkaggle_test['text']=kaggle_test['text'].str.replace(r'amp','',regex=True)\nkaggle_test['text']=kaggle_test['text'].str.replace(r'\\d','',regex=True)\nkaggle_test['text']=kaggle_test['text'].str.replace(r'\u00db.*.*','',regex=True)\nkaggle_test['text']=kaggle_test['text'].str.lower()\nkaggle_test['text']=kaggle_test['text'].str.replace('\\#','',regex=True)","dfb09b70":"#tokenizing the title of working papers\nkaggle_test['text'] = kaggle_test['text'].apply(lambda x: tokenizer.tokenize(x))","330f4e29":"#applying the lemmatizer and checking the title column\nkaggle_test['text'] = kaggle_test['text'].apply(lambda x: word_lemmatizer(x))\nkaggle_test['text'].head()","b9ee04ce":"#joining all words in text column\nkaggle_text = \" \".join(text for text in kaggle_test['text'])","a3b97553":"#Create and generate a word cloud image\nwordcloud_kaggle_text = WordCloud(stopwords = stopwords, collocations=False,background_color=\"white\", max_words=150).generate(kaggle_text)\n\n# Display the generated image\nplt.imshow(wordcloud_kaggle_text, interpolation='bilinear',aspect=\"auto\");\nplt.axis(\"off\");","ade98636":"#Using Logistic Regression to make predictions for kaggle_test dataset\nkaggle_predictions_lr = model_log.predict(kaggle_test['text'])\nkaggle_predictions_lr","76d58169":"#Using NB model to make predcitions for kaggle_test dataset\nkaggle_cvec = cvec.transform(kaggle_test['text'])\nkaggle_cvec","ae637e60":"#converting sparse matrix to dense array\nkaggle_cvec = kaggle_cvec.todense()","25ee431a":"kaggle_predictions_nb = model_nb.predict(kaggle_cvec)","f919f226":"kaggle_predictions_nb","ce14c88f":"#empty dataframe\nsubmission_lr = pd.DataFrame()","903a7d12":"submission_lr['Id'] = kaggle_test.id\nsubmission_lr['target'] = kaggle_predictions_lr","0f8c6d43":"submission_lr.head()","0d4f6086":"# saving predictions in a csv file\nsubmission_lr.loc[ :].to_csv('final_kaggle_lr.csv',index=False)","a960de5a":"#empty dataframe\nsubmission_nb = pd.DataFrame()","1a00e1ea":"submission_nb['Id'] = kaggle_test.id\nsubmission_nb['target'] = kaggle_predictions_nb","36ed27c5":"submission_nb.head()","6991099e":"# saving predictions in a csv file\nsubmission_nb.loc[ :].to_csv('final_kaggle_nb.csv',index=False)","de58c2f8":"#### For Naive Bayes Predictions","b4b0825b":"### Test Dataset (for Kaggle)","f5fae44a":"#### Naive Bayes Model","5d0d43aa":"### Modeling","f9f79023":"#### For Logistic Regression predictions","a800fd5d":"### Submission","0dcc43c1":"#### Logistic Regression"}}