{"cell_type":{"04ee4482":"code","ddb1ea9c":"code","25bfa3a9":"code","a9f1f4c0":"code","0d5490a6":"code","deef8dcd":"code","cf8a6eef":"code","dd589002":"code","e5905aff":"code","8e56d867":"code","8ab66d6c":"code","22b3f2c1":"code","d37c4c50":"code","abaf4c37":"code","09609807":"code","4f2def2d":"code","41a70e23":"markdown","9b251844":"markdown"},"source":{"04ee4482":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ddb1ea9c":"# read data\ndata = pd.read_csv('\/kaggle\/input\/parkinsons-disease-data-set\/parkinsons.data')\ndata.head() #showing the first 5 rows","25bfa3a9":"data.info(),\ndata.shape","a9f1f4c0":"# inspecting the status column\ndata['status'].value_counts()\n# it will be a binary classification","0d5490a6":"# drop name column, it will not be used in the classification\ndata.drop(['name'], axis = 1,inplace = True)","deef8dcd":"#  get the all features except \"status\"\nX = data.loc[:, data.columns != 'status'] # values use for array format\n\n# get status values in array format\ny = data.loc[:, 'status']","cf8a6eef":"#There is imbalanced in the data, using SMOTE\nimport imblearn\n# Oversample and plot imbalanced dataset with SMOTE\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n# transform the dataset\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","dd589002":"y.value_counts()","e5905aff":"from sklearn.preprocessing import StandardScaler \nscaler=StandardScaler()\n#transform data\n# fit_transform() method fits to the data and then transforms it\nX = scaler.fit_transform(X)","8e56d867":"#  import train_test_split from sklearn. \nfrom sklearn.model_selection import train_test_split\n\n# split the dataset into training and testing sets with 20% of testings\nx_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.15)","8ab66d6c":"#Using XGBoost for the classification\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,classification_report\n","22b3f2c1":"# fitting the model\nclassifier = XGBClassifier()\nclassifier.fit(x_train, y_train) # fit with x and y train","d37c4c50":"#  Finnaly predict the model\n\ny_prediction = classifier.predict(x_test)\n\nprint(\"Accuracy Score is\", accuracy_score(y_test, y_prediction) * 100)","abaf4c37":"from sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier() # using GB to get the importance of each feature, then applying it on XGBoost\nmodel.fit(X,y) # fitting on all data for getting the feature importance\nimportances=model.feature_importances_","09609807":"from sklearn.feature_selection import SelectFromModel\n\nthreshold=sorted(model.feature_importances_ , reverse=False)\nfor thresh in threshold:\n    selection=SelectFromModel(model,threshold=thresh,prefit=True)\n    select_X_train=selection.transform(x_train)\n    select_model=XGBClassifier()\n    select_model.fit(select_X_train,y_train)\n    select_X_test=selection.transform(x_test)\n    y_p=select_model.predict(select_X_test)\n    acc=accuracy_score(y_test,y_p)\n    print(\"threshold %0.3f, n=%2d acc: %0.2f\" %(thresh,select_X_train.shape[1],acc*100))","4f2def2d":"\nselect=SelectFromModel(model,threshold=0.026,prefit=True) # threshold variable will be for the seven feature (0.026), so model will use only the first 6 features\nselect_X_train=select.transform(x_train)\n#select_model=XGBClassifier()\nselect_model.fit(select_X_train,y_train)\nselect_X_test=select.transform(x_test)\ny_predict=select_model.predict(select_X_test)\nacc=accuracy_score(y_test,y_predict)\nprint(classification_report(y_test,y_predict))","41a70e23":"# from the last cell, If we used the first 6 top ranked features , it will give 100% accuracy.\n# threshold 0.058, n= 6 acc: 100.00\n","9b251844":"##  Feature selection using Gradient Boosting"}}