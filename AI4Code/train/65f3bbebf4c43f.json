{"cell_type":{"5761ab5a":"code","ccd25b89":"code","5e53e105":"code","f612f1aa":"code","fb17d758":"code","eee7f3d8":"code","23f08740":"code","2323a5bd":"code","226332eb":"code","1fd4df74":"code","61d6e160":"code","a9d36e5d":"code","cb83557f":"code","688f0b79":"code","764b0869":"code","a2d79de1":"code","d84439a0":"code","4b15074b":"code","08f2f546":"code","7cffc054":"code","278990d5":"code","859dcf48":"code","19ab0f34":"code","2ca5322a":"code","f7c353b8":"code","6da9e771":"code","dea7f1a2":"code","fcd07250":"code","3247d95a":"code","cfe51894":"code","5a573a26":"code","7b32ab5c":"code","06008b63":"code","d6d70cca":"code","7b74245c":"code","7dfbc3a0":"code","381e736e":"code","8d157f0c":"code","1e46a6eb":"code","071ef31e":"code","4656bc06":"code","8281d294":"code","3b3c7c92":"code","b49705d0":"code","a39fe1f5":"code","7b52880c":"code","a913c00e":"code","d6e503ce":"code","dda422b1":"code","a7436847":"code","3a22921c":"code","de02c859":"code","54025abe":"code","75f45195":"code","c14c8b5c":"code","93997843":"code","2b9be15b":"code","f079dc84":"code","00a7df50":"code","cc7fa08b":"code","173f1e61":"code","8e43e31c":"code","4780d368":"code","997c1c16":"code","75ca9d49":"code","6e181b0d":"code","70276f86":"code","58b32aca":"code","b35de4c3":"code","4f6c37d4":"code","6546db24":"code","98e01f1e":"code","a5c23fc0":"code","7485ff6c":"code","000ddf62":"code","f4e30dea":"code","03f9f29b":"code","014c3755":"code","4fde0207":"code","55d5e7ba":"code","3ac1b1b7":"code","907a3c77":"code","27d0f4a1":"code","04c91665":"code","9caa961d":"code","1db938a8":"code","6dd69df4":"code","f136af9c":"code","fecfb4eb":"code","e2cdc1c8":"code","99b7392c":"code","2b2996d5":"code","a27cbe1f":"code","4d0954f0":"code","46da0c17":"code","f07dc757":"code","acde9d33":"code","a416611b":"code","173b8375":"code","2f52d492":"code","5545741d":"code","aae3150f":"code","4951c9da":"code","a29e885a":"code","be11151c":"code","a892e031":"markdown","7c021520":"markdown","6b2d3eee":"markdown","ecd8c9ac":"markdown","538b9887":"markdown","5274737e":"markdown","0c53afbd":"markdown","a3add659":"markdown","d76e6b7e":"markdown","2c3f4459":"markdown","18ce60dd":"markdown","3b2a349f":"markdown","3930dfb6":"markdown","0007fd3c":"markdown","5617db38":"markdown","3a2dc92d":"markdown","b6be34a4":"markdown","33e4ab31":"markdown","8654f2a1":"markdown","04fea217":"markdown","24702315":"markdown","5f105137":"markdown","e057b293":"markdown","83f264bc":"markdown","d33c018c":"markdown","4188e5dd":"markdown","1e1e9354":"markdown","13039586":"markdown","0cbed5b5":"markdown","38997540":"markdown","3934fb96":"markdown","13796331":"markdown","af9670f6":"markdown","a5cc275e":"markdown","925c5af9":"markdown","120701b9":"markdown","8a8a4ae8":"markdown","01bf1705":"markdown","61df41b5":"markdown","16bbc266":"markdown","4246eb87":"markdown","32a0be4f":"markdown","f80dbe52":"markdown","015efda3":"markdown","349db323":"markdown","fbfd7b3b":"markdown","76349d69":"markdown","d10e0941":"markdown","130a96a8":"markdown","0c7a838e":"markdown","069e350d":"markdown","a3467e53":"markdown","acbac1ac":"markdown","b555c4d7":"markdown","0b1fcbab":"markdown","bdcdabc4":"markdown","8e3b7810":"markdown","2cc2893a":"markdown","ab89858f":"markdown","b0036d87":"markdown","75806467":"markdown","2015e791":"markdown","72a409fb":"markdown","2589ed1d":"markdown","380876f1":"markdown","4055f8fc":"markdown","bf6fb074":"markdown","fe912c16":"markdown","0402b169":"markdown","80f34df3":"markdown"},"source":{"5761ab5a":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import log_loss, silhouette_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, cross_validate, cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tqdm import tqdm","ccd25b89":"input_dir = '\/kaggle\/input\/lish-moa'\ntrain_features = pd.read_csv(os.path.join(input_dir, 'train_features.csv'))\ntrain_targets_scored = pd.read_csv(os.path.join(input_dir, 'train_targets_scored.csv'))\ntrain_targets_nonscored = pd.read_csv(os.path.join(input_dir, 'train_targets_nonscored.csv'))\ntest_features = pd.read_csv(os.path.join(input_dir, 'test_features.csv'))","5e53e105":"train_features.shape, train_targets_scored.shape, train_targets_nonscored.shape, test_features.shape","f612f1aa":"cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n\nplt.figure(figsize=(16,4))\n\nfor idx, col in enumerate(cat_cols):\n    plt.subplot(int(f'13{idx + 1}'))\n    labels = train_features[col].value_counts().index.values\n    vals = train_features[col].value_counts().values\n    sns.barplot(x=labels, y=vals)\n    plt.xlabel(f'{col}')\n    plt.ylabel('Count')\nplt.tight_layout()\nplt.show()","fb17d758":"# select all indices when 'cp_type' is 'ctl_vehicle'\nctl_vehicle_idx = (train_features['cp_type'] == 'ctl_vehicle')\n\n# evaluate number of 1s we have in the total train scores when cp_type = ctl_vehicle\ntrain_targets_scored.loc[ctl_vehicle_idx].iloc[:, 1:].sum().sum()","eee7f3d8":"# take a copy of all our training sig_ids for reference\ntrain_sig_ids = train_features['sig_id'].copy()","23f08740":"# drop cp_type column since we no longer need it\nX = train_features.drop(['sig_id', 'cp_type'], axis=1).copy()\nX = X.loc[~ctl_vehicle_idx].copy()\n\ny = train_targets_scored.drop('sig_id', axis=1).copy()\ny = y.loc[~ctl_vehicle_idx].copy()\n\nX.shape, y.shape","2323a5bd":"X.head(3)","226332eb":"cat_feats = X.iloc[:, :2].copy()\nX_cell_v = X.iloc[:, -100:].copy()\nX_gene_e = X.iloc[:, 2:772].copy()","1fd4df74":"cat_feats.head(3)","61d6e160":"X_cell_v.head(3)","a9d36e5d":"X_gene_e.head(3)","cb83557f":"sns.distplot(X_cell_v)\nplt.show()","688f0b79":"sns.distplot(X_gene_e)\nplt.show()","764b0869":"cat_feats = X.iloc[:, :2].copy()\nX_cell_v = X.iloc[:, -100:].copy()\nX_gene_e = X.iloc[:, 2:772].copy()","a2d79de1":"def plot_features(X, y, selected_idx, features_type, figsize=(14,10)):\n    x_range = range(1, X.shape[1] + 1)\n    \n    fig = plt.figure(figsize=(14,10))\n    \n    for i, idx in enumerate(selected_idx):\n        ax = fig.add_subplot(selected_idx.shape[0], 1, i + 1)\n        vals = X.iloc[idx].values\n    \n        if (y.iloc[idx] == 1).sum():\n            output_labels = list(y.iloc[idx][y.iloc[idx] == 1].index.values)\n        \n            labels = \" \".join(output_labels)\n        else:\n            labels = \"None (all labels zero)\"\n        \n        sns.lineplot(x_range, vals)\n        plt.title(f\"Row {idx}, Labels: {labels}\", weight='bold')\n        plt.xlim(0.0, X.shape[1])\n        plt.grid()\n\n    plt.xlabel(f\"{features_type}\", weight='bold', size=14)\n    plt.tight_layout()\n    plt.show()\n    \n    \ndef plot_mean_std(dataframe, feature_name, features_type, figsize=(14,6), alpha=0.3):\n    \"\"\" Plot rolling mean and standard deviation for given dataframe \"\"\"\n    \n    plt.figure(figsize=figsize)\n    \n    x_range = range(1, dataframe.shape[1] + 1)\n    \n    chosen_rows = y.loc[y[feature_name] == 1]\n    chosen_feats = dataframe.loc[y[feature_name] == 1]\n    \n    means = chosen_feats.mean()\n    stds = chosen_feats.std()\n    \n    plt.plot(x_range, means, label=feature_name)    \n    plt.fill_between(x_range, means - stds, means + stds, \n                         alpha=alpha)\n\n    plt.title(f'{features_type}: {feature_name} - Mean & Standard Deviation', weight='bold')\n    \n    plt.xlim(0.0, dataframe.shape[1])\n    \n    plt.show()","d84439a0":"# lets plot some random rows from our data\nrandom_idx = np.random.randint(X.shape[0], size=(5,))\n\nplot_features(X_cell_v, y, random_idx, features_type='Cell Features')","4b15074b":"plot_features(X_gene_e, y, random_idx, features_type='Gene Features')","08f2f546":"# select an output label to plot associated training features\nchosen_label = 'btk_inhibitor'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,), replace=False)","7cffc054":"plot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","278990d5":"plot_mean_std(X_gene_e, 'btk_inhibitor', 'Gene Features')","859dcf48":"# select an output label to plot associated training features\nchosen_label = 'histamine_receptor_antagonist'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","19ab0f34":"plot_mean_std(X_gene_e, 'histamine_receptor_antagonist', 'Gene Features')","2ca5322a":"# select an output label to plot associated training features\nchosen_label = 'free_radical_scavenger'\nchosen_rows = y.loc[y[chosen_label] == 1]\nchosen_feats = X_gene_e.loc[y[chosen_label] == 1]\n\n# select random rows from those available above for the chosen label\nrandom_idx = np.random.choice(range(0, chosen_rows.shape[0]), size=(5,))\n\nplot_features(chosen_feats, chosen_rows, random_idx, features_type='Gene Features')","f7c353b8":"plot_mean_std(X_gene_e, 'free_radical_scavenger', 'Gene Features')","6da9e771":"X_sample = X.sample(10000, random_state=12)\nX_cell_v = X_sample.iloc[:, -100:].copy()\nX_gene_e = X_sample.iloc[:, 2:772].copy()\nX_cell_gene = X_sample.iloc[:, 2:].copy()","dea7f1a2":"k_range = [x for x in range(1, 25, 1)]","fcd07250":"%time k_kmeans = [KMeans(n_clusters=k, random_state=12).fit(X_cell_v) for k in k_range]\ninertias = [model.inertia_ for model in k_kmeans]","3247d95a":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, inertias)\nsns.scatterplot(k_range, inertias)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Inertia\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","cfe51894":"%time silhouette_scores = [silhouette_score(X_cell_v, model.labels_) for model in k_kmeans[1:]]","5a573a26":"plt.figure(figsize=(14, 6))\nsns.lineplot(k_range[1:], silhouette_scores)\nsns.scatterplot(k_range[1:], silhouette_scores)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Silhoutte Score\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","7b32ab5c":"cell_k = 10\nkmeans = KMeans(n_clusters=cell_k)\nkm_cell_feats = kmeans.fit_transform(X_cell_v)\nkmeans_cell_labels = kmeans.predict(X_cell_v)\n\nkm_cell_feats.shape, kmeans_cell_labels.shape","06008b63":"tsne = TSNE(verbose=1, perplexity=100, n_jobs=-1)\n%time X_cell_embedded = tsne.fit_transform(km_cell_feats)","d6d70cca":"# sns settings\nsns.set(rc={'figure.figsize':(14,10)})\npalette = sns.hls_palette(cell_k, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_cell_embedded[:,0], X_cell_embedded[:,1], \n                hue=kmeans_cell_labels, legend='full', palette=palette)\nplt.title('t-SNE on our Cell data with K-Means Clustered labels', weight='bold')\nplt.show()","7b74245c":"k_range = [x for x in range(1, 10)]\nk_range.extend([x for x in range(10, 21, 2)])\naic_scores = []\nbic_scores = []\n\nfor k in tqdm(k_range):\n    gm_k = GaussianMixture(n_components=k, n_init=10, random_state=12).fit(X_cell_v)\n    aic_scores.append(gm_k.aic(X_cell_v))\n    bic_scores.append(gm_k.bic(X_cell_v))","7dfbc3a0":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, aic_scores, color=\"tab:blue\", label='AIC')\nsns.scatterplot(k_range, aic_scores, color=\"tab:blue\")\n\nsns.lineplot(k_range, bic_scores, color=\"tab:green\", label='BIC')\nsns.scatterplot(k_range, bic_scores, color=\"tab:blue\")\n\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Information Criterion\", fontsize=14, weight='bold')\nplt.legend()\nplt.grid()\nplt.show()","381e736e":"print(f\"AIC minimum at {k_range[np.argmin(aic_scores)]} clusters.\")\nprint(f\"BIC minimum at {k_range[np.argmin(bic_scores)]} clusters.\")","8d157f0c":"k_range = [x for x in range(1, 25, 1)]\nk_range.extend([50, 100, 150, 200, 250])","1e46a6eb":"%time k_kmeans = [KMeans(n_clusters=k, random_state=12).fit(X_gene_e) for k in k_range]\ninertias = [model.inertia_ for model in k_kmeans]","071ef31e":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, inertias)\nsns.scatterplot(k_range, inertias)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Inertia\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 25.0)\nplt.show()","4656bc06":"%time silhouette_scores = [silhouette_score(X_gene_e, model.labels_) for model in k_kmeans[1:]]","8281d294":"plt.figure(figsize=(14, 6))\nsns.lineplot(k_range[1:], silhouette_scores)\nsns.scatterplot(k_range[1:], silhouette_scores)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Silhoutte Score\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","3b3c7c92":"gene_k = 6\nkmeans = KMeans(n_clusters=gene_k)\nkm_gene_feats = kmeans.fit_transform(X_gene_e)\nkmeans_gene_labels = kmeans.predict(X_gene_e)\n\nkm_gene_feats.shape, kmeans_gene_labels.shape","b49705d0":"tsne = TSNE(verbose=1, perplexity=100, n_jobs=-1)\n%time X_gene_embedded = tsne.fit_transform(km_gene_feats)","a39fe1f5":"# sns settings\nsns.set(rc={'figure.figsize':(14,10)})\npalette = sns.hls_palette(gene_k, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_gene_embedded[:,0], X_gene_embedded[:,1], \n                hue=kmeans_gene_labels, legend='full', palette=palette)\nplt.title('t-SNE with labels obtained from K-Means Clustering', weight='bold')\nplt.show()","7b52880c":"pca_tf_gene = PCA(n_components=0.90)\nX_gene_e_red = pca_tf_gene.fit_transform(X_gene_e)\nprint(f\"Original data: {X_gene_e.shape} \\nPCA Reduced data: {X_gene_e_red.shape}\")","a913c00e":"k_range = [x for x in range(1, 11)]\nk_range.extend([12, 15, 30, 50])\ngene_aic_scores = []\ngene_bic_scores = []\n\nfor k in tqdm(k_range):\n    gm_k = GaussianMixture(n_components=k, n_init=10, random_state=12).fit(X_gene_e_red)\n    gene_aic_scores.append(gm_k.aic(X_gene_e_red))\n    gene_bic_scores.append(gm_k.bic(X_gene_e_red))","d6e503ce":"plt.figure(figsize=(12, 5))\nsns.lineplot(k_range, gene_aic_scores, color=\"tab:blue\", label='AIC')\nsns.scatterplot(k_range, gene_aic_scores, color=\"tab:blue\")\n\nsns.lineplot(k_range, gene_bic_scores, color=\"tab:green\", label='BIC')\nsns.scatterplot(k_range, gene_bic_scores, color=\"tab:blue\")\n\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Information Criterion\", fontsize=14, weight='bold')\nplt.title(\"Gene Features Gaussian Mixture Model Clustering\")\nplt.legend()\nplt.grid()\nplt.show()","dda422b1":"aic_arr = np.array(gene_aic_scores)\nbic_arr = np.array(gene_bic_scores)\ntotal = aic_arr + bic_arr\nprint(f\"Cluster number with minimum sum of AIC and BIC: {k_range[np.argmin(total)]}\")","a7436847":"k_range = [x for x in range(1, 25, 1)]\nk_range.extend([50, 100, 150, 200, 250])","3a22921c":"%time k_kmeans = [KMeans(n_clusters=k, random_state=12).fit(X_cell_gene) for k in k_range]\ninertias = [model.inertia_ for model in k_kmeans]","de02c859":"plt.figure(figsize=(12, 6))\nsns.lineplot(k_range, inertias)\nsns.scatterplot(k_range, inertias)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Inertia\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 50.0)\nplt.show()","54025abe":"%time silhouette_scores = [silhouette_score(X_cell_gene, model.labels_) for model in k_kmeans[1:]]","75f45195":"plt.figure(figsize=(14, 6))\nsns.lineplot(k_range[1:], silhouette_scores)\nsns.scatterplot(k_range[1:], silhouette_scores)\nplt.xlabel(\"Clusters, $k$\", fontsize=14, weight='bold')\nplt.ylabel(\"Silhoutte Score\", fontsize=14, weight='bold')\nplt.grid()\nplt.xlim(0.0, 15.0)\nplt.show()","c14c8b5c":"combined_k = 4\nkmeans = KMeans(n_clusters=combined_k)\nkm_comb_feats = kmeans.fit_transform(X_cell_gene)\nkmeans_comb_labels = kmeans.predict(X_cell_gene)","93997843":"tsne = TSNE(verbose=1, perplexity=100, n_jobs=-1)\n%time X_comb_embedded = tsne.fit_transform(km_comb_feats)","2b9be15b":"# sns settings\nsns.set(rc={'figure.figsize':(14,10)})\npalette = sns.hls_palette(combined_k, l=.4, s=.8)\n\n# plot t-SNE with annotations from k-means clustering\nsns.scatterplot(X_comb_embedded[:,0], X_comb_embedded[:,1], \n                hue=kmeans_comb_labels, legend='full', palette=palette)\nplt.title('t-SNE with labels obtained from K-Means Clustering', weight='bold')\nplt.show()","f079dc84":"pca_gene = PCA(n_components=0.99)\npca_combined = PCA(n_components=0.99)\n\nX_gene_e_rd = pca_gene.fit_transform(X_gene_e)\nX_cell_gene_rd = pca_combined.fit_transform(X_cell_gene)\n\nX_gene_e_rd.shape, X_cell_gene_rd.shape","00a7df50":"tsne_cell = TSNE(verbose=1, perplexity=100, n_jobs=-1)\ntnse_gene = TSNE(verbose=1, perplexity=100, n_jobs=-1)\ntnse_combined = TSNE(verbose=1, perplexity=100, n_jobs=-1)","cc7fa08b":"%time X_cell_v_tsne = tsne_cell.fit_transform(X_cell_v)","173f1e61":"%time X_gene_e_tsne = tnse_gene.fit_transform(X_gene_e_rd)","8e43e31c":"%time X_cell_gene_tsne = tnse_gene.fit_transform(X_gene_e_rd)","4780d368":"fig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(1, 3, 1)\nsns.scatterplot(X_cell_v_tsne[:,0], X_cell_v_tsne[:,1], legend='full')\nax.set_title('Cell Features t-SNE', weight='bold')\n\nax = fig.add_subplot(1, 3, 2)\nsns.scatterplot(X_gene_e_tsne[:,0], X_gene_e_tsne[:,1], legend='full', color='tab:orange')\nax.set_title('Gene Features t-SNE', weight='bold')\n\nax = fig.add_subplot(1, 3, 3)\nsns.scatterplot(X_cell_gene_tsne[:,0], X_cell_gene_tsne[:,1], legend='full', color='tab:red')\nax.set_title('Combined Gene and Cell Features t-SNE', weight='bold')\nplt.show()","997c1c16":"pre_dbs_gene_pca = PCA(n_components=10)\npre_dbs_cell_pca = PCA(n_components=10)\npre_dbs_comb_pca = PCA(n_components=10)\n\ncell_reduced = pre_dbs_gene_pca.fit_transform(X_gene_e)\ngene_reduced = pre_dbs_cell_pca.fit_transform(X_cell_v)\ncombined_reduced = pre_dbs_comb_pca.fit_transform(X_cell_gene)","75ca9d49":"dbscan_cell = DBSCAN(eps=13, min_samples=5)\ndbscan_cell.fit(cell_reduced)\nnp.unique(dbscan_cell.labels_, return_counts=True)","6e181b0d":"dbscan_gene = DBSCAN(eps=3, min_samples=4)\ndbscan_gene.fit(gene_reduced)\nnp.unique(dbscan_gene.labels_, return_counts=True)","70276f86":"dbscan_comb = DBSCAN(eps=3, min_samples=5)\ndbscan_comb.fit(combined_reduced)\nnp.unique(dbscan_comb.labels_, return_counts=True)","58b32aca":"fig = plt.figure(figsize=(17,6))\ncell_palette = sns.hls_palette(len(np.unique(dbscan_cell.labels_)), l=.4, s=.8)\nax = fig.add_subplot(1, 3, 1)\nsns.scatterplot(X_cell_v_tsne[:,0], X_cell_v_tsne[:,1], \n                hue=dbscan_cell.labels_, legend='full', palette=cell_palette)\nax.set_title('Cell t-SNE & DBSCAN Clusters', weight='bold')\n\nax = fig.add_subplot(1, 3, 2)\ngene_palette = sns.hls_palette(len(np.unique(dbscan_gene.labels_)), l=.4, s=.8)\nsns.scatterplot(X_gene_e_tsne[:,0], X_gene_e_tsne[:,1], color='tab:orange',\n                hue=dbscan_gene.labels_, legend='full', palette=gene_palette)\nax.set_title('Gene t-SNE & DBSCAN Clusters', weight='bold')\n\nax = fig.add_subplot(1, 3, 3)\ncomb_palette = sns.hls_palette(len(np.unique(dbscan_comb.labels_)), l=.4, s=.8)\nsns.scatterplot(X_cell_gene_tsne[:,0], X_cell_gene_tsne[:,1], color='tab:red',\n                hue=dbscan_comb.labels_, legend='full', palette=comb_palette)\nax.set_title('Combined Gene and Cell t-SNE & DBSCAN Clusters', weight='bold')\nplt.tight_layout()\nplt.show()","b35de4c3":"# standardise our numerical features data prior to clustering\nstd_scaler = StandardScaler()\nX.iloc[:, 2:] = std_scaler.fit_transform(X.iloc[:, 2:].values)","4f6c37d4":"cell_kmeans = KMeans(n_clusters=4)\ngene_kmeans = KMeans(n_clusters=4)\ncomb_kmeans = KMeans(n_clusters=4)","6546db24":"# one hot encode our categorical features\nX_cats = X.iloc[:, :2].copy()\nX_cats['cp_time'] = X_cats['cp_time'].astype('object')\nX_cats = pd.get_dummies(X_cats)\n\n# obtain our splits for gene and cell data\nX_cell_gene = X.iloc[:, 2:].copy()\nX_cell = X.iloc[:, -100:].copy()\nX_gene = X.iloc[:, 2:772].copy()\n\nX_cats.shape, X_cell_gene.shape, X_cell_v.shape, X_gene_e.shape","98e01f1e":"%time X_cell_rd = cell_kmeans.fit_transform(X_cell)","a5c23fc0":"%time X_gene_rd = gene_kmeans.fit_transform(X_gene)","7485ff6c":"%time X_cell_gene_rd = comb_kmeans.fit_transform(X_cell_gene)","000ddf62":"X_cell_rd.shape, X_gene_rd.shape, X_cell_gene_rd.shape","f4e30dea":"# combine all of our features into one\ncat_feats = list(X_cats.columns.values)\ncell_feats = [f\"cell_clust_{x}\" for x in range(1, X_cell_rd.shape[1] + 1)]\ngene_feats = [f\"gene_clust_{x}\" for x in range(1, X_gene_rd.shape[1] + 1)]\ncombined_feats = [f\"cell_gene_clust_{x}\" for x in range(1, X_cell_gene_rd.shape[1] + 1)]\n\ncombined = np.c_[X_cats, X_cell_rd, X_gene_rd, X_cell_gene_rd]\nX_all_rd = pd.DataFrame(combined, columns=cat_feats + cell_feats + gene_feats + combined_feats)\nX_all_rd.head(3)","03f9f29b":"original = np.c_[X_cats, X_cell, X_gene]\nX_original = pd.DataFrame(original, columns= cat_feats + \n                          list(X_cell.columns.values) + \n                          list(X_gene.columns.values))\nX_original.shape","014c3755":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_0 = cross_val_predict(lin_reg, X_original, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_0))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","4fde0207":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_1 = cross_val_predict(lin_reg, X_all_rd, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_1))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","55d5e7ba":"lr_model_1 = LinearRegression().fit(X_all_rd, y)","3ac1b1b7":"#et_clf = ExtraTreesClassifier(n_jobs=-1)\n#%time et_val_preds = cross_val_predict(et_clf, X_all_rd, y, cv=3)","907a3c77":"# in order to effective work out log loss, we need to flatten both arrays before computing log loss\n#et_log_loss = log_loss(np.ravel(y), np.ravel(et_val_preds))\n#print(f\"Log loss for Extra Trees Classifier: {et_log_loss:.5f}\\n\")","27d0f4a1":"all_combined = np.c_[X_cats, X_cell, X_gene, X_cell_rd, X_gene_rd]\nX_extended = pd.DataFrame(all_combined, columns=(cat_feats + list(X_cell.columns.values) +\n                                                 list(X_gene.columns.values)+ cell_feats + gene_feats))\n\nX_extended.shape","04c91665":"X_extended.head(3)","9caa961d":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_2 = cross_val_predict(lin_reg, X_extended, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_2))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","1db938a8":"clustered = np.c_[X_cats, X_cell_rd, X_gene_rd]\nX_clustered = pd.DataFrame(clustered, columns= cat_feats + cell_feats + gene_feats)\n\nX_clustered.shape","6dd69df4":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_3 = cross_val_predict(lin_reg, X_clustered, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_3))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","f136af9c":"lr_model_2 = LinearRegression().fit(X_clustered, y)","fecfb4eb":"clustered = np.c_[X_cats, X_cell_gene_rd]\nX_clustered = pd.DataFrame(clustered, columns= cat_feats + combined_feats)\n\nX_clustered.shape","e2cdc1c8":"# evaluate using cross-validation\nlin_reg = LinearRegression()\nlr_val_preds_4 = cross_val_predict(lin_reg, X_clustered, y, cv=5)\n\n# in order to effective work out log loss, we need to flatten both arrays before computing log loss\nlr_log_loss = log_loss(np.ravel(y), np.ravel(lr_val_preds_4))\nprint(f\"Log loss for our Linear Regression Model: {lr_log_loss:.5f}\\n\")","99b7392c":"lr_model_3 = LinearRegression().fit(X_clustered, y)","2b2996d5":"avg_val_preds = (lr_val_preds_1 + lr_val_preds_3 + lr_val_preds_4) \/ 3.0","a27cbe1f":"# in order to effective work out log loss, we need to flatten both arrays before computing log loss\ncomb_log_loss = log_loss(np.ravel(y), np.ravel(avg_val_preds))\nprint(f\"Log loss for our Linear Regression Model: {comb_log_loss:.5f}\\n\")","4d0954f0":"# take a copy of all our training sig_ids for reference\ntest_sig_ids = test_features['sig_id'].copy()\n\n# select all indices when 'cp_type' is 'ctl_vehicle'\ntest_ctl_vehicle_idx = (test_features['cp_type'] == 'ctl_vehicle')","46da0c17":"X_test = test_features.drop(['sig_id', 'cp_type'], axis=1).copy()\n\n# standardise our test set numerical features\nX_test.iloc[:, 2:] = std_scaler.fit_transform(X_test.iloc[:, 2:].values)","f07dc757":"X_test_cat = X_test.iloc[:, :2].copy()\nX_test_cat['cp_time'] = X_test_cat['cp_time'].astype('object')\nX_test_cat = pd.get_dummies(X_test_cat)\n\nX_test_cell = X_test.iloc[:, -100:].copy()\nX_test_gene = X_test.iloc[:, 2:772].copy()\nX_test_cell_gene = X_test.iloc[:, 2:].copy()\n\nX_test_cat.shape, X_test_cell.shape, X_test_gene.shape, X_test_cell_gene.shape","acde9d33":"X_test_cell_rd = cell_kmeans.transform(X_test_cell)\nX_test_gene_rd = gene_kmeans.transform(X_test_gene)\nX_test_cell_gene_rd = comb_kmeans.transform(X_test_cell_gene)","a416611b":"# combine all of our features into one\ntest_combined = np.c_[X_test_cat, X_test_cell_rd, X_test_gene_rd, X_test_cell_gene_rd]\nX_test_1 = pd.DataFrame(test_combined, columns=cat_feats + cell_feats + gene_feats + combined_feats)\n\n# make predicts on this data using model 1 (trained previously)\nmodel_1_preds = lr_model_1.predict(X_test_1)","173b8375":"test_clustered = np.c_[X_test_cat, X_test_cell_rd, X_test_gene_rd]\nX_test_2 = pd.DataFrame(test_clustered, columns= cat_feats + cell_feats + gene_feats)\n\n# make predicts on this data using model 2 (trained previously)\nmodel_2_preds = lr_model_2.predict(X_test_2)","2f52d492":"test_clust_comb = np.c_[X_test_cat, X_test_cell_gene_rd]\nX_test_3 = pd.DataFrame(test_clust_comb, columns= cat_feats + combined_feats)\n\n# make predicts on this data using model 3 (trained previously)\nmodel_3_preds = lr_model_3.predict(X_test_3)","5545741d":"test_preds = (model_1_preds + model_2_preds + model_3_preds) \/ 3.0\ntest_preds.shape","aae3150f":"# change all cp_type == ctl_vehicle predictions to zero\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values] = 0\n\n# confirm all values now sum to zero for these instances\ntest_preds[test_sig_ids[test_ctl_vehicle_idx].index.values].sum()","4951c9da":"# we have some values above 1 and below 0 - this needs amending since probs should only be 0-1\ntest_preds[test_preds > 1.0] = 1.0\ntest_preds[test_preds < 0.0] = 0.0\n\n# confirm these values are all corrected\ntest_preds.max(), test_preds.min()","a29e885a":"test_preds = pd.DataFrame(test_preds, columns=train_targets_scored.columns.values[1:])\ntest_submission = pd.DataFrame({'sig_id' : test_sig_ids})\ntest_submission[test_preds.columns] = test_preds\ntest_submission.head(3)","be11151c":"# save our submission as csv\ntest_submission.to_csv('submission.csv', index=False)","a892e031":"We now need to update all of the predictions for cp_type == ctl_vehicle so that they are zero.","7c021520":"#### Model 3 - Only combined cell and gene clustered features","6b2d3eee":"<a id=\"clustering-two\"><\/a>\n## 4. DBSCAN Clustering on our data","ecd8c9ac":"#### Visualisation of combined clusters using t-SNE","538b9887":"### Case 1: All clustered features together","5274737e":"The log loss is actually lowest when we use a smaller subset of clustered features.","0c53afbd":"### 4.2 Applying DBSCAN to our data","a3add659":"For our gene data, a good value of k clusters to choose could be around 8 in this case, since it provides a good compromise between both BIC and AIC.","d76e6b7e":"### 3.3 KMeans Clustering on combined cell and gene data using KMeans","2c3f4459":"There is a clear elbow located at k=2, which represents the optimal value of k to choose in this case. In general however, this might not always be the best choice, but somewhere around this point is usually a good start. We could experiment with both k=2 and k=3 and see what yields better results. \n\nWe can also evaluate this further using the silhouette score, which in practice can be a more effective technique. The only downside is the computational complexity, which we need to consider carefully if we want to evaluate a wide range of k values.","18ce60dd":"### Case 3: Only the individual clustered columns","3b2a349f":"### Case 5: Ensemble of our different clustered linear regression models","3930dfb6":"#### Plotting all gene \/ cell features for random samples:","0007fd3c":"#### Visualisation of these clusters using t-SNE","5617db38":"### 6.1 Preprocess our test set as required","3a2dc92d":"### 3.1 Clustering and exploring Cell features using KMeans","b6be34a4":"### Case 0 (Benchmark) - Linear Regression model on original processed features","33e4ab31":"In the case of our linear regressor, performance is actually worse when we include the additional features from the original dataframe. It's likely a simple linear regression model is not complex enough to exploit the large number of features effectively.","8654f2a1":"Unfortunately, our DBSCAN results were not great in this case. Perhaps the chosen method of dimensionality reduction using PCA was not a great choice, and has resulted in the poor results we see above. Better choices could be randomised and\/or non-dimensional forms of clustering, such as kernal PCA or spectral clustering.","04fea217":"We have some noticeable peaks throughout the features for some of the above instances. It could be worth plotting a range of data instances with the same output labels against one another, and compare their peaks. If they correlate in one or more areas, this could be insightful for developing further features with our dataset.\n\nLets now repeat above, but for data instances with the same output label(s).","24702315":"To speed up our clustering significantly, we'll only use a random subset of the total data, else it will take an extremely long time for some of our exploration, e.g. Gaussian Mixture models.","5f105137":"#### Average our individual model predictions into one final set","e057b293":"### 3.2 Clustering and exploring Gene features using KMeans","83f264bc":"Lets also look at the mean and standard deviation of this feature:","d33c018c":"Lets quickly assess how our cell data looks when plotted over all features for random instances:","4188e5dd":"<a id=\"test-predictions\"><\/a>\n## 6. Test set predictions","1e1e9354":"### Case 4: Only combined cell and gene clustered features","13039586":"#### Model 1 - All clustered features together","0cbed5b5":"Clearly some rows vary substancially in terms of their value range, and therefore it is worth standardising this data prior to training our models.\n\nNow lets do the same for our gene features:","38997540":"This log loss is much better than our original log loss on the entire dataset (Case 0 above).\n\nLets see how a tree based classifier does (extra trees classifier in this case):","3934fb96":"Transform our test set using the kmean clusters found earler:","13796331":"### Case 2: Clustered Features with Original Features","af9670f6":"We'll now combine the original features (one-hot encoded cat columns, unclustered cell and gene data, combined with the clustered cell and gene data).\n\nDue to the large number of dimensions of this case, we'll just experiment with a quick linear regression model:","a5cc275e":"---","925c5af9":"As an experiment we'll perform DBSCAN clustering on our data, and visualise our clusters on a t-SNE 2-D projection of our dimensionality reduced data (obtained using PCA for convenience).","120701b9":"---","8a8a4ae8":"First of all, lets reduce the dimensionality of our gene and combined data (since they are high-dimensional), and transform it to 2-dimensions using t-SNE:","01bf1705":"Since we dont want to use too few clusters (we need a lot of information to provide insights for the 206 output classes), we can compromise with this and select around 4 clusters.","61df41b5":"---","16bbc266":"### 4.1 Dimensionality reduction using PCA and t-SNE","4246eb87":"<a id=\"EDA\"><\/a>\n## 2. Basic Exploratory Data Analysis","32a0be4f":"#### Optional extra exploration for interest - Gaussian Mixture Model estimation\n\nSimilarly to previous, lets assess the performance of clustering using a Gaussian Mixture model.","f80dbe52":"Now lets reduce these to 2-dimensions using t-SNE:","015efda3":"Now lets apply DBSCAN to our data and attempt to cluster it. The issue with high-dimensional data is that as our dimensionality grows, the more everything tends to becoming an outlier, which is referred to as the curse of dimensionality. This is especially true for density-based techniques such as DBSCAN, and so reducing our dimensions to a lower number of features first is generally required. If we dont do this, we'll end up with an unreasonable number of outliers within our results, regardless of the amount we tweak the epsilon and number of sample parameters.\n\nWe'll apply basic PCA first to provide us with a low number of dimensions, and then apply DBSCAN. Rather than keeping 90%-95% variance like above, we'll have to reduce this considerably further, since density estimations can struggle significantly after around 10 dimensions.","349db323":"<a id=\"clustering-one\"><\/a>\n## 3. Clustering of our splits of features","fbfd7b3b":"For 'cp_type', the 'ctl_vehicle' refers to samples treated with a control perturbation. For control perturbations, our targets are all zero, since they have no Mechanism of Action (MoA).\n\nTo deal with this, a good strategy could be to identify samples that are ctl_vehicle (through training a classification model or simply using the feature as its in the test data!), and set all of these to zero. We can then process the test set accordingly, by first setting all test instance targets to zero if its a ctl_vehicle, followed by processing all of the others normally using our trained model.","76349d69":"<a id=\"imports\"><\/a>\n## 1. Import dependencies and data","d10e0941":"---","130a96a8":"#### Visualisation of these clusters using T-SNE","0c7a838e":"This analysis highlights the potential for performing advanced feature engineering, such as using the trends of gene and\/or cell features as additional features to our models. We could use such features to supplement the existing data in its standard form. We could also investigate the relationships of our unsupervised work to these types of trends for different features.","069e350d":"This performance is best overall combined to either of the attempts above. It's worth repeating this work for the test set and making a prediction accordingly.","a3467e53":"#### Model 2 - Only individual clustered columns","acbac1ac":"Overall, I think KMeans struggles to cluster our data effectively and into any meaningful splits. We could probably do better through applying a more complex clustering algorithm, such as a variation of Kernal PCA, or spectral clustering. Despite this, we'll use this work with KMeans clustering to produce a basic pipeline and compare how it impacts \/ improves our performance on the given problem.","b555c4d7":"We'll create a basic pipeline that combines our clustered (dimensionally reduced) features. For clarity, this will contain the clustered features from the cell data, the clustered features from the gene data, and also the clustered data from a combination both combined (which may produce additional different clusters than either alone).\n\nWith this data, we can perform evaluate of performance with the following configurations:\n\n- Original Data cross-validation\n- Individual Clustered data cross-validation\n- Combined clustered data cross-validation\n- Original data + variations of the clustered data\n\nWe'll use the optimal clusters for each of these as identified previously.","0b1fcbab":"The computational time increases significantly as the number of clusters increases in this case.","bdcdabc4":"As we can see, the Bayesian Information Criterion (BIC) penalises model complexity much more, which leads to BIC steadily increasing as we increase from 2 clusters. AIC on the other hand, continues to improve as we increase the number of clusters. \n\nFrom these criteria alone, it is not straightforward to choose the optimal number of clusters in this case.","8e3b7810":"# Unsupervised Learning and Exploration of MoA Gene and Cell Features","2cc2893a":"Lets transform each of these splits accordingly. We'll one-hot encode the categorical features, and cluster the numerical with the optimal cluster numbers found above:","ab89858f":"This notebook attempts to explore the MoA data through various clustering and unsupervised learning techniques. It then follows up on this by making a set of predictions on the test set for this multi-label classification problem.\n\nIn its default state, the dataset has a large number of dimensions, many of which are redundant and potentially related to one-another. Discovering of insights through clustering and unsupervised techniques can be useful for performing feature engineering and discovering hidden relationships within our data. During the basic work in this notebook, we isolate both the gene and cell data, and investigate the optimal number of clusters using common clustering techniques, such as KMeans, DBSCAN, t-SNE (for visualisation), and Gaussian Mixture Models. \n\nAfter performing this exploration, some simple linear models are produced and evaluated in terms of their performance with different sub-sets of clustered features. This could be extended to many different model types, but for the purpose of this short notebook only one simple model is tested.\n\n**Table of Contents:**\n\n1. [Imports](#imports)\n2. [EDA](#EDA)\n3. [KMeans Clustering and t-SNE Visualisation](#clustering-one)\n4. [PCA, t-SNE and DBSCAN Clustering](#clustering-two)\n5. [Model Production and Evaluation](#model-production)\n6. [Test Set Predictions](#test-predictions)","b0036d87":"We also have many values outside the range of 0 and 1, since we've used a regression model. Since our output results should be probabilities, we need to set any values greater than 1 to 1, and any negative values to zero.","75806467":"The total sum is zero, which confirms the statement above on all targets being zero for cases where cp_type is ctl_vehicle. The best thing to do with this is simply fill our targets for zero when this is the case.\n\nWe shall also remove all of these from the training set, since there is no need to unnecessarily complicate our model.","2015e791":"---","72a409fb":"Lets combine all of our previous models together (except Case 2 which was poor), and see how well it fares:","2589ed1d":"The log loss appears to be much worse on our extra trees classifier in this case. We've likely reduced too much information from our data as a result of the clustering performed, which has a tendency to reduce the performance of more complex model types such as random forests, gradient boosting and deep neural networks.","380876f1":"Lets repeat this process for some different output labels:","4055f8fc":"#### Optional extra exploration for interest - Fitment of a Gaussian Mixture Model to our data\n\nLets also try and fit a Gaussian Mixture model to our data. This is more difficult due to the very slow computation time, and so its essential that we use only our subset of data, rather than the entire training set.","bf6fb074":"AIC appears to keep decreasing after 4 clusters, but not at a significant amount. You can see that the rate at which it decreases slows considerably after it has reached 4 clusters. In addition, the Bayesian Information Criterion (BIC), seems to have the best score at 4 clusters, and then worsens as we increase clusters beyond this amount.\n\nThus, 4 clusters is probably a reasonable initial choice for the number of clusters in our model in this case.","fe912c16":"<a id=\"model-production\"><\/a>\n## 5. Basic Pipeline and Evaluation of Models with clustered features","0402b169":"### 6.2 Form our model variations and make predictions","80f34df3":"#### Final tuning of our predictions"}}