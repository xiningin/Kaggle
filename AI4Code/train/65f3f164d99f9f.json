{"cell_type":{"3c7a98c0":"code","b240169d":"code","1b945124":"code","0bb2ac73":"code","089d8a78":"code","325136ba":"code","c5033c6d":"code","1b42c96b":"code","8b2a8c89":"code","78223dd5":"code","1551bb07":"code","75f80892":"code","63570f96":"code","dd1d8e21":"code","93d936d8":"code","65f8c358":"code","86eda6f2":"code","f0b20948":"code","6972f840":"code","6dd259a1":"code","a7ea0835":"code","5df957b3":"code","f92db26d":"code","3a65035f":"code","ef2f202f":"code","7cbd129c":"code","4319cc2a":"code","ba260e8a":"code","e4547643":"code","be30315b":"code","c061494e":"code","9f203db5":"code","bc119b31":"code","8686fa0b":"code","54b8d403":"code","c0adde13":"code","99caec25":"code","520496ad":"code","15419bc3":"code","9ebc0fe4":"code","dbec751d":"code","3900d0ca":"code","78cf0ec6":"code","57abc1c7":"code","036f028b":"code","ca4c25e1":"code","7c975f84":"code","c3988283":"code","64770f5c":"code","c5137334":"code","a96309dc":"code","ca100ff9":"code","29052b8c":"code","267fb27f":"code","d5650fc0":"code","aa4e418d":"code","250965df":"code","271c7ffa":"code","e837a7ae":"code","eb64ee50":"code","db3801d7":"code","a50fa461":"code","a58d31e8":"code","3b96a3d5":"code","2dc91728":"code","9ffa1603":"code","0c537da9":"code","bd28e43e":"code","f7c53e7f":"code","e92075ce":"code","bfb73ffc":"code","e593c4ce":"code","3572c526":"code","40f3d955":"code","ee913ef6":"code","59b727ec":"code","40afc31c":"code","a407e300":"code","10b673f4":"code","20b7b81b":"code","13845ebe":"code","3dd78b40":"code","a602242c":"code","30d5755b":"code","05829720":"code","f3fc0371":"code","877f093e":"code","c42056de":"code","5f3fb6b4":"code","3bf64a3f":"code","7a13cbab":"code","4e610a55":"code","634e6588":"code","6dad1ff8":"code","64ef9ac2":"code","2e6989b7":"code","e78d2f54":"code","358fa287":"code","1bf6964a":"code","1170b508":"code","b0569ffe":"code","aff74169":"code","ea8c7ddd":"code","9d4ebded":"code","3d309289":"code","4065b4c0":"code","2dffb2b4":"code","03c132a4":"code","4eb3a7ec":"code","57c0c56c":"code","a8339f09":"code","5f14062b":"code","ee9d2e15":"code","a62a18eb":"code","27b219dc":"code","060160fe":"code","5dbae430":"code","90cf3b2c":"code","4189a0a4":"code","7363b2b5":"code","61cb9859":"code","7aa9b36a":"code","b24dabff":"code","70af8e01":"code","b37b9240":"code","0aeb1876":"code","4614d680":"code","edf2dc37":"code","c7157476":"code","d64b0c4d":"code","625c09c4":"code","5bf44e1f":"code","2f9fad52":"code","b3e9e701":"code","1a69da29":"code","86df5eea":"code","f3c62bc8":"code","71535eae":"code","d64bbc9b":"code","a8692161":"code","21c353e7":"code","4d119549":"code","b9f7e695":"code","f3346bd9":"code","c94c008f":"code","c0e3e8d4":"code","06fecb18":"code","a4ba7fb4":"code","1b6b1b1a":"code","3ff5253b":"code","e8e6df13":"code","492594cd":"code","7c157147":"code","4fe7956e":"code","73d32e22":"code","af452949":"code","e7d351c0":"code","08a90799":"code","2b500ccb":"code","f6a66a6f":"code","92d960bc":"code","83bdaead":"code","f932edaf":"code","17418192":"code","0743a66f":"code","a3eebe75":"code","080cc460":"code","2e41e0b5":"code","b4366856":"code","0f581c65":"code","71cf1d98":"code","472402ec":"markdown","3f9b4a2d":"markdown","29e53065":"markdown","1b5e844e":"markdown","efc9e545":"markdown","ccc24430":"markdown","3c1d706f":"markdown","97082345":"markdown","c6726be6":"markdown","39822eed":"markdown","33ff8c26":"markdown","8e25bce2":"markdown","2a49b434":"markdown","91ca8ab8":"markdown","e2b2815e":"markdown","5f859c32":"markdown","4dded8eb":"markdown","7663785b":"markdown","b5d83fb7":"markdown","81a0f03b":"markdown","3cc3de1d":"markdown","ea9bd36a":"markdown","97f6c9ae":"markdown","f44fc807":"markdown","efca9143":"markdown","677f2193":"markdown","d7e9ddd8":"markdown","bff0acd1":"markdown","fcaf9cbd":"markdown","4a2440f0":"markdown","3fb3010c":"markdown","b70c7673":"markdown","0b02af12":"markdown","9f085638":"markdown","080576f3":"markdown","e6cda4a8":"markdown","99dc4293":"markdown","96cd0c6c":"markdown","2d3ba1bd":"markdown","9389bb3f":"markdown","671d7640":"markdown","b68bb541":"markdown","655fa779":"markdown","17ba1c62":"markdown","abe6d0e9":"markdown","96f0910d":"markdown","7ad8175f":"markdown","f7349557":"markdown","997bf0dc":"markdown","5bc559ff":"markdown","5547f851":"markdown","598b6d39":"markdown","650a27ad":"markdown","44a4c7c2":"markdown","7b1372b8":"markdown","e7b1796c":"markdown","b5cc92d1":"markdown","aa34440a":"markdown","59f2a3c9":"markdown","08a2fa3d":"markdown","99981b36":"markdown","0bdaec50":"markdown","86623dae":"markdown","f371ab4e":"markdown","1f198259":"markdown","9a4b5a37":"markdown","5d73eba9":"markdown","cff49e27":"markdown"},"source":{"3c7a98c0":"import numpy as np\nimport pandas as pd\n\nimport datetime\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score \nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom xgboost import XGBClassifier\n\n# !pip install imblearn\nfrom imblearn.over_sampling import SMOTE\n\nfrom IPython.display import Image\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b240169d":"df=pd.read_csv('..\/input\/noshowappointments\/KaggleV2-May-2016.csv')","1b945124":"df.head()","0bb2ac73":"df.rename(columns={'Hipertension': 'Hypertension', 'Handcap': 'Handicapped' }, inplace=True)","089d8a78":"df.shape","325136ba":"df.tail()","c5033c6d":"df.describe(include='all')","1b42c96b":"df.dtypes","8b2a8c89":"col_names=['PatientId','AppointmentID', 'Gender','Neighbourhood','Scholarship', 'Hypertension','Diabetes','Alcoholism','Handicapped', 'SMS_received', 'No-show']\ndf[col_names] = df[col_names].astype('category')","78223dd5":"df.dtypes","1551bb07":"#Number of unique patientID's\nprint(\"The number of unique Patient ID's in the  data set\")\nprint(df['PatientId'].nunique())\n\n#Number of unique AppointmentID's\nprint(\"The number of unique Appointment ID's in the  data set\")\nprint(df['AppointmentID'].nunique())","75f80892":"df=df.drop(['AppointmentID'], axis=1)","63570f96":"df.dtypes","dd1d8e21":"df.Age.value_counts(sort=True, ascending=True)","93d936d8":"df.loc[df.Age < 0, ['Age']] = 0","65f8c358":"df['Age'].describe()","86eda6f2":"df['Age'].hist(figsize=(10, 5));","f0b20948":"def getProbOfNoShow(col, target, df, forGraph=True):\n    crosstab = pd.crosstab(index = df[col], columns = df[target])\n    crosstab.columns = pd.Index(list(crosstab.columns))\n    crosstab = crosstab.reset_index() \n    crosstab['TotalCount'] = crosstab['Yes'] + crosstab['No']\n    crosstab['probNoShowUp'] = crosstab['Yes'] \/ crosstab['TotalCount']\n    \n    print(crosstab.head())\n    if forGraph:\n        return crosstab[[col, 'probNoShowUp']]\n    else:\n        return crosstab[[col, 'TotalCount', 'probNoShowUp']]","6972f840":"data = getProbOfNoShow('Age', 'No-show', df)","6dd259a1":"sns.lmplot(data = data, x = 'Age', y = 'probNoShowUp', fit_reg = True)\nplt.xticks(np.arange(np.min(df['Age']), np.max(df['Age']), 10))\nplt.title('Probability of Not Showing up with respect to Age')\nplt.show()","a7ea0835":"### Bining the age column based on above plot\n\nbins = list(np.arange(np.min(df['Age']), np.max(df['Age'])+20, 20))\nprint(bins)","5df957b3":"labels = bins[:-1]\nprint(labels)\ndf['Age_binned'] = pd.cut(df['Age'], bins=bins, labels=labels, include_lowest=True)","f92db26d":"df.loc[df.Age > 100, ['Age','Age_binned']]","3a65035f":"df = df.drop(['Age'], axis=1)","ef2f202f":"df.describe(include='category')","7cbd129c":"## Custom Function for Bar Plots\n\ndef barplot(column,df):\n    bar_plot1 = sns.countplot(x=column, data=df)\n    \n    total = len(df[column])\n    for p in bar_plot1.patches:\n        percentage = '{:.2f}%'.format(100 * p.get_height()\/total)\n        height = p.get_height()\n        bar_plot1.text(p.get_x()+ p.get_width()\/2, height + 400, percentage, ha=\"center\")","4319cc2a":"barplot(\"Age_binned\",df)","ba260e8a":"barplot(\"Scholarship\",df)","e4547643":"barplot(\"Hypertension\",df)","be30315b":"barplot(\"Diabetes\",df)","c061494e":"barplot(\"Handicapped\",df)","9f203db5":"barplot(\"Alcoholism\",df)","bc119b31":"barplot(\"Gender\",df)","8686fa0b":"barplot(\"SMS_received\",df)","54b8d403":"df['Handicapped']=df['Handicapped'].astype('int')\ndf.loc[df.Handicapped > 1, 'Handicapped'] = 1\ndf['Handicapped']=df['Handicapped'].astype('category')","c0adde13":"df['Handicapped'].value_counts()","99caec25":"from scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\n\n## conda install -c anaconda statsmodels\nimport statsmodels.api as sm","520496ad":"cat_cols = df.select_dtypes('category').columns\ncat_cols","15419bc3":"# Add formula for chi-square","9ebc0fe4":"def find_chi2_independence(cat_col, target, df, alpha=0.05):\n    data = df[[target, cat_col]]\n    tab = sm.stats.Table.from_data(data)\n    tab = tab.table_orig.to_numpy()\n    print(f\"---------------{target} Vs {cat_col} Chi Square Test of Independence -------------------\")\n    print(f\"\\n Contingency table :\\n\")\n    print(tab)\n    \n    stat, p, dof, expected = chi2_contingency(tab)\n    print(f\"\\n Expected table :\\n\")\n    print(expected)\n    \n    print(f\"The p value returned = {p} and degrees of freedom returned = {dof}\")\n    \n    # interpret p-value\n    print('significance(alpha) = %.3f' % (alpha))\n\n    if p <= alpha:\n        print('Dependent (reject H0)')\n    else:\n        print('Independent (fail to reject H0)') ","dbec751d":"for col in cat_cols:\n    find_chi2_independence(col, 'No-show', df)","3900d0ca":"def probStatusCategorical(col_list):\n    rows = []\n    for item in col_list:\n        for level in df[item].unique():\n            row = {'Condition': item}\n            ## Finding count of rows with that level in categorical column\n            total = len(df[df[item] == level])\n            ## Finding count of rows where level and Yes in No-show match\n            n = len(df[(df[item] == level) & (df['No-show'] == 'Yes')])\n            row.update({'Level': level, 'Probability': n \/ total})\n            rows.append(row)\n    return pd.DataFrame(rows)","78cf0ec6":"plt.figure(figsize=(10,10))\nsns.barplot(data = probStatusCategorical(['Diabetes', 'Alcoholism', 'Handicapped', \n                                          'SMS_received', 'Gender', 'Hypertension','Scholarship']),\n            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\nplt.title('Probability of not showing up')\nplt.ylabel('Probability')\nplt.show()","57abc1c7":"df['Scheduled_DateTime'] = pd.to_datetime(df['ScheduledDay'])\ndf['Scheduled_date'] = df['Scheduled_DateTime'].dt.date\ndf['Scheduled_month']=df['Scheduled_DateTime'].dt.month\ndf['Scheduled_hour'] = df['Scheduled_DateTime'].dt.hour\ndf['Scheduled_weekday']=df['Scheduled_DateTime'].dt.weekday","036f028b":"df['Appointment_DateTime'] = pd.to_datetime(df['AppointmentDay'])\ndf['Appointment_date'] = df['Appointment_DateTime'].dt.date\ndf['Appointment_month']=df['Appointment_DateTime'].dt.month\ndf['Appointment_hour'] = df['Appointment_DateTime'].dt.hour\ndf['Appointment_weekday']=df['Appointment_DateTime'].dt.weekday","ca4c25e1":"np.min(df['Scheduled_date'])","7c975f84":"np.max(df['Scheduled_date'])","c3988283":"df[['Appointment_DateTime', 'Appointment_date', 'Appointment_month', 'Appointment_hour', 'Appointment_weekday']]","64770f5c":"fn = lambda row: (row.Appointment_date - row.Scheduled_date).days\ndf['num_of_days'] = df.apply (fn, axis=1)","c5137334":"df.num_of_days.describe()","a96309dc":"df.loc[df.num_of_days < 0, ['num_of_days']] = 0","ca100ff9":"df = df.drop(['ScheduledDay','Scheduled_DateTime','Scheduled_date'], axis=1)\ndf = df.drop(['AppointmentDay','Appointment_DateTime','Appointment_date'], axis=1)","29052b8c":"#0:MONDAY 1:TUESDAY 2:WEDNESDAY 3:THURSDAY 4:FRIDAY 5:SATURDAY\nbarplot(\"Appointment_weekday\",df)","267fb27f":"barplot(\"Appointment_hour\",df)","d5650fc0":"barplot(\"Scheduled_hour\",df)","aa4e418d":"barplot(\"Appointment_month\",df)","250965df":"barplot(\"Scheduled_month\",df)","271c7ffa":"df = df.drop(['Appointment_hour'], axis=1)","e837a7ae":"df.dtypes","eb64ee50":"for col in ['Scheduled_month', 'Scheduled_weekday', 'Appointment_month', 'Appointment_weekday']:\n    df[col] = df[col].astype('category')","db3801d7":"df.dtypes","a50fa461":"data = getProbOfNoShow('num_of_days', 'No-show', df)","a58d31e8":"sns.lmplot(data = data, x = 'num_of_days', y = 'probNoShowUp', fit_reg = True)\n# plt.xticks(np.arange(np.min(df['num_days']), np.max(df['num_days']), 10))\nplt.title('Probability of Not Showing up with respect to Num of Days to appointment')\nplt.show()","3b96a3d5":"data = getProbOfNoShow('Scheduled_hour', 'No-show', df)","2dc91728":"sns.lmplot(data = data, x = 'Scheduled_hour', y = 'probNoShowUp', fit_reg = True)\n# plt.xticks(np.arange(np.min(df['num_days']), np.max(df['num_days']), 10))\nplt.title('Probability of Not Showing up wrt Hour the appointment is booked')\nplt.show()","9ffa1603":"#Binning of Scheduled_time\ndef get_session_of_day(x):\n    if x < 8:\n        return 'EarlyMorning'\n    elif (x >= 8) and (x < 12 ):\n        return 'Morning'\n    elif (x >= 12) and (x < 16):\n        return'Noon'\n    elif (x >= 16) and (x < 20) :\n        return 'Eve'\n    elif (x >= 20) and (x < 24):\n        return'Night'","0c537da9":"df['Scheduled_session_of_day'] = df['Scheduled_hour'].apply(get_session_of_day)","bd28e43e":"df=df.drop('Scheduled_hour',axis=1)","f7c53e7f":"df['Scheduled_session_of_day']=df['Scheduled_session_of_day'].astype('category')\ndf.dtypes","e92075ce":"for col in ['Scheduled_month', 'Scheduled_weekday', 'Appointment_month', 'Appointment_weekday', 'Scheduled_session_of_day']:\n    find_chi2_independence(col, 'No-show', df)","bfb73ffc":"### Probabilistic Analysis of newly created features\n\n##### Scheduled Month and Appointment Month","e593c4ce":"plt.figure(figsize=(10,10))\nsns.barplot(data = probStatusCategorical(['Scheduled_month', 'Appointment_month']),\n            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\nplt.title('Probability of not showing up')\nplt.ylabel('Probability')\nplt.show()","3572c526":"plt.figure(figsize=(10,10))\nsns.barplot(data = probStatusCategorical(['Scheduled_weekday', 'Appointment_weekday']),\n            x = 'Condition', y = 'Probability', hue = 'Level', palette = 'Set2')\nplt.title('Probability of not showing up')\nplt.ylabel('Probability')\nplt.show()","40f3d955":"barplot(\"No-show\",df)","ee913ef6":"print(\"The count distribution target classes is as below:\")\n\ndf['No-show'].value_counts()","59b727ec":"print(\"The percentage distribution target classes is as below:\")\n\ndf['No-show'].value_counts('Yes')","40afc31c":"df['No-show'].value_counts()","a407e300":"df['No-show'] = df['No-show'].replace(to_replace=['No', 'Yes'], value=[0, 1])\ndf['No-show'].value_counts()","10b673f4":"y = df[\"No-show\"]\nX = df.drop('No-show', axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20,random_state=123, stratify=y)","20b7b81b":"print(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","13845ebe":"train = pd.concat([X_train, y_train], axis = 1)\ntrain[['Neighbourhood', 'No-show']].groupby(['Neighbourhood'])['No-show'].mean()","3dd78b40":"encoded_target = train.groupby(['Neighbourhood'])['No-show'].mean().to_dict()\nencoded_target","a602242c":"# Example to understand set, set union and set difference\nlist1 = ['ias', 'ips', 'jobless', 'jobless']\nlist2 = ['cricket', 'jobless', 'ias']\nprint(set(list1))\nprint(set(list2))\nprint(set(list1).difference(set(list2))) #= > unique elements present in list1 and unique elements not present in list2\nprint(set(list2).difference(set(list1))) #= > unique elements present in list1 and unique elements not present in list2\nprint(set(list2).union(set(list1))) ","30d5755b":"set(X_train['Neighbourhood']).difference(set(X_val['Neighbourhood']))","05829720":"set(X_val['Neighbourhood']).difference(set(X_train['Neighbourhood']))","f3fc0371":"# Since all the levels\/Neighbourhood values present in validation are present in train data too, we wouldn't get any \n# NAs while target encoding validation Neighbourhood data.","877f093e":"# Replace the actual labels with the encoded values in both train and validation\nX_train['Neighbourhood_encoded'] = X_train.Neighbourhood.map(encoded_target)\nX_val['Neighbourhood_encoded'] = X_val.Neighbourhood.map(encoded_target)","c42056de":"# Verify the encoded values and the acual values\nprint(X_train['Neighbourhood'].nunique())\nX_train[['Neighbourhood', 'Neighbourhood_encoded']]","5f3fb6b4":"# Dropping the actual variable\nX_train.drop(['Neighbourhood'], axis = 1, inplace = True)\nX_val.drop(['Neighbourhood'], axis = 1, inplace = True)","3bf64a3f":"set(X_train['PatientId']).difference(set(X_val['PatientId']))","7a13cbab":"len(set(X_train['PatientId']).difference(set(X_val['PatientId'])))","4e610a55":"set(X_val['PatientId']).difference(set(X_train['PatientId']))","634e6588":"len(set(X_val['PatientId']).difference(set(X_train['PatientId'])))","6dad1ff8":"# PatientId can't be target encoded.","64ef9ac2":"patient_appnt = pd.DataFrame(train.groupby('PatientId')['PatientId'].count())\npatient_appnt.columns = ['TotalAppointments']\npatient_appnt.reset_index(inplace = True)\npatient_appnt.shape","2e6989b7":"patient_appnt.head()","e78d2f54":"print(train.shape, patient_appnt.shape)\ntrain_merged = pd.merge(train, patient_appnt, how = 'left', on = 'PatientId')\ntrain_merged.shape","358fa287":"train_merged[['PatientId', 'TotalAppointments', 'No-show']]","1bf6964a":"train_noshow = train[train['No-show'] == 1]\nprint(train_noshow.shape)\npatient_noshow = pd.DataFrame(train_noshow.groupby('PatientId')['PatientId'].count())\npatient_noshow.columns = ['TotalNoshows']\npatient_noshow.reset_index(inplace = True)\npatient_noshow.shape","1170b508":"patient_noshow.head()","b0569ffe":"print(train.shape, patient_appnt.shape, patient_noshow.shape)\ntrain_final = pd.merge(train_merged, patient_noshow, how = 'left', on = 'PatientId')\ntrain_final.shape","aff74169":"train_final[['PatientId', 'TotalAppointments', 'TotalNoshows', 'No-show']]","ea8c7ddd":"train_final['NoShowProb'] = train_final['TotalNoshows']\/train_final['TotalAppointments']","9d4ebded":"train_final[['PatientId', 'TotalAppointments', 'TotalNoshows', 'NoShowProb']]","3d309289":"# Dropping the unwanted variables\ntrain_final.drop(['PatientId', 'TotalNoshows', 'TotalAppointments'], axis = 1, inplace = True)","4065b4c0":"# Same steps should be applied for validation and test data too.","2dffb2b4":"# For now let us build the model with out the high cardinal attributes.\n# Hence dropping Neighbourhood_Encoded and PaitentId from X_train, X_val\nX_train = X_train.drop(['Neighbourhood_encoded'], axis=1)\nX_val = X_val.drop(['Neighbourhood_encoded'], axis=1)\nX_train = X_train.drop(['PatientId'], axis=1)\nX_val = X_val.drop(['PatientId'], axis=1)","03c132a4":"num_attr = X_train.select_dtypes(['int64']).columns\nnum_attr","4eb3a7ec":"cat_attr = X_train.select_dtypes('category').columns\ncat_attr","57c0c56c":"imputer = SimpleImputer(strategy='median')\n\nimputer = imputer.fit(X_train[num_attr])\n\nX_train[num_attr] = imputer.transform(X_train[num_attr])\nX_val[num_attr] = imputer.transform(X_val[num_attr])","a8339f09":"print(X_train.isnull().sum())\nprint(X_val.isnull().sum())","5f14062b":"imputer = SimpleImputer(strategy='most_frequent')\n\nimputer = imputer.fit(X_train[cat_attr])\n\nX_train[cat_attr] = imputer.transform(X_train[cat_attr])\nX_val[cat_attr] = imputer.transform(X_val[cat_attr])","ee9d2e15":"# DataFrameMapper, a class for mapping pandas data frame columns to different sklearn transformations\nmapper = DataFrameMapper(\n  [([continuous_col], StandardScaler()) for continuous_col in num_attr] +\n  [([categorical_col], OneHotEncoder(handle_unknown='error')) for categorical_col in cat_attr]\n, df_out=True)","a62a18eb":"print(type(mapper))","27b219dc":"mapper.fit(X_train)\n\nX_train_final = mapper.transform(X_train)\nX_val_final = mapper.transform(X_val)","060160fe":"X_train_final.columns","5dbae430":"X_train_final.head()","90cf3b2c":"X_train_final.shape","4189a0a4":"X_val_final.head()","7363b2b5":"def get_CR_CM(train_actual,train_predicted,test_actual,test_predicted):\n    print('''\n         ========================================\n           CLASSIFICATION REPORT FOR TRAIN DATA\n         ========================================\n        ''')\n    print(classification_report(train_actual, train_predicted, digits=4))\n\n    print('''\n             =============================================\n               CLASSIFICATION REPORT FOR VALIDATION DATA\n             =============================================\n            ''')\n    print(classification_report(test_actual, test_predicted, digits=4))\n\n    print('''\n             ========================================\n               Confusion Matrix FOR TRAIN DATA\n             ========================================\n            ''')\n    print(confusion_matrix(train_actual, train_predicted))\n\n    print('''\n             =============================================\n               Confusion matrix FOR VALIDATION DATA\n             =============================================\n            ''')\n    print(confusion_matrix(test_actual, test_predicted))\n    ","61cb9859":"scores = pd.DataFrame(columns=['Model','Train_Accuracy','Train_Recall','Train_Precision','Train_F1_Score','Test_Accuracy','Test_Recall','Test_Precision','Test_F1_Score'])\n\ndef get_metrics(train_actual,train_predicted,test_actual,test_predicted,model_description,dataframe):\n    get_CR_CM(train_actual,train_predicted,test_actual,test_predicted)\n    train_accuracy = accuracy_score(train_actual,train_predicted)\n    train_recall   = recall_score(train_actual,train_predicted)\n    train_precision= precision_score(train_actual,train_predicted)\n    train_f1score  = f1_score(train_actual,train_predicted)\n    test_accuracy = accuracy_score(test_actual,test_predicted)\n    test_recall   = recall_score(test_actual,test_predicted)\n    test_precision= precision_score(test_actual,test_predicted)\n    test_f1score  = f1_score(test_actual,test_predicted)\n    dataframe = dataframe.append(pd.Series([model_description, train_accuracy,train_recall,train_precision,train_f1score,\n                                            test_accuracy,test_recall,test_precision,test_f1score],\n                                           index=scores.columns ), ignore_index=True)\n    return(dataframe)","7aa9b36a":"log_mod = LogisticRegression(random_state=123,solver = 'liblinear')","b24dabff":"log_mod.fit(X_train_final, y_train)","70af8e01":"y_pred_train = log_mod.predict(X_train_final)\ny_pred_val = log_mod.predict(X_val_final)","b37b9240":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"LogisticRegression\",scores)\nscores","0aeb1876":"log_mod = LogisticRegression(class_weight='balanced', random_state=123,solver = 'liblinear')","4614d680":"log_mod.fit(X_train_final, y_train)","edf2dc37":"y_pred_train = log_mod.predict(X_train_final)\ny_pred_val = log_mod.predict(X_val_final)","c7157476":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"LogisticRegression_Balanced\",scores)\nscores","d64b0c4d":"# Create Decision Tree classifer object\nclf_dt = DecisionTreeClassifier(random_state=123)\n\n# Train Decision Tree Classifer\nclf_dt.fit(X_train_final,y_train)","625c09c4":"y_pred_train = clf_dt.predict(X_train_final)\ny_pred_val = clf_dt.predict(X_val_final)","5bf44e1f":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"DecisionTree\",scores)\nscores","2f9fad52":"# set of parameters to test\nparam_grid = {\"class_weight\":['balanced', None],\n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_depth\": [3, 5, 6],\n              \"min_samples_leaf\": [2, 5, 10],\n               \"max_leaf_nodes\": [None, 5, 10, 20]\n              }","b3e9e701":"dt = tree.DecisionTreeClassifier(random_state=123)\nclf_dt_grid = GridSearchCV(dt, param_grid,cv=5,scoring='recall')\nclf_dt_grid.fit(X_train_final, y_train)","1a69da29":"clf_dt_grid.best_params_","86df5eea":"y_pred_train = clf_dt_grid.predict(X_train_final)\ny_pred_val = clf_dt_grid.predict(X_val_final)","f3c62bc8":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"DecisionTree_BestParameters\",scores)\nscores","71535eae":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, truncnorm, randint\n\nclf3_dt = DecisionTreeClassifier(random_state=123, class_weight='balanced') ","d64bbc9b":"max_leaf_nodes = np.random.normal(loc=5, scale=1, size=5).astype(int)\nmax_leaf_nodes[max_leaf_nodes <1] = 1\nprint(max_leaf_nodes)\nmax_depth = np.random.uniform(2,5,4).astype(int)\nprint(max_depth)\nmin_samples_split = np.random.uniform(2, 6, 5).astype(int)\nprint(min_samples_split)","a8692161":"## Set Up Hyperparameter Distributions\n\n# normally distributed max_leaf_nodes, with mean 5 stddev 1\nmax_leaf_nodes = np.random.normal(loc=5, scale=1, size=5).astype(int)\n\n# uniform distribution from 2 to 5 \nmax_depth = np.random.uniform(2,5,4).astype(int)\n\n# uniform distribution from 2 to 6\nmin_samples_split = np.random.uniform(2, 6, 5).astype(int)\n\nmodel_params = {\n    'max_depth': list(max_depth),\n    'max_leaf_nodes': list(max_leaf_nodes),\n    'min_samples_split': list(min_samples_split)\n}","21c353e7":"clf_random = RandomizedSearchCV(estimator=clf3_dt, param_distributions=model_params, n_iter=600, cv=5, scoring='recall', n_jobs=-1)\nclf_random.fit(X_train_final, y_train)","4d119549":"print(clf_random.best_score_, clf_random.best_params_)","b9f7e695":"clf_dt_random = clf_random.best_estimator_\nprint(clf_dt_random)","f3346bd9":"y_pred_train = clf_dt_random.predict(X_train_final)\ny_pred_val = clf_dt_random.predict(X_val_final)","c94c008f":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"DecisionTree_RandomizedSearch\",scores)\nscores","c0e3e8d4":"clf_ada = AdaBoostClassifier(DecisionTreeClassifier(criterion=\"gini\",class_weight='balanced'),n_estimators=100,learning_rate = 0.4,random_state=123)\nclf_ada.fit(X_train_final, y_train)","06fecb18":"y_pred_train = clf_ada.predict(X_train_final)\ny_pred_val = clf_ada.predict(X_val_final)","a4ba7fb4":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"Adaboost\",scores)\nscores","1b6b1b1a":"# GridSearchCV\nparam_grid = {'n_estimators' : [100, 200, 300],\n              'learning_rate' : [0.2, 0.3, 0.4]}\n\nclf_grid_ada = GridSearchCV(AdaBoostClassifier(DecisionTreeClassifier()), param_grid, scoring='recall',n_jobs=-1)\n","3ff5253b":"clf_grid_ada.fit(X_train_final, y_train)","e8e6df13":"best_ada_model = clf_grid_ada.best_estimator_\nprint(best_ada_model)\nprint (clf_grid_ada.best_score_, clf_grid_ada.best_params_) ","492594cd":"y_pred_train = best_ada_model.predict(X_train_final)\ny_pred_val = best_ada_model.predict(X_val_final)","7c157147":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"Adaboost_BestParameters\",scores)\nscores","4fe7956e":"clf_GBM = GradientBoostingClassifier(n_estimators=50,\n                                       learning_rate=0.3,\n                                       subsample=0.8)","73d32e22":"clf_GBM.fit(X=X_train_final, y=y_train)","af452949":"y_pred_train = clf_GBM.predict(X_train_final)\ny_pred_val = clf_GBM.predict(X_val_final)","e7d351c0":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"GBM\",scores)\nscores","08a90799":"smote = SMOTE(random_state=123)\nos_train_X, os_train_y = smote.fit_resample(X_train_final, y_train)","2b500ccb":"# observe that data has been balanced\npd.Series(os_train_y).value_counts().plot.bar()\nplt.show()","f6a66a6f":"os_train_X = pd.DataFrame(data=os_train_X)\nos_train_y= pd.DataFrame(data=os_train_y)","92d960bc":"print(os_train_y.shape)\nprint(os_train_X.shape)","83bdaead":"os_train_y.columns","f932edaf":"# Checking the numbers of our data\nprint(\"length of oversampled data is \",len(os_train_X))\nprint(\"Number of 'No-show' patients in oversampled data\",len(os_train_y[os_train_y['No-show']==1]))\nprint(\"Number of non 'No-show' patients \",len(os_train_y[os_train_y['No-show']==0]))\n\nprint(\"Proportion of 'No-show' patients in oversampled data is\",len(os_train_y[os_train_y['No-show']==1])\/len(os_train_X))\nprint(\"Proportion of non 'No-show' patients in oversampled data is\",len(os_train_y[os_train_y['No-show']==0])\/len(os_train_X))\n","17418192":"clf_GBM_smot = GradientBoostingClassifier(n_estimators=100,\n                                       learning_rate=0.01,\n                                       subsample=0.8, random_state=123)","0743a66f":"clf_GBM_smot.fit(os_train_X, os_train_y)","a3eebe75":"y_pred_train = clf_GBM_smot.predict(os_train_X)\ny_pred_val = clf_GBM_smot.predict(X_val_final)","080cc460":"scores = get_metrics(os_train_y,y_pred_train,y_val,y_pred_val,\"GBM_SMOTe\",scores)\nscores","2e41e0b5":"from xgboost import XGBClassifier","b4366856":"clf_XGB = XGBClassifier(n_estimators=400, gamma=0.5,learning_rate=0.1,n_jobs=-1)\nclf_XGB.fit(X_train_final, y_train)","0f581c65":"y_pred_train = clf_XGB.predict(X_train_final)\ny_pred_val = clf_XGB.predict(X_val_final)","71cf1d98":"scores = get_metrics(y_train,y_pred_train,y_val,y_pred_val,\"XGBoost\",scores)\nscores","472402ec":"#### Univariate Analysis on Categorical Columns","3f9b4a2d":"__Observation__: Using Univariate plots on categorical columns, it is very difficult to find out which attribute is related to the target or not. \n\nFrom the above plots, few things can be inferred though\n- Handicapped attribute can be merged to binary as other levels have less representation and also no proper information shared by the client on other levels.\n\n- Chi-square test of independence and Bivariate plots can be meaningful ","29e53065":"### Defining Error Metrics","1b5e844e":"__Observation:__ Num of days prior to appointment has no clear pattern in it.","efc9e545":"### 2. Logistic Regression with class_weight='balanced'","ccc24430":"### 5. Decision Tree with RandomizedSearchCV - Hyper-parameter Tuning","3c1d706f":"__Observation:__ \n- Clearly Appointment ID is unique for each row and offers zero variance. Hence it can be deleted. \n\n- Patient ID is not unique and hence it cannot be deleted. Will address this attribute later with other categorical columns","97082345":"### 3. Decision Tree Classifier","c6726be6":"### 8. Gradient Boosting Classifier","39822eed":"### Effect of Scheduled_hour on No-show attribute","33ff8c26":"## Feature Engineering","8e25bce2":"# Preparation for Model buillding\n\n## Imputing missing values with median for numerical attributes","2a49b434":"__Observation__: There is clear trend in above graph, showcasing the effect of scheduled hour on no-show up. We can bin Scheduled hour with bin size of 2 hours","91ca8ab8":"# MODEL BUILDING","e2b2815e":"##### Evaluating the model performance","5f859c32":"#### Target Encoding variables with high cardinality","4dded8eb":"## Checking  target feature distribution","7663785b":"## Reading the data","b5d83fb7":"### Summary Stats","81a0f03b":"## _Handling Imbalanced Data_\n\n### Class Weights of loss function\n\nThe \u201cbalanced\u201d mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples \/ (n_classes * np.bincount(y))\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html","3cc3de1d":"## Error Metric : Recall\n\nFor this problem, minimizing Type II errors is a priority. A Type II error here is a situation where the algorithm incorrectly posits a patient will turn up for an appointment, but they do not present themselves for the scheduled appointment. The inverse situation, in which the prediction is a patient no-attendance, but they do attend the appointment is less of a burden on the practice and has fewer negative impacts for the patient than not receiving care. The prediction as no-attendance, but the patient does attend, is the Type I error. The most successful algorithm is defined as one with high recall.","ea9bd36a":"__Observation__: Age cannot have negative values, but very high chance that it can 0 or 1 and as both levels have equal distribution adding it to either will not make difference","97f6c9ae":"__Observation__: Suprised to see that appointments are allowed\/booked for only three months","f44fc807":"### Analysing Numerical features","efca9143":"__Observation__: We can see some negative values indicating there might be some outliers in the data as appointment date cannot happen before the scheduled date","677f2193":"#### Neighbourhood","d7e9ddd8":"##### Evaluating the model performance","bff0acd1":"### Handling Class Imbalance using Synthetic Minority Oversampling Technique\n#### Oversample Using SMOTE","fcaf9cbd":"### 1. Logistic Regression","4a2440f0":"### Typecasting newly created features","3fb3010c":"#### Effect of Age on No-show attribute","b70c7673":"# Split the data into train and test","0b02af12":"__Observation__: Weekday seems to have similar\/equal pattern on no show ups. \n\nSome interesting insights are\n- Significantly less people called\/scheduled for an appointment on Saturday. \n- More people tend to not show up on Saturday","9f085638":"## Split the attributes into numerical and categorical types","080576f3":"### Creating 'number of days to appointment' \n\nThis feature may have an impact on our target feature,hence retrieving new feature from scheduled date and appointment date","e6cda4a8":"### 4. Decision Tree with Grid-Search CV - Hyper-parameter Tuning","99dc4293":"### Effect of num_days on No-show attribute","96cd0c6c":"#### Dropping Zero Variance features like Appointment ID","2d3ba1bd":"__Observation__: \n- PatientId, Neighbourhood, Scholarship, Hypertension, Diabetes, SMS_received, Age_binned attributes tend to influence the target\n\n- Gender & Alcoholism doesn't have any dependency with target\n\nLet us also run probabilistic approach for categorical columns","9389bb3f":"### Checking Datatypes & Typecasting obvious ones","671d7640":"Based on the above scores, considering Recall score as the evaluation metrix, we can use \"Decision Tree_BestParameters\" model to predict the patient not showing up.\n","b68bb541":"## Exploratory Data Analysis","655fa779":"### Renaming all columns with correct spellings","17ba1c62":"## Importing the required libraries and packages","abe6d0e9":"### Generating new features from scheduled and appointment date features","96f0910d":"__Observation:__ \n- 1) Clearly SMS received status and Scholarship Status drive no-show up's\n- 2) Gender and Alcoholism are not contributing to the target (No-show class)","7ad8175f":"__Observation:__ appointment hour has zero variance. So can be deleted","f7349557":"### Prediction of Patient no-show in a clinic\/hospital\n\nHospital appointment where the patient does not turn-up is defined as an appointment in which the patient did not visit for the treatment or cancelled the same day. As these appointments are problematic for practices at all levels of the health care system, patients not turning up is a missed revenue opportunity which cannot be re-captured for the practice, and which contribute to both decreased patient and staff satisfaction. Such appointments negatively impact both patients and care teams. The objective is to predict the probability of a patient not turning up for a medical appointment.","997bf0dc":"##### Evaluating the model performance","5bc559ff":"## Imputing missing values with mode for categorical attributes","5547f851":"### Analysis on Patient ID and Appointment ID","598b6d39":"### 10. XGBoost Classifier\nXGBoost (eXtreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm.","650a27ad":"#### PatientId","44a4c7c2":"### Chi-Square test of independence\n\nThe Pearson\u2019s Chi-Square statistical hypothesis is a test for independence between categorical variables.\n\n- Null hypothesis (H0) : There is no relation between the variables. \n- An alternate hypothesis(H1): There is a significant relation between the two.\n\n- Significance factor or alpha value of 0.05 is chosen. This alpha value denotes the probability of erroneously rejecting H0 when it is true.\n\n- If the p-value for the test comes out to be strictly greater than the alpha value, then H0 holds true.\n    \nThe chi2_contingency() function of scipy.stats module takes, \n- Input as the contingency table in 2d array format. \n- Returns a tuple containing test statistics, the p-value, degrees of freedom and expected table(the one we created from the calculated values) in that order. \n\nHence, we need to compare the obtained p-value with alpha value of 0.05 (default).","7b1372b8":"__Observation:__ Age is clearly a right skewed one. Applying transforms? Log ?","e7b1796c":"### 7. Adaboost Classifier with Grid Search CV","b5cc92d1":"### Analysing Categorical Columns","aa34440a":"##### Evaluating the model performance","59f2a3c9":"#### Handling Age Attribute","08a2fa3d":"### Standardizing the numerical attributes and One-hot encoding categorical attributes ","99981b36":"### 9. GBM with SMOTe","0bdaec50":"#### Binning of Handicapped attrribute","86623dae":"##### Scheduled Weekday and Appointment Weekday","f371ab4e":"__Observation:__ We see most of the newly engineered features show some relation w.r.t to target","1f198259":"### Probabilistic Analysis of Categorical columns on No show","9a4b5a37":"### Replacing Yes\/No in the target column with 1\/0","5d73eba9":"### 6. Adaboost Classifier","cff49e27":"### Performing Chisquare test of independence on newly added categorical columns"}}