{"cell_type":{"99c7dbff":"code","a91311fb":"code","e4fb32c8":"code","68df12c8":"code","cae07561":"code","7e755835":"code","c0e24ff4":"code","c5715c6c":"code","3cbaca17":"code","7560f080":"code","2fc68ca8":"code","642e15fb":"code","313e968f":"code","7c666dcd":"code","90c1700d":"code","681473d1":"code","1940fda7":"code","def7daff":"code","a6c8626f":"code","dfeecebf":"code","54464cc8":"code","4ade8377":"code","4911c541":"code","628ca39e":"code","ca9a1dc1":"code","ecae6790":"code","4434701a":"code","5a4cc81c":"code","b18de37d":"code","3e6b42dc":"code","46c38fa6":"code","8812d27f":"code","2f6266fb":"code","e6bb73e4":"code","868807e0":"code","f4b33bc1":"code","efa2bac2":"code","7fbd6193":"code","d3408fa7":"code","520a79ba":"code","d67e5c59":"code","8b56ffa3":"code","48199136":"code","151c169d":"code","222ec255":"code","314fca49":"code","23d7e19f":"code","aeab7ac3":"code","9969795d":"code","cc7b7a19":"code","7b623a2f":"code","065af913":"code","f27e09e4":"code","e8b48541":"markdown","222d350b":"markdown","b18616a0":"markdown","5a3ffa2a":"markdown","76b9bfbb":"markdown","9d0067c6":"markdown","a0df28c4":"markdown","f130e598":"markdown","2eaedd21":"markdown"},"source":{"99c7dbff":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a91311fb":"train_data = pd.read_csv(\"..\/input\/ltfs-av-data\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/ltfs-av-data\/test_bqCt9Pv.csv\")","e4fb32c8":"train_data.columns=train_data.columns.str.replace('.','_')\ntest_data.columns=test_data.columns.str.replace('.','_')","68df12c8":"print(\" Shape of training dataframe: \", train_data.shape)\nprint(\" Shape of testing dataframe: \", test_data.shape)\n# Drop duplicates\ntrain_data.drop_duplicates()\ntest_data.drop_duplicates()\nprint(train_data.shape)\nprint(test_data.shape)","cae07561":"print(train_data.info())\nprint(train_data.isna().sum())","7e755835":"print(test_data.info())\nprint(test_data.isna().sum())","c0e24ff4":"# Replacing the missing Employment_type by it's mode value.\ntrain_data['Employment_Type'] = train_data['Employment_Type'].fillna( train_data['Employment_Type'].dropna().mode().values[0] )\ntest_data['Employment_Type'] = test_data['Employment_Type'].fillna( test_data['Employment_Type'].dropna().mode().values[0] )","c5715c6c":"# Changing AVERAGE_ACCT_AGE & CREDIT_HISTORY_LENGTH\n\ntrain_data[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = train_data['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ntrain_data[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = train_data['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ntrain_data[\"AVERAGE_ACCT_AGE\"]= train_data[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((train_data[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))\/12)\ntrain_data= train_data.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])\n\ntest_data[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = test_data['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ntest_data[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = test_data['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ntest_data[\"AVERAGE_ACCT_AGE\"]= test_data[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((test_data[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))\/12)\ntest_data= test_data.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])\n\ntrain_data[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = train_data['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ntrain_data[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = train_data['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ntrain_data[\"CREDIT_HISTORY_LENGTH\"]= train_data[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((train_data[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))\/12)\ntrain_data= train_data.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])\n\ntest_data[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = test_data['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ntest_data[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = test_data['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ntest_data[\"CREDIT_HISTORY_LENGTH\"]= test_data[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((test_data[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))\/12)\ntest_data= test_data.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])","3cbaca17":"print(train_data['AVERAGE_ACCT_AGE'])","7560f080":"train_data['Date_of_Birth'] =  pd.to_datetime(train_data['Date_of_Birth'], format='%d-%m-%y')\ntest_data['Date_of_Birth'] =  pd.to_datetime(test_data['Date_of_Birth'], format='%d-%m-%y')\ntrain_data['DisbursalDate'] =  pd.to_datetime(train_data['DisbursalDate'], format='%d-%m-%y')\ntest_data['DisbursalDate'] =  pd.to_datetime(test_data['DisbursalDate'], format='%d-%m-%y')","2fc68ca8":"now = pd.Timestamp('now')\ntrain_data['age'] = (now - train_data['Date_of_Birth'])  \n\ntrain_data['age']= train_data['age'].astype(str)\ntrain_data[['age','age_waste']] = train_data['age'].str.split(\"days\",expand=True)\ntrain_data['age']= train_data['age'].astype(str).astype(int)\ntrain_data= train_data.drop(columns= ['age_waste', 'Date_of_Birth'])\n\nprint(train_data['age'].head())","642e15fb":"now = pd.Timestamp('now')\ntest_data['age'] = (now - test_data['Date_of_Birth'])  \n\ntest_data['age']= test_data['age'].astype(str)\ntest_data[['age','age_waste']] = test_data['age'].str.split(\"days\",expand=True)\ntest_data['age']= test_data['age'].astype(str).astype(int)\ntest_data= test_data.drop(columns= ['age_waste', 'Date_of_Birth'])\n\nprint(test_data['age'].head())","313e968f":"train_data['disbursal_time'] = (now - train_data['DisbursalDate'])  \n\ntrain_data['disbursal_time']= train_data['disbursal_time'].astype(str)\ntrain_data[['disbursal_time','disbursal_time_waste']] = train_data['disbursal_time'].str.split(\"days\",expand=True)\ntrain_data['disbursal_time']= train_data['disbursal_time'].astype(str).astype(int)\ntrain_data= train_data.drop(columns= ['disbursal_time_waste'])\n\ntrain_data = train_data.drop(columns=['DisbursalDate'])\nprint(train_data['disbursal_time'].head())","7c666dcd":"test_data['disbursal_time'] = (now - test_data['DisbursalDate'])  \n\ntest_data['disbursal_time']= test_data['disbursal_time'].astype(str)\ntest_data[['disbursal_time','disbursal_time_waste']] = test_data['disbursal_time'].str.split(\"days\",expand=True)\ntest_data['disbursal_time']= test_data['disbursal_time'].astype(str).astype(int)\ntest_data= test_data.drop(columns= ['disbursal_time_waste'])\n\ntest_data = test_data.drop(columns=['DisbursalDate'])\nprint(test_data['disbursal_time'].head())","90c1700d":"train_data['Employment_Type'] = train_data['Employment_Type'].map({'Salaried':0, 'Self employed':1}).astype(np.int)\ntest_data['Employment_Type'] = test_data['Employment_Type'].map({'Salaried':0, 'Self employed':1}).astype(np.int)","681473d1":"train_data['PERFORM_CNS_SCORE_DESCRIPTION'].value_counts()","1940fda7":"train_data['PERFORM_CNS_SCORE_DESCRIPTION'] = train_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('No Bureau History Available', \n                                     'Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer',\n                                     'Not Scored: No Activity seen on the customer (Inactive)', \n                                     'Not Scored: No Updates available in last 36 months', 'Not Scored: Only a Guarantor', \n                                     'Not Scored: More than 50 active Accounts found'),(0, 0, 0, 0, 0, 0, 0))\n\ntrain_data['PERFORM_CNS_SCORE_DESCRIPTION'] = train_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('L-Very High Risk', 'M-Very High Risk'), (1, 1))\n\ntrain_data['PERFORM_CNS_SCORE_DESCRIPTION'] = train_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('J-High Risk', 'K-High Risk'), (2, 2))\n\ntrain_data['PERFORM_CNS_SCORE_DESCRIPTION'] = train_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('H-Medium Risk', 'I-Medium Risk'), (3, 3))\n\ntrain_data['PERFORM_CNS_SCORE_DESCRIPTION'] = train_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('E-Low Risk', 'F-Low Risk', 'G-Low Risk'), (4, 4, 4))\n\ntrain_data['PERFORM_CNS_SCORE_DESCRIPTION'] = train_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('A-Very Low Risk', 'B-Very Low Risk',\n                                      'C-Very Low Risk', 'D-Very Low Risk'), (5, 5, 5, 5))","def7daff":"train_data['PERFORM_CNS_SCORE_DESCRIPTION'].value_counts()","a6c8626f":"test_data['PERFORM_CNS_SCORE_DESCRIPTION'] = test_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('No Bureau History Available', \n                                     'Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer',\n                                     'Not Scored: No Activity seen on the customer (Inactive)', \n                                     'Not Scored: No Updates available in last 36 months', 'Not Scored: Only a Guarantor', \n                                     'Not Scored: More than 50 active Accounts found'),(0, 0, 0, 0, 0, 0, 0))\n\ntest_data['PERFORM_CNS_SCORE_DESCRIPTION'] = test_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('L-Very High Risk', 'M-Very High Risk'), (1, 1))\n\ntest_data['PERFORM_CNS_SCORE_DESCRIPTION'] = test_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('J-High Risk', 'K-High Risk'), (2, 2))\n\ntest_data['PERFORM_CNS_SCORE_DESCRIPTION'] = test_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('H-Medium Risk', 'I-Medium Risk'), (3, 3))\n\ntest_data['PERFORM_CNS_SCORE_DESCRIPTION'] = test_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('E-Low Risk', 'F-Low Risk', 'G-Low Risk'), (4, 4, 4))\n\ntest_data['PERFORM_CNS_SCORE_DESCRIPTION'] = test_data['PERFORM_CNS_SCORE_DESCRIPTION'].replace(('A-Very Low Risk', 'B-Very Low Risk',\n                                      'C-Very Low Risk', 'D-Very Low Risk'), (5, 5, 5, 5))","dfeecebf":"test_data['PERFORM_CNS_SCORE_DESCRIPTION'].value_counts()","54464cc8":"print(train_data.info())\nprint(train_data.isna().sum())","4ade8377":"plt.figure(figsize= (12,9), dpi=100)\nsns.heatmap(train_data.corr())","4911c541":"# check the distribution of disbursed amount\n\nplt.rcParams['figure.figsize'] = (18, 5)\n\nplt.subplot(1, 3, 1)\nsns.distplot(train_data['disbursed_amount'],  color = 'orange')\nplt.title('Disbursed Amount')\n\nplt.subplot(1, 3, 2)\nsns.distplot(train_data['asset_cost'], color = 'pink')\nplt.title('Asset Cost')\n\nplt.subplot(1, 3, 3)\nsns.distplot(train_data['ltv'], color = 'red')\nplt.title('Loan to value of the asset')\n\nplt.show()","628ca39e":"\n#performing log transformations on disbursed amount, ltv, and asset cost\n\ntrain_data['disbursed_amount'] = np.log1p(train_data['disbursed_amount'])\ntrain_data['ltv'] = np.log1p(train_data['ltv'])\ntrain_data['asset_cost'] = np.log1p(train_data['asset_cost'])\n\n\nplt.rcParams['figure.figsize'] = (18, 5)\n\nplt.subplot(1, 3, 1)\nsns.distplot(train_data['disbursed_amount'],  color = 'orange')\nplt.title('Disbursed Amount')\n\nplt.subplot(1, 3, 2)\nsns.distplot(train_data['asset_cost'], color = 'pink')\nplt.title('Asset Cost')\n\nplt.subplot(1, 3, 3)\nsns.distplot(train_data['ltv'], color = 'red')\nplt.title('Loan to value of the asset')\n\nplt.show()","ca9a1dc1":"sns.distplot(train_data['age'], color = 'blue')\nplt.title('Distribution of Date of birth')","ecae6790":"sns.countplot(train_data['NO_OF_INQUIRIES'], palette = 'muted')\nplt.title('No. of Inquiries',  fontsize = 30)","4434701a":"plt.rcParams['figure.figsize'] = (18, 5)\nsns.countplot(train_data['CREDIT_HISTORY_LENGTH'].head(10))\nplt.title('Credit History')\nplt.xticks(rotation = 45)","5a4cc81c":"sns.countplot(train_data['AVERAGE_ACCT_AGE'].head(10), palette = 'colorblind')\nplt.title('Average Loan Tenure')\nplt.xticks(rotation = 50)","b18de37d":"# let's apply log transformations on EMI Amount of the Primary Loan and Secondary loan\n\ntrain_data['PRIMARY_INSTAL_AMT'] = np.log1p(train_data['PRIMARY_INSTAL_AMT'])\ntrain_data['SEC_INSTAL_AMT'] = np.log1p(train_data['SEC_INSTAL_AMT'])\n\nplt.subplot(1, 2, 1)\nsns.distplot(train_data['SEC_INSTAL_AMT'], color = 'yellow', kde_kws={'bw':0.1})\nplt.title('EMI Amount Secondary Plan', fontsize = 20)\nplt.xticks(rotation = 45)\n\nplt.subplot(1, 2, 2)\nsns.distplot(train_data['PRIMARY_INSTAL_AMT'],color = 'yellow', kde_kws={'bw':0.1})\nplt.title('EMI Amount Primary Plan', fontsize = 20)\nplt.xticks(rotation = 45)\n\nplt.show()","3e6b42dc":"\n# distribution for different attributesof secondary accounts\n\n\nplt.rcParams['figure.figsize'] = (18, 12)    \nplt.subplot(2, 3, 1)\nsns.distplot(train_data['SEC_NO_OF_ACCTS'], color = 'green', kde_kws={'bw':0.1})\nplt.title('Total loan taken by customer')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 2)\nsns.distplot(train_data['SEC_ACTIVE_ACCTS'], color = 'green', kde_kws={'bw':0.1})\nplt.title('Active loan taken by customer')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 3)\nsns.distplot(train_data['SEC_OVERDUE_ACCTS'], color = 'green', kde_kws={'bw':0.1})\nplt.title('Default Accounts at the time of disbursement')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 4)\nsns.distplot(train_data['SEC_CURRENT_BALANCE'], color = 'green', kde_kws={'bw':0.1})\nplt.title('Principal Outstanding amount')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 5)\nsns.distplot(train_data['SEC_SANCTIONED_AMOUNT'], color = 'green', kde_kws={'bw':0.1})\nplt.title('Total Sanctioned Amount')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 6)\nsns.distplot(train_data['SEC_DISBURSED_AMOUNT'], color = 'green', kde_kws={'bw':0.1})\nplt.title('Total Disbured Amount')\nplt.xticks(rotation = 45)","46c38fa6":"# plotting distribution plots for these attributes\n\nplt.rcParams['figure.figsize'] = (18, 12)    \nplt.subplot(2, 3, 1)\nsns.distplot(train_data['PRI_NO_OF_ACCTS'], color = 'violet', kde_kws={'bw':0.1})\nplt.title('Total loan taken by customer')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 2)\nsns.distplot(train_data['PRI_ACTIVE_ACCTS'], color = 'violet', kde_kws={'bw':0.1})\nplt.title('Active loan taken by customer')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 3)\nsns.distplot(train_data['PRI_OVERDUE_ACCTS'], color = 'violet', kde_kws={'bw':0.1})\nplt.title('Default Accounts')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 4)\nsns.distplot(train_data['PRI_CURRENT_BALANCE'], color = 'violet', kde_kws={'bw':0.1})\nplt.title('Principal Outstanding amount')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 5)\nsns.distplot(train_data['PRI_SANCTIONED_AMOUNT'], color = 'violet', kde_kws={'bw':0.1})\nplt.title('Total Sanctioned Amount')\nplt.xticks(rotation = 45)\n\nplt.subplot(2, 3, 6)\nsns.distplot(train_data['PRI_DISBURSED_AMOUNT'], color = 'violet')\nplt.title('Total Disbured Amount')\nplt.xticks(rotation = 45)\n\nplt.show()","8812d27f":"plt.rcParams['figure.figsize'] = (19, 6)\nsns.countplot(train_data['PERFORM_CNS_SCORE_DESCRIPTION'], palette = 'pastel')\nplt.title('Bureau Score Description', fontsize = 30)\nplt.xticks(rotation = 90)\nplt.show()","2f6266fb":"plt.rcParams['figure.figsize'] = (15, 5)\nplt.subplot(1, 2, 1)\nsns.distplot(train_data['PERFORM_CNS_SCORE'], color = 'purple')\nplt.title('Before Log transformations')\n\nplt.subplot(1, 2, 2)\ntrain_data['PERFORM_CNS_SCORE'] = np.log1p(train_data['PERFORM_CNS_SCORE'])\nsns.distplot(train_data['PERFORM_CNS_SCORE'], color = 'maroon')\nplt.title('After Log transformations')\n\nplt.show()","e6bb73e4":"# Models Import\n\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import balanced_accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","868807e0":"#Useless features\ntrain_data = train_data.drop(['UniqueID', 'State_ID', 'Employee_code_ID', 'supplier_id', 'manufacturer_id', 'Current_pincode_ID','branch_id'],axis=1)\ntest_data = test_data.drop(['State_ID', 'Employee_code_ID', 'supplier_id', 'manufacturer_id', 'Current_pincode_ID','branch_id'],axis=1)","f4b33bc1":"y = train_data[['loan_default']]\nX= train_data.loc[:, train_data.columns != 'loan_default']","efa2bac2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 101)\nprint(\"Shape of train :\", X_train.shape)\nprint(\"Shape of test :\", X_test.shape)","7fbd6193":"logmodel = LogisticRegression() \nlogmodel.fit(X_train,y_train)\nlogpred = logmodel.predict(X_test)\n\n\nprint(confusion_matrix(y_test, logpred))\nprint(round(accuracy_score(y_test, logpred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","d3408fa7":"print(\"Accuracy of model \",accuracy_score(y_test, logpred))\nprint(\"F1 Score \",f1_score(y_test, logpred))\nprint(\"Recall Score \",recall_score(y_test, logpred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, logpred))","520a79ba":"# train model\nrfc = RandomForestClassifier(n_estimators=10).fit(X_train, y_train)\n\n# predict on test set\nrfc_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test, rfc_pred))\nprint(round(accuracy_score(y_test, rfc_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","d67e5c59":"print(\"Accuracy of model \",accuracy_score(y_test, rfc_pred))\nprint(\"F1 Score \",f1_score(y_test, rfc_pred))\nprint(\"Recall Score \",recall_score(y_test, rfc_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, rfc_pred))","8b56ffa3":"# train model\nnb = GaussianNB().fit(X_train, y_train)\n\n# predict on test set\nnb_pred = nb.predict(X_test)\nprint(confusion_matrix(y_test, nb_pred))\nprint(round(accuracy_score(y_test, nb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","48199136":"print(\"Accuracy of model \",accuracy_score(y_test, nb_pred))\nprint(\"F1 Score \",f1_score(y_test, nb_pred))\nprint(\"Recall Score \",recall_score(y_test, nb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, nb_pred))","151c169d":"# train model\nsgd = SGDClassifier(loss= \"modified_huber\", shuffle = True, random_state= 101).fit(X_train, y_train)\n\n# predict on test set\nsgd_pred = sgd.predict(X_test)\nprint(confusion_matrix(y_test, sgd_pred))\nprint(round(accuracy_score(y_test, sgd_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","222ec255":"print(\"Accuracy of model \",accuracy_score(y_test, sgd_pred))\nprint(\"F1 Score \",f1_score(y_test, sgd_pred))\nprint(\"Recall Score \",recall_score(y_test, sgd_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, sgd_pred))","314fca49":"# train model\ndtree = DecisionTreeClassifier(max_depth = 10, random_state= 101, max_features =None , min_samples_leaf = 30).fit(X_train, y_train)\n\n# predict on test set\ndtree_pred = dtree.predict(X_test)\nprint(confusion_matrix(y_test, dtree_pred))\nprint(round(accuracy_score(y_test, dtree_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","23d7e19f":"print(\"Accuracy of model \",accuracy_score(y_test, dtree_pred))\nprint(\"F1 Score \",f1_score(y_test, dtree_pred))\nprint(\"Recall Score \",recall_score(y_test, dtree_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, dtree_pred))","aeab7ac3":"lgb = LGBMClassifier()\nlgb.fit(X_train, y_train)\n\nlgb_pred = lgb.predict(X_test)\n\nprint(confusion_matrix(y_test, lgb_pred))\nprint(round(accuracy_score(y_test, lgb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","9969795d":"print(\"Accuracy of model \",accuracy_score(y_test, lgb_pred))\nprint(\"F1 Score \",f1_score(y_test, lgb_pred))\nprint(\"Recall Score \",recall_score(y_test, lgb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, lgb_pred))","cc7b7a19":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\nxgb_pred = xgb.predict(X_test)\n\nprint(confusion_matrix(y_test, xgb_pred))\nprint(round(accuracy_score(y_test, xgb_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","7b623a2f":"print(\"Accuracy of model \",accuracy_score(y_test, xgb_pred))\nprint(\"F1 Score \",f1_score(y_test, xgb_pred))\nprint(\"Recall Score \",recall_score(y_test, xgb_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, xgb_pred))","065af913":"ada = AdaBoostClassifier()\nada.fit(X_train, y_train)\n\nada_pred = ada.predict(X_test)\n\nprint(confusion_matrix(y_test, ada_pred))\nprint(round(accuracy_score(y_test, ada_pred),2)*100)\nLOGCV = (cross_val_score(logmodel, X_train, y_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","f27e09e4":"print(\"Accuracy of model \",accuracy_score(y_test, ada_pred))\nprint(\"F1 Score \",f1_score(y_test, ada_pred))\nprint(\"Recall Score \",recall_score(y_test, ada_pred))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, ada_pred))","e8b48541":"## Visualizing the data","222d350b":"Converting the DOB and Disbursal-Date into DateTime Format.\n<br> Also Converting into Days.","b18616a0":"As we can see the Data is Cleaned and all the object are converted into int64","5a3ffa2a":"## Data-Cleaning","76b9bfbb":"## Training Models","9d0067c6":"We first bring the Average_acct_age and Credit_history_length into normal date_time format.\n<br> we split the given age type into it's respective part that is years and months\n<br> Then convert it into str and int then concatenate to bring it into normal form.\n<br> Then Divide the Age by 12 to get the Average ","a0df28c4":"There are 7661 missing values of Employment_Type in train_data and 3443 in test_data.\nWe perform Data-Cleaning.","f130e598":"### Accuracy of Different Models.\n\n#### LogisticRegression <br> Accuracy = 78% <br> Balanced Accuracy Score  0.50\n#### RandomForestClassifier <br> Accuracy = 77% <br> Balanced Accuracy Score  0.51\n#### GaussianNB <br> Accuracy = 78% <br> Balanced Accuracy Score  0.49\n#### SGDClassifier <br> Accuracy = 53% <br> Balanced Accuracy Score  0.49\n#### DecisionTreeClassifier <br> Accuracy = 78% <br> Balanced Accuracy Score  0.51\n#### LGBMClassifier <br> Accuracy = 78%<br> Balanced Accuracy Score  0.50\n#### XGBClassifier<br> Accuracy = 78%<br> Balanced Accuracy Score  0.51\n#### AdaBoostClassifier <br>  Accuracy = 78%<br> Balanced Accuracy Score  0.50","2eaedd21":"#### ONE-HOT Encoding.\nWe will Manually one hot encode the Employment_type and PERFORM_CNS_SCORE_DESCRIPTION."}}