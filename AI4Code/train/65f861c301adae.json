{"cell_type":{"b8f53fe7":"code","0b170e5f":"code","2c20af0c":"code","bab602ef":"code","c1954ee9":"markdown","10989457":"markdown"},"source":{"b8f53fe7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b170e5f":"# K-means clustering \n# Perform k-means for 3 clusters naively using only numpy and basic functions\n\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\npoints, y = make_classification(n_features=10, n_redundant=0,\n                           n_informative=10, random_state=1,\n                           n_clusters_per_class=1,\n                           n_classes=3)\n","2c20af0c":"# Initialize labels\nlabels = np.random.randint(0,3,100)\nprint(labels.shape)\n\nimprovement = 1e6\niteration = 0\nprev_dist = 1e6\n\nwhile improvement > 1e-20:\n    iteration +=1\n    print(iteration, prev_dist)\n    # E-step - computer centroid\n    centroids = np.vstack([points[labels==i,:].mean(axis=0) for i in range(3)])\n\n    # M-step - Calculate distance to centroids and reassign\n    dist = np.vstack([np.linalg.norm(points - centroids[i,:],axis=1) for i in range(3)])\n    labels = dist.argmin(axis=0)\n    min_dist = dist.min(axis=0).sum()\n    improvement = prev_dist - min_dist\n    prev_dist = min_dist\n\nprint(\"Naive\",prev_dist)","bab602ef":"## Let's compare with the sklearn\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(3)\nkmeans.fit(points)\nidentified_clusters = kmeans.fit_predict(points)\n\nprint(kmeans.cluster_centers_)\ndist = np.vstack([np.linalg.norm(points - kmeans.cluster_centers_[i],axis=1) for i in range(3)])\nprint(dist)\nmin_dist = dist.min(axis=0).sum()\nprint(\"Sklearn implementation\",min_dist)\n","c1954ee9":"# Questions to ask:\n     - Is KMeans a supervised or an unsupervised learning algorithm?\n     - What is the objective function optimizing in kmeans?\n           - The objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function\n     - What are the key steps in each iteration of the optimization\n     - How to decide on the best number of clusters?\n     - Have you used k-means for your data science \/ ML related projects?\n           - What is the business impact of implementing the k-means algorithm\n\n# General Questions to ask:\n    \n# - Classification Algorithms:\n     - What algorithms can you use for classifying binary outcomes when you have a small dataset? Name 3.\n         - Logistic Regression, Random Forest, Support Vector Machine \n     - Compare and contrast the advantages and disadvantages of each algorithm.\n     - Explain the bias-variance tradeoff\n     - How can we prevent model from overfitting?\n     \n# - Imbalanced Data:\n     - How do you improve model performance when training on imbalanced dataset( 0.0001% True for example)\n\n# - Outliers:\n     - Between SVM and Random Forest, which one is less susceptible to outliers?\n","10989457":"# Answer"}}