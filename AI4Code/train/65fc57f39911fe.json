{"cell_type":{"d0a26d8e":"code","48b29318":"code","7f03da03":"code","0a2ebd80":"code","4dbae266":"code","cd958d77":"code","68a3bbbe":"code","404e4e97":"code","2d31ccc9":"code","921bd277":"code","cab5a128":"code","c4adcb26":"code","3eb21391":"code","d0e88d87":"code","e309e056":"code","c03b6acc":"code","c24eee46":"code","18ec92bb":"code","8c230761":"code","eae17aa2":"code","6e90436f":"code","4a60a16c":"code","b034975b":"code","758048b3":"code","76583294":"code","d1d4df98":"code","0f1acd42":"code","beb9c94e":"code","ec68741c":"code","42fcec44":"code","55079889":"code","1c8a0855":"code","b4eed842":"code","acfcd321":"code","3066f6e5":"code","6cddbdf4":"code","fe9b0e8b":"code","887581ab":"code","2b216c8e":"code","bdf48397":"markdown","4f0c7f51":"markdown","4b7b6306":"markdown","136f1dfd":"markdown","84face80":"markdown","40877d49":"markdown","b9bc0986":"markdown","bec72723":"markdown","ac416b45":"markdown","c800230c":"markdown","bedb2031":"markdown","1f6f73a1":"markdown","34c1980f":"markdown","83cb5e37":"markdown","20d58fd2":"markdown","d33d7bc3":"markdown","1c4b11b7":"markdown","d83f2621":"markdown","3b7e9926":"markdown","30f00047":"markdown","6e8a751f":"markdown","9d3a90c0":"markdown","500a2194":"markdown","b4ce943e":"markdown"},"source":{"d0a26d8e":"# \u5b89\u88c5 pycocotools\n!pip install pycocotools\n","48b29318":"# \u4e0b\u8f7d\u5b57\u4f53\uff0c\u7528\u4e8e\u663e\u793a\u76ee\u7c7b\u578b\n!rm -rf font\n!mkdir font\n!wget https:\/\/www.fontsquirrel.com\/fonts\/download\/open-sans \n!unzip open-sans -d font","7f03da03":"# \u4e0b\u8f7d torch.vison \u4e2d\u51e0\u4e2a\u5de5\u5177\u7c7b\uff0c\u4e3b\u8981\u8bad\u7ec3\u548ceval\u4e2d\u65b9\u4fbf\u8bb0\u5f55\u53c2\u6570\u4fe1\u606f\n!git clone https:\/\/github.com\/pytorch\/vision.git\n!cd vision\n!git checkout v0.3.0\n\n!cp vision\/references\/detection\/utils.py .\/\n!cp vision\/references\/detection\/transforms.py .\/\n!cp vision\/references\/detection\/coco_eval.py .\/\n!cp vision\/references\/detection\/engine.py .\/\n!cp vision\/references\/detection\/coco_utils.py .\/","0a2ebd80":"!ls","4dbae266":"!nvidia-smi","cd958d77":"import torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader, Subset\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image, ImageDraw, ImageFont\n\nfrom pycocotools.coco import COCO\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","68a3bbbe":"class CFG:\n    # dir\n    DATA_DIR = \"..\/input\/cowboyoutfits\/\"\n    TRAIN_JSON_PATH = os.path.join(DATA_DIR, \"train.json\")\n    TEST_CSV_PATH = os.path.join(DATA_DIR, \"test.csv\")\n    VALID_CSV_PATH = os.path.join(DATA_DIR, \"valid.csv\")\n    IMG_DIR = os.path.join(DATA_DIR, \"images\")\n    FONT_PATH = 'font\/OpenSans-Semibold.ttf'\n\n    # dataset\n    BATCH_SIZE = 2\n    VALID_IMG_NUM = 500\n    NUM_WORKERS = 2\n    \n    \n    # train & model\n    NUM_CLASS = 6  # num_classes = class_num + 1\n    \n    # \u4e3a\u4e86\u8fd0\u884c\u5feb\uff0c\u8fd9\u91cc\u7528\u7684 3\uff0c\u8c03\u5230 10 \u63d0\u4ea4\u540e\u9a8c\u8bc1\u96c6 map 35 \u5de6\u53f3\uff0c\u699c\u5355\u4e0a\u5927\u6982\u80fd\u5230 60\n    EPOCH = 15\n    PRINT_FREQ_TRAIN = 200\n    PRINT_FREQ_EVAL = 100\n    \n    # optim\n    LR = 0.005\n    WEIGHT_DECAY = 0.0005\n    MOMEMTUM = 0.9\n    \n    # lr_scheduler\n    STEP_SIZE = 5\n    GAMMA = 0.1\n    \n    # pre dict keys\n    KEY_SCORE = \"scores\"\n    KEY_LABELS = \"labels\"\n    KEY_BOXES = \"boxes\"","404e4e97":"coco = COCO(CFG.TRAIN_JSON_PATH)","2d31ccc9":"# \u521b\u5efa id\uff0c \u770b\u4e0b\u8bad\u7ec3\u96c6\u4e2d\u56fe\u7247\u6570\u91cf\nids = list(sorted(coco.imgs.keys()))\nlen(ids)","921bd277":"# \u521b\u5efa\u7528\u4e8e\u7c7b\u578b\u6620\u5c04\u7684\u5b57\u5178\ncat_map = {v['id']: v['name'] for k,v in coco.cats.items()}\n# \u771f\u5b9e\u7269\u4f53\u7c7b\u522b\u4ece1\u5f00\u59cb\ncatid_2_label = {cat_id: index + 1  for index, cat_id in enumerate(list(sorted(cat_map.keys())))}\nlabel_2_catid = {v: k for k, v in catid_2_label.items()}","cab5a128":"catid_2_label","c4adcb26":"# \u663e\u793a 100 - 101 \u5f20\u56fe\u7247\nsmp_ids = ids[100:100 + 2]\nsmp_file = coco.loadImgs(smp_ids)\nsmp_anno_ids = coco.getAnnIds(smp_ids)","3eb21391":"# \u901a\u8fc7 coco\uff0c\u53d6\u51fa\u56fe\u7247\u5bf9\u5e94 anno \u4fe1\u606f\uff0c\u753b\u5230\u56fe\u7247\u4e0a\nfont_size = 40\nfont = ImageFont.truetype(CFG.FONT_PATH, font_size)\nshow_imgs = []\n\nfor index in range(len(smp_ids)):\n    file_name = smp_file[index]['file_name']\n    file_path = os.path.join(CFG.IMG_DIR, file_name)\n    img = Image.open(file_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(img)\n    \n    anno_id = smp_anno_ids[index]\n    targets = coco.loadAnns(anno_id)\n    for target in targets:\n        x_min, y_min, w, h = target['bbox']\n        x_max = x_min + w\n        y_max = y_min + h\n        \n        draw.rectangle((int(x_min), int(y_min), int(x_max), int(y_max)), outline='red', width=5)\n        \n        cat_name = cat_map[target['category_id']]\n        draw.text((int(x_min), int(y_min) - font_size - 10), cat_name, 'red', font, )\n        show_imgs.append(img)","d0e88d87":"plt.imshow(show_imgs[0])","e309e056":"# \u4ece 101 \u5f20\u56fe\u7247\u53ef\u4ee5\u770b\u51fa\uff0c\u633a\u591a\u592a\u9633\u955c\u6ca1\u6709\u6807\u51fa\u6765\nplt.imshow(show_imgs[1])","c03b6acc":"import transforms as T\ntrainforms_train = T.Compose([\n    T.ToTensor(),\n    \n    # \u5185\u90e8\u505a\u4e86 bbox \u53d8\u5e7b\n    T.RandomHorizontalFlip(0.5) \n    \n])\n\ntrainforms_test = T.Compose([\n    T.ToTensor()\n])","c24eee46":"class CowBoyDataSet(Dataset):\n    def __init__(self, coco, img_dir, transforms):\n        self.coco = coco\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.img_ids = list(sorted(coco.imgs.keys()))\n        \n    def __getitem__(self, idx):\n        # \u8fd4\u56de img tensor, bbox, cat_id\n        img_id = self.img_ids[idx]\n        img_name = self.coco.loadImgs(img_id)[0]['file_name']\n        img_path = os.path.join(self.img_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        anno_ids = self.coco.getAnnIds(img_id)\n        annos = self.coco.loadAnns(anno_ids)\n        \n        boxes = []\n        labels = []\n        areas = []\n        iscrowds = []\n        \n        for anno in annos:\n            # anno \u4e2d box \u4e3a x,y,w,h\n            # vison \u4e2d faster-rcnn box \u4e3a x_min, y_min, x_max, y_max\n            x_min, y_min, w, h = anno['bbox']\n            x_max, y_max = x_min + w, y_min + h\n            boxes.append([x_min, y_min, x_max, y_max])\n            \n            cat_id = anno['category_id']\n            label = catid_2_label[cat_id]\n            labels.append(label)\n            areas.append(anno['area'])\n            \n            # \u8868\u793a\u662f\u5426\u591a\u4e2a\u5c0f\u7269\u4f53\u805a\u5728\u4e00\u8d77, false \u4e3a\u5e72\u51c0\u7684\u5355\u4e2a\u7269\u4f53\n            iscrowds.append(anno['iscrowd'])\n            \n        img_id = torch.as_tensor([idx], dtype=torch.int64)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        areas = torch.tensor(areas)\n        iscrowds = torch.tensor(iscrowds)\n            \n        # \u8bad\u7ec3\u4e2d model \u53ea\u7528\u5230 boxes \u548c labels\uff0c\u5176\u4ed6\u53c2\u6570\u662f\u7ed9 coco \u5de5\u5177 evalueate \u65f6\u7528\u7684\n        targets = {\n            'boxes': boxes,\n            'labels': labels,\n            'image_id': img_id,\n            'area': areas,\n            'iscrowd': iscrowds\n        }\n            \n        if self.transforms is not None:\n            img, targets = self.transforms(img, targets)\n            \n        return img, targets\n            \n        \n        \n    def __len__(self):\n        return len(self.img_ids)","18ec92bb":"# \u7531\u4e8e\u76ee\u6807\u68c0\u6d4b\u6bcf\u5f20\u56fe\u7247\u5c3a\u5bf8\u4e0d\u4e00\u6837\uff0c\u8fd9\u4e2a\u51fd\u6570\u4f7f\u5f97 dataloader \u80fd\u8fc7\u6b63\u786e\u5904\u7406\u8fd9\u79cd\u60c5\u51b5\uff0c\u5426\u5219\u4f1a\u62a5\u9519\ndef collate_fn(batch):\n    return tuple(zip(*batch))","8c230761":"# \u8fd9\u91cc\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\uff0c\u6bcf\u6b21\u8fd0\u884c\u90fd\u662f\u968f\u673a\u5212\u5206\u7684\uff0c\u5982\u679c\u9700\u8981\u56fa\u5b9a\u5212\u5206\u4fbf\u4e8e\u7ee7\u7eed\u8bad\u7ec3\uff0c\u9700\u8981\u4fdd\u5b58 indices\ndataset_train = CowBoyDataSet(coco, CFG.IMG_DIR, trainforms_train)\ndataset_eval = CowBoyDataSet(coco, CFG.IMG_DIR, trainforms_test)\n\nindices = torch.randperm(len(dataset_train)).tolist()\ndataset_train = Subset(dataset_train, indices[: -CFG.VALID_IMG_NUM])\ndataset_eval = Subset(dataset_eval, indices[-CFG.VALID_IMG_NUM:])\n\ndataloader_train = DataLoader(\n    dataset_train, CFG.BATCH_SIZE, shuffle=True, num_workers=CFG.NUM_WORKERS,\n    collate_fn=collate_fn)\n\ndataloader_eval = DataLoader(\n    dataset_eval, CFG.BATCH_SIZE, shuffle=False, num_workers=CFG.NUM_WORKERS,\n    collate_fn=collate_fn)","eae17aa2":"def get_fast_rcnn_model():\n    model = fasterrcnn_resnet50_fpn(pretrained=True)\n    \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, CFG.NUM_CLASS)\n    return model","6e90436f":"# \u7531\u4e8e\u5c1d\u8bd5 retinanet \u6548\u679c\u8f83\u5dee\uff0c\u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u8fd9\u79cdweigth\u521b\u5efa\u65b9\u5f0f\u5b9e\u5206\u6709\u95ee\u9898\uff0c\u7ed3\u679c\u548c\u7b2c\u4e00\u79cd\u6bd4\u6ca1\u95ee\u9898\ndef get_faster_rcnn_model_v2():\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n    model_new = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n        num_classes=CFG.NUM_CLASS)\n    \n    pretrained_state_dict = model.state_dict()\n    change_keys = [\n        'roi_heads.box_predictor.cls_score.weight',\n        'roi_heads.box_predictor.cls_score.bias',\n        'roi_heads.box_predictor.bbox_pred.weight',\n        'roi_heads.box_predictor.bbox_pred.bias'\n    ]\n    \n    \n    from_scratch_state_dict = model_new.state_dict()\n    \n   \n    for change_key in change_keys:\n        pretrained_state_dict[change_key] = from_scratch_state_dict[change_key]\n    \n    model_new.load_state_dict(pretrained_state_dict)\n    return model_new","4a60a16c":"# retinanet \u4f7f\u7528\u66f4\u5077\u61d2\u7684\u65b9\u5f0f\u4fee\u6539 weight \uff0c\u4ec5\u6709\u53d7 num_classes \u5f71\u54cd\u7684\u5c42\u7528\u9ed8\u8ba4\u521d\u59cb\u5316\uff0c\u5176\u4ed6\u5c42\u4f7f\u7528 pretrain \u6743\u91cd \n# \u5b58\u5728\u95ee\u9898\uff0c\u8bad\u7ec3\u7ed3\u679c\u8f83\u5dee\uff0c\u7528\u540c\u6837\u65b9\u5f0f\u521d\u59cb\u5316 faster-rcnn \u6ca1\u95ee\u9898\uff0c\u56e0\u6b64\u76ee\u524d\u8fd8\u672a\u5b9a\u4f4d retinanet \u7684\u95ee\u9898\u5728\u54ea\u91cc\ndef get_retinanet_model():\n    model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n    model_new = torchvision.models.detection.retinanet_resnet50_fpn(\n        num_classes=CFG.NUM_CLASS)\n    \n    pretrained_state_dict = model.state_dict()\n    change_keys = [\n        'head.classification_head.cls_logits.weight',\n        'head.classification_head.cls_logits.bias'\n    ]\n    \n    \n    from_scratch_state_dict = model_new.state_dict()\n    \n   \n    for change_key in change_keys:\n        pretrained_state_dict[change_key] = from_scratch_state_dict[change_key]\n    \n    model_new.load_state_dict(pretrained_state_dict)\n    return model_new","b034975b":"model = get_retinanet_model()\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nparam = [p for p in model.parameters() if p.requires_grad]\noptmizer = torch.optim.SGD(param, lr=CFG.LR, momentum=CFG.MOMEMTUM)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optmizer, step_size=CFG.STEP_SIZE, gamma=CFG.GAMMA)","758048b3":"# \u7531\u4e8efaster-rcnn \u5df2\u7ecf\u5c01\u88c5\u597d\u4e86 cls \u548c bbox \u7684 loss\uff0c\u5176\u5b9e\u81ea\u5df1\u5199\u4e00\u4e2a train \u51fd\u6570\u5f88\u7b80\u5355\uff0c\u548c\u56fe\u7247\u5206\u7c7b\u51e0\u4e4e\u4e00\u6837\n# \u8fd9\u91cc\u4f7f\u7528\u4ece github: torch.vison \u62f7\u8d1d\u7684\u5de5\u5177\u7c7b\uff0c\u4e3b\u8981\u662f\u80fd\u8be6\u7ec6\u6253\u5370\u8bad\u7ec3\/\u9a8c\u8bc1\u65f6\u591a\u4e2a\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u4e14\u7ec4\u7ec7\u8f83\u7f8e\u89c2\n# \u8fd8\u6709\u4e00\u70b9\u5f71\u54cd\u8bad\u7ec3\u7684\u662f\uff0c\u5de5\u5177\u7c7b\u4e2d\u4f7f\u7528\u4e86\u53e6\u4e00\u4e2a warmup scheduler\uff0c\u4ee5\u964d\u4f4e\u8bad\u7ec3\u521d\u59cb\u5b66\u4e60\u7387\nfrom engine import train_one_epoch, evaluate\nimport utils","76583294":"model.to(device)","d1d4df98":"!mkdir train_state","0f1acd42":"# \u8fd9\u91cc\u4e3a\u4e86\u901f\u5ea6\u5feb\uff0c\u6f14\u793a\u4ee3\u7801\u53ea\u8bad\u7ec3 3 epoch\uff08\u534a\u5c0f\u65f6\u5de6\u53f3\u6267\u884c\u5b8c\uff09\uff0c\u8fbe\u5230 10 epoch\uff08\u7ea62\u5c0f\u65f6\uff09 85+ \u6c34\u5e73\n# \u8fd9\u4f1a\u5bfc\u81f4\u6211\u4f7f\u7528\u7684 lr_scheduler \u672a\u751f\u6548\uff08\u6bcf 3epoch \u964d\u4f4e 90% \u5b66\u4e60\u7387\uff09\uff0c\u7136\u540e\u4fdd\u5b58\u6a21\u578b\u8bad\u7ec3\u72b6\u6001\uff0c\u65b9\u4fbf\u540e\u7eed\u7ee7\u7eed\u8bad\u7ec3\n# \u5982\u679c\u7528\u4e8e\u6bd4\u8d5b\u63d0\u4ea4\uff0c\u53ef\u4ee5\u8bad\u7ec3 10+ epoch\nfor epoch in range(CFG.EPOCH):\n    # train\n    train_one_epoch(model, optmizer, dataloader_train, device, epoch, CFG.PRINT_FREQ_TRAIN)\n    lr_scheduler.step()\n    \n    # eval\n    evaluate(model, dataloader_eval, device)\n    \n    # save\n    if epoch %3 == 0 and epoch > 1:\n      train_state_dict = {\n          'model':model.state_dict(),\n          'optim':optmizer.state_dict(),\n          'lr_scheduler':lr_scheduler.state_dict(),\n          'indices':indices,\n          'info':'cow_bow_retinanet-epoch' + str(epoch)\n      }\n      torch.save(train_state_dict, \n                 'train_state\/retinanet-epoch' + str(epoch))","beb9c94e":"# \u4fdd\u5b58\u8bad\u7ec3\u72b6\u6001\uff0c\u4ee5\u4fbf\u968f\u65f6\u4e2d\u65ad\u3001\u540e\u7eed\u7ee7\u7eed\u8bad\u7ec3\ntrain_state_dict = {\n    'model':model.state_dict(),\n    'optim':optmizer.state_dict(),\n    'lr_scheduler':lr_scheduler.state_dict(),\n    'indices':indices,\n    'info':'cow_bow_fascter-rcnn-epoch2'\n}\n\ntorch.save(\n    train_state_dict, \n    'train_state_retinanet_epoch' + str(CFG.EPOCH))","ec68741c":"class CowBoyPredictDataSet(Dataset):\n    def __init__(self, csv_path, img_dir, transforms):\n        self.csv_path = csv_path\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.df_predict = pd.read_csv(self.csv_path)\n        \n    def __getitem__(self, idx):\n        # \u8fd4\u56de img tensor\n        row = self.df_predict.iloc[idx]\n        row_id = row['id']\n        img_name = row['file_name']\n        \n        img_path = os.path.join(self.img_dir, img_name)\n        img = Image.open(img_path).convert(\"RGB\")\n        \n            \n        if self.transforms is not None:\n            img, _ = self.transforms(img, None)\n            \n        return img, row_id\n            \n        \n        \n    def __len__(self):\n        return self.df_predict.shape[0]","42fcec44":"dataset_predict = CowBoyPredictDataSet(CFG.VALID_CSV_PATH, CFG.IMG_DIR, trainforms_test)\n\ndataloader_predict = DataLoader(\n    dataset_predict, 1, shuffle=False, num_workers=CFG.NUM_WORKERS,\n    collate_fn=collate_fn)","55079889":"font_size = 40\nfont = ImageFont.truetype(CFG.FONT_PATH, font_size)\ntramsform_pil = torchvision.transforms.ToPILImage()\n\n\ndef see_pre_result(index, device='cpu', threshold = 0.5):\n    img_tensor, image_ids = dataset_predict[index]\n    img = tramsform_pil(img_tensor)\n    \n    model.eval()\n    with torch.no_grad():\n        img_tensor = img_tensor.unsqueeze(0)\n        img_tensor = img_tensor.to(device)\n        pre_batch = model(img_tensor)\n\n    \n        \n    pre = pre_batch[0]\n    print(pre['labels'])\n    print(pre['scores'])\n    \n    draw = ImageDraw.Draw(img)\n    length = pre['labels'].shape[0]\n    for i in range(length):\n        score = pre['scores'][i].item()\n        if score < threshold:\n            continue\n        \n        x_min, y_min, x_max, y_max = pre['boxes'][i]\n        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n\n        label_id = pre['labels'][i].item()\n        cat_name = cat_map[label_2_catid[label_id]]\n        color = (100 + 30 * label_id, 0, 0)\n\n        draw.rectangle((x_min, y_min, x_max, y_max), outline=color, width=5)\n        draw.text((x_min, y_min - font_size), cat_name, color, font)\n\n    plt.imshow(img)\n    \n    return pre_batch, image_ids","1c8a0855":"pre_batch = see_pre_result(100, device)","b4eed842":"pre_batch = see_pre_result(500, device)","acfcd321":"pre_batch = see_pre_result(700, device)","3066f6e5":"# \u8fdb\u884c\u9884\u6d4b\npre_results = []\nmodel.eval()\nwith torch.no_grad():\n  for img_tensor, image_ids in dataloader_predict:\n    img_tensor = img_tensor[0]\n    image_ids = image_ids[0]\n    img_tensor = img_tensor.unsqueeze(0)\n    img_tensor = img_tensor.to(device)\n    pre_batch = model(img_tensor)\n    pre_batch = pre_batch\n    pre_results.append((pre_batch, image_ids))","6cddbdf4":"# \u6a21\u578b\u8f93\u51fa\uff0c\u8f6c\u5316\u4e3a\u63d0\u4ea4\u7528\u7684 dict \u7684\u51fd\u6570\uff0c\u65b9\u4fbf\u540e\u7eed\u76f4\u63a5 josn dump\n# \uff01\u6ce8\u610f\uff1a\u8fd9\u4e2a\u51fd\u6570\u5199\u7684\u7b80\u5355\uff0c\u4ec5\u652f\u6301\u9884\u6d4b\u65f6 batch size \u4e3a 1\ndef out_to_dict(pre_results, threshold=0.5):\n    results = []\n    for one_pre in pre_results:\n        image_id = int(one_pre[1])\n        \n        # \u9700\u8981\u56fa\u5b9a\u9884\u6d4b\u8bd5\u662f\uff0cbatch size \u4e3a 1\uff0c\u5426\u5219\u51fa\u9519\n        data_dict = one_pre[0][0]\n        if CFG.KEY_SCORE not in data_dict:\n            continue\n            \n        pre_num = len(data_dict[CFG.KEY_SCORE])\n        for i in range(pre_num):\n            score = data_dict[CFG.KEY_SCORE][i].to('cpu').item()\n            if score < threshold:\n                continue\n                \n            label_id = int(data_dict[CFG.KEY_LABELS][i].to('cpu').item())\n            bbox = data_dict[CFG.KEY_BOXES][i].to('cpu').tolist()\n            w = bbox[2] - bbox[0]\n            h = bbox[3] - bbox[1]\n            bbox[2] = w\n            bbox[3] = h\n            out = {\n                'image_id': image_id,\n                'category_id': label_2_catid[label_id],\n                'bbox': bbox,\n                'score': score\n            }\n            results.append(out)\n            \n    return results","fe9b0e8b":"# \u68c0\u67e5 bbox\uff0c \u6a21\u578b\u8f93\u51fa\u662f x_min, y_min, x_max, y_max, \u4e0a\u4f20\u9700\u8981 x_min, y_min, w, h\nresult_dict = out_to_dict(pre_results)","887581ab":"# \u770b\u4e00\u884c\u7528\u4e8e\u63d0\u4ea4 bbox \u7684\u7ed3\u679c\uff0c\u8fd9\u5e76\u4e0d\u662f\u4e00\u5f20\u56fe\u7247\u4e0a\u6240\u6709\u9884\u6d4b\u51fa\u7684 bbox\uff0c\u56e0\u4e3a\u6309\u63d0\u4ea4\u683c\u5f0f\u4e00\u5f20\u56fe\u7247\u591a\u4e2a bbox \u4f1a\u5206\u591a\u884c\u8f93\u51fa\ndf_valid = pd.read_csv(CFG.VALID_CSV_PATH)\ndata1 = result_dict[1]\nimg1_id = data1['image_id']\nfile1 = os.path.join(CFG.IMG_DIR, df_valid[df_valid.id == img1_id].iloc[0].file_name)\nimg1 = Image.open(file1).convert(\"RGB\")\n\ndraw = ImageDraw.Draw(img1)\nx_min, y_min, w, h = data1['bbox']\nx_max = x_min + w\ny_max = y_min + h\n\ndraw.rectangle((int(x_min), int(y_min), int(x_max), int(y_max)), outline='red', width=5)\n\ncat_name = cat_map[data1['category_id']]\ndraw.text((int(x_min), int(y_min) - font_size), cat_name, 'red', font, )\nplt.imshow(img1)","2b216c8e":"import json\n# \u8f93\u51fa\u8981\u6c42 zip \u6587\u4ef6\uff0c zip \u4e2d\u4fdd\u62a4\u4e00\u4e2a\u53eb answer.json \u7684\u6587\u4ef6\nout_path = 'answer.json'\nwith open(out_path, 'w') as f:\n  json.dump(result_dict, f)","bdf48397":"# \u5b89\u88c5\u5de5\u5177\u5305\uff0c\u4e0b\u8f7d\u5b57\u4f53\u3001torch.vision \u4e2d \u51e0\u4e2a\u8f85\u52a9\u5de5\u5177\u7c7b","4f0c7f51":"# \u5b9a\u4e49\u6a21\u578b - \u57fa\u4e8e torch \u81ea\u5e26\u7684 faster-rcnn \u548c retinanet \u8fdb\u4e0b\u4fee\u6539","4b7b6306":"## \u5212\u5206\u8bad\u7ec3\u3001\u9a8c\u8bc1\u96c6\uff0c\u521b\u5efa dataloader","136f1dfd":"## \u62ff\u4e00\u5f20\u770b\u770b\uff0c\u4e3b\u8981\u68c0\u67e5\u63d0\u4ea4\u7528\u6587\u4ef6\u7684\u5750\u6807\u95ee\u9898","84face80":"* evaluate \u7684\u7b2c\u4e00\u884c\uff0c\u5373\u4e3a\u6bd4\u8d5b\u6240\u7528\u7684\u8bc4\u4f30\u6307\u6807\uff1a\n* Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261","40877d49":"## torch \u81ea\u5e26\u7684 faster-rcnn\uff0c\u9700\u8981\u4fee\u6539 box_predictor \u4ee5\u6539\u53d8 num_classes","b9bc0986":"# \u663e\u793a 2 \u5f20\u56fe\u7247\u770b\u770b","bec72723":"## \u8bc4\u4f30\u6307\u6807","ac416b45":"## \u770b\u51e0\u5f20\u9884\u6d4b\u7ed3\u679c","c800230c":"# \u7528 coco \u8bfb\u53d6\u6570\u636e","bedb2031":"* \u4f7f\u7528 pytorch \u81ea\u5e26\u7684 fastrer-rcnn\uff0c \u8bad\u7ec32\u5c0f\u65f6\uff0c\u80fd\u62ff\u5230\u516c\u699c mAP 58.9 \u7684\u6548\u679c\n* notebool \u5c55\u793a\u4e86\u4f7f\u7528 torchvision \u4e2d fastrer-rcnn\uff0c\u9a8c\u8bc1\u96c6\u53d6\u5f97 map 35 \u5de6\u53f3\uff0cPB \u80fd\u5230 58.99 \u7684\u4ee3\u7801\u6d41\u7a0b \n* \u4ee5\u4e0a\u662f\u8bad\u7ec3 10 \u4e2a epoch \u7ed3\u679c\uff0c\u9700\u8981\u5927\u7ea62h\uff1b\u8fd9\u91cc\u6f14\u793a\u4e3a\u4e86\u8fd0\u884c\u901f\u5ea6\u5feb\u4e00\u70b9\uff0c epoch \u4ec5\u8bbe\u4e86 3","1f6f73a1":"## train model","34c1980f":"# \u9884\u6d4b","83cb5e37":"## \u7ed3\u679c\u8f6c\u6362\u4e3a json","20d58fd2":"## \u5f15\u5165 torch.vison github \u4e2d\u7684 Transforms\uff0c\u80fd\u652f\u6301\u7b80\u5355\u7684\u57fa\u4e8e\u76ee\u6807\u68c0\u6d4b\u7684\u53d8\u6362","d33d7bc3":"# \u8bad\u7ec3","1c4b11b7":"## \u5c06 dict dump \u4e3a\u63d0\u4ea4\u6240\u7528\u6587\u4ef6 ","d83f2621":"## CowBoyDataSet","3b7e9926":"# \u53c2\u6570\u5b9a\u4e49","30f00047":"## \u521b\u5efa\u6a21\u578b\u3001\u4f18\u5316\u5668\u3001lr_scheduler","6e8a751f":"## \u9884\u6d4b\u7528\u7684 dataset \u548c dataloader","9d3a90c0":"# \u7b80\u4ecb","500a2194":"# \u4fdd\u5b58\u8bad\u7ec3\u72b6\u6001","b4ce943e":"# \u5b9a\u4e49 Transforms\u3001 DataSet \u3001DataLoader"}}