{"cell_type":{"62467bb9":"code","69a9103b":"code","4854f447":"code","f2b9216a":"code","64bd4907":"code","fd0f675e":"code","aa5cd2d3":"code","33254deb":"code","c05d7114":"code","add60036":"code","bf9e6adb":"code","e7177553":"code","69e75a72":"code","945e1106":"code","20a4a2c9":"code","71af64e0":"code","dc205b7c":"code","4c6b9b50":"code","e6414f19":"code","65f6303d":"code","5eeab92a":"code","35da0fec":"code","4c4e02cb":"code","d680f68d":"code","96c591bb":"code","9c53cd37":"code","6a4cae9c":"code","4c641fdd":"code","be6407ee":"code","6f02494a":"code","fd297744":"code","6538746c":"code","2d852092":"code","795ef2c6":"code","51be8243":"code","453f0065":"markdown","e553e75e":"markdown","0894323e":"markdown","42510c23":"markdown","553e76d4":"markdown","7abdf3ac":"markdown","e4484b0d":"markdown","c4b90b13":"markdown","12c65077":"markdown","8e5089a4":"markdown","6e4dc826":"markdown","bbd1fcb2":"markdown","2cfca3e2":"markdown","f428f98d":"markdown","1f1e2b69":"markdown","edf8a09d":"markdown"},"source":{"62467bb9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","69a9103b":"train_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\", dtype = \"int16\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\", dtype = \"int16\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\", dtype = \"int16\")\ntrain_y = train_df.iloc[:,0]\nall_df = np.concatenate((train_df.values[:,1:], test_df.values))\nn_train = train_y.shape[0]\nn_test = test_df.shape[0]","4854f447":"n_train, n_test","f2b9216a":"Cov_mat = np.cov(all_df, rowvar = False, bias = True)","64bd4907":"variances = np.sort(np.diagonal(Cov_mat))[::-1]\nvariances_pca = np.sort(np.linalg.eigvalsh(Cov_mat))[::-1]","fd0f675e":"cumsum_var = np.cumsum(variances)\ncumsum_var_pca =  np.cumsum(variances_pca)","aa5cd2d3":"fig, ax = plt.subplots(figsize = (8,5))\nax = plt.plot(cumsum_var\/cumsum_var[-1], label = \"original\", lw = 2)\nax = plt.plot(cumsum_var_pca\/cumsum_var_pca[-1], label = \"pca\", lw = 2)\nplt.legend(fontsize=14)\nplt.ylabel(\"Fraction of total variance\",fontsize=14)\nplt.xlabel(\"Dimensions\",fontsize=14)\nplt.show()","33254deb":"ori_90_percent = np.where(cumsum_var\/cumsum_var[-1] > 0.9)[0][0]\npca_90_percent = np.where(cumsum_var_pca\/cumsum_var_pca[-1] > 0.9)[0][0]\nori_95_percent = np.where(cumsum_var\/cumsum_var[-1] > 0.95)[0][0]\npca_95_percent = np.where(cumsum_var_pca\/cumsum_var_pca[-1] > 0.95)[0][0]\nprint(\"Before PCA, 90% of total variances is in\", ori_90_percent, \"dimensions\")\nprint(\" After PCA, 90% of total variances is in\", pca_90_percent, \"dimensions\")\nprint(\"Before PCA, 95% of total variances is in\", ori_95_percent, \"dimensions\")\nprint(\" After PCA, 95% of total variances is in\", pca_95_percent, \"dimensions\")","c05d7114":"eigenvalues, eigenvectors = np.linalg.eigh(Cov_mat)","add60036":"fix1, ax1 = plt.subplots(3,8, figsize = (16,7))\nfor i in range(0,24):\n    y = i % 8\n    x = int(i \/ 8)\n    ax1[x, y].imshow(np.reshape(eigenvectors[:,-(i+1)], (28,28)), cmap = \"gray\")\n    ax1[x, y].set_title(\"mode\" + str((i+1)))  \n    ax1[x, y].set_xticks([], [])\n    ax1[x, y].set_yticks([], [])\nplt.show()\n","bf9e6adb":"def spectrum_plot_multiple(value):\n    sample = np.random.choice(np.where(train_y == value)[0],3)\n    range_x = 20\n    fig3, ax3 = plt.subplots(3,2, figsize = (15,16))\n    for i in range(3):\n        \n        ax3[i,0].imshow(np.reshape(all_df[sample[i],:],(28,28)), cmap = \"gray\")\n        ax3[i,0].set_title(\"Original Image\", fontsize=14)\n        ax3[i,1].bar(range(1,range_x+1),np.flip(np.dot(all_df[sample[i],:], eigenvectors)[-range_x:]))\n        ax3[i,1].set_xlabel(\"Modes\", fontsize=14)\n        ax3[i,1].set_ylabel(\"Amplitude\", fontsize=14)\n    \n    plt.show()    ","e7177553":"spectrum_plot_multiple(value = 0)","69e75a72":"spectrum_plot_multiple(value = 1)","945e1106":"spectrum_plot_multiple(value = 2)","20a4a2c9":"spectrum_plot_multiple(value = 4)","71af64e0":"spectrum_plot_multiple(value = 9)","dc205b7c":"def pca_projection(mode, sample):\n    \n    U = eigenvectors[:,-mode:]\n    U2 = np.dot(all_df[sample,:], U)\n    U.shape, U2.shape, (U2*U).shape\n    P = np.sum(U2*U, axis = 1)\n    return np.reshape(P,(28,28))\n\ndef plot_projections(sample):\n    PR1 = pca_projection(mode=86, sample = sample)\n    PR2 = pca_projection(mode=153, sample = sample)\n    fig2, ax2 = plt.subplots(1,3, figsize=(10,4))\n    ax2[0].imshow(PR1, cmap = \"gray\")\n    ax2[0].set_title(\"PCA 86 dimensions\")  \n    ax2[1].imshow(PR1, cmap = \"gray\")\n    ax2[1].set_title(\"PCA 153 dimensions\")  \n    ax2[2].imshow(np.reshape(all_df[sample,:], (28,28)), cmap = \"gray\")\n    ax2[2].set_title(\"Original Image\")  \n    \n    for i in range(3):\n        ax2[i].set_xticks([], [])\n        ax2[i].set_yticks([], [])\n    \n    plt.show()","4c6b9b50":"plot_projections(sample = 55)","e6414f19":"plot_projections(sample = 1001)","65f6303d":"plot_projections(sample = 9000)","5eeab92a":"all_pca = np.dot(all_df, eigenvectors[:,-86:])","35da0fec":"pca_df = pd.DataFrame(np.flip(all_pca[:n_train], axis = 1))\npca_df[\"label\"] = train_y","4c4e02cb":"plt.figure(figsize=(7,7))\nsns.scatterplot(data = pca_df.iloc[4000:7000,:], x=0, y=1, hue = \"label\", palette = 'Set1', alpha = 0.8)","d680f68d":"sns.relplot(data = pca_df.iloc[4000:15000,:], x=0, y=1, col = \"label\", hue = \"label\", palette = 'Set1', alpha = 0.4, col_wrap = 4)","96c591bb":"from sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","9c53cd37":"X_train, X_val, y_train, y_val = train_test_split(all_pca[:n_train], train_y.values, test_size = 0.03)","6a4cae9c":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","4c641fdd":"model_svc = SVC(kernel = \"poly\", C = 1.0, coef0 = 1.0, degree = 2)","be6407ee":"time1 = time.time()\nmodel_svc.fit(X_train, y_train)\ntime2 = time.time()\nprint(\"Training by SVM (quadratic kernel) took only\", int(time2 - time1), \"sec\" )","6f02494a":"pred_train = model_svc.predict(X_train)\nprint(\"train error = \", np.sum(pred_train != y_train)\/y_train.shape[0])","fd297744":"pred_val = model_svc.predict(X_val)\nprint(\"validation error = \", np.sum(pred_val != y_val)\/y_val.shape[0])","6538746c":"confusion_matrix(y_val, pred_val)","2d852092":"pred_test = model_svc.predict(all_pca[n_train:])","795ef2c6":"test_submission = sample_submission\ntest_submission[\"Label\"] = pred_test","51be8243":"test_submission.to_csv('submission.csv',index=False)","453f0065":"### 2.4 Distribution \nFollowing figure is scatterplot of each digit along with two principal components (1st and 2nd modes). It looks like decision boundary may be curved. But it is still hard to see.","e553e75e":"### 2.1 Mode Vectors\n24 eigenvectors (mode vectors) are plotted in 28x28 images. All images consist of those eigenvectors times eigenvalues.\nBtw, mode1 looks like \"0\"!. So, the digit \"0\" must have strong amplitude in mode1. Lets see the results.","0894323e":"## 1. Variances and Eigenvalues\nTo check effectiveness of PCA, variances of original data and eigenvalues shall be compared.","42510c23":"## 2. Spectrum Analysis by PCA\nFirst, to calculate eigenvalues and eigenvectors.","553e76d4":"But when they are plotted separately, distribution of each digit looks like concentrated like cluster. It must be interesting to further sub-categorize a digit by k-means cluster analysis.","7abdf3ac":"### 3.2 Training error\nSurprisingly, quadratic kernel perfectly classified train data. That's why training finished very fast. In this case, further parameter tuning is unnecessary if validation accuracy is also high.","e4484b0d":"### 2.2 Spectrum Analysis","c4b90b13":"## 4. Conclusion\nWhile CNN gets very high accuracy, in my submission it was 0.994, with longer training time, SVM recorded 0.978 after PCA. Considering this short training time (about 20sec), SVM is still good classifier. However, when training data is not clearly separable, it would take much longer time to converge. Furthermore, it must be interesting to analyse some digits by k-means cluster analysis.","12c65077":"This function randomly choose 3 samples of specified value. Then, it plots original image and amplitudes of each spectra","8e5089a4":"## 3. Support Vector Machine\n### 3.1 Model selection\nBecause distributions of each digits are consentrated and those shapes are round, quadratic kernel would be appropriate for classification by SVM.","6e4dc826":"## PCA, Spectrum Analysis and SVM\n## Background\nAlmost everyone predicts digit by CNN which easily gives more than 99% accuracy. It is still curious, however, how good performance of other methods such as Support Vector Machine especially if images are converted by Principal Component Analysis.","bbd1fcb2":"### Submission","2cfca3e2":"As anticipated, digit 0 has strong response (netative response) in the 1st mode. ","f428f98d":"### 3.3 Validation error\nValidation error is very small although a bit higher than training error. Therefore, SVM model is chosen.","1f1e2b69":"### Result\nBy PCA, the numbers of dimension can be reduced from 784 to 86. It is really significant reduction if the prediction still gives high accuracy. Before machine learning, let's see what is going on in detail when images are transformed into principal components (PCA).","edf8a09d":"### 2.3 Projection to eigenvectors\nImage can be re-constructed by following function \"pca_projection\". According to the result of projection in 86 and 153 dimensions, it is still possible to see which digit they are. Moreover, almost no difference between 86 and 153. Parhaps, just 5% of variance does not have significant impact at least for human's perceptions."}}