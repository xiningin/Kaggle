{"cell_type":{"60d5dce8":"code","b3a4c88f":"code","b1ee81d9":"code","830bc0e2":"code","1510406b":"code","22afb577":"code","84336c46":"code","a41fc83d":"code","f357665a":"code","a7d3c28c":"code","18807c4c":"code","b73cde6e":"code","93177b65":"code","363ad971":"code","28287342":"code","ad8257f2":"code","72e9c691":"code","792b7bb0":"code","d7ac0c98":"code","54fac0c3":"code","3dcc1647":"code","18b1b572":"code","69600d61":"code","39ca6823":"code","167763a6":"code","f164a91e":"code","c2b13d6b":"code","eb6b5959":"code","ad2e1c01":"code","a24cf119":"code","4af416d6":"code","c33aff58":"code","f2e64443":"code","d8c0a67c":"code","8205355e":"code","f32e2ea2":"code","69f53a73":"code","6378738b":"code","2d2f9ce3":"code","a28d6e4d":"code","8977e890":"code","f9926f99":"code","6c01f47a":"code","cef4db03":"code","038a2400":"code","db9cfc70":"code","64615485":"code","72d9b657":"markdown","9d04af19":"markdown","5e3868df":"markdown","096d9ab6":"markdown","2952e3b6":"markdown","e27f71bb":"markdown","4e82873d":"markdown","bb17f865":"markdown"},"source":{"60d5dce8":"import pandas as pd\n\n'''\n\nBasically the training_dataset contains ~3.8 years of data points for each asset\nDuring Forecasting I have taken only 1 year worth of recent data (only for BTC)\n\n'''\n\n\ndf_asset_details = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\nprint('df_asset_details: ',df_asset_details.shape)\ndf_sub_sample = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_sample_submission.csv')\nprint('sub_sample: ',df_sub_sample.shape)\ndf_sup_train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv').iloc[-5000000:]\nprint('sup_train: ',df_sup_train.shape)\ndf_train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv').iloc[-5000000:]\nprint('train_shape: ', df_train.shape)","b3a4c88f":"print('Train_set_time_entry_range: ',df_train['timestamp'].iloc[0], ' - ', df_train['timestamp'].iloc[-1])\nprint()\nprint('Supplemetary_train_set_time_entry_range: ', df_sup_train['timestamp'].iloc[0], ' - ', df_sup_train['timestamp'].iloc[-1])","b1ee81d9":"df_temp = pd.concat([df_train, df_sup_train], axis = 0 )\nprint('df_temp_before: ', df_temp.shape)\ndf_temp = df_temp.drop_duplicates()\nprint('df_temp_after: ', df_temp.shape)","830bc0e2":"df_temp.sort_values('timestamp')\nprint('Train\/Sup_set_time_entry_range: ',df_temp['timestamp'].iloc[0], ' - ', df_temp['timestamp'].iloc[-1])\nprint()\nprint(df_temp['timestamp'].value_counts())","1510406b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('darkgrid')\n\nfig, axs = plt.subplots(1,1, figsize=(16,12), dpi = 80)\n\nsns.histplot(data = df_temp.Asset_ID, color='black', discrete=True, ax = axs, stat = 'density', kde=False)\nplt.legend()\n\nplt.tight_layout(pad=2)\nplt.show()","22afb577":"'''\ntimestamp - A timestamp for the minute covered by the row.\nAsset_ID - An ID code for the cryptoasset.\nCount - The number of trades that took place this minute.\nOpen - The USD price at the beginning of the minute.\nHigh - The highest USD price during the minute.\nLow - The lowest USD price during the minute.\nClose - The USD price at the end of the minute.\nVolume - The number of cryptoasset units traded during the minute.\nVWAP - The volume weighted average price for the minute.\nTarget - Residual log-return over 15 minute time-data. See the 'Prediction and Evaluation' section of this notebook for details of how the target is calculated.\n'''\nprint('Done!')","84336c46":"from IPython.display import display\nprint('Asset_Details: ')\nprint()\ndisplay(df_asset_details.head())\nprint('Sample_Submission: ')\nprint()\ndisplay(df_sub_sample.head())\nprint('Supplementary_train_samples: ')\nprint()\ndisplay(df_sup_train.head())\nprint('Train_samples: ')\nprint()\ndisplay(df_train.head())\nprint('--'*20)\nasset_info = df_sup_train.groupby('Asset_ID')['timestamp'].agg('count')\nprint(asset_info)\nprint('--'*20)\nprint(df_train.info())\nprint('--'*20)\nprint(df_train.isnull().sum())","a41fc83d":"import gc\ndel df_train\ngc.collect()\ndf_train = df_temp[:]\nasset_info = df_train.groupby('Asset_ID')['timestamp']\nprint(asset_info.agg('value_counts'))\nasset_info_time = asset_info.agg('unique')\n","f357665a":"from pprint import pprint\ninv_asset_dict = { key:value for key, value in zip(df_asset_details['Asset_ID'],df_asset_details['Asset_Name'])}\npprint(inv_asset_dict)\n\nasset_name = list(inv_asset_dict)\nprint('num_assets: ', len(asset_name))\n","a7d3c28c":"asset_info_open = df_train.groupby('Asset_ID')['Open']\nasset_info_open = asset_info_open.agg(lambda x: list(x)) ## using anonymous function to form a list of all gropued \nasset_info_close = df_train.groupby('Asset_ID')['Close']\nasset_info_close = asset_info_close.agg(lambda x: list(x)) ## using anonymous function to form a list of all gropued \nasset_info_high = df_train.groupby('Asset_ID')['High']\nasset_info_high = asset_info_high.agg(lambda x: list(x)) ## using anonymous function to form a list of all gropued \nasset_info_low = df_train.groupby('Asset_ID')['Low']\nasset_info_low = asset_info_low.agg(lambda x: list(x)) ## using anonymous function to form a list of all gropued \n\n## Asset_IDs\nprint(asset_info_open.head())\nprint()\nprint(asset_info_close.head())\nprint()\nprint(asset_info_high.head())\nprint()\nprint(asset_info_low.head())\n","18807c4c":"#'''## EDA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint('**'*20,'1000 mins | 16.66 hrs data_represented', '**'*20)\nprint()\nsns.set_style('darkgrid')\nfig, axs = plt.subplots(7,2, figsize=(16,14))\nstep = 0\nfor i in range(2):\n    for j in range(7):\n        axs[j,i].set_title(inv_asset_dict.get(step))\n        axs[j,i].scatter(asset_info_time[step][:1000],asset_info_open[step][:1000], color='red', marker='.', label = 'Open')\n        axs[j,i].scatter(asset_info_time[step][:1000],asset_info_close[step][:1000], color = 'yellow', alpha = 0.2, marker='.', label='Close')\n        axs[j,i].legend()\n        axs[j,i].set_xlabel('time')\n        axs[j,i].set_ylabel('Open\/Close Price')\n        step+=1\n\nplt.tight_layout(pad=2)\n#'''\nprint('Done!')","b73cde6e":"#'''\nprint('**'*20,'1000 mins | 16.66 hrs data_represented', '**'*20)\nprint()\nsns.set_style('darkgrid')\nfig, axs = plt.subplots(7,2, figsize=(16,14))\nstep = 0\nfor i in range(2):\n    for j in range(7):\n        axs[j,i].set_title(inv_asset_dict.get(step))\n        axs[j,i].plot(asset_info_time[step][:1000],asset_info_high[step][:1000], ',b' ,label = 'High')\n        axs[j,i].plot(asset_info_time[step][:1000],asset_info_low[step][:1000], ',r',label='Low')\n        axs[j,i].legend()\n        step+=1\nplt.tight_layout(pad=2)#'''\n\nprint('Done!')","93177b65":"asset_info_count = df_train.groupby('Asset_ID')['Count']\nasset_info_count = asset_info_count.agg(lambda x: list(x))\nasset_info_vol = df_train.groupby('Asset_ID')['Volume']\nasset_info_vol = asset_info_vol.agg(lambda x: list(x)) \n\nprint(asset_info_count.head())\nprint()\nprint(asset_info_vol.head())","363ad971":"#'''\nprint()\nprint('**'*20,'|| Trades\/Transactions per minute ||', '**'*20)\nprint()\nsns.set_style('darkgrid')\nfig, axs = plt.subplots(7,2, figsize=(20,18))\nstep = 0\nfor i in range(2):\n    for j in range(7):\n        axs[j,i].set_title(inv_asset_dict.get(step))\n        axs[j,i].bar(asset_info_time[step][:1000],asset_info_count[step][:1000], edgecolor = 'black')\n        axs[j,i].set_xlabel('timestamps')\n        axs[j,i].set_ylabel('Trade_Count')\n        step+=1\nplt.tight_layout(pad=2)\nplt.show()\nprint()\nprint('**'*20,'|| Volume per minute ||', '**'*20)\nprint()\nfig, axs = plt.subplots(7,2, figsize=(20,18))\nstep = 0\nfor i in range(2):\n    for j in range(7):\n        axs[j,i].set_title(inv_asset_dict.get(step))\n        axs[j,i].bar(asset_info_time[step][:1000],asset_info_vol[step][:1000], edgecolor = 'black')\n        axs[j,i].set_xlabel('timestamps')\n        axs[j,i].set_ylabel('Volume')\n        step+=1\nplt.tight_layout(pad=2)\nplt.show()\n#'''\nprint('Done!')","28287342":"asset_info_targ = df_train.groupby('Asset_ID')['Target']\nasset_info_targ = asset_info_targ.agg(lambda x: list(x)) \n\nprint(asset_info_targ.head())\nasset_dict = {value:key for key, value in inv_asset_dict.items()}","ad8257f2":"#'''\nimport random\nprint()\nprint('**'*20,'|| Residualised Return per minute ||', '**'*20)\nprint()\n\ncolors = ['cyan','green','red','blue','gold','red','black','orange','magenta','deeppink','lime','slategray','yellow','darkviolet'] ## all possible colors \nname_plot = ['Bitcoin', 'Ethereum', 'Dogecoin']\n\n\nfig, axs = plt.subplots(1,1, figsize=(18,14))\n#axs[0].set_title(inv_asset_dict.get(step))\nfor name in name_plot:\n    #step = random.randint(0,len(list(asset_dict))-1) ## random_sample 3 coins\n    step = asset_dict.get(name)\n    axs.plot(asset_info_time[step][:10000],asset_info_targ[step][:10000], c=colors[step], label=inv_asset_dict.get(step))\naxs.set_xlabel('Timestamps')\naxs.set_ylabel('Res. Returns')\naxs.legend()\n#plt.tight_layout(pad=2)\nplt.show()\n#'''\nprint('Done!')","72e9c691":"print(df_temp.info())\nprint('**'*40)\nprint(df_temp.isnull().sum())\nprint('**'*40)\ndf_temp_2 = df_temp.fillna(method = 'ffill')\nprint(df_temp_2.isnull().sum())","792b7bb0":"import numpy as np\nimport gc\n\ndf_sep_dict = {}\n\nprint('df_temp_2_shape : ', df_temp_2.shape)\n\nfor key in list(asset_dict):\n    asset_dict = {value:key for key, value in inv_asset_dict.items()}\n    mid = asset_dict[key]\n    df_mid = df_temp_2[df_temp_2['Asset_ID'] == mid] \n    df_mid.index = df_mid.timestamp\n    df_mid = df_mid.drop('timestamp', axis = 1)\n    print(key+' _before : ')\n    print((df_mid.index[1:] - df_mid.index[:-1]).value_counts().head())\n    df_mid = df_mid.reindex(range(df_mid.index[0], df_mid.index[-1]+60, 60), method = 'pad') ## gap filling in the continumm to a constant value\n    print()\n    print(key+' _after : ')\n    print((df_mid.index[1:] - df_mid.index[:-1]).value_counts().head())\n    df_sep_dict[key] = df_mid\n    print('**'*20)\n\n\ndel df_train\ndel df_temp    \ndel df_temp_2\ngc.collect()\n","d7ac0c98":"#'''\nfig, axs = plt.subplots(7,2, figsize=(24,18))\n\nreq_name = ['Bitcoin', 'Ethereum','Dogecoin']\nfor step, i in enumerate(df_asset_details['Asset_Name'].tolist()):\n    if step < 7:\n        axs[step,0].plot(df_sep_dict[i].index, df_sep_dict[i].Close, ',k')\n        axs[step,0].set_title(i + ' (Complete_train_space) ')\n        axs[step,0].set_xlabel('Time_Axis')\n        axs[step,0].set_ylabel('Closing_Price')\n    else:\n        axs[step-7,1].plot(df_sep_dict[i].index, df_sep_dict[i].Close, ',k')\n        axs[step-7,1].set_title(i + ' (Complete_train_space) ')\n        axs[step-7,1].set_xlabel('Time_Axis')\n        axs[step-7,1].set_ylabel('Closing_Price')\n\nplt.tight_layout(pad=2)\nplt.show()\n\nprint()\nprint('**'*20, 'VWAP_plot (on_complete_trian_space)', \"**\"*20)\nprint()\n\nfig, axs = plt.subplots(7,2, figsize=(24,18))\n\nreq_name = ['Bitcoin', 'Ethereum','Dogecoin']\nfor step, i in enumerate(df_asset_details['Asset_Name'].tolist()):\n    if step < 7:\n        axs[step,0].plot(df_sep_dict[i].index, df_sep_dict[i].VWAP, 'b')\n        axs[step,0].set_title(i + ' (Complete_train_space) ')\n        axs[step,0].set_xlabel('Time_Axis')\n        axs[step,0].set_ylabel('Closing_Price')\n    else:\n        axs[step-7,1].plot(df_sep_dict[i].index, df_sep_dict[i].VWAP, 'b')\n        axs[step-7,1].set_title(i + ' (Complete_train_space) ')\n        axs[step-7,1].set_xlabel('Time_Axis')\n        axs[step-7,1].set_ylabel('Closing_Price')\n\nplt.tight_layout(pad=2)\nplt.show()\n#'''\nprint('Done!')","54fac0c3":"import tensorflow as tf\nimport warnings \nwarnings.filterwarnings('ignore')\n\ntpu  = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strat = tf.distribute.TPUStrategy(tpu)\n\nprint(\"Number of accelerators: \", tpu_strat.num_replicas_in_sync)","3dcc1647":"import pickle\n\n''' Dumping the Data '''\n# dict_file = open('BTC_dict_data(last_btc_520k).pkl', 'wb')\n# pickle.dump(df_sep_dict['Bitcoin'], dict_file)\n# dict_file.close()\n\ndict_file = open('..\/input\/520k-btc-only\/BTC_dict_data(last_btc_520k).pkl', 'rb')\ndf_btc = pickle.load(dict_file)\ndict_file.close()","18b1b572":"import pandas as pd\n\ndf_btc.index = pd.date_range('2020-12-28', periods = len(df_btc), freq='min') # use freq = 'min' to get minute data","69600d61":"df_btc.Close.plot(figsize=(16,12)) ## They daily update the data to recent one | last year data","39ca6823":"import matplotlib.pyplot as plt\ndf_btc.loc['2021-11-1':].Close.plot(figsize=(16,12)) #localising recent past \ndf_btc.iloc[-50000:].Close.plot(figsize=(16,12), c='r', alpha=0.5)\nplt.show()","167763a6":"new_col_order = ['Count','Open','High','Low','Volume','VWAP','Target','Close']\n\n\ndisplay(df_btc.head())\ndf_btc = df_btc.reindex(columns = new_col_order)\ndisplay(df_btc.head())\n","f164a91e":"df_btc.iloc[-50000:].Close.plot(figsize=(10,8))","c2b13d6b":"import tensorflow as tf\nimport numpy as np\nimport gc\nimport math as mt\n\nseed = 28\nnp.random.seed(seed)\n\n\ndef Data_func(data, past, delay, minn, maxx = None, shuffle=False, batch_size=128, rate=6):\n    if maxx is None:\n        maxx = len(data) - delay -1\n    i = minn + past\n    batch_no = 1\n\n    while True:\n\n        if shuffle:\n            rows = np.random.randint(minn + past, maxx, size = batch_size)\n        else:\n            rows = np.arange(i, min(i + batch_size, maxx))\n        \n        i+=len(rows)\n        if i + batch_size > maxx: ## when you increase the past too much this statement gets executed at the first batch creation \/\/ add more data to overcome\n            break\n        \n        samples = np.zeros((len(rows), past \/\/ rate, data.shape[-1]))\n        targets = np.zeros((len(rows),))\n        cbase_preds = np.zeros((len(rows),))\n    \n        for j, row in enumerate(rows):\n            \n            indices = range(rows[j] - past, rows[j], rate) ## take every point\n            samples[j] = data[indices]\n            targets[j] = data[rows[j] + delay - 1][-1]\n       \n        if batch_no == 1:\n            print('im here')\n            new_sample = samples\n            new_targets = targets\n            print('Ini_samples: ', samples.shape, 'Ini_targets: ', targets.shape)\n        else:\n            new_sample = np.concatenate([new_sample, samples], axis=0)\n            new_targets = np.concatenate([new_targets, targets], axis=0)\n        batch_no+=1\n        \n    return new_sample, new_targets\n\ndef Baseline_func(data, past, delay, minn, maxx = None , shuffle=False, batch_size=128, rate=6):\n    if maxx is None:\n        maxx = len(data) - delay -1\n    i = minn + past\n    batch_no = 1\n\n    while True:\n\n        if shuffle:\n            rows = np.random.randint(minn + past, maxx, size = batch_size)\n        else:\n            rows = np.arange(i, min(i + batch_size, maxx))\n        \n        i+=len(rows)\n        if i + batch_size > maxx:\n            break\n        \n        cbase_preds = np.zeros((len(rows),))\n        \n        for j, row in enumerate(rows):\n            cbase_preds[j] = data[rows[j]][-1]\n            \n        if batch_no == 1:\n            new_cbase_preds = cbase_preds\n            print('Ini_cbase_pred: ', new_cbase_preds.shape)\n        else:\n            new_cbase_preds = np.concatenate([new_cbase_preds, cbase_preds], axis=0)\n        batch_no+=1\n        \n    return new_cbase_preds\n\n\n\n#'''    \nx_train = df_btc.iloc[-50000:].copy() ## Localising training => prediction space\n\nprint()\nx_train = x_train.to_numpy()\n\nprint('BTC_train: ',x_train.shape)\n\npast = 360 ## ==>> looks !1 but 6hrs days back \n\n''' The problem statement involves being able to predict next 15 mins (very much possible!) residualized returns | NO DELAY '''\n''' Target 15 datapoints from current point'''\ndelay = 0 ## Targeting after 15 mins | NOT correctt implementation \n\ntrain_split, test_split = 0.60, 0.25\ntrain_max = round(train_split * x_train.shape[0])\nval_max = (1-train_split)*x_train.shape[0]\ntest_max = round(test_split*val_max)\nval_max = round(val_max - test_max)\n\n''' use batch_size = 15 in accordance to the problem statement'''\nbs = 15\n\nmean = x_train[:train_max].mean(axis = 0)\nx_train -= mean\nstd = x_train[:train_max].std(axis = 0)\nx_train \/= std\n\nval_steps =  (train_max+val_max) - (train_max+1) - past ## (-past) because no target values for last  720 points\ntest_steps = (train_max+val_max+test_max)-(train_max+val_max+1) - past\n\nprint('Train_Max: ', mt.floor(train_max),', Val_Max: ', mt.floor(val_max),', Test_Max: ', mt.floor(test_max))\nprint()\n#'''        \ntrain_data = Data_func(x_train, past, delay, minn = 0, maxx = train_max - 1, batch_size=bs, shuffle=True, rate=5) ## sampling data every 5 minutes\ncbase_train_data = Baseline_func(x_train, past, delay, minn = 0, maxx = train_max - 1, batch_size=bs, shuffle=True, rate=5) ## sampling data every 5 minutes\n\ntemp, _ = train_data\nprint('--'*20)\nprint('Done - Train_data: ', temp.shape)\nprint('Cbase_data: ', cbase_train_data.shape)\nprint('--'*20)\nval_data = Data_func(x_train, past, delay, minn = train_max+1, maxx = train_max + val_max - 1, batch_size=bs, rate=5)\ncbase_val_data = Baseline_func(x_train, past, delay, minn = train_max+1, maxx = train_max + val_max - 1, batch_size= bs, rate=5)\n\ntemp, _ = val_data \nprint('--'*20)\nprint('Done - Val_data: ', temp.shape)\nprint('Cbase_data: ', cbase_val_data.shape)\nprint('--'*20)\n#'''\ntest_data = Data_func(x_train, past, delay, minn = train_max+val_max+1, batch_size=bs, rate = 5)\ncbase_test_data = Baseline_func(x_train, past, delay, minn = train_max+val_max+1, batch_size= bs, rate = 5)\n\ntemp, _ = test_data\nprint('--'*20)\nprint('Done - Test_data: ', temp.shape)\nprint('Cbase_data: ', cbase_test_data.shape)\nprint('--'*20)\n\ntrain_sample, train_label = train_data ## sample contains return \nval_sample, val_label = val_data\ntest_sample, test_label = test_data\n\ndel temp\ndel train_data\ndel val_data\ndel test_data\ngc.collect()","eb6b5959":"import numpy as np\nimport tensorflow as tf\nimport gc\n\nprint('train_sample: ', train_sample.shape, 'train_label: ', train_label.shape)\ntr_dataset = tf.data.Dataset.from_tensor_slices((train_sample, train_label)).repeat().batch(bs) #.cache().prefetch(tf.data.AUTOTUNE) ## use repeat() before batching while using TPU\nval_dataset = tf.data.Dataset.from_tensor_slices((val_sample, val_label)).repeat().batch(bs) #.cache().prefetch(tf.data.AUTOTUNE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_sample, test_label)).repeat().batch(bs) #.cache().prefetch(tf.data.AUTOTUNE)\ndel train_sample \ndel train_label\ngc.collect()","ad2e1c01":"### CommonSense Model\nimport numpy as np\nfrom statsmodels.tools.eval_measures import meanabs, rmse\n\ntarget_std = np.std(val_label)\n\n'''always predicting the current return to be the 15 min later return '''\n\nprint('CommonSense - BaseLines - MAE -----')\nprint()\nprint('__Val__')\n## using present info (latest info) as our prediction after 'delay' mins\nprint('MAE: ',meanabs(cbase_val_data, val_label))\nprint('RMSE: ', rmse(cbase_val_data, val_label))\nprint()\nprint('__Test__')\n## using present info (latest info) as our prediction after 'delay' mins\nprint('MAE: ',meanabs(cbase_test_data, test_label))\nprint('RMSE: ', rmse(cbase_test_data, test_label))\nprint()","a24cf119":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\n\nfig, axs = plt.subplots(1,1, figsize = (15,8))\n\naxs.plot(np.arange(0,1500), cbase_test_data[:1500], label = 'Common_Baseline') \naxs.plot(np.arange(0,1500), test_label[:1500], label = 'Real_Data')\naxs.legend()\nplt.show()","4af416d6":"dnn_model = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape=(72,8)),\n    tf.keras.layers.Dense(128, activation = 'linear'),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(128, activation = 'relu'),\n    tf.keras.layers.Dense(1)\n    \n])\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\ndnn_model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\ndnn_model.fit(tr_dataset, epochs=50, validation_data = val_dataset, validation_steps = val_steps, steps_per_epoch =128, callbacks=[es])\nprint('Training_Done ... ')","c33aff58":"dnn_model = tf.keras.models.load_model('..\/input\/all-models\/dnn_model.h5')","f2e64443":"print(dnn_model.evaluate(val_dataset, steps=128))\nprint(dnn_model.evaluate(test_dataset, steps=128))","d8c0a67c":"test_pred_data = [(sample, target) for sample, target in test_dataset.take(1)]\ntest_sample, test_target = test_pred_data[0] ## next 32 data-points | accounting for 32 mins prediction\nx_lin = np.arange(0,test_target.shape[0]) \ndef inv_norm(test_target):\n    test_target = np.ravel(test_target)\n    test_target = test_target * std[-1]\n    test_target = test_target + mean[-1]\n    return test_target\n\ntest_pred = dnn_model.predict(test_sample)\ninv_test_pred = inv_norm(test_pred)\n# test_pred_parent = inv_stdn(parent_model.predict(test_sample))\ninv_test_target = inv_norm(test_target)\n\nfig = plt.figure(figsize=(15,8))\nplt.plot(x_lin, inv_test_target, 'k', label = 'test')\nplt.plot(x_lin, inv_test_pred, 'r',  label = 'dnn_model')\n# plt.plot(x_lin, test_pred_parent, 'g', label = 'parent_model')\nplt.legend()\nplt.show()","8205355e":"from statsmodels.tools.eval_measures import meanabs\n\nprint('MAE: ', meanabs(inv_test_target, inv_test_pred))\nprint('Mean_target: ', np.mean(inv_test_target))\nprint('Mean_P_Error: ', (meanabs(inv_test_target, inv_test_pred) \/ np.mean(inv_test_target)) * 100, '%')","f32e2ea2":"dnn_model.save('.\/dnn_model.h5')","69f53a73":"cnn_rnn_model = tf.keras.models.Sequential([\n    tf.keras.layers.Input(shape=(72,8)),\n    tf.keras.layers.Conv1D(128, 5),\n    tf.keras.layers.Dense(264, activation='relu'),\n    tf.keras.layers.GRU(264, return_sequences=True),\n    tf.keras.layers.GRU(264),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1)\n]) \n\ncnn_rnn_model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\ncnn_rnn_model.summary()\n\nes = tf.keras.callbacks.EarlyStopping(patience=5)\ncnn_rnn_model.fit(tr_dataset, epochs=20, validation_data = val_dataset, validation_steps = val_steps, steps_per_epoch =128, callbacks=[es])\nprint('Training_Done...')","6378738b":"cnn_rnn_model = tf.keras.models.load_model('..\/input\/all-models\/cnn_rnn_model.h5')","2d2f9ce3":"print(cnn_rnn_model.evaluate(val_dataset, steps=128))\nprint(cnn_rnn_model.evaluate(test_dataset, steps=128))","a28d6e4d":"cnn_rnn_model.save('.\/cnn_rnn_model.h5')\nprint('saved cnn_rnn_model')","8977e890":"test_pred_data = [(sample, target) for sample, target in test_dataset.take(1)]\ntest_sample, test_target = test_pred_data[0] ##after validation set => next 32 datapoints | accounting for next 32 min prediction\nx_lin = np.arange(0,test_target.shape[0]) \ndef inv_norm(test_target):\n    test_target = np.ravel(test_target)\n    test_target = test_target * std[-1]\n    test_target = test_target + mean[-1]\n    return test_target\n\ntest_pred = cnn_rnn_model.predict(test_sample)\ninv_test_pred = inv_norm(test_pred)\n# test_pred_parent = inv_stdn(parent_model.predict(test_sample))\ninv_test_target = inv_norm(test_target)\n\nfig = plt.figure(figsize=(15,8))\nplt.plot(x_lin, inv_test_target, 'k', label = 'test')\nplt.plot(x_lin, inv_test_pred, 'r',  label = 'dnn_model')\n# plt.plot(x_lin, test_pred_parent, 'g', label = 'parent_model')\nplt.legend()\nplt.show()","f9926f99":"print('MAE: $ ' + str(meanabs(inv_test_target, inv_test_pred)))\nprint('Mean_target: $ ' + str(np.mean(inv_test_target)))\nprint('Mean_P_Error: ', (meanabs(inv_test_target, inv_test_pred) \/ np.mean(inv_test_target)) * 100, '%')","6c01f47a":"test_pred_data = [(sample, target) for sample, target in test_dataset.take(3)]\n\ntest_list, target_list = test_pred_data[0]\n\nfor x_test, x_test_target in test_pred_data[1:]:\n    test_list = tf.concat([test_list, x_test], axis=0)\n    target_list = tf.concat([target_list, x_test_target], axis=0)\n\n","cef4db03":"x_lin = np.arange(0,target_list.shape[0]) \ntest_pred = dnn_model.predict(test_list)\n\nprint('Prediction_Shape: ', test_pred.shape,'Target_Shape: ',target_list.shape)\n\ndef inv_norm(test_target):\n    test_target = np.ravel(test_target)\n    test_target = test_target * std[-1]\n    test_target = test_target + mean[-1]\n    return test_target\n\ninv_target_list = inv_norm(target_list)\ninv_test_pred = inv_norm(test_pred)\n\nfig = plt.figure(figsize=(15,8))\nplt.plot(x_lin, inv_target_list, 'k', label = 'test')\nplt.plot(x_lin, inv_test_pred, 'r',  label = 'dnn_model')\n# plt.plot(x_lin, test_pred_parent, 'g', label = 'parent_model')\nplt.legend()\nplt.show()","038a2400":"from statsmodels.tools.eval_measures import rmse\n\nprint('MAE: $ ' + str(meanabs(inv_target_list, inv_test_pred)))\nprint('Mean_target: $ ' + str(np.mean(inv_target_list)))\nprint('Mean_P_Error: ', (meanabs(inv_target_list, inv_test_pred) \/ np.mean(inv_target_list)) * 100, '%')\n\nprint('RMSE: $ ' + str(rmse(inv_target_list, inv_test_pred)))","db9cfc70":"import time\nfor i in range(50):\n    print('Time: ', i)\n    time.sleep(30*60)","64615485":"'''import gresearch_crypto as gs\nenv = gs.make_env()\niter_test = env.iter_test()\nfor (test_df, _) in iter_test:\n    sample_prediction_df['Target'] = 0  \n    env.predict(sample_prediction_df)'''","72d9b657":"### Visualizaton (~ Raw)","9d04af19":"All the given data after parsing & pre-processing are saved then loaded again, as this notebook is aimed at analysing performance of Basic DeepLearning Models | For forecasting only considering BTC  ","5e3868df":"#### EXtended PREDiction | For DNN Model","096d9ab6":"Well thats it for now... next we will use Some advanced Sequential models & some financial models for gaining insights + forecasting  ","2952e3b6":"SESSION STOPPER","e27f71bb":"<img style = \"width:100%; height:auto;\" src = \"https:\/\/wallpapersmug.com\/download\/1920x1080\/a8a1e4\/coin-money-bitcoin.jpg\">\n<h1 style = \"text-align:center; background-color:black; color:white\"> G-Research | <i>BaseLine<\/i> | DNN, RNN + 1D-CNN  | <i>v1<\/i> <\/h1> ","4e82873d":"### USE DNN and 1D_CNN + RNN","bb17f865":"### Better-Visualization (~preprocessed)"}}