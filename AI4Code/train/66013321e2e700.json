{"cell_type":{"af7102d1":"code","65424c48":"code","a92edad1":"code","74cd18e6":"code","732f9e53":"code","8b470e6e":"code","5d6ed805":"code","ee46a176":"code","fcf386c0":"code","ab414827":"code","bff9a4ad":"code","283f4ea4":"code","37b79fd9":"code","6c397fd9":"code","a38f0d08":"code","287e5663":"code","9aaffb1b":"code","46888faa":"code","4a36aabe":"code","c5562f5a":"code","49d91898":"code","867d4764":"code","a44902d6":"code","559dac7b":"code","83781a01":"code","1813dc55":"code","4f76300b":"code","9b7f362a":"code","aa702c8c":"code","d2ca2a76":"code","dceffac6":"code","e64feb13":"code","1c39fb37":"code","c7852b22":"code","c111cbe4":"code","c6fb878b":"code","21151cad":"code","6b376aa6":"code","324cb84d":"code","816e3aa0":"code","94ace14c":"code","7cdcc20a":"code","d1dd1aa0":"code","7f695c5d":"code","40f3e692":"code","8699761e":"code","b444314d":"code","92bb516f":"code","8f69a4f4":"code","1e342a4f":"code","8526eb92":"code","74c5305c":"code","2903367f":"code","2da4a839":"code","a8a64df9":"code","0b3de833":"code","76946259":"code","daaa3d20":"code","07e8118e":"code","b82da6ba":"code","09ff4756":"code","c7360579":"code","b7068379":"code","e4e44e4d":"code","69f51607":"code","258f7cca":"code","45a963e9":"code","8db2641f":"code","0dca4c81":"code","ffa67650":"code","74f8af85":"code","706bc0b8":"code","787a546a":"code","6ce049e1":"code","1972e1e2":"code","7e1d9393":"code","43eae9c8":"code","defa2bc6":"code","89325019":"code","f3cc663d":"code","eb76eb51":"code","9616dd6c":"code","1237f2ae":"code","afeba8d1":"code","586203c7":"code","b6f4f8e1":"code","61cc5bad":"code","0b20e6d6":"code","64e519d0":"markdown","20d68a0d":"markdown","8a365c26":"markdown","fa34f46d":"markdown","7a58b1d4":"markdown","f2f13584":"markdown","adca874a":"markdown","528fc526":"markdown","d9e8c289":"markdown","4060a6d5":"markdown","e0ca2850":"markdown","e2fcf8c7":"markdown","c6053c6f":"markdown","7fbeabb4":"markdown","ae6b104a":"markdown","7dce9039":"markdown","ecc16894":"markdown","b65a06fe":"markdown","8a30846f":"markdown","7c9ddf17":"markdown","73901747":"markdown","b6831b97":"markdown","38c5815d":"markdown","0084efde":"markdown","be705c99":"markdown","b40313ac":"markdown","4e386d34":"markdown","0ad1294b":"markdown","c8c9b915":"markdown","90403319":"markdown","c73ea548":"markdown","3c4e61eb":"markdown","64b810b9":"markdown","a68704b4":"markdown","ff1b8adc":"markdown","adc93187":"markdown","831c027b":"markdown","a87c70c1":"markdown","b26960fa":"markdown"},"source":{"af7102d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns\nimport matplotlib.pylab as plt","65424c48":"items = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitems.tail()","a92edad1":"categories = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ncategories.tail()","74cd18e6":"shops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nshops","732f9e53":"sales = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsales.tail()","8b470e6e":"sales_test = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nsales_test.tail()","5d6ed805":"print(sum(items.duplicated(['item_name'])))\nprint(sum(categories.duplicated(['item_category_name'])))\nprint(sum(shops.duplicated(['shop_name'])))\n# We can see that the names of shops 10 and 11 differ only by one letter. It is probably the same shop.  \n# Also 0 and 57, 1 and 58, ?39 and 40?","ee46a176":"# Let's find out if shops 10,11,0,57,1,58 present in the dataframe for forecasting.\nuniq_shops = sales_test['shop_id'].unique()\nfor shop in list([10,11,0,57,1,58]):\n    print(shop, shop in uniq_shops)","fcf386c0":"new_shop_id = {11: 10, 0: 57, 1: 58}\nshops['shop_id'] = shops['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)\nsales['shop_id'] = sales['shop_id'].apply(lambda x: new_shop_id[x] if x in new_shop_id.keys() else x)","ab414827":"sales = pd.merge(sales_test, sales, on = ('shop_id', 'item_id'), how = 'left')","bff9a4ad":"sales.tail()","283f4ea4":"print(sum(sales.duplicated()))","37b79fd9":"# drop the duplicate rows from sales\nsales = sales.drop_duplicates()\nsales.shape","6c397fd9":"print(sum(sales.duplicated(['ID','date','date_block_num','item_price'])))\nprint(sum(sales.duplicated(['ID','date','date_block_num','item_cnt_day'])))\nsales[sales.duplicated(['ID','date','date_block_num','item_cnt_day'])]","a38f0d08":"sales[sales.duplicated(['ID','date','date_block_num','item_cnt_day'], keep = 'last')]","287e5663":"# We should think carefully which row should be dropped. Price will help us. But now we will just keep first duplicate and drop later.\nsales = sales.drop_duplicates(['date','date_block_num','shop_id','item_id','item_cnt_day'])\nsales.shape","9aaffb1b":"sales[sales.duplicated(['ID','date','date_block_num'])]","46888faa":"sales[sales.duplicated(['ID','date','date_block_num'], keep = 'last')]","4a36aabe":"sales = sales.drop_duplicates(['ID','date','date_block_num'], keep = 'last')\nsales.shape","c5562f5a":"print(items.isnull().sum().sum())\nprint(categories.isnull().sum().sum())\nprint(shops.isnull().sum().sum())\nprint(sales.isnull().sum().sum())\n# There are missing values in the data. Most of them corresponds to IDs from the forcast set that doesn't represent in training set.","49d91898":"sales.describe()\n# It is possible that item_price and item_cnt_day has outliers (max >> 0.75-quantile), and item_cnt_day has wrong values (min < 0)","867d4764":"sum(sales.item_cnt_day < 0)","a44902d6":"# change a sign of negative values\nsales.loc[sales.item_cnt_day < 0, 'item_cnt_day'] = -1. * sales.loc[sales.item_cnt_day < 0, 'item_cnt_day']","559dac7b":"ax = sns.boxplot(x = sales.item_cnt_day)","83781a01":"ax = sns.boxplot(x = sales.loc[sales.item_id == sales.item_id[sales.item_cnt_day.idxmax()],'item_cnt_day'])","1813dc55":"ax = sns.boxplot(x = sales.item_price)","4f76300b":"sales.loc[sales.item_price > 35000,'item_id'].unique()","9b7f362a":"items.loc[items.item_id == 13403,:]","aa702c8c":"sales_month = sales.sort_values('date_block_num').groupby(['ID', 'date_block_num'], as_index = False).agg({'item_cnt_day': ['sum'], 'item_price': ['mean']})\nsales_month.columns = ['ID', 'date_block_num', 'item_cnt_month', 'item_price']\nsales_month.sample(10)\n# after we grouped and aggregate data we delete all rows corresponding to IDs that don't present in train data set (and preset just in forcasting set)","d2ca2a76":"sales_month.describe()","dceffac6":"sns.distplot(sales_month.loc[:,'item_cnt_month'])","e64feb13":"sns.distplot(sales_month.loc[:,'item_price'], kde=False)","1c39fb37":"plt.scatter(sales_month.item_cnt_month, sales_month.item_price)","c7852b22":"ax = plt.subplots(figsize=(25, 12))\nax = sns.heatmap(sales.pivot_table(index = 'date_block_num', columns = 'shop_id', values = 'item_cnt_day', aggfunc = 'sum'), cmap=\"YlGnBu\")\nplt.title(\"Items sold by each shop per month\")\nplt.show()","c111cbe4":"ax = plt.subplots(figsize=(15, 5))\nplt.plot(sales_month.groupby(['date_block_num'], as_index = False).agg({'ID':'count'}).iloc[:,1], 'o')\nplt.title(\"Number of the unique IDs over months\")\nplt.xlabel(\"date_block_num\")\nplt.show()\n# We have over 100,000 unique IDs but less then 30,000 of them was sold in any month","c6fb878b":"ax = plt.subplots(figsize=(25, 5))\nplt.plot(sales_month.groupby(['ID'], as_index = False).agg({'date_block_num':'count'}).iloc[:,1], '.')\nplt.title(\"Number of the month in which each ID was sold\")\nplt.xlabel(\"unique ID\")\nplt.show()\n# The most of ID has information about sales less then for 10 months.","21151cad":"def to_IDs(np_data, col_ID):\n    # np_data - sales converted to numpy array\n    # col_ID - name of ID column\n    sales_by_ID = list() #dict()\n    IDs = np.unique(np_data[:,col_ID])\n    #IDs = np_data[col_ID].unique()\n    for i in IDs:\n        positions = np_data[:,col_ID] == i\n        sales_ID = np_data[positions,:]\n        #positions = np_data[col_ID] == i\n        #sales_by_ID[i] = np_data.loc[positions,:]\n        sales_by_ID.append(sales_ID)\n    return sales_by_ID","6b376aa6":"sales_by_ID = to_IDs(sales_month.values,0)\n#sales_by_ID = to_IDs(sales_month,'ID')\nprint(len(sales_by_ID))","324cb84d":"# to decrease calculation time during a code debugging we remove IDs that don't have observtions for last 6 months\ndef remove_ID_nan_last_year(np_data):\n    N_IDs = len(np_data)\n    col_date = 1\n    clear_data = list()\n    cut_month = 33 - 6\n    for i in range(N_IDs):\n        ID_data = np_data[i]\n        if len(ID_data[ID_data[:,col_date] >= cut_month,2]) != 0:\n            clear_data.append(ID_data)\n    return clear_data","816e3aa0":"#sales_by_ID = remove_ID_nan_last_year(sales_by_ID)\n#len(sales_by_ID)","94ace14c":"#def split_train_test(np_data, col_date = 'date_block_num', last_month = 33):\ndef split_train_test(np_data, col_date = 1, last_month = 33):\n    col_TS = 2 # numbe of item_cnt_month column\n    N_IDs = len(np_data)\n    train = list()\n    test = list()\n    empty_train, empty_test = 0, 0 # we will count train and test sets that have zero length\n    #for ID_data in np_data.values():\n    for i in range(N_IDs):\n        ID_data = np.array(np_data[i])\n        # ID_data = np.array(ID_data)\n        train_rows = ID_data[ID_data[:,col_date] < last_month, :]\n        test_response = ID_data[ID_data[:,col_date] >= last_month, col_TS]\n        #train_rows = ID_data.loc[ID_data[col_date] < last_month, :]\n        #test_rows = ID_data.loc[ID_data[col_date] >= last_month, :]\n        if len(train_rows) == 0: #or (len(train_rows) == 1 and len(test_response) == 0):\n            empty_train = 1 + empty_train\n            continue\n        if len(test_response) == 0:\n            empty_test = 1 + empty_test\n            test.append(np.nan)\n        else:\n            test.append(test_response[0])\n        train.append(train_rows)\n    print(empty_train,' IDs do not have observations in TRAIN set')\n    print(empty_test,' IDs do not have observations in TEST set')\n    return train, np.array(test)","7cdcc20a":"train, test_actual = split_train_test(sales_by_ID, 1, 33)\nprint(len(test_actual), 'IDs will be used for modeling')\nprint(len(train))\n# We have a lot of IDs that don't have observations for last month, \n# so these are useless for a metric calculating but we keep it for modeling.","d1dd1aa0":"test_actual = np.nan_to_num(test_actual, nan = 0)","7f695c5d":"# Let's fill the missing date_block_num by NaN for paticular ID\ndef missing_months(np_data, col_date, col_TS, N_months = 33):\n    # col_date - index of date_block_num column\n    # col_TS - index of item_price column and item_cnt_month column\n    # at first fill time series by NaN for all months\n    series = [np.nan for _ in range(N_months)]\n    for i in range(len(np_data)):\n        position = int(np_data[i, col_date] - 1)\n        # fill positions that present in data\n        series[position] = np_data[i, col_TS]\n    return series","40f3e692":"# Plot time series for particular ID to find out missing months\ndef plot_TS(np_data, n_vars = 1, N_months = 33, flag = 0):\n    # n_vars = 1 or 2 (plot item_cnt OR item_cnt and item_price)\n    plt.figure()\n    if flag == 1:\n        TSs = to_fill_missing(np_data, N_months)\n    for i in range(n_vars):\n        col_plot = i + 2 # index of column to plot\n        if flag == 1:\n            series = TSs[:,col_plot]\n        else:\n            series = missing_months(np_data, 1, col_plot, N_months)\n        ax = plt.subplot(n_vars, 1, i+1)\n        plt.plot(series, 'o')\n        plt.plot(series)\n    plt.show()","8699761e":"plot_TS(train[10],2,33, flag = 0)\n# in the up plot there is represented the item_cnt_month\n# in the low -- item_price for ID=11\n# We can use an interpolation to fill missing value for some IDs but not for all","b444314d":"plot_TS(train[1563],2,33)\n# In this case we can fill the data by 0 or 1","92bb516f":"plt.scatter(train[1563][:,2], train[1563][:,3])","8f69a4f4":"plot_TS(train[80059],2,33)\n# In this case it seems resonable to fill by 0 (because of high price)","1e342a4f":"plot_TS(train[30111],2,33)","8526eb92":"# Let's fill the missing item_cnt_month and item_price for particular ID\ndef to_fill_missing(np_data, N_months = 33):\n    date_month = pd.DataFrame(range(N_months),columns = ['date_block_num'])\n    col = ['ID','date_block_num','item_cnt_month','item_price']\n    sales_ID = pd.DataFrame(np_data, columns = col)\n    #sales_ID['missing_flag'] = 1\n    if sales_ID.shape[0] < N_months:\n        sales_ID = pd.merge(date_month, sales_ID, on = ('date_block_num'), how = 'left')\n        sales_ID = sales_ID.reindex(columns = col)\n     #   sales_ID['missing_flag'] = sales_ID['item_cnt_month'].isnull().astype('uint')\n        sales_ID['ID'] = sales_ID['ID'].fillna(sales_ID['ID'].loc[sales_ID['ID'].first_valid_index()])\n        sales_ID['item_cnt_month'] = sales_ID['item_cnt_month'].fillna(0)\n        sales_ID['item_price'] = sales_ID['item_price'].interpolate(method ='linear', limit_direction ='both')\n    return sales_ID.to_numpy()","74c5305c":"plot_TS(train[30111],2,33,flag = 0)","2903367f":"plot_TS(train[30111],2,33,flag = 1)","2da4a839":"plt.scatter(train[30111][:,2], train[30111][:,3])","a8a64df9":"plot_TS(train[5127],2,33,flag = 0)","0b3de833":"plot_TS(train[5127],2,33,flag = 1)","76946259":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf","daaa3d20":"def plot_acf_pacf(np_data, n_vars = 1, N_months = 33):\n    # n_vars = 1 or 2 (plot item_cnt OR item_cnt and item_price)\n    plt.figure()\n    col_plot = 2 # index of column to plot\n    for i in range(1, 2*n_vars, 2): \n        series = to_fill_missing(np_data, N_months)\n        series = series[:,col_plot]\n        ax = plt.subplot(n_vars, 2, i)\n        plot_acf(series, ax = ax)\n        ax = plt.subplot(n_vars, 2, i+1)\n        plot_pacf(series, ax = ax)\n        col_plot = col_plot + 1\n    plt.show()","07e8118e":"plot_acf_pacf(train[61559],1,33)","b82da6ba":"plot_acf_pacf(train[30112])","09ff4756":"# Let's modify the data of particular ID\ndef to_make_lag_features(np_data, n_lag = 2, N_months=33):\n    # in_out = np.empty((N_months-n_lag, 2*n_lag+1))\n    ID_TS = to_fill_missing(np_data, N_months)\n    N_col = ID_TS.shape[1]\n    in_out = np.empty((N_months-n_lag, n_lag+N_col))\n    count_TS = ID_TS[:,2]\n    for i in range(n_lag, N_months):\n        # input features: n_lags of item_cnt_month and n_lags of item_price\n        # output: item_cnt_month\n        # in_out[i-n_lag,:] = np.concatenate([count_TS[i-n_lag:i], np.array([count_TS[i]])]) # price_TS[i-n_lag:i],  np.array([count_TS[i]])])\n        in_out[i-n_lag,:] = np.concatenate([np.delete(ID_TS[i,:],2,axis=0), count_TS[i-n_lag:i],  np.array([count_TS[i]])])\n    # the last array contains n_lags of item_cnt_month and n_lags of item_price that will be features for prediction of 34th month\n    test_df = np.concatenate([np.array([ID_TS[i,0], N_months, ID_TS[i,3]]), count_TS[i+1-n_lag:i+1]])\n    # test_df = np.concatenate([count_TS[i+1-n_lag:i+1], price_TS[i+1-n_lag:i+1]])\n    # test_df = count_TS[i+1-n_lag:i+1]\n    return in_out, test_df","c7360579":"def data_preparation(data_IDs, N_months = 33, n_lag = 2):\n    # data_IDs - list of train data, each element of list contains train data for particular ID\n    N_IDs = len(data_IDs)\n    N_col = data_IDs[0].shape[1]\n    N_rows = N_months-n_lag\n    train_data = np.empty((N_IDs*N_rows, n_lag+N_col)) # 2*n_lag+1\n    test_data = np.empty((N_IDs, n_lag+N_col-1)) # 2*n_lag\n    list_IDs = np.empty((1,N_IDs))\n    # col_TS - index of item_cnt_month column\n    col_TS = 2\n    for i in range(N_IDs): # each particular ID\n        ID_data = data_IDs[i]\n        train, test = to_make_lag_features(ID_data, n_lag, N_months)\n        train_data[i*N_rows:(i+1)*N_rows,:] = train\n        test_data[i,:] = test\n       # list_IDs[0,i] = ID_data[0,0]\n    return np.array(train_data), np.array(test_data)#, list_IDs","b7068379":"# Let's fit model for item_cnt_month of all IDs\ndef to_fit_model(model, train_df):\n    features, label = train_df.drop('item_cnt_month', axis = 1), train_df.loc[:,'item_cnt_month']\n    model.fit(features, label)\n    return model","e4e44e4d":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics","69f51607":"n_lag = 2\ntr, ts = data_preparation(train, n_lag = n_lag)","258f7cca":"train_features_label = pd.DataFrame(tr, columns = ['ID', 'date_block_num', 'item_price', \n                                                   'item_cnt_month_lag-2', 'item_cnt_month_lag-1', 'item_cnt_month'])\ntest_features = pd.DataFrame(ts, columns = ['ID', 'date_block_num', 'item_price', \n                                                   'item_cnt_month_lag-2', 'item_cnt_month_lag-1'])","45a963e9":"train_features_label.head()","8db2641f":"train_features_label = pd.merge(train_features_label, sales_test.loc[:,['ID', 'shop_id', 'item_id']], on = ('ID'), how = 'left')\ntrain_features_label = pd.merge(train_features_label, items.loc[:,['item_id', 'item_category_id']], on = ('item_id'), how = 'left')","0dca4c81":"#aa = aa.drop(aa[(aa['item_cnt_month_lag-2'] == 0) & (aa['item_cnt_month_lag-1'] == 0) & (aa['item_cnt_month'] == 0)].index)","ffa67650":"train_features_label = train_features_label.astype({'ID': 'uint32', 'date_block_num': 'uint8', 'item_cnt_month_lag-2': 'uint16', 'item_cnt_month_lag-1': 'uint16',        'item_cnt_month': 'uint16'})","74f8af85":"test_features = pd.merge(test_features, sales_test.loc[:,['ID', 'shop_id', 'item_id']], on = ('ID'), how = 'left')\ntest_features = pd.merge(test_features, items.loc[:,['item_id', 'item_category_id']], on = ('item_id'), how = 'left')","706bc0b8":"test_features = test_features.astype({'ID': 'uint32', 'date_block_num': 'uint8', 'item_cnt_month_lag-2': 'uint16', 'item_cnt_month_lag-1': 'uint16'})","787a546a":"best_random = {'n_estimators': 94, 'min_samples_split': 10, 'max_features': 'sqrt', 'bootstrap': True}\n#{'n_estimators': 1000, 'min_samples_split': 50, 'max_features': 'auto'} # score = 0.6","6ce049e1":"model = RandomForestRegressor(n_estimators = 500, min_samples_split = 10, criterion = \"mse\", bootstrap = True, verbose = 1)","1972e1e2":"fitt = to_fit_model(model, train_features_label)","7e1d9393":"predictions = fitt.predict(test_features)","43eae9c8":"RMSE = metrics.mean_squared_error(test_actual, predictions) #to_calculate_RMSE(test_actual, predictions)\nprint(RMSE)","defa2bc6":"predictions[range(20)]","89325019":"test_actual[range(20)]","f3cc663d":"importance = fitt.feature_importances_\nN_features = len(importance)","eb76eb51":"importance","9616dd6c":"plt.figure(figsize=(15,5))\nplt.title(\"Feature importance\")\nbars = plt.bar(range(N_features), importance, align=\"center\")\nx = plt.xticks(range(N_features), list(test_features.columns))","1237f2ae":"# concatenate the columns\nfeatures_last_month = test_features.copy()\nfeatures_last_month['date_block_num'] = 34\nfeatures_last_month['item_cnt_month_lag-2'] = test_features['item_cnt_month_lag-1']\nfeatures_last_month['item_cnt_month_lag-1'] = test_actual\nfeatures_last_month.head()","afeba8d1":"last_month_predict = fitt.predict(features_last_month)","586203c7":"submission = pd.DataFrame({\n        'ID': features_last_month['ID'],\n        'item_cnt_month': last_month_predict\n    })\nsubmission.head()","b6f4f8e1":"submission = pd.merge(sales_test.ID, submission, on = ('ID'), how = 'left')","61cc5bad":"submission = submission.fillna(0)\nsubmission.tail()","0b20e6d6":"submission.to_csv('submission.csv', index=False)","64e519d0":"#we must transform features if we will use SVM model, KNN model\nfrom sklearn.preprocessing import PowerTransformer\n#from sklearn.preprocessing import QuantileTransformer\nBoxCox = PowerTransformer(method = 'box-cox').fit(train.loc[:,['item_cnt_month','item_price']])\ntrain.loc[:,['item_cnt_month','item_price']] = BoxCox.transform(train.loc[:,['item_cnt_month','item_price']])\ntest.loc[:,['item_cnt_month','item_price']] = BoxCox.transform(test.loc[:,['item_cnt_month','item_price']])\n#sales_month.loc[:,['item_cnt_month','item_price']] = QuantileTransformer(output_distribution='normal').fit_transform(sales_month.loc[:,['item_cnt_month','item_price']])","20d68a0d":"# Fill data for missing months","8a365c26":"Let's take a look on the plots of several IDs","fa34f46d":"We have totaly different numbers and positions of missing months for different IDs. Let's fill item_cnt_month by 0 and add column of missing flag.\nItem_price will be interpolated.","7a58b1d4":"#choose randomly n_iter combinations of parameters\nmodels_hp = RandomizedSearchCV(estimator = model, param_distributions = hyper_params, n_iter = 3, cv = 3, verbose = 2, random_state = 42)","f2f13584":"Let's split the data by ID. We will store ID and corresponding data in a list.","adca874a":"# Model evaluation","528fc526":"#best_model = models_hp.best_estimator_\nmodels_hp.best_params_","d9e8c289":"# Feature engineering","4060a6d5":"model = RandomForestRegressor(n_estimators = 100, min_samples_split = 10, criterion = \"mse\", bootstrap = True, verbose = 1)\nfitting = to_fit_model(model, train_set)","e0ca2850":"Let's transform the data. We will use the Box-Cox transforms in order to stabilize variance and minimize skewness.","e2fcf8c7":"predictions = fitting.predict(test_set)\nmetrics.mean_squared_error(actual_label, predictions)","c6053c6f":"A lot of IDs have 1 lag of acf and 1 lag of pacf, ohers - 0 significant lags. So we will create two more features: 1 lag observations and 2 lag observations.","7fbeabb4":"# Model fits the data in the which missing months are not filled","ae6b104a":"dataset = sales_month.copy()","7dce9039":"Let's take a look on graphs of data with missing values and data with filled missing values","ecc16894":"# ACF and PACF","b65a06fe":"By scatter plots It seems that the price of particular ID does not have influence on the count. But we have seen just the scatter plots of three IDs.","8a30846f":"# Hyperparameters tuning","7c9ddf17":"Let's find out if there are missing month for any ID.","73901747":"hyper_params = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 1000, num = 3)], \n                'max_features': ['auto', 'sqrt'], \n                'min_samples_split': [2, 10, 50]} \n                #'bootstrap': [True, False]}","b6831b97":"Let's attach shop_id, iem_id, item_category_id columns to features","38c5815d":"In order to execute supervised algorithms we have to modify the data this way:\nseveral lags like an input and next lag like an output.\n\nLet's assume n_lag = 3 than modified time series of item_cnt_month will be like the following:\n\n---------------------input--------------- ||         output \n\ndata_month1 data_mont2 data_month3 ||        data_month4 \n\ndata_month2 data_mont3 data_month4 ||        data_month5 \n\ndata_month3 data_mont4 data_month5 ||        data_month6 \n\n.....","0084efde":"evaluate_model(model, train_df, test_df, actual)","be705c99":"# Outliers and negative values","b40313ac":"dataset = dataset.fillna(0)","4e386d34":"Let's calculate how many unique IDs was sold in each month","0ad1294b":"#Mean cross-validated score of the best_estimator\nmodels_hp.best_score_","c8c9b915":"Let's drop the pairs (shop_id, item_id) that are not represented in the dataframe for forecasting. And merge two dataframes.","90403319":"train_set = dataset.copy()\ntest_set = dataset[dataset.date_block_num == 33]\nactual_label = test_set['item_cnt_month']\ntest_set = test_set.drop('item_cnt_month', axis = 1)\ntrain_set = train_set.drop(test_set.index, axis = 0)","c73ea548":"def to_make_lag(df, lag):\n    lag_df = pd.DataFrame({'ID': [], 'date_block_num': [], 'item_cnt_month_lag-'+str(lag): []})\n    lag_df['ID'] = df['ID']\n    lag_df['item_cnt_month_lag-'+str(lag)] = df['item_cnt_month']\n    lag_df['date_block_num'] = df['date_block_num'] + lag\n    df = pd.merge(df, lag_df, on = ('ID', 'date_block_num'), how = 'left') #'both'\n    return df","3c4e61eb":"#we can use method metrics instead of self-function to_calculate_MAE\ndef to_calculate_RMSE(actual, predictions):\n    mse = 0\n    N_IDs = len(actual)\n    for i in range(N_IDs):\n        if np.isnan(actual[i]):\n            continue\n        error = (predictions[i] - actual[i])**2\n        mse += error\n    mse \/= sum(~np.isnan(actual))\n    return np.sqrt(mse)","64b810b9":"models_hp.fit(aa.drop('item_cnt_month', axis = 1), aa.loc[:,'item_cnt_month'])","a68704b4":"# Convert to month data","ff1b8adc":"lags = [1,2]\nfor ilag in range(len(lags)):\n    dataset = to_make_lag(dataset, lags[ilag])","adc93187":"We have 34 months of observations. Let's split the data into train (33 months) and test (last month) samples.","831c027b":"Let's group the sales by ID and calculate month number of sold items and average price.","a87c70c1":"# Delete the duplicates","b26960fa":"Let's take a look on sales of each shop over time.\n\nWe can see that there are several leader-shops and outsider-shops."}}