{"cell_type":{"dedb1262":"code","a47e313c":"code","d738eeaa":"code","d43b96bd":"code","1df450a5":"code","e59fa6e8":"code","4de42c4a":"code","92394b7b":"code","f546d690":"code","6f906981":"code","1d9931ba":"code","d9bd8fce":"code","e7e020a6":"code","0511c3d5":"code","925538a1":"code","4bd6faaf":"code","2cec0871":"code","3ecd89fb":"code","c4b8231e":"code","39d68c89":"code","a500c421":"code","ec67529d":"code","627cb51a":"markdown","13da72f4":"markdown","ab9fac66":"markdown","a812622a":"markdown","f8ca1307":"markdown","f931eb01":"markdown","361e5c51":"markdown","120e90b2":"markdown","488127b2":"markdown","59ef445c":"markdown","116456f0":"markdown","cef210bf":"markdown","41648940":"markdown","d2ff4f68":"markdown"},"source":{"dedb1262":"!nvidia-smi","a47e313c":"# \u74b0\u5883\u306b\u3088\u3063\u3066\u51e6\u7406\u3092\u5909\u3048\u308b\u305f\u3081\u306e\u3082\u306e\nimport sys\nimport os\nIN_COLAB = 'google.colab' in sys.modules\nIN_KAGGLE = 'kaggle_web_client' in sys.modules\nLOCAL = not (IN_KAGGLE or IN_COLAB)\nprint(f'IN_COLAB:{IN_COLAB}, IN_KAGGLE:{IN_KAGGLE}, LOCAL:{LOCAL}')","d738eeaa":"if IN_KAGGLE or IN_COLAB:\n    !pip install --upgrade -q wandb\n    !pip install -q pytorch-lightning\n    !pip install torch_optimizer==0.1.0\n    !pip install einops\n    !pip install timm","d43b96bd":"# Hide Warning\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# Python Libraries\nimport os\nimport math\nimport random\nimport glob\nimport pickle\nimport gc\nfrom collections import defaultdict\nfrom pathlib import Path\n\n# Third party\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Visualizations\n# from PIL import Image\n# import cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Utilities and Metrics\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import RobustScaler, normalize, QuantileTransformer, StandardScaler\n# from sklearn.metrics import mean_absolute_error #[roc_auc_score, accuracy_score]\n\n# Pytorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.optim.optimizer import Optimizer, required\nimport torch_optimizer as optim\nfrom einops.layers.torch import Rearrange\n\n# Pytorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Callback, seed_everything\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger, CSVLogger\n\n# Pytorch Image Models\nimport timm\nfrom torchvision.io import read_image\nimport torchvision.transforms as T\n\n# Weights and Biases Tool\nimport wandb\n#os.environ[\"WANDB_API_KEY\"]='hoge'\nwandb.login()","1df450a5":"class CFG:\n    debug = True\n    competition='PetFinder2'\n    exp_name = \"kaggle_test\"\n    seed = 29\n    # model\n    model_name = 'dm_nfnet_f3'\n    pretrained = True\n    img_size = 512\n    in_chans = 3\n    # data\n    target_col = 'Pawpularity'\n    target_size = 1\n    # optimizer\n    optimizer_name = 'AdamW'#['RAdam', 'sgd', 'AdamW']\n    lr = 1e-6\n    weight_decay = 1e-6\n    amsgrad = False\n    \n    # scheduler\n    epochs = 20\n    scheduler = 'CosineAnnealingLR' #['CosineAnnealingLR', 'ReduceLROnPlateau']\n    T_max = 300\n    min_lr = 1e-7\n    # scheduler = 'ReduceLROnPlateau' #['CosineAnnealingLR', 'ReduceLROnPlateau']\n    # factor = 0.5\n    # patience = 10\n    # eps = 1e-6\n    # min_lr = 1e-05\n\n    # criterion\n    criterion_name = 'BCEWithLogitsLoss'\n\n    mixup = {'alpha':2}\n    \n    # training\n    train = True\n    inference = True\n    n_fold = 5\n    trn_fold = [0]\n    precision = 16 #[16, 32, 64]\n    grad_acc = 1\n    # DataLoader\n    loader = {\n        \"train\": {\n            \"batch_size\": 8,\n            \"num_workers\": 0,\n            \"shuffle\": True,\n            \"pin_memory\": True,\n            \"drop_last\": True\n        },\n        \"valid\": {\n            \"batch_size\": 8,\n            \"num_workers\": 0,\n            \"shuffle\": False,\n            \"pin_memory\": True,\n            \"drop_last\": False\n        }\n    }\n    # pl\n    trainer = {\n        'gpus': 1,\n        'progress_bar_refresh_rate': 1,\n        'benchmark': False,\n        'deterministic': True,\n        }\n\n    feature_cols = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n        'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur',\n        ]\n    \nseed_everything(CFG.seed)\nif not LOCAL:\n    CFG.loader[\"train\"][\"num_workers\"] = 4\n    CFG.loader[\"valid\"][\"num_workers\"] = 4","e59fa6e8":"if IN_KAGGLE:\n    INPUT_DIR = Path('..\/input\/petfinder-pawpularity-score\/')\n    OUTPUT_DIR = '.\/'\nelif IN_COLAB:\n    INPUT_DIR = Path('\/content\/input\/')\n    OUTPUT_DIR = f'\/content\/drive\/MyDrive\/kaggle\/petfinder-pawpularity-score\/{CFG.exp_name}\/'\nif LOCAL:\n    INPUT_DIR = Path(\"F:\/Kaggle\/petfinder-pawpularity-score\/data\/input\/\")\n    OUTPUT_DIR = f'F:\/Kaggle\/petfinder-pawpularity-score\/data\/output\/{CFG.exp_name}\/'\n    \ndf_train = pd.read_csv(INPUT_DIR \/ \"train.csv\")\ndf_oof = df_train[[\"Id\",\"Pawpularity\"]].copy()\n\ndf_train[\"Id\"] = df_train[\"Id\"].apply(lambda x: os.path.join(INPUT_DIR, \"train\", x + \".jpg\"))\n\nsubmission = pd.read_csv(INPUT_DIR \/ \"sample_submission.csv\")\ndisplay(df_train.head())\n\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nif CFG.debug:\n    CFG.epochs = 2\n    CFG.loader[\"train\"][\"batch_size\"] = 2\n    CFG.trn_fold = [0]\n    df_train  =df_train.head(500)\n    ","4de42c4a":"df_train.isnull().sum()","92394b7b":"# LINE\u306b\u901a\u77e5\nimport requests\ndef send_line_notification(message):\n    env = \"\"\n    if IN_COLAB: env = \"colab\"\n    elif IN_KAGGLE: env = \"kaggle\"\n    elif LOCAL: env = \"local\"\n        \n    line_token = 'hoge'\n    endpoint = 'https:\/\/notify-api.line.me\/api\/notify'\n    message = f\"[{env}]{message}\"\n    payload = {'message': message}\n    headers = {'Authorization': 'Bearer {}'.format(line_token)}\n    requests.post(endpoint, data=payload, headers=headers)","f546d690":"df_train[\"fold\"] = -1\nFold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[CFG.target_col])):\n    df_train.loc[val_index, 'fold'] = int(n)\ndf_train['fold'] = df_train['fold'].astype(int)\ndf_oof['fold'] = df_train['fold']\nprint(df_train.groupby(['fold', CFG.target_col]).size())","6f906981":"IMAGENET_MEAN = [0.485, 0.456, 0.406]  # RGB\nIMAGENET_STD = [0.229, 0.224, 0.225]  # RGB\ndef get_default_transforms():\n    transform = {\n        \"train\": T.Compose(\n            [\n                T.RandomHorizontalFlip(),\n#                 T.RandomVerticalFlip(),\n                T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n#                 T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n        \"val\": T.Compose(\n            [\n                T.ConvertImageDtype(torch.float),\n                T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n            ]\n        ),\n    }\n    return transform","1d9931ba":"class PetfinderDataset(Dataset):\n    def __init__(self, df, feature_cols, image_size=224):\n        self._X = df[\"Id\"].values\n        self.meta = df[feature_cols].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n        self._transform = T.Compose([\n                                        T.Resize(image_size),  # 1\n                                        T.CenterCrop([image_size, image_size]),  # 2\n                                    ]\n                                    )\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        image = read_image(image_path)\n        image = self._transform(image)\n        features = torch.FloatTensor(self.meta[idx, :])\n        \n        if self._y is not None:\n            label = self._y[idx]\n            return image, features,  label\n        return image, features \n","d9bd8fce":"ds = PetfinderDataset(df_train, CFG.feature_cols, CFG.img_size)\nfor i in range(3):\n    print(\"=\"*50)\n    print(ds[0][i])\ndel ds","e7e020a6":"class DataModule(pl.LightningDataModule):\n    def __init__(self, \n                 df_train,\n                 df_val,\n                 df_test,\n                 cfg):\n        super().__init__()\n        self._df_train = df_train\n        self._df_val = df_val\n        self._df_test = df_test\n        self._cfg = cfg\n        \n    def setup(self, stage=None):\n        self.train_dataset = PetfinderDataset(self._df_train, self._cfg.feature_cols, self._cfg.img_size)\n        self.valid_dataset = PetfinderDataset(self._df_val, self._cfg.feature_cols, self._cfg.img_size)\n        self.test_dataset = PetfinderDataset(self._df_test, self._cfg.feature_cols, self._cfg.img_size)\n        \n    # Trainer.fit() \u6642\u306b\u547c\u3073\u51fa\u3055\u308c\u308b\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, **self._cfg.loader['train'])\n\n    # Trainer.fit() \u6642\u306b\u547c\u3073\u51fa\u3055\u308c\u308b\n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset, **self._cfg.loader['valid'])\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, **self._cfg.loader['valid'])","0511c3d5":"Data = DataModule(df_train,df_train,df_train, CFG)\nData.setup()\nsample_dataloader = Data.train_dataloader()\nimages, features, labels = iter(sample_dataloader).next()\nplt.figure(figsize=(12, 12))\nfor it, (image, label) in enumerate(zip(images[:16], labels[:16])):\n    plt.subplot(4, 4, it+1)\n    plt.imshow(image.permute(1, 2, 0))\n    plt.axis('off')\n    plt.title(f'Pawpularity: {int(label)}')","925538a1":"# ====================================================\n# criterion\n# ====================================================\ndef get_criterion(cfg):\n    if cfg.criterion_name == 'BCEWithLogitsLoss':\n        # pl\u3060\u3068to(device)\u3044\u3089\u306a\u3044\n        criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n    elif cfg.criterion_name == 'CrossEntropyLoss':\n        criterion = nn.CrossEntropyLoss()\n    else:\n        raise NotImplementedError\n    return criterion\n# ====================================================\n# optimizer\n# ====================================================\ndef get_optimizer(model: nn.Module, config: dict):\n    \"\"\"\n    input:\n    model:model\n    config:optimizer_name\u3084lr\u304c\u5165\u3063\u305f\u3082\u306e\u3092\u6e21\u3059\n    \n    output:optimizer\n    \"\"\"\n    optimizer_name = config.optimizer_name\n    if 'Adam' == optimizer_name:\n        return Adam(model.parameters(),\n                    lr=config.lr,\n                    weight_decay=config.weight_decay,\n                    amsgrad=config.amsgrad)\n    elif 'RAdam' == optimizer_name:\n        return optim.RAdam(model.parameters(),\n                           lr=config.lr,\n                           weight_decay=config.weight_decay)\n    elif 'AdamW' == optimizer_name:\n        return torch.optim.AdamW(model.parameters(),\n                                 lr=config.lr,\n                                 weight_decay=config.weight_decay)\n    elif 'Ranger' == optimizer_name:\n        return optim.Ranger(model.parameters(),\n                            lr=config.lr)\n    elif 'sgd' == optimizer_name:\n        return SGD(model.parameters(),\n                   lr=config.lr,\n                   momentum=0.9,\n                   nesterov=True,\n                   weight_decay=config.weight_decay,)\n    else:\n        raise NotImplementedError\n\n# ====================================================\n# scheduler\n# ====================================================\ndef get_scheduler(cfg, optimizer):\n    if cfg.scheduler=='ReduceLROnPlateau':\n        \"\"\"\n        factor : \u5b66\u7fd2\u7387\u306e\u6e1b\u8870\u7387\n        patience : \u4f55\u30b9\u30c6\u30c3\u30d7\u5411\u4e0a\u3057\u306a\u3051\u308c\u3070\u6e1b\u8870\u3059\u308b\u304b\u306e\u5024\n        eps : nan\u3068\u304bInf\u56de\u907f\u7528\u306e\u5fae\u5c0f\u6570\n        \"\"\"\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=cfg.factor, patience=cfg.patience, verbose=True, eps=cfg.eps, min_lr=cfg.min_lr)\n    elif cfg.scheduler=='CosineAnnealingLR':\n        \"\"\"\n        T_max : 1 \u534a\u5468\u671f\u306e\u30b9\u30c6\u30c3\u30d7\u30b5\u30a4\u30ba\n        eta_min : \u6700\u5c0f\u5b66\u7fd2\u7387(\u6975\u5c0f\u5024)\n        \"\"\"\n        scheduler = CosineAnnealingLR(optimizer, T_max=cfg.T_max, eta_min=cfg.min_lr, last_epoch=-1)\n    elif cfg.scheduler=='CosineAnnealingWarmRestarts':\n        \"\"\"\n        T_0 : \u521d\u671f\u306e\u7e70\u308a\u304b\u3048\u3057\u56de\u6570\n        T_mult : \u30b5\u30a4\u30af\u30eb\u306e\u30b9\u30b1\u30fc\u30eb\u500d\u7387\n        \"\"\"\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=cfg.T_0, T_mult=1, eta_min=cfg.min_lr, last_epoch=-1)\n    else:\n        raise NotImplementedError\n    return scheduler\n\ndef get_lightning_scheduler(cfg, optimizer):\n    scheduler = get_scheduler(cfg, optimizer)\n    if cfg.scheduler=='ReduceLROnPlateau':\n        return {'scheduler': scheduler,\n                'monitor': 'val_loss_epoch',\n                'interval': 'epoch',\n                'frequency': 1}\n    else:\n        return {'scheduler': scheduler,\n                'interval': 'step',\n                'frequency': 1}\n","4bd6faaf":"# ====================================================\n# model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone = timm.create_model(model_name=self.cfg.model_name,\n                                          pretrained=pretrained,\n                                          in_chans=self.cfg.in_chans,\n                                          num_classes=0)\n        self.dropout1 = nn.Dropout(p=0.5)\n        self.dropout2 = nn.Dropout(p=0.5)\n        self.fc = nn.LazyLinear(self.cfg.target_size)\n        \n    def forward(self, x, features):\n        f = self.backbone(x) # (bs, embedding_size)\n        f = self.dropout1(f)\n        if features.shape[1] != 0:\n            f = torch.cat([f, features],dim=1)\n            f =  self.dropout2(f)\n        out = self.fc(f)\n        return out\n    \ndef get_model(cfg):\n    model = CustomModel(cfg, pretrained=cfg.pretrained)\n    return model\n","2cec0871":"# # model\u306e\u52d5\u4f5c\u78ba\u8a8d\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model = get_model(CFG).to(device)\n# Data = DataModule(df_train,df_train,df_train, CFG)\n# Data.setup()\n# sample_dataloader = Data.train_dataloader()\n# images, features, labels = iter(sample_dataloader).next()\n# images = images.to(device)\n# features = features.to(device)\n# labels = labels.to(device)\n# labels = labels.float() \/ 100.0\n# transform = get_default_transforms()\n# images = transform[\"train\"](images)\n# output = model(images,features).squeeze(1)\n# print(output)\n# del Data, model, images, features, labels, output\n# gc.collect()","3ecd89fb":"# criterion = get_criterion()\n# criterion(output, labels)","c4b8231e":"#scheduler\u306e\u78ba\u8a8d\nmodel = get_model(CFG)\noptimizer = get_optimizer(model, CFG)\nscheduler = get_scheduler(CFG,optimizer)\nfrom pylab import rcParams\nlrs = []\nfor epoch in range(1, CFG.epochs+1):\n    scheduler.step(epoch-1)\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nrcParams['figure.figsize'] = 20,3\nprint(lrs)\nplt.plot(lrs)","39d68c89":"class Trainer(pl.LightningModule):\n    \"\"\"\n    label\u3092[0,100]\u304b\u3089[0,1]\u306b\u5909\u3048\u30662\u30af\u30e9\u30b9\u5206\u985e\u30bf\u30b9\u30af\u306b\u3059\u308b\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.model = get_model(cfg)\n        self.criterion = get_criterion(cfg)\n        self.transform = get_default_transforms()\n    \n    def forward(self, x, features):\n        output = self.model(x, features)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'train')\n        return {'loss': loss, 'pred': pred, 'labels': labels}\n    \n    def validation_step(self, batch, batch_idx):\n        loss, pred, labels = self.__share_step(batch, 'val')\n        return {'pred': pred, 'labels': labels}\n\n    def __share_step(self, batch, mode):\n        images, features, labels = batch\n        labels = labels.float() \/ 100.0\n        images = self.transform[mode](images)\n\n        logits = self.forward(images, features).squeeze(1)\n        loss = self.criterion(logits, labels)\n\n        pred = logits.sigmoid().detach().cpu() * 100.\n        labels = labels.detach().cpu() * 100.\n        \n        return loss, pred, labels\n\n    def training_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'train') \n        self.log(\"lr\", self.optimizer.param_groups[0]['lr'], prog_bar=True, logger=True)\n    \n    \n    def validation_epoch_end(self, outputs):\n        self.__share_epoch_end(outputs, 'val') \n\n    def __share_epoch_end(self, outputs, mode):\n        preds, labels = [], []\n        for output in outputs:\n            pred, label = output['pred'], output['labels']\n            preds.append(pred)\n            labels.append(label)\n        preds = torch.cat(preds)\n        labels = torch.cat(labels)\n        metrics = torch.sqrt(((labels - preds) ** 2).mean())\n        \n        self.log(f\"{mode}_loss_epoch\", metrics)\n        \n    def predict_step(self, batch, batch_idx):\n        _, pred, labels = self.__share_step(batch, 'val')\n        return pred\n    \n    def configure_optimizers(self):\n        self.optimizer = get_optimizer(self, self.cfg)\n        self.scheduler = get_lightning_scheduler(self.cfg, self.optimizer)\n        return {'optimizer': self.optimizer, 'lr_scheduler': self.scheduler}","a500c421":"def train() -> None:\n    for fold in range(CFG.n_fold):\n        if not fold in CFG.trn_fold:\n            continue\n        print(f\"{'='*38} Fold: {fold} {'='*38}\")\n        # Logger\n        #======================================================\n        lr_monitor = LearningRateMonitor(logging_interval='step')\n        \n        loss_checkpoint = ModelCheckpoint(\n            dirpath=OUTPUT_DIR,\n            filename=f\"best_loss_fold{fold}\",\n            monitor=\"val_loss_epoch\",\n            save_last=True,\n            save_top_k=1,\n            save_weights_only=True,\n            mode=\"min\",\n        )\n        csv_logger = CSVLogger(save_dir=str(OUTPUT_DIR), name=f\"fold_{fold}\")\n        wandb_logger = WandbLogger(\n            project=f'{CFG.competition}',\n            group= f'{CFG.exp_name}',\n            name = f'Fold{fold}',\n            save_dir=OUTPUT_DIR\n        )\n        data_module = DataModule(\n          df_train[df_train['fold']!=fold],\n          df_train[df_train['fold']==fold], \n          df_train[df_train['fold']==fold], \n          CFG\n        )\n        data_module.setup()\n        \n        CFG.T_max = int(math.ceil(len(data_module.train_dataloader())\/CFG.grad_acc)*CFG.epochs)\n        print(f\"set schedular T_max {CFG.T_max}\")\n        early_stopping_callback = EarlyStopping(monitor='val_loss_epoch', mode=\"min\", patience=20)\n        \n        trainer = pl.Trainer(\n            logger=[csv_logger,wandb_logger],\n            callbacks=[loss_checkpoint, early_stopping_callback],#lr_monitor,early_stopping_callback\n            default_root_dir=OUTPUT_DIR,\n            accumulate_grad_batches=CFG.grad_acc,\n            max_epochs=CFG.epochs,\n            precision=CFG.precision,\n            **CFG.trainer\n        )\n        # ================================\n        # Train\n        # ================================\n        model = Trainer(CFG)\n        trainer.fit(model, data_module)\n        \n        del model, #data_module\n        \n        if CFG.inference:\n            # oof\n            # ================================\n            # Load best loss model\n            best_model = Trainer.load_from_checkpoint(cfg=CFG,checkpoint_path=loss_checkpoint.best_model_path)\n            torch.save(best_model.model.state_dict(),OUTPUT_DIR + '\/' + f'{CFG.exp_name}_fold{fold}_best.pth')\n            \n            predictions = trainer.predict(best_model, data_module.test_dataloader())\n            preds = []\n            for p in predictions:\n                preds += p\n            preds = torch.stack(preds).flatten()\n            df_oof.loc[df_oof[\"fold\"] == fold, ['pred']] = preds.to('cpu').detach().numpy()\n            df_oof.to_csv(OUTPUT_DIR + '\/' + f'oof.csv',index=False)\n        \n        wandb.finish()      ","ec67529d":"train()\n# send_line_notification(\"finished\")\nwandb.finish()","627cb51a":"## Import Libraries","13da72f4":"## Get env","ab9fac66":"## Config","a812622a":"## CV Split","f8ca1307":"## Utils","f931eb01":"## About this notebook\n\n+ This notebook is training NFNet-F3 that is part of the 2nd place solution.\n+ Details of the solution are in the following link\n  + [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/300929)\n+ CV : 17.59169\n  + I don't do submissions on single models.\n+ Training environment : RTX3090\n  \n### Reference\n\nSome of the code is based on this [notebook](https:\/\/www.kaggle.com\/phalanx\/train-swin-t-pytorch-lightning). Thank you\n","361e5c51":"## DataModule","120e90b2":"## Dataset","488127b2":"## Train","59ef445c":"## Transforms","116456f0":"## Directory & LoadData","cef210bf":"### Training log","41648940":"## Pytorch Lightning Module","d2ff4f68":"![image.png](attachment:672d9770-c711-4ed9-b186-80ff330a9c03.png)"}}