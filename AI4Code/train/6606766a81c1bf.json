{"cell_type":{"ff39e952":"code","637616a8":"code","1abd3215":"code","e1e53229":"code","e72982a1":"code","f27ff0a0":"code","028abc45":"code","40e59b4c":"code","fd4bf208":"code","23934299":"code","eee530ce":"code","b993dd6f":"code","6cfbf382":"code","477349da":"code","19a0f073":"code","053a843c":"code","b7316fe6":"code","031dcb1a":"code","44fc4e5f":"code","4b21adc9":"code","85f8e715":"code","16a77611":"code","4f26a149":"code","c04fa4a7":"code","85eb0faa":"code","bdc385e9":"code","0cb1fc5f":"code","6872c5d6":"code","a6a06352":"code","58bf208c":"code","a6241e1f":"code","48958cb0":"code","1016374e":"code","4e327062":"code","c0369951":"code","084540a6":"code","3fb51052":"code","d6611506":"code","6d3b1b78":"code","5d6fdbb6":"code","21c46ec7":"code","56c1c1ec":"code","ee11fb0e":"code","2d53b964":"code","27380ded":"code","b19aa8a2":"code","5cebb147":"code","eb6db710":"code","f0a5b0c9":"markdown","8fa2e692":"markdown","5d9ba550":"markdown","57c7e1f5":"markdown","8076e7ec":"markdown","e212aa03":"markdown","b7294ecd":"markdown"},"source":{"ff39e952":"!pip install fastai2\n!pip install rfpimp","637616a8":"import numpy as np\nimport pandas as pd\nimport sys\nimport sklearn\nimport os\nimport pathlib\nimport fastai2\nimport numpy as np\nimport pandas as pd\nfrom fastai2.tabular.all import *\nfrom fastai2.basics import *\nfrom rfpimp import *\n\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nprint(fastai2.__version__)\n\n","1abd3215":"path = pathlib.Path('\/kaggle\/input\/cat-in-the-dat-ii\/')\ndf =  pd.read_csv(path\/'train.csv')\ntestdf = pd.read_csv(path\/'test.csv')\ntest_id = testdf['id']\n","e1e53229":"df.head(10).T","e72982a1":"df.columns","f27ff0a0":"df['target'] = df['target'].astype('category')","028abc45":"df.info()","40e59b4c":"df.nunique()","fd4bf208":"cat_names = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n       'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month']\ncont_names = ['ord_0']\nprocs = [FillMissing, Categorify, Normalize]\ndep_var = 'target'\n#block_y = CategoryBlock()\n#splits = RandomSplitter()(range_of(df))\nsplits = TrainTestSplitter(test_size=0.20, stratify= df[dep_var])(range_of(df))","23934299":"splits","eee530ce":"pd.options.mode.chained_assignment=None","b993dd6f":"to = TabularPandas(df, procs, cat_names, cont_names, dep_var, y_block=CategoryBlock(),\n                   splits=splits, inplace=True, reduce_memory=True)","6cfbf382":"config = tabular_config(embed_p = 0.04) #ps=[0.001,0.01]","477349da":"import torch\nfrom torch import nn\n\ndls = to.dataloaders(bs = 512)\ndls.c = 2\nfrom fastai2.metrics import *\nlearn = tabular_learner(dls,\n                        layers=[100,50],\n                        config = config,\n                        metrics=[accuracy, RocAuc(average='weighted'), F1Score(), Precision(), Recall()])","19a0f073":"learn.summary()","053a843c":"lr_best, _ = learn.lr_find()","b7316fe6":"lr_best, _","031dcb1a":"learn.fit_one_cycle(30, lr_best,\n                    cbs=[SaveModelCallback(),\n                         EarlyStoppingCallback(monitor='valid_loss', min_delta=0.01, patience=10)])\ndl = learn.dls.test_dl(testdf)\nraw_test_preds = learn.get_preds(dl=dl)\nraw_test_preds[0].numpy()\ntest_preds = raw_test_preds[0].numpy().T[1]","44fc4e5f":"class PermutationImportance():\n    \"Calculate and plot the permutation importance\"\n    def __init__(self, learn:Learner, df=None, bs=None):\n        \"Initialize with a test dataframe, a learner, and a metric\"\n        self.learn = learn\n        self.df = df if df is not None else None\n        bs = bs if bs is not None else learn.dls.bs\n        self.dl = learn.dls.test_dl(self.df, bs=bs) if self.df is not None else learn.dls[1]\n        self.x_names = learn.dls.x_names.filter(lambda x: '_na' not in x)\n        self.na = learn.dls.x_names.filter(lambda x: '_na' in x)\n        self.y = dls.y_names\n        self.results = self.calc_feat_importance()\n        self.plot_importance(self.ord_dic_to_df(self.results))\n\n    def measure_col(self, name:str):\n        \"Measures change after column shuffle\"\n        col = [name]\n        if f'{name}_na' in self.na: col.append(name)\n        orig = self.dl.items[col].values\n        perm = np.random.permutation(len(orig))\n        self.dl.items[col] = self.dl.items[col].values[perm]\n        metric = learn.validate(dl=self.dl)[1]\n        self.dl.items[col] = orig\n        return metric\n\n    def calc_feat_importance(self):\n        \"Calculates permutation importance by shuffling a column on a percentage scale\"\n        print('Getting base error')\n        base_error = self.learn.validate(dl=self.dl)[1]\n        self.importance = {}\n        pbar = progress_bar(self.x_names)\n        print('Calculating Permutation Importance')\n        for col in pbar:\n            self.importance[col] = self.measure_col(col)\n        for key, value in self.importance.items():\n            self.importance[key] = (base_error-value)\/base_error #this can be adjusted\n        return OrderedDict(sorted(self.importance.items(), key=lambda kv: kv[1], reverse=True))\n\n    def ord_dic_to_df(self, dict:OrderedDict):\n        return pd.DataFrame([[k, v] for k, v in dict.items()], columns=['feature', 'importance'])\n\n    def plot_importance(self, df:pd.DataFrame, limit=20, asc=False, **kwargs):\n        \"Plot importance with an optional limit to how many variables shown\"\n        df_copy = df.copy()\n        df_copy['feature'] = df_copy['feature'].str.slice(0,25)\n        df_copy = df_copy.sort_values(by='importance', ascending=asc)[:limit].sort_values(by='importance', ascending=not(asc))\n        ax = df_copy.plot.barh(x='feature', y='importance', sort_columns=True, **kwargs)\n        for p in ax.patches:\n            ax.annotate(f'{p.get_width():.4f}', ((p.get_width() * 1.005), p.get_y()  * 1.005))\n            \nimp = PermutationImportance(learn)","4b21adc9":"tst = dl.xs","85f8e715":"type(tst), len(tst)\ntst.head()","16a77611":"import xgboost as xgb\nX_train, y_train = to.train.xs, to.train.ys.values.ravel()\nX_test, y_test = to.valid.xs, to.valid.ys.values.ravel()","4f26a149":"\nmodel = xgb.XGBClassifier(n_estimators = 100,\n                          max_depth=10,\n                          learning_rate=0.1,\n                          subsample=0.5,\n                          nthread = -1,\n                          max_delta_step = 5\n                         )\neval_set = [(X_train, y_train), (X_test, y_test)]\nxgb_model = model.fit(X_train, y_train,\n                      eval_metric=[\"error\", \"logloss\"],\n                      eval_set=eval_set, verbose=True,\n                      early_stopping_rounds=10)\n\n","c04fa4a7":" # retrieve performance metrics\n    results = xgb_model.evals_result()\n    epochs = len(results['validation_0']['logloss'])\n    x_axis = range(0, epochs)\n    \n    # plot log loss\n    fig, ax = plt.subplots(figsize=(8,8))\n    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n    ax.legend()\n    \n    plt.ylabel('Log Loss')\n    plt.title('XGBoost Log Loss')\n    plt.show()","85eb0faa":"from xgboost import plot_importance\nplot_importance(xgb_model)","bdc385e9":"xgb_valid = xgb_model.predict_proba(X_test)\nprint(accuracy(tensor(xgb_valid), tensor(y_test)))","0cb1fc5f":"# xgb_preds = xgb_model.predict_proba(X_test)\nxgb_preds = xgb_model.predict_proba(tst)[:, 1]\nprint(xgb_preds)\nprint(xgb_preds.shape)\n","6872c5d6":"from sklearn.ensemble import RandomForestClassifier\ntree = RandomForestClassifier(n_estimators=100)\ntree.fit(X_train, y_train)\nimp = importances(tree, X_test, to.valid.ys)","a6a06352":"plot_importances(imp)","58bf208c":"type(X_test), type(y_test)","a6241e1f":"tree_valid = np.argmax(tree.predict_proba(X_test), axis = 1)\ntree_valid","48958cb0":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\nprint(accuracy_score(tree_valid, y_test))","1016374e":"confusion_matrix(tree_valid, y_test)","4e327062":"#forest_preds = tree.predict_proba(X_test)\nforest_preds = tree.predict_proba(tst)[:, 1]","c0369951":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)","084540a6":"nb_valid = np.argmax(clf.predict_proba(X_test), axis = 1)\nprint(accuracy_score(nb_valid, y_test))\nprint(confusion_matrix(nb_valid, y_test))\nprint(roc_auc_score(nb_valid, y_test))","3fb51052":"#NB_preds = clf.predict_proba(X_test)\nNB_preds = clf.predict_proba(tst)[:, 1]","d6611506":"from sklearn import neighbors\nn_neighbors = 4\nclf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\nclf.fit(X_train, y_train)\nKNN4_preds = clf.predict_proba(tst)[:, 1]\nKNN4_preds\nknn_valid = np.argmax(clf.predict_proba(X_test), axis = 1)\nprint(accuracy_score(knn_valid, y_test))\nprint(confusion_matrix(knn_valid, y_test))\nprint(roc_auc_score(knn_valid, y_test))","6d3b1b78":"n_neighbors = 8\nclf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\nclf.fit(X_train, y_train)","5d6fdbb6":"knn_valid = np.argmax(clf.predict_proba(X_test), axis = 1)\nprint(accuracy_score(knn_valid, y_test))\nprint(confusion_matrix(knn_valid, y_test))\nprint(roc_auc_score(knn_valid, y_test))","21c46ec7":"KNN8_preds = clf.predict_proba(tst)[:, 1]\nKNN8_preds","56c1c1ec":"n_neighbors = 16\nclf = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\nclf.fit(X_train, y_train)\nKNN32_preds = clf.predict_proba(tst)[:, 1]\nKNN32_preds","ee11fb0e":"knn_valid = np.argmax(clf.predict_proba(X_test), axis = 1)\nprint(accuracy_score(knn_valid, y_test))\nprint(confusion_matrix(knn_valid, y_test))\nprint(roc_auc_score(knn_valid, y_test))","2d53b964":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0, max_iter=1000)\nclf.fit(X_train, y_train)\nLR_preds = clf.predict_proba(tst)[:, 1]\n\nLR_valid = np.argmax(clf.predict_proba(X_test), axis = 1)\nprint(accuracy_score(LR_valid, y_test))\nprint(confusion_matrix(LR_valid, y_test))\nprint(roc_auc_score(LR_valid, y_test))","27380ded":"data = {\n        'KNN4':KNN4_preds,\n        'KNN8':KNN8_preds,\n        'LR':LR_preds,\n        'NB':NB_preds,\n        'RF':forest_preds,\n        'XGB':xgb_preds,\n        'NN': test_preds\n       } \nresult = pd.DataFrame(data)","b19aa8a2":"result.head()","5cebb147":"result['Avg'] = (result.KNN8 + result.NB + result.RF + result.LR +\n                 result.XGB + result.NN)\/len(result.columns)\nresult.head()","eb6db710":"print(\"Saving submission file\")\nsubmission = pd.DataFrame.from_dict({\n    'id': test_id,\n    'target': result['Avg']\n})\nsubmission.to_csv(\"submission_stacking_v3.csv\", index=False)","f0a5b0c9":"## Demonstration to Model Stacking in combination with Fastai2\n**This work is inspired from the Zachary Mueller demonstration of Ensembling in Fastai2** - [Link](https:\/\/github.com\/muellerzr\/Practical-Deep-Learning-for-Coders-2.0\/tree\/master\/Tabular%20Notebooks)","8fa2e692":"## XGBOOST","5d9ba550":"## Categorical NaiveBayes","57c7e1f5":"## Logistic Regression","8076e7ec":"## Random Forest","e212aa03":"## Deep Learning Model","b7294ecd":"# KNN"}}