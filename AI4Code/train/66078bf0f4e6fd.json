{"cell_type":{"67f7906a":"code","667737f9":"code","8ea77a5d":"code","8fa4f438":"code","92c5b149":"code","5772ba0c":"code","fa76efe7":"code","d4bfb90c":"code","f36abcb4":"code","8d9651f0":"code","e4edd63b":"code","305e8472":"code","aefb7453":"code","347ed50b":"code","2f91c38d":"code","0e6e8710":"code","8bce7e93":"code","00104839":"code","7e5792fb":"code","8977cdb7":"code","95a8a46b":"code","67a4435a":"code","0feeede6":"code","69bae613":"code","9c01bbb7":"code","71bdcafd":"code","3340e0fa":"code","65021a7a":"markdown","b4e6b603":"markdown","146c64d7":"markdown","a9245edb":"markdown","9159e607":"markdown","4171a8aa":"markdown","10eb9d48":"markdown","54fca496":"markdown","2b690621":"markdown","c028b2f2":"markdown","9315f294":"markdown","f13f8a0a":"markdown","7250eb8f":"markdown","8432762e":"markdown","94733002":"markdown","871c59f2":"markdown","a869421b":"markdown","dc31be37":"markdown"},"source":{"67f7906a":"# import necessary libraries \nfrom sqlalchemy import create_engine\nimport nltk\nnltk.download(['punkt', 'wordnet'])\n\nimport re\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport pickle\n\nimport warnings\nwarnings.filterwarnings('ignore')","667737f9":"# load messages dataset\nmsg_df = pd.read_csv(\"..\/input\/disaster-response-messages\/disaster_messages.csv\")\nmsg_df.head()","8ea77a5d":"# load categories dataset\ncat_df = pd.read_csv(\"..\/input\/disaster-response-messages\/disaster_categories.csv\")\ncat_df.head()","8fa4f438":"# merge the datasets\ndf = msg_df.merge(cat_df, left_on='id', right_on='id', how='inner')\ndf.head()","92c5b149":"# create a dataframe of the 36 individual category columns\ncategories = df[\"categories\"].str.split(';', expand=True)\ncategories.head()","5772ba0c":"# select the first row of the categories dataframe\nrow = categories[0:1]\n\n# use this row to extract a list of new column names for categories.\n# one way is to apply a lambda function that takes everything \n# up to the second to last character of each string with slicing\ncategory_col = row.apply(lambda x: x.str[:-2]).values.tolist()\nprint(category_col)","fa76efe7":"# rename the columns of `categories`\ncategories.columns = category_col\ncategories.head()","d4bfb90c":"for column in categories:\n    # set each value to be the last character of the string\n    categories[column] = categories[column].str[-1]\n    \n    # convert column from string to numeric\n    categories[column] = pd.to_numeric(categories[column])\ncategories.head()","f36abcb4":"# drop the original categories column from `df`\n\ndf.drop(['categories'], axis=1, inplace = True)\ndf.head()","8d9651f0":"# concatenate the original dataframe with the new `categories` dataframe\ndf = pd.concat([df, categories], axis=1)\ndf.head()","e4edd63b":"# check number of duplicates\ndf.duplicated().sum()","305e8472":"# drop duplicates\ndf = df.drop_duplicates()","aefb7453":"# check number of duplicates\ndf.duplicated().sum()","347ed50b":"# split the dataset\n\nX = df.message\ny = df.iloc[:,4:]\ncategory_names = y.columns","2f91c38d":"def tokenize(text):\n    url_regex = 'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    # detect all URL present in the messages\n    detected_urls = re.findall(url_regex, text)\n    # replace URL with \"urlplaceholder\"\n    for url in detected_urls:\n        text = text.replace(url, \"urlplaceholder\")\n\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n\n    clean_tokens = []\n    for tok in tokens:\n        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n        clean_tokens.append(clean_tok)\n\n    return clean_tokens","0e6e8710":"pipeline = Pipeline([\n    ('cvect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n])","8bce7e93":"X_train, X_test, y_train, y_test = train_test_split(X, y)\npipeline.fit(X_train, y_train)","00104839":"y_pred = pipeline.predict(X_test)\ny_pred[123].shape","7e5792fb":"# y_pred = pipeline.predict(X_test)\n\nfor i in range(36):\n    print(\"=======================\",y_test.columns[i],\"======================\")\n    print(classification_report(y_test.iloc[:,i], y_pred[:,i]))","8977cdb7":"pipeline.get_params()","95a8a46b":"parameters = {\n    'clf__estimator__n_estimators': [ 100, 150],\n    'clf__estimator__min_samples_split': [2, 4],\n    \n}+\n\ncv = GridSearchCV(pipeline, param_grid=parameters, verbose=2,)","67a4435a":"cv.fit(X_train, y_train)","0feeede6":"cv.cv_results_","69bae613":"#finding the best paramesters based on grip search\nprint(cv.best_params_)","9c01bbb7":"#building new model\noptimised_model = cv.best_estimator_\nprint (cv.best_estimator_)","71bdcafd":"y_pred = optimised_model.predict(X_test)\n\nfor i in range(36):\n    print(\"=============================\",y_test.columns[i], '=================================')\n    print(classification_report(y_test.iloc[:,i], y_pred[:,i]))","3340e0fa":"pickle.dump(optimised_model, open('model.pkl', 'wb'))","65021a7a":"### 7. Test your model\nShow the accuracy, precision, and recall of the tuned model.  \n\nSince this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!","b4e6b603":"### 8. Try improving your model further. Here are a few ideas:\n* try other machine learning algorithms\n* add other features besides the TF-IDF","146c64d7":"## To access the full project. Click [here](https:\/\/github.com\/sidharth178\/Disaster-Response-Pipeline).","a9245edb":"### 4. Convert category values to just numbers 0 or 1.\n- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n- You can perform [normal string actions on Pandas Series](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`.","9159e607":"### 6. Improve your model\nUse grid search to find better parameters. ","4171a8aa":"### 10. Use this notebook to complete `train.py`\nUse the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user.","10eb9d48":"## If you find this notebook useful, don't forget to UPVOTE \u23eb.\n## Follow me on [Github](https:\/\/github.com\/sidharth178). I used to upload good data science projects.","54fca496":"### 5. Replace `categories` column in `df` with new category columns.\n- Drop the categories column from the df dataframe since it is no longer needed.\n- Concatenate df and categories data frames.","2b690621":"### 4. Train pipeline\n- Split data into train and test sets\n- Train pipeline","c028b2f2":"### 3. Split `categories` into separate category columns.\n- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23\/generated\/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n- Use the first row of categories dataframe to create column names for the categories data.\n- Rename columns of `categories` with new column names.","9315f294":"### 3. Build a machine learning pipeline\nThis machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables.","f13f8a0a":"### 2. Merge datasets.\n- Merge the messages and categories datasets using the common id\n- Assign this combined dataset to `df`, which will be cleaned in the following steps","7250eb8f":"### 5. Test your model\nReport the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each.","8432762e":"# Project: Disaster Response Pipeline App\n\n## ETL Pipeline Preparation For Disaster Messages Classification\nFollow the instructions below to help you create your ETL pipeline.\n### 1. Import libraries and load datasets.\n- Import Python libraries\n- Load `messages.csv` into a dataframe and inspect the first few lines.\n- Load `categories.csv` into a dataframe and inspect the first few lines.","94733002":"### 2. Write a tokenization function to process your text data","871c59f2":"## ML Pipeline Preparation For Disaster Messages Classification\nFollow the instructions below to help you create your ML pipeline.\n","a869421b":"### 9. Export your model as a pickle file","dc31be37":"### 6. Remove duplicates.\n- Check how many duplicates are in this dataset.\n- Drop the duplicates.\n- Confirm duplicates were removed."}}