{"cell_type":{"5e8f5d73":"code","d865dba1":"code","076cc509":"code","d2330a27":"code","18896c43":"code","c6369927":"code","f1ce988e":"code","93ae75a4":"code","eef5e69d":"code","ce077822":"code","51bf8ead":"code","ad92c6d5":"code","6fdddedf":"code","e22d5cd9":"code","c8bdb1d1":"code","606c7d32":"code","d658b884":"markdown","01ca6dc6":"markdown","ce65a997":"markdown","a6322c13":"markdown","320b8d0e":"markdown","c391780c":"markdown","c78e8cdb":"markdown","5e59f0c9":"markdown","7ef2576a":"markdown","26fc0e2e":"markdown","a75cebd9":"markdown","01d358f0":"markdown","2edbf1eb":"markdown"},"source":{"5e8f5d73":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n\nimport lightgbm as lgb\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nimport gc","d865dba1":"df = pd.read_csv(r\"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")","076cc509":"df.info()","d2330a27":"def missing_KNN_imputer (df, n_neig, metric):\n    \n    \"\"\" This function replaces all missing numeric data using the KNN imputer\"\"\"\n    \n    #Define imputer\n    imputer = KNNImputer(n_neighbors=n_neig, metric=metric)\n\n    #Find columns with NaN and apply Imputer\n    df.loc[:, df.isnull().any()] = imputer.fit_transform(df.loc[:, df.isnull().any()])\n\n    #Return dataframe\n    return df","18896c43":"def cat_encoding (column):\n\n    \"\"\" This function takes a column and encodes the categorical variables with a unique key\"\"\"\n    \n    #List of unique values\n    value_list = df[column].unique()\n\n    #Creates a dict for encoding\n    e_list = {}\n\n    #Loop over all values in column and replace them with a number\n    for count, value in enumerate(value_list):\n        \te_list.update({value:count})\n    df[column] = df[column].replace(e_list)\n    \n    #Returns altered column\n    return df[column]\n","c6369927":"def replace_negative (column_list):\n    \n    \"\"\" This function replaces the negative values in some columns with a zero\"\"\"\n    \n    #Loops over columns in column list and replaces -1 with 0\n    for column in column_list:\n\n        df[column] = df[column].replace(-1,0)\n\n    return df","f1ce988e":"column_list = [\"Administrative_Duration\",\"Informational_Duration\",\"ProductRelated_Duration\"]\ndf = replace_negative(column_list)\n\ndf[\"Month\"] = cat_encoding(\"Month\")\ndf[\"VisitorType\"] = cat_encoding(\"VisitorType\")\n\ndf = missing_KNN_imputer(df, 10, \"nan_euclidean\")\n\ngc.collect()\n\ndel column_list","93ae75a4":"#Def X and y\ny = df[\"Revenue\"]\nX = df.drop(\"Revenue\",axis=1)\n\ngc.collect()\n\ndel df","eef5e69d":"#Train and test splits\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=22)\nkf = StratifiedKFold(n_splits=5,random_state=33)\n\ndel X,y","ce077822":"#Hyperopt Bayesian Optimization\ndef objective(params):\n    \n    \"\"\"\"\"\"\n\n    params = {'learning_rate': params['learning_rate'],\n     'max_depth': int(params['max_depth']),\n     'subsample': params['subsample'],\n     'num_leaves': int(params['num_leaves'])}\n\n    model = lgb.LGBMClassifier(**params,random_state=19)\n\n    score = cross_val_score(model, X_train, y_train, scoring=\"accuracy\", cv=StratifiedKFold()).mean()\n\n    print(\"Accuracy {:.3f} params {}\".format(score, params))\n\n    return score\n\nspace = {\n    \"learning_rate\" : hp.choice(\"learning_rate\", np.arange(0.05,0.31,0.05)),\n    \"max_depth\" : hp.choice(\"max_depth\", np.arange(5,80,1,dtype=int)),\n    \"subsample\" : hp.uniform(\"subsample\",0.8,1),\n    'num_leaves': hp.quniform('num_leaves', 8, 128, 2),\n    }\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=50)","51bf8ead":"print(\"Hyperopt estimated optimum {}\".format(best))","ad92c6d5":"def lightgbm_model(X_train,y_train,X_test,cat_features,parms):\n\n    test_pred = np.zeros(len(X_test))\n\n    for train_index, test_index in kf.split(X_train,y_train):\n        X_cv_train, X_cv_test = X_train.iloc[train_index], X_train.iloc[test_index]\n        y_cv_train, y_cv_test = y_train.iloc[train_index], y_train.iloc[test_index]\n\n        lgb_train = lgb.Dataset(X_cv_train,label = y_cv_train)\n        lgb_test = lgb.Dataset(X_cv_test,label = y_cv_test)\n\n        model = lgb.train(parms,train_set=lgb_train,num_boost_round=500,categorical_feature=cat_features,       valid_sets=lgb_test,verbose_eval=20, early_stopping_rounds=200)\n\n        test_pred += model.predict(X_test,num_iteration=model.best_iteration)\/5\n\n    return test_pred, model","6fdddedf":"cat_features = [\"SpecialDay\",'Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType','Weekend']\n\nparms = {\"objective\" : \"binary\",\n        'learning_rate': 0.04,\n        'max_depth': 56, \n        'num_leaves': 50, \n        'subsample': 0.95}\n\ntest_pred, best_model = lightgbm_model(X_train,y_train,X_test,cat_features,parms)\n\ngc.collect()\n\ndel cat_features, parms","e22d5cd9":"print(\"The accuracy score for this model is {}\".format(accuracy_score(y_test,test_pred.round())))\nprint(\"The f1 score for this model is {}\".format(f1_score(y_test,test_pred.round())))","c8bdb1d1":"conf = confusion_matrix(y_test,test_pred.round())\nprint(conf)\n\ngc.collect()","606c7d32":"lgb.plot_importance(best_model)","d658b884":"Time to apply the functions to the designated columns.","01ca6dc6":"Some of the features expectedly have a high ranking like exit rates, product-related pages, bounce rates etc. Nonetheless, a good amount of importance goes to \"secondary factors\" that are not immediately obvious. Those include the administrative part of the website, the categorical weekend function, region, month and informational duration. \n\nWe cannot make a definitive conclusion about these factors as we lack information. However, we can point the company in the right direction that could yield more improvements.","ce65a997":"# **Feature importance and analysis**\n\nPlease keep two things in mind while looking at the plot. The first one being that the F score is the number of times the feature is used in the splitting process. The second and most important one is that this is how the machine learning model ranks the features. To put it more clearly, the model might miss some conclusions that a human can make from the data. That doesn't mean that the model is bad, but that it has a different perspective and that is what we are after.","a6322c13":"Hyperparameter optimization is generally one of the more time-consuming and costly tasks. The Bayesian method alleviates some those issues in comparison to other available strategies. It focuses is more on the selection process of the hyperparameters that might yield improvement to the model rather than brute-forcing or random selection. The general idea behind it is that the algorithm proposes a set of hyperparameter candidates and evaluates them using the actual objective function. Those results are stored along with their respective candidates and used to construct\/improve a probability model of the objective function. Repeating the evaluation process improves the probability function with every iteration and subsequently uses the probability function to select the hyperparameters with the greatest \"Expected improvement\".\n\nThe algorithm spends some more time with the selection process of the next hyperparameter to maximize the \"Expected improvement\" compared to the alternatives. However, it is still much cheaper in computational cost by spending less time evaluating poor hyperparameter choices.","320b8d0e":"# **Conclusion**\n\nAnother fun little classification project and I am pretty content with the results. Originally I wanted to create just a Bayesian optimization notebook. However, after playing around the data a bit I decided to expand it a little.\n\nThank you for your time & I hope you enjoyed it as much as I enjoyed making it.","c391780c":"# **Scores**","c78e8cdb":"We are going to evaluate the model using the accuracy score, f1 score and a confusion matrix.The scores should give us a pretty good idea on the general quality of the prediction.","5e59f0c9":"# **Data preparation**","7ef2576a":"# **Importing libraries and data**","26fc0e2e":"I am going to do a classical train\/test split (80\/20) for the model. ","a75cebd9":"# **Introduction**\n\nHaving a revenue-generating website is both an art and a science. Users are more and more demanding while their attention spans are shorter and shorter.\n\nEvery aspect of the web site gets designed to combat this trend. Analytics departments gather all the possible data to try finding bottlenecks. Design teams spend countless hours polishing the page and trying to make it as user-friendly as possible. In the end, they all have 15 seconds to impress the user and gain his interest before he leaves the page.\n\nIn this notebook, I am going to try to predict if the user will buy something from the website or not. The prediction is a simple classification problem with almost all possible website metrics given. The question is why to do it on a full dataset when you need to gather all the data again to replicate it in real life. \n\nI believe that this problem can use a new standpoint and that predicting is not the end goal here.  The machine learning algorithm and the way it ranks the importance of individual features is just as important as the final prediction. Finishing the best possible model and taking into account its feature importance ranking can help us determine what features should be the focus of the company when making alterations to the website. \n\nWith this in mind, the goals of this notebook are:\n\n* Perform a quick data clean\n* Construct a Bayesian Hyperparameter Optimization function\n* Train and test the best possible LightGBM model\n* Analyze the feature importance graph","01d358f0":"# **LightGBM**\n\nThe classification problem was tackled with LightGBM, a tree-based machine-learning algorithm that dominated multiple Kaggle competitions. It is generally a great choice due to its many benefits regarding the speed and accuracy of predictions. Also, its GOSS and EFB inner workings allow me to play around with data on a decade-old laptop.","2edbf1eb":"# **Bayesian Hyperparameter Optimization**"}}