{"cell_type":{"b9197d74":"code","b1da5d39":"code","c2218752":"code","567fd025":"code","dbb244ae":"code","51c1c62c":"code","ce2bef99":"code","1d1be2be":"code","2dda6b5c":"code","e8cbf709":"code","0c18c5b2":"code","3f581a14":"code","6445404a":"code","4e630d21":"markdown","5c311df1":"markdown","aa2c5fab":"markdown","0a1d2f1b":"markdown","f887a3d0":"markdown","7e8be46b":"markdown","737ba3e6":"markdown","299a3646":"markdown","7b074284":"markdown","dba044ec":"markdown","e787f65a":"markdown","882606a8":"markdown","afc65674":"markdown","e518ad1e":"markdown","2b15bdb2":"markdown","77f6a4b1":"markdown","7b60b986":"markdown","80b080c2":"markdown","14b49f04":"markdown","f4369119":"markdown","dda15644":"markdown","23bc3eef":"markdown","a030a5aa":"markdown","01af4e5f":"markdown","b838086a":"markdown","4665b607":"markdown","8363f643":"markdown","954edff2":"markdown","cdb4536d":"markdown","629433c4":"markdown","967211a3":"markdown","d5eb4f2c":"markdown","c4cb12ad":"markdown","ca0e1540":"markdown","ddb37118":"markdown","5a0e63d0":"markdown","51e78651":"markdown","f50c9e29":"markdown","f02af378":"markdown","ca878b62":"markdown","0bd1ea01":"markdown"},"source":{"b9197d74":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy.random import normal\n# generate a sample\nsample= normal(size= 1000)\n#plot a histogram\nplt.hist(sample, bins= 10)\nplt.show()","b1da5d39":"plt.hist(sample, bins= 5)\nplt.show()","c2218752":"plt.hist(sample, bins= 3)\nplt.show()","567fd025":"sample= normal(loc= 50, scale= 5, size= 1000)","dbb244ae":"sample_mean= np.mean(sample)\nsample_std= np.std(sample)","51c1c62c":"print('Mean=%.3f, Standard Deviation=%.3f' % (sample_mean, sample_std))","ce2bef99":"from scipy.stats import norm\ndist = norm(sample_mean, sample_std)","1d1be2be":"values = [value for value in range(30, 70)]\nprobabilities = [dist.pdf(value) for value in values]","2dda6b5c":"# plot the histogram and pdf\nplt.hist(sample, bins=10, density=True)\nplt.plot(values, probabilities)","e8cbf709":"from numpy import hstack\n# generate a sample\nsample1 = normal(loc=20, scale=5, size=300)\nsample2 = normal(loc=40, scale=5, size=700)\nsample = hstack((sample1, sample2))\n# plot the histogram\nplt.hist(sample, bins=50)\nplt.show()","0c18c5b2":"from sklearn.neighbors import KernelDensity\n# fit density\nmodel = KernelDensity(bandwidth=3, kernel='gaussian')\nsample = sample.reshape((len(sample), 1))\nmodel.fit(sample)","3f581a14":"# sample probabilities for a range of outcomes\nvalues = np.asarray([value for value in range(1, 60)])\nvalues = values.reshape((len(values), 1))\nprobabilities = model.score_samples(values)\nprobabilities = np.exp(probabilities)","6445404a":"plt.hist(sample, bins=50, density=True)\nplt.plot(values[:], probabilities)\nplt.show()","4e630d21":"- In some cases, a data sample may not resemble a common probability distribution or cannot be easily made to fit the distribution.\n\n- This is often the case when the data has two peaks (bimodal distribution) or many peaks (multimodal distribution).\n\n- In this case, parametric density estimation is not feasible and alternative methods can be used that do not use a common distribution. Instead, an algorithm is used to approximate the probability distribution of the data without a pre-defined distribution, referred to as a nonparametric method.\n\n- The distributions will still have parameters but are not directly controllable in the same way as simple probability distributions.\n\n- The most common nonparametric approach for estimating the probability density function of a continuous random variable is called kernel smoothing, or kernel density estimation, KDE","5c311df1":"**The problem is, we may not know the probability distribution for a random variable. We rarely do know the distribution because we don\u2019t have access to all possible outcomes for a random variable. In fact, all we have access to is a sample of observations. As such, we must select a probability distribution.\nThis problem is referred to as probability density estimation.**","aa2c5fab":"In this case, a kernel is a mathematical function that returns a probability for a given value of a random variable. The kernel effectively smooths or interpolates the probabilities across the range of outcomes for a random variable such that the sum of probabilities equals one, a requirement of well-behaved probabilities.","0a1d2f1b":"# 2. Parametric Density Estimation","f887a3d0":"We can then pretend that we don\u2019t know the probability distribution and maybe look at a histogram and guess that it is normal. Assuming that it is normal, we can then calculate the parameters of the distribution, specifically the mean and standard deviation.","7e8be46b":"The class is then fit on a data sample via the fit() function. The function expects the data to have a 2D shape with the form [rows, columns], therefore we can reshape our data sample to have 1,000 rows and 1 column.","737ba3e6":"**Kernel Density Estimation: Nonparametric method for using a dataset to estimating probabilities for new points.**","299a3646":"# Thanks....","7b074284":"# 1. Histogram\n- The first step is to review the density of observations in the random sample with a simple histogram. From the histogram, we might be able to identify a common and well-understood probability distribution that can be used, such as a normal distribution. If not, we may have to fit a model to estimate the distribution.\n\n- A histogram is a plot that involves first grouping the observations into bins and counting the number of events that fall into each bin. The counts, or frequencies of observations, in each bin are then plotted as a bar graph with the bins on the x-axis and the frequency on the y-axis.","dba044ec":"The scikit-learn machine learning library provides the KernelDensity class that implements kernel density estimation.","e787f65a":"In this case, we can see that the PDF is a good fit for the histogram. It is not very smooth and could be made more so by setting the \u201cbandwidth\u201d argument to 4 samples or higher. Experiment with different values of the bandwidth and the kernel function.","882606a8":"For Power transformation you can refer my notebook: <link>https:\/\/www.kaggle.com\/mukeshchoudhary\/power-transformation<\/link>","afc65674":"Finally, we can plot a histogram of the data sample and overlay a line plot of the probabilities calculated for the range of values from the PDF.","e518ad1e":"We can clearly see the shape of the normal distribution. Note that our results will differ given the random nature of the data sample. Try running this a few times.","2b15bdb2":"We would not expect the mean and standard deviation to be 50 and 5 exactly given the small sample size and noise in the sampling process.","77f6a4b1":"# Probability Density Estimation:","7b60b986":"# 2. Probability Density:\n- A rdom variable x has a probability distribution p(x).\n\n- The relationship between the outcomes of a random variable and its probability is referred to as the probability density, or simply the density.\n\n- If a random variable is continuous, then the probability can be calculated via probability density function(PDF).\n\n- If a random variable is discrete, then the probability can be calculated via probability mass function(PMF).\n\n- The shape of the probability density function across the domain for a random variable is referred to as the probability distribution and common probability distributions have names, such as uniform, normal, exponential, etc.\n\n- Given a random variable, we are interested in the density of its probabilities.","80b080c2":"--> You might also see complex distributions, such as multiple peaks that don\u2019t disappear with different numbers of bins, referred to as a bimodal distribution, or multiple peaks, referred to as a multimodal distribution. You might also see a large spike in density for a given value or small range of values indicating outliers, often occurring on the tail of a distribution far away from the rest of the density.","14b49f04":"We can then sample the probabilities from this distribution for a range of values in our domain, in this case between 30 and 70.","f4369119":"--> In most cases, you will see a unimodal distribution, such as the familiar bell shape of the normal, the flat shape of the uniform, or the descending or ascending shape of an exponential or Pareto distribution.","dda15644":"- Then fit the distribution with these parameters, so-called parametric density estimation of our data sample.","23bc3eef":"- Probability density is the relationship between observations and their probability.\n\n- Some outcomes of a random variable will have low probability density and other outcomes will have a high probability density.\n\n- The overall shape of the probability density is referred to as a probability distribution.\n\n- The calculation of probabilities for specific outcomes of a random variable is performed by a probability density function(PDF).","a030a5aa":"- The common distributions are common because they occur again and again in different and sometimes unexpected domains.\n\n- Get familiar with the common probability distributions as it will help you to identify a given distribution from a histogram.\n\n- Once identified, you can attempt to estimate the density of the random variable with a chosen probability distribution. This can be achieved by estimating the parameters of the distribution from a random sample of data.\n\n- We refer to this process as parametric density estimation.\n\nFor example, the normal distribution has two parameters: the mean and the standard deviation. Given these two parameters, we now know the probability distribution function. These parameters can be estimated from data by calculating the sample mean and sample standard deviation.","01af4e5f":"**Once we have estimated the density, we can check if it is a good fit. This can be done in many ways, such as:**","b838086a":"# 1. Need\n\n- It is useful to know the probability density function for a sample of data in order to know whether a given observation is unlikely, or so unlikely as to be considered an outlier or anomaly and whether it should be removed. It is also helpful in order to choose appropriate learning methods that require input data to have a specific probability distribution.","4665b607":"1. Plotting the density function and comparing the shape to the histogram.\n2. Sampling the density function and comparing the generated sample to the real sample.\n3. Using a statistical test to confirm the data fits the distribution.","8363f643":"Reviewing a histogram of a data sample with a range of different numbers of bins will help to identify whether the density looks like a common probability distribution or not.","954edff2":"We have fewer samples with a mean of 20 than samples with a mean of 40, which we can see reflected in the histogram with a larger density of samples around 40 than around 20.","cdb4536d":"**Note:**\n    It is possible that the data does match a common probability distribution, but requires a transformation before parametric density estimation.\n\nFor example, you may have outlier values that are far from the mean or center of mass of the distribution. This may have the effect of giving incorrect estimates of the distribution parameters and, in turn, causing a poor fit to the data. These outliers should be removed prior to estimating the distribution parameters.\n\nAnother example is the data may have a skew or be shifted left or right. In this case, you might need to transform the data prior to estimating the parameters, such as taking the log or square root, or more generally, using a power transform like the Box-Cox transform.\n\nThese types of modifications to the data may not be obvious and effective parametric density estimation may require an iterative process of:\n\nLoop Until Fit of Distribution to Data is Good Enough:\n1. Estimating distribution parameters\n2. Reviewing the resulting PDF against the data\n3. Transforming the data to better fit the distribution","629433c4":"A parameter, called the smoothing parameter or the bandwidth, controls the scope, or window of observations, from the data sample that contributes to estimating the probability for a given sample","967211a3":"For example, given a random sample of a variable, we might want to know things like the shape of the probability distribution, the most likely value, the spread of values, and other properties.\n\nKnowing the probability distribution for a random variable can help to calculate moments of the distribution, like the mean and variance, but can also be useful for other more general considerations, like determining whether an observation is unlikely or very unlikely and might be an outlier or anomaly.","d5eb4f2c":"First, the class is constructed with the desired bandwidth (window size) and kernel (basis function) arguments. It is a good idea to test different configurations on your data. In this case, we will try a bandwidth of 3 and a Gaussian kernel.","c4cb12ad":"In this case, we can use the norm() SciPy function.","ca0e1540":"We can then evaluate how well the density estimate matches our data by calculating the probabilities for a range of observations and comparing the shape to the histogram, just like we did for the parametric case in the prior section.","ddb37118":"- There are a few steps in the process of density estimation for a random variable.","5a0e63d0":"For Statistical tests you can refer my notebook: <link>https:\/\/www.kaggle.com\/mukeshchoudhary\/statistical-tests<\/link>","51e78651":"# 3. Non-Parametric Density Estimation","f50c9e29":"Finally, we can create a histogram with normalized frequencies and an overlay line plot of values to estimated probabilities.","f02af378":"We will generate a random sample of 1,000 observations from a normal distribution with a mean of 50 and a standard deviation of 5.","ca878b62":"- Data with this distribution does not nicely fit into a common probability distribution, by design. It is a good case for using a nonparametric kernel density estimation method.\n\n","0bd1ea01":"The PDF is fit using the estimated parameters and the histogram of the data with 10 bins is compared to probabilities for a range of values sampled from the PDF.We can see that the PDF is a good match for our data."}}