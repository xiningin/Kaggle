{"cell_type":{"aeb1b3d6":"code","56b530dd":"code","c2dbfb3d":"code","4a40f9ff":"code","81cba2b4":"code","329bc86a":"code","73a75768":"code","61fdf6b2":"code","582d91fc":"code","3f537fa2":"code","560a6903":"code","17e03d65":"code","0e646f6e":"code","7fdb1dd4":"code","9ee0286d":"code","2383038b":"code","891f35a6":"code","b19f4c22":"code","dd860f15":"code","7b3bc21a":"code","2bd4452a":"code","fa8e9919":"code","46855176":"code","530a3af8":"code","a951492c":"code","be4571fe":"code","e5dfaf59":"code","286ad601":"code","4d3af847":"markdown","5546af90":"markdown","8fba0617":"markdown","09747c26":"markdown","294e96b3":"markdown","2fd24845":"markdown","ee5bba36":"markdown","aa5174a1":"markdown"},"source":{"aeb1b3d6":"import pandas as pd\n\n#Load data\ntrain_orig = pd.read_csv('..\/input\/train.csv')\ntest_orig = pd.read_csv('..\/input\/test.csv')\n\nprint(train_orig.shape,test_orig.shape)","56b530dd":"train_orig.info()\nprint('-'*40)\ntest_orig.info()","c2dbfb3d":"# Create label Y\nY = train_orig.SalePrice\n\n# Clean train and test data\ntrain=train_orig.drop(['Id','SalePrice'],axis=1)\ntest=test_orig.drop(['Id'],axis=1)","4a40f9ff":"#encode and creat X\ntrain_encoded=pd.get_dummies(train)\ntest_encoded = pd.get_dummies(test)\n\nfinal_train, final_test = train_encoded.align(test_encoded, join='left', axis=1)\n\nX=final_train","81cba2b4":"#show columns with missing data\n#train_column_with_missing = (final_train.isnull().sum())\n#print('final_train columns with missing data:' + str(dict(train_column_with_missing[train_column_with_missing > 0])))\n\n#test_column_with_missing = (final_test.isnull().sum())\n#print('final_test has columns with missing data'+ str(dict(test_column_with_missing[test_column_with_missing > 0])))","329bc86a":"# Split train data into validation and training data\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_Y, val_Y = train_test_split(X, Y,random_state=1, test_size=0.25)","73a75768":"#cross validation on XGBRegressor\n#from xgboost import XGBRegressor\n#from sklearn.pipeline import make_pipeline\n#from sklearn.impute import SimpleImputer\n#from sklearn.model_selection import cross_val_score\n\n#my_pipeline_imp=make_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, learn_rate=0.05))\n#scores=cross_val_score(my_pipeline_imp, X, y, scoring='neg_mean_absolute_error', cv=5)\n#print(scores)\n#print('Mean Absolute Error with impu%.2f' %(-1 * scores.mean()))\n\n#my_pipeline_noimp=make_pipeline(XGBRegressor())\n#scores=cross_val_score(my_pipeline_noimp, X, y, scoring='neg_mean_absolute_error', cv=5)\n#print(scores)\n#print('Mean Absolute Error without imp %.2f' %(-1 * scores.mean()))","61fdf6b2":"#my_pipeline_imp2=make_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, learn_rate=0.05))\n#scores=cross_val_score(my_pipeline_imp2, X, y, scoring='neg_mean_absolute_error', cv=5)\n#print(scores)\n#print('Mean Absolute Error with impu2 %.2f' %(-1 * scores.mean()))","582d91fc":"#Create a measurer\nfrom xgboost import XGBRegressor\n\ndef score_dataset(train_X, val_X, train_y, val_y):\n    model = XGBRegressor(n_estimators=1000, learn_rate=0.05,random_state=29)\n    model.fit(train_X, train_y)\n    preds = model.predict(val_X)\n    return mean_absolute_error(val_y, preds)","3f537fa2":"#drop missing values\n\n'''from xgboost import XGBRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\ncols_with_missing = [col for col in X.columns \n                               if X[col].isnull().any()]\nX_noMis = X.drop(cols_with_missing, axis=1)\nmy_pipeline_drop=make_pipeline(XGBRegressor(n_estimators=1000, learn_rate=0.05))\nscores=cross_val_score(my_pipeline_drop, X_noMis, y, scoring='neg_mean_absolute_error', cv=5)\nprint(scores)\nprint('Mean Absolute Error with drop %.2f' %(-1 * scores.mean()))\n'''\nfrom sklearn.metrics import mean_absolute_error\ncols_with_missing = [col for col in train_X.columns \n                                 if train_X[col].isnull().any()]\ntrain_X_noMis = train_X.drop(cols_with_missing, axis=1)\nval_X_noMis  = val_X.drop(cols_with_missing, axis=1)\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(train_X_noMis, val_X_noMis, train_Y, val_Y))","560a6903":"#imputation\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom xgboost import XGBRegressor\n\nmy_imputer = SimpleImputer()\ntrain_X_impu = my_imputer.fit_transform(train_X)\nval_X_impu = my_imputer.transform(val_X)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(train_X_impu,val_X_impu, train_Y, val_Y))\n\n'''my_pipeline_impute=make_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, learn_rate=0.05))\nscores=cross_val_score(my_pipeline_impute, X, y, scoring='neg_mean_absolute_error', cv=5)\nprint(scores)\nprint('Mean Absolute Error with imputer %.2f' %(-1 * scores.mean()))'''","17e03d65":"# impute and fit&predict using pipeline\n#my_pipeline=make_pipeline(SimpleImputer(),XGBRegressor(n_estimators=1000, learn_rate=0.05) )\n#my_pipeline.fit(train_X,train_y)\n#preds=my_pipeline.predict(val_X)\n#mean_absolute_error(val_y, preds)","0e646f6e":"#imputation plus\n'''X_impuPlus = X.copy()\ncols_with_missing = (cl for cl in X.columns \n                              if X[cl].isnull().any())\nfor c in cols_with_missing:\n   X_impuPlus[c + '_was_missing'] = X_impuPlus[c].isnull()\n\nmy_pipeline_imputePlus=make_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, learn_rate=0.05))\nscores=cross_val_score(my_pipeline_imputePlus, X_impuPlus, y, scoring='neg_mean_absolute_error', cv=5)\nprint(scores)\nprint('Mean Absolute Error with imputerPlus %.2f' %(-1 * scores.mean()))'''\n\ntrain_X_impuPlus = train_X.copy()\nval_X_impuPlus = val_X.copy()\n\ncols_with_missing = (col for col in train_X.columns \n                               if train_X[col].isnull().any())\nfor col in cols_with_missing:\n   train_X_impuPlus[col + '_was_missing'] = train_X_impuPlus[col].isnull()\n   val_X_impuPlus[col + '_was_missing'] = val_X_impuPlus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\ntrain_X_impuPlus = my_imputer.fit_transform(train_X_impuPlus)\nval_X_impuPlus = my_imputer.transform(val_X_impuPlus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(train_X_impuPlus, val_X_impuPlus,train_Y, val_Y))","7fdb1dd4":"#drop columns with missing\n#cols_with_missing = [col for col in X.columns \n                             #  if X[col].isnull().any()]\n#X_noMis = X.drop(cols_with_missing, axis=1)\n\n#test_noMis=final_test.drop(cols_with_missing, axis=1)","9ee0286d":"#show columns with missing data\n#train_column_with_missing = (X_noMis.isnull().sum())\n#print('final_train columns with missing data:' + str(dict(train_column_with_missing[train_column_with_missing > 0])))\n\n#test_column_with_missing = (test_noMis.isnull().sum())\n#print(test_column_with_missing[test_column_with_missing > 0])","2383038b":"#Imputation of test and train data\nX_umimputed = X.copy()\ntest_umimputed = final_test.copy()\n#print('train_umimputed.shape'+ str(X_umimputed.shape))\n#print('test_umimputed.shape'+str(test_umimputed.shape))\n\n#train_cols_with_missing = [col for col in train_umimputed.columns \n                                 #if train_umimputed[col].isnull().any()]\n#print('+train_cols_with_missing'+ str(train_cols_with_missing))\n#for col in train_cols_with_missing:\n   # train_umimputed[col + '_was_missing'] = train_umimputed[col].isnull()\n#print('train_umimputed.shape'+ str(train_umimputed.shape))\n\n#test_cols_with_missing = (cl for cl in test_umimputed.columns \n                                 #if test_umimputed[cl].isnull().any())\n#print(list(test_cols_with_missing))\n#for cl in test_cols_with_missing:\n   # test_umimputed[cl + '_was_missing'] = test_umimputed[cl].isnull()\n#print('test_umimputed.shape'+ str(test_umimputed.shape))\n\n# Imputation\nmy_imputer = SimpleImputer()\nX_imputed = my_imputer.fit_transform(X_umimputed)\ntest_imputed = my_imputer.transform(test_umimputed)","891f35a6":"# Split imputed train data into validation and training data\ntrain_X_final, val_X_final, train_Y_final, val_Y_final = train_test_split(X_imputed, Y,random_state=1)","b19f4c22":"'''from xgboost import XGBRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\nmy_pipeline_x=make_pipeline(SimpleImputer(), XGBRegressor(n_estimators=100, learn_rate=0.05))\nscores=cross_val_score(my_pipeline_x, X_noMis, y, scoring='neg_mean_absolute_error', cv=5)\nprint(scores)\nprint('Mean Absolute Error x %.2f' %(-1 * scores.mean()))'''","dd860f15":"'''from sklearn.ensemble import RandomForestRegressor\nmy_pipeline_R=make_pipeline(SimpleImputer(), RandomForestRegressor(n_estimators=1000, max_leaf_nodes=401,random_state=3))\nscoresR=cross_val_score(my_pipeline_R, X_noMis, y, scoring='neg_mean_absolute_error', cv=5)\nprint(scoresR)\nprint('Mean Absolute Error RF %.2f' %(-1 * scoresR.mean()))'''","7b3bc21a":"#from sklearn.tree import DecisionTreeRegressor\n\n#my_pipeline_D=make_pipeline(SimpleImputer(), DecisionTreeRegressor(random_state=3))\n#scoresD=cross_val_score(my_pipeline_D, X_noMis, y, scoring='neg_mean_absolute_error', cv=5)\n#print(scoresD)\n#print('Mean Absolute Error DT %.2f' %(-1 * scoresD.mean()))","2bd4452a":"#Random Forest Model\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=100, max_leaf_nodes=401,random_state=3)\nrf_model.fit(train_X_final, train_Y_final)\nrf_val_predictions = rf_model.predict(val_X_final)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_Y_final)\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n\nrf_model = RandomForestRegressor(n_estimators=100,max_leaf_nodes=4010,random_state=3)\nrf_model.fit(train_X_final, train_Y_final)\nrf_val_predictions = rf_model.predict(val_X_final)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_Y_final)\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n\nrf_model = RandomForestRegressor(n_estimators=100,max_leaf_nodes=20000,random_state=29)\nrf_model.fit(train_X_final, train_Y_final)\nrf_val_predictions = rf_model.predict(val_X_final)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_Y_final)\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))","fa8e9919":"#XGBoost\nfrom xgboost import XGBRegressor\nxg_model = XGBRegressor(n_estimators=1000, learn_rate=0.05, random_state=1)\n# Add silent=True to avoid printing out updates with each cycle\nxg_model.fit(train_X_final, train_Y_final, verbose=False, early_stopping_rounds=500, \n             eval_set=[(val_X_final, val_Y_final)])\npredictions = xg_model.predict(val_X_final)\nprint(\"Mean Absolute Error : {:,.0f}\".format(mean_absolute_error(predictions, val_Y_final)))","46855176":"'''xg_model = XGBRegressor(n_estimators=1000, learn_rate=0.05)\nxg_model.fit(train_X_final, train_y_final, verbose=False, early_stopping_rounds=1000, \n             eval_set=[(val_X_final, val_y_final)])\npredictions = xg_model.predict(val_X_final)\nprint(\"Mean Absolute Error 1: {:,.0f}\".format(mean_absolute_error(predictions, val_y_final)))\n\nxg_model = XGBRegressor(n_estimators=500, learn_rate=0.01)\nxg_model.fit(train_X_final, train_y_final, verbose=False, early_stopping_rounds=5, \n             eval_set=[(val_X_final, val_y_final)])\npredictions = xg_model.predict(val_X_final)\nprint(\"Mean Absolute Error 2: {:,.0f}\".format(mean_absolute_error(predictions, val_y_final)))\n\nxg_model = XGBRegressor(n_estimators=100, learn_rate=0.001)\nxg_model.fit(train_X_final, train_y_final, verbose=False, early_stopping_rounds=7000, \n             eval_set=[(val_X_final, val_y_final)])\npredictions = xg_model.predict(val_X_final)\nprint(\"Mean Absolute Error 3: {:,.0f}\".format(mean_absolute_error(predictions, val_y_final)))'''","530a3af8":"from sklearn.model_selection import cross_val_score\n\nmy_pipeline=make_pipeline(XGBRegressor(n_estimators=1000, learn_rate=0.05, random_state=29))\nscores=cross_val_score(my_pipeline, X_imputed, Y, scoring='neg_mean_absolute_error', cv=5)\nprint(scores)\nprint('Mean Absolute Error with %.2f' %(-1 * scores.mean()))","a951492c":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\n\nmodel_full = XGBRegressor(n_estimators=1000, learn_rate=0.05, random_state=29)\nmodel_full.fit(X_imputed, Y,verbose=False, early_stopping_rounds=500, \n             eval_set=[(val_X_final, val_Y_final)])\npred = model_full.predict(test_imputed)","be4571fe":"#drop missing values in test data\n#test_with_missing = [col for col in predictors.columns \n                                 #if predictors[col].isnull().any()]\n#predictors_noMis = predictors.drop(test_with_missing, axis=1)","e5dfaf59":"#missing_val_count_by_column = (predictors_noMis.isnull().sum())\n#print(missing_val_count_by_column[missing_val_count_by_column > 0])","286ad601":"# make predictions which we will submit. \n#test_preds = xg_model_full_data.predict(test_imputed)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\noutput = pd.DataFrame({'Id': test_orig.Id,\n                       'SalePrice': pred})\n\noutput.to_csv('submission10.csv', index=False)","4d3af847":"Predicting housing prices.\n\nLet's greet the data first.","5546af90":"Now let's experiment different ways of handling the missing values.","8fba0617":"cross validation on the model","09747c26":"Choosing model:","294e96b3":"# Creating a Model For the Competition\n\nBuild a XGBOOST model and train it on all of **X** and **Y**.  ","2fd24845":"Imputation approach is taken. ","ee5bba36":"# Make Predictions\nRead the file of \"test\" data. And apply your model to make predictions","aa5174a1":"XGBoost is chosen. Tune the model:"}}