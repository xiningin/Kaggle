{"cell_type":{"c6c60598":"code","3e9d78b5":"code","c580596a":"code","224bfd21":"code","0b9e8019":"code","76364044":"code","bba15e7b":"code","4b2cb7bc":"code","a9b523f6":"code","11d1c37b":"code","5619b29b":"markdown","a4ec162b":"markdown","ea194652":"markdown","9986a2ca":"markdown","043a210f":"markdown","5e654ce3":"markdown","e8198049":"markdown","06398b40":"markdown","e9de7aa2":"markdown"},"source":{"c6c60598":"\nimport pandas as pd\n\n# Path of the file to read\niowa_file_path = '..\/input\/home-data\/train.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\n\n\n\nprint(\"Setup Complete\")","3e9d78b5":"home_data.head()\n#home_data.describe()","c580596a":"y = home_data[\"SalePrice\"]\nfeature_names = [\"LotArea\", \"YearBuilt\", \"1stFlrSF\", \"2ndFlrSF\", \"FullBath\", \"BedroomAbvGr\", \"TotRmsAbvGrd\"]\nX = home_data[feature_names]\n","224bfd21":"from sklearn.tree import DecisionTreeRegressor\n\nmodel =  DecisionTreeRegressor(random_state=1)\nmodel.fit(X, y)\n","0b9e8019":"from sklearn.metrics import mean_absolute_error\n\npredictions = model.predict(X)\nprint(predictions)\n\nval_mae = mean_absolute_error(y, predictions)\nprint(val_mae)\n","76364044":"from sklearn.model_selection import train_test_split\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\n\n#model definition\nmodel2 = DecisionTreeRegressor(random_state=1)\nmodel2.fit(train_X, train_y)\n#prediction on validation\nval_predictions = model2.predict(val_X)\n# MSE calculation\nval_mae2 = mean_absolute_error(val_y, val_predictions)\n\nprint(val_mae2)","bba15e7b":"print(\"MSE train on entire dataset:\",val_mae)\nprint(\"MSE train on solitted dataset:\",val_mae2)","4b2cb7bc":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)","a9b523f6":"candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\n#Alternative 1\nmin_mae= 0\n# Write loop to find the ideal tree size from candidate_max_leaf_nodes\nfor max_leaf_nodes in [5, 50,100, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n    if min_mae== 0 or my_mae<min_mae:\n        min_mae=my_mae\n        best_tree_size=max_leaf_nodes\n        \n        \n#Alternative 2\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\nbest_tree_size = min(scores, key=scores.get)\n\nprint(best_tree_size)","11d1c37b":"final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n# fit the final model\nfinal_model.fit(train_X,train_y)\nfinal_mae=get_mae(best_tree_size, train_X, val_X, train_y, val_y)\nprint(\"MSE train on solitted dataset:\",val_mae2)\nprint(\"MSE train on entire dataset, optimized leaf max num:\",final_mae)","5619b29b":"Creation of sub-sets and retraining","a4ec162b":"Prediction on the entire dataset and MAE calculation","ea194652":"Calcolo differenza dei due MSE","9986a2ca":"Data loading and visualization","043a210f":"now we will iterate on different parameters of leaf sizes in order to define the best one","5e654ce3":"# **Model parameters selection and optimization**\n# \nCreation of a function for re-training with different numbers of max leaf nodes and return the MAE","e8198049":"Data selection for prediction (all the dataset)","06398b40":"retraining and final model MAE","e9de7aa2":"Model preparation and training. random state 1 means that i will reach every time the same result (same init seed)"}}