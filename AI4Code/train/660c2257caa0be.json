{"cell_type":{"680e553e":"code","3581a027":"code","52ceb40e":"code","7b85aa4d":"code","457aac46":"code","32d9c5a0":"code","f73e7ce0":"code","1954fce7":"code","a20c3b90":"code","221f650d":"code","196f0520":"code","2cbaed41":"code","c414e9af":"code","670a8d18":"code","a375cef0":"code","c4fe6c69":"code","31a1dd75":"code","315a9338":"code","cabe1597":"code","321ed814":"code","ab4a1f22":"code","a30b62fd":"code","a7dcc98c":"markdown","e46a6c03":"markdown","9684aa16":"markdown","aad3863e":"markdown","3cc33654":"markdown","eb7ac6cd":"markdown","1dc0edc9":"markdown"},"source":{"680e553e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3581a027":"train_df = pd.read_csv(\"..\/input\/train.csv\")\nX_train = train_df[\"question_text\"].fillna(\"Is it unethical to sabotage a public kernel?\").values\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nX_test = test_df[\"question_text\"].fillna(\"Is it unethical to sabotage a public kernel?\").values\ny = train_df[\"target\"]","52ceb40e":"\ntrain_df[train_df[\"target\"] != 0][[\"question_text\", \"target\"]]","7b85aa4d":"\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.preprocessing import text, sequence","457aac46":"maxlen = 10\nmax_features = 30000\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n","32d9c5a0":"train_df[\"seq\"] = sequence.pad_sequences(X_train, maxlen=maxlen).tolist()\ntest_df[\"seq\"] = sequence.pad_sequences(X_test, maxlen=maxlen).tolist()","f73e7ce0":"import os\nimport zipfile\nimport numpy as np\nembeddings_index = {}\nGLOVE_DIR = \"..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt\"\nprint(os.listdir(\"..\/input\/embeddings\/glove.840B.300d\"))\n\nf = open(GLOVE_DIR)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    #print(values)\n    try:\n        coefs = np.asarray(values[1:], dtype='float32')\n    except:\n        pass\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\n\n\n","1954fce7":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nfrom keras.layers import Embedding","a20c3b90":"embedding_layer = Embedding(len(word_index) + 1,\n                            300,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)","221f650d":"from keras.layers import concatenate, Flatten, Lambda, Permute, Reshape, merge\ndef get_model():\n    inp = Input(shape=(maxlen, ))\n    x = embedding_layer(inp)\n    x = Bidirectional(CuDNNGRU(10, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(10, return_sequences=True))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([\n                        avg_pool, \n                        max_pool])\n\n    outp = Dense(1, activation=\"sigmoid\")(conc)\n    \n    model = Model(inputs=[inp], outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()","196f0520":"batch_size = 512\nepochs = 8","2cbaed41":"from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(train_df, y, test_size = 0.05, random_state=42)","c414e9af":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n\nmodel.compile(loss='binary_crossentropy',\n          optimizer= \"adam\",\n          metrics=[\"acc\", f1])","670a8d18":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\ncheckpoint = ModelCheckpoint('gru.h5', monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = False)\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n                                   verbose=1, mode='min', epsilon=0.0001)\nearly = EarlyStopping(monitor='val_loss', \n                      mode=\"min\", \n                      patience=10)\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","a375cef0":"#with self train\nfrom keras.models import load_model\n\nhist = model.fit([np.array(X_tra[\"seq\"].tolist())], y_tra, batch_size=batch_size, epochs=epochs,\n                  validation_data=([np.array(X_val[\"seq\"].tolist())], y_val),\n                  verbose=True, callbacks = callbacks_list)\n\n\n","c4fe6c69":"model = load_model('gru.h5', custom_objects={'f1': f1})\n\nval_pred1 = model.predict([np.array(X_val[\"seq\"].tolist())], batch_size=128)","31a1dd75":"positives = y_val[y_val > 0]","315a9338":"positive_scores = val_pred1[:, 0][y_val > 0]","cabe1597":"pos_text = X_val.loc[positives.index.values]\npos_text","321ed814":"pd.DataFrame(val_pred1).describe()","ab4a1f22":"pred_sort = val_pred1[:, 0][y_val > 0].argsort()[:250][::-1]\npd.DataFrame(val_pred1[y_val > 0][pred_sort]).describe()","a30b62fd":"X_val[y_val > 0].iloc[pred_sort]","a7dcc98c":"Now lets look at our predictions for just the positives and then sort them and take the 250 worst predictions. Then we can look at the text for these and get an idea of why our model might be having such a hard time with these","e46a6c03":"Let's look at what our predictions across the whole set look like","9684aa16":"Here are the positives from our validation set","aad3863e":"Distribution is interesting. Let's look at the text","3cc33654":"Let's load in some embeddings and run a quick model","eb7ac6cd":"Lets look at some of these insightful questions.","1dc0edc9":"Let's look at the predictions of the positives. My hypothesis is it will be very difficult to detect some of these as they are using difficult sarcasm or words out of vocabulary"}}