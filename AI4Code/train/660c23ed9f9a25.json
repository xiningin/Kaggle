{"cell_type":{"bbfab3cd":"code","e0ecc704":"code","73b4a467":"code","2b01f099":"code","aa28af31":"code","beed1c31":"code","9f239244":"code","387ebf3e":"code","8bbbb080":"code","0eacef90":"code","13d8668f":"code","2af112a5":"code","40f6aac1":"code","34a87f88":"code","5ee2006f":"code","c4b675fe":"code","bc48edc4":"code","81e9baf5":"code","0e4882f8":"markdown","5c944f8d":"markdown","64210d45":"markdown","be0413e2":"markdown","809da3a1":"markdown","52f08ec1":"markdown","8112f329":"markdown","f24de2c3":"markdown","af7d04f9":"markdown","605614c0":"markdown"},"source":{"bbfab3cd":"!pip install 'kaggle-environments==0.1.6' > \/dev\/null 2>&1","e0ecc704":"import numpy as np\nimport gym\nimport random\nimport matplotlib.pyplot as plt\nfrom random import choice\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make","73b4a467":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5):\n        self.env = make('connectx', debug=True)\n        self.pair = [None, 'negamax']\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        # Define required gym fields (examples):\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n\n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        if random.uniform(0, 1) < self.switch_prob:\n            self.switch_trainer()\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n\n\nclass QTable:\n    def __init__(self, action_space):\n        self.table = dict()\n        self.action_space = action_space\n        \n    def add_item(self, state_key):\n        self.table[state_key] = list(np.zeros(self.action_space.n))\n        \n    def __call__(self, state):\n        board = state.board[:] # Get a copy\n        board.append(state.mark)\n        state_key = np.array(board).astype(str)\n        state_key = hex(int(''.join(state_key), 3))[2:]\n        if state_key not in self.table.keys():\n            self.add_item(state_key)\n        \n        return self.table[state_key]","2b01f099":"env = ConnectX()","aa28af31":"alpha = 0.1\ngamma = 0.6\nepsilon = 0.99\nmin_epsilon = 0.1\n\nepisodes = 10000\n\nalpha_decay_step = 1000\nalpha_decay_rate = 0.9\nepsilon_decay_rate = 0.9999","beed1c31":"q_table = QTable(env.action_space)\n\nall_epochs = []\nall_total_rewards = []\nall_avg_rewards = [] # Last 100 steps\nall_qtable_rows = []\nall_epsilons = []\n\nfor i in tqdm(range(episodes)):\n    state = env.reset()\n\n    epsilon = max(min_epsilon, epsilon * epsilon_decay_rate)\n    epochs, total_rewards = 0, 0\n    done = False\n    \n    while not done:\n        if random.uniform(0, 1) < epsilon:\n            action = choice([c for c in range(env.action_space.n) if state.board[c] == 0])\n        else:\n            row = q_table(state)[:]\n            selected_items = []\n            for j in range(env.action_space.n):\n                if state.board[j] == 0:\n                    selected_items.append(row[j])\n                else:\n                    selected_items.append(-1e7)\n            action = int(np.argmax(selected_items))\n\n        next_state, reward, done, info = env.step(action)\n\n        # Apply new rules\n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05 # Try to prevent the agent from taking a long move\n\n        old_value = q_table(state)[action]\n        next_max = np.max(q_table(next_state))\n        \n        # Update Q-value\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table(state)[action] = new_value\n\n        state = next_state\n        epochs += 1\n        total_rewards += reward\n\n\n    all_epochs.append(epochs)\n    all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(all_total_rewards[max(0, i-100):(i+1)])\n    all_avg_rewards.append(avg_rewards)\n    all_qtable_rows.append(len(q_table.table))\n    all_epsilons.append(epsilon)\n\n    if (i+1) % alpha_decay_step == 0:\n        alpha *= alpha_decay_rate","9f239244":"len(q_table.table)","387ebf3e":"# for k in q_table.table.keys():\n#     print('State:', k)\n#     print('Action-Value:', list(q_table.table[k]), '\\n')","8bbbb080":"# plt.plot(all_epochs)\n# plt.xlabel('Episode')\n# plt.ylabel('Num of steps')\n# plt.show()","0eacef90":"# plt.plot(all_total_rewards)\n# plt.xlabel('Episode')\n# plt.ylabel('Total rewards')\n# plt.show()","13d8668f":"plt.plot(all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","2af112a5":"plt.plot(all_qtable_rows)\nplt.xlabel('Episode')\nplt.ylabel('Explored states')\nplt.show()","40f6aac1":"plt.plot(all_epsilons)\nplt.xlabel('Episode')\nplt.ylabel('Epsilon')\nplt.show()","34a87f88":"tmp_dict_q_table = q_table.table.copy()\ndict_q_table = dict()\n\nfor k in tmp_dict_q_table:\n    if np.count_nonzero(tmp_dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(tmp_dict_q_table[k]))","5ee2006f":"my_agent = '''def my_agent(observation, configuration):\n    from random import choice\n\n    q_table = ''' \\\n    + str(dict_q_table).replace(' ', '') \\\n    + '''\n\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    action = q_table[state_key]\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    return action\n    '''","c4b675fe":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","bc48edc4":"from submission import my_agent","81e9baf5":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate agent's performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","0e4882f8":"<a class=\"anchor\" id=\"define_useful_classes\"><\/a>\n# Define useful classes\nNOTE: It's not easy to generate a Q-Table with all possible states; and even if I can do so, the huge number of states will cost much of memory. So, I use the approach that dynamically adding newly discovered states into an object of QTable class created below.\n\n[Back to Table of Contents](#ToC)","5c944f8d":"<a class=\"anchor\" id=\"create_connectx_environment\"><\/a>\n# Create ConnectX environment\n[Back to Table of Contents](#ToC)","64210d45":"<a class=\"anchor\" id=\"import_libraries\"><\/a>\n# Import libraries\n[Back to Table of Contents](#ToC)","be0413e2":"<a class=\"anchor\" id=\"ToC\"><\/a>\n# Table of Contents\n1. [Install libraries](#install_libraries)\n1. [Import libraries](#import_libraries)\n1. [Define useful classes](#define_useful_classes)\n1. [Create ConnectX environment](#create_connectx_environment)\n1. [Configure hyper-parameters](#configure_hyper_parameters)\n1. [Train the agent](#train_the_agent)\n1. [Create an agent](#create_an_agent)\n1. [Evaluate the agent](#evaluate_the_agent)","809da3a1":"<a class=\"anchor\" id=\"create_an_agent\"><\/a>\n# Create an Agent\n[Back to Table of Contents](#ToC)","52f08ec1":"# About Q-Learning algorithm\n> \"Q-learning is a model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\"\n[*wiki*](https:\/\/en.wikipedia.org\/wiki\/Q-learning)","8112f329":"<a class=\"anchor\" id=\"install_libraries\"><\/a>\n# Install libraries\n[Back to Table of Contents](#ToC)","f24de2c3":"<a class=\"anchor\" id=\"train_the_agent\"><\/a>\n# Train the agent\n[Back to Table of Contents](#ToC)","af7d04f9":"<a class=\"anchor\" id=\"evaluate_the_agent\"><\/a>\n# Evaluate the agent\n[Back to Table of Contents](#ToC)","605614c0":"<a class=\"anchor\" id=\"configure_hyper_parameters\"><\/a>\n# Configure hyper-parameters\n[Back to Table of Contents](#ToC)"}}