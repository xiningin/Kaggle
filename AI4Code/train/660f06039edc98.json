{"cell_type":{"c3f6547e":"code","88608771":"code","3959bf1d":"code","4fc51c06":"code","30c9ca7b":"code","ff7d8d59":"code","fce85ba9":"code","ba93f784":"code","ccd5bf4e":"markdown","a59662ac":"markdown","78a23fe1":"markdown","bea6c3d9":"markdown","688b88c7":"markdown","1bbda696":"markdown","2702ca4e":"markdown","0e86487c":"markdown","73aa6a4c":"markdown","d9228efe":"markdown","16921819":"markdown","b182bafb":"markdown","d2e06f7d":"markdown","2b3dc439":"markdown","812b4bb0":"markdown","8eaf52ca":"markdown","84834344":"markdown","b6a8d622":"markdown","6e62a0db":"markdown","b7de40c5":"markdown","e79a1663":"markdown","4b470740":"markdown","419d1d62":"markdown","3eba219f":"markdown","67da0f45":"markdown","2bd77c75":"markdown","6fac60fe":"markdown","5e1a9d09":"markdown","c2624edd":"markdown","40ab6e8d":"markdown"},"source":{"c3f6547e":"# Calculate Hamming Distance:\n\ndef hamming_distance(a,b):\n    return sum(abs(e1-e2) for e1,e2 in zip(a,b))\/len(b)\n\n# define data\nrow1 = [0, 0, 0, 0, 0, 1]\nrow2 = [0, 0, 0, 0, 1, 0]\n# calculate distance\ndist = hamming_distance(row1, row2)\nprint(dist)","88608771":"from scipy.spatial.distance import hamming\n\n# define data\nrow1 = [0, 0, 0, 0, 0, 1]\nrow2 = [0, 0, 0, 0, 1, 0]\n# calculate distance\ndist = hamming(row1, row2)\nprint(dist)","3959bf1d":"# calculating euclidean distance between vectors\nfrom math import sqrt\n \n# calculate euclidean distance\ndef euclidean_distance(a, b):\n\treturn sqrt(sum((e1-e2)**2 for e1, e2 in zip(a,b)))\n \n# define data\nrow1 = [10, 20, 15, 10, 5]\nrow2 = [12, 24, 18, 8, 7]\n# calculate distance\ndist = euclidean_distance(row1, row2)\nprint(dist)","4fc51c06":"from scipy.spatial.distance import euclidean\n\n# define data\nrow1 = [10, 20, 15, 10, 5]\nrow2 = [12, 24, 18, 8, 7]\n# calculate distance\ndist = euclidean(row1, row2)\nprint(dist)","30c9ca7b":"# calculate manhattan distance\ndef manhattan_distance(a, b):\n\treturn sum(abs(e1-e2) for e1, e2 in zip(a,b))\n \n# define data\nrow1 = [10, 20, 15, 10, 5]\nrow2 = [12, 24, 18, 8, 7]\n# calculate distance\ndist = manhattan_distance(row1, row2)\nprint(dist)","ff7d8d59":"# calculating manhattan distance between vectors\nfrom scipy.spatial.distance import cityblock\n# define data\nrow1 = [10, 20, 15, 10, 5]\nrow2 = [12, 24, 18, 8, 7]\n# calculate distance\ndist = cityblock(row1, row2)\nprint(dist)","fce85ba9":"# calculate minkowski distance\ndef minkowski_distance(a, b, p):\n\treturn sum(abs(e1-e2)**p for e1, e2 in zip(a,b))**(1\/p)\n \n# define data\nrow1 = [10, 20, 15, 10, 5]\nrow2 = [12, 24, 18, 8, 7]\n# calculate distance (p=1)\ndist = minkowski_distance(row1, row2, 1)\nprint(dist)\n# calculate distance (p=2)\ndist = minkowski_distance(row1, row2, 2)\nprint(dist)","ba93f784":"from scipy.spatial import minkowski_distance\n# define data\nrow1 = [10, 20, 15, 10, 5]\nrow2 = [12, 24, 18, 8, 7]\n# calculate distance (p=1)\ndist = minkowski_distance(row1, row2, 1)\nprint(dist)\n# calculate distance (p=2)\ndist = minkowski_distance(row1, row2, 2)\nprint(dist)","ccd5bf4e":"### *Distance measures play an important role in machine learning.*\n\n- They provide the foundation for many popular and effective machine learning algorithms like `k-nearest neighbors` for supervised learning and `k-means clustering` for unsupervised learning.\n\n- Different distance measures must be chosen and used depending on the types of the data. As such, it is important to know how to implement and calculate a range of different popular distance measures and the intuitions for the resulting scores.","a59662ac":"As we can see, distance measures play an important role in machine learning. Perhaps four of the most commonly used distance measures in machine learning are as follows:\n\n- Hamming Distance\n- Euclidean Distance\n- Manhattan Distance\n- Minkowski Distance","78a23fe1":"- In the `KNN algorithm`, a classification or regression prediction is made for new examples by calculating the distance between the new example (row) and all examples (rows) in the training dataset. The k examples in the training dataset with the smallest distance are then selected and a prediction is made by averaging the outcome (mode of the class label or mean of the real value for regression).\n\n","bea6c3d9":"## Manhattan Distance (Taxicab or City Block Distance)\n\nThe [Manhattan distance](https:\/\/en.wikipedia.org\/wiki\/Taxicab_geometry), also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n\nIt is perhaps more useful to vectors that describe objects on a uniform grid, like a chessboard or city blocks. The taxicab name for the measure refers to the intuition for what the measure calculates: the shortest path that a taxicab would take between city blocks (coordinates on the grid).\n\nIt might make sense to calculate Manhattan distance instead of Euclidean distance for two vectors in an integer feature space.\n\nManhattan distance is calculated as the sum of the absolute differences between the two vectors.\n\n- ManhattanDistance = sum for i to N sum |v1[i] \u2013 v2[i]|\n\nThe Manhattan distance is related to the L1 vector norm and the sum absolute error and mean absolute error metric.","688b88c7":"- Euclidean distance is calculated as the square root of the sum of the squared differences between the two vectors.\n\nEuclideanDistance = sqrt(sum for i to N (v1[i] \u2013 v2[i])^2)","1bbda696":"We can see that there are two differences between the strings, or 2 out of 6 bit positions different, which averaged (2\/6) is about 1\/3 or 0.333.","2702ca4e":"- Distance measures play an important role in machine learning.\n\n","0e86487c":"When p is set to 1, the calculation is the same as the Manhattan distance. When p is set to 2, it is the same as the Euclidean distance.\n\n- p=1: Manhattan distance.\n- p=2: Euclidean distance.\n\nIntermediate values provide a controlled balance between the two measures.\n\nIt is common to use Minkowski distance when implementing a machine learning algorithm that uses distance measures as it gives control over the type of distance measure used for real-valued vectors via a hyperparameter \u201cp\u201d that can be tuned.","73aa6a4c":"[Hamming Distance](https:\/\/en.wikipedia.org\/wiki\/Hamming_distance) calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n\nYou are most likely going to encounter bitstrings when you `one-hot encode` categorical columns of data.\n\nFor example, if a column had the categories \u2018red,\u2019 \u2018green,\u2019 and \u2018blue,\u2019 you might one hot encode each example as a bitstring with one bit for each column.\n\nin Simplw Words we can say that Hanning Distance ","d9228efe":"- Perhaps the most likely way you will encounter distance measures is when you are using a specific machine learning algorithm that uses distance measures at its core. The most famous algorithm of this type is the `k-nearest neighbors algorithm, or KNN` for short.\n\n","16921819":"### Minkowski Distance Using Scipy","b182bafb":"### Hamming Function using Scipy","d2e06f7d":"## Euclidean Distance:\n\n![](data:image\/png;base64,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)","2b3dc439":"Related is the self-organizing map algorithm, or SOM, that also uses distance measures and can be used for supervised or unsupervised learning. Another unsupervised learning algorithm that uses distance measures at its core is the K-means clustering algorithm.","812b4bb0":"- KNN belongs to a broader field of algorithms called case-based or `instance-based learning`, most of which use distance measures in a similar manner. Another popular instance-based algorithm that uses distance measures is the `learning vector quantization, or LVQ,` algorithm that may also be considered a type of neural network.","8eaf52ca":"## Role of Distance Measures:","84834344":"This calculation is related to the L2 vector norm and is equivalent to the sum squared error and the root sum squared error if the square root is added.","b6a8d622":"red = [1, 0, 0]\n\ngreen = [0, 1, 0]\n\nblue = [0, 0, 1]","6e62a0db":"- A distance measure is an objective score that summarizes the relative difference between two objects in a problem domain.\n\n","b7de40c5":"## Minkowski Distance\n\n[Minkowski distance](https:\/\/en.wikipedia.org\/wiki\/Minkowski_distance) calculates the distance between two real-valued vectors.\n\nIt is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the \u201corder\u201d or \u201cp\u201c, that allows different distance measures to be calculated.\n\nThe Minkowski distance measure is calculated as follows:\n\n- EuclideanDistance = (sum for i to N (abs(v1[i] \u2013 v2[i]))^p)^(1\/p)","e79a1663":"## Euclidean Distances using Scipy","4b470740":"# Distance Measures for Machine Learning","419d1d62":"### Manhattan Distance Using Scipy","3eba219f":"### Resources Used\n\n","67da0f45":"A short list of some of the more popular machine learning algorithms that use distance measures at their core is as follows:\n\n- K-Nearest Neighbors\n- Learning Vector Quantization (LVQ)\n- Self-Organizing Map (SOM)\n- K-Means Clustering","2bd77c75":"[Euclidean distance](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance) calculates the distance between two real-valued vectors.\n\nYou are most likely to use Euclidean distance when calculating the distance between two rows of data that have numerical values, such a floating point or integer values.\n\nIf columns have values with differing scales, it is common to normalize or standardize the numerical values across all columns prior to calculating the Euclidean distance. Otherwise, columns that have large values will dominate the distance measure.","6fac60fe":"- [4 Distance Measures for Machine Learning](https:\/\/machinelearningmastery.com\/distance-measures-for-machine-learning\/)\n\n- [9 Distance Measures in Data Science](https:\/\/towardsdatascience.com\/9-distance-measures-in-data-science-918109d069fa)\n\n- [4 Types of Distance Metrics in Machine Learning](https:\/\/www.analyticsvidhya.com\/blog\/2020\/02\/4-types-of-distance-metrics-in-machine-learning\/)","5e1a9d09":"When calculating the distance between two examples or rows of data, it is possible that different data types are used for different columns of the examples. An example might have real values, boolean values, categorical values, and ordinal values. Different distance measures may be required for each that are summed together into a single distance score.\n\nNumerical values may have different scales. This can greatly impact the calculation of distance measure and it is often a good practice to normalize or standardize numerical values prior to calculating the distance measure.\n\nNumerical error in regression problems may also be considered a distance. For example, the error between the expected value and the predicted value is a one-dimensional distance measure that can be summed or averaged over all examples in a test set to give a total distance between the expected and predicted outcomes in the dataset. The calculation of the error, such as the mean squared error or mean absolute error, may resemble a standard distance measure.","c2624edd":"The distance between red and green could be calculated as the sum or the average number of bit differences between the two bitstrings. This is the Hamming distance.\n\n#### *For a one-hot encoded string, it might make more sense to summarize to the sum of the bit differences between the strings, which will always be a 0 or 1.*\n\n- HammingDistance = sum for i to N abs(v1[i] \u2013 v2[i])\n\n#### *For bitstrings that may have many 1 bits, it is more common to calculate the average number of bit differences to give a hamming distance score between 0 (identical) and 1 (all different).*\n\n- HammingDistance = (sum for i to N abs(v1[i] \u2013 v2[i])) \/ N\n","40ab6e8d":"## Hamming Distance "}}