{"cell_type":{"bc0c0c6f":"code","2cb7ab63":"code","c3406066":"code","2bad1099":"code","bb12a615":"code","a6338ef2":"code","01ca695c":"code","fe28a259":"code","261a61dd":"markdown","daecfcfd":"markdown","4e1dbd5d":"markdown","73136db7":"markdown","ca8324e4":"markdown","4d60190f":"markdown"},"source":{"bc0c0c6f":"import numpy as np\nimport pandas as pd \n\nfrom collections import Counter","2cb7ab63":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","c3406066":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","2bad1099":"train.head(3)","bb12a615":"print(\"train shape = \",train.shape)\nprint(\"test shape = \",test.shape)\ntrain.info()","a6338ef2":"# 1.detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n\n# 2.get train row size \ntrain_len = len(train)\n\n# 3.concat train + test \ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n\n# 4. empty data => np.nan\ndataset = dataset.fillna(np.nan)\n\n# 5.column encoding and create new features\n# 5.1.Fare\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\n# 5.2.Embarked\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")\n\n# 5.3.Sex\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})\n\n# 5.4.Age\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med\n        \n        \n# 5.5.Name         \ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)\ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\n\n# 5.6.SibSp(Number of Siblings\/Spouses Aboard) + Parch(Number of Parents\/Children Aboard) =>Fsize\ndataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\ndataset['IsSingle'] = dataset['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ndataset['IsSmallFamily'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['IsMedFamily'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['IsLargeFamily'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\n#5.7.Cabin\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")\n\n#5.8.Ticket \nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")\n\n# 5.9.Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\n\n# 5.10.PassengerId\ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)\n\n# 6.devide train test \ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","01ca695c":"train.head(3)","fe28a259":"print(\"train shape = \",train.shape)\nprint(\"test shape = \",test.shape)\ntrain.info()","261a61dd":"# check data size before preprocessing","daecfcfd":"# import libraries","4e1dbd5d":"# common functions","73136db7":"# check data size after preprocessing","ca8324e4":"# quick preprocessing","4d60190f":"# load data "}}