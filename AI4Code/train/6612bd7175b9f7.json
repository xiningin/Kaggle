{"cell_type":{"c7bb69fc":"code","1bed65c6":"code","57459918":"code","0aff6ace":"code","af34e8be":"code","31f3dc7a":"code","347552a2":"code","47d115b2":"code","7e04432c":"code","5438f511":"code","e6029924":"code","d31b1ec4":"code","6d69333d":"code","6847443e":"code","cfef5d31":"code","43733ad3":"code","2f0fc322":"code","65e8f102":"code","3f7b9060":"code","bbe281be":"code","31f5f190":"code","51480474":"code","ad593aba":"code","496d8ef6":"code","3c7318bd":"code","7705e84f":"code","040a7344":"code","ff13641a":"code","e274b051":"code","47484055":"code","98387f1c":"code","96fb8235":"code","c1c310df":"code","3619ae13":"code","c9fab37e":"markdown","59c10812":"markdown","807b4c7c":"markdown","8b431daf":"markdown","567d6515":"markdown","6905abbe":"markdown","17e844f6":"markdown","2f6b0d6d":"markdown","4f44f7b5":"markdown","5f6c6e08":"markdown","3465f628":"markdown","6a354b3b":"markdown","c8f7c6ac":"markdown","38b6666f":"markdown","839a67c0":"markdown","037c1100":"markdown","02dcb659":"markdown","e0aa9d9e":"markdown","0e4e853d":"markdown","b75cb8e7":"markdown","fb287426":"markdown","5c921769":"markdown","20d24bb2":"markdown","bc4cceba":"markdown","baa43cd1":"markdown"},"source":{"c7bb69fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\ndata=pd.read_csv(\"..\/input\/dataset.csv\")\n\ndata=data.iloc[:,1:]\n# Any results you write to the current directory are saved as output.","1bed65c6":"data=data.sample(frac=1)","57459918":"data.head()","0aff6ace":"train=data","af34e8be":"train['clean_reviews']=train['reviews'].str.replace(\"[^a-zA-Z#]\",\" \")\n","31f3dc7a":"train['clean_reviews'].head()","347552a2":"train['clean_reviews']=train['clean_reviews'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","47d115b2":"train['clean_reviews'].head()","7e04432c":"train['clean_reviews']=train['clean_reviews'].apply(lambda x: x.split())","5438f511":"train['clean_reviews'].head()","e6029924":"from nltk.stem.porter import *\nstemmer=PorterStemmer()\ntrain['clean_reviews']=train['clean_reviews'].apply(lambda x: [stemmer.stem(i) for i in x])","d31b1ec4":"train['clean_reviews'].head()","6d69333d":"train['clean_reviews']=train['clean_reviews'].apply(lambda x:' '.join([w for w in x]))\n#train['clean_reviews']=join[x for x in train['clean_reviews']]","6847443e":"train['clean_reviews'].head(10)","cfef5d31":"from wordcloud import WordCloud","43733ad3":"good_words=' '.join([text for text in train['clean_reviews'][train['label']==1]])\nwc=WordCloud(height=500, width=500, random_state=21, max_font_size=110).generate(good_words)\nplt.figure(figsize=(7,7))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","2f0fc322":"bad_words=' '.join([text for text in train['clean_reviews'][train['label']==0]])\nwc=WordCloud(height=500, width=500, random_state=21, max_font_size=110).generate(bad_words)\nplt.figure(figsize=(7,7))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","65e8f102":"from sklearn.feature_extraction.text import HashingVectorizer\nhv=HashingVectorizer(n_features=2**10)\nhvr=hv.fit_transform(train['clean_reviews'])\nfea_hvr=pd.DataFrame(hvr.toarray())","3f7b9060":"data_hvr=pd.concat([pd.DataFrame(fea_hvr),train['label']], axis=1)","bbe281be":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(min_df=2, max_df=0.90, max_features=1000,stop_words='english')# selects words that have min frequency above 2 and max frequnecy below 0.9*size of vocalubary.\n#among these words, it chooses 1000 words with highest frequency\nbow=cv.fit_transform(train['clean_reviews'])\n#print(bow)","31f5f190":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = ['This is the first document.','This document is the second document.',\n     'And this is the third one.',\n     'Is this the first document?']\nvec=TfidfVectorizer()\nX=vec.fit_transform(corpus)\nprint(vec.get_feature_names())\nprint(X.shape)\nprint(X)\n","51480474":"\ntv=TfidfVectorizer(min_df=2, max_df=0.90,  max_features=1000, stop_words='english')\ntfidf=tv.fit_transform(train['clean_reviews'])\nvocab=tv.get_feature_names()\nprint(len(vocab))# length of vocabulary\n#print(tfidf)\n","ad593aba":"fea_bow=pd.DataFrame(bow.toarray())\nfea_tf=pd.DataFrame(tfidf.toarray())","496d8ef6":"from sklearn.decomposition import PCA, TruncatedSVD\n\nx_tsvd_bow=TruncatedSVD(n_components=100,algorithm='randomized',random_state=42).fit_transform(fea_bow.values)\nx_tsvd_tf=TruncatedSVD(n_components=100,algorithm='randomized',random_state=42).fit_transform(fea_tf.values)","3c7318bd":"data_bow=pd.concat([pd.DataFrame(x_tsvd_bow),train['label']], axis=1)\ndata_tf=pd.concat([pd.DataFrame(x_tsvd_tf),train['label']],axis=1)\ndata_bow.shape","7705e84f":"corr_bow=data_bow.corr()\ncorr_tf=data_tf.corr()","040a7344":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,24))\nsns.heatmap(corr_bow, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"BOW model\", fontsize=14)\n\nsns.heatmap(corr_tf, cmap='coolwarm_r', annot_kws={'size':23})\nax2.set_title('Tfidf Model', fontsize=14)\nplt.show()","ff13641a":"\npos_corr=corr_bow.index[corr_bow['label'] >0.010].tolist()#these eatures have strong +ve correlation\nneg_corr=corr_tf.index[corr_tf['label'] <-0.011].tolist()#these eatures have strong -ve correlation\nprint(pos_corr,neg_corr)","e274b051":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","47484055":"tr_bow=bow\n\nx_tr_bow, x_val_bow, y_tr_bow, y_val_bow=train_test_split(tr_bow, train['label'],random_state=42,test_size=0.3)\nmodel_bow=LogisticRegression()\ny_tr_bow=pd.DataFrame(y_tr_bow)\nmodel_bow.fit(x_tr_bow,y_tr_bow)\nans=model_bow.predict(x_val_bow)\nf1_bow=f1_score(ans, y_val_bow)\nprint(f1_bow)","98387f1c":"train_tfidf = tfidf\n\nx_tr_tf,x_val_tf,y_tr_tf,y_val_tf=train_test_split(train_tfidf,train['label'], test_size=0.3)\nmodel_tf=LogisticRegression()\nmodel_tf.fit(x_tr_tf,y_tr_tf)\ny_pre_tf=model_tf.predict(x_val_tf)\nf1_tf=f1_score(y_pre_tf,y_val_tf)\nf1_tf\n","96fb8235":"x_tr_tf_tsvd,x_val_tf_tsvd,y_tr_tf_tsvd,y_val_tf_tsvd=train_test_split(x_tsvd_tf,train['label'], test_size=0.3)\nmodel_tsvd_tf=LogisticRegression()\nmodel_tsvd_tf.fit(x_tr_tf_tsvd,y_tr_tf_tsvd)\ny_pre_tf_tsvd=model_tsvd_tf.predict(x_val_tf_tsvd)\nf1_tf_tsvd=f1_score(y_pre_tf_tsvd,y_val_tf_tsvd)\nf1_tf_tsvd","c1c310df":"train_tfidf = tfidf\n\nx_tr_tf,x_val_tf,y_tr_tf,y_val_tf=train_test_split(train_tfidf,train['label'], test_size=0.3)\nfrom sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(x_tr_tf,y_tr_tf)\ny_pre_tf=clf.predict(x_val_tf)\nf1_tf=f1_score(y_pre_tf,y_val_tf)\nf1_tf","3619ae13":"x_tr_hvr,x_val_hvr,y_tr_hvr,y_val_hvr=train_test_split(data_hvr, train['label'],test_size=0.3 )\nmodel_hvr=LogisticRegression()\nmodel_hvr.fit(x_tr_hvr,y_tr_hvr)\ny_pr_hvr=model_hvr.predict(x_val_hvr)\nf1_hvr=f1_score(y_pr_hvr,y_val_hvr)\nf1_hvr","c9fab37e":"## Model Fitting","59c10812":"**Reshuffling Data**","807b4c7c":"**Plotting correlatiton matrices**","8b431daf":"**Tokenizing**","567d6515":"**Creating correlation matrix**","6905abbe":"# This is a simple kernel for text classification on Stanford Movie Dataset. It uses tf-idf, CountVectorizer and Hashing Vectorizer. Algorithms used for fitting are Naive Bayes and Logistic Regression.\n\n**Do not forget to upvote if you like it ;) Cheers!!**","17e844f6":"## Preprocessing","2f6b0d6d":"**Replace everything except letters and hashtags with blank space**","4f44f7b5":"**Fitting BOW model**","5f6c6e08":"**Trying Naive Bayes**","3465f628":"**Removing words of size<4 as they do not contribute much**","6a354b3b":"**bow and tfidf are sparse matrices (Sparse matrix: matrix which has many zeros.) We need to convert them to Dataframe to work with them**","c8f7c6ac":"**Tfidf Model**","38b6666f":"## Visualizations","839a67c0":"**Reducing Dimensions using Truncated SVD (Trucated SVD works well with sparse matrices).**","037c1100":"**Correlation Matrices don't help much**","02dcb659":"**Bag of Words Model(BOW)**","e0aa9d9e":"**Fitting tfidf model with reduced dimensions**","0e4e853d":"**bow is a feature matrix of shape 25000x1000(# of training examples,ie documents X # of features, ie. vocabulary**","b75cb8e7":"**Fitting tfidf model**","fb287426":"## Generating Feature Matrix: BOW and Tfidf Models","5c921769":"**Tfidf Model: For understanding**","20d24bb2":"## Preprocessing done","bc4cceba":"**Stemming: Stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word**","baa43cd1":"**tfidf is a feature matrix of shape 25000x1000(# of trainign examples,ie documents X # of features, ie. vocabulary)**"}}