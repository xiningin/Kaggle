{"cell_type":{"9279619d":"code","d78ac7bd":"code","c4247ef2":"code","8af24fdf":"markdown"},"source":{"9279619d":"import numpy as np\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.filterwarnings('ignore')","d78ac7bd":"class Utils:\n    @staticmethod\n    def sigmoid(Z): \n        A = 1\/(1+np.exp(-Z))\n        cache = Z\n        return A, cache\n    \n    @staticmethod\n    def relu(Z):\n        A = np.maximum(0,Z)\n        cache = Z \n        return A, cache\n    \n    @staticmethod\n    def relu_backward(dA, cache):\n        Z = cache\n        dZ = np.array(dA, copy=True)\n        dZ[Z <= 0] = 0\n        return dZ\n    \n    @staticmethod\n    def sigmoid_backward(dA, cache):\n        Z = cache\n        s = 1\/(1+np.exp(-Z))\n        dZ = dA * s * (1-s) \n        return dZ","c4247ef2":"class DNN(Utils):\n    \n    '''\n    input:\n        X: input features\n        y: target\n        L: (array-like) number of hidden layers with units \n        lr: learning rate\n        \n        example:\n        import numpy as np\n        features = np.random.randn(100,8)\n        targets = np.random.randn(100,1)\n        L = [16,32,32,64]\n        lr = 1e-3\n        model = DNN(X = features,y=targets,layers=L,learning_rate=lr)\n    '''\n    \n    def __init__(self,X,y,layers,learning_rate):\n        super().__init__()\n        self.x = X\n        self.y = y\n        self.L = layers\n        self.lr = learning_rate\n    \n    def initialize_parameters(self):\n        layer = len(self.L)\n        n = self.x.shape[1]\n        parameters = {}\n        for l in range(1,layer):\n            parameters[\"W\"+str(l)] = np.random.randn(self.L[l],self.L[l-1]) * 1\/np.sqrt(n ** (layer-1))\n            parameters[\"b\"+str(l)] = np.zeros((self.L[l],1))\n        \n        return parameters\n    \n    def linear_forward(self,A,W,b):\n        Z = np.dot(W,A) + b\n        cache = (A,W,b)\n        return Z,cache\n    \n    def linear_activation_forward(self,A_prev,W,b,activation):\n        if activation == \"sigmoid\":\n            Z, linear_cache = self.linear_forward(A_prev, W, b)\n            A,activation_cache = Utils.sigmoid(Z)\n        elif activation == \"relu\":\n            Z, linear_cache = self.linear_forward(A_prev, W, b)\n            A,activation_cache = Utils.relu(Z)\n        \n        cache = (linear_cache,activation_cache)\n        return A,cache\n    \n    def forward(self,parameters):\n        caches = []\n        A = self.x\n        L = len(parameters)\/\/2\n        \n        for l in range(1,L):\n            A_prev = A\n            A,cache = self.linear_activation_forward(A_prev,\n                                               parameters[\"W\"+str(l)],\n                                               parameters[\"b\"+str(l)],\n                                               activation = \"relu\")\n            caches.append(cache)\n        \n        AL,cache = self.linear_activation_forward(A,\n                                             parameters[\"W\"+str(L)],\n                                            parameters[\"b\"+str(L)],\n                                            activation = \"sigmoid\")\n        caches.append(cache)\n        \n        return AL,caches\n    \n    def compute_cost(self,AL):\n        Y = self.y\n        m = Y.shape[1]\n        cost = (-1 \/ m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n        cost = np.squeeze(cost)\n        return cost\n    \n    def linear_backward(self,dZ,cache):\n        A_prev, W, b = cache\n        m = A_prev.shape[1]\n        dW = 1\/m*np.dot(dZ ,A_prev.T)\n        db = 1\/m*np.sum(dZ , axis = 1, keepdims=True)\n        dA_prev = np.dot(W.T ,dZ)\n        return dA_prev, dW, db\n    \n    def linear_activation_backward(dA, cache, activation):\n        linear_cache, activation_cache = cache\n        if activation == \"relu\":\n            dZ = Utils.relu_backward(dA, activation_cache)\n            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n        elif activation == \"sigmoid\":\n            dZ = Utils.sigmoid_backward(dA, activation_cache)\n            dA_prev, dW, db = self.linear_backward(dZ, linear_cache)\n        return dA_prev, dW, db\n    \n    def backward(self,AL,caches):\n        Y = self.y\n        grads = {}\n        m = AL.shape[1]\n        L = len(caches)\n        Y = Y.reshape(AL.shape)\n        \n        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n        current_cache = caches[-1]\n        grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = self.linear_backward(Utils.sigmoid_backward(dAL, \n                                                                                                        current_cache[1]), \n                                                                                                        current_cache[0])\n        for l in reversed(range(L-1)):\n            current_cache = caches[l]\n            dA_prev_temp, dW_temp, db_temp = self.linear_backward(Utils.sigmoid_backward(dAL,\n                                                                              current_cache[1]),\n                                                             current_cache[0])\n            grads[\"dA\" + str(l + 1)] = dA_prev_temp\n            grads[\"dW\" + str(l + 1)] = dW_temp\n            grads[\"db\" + str(l + 1)] = db_temp\n            \n        return grads\n    \n    def update_parameters(self,parameters,grads):\n        learning_rate = self.lr\n        L = len(parameters) \/\/ 2\n        for l in range(L):\n            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        return parameters\n    \n    def training(self,iterations=100,print_cost=False,print_interval=10):\n        parameters = self.initialize_parameters()\n        costs = []\n        for i in tqdm_notebook(range(0,iterations)):\n            AL,caches = self.forward(parameters)\n            cost = self.compute_cost(AL)\n            grads = self.backward(AL,caches)\n            parameters = self.update_parameters(parameters,grads)\n            if print_cost & i % print_interval:\n                print(f'cost after interval {i}: {cost}')\n                costs.append(cost)\n        return costs","8af24fdf":"## Deep neural Networks with L layers.\n\n### Please **upvote** this kernal it you like it\n\n#### I have implemented a 2 layer neural network with mathemtical explanation please look that kernel if you are facing any problem to understand this algorithm.\n\nhttps:\/\/www.kaggle.com\/adarshpathak\/simple-back-propagation"}}