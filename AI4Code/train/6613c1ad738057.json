{"cell_type":{"23bdd7e9":"code","dfd6b3d0":"code","fedddf93":"code","20ad8a1f":"code","b17abb57":"code","8bbc8979":"code","c847f3a5":"code","4364658e":"code","411e78da":"markdown","4bef140f":"markdown","7769c365":"markdown"},"source":{"23bdd7e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dfd6b3d0":"train = pd.read_csv( '..\/input\/train.csv' )\ntest = pd.read_csv( '..\/input\/test.csv' )\n\ntrain = train.drop( ['SalePrice'], axis = 1 )\nprint(train.shape, test.shape)","fedddf93":"# Quick Missing Value Imputation\nfor i in train.columns:\n    if train[i].dtype == 'object':\n        train[i] = train[i].fillna('None')\n    else:\n        train[i] = train[i].fillna(-1)\n        \nfor i in test.columns:\n    if test[i].dtype == 'object':\n        test[i] = test[i].fillna('None')\n    else:\n        test[i] = test[i].fillna(-1)","20ad8a1f":"# One Hot Encoding\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\nprint(train.shape)\nprint(test.shape)","b17abb57":"#Which columns are in train but not in test?\nprint([value for value in train.columns if value not in test.columns])","8bbc8979":"#Let's take the intersection of these columns\ncols = [value for value in test.columns if value in train.columns]\ntrain = train[cols]\ntest = test[cols]\nprint(train.shape)\nprint(test.shape)","c847f3a5":"#Adversarial Validation Steps:\ntrain['TARGET'] = 1\ntest['TARGET'] = 0\n\ndata = pd.concat(( train, test ))\n\nx = data.drop( [ 'TARGET', 'Id' ], axis = 1 )\ny = data.TARGET\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( x, y, train_size = 0.8 )\n\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","4364658e":"xgb_params = {\n        'learning_rate': 0.05, 'max_depth': 5,'subsample': 0.9,\n        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n        'n_estimators':500, 'gamma':1,\n        'min_child_weight':1\n        }\nclf = xgb.XGBClassifier(**xgb_params, seed = 10)\n\nclf.fit(x_train, y_train, eval_set=[(x_test, y_test)],\n       eval_metric='auc', verbose=True,early_stopping_rounds=20)\n\nprval = clf.predict_proba(x_test)[:,1]\nprint(roc_auc_score(y_test,prval))","411e78da":"**This notebook checks whether the test dataset differs significantly from the train dataset and what to do if it does to avoid getting a completely different evaluation score between the validation and test datasets.**","4bef140f":"Now, let's train the model using XGBoost (what else?!)\nNext, we will print the AUC to see the degree of separation between train and test. If AUC is close to 0.5, then that means there is no separation and we're good. If AUC is higher ~0.8-0.9, then there is a high degree of separation and the train and test datasets might differ significantly. In such a case, adversarial validation is useful in selecting a validation dataset which is closest to the test data.","7769c365":"So the model cannot distinguish between the train and test dataset. That means we are good to split the dataset between train and validation randomly.\n\nIf the model could distinguish between both datasets that easily, then we could select a validation sample using adversarial validation to make it most representative of the test dataset. The steps for selecting the validation datset would be:\n\n1. Score the original train dataset using the XGBoost model\n2. Sort the predictions by descending order (highest probability first)\n3. For a 80-20 split, choose the first 20% as validation and the rest as train"}}