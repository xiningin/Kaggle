{"cell_type":{"5de76d36":"code","967c4dba":"code","95b1b11b":"code","318ff688":"code","298bd845":"code","1d2d5fe0":"code","15ffc66c":"code","4d6589b2":"code","2d0982d1":"code","7750a82b":"code","ce34820a":"code","390f9b97":"code","fc19edbd":"code","2234461f":"code","3089932f":"code","582120e6":"code","c5c82ad6":"code","e22c49cc":"code","76c0dc9f":"code","3bb267aa":"code","ddf12645":"code","87a83d07":"code","ca87019e":"code","b9c57f37":"code","0f945239":"code","73c14dc9":"code","4a643c21":"code","ef2b41ed":"code","481d14a4":"code","ce1a5144":"code","58d1ec88":"code","17dd832e":"code","494a4cbf":"code","ec6e9ea4":"code","819a00e3":"code","58f32161":"code","fd9313c6":"code","1d37aee0":"code","eaaeda22":"code","c837bf4e":"code","6c8a139f":"code","2fdcdaa0":"code","b67d5654":"code","517abb9b":"code","52249fc2":"code","7b33536c":"code","43ab6db8":"code","d702ac70":"code","3bb27523":"code","f9f7dd72":"code","df2204f3":"markdown","c337c16b":"markdown","2d22c969":"markdown","94340a14":"markdown","aa8f96d9":"markdown","de9d0c35":"markdown","2d330580":"markdown","4ad0db92":"markdown","317f2dee":"markdown","7a360954":"markdown","7324e382":"markdown","84953805":"markdown","d77e960d":"markdown","fc5c0162":"markdown","dcb04080":"markdown","dbfdff50":"markdown","37e7f854":"markdown","f25f5f3a":"markdown","67efb851":"markdown","5787abdf":"markdown","8bd561b0":"markdown","fc745705":"markdown","6cfef3bd":"markdown","b8e03bd6":"markdown","d27c1220":"markdown","62c36b31":"markdown","5ac67c1a":"markdown","d3166c9b":"markdown","af4785cf":"markdown","bb07bdec":"markdown","cfd70482":"markdown"},"source":{"5de76d36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","967c4dba":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n","95b1b11b":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n","318ff688":"train_df.head()","298bd845":"train_df.keyword.value_counts()","1d2d5fe0":"train_df.location.value_counts()","15ffc66c":"train_df.target.value_counts()","4d6589b2":"train_df['text'].head()[0]","2d0982d1":"\nsample_text = ['Hello world', '. ~ #### world', 'this is #sample text']\ncount_vectorizer = feature_extraction.text.CountVectorizer()\ncount_vectorizer.fit_transform(sample_text).todense()","7750a82b":"count_vectorizer.vocabulary_","ce34820a":"tfidvectorizer = feature_extraction.text.TfidfVectorizer()\n\ntfidvectorizer.fit_transform(sample_text).todense()","390f9b97":"tfidvectorizer.vocabulary_ # same as count_vectorizer.","fc19edbd":"hashvectorizer = feature_extraction.text.HashingVectorizer()\n\nhashvectorizer.fit_transform(sample_text).todense()","2234461f":"hashvectorizer.get_stop_words","3089932f":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer()\ncount_vectorizer = feature_extraction.text.CountVectorizer()\nhash_vectorizer = feature_extraction.text.HashingVectorizer()","582120e6":"\ndef model_score(model, features, target):        \n    return model_selection.cross_val_score(model, features, target, cv=5, scoring=\"f1\")\n\ndef clean(data):    \n    mean = round(data.mean(), 2)\n    std = round(data.std(), 2)\n    \n    return f'mean: {mean} +\/- {std}' ","c5c82ad6":"tfidf_features = tfidf_vectorizer.fit_transform(train_df[\"text\"]).todense()\ncount_features = count_vectorizer.fit_transform(train_df[\"text\"]).todense()","e22c49cc":"hash_score = model_score(linear_model.RidgeClassifier(), tfidf_features, train_df[\"target\"])\nclean(hash_score)","76c0dc9f":"from sklearn.linear_model import LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, Perceptron\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB","3bb267aa":"import re\ndef only_alpha(x):\n    return re.sub(r'[\\W_]', ' ', x).lower()\n\ntrain_df['text_alpha_num'] = train_df['text'].apply(func = only_alpha)","ddf12645":"clean(model_score(LogisticRegression(solver = 'lbfgs'),tfidf_vectorizer.fit_transform(train_df['text_alpha_num']).todense() , train_df[\"target\"]))","87a83d07":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","ca87019e":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer(stop_words = stop_words)\nclean(model_score(LogisticRegression(solver = 'lbfgs'),tfidf_vectorizer.fit_transform(train_df['text_alpha_num']).todense() , train_df[\"target\"]))","b9c57f37":"from nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n\ndef stemmer(x):\n    return ' '.join([porter.stem(x) for x in x.split(' ')])\n\ntrain_df['stemmed_words'] = train_df['text_alpha_num'].apply(func = stemmer)","0f945239":"train_df['stemmed_words'][0]","73c14dc9":"clean(model_score(LogisticRegression(solver = 'lbfgs'),tfidf_vectorizer.fit_transform(train_df['stemmed_words']).todense() , train_df[\"target\"]))","4a643c21":"stemmed = tfidf_vectorizer.fit_transform(train_df[\"stemmed_words\"]).todense()\nstemmed_data = pd.DataFrame(stemmed, columns=tfidf_vectorizer.get_feature_names())\n\nalpha_num = tfidf_vectorizer.fit_transform(train_df[\"text_alpha_num\"]).todense()\nalpha_data = pd.DataFrame(alpha_num, columns=tfidf_vectorizer.get_feature_names())\n\ndata = pd.concat([stemmed_data,alpha_data], axis =1)\n\nclean(model_score(LogisticRegression(solver = 'lbfgs'), data , train_df[\"target\"]))","ef2b41ed":"train_df['keyword'].value_counts()","481d14a4":"train_df['keyword_test'] = train_df['keyword'].fillna('no keyword')","ce1a5144":"clean(model_score(LogisticRegression(solver = 'lbfgs'),tfidf_vectorizer.fit_transform(train_df['keyword_test']).todense() , train_df[\"target\"]))","58d1ec88":"clean(model_score(GaussianNB(),tfidf_vectorizer.fit_transform(train_df['keyword_test']).todense() , train_df[\"target\"]))","17dd832e":"def get_hashtag(x):\n    return ' '.join(list(re.findall(r\"#(\\w+)\", x)))\n\ntrain_df['tags'] = train_df['text'].apply(func = get_hashtag)","494a4cbf":"train_df['tags'].value_counts()","ec6e9ea4":"train_df.head()","819a00e3":"\ndef insert_tags(x):\n    if x == '':\n        return 'no tags'\n    else:\n        return x\n\ntrain_df['tags'] = train_df['tags'].apply(func = insert_tags)","58f32161":"train_df['tags'].value_counts()","fd9313c6":"clean(model_score(GaussianNB(),tfidf_vectorizer.fit_transform(train_df['tags']).todense() , train_df[\"target\"]))","1d37aee0":"train_df['location'].fillna('no location').value_counts()","eaaeda22":"train_df['location'] = train_df['location'].fillna('no location')","c837bf4e":"clean(model_score(GaussianNB(),tfidf_vectorizer.fit_transform(train_df['location']).todense() , train_df[\"target\"]))","6c8a139f":"stemmed = tfidf_vectorizer.fit_transform(train_df[\"stemmed_words\"]).todense()\nstemmed_data = pd.DataFrame(stemmed, columns=tfidf_vectorizer.get_feature_names())\n\nalpha_num = tfidf_vectorizer.fit_transform(train_df[\"text_alpha_num\"]).todense()\nalpha_data = pd.DataFrame(alpha_num, columns=tfidf_vectorizer.get_feature_names())\n\ndata = pd.concat([stemmed_data,alpha_data], axis =1)\n\nclean(model_score(LogisticRegression(solver = 'lbfgs'), data , train_df[\"target\"]))","2fdcdaa0":"alpha_num = tfidf_vectorizer.fit_transform(train_df[\"text_alpha_num\"]).todense()\nalpha_data = pd.DataFrame(alpha_num, columns=tfidf_vectorizer.get_feature_names())\n\ntags_tfidf = tfidf_vectorizer.fit_transform(train_df[\"tags\"]).todense()\ntags_data = pd.DataFrame(tags_tfidf, columns=tfidf_vectorizer.get_feature_names())\n\nkeys_tfidf = tfidf_vectorizer.fit_transform(train_df[\"keyword_test\"]).todense()\nkeys_data = pd.DataFrame(keys_tfidf, columns=tfidf_vectorizer.get_feature_names())\n\n\nlocation_tfidf = tfidf_vectorizer.fit_transform(train_df[\"location\"]).todense()\nlocation_data = pd.DataFrame(location_tfidf, columns=tfidf_vectorizer.get_feature_names())\n\ndata = pd.concat([alpha_data,tags_data,keys_data,location_data], axis =1)","b67d5654":"clean(model_score(GaussianNB(),data , train_df[\"target\"]))","517abb9b":"train_df['tags'][0] + ' ' + train_df['location'][0]","52249fc2":"data['combined'] = train_df['tags'] + ' ' + train_df['location'] + ' ' + train_df[\"keyword_test\"] + ' ' + train_df[\"text_alpha_num\"]","7b33536c":"clean(model_score(GaussianNB(), tfidf_vectorizer.fit_transform(data['combined']).todense() , train_df[\"target\"]))","43ab6db8":"tfidf_vectorizer = feature_extraction.text.TfidfVectorizer()\nalpha_num = tfidf_vectorizer.fit_transform(train_df[\"text_alpha_num\"]).todense()\nalpha_data = pd.DataFrame(alpha_num, columns=tfidf_vectorizer.get_feature_names())\n\n","d702ac70":"clf = LogisticRegression(n_jobs = -1, random_state = 1337)\nclf.fit(alpha_data , train_df[\"target\"])","3bb27523":"\ntest_df['text_alpha_num'] = test_df['text'].apply(func = only_alpha)\n\nalpha_num_test = tfidf_vectorizer.transform(test_df[\"text_alpha_num\"]).todense()\nalpha_data_test = pd.DataFrame(alpha_num_test, columns=tfidf_vectorizer.get_feature_names())","f9f7dd72":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nsample_submission[\"target\"] = clf.predict(alpha_data_test)\n\nsample_submission.head()\n\nsample_submission.to_csv('submission.csv', index=False)\n\n\n","df2204f3":"LogisticRegression: 'mean: 0.46 +\/- 0.08'\n\nInteresting, when we combine features, the model gets very poor.\n\nMaybe, if I add the keywrds and tags etc to the text it will be better?","c337c16b":"LogisticRegression = 'mean: 0.29 +\/- 0.02'","2d22c969":"Let's fill in the tags column","94340a14":"### Let's have a look at the keyword column","aa8f96d9":"Looking at the columns we have the obvious one which is text, I am not sure what keyword or location would be. Let's explore","de9d0c35":"What about extracting hashtags from the twwets?","2d330580":"No change whatsoever! That is good in one respect as we have the same result with fewer 'words'.\n\nHow about using stop words? This works by removing the really common words, such as 'the', 'a' 'etc'. However, seeing as we are using tfidf I am not sure it will help.","4ad0db92":"So, using the best, I have come up with so far, I shall try and optimise the model","317f2dee":"So location, vague to precise, and I am still not quite sure what keyword is. \n\nI wonder how the target is spread, knowing kaggle and the learner challenges it will be nice and evenly spread.","7a360954":"max_iter = [10,100,1000,10000]\n\n    for i in max_iter:\n        print(i)\n        print(clean(model_score(LogisticRegression(solver = 'lbfgs' ,max_iter = i, n_jobs = -1), alpha_data , train_df[\"target\"])))\n\n    10\n    mean: 0.64 +\/- 0.05\n    100\n    mean: 0.64 +\/- 0.05\n    1000\n    mean: 0.64 +\/- 0.05","7324e382":"#### Hash vectorizer\n\nFrom what I have read this implementation is designed to be memory efficient, and you can select how many features you want to pick-up.","84953805":"Not quite a perfect distribution, although I have no idea at how imbalanced a data set has to be before the model is impaired. I suppose it would be nice to no at which location the most misleading tweets are from.\n\nLet's have a look at the text","d77e960d":"Perhaps removing non alphanumeric will help? For good measure I threw in a 'lower()' as I read that it is common to do with NLP","fc5c0162":"Looking at the output from my test example it is less obvious to me how this is working","dcb04080":"## Modelling","dbfdff50":"If my understanding is correct, count_vectorizer looks finds all the words in the entire list (in this case) and that is the length of each vector, each unique word in input is assigned a position. In the case above 'world' is the 5th entry of each vector. If 'world' is in the input text the 5th entry for its vector representation is set to 1, else 0. ","37e7f854":"with \n\nfeatures = tfidvectorizer.fit_transform(train_df[\"text\"]).todense()\n\n    - clean(model_score(LogisticRegression(solver = 'lbfgs'), features, train_df[\"target\"])) = 'mean: 0.64 +\/- 0.05'\n    - clean(model_score(SGDClassifier(), features, train_df[\"target\"])) = 'mean: 0.63 +\/- 0.05'\n    - clean(model_score(PassiveAggressiveClassifier(), features, train_df[\"target\"])) = 'mean: 0.59 +\/- 0.07'\n    - clean(model_score(Perceptron(), features, train_df[\"target\"])) = 'mean: 0.58 +\/- 0.05'\n    - clean(model_score(ExtraTreeClassifier(), features, train_df[\"target\"])) = 'mean: 0.53 +\/- 0.06'\n    - clean(model_score(GaussianNB(), features, train_df[\"target\"])) = 'mean: 0.59 +\/- 0.02'\n    \nAdd the different methods of vectorizing as additinal features?","f25f5f3a":"Slightly worse. What about using both new features?","67efb851":"Interesting that GuassianNB does so well with this... LogisiticRegression scores 0.17, which is what I would have expected.\n\nlet's sort out location","5787abdf":"LogisticRegression = 'mean: 0.52 +\/- 0.08'\n","8bd561b0":"Looks like we could clean this up a bit. removing the '%20' which I believe represents a space.","fc745705":"This seems like a very simple way of representing a document or documents of text and is easily understandale, but what of the other offering of sci-kit learn.\n\n#### Term frequenct Inverse document frequency TF-IDF\n\nfrom sklearn - Equivalent to CountVectorizer followed by TfidfTransformer.\n\nTerm frequency - how often a term appears\nInverse document frequency - scales words based on there appearance in the document, therefore words like 'the' will count less towards importance?\n\nThis may be more usefull in extracting those words which are less frequent common in the english langauge such as 'volcano'?\n\nFrom what I understand, it takes word in the document and divides it by the total number of words int he document. In the tweets example, each tweet is a document.\n\nAnd then the inverse frequency portion does some maths and gives a weighting to rare words. [Here](#https:\/\/towardsdatascience.com\/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76) is a good article on it ","6cfef3bd":"I don't think I have ever seen a score that bad before! I would stuggle to design a model that bad.","b8e03bd6":"So, 0.63 is our target to beat","d27c1220":"OKay, so nothing I have done has improved the score, let's submit!","62c36b31":"Seems unlikely that this will help us as the vast majority of people do not use hash tags it turns out. Although from the ones we can see they don't seem to be about disasters","5ac67c1a":"Even worse...","d3166c9b":"I am loosely following the tutorial [here](#https:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial) as I know nothing of NLP.\n\n\nThis is the first time I have done NLP and I have purposefully not looked at anyone else's notebook and therefore I do not know what the best practices are for this. I shall go through and do the things that seem intuitive. Hopefully I will make lots of mistakes and learn a lot.\n\nAfter I have submitted this, I shall start a new kernel and learn from everyone else who has submitted a notebook.\n\n:)","af4785cf":"solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n\n    for i in solvers:\n        print(i)\n        print(clean(model_score(LogisticRegression(solver = i), alpha_data , train_df[\"target\"])))\n\n\n        newton-cg\n        mean: 0.64 +\/- 0.05\n        lbfgs\n        mean: 0.64 +\/- 0.05\n        liblinear\n        mean: 0.64 +\/- 0.05\n        sag\n        mean: 0.64 +\/- 0.05\n        saga\n        mean: 0.64 +\/- 0.05\n\n","bb07bdec":"Interesting...\n\nAs someone who finds religion hilarious, this is an amusing tweet to get for the first one.\n\n\nSo now i need to figure out how to turn the text into a feature that can be used by any modelling package.\n\n","cfd70482":"Worse with stopwords!"}}