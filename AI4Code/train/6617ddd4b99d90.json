{"cell_type":{"86d737c5":"code","3cc03b97":"code","3d62f7fe":"code","d24e950b":"code","72d69a26":"code","6594d267":"code","0c073c96":"code","8402600f":"code","5f401e98":"code","ae363855":"code","6bbfa4e7":"code","dfc8010a":"code","3e4c52a0":"code","41a0c148":"code","e1488074":"code","5d2e43e1":"code","89c1e6c9":"code","e9a4c053":"code","0cecdd7c":"code","97022aaf":"code","45789aa3":"code","20becc80":"code","8a237e36":"code","bc6ff2b4":"code","06fba530":"markdown","f58005e6":"markdown","e9af9f89":"markdown","0724b8cd":"markdown","4418dc0d":"markdown","1ba9c170":"markdown","63141eea":"markdown","86d98070":"markdown","2623201b":"markdown","4e16508c":"markdown","14a2f4b9":"markdown","0758257a":"markdown","8ade6bc4":"markdown"},"source":{"86d737c5":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n#Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","3cc03b97":"# Import Data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","3d62f7fe":"# Defining the Combine Function \ndef combine_df(df1,df2):\n    combined_df = pd.concat([df1,df2],sort=True).reset_index(drop=True)\n    return combined_df\n# Defining the Divide Function\ndef divide_df(df):\n    train_df = df.iloc[:891,:]\n    test_df = df.iloc[891:,:]\n    return train_df, test_df\n# Creating a Combined Dataframe: Data\ndata = combine_df(train,test)","d24e950b":"# View the data\ndata.head()","72d69a26":"# Describe Data\ndata.info()","6594d267":"# Numeric Feature Distribution\nfeatures = data.drop('Survived',axis=1)\nnum = features.select_dtypes('float64')\nnum_cols = num.columns\nfig, axs = plt.subplots(nrows=1,ncols=2,figsize=(20,7.5))\nfor i,col in enumerate(num_cols):\n    sns.distplot(data[col],ax=axs[i])","0c073c96":"# Categoric Feature Distribution (High Card Removed)\ncat = features.drop(num,axis=1)\nhighcard = [col for col in cat if cat[col].nunique() > 10]\ncat = cat.drop(highcard,axis=1)\ncat_cols = cat.columns\nfig, axs = plt.subplots(nrows=3,ncols=2,figsize=(20,22.5))\nfor i,col in enumerate(cat_cols, 1):\n    plt.subplot(3,2,i)\n    sns.countplot(x=col,data=cat,palette='Blues')\nfig.delaxes(axs[2][1])","8402600f":"# Target Distribution\ntarget = data['Survived']\nfig, axs = plt.subplots(figsize=(5,5))\nsns.countplot(x=target,data=data,palette='Blues')","5f401e98":"# Target Distribution in Numeric Features\npositive = target == 1\nfig, axs = plt.subplots(nrows=1,ncols=2,figsize=(20,7.5))\nfor i,col in enumerate(num_cols):\n    sns.distplot(data[~positive][col],ax=axs[i],label='Survived')\n    sns.distplot(data[positive][col],ax=axs[i],label='Did Not Survive',color='Gray')\n    axs[i].legend(loc='upper right')","ae363855":"# Target Distribution in Categoric Features\nfig, axs = plt.subplots(nrows=3,ncols=2,figsize=(20,22.5))\nfor i,col in enumerate(cat_cols, 1):\n    plt.subplot(3,2,i)\n    sns.countplot(x=col,data=data,hue='Survived',palette='Blues')\n    plt.legend(['Survived','Did Not Survive'],loc='upper center')\nfig.delaxes(axs[2][1])","6bbfa4e7":"# Age: Replace Missing w\/ Median Value by Pclass, Sex\ndata['Age'].fillna(data.groupby(['Pclass','Sex'])['Age'].transform('median'),inplace = True)\n# Cabin: Replace Missing w\/ Constant: U\ndata['Cabin'] = data['Cabin'].apply(lambda x: str(x)[0] if pd.notnull(x) else x)\ndata['Cabin'].fillna('U',inplace=True)\n# Embarked: Replace Missing w\/ Mode\ndata['Embarked'].fillna('S', inplace = True)\n# Fare: Replace Fare w\/ Mean Value by Pclass\ndata['Fare'].fillna(data.groupby('Pclass')['Fare'].transform('mean'),inplace=True)","dfc8010a":"# SipSp, Parch > Family\ndata['FamilyS'] = data['SibSp'] + data['Parch'] + 1\nfamily_conditions = [\n    (data['FamilyS'] == 1),\n    (data['FamilyS'] >= 2) & (data['FamilyS'] <= 4),\n    (data['FamilyS'] >= 5) & (data['FamilyS'] <= 7),\n    (8 <= data['FamilyS'])\n]\nfamily_values = ['Alone','Small','Medium','Large']\ndata['Family'] = np.select(family_conditions,family_values)\n# Ticket > Travelling\ndata['Travellers'] = data['Ticket'].map(data['Ticket'].value_counts())\ntraveller_conditions = [\n    (data['FamilyS'] == data['Travellers']) & (data['FamilyS'] == 1) & (data['Travellers'] == 1),\n    (data['FamilyS'] == data['Travellers']),\n    (data['FamilyS'] >= data['Travellers']),\n    (data['FamilyS'] <= data['Travellers'])\n]\ntravller_values = ['Alone','Family','Seperate','Companion']\ndata['Travellers'] = np.select(family_conditions,family_values)\n# Name > Title\ndata['Title'] = data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nNoble = ['Don','Rev','Dr','Major','Sir','Col','Capt','Jonkheer']\nWoman = ['Mrs','Ms','Lady','the Countess','Dona']\nMan = ['Mr']\nGirl = ['Miss','Mme','Mlle']\nBoy = ['Master']\ndata['Title'] = data['Title'].map(lambda x: 'Noble' if x in Noble else x)\ndata['Title'] = data['Title'].map(lambda x: 'Woman' if x in Woman else x)\ndata['Title'] = data['Title'].map(lambda x: 'Man' if x in Man else x)\ndata['Title'] = data['Title'].map(lambda x: 'Girl' if x in Girl else x)\ndata['Title'] = data['Title'].map(lambda x: 'Boy' if x in Boy else x)\n# Drop Intermediaries\ndata.drop(['SibSp','Parch','FamilyS','Ticket','Name'],axis=1,inplace=True)","3e4c52a0":"# Import: Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\n# Import: One Hot Encoder\nfrom sklearn.preprocessing import OneHotEncoder\n# Import: Chain\nfrom itertools import chain\n# Import: Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n# Import: Cross Val Score\nfrom sklearn.model_selection import cross_val_score\n# Defining Prediction Evaluation Function\ndef evaluate_predict(train):\n    # Specifying Feature Type\n    numeric_cols = list(train.select_dtypes('float64'))\n    categoric_cols = train.drop(train[numeric_cols],axis=1)\n    highcard = [col for col in categoric_cols if categoric_cols[col].nunique() > 10]\n    categoric_cols = categoric_cols.drop(highcard,axis=1)\n    categoric_cols = list(categoric_cols.columns)\n    # Preprocessing\n    for col in numeric_cols:\n        train[col] = LabelEncoder().fit_transform(train[col])\n    cols = []\n    for col in categoric_cols:\n        n = train[col].nunique()\n        cols.append(['{}_{}'.format(col,n) for n in range(1,n+1)])\n    cols = list(chain.from_iterable(cols))\n    onehot = pd.DataFrame(OneHotEncoder(handle_unknown='ignore',sparse=False).fit_transform(train[categoric_cols]))\n    onehot.columns = cols\n    onehot.index = train.index\n    data_numeric = train.drop(train[categoric_cols],axis=1) \n    data_encoded = pd.concat([data_numeric,onehot],axis=1)\n    train_encoded,test_encoded = divide_df(data_encoded)\n    # Splitting the Data\n    X_enc = train_encoded.drop('Survived',axis=1)\n    y_enc = train_encoded['Survived']\n    # Defining the Model\n    model = RandomForestClassifier(random_state=0)\n    # Scoring\n    score = cross_val_score(model,X_enc,y_enc,cv=3,scoring='accuracy')\n    accuracy = score.mean()\n    return accuracy\n# Splitting Data\ntrain,test=divide_df(data)\n# Testing Age Bins\nfor i in range(3,11):\n    age_test = train.copy()\n    age_test['Age'] = pd.qcut(age_test['Age'],q=i)\n    accuracy = evaluate_predict(age_test)\n    print('Age Bins: {}\\t\\tAccuracy: {}'.format(i,accuracy))\n# Testing Fare Bins\nfor i in range(3,11):\n    fare_test = train.copy()\n    fare_test['Fare'] = pd.qcut(fare_test['Fare'],q=i)\n    accuracy = evaluate_predict(fare_test)\n    print('Fare Bins: {}\\t\\tAccuracy: {}'.format(i,accuracy))","41a0c148":"# Age\ndata['Age'] = pd.qcut(data['Age'],q=9)\n# Fare\ndata['Fare'] = pd.qcut(data['Fare'],q=9)","e1488074":"# Label Encode Numeric Features\nnumeric = ['Age','Fare']\nfor col in numeric:\n    data[col] = LabelEncoder().fit_transform(data[col])","5d2e43e1":"# One Hot Encode Categoric Features\ncategoric = ['Cabin','Embarked','Pclass','Sex','Family','Title','Travellers']\ncols = []\nfor col in categoric:\n    n = data[col].nunique()\n    cols.append(['{}_{}'.format(col,n) for n in range(1,n+1)])\ncols = list(chain.from_iterable(cols))\nonehot = pd.DataFrame(OneHotEncoder(handle_unknown='ignore',sparse=False).fit_transform(data[categoric]))\nonehot.columns = cols\nonehot.index = data.index\ndata_numeric = data.drop(data[categoric],axis=1) \ndata_encoded = pd.concat([data_numeric,onehot],axis=1)\ntrain_encoded,test_encoded = divide_df(data_encoded)","89c1e6c9":"# Defining Model Evaluation Function\ndef evaluate(estimator,X,y):\n    scores = cross_val_score(forest,X,y,cv=10,scoring='accuracy')\n    print('Model Accuracy: {}'.format(scores.mean()))\n# Defining Model\nforest = RandomForestClassifier(random_state=0)\n# Splitting Data\nX = train_encoded.drop('Survived',axis=1)\ny = train_encoded['Survived']\n# Evaluating Random Forest Model\nevaluate(forest,X,y)","e9a4c053":"def tune_estimators(x):\n    model = RandomForestClassifier(n_estimators=x,\n                                   random_state=0)\n    scores = cross_val_score(model,X,y,cv=3,scoring='accuracy')\n    return scores.mean()\nfor i in range(1,9):\n    results = tune_estimators(i*150)\n    print('Number of Estimators: {} \\t\\t Accuracy: {}'.format(i*150,results))","0cecdd7c":"def tune_depth(x):\n    model = RandomForestClassifier(max_depth=x,\n                                   random_state=0)\n    scores = cross_val_score(model,X,y,cv=3,scoring='accuracy')\n    return scores.mean()\nfor i in range(1,9):\n    results = tune_depth(i)\n    print('Max Depth: {} \\t\\t Accuracy: {}'.format(i,results))","97022aaf":"def tune_minsamplessplit(x):\n    model = RandomForestClassifier(max_depth=8,\n                                   min_samples_split=x,\n                                   random_state=0)\n    scores = cross_val_score(model,X,y,cv=3,scoring='accuracy')\n    return scores.mean()\nfor i in range(2,10):\n    results = tune_minsamplessplit(i)\n    print('Min Sample Split: {} \\t\\t Accuracy: {}'.format(i,results))","45789aa3":"def tune_minsamplesleaf(x):\n    model = RandomForestClassifier(max_depth=7,\n                                   min_samples_split=8,\n                                   min_samples_leaf=x,\n                                   random_state=0)\n    scores = cross_val_score(model,X,y,cv=3,scoring='accuracy')\n    return scores.mean()\nfor i in range(1,9):\n    results = tune_minsamplesleaf(i)\n    print('Min Sample Leaf: {} \\t\\t Accuracy: {}'.format(i,results))","20becc80":"def tune_maxleafnodes(x):\n    model = RandomForestClassifier(max_depth=7,\n                                   min_samples_split=8,\n                                   min_samples_leaf=2,\n                                   max_leaf_nodes=x,\n                                   random_state=0)\n    scores = cross_val_score(model,X,y,cv=3,scoring='accuracy')\n    return scores.mean()\nfor i in range(2,10):\n    results = tune_maxleafnodes(i*5)\n    print('Max Leaf Nodes: {} \\t\\t Accuracy: {}'.format(i*5,results))","8a237e36":"# Model Selection\ntune_forest = RandomForestClassifier(n_estimators=750,\n                                     min_samples_split=8,\n                                     min_samples_leaf=3,\n                                     max_leaf_nodes=10,\n                                     random_state=0)\n# Splitting Data\ntrain_X = train_encoded.drop('Survived',axis=1)\ntrain_y = train_encoded['Survived']\ntest_X = test_encoded.drop('Survived',axis=1)\n# Fitting the Model\ntune_forest.fit(train_X,train_y)\n# Predictions\npredictions = tune_forest.predict(test_X)","bc6ff2b4":"# Creating Submission Dataframe\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions.astype(int)\n# Converting Submission to .csv\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","06fba530":"**Feature Engineering**","f58005e6":"**Feature Distribution**","e9af9f89":"**Model Tuning: Random Forest**","0724b8cd":"**Data Exploration**","4418dc0d":"**Titanic Machine Learning Competition**\n\nThe **Titanic: Machine Learning from Disaster** competition tasks participants with predicting which passengers would have survived the crash of the famous RMS Titanic using various data manipulation and analysis techniques. The goal of this notebook is to serve as a chance to organize what I have learned via Kaggle's **Python**, **Pandas**, **Data Cleaning**, **Data Visualization**, **Intro to Machine Learning**, **Intermediate Machine Learning**, and **Feature Engineering** courses and to better my understanding of these topics and how they are interrelated through a single, continous application.","1ba9c170":"**Model Selection and Prediction: Random Forest**","63141eea":"**Submission**","86d98070":"**One Hot Encoding Categoric Features**","2623201b":"**Model: Random Forest**","4e16508c":"**Binning Continuous Features**","14a2f4b9":"**Target Distribution**","0758257a":"**Label Encoding Numeric Features**","8ade6bc4":"**Missing Values**"}}