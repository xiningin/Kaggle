{"cell_type":{"acfaff52":"code","ca6fc40f":"code","44ac61df":"code","ded59696":"code","de23fbfa":"code","930e5041":"code","50c46e2b":"code","c96fb898":"code","832e9169":"code","5bd98909":"code","1ca101b9":"code","79eb3bad":"code","83caa86a":"code","39ca640c":"code","3ce4a7b8":"code","bb0464e1":"code","3d5a71f0":"code","8cac303f":"code","b4ba4a52":"code","90637f38":"code","201dac5a":"code","226f9210":"code","b0ab64d2":"code","828218b2":"code","0a27d3b9":"code","60e6e62f":"code","a0d6c360":"code","c83a0c58":"code","34e12d76":"code","e7e6ffab":"code","2f491420":"code","058ccff0":"code","db72d23f":"code","016e036d":"code","05222329":"code","f72fd07a":"code","745923ab":"code","4151c60e":"code","ca66f7c3":"code","5875eef9":"code","009fc1e1":"code","1fab9507":"code","52e0003c":"code","2bbb8355":"code","29950aaf":"code","253047f0":"code","de01f6d7":"code","1efb773a":"markdown","0c7af4a2":"markdown","08ade86d":"markdown","84a79d1f":"markdown","3b247836":"markdown","44d0bc03":"markdown","58cca18a":"markdown","a70f464f":"markdown","dbce4fd5":"markdown","5a91ebd4":"markdown","e649c210":"markdown","4e484958":"markdown","d3041378":"markdown","d2722a0b":"markdown","b1f48414":"markdown","099be131":"markdown","e5b14f26":"markdown","f5ba7300":"markdown","03bfa25e":"markdown"},"source":{"acfaff52":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np  \nimport pandas as pd \nfrom numpy import ma\nimport pandas as pd\nimport math\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom matplotlib import ticker, cm\nimport matplotlib.gridspec as gridspec\nimport matplotlib.colors as colors\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom scipy.stats import multivariate_normal\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import multivariate_normal\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler \n\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.layers import Embedding\nfrom keras.utils import np_utils, to_categorical\nfrom keras.datasets import imdb\n#from keras import preprocessing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import Callback,ModelCheckpoint\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout, GlobalAveragePooling1D\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport keras.backend as K\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca6fc40f":"# Read data\n\ndataRaw = pd.read_csv('\/kaggle\/input\/time-series\/TimeSeries.csv')\nRawTS = dataRaw.copy()\nprint(RawTS.shape)\nRawTS.head()","44ac61df":"labels = pd.read_csv('\/kaggle\/input\/time-series\/labelsTimeSeries.csv')\nprint(labels.shape)\nlabels.head()","ded59696":"labels.label.value_counts(), labels.label.value_counts(normalize=True)","de23fbfa":"# Check for MISSING data\nprint('Any missing value ?',RawTS.isnull().values.any())","930e5041":"RawTS.info()","50c46e2b":"RawTS.describe()","c96fb898":"# NORMALIZE data\n\nx = RawTS.values #returns a numpy array\n#min_max_scaler = preprocessing.MinMaxScaler()\nStandardScaler = StandardScaler()\nx_scaled = StandardScaler.fit_transform(x)\ndfNorm = pd.DataFrame(x_scaled, columns=RawTS.columns)\n\nprint(dfNorm.shape)\ndfNorm.head()","832e9169":"plt.style.use('seaborn-whitegrid')\nfig = plt.figure()\nax = plt.axes()\n\nx = np.linspace(0, dfNorm.shape[0])\nax.plot(dfNorm['v1'])\nax.plot(labels['label']\/10)","5bd98909":"for col in dfNorm.columns:\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure()\n    ax = plt.axes()\n    plt.title(col)\n\n    x = np.linspace(0, dfNorm.shape[0])\n    ax.plot(dfNorm[col])\n    ax.plot(labels['label']\/10)\n    ","1ca101b9":"# Some viz of normalized data ONE EVENT\n\neventRow = 10702\n\nminX = eventRow - 10\nmaxX = eventRow + 10\n\nfor col in dfNorm.columns:\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure()\n    ax = plt.axes()\n    plt.title(col)\n    \n    x = np.linspace(minX, maxX)\n    ax.plot(dfNorm[col][minX:maxX])\n    ax.plot(labels['label'][minX:maxX])","79eb3bad":"dfNorm['label'] = labels['label']\ndfNorm.shape","83caa86a":"NoEvent = dfNorm[dfNorm['label'] == 0]\nOnlyEvent = dfNorm[dfNorm['label'] == 1]\nprint(NoEvent.shape)\nprint(OnlyEvent.shape)","39ca640c":"NoEvent.describe()","3ce4a7b8":"OnlyEvent.describe()","bb0464e1":"# Viz the distribution of each feature: no-event vs event\n\nfor col in dfNorm.columns:\n    plt.figure()\n    sns.distplot(OnlyEvent[col]).set_title(col)\n    sns.distplot(NoEvent[col])\n","3d5a71f0":"# As the above dist differences between event and no-event may be because of the size differences...\n# Repeat the above with a sample of the same size for no-event ... downsample\n\nNoEventSample = NoEvent.sample(443)\nNoEventSample.shape","8cac303f":"# Viz the distribution of each feature: no event vs event - SAME size\n\nfor col in dfNorm.columns:\n    plt.figure()\n    sns.distplot(OnlyEvent[col],fit_kws={\"color\":\"red\"}).set_title(col)\n    sns.distplot(NoEventSample[col])","b4ba4a52":"print(NoEvent.shape)\nprint(OnlyEvent.shape)\n\nCols = list(NoEvent)[:-1] \nNoEvent = NoEvent[Cols]\nOnlyEvent = OnlyEvent[Cols]\nprint(NoEvent.shape)\nprint(OnlyEvent.shape)","90637f38":"# CREATE the TRAIN and TEST sets\n# Anom data is ONLY in TEST - not in TRAIN\n\nnum_test = 100000\nshuffled_data = NoEvent.sample(frac=1, random_state=47)[:-num_test].values\nX_train = shuffled_data\n\n#X_valid = np.concatenate([shuffled_data[-2*num_test:-num_test], fraud_pca_data[:246]])\n#y_valid = np.concatenate([np.zeros(num_test), np.ones(246)])\n\nX_test = np.concatenate([shuffled_data[-num_test:], OnlyEvent[:]])\ny_test = np.concatenate([np.zeros(num_test), np.ones(OnlyEvent.shape[0])])\n\nprint(\"normal \", dfNorm.shape)\nprint(\"OnlyEvents\", OnlyEvent.shape)\nprint(\"OnlyEvents data only in Test with NONE in the training\")\nprint(\"X_train \", X_train.shape)\nprint(\"X_test \", X_test.shape)\nprint(\"y_test \", y_test.shape)","201dac5a":"\np = multivariate_normal(mean=np.mean(X_train,axis=0), cov=np.cov(X_train.T))\n\nx = p.pdf(X_train) \nprobNorm = x \/ x.sum() # Normalize pdf\nprint(\"max prob of x on X_train\", max(probNorm))\nprint(\"mean prob of x on X_train\", np.mean(probNorm))\nprint('-' * 60)\nMyTrain = np.mean(probNorm)\n\nx = p.pdf(X_test) \nprobNorm = x \/ x.sum() # Normalize pdf\nprint(\"max prob of x on X_test\", max(probNorm))\nprint(\"mean prob of x on X_test\", np.mean(probNorm))\nprint('-' * 60)\nMyTest = np.mean(probNorm)\n\nx = p.pdf(OnlyEvent) \nprobNorm = x \/ x.sum() # Normalize pdf\nprint(\"max prob of x on OnlyEvent\", max(probNorm))\nprint(\"mean prob of x on OnlyEvent\", np.mean(probNorm))\nprint('-' * 60)\n\nprint('Difference between mean prob of Train vs Test ', MyTrain - MyTest)","226f9210":"# Find best epsilon re F1 score\n\nx = p.pdf(X_test)\nx = x \/ x.sum() # Normalize pdf\n\nEpsF1 = []\n\n\nepsilons = [1e-5, 1e-10, 1e-15, 1e-20, 1e-25, 1e-30,1e-35, 1e-40, 1e-45, 1e-50, 1e-55, 1e-60, 1e-65, 1e-70, 1e-75, 1e-80, 1e-85, 1e-90]\n\n\nfor e in range(len(epsilons)):\n    eps = epsilons[e]\n    pred = (x <= eps)\n    f = f1_score(y_test, pred, average='binary')\n    #print(\"F1 score on test\", round(f,4), \" with epsilon \", eps)\n    EpsF1.append([eps, round(f,4)])\n    \nEpsF1df = pd.DataFrame(EpsF1, columns = ['epsilon', 'F1'])\nEpsF1df.head(20)","b0ab64d2":"EpsF1df.plot.line(\"epsilon\",\"F1\")\nplt.xscale('log')\nplt.xlim(1e-5, 1e-95)\nplt.title(\"F1 vs decreasing log Epsilon\")\nplt.show()","828218b2":"# Identify the row where event\nEventsRows = dfNorm.index[dfNorm['label'] == 1]\nlen(EventsRows)\n","0a27d3b9":"# Returns minimum difference between any pair \ndef findMinDiff(arr, n): \n    # Initialize difference as infinite \n    diff = 10**20\n      \n    # Find the min diff by comparing difference \n    # of all possible pairs in given array \n    for i in range(n-1): \n        for j in range(i+1,n): \n            if abs(arr[i]-arr[j]) < diff: \n                diff = abs(arr[i] - arr[j]) \n  \n    # Return min diff \n    return diff ","60e6e62f":"n = len(EventsRows) \nprint(\"Minimum difference between events is \" + str(findMinDiff(EventsRows, n)),'time steps')","a0d6c360":"def chunk(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n","c83a0c58":"ChunkCounter = 0\nresult_arrList = []\nchunkSize = 9\n\nfor df_chunk in chunk(dfNorm, chunkSize):\n    if df_chunk.label.any() == 1 or df_chunk.shape[0] < chunkSize:\n        pass\n    else:\n        result_arrList.append(df_chunk.iloc[:,:-1])\n        ChunkCounter = ChunkCounter + 1\n        #print(df_chunk)\n    \nChunkCounter","34e12d76":"# No-event 56k periods - chunks of 9 time steps and 11 features each:\n\nNoEvent_arr = np.stack(result_arrList, axis=0)\nNoEvent_arr.shape","e7e6ffab":"# For the OnlyEvent chunking\n\nidx = dfNorm.index.get_indexer_for(dfNorm[dfNorm.label == 1].index)\nprint(idx.shape)\n\nn=4\n\nOnlyEvent_arr = dfNorm.iloc[np.unique(np.concatenate([np.arange(max(i-n,0), min(i+n+1, len(dfNorm)))\n                                            for i in idx]))]\n\npd.options.display.max_rows = 100\nOnlyEvent_arr.head(100)","2f491420":"# After checking the event label is in the middle of each OnlyEvent period, remove the label and make an array of chunks\n\nOnlyEvent_arr = OnlyEvent_arr.iloc[:,:-1]\nOnlyEvent_arr.head()","058ccff0":"ChunkCounter = 0\nresult_arrList = []\nchunkSize = 9\n\nfor df_chunk in chunk(OnlyEvent_arr, chunkSize):\n    if df_chunk.shape[0] < chunkSize:\n        pass\n    else:\n        result_arrList.append(df_chunk)\n        ChunkCounter = ChunkCounter + 1\n        #print(df_chunk)\n    \nChunkCounter","db72d23f":"# OnlyEvent 443 periods - chunks of 9 time steps and 11 features each:\n\nOnlyEvent_arr = np.stack(result_arrList, axis=0)\nOnlyEvent_arr.shape","016e036d":"# Concatenate the 2 arrays before split into train and test\n\nAllFeat_arr = np.append(NoEvent_arr, OnlyEvent_arr, axis = 0)\nAllFeat_arr.shape\n","05222329":"# Create the label ... y\n\ny0 = np.zeros(NoEvent_arr.shape[0])\n\ny1 = np.ones(OnlyEvent_arr.shape[0])\n\ny = np.append(y0, y1, axis = 0)\ny.shape","f72fd07a":"# Split into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(AllFeat_arr, y, test_size=0.2, random_state=7)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","745923ab":"# We have to redimension the arrays for the Conv2D digestion benefit\n\ndata_train_wide = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\ndata_test_wide = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))\n\nprint(data_train_wide.shape)\nprint(data_test_wide.shape)","4151c60e":"# F1 function as there is none in Keras metrics\n\n#https:\/\/medium.com\/@aakashgoel12\/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n\nimport keras.backend as K\n\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","ca66f7c3":"# NN model\n\nn_filters = 64\nfsize = 5  # Note that kernel size (1, fsize) = it is not a square kernel...it is rectangular\nwindow_size = 9   # Number of time steps in one period\nn_features = 11 # Number of cols in one sample (one table)\n\n\nMyModel = models.Sequential()\nMyModel.add(layers.Conv2D(n_filters, fsize, activation='relu', input_shape=(window_size, n_features, 1)))\nMyModel.add(layers.Flatten())\nMyModel.add(layers.Dense(256, activation='relu'))\n#MyModel.add(layers.Dropout(0.2))\n\nMyModel.add(layers.Dense(1, activation='sigmoid'))\n\nMyModel.compile(optimizer=optimizers.Adam(lr=1e-4), \n              loss='binary_crossentropy', \n              metrics=[get_f1])\n              #metrics=['binary_accuracy'])\n\nprint(MyModel.summary())","5875eef9":"# Train \/ fit\n\nhistory = MyModel.fit(data_train_wide, y_train, \n                      validation_split=0.2, \n                      epochs = 20, \n                      batch_size = 16)\n","009fc1e1":"#Learning curves ... F1\n\nacc = history.history['get_f1'] \nval_acc = history.history['val_get_f1'] \nloss = history.history['loss'] \nval_loss = history.history['val_loss'] \nepochs = range(1, len(acc) + 1) \nplt.plot(epochs, acc, 'bo', label='Training F1') \nplt.plot(epochs, val_acc, 'b', label='Validation F1') \nplt.title('Training and validation F1') \nplt.legend() \nplt.figure() \nplt.plot(epochs, loss, 'bo', label='Training loss') \nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss') \nplt.legend() \nplt.show()","1fab9507":"# Final Predict\n# NOTE final_predictions is a list of probabilities\n\nfinal_predictions = MyModel.predict(data_test_wide)\nfinal_predictions.shape","52e0003c":"# Modify the raw final_predictions - prediction probs into 0 and 1\n\nPreds = final_predictions.copy()\n#print(len(Preds))\n#print(Preds)\nPreds[ np.where( Preds >= 0.5 ) ] = 1\nPreds[ np.where( Preds < 0.5 ) ] = 0\n\nPreds.shape","2bbb8355":"# Confusion matrix\n\nfrom sklearn import metrics\nconf_mx = metrics.confusion_matrix(y_test, Preds)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint (recall, precision)","29950aaf":"def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,\n                          normalize=False):\n    import itertools\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n        \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","253047f0":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['NoEvent', 'Event'],\n                      title        = \"Confusion Matrix on test\")","de01f6d7":"from sklearn.metrics import precision_recall_curve, average_precision_score, auc\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, Preds)\n# calculate F1 score\nf1 = metrics.f1_score(y_test, Preds)\nprint('f1=%.3f' % (f1))\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n# show the plot\nplt.show()","1efb773a":"# Plan for further experimentations\n\n* Check various lr, pooling after Conv2D instead of Flatten, w\/without GRU after Conv2D\n* Under, oversampling, SMOTE, ADASYN\n","0c7af4a2":"### Time series approach\n\n* The idea is to use a Keras **Conv2D** (usually used for image analysis) on this time series\n* Prep the data as chunks or buckets of 9 time steps and 11 features each\n* Normal or no-event chunks have no events within their time periods\n* Events are in the middle (row # 4 out of 9) of the time periods\n* Exposing a Conv2D to the above prepped data, which should hopefully make the algorithm task easier\n\nExcellent explanation on use of Conv2D for a time series *forecasting* challenge at https:\/\/github.com\/walesdata\/2Dconv_pub\/blob\/master\/gefcom_multiconv.ipynb\n","08ade86d":"### Multi-variate Gaussian probability density function - as the baseline","84a79d1f":"# Results\n\nAdam(lr=1e-4) , batch size = 16, optimized on get_f1, square kernel size = (fsize, fsize) = fsize, 20 epochs\n\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, fsize=2  ... F1 = 0.975\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, fsize=3  ... F1 = 0.982\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, fsize=4  ... F1 = **0.988**\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, fsize=5  ... F1 = 0.988\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, fsize=6  ... F1 = 0.988\n* Conv2D 64 filters, Flatten, Dense 256, Dropout 0.2, Sigmoid, fsize=5  ... F1 = 0.976\n* Conv2D 64 filters, Flatten, Dense 256, Dropout 0.2, Sigmoid, rectangular kernel size = (1, 5)   ... F1 = 0.982\n\n\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, 20 epochs, optimized on get_f1 ... F1 = 0.982\n* Conv2D 64 filters, Flatten, Dense 256, Sigmoid, 20 epochs, optimized on binary_acc ... F1 = 0.956\n* Conv2D 64 filters, Flatten, Sigmoid, 50 epochs ... F1 = 0.91\n","3b247836":"## Best results ... F1 score on test = 0.985","44d0bc03":"#Learning curves ... Binary accuracy\n\nacc = history.history['binary_accuracy'] \nval_acc = history.history['val_binary_accuracy'] \nloss = history.history['loss'] \nval_loss = history.history['val_loss'] \nepochs = range(1, len(acc) + 1) \nplt.plot(epochs, acc, 'bo', label='Training acc') \nplt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.legend() \nplt.figure() \nplt.plot(epochs, loss, 'bo', label='Training loss') \nplt.plot(epochs, val_loss, 'b', label='Validation loss') \nplt.title('Training and validation loss') \nplt.legend() \nplt.show()","58cca18a":"#As v7-v11 do not change much during an event - I removed them, to see if it improves performance ...It does NOT\n\nNoEvent = dfNorm[dfNorm['label'] == 0]\nOnlyEvent = dfNorm[dfNorm['label'] == 1]\nprint(NoEvent.shape)\nprint(OnlyEvent.shape)\n\nNoEvent.drop(['v7','v8','v9', 'v10', 'v11'], axis=1, inplace=True)\nOnlyEvent.drop(['v7','v8','v9', 'v10', 'v11'], axis=1, inplace=True)\nprint(NoEvent.shape)\nprint(OnlyEvent.shape)\n","a70f464f":"### Baseline accuracy by majority voting = 100 - 0.79% = 99.21%","dbce4fd5":"# Conv2D NN","5a91ebd4":"### Split into train and test","e649c210":"# Data","4e484958":"### 56k samples, each has 9 time steps and 11 features (56k tables of 9 rows X 11 cols)","d3041378":"# Multi variate time series - anomaly detection\n\n* There are 509k samples with 11 features\n* Each instance \/ row is one moment in time. \n* I don't know what the time step is: 100 ms, 1ms, ?\n* 443 rows are identified as events, basically rare, outliers \/ anomalies .. **0.09%** \n\n\n### Task is to identify these events in this time series - time series classification (not forecasting)\n\n* As dataset is unbalanced, the metric to use is **F1 score** (not accuracy, nor ROC AUC)\n* Initial approach for a baseline - NOT as a time series but as individual instances, I've tried a Multi-variate Gaussian probability density function and got a very low F1 = 0.312\n\n### An event is actually a series of time steps around it...approx 4 before and 4 after\n","d2722a0b":"### Chunk into buckets the NoEvent and OnlyEvent ... chunk size = 9 time steps","b1f48414":"### Now the anomaly prob is 443 \/ 56,182 = 0.79% ... much easier, about 9 times easier for the model...","099be131":"Excellent overview https:\/\/towardsdatascience.com\/time-series-forecasting-with-2d-convolutions-4f1a0f33dff6","e5b14f26":"Seems that v7-v11 don't change much during ONE event...let's check the stats on no-event vs events","f5ba7300":"### Note that ONE event is actually a TIME SERIES of events around it","03bfa25e":"# Time series approach\n\n* Create periods of 9 time steps labeled as NoEvent or OnlyEvent\n* Events are in the middle (there are 4 rows before and 4 after) of the time period, and the chunk is labeled as OnlyEvent\n* No-events time periods have no event within their 9 time steps, labeled NoEvent\n* So the algorithm would hopefully be able to separate the events"}}