{"cell_type":{"25181c2e":"code","e6b1efb2":"code","5ab2aa50":"code","1680334f":"code","ae851625":"code","ee1cf561":"code","aa57de89":"code","96dfb6e2":"code","afee7d21":"code","7b674f0d":"code","a85b4974":"code","842472dd":"code","912cd991":"code","09725fc2":"code","918c9e98":"code","6f1d432d":"code","f19e05e5":"code","6b04e3f7":"code","48bfb21b":"code","72efc1dc":"code","c1dbe36d":"code","556990dd":"code","63e37be5":"code","2d0a6f6b":"code","fa92ce99":"code","195b33c3":"code","b8cb0a82":"code","0528d972":"code","a59bb3f7":"code","47a8d573":"code","142254c3":"code","28119edf":"code","de9cabcf":"code","3be6a329":"code","148db884":"code","143cf89b":"code","0bf66fb7":"code","643e6619":"code","ffd147cc":"code","d3f7055e":"markdown","623c2b9b":"markdown","be0f496f":"markdown","68d79e5c":"markdown","63417b2f":"markdown","0fa59e70":"markdown","1b5c87d9":"markdown","6a4db878":"markdown","ef1d3bbd":"markdown","d88a97ff":"markdown","d6719cca":"markdown","cb26d04b":"markdown","22e515de":"markdown","850c9cd6":"markdown","8da924cb":"markdown","f956c545":"markdown","4c5d1c97":"markdown","687f626a":"markdown","0e353a6d":"markdown"},"source":{"25181c2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6b1efb2":"tr = pd.read_csv('..\/input\/train.csv')\nte = pd.read_csv('..\/input\/test.csv')","5ab2aa50":"tr.info()","1680334f":"inspect_train = pd.DataFrame({'Dtype': tr.dtypes, 'Unique values': tr.nunique() ,\n             'Number of Missing values': tr.isnull().sum() ,\n              'Percentage Missing': (tr.isnull().sum() \/ len(tr)) * 100\n             }).sort_values(by='Number of Missing values',ascending = False)\nprint('There are {} columns with no missing values'.format((inspect_train['Percentage Missing']==0.0).sum()))\ninspect_train","ae851625":"inspect_test = pd.DataFrame({'Dtype': te.dtypes, 'Unique values': te.nunique() ,\n             'Number of Missing values': te.isnull().sum() ,\n              'Percentage Missing': (te.isnull().sum() \/ len(te)) * 100\n             }).sort_values(by='Number of Missing values',ascending = False)\nprint('There are {} column with no missing values'.format((inspect_test['Percentage Missing']==0.0).sum()))\ninspect_test","ee1cf561":"_ = plt.figure(figsize=[20,5])\n_ = plt.plot(inspect_train['Percentage Missing'])\n_ = plt.plot(inspect_test['Percentage Missing'],color='r')\n_ = plt.legend(['train','test'])\n_ = plt.grid()\n_ = plt.title(' Missing value % in each of 79 predictors, plotted in descending order')\n_ = plt.ylabel('Missing Percentage')\n_ = plt.xlabel('Column index, when arranged according to the number of missing entries')","aa57de89":"print('There are {} and {} columns with more than 90% missing values in test and train'.format((inspect_test['Percentage Missing']> 90.0).sum(),(inspect_train.drop('SalePrice')['Percentage Missing'] > 90.0).sum()))\nprint('There are {} and {} columns with more than 10% missing values in test and train'.format((inspect_test['Percentage Missing']> 10.0).sum(),(inspect_train.drop('SalePrice')['Percentage Missing'] > 10.0).sum()))\nprint('There are {} and {} columns with more than 50% missing values in test and train'.format((inspect_test['Percentage Missing']> 50.0).sum(),(inspect_train.drop('SalePrice')['Percentage Missing'] > 50.0).sum()))","96dfb6e2":"inspect_test.loc[(inspect_test['Percentage Missing']==0.0),:].index","afee7d21":"# Select data where there are <= 0% missing values\ntr1 = tr.loc[:,inspect_test.loc[(inspect_test['Percentage Missing']==0.0),:].index]\ntr1['SalePrice']=tr['SalePrice']\nte1 = te.loc[:,inspect_test.loc[(inspect_test['Percentage Missing']==0.0),:].index]","7b674f0d":"categorical_variables = te1.drop('Id',axis=1).select_dtypes(exclude=['int64', 'float64', 'bool']).columns\nnumeric_variables = te1.drop('Id',axis=1).select_dtypes(include=['int64', 'float64']).columns\ntr1.loc[:,numeric_variables].nunique()","a85b4974":"te1.nunique()","842472dd":"from datetime import date\ndef modify_data(data):\n    data['YearBuilt'].apply(lambda x : np.nan if (date.today().year - x) < 0 else x) \n    data['YearRemodAdd'].apply(lambda x : np.nan if (date.today().year - x) < 0 else x) \n    data['age_of_house'] = date.today().year - data['YearBuilt'] \n    data['log_age'] = np.log(data.age_of_house+0.00001)\n    data['time_since_remod'] = date.today().year - data['YearRemodAdd']\n    data['log_remod'] = np.log(data.time_since_remod+0.00001)\n    data.drop(['age_of_house','time_since_remod','YearBuilt','YearRemodAdd'],axis=1,inplace=True)\n    return data\ndef standardize_data(data,num_var):\n    epsilon=1e-8\n    data.loc[:,num_var] = data.loc[:,num_var].transform(lambda x: (x - x.mean())\/(x.std()+epsilon))\n    return data","912cd991":"tr1 = modify_data(tr1)\nte1 = modify_data(te1)","09725fc2":"numeric_variables = list(te1.drop('Id',axis=1).select_dtypes(include=['int64', 'float64']).columns)\nnumeric_variables_tr = list(tr1.drop('Id',axis=1).select_dtypes(include=['int64', 'float64']).columns)","918c9e98":"numeric_variables","6f1d432d":"tr1.loc[:,numeric_variables].nunique()","f19e05e5":"te1.loc[:,numeric_variables].nunique()","6b04e3f7":"correlation_threshold = 0.95\n\ncorr_matrix = tr1.loc[:,numeric_variables_tr].corr()\n# Extract the upper triangle of the correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n# Select the features with correlations above the threshold\n# Need to use the absolute value\nto_drop = [column for column in upper.columns if any(upper[column].abs() > correlation_threshold)]\nprint(\"There are \"+str(len(to_drop))+\" columns with correlations above certain threshold\")","48bfb21b":"# Conversion to one hot\ndef conv_one_hot(data,categorical_variables) :\n    #if(tr1.loc[:,var].dtype is object): \n    #tr1.loc[:,var].fillna('UNK',inplace=True)\n    #    te1.loc[:,var].fillna('UNK',inplace=True)\n    # Convert Categorical variables to dummies\n    categorical_variables = data.select_dtypes(exclude=['int64', 'float64', 'bool']).columns\n    cat_var = pd.get_dummies(data.loc[:,categorical_variables],drop_first=True)\n    # Remove originals\n    data = data.drop(categorical_variables,axis=1)\n    data = pd.concat([data,cat_var],axis=1)\n    #removing dulpicate columns - useful in case two variables are closely related.\n    _, i = np.unique(data.columns, return_index=True)  #Always better to have\n    data=data.iloc[:, i] \n    return data","72efc1dc":"tr1.shape,te1.shape","c1dbe36d":"tr1 = conv_one_hot(tr1,categorical_variables)\nte1 = conv_one_hot(te1,categorical_variables)","556990dd":"tr1.shape,te1.shape","63e37be5":"Sprice = tr1.SalePrice\ntr1.drop(list(set(tr1.columns)-set(te1.columns)),axis=1,inplace=True)\n# Add target back\ntr1['SalePrice'] = Sprice","2d0a6f6b":"_ = sns.distplot(tr1.SalePrice\/1000)\n_ = plt.title('Distribution of SalesPrice')\n_ = plt.xlabel('SalesPrice in multiples of 1k dollars')\n_ = plt.ylabel('Distribution')","fa92ce99":"_ = plt.scatter(tr1.BedroomAbvGr,tr1.SalePrice)\nprint(tr1.groupby('BedroomAbvGr').agg({'SalePrice':['mean','std','min','max','count']}))","195b33c3":"_ = sns.regplot(x='KitchenAbvGr',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs KitchenAbvGr')","b8cb0a82":"_ = plt.plot(tr1.groupby('MoSold').SalePrice.mean())\n_ = plt.title('SalePrice Vs Month')\ntr1.groupby('MoSold').SalePrice.mean()","0528d972":"fig = plt.figure(figsize=(20, 10))\n_ = plt.grid()\n_ = plt.subplot(3,3,1)\n_ = sns.regplot(x='LotArea',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs Lot Area')\n_ = plt.subplot(3,3,2)\n_ = sns.regplot(x='ScreenPorch',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs ScreenPorch')\n_ = plt.subplot(3,3,3)\n_ = sns.regplot(x='EnclosedPorch',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs EnclosedPorch')\n_ = plt.subplot(3,3,4)\n_ = sns.regplot(x='GrLivArea',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs GrLivArea')\n_ = plt.subplot(3,3,5)\n_ = sns.regplot(x='OpenPorchSF',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs OpenPorchSF')\n_ = plt.subplot(3,3,6)\n_ = sns.regplot(x='WoodDeckSF',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs WoodDeckSF')\n_ = plt.subplot(3,3,7)\n_ = sns.regplot(x='1stFlrSF',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs 1stFlrSF')\n_ = plt.subplot(3,3,8)\n_ = sns.regplot(x='2ndFlrSF',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs 2ndFlrSF')\n_ = plt.subplot(3,3,9)\n_ = sns.regplot(x='log_age',y='SalePrice',data=tr1,fit_reg=False)\n_ = plt.title('SalePrice Vs Log of Age of the House')","a59bb3f7":"tr1 = conv_one_hot(tr1,['MoSold','YrSold'])\nte1 = conv_one_hot(te1,['MoSold','YrSold'])\ntr1 = standardize_data(tr1,numeric_variables) \nte1 = standardize_data(te1,numeric_variables)","47a8d573":"from sklearn.model_selection import train_test_split,KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\ndef compute_rmse(y,ypred):\n    mse = np.mean((y-ypred)*(y-ypred))\n    rmse = np.sqrt(mse)\n    return rmse\nnp.random.seed(13)","142254c3":"X = tr1.drop(['Id','SalePrice'],axis=1)\ny = tr1['SalePrice']","28119edf":"Xtr,Xte,ytr,yte = train_test_split(X,y,test_size=0.1,random_state=43)","de9cabcf":"# Print certain benchmarks\ndef print_measures(Xte,yte,ypred):\n    print(\"Number of predictor variables: \",Xte.shape[1])\n    rsq = r2_score(ypred,yte)\n    print(\"R2 score on test set:  \",np.round(rsq,4))\n    adj_rsq = 1-((1-rsq)*(Xte.shape[0]-1)\/(Xte.shape[0]-Xte.shape[1]-1))\n    print(\"Adjusted R2 score on test set: \",np.round(adj_rsq,4))\n    print('RMSE on test set: ',np.round(compute_rmse(ypred,yte),4))","3be6a329":"# Select those numeric variables which are really numeric and not ordinal.\nselect_variables = ['MiscVal',\n 'ScreenPorch',\n '3SsnPorch',\n 'EnclosedPorch',\n 'OpenPorchSF',\n 'WoodDeckSF',\n 'GrLivArea',\n 'LotArea',\n '1stFlrSF',\n '2ndFlrSF',\n 'LowQualFinSF',\n 'log_age',\n 'log_remod']\nlr = LinearRegression()\nlr.fit(Xtr.loc[:,select_variables],ytr)\nypred = lr.predict(Xte.loc[:,select_variables])\nprint_measures(Xte.loc[:,select_variables],yte,ypred)\ncoeff=lr.coef_\nintercept = lr.intercept_\ncoeffs_b= lr.coef_[np.argsort(abs(lr.coef_))[::-1]]\nnames_b = list(Xte.loc[:,select_variables].columns[np.argsort(abs(lr.coef_))[::-1]])\nlrimp = pd.DataFrame(np.round(coeffs_b,3),index=names_b,columns=['Coeff value'])\n_ = np.log(np.abs(lrimp)).plot.bar(color='purple')\n_ = plt.title('Feature Importance (Linear Reg)')\n_ = plt.ylabel('Coefficient value')\n_ = plt.xlabel('Features')\nlrimp","148db884":"# First model using only 25 numeric variables. This model has neither converted nor used categorical variables.\nlr = LinearRegression()\nlr.fit(Xtr.loc[:,numeric_variables],ytr)\nypred = lr.predict(Xte.loc[:,numeric_variables])","143cf89b":"print_measures(Xte.loc[:,numeric_variables],yte,ypred)","0bf66fb7":"coeff=lr.coef_\nintercept = lr.intercept_\ncoeffs_b= lr.coef_[np.argsort(abs(lr.coef_))[::-1]]\nnames_b = list(Xte.loc[:,numeric_variables].columns[np.argsort(abs(lr.coef_))[::-1]])\nlrimp = pd.DataFrame(np.round(coeffs_b,3),index=names_b,columns=['Coeff value'])\n_ = np.log(np.abs(lrimp)).plot.bar(color='purple')\n_ = plt.title('Feature Importance (Linear Reg)')\n_ = plt.ylabel('Coefficient value')\n_ = plt.xlabel('Features')\nlrimp","643e6619":"lr = LinearRegression()\nlr.fit(Xtr,ytr)\nypred = lr.predict(Xte)\nprint_measures(Xte,yte,ypred)","ffd147cc":"ysub = lr.predict(te1.drop('Id',axis=1))\nte1 = te1.assign(SalePrice = ysub)\nte1[['Id','SalePrice']].to_csv('aparna_housing_price.csv',index=False)","d3f7055e":"**QUICK INSPECTION**","623c2b9b":"The above plot gives a better idea on missing value percentage and it is helpful in intuitively deciding a threshold t. It looks like if t=10%, we lose only a smal number of predictors, i.e. 6 in both test and train set. Let us examine.","be0f496f":"There are 79 predictors.  Train set has 62 variables (that includes target and id) with no missing values, however test set has only 47 (that includes Id) with no missing values. \nFirst interesting question. What should be the threshold t on percentage of missing values on a feature(predictor variable), to say that if missing value percentage > t, discard that predictor?","68d79e5c":"![](http:\/\/)**Print the names of 46 predictors**","63417b2f":"The year built or year remod by itself does not make sense. We can convert it into how many years old by subtracting by 2018. We should examine data to see if all data are valid. We should repeat on test set too. Yr sold has to be mostly converted to certain interval.","0fa59e70":"**DATA LOADING**","1b5c87d9":"This is fine because the one extra in train set is due to target variable. Also note that 'Id' column is not a predictor.  In effect, there are equal number of variables of interest, if we choose this threshold. But there is no guarantee that in a new test set, 10% threshold will give 74 predictors! We should take care of it later if that happens.","6a4db878":"Note that after one hot conversion, the train set has 12 variables more than test set. It is because some values never appear in test set. They cause problem in prediction, hence they are removed from train set","ef1d3bbd":"We can see that adding all variables, even though one hot encoded, does increase R^2, but decreases adjusted R^2 from 0.8 to -0.54.\nThis means, the model with categorical variables, is not necessarily better than the model with just 25 numeric variables.\nBut this last model will give a better position in leaderboard.","d88a97ff":"**PREDICTION**      \nChoice of Evaluation Metric:\nWe would go with RMSE to measure the final model. To evaluate models, we can use several.","d6719cca":"1. **DATA VISUALIZATIONS**","cb26d04b":"Find Multicollinearity and remove those with above 0.95 correlation value","22e515de":"We would choose to visualize scatterplots of those with many unique values. They are the real continuous variables. Remaining are ordinal variables for which barplot is suitable","850c9cd6":"**DATA PREPARATION**","8da924cb":"[Why Encoding](https:\/\/datascience.stackexchange.com\/questions\/5226\/strings-as-features-in-decision-tree-random-forest)","f956c545":"Note that just the addition of 12 more ordinal predictors has increased both R^2 and adj R^2.","4c5d1c97":"There is good variation with respect to month, but Month is neither numeric nor ordinal variable. It is categorical by definition!\nHence it is important to take care of this later.","687f626a":"We see quite a bit of linear relationships here. Especially, GrLivArea seems to be a strong predictor. Some of the predictors have lot of 0's. It is not invalid data, such houses may not have that provision (like ScreenPorch).","0e353a6d":"For the time being we will focus on 73 out of 79. To start with, we could even try to predict only with those variables which have no missing values in both train and test set. As first step, let's analyze the data bit more and understand these first 46 predictors."}}