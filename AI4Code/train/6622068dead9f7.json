{"cell_type":{"9db1826f":"code","66c064c0":"code","b5d4b7a8":"code","0d238a05":"code","e2953cd3":"code","e3ad48ca":"code","bef8d25f":"code","dbc1a6e5":"code","216fe14d":"code","0fdcea3a":"code","395fa0d4":"code","a93ff384":"code","582853d1":"code","82b323ba":"code","b559f81f":"code","39be43b3":"code","28dc71fd":"code","bb7b33f3":"code","3ef710bb":"code","99d66fa2":"code","43639f95":"code","bceeebd6":"code","32b49183":"code","d84ba17e":"code","22db5041":"code","da4ea411":"code","f0a3877b":"code","8d6f248b":"code","f13f2851":"code","fbc0fb2a":"code","b2baedb0":"code","f3da3200":"code","17e016e4":"markdown","ce54cb18":"markdown","716551e4":"markdown","c990262f":"markdown","7ee708ed":"markdown","0701a409":"markdown","60ce6a6e":"markdown","c575714a":"markdown","686a6beb":"markdown","97bf7979":"markdown","97a21e3b":"markdown","aca9f411":"markdown","c3881839":"markdown","a1dbb5b7":"markdown","6592ea84":"markdown","8ccf302c":"markdown","ed64f1e3":"markdown","d6039a50":"markdown"},"source":{"9db1826f":"# Data analysis and cleaning\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom matplotlib import cycler\n%matplotlib inline\nimport seaborn as sns","66c064c0":"# My default visualization Styling\ncolors_default = ['#17869E', '#D35151', '#e5ae38', '#6d904f', '#264D58', '#E9DAB4']\ncolors_highlight = ['#51C4D3', '#fc4f30','#ffd966', '#9AE19D', '#008fd5']\n\ncolors = cycler('color',colors_default)\n\nstyle=plt.style.use('fivethirtyeight')\n\nplt.rc('axes', prop_cycle=colors)\nplt.rc('lines', linewidth=1)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('font', size=10)\n\n# My color Library\npastel = ['#e06666','#f9a650','#ffd966','#93c47d','#76a5af']\naft = ['#2C302E', '#474A48', '#909590', '#9AE19D', '#537A5A']\nblues = [\"#132C33\", \"#264D58\", '#17869E', '#51C4D3', '#B4DBE9']\ndarks = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\nreds = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\nearth = ['#883530','#9bb282', '#f8daa1', '#d89f01', '#b77946']\nearth2 = ['#e4da98', '#036477', '#d65b49', '#1e4437','#a9b68b']\nfivethirtyeight_style = ['#008fd5', '#fc4f30', '#e5ae38', '#6d904f', '#8b8b8b', '#810f7c']\n\nsns.palplot(colors_default)\nsns.palplot(colors_highlight)\nsns.palplot(pastel)\nsns.palplot(darks)\nsns.palplot(aft)\nsns.palplot(blues)\nsns.palplot(reds)\nsns.palplot(earth)\nsns.palplot(earth2)\nsns.palplot(fivethirtyeight_style)","b5d4b7a8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombined_df = pd.concat([train_df, test_df], ignore_index=True)\ndatasets = [train_df, test_df, combined_df]\n\ntrain_df.info()\nprint('_'*40)\ntest_df.info()\nprint('\\n')\ntrain_df.head()","0d238a05":"train_df.corr()","e2953cd3":"for dataset in datasets:\n    # Age null values will be imputed with the mean Age of each Pclass due to their high correlation\n    Age_mean = combined_df.groupby('Pclass')['Age'].mean()\n    dataset['Age'] = dataset.apply(lambda row: Age_mean[row['Pclass']]\n                                   if np.isnan(row['Age'])\n                                   else row['Age'], axis=1)\n    \n    # Note: the line below is an efficient way of completing the same task if the mean used to impute was\n    # from the same dataset that's being transformed. That is not the case here because the train and test\n    # means vary, so the combine_df means are used. Use code below if train\/test data was not already split\n    # dataset['Age'] = dataset.groupby('Pclass')['Age'].transform('mean')\n    \n    # Fare null values will be imputed with the mean Age of each Pclass due to their high correlation\n    Fare_mean = combined_df.groupby('Pclass')['Fare'].mean()\n    dataset['Fare'] = dataset.apply(lambda row: Fare_mean[row['Pclass']]\n                                   if np.isnan(row['Fare'])\n                                   else row['Fare'], axis=1)\n    \n    # Embarked null values will be imputed based on the mode\n    Mode = combined_df['Embarked'].mode()[0]\n    dataset['Embarked'].fillna(Mode, inplace=True)","e3ad48ca":"for dataset in datasets:    \n    # Crate a column indicating total family size\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    \n    # Create a column indicating if passenger has immediate family aboard\n    dataset['FamilyAboard'] = 0 # Initialize to No\/0\n    dataset['FamilyAboard'].loc[dataset['FamilySize'] > 1] = 1 # Update to Yes\/1 if family size is greater than 1\n    \n    # Create continuous variable bins. qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    # Fare Bins\/Buckets using qcut or frequency bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 5)\n    \n    #Age Bins\/Buckets using cut or value bins: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(x=train_df['Age'], bins=[0,18,30,40,50,65,85])\n   \n    # Create a column for the passenger's title -- http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n        \n# Cleanup rare title names\nstat_min = 10 # While small is arbitrary, we'll use the common minimum in statistics\n              # http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (combined_df['Title'].value_counts() < stat_min) # This will create a true\/false series with title as index\n    \n# Apply and lambda functions are quick code to find and replace with fewer lines of code\n# https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\nfor dataset in datasets:\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)","bef8d25f":"# PassengerId, Ticket, Name columns will be dropped because they are arbitrary (title already extracted)\n# Cabin column will be dropped because it is missing a high portion of data\nfor dataset in datasets:\n    dataset.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntrain_df.drop('PassengerId',axis=1, inplace=True) # Column kept in test_df for submission ","dbc1a6e5":"# Discrete Variable Correlation by Survival using group by aka pivot table:\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.groupby.html\nfor col in train_df:\n    if (train_df[col].dtype != 'float64') and (col != 'Survived'):\n        print('Survival Correlation by:', col)\n        print(train_df[[col, 'Survived']].groupby(col, as_index=False).mean().sort_values(by='Survived', ascending=False))\n        print('-'*10, '\\n')      \n        \n#using crosstabs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.crosstab.html\nprint(pd.crosstab(train_df['Title'], train_df['Survived']))","216fe14d":"# This is one way to create subplots, but the prefered method is used in all the upcoming visualizations\n\nplt.figure(figsize=[12,8])\n\nplt.subplot(231)\nsns.boxplot(x=train_df['Survived'], y=train_df['Fare'], showmeans = True, meanline = True, fliersize=4)\nplt.ylabel('Fare')\nplt.xlabel('')\nplt.xticks(ticks=[0,1], labels=['Died','Survived'])\n\nplt.subplot(232)\nsns.boxplot(x=train_df['Survived'], y=train_df['Age'], showmeans = True, meanline = True, fliersize=4)\nplt.ylabel('Age')\nplt.xlabel('')\nplt.xticks([])\n\nplt.subplot(233)\nsns.boxplot(x=train_df['Survived'], y=train_df['FamilySize'], showmeans = True, meanline = True, fliersize=4)\nplt.ylabel('Family Size')\nplt.ylim(-0.5,12.5)\nplt.xlabel('')\nplt.xticks([])\n\nplt.subplot(234)\nsns.histplot(data=train_df, x='Fare', hue='Survived', bins=30)\nplt.xlabel('Fare')\nplt.ylabel('# of Passengers')\nplt.legend(['Survived','Died'])\n\nplt.subplot(235)\nsns.histplot(data=train_df, x='Age', hue='Survived', bins=30)\nplt.ylabel('')\nplt.legend()\n\nplt.subplot(236)\nsns.histplot(data=train_df, x='FamilySize', hue='Survived', bins=10)\nplt.xlabel('Family Size')\nplt.ylabel('')\nplt.legend()\n\n# Title and subtitle\n# I guessed and checked for the x and y values. See the following examples more effecient way\n# when using the other subplots method.\nplt.subplot(231)\nplt.text(s=\"Fare, Age, and Family Size distributions based on Survival\", ha='left', x=-0.5, y=600, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1, color=darks[1]) ","0fdcea3a":"# Create Figure and subplots\nfig, axs = plt.subplots(2, 3, figsize=(12,10), sharey=True)\n\n# Graph\nsns.barplot(data=train_df, x='Embarked', y='Survived', ax=axs[0,0])\nsns.barplot(data=train_df, x='Pclass', y='Survived', ax=axs[0,1])\nsns.barplot(data=train_df, x='FamilyAboard', y='Survived', ax=axs[0,2])\nsns.pointplot(data=train_df, x='AgeBin', y='Survived', ax=axs[1,0])\nsns.pointplot(data=train_df, x='FareBin', y='Survived', ax=axs[1,1])\nsns.pointplot(data=train_df, x='FamilySize', y='Survived', ax=axs[1,2])\n\n# Format\n\naxs[0,1].set(xlabel='Passenger Class', ylabel='')\naxs[0,2].set(xlabel='Family Aboard', ylabel='')\naxs[1,0].set(xlabel='Age', \n             xticklabels=['0-17','18-29','30-39','40-49','50-64', '65+'],\n             ylim=(-0.01,0.89))\naxs[1,0].tick_params(axis='x',rotation=70)\naxs[1,1].set(ylabel='', xlabel='Fare', xticklabels=['0-7.85','7.85-10.5','10.51-21.67','21.68-39.68','39.69-512.33'])\naxs[1,1].tick_params(axis='x', rotation=70)\naxs[1,2].set(xlabel='Family Size',ylabel='')\n\n# Title and Subtitle\nxmin, xmax = axs[0,0].get_xlim()\nymin, ymax = axs[0,0].get_ylim()\n\nplt.subplot(231)\nplt.text(s=\"Survival Rates by Feature\", x=xmin, y=ymax*1.1, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1, color=darks[1]) \n# can also use:   axs[0,0].set(title ='')","395fa0d4":"fig, axs = plt.subplots(1,3,figsize=(12,6))\n\nsns.boxplot(data=train_df, x='Pclass', y='Fare', hue='Survived', ax=axs[0])\naxs[0].set(xlabel='Passanger Class', ylim=(-5,525))\nplt.legend(['Survived','Died'])\n\nsns.violinplot(data=train_df, x='Pclass', y='Age', hue='Survived', split=True, ax=axs[1])\naxs[1].set(xlabel='', ylim=(-1,100.5))\naxs[1].legend('')\n\nsns.boxplot(data=train_df, x='Pclass', y='FamilySize', hue='Survived', ax=axs[2])\naxs[2].set(xlabel='', ylim=(-0.1,12.1))\naxs[2].legend('')\n\n# Title and Subtitle\nxmin, xmax = axs[0].get_xlim()\nymin, ymax = axs[0].get_ylim()\n\nplt.subplot(131)\nplt.text(s='Fare, Age, and Family Size distributions based on Passenger Class',\n         x=xmin, y=ymax*1.18, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1.1, color=darks[1]) ","a93ff384":"fig, axs = plt.subplots(1, 3, figsize=(12,6), sharey=True)\n\nsns.barplot(data=train_df, x='Pclass', y='Survived', hue='Sex', ax=axs[0])\naxs[0].set(xlabel='Passenger Class')\n\nsns.barplot(data=train_df, x='FamilyAboard', y='Survived', hue='Sex', ax=axs[1])\naxs[1].set(xlabel='Family Aboard', ylabel='')\naxs[1].legend('')\n\nsns.barplot(data=train_df, x='Embarked', y='Survived', hue='Sex', ax=axs[2])\naxs[2].set(ylabel='')\naxs[2].legend('')\n\n# Title and Subtitle\nxmin, xmax = axs[0].get_xlim()\nymin, ymax = axs[0].get_ylim()\n\nplt.subplot(131)\nplt.text(s='Survival Rates by Passenger Class, Family Aboard, and Embarked',\n         x=xmin, y=ymax*1.1, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1.04, color=darks[1]) ","582853d1":"fig, axs = plt.subplots(1,2, figsize=(15,6), gridspec_kw={'width_ratios':[1,3]}, sharey=True)\n\nsns.pointplot(data=train_df, x='FamilySize', y='Survived', hue='Sex', ax=axs[0])\naxs[0].set(xlabel='Family Size')\n\nsns.barplot(data=train_df, x='AgeBin', y='Survived', hue='Sex', ax=axs[1])\naxs[1].set(xlabel='Age', ylabel='', \n             xticklabels=['0-17','18-29','30-39','40-49','50-64', '65+'],\n             ylim=(-0.01,0.89))\naxs[1].tick_params(axis='x', rotation=30)\naxs[1].legend('')\n\n# Title and Subtitle\nxmin, xmax = axs[0].get_xlim()\nymin, ymax = axs[0].get_ylim()\n\nplt.subplot(121)\nplt.text(s='Survival Rates by Age',\n         x=xmin, y=ymax*1.1, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1.04, color=darks[1]) ","82b323ba":"grid = sns.FacetGrid(train_df, col='Pclass', row='Sex', hue='Survived')\nplt.figure(figsize=(14,8))\ngrid.map(plt.hist, 'Age', bins=20, alpha=0.7)\ngrid.add_legend()","b559f81f":"# Raincloud plots\n# https:\/\/wellcomeopenresearch.org\/articles\/4-63\/v2\n# The above expresses the benefit of rainclouds and excellent code examples. \n# The plot will not display any new\/informative information at this point, but serves as rainplot reference code\n!pip install --upgrade ptitprince\nimport ptitprince as pt","39be43b3":"# Rainplot\nf, ax = plt.subplots(figsize=(10, 8))\n\nax=pt.half_violinplot(data=train_df, x='Age', y='Pclass', palette=colors_default, orient='h', alpha=0.7,\n                      bw=.2, linewidth=1,cut=0., scale=\"area\", width=.8, inner=None)\nax=sns.stripplot(data=train_df, x='Age', y='Pclass', palette=colors_default, orient='h',\n                 edgecolor=\"white\",size=2,jitter=1,zorder=0)\nax=sns.boxplot(data=train_df, x='Age', y='Pclass', color=\"black\",orient='h',\n               width=.15,zorder=10,showcaps=True,boxprops={'facecolor':'none', \"zorder\":10},\n               showfliers=False,whiskerprops={'linewidth':2, \"zorder\":10},saturation=1)\n\n# Title and Subtitle\nxmin, xmax = ax.get_xlim()\nymin, ymax = ax.get_ylim()\n\nplt.text(s='Age Distribution by Passenger Class',\n         x=xmin, y=ymax*1.25, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1, color=darks[1]) ","28dc71fd":"# Below is a more automated version, but I haven't figured out how to finetune it as well as I'd like\n# for example, I havan't been able to remove the boxplot fliers\nf, axs = plt.subplots(figsize=(10, 8))\naxs=pt.RainCloud(data=train_df, x='Survived', y='Age', alpha=0.7,\n                bw=0.2, ax = axs, orient = 'h', palette=colors_default, dodge=True)\n\n# Title and Subtitle\nxmin, xmax = axs.get_xlim()\nymin, ymax = axs.get_ylim()\n\nplt.text(s='Age Distribution by Survival',\n         x=xmin, y=ymax*1.15, fontsize=18, fontweight='bold', color=darks[1])\nplt.title(\"Write more title text here\", loc='left', fontsize=12, y=1, color=darks[1]) ","bb7b33f3":"plt.figure(figsize=(10,8))\nsns.heatmap(train_df.corr(), \n            cmap = sns.diverging_palette(10, 220, as_cmap=True, l=45, s=90), \n            annot=True, \n            square=True,\n            vmin=-1,\n            vmax=1,\n            cbar_kws={'shrink':.75 },\n            linewidths=0.08, linecolor='white')","3ef710bb":"#from sklearn.preprocessing import OneHotEncoder, LabelEncoder (alternative to pd.get_dummies)\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import RFECV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb \n\nfrom sklearn import metrics","99d66fa2":"# Create dummy variables for categorical columns\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df[['Sex','Embarked','Title']], drop_first=True)], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df[['Sex','Embarked','Title']], drop_first=True)], axis=1)\n\n# Remove original categorical columns\ntrain_df.drop(['Sex', 'Embarked', 'Title'], axis=1, inplace=True)\ntest_df.drop(['Sex', 'Embarked', 'Title'], axis=1, inplace=True)\n\n# Drop bin columns used for visualization. This data is already present in numerical columns\ntrain_df.drop(['AgeBin', 'FareBin'], axis=1, inplace=True)\ntest_df.drop(['AgeBin', 'FareBin'], axis=1, inplace=True)","43639f95":"# Training data will be used to fit models\n# Validation data will be used to gather metrics for each model\n# test_df is the data used for final submission predictions\nX=train_df.drop('Survived', axis=1)\ny=train_df['Survived']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=101)","bceeebd6":"# Create RFC instance\nrfc = RandomForestClassifier(random_state=101)\n\n# Fit model to training data\nrfc.fit(X,y)\n\n# Predict validation data\nrfc_predict = rfc.predict(X_val)\n\n# Use GridSearchCV to improve Random Forest Classifier parameters\nrfc_params = {'n_estimators':[100, 150, 200],\n              'criterion': ['gini', 'entropy'],  \n              'max_depth': [4,6,8,10,None],\n              'min_samples_split': [2,5,10,20],\n              'random_state': [101]}\n\nrfc_tuned = GridSearchCV(RandomForestClassifier(random_state=101),rfc_params,verbose=1, cv=5)\nrfc_tuned.fit(X, y)\n\n# Predict and print best parameters\nprint('RFC Best Parameters:', rfc_tuned.best_params_, '\\n')\nrfc_tuned_predict = rfc_tuned.predict(X_val)\n\n# Random Forest Classifier with Feature Selection\n### This is not particularly helpful with random forest classifiers because the model determines \n### feature importance based on the 'gini' and 'entropy' criterion.\nrfc_RFECV = RFECV(rfc, step = 1, scoring = 'accuracy')\nrfc_RFECV.fit(X, y)\nselected_features = X.columns.values[rfc_RFECV.get_support()]\n\n# Adjust parameters for RFC model after Feature Selection, then predict\nrfc_featured = GridSearchCV(RandomForestClassifier(random_state=101),rfc_params,verbose=1, cv=5)\nrfc_featured.fit(X[selected_features], y)\nrfc_featured_predict = rfc_featured.predict(X_val[selected_features])\n\n# Print best parameters and selected features\nprint('RFC_Featured Best Parameters:', rfc_featured.best_params_, '\\n')\nprint('Selected Features:', selected_features)","32b49183":"# To visualize an individual decision tree, use graphviz\n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html","d84ba17e":"## An alternative to [selected_features]:\n# from sklearn.feature_selection import SelectFromModel\n\n## Create a random forest classifier\n# rfc = RandomForestClassifier(n_estimators=1000, random_state=101, n_jobs=-1)\n\n## Train the classifier\n# rfc.fit(X_train1, y_train1)\n\n## Print the feature name and gini importance of each feature\n# for feature in zip(X_train1.columns.values, rfc.feature_importances_):\n#    print(feature)\n\n## Create a selector object that will use the random forest classifier to identify features\n## that have an importance of more a threashold determined by the previously printed feature importance\n# sfm = SelectFromModel(rfc, threshold=0.15)\n\n## Train the selector\n# sfm.fit(X_train1, y_train1)\n\n## Print the names of the most important features\n# for feature_list_index in sfm.get_support(indices=True):\n#    print(X_train1.columns.values[feature_list_index])\n\n## Transform the data to create a new dataset containing only the most important features\n## Note: We have to apply the transform to both the training X and test X data.\n# X_important_train1 = sfm.transform(X_train1)\n# X_important_train2 = sfm.transform(X_train2)\n# X_important_test = sfm.transform(X_test)\n\n## Create a new random forest classifier for the most important features\n# rfc_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n\n## Train the new classifier on the new dataset containing the most important features\n# rfc_important.fit(X_important_train1, y_train)","22db5041":"# Use GridSearchCV to identify effective SVC parameters\nsvc_params = {'C':[0.1,1,10,100,1000, 10000], 'gamma':[1,0.1,0.01,0.001,0.0001]}\nsvc_grid = GridSearchCV(SVC(),svc_params,verbose=1, cv=5)\nsvc_grid.fit(X,y)\n\n# Predict and Print Metrics\nprint('SVC Best Parameters:', svc_grid.best_params_, '\\n')\nsvc_predict = svc_grid.predict(X_val)","da4ea411":"# Notes:\n# XGBoost can handle missing values if they are not dealt with in preprocessing\n# XGBoost cannot handle categorical features\n# XBG library is imported as xgb, so be careful not to name anything else xgb\n\n# Transform data to DMatrix structure\n# \"XGBoost provides a way to convert our training and testing data into DMatrix.\n# DMatrix is an optimized data structure that provides better memory efficiency and training speed.\"\nd_train = xgb.DMatrix(data=X, label=y)\nd_test = xgb.DMatrix(data=X_val, label=y_val)\n\n# First obtaining a benchmark xgb model that has not been parameterized\n# Get default parameters\nxgb_class = xgb.XGBClassifier()\nxgb_class.fit(X, y)\n\n# Extract default parameters from benchmark model\ndefault_params = {}\nxgb_params = xgb_class.get_params()\n\n# Default parameters need to be wrapped in lists so GridSearchCV can take them as inputs\nfor key in xgb_params.keys():\n    xgb_p = xgb_params[key]\n    default_params[key] = [xgb_p]\n\n# GridSearch is not actually searching paramaters because given params are only single (default) values\n# however, CV is still performed\nxgb_clf = GridSearchCV(estimator=xgb_class,\n                         scoring='accuracy', \n                         param_grid=default_params,\n                         return_train_score=True,\n                         verbose=1,\n                         cv=4)\n\nxgb_clf.fit(X, y)\n\n# Print best params\nprint(xgb_clf.best_params_)\n\n# Results as dataframe\nxgb_clf_df = pd.DataFrame(xgb_clf.cv_results_)\n\n# Predict\nxgb_clf_predict = xgb_clf.predict(X_val)\n\n# This is another way to cross validate with early stopping\n# I *think* this is not meaningful if you are already going to perform any sort of gridsearchcv\nxgb_cv = xgb.cv(dtrain=d_train,\n                 params=xgb_params,\n                 nfold=5,\n                 num_boost_round=500, \n                 early_stopping_rounds=10, \n                 metrics='error', \n                 as_pandas=True,\n                 verbose_eval=10,\n                 seed=101)\n\n# Compare metrics from GridSeach and xgb.cv\nprint('\\nBest Parameters:\\n',xgb_clf.best_params_)\nprint('\\nGridSearchCV Metrics:\\n', xgb_clf_df)\nprint('\\nxgb.cv Metrics:\\n', xgb_cv)","f0a3877b":"# Tune XGBoost Parameters\n\n# Set grid of selected parameters for iteration\nparam_grid = {'gamma': [0,0.5,1,50,100],\n              'learning_rate': [0.01, 0.03, 0.6, 1],\n              'max_depth': [4,5,None],\n              'n_estimators': [500],\n              'reg_alpha': [0,0.5,1,25,50],\n              'reg_lambda': [0,0.5,1,25,50],\n              'eval_metric':['logloss']}\n\n# Make sure all best parameters from benchmark model are included in the gridsearch\nfor key in xgb_clf.best_params_:\n    if key not in param_grid.keys():\n        param_grid[key]= [xgb_clf.best_params_[key]] # Note param is wrapped in a list\n    else: \n        if xgb_clf.best_params_[key] not in param_grid[key]:\n            param_grid[key].append(xgb_clf.best_params_[key])\n\n# GridSearchCV       \nxgb_tuned = GridSearchCV(estimator=xgb_class,\n                         scoring='accuracy', \n                         param_grid=param_grid,\n                         return_train_score=True,\n                         verbose=1,\n                         cv=3)\n\n# Fit\nxgb_tuned.fit(X,y)\n\n# Results as dataframe\nxgb_tuned_df = pd.DataFrame(xgb_tuned.cv_results_)\n\n# Predict\nxgb_tuned_predict = xgb_tuned.predict(X_val)","8d6f248b":"param_grid","f13f2851":"print('RFC Report:\\n\\n',\n      metrics.classification_report(y_val, rfc_predict),\n      '-'*60, '\\n')\n\nprint('RFC Tuned Report:\\n\\n',\n      metrics.classification_report(y_val, rfc_tuned_predict),\n      '-'*60, '\\n')\n\nprint('RFC Report After Feature Selection:\\n\\n',\n      metrics.classification_report(y_val, rfc_featured_predict),\n      '-'*60, '\\n')\n\nprint('SVC Report:\\n\\n',\n      metrics.classification_report(y_val, svc_predict),\n      '-'*60, '\\n')\n\nprint('XGB Report:\\n\\n',\n      metrics.classification_report(y_val, xgb_clf_predict),\n      '-'*60, '\\n')\n\nprint('XGB Tuned Report:\\n\\n',\n      metrics.classification_report(y_val, xgb_tuned_predict),\n      '-'*60, '\\n')\n\n# I have not yet determined why the tuned rfc did not perform as well as the rfc, \n# and same for the XGB (especially since all parameters are included in the gridsearch)\n# Is something wrong with the RFC and XGB models? ","fbc0fb2a":"!pip install --upgrade pycaret-nightly\nfrom pycaret.classification import *","b2baedb0":"auto_setup = setup(data = train_df,\n             target = 'Survived',\n             normalize = True,\n             transformation = True,\n             remove_multicollinearity = True,\n             normalize_method='robust',\n             session_id=42,\n             train_size = 0.8,\n             pca = True, \n             pca_components = 0.95,\n             fix_imbalance = True,\n             numeric_features=['Age', 'Fare', 'FamilySize'])\n\nauto_models = compare_models(sort = 'AUC') #This will return the best model based on AUC ","f3da3200":"submission_rfc_predict = rfc.predict(test_df.drop('PassengerId', axis=1))\nsubmission_rfc = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': submission_rfc_predict})\nsubmission_rfc.to_csv('submission_rfc.csv', index=False)\n\nsubmission_xgb_predict = rfc.predict(test_df.drop('PassengerId', axis=1))\nsubmission_xgb = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': submission_xgb_predict})\nsubmission_xgb.to_csv('submission_xgb.csv', index=False)\n\nsubmission_lr_predict = auto_models.predict(test_df.drop(['PassengerId','Age', 'Fare', 'FamilySize'], axis=1))\nsubmission_lr = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': submission_lr_predict})\nsubmission_lr.to_csv('submission_lr.csv', index=False)","17e016e4":"**Split Data**","ce54cb18":"Much of the following EDA and data preprocessing was guided https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#Step-3:-Prepare-Data-for-Consumption for learning purposes. Many other resources were utilized as well, several are referenced throughout","716551e4":"### **Random Forest Classifier Model**","c990262f":"**Create:** Feature Engineering for train and test\/validation dataset","7ee708ed":"# 2. Data Cleaning","0701a409":"**Automated ML Library to obtain baseline models**\n\nIt's probably worth doing this prior to other model tuning, but this is a new library to me","60ce6a6e":"# 3. Exploratory Data Analysis","c575714a":"**Predict On Test Data and Submit**","686a6beb":"# 4. Model and Predict Data","97bf7979":"### **Support Vector Classifier Model**","97a21e3b":"**Convert Data**","aca9f411":"Some visualization inspiration:\n* https:\/\/fivethirtyeight.com\/features\/the-52-best-and-weirdest-charts-we-made-in-2016\/\n* https:\/\/fivethirtyeight.com\/features\/the-56-best-and-weirdest-charts-we-made-in-2019\/\n* https:\/\/fivethirtyeight.com\/features\/the-40-weirdest-and-best-charts-we-made-in-2020\/\n\nfivethirtyeight default styling sheet: https:\/\/github.com\/matplotlib\/matplotlib\/blob\/38be7aeaaac3691560aeadafe46722dda427ef47\/lib\/matplotlib\/mpl-data\/stylelib\/fivethirtyeight.mplstyle\n\nExample of making a simple fivethirtyeight style\/quality figure:\nhttps:\/\/www.dataquest.io\/blog\/making-538-plots\/\n\nDocumentation for customizing with stylesheets and rcParams:\nhttps:\/\/matplotlib.org\/stable\/tutorials\/introductory\/customizing.html\n","c3881839":"Check out lollipop charts to replace barplots with many bars.\n\nCheck out rain plots to help communicate more detailed distribution than histograms or boxplots.","a1dbb5b7":"# 1. Import Libraries and Acquire Data","6592ea84":"**Compare Accuracy Metrics**","8ccf302c":"**Import Libraries**","ed64f1e3":"### **XGBoost Model**\nXGBoost resources: \n\nhttps:\/\/www.kaggle.com\/prashant111\/xgboost-k-fold-cv-feature-importance\n\nhttps:\/\/towardsdatascience.com\/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nhttps:\/\/debuggercafe.com\/a-guide-to-xgboost-in-python\/","d6039a50":"**Complete:** Impute or delete missing values in train and test\/validation dataset"}}