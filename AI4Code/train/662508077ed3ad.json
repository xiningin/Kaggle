{"cell_type":{"671fd616":"code","4fa58c8f":"code","76372e93":"code","1f38aa0a":"code","2e2e360d":"code","bb78e8c8":"code","2304a0e0":"code","26224aaa":"code","7203a121":"code","f817af26":"code","6dcc9bc9":"code","e2b7ce4d":"code","48fde3b3":"code","6119bdb5":"code","0656bc2b":"code","3ed552e0":"markdown","d8e9ca56":"markdown","ca565019":"markdown","a8287380":"markdown","d59f9800":"markdown","74a96e64":"markdown","1283c4e2":"markdown","13959297":"markdown","1ae4986e":"markdown","bb0091cb":"markdown","a41ac287":"markdown","8aabcd47":"markdown","189ef1f6":"markdown","ffe4061a":"markdown","e8bc6fcf":"markdown","ed46fd74":"markdown"},"source":{"671fd616":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nrandom_seed = 0\nnp.random.seed(random_seed)\n\n# Load CSV as pandas data frame:\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\n\n# Visualize the dataframe structure and data types:\ndf.head()","4fa58c8f":"# Get a general description of the dataset:\nN = df.shape[0]\nprint(f\"Total number of transactions: {N}\")\n\nNf = sum(df['Class']==1)\nprint(f\"Number of fraud transactions: {Nf} ({Nf\/N*100:.2} % of the total)\")\n\nprint(\"\\n\\nFurther information about this dataset:\\n\")\n#df.info() # Here we can see there are no NULL elements, and all data types are float, except for the 'Class' field\nprint(f\"The column with more NULL values has {df.isnull().sum().max()} NULL values.\")\n\ndf.describe() # Show statistical information: min, max, mean, std, and percentiles of each feature","76372e93":"# Get statistical information about the 'Amount' feature for each class:\nprint(\"\\n\\nStatistical description per class:\")\nprint(df.groupby('Class')['Amount'].agg(['min', 'max', 'median', 'mean', 'std']))\n# At first sight, the amount feature does not seem to provide much information regarding the type of transaction\n# Fraud transactions are characterized by more or less the same distribution of amounts","1f38aa0a":"# Normalize time feature, convert to hours:\ndf['Time'] = df['Time'].apply(lambda x : x \/ 3600)\nprint(round(df['Time'].max()))  # Assuming time is measured in seconds, the dataset span 48 hours of transactions:\n\n# Let's see if fraud transactions are time-related:\nprint(df.groupby('Class')['Time'].agg(['min', 'max', 'median', 'mean', 'std']))\n\n# Let's map into two 24-hour periods:\ndf['Time'] = df['Time'].apply(lambda x : x % 24)\n\n# Let's see if fraud transactions are time-related:\nprint(df.groupby('Class')['Time'].agg(['min', 'max', 'median', 'mean', 'std']))\n# It seems that non-fraud and fraud transactions may occur in any moment of the day","2e2e360d":"X_non_fraud_balanced = df[df['Class'] == 0].sample(Nf)  # Get a random sample of non-fraud transactions (balanced 50%-50%)\nX_fraud_balanced = df[df['Class'] == 1]\n\ndf_balanced = X_non_fraud_balanced.append(X_fraud_balanced).sample(frac=1).reset_index(drop=True) # Shuffle\nX_balanced = df_balanced.drop(['Class'], axis = 1).values\ny_balanced = df_balanced[\"Class\"].values\n\n# Normalization:\nfrom sklearn import preprocessing\nX_balanced = preprocessing.MaxAbsScaler().fit_transform(X_balanced)","bb78e8c8":"from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\ndef tsne_plot(x, y):\n    # Visualize high-dimensional data. It converts similarities between data points to joint probabilities \n    # and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional \n    # embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different \n    # initializations we can get different results. (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html)\n    # t-distributed Stochastic Neighbor Embedding (no need for dimensionality reduction, since the number of features is less than 50)\n    tsne = TSNE(n_components=2, random_state=random_seed) \n    \n    X_tsne = tsne.fit_transform(x)\n\n    plt.figure(figsize=(8, 8))\n    plt.scatter(X_tsne[np.where(y == 0), 0], X_tsne[np.where(y == 0), 1], color='g', label='Non Fraud')\n    plt.scatter(X_tsne[np.where(y == 1), 0], X_tsne[np.where(y == 1), 1], color='r', label='Fraud')\n\n    plt.legend(loc='best')\n    plt.show()\n    \ndef pca_plot(x, y):\n    # Principal component analysis (PCA).\n    # Linear dimensionality reduction using Singular Value Decomposition of the data to project it \n    # to a lower dimensional space. The input data is centered but not scaled for each feature before applying \n    # the SVD. (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n    pca = PCA(n_components=2, random_state=random_seed)\n    X_pca = pca.fit_transform(x)\n    \n    plt.figure(figsize=(8, 8))\n    plt.scatter(X_pca[np.where(y == 0), 0], X_pca[np.where(y == 0), 1], color='g', label='Non Fraud')\n    plt.scatter(X_pca[np.where(y == 1), 0], X_pca[np.where(y == 1), 1], color='r', label='Fraud')\n\n    plt.legend(loc='best')\n    plt.show()\n    \ntsne_plot(X_balanced, y_balanced)\npca_plot(X_balanced, y_balanced)\n# In these scatter plots, we see that about a half of the frauds are quite different from normal transactios,\n# while the other half is more difficult to distinguish","2304a0e0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nX_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=random_seed)\n\nlog_reg_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\nlog_reg_model.fit(X_train_b, y_train_b)\n\ny_pred_b = log_reg_model.predict(X_test_b)\n\nprint (\"\")\nprint (\"Classification Report: \")\nprint (classification_report(y_test_b, y_pred_b))\n\nprint (\"\")\nprint (f\"Accuracy Score: {accuracy_score(y_test_b, y_pred_b)*100:.3} %\")","26224aaa":"from sklearn.inspection import permutation_importance\n\ntotal_importance = 90.0\n\nr = permutation_importance(log_reg_model, X_balanced, y_balanced, n_repeats=30, random_state=random_seed)\n\nimp_sum = 0\nprint('Feature importance:')\nmost_important_features = []\nfor i in r.importances_mean.argsort()[::-1]:\n    imp_sum += r.importances_mean[i]\n    print(f\"{df.columns[i]:<10}\", f\"{r.importances_mean[i]*100:.1f}\", f\" +\/- {r.importances_std[i]*100:.1f} %\")\n    if (imp_sum <= total_importance\/100) and (r.importances_mean[i]-r.importances_std[i]) > 0:\n        most_important_features.append(df.columns[i])\n        \nprint(f\"\\nThe most important features in this dataset are: \\n{most_important_features}\")","7203a121":"# Let's keep only the most important features:\nmost_important_features_plus_class = most_important_features[:]\nmost_important_features_plus_class.append('Class')\ndf_balanced_important = df_balanced[most_important_features_plus_class]\n\nX_balanced_important = df_balanced_important.drop(['Class'], axis = 1).values\ny_balanced_important = df_balanced_important[\"Class\"].values","f817af26":"from sklearn.tree import DecisionTreeClassifier\n\n\nX_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_balanced_important, y_balanced_important, test_size=0.2, random_state=random_seed)\n\ndec_tree_model = DecisionTreeClassifier()\ndec_tree_model.fit(X_train_b, y_train_b)\n\ny_pred_b = dec_tree_model.predict(X_test_b)\n\nprint (\"\")\nprint (\"Classification Report: \")\nprint (classification_report(y_test_b, y_pred_b))\n\nprint (\"\")\nprint (f\"Accuracy Score: {accuracy_score(y_test_b, y_pred_b)*100:.3} %\")","6dcc9bc9":"from sklearn import preprocessing\n\n# Get the most important features from all dataset, normalize them:\nX_all = df.drop([\"Class\"], axis=1)\nX_all_imp = X_all[most_important_features]\ny_all_imp = df[\"Class\"].values\n\nX_all_imp_scale = preprocessing.MaxAbsScaler().fit_transform(X_all_imp.values)\nX_all_normal_imp_scale = X_all_imp_scale[y_all_imp == 0]\nX_all_fraud_imp_scale = X_all_imp_scale[y_all_imp == 1]","e2b7ce4d":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras import regularizers\n\n## Define model architecture:\n# input layer \ninput_layer = Input(shape=(X_all_imp_scale.shape[1],))\n\n# encoding layers\nencoded = Dense(50, activation='sigmoid', activity_regularizer=regularizers.l2(10e-5))(input_layer)\nencoded = Dense(20, activation='relu')(encoded)\n\n# decoding layers\ndecoded = Dense(20, activation='sigmoid')(encoded)\ndecoded = Dense(50, activation='sigmoid')(decoded)\n\n# output layer\noutput_layer = Dense(X_all_imp_scale.shape[1], activation='relu')(decoded)\n\n## Compile model:\nautoencoder = Model(input_layer, output_layer)\nautoencoder.compile(optimizer=\"adadelta\", loss=\"mse\")\n\n## Fit model:\nautoencoder.fit(X_all_normal_imp_scale, X_all_normal_imp_scale, \n                batch_size = 256, epochs = 10, \n                shuffle = True, validation_split = 0.20);","48fde3b3":"from keras.models import Sequential\n\n# Actual classifier, using the first layers (encoders) of the pretrained model:\nhidden_representation = Sequential()\nhidden_representation.add(autoencoder.layers[0])\nhidden_representation.add(autoencoder.layers[1])\nhidden_representation.add(autoencoder.layers[2])\n\nnorm_hid_rep = hidden_representation.predict(X_all_normal_imp_scale[:Nf]) # Balanced dataset\nfraud_hid_rep = hidden_representation.predict(X_all_fraud_imp_scale)\n\nrep_X = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)\ny_normal = np.zeros(norm_hid_rep.shape[0])\ny_fraud = np.ones(fraud_hid_rep.shape[0])\nrep_y = np.append(y_normal, y_fraud)\n\ntsne_plot(rep_X, rep_y)\npca_plot(rep_X, rep_y)","6119bdb5":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(rep_X, rep_y, test_size=0.2, random_state=random_seed)\ndec_tree_model1 = DecisionTreeClassifier()\ndec_tree_model1.fit(X_train, y_train)\ny_pred = dec_tree_model1.predict(X_test)\n\nprint (\"\")\nprint (\"Classification Report: \")\nprint (classification_report(y_test, y_pred))\n\nprint (\"\")\nprint (f\"Accuracy Score: {accuracy_score(y_test, y_pred)*100:.3} %\")","0656bc2b":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import auc\n\n\naverage_precision = average_precision_score(y_test, y_pred)\n\nprint(f'Average precision-recall score: {average_precision*100:0.3} %')\n\ndisp = plot_precision_recall_curve(dec_tree_model1, X_test, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\narea = auc(recall, precision)\nprint(f\"Area Under the Precision-Recall Curve = {area*100:.3} %\")","3ed552e0":"# Baseline classifier: logistic regression\n\nAfter dimensionality reduction, it seems that most of the fraud transactions can be easily separated from non-fraud transactions using a straight line. Let's consider a simple model as baseline: logistic regression.","d8e9ca56":"# Credit Card Fraud Detection\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nIn this project, we are asked to classify credit card transactions as fraudulent or genuine, learning from a large and highly unbalanced dataset with anonymized transactions made by credit cards in September 2013 by european cardholders.\n\nThe original challenge can be found here: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud","ca565019":"## Area Under the Precision-Recall Curve\nFinally, let's analyze the Precision-Recall Curve and measure the Area Under the Precision-Recall Curve, as recommended by the author of this challenge.","a8287380":"After removing noise features, we have a better accuracy, above 95 %.","d59f9800":"# Extract low-level features: autoencoder\nAn autoencoder is neural network that we train to perform regression on the input data by extracting low-level features. This helps to better characterize our classes.","74a96e64":"# More details about the dataset","1283c4e2":"# First glance to our dataset","13959297":"## Decision tree classifier\nLet's try a non-linear model for our dataset using only the most important features.","1ae4986e":"# Visualize in two dimensions to get a first glance of the classification problem. \n## Dimensionality reduction: t-SNE and PCA","bb0091cb":"Let's take a first glance to our dataset, ","a41ac287":"Excellent! In the validation set, we reach about 97.5 %.","8aabcd47":"# Is the amount of money a decisive feature to detect a fraud?","189ef1f6":"# Is the time of the day a decisive feature to detect a fraud?","ffe4061a":"Look how well separated are our classes. We expect a higher accuracy using the Decision Tree model of the previous section.","e8bc6fcf":"# Tackle unbalanced classes","ed46fd74":"Awesome! Just as expected, the accuracy in the validation set is very high, above 90 %.\nThat is a great start. Now, let's try to improve that result. \n\n## Feature importance\nThere are 30 features in this dataset, probably some of them are just noise. Let's estimate the importance of each one, and select just the features that explain most of the classification."}}