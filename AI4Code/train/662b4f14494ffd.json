{"cell_type":{"cc50befc":"code","7b46d234":"code","a3b0570e":"code","46b42028":"code","82b62584":"code","bebac874":"code","a5b3a5ac":"code","d2f74855":"code","582958fd":"code","6bdfab17":"code","e5732824":"markdown"},"source":{"cc50befc":"import pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split","7b46d234":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","a3b0570e":"labels = df['label'].values\nfeatures = df[list(df.columns)[1:]].values.astype('float32') \/ 255.\nfeatures = features.reshape(-1, 28, 28, 1)\nlabels = np.eye(10)[labels]\nprint(labels.shape, features.shape)","46b42028":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)","82b62584":"def define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","bebac874":"def evaluate_model(dataX, dataY, n_folds=5):\n    scores, histories = list(), list()\n    kfold = KFold(n_folds, shuffle=True, random_state=1)\n\n    for train_ix, test_ix in kfold.split(dataX):\n        model = define_model()\n\n        trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n        history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n        \n        _, acc = model.evaluate(testX, testY, verbose=0)\n        print('> %.3f' % (acc * 100.0))\n\n        scores.append(acc)\n        histories.append(history)\n    return scores, histories","a5b3a5ac":"scores, histories = evaluate_model(X_train, y_train)","d2f74855":"def summarize_diagnostics(histories):\n    for i in range(len(histories)):\n        # plot loss\n        pyplot.subplot(2, 1, 1)\n        pyplot.title('Cross Entropy Loss')\n        pyplot.plot(histories[i].history['loss'], color='blue', label='train')\n        pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')\n        # plot accuracy\n        pyplot.subplot(2, 1, 2)\n        pyplot.title('Classification Accuracy')\n        pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')\n        pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n    pyplot.show()\n\ndef summarize_performance(scores):\n    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))\n    pyplot.boxplot(scores)\n    pyplot.show()","582958fd":"summarize_diagnostics(histories)","6bdfab17":"summarize_performance(scores)","e5732824":"# MNIST"}}