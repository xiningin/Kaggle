{"cell_type":{"d282a190":"code","34d9d20e":"code","a1bcc246":"code","189c0999":"code","ce58a863":"code","490abce8":"code","327ed7b9":"code","40e71ffd":"code","2b26c365":"code","d141897f":"code","bc08180b":"code","08de2631":"code","b5756eb0":"code","35c6dfda":"code","c35813fc":"code","f1918718":"code","1236701f":"code","2aa92983":"code","68a9222c":"code","29c68a24":"code","6935ad50":"code","ed59f459":"code","842d75df":"code","87d52acc":"code","2ef31b41":"code","5dda2149":"code","a2a667d0":"code","72de9fda":"code","36abab11":"code","c1c28d91":"code","16d46865":"code","35d44133":"code","823efb78":"code","cccaf74d":"code","cd6f72e8":"code","9a075169":"code","6ff824ea":"code","8a20e11c":"code","670ce4b9":"code","f0a99b3d":"code","45a1aafb":"code","109857c8":"code","862f65f9":"code","0557456f":"code","d554541b":"code","1077762e":"code","71dbd745":"code","14323587":"markdown","732638a3":"markdown","a828464b":"markdown","8df99641":"markdown","b2e1e197":"markdown","04c3a6d4":"markdown","e850190a":"markdown","290ce9a0":"markdown","d43fa1ca":"markdown","3bc2b094":"markdown","22fda058":"markdown","8def6546":"markdown","41d5ca35":"markdown","3a1d19b3":"markdown","91becd69":"markdown"},"source":{"d282a190":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")        ","34d9d20e":"data=pd.read_csv(\"\/kaggle\/input\/hr-employee-attrition\/HR-Employee-Attrition.csv\")","a1bcc246":"data.head(10)","189c0999":"data.info()","ce58a863":"data.describe() ","490abce8":"f,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","327ed7b9":"# count of people doing overtime\nsns.countplot(data['OverTime'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('OverTime')","40e71ffd":"#count of people based on their marital status\nsns.countplot(data['MaritalStatus'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('Marital Status')","2b26c365":"#count of people based on their Jobrole\nsns.countplot(data['JobRole'])\nfig = plt.gcf()\nfig.set_size_inches(20,14)\nplt.title('Job Role')","d141897f":"#count of people based on their Gender\nsns.countplot(data['Gender'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('Gender')","bc08180b":"#count of people based on their field of education\nsns.countplot(data['EducationField'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('Education Field')","08de2631":"#count of people based on the department in which they work\nsns.countplot(data['Department'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('Department')","b5756eb0":"#count of people based on their Travel Frequency\nsns.countplot(data['BusinessTravel'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nplt.title('Business travel')","35c6dfda":"# analyzing the age group of the peole doing overtime\nsns.catplot(x=\"OverTime\", y=\"Age\", kind=\"swarm\", data=data)","c35813fc":"#count of people based on their total working years\nplt.subplots(figsize=(15,5))\nsns.countplot(data.TotalWorkingYears)","f1918718":"#count of people based on their Education\nplt.subplots(figsize=(15,5))\nsns.countplot(data.Education)","1236701f":"#count of people based on the number of companies they have worked for\nsns.countplot(data.NumCompaniesWorked)","2aa92983":"#count of people based on the distance from home they are working\nplt.subplots(figsize=(18,5))\nsns.countplot(data.DistanceFromHome)","68a9222c":"# necessary imports\nimport time\nimport math\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport scipy as sci\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt import fmin\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight') \n%matplotlib inline","29c68a24":"ibm_df = data # copying data to ibm_df\nibm_df.head()","6935ad50":"description = pd.DataFrame(index=['observations(rows)', 'percent missing', 'dtype', 'range']) # creating a dataframe with custom indices","ed59f459":"# initializing empty lists\nnumerical = []\ncategorical = []\n\n\nfor col in ibm_df.columns:\n    obs = ibm_df[col].size\n    p_nan = round(ibm_df[col].isna().sum()\/obs, 2) # caluclating the number of missing values\n    num_nan = f'{p_nan}% ({ibm_df[col].isna().sum()}\/{obs})' # caluclating the number of missing values\n    dtype = 'categorical' if ibm_df[col].dtype == object else 'numerical' # defining the criteria for numerical and categorical columns\n    numerical.append(col) if dtype == 'numerical' else categorical.append(col) # preparing the list of categorcal and numerical columns\n    rng = f'{len(ibm_df[col].unique())} labels' if dtype == 'categorical' else f'{ibm_df[col].min()}-{ibm_df[col].max()}' # the range of values in the columns\n    description[col] = [obs, num_nan, dtype, rng]","842d75df":"numerical.remove('EmployeeCount') # removing EmployeeCount from numerical columns\nnumerical.remove('StandardHours') # removing StandardHours from numerical columns\npd.set_option('display.max_columns', 100) # displaying 100 columns atmost\ndisplay(description)\ndisplay(ibm_df.head())","87d52acc":"# defining a method for calculating accuracy,search time, hyperparameters etc for different models\ndef org_results(trials, hyperparams, model_name):\n    fit_idx = -1\n    for idx, fit  in enumerate(trials):\n        hyp = fit['misc']['vals']\n        xgb_hyp = {key:[val] for key, val in hyperparams.items()}\n        if hyp == xgb_hyp:\n            fit_idx = idx\n            break\n            \n    train_time = str(trials[-1]['refresh_time'] - trials[0]['book_time'])\n    acc = round(trials[fit_idx]['result']['accuracy'], 3)\n    train_auc = round(trials[fit_idx]['result']['train auc'], 3)\n    test_auc = round(trials[fit_idx]['result']['test auc'], 3)\n\n    results = {\n        'model': model_name,\n        'parameter search time': train_time,\n        'accuracy': acc,\n        'test auc score': test_auc,\n        'training auc score': train_auc,\n        'parameters': hyperparams\n    }\n    return results","2ef31b41":"lgb_data = ibm_df.copy()\nlgb_data.head()","5dda2149":"lgb_dummy = pd.get_dummies(lgb_data[categorical], drop_first=True) # creating dummy values for categoriccal columns\nlgb_dummy.head()","a2a667d0":"lgb_data = pd.concat([lgb_dummy, lgb_data], axis=1) # concatenating dummy columns to the original dataframe\nlgb_data.head()","72de9fda":"lgb_data.drop(columns = categorical, inplace=True) # dropping the old categorical columns\nlgb_data.head()","36abab11":"lgb_data.rename(columns={'Attrition_Yes': 'Attrition'}, inplace=True) # renaming the column\nlgb_data.head()","c1c28d91":"y_df = lgb_data['Attrition'].reset_index(drop=True) #  creating the label column\ndisplay(y_df.head())","16d46865":"x_df = lgb_data.drop(columns='Attrition') # selecting the feature columns\nx_df.head()","35d44133":"from sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(x_df, y_df, test_size=0.20) # splitting the training and test data","823efb78":"print('Sample Training Data')\ndisplay(train_x.head())\nprint('Sample Test Data')\ndisplay(test_x.head())","cccaf74d":"print('Sample Training Labels')\ndisplay(train_y.head())\nprint('Sample Test Labels')\ndisplay(test_y.head())","cd6f72e8":"# method to return the loss and the different accuracies for LGB model\ndef lgb_objective(space, early_stopping_rounds=50):\n    \n    lgbm = LGBMClassifier(\n        learning_rate = space['learning_rate'],\n        n_estimators= int(space['n_estimators']), \n        max_depth = int(space['max_depth']),\n        num_leaves = int(space['num_leaves']),\n        colsample_bytree = space['colsample_bytree'],\n        feature_fraction = space['feature_fraction'],\n        reg_lambda = space['reg_lambda'],\n        reg_alpha = space['reg_alpha'],\n        min_split_gain = space['min_split_gain']\n    )\n    \n    lgbm.fit(train_x, train_y, \n            eval_set = [(train_x, train_y), (test_x, test_y)],\n            early_stopping_rounds = early_stopping_rounds,\n            eval_metric = 'auc',\n            verbose = False)\n    \n    predictions = lgbm.predict(test_x)\n    test_preds = lgbm.predict_proba(test_x)[:,1]\n    train_preds = lgbm.predict_proba(train_x)[:,1]\n    \n    train_auc = roc_auc_score(train_y, train_preds)\n    test_auc = roc_auc_score(test_y, test_preds)\n    accuracy = accuracy_score(test_y, predictions)  \n\n    return {'status': STATUS_OK, 'loss': 1-test_auc, 'accuracy': accuracy,\n            'test auc': test_auc, 'train auc': train_auc\n           }\n","9a075169":"\ntrials = Trials()\n# defining parameters for LGB \nspace = {\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.3)),\n    'n_estimators': hp.quniform('n_estimators', 50, 1200, 25),\n    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n    'num_leaves': hp.quniform('num_leaves', 10, 150, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0), \n    'feature_fraction': hp.uniform('feature_fraction', .3, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'min_split_gain': hp.uniform('min_split_gain', 0.0001, 0.1)\n}\n\n#  defining the hyper parameters\nlgb_hyperparams = fmin(fn = lgb_objective, \n                 max_evals = 150, \n                 trials = trials,\n                 algo = tpe.suggest,\n                 space = space\n                 )\n\nlgb_results = org_results(trials.trials, lgb_hyperparams, 'LightGBM') # calling the org_results function previously defined\ndisplay(lgb_results)","6ff824ea":"# creating a dataframe that shows values based on grouping by age\nage=pd.DataFrame(data.groupby(\"Age\")[[\"MonthlyIncome\",\"Education\",\"JobLevel\",\"JobInvolvement\",\"PerformanceRating\",\"JobSatisfaction\",\"EnvironmentSatisfaction\",\"RelationshipSatisfaction\",\"WorkLifeBalance\",\"DailyRate\",\"MonthlyRate\"]].mean())\nage.head()","8a20e11c":"age[\"Count\"]=data.Age.value_counts(dropna=False) # adding new column to age\nage.reset_index(level=0, inplace=True)\nage.head()","670ce4b9":"# showing the counts for different ages\nplt.figure(figsize=(15,10))\nax=sns.barplot(x=age.Age,y=age.Count)\nplt.xticks(rotation=180)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Counts\")\nplt.title(\"Age Counts\")\nplt.show()","f0a99b3d":"# showing monthly income according to age\nplt.figure(figsize=(15,10))\nax=sns.barplot(x=age.Age,y=age.MonthlyIncome,palette = sns.cubehelix_palette(len(age.index)))\nplt.xticks(rotation=180)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Monthly Income\")\nplt.title(\"Monthly Income According to Age\")\nplt.show()","45a1aafb":"# creating a dataframe called income based on the jobroles\nincome=pd.DataFrame(data.groupby(\"JobRole\").MonthlyIncome.mean().sort_values(ascending=False))","109857c8":"# showing the job roles with their monthly incomes\nplt.figure(figsize=(15,10))\nax=sns.barplot(x=income.index,y=income.MonthlyIncome)\nplt.xticks(rotation=90)\nplt.xlabel(\"Job Roles\")\nplt.ylabel(\"Monthly Income\")\nplt.title(\"Job Roles with Monthly Income\")\nplt.show()","862f65f9":"# a dataframe with mean of the selected columns grouped by job role\njobrole=pd.DataFrame(data.groupby(\"JobRole\")[\"PercentSalaryHike\",\"YearsAtCompany\",\"TotalWorkingYears\",\"YearsInCurrentRole\",\"WorkLifeBalance\"].mean())\njobrole","0557456f":"labels=data.EducationField.value_counts().index\nlabels","d554541b":"colors=[\"cyan\",\"orange\",\"hotpink\",\"green\",\"navy\",\"#9b59b6\"]","1077762e":"sizes=data.EducationField.value_counts().values\nsizes","71dbd745":"plt.figure(figsize=(7,7))\nplt.pie(sizes,labels=labels,colors=colors,autopct=\"%1.1f%%\")\nplt.title(\"Education Field Counts\",color=\"saddlebrown\",fontsize=15)","14323587":"# Importing the Libraries","732638a3":"# Understanding the Attrition Problem and the Dataset","a828464b":"## Plot a correlation map for all numeric variables","8df99641":"### All the exploratory data analysis above shall help the HR team to find a pattern or find and filter the criteria which are most reponsible for attrition.","b2e1e197":"## Loading the Dataset","04c3a6d4":"# Breakdown of this notebook:\n1) **Importing the Libraries**\n\n2) **Loading the dataset**\n\n3) **Understanding the Attrition Problem and the Dataset**\n\n4) **Data Visualization:**   \n   - Plot a correlation map for all numeric variables\n   - Overtime\n   - Marital Status\n   - Job Role\n   - Gender\n   - Education Field\n   - Department\n   - Buisness Travel\n   - Relation between Overtime and Age\n   - Total Working Years\n   - Education Level\n   - Number of Companies Worked\n   - Distance from Home\n\n5) **Prediction of Attrition**: LGBM Classifier","e850190a":"### Getting information about the dataset","290ce9a0":"### Print first 10 datapoints","d43fa1ca":"the Count of people doing overtime < the count of people not doing overtime\n\n","3bc2b094":"<h1 align=\"center\"> HR Analysis, Prediction and Visualization <\/b> <\/h1><br>\n<img src=https:\/\/blueprintbusinesssolutionscorp.com\/wp-content\/uploads\/2017\/12\/attrition.png>","22fda058":" HR Analytics helps us with interpreting organizational data. It finds out the people-related trends in the data and helps the HR Department take the appropriate steps to keep the organization running smoothly and profitably.Attrition is a corporate setup is one of the complex challenges that the people managers and the HRs personnel have to deal with. \n \n Interestingly, machine learning models can be deployed to predict potential attrition cases, thereby helping the appropriate HR Personnel take the necessary steps to retain the employee.","8def6546":"#### creating a pie chart based on the educational field.\n","41d5ca35":"Generate descriptive statistics that summarize the central tendency,dispersion and shape of a dataset's distribution","3a1d19b3":"###### We'd now eplore the number of people based on different criteria to see which criteria is the dominat one for attrition","91becd69":"## Describe the dataset"}}