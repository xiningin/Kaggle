{"cell_type":{"44f68278":"code","91cff9aa":"code","541a863c":"code","2a5700bb":"code","54cb31b1":"code","93a44764":"code","5acf2f43":"code","38230b28":"code","efbab6a3":"code","23e8c894":"code","382eaba2":"code","f3f38233":"code","cb636b27":"code","be120442":"code","5577002d":"code","ab806ed0":"code","0d6d0a12":"code","9d106f43":"code","de7b2a3a":"code","ced89664":"code","6e4c6de4":"code","888e5cc7":"code","388f88ad":"code","2fe197ff":"code","b57af48a":"code","6ce1673a":"code","140a2ec9":"code","96d38201":"code","1cc674ef":"code","bb87a94b":"code","0f2fe115":"code","33910c77":"code","18d5e346":"code","40890d97":"code","eef94d74":"code","49175b06":"code","3fb0a346":"code","edacecd2":"code","243fd336":"code","b3153ed9":"code","b5ae8403":"code","72ed1582":"code","03948914":"code","ce520188":"code","94f76d67":"code","4e814877":"code","c9a9c869":"code","5cc15df1":"markdown","2d901194":"markdown","795ee273":"markdown","489ef94a":"markdown","2d81492d":"markdown","f98359f0":"markdown","6386e41c":"markdown","8af1d15e":"markdown","bfb7e784":"markdown","fccd7e1a":"markdown","b627dbcf":"markdown","53712a86":"markdown","5c2beb83":"markdown","88439b20":"markdown","0186e0e3":"markdown","bd5ddb6f":"markdown","2964da49":"markdown","9072ba47":"markdown"},"source":{"44f68278":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Gradient Boosting\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Scikit-learn\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\n# Graphics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Dataset\nfrom sklearn.datasets import load_boston","91cff9aa":"# Uploading the Boston dataset\nX, y = load_boston(return_X_y=True)\n\n# Transforming the problem into a classification (unbalanced)\ny_bin = (y > np.percentile(y, 90)).astype(int)","541a863c":"#CRIM - per capita crime rate by town\n#ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n#INDUS - proportion of non-retail business acres per town.\n#CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n#NOX - nitric oxides concentration (parts per 10 million)\n#RM - average number of rooms per dwelling\n#AGE - proportion of owner-occupied units built prior to 1940\n#DIS - weighted distances to five Boston employment centres\n#RAD - index of accessibility to radial highways\n#TAX - full-value property-tax rate per $10,000\n#PTRATIO - pupil-teacher ratio by town\n#B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n#LSTAT - % lower status of the population\n#MEDV - Median value of owner-occupied homes in $1000's this is our target variable","2a5700bb":"# Histogram highlighting the top 10% we use as a target\nplt.hist(y[y <= np.percentile(y, 90)], bins='auto', alpha=0.7, label='0', color='b')\nplt.hist(y[y > np.percentile(y, 90)], bins=8, alpha=0.7, label='1', color='r')\nplt.title(\"Histogram of MEDV\")\nplt.legend(loc='upper right')\nplt.show()","54cb31b1":"# For convenience, we will create a Pandas dataframe from X\ntrain = pd.DataFrame(X)\ntrain = train.add_prefix('var_')","93a44764":"# Checking about the shape of our training set\nprint(train.shape)","5acf2f43":"# Setting a 5-fold stratified cross-validation (note: shuffle=True)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=8)","38230b28":"MAX_ROUNDS = 2000\nlgb_iter1 = []\nsklearn_gbm_iter1 = []\nxgb_gbm_iter1 = []\n\nlgb_ap1 = []\nsklearn_gbm_ap1 = []\nxgb_gbm_ap1 = []","efbab6a3":"# Set up the classifier with standard configuration\n# Later we will more performing parameters with Bayesian Optimization\nparams = {\n    'learning_rate':  0.06, \n    'max_depth': 6, \n    #'lambda_l1': 16.7,\n    'min_data_in_leaf':5,\n    'boosting': 'gbdt', \n    'objective': 'binary', \n    'metric': 'auc',\n    'feature_fraction': .9,\n    'is_training_metric': False, \n    'seed': 1\n}","23e8c894":"for i, (train_index, test_index) in enumerate(skf.split(train,y_bin)):\n    \n    # Create data for this fold\n    y_train, y_valid = y_bin[train_index], y_bin[test_index]\n    X_train, X_valid = train.iloc[train_index,:], train.iloc[test_index,:]\n        \n    print( \"\\nFold \", i)\n\n    # Running models for this fold\n    \n    # ->LightGBM\n    lgb_gbm = lgb.train(params, \n                          lgb.Dataset(X_train, label=y_train), \n                          MAX_ROUNDS, \n                          lgb.Dataset(X_valid, label=y_valid), \n                          verbose_eval=False, \n                          #feval= auc, \n                          early_stopping_rounds=50)\n    \n    print( \" Best iteration lgb = \", lgb_gbm.best_iteration)\n    \n    # ->Scikit-learn GBM\n    sklearn_gbm = GradientBoostingClassifier(n_estimators=MAX_ROUNDS, \n                                    learning_rate = 0.06,\n                                    max_features=2, \n                                    max_depth = 6, \n                                    n_iter_no_change=50, \n                                    tol=0.01,\n                                    random_state = 0)\n    \n    sklearn_gbm.fit(X_train, y_train)\n    print( \" Best iteration sklearn_gbm = \", sklearn_gbm.n_estimators_)\n    \n    # ->XGBoost\n    xgb_gbm = xgb.XGBClassifier(max_depth=6, \n                                n_estimators=MAX_ROUNDS,\n                                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                                learning_rate=0.06,\n                                early_stopping_rounds=50)\n\n    xgb_gbm.fit(X_train, y_train)\n    \n    print( \" Best iteration xgboost_gbm = \", xgb_gbm.get_booster().best_iteration)\n    \n    # Storing and reporting results of the fold\n    lgb_iter1 = np.append(lgb_iter1, lgb_gbm.best_iteration)\n    sklearn_gbm_iter1 = np.append(sklearn_gbm_iter1, sklearn_gbm.n_estimators_)\n    xgb_gbm_iter1 = np.append(xgb_gbm_iter1, xgb_gbm.get_booster().best_iteration)\n   \n    pred = lgb_gbm.predict(X_valid, num_iteration=lgb_gbm.best_iteration)\n    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n    print('lgb ', ap)\n    lgb_ap1 = np.append(lgb_ap1, ap)\n    \n    pred = sklearn_gbm.predict(X_valid)\n    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n    print('sklearn_gbn ', ap)\n    sklearn_gbm_ap1 = np.append(sklearn_gbm_ap1, ap)\n    \n    pred  = xgb_gbm.predict(X_valid)\n    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n    print('xgboost ', ap)\n    xgb_gbm_ap1 = np.append(xgb_gbm_ap1, ap)","382eaba2":"print('lgb_iter1: ', np.mean(lgb_iter1))\nprint('sklearn_gbm_iter1: ', np.mean(sklearn_gbm_iter1))\nprint('xgb_gbm_iter1: ',np.mean(xgb_gbm_iter1))\n\nprint('lgb_ap1: ', np.mean(lgb_ap1))\nprint('sklearn_gbm_ap1: ', np.mean(sklearn_gbm_ap1))\nprint('xgb_gbm_ap1: ', np.mean(xgb_gbm_ap1))","f3f38233":"poly = PolynomialFeatures(2)\npoly_train = poly.fit_transform(train)","cb636b27":"poly_train = pd.DataFrame(poly_train)","be120442":"poly_train.head()","5577002d":"poly_train = poly_train.add_prefix('poly_')","ab806ed0":"train = pd.concat([train,poly_train], axis=1)","0d6d0a12":"train.head()","9d106f43":"MAX_ROUNDS = 2000\nlgb_iter2 = []\nsklearn_gbm_iter2 = []\nxgb_gbm_iter2 = []\n\nlgb_ap2 = []\nsklearn_gbm_ap2 = []\nxgb_gbm_ap2 = []","de7b2a3a":"for i, (train_index, test_index) in enumerate(skf.split(train,y_bin)):\n    \n    # Create data for this fold\n    y_train, y_valid = y_bin[train_index], y_bin[test_index]\n    X_train, X_valid = train.iloc[train_index,:], train.iloc[test_index,:]\n        \n    print( \"\\nFold \", i)\n\n    # Run model for this fold\n\n    lgb_gbm = lgb.train(params, \n                          lgb.Dataset(X_train, label=y_train), \n                          MAX_ROUNDS, \n                          lgb.Dataset(X_valid, label=y_valid), \n                          verbose_eval=False, \n                          #feval= auc, \n                          early_stopping_rounds=50)\n    \n    print( \" Best iteration lgb = \", lgb_gbm.best_iteration)\n    \n    sklearn_gbm = GradientBoostingClassifier(n_estimators=MAX_ROUNDS, \n                                    learning_rate = 0.06,\n                                    max_features=2, \n                                    max_depth = 6, \n                                    n_iter_no_change=50, \n                                    tol=0.01,\n                                    random_state = 0)\n    \n    sklearn_gbm.fit(X_train, y_train)\n    print( \" Best iteration sklearn_gbm = \", sklearn_gbm.n_estimators_)\n    \n    \n    xgb_gbm = xgb.XGBClassifier(max_depth=6, \n                                n_estimators=MAX_ROUNDS,\n                                eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                                learning_rate=0.06,\n                                early_stopping_rounds=50)\n\n    xgb_gbm.fit(X_train, y_train)\n    \n    print( \" Best iteration xgboost_gbm = \", xgb_gbm.get_booster().best_iteration)\n        \n    lgb_iter2 = np.append(lgb_iter2, lgb_gbm.best_iteration)\n    sklearn_gbm_iter2 = np.append(sklearn_gbm_iter2, sklearn_gbm.n_estimators_)\n    xgb_gbm_iter2 = np.append(xgb_gbm_iter2, xgb_gbm.get_booster().best_iteration)\n    \n    pred = lgb_gbm.predict(X_valid, num_iteration=lgb_gbm.best_iteration)\n    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n    print('lgb ', ap)\n    lgb_ap2 = np.append(lgb_ap2, ap)\n    \n    pred = sklearn_gbm.predict(X_valid)\n    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n    print('sklearn_gbn ', ap)\n    sklearn_gbm_ap2 = np.append(sklearn_gbm_ap2, ap)\n    \n    pred  = xgb_gbm.predict(X_valid)\n    ap = average_precision_score(y_valid, pred, average='macro', pos_label=1, sample_weight=None)\n    print('xgboost ', ap)\n    xgb_gbm_ap2 = np.append(xgb_gbm_ap2, ap)","ced89664":"print('lgb_iter1: ', np.mean(lgb_iter1),' lgb_iter2: ', np.mean(lgb_iter2))\nprint('sklearn_gbm_iter1: ', np.mean(sklearn_gbm_iter1), ' sklearn_gbm_iter2: ', np.mean(sklearn_gbm_iter2))\nprint('xgb_gbm_iter1: ',np.mean(xgb_gbm_iter1), ' xgb_gbm_iter2: ',np.mean(xgb_gbm_iter2) )\n\nprint('lgb_ap1: ', np.mean(lgb_ap1), ' lgb_ap2: ', np.mean(lgb_ap2))\nprint('sklearn_gbm_ap1: ', np.mean(sklearn_gbm_ap1), ' sklearn_gbm_ap2: ', np.mean(sklearn_gbm_ap2))\nprint('xgb_gbm_ap1: ', np.mean(xgb_gbm_ap1), ' xgb_gbm_ap2: ', np.mean(xgb_gbm_ap2))","6e4c6de4":"# Installing the most recent version of skopt directly from Github\n!pip install git+https:\/\/github.com\/scikit-optimize\/scikit-optimize.git","888e5cc7":"# Assuring you have the most recent CatBoost release\n!pip install catboost -U","388f88ad":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Our example dataset\nfrom sklearn.datasets import load_boston\n\n# Classifiers\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Hyperparameters distributions\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# Metrics\nfrom sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\nfrom skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\nfrom skopt.callbacks import VerboseCallback # Callback to control the verbosity\nfrom skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta","2fe197ff":"# Uploading the Boston dataset\nX, y = load_boston(return_X_y=True)","b57af48a":"# Transforming the problem into a classification (unbalanced)\ny_bin = (y > np.percentile(y, 90)).astype(int)","6ce1673a":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title, callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    if callbacks:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n    best_score = optimizer.best_score_\n    best_score_std = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n    best_params = optimizer.best_params_\n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                  len(optimizer.cv_results_['params']),\n                                  best_score,\n                                  best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","140a2ec9":"# Converting average precision score into a scorer suitable for model selection\navg_prec = make_scorer(average_precision_score, greater_is_better=True, needs_proba=True)","96d38201":"# Setting a 5-fold stratified cross-validation (note: shuffle=True)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","1cc674ef":"# A Scikit-learn GBM classifier\nclf = GradientBoostingClassifier(n_estimators=20, random_state=0)","bb87a94b":"# GridSearchCV needs a predefined plan of the experiments\ngrid_search = GridSearchCV(clf, \n                           param_grid={\"learning_rate\": [0.01, 1.0],\n                                       \"n_estimators\": [10, 500],\n                                       \"subsample\": [1.0, 0.5],\n                                       \"min_samples_split\": [2, 10],\n                                       \"min_samples_leaf\": [1, 10],\n                                       \"max_features\": ['sqrt', 'log2', None]\n                                       },\n                           n_jobs=-1,\n                           cv=skf,\n                           scoring=avg_prec,\n                           iid=False, # just return the average score across folds\n                           return_train_score=False)\n\nbest_params = report_perf(grid_search, X, y_bin,'GridSearchCV')","0f2fe115":"# RandomizedSearchCV needs the distribution of the experiments to be tested\n# If you can provide the right distribution, the sampling will lead to faster and better results.\n\nrandom_search = RandomizedSearchCV(clf, \n                           param_distributions={\"learning_rate\": uniform(0.01, 1.0),\n                                                \"n_estimators\": randint(10, 500),\n                                                \"subsample\": uniform(0.5, 0.5),\n                                                \"min_samples_split\": randint(2, 10),\n                                                \"min_samples_leaf\": randint(1, 10),\n                                                \"max_features\": ['sqrt', 'log2', None]\n                                       },\n                                   n_iter=40,\n                                   n_jobs=-1,\n                                   cv=skf,\n                                   scoring=avg_prec,\n                                   iid=False, # just return the average score across folds\n                                   return_train_score=False,\n                                   random_state=0)\n\nbest_params = report_perf(random_search, X, y_bin, 'RandomizedSearchCV')","33910c77":"# also BayesSearchCV needs to work on the distributions of the experiments but it is less sensible to them\n\nsearch_spaces = {\"learning_rate\": Real(0.01, 1.0),\n                 \"n_estimators\": Integer(10, 500),\n                 \"subsample\": Real(0.5, 1.0),\n                 \"min_samples_split\": Integer(2, 10),\n                 \"min_samples_leaf\": Integer(1, 10),\n                 \"max_features\": Categorical(categories=['sqrt', 'log2', None])}\n\nfor baseEstimator in ['GP', 'RF', 'ET', 'GBRT']:\n    opt = BayesSearchCV(clf,\n                        search_spaces,\n                        scoring=avg_prec,\n                        cv=skf,\n                        n_iter=40,\n                        n_jobs=-1,\n                        return_train_score=False,\n                        optimizer_kwargs={'base_estimator': baseEstimator},\n                        random_state=4)\n    \n    best_params = report_perf(opt, X, y_bin,'BayesSearchCV_'+baseEstimator)","18d5e346":"# Initialize a pipeline with a model\npipe = Pipeline([('model', GradientBoostingClassifier(n_estimators=20, random_state=0))])\n\n# Define search space for GBM;\nsearch_space_GBM = {\"model\": Categorical([GradientBoostingClassifier(n_estimators=20, random_state=0)]),\n                    \"model__learning_rate\": Real(0.01, 1.0),\n                    \"model__n_estimators\": Integer(10, 500),\n                    \"model__subsample\": Real(0.5, 1.0),\n                    \"model__min_samples_split\": Integer(2, 10),\n                    \"model__min_samples_leaf\": Integer(1, 10),\n                    \"model__max_features\": Categorical(categories=['sqrt', 'log2', None])}\n\n# Define search space for RF\nsearch_space_RF  = {\"model\": Categorical([RandomForestClassifier(n_estimators=20, random_state=0)]),\n                    \"model__n_estimators\": Integer(10, 200),\n                    \"model__min_samples_split\": Integer(2, 10),\n                    \"model__min_samples_leaf\": Integer(1, 10),\n                    \"model__max_features\": Categorical(categories=['sqrt', 'log2', None])}\n\nopt = BayesSearchCV(pipe,\n                        search_spaces=[(search_space_GBM, 20), (search_space_RF, 20)],\n                        scoring=avg_prec,\n                        cv=skf,\n                        n_jobs=-1,\n                        return_train_score=False,\n                        optimizer_kwargs={'base_estimator': 'GP'},\n                        random_state=4)\n    \nbest_params = report_perf(opt, X, y_bin,'BayesSearchCV_GP')","40890d97":"counter = 0\ndef onstep(res):\n    global counter\n    x0 = res.x_iters   # List of input points\n    y0 = res.func_vals # Evaluation of input points\n    print('Last eval: ', x0[-1], \n          ' - Score ', y0[-1])\n    print('Current iter: ', counter, \n          ' - Score ', res.fun, \n          ' - Args: ', res.x)\n    joblib.dump((x0, y0), 'checkpoint.pkl') # Saving a checkpoint to disk\n    counter += 1\n\n# Our search space\ndimensions = [Real(0.01, 1.0, name=\"learning_rate\"),\n              Integer(10, 500, name=\"n_estimators\"),\n              Real(0.5, 1.0, name=\"subsample\"),\n              Integer(2, 10, name=\"min_samples_split\"),\n              Integer(1, 10, name=\"min_samples_leaf\"),\n              Categorical(categories=['sqrt', 'log2', None], name=\"max_features\")]\n\n# The objective function to be minimized\ndef make_objective(model, X, y, space, cv, scoring):\n    # This decorator converts your objective function with named arguments into one that\n    # accepts a list as argument, while doing the conversion automatically.\n    @use_named_args(space) \n    def objective(**params):\n        model.set_params(**params)\n        return -np.mean(cross_val_score(model, \n                                        X, y, \n                                        cv=cv, \n                                        n_jobs=-1,\n                                        scoring=scoring))\n\n    return objective\n\nobjective = make_objective(clf,\n                           X, y_bin,\n                           space=dimensions,\n                           cv=skf,\n                           scoring=avg_prec)","eef94d74":"gp_round = gp_minimize(func=objective,\n                       dimensions=dimensions,\n                       acq_func='gp_hedge', # Defining what to minimize \n                       n_calls=10,\n                       callback=[onstep],\n                       random_state=22)","49175b06":"x0, y0 = joblib.load('checkpoint.pkl')\n\ngp_round = gp_minimize(func=objective,\n                       x0=x0,              # already examined values for x\n                       y0=y0,              # observed values for x0\n                       dimensions=dimensions,\n                       acq_func='gp_hedge', # Expected Improvement.\n                       n_calls=10,\n                       callback=[onstep],\n                       random_state=0)","3fb0a346":"best_parameters = gp_round.x\nbest_result = gp_round.fun\nprint(best_parameters, best_result)","edacecd2":"clf = lgb.LGBMClassifier(boosting_type='gbdt',\n                         class_weight='balanced',\n                         objective='binary',\n                         n_jobs=1, \n                         verbose=0)\n\nsearch_spaces = {\n        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n        'num_leaves': Integer(2, 500),\n        'max_depth': Integer(0, 500),\n        'min_child_samples': Integer(0, 200),\n        'max_bin': Integer(100, 100000),\n        'subsample': Real(0.01, 1.0, 'uniform'),\n        'subsample_freq': Integer(0, 10),\n        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n        'min_child_weight': Integer(0, 10),\n        'subsample_for_bin': Integer(100000, 500000),\n        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n        'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n        'n_estimators': Integer(10, 10000)        \n        }\n\nopt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=avg_prec,\n                    cv=skf,\n                    n_iter=40,\n                    n_jobs=-1,\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=22)\n    \nbest_params = report_perf(opt, X, y_bin,'LightGBM', \n                          callbacks=[DeltaXStopper(0.001), \n                                     DeadlineStopper(60*5)])","243fd336":"counter = 0\n\nclf = lgb.LGBMClassifier(boosting_type='gbdt',\n                         class_weight='balanced',\n                         objective='binary',\n                         n_jobs=1, \n                         verbose=0)\n\ndimensions = [Real(0.01, 1.0, 'log-uniform', name='learning_rate'),\n              Integer(2, 500, name='num_leaves'),\n              Integer(0, 500, name='max_depth'),\n              Integer(0, 200, name='min_child_samples'),\n              Integer(100, 100000, name='max_bin'),\n              Real(0.01, 1.0, 'uniform', name='subsample'),\n              Integer(0, 10, name='subsample_freq'),\n              Real(0.01, 1.0, 'uniform', name='colsample_bytree'),\n              Integer(0, 10, name='min_child_weight'),\n              Integer(100000, 500000, name='subsample_for_bin'),\n              Real(1e-9, 1000, 'log-uniform', name='reg_lambda'),\n              Real(1e-9, 1.0, 'log-uniform', name='reg_alpha'),\n              Real(1e-6, 500, 'log-uniform', name='scale_pos_weight'),\n              Integer(10, 10000, name='n_estimators')]\n\nobjective = make_objective(clf,\n                           X, y_bin,\n                           space=dimensions,\n                           cv=skf,\n                           scoring=avg_prec)","b3153ed9":"gp_round = gp_minimize(func=objective,\n                       dimensions=dimensions,\n                       acq_func='gp_hedge',\n                       n_calls=10, # Minimum is 10 calls\n                       callback=[onstep],\n                       random_state=7)","b5ae8403":"x0, y0 = joblib.load('checkpoint.pkl')\n\ngp_round = gp_minimize(func=objective,\n                       x0=x0,              # already examined values for x\n                       y0=y0,              # observed values for x0\n                       dimensions=dimensions,\n                       acq_func='gp_hedge', # Expected Improvement.\n                       n_calls=10,\n                       #callback=[onstep],\n                       random_state=3)\n\nbest_parameters = gp_round.x\nbest_result = gp_round.fun\nprint(best_parameters, best_result)","72ed1582":"clf = xgb.XGBClassifier(\n        n_jobs = 1,\n        objective = 'binary:logistic',\n        silent=1,\n        tree_method='approx')","03948914":"search_spaces = {'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n                 'min_child_weight': Integer(0, 10),\n                 'max_depth': Integer(0, 50),\n                 'max_delta_step': Integer(0, 20),\n                 'subsample': Real(0.01, 1.0, 'uniform'),\n                 'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n                 'colsample_bylevel': Real(0.01, 1.0, 'uniform'),\n                 'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n                 'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n                 'gamma': Real(1e-9, 0.5, 'log-uniform'),\n                 'min_child_weight': Integer(0, 5),\n                 'n_estimators': Integer(50, 100),\n                 'scale_pos_weight': Real(1e-6, 500, 'log-uniform')}","ce520188":"opt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=avg_prec,\n                    cv=skf,\n                    n_iter=40,\n                    n_jobs=-1,\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=22)\n    \nbest_params = report_perf(opt, X, y_bin,'XGBoost',                           \n                          callbacks=[DeltaXStopper(0.001), \n                                     DeadlineStopper(60*5)])","94f76d67":"clf = CatBoostClassifier(loss_function='Logloss',\n                         verbose = False)","4e814877":"search_spaces = {'iterations': Integer(10, 100),\n                 'depth': Integer(1, 8),\n                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n                 'bagging_temperature': Real(0.0, 1.0),\n                 'border_count': Integer(1, 255),\n                 'ctr_border_count': Integer(1, 255),\n                 'l2_leaf_reg': Integer(2, 30),\n                 'scale_pos_weight':Real(0.01, 1.0, 'uniform')}","c9a9c869":"opt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=avg_prec,\n                    cv=skf,\n                    n_iter=40,\n                    n_jobs=1,  # use just 1 job with CatBoost in order to avoid segmentation fault\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=22)\n\nbest_params = report_perf(opt, X, y_bin,'CatBoost', \n                          callbacks=[DeltaXStopper(0.001), \n                                     DeadlineStopper(60*5)])","5cc15df1":"Grid search exhaustively searches through the hyperparameters and is not feasible in high dimensional space\nThis is a very simple algorithm and suffers from the curse of dimensionality, though it's embarrassingly parallel.\n\nHere we use, GridSearchCV, a function from Scikit-learn. ","2d901194":"Before we proceed with optimization, some important questions and insights about model specification.\n\n**1. Should we use all methods and mix them up in a competition?**\n\n**2. Should we use all methods and mix them up in a work project?**\n\n**3. Should we add the polynomial features in a competition?**\n\n**4. Should we add the polynomial features in a work project?**","795ee273":"A brief introduction about the key models we will be using during this works.\nsource : BOSCHETTI, Alberto; MASSARON, Luca. Python data science essentials. Packt Publishing Ltd, 3rd ed., 2018\n\n### Gradient Tree Boosting\n\nGradient Tree boosting or Gradient Boosting Decision Trees (GBDT) is another improved version of boosting (fitting a sequence of weak learners on reweighted versions of the data). Like AdaBoost, GBDT is based on a gradient descent function. The algorithm has proven to be one of the most proficient ones from the ensemble, though it is characterized by an increased variance of estimates, more sensibility to noise in data (both problems could be attenuated by using sub-sampling), and significant computational costs due to nonparallel operations.\n\nApart from deep learning, gradient boosting is actually the most developed machine learning algorithm. Since Adaboost and the following Gradient Boosting implementation as developed by Jerome Friedman, there appeared various implementations of the algorithms, the most recent ones being XGBoost, LightGBM, and CatBoost\n\n### LightGbm\nThe high-performance [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM) algorithm is capable of being distributed and of fast-handling large amounts of data. It has been developed by a team at Microsoft as an open source project on GitHub (there is also an [academic paper](https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficientgradient-boosting-decision-tree)). \n\nLightGBM is based on decision trees, as well as XGBoost, yet it follows a different strategy.\nWhereas XGBoost uses decision trees to split on a variable and exploring different cuts at that variable (the level-wise tree growth strategy), LightGBM concentrates on a split and goes on splitting from there in order to achieve a better fitting (this is the leaf-wise tree\ngrowth strategy). This allows LightGBM to reach first and fast a good fit of the data, and to generate alternative solutions compared to XGBoost (which is good, if you expect to blend, i.e. average, the two solutions together in order to reduce the variance of the estimated). Algorithmically talking, figuring out as a graph the structures of cuts operated by a decision tree, XGBoost peruses a breadth-first search (BFS), whereas LightGBM a depthfirst search (DFS).\n\nTuning LightGBM may appear daunting with more than a [hundred parameters](https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst) (also to be found [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html)) to tune.\n\n### XGBoost\n\n[XGBoost](https:\/\/github.com\/dmlc\/XGBoost) stands for eXtreme Gradient Boosting, an open source project that is not part of Scikit-learn, though recently it has been expanded by a Scikit-Learn wrapper interface that renders using models based on XGBoost more integrated into your data pipeline.\n\nThe XGBoost algorithm has gained recently gained momentum and popularity in datascience competitions such as Kaggle and the KDD-cup 2015. As the authors (Tianqui Chen, Tong He, and Carlos Guestrin) report on papers they wrote on the algorithm, among 29 challenges held on Kaggle during 2015, 17 winning solutions used XGBoost as a standalone solution or as part of an ensemble of multiple different models.\n\nApart from the successful performances in both accuracy and computational efficiency, XGBoost is also a scalable solution under different points of view. XGBoost represents a new generation of GBM algorithms thanks to important tweaks to the initial tree boost GBM algorithm:\n\n* A sparse-aware algorithm; it can leverage sparse matrices, saving both memory (no need for dense matrices) and computation time (zero values are handled in a special way).\n* Approximate tree learning (weighted quantile sketch), which bears similar results but in much less time than the classical complete explorations of possible branch cuts.\n* Parallel computing on a single machine (using multi-threading in the phase of the search for the best split) and similarly distributed computations on multiple ones. \n* Out-of-core computations on a single machine leveraging a data storage solution called Column Block. This arranges data on a disk by columns, thus saving time by pulling data from the disk as the optimization algorithm (which works on column vectors) expects it.\n* XGBoost can also deal with missing data in an effective way. Other tree ensembles based on standard decision trees require missing data first to be imputed using an off-scale value, such as a negative number, in order to develop an appropriate branching of the tree to deal with missing values.\n\nAs for as XGBoost's [parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html), we have decided to work on a few key ones you will find across competitions and projects.\n\n### CatBoost\nIn July 2017, another interesting GBM algorithm was made public by Yandex, the Russian search engine: it is [CatBoost](https:\/\/catboost.yandex\/), whose name comes from putting together the two words Category and Boosting. In fact, its strongest point is the capability of handling categorical variables, which actually make the most of information in most relational databases, by adopting a mixed strategy of one-hot-encoding and mean encoding (a way to express categorical levels by assigning them an appropriate numeric value for the problem at hand; more on that later).\n\nThe idea used by CatBoost to encode the categorical variables is not new, but it has been a kind of feature engineering used various times, mostly in data science competitions like at Kaggle\u2019s. Mean encoding, also known as likelihood encoding, impact coding, or target coding, is simply a way to transform your labels into a number based on their association with the target variable. If you have a regression, you could transform labels based on the mean target value typical of that level; if it is a classification, it is simply the probability of classification of your target given that label (probability of your target, conditional on each category value). It may appear as a simple and smart feature engineering trick, but actually, it has side effects, mostly in terms of overfitting because you are taking information from the target into your predictors.\n\nCatBoost has quite a few [parameters](https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-reference_parameters-list-docpage\/#python-reference_parameters-list), we have delimited our search to the 9 most important ones.","489ef94a":"## Searching more complex spaces\nIf you have multiple models to optimize, you can leverage the *Pipeline* command in order to search different search spaces based on different models. That's requires to access to the hyper-parameters accordingly to *Pipeline* specifications, anyway.","2d81492d":"After examining the classical and most known approaches, it is time to dwelve into Bayesian optimization.\n\nBayesian optimization is behind [Google Cloud Machine Learning Engine](https:\/\/cloud.google.com\/blog\/products\/gcp\/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization) services.\n\nThe key idea behind Bayesian optimization is that we optimize a proxy function instead than the true objective function (what actually grid search and random search both do). This holds if testing the true objective function is costly (if it is not, then we simply go for random search :-))\n\nBayesian search balances exploration against exploitation. At start it randomly explores, doing so it builds up a surrogate function of the objective. Based on that surrogate function it exploits an initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function at a global level, not a local one.\n\nAs the Bayesian part of the title suggests, we use priors in order to make smarter decisions about sampling during optimizing in order to reach a minimization faster by limiting the number of evaluations we need to make.\n\nBayesian Optimization uses an acquisition function to tell us how promising an observation will be.\nIn fact, to rule the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point.\n\nFrom the figure taken from [Skopt API documentation](https:\/\/scikit-optimize.github.io\/notebooks\/bayesian-optimization.html), you can figure out that the surrogate function (the green dotted line, whose error band is represented by the light green area) has somehow approximated the true cost function (the red dotted line):\n\n![figure_1](https:\/\/scikit-optimize.github.io\/notebooks\/bayesian-optimization_files\/bayesian-optimization_21_0.png)\n\nThe observations supporting the construction of the surrogate function are not randomly sparse around, because, through an acquisition function (in a gaussian processes it is a function guiding the selection of the next evaluation points), they have been picked as the most useful examples in order to guess how to minimize the cost function.\n\nIn respect of a random optimization, a bayesian optimization is more of an educated guess, then, first sampling randomly, but then focussing on the most important combination of hyper-parameters in order to figure out, first the surrogate function of the cost function, then the global minimum of the cost function:\n\n![figure_2](https:\/\/scikit-optimize.github.io\/notebooks\/bayesian-optimization_files\/bayesian-optimization_18_1.png)\n\nGaussian process (GP) is one of the possible ways to build a surrogate function: it consists of a distribution on functions.\nOriginally GPs were developed to help search for gold ([kriging](https:\/\/en.wikipedia.org\/wiki\/Kriging)). Please note that the approach is closely related to the statistical ideas in the optimal design of experiments.\nIn a gaussian process, based on a distribution of functions resembling the true cost function, the alogorithm operates in:\n\n* Exploration -> seeking points and areas on the optimization surface with high variance\n* Exploitation -> seeking points with low mean\n\nThis is done by a second, specialized function, the acquisition function.\n\nOther approaches are 1) ensembles of decision trees 2) Tree of Parzen Estimators (TPE used by [Hyperopt](http:\/\/hyperopt.github.io\/hyperopt\/) \nanother Bayesian optimization package package) \n\nGaussian Processes are just models, and they're much more like k-nearest neighbors and linear regression than may at first be apparent. If you want to understand more of GPs, you can read the post: [https:\/\/planspace.org\/20181226-gaussian_processes_are_not_so_fancy\/](https:\/\/planspace.org\/20181226-gaussian_processes_are_not_so_fancy)by Aaron Schumacher.","f98359f0":"# Optimizing Scikit-learn GradientBoostingClassifier\n\nGridSearchCV, RandomizedSearchCV (from Scikit-learn) and BayesSearchCV (from Scikit-optimize) all have the same API. A wrapper can just put together optimization, callbacks, best results reporting and time monitoring.","6386e41c":"![Kaggle Days Paris](https:\/\/kaggledays.com\/wp-content\/uploads\/sites\/2\/2018\/11\/46508555_1939772529664297_1579296553191866368_n-1024x536.png)","8af1d15e":"## Controlling the time cost of Bayesian optimization\n\nRunning a single LightGBM model could take long time and in a Kaggle competition time is often a luxury. \n\n*DeadlineStopper* and *DeltaXStopper* are skopt callbacks that control the total time spent and the improvement of a BayesSearchCV (in our implementation to be called with *report_perf*, using the parameter *callbacks=[]*). \n\nAnyway, sometimes it is easier to control manually the optimization steps, hence the usage of low-level optimizers. \n\nWe start defining a custom callback, using a different approach to search spaces (a list instead of a dictionary), and to manually create our objective function to be minimized.\n\nIn our custom callback, we print the last evaluation point (so you know what's happening) and the best score and parameters foudn up so far. We also record the list of explored points (*x0*) and their relative results (*y0*). This will help us to reprise the learning at a later time. ","bfb7e784":"# Practical example: Optimizing XGBoost\n\n[XGBoost](https:\/\/github.com\/dmlc\/XGBoost) stands for eXtreme Gradient Boosting, an open source project that is not part of Scikit-learn, though recently it has been expanded by a Scikit-Learn wrapper interface that renders using models based on XGBoost more integrated into your data pipeline.\n\nThe XGBoost algorithm has gained recently gained momentum and popularity in datascience competitions such as Kaggle and the KDD-cup 2015. As the authors (Tianqui Chen, Tong He, and Carlos Guestrin) report on papers they wrote on the algorithm, among 29 challenges held on Kaggle during 2015, 17 winning solutions used XGBoost as a standalone solution or as part of an ensemble of multiple different models.\n\nApart from the successful performances in both accuracy and computational efficiency, XGBoost is also a scalable solution under different points of view. XGBoost represents a new generation of GBM algorithms thanks to important tweaks to the initial tree boost GBM algorithm:\n\n* A sparse-aware algorithm; it can leverage sparse matrices, saving both memory (no need for dense matrices) and computation time (zero values are handled in a special way).\n* Approximate tree learning (weighted quantile sketch), which bears similar results but in much less time than the classical complete explorations of possible branch cuts.\n* Parallel computing on a single machine (using multi-threading in the phase of the search for the best split) and similarly distributed computations on multiple ones. \n* Out-of-core computations on a single machine leveraging a data storage solution called Column Block. This arranges data on a disk by columns, thus saving time by pulling data from the disk as the optimization algorithm (which works on column vectors) expects it.\n* XGBoost can also deal with missing data in an effective way. Other tree ensembles based on standard decision trees require missing data first to be imputed using an off-scale value, such as a negative number, in order to develop an appropriate branching of the tree to deal with missing values.\n\nAs for as XGBoost's [parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html), we have decided to work on 13 key ones.  ","fccd7e1a":"In conclusion, just keep in mind a few points from the workshop:\n\n* Bayesian Optimization has its own hyper-parameters (therefore use defaults, [unless you know what you doing](https:\/\/i0.kym-cdn.com\/entries\/icons\/original\/000\/008\/342\/ihave.jpg))\n\n* Experiments are run sequentially (skopt can leverage some parallelism, though), having multiple cores is helpful for your learning algorithm,but Bayesian Optimization will always be slower than Random Search. Use it only when needed.\n\n* Packages are not all that friendly (hence the workshop :-)) but you can reuse some simple wrappers re-adaptable to being used in different Kaggle competitions.","b627dbcf":"# Practical Example: Optimizing CatBoost\n\nIn July 2017, another interesting GBM algorithm was made public by Yandex, the Russian search engine: it is [CatBoost](https:\/\/catboost.yandex\/), whose name comes from putting together the two words Category and Boosting. In fact, its strongest point is the capability of handling categorical variables, which actually make the most of information in most relational databases, by adopting a mixed strategy of one-hot-encoding and mean encoding (a way to express categorical levels by assigning them an appropriate numeric value for the problem at hand; more on that later).\n\nThe idea used by CatBoost to encode the categorical variables is not new, but it has been a kind of feature engineering used various times, mostly in data science competitions like at Kaggle\u2019s. Mean encoding, also known as likelihood encoding, impact coding, or target coding, is simply a way to transform your labels into a number based on their association with the target variable. If you have a regression, you could transform labels based on the mean target value typical of that level; if it is a classification, it is simply the probability of classification of your target given that label (probability of your target, conditional on each category value). It may appear as a simple and smart feature engineering trick, but actually, it has side effects, mostly in terms of overfitting because you are taking information from the target into your predictors.\n\nCatBoost has quite a few [parameters](https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-reference_parameters-list-docpage\/#python-reference_parameters-list), we have delimited our search to the 9 most important ones. ","53712a86":"# PART I : Specifying models","5c2beb83":"Random search, which simply samples the search space randomly, is feasible in high dimensional spaces, and is widely used in practice. The downside of random search, however, is that it doesn\u2019t use information from prior experiments to select the next setting.\n\nYou simply need to be lucky to catch the right hyper-parameters, or just try more ;-).\n\nIn fact, the 2\u00d7Random Search is the Random Search algorithm when it was allowed to sample two points for each point the other algorithms evaluated. While some authors have claimed that 2\u00d7Random Search is highly competitive with Bayesian Optimization methods, a [study by Google](http:\/\/delivery.acm.org\/10.1145\/3100000\/3098043\/p1487-golovin.pdf) (GOLOVIN, Daniel, et al. Google vizier: A service for black-box optimization. In: Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017. p. 1487-1495) suggests that this is only true when the dimensionality of the problem is sufficiently high (e.g., over 16)\n\nRandomizedSearchCV is a function from Scikit-learn, though skopt has it own random optimizer, *[dummy_minimize](https:\/\/scikit-optimize.github.io\/#skopt.dummy_minimize)*.","88439b20":"# Competitive GBDT Specification and Optimization Workshop\n\n\n## Instructors\n* Luca Massaron [@lmassaron](https:\/\/www.linkedin.com\/in\/lmassaron\/) - Data Scientist \/ Author \/ Google Developer Expert in Machine Learning \n* Pietro Marinelli [@pietro-marinelli-0098b427](https:\/\/www.linkedin.com\/in\/pietro-marinelli-0098b427\/) - Freelance Data Scientist\n\n## About the workshop\n\nGradient Boosting Decision Trees (GBDT) presently represent the state of the art for building predictors for flat table data. However, they seldom perform the best out-of-the-box (using default values) because of the many hyper-parameters to tune. Especially in the most recent GBDT implementations, such as LightGBM, the over-sophistication of hyper-parameters renders finding the optimal settings by hand or simple grid search difficult because of high combinatorial complexity and long running times for experiments. \n\n[Random Optimization](https:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (BERGSTRA, James; BENGIO, Yoshua. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 2012, 13.Feb: 281-305.) and [Bayesian Optimization](https:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (SNOEK, Jasper; LAROCHELLE, Hugo; ADAMS, Ryan P. Practical bayesian optimization of machine learning algorithms. In: Advances in neural information processing systems. 2012. p. 2951-2959) are often the answer you'll find from experts.\n\nIn this workshop we demonstrate how to use different optimization approaches based on [Scikit-Optimize](https:\/\/github.com\/scikit-optimize\/scikit-optimize), a library built on top of NumPy, SciPy and Scikit-Learn, and we present an easy and fast approach to set them ready and usable.\n\n## Prerequisites\n\nYou should be aware of the role and importance of hyper-parameter optimization in machine learning.  \n\n## Obtaining the Tutorial Material\nIn order to make the workshop easily accessible, we are offering cloud access:\n* Using [Google Colab](https:\/\/colab.research.google.com\/github\/lmassaron\/kaggledays-2019-gbdt\/blob\/master\/Kaggle%20Days%20Paris%20-%20%20GBDT%20workshop.ipynb) \n* Using [Kaggle Kernels]()\n\nWe also have a brief exercise that can be found at:\n* Using [Google Colab](https:\/\/colab.research.google.com\/github\/lmassaron\/kaggledays-2019-gbdt\/blob\/master\/Kaggle%20Days%20Paris%20-%20Skopt%20%2B%20CatBoost%20exercise.ipynb)\n* Using [Kaggle Kernels]()\n\nThe solution can be found [here](https:\/\/github.com\/lmassaron\/kaggledays-2019-gbdt\/blob\/master\/Kaggle%20Days%20Paris%20-%20Skopt%20%2B%20CatBoost%20solution.ipynb).\n\nAll the materials can be cloned from Github at the [kaggledays-2019-gbdt](https:\/\/github.com\/lmassaron\/kaggledays-2019-gbdt) repository.\n\n## Local installation notes\n\nIn order to successfully run this workshop on your local computer, you need a Python3 installation (we suggest installing the most recent [Anaconda](https:\/\/www.anaconda.com\/download\/) distribution) and at least the following packages:\n\n* numpy >= 1.15.4\n* pandas >= 0.23.4\n* scipy >= 1.1.0\n* skopt >= 0.5.2\n* sklearn >= 0.20.2\n* lightgbm >= 2.2.2\n* xgboost >= 0.81\n* catboost >= 0.12.2\n","0186e0e3":"# A Practical Example: Optimizing LightGBM\n\nThe high-performance [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM) algorithm is capable of being distributed and of fast-handling large amounts of data. It has been developed by a team at Microsoft as an open source project on GitHub (there is also an [academic paper](https:\/ \/ papers. nips. cc\/ paper\/ 6907- lightgbm- a- highly- efficientgradient-boosting- decision- tree)). \n\nLightGBM is based on decision trees, as well as XGBoost, yet it follows a different strategy.\nWhereas XGBoost uses decision trees to split on a variable and exploring different cuts at that variable (the level-wise tree growth strategy), LightGBM concentrates on a split and goes on splitting from there in order to achieve a better fitting (this is the leaf-wise tree\ngrowth strategy). This allows LightGBM to reach first and fast a good fit of the data, and to generate alternative solutions compared to XGBoost (which is good, if you expect to blend, i.e. average, the two solutions together in order to reduce the variance of the estimated). Algorithmically talking, figuring out as a graph the structures of cuts operated by a decision tree, XGBoost peruses a breadth-first search (BFS), whereas LightGBM a depthfirst search (DFS).\n\nTuning LightGBM may appear daunting with more than a [hundred parameters](https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst) (also to be found [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html)) to fix.","bd5ddb6f":"There are different low-level optimizers that can be used for the purpose:\n* **gp_minimize** Bayesian optimization using Gaussian Processes.\n* **forest_minimize** Sequential optimisation using decision trees\n* **gbrt_minimize** Sequential optimization using gradient boosted trees\n* **dummy_minimize** Random search by uniform sampling within the given bounds (a replacement for Scikit-learn's RandomSearch)\n\nEach optimizer has its own parameters, so they cannot be just automatically switched, though they share most of the key parameters.\n\nHere we encounter also a new parameter, **acq_func**, useful for defining how the acquisition function should behave, that is, if to take as minimum the lower confidence bound, the minimum expected value or probability (the suggested default is usually a good choice).","2964da49":"# PART II : Optimizing hyper-parameters\n\nThe second topic of this workshop is to illustrate how to best optimize the hyperparameters of a gradient boosting model (lightGBM before all, but also XGBoost and CatBoost) in a performing and efficient way. We will also compare the strong and weak points of different tuning approaches, such grid-search, random search and bayesian optimization by Scikit-optimize.\n\nLeaving apart grid-search (feasible only when the space of experiments is limited), the usual choice for the practitioner is to apply random search optimization or try some [Bayesian Optimization](https:\/\/papers.nips.cc\/paper\/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (BO) technique, which require a more complex setup. \n\nAs for as BO, there are quite a few choices (for instance Hyperopt) but we decided for Scikit-Optimize, or skopt, because it is a simple and efficient library to minimize (very) expensive and noisy black-box functions and it works with an API similar to Scikit-learn. It can be found at https:\/\/github.com\/scikit-optimize\/scikit-optimize\/","9072ba47":"Optimizing hyper-parameters requires time and resources. In order to speed up the demonstration we will be using a toy dataset, the Boston Houseprice dataset for a classification task, to predicted the top 10% most expensive houses.\n\nThe dataset presents information collected by the U.S Census Service concerning housing proces and conditions in the area of Boston Mass. Originally found in the [StatLib archive](http:\/\/lib.stat.cmu.edu\/datasets\/boston), the dataset has been used extensively throughout the literature to benchmark machine learning algorithms. The data was originally published by :\n> Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air', J. Environ. Economics & Management, vol.5, 81-102, 1978.\n\nThe dataset contains 14 variabile relative to 506 house that were sold in the suburbs of Boston. Among the variables, the 14th, MEDV - Median value of owner-occupied homes in $1000's - is commonly used as a target for regression problems. In our example we will use it for classification, after binarizing it at the 90th percentile (also creating an unbalanced classification problem, since the positive cases are just 10 percent of the total). "}}