{"cell_type":{"cb5b5921":"code","cb872de2":"code","9819aa48":"code","051b0ec8":"code","f2c84341":"code","f5b2745f":"code","02a1bb5a":"code","77a4a42f":"code","22f54311":"code","45be4c14":"code","48a80679":"code","dda8d3d7":"code","d17ba54a":"code","3306015b":"code","a24d046d":"code","26b1b4a6":"code","5c587c97":"code","d8ad94c1":"code","a8a7a2b0":"code","0b8db23c":"code","af403c54":"code","e5b18631":"code","2ab25469":"code","e941f276":"code","210921e6":"code","ecbfbb86":"code","1d9d9cba":"code","26862cd2":"code","2450b4d4":"code","397aa30d":"code","7950a95e":"code","65da04c8":"code","ba9f164e":"code","fd9a6190":"code","12d9aad4":"code","e0b57a1b":"code","d763bc80":"code","6c518171":"code","ed13b8fa":"code","950e30de":"code","721e5a56":"markdown","499c1da2":"markdown","2c74b15a":"markdown","d5d3ef46":"markdown","6439c37a":"markdown","72a5f750":"markdown","4bb2d355":"markdown","2c055258":"markdown","c71aeced":"markdown","70e9d8f8":"markdown","f2233ebe":"markdown","c387e985":"markdown","7a0d1d57":"markdown","c23f40c9":"markdown","eef1aaf0":"markdown","b3717460":"markdown","9d776933":"markdown","68e12bca":"markdown","2ed3e6ed":"markdown"},"source":{"cb5b5921":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb872de2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.exceptions import FitFailedWarning\nwarnings.filterwarnings(action='ignore', category=FitFailedWarning)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n","9819aa48":"df = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf = df.rename({'concave points_mean': 'concave_points_mean','concave points_worst': 'concave_points_worst'}, axis=1) \npd.set_option(\"display.max_columns\", None)","051b0ec8":"df.shape","f2c84341":"df.isnull().sum()","f5b2745f":"df=df.drop(['id','Unnamed: 32'],axis=1)\ndf=pd.get_dummies(df,drop_first=True)","02a1bb5a":"plt.figure(figsize=(20,20))\ncor=df.corr()\nsns.heatmap(cor,annot=True)","77a4a42f":"max_thresold2 = df['perimeter_mean'].quantile(0.99)\nmax_thresold2\ndf[df['perimeter_mean']>max_thresold2]","22f54311":"min_thresold2 = df['perimeter_mean'].quantile(0.01)\nmin_thresold2\ndf[df['perimeter_mean']<min_thresold2]","45be4c14":"max_thresold3 = df['area_mean'].quantile(0.99)\nmax_thresold3\ndf[df['area_mean']>max_thresold3]","48a80679":"min_thresold3 = df['area_mean'].quantile(0.01)\nmin_thresold3\ndf[df['area_mean']<min_thresold3]","dda8d3d7":"max_thresold = df['radius_worst'].quantile(0.99)\nmax_thresold\ndf[df['radius_worst']>max_thresold]","d17ba54a":"min_thresold = df['radius_worst'].quantile(0.01)\nmin_thresold\ndf[df['radius_worst']<min_thresold]","3306015b":"max_thresold4 = df['concave_points_mean'].quantile(0.99)\nmax_thresold4\ndf[df['concave_points_mean']>max_thresold4]","a24d046d":"min_thresold4 = df['concave_points_mean'].quantile(0.01)\nmin_thresold4\ndf[df['concave_points_mean']<min_thresold4]","26b1b4a6":"max_thresold5 = df['perimeter_worst'].quantile(0.99)\nmax_thresold5\ndf[df['perimeter_worst']>max_thresold5]","5c587c97":"min_thresold5 = df['perimeter_worst'].quantile(0.01)\nmin_thresold5\ndf[df['perimeter_worst']<min_thresold5]","d8ad94c1":"max_thresold6 = df['area_worst'].quantile(0.99)\nmax_thresold6\ndf[df['area_worst']>max_thresold6]","a8a7a2b0":"min_thresold10 = df['area_worst'].quantile(0.01)\nmin_thresold10\ndf[df['area_worst']<min_thresold10]","0b8db23c":"max_thresold7 = df['concave_points_worst'].quantile(0.99)\nmax_thresold7\ndf[df['concave_points_worst']>max_thresold7]","af403c54":"min_thresold8 = df['concave_points_worst'].quantile(0.01)\nmin_thresold8\ndf[df['concave_points_worst']<min_thresold8]","e5b18631":" df = df.drop(df.index[[46,82,101,151,180,352,461,538,539,568]])","2ab25469":"X=df[['radius_worst','concave_points_mean','perimeter_worst','concave_points_worst']]\ny=pd.DataFrame(df['diagnosis_M'])\ny=pd.get_dummies(y,drop_first=True)","e941f276":"plt.figure(figsize=(20,20))\ncorr=X.corr()\nsns.heatmap(corr,annot=True)","210921e6":"X1=df[['concave_points_mean','perimeter_worst','concave_points_worst']]","ecbfbb86":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X1,y,test_size=0.23)","1d9d9cba":"gnb = GaussianNB()\ngnb.fit(X_train,y_train)","26862cd2":"y_pred1 = gnb.predict(X_test)","2450b4d4":"print(confusion_matrix(y_test,y_pred1))\nprint(accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","397aa30d":"X2=df[['radius_mean','perimeter_mean','area_mean','concavity_mean','concave_points_mean','radius_worst','perimeter_worst','area_worst','concavity_worst','concave_points_worst']]","7950a95e":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X2,y,test_size=0.23)","65da04c8":"logReg=LogisticRegression(solver='liblinear')\n","ba9f164e":"logReg.fit(X_train,y_train)","fd9a6190":"pred2=logReg.predict(X_test)","12d9aad4":"print(confusion_matrix(y_test,pred2))\nprint(accuracy_score(y_test,pred2))\nprint(classification_report(y_test,pred2))","e0b57a1b":" #Due credits to Mr.Kunal Naik for the code: https:\/\/www.kaggle.com\/funxexcel\/p2-logistic-regression-hyperparameter-tuning\nparam_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]","d763bc80":"clf = GridSearchCV(logReg, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)","6c518171":"best_clf = clf.fit(X2,y)","ed13b8fa":"best_clf.best_estimator_","950e30de":"print (f'Accuracy - : {best_clf.score(X2,y):.3f}')","721e5a56":"###   Discussion: So, what we just did from cell 7 till 20? We are getting top 1%(odd cells from 7 till 19) and bottom 1%(even cells from 8 till 20) of the features that are highly correlated with our output, we observe that the values with row# 82, 180, 352, and 461 are repeatedly coming as outlier in the top 1% and the values with row# 46, 101, 151, 538, 539 and 568 are repeatedly coming as outlier in the bottom 1%, it would be better to drop these rows.","499c1da2":"# Feature Selection","2c74b15a":"###   Discussion: Observe that the feature 'radius_worst' has a correlation of 0.99 'perimeter_worst', and we don't want that. Ideally, we want our independent features to be highly correlated with the target variable but we don't want the independnet features to be highly correlated with each other because that complicates our model and secondly it acts as a redundant feature. Let's drop 'radius_worst'","d5d3ef46":"# **Conclusion:** By using only 3 highly positively correlated features and dropping 27, we were able to get ~95% accuracy and by using top 10 highly positively correlated features and hyper parameter tuning we were able to achieve ~97% accuracy.  ","6439c37a":"###   Discussion: Let's select all the feature with correlation greater than 0.75","72a5f750":"###   Discussion: Observe the diagnosis_M feature(which is our target) and note that features with the highest correlation with diagnosis_M lie between 0.7 and 0.8, we will select these features (as they are having the most impact) and find the the recurring outlier using percentile method.","4bb2d355":"# **Here we go**\n1. You can get dataset from here https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data \n2. Our aim is not to get 100% accurate model, but to select minimum number of features that carry maximum information and hence saves time and computational power.\n3. I am yet to see solutions from the community, didn't see it while making the model, in hope to add something new.","2c055258":"# **Feature Engineering**","c71aeced":"# Hyper parameter Tuning ","70e9d8f8":"##  Quick Note: According to my best knowledge we cant perform hyperparameter tuning on Na\u00efve Bayes.","f2233ebe":"##   2) Handling Outliers","c387e985":"###   Discussion Seems like the column \"Unnamed: 32\" is empty, we can drop \"id\" and \"Unnamed\".","7a0d1d57":"###    1) Handling Missing Values","c23f40c9":"###   Observation: Logistic Regression is giving better results this time with multiple runs.","eef1aaf0":"# **The End**","b3717460":"###   Approach: We will first select the independent features (X) that are highly (positively or negatively) correlated with the dependent feature (y), and then drop the the common coutliers of these highly correlated features.","9d776933":"#  **Calling all the Necessary Libraries**","68e12bca":"# Second Approach: Let's select all features that have a correlation greater than 0.7 with the output","2ed3e6ed":"###   **Observation**: Among all the classification algorithms including Logistic Regression, Na\u00efve Bayes, Stochastic Gradient Descent, K-Nearest Neighbours, Decision Tree, Random Forest, Support Vector Machine, the \"Na\u00efve Bayes\" algorithm is giving best accuracy with multiple runs."}}