{"cell_type":{"41480bd7":"code","b8a09f87":"code","4512b6c1":"code","202d83e6":"code","d2db7ea6":"code","7a3d3a00":"code","910054bc":"code","ddb6515b":"code","ff3e9768":"code","59d5b0d5":"code","6cec454b":"code","0d84d712":"code","d78c5a86":"code","62e2711f":"code","cf144635":"code","96989968":"code","a7385079":"code","9fc13c04":"code","244d4a91":"code","79a9bc4f":"code","463bc2f3":"code","56c27533":"markdown","e23c5df5":"markdown","95b3678a":"markdown","53b06399":"markdown","5ac600b2":"markdown","1f478026":"markdown","e47bbf99":"markdown","55678d07":"markdown","ca6e6d62":"markdown","69f478d7":"markdown","44fb10d0":"markdown","f9fb0486":"markdown","dfae39d7":"markdown"},"source":{"41480bd7":"!pip install missingno","b8a09f87":"import numpy as np\nimport regex as re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport nltk\nimport re\nimport csv\nimport seaborn as sns\n","4512b6c1":"tweets=pd.read_csv('Random Tweets from Pakistan- Cleaned- Anonymous.csv')","202d83e6":"\ntweets.head()","d2db7ea6":"tweets.shape","7a3d3a00":"tweets.drop_duplicates(subset =\"full_text\",\n                     keep = False, inplace = True)","910054bc":"tweets.head()","ddb6515b":"tweets.columns","ff3e9768":"location = tweets.groupby('location').count()\nlocation.sort_values(by='full_text', ascending=False).iloc[0]","59d5b0d5":"tweets['location'].value_counts()","6cec454b":"location","0d84d712":"tweets.groupby('location').count().sort_values(by='full_text').iloc[-1]","d78c5a86":"tweets.shape","62e2711f":"#PREPROCESSING USING REGULAR EXPRESSION\ndef clean_tweets(tweet):\n    \n    # remove URL\n#    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n    \n    # Remove usernames\n    tweet = re.sub(r\"@[^\\s]+[\\s]?\",'',tweet)\n    \n    # remove special characters \n#    tweet = re.sub('[^ a-zA-Z0-9]', '', tweet)\n    \n    \n    # remove Numbers\n    tweet = re.sub('[0-9]', '', tweet)\n    \n    \n    tweet = re.sub(r\"^https:\/\/t.co\/[a-zA-Z0-9]*\\s\", \" \", tweet)\n    tweet = re.sub(r\"\\s+https:\/\/t.co\/[a-zA-Z0-9]*\\s\", \" \", tweet)\n    tweet = re.sub(r\"\\s+https:\/\/t.co\/[a-zA-Z0-9]*$\", \" \", tweet)\n    tweet = tweet.lower()\n    tweet = re.sub(r\"that's\",\"that is\",tweet)\n    tweet = re.sub(r\"there's\",\"there is\",tweet)\n    tweet = re.sub(r\"what's\",\"what is\",tweet)\n    tweet = re.sub(r\"where's\",\"where is\",tweet)\n    tweet = re.sub(r\"it's\",\"it is\",tweet)\n    tweet = re.sub(r\"who's\",\"who is\",tweet)\n    tweet = re.sub(r\"i'm\",\"i am\",tweet)\n    tweet = re.sub(r\"she's\",\"she is\",tweet)\n    tweet = re.sub(r\"he's\",\"he is\",tweet)\n    tweet = re.sub(r\"they're\",\"they are\",tweet)\n    tweet = re.sub(r\"who're\",\"who are\",tweet)\n    tweet = re.sub(r\"ain't\",\"am not\",tweet)\n    tweet = re.sub(r\"wouldn't\",\"would not\",tweet)\n    tweet = re.sub(r\"shouldn't\",\"should not\",tweet)\n    tweet = re.sub(r\"can't\",\"can not\",tweet)\n    tweet = re.sub(r\"couldn't\",\"could not\",tweet)\n    tweet = re.sub(r\"won't\",\"will not\",tweet)\n#    tweet = re.sub(r\"\\W\",\" \",tweet)\n    tweet = re.sub(r\"\\d\",\" \",tweet)\n    tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \",tweet)\n    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet)\n    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet)\n    tweet = re.sub(r\"\\s+\",\" \",tweet)\n    \n    \n    return tweet\n\n\n\n","cf144635":"\ntweets['full_text'] = tweets['full_text'].apply(clean_tweets)\n\n\n\n","96989968":"tweets.head()\n","a7385079":"tweets.isnull().sum()\n","9fc13c04":"#tweets = tweets.drop(['id_tweet','retweet_count','favorite_count','reply_count','name','screen_name','user','date','time'], axis=1)","244d4a91":"tweets.head()","79a9bc4f":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\ncomment_words = ''\nstopwords = set(STOPWORDS)\n\nfor val in tweets.full_text:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,background_color ='black',stopwords = stopwords,min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (15, 15), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","463bc2f3":"import emoji\n\ne = {x for c in tweets['full_text'].values for x in c if x in emoji.UNICODE_EMOJI['en']}\n\nf = '|'.join(e)\n\n# Symbola has more emojis, just wanted to be unique\nemo = WordCloud(background_color='white',font_path='Symbola.ttf', \n                max_words=500, regexp=f, width=1500, height=1000).generate(\" \".join(tweets['full_text'].values))\n\nplt.figure(figsize=(15, 15))\nfig = plt.imshow(emo)\nplt.show()\n","56c27533":"# Furqan Amjad","e23c5df5":"# Words Cloud(use in dataset)","95b3678a":"# Merging Datasets in one Data Frame 'tweets'","53b06399":"# Drop_duplicates","5ac600b2":"# Cleaning Dataset for better analysis","1f478026":"##### Now we have a get in clean foam so we go for furthers steps","e47bbf99":"#### Lahore is a city that user tweets most","55678d07":"# Most common Location In dataset\n","ca6e6d62":"# Loading And Merging of Dataset Extraction Common Features","69f478d7":"### Thanks","44fb10d0":"# Emogi Cloud(use in dataset)","f9fb0486":"# Total Size of dataset","dfae39d7":"# Important Libaries"}}