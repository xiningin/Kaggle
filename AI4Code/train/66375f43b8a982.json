{"cell_type":{"53724286":"code","c014eb56":"code","098c4853":"code","8dfb58fd":"code","c06534af":"code","13a75457":"code","5227743c":"code","210d70b8":"code","f50a343b":"code","5177bc60":"code","e4826a5d":"code","adb01ce6":"code","f7593cc1":"markdown","e275da10":"markdown","a46e9be9":"markdown","33922bf6":"markdown","ff45e6d0":"markdown","fef605c5":"markdown","c48f226a":"markdown","14c64e6c":"markdown","8dae19da":"markdown","728f1646":"markdown","4e4af447":"markdown","0d32a500":"markdown","55f47e8a":"markdown"},"source":{"53724286":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy import stats\nimport warnings\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import Imputer\nfrom xgboost import XGBRegressor\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","c014eb56":"iowa_file_path = '..\/input\/train.csv'\nhome_data = pd.read_csv(iowa_file_path)\n\ntest_file_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_file_path)","098c4853":"# Display vars that have 1 or more missing data points\nprint('Training Data:')\nprint(home_data.isnull().sum().sort_values(ascending=False).head(20))# display top categories with missing data\nprint('---------------------------')\nprint('Test Data:')\nprint(test_data.isnull().sum().sort_values(ascending=False).head(35)) # check in Test data too so you don't build model around vars that don't exist","8dfb58fd":"home_quan = list( home_data.loc[:,home_data.dtypes != 'object'].drop('Id',axis=1).columns.values )\nhome_qual = list( home_data.loc[:,home_data.dtypes == 'object'].columns.values )\n\ntest_quan = list( test_data.loc[:,test_data.dtypes != 'object'].drop('Id',axis=1).columns.values )\ntest_qual = list( test_data.loc[:,test_data.dtypes == 'object'].columns.values )\n\nhome_data_imputed = home_data.copy()\ntest_data_imputed = test_data.copy()","c06534af":"# Impute quantitative data\n# Some Values probably should be 0, i.e. if there is no garage it should have 0 SF area...   \nhome_data_imputed.BsmtHalfBath.fillna(0, inplace=True)\nhome_data_imputed.BsmtFullBath.fillna(0, inplace=True)\nhome_data_imputed.GarageArea.fillna(0, inplace=True)\nhome_data_imputed.GarageCars.fillna(0, inplace=True)    \nhome_data_imputed.TotalBsmtSF.fillna(0, inplace=True)   \nhome_data_imputed.BsmtUnfSF.fillna(0, inplace=True)     \nhome_data_imputed.BsmtFinSF2.fillna(0, inplace=True)    \nhome_data_imputed.BsmtFinSF1.fillna(0, inplace=True)\n\ntest_data_imputed.BsmtHalfBath.fillna(0, inplace=True)\ntest_data_imputed.BsmtFullBath.fillna(0, inplace=True)\ntest_data_imputed.GarageArea.fillna(0, inplace=True)\ntest_data_imputed.GarageCars.fillna(0, inplace=True)    \ntest_data_imputed.TotalBsmtSF.fillna(0, inplace=True)   \ntest_data_imputed.BsmtUnfSF.fillna(0, inplace=True)     \ntest_data_imputed.BsmtFinSF2.fillna(0, inplace=True)    \ntest_data_imputed.BsmtFinSF1.fillna(0, inplace=True)\n\n# Others we will fill with the mean using Imputer:\nquan_imputer = Imputer()\nhome_quan_imputed = quan_imputer.fit_transform(home_data_imputed[home_quan]) # ... this changes it from DataFrame to ndarray\ntest_quan_imputed = quan_imputer.fit_transform(test_data_imputed[test_quan])\n\nhome_quan_imputed = pd.DataFrame(data=home_quan_imputed,columns=home_quan) #This converts back to a dataframe\ntest_quan_imputed = pd.DataFrame(data=test_quan_imputed,columns=test_quan)\n\n# Impute Qualitative data (I don't have SimpleImputer... so we will do manually for selected vars)\n\n# Filling missing values for categorical features# Filli \nhome_data_imputed.GarageCond.fillna('NA', inplace=True)    # Replace with NA\nhome_data_imputed.GarageQual.fillna('NA', inplace=True)       \nhome_data_imputed.GarageType.fillna('NA', inplace=True)          \nhome_data_imputed.BsmtCond.fillna('NA', inplace=True)        \nhome_data_imputed.BsmtQual.fillna('NA', inplace=True)        \nhome_data_imputed.Functional.fillna(home_data_imputed.Functional.mode()[0], inplace=True)  # Replace with mode \nhome_data_imputed.SaleType.fillna(home_data_imputed.SaleType.mode()[0], inplace=True)                \nhome_data_imputed.KitchenQual.fillna(home_data_imputed.KitchenQual.mode()[0], inplace=True)        \nhome_data_imputed.Electrical.fillna(home_data_imputed.Electrical.mode()[0], inplace=True) \n\ntest_data_imputed.GarageCond.fillna('NA', inplace=True)    # Replace with NA\ntest_data_imputed.GarageQual.fillna('NA', inplace=True)       \ntest_data_imputed.GarageType.fillna('NA', inplace=True)          \ntest_data_imputed.BsmtCond.fillna('NA', inplace=True)        \ntest_data_imputed.BsmtQual.fillna('NA', inplace=True)        \ntest_data_imputed.Functional.fillna(test_data_imputed.Functional.mode()[0], inplace=True)  # Replace with mode \ntest_data_imputed.SaleType.fillna(test_data_imputed.SaleType.mode()[0], inplace=True)                \ntest_data_imputed.KitchenQual.fillna(test_data_imputed.KitchenQual.mode()[0], inplace=True)        \ntest_data_imputed.Electrical.fillna(test_data_imputed.Electrical.mode()[0], inplace=True)\ntest_data_imputed.MSZoning.fillna(test_data_imputed.MSZoning.mode()[0], inplace=True) #whoops, missed this originally\n\n# Manually Encode Neighborhood data?\n\n\n# convert qual data back into dataframe\nhome_qual_imputed = home_data_imputed[home_qual]\ntest_qual_imputed = test_data_imputed[test_qual]\n\n# Combine imputed quan, qual data into single data frames... already edited home_data_imputed?","13a75457":"# Lets also Manually encode some importand variables that may have too many dimensions for one-hot encoding\n\n# Neighborhood:\nplt.figure(figsize=(12,7))\nsns.boxplot(x = home_data_imputed['Neighborhood'], y = home_data_imputed['SalePrice'])\n\n# We'll work on this more later...","5227743c":"y = home_data_imputed.SalePrice","210d70b8":"# Distribution Plot:\nsns.distplot(home_data_imputed['SalePrice'], fit = norm)\n(mu, sigma) = norm.fit(home_data_imputed['SalePrice'])\nprint('mu = {:,.2f} and sigma = {:,.2f}'.format(mu, sigma))\n# Q-Q plot:\nfig = plt.figure()\nres = stats.probplot(home_data_imputed['SalePrice'], plot=plt)\nplt.show()\n\n# Apply Log transform to SalePrice (y in training data)\ny = np.log(y)\n# Distribution Plot\nsns.distplot(y, fit = norm)\n(mu, sigma) = norm.fit(y)\nprint('transformed mu = {:,.2f} and transformed sigma = {:,.2f}'.format(mu, sigma))\n# QQ Plot\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)\nplt.show()\n\ny.head(10)","f50a343b":"# Create X\nfeatures = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', \\\n            'BedroomAbvGr', 'TotRmsAbvGrd', 'YearRemodAdd', 'OverallCond', \\\n            'OverallQual', 'BsmtFinSF1', 'GarageCars', 'HalfBath','GrLivArea', \\\n            'TotalBsmtSF', 'BsmtFinSF2', 'BsmtFullBath', 'Fireplaces', 'PoolArea'] # maybe include , 'YrSold'\n\ncat_features = ['BldgType', 'CentralAir', 'BsmtQual', 'Street', 'MSZoning', \\\n                'BsmtExposure','FireplaceQu','PavedDrive','Neighborhood', \\\n               'GarageType', 'LotConfig']; #, 'LotShape', 'LotConfig', 'Electrical', 'Functional', 'GarageType', , 'BsmtQual', 'BsmtCond', 'KitchenQual', 'Street', 'Condition',,'HouseStyle'\n\nall_features = features + cat_features # add new categorical features (so as not to break the sections before)\nX = home_data_imputed.copy()[all_features]\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ntest_X = test_data_imputed.copy()[all_features]","5177bc60":"# One Hot Encode the Categorical Variables\n# Originally, I only one-hot encoded vars that would result in <10 new input cols, however, to include neighborhood, I ended up bumping up that number to 30...\nlow_cardinality_cols_train = [cname for cname in train_X.columns if \n                                train_X[cname].nunique() < 30 and\n                                train_X[cname].dtype == \"object\"] # eligible to be one-hot encoded if it is type object and <10 unique values\nnumeric_cols_train = [cname for cname in train_X.columns if \n                                train_X[cname].dtype in ['int64', 'float64']] # numerical data if integer or float\n\nmy_cols_train = low_cardinality_cols_train + numeric_cols_train\n#my_cols_val = low_cardinality_cols_val + numeric_cols_val\n#print(my_cols_train)\n\ntrain_predictors = train_X[my_cols_train]\nval_predictors = val_X[my_cols_train] #no need to include any columns that don't exist in training set?\n\n# One-hot Encode Categorical data for trianing and validation sets. include only data that exists in both\none_hot_train_predictors = pd.get_dummies(train_predictors)\none_hot_val_predictors = pd.get_dummies(val_predictors)\none_hot_train_X, one_hot_val_X = one_hot_train_predictors.align(one_hot_val_predictors,\n                                                                   join='inner', \n                                                                    axis=1)\none_hot_X = one_hot_train_X.append(one_hot_val_X) # Train from this!!! (with y)\n\n# One Hot Encode Test data for contest submission:\ntest_predictors = test_X[my_cols_train]\none_hot_test_predictors = pd.get_dummies(test_predictors)","e4826a5d":"# Initial Pass:\nmy_XGB_model = XGBRegressor()\nmy_XGB_model.fit(one_hot_train_X, train_y, verbose=False)\n\n# make predictions\nXGB_predictions = my_XGB_model.predict(one_hot_val_X)\nXGB_predictions = np.exp(XGB_predictions)\n# Print MAE for initial XGB model\nXGB_mae = mean_absolute_error(XGB_predictions, np.exp(val_y))\nprint(\"Validation MAE for XGBoost Model : \" + str(XGB_mae))\n      \n# Additional Passes\nmy_XGB_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_XGB_model.fit(one_hot_train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(one_hot_val_X, val_y)], verbose=False)\nXGB_predictions = my_XGB_model.predict(one_hot_val_X)\nXGB_predictions = np.exp(XGB_predictions)\nXGB_mult_mae = mean_absolute_error(XGB_predictions, np.exp(val_y))\nprint(\"Validation MAE for multi-pass XGBoost Model : \" + str(XGB_mult_mae))\n\n# Predict SalePrice on Test Data:\nfinal_predictions = my_XGB_model.predict(one_hot_test_predictors)\nfinal_predictions = np.exp(final_predictions)\n\nprint('\\n\\n ---------------------------------------- \\n\\n')\nprint(final_predictions)","adb01ce6":"output = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': final_predictions})\noutput.to_csv('submission.csv', index=False)","f7593cc1":"# Let's also try transforming our target variable, since it's not normally distributed..","e275da10":"# Test Your Work\nAfter filling in the code above:\n1. Click the **Commit and Run** button. \n2. After your code has finished running, click the small double brackets **<<** in the upper left of your screen.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n3. Go to the output tab at top of your screen. Select the button to submit your file to the competition.  \n4. If you want to keep working to improve your model, select the edit button. Then you can change your model and repeat the process.\n\nCongratulations, you've started competing in Machine Learning competitions.\n\n# Continuing Your Progress\nThere are many ways to improve your model, and **experimenting is a great way to learn at this point.**\n\nThe best way to improve your model is to add features.  Look at the list of columns and think about what might affect home prices.  Some features will cause errors because of issues like missing values or non-numeric data types. \n\nLevel 2 of this course will teach you how to handle these types of features. You will also learn to use **xgboost**, a technique giving even better accuracy than Random Forest.\n\n\n# Other Courses\nThe **[Pandas course](https:\/\/kaggle.com\/Learn\/Pandas)** will give you the data manipulation skills to quickly go from conceptual idea to implementation in your data science projects. \n\nYou are also ready for the **[Deep Learning](https:\/\/kaggle.com\/Learn\/Deep-Learning)** course, where you will build models with better-than-human level performance at computer vision tasks.\n\n---\n**[Course Home Page](https:\/\/www.kaggle.com\/learn\/machine-learning)**\n\n**[Learn Discussion Forum](https:\/\/kaggle.com\/learn-forum)**.\n","a46e9be9":"# Explore Missing Data","33922bf6":"# Load Data:","ff45e6d0":"# One-hot Encode Categorical Variables:","fef605c5":"# Solution For House Price Contest\n\n![](https:\/\/media1.tenor.com\/images\/286156bd33ce64d69f6a2367557392b5\/tenor.gif?itemid=10804810)\n\nThis is my first contest submission. It includes data imputation (manual and automatic), one-hot variable encoding, and random forest model (XGBoosted)\n\nWhile this solution was good enough for an top ~10% entry, I was limited in the features that I was able to use by when I chose to one-hot encode variables. Ideally I would have one-hot encoded the catagorical variables early on, and with the test and training data combined into a single DataFrame. Because I didn't do that, there were some variables missing from the Test data set, that would throw errors and need to be left out. ","c48f226a":"# Impute Data:","14c64e6c":"# XG Boost Model and Validation","8dae19da":"# Manually Encode A Couple of Variables","728f1646":"# Split Training Data into target  (sale price) and explanatory vars","4e4af447":"# Split home_data and test_data into quantitative and qualitative (will be imputed differently)","0d32a500":"# Imports:","55f47e8a":"#  Write output CSV file of predictions from test data for contest submission"}}