{"cell_type":{"40b2880b":"code","c5d0d8c4":"code","21940809":"code","502cbdaa":"code","b093b68e":"code","1b5a625a":"code","f163b523":"code","aeb7d40f":"markdown","1b7e0762":"markdown","657173c7":"markdown","d0bef338":"markdown"},"source":{"40b2880b":"%pwd","c5d0d8c4":"import pandas as pd\nCANCERDATA = \"..\/input\/DS_WDBC_NOIDFIELD.data\"\ntd = pd.read_csv(CANCERDATA, header=None)\ntd.head()","21940809":"import math\nimport collections\ndef dist(a, b):\n    sqSum = 0\n    for i in range(len(a)):\n        sqSum += (a[i] - b[i]) ** 2\n    return math.sqrt(sqSum)\n# ------------------------------------------------ #\n# We are assuming that the label is the last field #\n# If not, munge the data to make it so!            #\n# ------------------------------------------------ #\ndef kNN(k, train, given):\n    distances = []\n    for t in train:\n        distances.append((dist(t[:-1], given), t[-1]))\n    distances.sort()\n    #print(distances[:k])\n    return distances[:k]\n\ndef kNN_classify(k, train, given):\n    tally = collections.Counter()\n    for nn in kNN(k, train, given):\n        tally.update(nn[-1])\n    return tally.most_common(1)[0]","502cbdaa":"wdbc = pd.read_csv(CANCERDATA, header=None)\nwdbc.values","b093b68e":"import random\nTRAIN_TEST_RATIO = 0.8\ntrain = []\ntest = []\ndata = wdbc.values\nfor one in data:\n    if random.random() < TRAIN_TEST_RATIO:\n        train.append(one)  \n    else:\n        test.append(one)","1b5a625a":"print(kNN_classify(5, train, test[0])[0])\n\nprint(kNN_classify(5, train, test[0])[0], test[0][-1])\nprint(kNN_classify(5, train, test[4])[0], test[4][-1])\n","f163b523":"results=[]\nfor i,t in enumerate(test):\n  results.append(kNN_classify(5,train,t)[0] == test[i][-1])\nprint(results.count(True), \"are correct\")\nprint(results.count(True)\/len(test))","aeb7d40f":"### Data Attributes\nNumber of instances: 569\n\nNumber of attributes: 31 (diagnosis, 30 real-valued input features)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 \/ area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry \nj) fractal dimension (\"coastline approximation\" - 1)\nThe mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 1 is Mean Radius, field 11 is Radius SE, field 21 is Worst Radius. All feature values are recoded with four significant digits.\n\nThe last field is diagnosis: M for Malignant and B for Benign\n\nClass distribution: 357 benign, 212 malignant","1b7e0762":"But where are the test data? In practice, as a designer of the algorithm you are given only one set of data. The real test data is with your teacher\/examiner\/customer. The standard way is to create a small validation data from your training data and use it for evaluating the performance and also for parameter tuning. Let us randomly split the data into 80:20 ratio. We will use 80% for training and the rest 20% for evaluating the performance.","657173c7":"Implement the euclidean distance using math and collection libraries","d0bef338":"In this experiment, we will use Wisconsin Breast Cancer data to detect malignant cells.\n\n### Data Source\n\nhttps:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/breast-cancer-wisconsin\/wdbc.data\n\nThe data has been modified:\n\nThe id field has been removed\nThe diagnosis field has been moved to the end"}}