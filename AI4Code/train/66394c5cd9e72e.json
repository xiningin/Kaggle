{"cell_type":{"bf79ad7e":"code","555e1ec0":"code","0c07fd5c":"code","765ecd59":"code","3493dde0":"code","fbcf1da0":"code","8bd8af8b":"code","2d7132c3":"code","7ef26868":"code","7112dd55":"code","4b3b41c9":"code","8be1c9a2":"code","b2b66541":"code","ef729279":"code","4f89c389":"code","e21e423a":"code","9a3bfd89":"markdown","4a0a0ed0":"markdown","5c27d9e2":"markdown","f627dd66":"markdown","e0e2b768":"markdown","267ef2ce":"markdown","ed26c379":"markdown","ed1c0370":"markdown","6bb9556b":"markdown","9f5bf939":"markdown","fb01212f":"markdown","d07cdf0f":"markdown","30018c9b":"markdown","69eb6ab9":"markdown","750958f6":"markdown","56c61da4":"markdown","a13f06fc":"markdown","7f08f478":"markdown","38b32bb3":"markdown","814e5d9a":"markdown","429f037b":"markdown"},"source":{"bf79ad7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","555e1ec0":"data = pd.read_csv(\"\/kaggle\/input\/machine-learning-for-diabetes-with-python\/diabetes_data.csv\")","0c07fd5c":"data.head(10)","765ecd59":"y = data.Outcome.values\nx_data = data.drop([\"Outcome\"],axis=1)","3493dde0":"x_data.head()","fbcf1da0":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values","8bd8af8b":"x.head()","2d7132c3":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n","7ef26868":"#%% Parameter initialize and sigmoid function\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    \n    y_head = 1 \/ (1+np.exp(-z))\n    \n    return y_head","7112dd55":"#%% Forward and Backward Propagation\n\ndef forward_backward_propagation(w,b,x_train,y_head):\n    \n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head) - (1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    \n    #backward propogation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients\n","4b3b41c9":"#%% Updating (Learning) Parameters\n    \ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost)) #if section defined to print our cost values in every 10 iteration. We do not need to do that. It's optional.\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","8be1c9a2":"#%% Prediction\n\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is one means has diabete (y_head=1),\n    # if z is smaller than 0.5, our prediction is zero means does not have diabete (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n#predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","b2b66541":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n\n    # Print train\/test Errors\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","ef729279":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 200)","4f89c389":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 4, num_iterations = 100)","e21e423a":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Test Accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n","9a3bfd89":"<a id = '3'><\/a><br>\n# Train-Test Split","4a0a0ed0":"<a id = '8'><\/a><br>\n## Prediction","5c27d9e2":"Sklearn has a great library to do all the codes down below. I'm doing this in order to understand basic maths behind it.","f627dd66":"### * We will convert this Logistic Regression graph to python code!","e0e2b768":"# Table of Contents\n<font color = 'blue'>\n\n1. [Load and Check Data](#1)\n2. [Normalization of x_data Feature's](#2)\n1. [Train-Test Split](#3)\n1. [Defining Neccesary Functions](#4)\n    1. [Parameter Initialize and Sigmoid Function](#5)\n    1. [Forward and Backward Propagation](#6)\n    1. [Updating (Learning) Parameters](#7)\n    1. [Prediction](#8)\n    1. [Defining Logistic Regression Function - FINAL STEP AND RESULT](#9)\n1. [With Sklearn](#10)\n    ","267ef2ce":"<a id = '2'><\/a><br>\n# Normalization of x_data Feature's","ed26c379":"* You can get higher accuracy with adjusting the larning speed and number of iterations. But after certain point your accuracy won't change. You can also see that from the cost graph. The graph's derrivative (slope) is decreasing (assume that the slope is positive) with increasing number of iteration. So after certain amount of iteration the cost function won't decrease.","ed1c0370":"In first function we will initialize weights and bias. In second function we will define the sigmoid function.","6bb9556b":"<a id = '10'><\/a><br>\n# With Sklearn","9f5bf939":"<a id = '7'><\/a><br>\n## Updating (Learning) Parameters","fb01212f":"<a id = '9'><\/a><br>\n## Defining Logistic Regression Function","d07cdf0f":"<a id = '1'><\/a><br>\n# Load And Read Data","30018c9b":"<a id = '5'><\/a><br>\n## Parameter Initialize and Sigmoid Function","69eb6ab9":"<a id = '6'><\/a><br>\n## Forward and Backward Propagation","750958f6":"![image.png](attachment:image.png)","56c61da4":"* I hope you can understand the mathematics (purpose of this notebook) behind Logistic Regression. Down below I did logistic regression with sklearn. It may has higher accuracy. If It is, it's because sklearn use other entries. I did not include them in here to make our codes simpler. Main idea in here was understanding the logic.","a13f06fc":"To do backward propagation we need to read the mathematical graph backward. It means doing derrivative operation. After doing that we will store our cost and gradient values.","7f08f478":"After doing backward propagation we will see that we need to update our parameters which are weights and bias. Why we need to do that ? In order to increase accuracy of model.","38b32bb3":"<a id = '4'><\/a><br>\n# Defining Neccesary Functions","814e5d9a":"To do forward propagation we need value of z. It can be seen on graph. After calculating z we put it into sigmoid function we defined. Then we will need losses which it's mathematical model is down below. After calculating loss value we will define cost which is the summation of loss function.","429f037b":"Loss Function: \n![image.png](attachment:image.png)"}}