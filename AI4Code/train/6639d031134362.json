{"cell_type":{"db292895":"code","b42c4caf":"code","11c217cc":"code","ef09a213":"code","0d8dd5b5":"code","b517fefe":"code","9db79a91":"code","72f363f9":"code","7e4047ea":"code","b4b74609":"code","aac1529d":"code","462fe434":"code","62e9f16b":"code","d359a870":"code","849fccaa":"code","95f53597":"code","07904108":"code","7f03302c":"code","3d121de2":"code","3f3571f3":"code","7440c66a":"code","7f396779":"code","ffc9024e":"code","6da37b6d":"code","cf6db2a9":"code","ed38c138":"code","0dcc441e":"code","5e2e5a50":"code","771529d0":"code","b05b55cd":"code","a52bb278":"code","c74ef022":"code","1b6505c5":"code","e21b39c5":"code","590d7598":"code","3502e3af":"markdown","b895a3d8":"markdown","cacf8bb0":"markdown","70659c78":"markdown"},"source":{"db292895":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.datasets import cifar10\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import models,layers\nfrom keras.layers import Dense, Dropout, Activation, Flatten,GlobalAveragePooling2D ,Conv2D, MaxPooling2D\nimport matplotlib.pyplot as plt\n","b42c4caf":"(X_train,y_train),(X_test,y_test)=cifar10.load_data()\nprint(\"There are {} train images and {} test images.\".format(X_train.shape[0], X_test.shape[0]))\nprint('There are {} unique classes to predict.'.format(np.unique(y_train).shape[0]))\nprint( \"There Classes to predict are : {}\".format(np.unique(y_train)))","11c217cc":"class_names=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship''truck']\n","ef09a213":"fig,axis=plt.subplots(nrows=5,ncols=5,figsize=(12,8))\naxis=axis.flatten()\nfor i,ax in zip(range(30),axis):\n    ax.imshow(X_train[i])\n    ind=y_train[i]\n    ax.set_title(y_train[i])\n    ax.axis('off')\nplt.show()\nplt.tight_layout()","0d8dd5b5":"print(X_train.shape)\nprint(X_test.shape)\nprint(X_train[0].shape)  # SHowing shape of image","b517fefe":"# scalling the data\nX_train = X_train\/255\nX_test = X_test\/255","9db79a91":"y_train.shape[0]","72f363f9":"## One Hot Encoding of images\nfrom  tensorflow.keras.utils import to_categorical,plot_model \nprint(y_train.shape,y_test.shape)\nprint(np.unique(y_test),np.unique(y_train))\ny_train=to_categorical(y_train)\ny_test=to_categorical(y_test)\nprint(np.unique(y_test),np.unique(y_train))\nprint(y_train.shape,y_test.shape)","7e4047ea":"from keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import SGD,Adam\nlrr= ReduceLROnPlateau(monitor='val_acc', #Metric to be measured\n                       factor=.01, #Factor by which learning rate will be reduced\n                       patience=3,  #No. of epochs after which if there is no improvement in the val_acc, the learning rate is reduced\n                       min_lr=1e-5) #The minimum learning rate \nbatch_size1= 100\nepochs=50\nlearn_rate=.001\n\nsgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\nadam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","b4b74609":"model=models.Sequential()\n## Add the Layers for Convocalation \nmodel.add(layers.Conv2D(filters=12,kernel_size=(3,3),input_shape=X_train.shape[1:],activation='relu'))\nmodel.add(layers.Conv2D(filters=14,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n#model.add(layers.Dropout(0.2)) \n\n## Add Second Conventional Layer to Model\nmodel.add(layers.Conv2D(filters=16,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(layers.Conv2D(filters=18,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n#model.add(layers.Dropout(0.1)) \n\n## Add third Conventional Layer to Model\nmodel.add(layers.Conv2D(filters=20,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(layers.Conv2D(filters=24,kernel_size=(3,3),activation='relu',padding='same'))\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\n## Flattne Layer\nmodel.add(layers.Flatten())\n#model.add(layers.Dropout(0.2)) \n\n### Classification Segemention to the  \nmodel.add(layers.Dense(36,activation='relu'))\n#model.add(layers.Dropout(0.1)) \nmodel.add(layers.Dense(48,activation='relu'))\n#model.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(64,activation='relu'))\nmodel.add(layers.Dropout(0.1)) \n\n\n#model.add(GlobalAveragePooling2D())\n\n########Output layer for \nmodel.add(layers.Dense(10,activation='softmax'))\nmodel.summary()","aac1529d":"## Printing the Model \nplot_model(model)","462fe434":"## Compile Model \n#model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n","62e9f16b":"## Fit or Run the Model \nhistory=model.fit(X_train,y_train,epochs=40,batch_size=250,verbose=True,validation_data=(X_test,y_test))","d359a870":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.title(\"Showing the Train Vs Test Accuracy\")\nplt.legend(loc='lower right')\n\ntest_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)","849fccaa":"from tensorflow.keras.applications import resnet50 ,VGG19\nresnet_model1 = resnet50.ResNet50(weights='imagenet')\n#VGG19_model2=  VGG19(weights='imagenet')\n#resnet_model.summary()","95f53597":"input_layer = layers.Input(shape=(X_train.shape[1:]))\nresnet_model1 = resnet50.ResNet50(weights='imagenet',input_tensor=input_layer,include_top=False)\n#VGG19_model2 =  VGG19(include_top=False,weights='imagenet',input_shape=input_layer,classes=y_train.shape[1])\nresnet_model1.summary()\n#print(vgg.summary())","07904108":"#access the current last layer of the model and add flatten and dense after it\nlast_layer = resnet_model1.output\nflatten = layers.Flatten()(last_layer)\n#dense1  = layers.Dense(100,activation='relu')(flatten)\n#drop_out1= layers.Dropout(0.1)(dense1)\n#dense1  = layers.Dense(48,activation='relu')(drop_out1)\noutput_layer = layers.Dense(10,activation='softmax')(flatten)\nmodel2 = models.Model(inputs=input_layer,outputs=output_layer)\nmodel2.summary()","7f03302c":"for layer in model2.layers[:-1]:\n  layer.trainable=False\n\nmodel.summary()","3d121de2":"## Reloading the Cifar dataset \n(X_train,y_train),(X_test,y_test)=cifar10.load_data()\n###########Scaling down \nX_train = X_train\/255\nX_test  = X_test\/255\n## One Hot Encoding . \ny_train = to_categorical(y_train) ## One Hot Encoding \ny_test  = to_categorical(y_test)   ## One Hote Encoding \nX_train = resnet50.preprocess_input(X_train)\nX_test  = resnet50.preprocess_input(X_test)","3f3571f3":"from tensorflow import keras\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel2.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\nhistory=model2.fit(X_train,y_train,epochs=25,batch_size=400,verbose=True,validation_data=(X_test,y_test))","7440c66a":"## Experiment 3","7f396779":"## Reloading the Cifar dataset \nfrom sklearn.model_selection import train_test_split\n(x_train,y_train),(x_test,y_test)=cifar10.load_data()\n#x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)\nX_train, X_test, y_train, y_test =train_test_split(x_train,y_train,test_size=.3)\n###########Scaling down \nX_train = X_train\/255\nX_test  = X_test\/255\n## One Hot Encoding . \ny_train=to_categorical(y_train)\ny_test=to_categorical(y_test)\n","ffc9024e":"from keras.preprocessing.image import ImageDataGenerator\n#Data Augmentation Function: Let's define an instance of the ImageDataGenerator class and set the parameters.We have to instantiate for the Train,Validation and Test datasets\ntrain_generator = ImageDataGenerator(rotation_range=2, \n                                    horizontal_flip=True,\n                                    zoom_range=.1 )\n\nval_generator = ImageDataGenerator( rotation_range=2, \n                                    horizontal_flip=True,\n                                    zoom_range=.1)\n\ntest_generator = ImageDataGenerator(rotation_range=2, \n                                    horizontal_flip= True,\n                                    zoom_range=.1) ","6da37b6d":"train_generator.fit(X_train)\nval_generator.fit(X_test)\ntest_generator.fit(X_test)","cf6db2a9":"lrr= ReduceLROnPlateau(\n                       monitor='val_acc', #Metric to be measured\n                       factor=.01, #Factor by which learning rate will be reduced\n                       patience=3,  #No. of epochs after which if there is no improvement in the val_acc, the learning rate is reduced\n                       min_lr=1e-5) #The minimum learning rate ","ed38c138":"#Build the model\nfrom keras.applications import VGG19,ResNet50\n\n'The first base model used is VGG19. The pretrained weights from the imagenet challenge are used'\nbase_model_1 = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n\n'For the 2nd base model we will use Resnet 50 and compare the performance against the previous one.The hypothesis is that Resnet 50 should perform better because of its deeper architecture'\nbase_model_2 = ResNet50(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])","0dcc441e":"model_1= models.Sequential()\nmodel_1.add(base_model_1) #Adds the base model (in this case vgg19 to model_1)\nmodel_1.add(Flatten()) #Since the output before the flatten layer is a matrix we have to use this function to get a vector of the form nX1 to feed it into the fully connected layers\nmodel_1.summary()","5e2e5a50":"#Add the Dense layers along with activation and batch normalization\nmodel_1.add(Dense(1024,activation=('relu'),input_dim=512))\nmodel_1.add(Dense(512,activation=('relu'))) \nmodel_1.add(Dense(256,activation=('relu'))) \n#model_1.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\nmodel_1.add(Dense(128,activation=('relu')))\n#model_1.add(Dropout(.2))\nmodel_1.add(Dense(10,activation=('softmax'))) #This is the classification layer\nmodel_1.summary()","771529d0":"batch_size= 500\nepochs=20\nlearn_rate=.001\n\nsgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\nadam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)","b05b55cd":"model_1.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\nmodel_1.fit_generator(train_generator.flow(X_train,y_train,batch_size=batch_size),\n                      epochs=epochs,\n                      steps_per_epoch=x_train.shape[0]\/\/batch_size,\n                      validation_data=val_generator.flow(X_test,y_test,batch_size=batch_size),validation_steps=250,\n                      callbacks=[lrr],verbose=True)","a52bb278":"## Using Rest Net","c74ef022":"model_2=models.Sequential()\n#Add the Dense layers along with activation and batch normalization\nmodel_2.add(base_model_2)\nmodel_2.add(Flatten())\n\n\n#Add the Dense layers along with activation and batch normalization\nmodel_2.add(Dense(4000,activation=('relu'),input_dim=512))\nmodel_2.add(Dense(2000,activation=('relu'))) \nmodel_2.add(Dropout(.4))\nmodel_2.add(Dense(1000,activation=('relu'))) \nmodel_2.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\nmodel_2.add(Dense(500,activation=('relu')))\nmodel_2.add(Dropout(.2))\nmodel_2.add(Dense(10,activation=('softmax'))) #This is the classification layer","1b6505c5":"model_2.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])","e21b39c5":"history=model_2.fit_generator(train_generator.flow(X_train,y_train,batch_size=batch_size),\n                     epochs=epochs,steps_per_epoch=X_train.shape[0]\/\/batch_size,\n                     validation_data=val_generator.flow(X_test,y_test,batch_size=batch_size),validation_steps=250,\n                      callbacks=[lrr],verbose=True)","590d7598":"f,ax=plt.subplots(2,1) #Creates 2 subplots under 1 column\n\n#Assign the first subplot to graph training loss and validation loss\nax[0].plot(model_2.history.history['loss'],color='b',label='Training Loss')\nax[0].plot(model_2.history.history['val_loss'],color='r',label='Validation Loss')\n\n#Next lets plot the training accuracy and validation accuracy\nax[1].plot(model_2.history.history['accuracy'],color='b',label='Training  Accuracy')\nax[1].plot(model_2.history.history['val_accuracy'],color='r',label='Validation Accuracy')","3502e3af":"# Transfer Learning ","b895a3d8":"# Loading the Dataset ","cacf8bb0":"Making CNN Model","70659c78":"Printing Different Images\n"}}