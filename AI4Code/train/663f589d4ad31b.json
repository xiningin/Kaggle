{"cell_type":{"3a83098c":"code","55a55d68":"code","19561032":"code","e045e358":"code","97a5ad85":"code","9688cf2c":"code","06b2792e":"code","37f3c761":"code","fd46d037":"code","8c99a81c":"code","4544e971":"code","806f8c17":"code","19c1cf31":"code","5b7642ee":"code","625f3566":"markdown","62695881":"markdown","e02ae0cd":"markdown","a1a70cf5":"markdown","baaf128e":"markdown","72cdaed9":"markdown","212fbfbe":"markdown","f3459aa4":"markdown","c93c1834":"markdown"},"source":{"3a83098c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.special import boxcox, inv_boxcox\n\ntrain_df=pd.read_csv('..\/input\/train.csv')\n\ntrain_df.describe()","55a55d68":"sns.boxplot(train_df['count'])\nplt.show()\n\ncnt=train_df['count'].values\nq99=np.percentile(cnt,[99])\n\ntrain_df=train_df[train_df['count']<q99[0]]\nsns.distplot(train_df['count'])\nplt.show()","19561032":"from scipy.stats import boxcox\nsns.distplot(boxcox(train_df['count'])[0])\nplt.show()\n\nsns.distplot(train_df['count'].apply(lambda x:np.log(x)))\nplt.show()\n\nsns.distplot(train_df['count'].apply(lambda x:x**0.5))\nplt.show()\ntrain_df['count']=train_df['count'].apply(lambda x:np.log(x))","e045e358":"\n\nfrom datetime import datetime\n\n#converting string dattime to datetime\n\n\ntrain_df['datetime']=train_df['datetime'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n\nnew_df=train_df\n\nnew_df['month']=new_df['datetime'].apply(lambda x:x.month)\nnew_df['hour']=new_df['datetime'].apply(lambda x:x.hour)","97a5ad85":"new_df.cov()\nsns.heatmap(new_df.corr())\nplt.show()\n\nsns.boxplot('windspeed',data=train_df)\n","9688cf2c":"new_df.corr()","06b2792e":"'''\nhumid + temp + month + hour + season + weather\nhumid + temp + month + hour\nhumid + temp + month\nhumid + temp\n'''\n\nfinal_df=new_df.drop(['datetime', 'holiday', 'workingday', 'atemp', 'windspeed', 'casual', 'registered'], axis=1)\n\n#final_df=new_df.drop(['datetime', 'holiday', 'workingday', 'atemp', 'windspeed', 'casual', 'registered', 'season', 'weather'], axis=1)\n\n#final_df=new_df.drop(['datetime', 'holiday', 'workingday', 'atemp', 'windspeed', 'casual', 'registered', 'season', 'weather', 'hour'], axis=1)\n\n#final_df=new_df.drop(['datetime', 'holiday', 'workingday', 'atemp', 'windspeed', 'casual', 'registered', 'season', 'weather', 'hour', 'month'], axis=1)\n\n\nfinal_df.head()\n\n\n","37f3c761":"\nweather_df=pd.get_dummies(new_df['weather'],prefix='w',drop_first=True)\n#year_df=pd.get_dummies(new_df['year'],prefix='y',drop_first=True)\nmonth_df=pd.get_dummies(new_df['month'],prefix='m',drop_first=True)\nhour_df=pd.get_dummies(new_df['hour'],prefix='h',drop_first=True)\nseason_df=pd.get_dummies(new_df['season'],prefix='s',drop_first=True)\n                     \n\n\nfinal_df=final_df.join(weather_df)\n#final_df=final_df.join(year_df)\nfinal_df=final_df.join(month_df)                     \nfinal_df=final_df.join(hour_df)\nfinal_df=final_df.join(season_df)\n                     \nfinal_df.head()\n\n'''\n#weather_df=pd.get_dummies(new_df['weather'],prefix='w',drop_first=True)\n# year_df=pd.get_dummies(new_df['year'],prefix='y',drop_first=True)\nmonth_df=pd.get_dummies(new_df['month'],prefix='m',drop_first=True)\nhour_df=pd.get_dummies(new_df['hour'],prefix='h',drop_first=True)\n#season_df=pd.get_dummies(new_df['season'],prefix='s',drop_first=True)\n                     \n#final_df=final_df.join(weather_df)\n# final_df=final_df.join(year_df)\nfinal_df=final_df.join(month_df)                     \nfinal_df=final_df.join(hour_df)\n#final_df=final_df.join(season_df)\n'''\n\n'''\n#weather_df=pd.get_dummies(new_df['weather'],prefix='w',drop_first=True)\n# year_df=pd.get_dummies(new_df['year'],prefix='y',drop_first=True)\nmonth_df=pd.get_dummies(new_df['month'],prefix='m',drop_first=True)\n#hour_df=pd.get_dummies(new_df['hour'],prefix='h',drop_first=True)\n#season_df=pd.get_dummies(new_df['season'],prefix='s',drop_first=True)\n                     \n#final_df=final_df.join(weather_df)\n# final_df=final_df.join(year_df)\nfinal_df=final_df.join(month_df)                     \n#final_df=final_df.join(hour_df)\n#final_df=final_df.join(season_df)\n'''\n\n'''\n#weather_df=pd.get_dummies(new_df['weather'],prefix='w',drop_first=True)\n# year_df=pd.get_dummies(new_df['year'],prefix='y',drop_first=True)\n#month_df=pd.get_dummies(new_df['month'],prefix='m',drop_first=True)\n#hour_df=pd.get_dummies(new_df['hour'],prefix='h',drop_first=True)\n#season_df=pd.get_dummies(new_df['season'],prefix='s',drop_first=True)\n                     \n#final_df=final_df.join(weather_df)\n# final_df=final_df.join(year_df)\n#final_df=final_df.join(month_df)                     \n#final_df=final_df.join(hour_df)\n#final_df=final_df.join(season_df)\n'''\n","fd46d037":"\nX=final_df.iloc[:,final_df.columns!='count'].values\nprint (X)\n\nY=final_df.iloc[:,4].values\n# Y=final_df.iloc[:,2].values\n\nprint (Y)","8c99a81c":"import xgboost as xg\nxgr=xg.XGBRegressor(max_depth=8,min_child_weight=6,gamma=0.4)\nxgr.fit(X,Y)\n\n'''import xgboost as xg\nfrom sklearn.model_selection import GridSearchCV\n\ndef grid_search():\n    \n    xgr=xg.XGBRegressor(max_depth=8,min_child_weight=6,gamma=0.4)\n    xgr.fit(X,Y)\n\n    #rf=RandomForestRegressor(n_estimators=100,random_state=0)\n    #rf.fit(X,Y)\n\n    #parameters=[{'max_depth':[8,9,10,11,12],'min_child_weight':[4,5,6,7,8]}]\n    #parameters=[{'gamma':[i\/10.0 for i in range(0,5)]}]\n    parameters=[{'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]}]\n\n    grid_search= GridSearchCV(estimator=xgr, param_grid=parameters, cv=10,n_jobs=-1)\n\n    print (1)\n    grid_search=grid_search.fit(X,Y)\n    print (2)\n    best_accuracy=grid_search.best_score_\n    best_parameters=grid_search.best_params_\n    print (best_accuracy)\n    print (best_parameters)\n\n#if __name__ == '__main__':\n   #grid_search()'''","4544e971":"'''\nfrom sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor(n_estimators=100,random_state=0)\nrf.fit(X,Y)\nimp_list=rf.feature_importances_\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(final_df.columns, rf.feature_importances_):\n    feats[feature] = importance #add the name\/value pair\n''' ","806f8c17":"new_df.head()","19c1cf31":"new_df=pd.read_csv('..\/input\/test.csv')\ntest_df=pd.read_csv('..\/input\/test.csv')\n\nnew_df['datetime']=new_df['datetime'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n\nnew_df['month']=new_df['datetime'].apply(lambda x:x.month)\nnew_df['hour']=new_df['datetime'].apply(lambda x:x.hour)\n\nweather_df=pd.get_dummies(new_df['weather'],prefix='w',drop_first=True)\nmonth_df=pd.get_dummies(new_df['month'],prefix='m',drop_first=True)\nhour_df=pd.get_dummies(new_df['hour'],prefix='h',drop_first=True)\nseason_df=pd.get_dummies(new_df['season'],prefix='s',drop_first=True)\n\n\nnew_df=new_df.join(weather_df)\nnew_df=new_df.join(month_df)                     \nnew_df=new_df.join(hour_df)\nnew_df=new_df.join(season_df)\n\nnew_df=new_df.drop(['datetime', 'holiday', 'workingday', 'atemp', 'windspeed'], axis=1)\nnew_df.head()\n                     ","5b7642ee":"import xgboost as xg\nxgr=xg.XGBRegressor(max_depth=8,min_child_weight=6,gamma=0.4,colsample_bytree=0.6,subsample=0.6)\nxgr.fit(X,Y)\n\nX_test=new_df.iloc[:,:].values\nX_test.shape\n#print (new_df.columns)\n\ny_output=xgr.predict(X_test)\ny_output\n\ntest_df['count'] = pd.Series(np.exp(y_output))\ntest_df = test_df.drop(['humidity', 'temp', 'season', 'holiday', 'workingday', 'weather', 'atemp', 'windspeed'], axis=1)\ntest_df.to_csv('sub1.csv', index=False)\n\n","625f3566":"Pode se notar pelo heatmap o grau de correla\u00e7\u00e3o entre as vari\u00e1veis. Partindo das seguintes infer\u00eancias, foram determinadas quais vari\u00e1veis descartar:\n- Temp(temperatura) e atemp(sensa\u00e7\u00e3o t\u00e9rmica) s\u00e3o fortemente correlacionadas, atemp ser\u00e1 removida para evitar multi-collinearity.\n- O n\u00famero de holidays \u00e9 muito pequeno para poder extrair alguma informa\u00e7\u00e3o \u00fatil(workinday tamb\u00e9m ser\u00e1 removido).\n- A divis\u00e3o do count em casual ou registered n\u00e3o aparenta ter um comportamento significativamente diferente com a maioria das vari\u00e1veis.\n- O windspeed possui muitos outliers, o que pode prejudicar o desempenho.","62695881":"Adding dummy varibles to categorical data","e02ae0cd":"https:\/\/medium.com\/@ODSC\/transforming-skewed-data-for-machine-learning-90e6cc364b0\nComo pode ser visto no [gr\u00e1fico 2] acima, isso \u00e9 um highly skewed data,ou seja, uma distribui\u00e7\u00e3o\"injusta\" que prejudica a classe dos valores menos representados.\n3 modelos foram testados para melhorar isso, sendo respectivamente: o boxcox transformation[explicado no link acima], a raiz quadrada, e o log.\nAs this is a highly skewed data, we will try to transform this data using either log, square-root or box-cox  transformation.\nVisualmente, o \u00faltimo(log), deixa a distribui\u00e7\u00e3o um pouco mais \"balanceada\", al\u00e9m de ajudar a diminuir o impacto da diferen\u00e7a nos valores mais altos.","a1a70cf5":"https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python\n\nO algoritmo escolhido foi o XGBoost(Extreme Gradient Boosting) por ser paraleliz\u00e1vel, ter uma \u00f3tima performance de tempo, \u00e9 ser robusto.\n\nUsa o principio de ensemble, onde v\u00e1rios algoritmos mais fracos s\u00e3o combinados(decision trees) para gerar a saida final. Os que tiverem pior performance recebem um peso maior, ou seja, ter\u00e3o maior foco no treinamento nas demais itera\u00e7\u00f5es.\n\nParameters\nmax_depth (int) \u2013 Maximum tree depth for base learners.\n\nmin_child_weight (int) \u2013 Minimum sum of instance weight(hessian) needed in a child.\n\ngamma (float) \u2013 Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\nsubsample (float) \u2013 Subsample ratio of the training instance.\n\ncolsample_bytree (float) \u2013 Subsample ratio of columns when constructing each tree.\n\n","baaf128e":"\nhttps:\/\/stats.libretexts.org\/Bookshelves\/Introductory_Statistics\/Book%3A_Introductory_Statistics_(Shafer_and_Zhang)\/02%3A_Descriptive_Statistics\/2.5%3A_The_Empirical_Rule_and_Chebyshev's_Theorem\n\nChebychev's rule \nEssa regra preve empiricamente que 99% dos dados est\u00e3o contidos noo intervalo de at\u00e9 3 desvios padr\u00f5es da m\u00e9dia. Isso pode ser usado para limpar alguns outliers [gr\u00e1fico 1]. ","72cdaed9":"Introdu\u00e7\u00e3o\n\nO projeto consiste em dar uma resposta para o desafio \"Bike Sharing Demand\". Um problema de regress\u00e3o onde se \u00e9 dado o n\u00famero de alugueis de bicicleta por hora dos primeiros 20 dias do m\u00eas e \u00e9 pedido uma previs\u00e3o do n\u00famero de alugueis para os demais dias de cada m\u00eas. Ap\u00f3s um pequeno tratamento dos dados e uma an\u00e1lise das vari\u00e1veis, foi usado o XGBoost Regressor para fazer a previs\u00e3o.","212fbfbe":"Centro de Inform\u00e1tica - CIn\/UFPE\nProfessor:Cleber Zanchettin\n\nPROJETO APRENDIZAGEM DE M\u00c1QUINA\n\n \nAlunos Gradua\u00e7\u00e3o:\nGiulio Carvalho Cavalcante - gcc\nPedro Kempter Brant - pkb","f3459aa4":"Score calculado a partir do erro quadr\u00e1tico m\u00e9dio\n\nResultados \n\nhumid + temp\nPublic Score\t1.20115\n\nhumid + temp + month\nPublic Score\t1.23954\n\nhumid + temp + month + hour\nPublic Score\t0.68263\n\nhumid + temp + month + hour + season + weather\nPublic Score\t0.65384\n\nNota-se uma melhora significativa quando a hora \u00e9 levada em considera\u00e7\u00e3o. O resultado final ficou relativamente pr\u00f3ximo dos primeiros colocados(~0.3), demonstrando a efetividade do XGBoost.\n","c93c1834":"Separando o datetime em vari\u00e1veis possivelmente mais significativas, como hora\/dia\/mes."}}