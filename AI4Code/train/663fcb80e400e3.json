{"cell_type":{"78ee1913":"code","3416e8a8":"code","84574abd":"code","4456f62b":"code","446d1782":"code","90ef77ed":"code","6911a545":"code","90bb8f2f":"code","99263994":"code","cfc3436e":"code","092a18cc":"code","28c04153":"code","d021d121":"code","d0d295ac":"code","f7bb473f":"code","9a123de4":"code","44d78993":"code","0a6f7bc5":"code","ff6e366e":"code","f1fcac7a":"code","11c10df8":"code","d1f32c15":"code","d5769f48":"code","28f50511":"code","39104c78":"code","4521dbb2":"code","4b650ad2":"code","5797c934":"code","35fa6811":"code","6a7696df":"code","06eb5d7e":"code","f8653026":"code","5405f6a9":"code","13318095":"code","985d1b94":"markdown","87fd738e":"markdown","7beddb02":"markdown","32966d9c":"markdown","57fa3db5":"markdown","0a8267fd":"markdown","c7e2791d":"markdown","595ce8d0":"markdown","c4fb7c6b":"markdown","3d40ed53":"markdown","c07beb18":"markdown","18cafc5f":"markdown","0e5c9cac":"markdown","a8bbddac":"markdown","196753e9":"markdown","24a1151b":"markdown","bd0622fb":"markdown","607ef17a":"markdown"},"source":{"78ee1913":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom xgboost import XGBRegressor\n\n# for feature importance study\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\n\n# Custom theme\nplt.style.use('fivethirtyeight')\n\nfigure = {'dpi': '200'}\nfont = {'family': 'serif'}\ngrid = {'linestyle': ':', 'alpha': .9}\naxes = {'titlecolor': 'black', 'titlesize': 20, 'titleweight': 'bold',\n        'labelsize': 12, 'labelweight': 'bold'}\n\nplt.rc('font', **font)\nplt.rc('figure', **figure)\nplt.rc('grid', **grid)\nplt.rc('axes', **axes)\n\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', \n             '#4ddbff', '#1E90FF', '#663399', '#708090']\n\ncaption = \"\u00a9 maksymshkliarevskyi\"\n\n# Show our custom palette\nsns.palplot(sns.color_palette(my_colors))\nplt.title('Custom palette')\nplt.text(6.9, 0.75, caption, size = 8)\nplt.show()","3416e8a8":"base_train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv', \n                         index_col = 0)\nbase_train.index = base_train.index.astype('datetime64[ns]')\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv', \n                    index_col = 0)\ntrain.index = train.index.astype('datetime64[ns]')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv', \n                   index_col = 0)\ntest.index = test.index.astype('datetime64[ns]')\nss = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","84574abd":"train.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","4456f62b":"test.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","446d1782":"dtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1],\n         label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13,\n           bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(10, -0.9, caption, size = 8)\nplt.show()","90ef77ed":"target_carbon_monoxide = train.target_carbon_monoxide\ntarget_benzene = train.target_benzene\ntarget_nitrogen_oxides = train.target_nitrogen_oxides\n\ntrain.drop(['target_carbon_monoxide', 'target_benzene', \n            'target_nitrogen_oxides'], \n           axis = 1, inplace = True)","6911a545":"# Concatenate train and test datasets\nall_data = pd.concat([train, test], axis = 0)\n\n# columns with missing values\ncols_with_na = all_data.isna().sum()[all_data.isna().sum() > 0].sort_values(ascending = False)\ncols_with_na","90bb8f2f":"fig = plt.figure(figsize = (15, 10))\nfig.suptitle('Train data', size = 25, weight = 'bold')\nfor idx, i in enumerate(train.columns):\n    fig.add_subplot(np.ceil(len(train.columns)\/4), 4, idx+1)\n    train.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(1000, -200, caption, size = 12)\nplt.show()","99263994":"targets = [target_carbon_monoxide, target_benzene, target_nitrogen_oxides]\ntarget_name = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']\n\nfig = plt.figure(figsize = (15, 10))\nfig.suptitle('Train targets', size = 25, weight = 'bold')\nfor i in range(len(targets)):\n    fig.add_subplot(3, 3, i+1)\n    targets[i].hist(bins = 20)\n    plt.title(target_name[i])\nplt.text(900, -450, caption, size = 12)\nplt.show()","cfc3436e":"fig = plt.figure(figsize = (15, 10))\nfig.suptitle('Test data', size = 25, weight = 'bold')\nfor idx, i in enumerate(test.columns):\n    fig.add_subplot(np.ceil(len(test.columns)\/4), 4, idx+1)\n    test.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(1000, -40, caption, size = 12)\nplt.show()","092a18cc":"corr = base_train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (13, 10))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5, annot = True)\nplt.text(11, 14.5, caption, size = 12)\nplt.show()","28c04153":"print('Min date in the train data: ', min(base_train.index))\nprint('Max date in the train data: ', max(base_train.index))","d021d121":"sns.pairplot(base_train, corner = True)\nplt.text(100, -250, caption, size = 14)\nplt.show()","d0d295ac":"sns.pairplot(test, corner = True)\nplt.text(100, -100, caption, size = 14)\nplt.show()","f7bb473f":"train.plot(figsize = (11, 25), subplots = True, linewidth = 0.8, \n           color = my_colors)\nplt.xlabel('')\nplt.show()","9a123de4":"base_train[target_name].plot(figsize = (10, 10), subplots = True, \n                             linewidth = 0.8, color = my_colors[3:6])\nplt.xlabel('')\nplt.show()","44d78993":"# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, base_train[target_name], \n                                                      test_size = 0.2,\n                                                      random_state = 0,\n                                                      shuffle = False)","0a6f7bc5":"# The basic model\nparams = {'n_estimators': 400,\n          'subsample': 0.8,\n          'max_depth': 8,\n          'learning_rate': 0.05,\n          'n_jobs': -1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.1,\n          'random_state': 0}\n\nmodel1 = XGBRegressor(**params).fit(X_train, y_train.iloc[:, 0])\nmodel2 = XGBRegressor(**params).fit(X_train, y_train.iloc[:, 1])\nmodel3 = XGBRegressor(**params).fit(X_train, y_train.iloc[:, 2])","ff6e366e":"y_pred1 = model1.predict(X_valid)\nprint('RMSLE ({}): {}'.format(target_name[0], round(np.sqrt(mean_squared_log_error(y_valid.iloc[:, 0], y_pred1)), 4)))\ny_pred2 = model2.predict(X_valid)\nprint('RMSLE ({}): {}'.format(target_name[1], round(np.sqrt(mean_squared_log_error(y_valid.iloc[:, 1], y_pred2)), 4)))\ny_pred3 = model3.predict(X_valid)\nprint('RMSLE ({}): {}'.format(target_name[2], round(np.sqrt(mean_squared_log_error(y_valid.iloc[:, 2], y_pred3)), 4)))","f1fcac7a":"date = pd.to_datetime(X_valid.reset_index().date_time).apply(lambda x: x.strftime('%Y\/%m\/%d'))\n\nvalid_preds = pd.DataFrame({'date': date,\n                            'target_carbon_monoxide': y_valid.iloc[:, 0].values,\n                            'target_benzene': y_valid.iloc[:, 1].values,\n                            'target_nitrogen_oxides': y_valid.iloc[:, 2].values,\n                            'preds_carbon_monoxide': y_pred1,\n                            'preds_benzene': y_pred2,\n                            'preds_nitrogen_oxides': y_pred3})\nvalid_preds = valid_preds.groupby('date').mean()","11c10df8":"plt.figure(figsize = (15, 5))\nvalid_preds['target_carbon_monoxide'].plot(color = my_colors[6])\nvalid_preds['preds_carbon_monoxide'].plot(color = my_colors[2])\nplt.legend()\nplt.xlabel('')\nplt.text(52, -0.7, caption, size = 14)\nplt.show()","d1f32c15":"plt.figure(figsize = (15, 5))\nvalid_preds['target_benzene'].plot(color = my_colors[6])\nvalid_preds['preds_benzene'].plot(color = my_colors[2])\nplt.legend()\nplt.xlabel('')\nplt.text(52, -6, caption, size = 14)\nplt.show()","d5769f48":"plt.figure(figsize = (15, 5))\nvalid_preds['target_nitrogen_oxides'].plot(color = my_colors[6])\nvalid_preds['preds_nitrogen_oxides'].plot(color = my_colors[2])\nplt.legend()\nplt.xlabel('')\nplt.text(52, -110, caption, size = 14)\nplt.show()","28f50511":"print('Permutation importance for Model#1 ({})'.format(target_name[0]))\npi1 = PermutationImportance(model1, random_state = 0).fit(X_valid, y_valid.iloc[:, 0])\neli5.show_weights(pi1, feature_names = X_valid.columns.tolist())","39104c78":"print('Permutation importance for Model#2 ({})'.format(target_name[1]))\npi2 = PermutationImportance(model2, random_state = 0).fit(X_valid, y_valid.iloc[:, 1])\neli5.show_weights(pi2, feature_names = X_valid.columns.tolist())","4521dbb2":"print('Permutation importance for Model#3 ({})'.format(target_name[2]))\npi3 = PermutationImportance(model3, random_state = 0).fit(X_valid, y_valid.iloc[:, 2])\neli5.show_weights(pi3, feature_names = X_valid.columns.tolist())","4b650ad2":"explainer = shap.TreeExplainer(model1)\nshap_values = explainer.shap_values(X_valid)\n\nplt.title('Model #1 ({})'.format(target_name[0]))\nshap.summary_plot(shap_values, X_valid)","5797c934":"explainer = shap.TreeExplainer(model2)\nshap_values = explainer.shap_values(X_valid)\n\nplt.title('Model #2 ({})'.format(target_name[1]))\nshap.summary_plot(shap_values, X_valid)","35fa6811":"explainer = shap.TreeExplainer(model3)\nshap_values = explainer.shap_values(X_valid)\n\nplt.title('Model #3 ({})'.format(target_name[2]))\nshap.summary_plot(shap_values, X_valid)","6a7696df":"# Train model on all the data\nparams = {'n_estimators': 400,\n          'subsample': 0.8,\n          'max_depth': 8,\n          'learning_rate': 0.05,\n          'n_jobs': -1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.1,\n          'random_state': 0}\n\nmodel1 = XGBRegressor(**params).fit(train, targets[0])\nmodel2 = XGBRegressor(**params).fit(train, targets[1])\nmodel3 = XGBRegressor(**params).fit(train, targets[2])","06eb5d7e":"ss['target_carbon_monoxide'] = model1.predict(test)\nss['target_benzene'] = model2.predict(test)\nss['target_nitrogen_oxides'] = model3.predict(test)\n\nss.head()","f8653026":"test_date = pd.to_datetime(test.reset_index().date_time).apply(lambda x: x.strftime('%Y\/%m\/%d'))\n\ntest_preds = pd.DataFrame({'date': test_date,\n                            'test_carbon_monoxide': ss['target_carbon_monoxide'],\n                            'test_benzene': ss['target_benzene'],\n                            'test_nitrogen_oxides': ss['target_nitrogen_oxides']})\ntest_preds = test_preds.groupby('date').mean()","5405f6a9":"test_preds.plot(color = my_colors[3:6], subplots = True, figsize = (15, 10))\nplt.xlabel('')\nplt.text(80, -120, caption, size = 14)\nplt.show()","13318095":"ss.to_csv('submission.csv', index = False)","985d1b94":"Let's visualize our data.","87fd738e":"<h2 style='color:white; background:#663399; border:0'><center>Baseline<\/center><\/h2>\n\n[**Back to the start**](#section-start)\n\nNow let's build the simplest XGBRegressor as Baseline.","7beddb02":"For the prediction of `target_carbon_monoxide` the second sensor is the most important.","32966d9c":"It's important to see if our data has missing values.","57fa3db5":"<h2 style='color:white; background:#663399; border:0'><center>WORK IN PROGRESS...<\/center><\/h2>\n\n[**Back to the start**](#section-start)","0a8267fd":"Let's check the results.","c7e2791d":"The relationships between sensors, as well as between sensors and targets, are linear, in some cases logarithmic.","595ce8d0":"<h1 style='color:white; background:#663399; border:0'><center>TPS-July: EDA, Baseline Analysis (XGBRegressor)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)\n\n<a id=\"section-start\"><\/a>\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is based on a real dataset, but has synthetic-generated aspects to it. The original dataset deals with predicting air pollution in a city via various input sensor values (e.g., a time series).\n\nIn this competition you are predicting the values of air pollution measurements over time, based on basic weather information (temperature and humidity) and the input values of 5 sensors.\n\nThe three target values to you to predict are: `target_carbon_monoxide`, `target_benzene`, and `target_nitrogen_oxides`","c4fb7c6b":"The data show a wide range of correlations, from highly negative to highly positive. The maximum linear relationship is observed between target_benzene and the second sensor.","3d40ed53":"<h2 style='color:white; background:#663399; border:0'><center>EDA<\/center><\/h2>\n\n[**Back to the start**](#section-start)","c07beb18":"As in the previous competitions, our data has no missing values. Now, let's look at the feature distributions.","18cafc5f":"The same conclusions about the importance of some indicators can be drawn after analyzing SHAP values. Large values of sensor 2 lead to large values of the predicted indicators (the nature of this dependence is also indicated by a high positive correlation).","0e5c9cac":"But for predicting `target_nitrogen_oxides`, almost all sensors (except for two) are quite important.","a8bbddac":"`target_benzene`, it seems, can only be predicted by the second sensor.","196753e9":"We should also look at the correlation between features.","24a1151b":"We have 7111 training and 2247 test observations. All our data is in `float32` format.\n\nBefore we continue, let's pull the targets into separate variables.","bd0622fb":"First, let's load the data and take a look at basic statistics.","607ef17a":"Our model predicts target_benzene almost perfectly and the other two targets are slightly worse.\n\nNow, we'll see at the permutation importance of features."}}