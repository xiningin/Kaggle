{"cell_type":{"c79961f8":"code","f1a5810c":"code","064e9f16":"code","6bd0dfc0":"code","ae1441ca":"code","c5b0847f":"code","2d2f1040":"code","65071644":"code","0c06cac4":"code","accd9d83":"code","fd4465a5":"markdown","5161062a":"markdown","76491452":"markdown","2184e28f":"markdown","722b7b15":"markdown","7d07abe4":"markdown","9b81fc24":"markdown","dff29321":"markdown","c30111b8":"markdown","90618953":"markdown","748f7b4b":"markdown","5deeebef":"markdown","d82078b7":"markdown","9e12ec38":"markdown","952014b9":"markdown","c4bce84c":"markdown"},"source":{"c79961f8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f1a5810c":"veri = pd.read_csv(\"..\/input\/creditcard.csv\")","064e9f16":"print(\"Veri Setimiz:\\n {} \\nVerinin uzunlu\u011fu: {} \".format(veri.head(),len(veri)))\npozitifler = veri[veri[\"Class\"]==1] # pozitif \u00f6rneklerin oldu\u011fu durumlar\u0131 ald\u0131k sadece\nnegatifler = veri[veri[\"Class\"]==0] # sadece negatif \u00f6rneklerin oldu\u011fu durumlar\nprint(\"\\n\\nPOZ\u0130T\u0130F \u00d6RNEKLER:\\n  {}\\nPozitif veri olan durumlar\u0131n say\u0131s\u0131: {}\".format(pozitifler.head(), len(pozitifler)))\nprint(\"\\n\\nNEGAT\u0130F \u00d6RNEKLER:\\n {}\\nNegatif veri olan durumlar\u0131n say\u0131s\u0131: {}\".format(negatifler.head(), len(negatifler)))","6bd0dfc0":"negatif_veri = negatifler.iloc[:19508,:] #ilk 19508 sat\u0131r\u0131 ald\u0131m. (Tam 20000 verimiz olsun diye :D)\nprint(len(negatif_veri))\nprint(negatif_veri.head())\n\ntemiz_veri = pd.concat([negatif_veri, pozitifler], axis=0, ) #pozitif ve negatifleri birle\u015ftiriyoruz.\nprint(temiz_veri)\nfrom sklearn.utils import shuffle\ntemiz_veri = shuffle(temiz_veri) #shuffle ile verimizin s\u0131ras\u0131n\u0131 kar\u0131\u015ft\u0131r\u0131yoruz ki 1'ler son tarafa toplanmas\u0131n\nprint(temiz_veri)","ae1441ca":"temiz_veri = temiz_veri.drop(columns=\"Time\") # Time kolonunu \u00e7\u0131kar\u0131yoruz.\nprint(temiz_veri.head())","c5b0847f":"kesif_degerleri = temiz_veri.describe()\nprint(kesif_degerleri)\n\nplt.plot(temiz_veri.std()) # standart sapmalar\u0131n grafi\u011fi\nprint(\"\\nAmount Kolonu Standart Sapmas\u0131: \", temiz_veri[\"Amount\"].std())","2d2f1040":"print(\"\\nVerinin \u00f6zellikleri:\\n\",temiz_veri.describe())\n\nx = temiz_veri.iloc[:,:29].values\ny = temiz_veri.iloc[:,29:].values\n\nprint(\"\\nX'in Boyutu: {}\\nY'nin Boyutu: {}\\n\".format(x.shape, y.shape))\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.33)\n\nprint(\"\\nx_train'in boyutu: {}\\ny_train'in boyutu: {}\\n\".format(x_train.shape, y_train.shape))\nprint(\"\\nx_test'in boyutu: {}\\ny_test'in boyutu: {}\\n\".format(x_test.shape, y_test.shape))\n","65071644":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nx_train, x_test = ss.fit_transform(x_train), ss.fit_transform(x_test)\nprint(temiz_veri)","0c06cac4":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(64, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\nmodel.add(Dense(64, activation=\"tanh\"))\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dense(64, activation=\"tanh\"))\nmodel.add(Dense(1))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=sgd, loss=\"mse\", metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=1000)","accd9d83":"y_pred = model.predict(x_test)\na = 0\nfor i in y_pred:\n    if i > 0.5:\n        y_pred[a] = 1\n    else:\n        y_pred[a] = 0\n    a += 1\nprint(y_test.shape, y_pred.shape)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_pred, y_test)\nprint(cm)\nprint(np.count_nonzero(y_pred))\nprint(np.count_nonzero(y_test))\ntoplam = cm.sum()\nhatal\u0131lar = cm[0,1] + cm[1,0]\nhata_oran\u0131 = (hatal\u0131lar\/toplam) * 100\nprint(\"Hata oran\u0131: %{}\".format(hata_oran\u0131))","fd4465a5":"**VER\u0130N\u0130N Y\u00dcKLENMES\u0130**\n\n\u00c7al\u0131\u015faca\u011f\u0131m\u0131z verisetinin linki: https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n","5161062a":"**Verinin \u0130\u015flenmesi**\n\nVerisetimiz 284807 \u00f6rnekten olu\u015fuyor. Yani nispeten b\u00fcy\u00fck bir veriseti. Modelimizi bu verinin tamam\u0131yla e\u011fitmeye kalkarsak \u00e7ok fazla zaman alacakt\u0131r. O y\u00fczden veriyi 3'e b\u00f6lerek train-test-validation k\u0131s\u0131mlar\u0131 olu\u015fturaca\u011f\u0131z. Veriyi b\u00f6lmek istememim sebebi sadece zamandan ve i\u015flem g\u00fcc\u00fcnden tasarruf etmek de\u011fil elbette.\n\n\nG\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi verisetinin b\u00fcy\u00fckl\u00fc\u011f\u00fc ile kar\u015f\u0131la\u015ft\u0131r\u0131ld\u0131\u011f\u0131nda pozitif, yani doland\u0131r\u0131c\u0131l\u0131k veya sahtecilik olan durumlar \u00e7ok d\u00fc\u015f\u00fck oranda kal\u0131yor. Bunun b\u00f6yle olmas\u0131 da do\u011fald\u0131r. Fakat bizim sa\u011fl\u0131kl\u0131 bir model ortaya \u00e7\u0131karabilmemiz i\u00e7in, doland\u0131r\u0131c\u0131l\u0131k oranlar\u0131n\u0131n daha \u00e7ok oldu\u011fu, yani modelimizin doland\u0131r\u0131c\u0131l\u0131k \u00f6rneklerinini daha s\u0131k bulabilece\u011fi yeni bir veriseti olu\u015fturmam\u0131z gerekiyor. \n\n\nBen burada verisetini olu\u015fturken 1\/40 oran\u0131nda pozitif \u00f6rneklerin oldu\u011fu bir durum se\u00e7mek istiyorum. Yani ortalama her 40 veriden birisi doland\u0131r\u0131c\u0131l\u0131k-sahtecilik \u00f6rne\u011fi i\u00e7erecek. B\u00f6ylece modeli e\u011fitmemiz daha kolay olacak diye d\u00fc\u015f\u00fcn\u00fcyorum. ","76491452":"# Ke\u015fifsel Veri Analizi\n\n1. \u0130statistiksel Da\u011f\u0131l\u0131mlar ve Anlamland\u0131r\u0131lmas\u0131\n2. Grafikle\u015ftirme ve Veriyi Anlamaya \u00c7al\u0131\u015fma","2184e28f":"# **KRED\u0130 KARTI SAHTEC\u0130L\u0130\u011e\u0130 TESP\u0130T MODEL\u0130**","722b7b15":"**Gereksiz Verilerin \u00c7\u0131kar\u0131lmas\u0131**\n\nVerinin i\u00e7inde time isimli bir kolonumuz var ve asl\u0131nda verisetimizle ve modelimizle bir ilgisi yok. Bu y\u00fczden time kolonunu almayaca\u011f\u0131z. \nPandas i\u00e7inden drop metoduyla bu kolonu verimizden \u00e7\u0131kar\u0131yoruz.","7d07abe4":"**Confusion Matrix:**\n\n# [[6441   20]\n # [   6  133]]\n\n**Comparing Positive Results:**\n\n139: y_pred positive samples\n153: y_test positive samples\n\n**Hata oran\u0131 (Error Rate): %0.393939393939394**","9b81fc24":"# Veri \u00d6n i\u015fleme A\u015famalar\u0131\n\n1.   **Verinin E\u011fitim ve Test K\u00fcmelerine B\u00f6l\u00fcnmesi**\n2.   **Verinin Standardizayonu**\n3.   **Boyut indirgeme**\n","dff29321":"Burada de\u011fi\u015fkenlerden birinin standart sapmas\u0131 di\u011ferlerine oranla \u00e7ok y\u00fcksek ger\u00e7ekle\u015fmi\u015f. Bunun bize g\u00f6sterdi\u011fi en \u00f6nemli \u015fey ise bu verinin standardizasyona tabi tutulmas\u0131 gerekti\u011fi. \u0130lerleyen k\u0131s\u0131mlarda standardizayon i\u015flemini yapmam\u0131z gerekiyor.","c30111b8":"Art\u0131k verimiz \u00fczerinde \u00e7al\u0131\u015fabiliriz.","90618953":"\u0130lk \u00f6nce train-test k\u00fcmelerini ay\u0131ral\u0131m","748f7b4b":"**K\u00dcT\u00dcPHANELER\u0130N Y\u00dcKLENMES\u0130**","5deeebef":"**Keras ile modelin olu\u015fturulmas\u0131 ve e\u011fitilmesi**","d82078b7":"**Verinin ayr\u0131lmas\u0131, train-test k\u0131s\u0131mlar\u0131n\u0131n olu\u015fturulmas\u0131**","9e12ec38":"Standard Scaler ile \u00f6l\u00e7ekleme yap\u0131yoruz","952014b9":"**Modelin test edilmesi**","c4bce84c":"# Model Olu\u015fturma"}}