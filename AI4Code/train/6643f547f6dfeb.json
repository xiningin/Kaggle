{"cell_type":{"1008ea22":"code","8bf43af3":"code","d10dbea5":"code","2f182fa0":"code","e5a28c4c":"code","e36787cc":"code","0b20203d":"code","6a73b45d":"code","fdef6eb9":"code","e5e370bd":"code","8d4a825d":"code","d20a3c0c":"code","309d317f":"code","fc1b95ee":"code","dffcb8b8":"code","ab93e261":"code","ab5239c2":"code","14dd597d":"code","0f7b353f":"code","9306f31e":"code","aecc7e8d":"code","f7889f57":"code","1fd536d5":"code","42a965fb":"code","caa526eb":"code","37bd0b17":"code","509806dc":"code","88d0dc68":"code","872a8fac":"code","c76e7526":"code","3a6fae02":"code","e875bd7e":"code","71cdaa16":"code","78330968":"code","96ac6d99":"code","448f75db":"code","86c0a0f7":"code","6cd63e2c":"code","70f4806b":"code","0616c8ba":"code","0a3d5edd":"markdown","25533939":"markdown","291da572":"markdown","0b3126ee":"markdown","307ba3b0":"markdown","ae2f06d6":"markdown","bfee01cc":"markdown","7b75989d":"markdown","10281f19":"markdown","38e40b68":"markdown","75898285":"markdown","2357cf5e":"markdown","753e9883":"markdown","afa3a536":"markdown","de085ea1":"markdown","2c859c72":"markdown","365da67c":"markdown","4b2c35e4":"markdown","4a58b66d":"markdown","ce7126cc":"markdown","ef5a21d3":"markdown","61b7644b":"markdown","dd4b950a":"markdown","8162b9b5":"markdown","8f747bbc":"markdown"},"source":{"1008ea22":"# Will be using the following libraries for the DEA\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8bf43af3":"customer_data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')","d10dbea5":"customer_data.head()","2f182fa0":"customer_data = customer_data.iloc[:, 1:-2]","e5a28c4c":"print(f\"Data Types:\\n{customer_data.dtypes}\")\nprint(f\"\\nRows and Columns:\\n{customer_data.shape}\")\nprint(f\"\\nColumn Names:\\n{customer_data.columns}\")\nprint(f\"\\nNull Values %:\\n{customer_data.apply(lambda x: sum(x.isnull()) \/ len(customer_data))}\")\nprint(f\"\\nUnknown Values %:\\n{customer_data.apply(lambda x: sum(x=='Unknown') \/ len(customer_data))}\")\n","e36787cc":"pd.crosstab(customer_data['Gender'], customer_data['Attrition_Flag'], margins = True, margins_name = \"Total\")","0b20203d":"sns.catplot(x=\"Card_Category\", hue=\"Gender\", col=\"Attrition_Flag\",\n                data=customer_data, kind=\"count\",\n                height=4, aspect=.7);","6a73b45d":"cross = pd.crosstab(customer_data['Gender'], \n            customer_data['Attrition_Flag'], \n            margins = True, \n            margins_name = \"% row\", \n            normalize='index')\ndisplay(cross)\nsns.heatmap(cross, annot=True, fmt='.0%', cmap='Blues')","fdef6eb9":"cross = pd.crosstab(customer_data['Gender'], \n            customer_data['Attrition_Flag'], \n            margins = True, \n            margins_name = \"% columns\", \n            normalize='columns')\ndisplay(cross)\nsns.heatmap(cross, annot=True, fmt='.0%', cmap='Blues')","e5e370bd":"cross = pd.crosstab(customer_data['Gender'], \n            customer_data['Attrition_Flag'], \n            margins = True, \n            margins_name = \"% all\", \n            normalize='all')\ndisplay(cross)\nsns.heatmap(cross, annot=True, fmt='.0%', cmap='Blues')","8d4a825d":"# list of features name that are int64, object, and float64\ns = (customer_data.dtypes == 'object')\ncat_cols = list(s[s].index)\nprint(f\"{len(cat_cols)} Categorical features:\\n{cat_cols}\")\n\ns = (customer_data.dtypes == 'float64')\nfloat64_cols = list(s[s].index)\nprint(f\"\\n{len(float64_cols)} float64 features:\\n{float64_cols}\")\n\ns = (customer_data.dtypes == 'int64')\nint64_cols = list(s[s].index)\nprint(f\"\\n{len(int64_cols)} int features:\\n{int64_cols}\")\ndel s","d20a3c0c":"s = (customer_data.dtypes == np.number)\nnum_cols = list(s[s].index)\nprint(f\"\\n{len(num_cols)} numpy.number features::\\n{num_cols}\")\ndel s","309d317f":"customer_data[cat_cols].describe()","fc1b95ee":"print(f'Memory Usage as Object type:\\n{customer_data[cat_cols].memory_usage()}')\ncustomer_data[cat_cols] = customer_data[cat_cols].astype('category')\nprint(f'\\nMemory Usage as Category type:\\n{customer_data[cat_cols].memory_usage()}')\n","dffcb8b8":"display(customer_data['Customer_Age'].skew())\nsns.histplot(customer_data.Customer_Age,kde=True)","ab93e261":"print(f\"Cutomer_Age std: {customer_data['Customer_Age'].std()}\")\nsns.histplot(customer_data.Customer_Age,kde=True, bins=int(customer_data['Customer_Age'].std()))","ab5239c2":"customer_data['Customer_Age_period'] = pd.cut(np.array(customer_data['Customer_Age']), \n                                              bins=int(customer_data.Customer_Age.std()),\n                                              precision=0)","14dd597d":"ax = sns.countplot(x=\"Customer_Age_period\", data=customer_data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","0f7b353f":"ax = sns.countplot(x=\"Customer_Age_period\", hue='Gender', data=customer_data, palette=\"spring\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","9306f31e":"ax = sns.countplot(x=\"Customer_Age_period\", hue='Attrition_Flag', data=customer_data, palette=\"spring\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","aecc7e8d":"print(f\"Number of unique values for 'Months_on_book' feature: {customer_data['Months_on_book'].nunique()}\")\ncustomer_data['Months_on_book_Period'] = pd.cut(customer_data['Months_on_book'],\n                                                bins=int(customer_data['Months_on_book'].std()))\nax = sns.countplot(x=\"Months_on_book_Period\", data=customer_data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","f7889f57":"print(f\"Number of unique values for 'Months_on_book' feature: {customer_data['Months_on_book'].nunique()}\")\ncustomer_data['Months_on_book_Period'] = pd.cut(customer_data['Months_on_book'],\n                                                bins=int(customer_data['Months_on_book'].std()))\nax = sns.countplot(x=\"Months_on_book_Period\", hue='Attrition_Flag', data=customer_data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.tight_layout()","1fd536d5":"# list of features name that are int64, object, and float64\ns = (customer_data.dtypes == 'int64')\nint64_cols = list(s[s].index)\nprint(f\"{len(int64_cols)} int64 features:\\n{int64_cols}\")\n\ns = (customer_data.dtypes == 'category')\ncat_cols = list(s[s].index)\nprint(f\"\\n{len(cat_cols)} Categorical features:\\n{cat_cols}\")\n\ns = (customer_data.dtypes == 'float64')\nfloat64_cols = list(s[s].index)\nprint(f\"\\n{len(float64_cols)} float64 features:\\n{float64_cols}\")\ndel s","42a965fb":"cat_df = customer_data[cat_cols].apply(lambda x : pd.factorize(x)[0])\ncat_df.info()","caa526eb":"concat_df = pd.concat([cat_df, customer_data[int64_cols]], axis=1)\nconcat_df.info()","37bd0b17":"sns.heatmap(concat_df.corr(), cmap='Blues')","509806dc":"len(concat_df.columns)","88d0dc68":"from scipy.stats import chi2_contingency\nfactors_paired = [(i,j) for i in concat_df.columns.values for j in concat_df.columns.values] \n\nchi2, p_values =[], []\n\nfor f in factors_paired:\n    if f[0] != f[1]:\n        chitest = chi2_contingency(pd.crosstab(concat_df[f[0]], concat_df[f[1]]))   \n        chi2.append(chitest[0])\n        p_values.append(chitest[1])\n    else:      # for same factor pair\n        chi2.append(0)\n        p_values.append(0)\nn_features = len(concat_df.columns)\nchi2 = np.array(chi2).reshape((n_features, n_features)) # shape it as a matrix\nchi2 = pd.DataFrame(chi2, index=concat_df.columns.values, columns=concat_df.columns.values) # then a df for convenience\n# p values is what will help us to decide dependency or independency\np_values = np.array(p_values).reshape((n_features, n_features))\np_values = pd.DataFrame(p_values, index=concat_df.columns.values, columns=concat_df.columns.values)","872a8fac":"p_values.head()","c76e7526":"# mask\nmask = np.triu(np.ones_like(p_values, dtype=np.bool))\nsns.heatmap(1 - p_values, mask=mask, fmt=\".2f\", cmap='Blues',\n           vmin=0.9, vmax=1, cbar_kws={\"shrink\": .8})\n# yticks\nplt.yticks(rotation=0)\nplt.show()","3a6fae02":"1 - p_values","e875bd7e":"customer_data.describe()","71cdaa16":"column_to_drop = ['Attrition_Flag',\n                  'Card_Category',\n                  'Dependent_count',\n                  'Customer_Age_period',\n                  'Months_on_book_Period',\n                  'Marital_Status']\n\ncolumn_to_drop_2nd_try = ['Attrition_Flag',\n                          'Card_Category',\n                          'Dependent_count',\n                          'Customer_Age',\n                          'Months_on_book',\n                          'Marital_Status']\n\nordinal_cols = ['Income_Category', 'Education_Level']\nnominal_cols = ['Gender']\ncols_to_scale = ['Customer_Age',\n                'Months_on_book',\n                'Total_Revolving_Bal',\n                'Total_Trans_Amt',\n                'Total_Trans_Ct',\n                'Credit_Limit',\n                'Avg_Open_To_Buy']\n ","78330968":"from sklearn import set_config\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\nset_config(display='diagram')","96ac6d99":"imp_enc = SimpleImputer(missing_values='Unknown', strategy='most_frequent')\n\nord_enc = OrdinalEncoder(categories=[\n                ['Less than $40K', '$40K - $60K', '$60K - $80K', '$80K - $120K', '$120K +'],\n                ['Uneducated', 'High School', 'College', 'Graduate', 'Post-Graduate', 'Doctorate']])\n\none_hot_enc = OneHotEncoder(handle_unknown='error', drop='if_binary')\n\ntarget_enc = OneHotEncoder(categories=[['Attrited Customer', 'Existing Customer']], \n                           handle_unknown='error', \n                           drop='if_binary',\n                           sparse=False)\n\nordinal_transformer = Pipeline(steps=[('unknown_imp', imp_enc), ('ordinal', ord_enc)])\n\nnumeric_transformer = Pipeline(steps=[('num_imp', SimpleImputer(strategy='median')),\n                                      ('scaler', StandardScaler())])\n\nnominal_transformer = Pipeline(steps=[('unknown_imp', imp_enc), ('one_hot', one_hot_enc)])\n\npreprocessor = make_column_transformer((ordinal_transformer, ordinal_cols),\n                                       (nominal_transformer, nominal_cols),\n                                       (numeric_transformer, cols_to_scale),\n                                        remainder = 'passthrough') \n\npreprocessor","448f75db":"customer_data_to_fit = customer_data.drop(columns=column_to_drop )","86c0a0f7":"X_train, X_test, y_train, y_test = train_test_split(customer_data_to_fit,\n                                                    target_enc.fit_transform(customer_data[['Attrition_Flag']]).ravel(),\n                                                    test_size=0.3,\n                                                    random_state=42, \n                                                    stratify=target_enc.fit_transform(customer_data[['Attrition_Flag']]).ravel())","6cd63e2c":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import plot_roc_curve\n\nclassifiers = [\n    SVC(C=0.025),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    BernoulliNB(),\n    LogisticRegression()]\n\n","70f4806b":"\nfig, ax = plt.subplots()\n\nx_train_preprocessed = preprocessor.fit_transform(X_train)\nx_test_preprocessed = preprocessor.transform(X_test)\nmodel_displays = {}\nfor clf in classifiers:\n    clf.fit(x_train_preprocessed, y_train)\n    name = type(clf).__name__\n    model_displays[type(clf).__name__] = plot_roc_curve(\n        clf, x_test_preprocessed, y_test, ax=ax, name=name)\n_ = ax.set_title('ROC curve')\n\n","0616c8ba":"potential_model = ['SVC', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'LogisticRegression']\nfor clf in classifiers:\n    name = type(clf).__name__\n    if name in potential_model:\n        predictions =clf.predict(x_test_preprocessed)\n        cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n        cr = classification_report(y_true=y_test, \n                                   y_pred=predictions,\n                                   target_names=['Existing Customer', 'Attrited Customer'])\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n        print(f\"\\n{name}:\\n{cr}\")\n        print(f\"{cm}\")\n        ","0a3d5edd":">***Note:*** *Another way to check which features are float or Real numbers is to use numpy.number as shown in the following code snippet. We deduce that numpy's number are all number type but `int`, so when getting an unknwon DataFrame it will be quicker to run this snippet to get all real numbers and then deal with the `int` columns ","25533939":"I will perform the same procedure in `Months_on_book` feature because it has 44 distinct value. ","291da572":"### Correlation ###\nI would like to see which feature are correlated in some way to the target. Since correlation works only on numerical type, I will encode all catgeorical values in the categorical features to numbers using [pandas.factorize](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.factorize.html). Once done I will show that we need a different approach to see correlation since most of our data is categorical.   ","0b3126ee":"So, we do are dealing with 10,127 samples and 23 features.\nWe do not have nulls but on the other hand we do have 14%, 7% and 11% of 'Unknown' string in `Education_Level`, `Marital_Status` and `Income_Category` features respectively. I will deal with it later on before running my model.\n\nWe have 14 numerical features (5 - `float64`, 9 - `int64`) and 6 `object`. \nSince this dataset is relatively small and we do have the features names, we can make some assumptions (that would be verified): for example `Gender`, `Marital_Status`, `Card_Category` are expected to be [nominal categorical features](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63) whereas `Education_Level` and `Income_Category` may be [oridinal](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63) ones, the `Customer_Age` can also be transformed to [oridinal](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63) values with age ranges.\n\nBefore manipulating the data for our classification model, let us 'play' a bit with the data to get grasp of what we have. I will use [pandas.crosstab](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.crosstab.html) to get views between our target `Attrition_Flag` and other categorical features, then, I will try to see if there is any dependencies between the categorical features. Once we are done we will manipulate the approriate features and evaluate some Classification models.\n\nThe table hereunder tells us that 930 female and 697 are attrited customers does that mean that women are more likely to be attrited? not necessarily! we need to see how may men vs female our data set contains more over we need to understand what is the likelihood of attrited customer from all the population. for this we need probabilities.\n","307ba3b0":"Let's start... getting the data","ae2f06d6":"I can conclude that std is around 8 and setting this value to number of bins gives us a good split for group age in the dataset as can be seen in the following graphs:","bfee01cc":"## Data Manipulation ##","7b75989d":"> The purpose of this notebook is to share best practices I think should use when getting new data, I've never seen...\n> Before I start, some disclaimers about me - I am new to AI\/ML and yet a great enthusiast it.... Anyone who will go through my notebook will probably find other ways and shorter ones to perform what I've done... Anyway, this is a real guide for myself... I hope it will be useful for others...\n>\n> I will using panda's crosstab, sklearn [Pipeline](), [ColumnTranformer](), [SVM](), [Random Forest](), [Naive Bayes](), [Logistic regression]()","10281f19":"The target to predict is `Attrition_Flag`.\nLet's remove the first column `CLIENTNUM` and the last two ones - they are not features and therefore are not needed.\nWe expect to have 20 columns.\nLet us get info of the dataset and see what type of data our features contain, how many samples, null values, 'Unknown' omes etc...\n","38e40b68":"I will change the `Object`'s columns' type to `Category` for two reasons:\n1. Less memory for categorical features\n2. Easier to transform them through API","75898285":"## *Data Exploration Analysis (EDA)*","2357cf5e":"Refresh *cat_cols, float64_cols*. All *int64_cols* are update to be ordinal values and *category* type","753e9883":"I have 3 table with probabilities. The last table are probabilities relative to all the population. For example, we have 9% of the total population that are female and attrited customer, we have a total of 53% female and 47% male. However, we have 16% of the population that is attrited customer and 83% that are existing customers. This means that we have a chance of 0.16 of being an attrited customer and 0.84 of being an existing customer if we were picking randomly a person from the population. In order for our model to predict well, we would need to 'balance' more the data... we would start be evaluating the conditional probabilities i.e. given some specific features what will be the probability of attrited vs. existing. The two first tables above gives us thoese kind of probabilities.\n\n1. First table above p(Attrited Customer\/Gender) - given gender (female\/male) what is the probability of attrited\/existing: for example we see 17% attrited from female and 85% existing from men \n2. Second table above p(Gender\/Attrited Customer)- given attrited\/existing what is the probability of gender (female\/male): for eample we see that 43% from attrited cutomer are men  and 52% from existing customer are female.\n\nwe do have other features like `Education_Level`, `Marital_Status`. `Customer_Age`, `Card_Category`, `Income_Category` and more that probably affect  the `Attrition_Flag` (target).\n\nWhat king of model will fit the best for this type of classification? since we are dealing with sveral categorical data features the essence here is to analysis probabilities. Therefore I will go for evaluating models that are based on them.\n\nMy first guess will be to go for `Decision Tree` and its ensmble models such as `Random Forest`, `Adaboost`. Then, I will also try NaiveBayes and run `Logistic Regression` just for the record.\n\nBefore running the models I would like to run some test on the categorical data to see if it is correlated. I will use [Chi-square test](https:\/\/www.ling.upenn.edu\/~clight\/chisquared.htm). More reading on how to deal with catgorical data can be found [here](https:\/\/datascience.stackexchange.com\/questions\/893\/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab) and [here](https:\/\/stackoverflow.com\/questions\/48035381\/correlation-among-multiple-categorical-variables-pandas)\n\n","afa3a536":"Are there any other catogorical features?\n\nLet's tackle the `int64` columns: `Dependent_count`, `Months_on_book`, `Total_Relationship_Count`, `Months_Inactive_12_mon`, `Contacts_Count_12_mon`, `Total_Revolving_Bal`, `Total_Trans_Amt`, `Total_Trans_Ct` are all *ordinal categorial* features. `customer_age` can also be transformed to *ordinal categorial* by spliting the age to age's ranges. The question is how many ranges should we split the feature? I will use `min`, `max` age and the `std`. Let us see how this feature looks like and its skewness:","de085ea1":"The graph above gives us more information and yet not enough to know what to do. We need to shift to probabilities. Let's see information in probabilities:","2c859c72":"So what information do I get here?\n\nAll our classifiers perform well on *`Existing Customer`*, however, our target is to have a good classifier for *`Attrited Customer`*. \nTherefore, the classification metrics that would be at most interest would be the following metrics:\n##### *precision, recall and F1* #####\nThe greater values the better performance (with respect to the folowwing respective order: *recall, precision and F1*\n\nIn our confusion matrices above we can see:\n1. `SVC`(*precision = 1.00, recall = 0.01, F1 = 0.02*)\n2. `DecisionTreeClassifier`(*precision = 0.77, recall = 0.80, F1 = 0.78*)\n3. `RandomForestClassifier`(*precision = 0.86, recall = 0.59, F1 = 0.70*)\n3. `LogisticRegression`(*precision = 0.76, recall = 0.54, F1 = 0.63*)\n\n#### The best classifier is `AdaBoost` with *precision = 0.88, recall = 0.82, F1 = 0.85 and it is consistent with the AUC metric - 0.98 in the ROC above \n\n\n### Thank you ###\n","365da67c":"In order to automate thinks I will seperate catgorical from numerical ones.\n\nWe have 14 numerical features (5 - `float64`, 9 - `int64`) and 6 `object`. Since this dataset is relatively small and we do have the features names, we can make some assumption that we should verify on some columns: for example `Gender`, `Marital_Status`, `Card_Category` are expected to be [nominal categorical features](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63) whereas `Education_Level` and `Income_Category` may be [oridinal](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63) ones\n> ***Note:*** *`Education_Level` can also be consider as a `orinal` feature however I will consider it as `nominal`. I will see later on if this decision has any effect on the model*","4b2c35e4":"What happens if we set the number of bins to `std` value? does it make sense?","4a58b66d":"#### Categorical features ####\n`Attrition_Flag` (our target feature), `Gender`, `Marital_Status`, `Card_Category` are nominal categorial values whereas, `Education_Level`, `Income_Category` are ordinal categorial values. Later on before running our models we will transform these features by using sklearn's [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html?highlight=onehot) and sklearn's [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html?\nighlight=labelen#sklearn.preprocessing.LabelEncoder) respectively.\n\n","ce7126cc":"In the table and heatmap above we can see that there is a dependency likelihood between `Attrition_Flag` and some of the other features. The table above is 1-p_value, so the closer to 1 the greater liklihood for dependency. For example, `Customer_Age` has 0.979 likelihood to be dependent with *Target* whereas `Customer_Age_period` has only 0.95, therefore I will use `Customer_Age` and discard `Customer_Age_period` that I've created previously. `Gender` and the five last features has a bigger likelihood for being dependent with `Attrition_Flag`\n\n### Model Evaluation ###\n\nNow I will run several models on the data and pick the one that predict best the `Attrition_Flag`. When saying predict best, I mean - predicting *attrited_customer*. [Precision and Recall](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall) will be of most interest here and I will use [ROC](https:\/\/www.displayr.com\/what-is-a-roc-curve-how-to-interpret-it\/) for evaluating the models.\n#### Why? ####\nI would like to see rlation between the true positive rate (TPR) against the false positive rate (FPR). TPR - True Positive Rate (TPR) is the *Recall* metric (see above). FPR - false positive rate measures the ratio of false positives within the negative samples - TPR is also equal to 1-*Specificity*. FPR and TPR (ROC metrics) measure the ability to distinguish between the classes. In our data set we have less *attrited_customer* then *existing_customer*. More details can be found [here](https:\/\/towardsdatascience.com\/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba)\n\nIn the following lines\n1. Determine group of features to work on\n2. deal with *Unknown* values by using [SimpleImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html) with *most frequent strategy*\n3. Split the data into training and test - drop unnecessary features (based on their `max`, `min` values)\n4. Build pipelines with column transformers and ML models to run.\n5. Evaluating models with [ROC](https:\/\/www.displayr.com\/what-is-a-roc-curve-how-to-interpret-it\/) curves and confusion matrices\n6. summarize\n","ef5a21d3":"Now, i would like to concatinate the cat_df with `int64` features","61b7644b":"### Conclusion ###\nBy analyzing the ROC curve, I will focus on *SVC*, *DecisionTree*, *RandomForest*,  *AdaBoost* and *LogisticRegression*.  Roc curve suggests to analys first *RandomForest* and *AdaBoost* (AUC values are 0.97 and 0.98 respectively more information [here](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)).\n\nLet's us dive into them and choose the best performer. Analyzing their confusion matrices will help reaching that goal:\n ","dd4b950a":"Examining Attrition_Age with other feature doesn\u2019t show us any correlation. However, by using [Chi-square test](https:\/\/www.ling.upenn.edu\/~clight\/chisquared.htm) we can see that some features are dependent with the target. I will use [chi2_contingency](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.chi2_contingency.html) (more information about chi2 can be found [here](https:\/\/machinelearningmastery.com\/chi-squared-test-for-machine-learning\/)) to compute chi2 and to get p-values. p-value is the Probability of H0 being True. if p-Value>0.05 then Accept the assumption(H0). The assumption H0 is our case will be independence","8162b9b5":"# Bank Churners - Data Exploration, Preparation and model valuations\n","8f747bbc":"#### Variable Identification: "}}