{"cell_type":{"ff8406dd":"code","4250d752":"code","989a880b":"code","7703e142":"code","1c972f58":"code","62bc940a":"code","8eb1979d":"code","36812553":"code","ee6b48fb":"code","73780ca0":"code","8d09aef9":"code","196ecf4f":"code","6662821f":"code","9b64c7a2":"code","66b6c5c4":"code","493b9514":"code","4751e3dd":"code","c1ce594d":"code","abbcfbda":"code","9f93f2bf":"code","c510220e":"code","a1cee680":"code","eb2c94f8":"code","f02c7a79":"code","6ea860c2":"code","3ba56719":"code","891755e9":"code","e4bb491c":"code","9ea6e0a3":"code","a7b50ee3":"code","03a1c053":"markdown","5217355a":"markdown","7f334d42":"markdown"},"source":{"ff8406dd":"#%%bash\n#apt-get install --no-install-recommends git cmake build-essential libboost-dev libboost-system-dev libboost-filesystem-dev\n#git clone --recursive https:\/\/github.com\/microsoft\/LightGBM\n#cd LightGBM\n#mkdir build\n#cd build\n#cmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\n#make -j$(nproc)\n#cd ..\n#cd LightGBM\/python-package\/;python3 setup.py install --precompile\n#mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n#rm -r LightGBM","4250d752":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15,15\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder,RobustScaler\nfrom sklearn.metrics import roc_auc_score,accuracy_score ,confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import ClusterCentroids , NearMiss\n\nfrom tqdm.notebook import tqdm ,tnrange","989a880b":"train_data = pd.read_csv('..\/input\/healthcareanalyticsii\/train.csv')\ntest_data = pd.read_csv('..\/input\/healthcareanalyticsii\/test.csv')","7703e142":"print(train_data.shape)\ntrain_data.head()","1c972f58":"print(test_data.shape)\ntest_data.head()","62bc940a":"def nullColumns(train_data):\n    list_of_nullcolumns =[]\n    for column in train_data.columns:\n        total= train_data[column].isna().sum()\n        try:\n            if total !=0:\n                print('Total Na values is {0} for column {1}' .format(total, column))\n                list_of_nullcolumns.append(column)\n        except:\n            print(column,\"-----\",total)\n    print('\\n')\n    return list_of_nullcolumns\n\n\ndef percentMissingFeature(data):\n    data_na = (data.isnull().sum() \/ len(data)) * 100\n    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n    return data_na\n\n\ndef plotMissingFeature(data_na):\n    f, ax = plt.subplots(figsize=(15, 12))\n    plt.xticks(rotation='90')\n    if(data_na.empty ==False):\n        sns.barplot(x=data_na.index, y=data_na)\n        plt.xlabel('Features', fontsize=15)\n        plt.ylabel('Percent of missing values', fontsize=15)\n        plt.title('Percent missing data by feature', fontsize=15)","8eb1979d":"print('train data')\nprint(nullColumns(train_data))\nprint(percentMissingFeature(train_data))\nprint('\\n')\nprint('test_data')\nprint(nullColumns(test_data))\nprint(percentMissingFeature(test_data))","36812553":"stay = train_data.loc[:,\"Stay\"].value_counts().rename('Count')\nplt.xlabel(\"Stay\")\nplt.ylabel('Count')\nsns.barplot(stay.index , stay.values).set_title('Stay')","ee6b48fb":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nf, axes = plt.subplots(3, 2, figsize=(15, 15))\n\nradiotherapy = train_data[train_data.Department =='radiotherapy'][\"Stay\"].value_counts().rename('Count')\n\nanesthesia = train_data[train_data.Department =='anesthesia'][\"Stay\"].value_counts().rename('Count')\n\ngynecology = train_data[train_data.Department =='gynecology'][\"Stay\"].value_counts().rename('Count')\n\nsurgery = train_data[train_data.Department =='surgery'][\"Stay\"].value_counts().rename('Count')\n\ntb = train_data[train_data.Department =='TB & Chest disease'][\"Stay\"].value_counts().rename('Count')\n\nsns.barplot(radiotherapy.index,radiotherapy,  color=\"b\", ax=axes[0, 0]).set_title('Department : radiotherapy')\n\nsns.barplot(anesthesia.index,anesthesia,   color=\"r\", ax=axes[0, 1]).set_title('Department : anesthesia')\n\nsns.barplot(gynecology.index,gynecology,  color=\"g\", ax=axes[1, 0]).set_title('Department : gynecology')\n\nsns.barplot(surgery.index,surgery, color=\"m\", ax=axes[1, 1]).set_title('Department : surgery')\n\nsns.barplot(tb.index,tb, color=\"m\", ax=axes[2, 0]).set_title('Department : TB & Chest disease')\n\nsns.barplot(stay.index,stay, color=\"m\", ax=axes[2, 1]).set_title('Department : ALL')\n\nplt.xlabel(\"Stay\")\n\nplt.setp(axes,yticks = np.arange(0,50000,5000))\n\nfor ax in f.axes:\n    \n    plt.sca(ax)\n    \n    plt.xticks(rotation=45)\n\nplt.tight_layout()\n","73780ca0":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nf, axes = plt.subplots(6, 2, figsize=(15, 15))\n\nstay0 = train_data[train_data.Stay =='0-10'][\"Department\"].value_counts().rename('Count')\n\nstay1 = train_data[train_data.Stay =='11-20'][\"Department\"].value_counts().rename('Count')\n\nstay2 = train_data[train_data.Stay =='21-30'][\"Department\"].value_counts().rename('Count')\n\nstay3 = train_data[train_data.Stay =='31-40'][\"Department\"].value_counts().rename('Count')\n\nstay4 = train_data[train_data.Stay =='41-50'][\"Department\"].value_counts().rename('Count')\n\nstay5 = train_data[train_data.Stay =='51-60'][\"Department\"].value_counts().rename('Count')\n\nstay6 = train_data[train_data.Stay =='61-70'][\"Department\"].value_counts().rename('Count')\n\nstay7 = train_data[train_data.Stay =='71-80'][\"Department\"].value_counts().rename('Count')\n\nstay8 = train_data[train_data.Stay =='81-90'][\"Department\"].value_counts().rename('Count')\n\nstay9 = train_data[train_data.Stay =='91-100'][\"Department\"].value_counts().rename('Count')\n\nstay10 = train_data[train_data.Stay =='More than 100 Days'][\"Department\"].value_counts().rename('Count')\n\nsns.barplot(stay0.index,stay0,  color=\"b\", ax=axes[0, 0]).set_title('Stay : 0-10')\n                   \nsns.barplot(stay1.index,stay2,  color=\"r\", ax=axes[0, 1]).set_title('Stay : 11-20')\n\nsns.barplot(stay2.index,stay2,  color=\"b\", ax=axes[1, 0]).set_title('Stay : 21-30')\n\nsns.barplot(stay3.index,stay3,  color=\"g\", ax=axes[1, 1]).set_title('Stay : 31-40')\n\nsns.barplot(stay4.index,stay4,  color=\"b\", ax=axes[2, 0]).set_title('Stay : 41-50')\n\nsns.barplot(stay5.index,stay5,  color=\"b\", ax=axes[2, 1]).set_title('Stay : 51-60')\n\nsns.barplot(stay6.index,stay6,  color=\"m\", ax=axes[3, 0]).set_title('Stay : 61-70')\n\nsns.barplot(stay7.index,stay7, color=\"b\", ax=axes[3, 1]).set_title('Stay : 71-80')\n\nsns.barplot(stay8.index,stay8,  color=\"b\", ax=axes[4, 0]).set_title('Stay : 81-90')\n\nsns.barplot(stay9.index,stay9,  color=\"g\", ax=axes[4, 1]).set_title('Stay : 91-100')\n\nsns.barplot(stay10.index,stay10, color=\"r\", ax=axes[5, 0]).set_title('Stay : >100')\n\nplt.setp(axes, yticks = np.arange(0,20000,5000))\n\n\nfor ax in f.axes:\n    \n    plt.sca(ax)\n    \n    plt.xticks(rotation=45)\n\nplt.tight_layout()","8d09aef9":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nf, axes = plt.subplots(3, 1, figsize=(15, 15))\n\nemergency = train_data[train_data['Type of Admission'] =='Emergency'][\"Stay\"].value_counts().rename('Count')\n\ntrauma = train_data[train_data['Type of Admission'] =='Trauma'][\"Stay\"].value_counts().rename('Count')\n\nurgent = train_data[train_data['Type of Admission'] =='Urgent'][\"Stay\"].value_counts().rename('Count')\n\nsns.barplot(emergency.index,emergency,  color=\"b\", ax=axes[0]).set_title('Admn. Type : Emergency')\n\nsns.barplot(trauma.index,trauma,   color=\"r\", ax=axes[1]).set_title('Admn. Type : Trauma')\n\nsns.barplot(urgent.index,urgent,  color=\"g\", ax=axes[2]).set_title('Admn. Type : Urgent')\n\nplt.setp(axes, yticks = np.arange(0,50000,10000))\n\nfor ax in f.axes:\n    \n    plt.sca(ax)\n    \n    plt.xticks(rotation=45)\n\nplt.tight_layout()","196ecf4f":"train_data['City_Code_Patient'] = train_data['City_Code_Patient'].fillna(-1)\ntrain_data['Bed Grade'] = train_data['Bed Grade'].fillna(-1)","6662821f":"test_data['City_Code_Patient'] = test_data['City_Code_Patient'].fillna(-1)\ntest_data['Bed Grade'] = test_data['Bed Grade'].fillna(-1)","9b64c7a2":"cat_cols = ['Hospital_code','Hospital_type_code','City_Code_Hospital','Hospital_region_code'\n            ,'Department','Ward_Type','Ward_Facility_Code','Bed Grade','City_Code_Patient',\n           # 'Type of Admission','Severity of Illness',\n            'Age']","66b6c5c4":"label = 'Stay'","493b9514":"def encode_cat_cols(train, test, cat_cols): #target\n\n    train_df = train_data.copy()\n    \n    test_df = test_data.copy()\n    \n    # Making a dictionary to store all the labelencoders for categroical columns to transform them later.\n    \n    le_dict = {}\n\n    for col in cat_cols:\n        \n        le = LabelEncoder()\n        \n        le.fit(train_df[col].unique().tolist() + test_df[col].unique().tolist())\n        \n        train_df[col] = le.transform(train_df[[col]])\n        \n        test_df[col] = le.transform(test_df[[col]])\n\n        le_dict[col] = le\n\n    le = LabelEncoder()\n    \n    train_df[label] = le.fit_transform(train_df[[label]])\n    \n    le_dict[label] = le\n    \n    train_df['Type of Admission'] = train_df['Type of Admission'].map({'Urgent':0,'Emergency':1,'Trauma':2})\n    \n    train_df['Severity of Illness'] = train_df['Severity of Illness'].map({'Minor':0,'Moderate':1,'Extreme':2})\n    \n    test_df['Type of Admission'] = test_df['Type of Admission'].map({'Urgent':0,'Emergency':1,'Trauma':2})\n    \n    test_df['Severity of Illness'] = test_df['Severity of Illness'].map({'Minor':0,'Moderate':1,'Extreme':2})\n    \n    return train_df, test_df, le_dict","4751e3dd":"def feature_importance(model, X_train):\n\n    fI = model.booster_.feature_importance(importance_type='gain')\n    \n    print(fI)\n    \n    names = X_train.columns.values\n    \n    ticks = [i for i in range(len(names))]\n    \n    plt.bar(ticks, fI)\n    \n    plt.xticks(ticks, names,rotation = 90)\n    \n    plt.show()","c1ce594d":"train_df, test_df, le_dict = encode_cat_cols(train_data,test_data,cat_cols)","abbcfbda":"#After Feature Engineering\n# https:\/\/www.kaggle.com\/gcspkmdr\/lets-get-rid-of-the-patients-feature-engineering\n\ncombined_data = pd.read_csv('..\/input\/lets-get-rid-of-the-patients-feature-engineering\/combined.csv')","9f93f2bf":"train_df = combined_data[combined_data['train']==1]\n\ntest_df = combined_data[combined_data['train']==0]","c510220e":"train_df.drop(columns = ['case_id','train','patientid','Hospital_code',\n                         'Hospital_type_code','City_Code_Hospital','Ward_Facility_Code'],inplace = True)\n\ntarget = train_df.pop('Stay')\n\ntest_df.drop(columns = ['case_id','train','Stay','patientid','Hospital_code',\n                        'Hospital_type_code','City_Code_Hospital','Ward_Facility_Code'],inplace = True)","a1cee680":"cat_features = ['Hospital_region_code','Department','Ward_Type','Bed Grade','City_Code_Patient','Type of Admission','Severity of Illness','Age']\n\nfor f in cat_features:\n    \n    train_df[f] = train_df[f].astype('category')\n    \n    test_df[f] = test_df[f].astype('category')","eb2c94f8":"%%time\n\n##LightGBM\n\nscores = []\n\navg_loss = []\n\nX_train_cv,y_train_cv = train_df.copy(), target.copy()\n\nsssf = StratifiedShuffleSplit(n_splits=5, test_size = 0.45 ,random_state=1)\n\nfor i, (idxT, idxV) in enumerate(sssf.split(X_train_cv, y_train_cv)):\n    \n    print('Fold',i)\n    \n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    \n    clf = lgb.LGBMClassifier(n_estimators=10000,\n                             max_depth=8,\n                             learning_rate=0.1,\n                             subsample=0.85,\n                             colsample_bytree=0.5,\n                             #device ='gpu',\n                             #gpu_platform_id = 0,\n                             #gpu_device_id =  0,\n                             objective ='multiclass',\n                             #max_bin=63,\n                             #gpu_use_dp=False,\n                             random_state = 1,\n                             #categorical_columns = cat_features\n                            )        \n    \n    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT], \n                eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                verbose=100,eval_metric=['multi_logloss'],\n                early_stopping_rounds=50)\n    \n    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))*100\n    \n    scores.append(acc)\n\n    avg_loss.append(clf.best_score_['valid_0']['multi_logloss'])\n    \n    print ('LGB Val CV=',acc)\n    \n    print('#'*100)\n    \n    if i==0:\n        feature_importance(clf,X_train_cv)\n\nprint(\"Multi Log Loss Stats {0:.5f},{1:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","f02c7a79":"trees = 5\n\nseeds = [32,432,73]\n\nsubmission = pd.read_csv('..\/input\/healthcareanalyticsii\/sample_submission.csv')\n\nprobs = np.zeros(shape=(len(test_df),11))\n\nsubmission_probs = pd.DataFrame(columns = ['case_id'] + list(le_dict['Stay'].classes_))\n\nsubmission_probs.iloc[:,0] = submission.iloc[:,0]\n\nsubmission_probs.iloc[:,1:] = 0","6ea860c2":"%%time\n\n##LightGBM\n\n#groups = train_df['patientid'].values\n\nscores = []\n\navg_loss = []\n\nsubmission_name = [] \n\nX_train_cv,y_train_cv = train_df.copy(), target.copy()\n\nfor seed in tnrange(len(seeds)):\n\n    sssf = StratifiedShuffleSplit(n_splits=5, test_size = 0.3 ,random_state=seeds[seed])\n\n    for j, (idxT, idxV) in tqdm(enumerate(sssf.split(X_train_cv, y_train_cv))):\n\n        print('Fold',j)\n\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n\n        model_lgb = [0] *trees\n\n        for i in tnrange(trees):\n\n            print('Tree',i)\n\n            model_lgb[i] = lgb.LGBMClassifier(n_estimators=1000,\n                                     max_depth=8,\n                                     learning_rate=0.1,\n                                     subsample=0.8,\n                                     colsample_bytree=0.5,\n                                     #device ='gpu',\n                                     #gpu_platform_id = 0,\n                                     #gpu_device_id =  0,\n                                     objective ='multiclass',\n                                     random_state = i*27\n                                    )        \n\n            model_lgb[i].fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT], \n                        eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                        verbose=100,eval_metric=['multi_logloss'],\n                        early_stopping_rounds=50)\n\n            probs_file_name = 'probs_'+str(seeds[seed])+'_'+str(j)+'_'+str(i)+\".csv\"\n            \n            submisssion_file_name  = 'submission_'+str(seeds[seed])+'_'+str(j)+'_'+str(i)+\".csv\"\n            \n            model_lgb_probs = model_lgb[i].predict_proba(test_df)\n            \n            submission_probs.iloc[:,1:] = model_lgb_probs\n            \n            # probablity file per seed per split per tree\n            submission_probs.to_csv(probs_file_name,index = False)\n            \n            submission['Stay'] = le_dict['Stay'].inverse_transform(np.argmax(model_lgb_probs,axis =1))\n            \n            # submission file per seed per split per tree\n            submission.to_csv(submisssion_file_name,index =False)\n            \n            probs += model_lgb_probs\n            \n            acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(model_lgb[i].predict_proba(X_train_cv.iloc[idxV]),axis =1))*100\n            \n            scores.append(acc)\n            \n            submission_name.append(submisssion_file_name)\n            \n            avg_loss.append(model_lgb[i].best_score_['valid_0']['multi_logloss'])\n\n            #print ('LGB Accuracy Split =',acc)\n            \n            print('#'*100)\n    \n\nprint(\"Average Multi Log Loss Stats {0:.5f},{1:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\n#print('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","3ba56719":"submission_probs.iloc[:,1:] = probs\n\n# probablity combined\nsubmission_probs.to_csv('probs.csv',index =False)","891755e9":"submission['Stay'] = le_dict['Stay'].inverse_transform(np.argmax(probs,axis =1))\n\n# submission file combined            \nsubmission.to_csv('submission.csv',index =False)\n            ","e4bb491c":"model_stats = pd.DataFrame({'submission':submission_name,'accuracy':scores,'validation_loss':avg_loss})\nmodel_stats.head()","9ea6e0a3":"model_stats.to_csv('model_stats.csv',index =False)","a7b50ee3":"# Ensembles\n# https:\/\/www.kaggle.com\/gcspkmdr\/lets-get-rid-of-the-patients-xgboost?select=probs.csv\n# https:\/\/www.kaggle.com\/gcspkmdr\/lets-get-rid-of-the-patients-catboost\/output    ","03a1c053":"# LightGBM GPU Build Installation\n\n* Poor accuracy\n\n* Didn't speedup at all(even after using recommended settings)\n\n[https:\/\/lightgbm.readthedocs.io\/en\/latest\/GPU-Performance.html#:~:text=In%20LightGBM%2C%20the%20main%20computation,ranking%2C%20regression%2C%20etc).]","5217355a":"# Model Building","7f334d42":"# Cross Validation\n![](https:\/\/4.bp.blogspot.com\/-wpr6O3EBAfU\/WbHyt6UCOVI\/AAAAAAAAjPw\/Y1DaO6qcV8oDYjJHzJ1PaPB2EXHmYtBBQCLcBGAs\/s1600\/%25E6%2593%25B7%25E5%258F%2596.JPG)\n\n* **The CV score generated using the methodology shown in the above figure is a better indicator of model performance than public LB**"}}