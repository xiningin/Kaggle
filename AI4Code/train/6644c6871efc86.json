{"cell_type":{"e5995fdf":"code","5ebc0f47":"code","514e430c":"code","58116e10":"code","3a499ba2":"code","289e90cd":"code","73d361c0":"code","a629cfce":"code","1061c3bf":"code","e7d820bb":"code","88a2ef1d":"code","e6223d2a":"code","f545a386":"code","ee6ed4fa":"code","9e13aaf9":"code","7d75b1bd":"code","94b120b0":"code","7ac17688":"code","9215f4c6":"code","9f5685f7":"code","adfca879":"code","d5d05ef7":"code","8b87faab":"code","11470a94":"code","20c8af1a":"code","cfb8c40f":"code","1b445d51":"code","8d2706d5":"code","9b59d3f9":"code","5780f554":"code","10b383c9":"code","0e8f510c":"code","9b3f713f":"code","5c0db19e":"code","9e03dcee":"code","41557fe3":"code","e1a80223":"code","d6343106":"code","d74f4ee5":"code","930d6ec9":"code","9e842448":"code","f1d93768":"code","87d81a9c":"code","5b01601c":"code","67a751cf":"code","02915320":"code","babb31bc":"code","ba76c4ec":"code","e00d0d10":"code","a30881c0":"code","0b869ae2":"code","5fe522d6":"markdown"},"source":{"e5995fdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json as js\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5ebc0f47":"import pandas as pd\n\ngames = pd.read_csv(\"..\/input\/league-of-legends\/games.csv\")\ngames","514e430c":"champ_data = pd.read_json(\"..\/input\/league-of-legends\/champion_info_2.json\")\nchampions = pd.read_json((champ_data[\"data\"]).to_json(), orient=\"index\")\nchampions.set_index([\"id\"], inplace = True)\nchampions.tail(20)","58116e10":"champList = [\"t1_champ1id\",\"t1_champ2id\",\"t1_champ3id\",\"t1_champ4id\",\"t1_champ5id\",\n             \"t2_champ1id\",\"t2_champ2id\",\"t2_champ3id\",\"t2_champ4id\",\"t2_champ5id\"]","3a499ba2":"data = pd.read_csv(\"..\/input\/league-of-legends\/games.csv\")","289e90cd":"def conversion(x):\n    champ = champions[\"name\"][x]\n    return champ\n\n\nfor column in champList:\n    data[column] = data[column].apply(lambda x: conversion(x))\n   \n\nbanList = [\"t1_ban1\",\"t1_ban2\", \"t1_ban3\", \"t1_ban4\", \"t1_ban5\",\n           \"t2_ban1\", \"t2_ban2\",\"t2_ban3\", \"t2_ban4\", \"t2_ban5\"]\n\nfor column in banList:\n    data[column] = data[column].apply(lambda x : conversion(x))\n   ","73d361c0":"summ_data = pd.read_json(\"..\/input\/league-of-legends\/summoner_spell_info.json\")\nsummoners = pd.read_json((summ_data[\"data\"]).to_json(), orient=\"index\")\n\ndef summ_conversion(x):\n    summoner = summoners[\"name\"][x]\n    return summoner\n\nsummList = [\"t1_champ1_sum1\",\"t1_champ1_sum2\",\"t1_champ2_sum1\",\"t1_champ2_sum2\",\"t1_champ3_sum1\",\n                 \"t1_champ3_sum2\",\"t1_champ4_sum1\",\"t1_champ4_sum2\",\"t1_champ5_sum1\",\"t1_champ5_sum2\",\n                 \"t2_champ1_sum1\",\"t2_champ1_sum2\",\"t2_champ2_sum1\",\"t2_champ2_sum2\",\"t2_champ3_sum1\",\n                 \"t2_champ3_sum2\",\"t2_champ4_sum1\",\"t2_champ4_sum2\",\n                 \"t2_champ5_sum1\",\"t2_champ5_sum2\"]\n\n\nfor column in summList:\n    data[column] = data[column].apply(lambda x : summ_conversion(x))\n\n\ndata","a629cfce":"data.columns","1061c3bf":"sumPicks = pd.concat([data['t1_champ1id'],data['t1_champ2id'],data['t1_champ3id'],data['t1_champ4id'],data['t1_champ5id'],\n                      data['t2_champ1id'],data['t2_champ2id'],data['t2_champ3id'],data['t2_champ4id'],data['t2_champ5id']],\n                      ignore_index=True)\nsortedPicks = sorted(sumPicks)\nsumBans = pd.concat([data['t1_ban1'],data['t1_ban2'],data['t1_ban3'],data['t1_ban4'],data['t1_ban5'],\n                     data['t2_ban1'],data['t2_ban2'],data['t2_ban3'],data['t2_ban4'],data['t2_ban5']],\n                     ignore_index=True)\nsortedBans = sorted(sumBans)","e7d820bb":"import matplotlib.pyplot as plt \nimport seaborn as sns\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=(15,30))\nplt.xticks(rotation=90)\nsns.countplot(y=sortedPicks, data=data, ax=ax1, order=sumPicks.value_counts().index)\nsns.countplot(y=sortedBans, data=data, ax=ax2 , order=sumBans.value_counts().index )\nax1.set_title('Champion Picks')\nax2.set_title('Champion Bans')\nplt.show()","88a2ef1d":"Ndata = pd.DataFrame(sumPicks.value_counts())\nNdata[\"Bans\"]= sumBans.value_counts()\nNdata","e6223d2a":"Ndata.rename(columns={0: \"Picks\"}, inplace = True)\nNdata","f545a386":"#ax = plt.gca()\n\n#Ndata.plot(kind='bar',y='Bans',ax=ax ,figsize=(50,25) )\n#Ndata.plot(kind='bar',y='Picks', color='red', ax=ax)\n#plt.xticks(rotation=90)\n#plt.show()\n\n\nNdata.plot(kind='bar',figsize=(50,25))","ee6ed4fa":"def create_label_encoder_dict(df):\n    from sklearn.preprocessing import LabelEncoder\n    \n    label_encoder_dict = {}\n    for column in df.columns:\n        # Only create encoder for categorical data types\n        if not np.issubdtype(df[column].dtype, np.number) and column != 'Age':\n            label_encoder_dict[column]= LabelEncoder().fit(df[column])\n    return label_encoder_dict","9e13aaf9":"label_encoders = create_label_encoder_dict(data)\nprint(\"Encoded Values for each Label\")\nprint(\"=\"*32)\nfor column in label_encoders:\n    print(\"=\"*32)\n    print('Encoder(%s) = %s' % (column, label_encoders[column].classes_ ))\n    print(pd.DataFrame([range(0,len(label_encoders[column].classes_))], columns=label_encoders[column].classes_, index=['Encoded Values']  ).T)","7d75b1bd":"data2 = data.copy() # create copy of initial data set\nfor column in data2.columns:\n    if column in label_encoders:\n        data2[column] = label_encoders[column].transform(data2[column])\n\nprint(\"Transformed data set\")\nprint(\"=\"*32)\ndata2.head(15)","94b120b0":"X_data = data2[[\"t1_ban1\",\"t1_ban2\", \"t1_ban3\", \"t1_ban4\", \"t1_ban5\",\n           \"t2_ban1\", \"t2_ban2\",\"t2_ban3\", \"t2_ban4\", \"t2_ban5\"]]\nY_data = data2['t1_champ1id']","7ac17688":"from sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.30)","9215f4c6":"from sklearn.neural_network import MLPClassifier","9f5685f7":"# Create an instance of linear regression\nreg = MLPClassifier()\n#reg = MLPClassifier(hidden_layer_sizes=(8,120))","adfca879":"reg.fit(X_train,y_train)","d5d05ef7":"reg.n_layers_ # Number of layers utilized","8b87faab":"test_predicted = reg.predict(X_test)\ntest_predicted","11470a94":"data3 = X_test.copy()\ndata3['predicted_pick']=test_predicted\ndata3['predicted_pick_en']=label_encoders['t1_champ1id'].inverse_transform(test_predicted)\ndata3['pick']=data3['t1_champ1id']=y_test\ndata3['pick_en']=label_encoders['t1_champ1id'].inverse_transform(y_test)\ndata3.head(40)","20c8af1a":"\nk=(reg.predict(X_test) == y_test)","cfb8c40f":"k.value_counts()","1b445d51":"from sklearn.metrics import confusion_matrix\n","8d2706d5":"cm=confusion_matrix(y_test, reg.predict(X_test), labels=y_test.unique())\ncm","9b59d3f9":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    import itertools\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5780f554":"plt.figure(figsize=(45,45),dpi = 300)\nplot_confusion_matrix(cm,data['t1_champ1id'].unique())","10b383c9":"X_data2 = data2[[\"t1_ban1\",\"t1_ban2\", \"t1_ban3\", \"t1_ban4\", \"t1_ban5\",\n           \"t2_ban1\", \"t2_ban2\",\"t2_ban3\", \"t2_ban4\", \"t2_ban5\",\"t1_champ1id\",\"t1_champ2id\",\"t1_champ3id\",\"t2_champ1id\",\"t2_champ2id\"]]\nY_data2 = data2['t2_champ3id']","0e8f510c":"# use new data for  train\/test split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_data2, Y_data2, test_size=0.30)","9b3f713f":"reg2= MLPClassifier()\nreg2.fit(X_train2,y_train2)","5c0db19e":"reg2.n_layers_ # Number of layers utilized","9e03dcee":"test_predicted2 = reg2.predict(X_test2)\ntest_predicted2","41557fe3":"data4 = X_test2.copy()\ndata4['predicted_pick']=test_predicted2\ndata4['predicted_pick_en']=label_encoders['t2_champ3id'].inverse_transform(test_predicted2)\ndata4['pick']=data3['t2_champ3id']=y_test2\ndata4['pick_en']=label_encoders['t2_champ3id'].inverse_transform(y_test2)\ndata4.head(40)","e1a80223":"k2=(reg2.predict(X_test2) == y_test2)","d6343106":"k2.value_counts()","d74f4ee5":"cm2=confusion_matrix(y_test2, reg2.predict(X_test2), labels=y_test2.unique())\ncm2","930d6ec9":"plt.figure(figsize=(45,45),dpi = 300)\nplot_confusion_matrix(cm2,data['t2_champ3id'].unique())","9e842448":"X_data3 = data2[[\"t1_ban1\",\"t1_ban2\", \"t1_ban3\", \"t1_ban4\", \"t1_ban5\",\n           \"t2_ban1\", \"t2_ban2\",\"t2_ban3\", \"t2_ban4\", \"t2_ban5\",\"t1_champ1id\",\"t1_champ2id\",\"t1_champ3id\",\"t1_champ4id\",\"t1_champ5id\",\n             \"t2_champ1id\",\"t2_champ2id\",\"t2_champ3id\",\"t2_champ4id\"]]\nY_data3 = data2['t2_champ5id']","f1d93768":"X_train3, X_test3, y_train3, y_test3 = train_test_split(X_data3, Y_data3, test_size=0.30)","87d81a9c":"reg3 = MLPClassifier()\nreg3.fit(X_train3,y_train3)","5b01601c":"reg3.n_layers_ # Number of layers utilized","67a751cf":"test_predicted3 = reg3.predict(X_test3)\ntest_predicted3","02915320":"data5 = X_test3.copy()\ndata5['predicted_pick']=test_predicted3\ndata5['predicted_pick_en']=label_encoders['t2_champ5id'].inverse_transform(test_predicted3)\ndata5['pick']=data3['t2_champ5id']=y_test3\ndata5['pick_en']=label_encoders['t2_champ5id'].inverse_transform(y_test3)\ndata5.head(40)","babb31bc":"k3=(reg3.predict(X_test3) == y_test3)","ba76c4ec":"k3.value_counts()","e00d0d10":"cm3=confusion_matrix(y_test3, reg3.predict(X_test3), labels=y_test3.unique())\ncm3","a30881c0":"plt.figure(figsize=(45,45),dpi = 300 )\nplot_confusion_matrix(cm3,data['t2_champ5id'].unique())","0b869ae2":"Xnew = [[99, 55,76,11,23,24,16,19,15,88]]\nNtest_predicted = reg.predict(Xnew)\nprint(\"X=%s, Predicted=%s\" % (Xnew, label_encoders['t1_champ1id'].inverse_transform(Ntest_predicted)))","5fe522d6":"**Trying with new x and y data**"}}