{"cell_type":{"2c191a2e":"code","b53e38a2":"code","cf749b4e":"code","da5af7fb":"code","8e2e9f5a":"code","2e29e3f8":"code","4140c396":"code","3a5a0656":"code","31f93a22":"code","6aa8eda2":"code","b47c8878":"code","d365d83b":"code","cf80da92":"code","17339f47":"code","3f72fcfb":"markdown","65aa12be":"markdown","d6f9b246":"markdown"},"source":{"2c191a2e":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random as rm\n","b53e38a2":"env = gym.make('FrozenLake-v0')\nenv.render()","cf749b4e":"print('Observation Space: ', env.observation_space.n)","da5af7fb":"print('Action Space: ', env.action_space.n)","8e2e9f5a":"# Random Action\nrandom_action = env.action_space.sample()\nrandom_action","2e29e3f8":"# Random Action\nactions = {'Left': 0, 'Down': 1, 'Right': 2, 'Up': 3}\nrm.seed(3)\nrandom_action = rm.sample(list(actions), 1)[0]\nprint(random_action)\nactions[random_action]","4140c396":"env.reset()\nnew_state, reward, done, info = env.step(actions[random_action])\nprint(new_state) # the new state of the environment\nprint(reward) # The reward\nprint(done) # a boolean flag indicating if the returned state is a terminal state  \nprint(info) # Additional information","3a5a0656":"env.render()","31f93a22":"# state-Action probabilites\nenv.P[1][actions[random_action]]\n# trans_prob, next_state, reward, if terminal node","6aa8eda2":"env.reset()\nenv.render()\nmax_iters = 10\nfor i in range(max_iters):\n    random_action = env.action_space.sample()\n    new_state, reward, done, info = env.step(random_action)\n    print(f'new_state: {new_state}\\nreward: {reward} \\nIf terminal state: {done}')\n    print('\\n\\n\\n')\n    env.render()\n    if done: \n        break","b47c8878":"# Stochastic Environment\nactions =  {'Left': 0, 'Down': 1, 'Right': 2, 'Up': 3}\nwinnig_seq = (2 * ['Right']) + (3 * ['Down']) + ['Right']\nprint(winnig_seq)\n\nenv = gym.make('FrozenLake-v0')\nenv.reset()\nenv.render()\n\nfor a in winnig_seq:\n    new_state, reward, done, info = env.step(actions[a])\n    print()\n    env.render()\n    print(f'reward: {reward}')\n    print(info)\n    if done:\n        break\n    print()\n    ","d365d83b":"# Deterministic Environment\nactions =  {'Left': 0, 'Down': 1, 'Right': 2, 'Up': 3}\nwinnig_seq = (2 * ['Right']) + (3 * ['Down']) + ['Right']\nprint(winnig_seq)\n\nenv = gym.make('FrozenLake-v0', is_slippery = False)\nenv.reset()\nenv.render()\n\nfor a in winnig_seq:\n    new_state, reward, done, info = env.step(actions[a])\n    print()\n    env.render()\n    print(f'reward: {reward}')\n    print(info)\n    if done:\n        break\n    print()\n    ","cf80da92":"# Custom Maps\n# 8x8 version of Map\n\nenv = gym.make('FrozenLake-v0', map_name = '8x8')\nenv.reset()\nenv.render()","17339f47":"# Custom Map\ncustom_map = [\n    'SFFFHFHFHF', \n    'FFHFFFFHFH',\n    'HFFFHHGFHF',\n    'HHHFGFHHFH', \n]\n\nenv = gym.make('FrozenLake-v0', desc = custom_map)\nenv.reset()\nenv.render()\n ","3f72fcfb":"#### S: initial state\n#### F: frozen lake\n#### H: hole\n#### G: the goal\n#### Red square: Indicate the current position of the player","65aa12be":"## Introduction to Frozen Lake Environment","d6f9b246":"##### 0: left\n##### 1: down\n##### 2: right\n##### 3: up"}}