{"cell_type":{"1d69acc1":"code","8a35e6d4":"code","5226ff76":"code","9a184cec":"code","b1c575be":"code","fed157cc":"markdown","0508d7ef":"markdown","4d094b17":"markdown","b0fb1fe7":"markdown","95c82992":"markdown"},"source":{"1d69acc1":"!pip install requests beautifulsoup4 fake-useragent --quiet\n\nimport os\nimport time\nfrom bs4 import BeautifulSoup\nimport requests\nfrom urllib.request import Request, urlopen\nfrom fake_useragent import UserAgent\nimport random\n\nprint(os.listdir('.\/'))","8a35e6d4":"ua = UserAgent() \n\ndef get_proxies():\n    req = Request('https:\/\/www.sslproxies.org\/')\n    req.add_header('User-Agent', ua.random)\n    doc = urlopen(req).read().decode('utf8')\n    soup = BeautifulSoup(doc, 'html.parser')\n    table = soup.find(id='proxylisttable')\n    proxies = []\n    for row in table.tbody.find_all('tr'):\n        proxies.append({\n            'protocol': 'https' if row.find_all('td')[6].string == 'yes' else 'http',\n            'ip': row.find_all('td')[0].string, \n            'port': row.find_all('td')[1].string \n        })\n    return proxies","5226ff76":"base_url = 'http:\/\/abcnotation.com'\n\n# Edit these values\nstart_page = 200\nlast_page = 300","9a184cec":"for i in range(start_page, last_page):\n    cur_page = str(i)\n    while len(cur_page) != 4:\n        cur_page = '0' + cur_page\n    page_link = base_url + '\/browseTunes?n=' + cur_page\n    page_response = requests.get(page_link)\n    page_soup = BeautifulSoup(page_response.text, 'html.parser')\n    pre_tag = page_soup.find('pre')\n    a_tag = pre_tag.find_all('a', href=True)\n    \n    print('Page:', i)\n    file_number = 0\n    proxies = get_proxies()\n    prev_proxy_index = 0\n    \n    for link in a_tag:\n        file_link = base_url + link['href']\n        \n#         Fake Proxy & UA => Can't break web security.\n        cur_proxy_index = random.randint(0, len(proxies) - 1)\n        while cur_proxy_index == prev_proxy_index:\n            cur_proxy_index = random.randint(0, len(proxies) - 1)\n        proxy = {proxies[cur_proxy_index]['protocol']: 'http:\/\/' + proxies[cur_proxy_index]['ip'] + ':' + proxies[cur_proxy_index]['port']}\n        prev_proxy_index = cur_proxy_index\n        headers = requests.utils.default_headers()\n        headers.update({'User-Agent': ua.random})\n        file_response = requests.get(file_link, proxies=proxy, headers=headers)\n\n        if file_response.status_code == 200:\n            file_soup = BeautifulSoup(file_response.text, 'html.parser')\n    #         print(file_soup.prettify())\n            text_area = file_soup.find('textarea')\n    #         print(text_area.contents[0])\n            with open('.\/scraping_' + str(i) + '_' + str(file_number) + '.abc', 'a') as file:\n                file.write(text_area.contents[0])\n#                 print('scraping_' + str(i) + '_' + str(file_number) + '.abc created') \n        else:\n            print('Failed at page ' + str(i) + ' file ' + str(file_number))\n        file_number+=1\n        time.sleep(1)","b1c575be":"!tar -zcf scraping.tar.gz .\/\n!rm -rf .\/scraping_*\n# !tar -zxf scraping.tar.gz # For extraction\n!ls .\/","fed157cc":"# Import & Install","0508d7ef":"# Detail of website\nNumber of all pages is 1380 and each page has 200 files. <br \/>\nThis site can't request under <b>1<\/b> second. (Fake proxy and UA are not worked.)","4d094b17":"# Fake Proxies","b0fb1fe7":"## <span style=\"color:red\">To get all data due to limit of resources. You need to edit following variables.<\/span>\n- <b>start_page<\/b> - first page is 0\n- <b>last_page<\/b> - not included (last page that gets request is <b>last_pages - 1<\/b>)","95c82992":"# Avoid limit of output files"}}