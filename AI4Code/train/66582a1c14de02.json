{"cell_type":{"4db9b54d":"code","14751b6b":"code","e432da36":"code","d1165603":"code","90cc8a87":"code","7c6b0559":"code","5ea1e470":"code","90304d39":"code","d569fb81":"code","a019bde8":"code","8defc677":"code","8a7dcfbc":"code","535d34f3":"code","1d7a365b":"code","c3623c74":"code","192ac99c":"code","bbd6ef68":"code","8219f303":"code","c1ebf04c":"code","a902033f":"code","5e8dd813":"code","75797900":"code","0dac2daa":"code","a56471ef":"code","77c4e7e0":"code","acbf7003":"code","7bbbbfe9":"code","94ee6a5f":"code","38185ae9":"code","f9f19d5e":"code","836a891f":"code","f93b7a27":"code","ac80745c":"code","674fab77":"markdown","66c87851":"markdown","cbf4a62d":"markdown","89ad1946":"markdown","b51eba64":"markdown","3a459cee":"markdown","cdeb7d7a":"markdown","968facd1":"markdown","46529e1d":"markdown","40280b42":"markdown","c191a45c":"markdown","c958e34d":"markdown","5a3faf6b":"markdown","fc2a9f95":"markdown","60ec06f7":"markdown","0bb0445b":"markdown"},"source":{"4db9b54d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport emoji\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom sklearn.utils import class_weight as cw\nimport string\nfrom nltk import pos_tag \nfrom keras.optimizers import Adam ,RMSprop\nfrom nltk.corpus import wordnet\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nimport re\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import Input, Add, concatenate, Dense, Activation, BatchNormalization,add, Dropout, Flatten,Concatenate,GlobalAveragePooling1D\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.models import Model\nfrom nltk.stem.isri import ISRIStemmer\nfrom keras.utils import to_categorical\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer \nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Embedding , LSTM ,GRU,Conv1D,GlobalMaxPooling1D,SpatialDropout1D,CuDNNLSTM,Bidirectional\nfrom keras.layers.embeddings import Embedding\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","14751b6b":"tweets_path = '..\/input\/Tweets.csv' #when forking the kernel just put the path for the csv file of the data on your os\ndf = pd.read_csv(tweets_path)","e432da36":"df.head()","d1165603":"sns.countplot(df[\"airline_sentiment\"])","90cc8a87":"X = df['text']\nY = df['airline_sentiment']","7c6b0559":"imp = ['against','not','don',\"don't\",\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"needn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\nstop_words = [w for w in stopwords.words('english') if w not in imp]","5ea1e470":"# def get_wordnet_pos(pos_tag):\n#     if pos_tag.startswith('J'):\n#         return wordnet.ADJ\n#     elif pos_tag.startswith('V'):\n#         return wordnet.VERB\n#     elif pos_tag.startswith('N'):\n#         return wordnet.NOUN\n#     elif pos_tag.startswith('R'):\n#         return wordnet.ADV\n#     else:\n#         return wordnet.NOUN\ndef clean_text(text):\n    # lower text\n    text = emoji.demojize(text) # demojize the emojis, emoji to text \n    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",text).split()) #for removing the batterns\n    text = text.lower() # lowering the text \n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # remove stop words\n    #stop = stop_words\n    #text = [x for x in text if x not in stop]\n    # remove empty tokens\n    text = [t for t in text if len(t) > 0]\n    #pos tag text\n    #pos_tags = pos_tag(text)\n    # lemmatize text\n    #text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    # remove words with only one letter\n    #ps = PorterStemmer() \n    #text = [ps.stem(t) for t in text]\n    text = [t for t in text if len(t) > 1]\n    # join all\n    text = \" \".join(text)\n    return(text)","90304d39":"X = X.apply(lambda x :clean_text(x)) #apply cleaning to every row in the data","d569fb81":"X.head()","a019bde8":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15) #splitting the data fpr train and test","8defc677":"vect = CountVectorizer(ngram_range=(1,2)).fit(X_train)\ntrain_df_vectorized = vect.transform(X_train)\nvect_tfidf = TfidfVectorizer(ngram_range=(1,2),min_df=2,max_df=0.5).fit(X_train)\ntrain_df_vectorized_tfidf = vect_tfidf.transform(X_train)\n","8a7dcfbc":"LB = LabelEncoder()\nY_train = LB.fit_transform(Y_train).reshape(-1, 1)\n#Y_train = to_categorical(Y_train)\nY_test = LB.transform(Y_test).reshape(-1,1)\n","535d34f3":"def get_weight(y):#getting class_weights to make the data balanced\n    class_weight_current =  cw.compute_class_weight('balanced', np.unique(y), y)\n    return class_weight_current\nclass_weight = get_weight(Y_train.flatten())","1d7a365b":"clfrNB = MultinomialNB(alpha = 0.1)\nclfrNB.fit(train_df_vectorized, Y_train)\nclfrNB_tfidf = MultinomialNB(alpha = 0.1)\nclfrNB_tfidf.fit(train_df_vectorized_tfidf, Y_train)","c3623c74":"acc_nb_tfidf = accuracy_score(clfrNB_tfidf.predict(vect_tfidf.transform(X_test)),Y_test),\nacc_nb_cv = accuracy_score(clfrNB.predict(vect.transform(X_test)),Y_test)\nprint(\"accuracy using tfidf vectorizer and Naive Bayes model is: \" +str(acc_nb_tfidf) )\nprint(\"accuracy using count vectorizer and Naive Bayes model is: \" +str(acc_nb_cv) )","192ac99c":"from sklearn.svm import SVC\nclf_svc = SVC()\nclf_svc.fit(train_df_vectorized, Y_train)\nclf_svc_tfidf = SVC()\nclf_svc_tfidf.fit(train_df_vectorized_tfidf, Y_train)\nacc_cv_svm = accuracy_score(Y_test,clf_svc.predict(vect.transform(X_test)))\nacc_tfidf_svm = accuracy_score(Y_test,clf_svc_tfidf.predict(vect_tfidf.transform(X_test)))","bbd6ef68":"print(\"accuracy using tfidf vectorizer and SVM model is: \" +str(acc_tfidf_svm) )\nprint(\"accuracy using count vectorizer and SVM model is: \" +str(acc_cv_svm) )","8219f303":"max_words = len(set(\" \".join(X_train).split())) #getting the number of unique words in the document\nmax_len = X_train.apply(lambda x: len(x)).max() #getting the longest sentence in the data\n\n# max_words = 1000\n# max_len = 150\nmax_words, max_len\ntokenizer = Tokenizer(num_words=max_words)#initialize a tokenizer with max unique words in the text\ntokenizer.fit_on_texts(X_train) \n\nX_train_seq = tokenizer.texts_to_sequences(X_train) #change every word to a unique number\nX_train_seq = pad_sequences(X_train_seq, maxlen=max_len) #pad the sentences ,to make every sentence have the same length\n","c1ebf04c":"X_train_seq.shape,Y_train.shape","a902033f":"EMBEDDDING_DIM = 100\nmodel = Sequential()\nmodel.add(Embedding(max_words,EMBEDDDING_DIM,input_length=max_len))\nmodel.add(Conv1D(1024, 3, padding='valid', activation='relu', strides=1))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3,activation='softmax'))","5e8dd813":"optimizer = Adam(lr=0.001)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","75797900":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,\n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nmodel.fit(X_train_seq,pd.Series(Y_train.reshape(-1)),epochs=30,batch_size = 1024,validation_split=0.2,callbacks=[learning_rate_reduction],class_weight =class_weight)","0dac2daa":"%matplotlib inline\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nhistory = model.history\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.figure(figsize=(10,7))\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.legend(['Training accuracy','Validation accuracy'])\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.figure(figsize=(10,7))\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.legend(['Training loss','Validation loss'])\nplt.figure()\nmax(val_acc) #the best validation accuracy the model have got","a56471ef":"test_X_seq = tokenizer.texts_to_sequences(X_test) \ntest_X_seq = pad_sequences(test_X_seq, maxlen=max_len)\naccuracy1 = model.evaluate(test_X_seq, Y_test)\nprint(\"accuracy of Embedding Conv1D model: \" + str(accuracy1[1]))\nprint(\"maximum validation accuracy is: \"+str(max(val_acc))+ \" \"+\"at epoch \" +str(val_acc.index(max(val_acc))))\n","77c4e7e0":"EMBEDDDING_DIM = 150\nmodel2 = Sequential()\nmodel2.add(Embedding(max_words,EMBEDDDING_DIM,input_length=max_len))\nmodel2.add(GRU(units=32,dropout=0.2,recurrent_dropout=0.2))\nmodel2.add(Dense(3,activation='softmax'))","acbf7003":"model2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])","7bbbbfe9":"learning_rate_reduction3 = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,\n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nmodel2.fit(X_train_seq,pd.Series(Y_train.reshape(-1)),epochs=15,batch_size = 1024,validation_split=0.2\n           ,callbacks=[learning_rate_reduction3])","94ee6a5f":"%matplotlib inline\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nhistory2 = model2.history\nacc=history2.history['acc']\nval_acc=history2.history['val_acc']\nloss=history2.history['loss']\nval_loss=history2.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.figure(figsize=(10,7))\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.legend(['Training accuracy','Validation accuracy'])\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.figure(figsize=(10,7))\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.legend(['Training loss','Validation loss'])\nplt.figure()\nmax(val_acc) #the best validation accuracy the model have got","38185ae9":"accuracy2 = model2.evaluate(test_X_seq, Y_test)\nprint(\"accuracy of Embedding GRU model: \" + str(accuracy2[1]))","f9f19d5e":"result_df = pd.DataFrame([{'RNN':accuracy2[1],'Conv1D':accuracy1[1],'Naive Bayes':acc_nb_cv,'SVM':acc_cv_svm}])","836a891f":"result_df","f93b7a27":"sns.barplot(x=result_df.columns,y=result_df.values[0])","ac80745c":"from sklearn import metrics\nprint(\"Classification report  \\n%s\\n\"\n      % ( metrics.classification_report(Y_test, clfrNB.predict(vect.transform(X_test)))))\nprint(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(Y_test,clfrNB.predict(vect.transform(X_test))))","674fab77":"using learning rate decay it is very effective in late epochs ","66c87851":"conluding that Naive Bayes model gave the best results so here is the confusion matrix and classification report for this model .","cbf4a62d":"counting the labels frequancy, it is clear that the data is unbalanced ,but it's ok for now there we'll be methods to solve this.","89ad1946":"below DataFrame contains every model crosseponding to its accuracy","b51eba64":"make the classes weighted so as to overcome the unbalancing in the data","3a459cee":"label encoding for the target output.","cdeb7d7a":"Fitting CountVectorizer and TfidfVectorizer to the training data","968facd1":"**preprocessing for sequence model**","46529e1d":"**naive bayes classifier works well wih sentiment analysis, i have tried using it, also used SVM,naive bayes gave better accuracy .**\nhere i tried naive bayes with both count vectrizer and tfidf vectorizer both of them almost gave the same test accuracy .","40280b42":" **function for cleaning any text **\n \n i have tried to lemmatize and to stem the text it didn't improve the accuracy so after trying many operations to do on the text i decided not to lemmatize or stem also not to remove the stop words from the text","c191a45c":"here i removed the negative stop words as they are somehow important ,after trying removing them accuracy increases sometimes","c958e34d":"**Structuring a deep learning model**\n\na deep learning model to do sentiment analysis ,here we may use RNN,LSTM,GRU... , but i have been trying using them and using one directional convolution , found one directional convolution gave better accuracy in this case.","5a3faf6b":"**fitting SVM model **\n\nit is also fitted using count and tfidf vectorzier ","fc2a9f95":"defining inputs and output labels for the model ","60ec06f7":"**GRU model with word embeddings **","0bb0445b":"compiling the model using adam optimizer simply this optimizer is a merge of two optimizers (momentum and Rmsprop),and simply those optimizers make the model reach the minimum faster."}}