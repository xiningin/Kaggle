{"cell_type":{"bd529cfe":"code","898f4481":"code","4bb95288":"code","303860b0":"code","9f209489":"code","a563625a":"code","4d3ada0f":"code","1a35d58a":"code","56de090d":"code","1f3005cf":"code","b6d70aac":"code","943a26ff":"code","4668e2ff":"code","9371475a":"code","f0a679ab":"code","8b60d513":"code","4254a7cf":"code","6035052b":"code","9dc848f2":"code","9e4cc2ed":"code","9c42ba5a":"code","6760679b":"code","3ff10c1e":"code","708316aa":"code","86d5d837":"code","07278619":"code","d63ed72e":"code","4bfaaa23":"code","fdb6428e":"code","8942b75d":"code","fe3f1a9e":"code","0e28030c":"code","5c89cc67":"code","3042c22d":"code","5f5ce585":"code","3c936d53":"code","b3fb3cef":"code","aa1ddb5c":"code","2e9886e5":"code","325c9d82":"code","745e3d9d":"code","badd8757":"code","306866f1":"code","cc3da988":"code","7ac77f90":"code","bfcc24ba":"code","fe2c8cf9":"code","0122c5c0":"code","b7b52757":"code","d3b175c6":"code","98946f12":"code","9004350f":"code","c9e72450":"code","b1151fc7":"code","16a72814":"code","ffd21916":"code","461d1c6a":"code","f9a46f55":"code","16653aad":"code","5a7b2815":"code","9b17c467":"code","703278ea":"code","429954e2":"code","12a22d43":"code","2b7b8b18":"code","1543323e":"code","20dcff7a":"code","30d6e8f7":"code","58b66992":"code","06b95f3d":"code","631a1be8":"code","61c10dfb":"code","8f2685dc":"code","a0b7e795":"code","ad0263ac":"code","b7fa95cc":"code","4732afe0":"code","4205824f":"code","513f29cf":"code","3b6f5b2c":"code","8bab1e66":"code","a3df67a8":"code","b1f095ca":"code","c4406c9b":"markdown","0f3a6d0a":"markdown","c94a6e8c":"markdown","bab6064a":"markdown","fac3f348":"markdown","28669dc6":"markdown","e7f009e8":"markdown","4aa32440":"markdown","7f1e301b":"markdown","84adc704":"markdown","3585d9e6":"markdown","29e5f993":"markdown","1b6bde92":"markdown","2f409c49":"markdown","d940878c":"markdown","09730e8c":"markdown","cf1f1610":"markdown","d4bf2af3":"markdown","ca666b0a":"markdown","632b87f4":"markdown","31a685bf":"markdown","b648f210":"markdown","1b4ab236":"markdown","bfec8074":"markdown","af52519c":"markdown","3de98aab":"markdown","d0d18432":"markdown","bdd83851":"markdown","89cab019":"markdown","88a2938e":"markdown","db4307ed":"markdown","5d495209":"markdown"},"source":{"bd529cfe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, train_test_split, KFold\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import mean_squared_error\n\npd.options.display.max_rows=200\npd.set_option('mode.chained_assignment', None)\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\nsimplefilter(\"ignore\", category=RuntimeWarning)\n\nimport optuna\nfrom functools import partial\n\nplt.style.use('fivethirtyeight')","898f4481":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","4bb95288":"train.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","303860b0":"X_train = train.drop('SalePrice', axis=1)\ny_train = np.log1p(train.SalePrice)\nX_test = test","9f209489":"def missing_value_imputation(X):\n    \n    numerical_features = [feature for feature in X.columns if X[feature].dtype !='O']\n        \n    ordinal_features = ['LotShape', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                        'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu',\n                        'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC']\n\n    categorical_features = [feature for feature in X.columns if feature not in ordinal_features and \n                           feature not in numerical_features]\n    \n    # Numerical feature imputation\n        \n    # we have selected Neighborhood feature as the feature to group by the data set to impute\n    # the other numerical feature on the basis of Neighborhood because it is quite common to have similar types of houses\n    # within a Neighborhood for eg:- a Neighborhood may have Expensive yet houses with small floor area, but on the other \n    # hand there might be houses in a Neighborhood where houses have lower prices yet they have larger total sq ft. area.\n    # hence this feature may help us to capture better median values for the missing value imputation.\n    \n    for feature in numerical_features:\n        X[feature] = X[feature].fillna(X.groupby('Neighborhood')[feature].transform('median'))\n        \n    \n    # Simplifying Categorical features and combining rare types\n    \n    X['MSZoning'].replace({'RL':'R', 'RM':'R', 'RP':'R', 'RM':'R', 'I':'O', 'A':'O', 'C (all)':'O', 'FV':'O'}, inplace=True)\n    \n    X['Alley'].replace({np.nan : 'No', 'Grvl':'Yes', 'Pave':'Yes'}, inplace=True)\n    \n    X['LandContour'].replace({'Lvl':'Lvl', 'HLS':'Slope', 'Bnk':'Slope', 'Low':'Slope'}, inplace=True)\n    \n    X['Condition1'].where(X['Condition1'] == 'Norm', 'Other', inplace=True)\n    \n    X['HouseStyle'].where((X['HouseStyle'] == '1Story') | (X['HouseStyle'] == '2Story') | (X['HouseStyle'] == '1.5Fin'), \n                          'rare', inplace=True)\n    \n    X['RoofStyle'].where((X['RoofStyle'] == 'Gable') | (X['RoofStyle'] == 'Hip'), 'rare', inplace=True)\n    \n    X['MasVnrType'].where(X['MasVnrType'] == 'None', 'yes', inplace=True)\n    \n    X['MasVnrType'].replace({np.nan : 'None'}, inplace=True)\n    \n    X['Exterior1st'].where((X['Exterior1st'] == 'VinylSd') |  (X['Exterior1st'] == 'HdBoard') | \n                           (X['Exterior1st'] == 'MetalSd') | (X['Exterior1st'] == 'Wd Sdng') | \n                           (X['Exterior1st'] == 'Plywood'), 'rare', inplace=True)\n    \n    X['PavedDrive'].where(X['PavedDrive'] == 'Y', 'N', inplace=True)\n    \n    X['Fence'].replace({'MnPrv':'Yes', 'GdPrv':'Yes', 'GdWo':'Yes', 'MnWw':'Yes', np.nan:'No'}, inplace=True)\n    \n    X['SaleType'].where(X['SaleType'] == 'WD', 'other', inplace=True)\n    \n    X['SaleCondition'].where((X['SaleCondition'] == 'Normal') | (X['SaleCondition'] == 'Partial') | \n                             (X['SaleCondition'] == 'Abnorml'), 'other', inplace=True)\n    \n    X['Neighborhood'].replace({'Blmngtn':'rare', 'BrDale':'rare', 'Veenker':'rare', 'NPkVill':'rare', \n                               'Blueste':'rare', 'ClearCr':'rare'}, \n                              inplace=True)\n    \n\n    # for missing values still left\n    \n    for feature in categorical_features:\n        val = X[feature].value_counts().index[0]\n        X[feature] = X[feature].fillna(val)\n    \n\n    # Ordinal feature encoding\n    \n    # you should not use LabelEncoder() as label encoder will not preserve the order!!\n\n    X['LotShape'].replace({'Reg':1, 'IR1':0, 'IR2':0, 'IR3':0}, inplace=True)\n    X['LandSlope'].replace({'Gtl':3, 'Mod':2, 'Sev':1}, inplace=True)\n    X['ExterQual'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['ExterCond'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['BsmtQual'].replace({np.nan:0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['BsmtCond'].replace({np.nan:0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['BsmtExposure'].replace({np.nan:0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}, inplace=True)\n    X['BsmtFinType1'].replace({np.nan:0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n    X['BsmtFinType2'].replace({np.nan:0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n    X['HeatingQC'].replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['KitchenQual'].replace({np.nan:3, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['Functional'].replace({np.nan:8, 'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, \n                              'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8}, inplace=True)\n    X['FireplaceQu'].replace({np.nan:0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['GarageFinish'].replace({np.nan:0, 'Unf':1, 'RFn':2, 'Fin':3}, inplace=True)\n    X['GarageQual'].replace({np.nan:0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['GarageCond'].replace({np.nan:0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n    X['PoolQC'].replace({np.nan:0, 'Fa': 1, 'Gd':2, 'Ex':3}, inplace=True)\n    \n    categorical_features = [feature for feature in X.columns if X[feature].dtype == 'O']\n    \n    for feature in categorical_features:\n        value_counts = X[feature].value_counts()\n        for val in value_counts[value_counts \/ len(X) < 0.015].index:\n            X[feature].loc[X[feature] == val] = 'rare'\n            \n    # the problem of many rare features in categorical columns is solved only to some extent with the above practice, \n    # if you experiment with different rare feature threshold, you will find that there will be still few categories,\n    # which were categorized as 'rare' in training set, but not in test set, hence the one hot encoded columns will \n    # not be same across the final training and test sets.\n    \n    return X\n\n\n\nclass FeatureEngineering(BaseEstimator, TransformerMixin):\n    \n    # you have to inherit from the BaseEstimator and TransformerMixin classes if you want to grid search the class params\n    # using grid search through a pipeline.\n    \n    def __init__(self, combined_features=True, drop_underlying_features=True, drop_features=True, correlation_threshold=0.05, \n                 polynomial_features=True, outlier_threshold=1.5,  empty_column_dropping_threshold=0.99, \n                 final_correlation_threshold=0.05, test=False):\n        \n        self.combined_features = combined_features\n        self.drop_underlying_features = drop_underlying_features\n        self.drop_features = drop_features\n        self.correlation_threshold = correlation_threshold\n        self.polynomial_features = polynomial_features\n        self.polynomial_features_list = []\n        self.outlier_threshold = outlier_threshold\n        self.empty_column_dropping_threshold = empty_column_dropping_threshold\n        self.low_correlation_drop_list = []\n        self.one_hot_features_to_drop = []\n        self.final_low_correlation_drop_list = []\n        self.final_correlation_threshold = final_correlation_threshold\n        self.test = test\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        \n        ordinal_features = ['LotShape', 'LandSlope', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                            'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu',\n                            'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC']\n        \n        numerical_features = [feature for feature in X.columns if X[feature].dtype !='O' and feature not in ordinal_features]\n        \n        categorical_features = [feature for feature in X.columns if feature not in ordinal_features and \n                               feature not in numerical_features]\n        \n        \n        # Converting temporal features to more useful values which is year past till present year.\n        X['GarageYrBlt'] = 2020 - X['GarageYrBlt']\n        X['YrSold'] = 2020 - X['YrSold']\n        X['YearBuilt'] = 2020 - X['YearBuilt']\n        X['YearRemodAdd'] = 2020 - X['YearRemodAdd']\n        \n\n       \n        if self.combined_features:\n            \n            X['OverallGrade'] =  X['OverallQual'] * X['OverallCond']\n            X['ExterGrade'] =  X['ExterQual'] * X['ExterCond']\n            X['BsmtGrade'] = X['BsmtQual'] * X['BsmtCond']\n            X['BsmtFinType'] = X['BsmtFinType1'] + X['BsmtFinType2']\n            X['BsmtFinSF'] = X['BsmtFinSF1'] + X['BsmtFinSF2']\n            X['FlrSF'] = X['1stFlrSF'] + X['2ndFlrSF']\n            X['BsmtBath'] = X['BsmtFullBath'] + X['BsmtHalfBath']\n            X['Bath'] = X['FullBath'] + X['HalfBath']\n            X['GarageGrade'] = X['GarageQual'] * X['GarageCond']\n            X['Porch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['3SsnPorch'] + X['ScreenPorch']\n            \n            numerical_features.extend(['OverallGrade', 'ExterGrade', 'BsmtGrade', 'BsmtFinType', 'BsmtFinSF', 'FlrSF',\n                                       'BsmtBath', 'Bath', 'GarageGrade', 'Porch'])\n            \n            \n        \n        if self.combined_features and self.drop_underlying_features: # do not wish to drop these features if we have not created\n                                                                     # any features using them.\n            to_drop = [ 'BsmtFinType1','BsmtFinType2', 'BsmtFinSF1', '1stFlrSF', 'BsmtFullBath',\n                       'BsmtHalfBath', 'FullBath', 'HalfBath', 'GarageCond', \n                       'OpenPorchSF', 'BsmtFinSF2', '2ndFlrSF', 'EnclosedPorch', \n                       '3SsnPorch', 'ScreenPorch']\n              \n            \n            X.drop(to_drop, axis=1, inplace=True)\n        \n            \n        if self.drop_features:\n            \n            to_drop = ['Utilities', 'PoolQC', 'MiscFeature', 'Street', \n                       'Condition2', 'MasVnrType', 'LowQualFinSF', 'Alley', 'MiscVal', 'Fence', \n                   'KitchenAbvGr', 'PoolArea', 'Street', 'RoofMatl', 'Exterior2nd', 'Heating']\n\n            X.drop(to_drop, axis=1, inplace=True)\n            \n        ordinal_features = [feature for feature in X.columns if X[feature].dtype != 'O' and feature not in numerical_features]\n        numerical_features = [feature for feature in X.columns if X[feature].dtype != 'O' and feature not in ordinal_features]\n        categorical_features = [feature for feature in X.columns if X[feature].dtype == 'O']\n        # we are refreshing our list of features names in ordinal and numerical lists because we have dropped few features\n        # from ordinal and numerical feature lists.\n        \n        \n            \n        if self.polynomial_features:\n            \n            if len(X) > 800 and not self.test:\n                y = train['SalePrice'].iloc[X.index].to_frame()\n                saleprice = pd.concat([X, y], axis=1).corr()['SalePrice']\n                self.polynomial_features_list = list(saleprice.sort_values(ascending=False).index[1:self.polynomial_features + 2])   \n                \n                for feature in self.polynomial_features_list:\n                    for i in [2, 3]:\n                        X[f'{feature} - sq{i}'] = X[feature] ** i\n                        \n            else: # test sets should add those polynomial features which were found worthy while going through training set.\n                for feature in self.polynomial_features_list:\n                    for i in [2, 3]:\n                        X[f'{feature} - sq{i}'] = X[feature] ** i\n                        \n                        \n        \n            \n        if self.outlier_threshold:\n            \n            numerical_features = [feature for feature in X.columns   \n                              if feature not in ordinal_features and X[feature].dtype != 'O' ]\n            # again we have to refresh the list because we might have generated a lot of numerical features if the execution\n            # went through the polynomial feature \"if statement\" above.\n            \n            for feature in numerical_features:\n                unique_vals = X[feature].nunique()\n                # if we look at the data set we can observe that generally numerical features with less than 10 unique values\n                # do not have any outliers, what they have is majority of only one value across instances, so it will be better\n                # and all the other distinct values are quite close magnitude wise to the value in majority, so i am not\n                # considering such features for outlier removal.\n                if unique_vals > 10:\n                    q1 = np.percentile(X[feature], 25)\n                    q3 = np.percentile(X[feature], 75)\n                    iqr = q3 - q1 \n                    if not iqr:  # will be executed if iqr = 0, to prevent creation of constant data columns\n                        iqr = 1  \n                    cut_off = iqr * self.outlier_threshold\n                    lower, upper = q1 - cut_off, q3 + cut_off\n                    X[feature].where(~(X[feature] > upper), upper, inplace=True)\n                    X[feature].where(~(X[feature] < lower), lower, inplace=True)\n                    \n                    \n            \n        if self.correlation_threshold:\n            \n            if len(X) > 800 and not self.test:             \n                \n                # cannot allow test set to pass through this code block, we will drop those columns in test set too which \n                # were found less correlated with target in training set\n                \n                self.low_correlation_drop_list = []            \n                y = train['SalePrice'].iloc[X.index].to_frame() \n                saleprice = pd.concat([X, y], axis=1).corr()['SalePrice']\n                corr_dict = saleprice.sort_values(ascending=False).to_frame().to_dict()['SalePrice']\n\n                for key, value in corr_dict.items():\n                    if value < self.correlation_threshold and value > - self.correlation_threshold:\n                        self.low_correlation_drop_list.append(key)\n            \n                X = X.drop(self.low_correlation_drop_list, axis=1)\n            \n            else:\n                X = X.drop(self.low_correlation_drop_list, axis=1)\n                \n                \n        numerical_features = [feature for feature in X.columns if X[feature].dtype != 'O']  \n        # now we do not have to separate ordinal features from numerical features\n        # all numeric types go for power transformation!!\n        \n        categorical_features = [feature for feature in X.columns if feature not in numerical_features]\n        \n        X.replace({0 : 1e-5}, inplace=True) # values for box-cox should be strictly positive.\n        X_index = X.index\n        \n        num_pipeline = Pipeline([('scale', MinMaxScaler(feature_range=(1e-5, 1))), \n                                 ('power_transform', PowerTransformer(method='box-cox'))])\n       \n        transformer = ColumnTransformer([('num_pipeline', num_pipeline, numerical_features)])\n        \n        numerical_features_transformed = transformer.fit_transform(X)\n        \n        numerical_df = pd.DataFrame(numerical_features_transformed, columns=numerical_features, index=X.index)   \n        \n        X = pd.concat([numerical_df, X.loc[:, categorical_features]], axis=1)\n        \n        if not self.test:\n            X_total = pd.concat([X, X_train_imputed.loc[:, categorical_features].loc[~(X_train_imputed.index.isin(X.index))]])\n            X_total = pd.get_dummies(X_total)\n            one_hot_features = [feature for feature in X_total.columns if feature not in numerical_features]\n        \n        # while cross validation it might be the case that some rare categories in categorical features might get \n        # concentrated in just the training set of the cross validation and test fold might not have even a single occurence\n        # this will cause the test and training sets to be of different column sizes after one hot encoding, to circumvent \n        # this issue i had to use only the categorical portion of those rows in main training set which are not present in the\n        # cross val training set. This is not the best way to avoid the problem and is somewhat controversial as test should not\n        # be exposed to the training set. any suggestions with regards to this problem will be highly appreciated. \n            \n            \n        else: \n            X = pd.get_dummies(X)\n        \n        \n        if self.empty_column_dropping_threshold:\n            if len(X) > 800 and not self.test:\n                self.one_hot_features_to_drop = [] \n                for feature in one_hot_features:\n                    zero_count = X_total[feature].value_counts()[0] \/ len(X_total)\n\n                    if zero_count > self.empty_column_dropping_threshold:\n                        self.one_hot_features_to_drop.append(feature)\n                        X_total.drop(feature, axis=1, inplace=True)\n                            \n        \n                \n            elif not self.test: # exclusively for test set of cross validation...\n                X_total.drop(self.one_hot_features_to_drop, axis=1, inplace=True)\n            \n            \n                \n            else: # for the final test set\n                X.drop(self.one_hot_features_to_drop, axis=1, inplace=True)\n            \n        if not self.test:\n            X = X_total.dropna() # drop the rows added for one hot encoding, those rows have nan values in numerical columns \n        \n            \n        if self.final_correlation_threshold:\n            \n            # lets just let the model decide if it finds dropping any more features less correlated with target useful or not!!\n            \n            if len(X) > 800 and not self.test:                  \n                                                               \n                self.final_low_correlation_drop_list = []           \n                y = train['SalePrice'].iloc[X.index].to_frame() \n                saleprice = pd.concat([X, y], axis=1).corr()['SalePrice']\n                corr_dict = saleprice.sort_values(ascending=False).to_frame().to_dict()['SalePrice']\n\n                for key, value in corr_dict.items():\n                    if value < self.final_correlation_threshold and value > - self.final_correlation_threshold:\n                        self.final_low_correlation_drop_list.append(key)\n            \n                X = X.drop(self.final_low_correlation_drop_list, axis=1)\n            \n            else:\n                X = X.drop(self.final_low_correlation_drop_list, axis=1)\n        \n        # finally, we can also add more functionality, such as dimensionality reduction using pca, lle.. but for the\n        # present dataset these were not useful so i did not add them, but there will be occasions where dimensionality\n        # reduction can help, but searching feature space will take a lot more time, if it does, than it would be better to \n        # find the best feature set and then see if reducing dimensions just only reduces training time or if it also helps\n        # boost score(which it generally does not!!)\n\n        return X","a563625a":"X_train_imputed = missing_value_imputation(X_train.copy())","4d3ada0f":"param_grid = {'feature_engineering__combined_features':[True],\n              'feature_engineering__drop_features':[True],\n              'feature_engineering__drop_underlying_features':[True],\n              'feature_engineering__polynomial_features':[9],\n              'feature_engineering__correlation_threshold': [0.07],\n              'feature_engineering__outlier_threshold':[1.5], \n              'feature_engineering__empty_column_dropping_threshold':[0.99],\n              'feature_engineering__final_correlation_threshold':[False]\n              }\n#these are the parameters that i found to be the best","1a35d58a":"linreg = Pipeline([('feature_engineering', FeatureEngineering()),\n                   ('linreg', Ridge())])\n\ngrid = GridSearchCV(linreg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\ngrid.fit(X_train_imputed.copy(), y_train)","56de090d":"grid.best_params_","1f3005cf":"fe = FeatureEngineering(**{'combined_features': True,\n                             'correlation_threshold': 0.07,\n                             'drop_features': True,\n                             'drop_underlying_features': True,\n                             'empty_column_dropping_threshold': 0.99,\n                             'final_correlation_threshold':False,\n                             'outlier_threshold': 1.5,\n                             'polynomial_features': 9})","b6d70aac":"X_train_imputed = missing_value_imputation(X_train.copy())\nX_test_imputed = missing_value_imputation(X_test.copy())","943a26ff":"X_train_imputed.shape, X_test_imputed.shape","4668e2ff":"X_train_fe = fe.transform(X_train_imputed.copy())","9371475a":"X_train_fe.shape","f0a679ab":"fe.test = True","8b60d513":"X_test_fe = fe.transform(X_test_imputed.copy())","4254a7cf":"X_test_fe.shape","6035052b":"fe.test = False","9dc848f2":"corr_sale = pd.concat([X_train_fe, y_train], axis=1).corr()['SalePrice']","9e4cc2ed":"corr_sale.sort_values(ascending=False)","9c42ba5a":"X_train_fe.columns == X_test_fe.columns","6760679b":"def results(cv_results_, n):\n    df = pd.DataFrame(cv_results_)[['params', 'mean_test_score']].nlargest(n, columns='mean_test_score')\n    for i in range(len(df)):\n        print(f'{df.iloc[i, 0]} : {df.iloc[i, 1]}')","3ff10c1e":"def parameter_plot(model, X, y, n_estimators=[100, 200, 300, 400, 600, 900, 1300], hyper_param=None, **kwargs):\n    param_name, param_vals = hyper_param\n    param_grid = {'n_estimators':n_estimators,\n                  f'{param_name}':param_vals}\n    \n    grid = GridSearchCV(model(**kwargs), param_grid, \n                        cv=RepeatedKFold(n_splits=8, n_repeats=1, random_state=42), \n                        scoring='neg_root_mean_squared_error', n_jobs=-1, verbose=2)\n    grid.fit(X, y)\n    results = pd.DataFrame(grid.cv_results_)['mean_test_score'].values\n    results = results.reshape(len(param_vals), len(n_estimators))\n    \n    plt.figure(figsize=(15, 9))\n    for i in range(1, len(param_vals) + 1):\n        plt.plot(n_estimators, results[i-1], label=f'{param_name} - {param_vals[i-1]}')\n      \n    plt.legend()\n    plt.show()","708316aa":"def learning_curve_plotter(Model, X, y, params_1, params_2, step=50):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n    \n    fig, ax = plt.subplots(1, 2, sharey=True, figsize=(16, 7))\n    for i, (name, params) in enumerate([params_1, params_2]):\n        train_score = []\n        val_score = []\n        for j in range(100, len(X_train), step):\n            model = Model(**params).fit(X_train[:j], y_train[:j])\n            y_train_preds = model.predict(X_train[:j])\n            y_test_preds = model.predict(X_test)\n            train_score.append(np.sqrt(mean_squared_error(y_train[:j], y_train_preds)))\n            val_score.append(np.sqrt(mean_squared_error(y_test, y_test_preds)))\n            \n        ax[i].plot(train_score, 'r-', label='Training error')\n        ax[i].plot(val_score, 'b-', label='Validation error')\n        ax[i].set_title(f'{name}', fontsize=18)\n        ax[i].set_xlabel('Training set size', fontsize=15, labelpad=10)\n        ax[i].set_ylabel('Root mean squared log error', fontsize=15, labelpad=10)\n        ax[i].legend()\n            \n    plt.show()","86d5d837":"param_grid_ridge = {'alpha':0.1 * np.arange(1, 70)}","07278619":"grid_ridge = GridSearchCV(Ridge(), param_grid_ridge, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)                  ","d63ed72e":"grid_ridge.fit(X_train_fe, y_train)","4bfaaa23":"results(grid_ridge.cv_results_, n=40)","fdb6428e":"best_params_ridge = {'alpha': 3.8000000000000003}","8942b75d":"param_grid_lasso = {'alpha': 0.0001 * np.arange(1, 100)}","fe3f1a9e":"grid_lasso = GridSearchCV(Lasso(), param_grid_lasso, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)","0e28030c":"grid_lasso.fit(X_train_fe, y_train)","5c89cc67":"results(grid_lasso.cv_results_, n=40)","3042c22d":"best_params_lasso = {'alpha': 0.0002}","5f5ce585":"param_grid_elastic = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10],\n                      'l1_ratio':0.01 * np.arange(10)}","3c936d53":"grid_elastic = GridSearchCV(ElasticNet(), param_grid_elastic, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)","b3fb3cef":"grid_elastic.fit(X_train_fe, y_train)","aa1ddb5c":"results(grid_elastic.cv_results_, n=40)","2e9886e5":"best_params_elastic = {'alpha': 0.001, 'l1_ratio': 0.09} ","325c9d82":"param_grid_svr = {'kernel': ['rbf'],\n                  'degree':[1], \n                  'epsilon':[0.001, 0.01, 0.008, 0.013],\n                  'C':[0.1, 0.5, 1,  20, 25],\n                  'gamma':[0.0004, 0.0006, 0.0007, 0.0008]}","745e3d9d":"grid_svr = GridSearchCV(SVR(), param_grid_svr, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)","badd8757":"grid_svr.fit(X_train_fe, y_train)","306866f1":"results(grid_svr.cv_results_, n=60)","cc3da988":"regularized_params = ('Regularized Model', {'C': 1, 'degree': 1, 'epsilon': 0.013, 'gamma': 0.0008, 'kernel': 'rbf'} )\nbest_params = ('Best Model from Grid Search', {'C': 25, 'degree': 1, 'epsilon': 0.008, 'gamma': 0.0004, 'kernel': 'rbf'})\nlearning_curve_plotter(SVR, X_train_fe, y_train, regularized_params, best_params)","7ac77f90":"best_params_svr = {'C': 25, 'degree': 1, 'epsilon': 0.008, 'gamma': 0.0004, 'kernel': 'rbf'}","bfcc24ba":"parameter_plot(ExtraTreesRegressor, X_train_fe, y_train, hyper_param=('max_depth', [3, 5, 7, 11, 13, 15, None]))","fe2c8cf9":"param_grid_extra = {'max_depth': [8, 11], \n                    'max_samples':[0.6, 0.8, None],\n                    'max_features':[0.5, 0.8, None],\n                    'n_estimators': [200, 600, 1000]}","0122c5c0":"grid_extra = GridSearchCV(ExtraTreesRegressor(), param_grid_extra, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)","b7b52757":"grid_extra.fit(X_train_fe, y_train)","d3b175c6":"results(grid_extra.cv_results_, n=40)","98946f12":"regularized_params = ('Regularized Model', {'max_depth': 8, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 200} )\nbest_params = ('Best Model from Grid Search', {'max_depth': 11, 'max_features': 0.5, 'max_samples': None, 'n_estimators': 1000})\nlearning_curve_plotter(ExtraTreesRegressor, X_train_fe, y_train, regularized_params, best_params)","9004350f":"best_params_extra = {'max_depth': 8, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 200}","c9e72450":"parameter_plot(RandomForestRegressor, X_train_fe, y_train, hyper_param=('max_depth', [3, 5, 7, 11, 13, 15, None]))","b1151fc7":"param_grid_random = {'max_depth': [8, 11],\n                     'max_samples':[0.6, 0.8, None], \n                     'max_features':[0.5, 0.7, None], \n                    'n_estimators': [200, 500, 1000]}","16a72814":"grid_random = GridSearchCV(RandomForestRegressor(), param_grid_random, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)","ffd21916":"grid_random.fit(X_train_fe, y_train)","461d1c6a":"results(grid_random.cv_results_, n=40)","f9a46f55":"regularized_params = ('Regularized Model', {'max_depth': 8, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 1000} )\nbest_params = ('Best Model from Grid Search', {'max_depth': 11, 'max_features': 0.5, 'max_samples': None, 'n_estimators': 500})\nlearning_curve_plotter(RandomForestRegressor, X_train_fe, y_train, regularized_params, best_params)","16653aad":"best_params_random = {'max_depth': 8, 'max_features': 0.5, 'max_samples': 0.8, 'n_estimators': 1000}","5a7b2815":"parameter_plot(GradientBoostingRegressor, X_train_fe, y_train, hyper_param=('max_depth', [3, 4, 5, 6, 7]))","9b17c467":"parameter_plot(GradientBoostingRegressor, X_train_fe, y_train, \n               n_estimators=[100, 250, 400, 600, 1000],\n               hyper_param=('learning_rate', [0.01, 0.02, 0.04, 0.07, 0.1]), \n               max_depth=3)","703278ea":"param_grid_gradient = {'n_estimators':[250, 400, 600],\n                       'learning_rate':[0.035, 0.05],\n                       'subsample':[0.5, 0.7],\n                       'max_depth':[3],\n                       'max_features':[0.5, 0.7]\n                       }","429954e2":"grid_gradient = GridSearchCV(GradientBoostingRegressor(), param_grid_gradient, \n                        cv=RepeatedKFold(n_splits=10, n_repeats=2, random_state=42),\n                              scoring='neg_root_mean_squared_error', verbose=2, n_jobs=-1)","12a22d43":"grid_gradient.fit(X_train_fe, y_train)","2b7b8b18":"results(grid_gradient.cv_results_, n=40)","1543323e":"regularized_params = ('Regularized Model', {'learning_rate': 0.035, 'max_depth': 3, 'max_features': 0.5, \n                                            'n_estimators': 400, 'subsample': 0.7} )\nbest_params = ('Best Model from Grid Search', {'learning_rate': 0.035, 'max_depth': 3, 'max_features': 0.5,\n                                               'n_estimators': 600, 'subsample': 0.5})\nlearning_curve_plotter(GradientBoostingRegressor, X_train_fe, y_train, regularized_params, best_params)","20dcff7a":"best_params_gradient = {'learning_rate': 0.035, 'max_depth': 3, 'max_features': 0.5, \n                                            'n_estimators': 400, 'subsample': 0.7}","30d6e8f7":"def objective(trial, X, y):\n    \n    params = {'n_estimators':2000,\n              # trail.suggest_unifrom() allows to pick out any value between the given range, values will be continuous and\n              # not just integers.\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.005, 0.01),\n              \n              # trial.suggest_categorical() allows only the passed categorical values to be suggested.\n              'subsample':trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              \n              # trial.suggest_int() will suggest integer values within the integer range. \n              'max_depth':trial.suggest_int('max_depth', 3, 11),\n              \n              'colsample_bylevel':trial.suggest_categorical('colsample_bylevel', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n              \n              # trail.suggest_loguniform() is used when the range of values have different scales.\n              'reg_lambda':trial.suggest_loguniform('reg_lambda', 1e-3, 100),\n              'reg_alpha':trial.suggest_loguniform('reg_alpha', 1e-3, 100),\n              'n_jobs':-1}\n    \n    model = XGBRegressor(**params)\n    \n    split = KFold(n_splits=5)\n    train_scores = []\n    test_scores = []\n    for train_idx, val_idx in split.split(X_train_fe):\n        X_tr = X_train_fe.iloc[train_idx]\n        X_val = X_train_fe.iloc[val_idx]\n        y_tr = y_train.iloc[train_idx]\n        y_val = y_train.iloc[val_idx]\n        \n        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                  eval_metric=['rmse'],\n                  early_stopping_rounds=30, verbose=0,\n                  # optuna allows us to pass pruning callback to xgboost callbacks, so any trial which does not seem to be \n                  # better or not qualify a given threshold of loss reduciton after some iterations will get pruned, that is\n                  # stopped in between hence saving time, we will see it in action below.\n                  callbacks=[optuna.integration.XGBoostPruningCallback(trial, observation_key=\"validation_0-rmse\")]\n                 )\n    \n        train_score = np.round(np.sqrt(mean_squared_error(y_tr, model.predict(X_tr))), 4)\n        test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 4)\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n        \n    \n    print(f'train score : {train_scores}')\n    print(f'test score : {test_scores}')\n    train_score = np.round(np.mean(train_scores), 4)\n    test_score = np.round(np.mean(test_scores), 4)\n    \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n    \n    # you can make this function as bespoke as possible... you can return any kind of modified value using the return function\n    # optuna will try to optimize it!!\n    \n    return test_score","58b66992":"optimize = partial(objective, X=X_train_fe, y=y_train)\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(optimize, n_trials=100) # we have passed 300 trials which are like steps optuna can take to get to the \n                                       # optimized value.","06b95f3d":"optuna.visualization.plot_optimization_history(study)","631a1be8":"optuna.visualization.plot_slice(study)","61c10dfb":"plt.style.use('fivethirtyeight')\ntraining_scores = []\nvalidation_scores = []\ntest_score_matrix = []\nparams = study.best_params\nparams['n_estimators'] = 10000\nparams['n_jobs'] = -1\nfor gamma in 0.001*np.arange(0, 100, 1):\n    params['gamma'] = gamma\n    model = XGBRegressor(**params)\n    train_scores = []\n    test_scores = []\n    split = KFold(n_splits=5)\n    for train_idx, val_idx in split.split(X_train_fe):\n        X_tr = X_train_fe.iloc[train_idx]\n        X_val = X_train_fe.iloc[val_idx]\n        y_tr = y_train.iloc[train_idx]\n        y_val = y_train.iloc[val_idx]\n        \n        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n                  eval_metric=['rmse'],\n                  early_stopping_rounds=30, verbose=0)\n                  #callbacks=[optuna.integration.XGBoostPruningCallback(trial, observation_key=\"validation_0-rmse\")])\n        \n        train_score = np.round(np.sqrt(mean_squared_error(y_tr, model.predict(X_tr))), 4)\n        test_score = np.round(np.sqrt(mean_squared_error(y_val, model.predict(X_val))), 4)\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    test_score_matrix.append(test_scores)\n \n    print(f'working on gamma : {gamma}')\n    print(f'train scores : {train_scores}')\n    print(f'test scores : {test_scores}')\n    print(f'Mean OOF RMSLE : {np.mean(test_scores)}')\n    train_score = np.mean(train_scores)\n    test_score = np.mean(test_scores)\n    \n    training_scores.append(train_score)\n    validation_scores.append(test_score)\n    \nplt.figure(figsize=(15, 8))\nplt.plot(training_scores, 'r-', label='Train-Gamma-Effect')\nplt.plot(validation_scores, 'b-', label='Test-Gamma-Effect')\nplt.legend()\nplt.grid()\nplt.show()","8f2685dc":"params = study.best_params\nparams['n_estimators'] = 1300\nparams['gamma'] = 0.05\nbest_params_xgb = params","a0b7e795":"def model_evaluation(model):\n    cv = RepeatedKFold(n_splits=8, n_repeats=2)\n    scores = cross_val_score(model, X_train_fe, y_train, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')\n    return np.sqrt(np.abs(scores))","ad0263ac":"lasso = Lasso(**best_params_lasso)\nridge = Ridge(**best_params_ridge)\nelastic = ElasticNet(**best_params_elastic)\nsvr = SVR(**best_params_svr)\nextra = ExtraTreesRegressor(**best_params_extra)\nrandom = RandomForestRegressor(**best_params_random)\ngradient = GradientBoostingRegressor(**best_params_gradient)\nxgb = XGBRegressor(**best_params_xgb)\n\n\n\nmodel_list1 = [('lasso', lasso),   \n          ('svr', svr),\n          ('extra', extra),\n          ('elastic', elastic),\n          ('ridge', ridge), \n          ('random', random),\n          ('gradient', gradient),\n          ('xgb', xgb),\n           ]","b7fa95cc":"stack_reg = StackingRegressor(estimators=model_list1, cv=8, n_jobs=-1)","4732afe0":"models = {'lasso': lasso, \n           'ridge': ridge, \n           'elastic': elastic, \n           'svr': svr, \n           'extra' : extra, \n           'random':random, \n           'gradient': gradient, \n           'xgb': xgb, \n           'stack_reg':stack_reg\n         }\n\n\nscores = []\nnames = []\n\nfor name, model in models.items():\n    score = model_evaluation(model)\n    scores.append(score)\n    names.append(name)\n    \nplt.figure(figsize=(16, 8))    \nplt.boxplot(scores, labels=names, showmeans=True)\nplt.show()","4205824f":"stack_reg.fit(X_train_fe, y_train)","513f29cf":"submission = pd.DataFrame(stack_reg.predict(X_test_fe), columns=['SalePrice'])","3b6f5b2c":"submission = np.expm1(submission)","8bab1e66":"submission.insert(loc=0, column='id', value=[i for i in range(1461, 2920)])","a3df67a8":"submission.to_csv('submission.csv', index=False)","b1f095ca":"pd.read_csv('submission.csv')","c4406c9b":"We can appreciate how the stacking has its spread lowest among all the other models!!","0f3a6d0a":"## Feature Space Search","c94a6e8c":"## Model Selection","bab6064a":"Best model looks good, the difference between training loss and validation loss for both of them is equal, plus regularized model is underfitting the data, so lets go with The best model!!","fac3f348":"## Objectives\n\nThe main objective of this notebook is to present some hyper parameter tuning techniques in an approachable way to beginners, using both GridSearchCV and Optuna library which is becoming quite popular for performing optimization.. we will optimizing XGBRegressor using optuna and will also check out some cool inbuilt plots that library provides to give deeper insights into the parameter selection and optimization process.\n* Optuna\n* GridSearchCV\n\n\nThis notebook also serves as a beginner tutorial for implementing a feature engineering class to grid search the feature space using a linear model through a pipeline. The best set of features hence found will then be used to search the best parameters,\nvalidated using learning curve, for base models to make predictions used for trainig meta model in stacking ensemble. We will\nalso look at the diminishing returns in the case of gradient boosting algorithms and how to use this knowledge to improve our \nstacking results. the objectives are laid down below:\n\n#### I - To create a Feature Engineering class whose parameters can be grid searched to find the features which helps achieve lowest loss, the class has following features:\n\n 1. add or not to add features which are combined using multiple features.\n \n 2. add or not to add polynomials of features most correlated with the target, if yes then how many features to select.\n \n 3. drop or not to drop features with lowest correlation with target, if yes then what should be the threshold.\n \n 4. remove or not to remove outliers from features, if yes than what should be the threshold.\n \n 5. to drop or not drop one hot encoded columns which are too sparse, if yes than what should be the threshold.\n \n#### II - To fine tune base model parameters with help of learning curves.\n\n#### III - To understand diminishing returns point for gradient boosting algortihms.\n\n#### IV - Use stacking to combine the predictions of our models.","28669dc6":"### ExtraTreesRegressor","e7f009e8":"### Optuna plots","4aa32440":"### RandomForestRegressor","7f1e301b":"## Final Data set creation","84adc704":"### Lasso","3585d9e6":"### GradientBoostingRegressor","29e5f993":"We can see that as the gamma value increases the overfitting decreases.... if the dataset had many more instances, we could have seen that increasing gamma initially improves loss and then it agains starts to increase... using gamma is important as it prunes leafs that are causing model to fit on the noise of the training data set.","1b6bde92":"Once you decide on what learning rate to choose, pass it to the function and it will be get caught by kwargs and passed to model in gridsearch.  ","2f409c49":"This plot lets us visualize a summary of the trials which optuna went through.","d940878c":"Using this plot we can easily visualize which parameters range optuna found useful for optimization.. in case of learning_rate we can see that optuna concentrates more on the right half which is evident from the cluster of dots in the right half of the plot.","09730e8c":"The above exercise was done so that we could understand our gradient boosting models dealing with the data set at hand... now we can narrow down our search with facing least overfitting","cf1f1610":"RandomForestRegressor has the same problem with the ExtraTreesRegressor.","d4bf2af3":"To work with Optuna you have to follow the following guideline:\n* Make a objective function, this function will return the value wish to optimize.. in our case it will be the RMSLE loss.\n* create a **study** using **`optuna.create_study(direction='minimize')`**, will use direction=minimize because we wish to minimize the loss which is RMSLE. \n* Finally enjoy looking at optuna optimizing your loss!!\n\nThats is it, optuna makes it so simple to optimize the parameters.","ca666b0a":"create a regularized model with params such as max depth, max features and max samples, then start tweeking the estimators.","632b87f4":"This looks tricky, both models are horribly overfitting the training data, regularized model is a little better but the difference between the too model is not great, we will have to test both of them to find out which one works the best.","31a685bf":"Again, overfitting is present in both models, but in regularized model it is not as severe as in the Best model.","b648f210":"### Ridge","1b4ab236":"### Elastic","bfec8074":"**If you found the kernel useful, upvote!**\n\n**If you have forked it but not upvoted yet, show support and upvote!! :)**\n\n**Please leave in the comments any suggestions and constructive criticism!!**","af52519c":"### SVR","3de98aab":"### Fine tuning 'gamma'","d0d18432":"gamma governs the minimum impurity reduction required to split the node. It is 0 by default which will make the algorithm keep on splitting nodes arbitrarily even with miniscule reduction in impurity. This cause birth of leaves that are fitted on the noise of training data... what we want to do is restrict this and improve generalization. lets fine tune gamma using a plot.","bdd83851":"### Finding the diminishing returns point\n\nFor a given data set there will be a point for a given a set of parameters after which gradient boosted algorithms will stop\nexperiencing substantial amount of decrease in loss with increase in number of estimators, if you still train the model after \nthat point you may experience some decrease in loss but the model will overfit the training data and you will not achieve\ngeneralized result in final test set. \n\nin the function below we will first grid search the learning rate with default n_estimators as provided in function params.\nthen we will use the best learning rate hence found to find the best max_depth param.\n\nwe have already set **subsample = 0.5**, this is because for gradient boosting having subsample from 0.4 - 0.6 is usually is always\nbetter than no subsampling.\n\nthis will give us a idea about gradient boosting as applied to our present dataset and will allow us to avoid overfitting\nand achieve the best possible loss before we hit the point of diminishing returns","89cab019":"## Stacking","88a2938e":"from the plot above we can see that the higher learning rates reach the point of diminishing returns quickly and after that as observed in the case of 0.03 and 0.05, loss starts to increase. 0.1 is behaving erratically!!\nso the learning rate for our model should be searched in range 0.015 - 0.025 and i would search with trees ranging from 350 - 650","db4307ed":"### XGBRegressor","5d495209":"Grid searching depth 3 and 4 would be best..."}}