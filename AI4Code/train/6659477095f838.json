{"cell_type":{"21cd566a":"code","e2c38b96":"code","07f0f2e0":"code","18bbe39b":"code","4da07f7b":"code","8736be5b":"code","f9777da3":"code","2e68bc16":"code","1ee527c0":"code","d9cb20b1":"code","d7161599":"code","f73dfb58":"code","f974b149":"code","dcb45655":"code","1c6a2da9":"code","210720cc":"code","2b2b108d":"code","14bcbf5f":"code","753d8900":"code","c80ce7f7":"code","4bcebc75":"code","d9bdff5a":"code","0406110b":"code","b5e65078":"code","b03bb5fd":"code","c5165fef":"code","a53f93be":"code","f12310c5":"code","56beeb3a":"code","4b865153":"code","69abaa32":"code","6c0db080":"code","fea4fbac":"code","98b2418a":"code","5afca648":"code","4173b0c6":"code","86657c0d":"code","507c1bd2":"code","54f05735":"code","0246258e":"code","fed70f61":"code","4e090805":"code","176aa3b3":"code","d8ccac9a":"code","521363b3":"code","ce0c949c":"code","d7f4e31d":"code","cbf76b8b":"code","dc898447":"code","028eb57f":"code","d85f0c86":"code","f2d7fa7f":"code","357a63e8":"code","4a96a274":"code","0572f6af":"code","71a4431f":"code","e322efb6":"code","f6e1b16c":"code","3f5e72cd":"code","3011db9c":"code","21a06049":"code","708ff75c":"code","66bdbab4":"code","0b9b6655":"code","3c49ed0c":"code","a3f48f2b":"code","cb02691a":"code","74c5bcc1":"code","769e734c":"code","c8a02d83":"code","196af069":"code","03a39855":"code","003d7bb2":"code","45baaee5":"code","4971f80d":"code","d69502b4":"code","72d9e4b6":"code","b2533b6b":"code","869530b4":"code","1c6d6534":"code","d749710b":"code","8816d749":"code","7edb91a4":"code","fa233e6b":"code","0ecd39ca":"code","e6f6d190":"code","4cb8443a":"code","566c2f99":"code","7145f90c":"code","9d78e403":"code","4834d9d2":"code","54c856c3":"code","2e703291":"code","3fff0abb":"code","462e323b":"code","31df80a0":"code","9b5cf778":"code","189ee8ae":"code","5457edb5":"code","a128136b":"code","c2f021d2":"code","378667e6":"code","35fed3a6":"code","1cf81b4e":"code","da918793":"code","416a268d":"code","5eaf9e72":"code","4fb1d13e":"code","b167e5a1":"code","0180f81a":"code","21b049a8":"code","3e29d39a":"code","a7b4218b":"code","19992568":"code","c8f2b983":"code","2726cf7a":"code","f3a0bf40":"code","6dbb6bbe":"code","144256ba":"code","4df0742c":"code","ac8beccc":"code","22efd3c1":"code","6335c5f9":"code","b8611c0f":"code","f7a0772f":"code","c73279a2":"code","997e98dd":"code","cca69181":"code","fce469f6":"code","d5e504c3":"code","2e2d486d":"code","61dd35df":"code","deef25b6":"code","c8658c31":"code","c69d481f":"code","f20d7313":"code","7a10834a":"code","06d9e529":"code","b37de71e":"code","15b8ca48":"code","9d83529c":"code","f69534bd":"code","17f6253d":"code","cda344c6":"code","6b68cfed":"code","43b4dd8d":"code","97645f19":"code","2fcc620b":"code","4dfcd0ef":"code","ff51297c":"code","fa1e8ec0":"code","ac18c9c8":"code","85b1acdc":"code","b56da6e2":"code","282ba6b6":"code","a5f6dead":"code","61da49d2":"code","b9374a31":"code","77dcedb1":"code","f0decf44":"code","af3c4d0c":"code","8dd3ec10":"code","34ffb8ba":"code","0b68c1d3":"code","a879b15c":"code","8c4e874e":"code","fdcf3e6e":"code","6f69f007":"code","641e4465":"code","c4f37294":"code","603853ac":"code","a5eaf69b":"code","12d41809":"code","aa4e3092":"code","222ca3b7":"code","7bacd20d":"code","ace8272c":"code","f6c12fa6":"code","93c6dab9":"code","b9302d8f":"code","9f704926":"code","8f2b057b":"code","e55e5e2b":"code","ba94f3f5":"code","707e3648":"code","0cb00e04":"code","1621adb3":"code","cd440b4b":"code","d8b50553":"code","8f7a5807":"code","6f4adc4d":"code","e6bcd434":"code","14d6641b":"code","734f3e4f":"code","bb855e72":"code","de1e8ff1":"code","433ad62d":"code","6a02297b":"code","3e48373b":"code","218a2fe9":"code","74880672":"code","21504320":"code","dd70faab":"code","33f9b4e3":"code","751f3cc7":"code","e691d5a6":"code","afdcd36d":"code","9ee6aa8e":"code","19c5c76d":"code","69559349":"code","808c3a85":"code","e9007d77":"code","72d260c0":"code","51697eab":"code","f29e6980":"code","244bddd2":"code","e2043b67":"code","a607f861":"code","1a756654":"code","3770e301":"code","cf4d5ec1":"code","38036f20":"code","0a747395":"code","4c683459":"code","e4db2ea7":"code","953a9bd9":"code","13dc6ed2":"code","1302ec2c":"code","ce547d53":"code","007a045e":"code","c61a99ce":"code","bd314ddf":"code","a2c08360":"code","75305299":"code","c0a82015":"code","2bb4d6f9":"code","25dcaa65":"code","e823672d":"code","1600789f":"code","b03b30c4":"code","32d2e8ac":"code","4d9d2cd5":"code","970e62d6":"code","5b3615f8":"code","c4ee216c":"code","2f2be450":"code","3571b044":"code","52f510c4":"code","7de5246d":"code","b64094ff":"code","46c7cd2d":"code","507341a2":"code","278d0c99":"code","823f2ecf":"code","e7f9fa1b":"code","f51cb250":"code","ecb7c904":"code","80d8ffbe":"code","85b6c1dd":"code","14491d96":"code","9df924d2":"code","2bc9a98e":"code","f9d7329a":"code","6d60df68":"code","e37ac4b5":"code","fa2024e6":"code","c66706e9":"code","028325dc":"code","ed1b1f71":"code","d46a9014":"code","194a7c41":"code","fb0f22c8":"code","736ed4df":"code","8c991388":"code","e6f75718":"code","f6cd0b01":"code","ce46997c":"code","595b0f8e":"code","6ac1d8b8":"code","5580dd42":"code","f99eb2df":"code","2f0cd216":"code","cb723c85":"code","bff47aa7":"code","162efaf6":"code","4cc6f940":"code","b58fc18b":"code","b52a0b04":"code","550a7804":"code","3eb9b7aa":"code","5ca97c7a":"code","e2c39022":"code","76173824":"markdown","133f63fb":"markdown","7d8cf14f":"markdown","c5a5ec7e":"markdown","e29653ff":"markdown","ec976ee1":"markdown","a163e942":"markdown","17dbdf80":"markdown","4943fa25":"markdown","de9723e6":"markdown","37a952aa":"markdown","2cc5b264":"markdown","a7db785c":"markdown","73189eb2":"markdown","42a3888f":"markdown","6ca7a02d":"markdown","d096961b":"markdown","2f45ec46":"markdown","4a2a500b":"markdown","7995e5ee":"markdown","dd086788":"markdown","00e6bc3e":"markdown","0466dc72":"markdown","31d4d5c4":"markdown","8b3e754d":"markdown","5d86ac1a":"markdown","bae64d82":"markdown","750ff9fe":"markdown","d6c32846":"markdown","b34c7e23":"markdown","47a3e666":"markdown","422a8996":"markdown","b866cf42":"markdown","7acdfd92":"markdown","14bb6114":"markdown","bdf50343":"markdown","bb4694a6":"markdown","67e67897":"markdown","0839c5e0":"markdown","fb3953f6":"markdown","7035ac4d":"markdown","eae66556":"markdown","ea8dd31b":"markdown","e5ffd44b":"markdown","54f79905":"markdown","703200bf":"markdown","fc199302":"markdown","e20b5a1e":"markdown","50ae82dd":"markdown","9f5289fc":"markdown","1754f52b":"markdown","cd87aa20":"markdown","026aea79":"markdown","6b2bfe09":"markdown","807a3bdc":"markdown","0dc50c92":"markdown","45e18460":"markdown","c6ef9ebe":"markdown","45badf51":"markdown","8631a652":"markdown","8dc30ed7":"markdown","9784c3da":"markdown","a078a44c":"markdown","ad01076b":"markdown","5f9ac350":"markdown","817934af":"markdown","3c28814c":"markdown","4f75c87f":"markdown","2773aee5":"markdown","45b458e1":"markdown","49f71239":"markdown","2d48ce02":"markdown","bd4e52bd":"markdown","0377c7b6":"markdown","f963d1da":"markdown","f0bb3179":"markdown","5f65b4f4":"markdown","6ae124e2":"markdown","dd856f51":"markdown","8d62b1da":"markdown","ca6dcc1a":"markdown","25b51782":"markdown","d1be9afd":"markdown","bcd80bb6":"markdown","b15c6785":"markdown","e61f29e3":"markdown","9eeab2c9":"markdown","96304a6d":"markdown","add9230b":"markdown","d9967522":"markdown","9bf1efe3":"markdown","885401e8":"markdown","d890634f":"markdown","0ec33843":"markdown","6e34cb2d":"markdown","567c4a95":"markdown","14ad87ec":"markdown","3db173e0":"markdown","daced399":"markdown","46a334a3":"markdown","24982e45":"markdown","9008ab97":"markdown","9d1abce5":"markdown","d08da9e7":"markdown","49af7f9c":"markdown","4567224f":"markdown","a752d747":"markdown","c7dc4574":"markdown","18bef292":"markdown","af1a0339":"markdown","d797798d":"markdown","a0e48465":"markdown","1f8627e7":"markdown","0d0817f1":"markdown","b850eb24":"markdown","63b2d93f":"markdown","e580aa35":"markdown","7e8eacb3":"markdown","27ce4c37":"markdown","09a9e601":"markdown","3c899934":"markdown","3ddd108d":"markdown","1a3f0ed2":"markdown","0180040d":"markdown","416f4af4":"markdown","fcc24b11":"markdown","aaf3ce31":"markdown","c2796834":"markdown","0b6107cd":"markdown","5f46c704":"markdown","1fc9cd7b":"markdown","6ccae8aa":"markdown","cef05feb":"markdown","b5fce2db":"markdown","3dfe0d56":"markdown","159a250d":"markdown","8bd4171c":"markdown","846c6b13":"markdown","72228b08":"markdown","c621668f":"markdown","b2dd421d":"markdown","41e38e93":"markdown","54ab90d3":"markdown","74c96c1d":"markdown"},"source":{"21cd566a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport re\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\nsns.set(style='darkgrid')\n%matplotlib inline","e2c38b96":"pd.read_csv('diabetic_data.zip').head()","07f0f2e0":"# csv contains \"?\" for missing values. We replace it with NaN\ndata = pd.read_csv('diabetic_data.zip', na_values=[\"?\"])\ndf= data.copy()\ndf.head()","18bbe39b":"features = pd.read_csv('features.csv',index_col='Unnamed: 0')\ninfo = lambda attribute:print(f\"{attribute.upper()} : {features[features['Feature']==attribute]['Description'].values[0]}\\n\")\nfeatures.head()","4da07f7b":"info('encounter_id')","8736be5b":"df.duplicated().value_counts()\n# df = df.drop_duplicates()","f9777da3":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str\n\ndisplay(summary(df).sort_values(by='Nulls', ascending=False))","2e68bc16":"df = df.drop(['citoglipton','examide','encounter_id'],axis=1)","1ee527c0":"df.gender.value_counts(dropna=False)","d9cb20b1":"gender_index = df[df.gender == 'Unknown\/Invalid'].index\ndf = df.drop(gender_index, axis=0)","d7161599":"# confirm removal\ndf.gender.value_counts(dropna=False)","f73dfb58":"df.readmitted.value_counts(dropna=False)","f974b149":"df = df.replace(['<30', '>30'], 'YES')","dcb45655":"info('patient_nbr')","1c6a2da9":"df['patient_nbr'].duplicated().value_counts(dropna=False)","210720cc":"# total unique patients\nlen(df.patient_nbr), df.patient_nbr.nunique()","2b2b108d":"# locate number of patient visits using patient_id\ndf.patient_nbr.value_counts()","14bcbf5f":"# keep only one record for each patient, the first visit\ndf = df.drop_duplicates(['patient_nbr'], keep='first')\ndf.shape","753d8900":"df.patient_nbr.nunique()","c80ce7f7":"df = df.drop('patient_nbr', axis=1)","4bcebc75":" def null_values(df):\n    \"\"\"a function to show null values with percentage\"\"\"\n    nv=pd.concat([df.isnull().sum(), 100 * df.isnull().sum()\/df.shape[0]],axis=1).rename(columns={0:'Missing_Records', 1:'Percentage (%)'})\n    return nv[nv.Missing_Records>0].sort_values('Missing_Records', ascending=False)","d9bdff5a":"# columns with missing values\nnull_values(df)","0406110b":"for i in ['weight','medical_specialty','payer_code']: info(i)","b5e65078":"df = df.drop(['weight','medical_specialty','payer_code'], axis=1)","b03bb5fd":"summary(df).sort_values(by='Uniques', ascending=False)[:20]","c5165fef":"for i in ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']: info(i)","a53f93be":"# drop columns\ndrop_cols = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\ndf = df.drop(drop_cols, axis=1)","f12310c5":"null_values(df)","56beeb3a":"df.race.value_counts(dropna=False)","4b865153":"df = df.dropna(axis=0, subset=['race'])\nnull_values(df)","69abaa32":"for i in ['diag_1', 'diag_2', 'diag_3']: info(i)","6c0db080":"info('number_diagnoses')","fea4fbac":"df[['diag_1', 'diag_2', 'diag_3','number_diagnoses']][df.diag_1.isnull() & df.diag_2.notnull() & df.diag_3.notnull() & df.number_diagnoses.notnull()]","98b2418a":"# remove rows where diagnosis 1 is missing\ndf = df.dropna(axis=0, subset=['diag_1'])","5afca648":"null_values(df)","4173b0c6":"df[['diag_1','diag_2', 'diag_3','number_diagnoses']][df.diag_2.isnull() & (df.diag_3.notnull()|(df.number_diagnoses > 1))]","86657c0d":"# remove rows where diagnosis 2 is missing and number of diagnoses is greater than 1\ndiag_2_indexes = df[df.diag_2.isnull() & (df.diag_3.notnull()|(df.number_diagnoses > 1))].index\ndf = df.drop(index = diag_2_indexes, axis=0)","507c1bd2":"null_values(df)","54f05735":"# list of affected rows\ndf[['diag_1','diag_2', 'diag_3', 'number_diagnoses']][df.diag_3.isnull() & (df.number_diagnoses > 2)]","0246258e":"# remove rows with missing diagnosis 3 and number of diagnoses is greater than 2\ndiag_3_indexes = df[(df.diag_3.isnull()) & (df.number_diagnoses > 2)].index\ndf = df.drop(index=diag_3_indexes, axis=0)","fed70f61":"null_values(df)","4e090805":"sns.heatmap(df[['diag_1','diag_2', 'diag_3','number_diagnoses']].isnull(),yticklabels=False,cbar=False,cmap='viridis');","176aa3b3":"# replace NaN with None in diagnosis 2 and 3 to show there is no additional diagnosis\ndf.fillna('None', inplace=True)","d8ccac9a":"# confirm there are no more NaN values\nnull_values(df)","521363b3":"summary(df[['diag_1','diag_2', 'diag_3']])","ce0c949c":"# Circulatory\ncodes =[str(i) for i in list(range(390,460)) + [785]]\ndf = df.replace(codes, 'Circulatory')","d7f4e31d":"# Respiratory\ncodes =[str(i) for i in list(range(460,520)) + [786]]\ndf = df.replace(codes, 'Respiratory')","cbf76b8b":"# Digestive\ncodes =[str(i) for i in list(range(520,580)) + [787]]\ndf = df.replace(codes, 'Digestive')","dc898447":"# Diabetes\ndf = df.replace(regex=r'^250.*', value='Diabetes')","028eb57f":"# Injury\ncodes =[str(i) for i in range(800,1000)]\ndf = df.replace(codes, 'Injury')","d85f0c86":"# Musculoskeletal\ncodes =[str(i) for i in range(710,740)]\ndf = df.replace(codes, 'Musculoskeletal')","f2d7fa7f":"# Genitourinary\ncodes =[str(i) for i in list(range(580,630)) + [788]]\ndf = df.replace(codes, 'Genitourinary')","357a63e8":"# Neoplasms\ncodes =[str(i) for i in range(140,240)]\ndf = df.replace(codes, 'Neoplasms')","4a96a274":"# Other\ndf = df.replace(regex=r'^[E,V].*', value='Other')\n\ncodes =[str(i) for i in range(0,1000)]\ndf = df.replace(codes, 'Other')","0572f6af":"df[['diag_1', 'diag_2', 'diag_3']].head()","71a4431f":"# Unique Values of Each Features:\nfor i in df[['diag_1', 'diag_2', 'diag_3']]:\n    print(f'{i}:\\n{sorted(df[i].unique())}\\n')","e322efb6":"# need to add 365.44 to Other\ndf = df.replace('365.44', 'Other') ","f6e1b16c":"plt.figure(figsize=(20, 8))\nfor diag in ['diag_1','diag_2','diag_3']:\n    sns.lineplot(x=df[diag].value_counts().sort_index().index, y= df[diag].value_counts().sort_index().values, marker='o')\nplt.legend(['diag_1','diag_2','diag_3'])\nplt.show()","3f5e72cd":"# drop diagnoses 2 and 3\ndf = df.drop(columns=['diag_2', 'diag_3'])","3011db9c":"plt.figure(figsize=(8,5))\nax = df.number_diagnoses.value_counts().sort_index().plot.bar()\ndef labels(ax, df=df):\n    for p in ax.patches:\n            ax.annotate('{:.0f}'.format(p.get_height()), \n                        (p.get_x(), p.get_height()+100),size=10)\nlabels(ax)","21a06049":"df.number_diagnoses = df.number_diagnoses.replace([10,11,12,13,14,15,16],9)","708ff75c":"df.describe().T","66bdbab4":"features = df.describe().columns","0b9b6655":"def col_plot(df,col_name):\n    plt.figure(figsize=(15,6))\n    \n    plt.subplot(141) # 1 satir x 4 sutun dan olusan ax in 1. sutununda calis\n    plt.hist(df[col_name], bins = 20)\n    f=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\n    \n    # \u00fc\u00e7 sigma aralikta(verinin %99.7 sini icine almasi beklenen bolum) iki kirmizi cizgi arasinda\n    plt.axvline(x=df[col_name].mean() + 3*df[col_name].std(),color='red')\n    plt.axvline(x=df[col_name].mean() - 3*df[col_name].std(),color='red')\n    plt.xlabel(col_name)\n    plt.tight_layout\n    plt.xlabel(\"Histogram \u00b13z\")\n    plt.ylabel(col_name)\n\n    plt.subplot(142)\n    plt.boxplot(df[col_name]) # IQR katsayisi, defaultu 1.5\n    plt.xlabel(\"IQR=1.5\")\n\n    plt.subplot(143)\n    plt.boxplot(df[col_name].apply(f), whis = 2.5)\n    plt.xlabel(\"ROOT SQUARE - IQR=2.5\")\n\n    plt.subplot(144)\n    plt.boxplot(np.log(df[col_name]+0.1), whis = 2.5)\n    plt.xlabel(\"LOGARITMIC - IQR=2.5\")\n    plt.show()","3c49ed0c":"for i in features:\n    col_plot(df,i)","a3f48f2b":"from scipy.stats.mstats import winsorize\n\ndef plot_winsorize(df,col_name,up=0.1,down=0):\n    plt.figure(figsize = (15, 6))\n\n    winsor=winsorize(df[col_name], (down,up))\n    logr=np.log(df[col_name]+0.1)\n\n    plt.subplot(141)\n    plt.hist(winsor, bins = 22)\n    plt.axvline(x=winsor.mean()+3*winsor.std(),color='red')\n    plt.axvline(x=winsor.mean()-3*winsor.std(),color='red')\n    plt.xlabel('Winsorize_Histogram')\n    plt.ylabel(col_name)\n    plt.tight_layout\n\n    plt.subplot(142)\n    plt.boxplot(winsor, whis = 1.5)\n    plt.xlabel('Winsorize - IQR:1.5')\n    \n    plt.subplot(143)\n    plt.hist(logr, bins=22)\n    plt.axvline(x=logr.mean()+3*logr.std(),color='red')\n    plt.axvline(x=logr.mean()-3*logr.std(),color='red')\n    plt.xlabel('Logr_col_name')\n\n    plt.subplot(144)\n    plt.boxplot(logr, whis = 1.5)\n    plt.xlabel(\"Logaritmic - IQR=1.5\")\n    plt.show()    \n","cb02691a":"for i in features:\n    plot_winsorize(df,i)","74c5bcc1":"df_winsorised=df.copy()\nfor i in features:\n    df_winsorised[i]=winsorize(df_winsorised[i], (0,0.1))","769e734c":"df_log=df.copy()\nfor i in features:\n    df_log[i]=np.log(df_log[i])","c8a02d83":"df_root=df.copy()\nf=lambda x:(np.sqrt(x) if x>=0 else -np.sqrt(-x))\nfor i in features:\n    df_root[i]=df_root[i].apply(f)","196af069":"from numpy import percentile\nfrom scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_zscore(df, col, min_z=1, max_z = 5, step = 0.1, print_list = False):\n    z_scores = zscore(df[col].dropna())\n    threshold_list = []\n    for threshold in np.arange(min_z, max_z, step):\n        threshold_list.append((threshold, len(np.where(z_scores > threshold)[0])))\n        df_outlier = pd.DataFrame(threshold_list, columns = ['threshold', 'outlier_count'])\n        df_outlier['pct'] = (df_outlier.outlier_count - df_outlier.outlier_count.shift(-1))\/df_outlier.outlier_count*100\n    plt.plot(df_outlier.threshold, df_outlier.outlier_count)\n    best_treshold = round(df_outlier.iloc[df_outlier.pct.argmax(), 0],2)\n    outlier_limit = int(df[col].dropna().mean() + (df[col].dropna().std()) * df_outlier.iloc[df_outlier.pct.argmax(), 0])\n    percentile_threshold = stats.percentileofscore(df[col].dropna(), outlier_limit)\n    plt.vlines(best_treshold, 0, df_outlier.outlier_count.max(), \n               colors=\"r\", ls = \":\"\n              )\n    plt.annotate(\"Zscore : {}\\nValue : {}\\nPercentile : {}\".format(best_treshold, outlier_limit, \n                                                                   (np.round(percentile_threshold, 3), \n                                                                    np.round(100-percentile_threshold, 3))), \n                 (best_treshold, df_outlier.outlier_count.max()\/2))\n    #plt.show()\n    if print_list:\n        print(df_outlier)\n    return (plt, df_outlier, best_treshold, outlier_limit, percentile)","03a39855":"from scipy.stats import zscore\nfrom scipy import stats\n\ndef outlier_inspect(df, col, min_z=1, max_z = 5, step = 0.5, max_hist = None, bins = 50):\n    fig = plt.figure(figsize=(20, 6))\n    fig.suptitle(col, fontsize=16)\n    plt.subplot(1,3,1)\n    if max_hist == None:\n        sns.distplot(df[col], kde=False, bins = 50)\n    else :\n        sns.distplot(df[df[col]<=max_hist][col], kde=False, bins = 50)\n   \n    plt.subplot(1,3,2)\n    sns.boxplot(df[col])\n    plt.subplot(1,3,3)\n    z_score_inspect = outlier_zscore(df, col, min_z=min_z, max_z = max_z, step = step)\n    \n    plt.subplot(1,3,1)\n    plt.axvline(x=df[col].mean() + z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.axvline(x=df[col].mean() - z_score_inspect[2]*df[col].std(),color='red',linewidth=1,linestyle =\"--\")\n    plt.show()\n    \n    return z_score_inspect","003d7bb2":"def detect_outliers(df:pd.DataFrame, col_name:str, p=1.5) ->int:\n    ''' \n    this function detects outliers based on 3 time IQR and\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(p*IQR)\n    lower_limit = first_quartile-(p*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","45baaee5":"k=3\nprint(f\"Number of Outliers for {k}*IQR\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df, col)[2] > 0:\n        outliers=detect_outliers(df, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","4971f80d":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Root Square\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_root, col)[2] > 0:\n        outliers=detect_outliers(df_root, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","d69502b4":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Winsorised\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_winsorised, col)[2] > 0:\n        outliers=detect_outliers(df_winsorised, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","72d9e4b6":"k=3\nprint(f\"Number of Outliers for {k}*IQR after Logarithmed\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_log, col)[2] > 0:\n        outliers=detect_outliers(df_log, col, k)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","b2533b6b":"z_scores=[]\nfor i in features:\n    z_scores.append(outlier_inspect(df,i)[2])","869530b4":"z_scores","1c6d6534":"features","d749710b":"# create columns for z scores, new column with z score\ndf_3z=df.copy()\n\nfor x in features:\n    df_3z[x + '_z'] = stats.zscore(df_3z[x])\n\nfor x in df_3z.columns[-len(features):]:\n    df_3z = df_3z[(df_3z[x] < 3) & (df_3z[x] > -3)]\n    \n# drop _z columns\ndf_3z = df_3z.drop(columns=df_3z.columns[-8:])\n\nprint('Number of Outliers:',len(df)-len(df_3z))","8816d749":"df_3z.describe().T.round(2)","7edb91a4":"df.describe().T.round(2)","fa233e6b":"summary(df_3z)","0ecd39ca":"df_3z = df_3z.drop(['acetohexamide','glimepiride-pioglitazone','metformin-rosiglitazone'],axis=1)","e6f6d190":"df_3z = df_3z.reset_index(drop=True)\ndf_3z.to_csv('diabetic_data_cleaned.csv')","4cb8443a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,6\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\nsns.set(style='darkgrid')\n%matplotlib inline","566c2f99":"data = pd.read_csv('diabetic_data_cleaned.csv', index_col=0)\ndf = data.copy()\ndf.head()","7145f90c":"features = pd.read_csv('features.csv',index_col='Unnamed: 0')\ninfo = lambda attribute:print(f\"{attribute.upper()} : {features[features['Feature']==attribute]['Description'].values[0]}\\n\")\nfeatures.head()","9d78e403":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str\n\nsummary(df)","4834d9d2":"round(df.describe(), 2)","54c856c3":"df.shape","2e703291":"sns.pairplot(df, hue='readmitted');","3fff0abb":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\");","462e323b":"info('readmitted')","31df80a0":"def labels(ax):\n    for p in ax.patches:\n            ax.annotate('%{:.1f}\\n{:.0f}'.format(100*p.get_height()\/len(df),p.get_height()), \n                        (p.get_x()+0.3, p.get_height()-1900),size=11)\n\nax = sns.countplot(x='readmitted', palette='husl', data=df)\nlabels(ax)\n\n# sns.catplot(x='readmitted', kind='count', palette='husl', data=df)  # alternative\nplt.title('Readmit Rates')\nplt.show()","9b5cf778":"def labels(ax):\n    for bar in ax.patches: \n        ax.annotate('%{:.1f}\\n{:.0f}'.format(100*bar.get_height()\/len(df),bar.get_height()), (bar.get_x() + bar.get_width() \/ 2,  \n                        bar.get_height()), ha='center', va='center', \n                       size=10, xytext=(0, 8), \n                       textcoords='offset points') \n\nrcParams['figure.figsize'] = 12,6\nax = sns.countplot(x='race', hue='readmitted', palette='husl', data=df)\nlabels(ax)\n# sns.catplot(x='race', hue='readmitted', kind='count', palette='husl', data=df, aspect=2, legend_out=False)\nplt.title('Patient Demographic Readmissions')\nplt.show()","189ee8ae":"pd.crosstab(df.race, df.readmitted, margins=True, margins_name='Total')","5457edb5":"rcParams['figure.figsize'] = 12,6\nax = sns.countplot(x='gender', hue='readmitted', palette='husl', data=df)\nlabels(ax)\nplt.title('Readmissions by Gender')\nplt.show()","a128136b":"pd.crosstab(df.gender, df.readmitted, margins=True, margins_name='Total')","c2f021d2":"ax = sns.countplot(x='age', palette='husl', data=df.sort_values('age'))\nlabels(ax)\nplt.title('Patient Demographics')\nplt.show()","378667e6":"ax = sns.countplot(x='age', hue='readmitted', palette='husl', data=df.sort_values('age'))\nlabels(ax)\nplt.title('Readmits By Age Group')\nplt.show()","35fed3a6":"pd.crosstab(df.age, df.readmitted, margins=True, margins_name='Total').T","1cf81b4e":"sns.countplot(x='time_in_hospital', palette='muted', data=df)\nmean, median = np.mean(df.time_in_hospital), np.median(df.time_in_hospital)\nplt.axvline(mean-df.time_in_hospital.min(), color='blue', label=f'mean:{round(mean,2)}')\nplt.axvline(median-df.time_in_hospital.min(), color='red', label=f'median:{round(median,2)}')\nplt.title('Duration of Hospital Visit in Days')\nplt.legend()\nplt.show()","da918793":"sns.catplot(x='time_in_hospital', hue='readmitted', kind='count', palette='husl', aspect=3, data=df, legend_out=False)\nplt.title('Readmission Based on Time in Hospital')\nplt.show()","416a268d":"sns.displot(x='time_in_hospital', hue='readmitted', data=df, height=7, aspect=3)\nplt.title('Readmission Based on Time in Hospital')\nplt.show()","5eaf9e72":"def box_labels(ax, df,col1,col2):\n    medians = df.groupby([col1])[col2].median()\n    vertical_offset = df[col2].median() * 0.05 # offset from median for display\n\n    for xtick in ax.get_xticks():\n        ax.text(xtick,medians[xtick] + vertical_offset,medians[xtick], \n                horizontalalignment='center',size='x-small',color='w',weight='semibold')\n\nax = sns.boxplot(x='age', y='time_in_hospital', data=df.sort_values('age'))\nbox_labels(ax, df.sort_values('age'),'age','time_in_hospital')    \nplt.title('Length of Hospital Stay Based on Age')\nplt.show()","4fb1d13e":"ax = sns.boxplot(x='readmitted', y='time_in_hospital', data=df.sort_values('readmitted'))\nbox_labels(ax, df.sort_values('readmitted'),'readmitted','time_in_hospital') \nplt.title('Length of Hospital Stay for Readmitted Patients')\nplt.show()","b167e5a1":"info(\"num_lab_procedures\")","0180f81a":"rcParams['figure.figsize'] = 25,10\nsns.countplot(x='num_lab_procedures', data=df)\nmean, median = np.mean(df.num_lab_procedures), np.median(df.num_lab_procedures)\nplt.axvline(mean-df.num_lab_procedures.min(), color='blue', label=f'mean:{round(mean,2)}')\nplt.axvline(median-df.num_lab_procedures.min(), color='black', label=f'median:{round(median,2)}')\nplt.title('Number of Lab Procedures Performed During Visit')\nplt.legend()\nplt.show()","21b049a8":"df.groupby('readmitted')['num_lab_procedures'].describe().round(2)","3e29d39a":"def box_labels(ax, df,col1,col2):\n    medians = df.groupby([col1])[col2].median()\n    vertical_offset = df[col2].median() * 0.05 # offset from median for display\n\n    for xtick in ax.get_xticks():\n        ax.text(xtick,medians[xtick] + vertical_offset,medians[xtick], \n                horizontalalignment='center',size=12,color='w',weight='semibold')\n\nax = sns.boxplot(x='time_in_hospital', y='num_lab_procedures', data=df.sort_values('time_in_hospital'))\n# box_labels(ax, df.sort_values('time_in_hospital'),'time_in_hospital','num_lab_procedures') \nplt.title('Lab Procedures Based on Length of Hospital Visit')\nplt.show()","a7b4218b":"plt.figure(figsize=(10, 8))\nax = sns.boxplot(x='readmitted', y='num_lab_procedures', data=df.sort_values('readmitted'))\nbox_labels(ax, df.sort_values('readmitted'),'readmitted','num_lab_procedures') \nplt.title('Lab Procedures for Readmitted Patients')\nplt.show()","19992568":"info('num_procedures')","c8f2b983":"sns.catplot(x='num_procedures', kind='count', palette='muted', data=df)\nmean, median = np.mean(df.num_procedures), np.median(df.num_procedures)\nplt.axvline(mean, color='blue', label=f'mean:{round(mean,2)}')\nplt.axvline(median, color='black', label=f'median:{round(median,2)}')\nplt.title('Number of Procedures Performed (Except Lab)')\nplt.legend()\nplt.show()","2726cf7a":"def labels(ax):\n    for bar in ax.patches: \n        ax.annotate('%{:.1f}\\n{:.0f}'.format(100*bar.get_height()\/len(df),bar.get_height()), (bar.get_x() + bar.get_width() \/ 2,  \n                        bar.get_height()-400), ha='center', va='center', \n                       size=14, xytext=(0, 8), \n                       textcoords='offset points') \n        \nax = sns.countplot(x='num_procedures', hue='readmitted', palette='husl', data=df)\nlabels(ax)\nplt.title('Readmits Based on Procedures (Sans Lab)')\nplt.show()","f3a0bf40":"info('num_medications')","6dbb6bbe":"rcParams['figure.figsize'] = 25,10\nsns.countplot(x='num_medications', data=df)\nmean, median = np.mean(df.num_medications), np.median(df.num_medications)\nplt.axvline(mean-df.num_medications.min(), color='blue', label=f'mean:{round(mean,2)}')\nplt.axvline(median-df.num_medications.min(), color='black', label=f'median:{round(median,2)}')\nplt.title('Number of Distinct Generic Medications Administered During Visit')\nplt.legend()\nplt.show()","144256ba":"df.groupby('readmitted')['num_medications'].describe()","4df0742c":"ax = sns.boxplot(x='time_in_hospital', y='num_medications', data=df)\n# box_labels(ax, df.sort_values('time_in_hospital'),'time_in_hospital','num_medications')\nplt.title('Medications Administered Based on Length of Hospital Visit')\nplt.show()","ac8beccc":"ax = sns.boxplot(x='readmitted', y='num_medications', data=df.sort_values('readmitted'))\nbox_labels(ax, df.sort_values('readmitted'),'readmitted','num_medications')\nplt.title('Medications Administered')\nplt.show()","22efd3c1":"info('number_outpatient')","6335c5f9":"def labels(ax):\n    for bar in ax.patches: \n        ax.annotate('%{:.1f}\\n{:.0f}'.format(100*bar.get_height()\/len(df),bar.get_height()), (bar.get_x() + bar.get_width() \/ 2,  \n                        bar.get_height()+750), ha='center', va='center', \n                       size=16, xytext=(0, 8), \n                       textcoords='offset points') \n        \nax = sns.countplot(x='number_outpatient',data=df)\nlabels(ax)\nplt.title('Number of Outpatient Visits Prior to Encounter')\nplt.show()","b8611c0f":"# outpatient visit stats\ndf.groupby('readmitted')['number_outpatient'].describe()","f7a0772f":"# outpatient vists and readmissions\nax = sns.countplot(x='number_outpatient',data=df, hue='readmitted')\nlabels(ax)\nplt.title('Outpatient Vists and Readmissions')\nplt.show()","c73279a2":"pd.crosstab(df.readmitted, df.number_outpatient, margins=True, margins_name='Total')","997e98dd":"info('number_emergency')","cca69181":"# plt.figure(figsize=(20,5))\nax = sns.countplot(x='number_emergency', data=df)\nlabels(ax)\nplt.title('Number of Emergency Visits Prior to Encounter')\nplt.show()","fce469f6":"# emergency vists and readmissions\nax = sns.countplot(x='number_emergency', hue='readmitted', data=df)\nlabels(ax)\nplt.title('Emergency Vists and Readmissions')\nplt.show()","d5e504c3":"pd.crosstab(df.readmitted, df.number_emergency, margins=True, margins_name='Total')","2e2d486d":"plt.figure(figsize=(5, 5))\nsns.boxplot(x='readmitted', y='number_emergency', data=df)\nplt.title('Readmits for Emergency Vists')\nplt.show()","61dd35df":"info('number_inpatient') # onceki yildaki yatarak tedavi sayisi","deef25b6":"ax = sns.countplot(x='number_inpatient',data=df)\nlabels(ax)\nplt.title('Number of Inpatient Visits Prior to Encounter')\nplt.show()","c8658c31":"# inpatient visits and readmissions\nax = sns.countplot(x='number_inpatient', hue='readmitted',data=df)\nlabels(ax)\nplt.title('Inpatient Visits and Readmissions')\nplt.show()","c69d481f":"pd.crosstab(df.readmitted, df.number_inpatient, margins=True, margins_name='Total')","f20d7313":"info('number_diagnoses')","7a10834a":"ax = sns.countplot(x='number_diagnoses',data=df)\nmean, median = np.mean(df.number_diagnoses), np.median(df.number_diagnoses)\nplt.axvline(mean-df.number_diagnoses.min(), color='blue', label=f'mean:{round(mean,2)}')\nplt.axvline(median-df.number_diagnoses.min(), color='red', label=f'median:{round(median,2)}')\nplt.title('Number of Diagnoses')\nplt.legend()\nplt.show()","06d9e529":"# number of diagnoses and readmit rate\nax = sns.countplot(x='number_diagnoses', hue='readmitted', palette='Accent', data=df)\n# labels(ax)\nplt.title('Readmits By Number of Diagnoses')\nplt.show()","b37de71e":"pd.DataFrame(df.number_diagnoses.describe()).T.round(2)","15b8ca48":"df.groupby('readmitted')['number_diagnoses'].describe().round(2)","9d83529c":"# number of diagnoses\npd.crosstab(df.readmitted, df.number_diagnoses, margins=True, margins_name='Total')","f69534bd":"plt.figure(figsize=(8, 6))\nax = sns.boxplot(x='readmitted', y='number_diagnoses', data=df.sort_values('readmitted'))\nbox_labels(ax, df.sort_values('readmitted'),'readmitted','number_diagnoses')\nplt.title('Number of Diagnoses for Re\/admitted Patients')\nplt.show()","17f6253d":"info('max_glu_serum')","cda344c6":"ax = sns.countplot(x='max_glu_serum', data=df)\nlabels(ax)\nplt.title('Glucose Serum Test Results')\nplt.show()","6b68cfed":"def labels(ax, df=df):\n    for p in ax.patches:\n            ax.annotate('%{:.1f}\\n{:.0f}'.format(100*p.get_height()\/len(df),p.get_height()), \n                        (p.get_x()+0.2, p.get_height()-27),size=16)\n\n# exclude patients without a glucose reading\nglucose_none = df[df.max_glu_serum != 'None']\n\n# glucose serum results and readmit impact\nax = sns.countplot(x='max_glu_serum', hue='readmitted', palette='Accent', data=glucose_none)\nlabels(ax,glucose_none)\nplt.title('Readmits By Glucose Serum Levels')\nplt.show()","43b4dd8d":"\npd.crosstab(df.readmitted, df.max_glu_serum, margins=True, margins_name='Total')","97645f19":"info('A1Cresult')","2fcc620b":"ax = sns.countplot(x='A1Cresult', palette='husl', data=df)\nlabels(ax)\nplt.title('A1c Test Results')\nplt.show()","4dfcd0ef":"# exclude patients without an A1C reading\nalc_none = df[df.A1Cresult != 'None']\n\n# A1C results and readmit impact\nax = sns.countplot(x='A1Cresult', hue='readmitted', palette='Accent', data=alc_none)\nlabels(ax, alc_none)\nplt.title('Readmits By A1C Test Results')\nplt.show()","ff51297c":"pd.crosstab(df.readmitted, df.A1Cresult, margins=True, margins_name='Total')","fa1e8ec0":"info('change')","ac18c9c8":"# change in medications\nax = sns.countplot(x='change', hue='readmitted', data=df)\nlabels(ax)\nplt.title('Change in Diabetic Medications')\nplt.show()","85b1acdc":"pd.crosstab(df.change, df.readmitted, margins=True, margins_name='Total')","b56da6e2":"ax = sns.countplot(x='gender', hue='change', palette='Set2', data=df)\nlabels(ax)\nplt.title('Change in Medication Based on Gender')\nplt.show()","282ba6b6":"pd.crosstab(df.gender, df.change, margins=True, margins_name='Total')","a5f6dead":"info('diabetesMed')","61da49d2":"ax = sns.countplot(x='diabetesMed', hue='readmitted', data=df)\nlabels(ax)\nplt.title('Prescribed Diabetic Medications During Visit')\nplt.show()","b9374a31":"pd.crosstab(df.diabetesMed, df.readmitted, margins=True, margins_name='Total')","77dcedb1":"sns.catplot(x='diabetesMed', hue='readmitted', col='gender', palette='Accent', data=df, kind='count', height=4, aspect=1)\nplt.show()","f0decf44":"columns=['metformin', 'repaglinide', 'nateglinide',\n       'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide',\n       'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol',\n       'troglitazone', 'tolazamide', 'insulin', 'glyburide-metformin',\n       'glipizide-metformin', 'metformin-pioglitazone']\n\nplt.figure(figsize=(26, 26))\nfor i,col in enumerate(columns):\n    plt.subplot(6,3,i+1)\n    sns.countplot(x=df[col])","af3c4d0c":"info('insulin')","8dd3ec10":"sns.countplot(x='insulin', hue='readmitted', data=df)\nplt.title('Readmit Rates by Medication: Insulin')\nplt.show()","34ffb8ba":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,6\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")\n\nsns.set(style='darkgrid')\n%matplotlib inline\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy.stats import chi2_contingency\nfrom numpy.random import seed\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler","0b68c1d3":"data = pd.read_csv('diabetic_data_cleaned.csv', index_col=0) # import data\ndf = data.copy() # save a copy of data as diabetes","a879b15c":"features = pd.read_csv('features.csv',index_col='Unnamed: 0')\ninfo = lambda attribute:print(f\"{attribute.upper()} : {features[features['Feature']==attribute]['Description'].values[0]}\\n\")","8c4e874e":"def summary(df, pred=None):\n    obs = df.shape[0]\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Min = df.min()\n    Max = df.max()\n    Uniques = df.apply(lambda x: x.unique().shape[0])\n    Nulls = df.apply(lambda x: x.isnull().sum())\n    print('Data shape:', df.shape)\n\n    if pred is None:\n        cols = ['Types', 'Counts', 'Uniques', 'Nulls', 'Min', 'Max']\n        str = pd.concat([Types, Counts, Uniques, Nulls, Min, Max], axis = 1, sort=True)\n\n    str.columns = cols\n    print('___________________________\\nData Types:')\n    print(str.Types.value_counts())\n    print('___________________________')\n    return str\n\nsummary(df)","fdcf3e6e":"df.describe().round(2).T","6f69f007":"plt.figure(figsize=(6,6))\n\nexplode = [0,0.1]\nplt.pie(df['readmitted'].value_counts(),explode=explode,autopct='%1.1f%%',shadow=True,startangle=60)\nplt.legend(labels=df.readmitted.value_counts().index)\nplt.title('Readmitted Patients')\nplt.axis('off')\nplt.show()","641e4465":"print('Unique Values of Each Features:\\n')\nfor i in df:\n    print(f'{i}:\\n{sorted(df[i].unique())}\\n')","c4f37294":"categorical=df.select_dtypes(include='object').columns.tolist()\nprint(categorical)","603853ac":"# define a function that returns a table, a chi-square value, and a p value\ndef chisquare_test(df, var_list, target, null_list=[]):\n    for var in var_list:\n        print(var.upper())\n        chi_test = pd.crosstab(df[var], df[target])\n        display(chi_test)\n    \n        chisq_value, pvalue, dataframe, expected = chi2_contingency(chi_test)\n    \n        print(f\"\"\"Chi-square value: {chisq_value:.2f}\np-value\\t\\t: {pvalue:.3f}\\n\"\"\")\n        \n        if pvalue > 0.01: # adds variables that fail to reject the null hypothesis\n            null_list.append(var)\n            \n    print(f'Fail to reject null hypothesis: {null_list}')","a5eaf69b":"cols_cat = ['race','gender', 'age', 'diag_1', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed']\nnull_list=[]\nchisquare_test(df, cols_cat,'readmitted',null_list)","12d41809":"medications = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n            'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n               'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n               'insulin', 'glyburide-metformin', 'glipizide-metformin', 'metformin-pioglitazone']\nchisquare_test(df, medications,'readmitted', null_list)","aa4e3092":"print(null_list)","222ca3b7":"# drop columns that do not pass the p-value test\ndf = df.drop(columns=null_list)","7bacd20d":"# The numerical variables \nnumerical=df.select_dtypes(include=['int64','float']).columns.tolist()\nprint(numerical)","ace8272c":"df.describe().T.round(2)","f6c12fa6":"# define a function that performs the ANOVA test and returns a table\ndef anova_table(var_list, null_list=[]):\n    for var in var_list:\n        print(var.upper())\n        \n        anova = ols('time_in_hospital ~ {}'.format(var), data=df).fit()\n        table = sm.stats.anova_lm(anova, typ=2)\n        pvalue=table['PR(>F)'][0]\n        if pvalue > 0.01: # adds variables that fail to reject the null hypothesis\n            null_list.append(var)\n        display(table)\n    print(f'Fail to reject null hypothesis: {null_list}')","93c6dab9":"anova_vars = ['readmitted']+numerical\nanova_table(anova_vars)","b9302d8f":"# drop number_emergency column\ndf = df.drop(columns=['number_emergency'])","9f704926":"# Unique Values of Each Features\nfor i in df:\n    print(f'{i}:\\n{sorted(df[i].unique())}\\n')","8f2b057b":"df_dummy = pd.get_dummies(df,drop_first=True)\ndf_dummy.head()","e55e5e2b":"plt.figure(figsize=(20,5))\ndf_dummy.corr()[\"readmitted_YES\"].sort_values()[:-1].plot.bar();","ba94f3f5":"plt.figure(figsize=(20,20))\nsns.heatmap(df_dummy.corr(), cmap=\"coolwarm\");","707e3648":"def corrank(X, threshold=0):\n    import itertools\n    df = pd.DataFrame([[i,j,X.corr().abs().loc[i,j]] for i,j in list(itertools.combinations(X.corr().abs(), 2))],columns=['Feature1','Feature2','corr'])    \n    df = df.sort_values(by='corr',ascending=False).reset_index(drop=True)\n    return df[df['corr']>threshold]\n\n# prints a descending list of correlation pair (Max on top)\ncorrank(df_dummy, 0.7)","0cb00e04":"# Remove the highly collinear features from data\ndef remove_collinear_features(x, threshold):\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n\n    return x","1621adb3":"#Remove columns having more than 70% correlation\n#Both positive and negative correlations are considered here\ndf_dummy = remove_collinear_features(df_dummy,0.70)","cd440b4b":"df_dummy.shape","d8b50553":"# save dataset to new file for machine learning\ndf_dummy.to_csv('diabetic_data_cleaned_dummy.csv')","8f7a5807":"# for basic operations\nimport numpy as np \nimport pandas as pd \n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\n# rcParams['figure.figsize'] = 4,4\n# plt.style.use('fivethirtyeight')\n\nfrom collections import Counter\n\n# for modeling \nimport sklearn\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_precision_recall_curve, precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.warn(\"this will not show\")","6f4adc4d":"data = pd.read_csv('diabetic_data_cleaned_dummy.csv', index_col=0)\ndf = data.copy()\n\ndf.head()","e6bcd434":"from lazypredict.Supervised import LazyClassifier\n\ndf_5000 = df.sample(5000,random_state=42)\ny = df_5000['readmitted_YES']\nX = df_5000.drop('readmitted_YES', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state =42)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(X_train, X_test, y_train, y_test)\nmodels","14d6641b":"ax = df['readmitted_YES'].value_counts(normalize=True).plot.bar()\ndef labels(ax):\n    for p in ax.patches:\n        ax.annotate(f\"%{p.get_height()*100:.2f}\", (p.get_x() + 0.15, p.get_height() * 1.005),size=11)\nlabels(ax)","734f3e4f":"# separating the dependent and independent data\nX = df.drop('readmitted_YES', axis=1)\ny = df['readmitted_YES']\n\n# the function train_test_split creates random data samples (default: 75-25%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# getting the shapes\nprint(f\"\"\"shape of X_train: {X_train.shape}\nshape of X_test\\t: {X_test.shape}\nshape of y_train: {y_train.shape}\nshape of y_test\\t: {y_test.shape}\"\"\")","bb855e72":"# creating a standard scaler\nsc = StandardScaler()\n\n# fitting independent data to the model\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","de1e8ff1":"cv_acc_train = {}\ncv_acc_test = {}\ncv_TPR = {}\ncv_FPR = {}\ncv_AUC = {}","433ad62d":"def plot_result(model, name:str):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')  \n    cv_acc_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (Max)\n    cv_FPR[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (Min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","6a02297b":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix","3e48373b":"dtc = DecisionTreeClassifier()\nplot_result(dtc, \"dtc\")","218a2fe9":"# plot tree\n# plt.figure(figsize=(16,6))\n# plot_tree(dtc, filled = True, class_names=[\"-1\", \"1\"], feature_names=X.columns, fontsize=11);","74880672":"cv_acc_train, cv_acc_test, cv_TPR, cv_FPR","21504320":"from sklearn.linear_model import LogisticRegression","dd70faab":"lr = LogisticRegression()\nplot_result(lr, \"lr\")","33f9b4e3":"# svc = SVC(probability=True)  # default values\n# plot_result(svc, \"svc\")","751f3cc7":"from sklearn.neighbors import NearestCentroid\nfrom sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix","e691d5a6":"nc = NearestCentroid()\nplot_result(nc, \"nc\")","afdcd36d":"from sklearn.ensemble import RandomForestClassifier","9ee6aa8e":"rfc = RandomForestClassifier()\nplot_result(rfc, \"rfc\")","19c5c76d":"def plot_feature_importances(model):\n    feature_imp = pd.Series(model.feature_importances_,index=X.columns).sort_values(ascending=False)[:10]\n\n    sns.barplot(x=feature_imp, y=feature_imp.index)\n    plt.title(\"Feature Importance\")\n    plt.show()\n\n    print(f\"Top 10 Feature Importance for {str(model).split('(')[0]}\\n\\n\",feature_imp[:10],sep='')","69559349":"plot_feature_importances(rfc)","808c3a85":"from sklearn.ensemble import GradientBoostingClassifier","e9007d77":"gbc = GradientBoostingClassifier(random_state=42)\nplot_result(gbc, \"gbc\")","72d260c0":"plot_feature_importances(gbc)","51697eab":"from sklearn.naive_bayes import GaussianNB","f29e6980":"nb = GaussianNB()\nplot_result(nb, \"nb\")","244bddd2":"from sklearn.neighbors import KNeighborsClassifier","e2043b67":"knn = KNeighborsClassifier()\nplot_result(knn, \"knn\")","a607f861":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score","1a756654":"xgb = XGBClassifier(eval_metric = \"logloss\")\nplot_result(xgb, \"xgb\")","3770e301":"plot_feature_importances(xgb)","cf4d5ec1":"from xgboost import plot_importance\nplot_importance(xgb,max_num_features=10)\nplt.xlabel('The F-Score for each features')\nplt.ylabel('Importances')\nplt.show()","38036f20":"\ndef AUC(cv_AUC, X_test=X_test):\n    dtc_auc= roc_auc_score(y_test,dtc.predict(X_test)) #Decision Tree Classifier\n    lr_auc= roc_auc_score(y_test, lr.decision_function(X_test))#logistic regression\n#     svc_auc= roc_auc_score(y_test, svc.decision_function(X_test))#Support Vector Classifier\n    nc_auc= roc_auc_score(y_test, nc.predict(X_test))#Nearest Centroid Classifier\n    rfc_auc= roc_auc_score(y_test, rfc.predict_proba(X_test)[:,1])#Randomforest Classifier\n    gbc_auc= roc_auc_score(y_test, gbc.predict_proba(X_test)[:,1])#GradientBoosting Classifier\n    nb_auc= roc_auc_score(y_test, nb.predict_proba(X_test)[:,1])#Naive Bayes Classifier\n    knn_auc= roc_auc_score(y_test, knn.predict(X_test))#KNeighbors Classifier\n    xgb_auc= roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1])#XGBoost Classifier\n\n    cv_AUC={'dtc': dtc_auc,\n           'lr': lr_auc,\n#            'svc':svc_auc,\n           'nc':nc_auc,\n           'rfc':rfc_auc,\n           'gbc':gbc_auc,\n           'nb':nb_auc,\n           'knn':knn_auc,\n           'xgb':xgb_auc}\n    return cv_AUC","0a747395":"cv_AUC = AUC(cv_AUC)\ndf_eval = pd.DataFrame(data={'model': list(cv_acc_test.keys()), \n                             'bal_acc_train':list(cv_acc_train.values()),\n                             'bal_acc_test': list(cv_acc_test.values()), \n                             'recall': list(cv_TPR.values()), \n                             'fallout':list(cv_FPR.values()),\n                              'AUC': list(cv_AUC.values())}).round(2)\ndf_eval","4c683459":"def plot_ROC(X_test=X_test, y_test=y_test):\n    fpr_dtc, tpr_dtc, thresholds = roc_curve(y_test,dtc.predict(X_test)) #Decision Tree Classifier\n    fpr_lr, tpr_lr, thresholds = roc_curve(y_test, lr.decision_function(X_test))#logistic regression\n#     fpr_svc, tpr_svc, thresholds = roc_curve(y_test, svc.decision_function(X_test))#Support Vector Classifier\n    fpr_nc, tpr_nc, thresholds = roc_curve(y_test, nc.predict(X_test))#Nearest Centroid Classifier\n    fpr_rfc, tpr_rfc, thresholds = roc_curve(y_test, rfc.predict_proba(X_test)[:,1])#Randomforest Classifier\n    fpr_gbc, tpr_gbc, thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:,1])#GradientBoosting Classifier\n    fpr_nb, tpr_nb, thresholds = roc_curve(y_test, nb.predict_proba(X_test)[:,1])#Naive Bayes Classifier\n    fpr_knn, tpr_knn, thresholds = roc_curve(y_test, knn.predict(X_test))#KNeighbors Classifier\n    fpr_xgb, tpr_xgb, thresholds = roc_curve(y_test, xgb.predict_proba(X_test)[:,1])#XGBoost Classifier\n\n    #compare the ROC curve between different models\n    plt.figure(figsize=(10,10))\n    plt.plot(fpr_dtc, tpr_dtc, label='Decision Tree Classifier')\n    plt.plot(fpr_lr, tpr_lr, label='Logistic Regression')\n#     plt.plot(fpr_svc, tpr_svc, label='Support Vector Classifier')\n    plt.plot(fpr_nc, tpr_nc, label='Nearest Centroid Classifier')\n    plt.plot(fpr_rfc, tpr_rfc, label='Randomforest Classifier')\n    plt.plot(fpr_gbc, tpr_gbc, label='GradientBoosting Classifier')\n    plt.plot(fpr_nb, tpr_nb, label='Naive Bayes Classifier')\n    plt.plot(fpr_knn, tpr_knn, label='KNeighbors Classifier')\n    plt.plot(fpr_xgb, tpr_xgb, label='XGBoost Classifier')\n\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='random', alpha=.8)\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.xticks(np.arange(0,1.1,0.1))\n    plt.yticks(np.arange(0,1.1,0.1))\n    plt.grid()\n    plt.legend()\n    plt.axes().set_aspect('equal')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\nplot_ROC()","e4db2ea7":"fig, ax = plt.subplots(1,4, figsize=(20, 4))\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0])\nax[0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[1])\nax[1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[2])\nax[2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[3])\nax[3].set_title(\"Unbalanced Test FPR\")\nplt.show()","953a9bd9":"y_test.value_counts(normalize=True)","13dc6ed2":"y_train.value_counts(normalize=True)","1302ec2c":"# pip install imblearn\nfrom imblearn import under_sampling, over_sampling\nfrom imblearn.over_sampling import SMOTE","ce547d53":"oversmote = SMOTE()\nX_train_os, y_train_os= oversmote.fit_resample(X_train, y_train)","007a045e":"ax = y_train_os.value_counts().plot.bar(color=[\"blue\", \"red\"])\ndef labels(ax):\n    for p in ax.patches:\n        ax.annotate(f\"{p.get_height()}\", (p.get_x() + 0.15, p.get_height()+200),size=8)\nlabels(ax)\nplt.show()","c61a99ce":"X_train_os.shape","bd314ddf":"cv_acc_balance_train = {}\ncv_acc_balance_test = {}\ncv_TPR_balance = {}\ncv_FPR_balance = {}\ncv_AUC_balance = {}","a2c08360":"def plot_result_smote(model, name:str):\n    model.fit(X_train_os, y_train_os)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_balance_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","75305299":"# Decision tree\ndtc = DecisionTreeClassifier()\n\nplot_result_smote(dtc, \"dtc\")","c0a82015":"# Logistic Regression\nlr = LogisticRegression()\nplot_result_smote(lr, \"lr\")","2bb4d6f9":"# NearestCentroid\nnc = NearestCentroid()\nplot_result_smote(nc, \"nc\")","25dcaa65":"# # SVC\n# svc = SVC()\n# plot_result_smote(svc, \"svc\")","e823672d":"# Random Forest\nrfc = RandomForestClassifier()\nplot_result_smote(rfc, \"rfc\")","1600789f":"# Gradient Boost\ngbc = GradientBoostingClassifier(random_state=42)\nplot_result_smote(gbc, \"gbc\")","b03b30c4":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smote(nb, \"nb\")","32d2e8ac":"# kNN\nknn = KNeighborsClassifier()\nplot_result_smote(knn, \"knn\")","4d9d2cd5":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\", random_state=42)\nplot_result_smote(xgb, \"xgb\")","970e62d6":"cv_AUC_balance = AUC(cv_AUC_balance)","5b3615f8":"df_eval_smote = pd.DataFrame(data={'model': list(cv_acc_balance_test.keys()), \n                                   'bal_acc_train':list(cv_acc_balance_train.values()),\n                                   'bal_acc_test': list(cv_acc_balance_test.values()),\n                                   'recall': list(cv_TPR_balance.values()), \n                                   'fallout':list(cv_FPR_balance.values()),\n                                   'AUC': list(cv_AUC_balance.values())}).round(2)\ndf_eval_smote","c4ee216c":"fig, ax = plt.subplots(2,4, figsize=(20, 8))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","2f2be450":"plot_ROC()","3571b044":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, NearMiss","52f510c4":"under_sampler = RandomUnderSampler(random_state=42)\nX_train_rus, y_train_rus = under_sampler.fit_sample(X_train, y_train)","7de5246d":"ax = y_train_rus.value_counts().plot.bar(color=[\"blue\", \"red\"])\nlabels(ax)\nplt.show()","b64094ff":"cv_acc_rus_train = {}\ncv_acc_rus_test = {}\ncv_TPR_rus = {}\ncv_FPR_rus = {}\ncv_AUC_rus = {}","46c7cd2d":"def plot_result_rus(model, name:str):\n    model.fit(X_train_rus, y_train_rus)\n    y_pred = model.predict(X_test)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train, y_train, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_rus_train[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_rus_test[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_rus[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_rus[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n    \n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot Confusion Matrix\n    plot_confusion_matrix(model, X_test, y_test)\n    plt.show()","507341a2":"# Decision tree\ndtc = DecisionTreeClassifier()\n\nplot_result_rus(dtc, \"dtc\")","278d0c99":"# Logistic Regression\nlr = LogisticRegression()\nplot_result_rus(lr, \"lr\")","823f2ecf":"# NearestCentroid\nnc = NearestCentroid()\nplot_result_rus(nc, \"nc\")","e7f9fa1b":"# # SVC\n# svc = SVC()\n# plot_result_rus(svc, \"svc\")","f51cb250":"# Random Forest\nrfc = RandomForestClassifier()\nplot_result_rus(rfc, \"rfc\")","ecb7c904":"# Gradient Boost\ngbc = GradientBoostingClassifier(random_state=42)\nplot_result_rus(gbc, \"gbc\")","80d8ffbe":"# Naive Bayes\nnb = GaussianNB()\nplot_result_rus(nb, \"nb\")","85b6c1dd":"# kNN\nknn = KNeighborsClassifier()\nplot_result_rus(knn, \"knn\")","14491d96":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\",random_state=42)\nplot_result_rus(xgb, \"xgb\");","9df924d2":"cv_AUC_rus = AUC(cv_AUC_rus)","2bc9a98e":"df_eval_rus = pd.DataFrame(data={'model': list(cv_acc_rus_train.keys()), \n                             'bal_acc_train':list(cv_acc_rus_train.values()),\n                             'bal_acc_test': list(cv_acc_rus_test.values()), \n                             'recall': list(cv_TPR_rus.values()), \n                             'fallout':list(cv_FPR_rus.values()),\n                             'AUC': list(cv_AUC_rus.values())}).round(2)    \ndf_eval_rus","f9d7329a":"fig, ax = plt.subplots(3,4, figsize=(20, 12))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"RUS_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,3])\nax[2,3].set_title(\"RUS_Featured Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","6d60df68":"plot_ROC()","e37ac4b5":"from sklearn.decomposition import PCA","fa2024e6":"pca = PCA().fit(X_train_os)","c66706e9":"fig, ax = plt.subplots(figsize=(20,8))\nxi = np.arange(0, 54, step=1)\ny = np.cumsum(pca.explained_variance_ratio_[0:160:1])\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='.', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 54, step=2), rotation=90) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","028325dc":"pca = PCA(n_components=43)\npca.fit(X_train_os)\nper_var = np.round(pca.explained_variance_ratio_ * 100, 1)\nlabels = ['PC' + str(x) for x in range(1,len(per_var)+1)]\n\nplt.figure(figsize=(20,6))\nplt.bar(x=range(len(per_var)), height=per_var, tick_label=labels)\nplt.title('Total explained variance {}'.format(np.round(sum(per_var),2)))\nplt.ylabel('Explained variance in percent')\nplt.xticks(rotation=90)\nplt.show()","ed1b1f71":"X_train_os_pca = pca.transform(X_train_os)\npd.DataFrame(X_train_os_pca)","d46a9014":"# Top 20 columns that have the greatest impact\nloading_scores = pd.Series(pca.components_[0], index=X.columns)\nloading_scores.abs().sort_values(ascending=False)[:20]","194a7c41":"X_test_pca = pca.transform(X_test)","fb0f22c8":"cv_acc_balance_train_pca = {}\ncv_acc_balance_test_pca = {}\ncv_TPR_balance_pca = {}\ncv_FPR_balance_pca = {}\ncv_AUC_balance_pca = {}","736ed4df":"def plot_result_smoted_pca(model, name:str):\n    model.fit(X_train_os_pca, y_train_os)\n    y_pred = model.predict(X_test_pca)\n\n    # Evaluation based on a 10-fold cross-validation\n    scoring = ['balanced_accuracy', 'recall_macro']\n    scores_train = cross_val_score(model, X_train_os_pca, y_train_os, cv=10, scoring = 'balanced_accuracy')\n    scores_test = cross_val_score(model, X_test_pca, y_test, cv=10, scoring = 'balanced_accuracy')\n    cv_acc_balance_train_pca[name] = round(scores_train.mean(), 4)*100  # balanced accuracy\n    cv_acc_balance_test_pca[name] = round(scores_test.mean(), 4)*100  # balanced accuracy\n    cv_TPR_balance_pca[name] = (confusion_matrix(y_test, y_pred)[1][1]\/confusion_matrix(y_test, y_pred)[1].sum())*100  # recall (max)\n    cv_FPR_balance_pca[name] = (confusion_matrix(y_test, y_pred)[0][1]\/confusion_matrix(y_test, y_pred)[0].sum())*100  # fallout (min)\n\n    # accuracy scores\n    print('Average Balanced Accuracy (CV=10), Test Set:', scores_test.mean())  \n    print('Average Balanced Accuracy (CV=10), Training Set: ', scores_train.mean())\n\n    # print classification report\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Plot confusion matrix\n    plt.figure(figsize=(3,3))\n    plot_confusion_matrix(model, X_test_pca, y_test)\n    plt.show()","8c991388":"# Decision tree\ndtc = DecisionTreeClassifier()\nplot_result_smoted_pca(dtc, \"dtc\")","e6f75718":"# Logistic Regression\nlr = LogisticRegression()\nplot_result_smoted_pca(lr, \"lr\")","f6cd0b01":"# NearestCentroid\nnc = NearestCentroid()\nplot_result_smoted_pca(nc, \"nc\")","ce46997c":"# # SVC\n# svc = SVC()\n# plot_result_smoted_pca(svc, \"svc\")","595b0f8e":"# Random Forest\nrfc = RandomForestClassifier()\nplot_result_smoted_pca(rfc, \"rfc\")","6ac1d8b8":"# Gradient Boost\ngbc = GradientBoostingClassifier()\nplot_result_smoted_pca(gbc, \"gbc\")","5580dd42":"# Naive Bayes\nnb = GaussianNB()\nplot_result_smoted_pca(nb, \"nb\")","f99eb2df":"# kNN\nknn = KNeighborsClassifier()\nplot_result_smoted_pca(knn, \"knn\")","2f0cd216":"# XGBOOST\nxgb = XGBClassifier(eval_metric = \"logloss\")\nplot_result_smoted_pca(xgb, \"xgb\");","cb723c85":"cv_AUC_balance_pca = AUC(cv_AUC_balance_pca, X_test_pca)\ncv_AUC_balance_pca","bff47aa7":"df_eval_smote_pca = pd.DataFrame(data={'model': list(cv_acc_balance_train_pca.keys()), \n                                       'bal_acc_train':list(cv_acc_balance_train_pca.values()),\n                                       'bal_acc_test': list(cv_acc_balance_test_pca.values()),\n                                       'recall': list(cv_TPR_balance_pca.values()), \n                                       'fallout':list(cv_FPR_balance_pca.values()),\n                                       'AUC': list(cv_AUC_rus.values())}).round(2)\ndf_eval_smote_pca","162efaf6":"fig, ax = plt.subplots(4,4, figsize=(20, 16))\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,0])\nax[0,0].set_title(\"Unbalanced Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,1])\nax[0,1].set_title(\"Unbalanced Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,2])\nax[0,2].set_title(\"Unbalanced Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval.sort_values(by=\"recall\"), ax=ax[0,3])\nax[0,3].set_title(\"Unbalanced Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,0])\nax[1,0].set_title(\"Smote Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,1])\nax[1,1].set_title(\"Smote Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,2])\nax[1,2].set_title(\"Smote Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote.sort_values(by=\"recall\"), ax=ax[1,3])\nax[1,3].set_title(\"Smote Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,0])\nax[2,0].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,1])\nax[2,1].set_title(\"RUS_Featured Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,2])\nax[2,2].set_title(\"RUS_Featured Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_rus.sort_values(by=\"recall\"), ax=ax[2,3])\nax[2,3].set_title(\"RUS_Featured Model Test FPR\")\n\nsns.barplot(x=\"bal_acc_train\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,0])\nax[3,0].set_title(\"Smoted_PCA Model Train Acc\")\nsns.barplot(x=\"bal_acc_test\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,1])\nax[3,1].set_title(\"Smoted_PCA Model Test Acc\")\nsns.barplot(x=\"recall\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,2])\nax[3,2].set_title(\"Smoted_PCA Model Test TPR\")\nsns.barplot(x=\"fallout\", y=\"model\", data=df_eval_smote_pca.sort_values(by=\"recall\"), ax=ax[3,3])\nax[3,3].set_title(\"Smoted_PCA Model Test FPR\")\n\nplt.tight_layout()\nplt.show()","4cc6f940":"plot_ROC(X_test_pca)","b58fc18b":"df_eval[\"type\"] = \"Unbalanced\"\ndf_eval_smote[\"type\"] = \"Smote\"\ndf_eval_rus[\"type\"] = \"RUS\"\ndf_eval_smote_pca[\"type\"] = \"Smote_PCA\"","b52a0b04":"frames = [df_eval, df_eval_smote, df_eval_rus, df_eval_smote_pca]\ndf_result = pd.concat(frames, ignore_index=True)\ndf_result['model'] = df_result['model'].str.upper()\ndf_result[[\"recall\", \"fallout\", \"bal_acc_train\", \"bal_acc_test\",'AUC']] = df_result[[\"recall\", \"fallout\",  \"bal_acc_train\", \"bal_acc_test\",'AUC']].apply(lambda x: np.round(x, 2))","550a7804":"df_result","3eb9b7aa":"sns.relplot(x=\"recall\", y=\"AUC\", hue=\"model\", size=\"bal_acc_test\", \n            sizes=(40, 400), col=\"type\", alpha=1, palette=\"bright\", height=4, legend='full', data=df_result)","5ca97c7a":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nunder_sampler = RandomUnderSampler(random_state=42)\nX_train_rus, y_train_rus = under_sampler.fit_sample(X_train, y_train)\n\nparams={\"learning_rate\": [1],\n     \"min_samples_split\": [50, 10, 2],\n       \"min_samples_leaf\": [1, 5, 10],\n       \"max_depth\":[3,4,5],\n       \"subsample\":[0.5, 1.0],\n       \"n_estimators\":[10, 50, 100],\n       \"random_state\":[42]}\n\ngbc_tunned = GridSearchCV(GradientBoostingClassifier(), \n                                params, \n                                n_jobs=-1, \n                                verbose=2, \n                                ).fit(X_train_rus, y_train_rus)","e2c39022":"from sklearn.metrics import plot_confusion_matrix, classification_report, confusion_matrix\nprint(gbc_tunned.best_estimator_)\ny_pred = gbc_tunned.predict(X_test)\n\n# AUC Score\nprint('AUC:', roc_auc_score(y_test, gbc_tunned.predict_proba(X_test)[:,1]))\n\n# print classification report\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# Plot confusion matrix\nplt.figure(figsize=(3,3))\nplot_confusion_matrix(gbc_tunned, X_test, y_test)\nplt.show()","76173824":"> Dosages for insulin shows the most activity out of all diabetic medications, most of which aren't prescribed to patients.","133f63fb":"### Import Dataset","7d8cf14f":"* You can reach the extensive diagnosis description on this website by querying with the ICD9 code:\nhttp:\/\/icd9.chrisendres.com\/","c5a5ec7e":"The number of diagnoses column shows the total number of conditions a patient is diagnosed with. Only the first three are recorded, so those that are missing the first diagnosis but still a second or third are in error.","e29653ff":"> **How many medications are patients receiving during their visit?**","ec976ee1":"### 4-NearestCentroid","a163e942":"### FOCUS ON \"race\"","17dbdf80":"> It looks like most patients are older, 50+ years old, though there aren't many patients over 90.","4943fa25":"### Check Duplicates","de9723e6":"### FOCUS ON \"`number of emergency`\" visits","37a952aa":"> **Which age group is spending the most time in hospitals during visits?**","2cc5b264":"# numerical variables","a7db785c":">In every age group, more patients are not readmitted. The 70-80 age group account has the highest number of readmitted and not readmitted patients.","73189eb2":"The tunned GradientBoost Model didnt give a better result.","42a3888f":"### Evaluation (iteration 1)","6ca7a02d":"### medications","d096961b":"# Are the features that affect readmissions correlated with each other?","2f45ec46":"### 1-Decision tree","4a2a500b":"# General Overview - Machine Learning","7995e5ee":"#  Summary: \n* In this project the diabetic_data.csv dataset was analyzed by machine learning methods with 5 iterations as a classification. For each iteration one tried little by little to achieve a better model result. \n* 8 different algorithms (DecisionTree, Logistic Regression, Random Forest, Gradient Boost, NaiveBayes, Nearest Centroid, XGBOOST and kNearestNeigbour) were used. \n* After the data cleaning and EDA process, the data set was scaled with StandartScaler because there were many large and small features. After that, something special (oversampling, FeatureSelection, FeatureExtraction, HyperParameter optimization) was applied in each iteration. \n* In the end, GradientBoost with only undersampled and scaled data set gave better results.","dd086788":"## Iteration 4: (with SMOTE and PCA)","00e6bc3e":"> **who is likely to have a change in medication?**","0466dc72":"### FOCUS ON \"`number of procedures`\" (other than lab)","31d4d5c4":"### Use algorithms","8b3e754d":"### 2-Logistic Regression","5d86ac1a":"# FOCUS ON \"`glucose serum test results`\"","bae64d82":"Now, we are down to three columns with missing information: diagnosis 1, 2, and 3. \n* Diagnosis 1 is described as the primary diagnosis made during the patient's visit while diagnosis 2 is the second and 3 is an any additional diagnoses made after that. \n* Looking at the patients' rows that are missing a primary diagnosis, most of them have a second diagnosis or even a third. \n* Since it doesn't make sense to have a second (or third) but not a primary diagnosis, we will remove these columns from the dataset.","750ff9fe":"> NaiveBayes gave high BalanceAccuracy and TPR_Score (Recall), but it gave the poor FPR_Score (Fallout) in this unbalanced data set.","d6c32846":"### FOCUS ON \"`A1C results`\"","b34c7e23":"* 'diag_1','diag_2' and 'diag_3' columns contain codes for the types of conditions patients are diagnosed with. \n* There are too much unique codes throughout this dataset.\n* We can group the related icd9 diagnosis codes among themselves. In this way, we use categorical group names instead of numerical codes.\n* The grouping is based on the research paper table (https:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/tab2\/)\n![image.png](attachment:image.png)","47a3e666":"* We are looking for correlations between the independent variables and the target variable, the likelihood of being readmitted to the hospital, using graphs and plots. \n* This is also a good time to get a better understanding of patient demographics, their experiences at the hospital, medications being used \/ not used, and any diagnosed conditions.","422a8996":"> We regard the observations of \"Unknown\/Invalid\" gender as null values and drop them.","b866cf42":"### medications used by patients","7acdfd92":"* For a small number of observations with number of diagnoses greater than 9, let's change the number of diagnoses to 9.","14bb6114":"### Descriptive Analysis","bdf50343":"There are two remaining diagnosis columns with missing values. Each number correlates to a specific condition so if there is a missing value, then it is likely that the patient only has one diagnosed condition. The number of diagnoses column lists the total number of diagnosed conditions. When looking at all three diagnosis columns, if the number is one, then diagnosis 2 and 3 can be filled in with a 0 to show that there is no additional diagnosis. If diagnosis 2 or 3 is missing a value and the number of diagnoses is greater than one, then some diagnoses were not recorded and the rows should be removed.","bb4694a6":"> **Do the number of tests performed indicate whether a patient will be readmitted?**","67e67897":"### Outlier Detection","0839c5e0":"* The medications: nateglinide, chlorpropamide, glimepiride, acetohexamide, glyburide, tolbutamide, miglitol, troglitazone, tolazamide, glyburide-metformin, glipizide-metformin, and metformin-pioglitazone all failed to pass the test since they have p-values greater than 0.01.\n\n* Since these variables are not independent of the target variable, we are removing them from the dataset.","fb3953f6":"## chi-square test for association","7035ac4d":"* we can think of 'patient_nbr' as the id number of each patient.\n* It turned out that the dataset is the data of 71515 unique patients.\n* Some patients visited the hospital multiple times for treatment so to avoid over-representing any particular individual, only the first encounter with a patient will be used \/ kept in this dataset.","eae66556":"* The categorical variables are: \n<br>`['race', 'gender', 'age', 'diag_1', 'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted']`\n\n\n* We are using the chi-square test for association with a p-value of 0.01 to reject the null hypothesis.","ea8dd31b":"### Export Cleaned Dataset","e5ffd44b":"> no duplicates detected!","54f79905":"If the correlation value is greater than 0.7 or less than -0.7, we have to drop one of the two columns.\n\nThe correlation map is quite large for this notebook. Instead, we are going to find each correlation coefficient individually and mark the ones that have a coefficient greater than 0.7 or less than -0.7.","703200bf":"### FOCUS ON \"age\" groups","fc199302":"### FOCUS ON \"readmitted\" patients overall","e20b5a1e":"It looks like n_components = 43 is suitable for% 95 total explained variance,","50ae82dd":"> Based on the chi-square value and p-value, we can safely say that there is no relation between the independent variables and the target variable.","9f5289fc":"* We dont need 'admission_type_id', 'discharge_disposition_id', 'admission_source_id' columns","1754f52b":"## statistical testing - analysis of variance (ANOVA)","cd87aa20":"* Using the analysis of variance (ANOVA) test, we want to determine if there is a statistically significant relationship between a numerical variable and the categorical target variable. Our p-value threshold is 0.01.","026aea79":"### FOCUS ON \"`diabetesMed`\"","6b2bfe09":"* Since there is no way to know the race of the patient using existing information, the best option is to remove those rows.","807a3bdc":"### 8-kNN","0dc50c92":"### Import Dataset","45e18460":"> **How many emergency visits did patients have prior to this visit?**","c6ef9ebe":"> **Do readmitted patients have more lab tests?**","45badf51":"# General Overview - Statistical Analysis","8631a652":"GradientBoosting yielded the optimized result as better FPR and relative mean strong recall scores. The balance accuracy is also relatively good.","8dc30ed7":"> Since the majority of patients do not have a glucose reading, they will be excluded for the next graph in order to show the readmit rates for patients who do have a reading.","9784c3da":"### FOCUS  ON \" patient_nbr \"","a078a44c":"## change in medications, dosage or brand","ad01076b":"Patients with a glucose serum reading of over 300 have a 50-50 chance of being readmitted. High blood sugar levels are often dangerous for older patients due to the medical complications involved, so it's understandable that more patients return to the hospital for additional care.","5f9ac350":"> Based on the ANOVA test, we can drop the number of emergency visits since we cannot reject the null hypothesis that the averages for each class are similar, the p-value is greater than our threshold of 0.01.","817934af":"### FOCUS ON \"number of lab procedures`","3c28814c":"> **Does the amount of time spent in the hospital impact a patient's chances of readmission?**","4f75c87f":"## Iteration 5: (with RUS and hyperparameter optimization)","2773aee5":"#### Use Algorithm","45b458e1":"> Most patients did not have any outpatient visits prior to the recorded one.","49f71239":"* Since patient_nbr is unique, it is no longer needed.","2d48ce02":"The loads (loading scores) indicate \"how high a variable X loads on a factor Y\". \n\n(The i-th principal components can be selected via i in pca.components_ [0].)","bd4e52bd":"# One Hot Encoding","0377c7b6":"Based on the basic statistics describing the dataset, it looks there are outliers that influence skewness in the data. In order to represent the majority of samples and build clean models, we are going to remove outliers that have [z-scores](https:\/\/www.statisticshowto.datasciencecentral.com\/probability-and-statistics\/z-score\/) greater than 3.0 or less than -3.0. This means that we are removing samples that are more (or less) than 3 times the standard deviation from the mean.","f963d1da":"![one-way-ANOVA-formulas.png](attachment:one-way-ANOVA-formulas.png)","f0bb3179":"* The majority of patients do not have a weight listed so this column can be dropped. \n* Medical specialty and payer code are also missing for about half of the patients. \n* We do not need to know how the patients paid for their treatments.\n* we do not have enough information to figure out which medical unit they went to.","5f65b4f4":"> Inpatient visits are not common for most patients prior to this visit.","6ae124e2":"Dataset Resources: [UC Irvine's Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008)\n\n[Kaggle Link](https:\/\/www.kaggle.com\/pavan2029\/diabetic-data)\n\n**Objective:**\nHospital readmission rates for certain conditions are now considered an indicator of hospital quality, and also affect the cost of care adversely. Hospital readmissions of diabetic patients are expensive as hospitals face penalties if their readmission rate is higher than expected and reflects the inadequacies in health care system. For these reasons, it is important for the hospitals to improve focus on reducing readmission rates. Identify the key factors that influence readmission for diabetes and to predict the probability of patient readmission. \n\nThe dataset represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. The data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.*","dd856f51":"### FOCUS ON \"`number of inpatient`\" visits","8d62b1da":"* Looking at the graph above, we can say that there is a high correlation between the diagnoses. So we drop diag_2 and diag_3.\n* Also since the most common diagnoses are prevalent in all three diagnoses listed, We are only using the primary diagnosis variable to build the machine learning model","ca6dcc1a":"Glikoz serum okumas\u0131 300'\u00fcn \u00fczerinde olan hastalar\u0131n readmit orani 50-50'dir. Y\u00fcksek kan \u015fekeri seviyeleri, t\u0131bbi komplikasyonlar nedeniyle genellikle ya\u015fl\u0131 hastalar i\u00e7in tehlikelidir, bu nedenle daha fazla hastan\u0131n ek bak\u0131m i\u00e7in hastaneye d\u00f6nmesi anla\u015f\u0131labilir bir durumdur.","25b51782":"### Import Libraries","d1be9afd":"* We want to analyze the variables in this dataset to understand any relationships between them and their overall effects.\n* To do this,\n        * `Chi-square test` for categorical variables relationship\n        * We have to analyze numerical variables using `analysis of variance` or `ANOVA test`.\n* The purpose of these tests is to determine whether there is a statistically significant relationship between the target variable, readmissions and independent variable. Our p-value is 0.01, if anything above that, we cannot reject the null hypothesis.\n* A machine learning model can interpret integers as well as process strings, so we must transform all categorical variables using dummy variables as numeric variables. This takes the string values \u200b\u200bin a variable and converts them to columns labeled 0 or 1 relative to the string. We will also standardize the original numerical variables with a mean of 0 and a standard deviation of 1.\n* Finally, we look at the correlation coefficients between the independent variables to make sure they do not have a strong influence on each other. The threshold we used is -0.7 <x <0.7.\n\n","bcd80bb6":"### 6-Gradient Boosting","b15c6785":"Binary columns will be replaced with 0 for No and 1 for Yes. In the gender column, Male and Female will be replaced with 0 and 1 respectively.","e61f29e3":"> **How many diagnoses do readmitted patients have?**","9eeab2c9":"### Import Libraries","96304a6d":"> Based on the graph, the longer a patient spends in the hospital, the likelier their chances are of being readmitted. Patients who spend more than a week in the hospital usually have a serious illness or complication that may reoccur depending on their ability to recover, which is why they may need to revisit the hospital.","add9230b":"### Import Features Dataset\nDescriptions of the features:\nhttps:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/tab1\/","d9967522":"### Lazy Predict with 5000 samples","9bf1efe3":"### Split Data","885401e8":"Diagnosis 3 is the last column left with unaccounted missing values. Since some patients have 1 or 2 diagnosed conditions, the diagnosis 3 column is left intentionally blank. The goal here is to remove the rows that have a diagnoses number greater than two.","d890634f":"> **Do the patients with longer hospital stays have more lab tests?**","0ec33843":"## Feature Selection\n![How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png](attachment:How-to-Choose-Feature-Selection-Methods-For-Machine-Learning.png)","6e34cb2d":"> **How many medications are patients receiving during their visit?**","567c4a95":"### FOCUS ON \"gender\"","14ad87ec":"### FOCUS ON \"number of medications\"","3db173e0":"### Import and Load","daced399":"* Most patients have up to nine diagnosed conditions during their visit, after that, only a handful have more than nine in one visit. \n* Readmitted patients tend to have more diagnosed conditions but their average is only slightly higher than those not readmitted.","46a334a3":"**Group Names**\n\n    1-Circulatory\n    2-Respiratory\n    3-Digestive\n    4-Diabetes\n    5-Injury\n    6-Musculoskeletal\n    7-Genitourinary\n    8-Neoplasms\n    9-Other","24982e45":"### categorical variables","9008ab97":"> Most patients did not visit the emergency room prior to their recorded visit.","9d1abce5":"* The average number of lab procedures is about equal for readmitted and not readmitted patients. \n* Not readmitted patients have a slightly lower number of lab procedures done during their visit.","d08da9e7":"# saving machine learning dataset","49af7f9c":"### FOCUS ON \"readmitted\"","4567224f":"### 7-Naive Bayes","a752d747":"### 9-XGBOOST","c7dc4574":"## Iteration 3: (with RUS)","18bef292":"### FOCUS ON \"`number_diagnoses`\"","af1a0339":"![resampling-techniques-in-machine-learning-15-638.jpg](attachment:resampling-techniques-in-machine-learning-15-638.jpg)","d797798d":"At the end of 4 iteration, GradientBoost with only undersampled and scaled data set gave better results. In this iteration, we try to improve the GradientBoost Model with hyperparameter optimization.","a0e48465":"> **What is the comparison of time in hospital for readmitted patients?**","1f8627e7":"### Balancing data","0d0817f1":"* In this plot it looks like GradientBoosting in Smote_PCA has the best scores. But There is a overfitting there, GradientBoosting in RUS(Random Under Sampling) is better. There is no overfitting. Recall:57.27, AUC:0.62, F1:58\n* In the last iteration we will make hyperparameter optimization with GradientBoosting in RUS. We try to reach a better scores.","b850eb24":"* There is a positive correlation between time spent in the hospital and number of lab tests completed. \n* This makes sense since patients with longer stays had more tests completed to properly diagnose their conditions.","63b2d93f":"# General Overview","e580aa35":"### 3-SVC","7e8eacb3":"### Iteration 2: (Oversampling with SMOTE)","27ce4c37":"> The distribution is almost equal for readmitted and not readmitted patients, with readmits being slightly higher on average.","09a9e601":"> Patients readmitted to the hospital within and after 30 days will be combined into one column, because these patients ultimately returned.","3c899934":"> Drop the columns that the number of uniques is 1","3ddd108d":"* 'citoglipton' and 'examide' features that the number of uniques is 1 are droped.\n* all values of 'encounter_id' column are unique. It has to be droped.","1a3f0ed2":"### Data Scaling","0180040d":"According to Smote and PCA, none of the models really gave relatively good results.","416f4af4":"#### Use Algorithm","fcc24b11":"### FOCUS ON \"time_in_hospital\"","aaf3ce31":"### Dropping irrelevant columns","c2796834":"### Analysis of Diagnosis","0b6107cd":"> **Who is likely or not likely to have a change in medication?**","5f46c704":"### Iteration 1: (Unbalanced data)\n![CM-1024x382.png](attachment:CM-1024x382.png)","1fc9cd7b":"### Grouping Diagnosis Codes","6ccae8aa":"# Visualization","cef05feb":"### Import Libraries","b5fce2db":"### 5-Random Forest","3dfe0d56":"### FOCUS ON \"Gender\"","159a250d":"Investigate the unique values of each column and look for error entries.","8bd4171c":"* Similar to the glucose reading, the majority of patients also do not have a HbA1c test reading. \n* In order to understand the impact of A1c tests on readmit rates, patients without a reading will be excluded in the graph below.","846c6b13":"### FOCUS ON \"`number of outpatient`\" visits","72228b08":"### Handling Missing Values","c621668f":"### Check Unique Values","b2dd421d":"> Patients who spend more time in the hospital receive more medications, but there are a few that receive over 60 different kinds of medications.","41e38e93":"> Readmitted patients stay longer in the hospital on average compared to those who are not readmitted.","54ab90d3":"### FOCUS ON \"`number of diagnoses`\"","74c96c1d":"### FOCUS ON \"`change`\" column"}}