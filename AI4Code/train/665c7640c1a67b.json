{"cell_type":{"6a57b137":"code","0da3b7b1":"code","c05e0b2b":"code","e418fe14":"code","2bfaf965":"code","c1852cb8":"code","7a3a69d4":"code","d786a222":"code","b45c4d38":"code","e23a708a":"code","cd00a887":"code","c54a90e5":"code","6f8372a7":"code","9ab44477":"code","9d1430e3":"code","c4d1abd9":"code","51677b60":"code","ea1c8589":"code","69cbdfc7":"code","f0de13e2":"code","8aaff4a2":"code","d8960e58":"code","6242210b":"code","65e7a1c8":"markdown","f53166b7":"markdown","5c6ecef3":"markdown","e8e3d443":"markdown","26c4e601":"markdown","19efb8e8":"markdown","0992bdd1":"markdown","e96a15d4":"markdown","52cf349a":"markdown","ba34a181":"markdown","aa589010":"markdown","a6d9cd08":"markdown","d84701d0":"markdown","5f7ca61b":"markdown","1b24ea70":"markdown","d374e159":"markdown","761d624a":"markdown","99052191":"markdown","880da6fe":"markdown","20f613d5":"markdown"},"source":{"6a57b137":"import os\n\nimport pandas as pd\nimport numpy as np\nimport scipy.sparse\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\nfrom mlxtend.feature_selection import ColumnSelector\nfrom mlxtend.classifier import StackingCVClassifier\n\nfrom catboost import CatBoostClassifier","0da3b7b1":"RANDOM_SEED = 42\n\nDATA_DIR = '..\/input\/mlcourse-dota2-win-prediction\/'\n\ntrain_data = pd.read_csv(os.path.join(DATA_DIR, 'train_features.csv'), index_col='match_id_hash')\ntest_data = pd.read_csv(os.path.join(DATA_DIR, 'test_features.csv'), index_col='match_id_hash')\ny_train = pd.read_csv(os.path.join(DATA_DIR, 'train_targets.csv'), index_col='match_id_hash')['radiant_win'].map({True: 1, False:0})","c05e0b2b":"# Features to compare hero\/team efficiency\nhero_efficiency_features = [\n 'kills',\n 'deaths',\n 'assists',\n 'denies',\n 'gold',\n 'lh',\n 'xp',\n 'health',\n 'max_health',\n 'max_mana',\n 'level',\n 'creeps_stacked',\n 'camps_stacked',\n 'rune_pickups',\n 'firstblood_claimed',\n 'teamfight_participation',\n 'towers_killed',\n 'roshans_killed',\n 'obs_placed',\n 'sen_placed',\n]    \n\n# Features \ntimed_efficiency_features = [\n 'kills',\n 'deaths', \n 'assists',\n 'denies',\n 'gold',\n 'lh',\n 'xp',\n 'creeps_stacked',\n 'camps_stacked',\n 'rune_pickups',\n]    \n\nHERO_CLASSES = 121\n\ndef make_rel_features(df, rel_type='absolute'):\n    relative_features = pd.DataFrame(index=df.index)\n    for feature in hero_efficiency_features:\n        # Radiant team \n        columns = ['r' + str(i) + '_' + feature for i in range(1,6)]\n        radiant_sum = df[columns].sum(axis=1)\n        # Dire team\n        columns = ['d' + str(i) + '_' + feature for i in range(1,6)]\n        dire_sum = df[columns].sum(axis=1)\n        if rel_type == 'absolute':\n            relative_features['diff_'+feature+'_'+rel_type] = radiant_sum - dire_sum\n        elif rel_type == 'ratio':\n            relative_features['diff_'+feature+'_'+rel_type] = (radiant_sum + 1) \/ (dire_sum + 1)\n        else:\n            raise Exception('Unknown rel_type: ' + rel_type)            \n    return relative_features\n\ndef make_timed_features(df, team_char):\n    timed_features = pd.DataFrame(index=df.index)\n    for feature in timed_efficiency_features:\n        columns = [team_char + str(i) + '_' + feature for i in range(1,6)]\n        feat_sum = df[columns].sum(axis=1)\n        timed_features['permin_'+feature+'_'+team_char] = feat_sum \/ ((df.game_time + 1) \/ 60) \n    return timed_features\n\ndef make_hero_features(df, team_prefix):\n    hero_id_features = [team_prefix + str(i) + '_hero_id' for i in range(1, 6)]\n    heroes_flat = df[hero_id_features].values.flatten()\n    heroes_sparse = scipy.sparse.csr_matrix(([1] * heroes_flat.shape[0],\n                                             heroes_flat,\n                                             range(0, heroes_flat.shape[0]  + 1, 5)))\n    if heroes_sparse.shape[1] < HERO_CLASSES:\n        heroes_sparse = scipy.sparse.hstack([heroes_sparse, \n                                             scipy.sparse.csr_matrix((heroes_sparse.shape[0], HERO_CLASSES-heroes_sparse.shape[1]))])\n    # An attentive reader here might question: \"Why on Earth do we create a sparse matrix\n    # and then convert it to a dense DataFrame?\" I have to agree with you. However, this\n    # (inefficiency) allows me to make the following code more concise.\n    heroes = pd.DataFrame(heroes_sparse.todense(), \n                          columns=['hero_%d' % (i,) for i in range(1, HERO_CLASSES+1)], \n                          index=df.index)    \n    return heroes\n","e418fe14":"def make_alice_features(df):\n    game_features = df[['game_time', \n                        'game_mode', \n                        'objectives_len', \n                        'chat_len']]\n    relative_features = make_rel_features(df, 'ratio')\n    timed_features_radiant = make_timed_features(df, 'r')\n    timed_features_dire = make_timed_features(df, 'd')\n    radiant_heroes = make_hero_features(df, 'r')\n    dire_heroes = make_hero_features(df, 'r')\n    dire_heroes.columns = dire_heroes.columns + '_dire'\n    return pd.concat([game_features,\n                      relative_features,\n                      timed_features_radiant,\n                      timed_features_dire,\n                      radiant_heroes,\n                      dire_heroes], axis=1)\n    \nalice_train_features = make_alice_features(train_data)\nalice_train_features.shape","2bfaf965":"alice_model = CatBoostClassifier(iterations=1000, \n                                 random_state=RANDOM_SEED, \n                                 silent=True)\n\nalice_scores = cross_val_score(alice_model, \n                               alice_train_features, \n                               y_train, \n                               cv=5, \n                               scoring='roc_auc')\n\nprint('Alice (CatBoost) scores:', alice_scores)\nprint('Alice (CatBoost) mean ROC AUC: %.4f (std dev: %.6f)' % \n      (alice_scores.mean(), alice_scores.std()))","c1852cb8":"def make_bob_features(df):\n    game_features = df[['game_time', \n                        'game_mode', \n                        'objectives_len', \n                        'chat_len']]\n    relative_features = make_rel_features(df)\n    timed_features_radiant = make_timed_features(df, 'r')\n    team_features = make_hero_features(df, 'r') - make_hero_features(df, 'd')\n    return pd.concat([game_features,\n                      relative_features,\n                      timed_features_radiant,\n                      team_features], axis=1)\n    \nbob_train_features = make_bob_features(train_data)\nbob_train_features.shape","7a3a69d4":"def make_indexes(df, column_names):\n    return [df.columns.get_loc(c) for c in column_names if c in df]","d786a222":"# Prepare lists with indexes of columns that require different approaches in transformation.\n# Though ColumnSelector accepts feature names (when processing a DataFrame), later we will\n# reuse this pipeline in a situation where column names won't be available\ngame_mode_idx = make_indexes(bob_train_features, ['game_mode'])\nhero_idx = make_indexes(bob_train_features, ['hero_%d' % (i,) for i in range(1, HERO_CLASSES+1)])\nother_idx = make_indexes(bob_train_features, \n                         list(filter(lambda x: x.startswith('diff_') or x.startswith('permin_'), \n                                     bob_train_features.columns)))\n# Categories definition for the game_mode feature\ngame_mode_categories = [sorted(list(bob_train_features.game_mode.unique()))]\n\n# Data processing. The resulting dataset for the logistic regression is\n# collected from the one-hot encoded game_mode, untouched hero features \n# and the rest features scaled\nlr_features = FeatureUnion([   \n    # OHE for game mode\n    ('game_mode', Pipeline([('select_game_mode', \n                                ColumnSelector(cols=game_mode_idx)),\n                             ('ohe_game_mode', \n                                OneHotEncoder(categories=game_mode_categories))])),\n    # Leave hero features as they are\n    ('hero_features', ColumnSelector(cols=hero_idx)),\n    # Standard scaling for the rest\n    ('other_features', Pipeline([('select_other_features', \n                                      ColumnSelector(cols=other_idx)),\n                                 ('standard_scaling', \n                                      StandardScaler())]))\n])\n\nbob_pipeline = Pipeline([('prepare_features', lr_features),\n                        ('logit', LogisticRegression(C=1, \n                                      random_state=RANDOM_SEED, \n                                      solver='lbfgs', \n                                      max_iter=500))])\n\nbob_scores = cross_val_score(bob_pipeline, \n                             bob_train_features, \n                             y_train, \n                             cv=5, \n                             scoring='roc_auc')\n\nprint('Bob (logit) scores:', bob_scores)\nprint('Bob (logit) mean ROC AUC: %.4f (std dev: %.6f)' % \n          (bob_scores.mean(), bob_scores.std()))","b45c4d38":"# Glue input data for different classifiers into one featureset\nbob_train_features.columns = bob_train_features.columns + '_bob'\ntrain_features = pd.concat([alice_train_features, bob_train_features], axis=1)\ntrain_features.shape","e23a708a":"## Define a pipeline for each first-level classifier\n\n# Features for the Alice's model (CatBoost) are \n# the first alice_train_features.shape[1] columns\nalice_col_subset = tuple(range(0, alice_train_features.shape[1]))\nalice_pipe = Pipeline([('catboost features', \n                             ColumnSelector(cols=alice_col_subset)),\n                       ('catboost', alice_model)])\n\n# Features for the Bob's model (logit) are \n# columns from alice_train_features.shape[1] to the end\nbob_col_subset = tuple(range(alice_train_features.shape[1], \n                             train_features.shape[1]))\nbob_pipe = Pipeline([('logit features', \n                             ColumnSelector(cols=bob_col_subset)),\n                     ('logit', bob_pipeline)])\n\n## Compose a stacking classifier\nsclf = StackingCVClassifier(\n           classifiers=[alice_pipe,\n                        bob_pipe], \n           meta_classifier=LogisticRegression(random_state=RANDOM_SEED, \n                                              solver='lbfgs'),\n           use_probas=True,  # if False, meta-features will\n                             # be formed from plain predictions of the\n                             # first-level classifiers\n           cv=3, # This is the number of splits used to\n                 # build the meta-feature dataset\n           random_state=RANDOM_SEED,\n           verbose=1)\n\nsclf.fit(train_features, y_train)","cd00a887":"sclf.clfs_[0]","c54a90e5":"sclf.meta_clf_","6f8372a7":"sclf = StackingCVClassifier(\n           classifiers=[alice_pipe,\n                        bob_pipe], \n           meta_classifier=LogisticRegression(random_state=RANDOM_SEED, \n                                              solver='lbfgs'),\n           use_probas=True,  # if False, meta-features will\n                             # be formed from plain predictions of the\n                             # first-level classifiers\n           cv=3, # This is the number of splits used to\n                 # build the meta-feature dataset\n           random_state=RANDOM_SEED,\n           verbose=0)\n\nteam_scores = cross_val_score(sclf, \n                              train_features, \n                              y_train, \n                              cv=5, \n                              scoring='roc_auc')\n\nprint('Team (stacked) scores:', team_scores)\nprint('Team (stacked) mean ROC AUC: %.4f (std dev: %.6f)' % \n          (team_scores.mean(), team_scores.std()))","9ab44477":"team_scores > alice_scores","9d1430e3":"team_scores > bob_scores","c4d1abd9":"def make_joint_features(df):\n    game_features = df[['game_time', 'game_mode', 'objectives_len', 'chat_len']]\n    relative_features_abs = make_rel_features(df, 'absolute')\n    relative_features_ratio = make_rel_features(df, 'ratio')\n    timed_features_radiant = make_timed_features(df, 'r')\n    timed_features_dire = make_timed_features(df, 'd')\n    team_features = make_hero_features(df, 'r') - make_hero_features(df, 'd')\n    return pd.concat([game_features,\n                      relative_features_abs,\n                      relative_features_ratio,\n                      timed_features_radiant,\n                      timed_features_dire,\n                      team_features], axis=1)\n    \njoint_train_features = make_joint_features(train_data)\njoint_train_features.shape","51677b60":"alice_model_joint = CatBoostClassifier(iterations=1000, \n                                       random_state=RANDOM_SEED, \n                                       silent=True)\n\nalice_scores_joint = cross_val_score(alice_model_joint, \n                                     joint_train_features, \n                                     y_train, \n                                     cv=5, \n                                     scoring='roc_auc')\n\nprint('Alice (CatBoost) scores:', alice_scores_joint)\nprint('Alice (CatBoost) mean ROC AUC: %.4f (std dev: %.6f)' \n          % (alice_scores_joint.mean(), alice_scores_joint.std()))","ea1c8589":"game_mode_idx = make_indexes(joint_train_features, ['game_mode'])\nhero_idx = make_indexes(joint_train_features, ['hero_%d' % (i,) for i in range(1, HERO_CLASSES+1)])\nother_idx = make_indexes(joint_train_features, \n                         list(filter(lambda x: x.startswith('diff_') or x.startswith('permin_'), \n                                     joint_train_features.columns)))\n# Categories definition for the game_mode feature\ngame_mode_categories = [sorted(list(joint_train_features.game_mode.unique()))]\n\n# Data processing. The resulting dataset for the logistic regression is\n# collected from the one-hot encoded game_mode, untouched hero features \n# and the rest features scaled\nlr_features = FeatureUnion([   \n    # OHE for game mode\n    ('game_mode', Pipeline([('select_game_mode', \n                                ColumnSelector(cols=game_mode_idx)),\n                             ('ohe_game_mode', \n                                OneHotEncoder(categories=game_mode_categories))])),\n    # Leave hero features as they are\n    ('hero_features', ColumnSelector(cols=hero_idx)),\n    # Standard scaling for the rest\n    ('other_features', Pipeline([('select_other_features', \n                                      ColumnSelector(cols=other_idx)),\n                                 ('standard_scaling', \n                                      StandardScaler())]))\n])\n\nbob_pipeline_joint = Pipeline([('prepare_features', lr_features),\n                               ('logit', LogisticRegression(C=1, \n                                             random_state=RANDOM_SEED, \n                                             solver='lbfgs', \n                                             max_iter=500))])\n\nbob_scores_joint = cross_val_score(bob_pipeline_joint, \n                                   joint_train_features, \n                                   y_train, \n                                   cv=5, \n                                   scoring='roc_auc')\n\nprint('Bob (logit) scores:', bob_scores_joint)\nprint('Bob (logit) mean ROC AUC: %.4f (std dev: %.6f)' % \n          (bob_scores_joint.mean(), bob_scores_joint.std()))","69cbdfc7":"sclf2 = StackingCVClassifier(\n            classifiers=[alice_model_joint,\n                         bob_pipeline_joint], \n            meta_classifier=LogisticRegression(random_state=RANDOM_SEED, \n                                               solver='lbfgs'),\n            use_probas=True,  # if False, meta-features will\n                              # be formed from plain predictions of the\n                              # first-level classifiers\n            cv=3, # This is the number of splits used to\n                  # build the meta-feature dataset\n            random_state=RANDOM_SEED,\n            verbose=0)\n\nteam_scores_joint = cross_val_score(sclf2, \n                                    joint_train_features, \n                                    y_train, \n                                    cv=5, \n                                    scoring='roc_auc')\n\nprint('Team (stacked) scores:', team_scores_joint)\nprint('Team (stacked) mean ROC AUC: %.4f (std dev: %.6f)' % \n          (team_scores_joint.mean(), team_scores_joint.std()))","f0de13e2":"team_scores_joint > alice_scores_joint","8aaff4a2":"team_scores_joint > bob_scores_joint","d8960e58":"team_scores_joint > team_scores","6242210b":"results = pd.DataFrame([[alice_scores.mean(), alice_scores.std(), \n                         alice_scores_joint.mean(), alice_scores_joint.std()],\n                        [bob_scores.mean(), bob_scores.std(),\n                         bob_scores_joint.mean(), bob_scores_joint.std()],\n                        [team_scores.mean(), team_scores.std(),\n                         team_scores_joint.mean(), team_scores_joint.std()]],\n                      index=['Alice', 'Bob', 'Team'])\nresults.columns = ['Mean (Individual)', 'Std (Individual)',\n                   'Mean (Merged)', 'Std (Merged)']\n\nresults","65e7a1c8":"## Define and Train a Stacking Classifier","f53166b7":"## Alice's Model on the Joint Dataset","5c6ecef3":"# Model Stacking\n\nTo implement stacking of the boosting and logit models Alice and Bob decided to use a very handy class `StackingCVClassifier` from `mlxtend` package. It splits the training set into a specified number of folds ($N_F$, by default $N_F=2$), trains each of the first-level classifiers on all the folds except one and then predicts values for that fold. After $N_F$ repetitions of this process a detaset with first-level model predictions (so-called metafeatures) for each training instance is obtained. Then, this metafeature dataset is used to train the meta-classifier. For more details, please refer to the [documentation](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/).\n\nIn terms of scikit-learn, `StackingCVClassifier` is an estimator with an ordinary `fit(X, y)` method, that means that Alice and Bob have to provide only one training dataset to it, but they have two, with different sets of features.\n\nThe simplest solution is to 'horizontally' concatenate two datasets and then use `ColumnSelector` instances for feeding different models with the respective data prepared for them.\n","e8e3d443":"Alice and Bob are happy with their joint solution and ready to submit it (note, however, that it is not quaranteed at all that their stacked solution would be better than both of the individual solutions, especially if the sets of features of the models are almost the same).","26c4e601":"As well as the fitted meta-classifier:","19efb8e8":"## Bob's Model on the Joint Dataset","0992bdd1":"# Feature Engineering\n\nObtaining a competitive submission is not a goal of this kernel, therefore, we use only the most basic features from csv, without even touching jsonl. There are many quality kernels that contain precious ideas about useful features. Some of them are:\n\n - https:\/\/www.kaggle.com\/artgor\/dota-eda-fe-and-models\n - https:\/\/www.kaggle.com\/karthur10\/data-extraction-from-json-additional-features\n - https:\/\/www.kaggle.com\/marketneutral\/dota-2-coordinates-eda-animated\n","e96a15d4":"For the logistic regression Bob had to do some preprocessing of the data: OHE of the categorical feature `game_mode` and standard scaling of the most features.","52cf349a":"## Prepare a 'Glued' Dataset (Optional)\n\n*NOTE: This is an optional step! If you apply several different classification models to the same features, you can account for differences in feature representation required for the models at the preprocessing stages of the respective pipelines. *\n","ba34a181":"We can see, that in this particular example, after the information exchange about features each individual model were able to achieve roughly the same prediction quality on cross-validation than the stacked model (a bit surprisingly, logistic regression model even slightly surpassed all other models, but the margin is unlikely significant).\n\nCertainly, this example shouldn't be viewed as a general rule, but it supports the idea that early teaming and tight collaboration on solution ideas could be beneficial not only socially, but also in terms of solution quality.\n\nFinally, let's make a stack of the improved models and see what happens. Note, that now we do not need to add `ColumnSelector`s to the stacked pipelines, as both models are based on the same features.\n","aa589010":"# Extra\n\nIn this toy example, datasets used by each of the teammates intentionally missed some interesting features present in the dataset of the other. But what if Alice and Bob worked more closely and exchanged the features they found useful? Would their 'plain' models trained with more useful features surpass the stacked solution?\n\nTo answer this question we will construct a merged dataset including the most promicing features present either in Alice's or in Bob's original datasets. Then we will adapt the initial models of Alice and Bob to use this dataset.","a6d9cd08":"Interestingly enough, even after merging their datasets, they could have been able to further improve their results on cross-validation by stacking their models.\n\nLet's make a table summarizing the results:","d84701d0":"![image.png](attachment:image.png)","5f7ca61b":"## ROC AUC Comparison\n\nBut Alice and Bob are mostly interested in checking if their stacked solution is better than individual ones:\n","1b24ea70":"# Acknowledgement\n\nI'd like to thank my teammate Tatiana Glazkova ([@panikads](https:\/\/www.kaggle.com\/panikads)) for showing me the stacking feature of `mlxtend` and laboriously implementing our joint solution with it during the competition. \n","d374e159":"# Alice: CatBoost Model\n\nTo make the situation more typical, we will create slightly different featuresets for Alice and Bob (after all, they most likely did their feature extraction and feature engineering independently).","761d624a":"# Stacking\n\nA very popular approach for obtaining a joint team solution is to form a linear combination of the individual models (with weights selected via 'common sense') and hope that it will improve the score (it usually does). This notebook shows a more systematic approach to obtaining a joint model with a help of `mlxtend.StackingCVClassifier`. In particular, it allows to automate the process of assessing different model mixtures with cross-validation and helps to select the most promising of them.\n\nSo, let's imagine a situation that two team members (Alice and Bob) came up with different solutions - Alice built a CatBoost model, and Bob crafted a logit one. Now they want to build a merged classifier.\n","99052191":"## Accessing the parameters of classifers\n\nAfter fitting the stacked classifier we can access the fitted first-level models:\n","880da6fe":"# Bob: Logistic Regression Model","20f613d5":"Please note, that training `StackingCVClassifier` for $K$ first-level classifiers requires fitting $N_F * K + 1$ models. If we want to estimate the quality of our stacking solution with `cross_val_score`, this number should be multiplied by the number of folds in our cross-validation scheme. \n\nHowever, if there is no shortage of time and computing resources, `StackingCVClassifier` goes fine with `GridSearchCV`, allowing to experiment with different values of hyperparameters of the meta-classifier as well as first-level classifiers. \n"}}