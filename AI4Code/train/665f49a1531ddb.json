{"cell_type":{"d3c059b1":"code","0c2c9844":"code","c46b38d3":"code","7dc94f5b":"code","f15f143f":"code","b4552abd":"code","d9907e92":"code","21eafa0a":"code","0becb33c":"code","48008724":"code","f86c10a4":"code","937ea15f":"code","d52498ab":"code","2ace30d9":"code","8712eb71":"code","b3c20f7b":"code","d8f76a9a":"code","6f9fdef6":"markdown","47316f16":"markdown","5a4e428a":"markdown","68327fa3":"markdown","83a32f5b":"markdown","5c96770e":"markdown"},"source":{"d3c059b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport time\nimport datetime\nfrom datetime import datetime\nimport collections\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0c2c9844":"data = pd.read_csv('..\/input\/earthquake.csv')\ndata.info()","c46b38d3":"data.corr()","7dc94f5b":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","f15f143f":"data.head(10)","b4552abd":"a=data.loc[:,\"date\"]                            \nb=data.loc[:,\"time\"]\nprint (a[0], b[0])\ntype(a)                                        \n\ntemp = a+\"_\"+b                               # this is the combined form we would like to achieve\ntimeformat=\"%Y.%m.%d_%H:%M:%S %p\"\n\nnew=[datetime.strptime(x, timeformat) for x in temp]\n\n#for i in temp:\n#    i=datetime.strptime(i,timeformat)\n#why not this way??\n\nprint(\"temp =\",type(temp),\"\\n\"\"new =\",type(new),\"\\n\"\"data.time =\",type(data.date))\n\ndata.time=new\n\ndata.rename(columns={'time': 'newtime'}, inplace=True) \ndel data[\"date\"]                            # we dont need it anymore as all stored in date.time","d9907e92":"print(data.iloc[:5,1])\ndata.head(5)","21eafa0a":"data[\"year\"]=[int(datetime.strftime(x,\"%Y\")) for x in data.newtime]\ndata[\"month\"]=[int(datetime.strftime(x,\"%m\"))+int(datetime.strftime(x,\"%Y\"))*12 for x in data.newtime]\ndata.info()\ndata.head(2)","0becb33c":"tur=data.country == \"turkey\"\nreal=data.richter > 1\n\ncit=data[tur & real].city\ncits=cit.unique()\n\nprint(\"Total Cities =\",cit.size)\n\na=0\nfor i in cits:\n    a=a+1\n    if a==len(cits):\n        print(\"Unique Cities = {}\".format(a))\n\nf=Counter(cit)\nnewf=f.most_common()\n\nprint(type(f))\nprint(type(newf))","48008724":"print(\"So in the table above we can see that, in given date range, {} eq happened in {} cities\" .format(cit.size, a))","f86c10a4":"maks=max(f, key=f.get) \nmost=f.most_common(5)[0]\nmost2=f.most_common(5)[1]\nprint(\"Max number of eq occured in {} with {} eq and second is {} with {}\" .format(maks.upper(),most[1],most2[0].upper(),most2[1]))","937ea15f":"data.head(5)","d52498ab":"def dist(baslik):\n    \n    \n    tur = data.country==\"turkey\"                # There arent many records before 2000 roughly, so lets filter after 1998, also just take magnitudes over 2 \n    richter = data.richter > 2\n    yearfilter = data.year > 1998\n    md = data.md > 2\n    \n    datatr= data[tur & richter & yearfilter & md]\n    \n    plt.figure(figsize=(10,5))\n    plt.hist(datatr[baslik], bins=30, color=\"blue\")\n    plt.ylabel(\"Frequency\")\n    plt.title(baslik)\n\nozet=[\"richter\", \"year\", \"md\", \"xm\",\"lat\",\"long\"]\n    \nfor each in ozet:\n    dist(each)\n    ","2ace30d9":"yearfilter = data.year > 1998\ndata[yearfilter][[\"year\",\"richter\"]].groupby([\"year\"], as_index = False).mean().sort_values(by = \"richter\", ascending = False)","8712eb71":"yearfilter = data.year > 1998\ndata[yearfilter][[\"year\",\"richter\"]].groupby([\"year\"], as_index = False).count().sort_values(by = \"richter\", ascending = False)","b3c20f7b":"data.columns[data.isnull().any()]  ","d8f76a9a":"data.isnull().sum()","6f9fdef6":"**We have some null values however these are irrevelant headers for our analysis so we can ignore**","47316f16":"**As we can see data dont show some of the bif eq in Turkey history which happen in 1999**","5a4e428a":"**First we will convert the csv file to Panda Data Frame**","68327fa3":"**As we can see here we have some null areas in given data like in: \"city\" \"area\" \"direction\" columns**","83a32f5b":"<font color = \"Purple\">\n\n* id:           id of the earthquake\n* date:         date\n* time:         time\n* lat:          latitude (enlem)\n* long:         longitude (boylam)\n* country:      country\n* city:         city\n* area:         area\n* direction:    direction\n* dist:         distance of eq\n* depth:        depth of eq\n* xm:           Biggest magnitude out of MD, ML, Mw, Ms and Mb\n* md:           Duration Magnitude\n* richter:      Richter magnitude or local magnitude (ML)\n* mw:           moment magnitude\n* ms:           surface-wave magnitude\n* mb:           body-wave magnitude","5c96770e":"**First we need to combine both date and time that we can use it as one parameter for each eq**"}}