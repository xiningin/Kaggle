{"cell_type":{"6d38779e":"code","afa643dc":"code","e99676e0":"code","9b98cdfb":"code","afcc1fa1":"code","31df35a0":"code","9082ca17":"code","1a33e807":"code","a1bd6ae5":"code","3cf540bc":"code","92f1565f":"code","a5fac385":"code","143d194f":"code","212224a6":"markdown","a8cdaf91":"markdown","fe8bc3d9":"markdown","e6656373":"markdown","f22ea568":"markdown","db624161":"markdown","7b585e4a":"markdown","6458c196":"markdown","b203b7eb":"markdown","80ffb18a":"markdown","db39d72b":"markdown","76060279":"markdown","d6dd88e3":"markdown"},"source":{"6d38779e":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git -q","afa643dc":"import pandas as pd\n\nfull_train_dataframe = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_dataframe = pd.read_csv('..\/input\/titanic\/test.csv')","e99676e0":"full_train_dataframe.head()","9b98cdfb":"import numpy as np\n\ndef fill_nan(df, mean_age):\n    df['Age'].fillna(value=mean_age, inplace=True)\n    \n# Create training and validation datasets\nval_dataframe = full_train_dataframe.sample(frac=0.2, random_state=1337)\ntrain_dataframe = full_train_dataframe.drop(val_dataframe.index)\nmean_age = np.mean(train_dataframe['Age'])\n    \nprint(\"Total number of training samples: %d\" % (len(full_train_dataframe)))\nprint(\"Total number of test samples: %d\" % (len(test_dataframe)))\nprint(\n    \"Using %d samples for training and %d for validation\"\n    % (len(train_dataframe), len(val_dataframe))\n)\n\nfill_nan(train_dataframe, mean_age)\nfill_nan(val_dataframe, mean_age)\nfill_nan(full_train_dataframe, mean_age)\nfill_nan(test_dataframe, mean_age)","afcc1fa1":"import tensorflow as tf\n\ndef dataframe_to_dataset(dataframe, train=True):\n    dataframe = dataframe.copy()\n\n    # Drop useless features\n    dataframe.pop(\"Cabin\")\n    dataframe.pop(\"Name\")\n    dataframe.pop(\"Ticket\")\n    dataframe.pop(\"Embarked\")\n    dataframe.pop(\"PassengerId\")\n    \n    if train:\n        # Set aside labels\n        labels = dataframe.pop(\"Survived\")\n        # Create dataset\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    return ds\n\ntrain_ds = dataframe_to_dataset(train_dataframe)\nval_ds = dataframe_to_dataset(val_dataframe)\ntest_ds = dataframe_to_dataset(test_dataframe, train=False)\nfull_train_ds = dataframe_to_dataset(full_train_dataframe)\n\n# Visualize the names and types of the features in one sample\nfor sample in train_ds.take(1):\n    for key in sample[0].keys():\n        print('Feature:', key, '- dtype:', sample[0][key].dtype.name)\n        \n# Batch the datasets and configure prefetching\ntrain_ds = train_ds.batch(32).prefetch(32)\nval_ds = val_ds.batch(32).prefetch(32)\ntest_ds = test_ds.batch(32).prefetch(32)\nfull_train_ds = full_train_ds.batch(32).prefetch(32)","31df35a0":"from tensorflow import keras\n\n# Numerical features\nage = keras.Input(shape=(1,), name='Age')\nfare = keras.Input(shape=(1,), name='Fare')\n\n# Integer categorical features\npclass = keras.Input(shape=(1,), name='Pclass', dtype='int64')\nsibsp = keras.Input(shape=(1,), name='SibSp', dtype='int64')\nparch = keras.Input(shape=(1,), name='Parch', dtype='int64')\n\n# String categorical features\nsex = keras.Input(shape=(1,), name='Sex', dtype='string')","9082ca17":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup\nfrom tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\n\n\ndef encode_numerical_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature\n\n\ndef encode_categorical_feature(feature, name, dataset):\n    # Create a Lookup layer which will turn strings into integer indices\n    if feature.dtype.name == 'string':\n        index = StringLookup()\n    else:\n        index = IntegerLookup()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the set of possible feature values and assign them a fixed integer index\n    index.adapt(feature_ds)\n\n    # Turn the values into integer indices\n    encoded_feature = index(feature)\n\n    # Create a CategoryEncoding for our integer indices\n    encoder = CategoryEncoding(output_mode=\"binary\")\n\n    # Prepare a dataset of indices\n    feature_ds = feature_ds.map(index)\n\n    # Learn the space of possible indices\n    encoder.adapt(feature_ds)\n\n    # Apply one-hot encoding to our indices\n    encoded_feature = encoder(encoded_feature)\n    return encoded_feature\n","1a33e807":"# Numerical features\nencoded_age = encode_numerical_feature(age, name='Age', dataset=train_ds)\nencoded_fare = encode_numerical_feature(fare, name='Fare', dataset=train_ds)\n\n# Integer categorical features\nencoded_pclass = encode_categorical_feature(pclass, name='Pclass', dataset=train_ds)\nencoded_sibsp = encode_categorical_feature(sibsp, name='SibSp', dataset=train_ds)\nencoded_parch = encode_categorical_feature(parch, name='Parch', dataset=train_ds)\n\n# String categorical features\nencoded_sex = encode_categorical_feature(sex, name='Sex', dataset=train_ds)","a1bd6ae5":"from tensorflow.keras import layers\n\ninputs = [age, fare, pclass, sibsp, parch, sex]\nfeatures = layers.concatenate([encoded_age, encoded_fare, encoded_pclass, encoded_sibsp, encoded_parch, encoded_sex])\n\ndef make_model(hp):\n    num_dense = hp.Int('num_dense', min_value=1, max_value=3, step=1)\n    x = features\n    for i in range(num_dense):\n        units = hp.Int('units_{i}'.format(i=i), min_value=8, max_value=256, step=8)\n        x = layers.Dense(units, activation='relu')(x)\n    outputs = layers.Dense(1)(x)\n    model = keras.Model(inputs, outputs)\n\n    learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-3)\n    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                  optimizer=optimizer,\n                  metrics=[keras.metrics.BinaryAccuracy(name='acc')])\n    model.summary()\n    return model","3cf540bc":"import kerastuner as kt\n\ntuner = kt.tuners.RandomSearch(\n    make_model,\n    objective='val_acc',\n    max_trials=100,\n    overwrite=True)\n\ncallbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=3)]\ntuner.search(train_ds, validation_data=val_ds, callbacks=callbacks, epochs=100)","92f1565f":"best_hp = tuner.get_best_hyperparameters()[0]\nmodel = make_model(best_hp)\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=100)","a5fac385":"val_acc_per_epoch = history.history['val_acc']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))\nmodel = make_model(best_hp)\nmodel.fit(full_train_ds, epochs=best_epoch)","143d194f":"import numpy as np\n\npredictions = tf.nn.sigmoid(model.predict(test_ds)).numpy()\npassenger_ids = test_dataframe.pop(\"PassengerId\")\nsubmission = pd.DataFrame({\"PassengerId\": passenger_ids,\n                           \"Survived\": np.ravel(np.round(predictions))})\nsubmission.to_csv(\"submission.csv\", index=False)","212224a6":"On the Kaggle CPU runtime, trying out 100 models takes 2 minutes. At the end of the search, our best validation accuracy is 84.8%.","a8cdaf91":"## Run random search over the search space\n\nWe run random search over this hyperparmater search space.","fe8bc3d9":"## Get the data\n\nNext, we load the data using Pandas.","e6656373":"We encode our features.","f22ea568":"We prepare Keras Inputs for the different features in the data.","db624161":"# Prepare a training and validation dataset\n\nNow, let's split the training data into a training split and a validation split.\n\nWe're going to be using the features Pclass, Sex, Age, SibSp, Parch, Fare.\nWe will drop the features Cabin, Name, PassengerId, Ticket, Embarked.\nBecause the feature Age contains NaN values, we will replace these values with the mean of the values found for this feature in the training data.","7b585e4a":"## Encode the features","6458c196":"# Keras and Keras Tuner best practices\n\nThis notebook presents how to use KerasTuner to find a high-performing model in just a few lines of code.\n\nFirst, let's start by installing the latest KerasTuner version:","b203b7eb":"We set up utilities to encode these features, using Keras Preprocessing Layers.","80ffb18a":"We turn the Pandas dataframes into TF Datasets.","db39d72b":"## Train the production model\n\nFinally, we can train the best model configuration from scratch for the optimal number of epochs.\n\nThis time, we train on the entirety of the training data -- no validation split. Our model parameters are already validated.","76060279":"## Find the best epoch\n\nNow, we can retrieve the best hyperparameters, use them to build the best model, and train the model for 100 epochs to find at which epoch training should stop.","d6dd88e3":"## Prepare a KerasTuner search space\n\nWe prepare a hyperparameter search space to find the best model to build on top of these features."}}