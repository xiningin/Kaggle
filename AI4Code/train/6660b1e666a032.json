{"cell_type":{"5ed5750f":"code","9a4f89f4":"code","81d5972d":"code","30b59ea9":"code","84cb5ff6":"code","a7ddcefd":"code","e85cdb36":"code","ca9d5249":"code","c309b510":"code","6bb6ad69":"code","6ae80d7f":"code","42d2a08f":"code","e1121dd7":"code","4d004977":"code","89e2970a":"code","604b022d":"code","fce47d47":"code","03bc7981":"code","7380d1d2":"code","83404a8d":"code","97dcf8c2":"code","63ee4665":"code","ef92f821":"code","7a7ef547":"code","f9a2de38":"code","b2da5df7":"code","7fa68502":"code","e7af13d9":"code","6d9b2718":"code","f3dc8043":"markdown","c9774391":"markdown","21b28b0e":"markdown","e2c0abf4":"markdown","bd3ae49b":"markdown","6367e875":"markdown","c6e4116b":"markdown","e62acf2f":"markdown","c4773d4c":"markdown","58d6dc53":"markdown","44e48d2e":"markdown","16be763e":"markdown","3892201d":"markdown","3cbc17d1":"markdown","f8802dd2":"markdown","4fb3f7fb":"markdown","3dc496ba":"markdown"},"source":{"5ed5750f":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing import text, sequence\nfrom gensim.models import KeyedVectors\nimport time","9a4f89f4":"# capture start time for metrics\nstart = time.time()\n# show all columns when printing out tables\npd.options.display.max_columns = None\n","81d5972d":"train_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv', nrows=100)\ntrain_df.head()","30b59ea9":"test_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=100)\ntest_df.head()","84cb5ff6":"cols_for_pairwise_correlation = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability', 'jewish', 'latino', 'male', 'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity', 'other_religion', 'other_sexual_orientation', 'physical_disability', 'psychiatric_or_mental_illness', 'transgender', 'white', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit', 'identity_annotator_count', 'toxicity_annotator_count']\ntarget_col_for_pairwise_correlation = 'target'\npairwise_correlation = []\n\nfor col in cols_for_pairwise_correlation:\n    # Compute pairwise correlation of columns, excluding NA\/null values.\n    corr = train_df[col].corr(train_df[target_col_for_pairwise_correlation])\n    #print(\"Correlation between {} and {}: {:f}\".format(target_col, col, corr))\n    pairwise_correlation.append({'Score': corr, 'Feature': col})\n\ncorrelation_df = pd.DataFrame(pairwise_correlation)\ncorrelation_df.sort_values(by=['Score'], ascending=False)","a7ddcefd":"print(\"Average comment length:\", train_df.comment_text.str.len().mean())\nprint(\"Max comment length:\", train_df.comment_text.str.len().max())","e85cdb36":"# Full data set takes a long time to plot \n# train_df['target'].plot.kde()\ntrain_df['target'].head(10000).plot.kde()","ca9d5249":"# Let's select the data that we're going to use in our experiment\n\nx_train = train_df['comment_text'].astype(str)\nx_test = test_df['comment_text'].astype(str)\ny_train = train_df['target'].values\n\n#Have the user do this ?\n# \ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n","c309b510":"# Convert values for most common identity labels to boolean\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nfor column in IDENTITY_COLUMNS + ['target']:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False) \n    \n#Have the user do this\n# output: train_df.head()","6bb6ad69":"\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0' \n\n# Let's first break down comment texts\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\n\n\n# call tokenizer method that fits on x_train and x_test, then display word_index dictionary for tokenizer, what can you observe?\n#Have the user do this\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n# fit_on_texts Updates internal vocabulary based on a list of texts. \n# It creates the vocabulary index based on word frequency, the lower value the more frequent the word is (value 0 is reserved for padding). \n# eg \"The cat sat on the mat.\" => will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 ..\n\n# display the dictionary of words created by tokenizer\n#Have the user do this\n# : tokenizer.word_index","6ae80d7f":"# Transform each comment_text to a sequence of integers. Each word in the comment should be replaced \n# with its corresponding integer value from the word_index dictionary. Then, display a sample comment\n#Have the user do this\nx_train = tokenizer.texts_to_sequences(x_train)\n\n# display the second comment in the training set: x_train[1]","42d2a08f":"# do the same for the test data\n# \nx_test = tokenizer.texts_to_sequences(x_test)\n#Have the user do this\n# display the second comment in the test set: x_test[1]","e1121dd7":"# We will be processing comment vectors so it's useful to make them the same length.\n# pad_sequences is used to ensure that all sequences in a list have the same length. \n# By default this is done by padding 0 in the beginning of each sequence \nMAX_LEN = 297\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n#Have the user do this\n# display a vector from the test sequence : x_train","4d004977":"#Have the user do this\n# do the same for text\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\n#Have the user do this\n# display a vector from the test sequence : x_test","89e2970a":"# We converted comments to vectors of numbers \n#\n# What about the meaning of words used in the comments?\n# \nEMBEDDING_FILES = [\n    '..\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim',\n    '..\/input\/gensim-embeddings-dataset\/glove.840B.300d.gensim'\n]\n\n# Lets load one of the embedding files to see what's there\nembeddings = KeyedVectors.load('..\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim', mmap='r')\n#\n# How are words represented in a numerical space?\n#\n# Display a word vector for a word 'apple'\n#Have the user do this\n#  embeddings.word_vec(\"apple\")","604b022d":"# What's the length of that vector? \n#Have the user do this\n# embeddings.word_vec(\"apple\").size","fce47d47":"# What is the \"distance\" between two words?\n#\nw1 = embeddings.word_vec('king')\nw2 = embeddings.word_vec('queen')\n\ndist = np.linalg.norm(w1-w2)\n\nprint(dist)","03bc7981":"# Calculate 'king' - 'man' + 'woman' = ?\n\n#Have the user do this\n# embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n","7380d1d2":"# What else can we do using embeddings?\n#\n# check which word doesn't match the rest \"cat mouse rose dog\"\n#Have the user do this\n# embeddings.doesnt_match(\"cat mouse rose dog\".split())","83404a8d":"#Have the user do this\nembeddings.similarity('cat', 'dog')","97dcf8c2":"#Have the user do this\nembeddings.similarity('cat', 'car')","63ee4665":"# Our model we're be building will require embedding information for the words used in the comments\n# We will construct an embedding_matrix for the words in word_index\n# using pre-trained embedding word vectors from resource in path\ndef build_matrix(word_index, path):\n    # we've seen the embeddings already\n    embedding_index = KeyedVectors.load(path, mmap='r')\n    \n    # embedding_matrix is a matrix of len(word_index)+1  x 300\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    # use word_index to get the index of each word \n    # and place its embedding in the matrix at that index\n    for word, i in word_index.items():\n        for candidate_word in [word, word.lower()]:\n            if candidate_word in embedding_index:\n                embedding_matrix[i] = embedding_index[candidate_word]\n                break\n    return embedding_matrix\n\n# concatenate results for each type of embedding\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n#Have the user do this\n# display the embedding matrix\n# embedding_matrix","ef92f821":"# To address the bias aspect we will create a set of weights that will reduce significance of samples containing identity words\n# Build an array to provide a weight for each training sample. \n# (we'll indicating the weight for each of those samples during the training)\nsample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df['target'] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df['target']) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\nsample_weights \/= sample_weights.mean()","7a7ef547":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D\n\n\n#units parameters in Keras.layers.LSTM\/cuDNNLSTM\n#it it the dimension of the output vector of each LSTM cell.\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n\n# embedding_matrix: The matrix we created in the preprocessing stage\n# num_aux_targets: The number of auxiliary target variables we have.\ndef build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    \n    #The Keras Embedding will turn our matrix of indexes into dense vectors of fixed size\n    #The first parameter represents our max index integer \n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    #Randomlly drop 0.2% of input values to help prevent overfitting (This prevents units from coadapting too much)\n    x = SpatialDropout1D(0.2)(x)\n    \n    #CuDNNLSTM is a LSTM implementation using the Nvidia cuDNN library \n    #The LSTM units is the dimension of the output vector of each LSTM cell\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        #Global max pooling has a pool size equal to the size of the input.\n        GlobalMaxPooling1D()(x),\n        GlobalMaxPooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model\n\n\n\n\n","f9a2de38":"# This is user Input\nmodel = build_model(embedding_matrix, y_aux_train.shape[-1])\n","b2da5df7":"#Use the summary function on the model we created above to help see a summary of the different layers in the model\n\n#This is user input\nmodel.summary()","7fa68502":"from keras.utils import plot_model\n\n#This is user input\nplot_model(model)","e7af13d9":"#this is the number of training samples to put in the model each step\nBATCH_SIZE = 512\n\nEPOCHS = 4\ncheckpoint_predictions = []\nweights = []\nfor global_epoch in range(EPOCHS):\n    model.fit(\n        x_train,\n        [y_train, y_aux_train],\n        batch_size=BATCH_SIZE,\n        epochs=1,\n        verbose=2,\n        sample_weight=[sample_weights.values, np.ones_like(sample_weights)]\n        )\n    checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n    weights.append(2 ** global_epoch)\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n\nsubmission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(predictions)\nend = time.time()\nprint(\"Total time taken:\", end - start)","6d9b2718":"#This is user input\n#print(test_df.comment_text[#])\n\nindex = 0\nfor prediction in predictions:\n    if prediction > 0.60:\n        print(prediction, index)\n    index = index + 1\n\n","f3dc8043":"Let's now call our build_model function:","c9774391":"Exploring the comment_text column ","21b28b0e":"**Clean up data**","e2c0abf4":"You can see the above model prints out two output layers, why does this make sense for our model?","bd3ae49b":"**Distribution**\n\nVisualize the distribution of the target variable using Kernel Density Estimate Plot.\nIn statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function (PDF) of a random variable. This function uses Gaussian kernels and includes automatic bandwidth determination.","6367e875":"We can see another representation of our model by using a Keras utility function called plot_model and passing in our model","c6e4116b":"**Preprocessing**","e62acf2f":"**Understand Data**\n\n\n","c4773d4c":"**Read in data**\n\n> At the end of 2017 the Civil Comments platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data\n\nWe can read in the input data files by storing them in a Pandas dataframes.\nYou can use the dataframe method 'head(n)' to view the first n rows of a dataframe","58d6dc53":"How do you think we did with our predictions? Let's compare our predictions with the original test comments and see if our predictions seem right.\n\nRecall that test_df was our original dataframe with the comments. Try playing around with the below for loop to compare comments with our predictions. Do they make sense?","44e48d2e":"**Visualizing Our Model**\n\nWe have now constructed a model that we can fit with our data. Before we do this, we can visualize our model to get a better understanding of what we just did.\n\nThe Keras Model has a summary() function that shows the layers of the model we've created:","16be763e":"Use tokenizer to create dictionary of words","3892201d":"We can now train our model and do some prediction. In the below code, we loop over our data 4 times, training and predicting on it each time, and then taking the average of those predictions.","3cbc17d1":"**Machine Learning Tasks**\n* Problem statement\n* Read in Data\n* Understand Data\/Visualize Data\n* Clean Data\n* Preprocessing\n    * Tokenization\n    * Embedding\n* Build Model\n* Train\n* Prediction \n* Problems with models","f8802dd2":"**Problem statement**\n\n> \"Can you help detect toxic comments \u2015 and minimize unintended model bias? That's your challenge in this competition.\n> \n> The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.\"\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/overview","4fb3f7fb":"**Correlation** \n\nRelationship between each feature and target variable:\n*     Positive Correlation: both variables change in the same direction.\n*     Neutral Correlation: No relationship in the change of the variables.\n*     Negative Correlation: variables change in opposite directions.","3dc496ba":"**Building Our Model**\n\nNow that we've preproccessed the data, it's time to build and train a model with the embeddings we created so that we can make predictions on comments.\nIn the below function, we create the different layers of the model using the Keras.layers libray."}}