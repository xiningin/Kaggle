{"cell_type":{"ff78bba4":"code","6b1c0744":"code","f7a2cf50":"code","ef88df13":"code","3117e5c6":"code","e66c9f14":"code","46d1f7f8":"code","93bb70d6":"code","27f043cb":"code","1b557064":"code","4c979b08":"code","6a793495":"code","0a9d5608":"markdown","aa3d439f":"markdown"},"source":{"ff78bba4":"!pip install --upgrade scikit-learn --progress-bar off","6b1c0744":"import numpy as np\nimport pandas as pd\n\n# Set display to diagram so pipelines can be shown better\nfrom sklearn import set_config\nset_config(display = 'diagram')","f7a2cf50":"full_df = pd.read_csv('..\/input\/chennai-house-price\/clean_data.csv')\nfull_df.tail()","ef88df13":"# Check for missing data.\n# Apparently we'll need imputation.\nfull_df.isnull().sum()","3117e5c6":"# Split dataset before anything else, so leakage of information is\n# as little as possible.\nfrom sklearn.model_selection import train_test_split\nX, X_test, y, y_test = train_test_split(full_df.drop('price', axis = 1), full_df['price'], test_size = 0.3)","e66c9f14":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import VotingRegressor, ExtraTreesRegressor\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# One-hot encode the columns with type \"object\"\n# In our case now this means one-hot encoding the string (categorical) columns\nencode = make_column_transformer(\n    (OneHotEncoder(handle_unknown = 'ignore', sparse = False), make_column_selector(dtype_include = object)),\n    remainder = \"passthrough\"\n)\n\n# Make a multivariate iterative imputer\n# More powerful than SimpleImputer\nimpute_estimator = ExtraTreesRegressor(n_estimators = 11)\nimpute = IterativeImputer(impute_estimator)\n\n# Define two models for prediction\nlinear = ElasticNet(max_iter = 10_000)\nknn = KNeighborsRegressor()\n\n# Implement a voting scheme for ensembling the two models\nmeta = VotingRegressor([\n    ('linear', linear),\n    ('knn', knn)\n])\n\n# Wrap everything in a pipeline.\n# This makes the process of fitting and predicting very simple\n# and free of accidental data leakage.\npipeline = make_pipeline(encode, impute, meta)\npipeline","46d1f7f8":"from sklearn.model_selection import GridSearchCV, RepeatedKFold\n\n# My choice of hyperparameter is so narrow and finegrained\n# because I have tried more general grids outside of this specific\n# version of notebook. If you're just starting out fresh on a new\n# dataset, you should try wider or more general ranges.\nparams = {\n    \"votingregressor__knn__n_neighbors\": [1, 3, 4],\n    \"votingregressor__linear__alpha\": [0.001, 0.003, 0.005],\n    \"votingregressor__linear__l1_ratio\": [0.875, 0.9, 0.925]\n}\n\nrkf = RepeatedKFold(n_splits = 2, n_repeats = 2)\n\n# Use grid search to find best hyperparameters.\n# By setting refit = False the grid search will not fit a full model after\n# finding best hyperparameters.\n# Instead, we will get the best hyperparameters and set it manually into\n# our pipeline.\ngs = GridSearchCV(pipeline, params, cv = rkf, scoring = 'neg_mean_absolute_error',\n                  verbose = 3, refit = False)\ngs.fit(X, y)\ngs.best_params_","93bb70d6":"pipeline.set_params(**gs.best_params_)","27f043cb":"# Simple variation:\n# Fit once on training set, predict once on test set.\nsimple_prediction = pipeline.fit(X, y).predict(X_test)","1b557064":"# \"Blended\" variation\n# This is popular in competitions:\n# Fitting the model multiple times on different subset of data each,\n# but predicting the same test set. At the end of the day, the predictions\n# are averaged.\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_absolute_error\n\ntest_predictions = []\nsplitter = RepeatedKFold(n_splits = 4, n_repeats = 5)\n\nfor fold, (idx_train, idx_val) in enumerate(splitter.split(X, y)):\n    print(\"FOLD:\", fold, \"===================\")\n    X_train, X_val = X.iloc[idx_train,:], X.iloc[idx_val,:]\n    y_train, y_val = y.iloc[idx_train], y.iloc[idx_val]\n    \n    pipeline.fit(X_train, y_train)\n    \n    val_pred = pipeline.predict(X_val)\n    val_score = mean_absolute_error(y_val, val_pred)\n    print(\"Validation score:\", val_score)\n    \n    test_pred = pipeline.predict(X_test)\n    test_predictions.append(test_pred)","4c979b08":"test_predictions = np.stack(test_predictions, axis = 1)\nblended_prediction = np.mean(test_predictions, axis = 1)","6a793495":"# Moment of truth:\n# How does our little experiment go?\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nprint(\"SIMPLE VARIATION ================\")\nprint(\"R2: \", r2_score(y_test, simple_prediction))\nprint(\"MAE:\", mean_absolute_error(y_test, simple_prediction))\n\nprint(\"BLENDED VARIATION ================\")\nprint(\"R2: \", r2_score(y_test, blended_prediction))\nprint(\"MAE:\", mean_absolute_error(y_test, blended_prediction))","0a9d5608":"# Introduction\n\nThis notebook will walk you through the process of making a powerful pipeline that handles all the flow of data from raw to prediction: transformation, imputation, and inference, all in one `fit` and `predict` call.\n\nBuilding the pipeline in the first place *is* the fun part. Each steps are annotated with helpful comments.\n\nEnjoy!","aa3d439f":"# Closing\n\nThere you have it! I hope you learn something from this little notebook.\n\nFeel free to upvote and fork if you'd like to try this yourself. Don't hesitate to use the comments section if you want to suggest improvements or ask about something.\n\nKeep learning and happy data-sciencing!"}}