{"cell_type":{"b08224da":"code","871b1163":"code","433811d8":"code","441e6e0a":"code","aa8a20ed":"code","ce52c378":"code","4af1a671":"code","7485bc04":"code","fa254472":"code","763bea93":"code","8b91c1b9":"code","9d39aee3":"code","76f78196":"code","31cf1cde":"code","24ab0058":"code","48d2ecf4":"code","c4310d19":"code","31b46460":"code","27a74eda":"code","b9a7fe7f":"code","0e064799":"code","0a5df094":"code","372480ce":"code","f712c895":"code","f450e219":"code","8465dad0":"code","0ca0235f":"code","b1914593":"code","a0bc5554":"code","790fd0bb":"code","34ffb80f":"code","3f51c1a1":"code","d5b56848":"code","d611ab16":"code","4ebb9df6":"code","012e21f1":"code","e5a934a2":"code","acf06db3":"code","4fa7d9b4":"code","cd4be245":"code","1316b227":"code","7ab105fd":"code","f1632124":"code","04ec246d":"code","1923ef59":"code","bbdd5328":"code","193ffd76":"code","81907613":"code","f72eec88":"code","166ceb7a":"code","44e6f47f":"code","cc8572c7":"code","fbaa8826":"code","9913ab75":"code","9aaa18e3":"code","3648b581":"code","d3b8da42":"code","917b388d":"code","009673d0":"code","e33aaaef":"code","0bbee257":"code","f25a72d8":"code","55082e66":"code","7d70f633":"code","d42c259d":"code","c3e55cee":"code","63d10336":"code","de80f86a":"code","c4c22269":"code","6e9b7a41":"code","e4c75e6f":"code","bcc4ad1b":"markdown","43e2cc36":"markdown","7b08c61e":"markdown","f4d403ed":"markdown","259776a9":"markdown","7e351192":"markdown","b47a54d2":"markdown","2da5d0d5":"markdown","15c05be8":"markdown","149091f9":"markdown","f2d664c1":"markdown","0698cf78":"markdown","8ab4f9c5":"markdown","2f43bdd5":"markdown","948abcda":"markdown","8421f03b":"markdown"},"source":{"b08224da":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import style\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","871b1163":"style.available","433811d8":"style.use('seaborn-darkgrid')","441e6e0a":"data = pd.read_csv(\"..\/input\/50-startups\/50_Startups.csv\")","aa8a20ed":"data.head()","ce52c378":"data.info()","4af1a671":"data.describe()","7485bc04":"data.columns","fa254472":"continuous=list(data.columns)\ncontinuous.remove(\"State\")\ncontinuous","763bea93":"figure, axes = plt.subplots(nrows=4, ncols=1, figsize=(20,40))\nfor col in continuous:\n    i = continuous.index(col)\n    sns.distplot(data[col], ax=axes[i], bins=13, kde_kws={\"lw\":3}, color=\"mediumslateblue\")\n    axes[i].set_xlabel(axes[i].get_xlabel(), fontsize=20)\n    axes[i].set_ylabel(axes[i].get_ylabel(), fontsize=10)\nfigure.suptitle(\"Distributions of Continuous Variables\", fontsize=35, y=0.92)\nplt.show()","8b91c1b9":"plt.figure(figsize=(15,7))\nsns.countplot(data=data, x=\"State\", palette=\"cool\")\nplt.title(\"Distribution of the State variable\", fontsize=30)\nplt.show()","9d39aee3":"plt.figure(figsize=(20,10))\nsk=dict()\nfor col in data.columns:\n    if col!=\"State\":\n        skewness = data[col].skew()\n        print(f\"Skewness of {col}: {skewness}\")\n        sk[col]=skewness\nsns.barplot(x=list(sk.keys()), y=list(sk.values()), palette=\"cool\")\nplt.title(\"Skewness of Continuous Variables\", fontsize=30)\nplt.show()","76f78196":"plt.figure(figsize=(20,10))\nkr=dict()\nfor col in data.columns:\n    if col!=\"State\":\n        kurtosis = data[col].kurt()\n        print(f\"Kurtosis of {col}: {kurtosis}\")\n        kr[col]=kurtosis\nsns.barplot(x=list(kr.keys()), y=list(kr.values()), palette=\"cool\")\nplt.title(\"Kurtosis of Continuous Variables\", fontsize=30)\nplt.show()","31cf1cde":"plt.figure(figsize=(20,8))\nsns.boxplot(y=\"Profit\", data=data, x=\"State\", palette=\"cool\")\nplt.xlabel(None)\nplt.ylabel(\"Profit\",fontsize=20)\nplt.title(\"Profit Distribution per State\", fontsize=30)\nplt.show()","24ab0058":"g=sns.pairplot(data, hue=\"State\", palette=\"Set1\", height=2.5)\ng.fig.suptitle(\"2x2 Continuous Distributions by State\\n\", fontsize=15, y=1)\nplt.show()","48d2ecf4":"fig = plt.figure(figsize=(20,10))\ncorr = data.corr()\nh = sns.heatmap(corr, annot=True, cmap=\"BuPu\",annot_kws={'fontsize':20},fmt='.2%')\nplt.title(\"Pearson Correlation between Continuous Features\\n\", fontsize=30)\nplt.show()","c4310d19":"from sklearn.feature_selection import mutual_info_classif as mic\nfrom sklearn.feature_selection import mutual_info_regression as mir\nfrom sklearn.metrics import mutual_info_score as mis","31b46460":"midata=pd.DataFrame(data.values, index=data.index, columns=data.columns)\nmidata[\"State\"]=midata[\"State\"].map({\"California\":0, \"Florida\":1, \"New York\":2})\nmat = np.empty(shape=(5,5))\ncols = list(data.columns)\nfor index in range(25):\n    i, j = index\/\/5, index%5\n    coli = midata[cols[i]].values.reshape(-1,1)\n    colj = midata[cols[j]].values.reshape(-1,1)\n    if (cols[i]==\"State\" and cols[j]==\"State\"):\n        mi = mis(coli.reshape(50,), colj.reshape(50,))\n    elif (cols[i]==\"State\"):\n        mi = mic(colj,coli)\n    elif cols[j]=='State':\n        mi = mic(coli, colj)\n    else:\n        mi = mir(coli,colj)\n    mat[i,j]=mi\nmi_mat = pd.DataFrame(mat, index=data.columns, columns=data.columns)\nfor col in data.columns:\n    mi_mat.loc[col,col]=2.6\nmi_mat = mi_mat\/2.6","27a74eda":"plt.figure(figsize=(20,10))\nsns.heatmap(mi_mat, annot=True, annot_kws={\"fontsize\":20}, fmt=\".2\", cmap=\"BuPu\")\nplt.title(\"Normalized Mutual Information between Features\\n\", fontsize=30)\nplt.show()","b9a7fe7f":"data = pd.get_dummies(data, drop_first=True)","0e064799":"data.head()","0a5df094":"from umap import UMAP","372480ce":"reducer = UMAP(random_state=11)","f712c895":"from sklearn.preprocessing import RobustScaler","f450e219":"rs = RobustScaler()","8465dad0":"datarr = rs.fit_transform(data.drop(\"Profit\",axis=1))","0ca0235f":"umap_output = reducer.fit_transform(datarr)","b1914593":"plt.figure(figsize=(20,10))\nsns.scatterplot(x=umap_output[:,0], y=umap_output[:,1], hue=data[\"Profit\"], palette=\"cubehelix_r\", s=1000)\nplt.title(\"UMAP\", fontsize=30)\nplt.show()","a0bc5554":"from sklearn.model_selection import train_test_split","790fd0bb":"rs = RobustScaler()","34ffb80f":"xtrain, xtest, ytrain, ytest = train_test_split(data.drop(\"Profit\", axis=1), data.Profit, test_size=0.2, random_state=0)","3f51c1a1":"xtrain = rs.fit_transform(xtrain)\nxtest = rs.transform(xtest)","d5b56848":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet","d611ab16":"lin_reg, ridge_reg, lasso_reg, elas_net = LinearRegression(), Ridge(), Lasso(), ElasticNet()","4ebb9df6":"names = ['LinReg','Ridge', 'Lasso', 'ElasticNet']\nmodel_list = [lin_reg, ridge_reg, lasso_reg, elas_net]","012e21f1":"models = dict()\nfor n in names:\n    models[n]=model_list[names.index(n)]","e5a934a2":"from sklearn.metrics import r2_score","acf06db3":"scores = pd.DataFrame(data=np.empty(shape=(4,2)), index=models.keys(), columns=[\"Train\",\"Test\"])\nfor model, name in zip(models.values(), models.keys()):\n    model.fit(xtrain, ytrain)\n    ytrpred=model.predict(xtrain)\n    ytspred=model.predict(xtest)\n    train_r2 = r2_score(ytrain, ytrpred)\n    test_r2 = r2_score(ytest, ytspred)\n    scores.loc[name,'Train']=train_r2\n    scores.loc[name,'Test']=test_r2","4fa7d9b4":"scores","cd4be245":"import statsmodels.api as sm","1316b227":"xtr = sm.add_constant(xtrain)","7ab105fd":"xts = sm.add_constant(xtest)","f1632124":"model = sm.OLS(ytrain, xtr).fit()","04ec246d":"model.summary()","1923ef59":"model.pvalues","bbdd5328":"xtr = np.delete(xtr, 5, 1)","193ffd76":"model = sm.OLS(ytrain, xtr).fit()","81907613":"model.summary()","f72eec88":"model.pvalues","166ceb7a":"xtr = np.delete(xtr, 4, 1)","44e6f47f":"model = sm.OLS(ytrain, xtr).fit()","cc8572c7":"model.summary()","fbaa8826":"model.pvalues","9913ab75":"xtr = np.delete(xtr, 2, 1)","9aaa18e3":"model = sm.OLS(ytrain, xtr).fit()","3648b581":"model.summary()","d3b8da42":"model.pvalues","917b388d":"xtr=np.delete(xtr,2,1)","009673d0":"model = sm.OLS(ytrain, xtr).fit()","e33aaaef":"model.summary()","0bbee257":"model.pvalues","f25a72d8":"spend = data[\"R&D Spend\"]","55082e66":"xtr = np.delete(xtr, 0, 1)","7d70f633":"xts.shape","d42c259d":"xts = np.delete(xts, [0,2,3,4,5] ,1)","c3e55cee":"scores_after_selection = pd.DataFrame(data=np.empty(shape=(4,2)), index=models.keys(), columns=[\"Train\",\"Test\"])\nfor model, name in zip(models.values(), models.keys()):\n    model.fit(xtr, ytrain)\n    ytrpred=model.predict(xtr)\n    ytspred=model.predict(xts)\n    train_r2 = r2_score(ytrain, ytrpred)\n    test_r2 = r2_score(ytest, ytspred)\n    scores_after_selection.loc[name,'Train']=train_r2\n    scores_after_selection.loc[name,'Test']=test_r2","63d10336":"print(f\"R2_Scores before Elimination:\\n\\n{scores}\\n\\n\\n\\n\")\nprint(f\"R2_Scores after Elimination:\\n\\n{scores_after_selection}\")","de80f86a":"umap_tr, umap_ts, ytrain, ytest = train_test_split(umap_output, data[\"Profit\"], random_state=112)","c4c22269":"umap_tr = rs.fit_transform(umap_tr)\numap_ts = rs.transform(umap_ts)","6e9b7a41":"scores_on_umap = pd.DataFrame(data=np.empty(shape=(4,2)), index=models.keys(), columns=[\"Train\",\"Test\"])\nfor model, name in zip(models.values(), models.keys()):\n    model.fit(umap_tr, ytrain)\n    ytrpred=model.predict(umap_tr)\n    ytspred=model.predict(umap_ts)\n    train_r2 = r2_score(ytrain, ytrpred)\n    test_r2 = r2_score(ytest, ytspred)\n    scores_on_umap.loc[name,'Train']=train_r2\n    scores_on_umap.loc[name,'Test']=test_r2","e4c75e6f":"scores_on_umap","bcc4ad1b":"#### Note:\nI scaled variables after splitting in order to avoid data leakage.","43e2cc36":"All features are approximately symmetric: Skewness is less than 0.5 (in absolute value).","7b08c61e":"### Final Note:\nAs the number of samples is extremely low, different random states & seeds result in very different scores.  \nOversampling techniques would be very useful -if not necessary- if this dataset was to be used in a real-life project.","f4d403ed":"# Backward Feature Elimination with Statsmodels","259776a9":"### Visualizing high-dimensional data with UMAP","7e351192":"# Analysis & Processing","b47a54d2":"# Modelling with SKLearn","2da5d0d5":"Kurtosis is significantly less than 3 for all features.  \nWe could say the distributions have somewhat \"light\" tails and thus we shouldn't worry about outliers.","15c05be8":"#### Note:\nI didn't use SKLearn's Normalized Mutual Info function, i normalized the scores myself.  \nI did so by simply dividing all values by the maximum value, which is the one between each variable and itself.  \nThe mutual information between the State variable and itself wasn't equal to the maximum value. This isn't wrong as it's actually entropy, but it could give the illusion that one variable is less dependent on itself than others are on themselves, which is absurd.  \nI dealt with this by simply replacing it with the maximum value, which was 2.6.","149091f9":"# Startup Profit Prediction\n### by: Mahmoud Limam  ","f2d664c1":"We can see that, although training scores decreased, testing scores increased.  ","0698cf78":"### Bonus: Fitting linear models on the output of UMAP","8ab4f9c5":"If the 3 states displayed profit distributions in 3 different ranges then it would be a good idea to do label encoding in such a way that the higher the label the higher the profit.  \nI don't think Target Encoding would be a good idea as all 3 states have basically the same profit average.  \nWe'll go with One Hot Encoding.","2f43bdd5":"The target is quite correlated with \"R&D Spend\" and \"Marketing Spend\".  \nThe data points seem a little scattered though when it comes to the Profit-MarketingSpend plot, as opposed to the one with \"R & D Spend\".","948abcda":"As expected, linear models gave impressive results.","8421f03b":"##### This is very interesting.  \nIf you add a third dimension representing profit, you would see these points somewhat distributed across a plane.  \nThis can be guessed by observing the fact that on the embedding space, the data points increase in profit as you go towards the left, and slightly as you go downwards.  \n#### Thus: A linear model should give very good results."}}