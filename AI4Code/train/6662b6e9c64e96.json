{"cell_type":{"a7ad1643":"code","37866dd4":"code","12ed71c2":"code","06667706":"code","f3a757b3":"code","a0efaae0":"code","d4aeee89":"code","d1461cc3":"code","e616382f":"code","a46ca55d":"code","ebbeabd9":"code","0cd7bfd1":"code","a3576b0b":"code","80cbec3d":"code","ffe6a25d":"code","4442ee09":"code","02e6e6bf":"markdown","8f93ca32":"markdown","275325de":"markdown","963ee35c":"markdown","5db0727a":"markdown","59537ec1":"markdown","e4afed10":"markdown","2c0178db":"markdown","84b84553":"markdown","ab77bc14":"markdown","491a3c8c":"markdown","4632d788":"markdown","3353660c":"markdown","c3f7ee8e":"markdown","68438e9c":"markdown"},"source":{"a7ad1643":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings, gc\nwarnings.filterwarnings(\"ignore\")\n\n# Sklearn Classifier Algorithm\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import NearestCentroid, RadiusNeighborsClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier\n\n# Sklearn (other)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom tabulate import tabulate\n","37866dd4":"url = '..\/input\/all-datasets-for-practicing-ml\/Class\/Class_Abalone.csv'\ndata = pd.read_csv(url, header='infer')","12ed71c2":"# Total Records\nprint(\"Total Records: \", data.shape[0])","06667706":"# Check for empty\/null\/missing records\nprint(\"Is Dataset Empty: \", data.empty)","f3a757b3":"# Records per Classes\ndata.Sex.value_counts()","a0efaae0":"# Instantiating Label Encoder\nencoder = LabelEncoder()\n\n# Columns List\ncolumns = data.columns\n\n# Encode the column \ndata['Sex']= encoder.fit_transform(data['Sex']) \n    ","d4aeee89":"# Inspect\ndata.head()","d1461cc3":"# Feature & Target Selection\ntarget = ['Sex']   \nfeatures = columns [1:]\n\nX = data[features]\ny = data[target]\n\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0, shuffle=True) \n\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_val = sc.transform(X_val)\n","e616382f":"# Instantiate Extra Tree Classifier\net = ExtraTreeClassifier(random_state=1)\n\n# Bagging Classifier\nbgc = BaggingClassifier(et, random_state=1, max_features=8, verbose=0)\n\n# Train \nbgc.fit(X_train, y_train)\n\n# Prediction\ny_pred = bgc.predict(X_val)\n\n# Accuracy\nprint(\"Extra Tree Classifier(Tree Module) Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\n\ntab_data = []\ntab_data.append(['Extra Tree(Tree)', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","a46ca55d":"# Instantiate Classifier\netc = ExtraTreesClassifier(n_estimators=100, max_depth= 5,\n                           verbose=0, random_state=1)\n\n# Train\netc.fit(X_train, y_train)\n\n# Prediction\ny_pred = etc.predict(X_val)\n\n# Accuracy\nprint(\"Extra Tree Classifier(Ensemble Module) Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Extra Tree(Ensemble)', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","ebbeabd9":"# Instantiate Classifier\nmlp = MLPClassifier(random_state=1, max_iter=300,solver='sgd',\n                    batch_size=200, learning_rate='adaptive', learning_rate_init=0.001,\n                    shuffle=True, verbose=0)\n\n# Train\nmlp.fit(X_train, y_train)\n\n# Prediction\ny_pred = mlp.predict(X_val)\n\n# Accuracy\nprint(\"MLP Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['MLP', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","0cd7bfd1":"# Instantiate Classifier\nnc = NearestCentroid()\n\n# Train\nnc.fit(X_train, y_train)\n\n# Prediction\ny_pred = nc.predict(X_val)\n\n# Accuracy\nprint(\"Nearest Centroid Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Nearest Centroid', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","a3576b0b":"# Instantiate Classifier\nqda = QuadraticDiscriminantAnalysis()\n\n# Train\nqda.fit(X_train, y_train)\n\n# Prediction\ny_pred = qda.predict(X_val)\n\n# Accuracy\nprint(\"Quadratic Discriminant Analysis Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Quadratic Discriminant Analysis', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","80cbec3d":"# Instantiate Classifier\nrnc = RadiusNeighborsClassifier(radius=2.0, )\n\n# Train\nrnc.fit(X_train, y_train)\n\n# Prediction\ny_pred = rnc.predict(X_val)\n\n# Accuracy\nprint(\"Radius Neighbours Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Radius Neighbours', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","ffe6a25d":"# Instantiate Classifier\nrc = RidgeClassifier(class_weight='balanced', random_state=1)\n\n# Train\nrc.fit(X_train, y_train)\n\n# Prediction\ny_pred = rc.predict(X_val)\n\n# Accuracy\nprint(\"Ridge Classifier Accuracy: \", '{:.2%}'.format(accuracy_score(y_val, y_pred)))\ntab_data.append(['Ridge Classifier', '{:.2%}'.format(accuracy_score(y_val, y_pred))])","4442ee09":"print(tabulate(tab_data, headers=['Classifiers','Accuracy'], tablefmt='pretty'))","02e6e6bf":"# Data","8f93ca32":"# Classification","275325de":"## Ridge Classifier\n\nClassifier using Ridge regression.\n\nThis classifier first converts the target values into {-1, 1} and then treats the problem as a regression task (multi-output regression in the multiclass case).","963ee35c":"As we can observe, the accuracy of these classifiers are fairely close to each other. The next logical step would be to fine-tune the parameters to increase the accuracy. ","5db0727a":"# Feature Engineering, Data Split & Feature Scaling","59537ec1":"### UPDATED - 18\/08  [Dataset Change]","e4afed10":"## Quadratic Discriminant Analysis\n\nQuadratic Discriminant Analysis\n\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\n\nThe model fits a Gaussian density to each class.","2c0178db":"# Data Prep\n\nWe'll use Label Encoder to convert the 'Sex' column to numerical format for easy ingestion by the algorithms","84b84553":"Some Lesser Known MultiClass Classifier from SKLearn\n\nIn this notebook, we're going to have a look at some of the lesser known **Inherently MultiClass Classifier Algorithm** from SK-Learn library. \n\n* Extra Tree Classifier [sklearn.tree module]\n* Extra Tree Classifier [sklearn.ensemble module]\n* MLP Classifier\n* Nearest Centroid\n* Quadratic Discriminant Analysis\n* Radius Neighbors Classifier\n* Ridge Classifier\n\nWe'll be using the [Abalone Dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/abalone) Dataset for the multi-class classification.\n\nAs always, I'll keep the notebook organized & well commented for easy reading. Please do consider to UPVOTE if you find it helpful.\n","ab77bc14":"## Extra Tree Classifier (Tree Module)\n\nAn extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n\n**Note**: Extra-trees should only be used within ensemble methods.\n\nIn this notebook, will be using another unknown ensemble classifier i.e. BaggingClassifier","491a3c8c":"# Libraries","4632d788":"## Nearest Centroid\n\nNearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to the class with the nearest centroid.","3353660c":"## Extra Tree Classifier (Ensemble Module)\n\nAn extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.","c3f7ee8e":"## MLP Classifier\n\nMulti-layer Perceptron classifier.\n\nThis model optimizes the log-loss function using LBFGS or stochastic gradient descent.","68438e9c":"## Radius Neighbours Classifier\n\nClassifier implementing a vote among neighbors within a given radius"}}