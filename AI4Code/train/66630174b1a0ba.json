{"cell_type":{"c6dc80dd":"code","af8da6e8":"code","585b77ef":"code","ee3cbbb1":"code","7db3dc0c":"code","467fbb32":"code","70d28952":"code","4f91b52e":"code","adea2d71":"code","b95db401":"code","91cdcb8a":"code","bebebf7e":"code","1e05fef8":"code","1ca3a25e":"code","d8015e48":"code","2bc1dc51":"code","479cb4cc":"markdown","2dd6c814":"markdown","10a3f725":"markdown","f8c55107":"markdown","809ff787":"markdown","037938f6":"markdown","464297fc":"markdown","538d6668":"markdown","779a7eed":"markdown","f89f9cdc":"markdown"},"source":{"c6dc80dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af8da6e8":"!pip install scrapy\n!pip install beautifulsoup4\n\n#Scraping libraries\nimport requests\nfrom scrapy.selector import Selector\nfrom bs4 import BeautifulSoup\nimport warnings\n\nimport re\nfrom collections import deque\nfrom time import sleep\n\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import truncnorm","585b77ef":"train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")","ee3cbbb1":"#Parent class\n\nclass Extractor:\n    \"\"\"\n    Parent class with all the attributes that a Extractor must have\n    \n    Parameters\n    ----------\n    url (str):\n        The url to scrape\n    \n    errors (bool): False:\n        If is True, if the response of the page scrape doesn't belong to 20*\n        them the scraping will stop. If it's false will continue.\n    \n    Attributes:\n    -----------\n    response: tell if the scraping is correct or not\n    \n    status_code: tell the code response of the url 200, 300, 400, etc.\n    \n    selector: is the selector for parse the page.\n    \"\"\"\n    \n    def __init__(self, url, errors=False) -> None:\n        self.url = url\n        rq = requests.get(url)\n        self.response = rq.reason\n        self.status_code = rq.status_code\n\n        if \"20\" in str(self.status_code):\n            self.selector = Selector(text=rq.text)\n        elif errors:\n            raise ValueError(\"La p\u00e1gina no tiene un c\u00f3digo 200\")","7db3dc0c":"#Child classes\n\nclass WikipediaExtractor(Extractor):\n    \"\"\"\n    Inhteir from Extractor, scrape the content of wikipedia pages\n    \n    Method:\n    --------\n    parse_text: return to us wikipedia page parsed without HTML text.\n    \"\"\"\n\n    def __init__(self, url, errors=False) -> None:\n        super().__init__(url, errors=False)\n\n    def parse_text(self):\n        #Select the text\n        text_html = self.selector.xpath(\"\/\/div[@class='mw-parser-output']\/p\").extract()\n\n        text_parsed = []\n        \n        #Clean HTML and other stuff from wikipedia\n        for paragraph in text_html:\n            text_souped = BeautifulSoup(paragraph, \"lxml\").get_text()\n            soup_cleaned = re.sub(\"\\[.*?\\]\", \"\", text_souped) #quitamos los corchetes\n            text_parsed.append(soup_cleaned)\n        \n        #keep the white spaces correctly\n        self.text = \"\\n\".join(text_parsed).replace(\"\\n\", \" \").replace(\"  \", \" \").strip()\n        return self.text\n\n\nclass FrontiersinExtractor(Extractor):\n    \"\"\"\n    Inhteir from Extractor, scrape the content of wikipedia pages\n    \n    Method:\n    --------\n    parse_text: return to us a Frontiersin page parsed without HTML text.\n    \"\"\"\n        \n    def __init__(self, url, errors=False) -> None:\n        super().__init__(url, errors=False)\n\n    def parse_text(self):\n        #Clean HTML and other stuff from Frontiersin\n        text_html = self.selector.xpath(\"\/\/div[@class='size size-small fulltext-content']\").extract()\n        text_souped = BeautifulSoup(text_html[0], \"lxml\").get_text()\n        text_clean = re.sub(\"\\[.*?\\]\", \"\", text_souped)\n        text_clean = re.sub(\"\\s+\", \" \", text_clean)\n\n        soup_splited = text_clean.split(\" \")\n        \n        #We don't want that the references of the article appear be part of our data.\n        soup_without_references = []\n        for idx, word in enumerate(soup_splited):\n            if word == \"Conflict\" and soup_splited[idx+1] == \"of\" and soup_splited[idx+2] == \"Interest\":\n                break\n            else:\n                soup_without_references.append(word)\n\n        soup_without_references = \" \".join(soup_without_references)\n        soup_without_references = soup_without_references.strip()\n\n        self.text = soup_without_references\n        return self.text","467fbb32":"def urls_to_scrape(urls, target):\n    \"\"\"\n    Return the urls with their target. \n\n    Warning, urls and target must be ordered in the same way \n    \n    Parameters:\n    ------------\n    urls: (list(str))\n        A list of urls to scrape\n        \n    target: (list(float))\n        A list of target that have earch url\n    \"\"\"\n    \n    urls_to_scrape = []\n    targets_urls_to_scrape = []\n\n    for url, target in zip(urls, target):\n        #We only are doing with wikipedia and frontiersin\n        if type(url) == str and (\"wikipedia\" in url or \"frontiersin\" in url):\n            urls_to_scrape.append(url)\n            targets_urls_to_scrape.append(target)\n            \n    return urls_to_scrape, targets_urls_to_scrape","70d28952":"def scraper(urls_to_scrape):\n    \"\"\"\n    Scrape all the urls data from wikipedia and frontiersin\n    \n    Parameters\n    ----------\n    urls_to_scrape: (list(str))\n    \"\"\"\n    \n    texts_parsed = []\n    for url in tqdm(urls_to_scrape):\n        sleep(0.1)\n        if \"wikipedia\" in url:\n            wkex = WikipediaExtractor(url, True)\n            try:\n                texts_parsed.append(wkex.parse_text())\n            except:\n                warnings.warn(f\"The url {url} can't be scraped\")\n                texts_parsed.append(np.nan)\n                \n        elif \"kids.frontiersin\" in url:\n            frontex = FrontiersinExtractor(url, True)\n            try:\n                texts_parsed.append(frontex.parse_text())\n            except:\n                warnings.warn(f\"The url {url} can't be scraped\")\n                texts_parsed.append(np.nan)\n            \n    return texts_parsed","4f91b52e":"train","adea2d71":"# Extract the urls from the dataframe\nurls, target = urls_to_scrape(train.url_legal.values, train.target.values)\nurls[:5]","b95db401":"#Scrape\ntexts_parsed = scraper(urls)","91cdcb8a":" #Create a DataFrame\ndf_text_parsed = pd.DataFrame(texts_parsed)\ndf_text_parsed.columns = [\"excerpt\"]\ndf_text_parsed[\"target\"] = target\n\n\ndf_text_parsed = df_text_parsed.dropna().reset_index(drop=True)\n\n#Save the data\ndf_text_parsed.to_csv(\".\/data_augmentation_raw.csv\", index=False)\ndf_text_parsed","bebebf7e":"def truncated_normal(mean=180, sd=17, low=135, high=205):\n    \"\"\"\n    Return a number that belong to a normal distribution\n    \n    Parameters:\n    -----------\n    \n    mean: (int\/float)\n        Mean of the distribution\n        \n    sd: (int\/float)\n        Standar deviation of the distribution\n        \n    low: (int\/float)\n        Lowest number fo the distribution\n        \n    high: (int\/float)\n    \"\"\"\n    return truncnorm( (low - mean) \/ sd, (high - mean) \/ sd, loc=mean, scale=sd ).rvs()\n\n\ndef clean_references(sentence):\n    \"Delete the 'references' in the article\"\n    \n    sentence_split = sentence.split(\" \")\n\n    new_sentence = []\n\n    for word in sentence_split:\n        normalize_word = word.lower()\n        if normalize_word == \"references\" or normalize_word == \"reference\":\n            break\n        else:\n            new_sentence.append(word)\n\n    return \" \".join(new_sentence)\n\ndef clean_simbols(sentence):\n    \"\"\"\n    Delete all inside brackets, parentheses, and other symbols that frequently appear\n    in wikipedia and frontiersin\n    \"\"\"\n    sentence = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", sentence) #clean brackets\n    sentence = re.sub(r\"http\\S+\", \"\", sentence) #clean urls\n    sentence = sentence.replace(\"\u2191\", \"\") #clean this symbol\n    sentence = re.sub(\"\\s+\", \" \", sentence) #spaces\n    return sentence\n\n\ndef gen_data(data_augmented_raw):\n    \"\"\"\n    Pass a DataFrame with the text column 'excerpt' and target column to create the augmented DataFrame\n    \"\"\"\n    \n    all_data_aumented = []\n\n    for row_idx in range(data_augmented_raw.shape[0]):\n        rand_div = truncated_normal()\n        dq_data_raw = deque(data_augmented_raw[\"excerpt\"][row_idx].split(\" \"))\n        n_divs = len(data_augmented_raw[\"excerpt\"][0].split(\" \")) \/ rand_div\n        differents_text = []\n        for i in range(int(n_divs)):\n            div = []\n            for j in range(int(rand_div)):\n                try:\n                    div.append(dq_data_raw.popleft())\n                except:\n                    break\n\n            length_div = len(div)\n            if length_div > 125:\n                div = \" \".join(div)\n                target_row = data_augmented_raw[\"target\"][row_idx]\n                min_target = target_row - 0.10\n                final_target = np.random.uniform(low=min_target, high=target_row)\n                differents_text.append([div, final_target])\n\n        all_data_aumented.append(differents_text)\n    return all_data_aumented","1e05fef8":"#The train have a mean leangth of 171.65\nlength_excerpt = train.excerpt.map(lambda x: len(x.split(\" \"))).values\nprint(length_excerpt)\n\n#distribution\nplt.hist(length_excerpt)","1ca3a25e":"df_text_parsed[\"excerpt\"] = df_text_parsed[\"excerpt\"].map(clean_references)\ndf_text_parsed[\"excerpt\"] = df_text_parsed[\"excerpt\"].map(clean_simbols)\ndf_text_parsed","d8015e48":"final_data_augment = gen_data(df_text_parsed)\nfinal_data_augment = [data for groups_data in final_data_augment for data in groups_data]\n\n\ndf_final_data_augment = pd.DataFrame(final_data_augment, columns=[\"excerpt\", \"target\"])\ndf_final_data_augment.to_csv(\"data_augmented_clean.csv\", index=False)\ndf_final_data_augment","2bc1dc51":"#The train have a mean leangth of 171.65\nlength_excerpt_augment = df_final_data_augment.excerpt.map(lambda x: len(x.split(\" \"))).values\nprint(length_excerpt_augment)\n\n#distribution\nplt.hist(length_excerpt_augment)","479cb4cc":"# 4.4 Prove that the length and the distribution are similar to the train","2dd6c814":"# 4. Prepare and cleaning the text\n\nThe last point is to **complete clean and make the new texts similar to our train**. \n\nIn this case I take the following decisions:\n\n1. Complete cleaning of the text.\n\n2. The scraped articles have thousands of words, but in our train each data have an average of ~175 words length. It's needed to divide the articles in diferent parts, in order to do that we will create differents parts **following a normal distribution** to have a mean of 175 words lenght each paragraph. This mean that for one article of 1000 words length we will have ~5 new data.\n\n3. To **avoid same target** we will change randomly the target between a 0.05 or -0.05\n\n**First will create the functions that will use in this part:**","10a3f725":"## 4.1 Prove what I said at the begining of point 4","f8c55107":"## 4.2 Complete cleaning of the text","809ff787":"# 5. Conclusions\n\nWe created around 2000 new data that can be added to our training!!!\n\n1. First of all remember that the main hipothesis was done under the assumption of the text target being representative of the full scraped article.\n\n2. This notebok was make with the purpose of testing this data augmentation concept. Functions can be improved, clean can be improved, and adding other extractors can be made, etc.","037938f6":"# 3. Scrape the data\n\nNow that we have all we need, we can start the scraping and get the data!","464297fc":"# 1. Import libraries and load data","538d6668":"## 4.3 Generate the new data","779a7eed":"# Scraping for data augmentation\n\nIn this notebook we are going to use scraping to increse the size of our dataset.\n\n**How is done?**\n\nIn the train data there are urls that reference where the data to train come from. All text in the train data come from other bigger article. We can scrape the entire article and divide it in paragrahps with similar target. \n\nFor example we can take this url https:\/\/en.wikipedia.org\/wiki\/Battle_of_Britain and scrape the complete page. In the train data there is only paragraph of ~175 words that belong to this article. we can make the **assumption** that if the target of this paragraph is -0.165467 all the article will have a similar target.\n\n**Let rock it!**","f89f9cdc":"# 2. Create utils to scrape\n\nHere we are going to create 3 Classes, one parent class called `Extractor` with the main attributes that every `Extractor` must have, and their two child class that inherit from `Extractor`.\n\n**Every child class will be for scrape a different page**, for example for scrape Wikipedia we will create `WikipediaExtractor` and for scrape Frontiersin we will create `FrontiersinExtractor` \n\nWith this two extractor **we cover the ~80%** of the urls that a appear in the dataset, you can create others extractors if you want to scrape other pages."}}