{"cell_type":{"f5c76907":"code","69384ef9":"code","81e76c6b":"code","b3f07505":"code","6c161917":"code","d5649808":"code","b50b07c1":"code","086caa36":"code","95b908dc":"markdown","dca2c551":"markdown","fd35d7ca":"markdown","456247bf":"markdown","41f4e113":"markdown","f84b2634":"markdown","12e32971":"markdown","d5f480ec":"markdown","193fd3e5":"markdown"},"source":{"f5c76907":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom fancyimpute import KNN\nfrom sklearn.preprocessing import OrdinalEncoder","69384ef9":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","81e76c6b":"# Remove index data\/irrelevant data from set\ntrain.drop('enrollee_id', inplace=True, axis=1)\ntest.drop('enrollee_id', inplace=True, axis=1)","b3f07505":"# Columns which are required for modelling\ncolumns = ['city',\n 'city_development_index',\n 'gender',\n 'relevent_experience',\n 'enrolled_university',\n 'education_level',\n 'major_discipline',\n 'experience',\n 'company_size',\n 'company_type',\n 'last_new_job',\n 'training_hours']","6c161917":"# Check for columns with null values\nnull_cols = train.isnull().sum()\nlist(train.columns[null_cols>0])","d5649808":"#instantiate both packages to use\nencoder = OrdinalEncoder()\n\ndef encode(train_data, test_data):\n    '''function to encode non-null data and replace it in the original data'''\n    #retains only non-null values\n    train_data_nonulls = np.array(train_data.dropna())\n    test_data_nonulls = np.array(test_data.dropna())\n    #reshapes the data for encoding\n    train_impute_reshape = train_data_nonulls.reshape(-1,1)\n    test_impute_reshape = test_data_nonulls.reshape(-1,1)\n    #encode date\n    train_impute_ordinal = encoder.fit_transform(train_impute_reshape)\n    test_impute_ordinal = encoder.transform(test_impute_reshape)\n    #Assign back encoded values to non-null values\n    train_data.loc[train_data.notnull()] = np.squeeze(train_impute_ordinal)\n    test_data.loc[test_data.notnull()] = np.squeeze(test_impute_ordinal)\n    return train_data, test_data\n\n#create a for loop to iterate through each column in the data\nfor column in columns:\n    train[column], test[column] = encode(train[column], test[column])","b50b07c1":"imputer = KNN(k=3)\ntrain_ = train[columns]\nfullset = pd.concat([train_, test]) \n#fancy impute removes column names\nfullset = pd.DataFrame(imputer.fit_transform(fullset))\nfullset.columns = columns","086caa36":"train_df = fullset[:train.shape[0]]\ntest_df = fullset[train.shape[0]:]","95b908dc":"# 1. Load Data","dca2c551":"# 4. Imputation using fancyimpute KNN (KNeighbour Nearest Algo)","fd35d7ca":"# Please **upvote** and **comment** if you find this useful.","456247bf":"# 2. Preprocess ","41f4e113":"# 5. Conclusion\n\n**Other fancyimpute methods** >> https:\/\/pypi.org\/project\/fancyimpute\/\n\n**SimpleFill**: Replaces missing entries with the mean or median of each column.\n\n**KNN**: Nearest neighbor imputations which weights samples using the mean squared difference on features for which two rows both have observed data.\n\n**SoftImpute**: Matrix completion by iterative soft thresholding of SVD decompositions. Inspired by the softImpute package for R, which is based on Spectral Regularization Algorithms for Learning Large Incomplete Matrices by Mazumder et. al.\n\n**IterativeImputer**: A strategy for imputing missing values by modeling each feature with missing values as a function of other features in a round-robin fashion. A stub that links to scikit-learn's IterativeImputer.\n\n**IterativeSVD**: Matrix completion by iterative low-rank SVD decomposition. Should be similar to SVDimpute from Missing value estimation methods for DNA microarrays by Troyanskaya et. al.\n\n**MatrixFactorization**: Direct factorization of the incomplete matrix into low-rank U and V, with an L1 sparsity penalty on the elements of U and an L2 penalty on the elements of V. Solved by gradient descent.\n\n**NuclearNormMinimization**: Simple implementation of Exact Matrix Completion via Convex Optimization by Emmanuel Candes and Benjamin Recht using cvxpy. Too slow for large matrices.\n\n**BiScaler**: Iterative estimation of row\/column means and standard deviations to get doubly normalized matrix. Not guaranteed to converge but works well in practice. Taken from Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares.\n","f84b2634":"# Table of Index\n\n* [Load Data](#1.-Load-Data)\n* [Preprocess ](#2.-Preprocess)\n* [Encode Categorical Data](#3.-Encode-Categorical-Data)\n* [Imputation using fancyimpute KNN (KNeighbour Nearest Algo)](#4.-Imputation-using-fancyimpute-KNN-(KNeighbour-Nearest-Algo))\n* [Conclusion](#5.-Conclusion)\n","12e32971":"# 3. Encode Categorical Data","d5f480ec":"# Acknowledgements\n\nReferences are: <br>\nhttps:\/\/www.kaggle.com\/mnoori\/fancy-imputing-the-missing-inc-angles-beginner <br>\nhttps:\/\/towardsdatascience.com\/preprocessing-encode-and-knn-impute-all-categorical-features-fast-b05f50b4dfaa\n","193fd3e5":"# Context and Notebook Objective\n\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\nThis dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.\n\nDataset: https:\/\/www.kaggle.com\/arashnic\/hr-analytics-job-change-of-data-scientists\n\n**This notebook tries to perform Imputation using fancyimpute library**"}}