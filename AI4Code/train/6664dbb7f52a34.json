{"cell_type":{"eb73330d":"code","f3dc6b9d":"code","7b57fe08":"code","d9bee5ee":"code","bb950910":"code","d1a8527b":"code","621f5acc":"code","10360190":"code","3fa42755":"code","a8fba7aa":"code","ca7e545a":"code","fc277f4d":"code","33b22687":"code","82e2bd01":"code","7749b6c0":"code","bf88a272":"code","eedb82ae":"code","dca29c71":"code","72081e8a":"code","729ef92e":"code","a4e568eb":"code","a1d2a178":"code","1701ef90":"code","33cd1730":"code","4fbd207b":"code","eb7312b0":"code","c49bb048":"code","20ef56c3":"code","c9ebe2c6":"code","fa2f1067":"code","7f9aa92b":"code","99567082":"code","854e0bee":"code","383d0077":"code","8630d757":"code","780646ed":"markdown","46dcd13b":"markdown","94afd676":"markdown","eafbf787":"markdown","40b53932":"markdown","5569b1e1":"markdown","aede8bbf":"markdown","6289b342":"markdown","b923243e":"markdown","ccf8b865":"markdown","f235486a":"markdown","5ed2c477":"markdown","74edd1ba":"markdown","d28d334b":"markdown","955b91f2":"markdown"},"source":{"eb73330d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3dc6b9d":"## Import the useful libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","7b57fe08":"train_DF = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_DF = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","d9bee5ee":"print(sns.distplot(train_DF['Age'].dropna(),kde=False,bins=30))","bb950910":"print(sns.distplot(test_DF['Age'].dropna(),kde=False,bins=30))","d1a8527b":"plt.figure(figsize=(10,7))\nprint(sns.boxplot(x='Survived',y='Age',data=train_DF))","621f5acc":"plt.figure(figsize=(10,7))\nprint(sns.boxplot(x='Pclass',y='Age',data=train_DF))","10360190":"print(sns.heatmap(train_DF.isnull(),yticklabels=False,cmap='viridis'))","3fa42755":"print(sns.heatmap(test_DF.isnull(),yticklabels=False,cmap='viridis'))","a8fba7aa":"print(\"Average_Age_Of_First_Class_Pass\",round(train_DF['Age'][train_DF['Pclass']==1].mean()))\nprint(\"Average_Age_Of_Second_Class_Pass\",round(train_DF['Age'][train_DF['Pclass']==2].mean()))\nprint(\"Average_Age_Of_Third_Class_Pass\",round(train_DF['Age'][train_DF['Pclass']==3].mean()))\nprint(\"\\n\")\nprint(\"Average_Age_Of_First_Class_Pass\",round(test_DF['Age'][test_DF['Pclass']==1].mean()))\nprint(\"Average_Age_Of_Second_Class_Pass\",round(test_DF['Age'][test_DF['Pclass']==2].mean()))\nprint(\"Average_Age_Of_Third_Class_Pass\",round(test_DF['Age'][test_DF['Pclass']==3].mean()))","ca7e545a":"## For the train DataSet :\n\nPclass1_age = round(train_DF['Age'][train_DF['Pclass']==1].mean())\nPclass2_age = round(train_DF['Age'][train_DF['Pclass']==2].mean())\nPclass3_age = round(train_DF['Age'][train_DF['Pclass']==3].mean())\n\ndef fix_age_train_DF(col):\n    Age=col[0]\n    Pclass=col[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return Pclass1_age\n        elif Pclass == 2:\n            return Pclass2_age\n        else:\n            return Pclass3_age\n    else:\n        return Age","fc277f4d":"## For the test DataSet :\n\nPclass1_age = round(test_DF['Age'][test_DF['Pclass']==1].mean())\nPclass2_age = round(test_DF['Age'][test_DF['Pclass']==2].mean())\nPclass3_age = round(test_DF['Age'][test_DF['Pclass']==3].mean())\n\ndef fix_age_test_DF(col):\n    Age=col[0]\n    Pclass=col[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return Pclass1_age\n        elif Pclass == 2:\n            return Pclass2_age\n        else:\n            return Pclass3_age\n    else:\n        return Age","33b22687":"train_DF['Age'] = train_DF[['Age','Pclass']].apply(fix_age_train_DF,axis=1)\ntest_DF['Age'] = test_DF[['Age','Pclass']].apply(fix_age_test_DF,axis=1)","82e2bd01":"print(sns.heatmap(test_DF.isnull(),yticklabels=False,cmap='viridis'))","7749b6c0":"print(sns.heatmap(test_DF.isnull(),yticklabels=False,cmap='viridis'))","bf88a272":"train_DF.drop('Cabin',axis=1,inplace=True)\ntest_DF.drop('Cabin',axis=1,inplace=True)","eedb82ae":"train_DF.info()","dca29c71":"train_DF.dropna(inplace=True)  ## Drop any more rows with NULL values\ntest_DF.fillna(value=test_DF['Age'].min(),inplace=True) ## There is one row that is null\/NaN ... so replacing with the minimum value.","72081e8a":"train_DF.info()\ntest_DF.info()","729ef92e":"sex=pd.get_dummies(train_DF['Sex'],drop_first=True)\nsex1=pd.get_dummies(test_DF['Sex'],drop_first=True)\n\nembark=pd.get_dummies(train_DF['Embarked'],drop_first=True)\nembark1=pd.get_dummies(test_DF['Embarked'],drop_first=True)\n\npclass=pd.get_dummies(train_DF['Pclass'],drop_first=True)\npclass1=pd.get_dummies(test_DF['Pclass'],drop_first=True)\n\ntrain_DF=pd.concat([train_DF,sex,embark,pclass],axis=1)\ntest_DF=pd.concat([test_DF,sex1,embark1,pclass1],axis=1)","a4e568eb":"train_DF.head()","a1d2a178":"test_DF.head()","1701ef90":"train_DF.drop(['Sex','Embarked','Name','Ticket','PassengerId','Pclass'],axis=1,inplace=True)\ntest_DF.drop(['Sex','Embarked','Name','Ticket','Pclass'],axis=1,inplace=True)","33cd1730":"print(train_DF.head())\nprint(test_DF.head())","4fbd207b":"X_train = train_DF.drop('Survived',axis=1)\ny_train = train_DF['Survived']","eb7312b0":"print(X_train.head())\nprint(y_train.head())","c49bb048":"X_test = test_DF.drop('PassengerId',axis=1)","20ef56c3":"print(X_test.head())","c9ebe2c6":"from sklearn.linear_model import LogisticRegression","fa2f1067":"logmodel = LogisticRegression(random_state=0)","7f9aa92b":"logmodel.fit(X_train,y_train)","99567082":"predictions = logmodel.predict(X_test)","854e0bee":"AgeBased_Submission=pd.DataFrame(index=test_DF.PassengerId, data={'Survived': predictions})","383d0077":"print(AgeBased_Submission.head())\nprint(AgeBased_Submission.info())","8630d757":"AgeBased_Submission.to_csv('\/kaggle\/working\/AgeBased_Submission_2.csv')","780646ed":"**** In both the train and test datasets, NaN values are observed for the Age Feature and the Cabin Feature. As you can see in the heat map, the Cabin feature has too many NaN values. So, it cannot be fixed easily. We will focus on fixing the 'Age' feature and we will need it for our prediction model.****","46dcd13b":"**** Checking for Missing values in both data sets and try and fix them. ****","94afd676":"**** The boxplot below shows that the survival rate is lesser for people in that age bracket. ****","eafbf787":"**** Now that the train and test datasets are ready, we will use the sklearn Logistic Regression module to create a model for prediction ****","40b53932":"**** Further, most people in Class 2 and Class 3 who did not survive, belong to the age Bracket 20 - 30. This indicates that majority of the survivors were from Class 1. The richer and older the passengers, the more likely he\/she was to survive. Probably because of the preferences or priority given to upper class passengers in the evacuation operation. ****","5569b1e1":"**** Titanic Survivor Predictor -- Given a set of features: \u201cwhat sorts of people were more likely to survive in the Titanic Accident?\u201d ********","aede8bbf":"**** The new heatmap above shows that the Age feature is now fixed. We will now drop the 'Cabin' feature as we will not need it. (Although it would have been a good choice for a more accurate prediction if the full data was available. From the cabin details, we can make out that more casualties would occur in the lower classed cabins as they would be nearer the waterlevel when the Ship hits the iceberg. ****","6289b342":"**** Ok, the below steps is to actually convert the relevant \"text\" based categorical features into binary features so that the model can actually use them to make a better prediction. ****","b923243e":"**** That gives the predicted surviver list of passengers based on the model that assumes that the feature Age and Class of passengers mattered to the survivability of the passengers ****","ccf8b865":"**** Prepare the X-Train and Y-Train datasets for the Logistic Regression ****","f235486a":"**** Remove all the features that is not required or are not relevant for the model especially the text features. ****","5ed2c477":"**** One way to do it is to take a mean of the 'Age' for each class of passengers and fill in the respective null values with it. The following function does that for the train and test datasets.****","74edd1ba":"**** The model is now successfully created. The data that was the test data is run by the model and the predicted values we now have. ****\n\nNote that I am adding the result in a DataFrame named AgeBased_Submission.","d28d334b":"**** Read in the CSV files to a DataFrame. Train_DF will be used to train the predictive model and Test_DF will be used to test the model ****","955b91f2":"**** We will try and predict the data on the Age factor. Looking at the 'Age' Distribution, we clearly see that majority of the people in the ship were between the Age of 20 and 30. ****"}}