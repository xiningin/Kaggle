{"cell_type":{"092e3aec":"code","a90db61c":"code","5feaf9e8":"code","faf9fac3":"code","ab9ffba0":"code","08c23e29":"code","8360767b":"code","bcb4d177":"code","f2401094":"code","bbec3c37":"code","79539f06":"code","f491b5dc":"code","50c5713b":"code","63e9e50a":"code","96a3403b":"code","057a0313":"code","e139d0d1":"code","fc54c8b9":"code","0ff2ddd5":"code","5d24a8c9":"code","a7eb5c2e":"code","af53aac7":"code","3ebd7d2a":"code","2b393f97":"code","e76b4a64":"code","63a326c4":"code","33da9880":"code","ee52ed79":"code","81579c6d":"code","3f6e680f":"code","ebc53350":"code","fe300ec0":"code","b43d7728":"code","6137b1a6":"code","c5d174fd":"code","e5ee2e57":"code","eaa4ffbe":"code","2f418305":"code","55daf2f8":"code","439184a8":"code","0678bc4f":"code","c1062337":"code","19c4cc30":"code","d4f0054f":"code","45f4da57":"code","ded97d68":"code","9ae2abc1":"code","8e9bb9cd":"code","c27cb06f":"code","97c03689":"code","f712239a":"code","2f9e4fcf":"code","58c9d885":"code","ac657e3a":"code","a245b764":"code","00437890":"code","4988ce82":"code","9952f9f5":"code","57965a79":"code","a25d18d5":"code","72ca2e77":"code","964c34f3":"code","977d25b0":"code","e61a1d25":"code","368fb64a":"code","11c0836a":"code","2a480ca6":"code","acee3f9e":"code","93237a5f":"code","da16fff7":"code","24a79d84":"code","1f5b3c09":"code","fbc57924":"code","b7f85135":"code","bf489b77":"code","2d7628b1":"code","44bd8651":"code","e5d295d6":"code","e90fbd9f":"code","a62d6d17":"code","3249439a":"code","a770360a":"code","699545ec":"code","c234ae62":"code","9a154673":"code","9f5f68d9":"code","1840b6fc":"code","69b7fe1b":"code","eab8a303":"code","ccc81af9":"code","3eacab59":"code","9f002334":"code","47ef5faf":"code","66f34ca6":"code","a9fd319c":"code","a945baff":"code","42bf1bfa":"code","fb5462c5":"code","0458c82b":"code","0f4e9afa":"code","30625f6b":"code","464055f2":"code","9b8c22a1":"code","3c304974":"code","5d747889":"code","2f1fe5ef":"code","1771c03d":"code","814be350":"code","96be1f89":"code","74346e29":"code","a99c1f71":"markdown","09d60f90":"markdown","87ff7ef3":"markdown","cfa6a775":"markdown","059ce226":"markdown","269e3ffd":"markdown","cc17283d":"markdown","d50c792d":"markdown","25d86985":"markdown","cda60d1b":"markdown","5baf3aa5":"markdown","0518f27b":"markdown","d6c5b1e0":"markdown","0b12fda7":"markdown","a7908a77":"markdown","91769bbe":"markdown","0e04c347":"markdown","6e0ff436":"markdown","ebbeb780":"markdown","c2c78219":"markdown","fe05e9ae":"markdown","2351d083":"markdown","cf418753":"markdown","2c40ef29":"markdown","edb79445":"markdown","4e54e949":"markdown","d2e762b6":"markdown","a683a092":"markdown","1550f6ee":"markdown"},"source":{"092e3aec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a90db61c":"# import libraries\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom pprint import pprint\nimport xgboost\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom xgboost import XGBClassifier\nimport time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","5feaf9e8":"# print cross validation metrics\ndef get_cv_roc_pr_rc(clf, skf, X, y):\n    roc_auc = []\n    pr_auc = []\n    precision = []\n    recall = []\n    models = []\n    for train, test in skf.split(X, y):\n        models.append(clf.fit(X.iloc[train,:], y.iloc[train]))\n        y_pred = clf.predict(X.iloc[test,:])\n        y_pred_prob = clf.predict_proba(X.iloc[test,:])[:,1]\n        roc_auc.append(roc_auc_score(y.iloc[test], y_pred_prob))\n        pr_auc.append(average_precision_score(y.iloc[test], y_pred_prob))\n        precision.append(precision_score(y.iloc[test], y_pred))\n        recall.append(recall_score(y.iloc[test], y_pred))\n\n    print ('ROC-AUC:', np.mean(roc_auc))\n    print ('PR-AUC:', np.mean(pr_auc))\n    print ('Precission:', np.mean(precision))\n    print ('Recall:', np.mean(recall))\n    \n    return roc_auc, pr_auc, precision, recall, models","faf9fac3":"# print cross validation metrics two\ndef get_cv_roc_pr_rc_2(clf, skf, X, y):\n    roc_auc = []\n    pr_auc = []\n    precision = []\n    recall = []\n    models = []\n    for train, test in skf.split(X, y):\n        models.append(clf.fit(X.iloc[train,:], y.iloc[train]))\n        y_pred = clf.predict(X.iloc[test,:])\n        #y_pred_prob = clf.predict_proba(X.iloc[test,:])[:,1]\n        roc_auc.append(roc_auc_score(y.iloc[test], y_pred))\n        pr_auc.append(average_precision_score(y.iloc[test], y_pred))\n        precision.append(precision_score(y.iloc[test], y_pred))\n        recall.append(recall_score(y.iloc[test], y_pred))\n\n    print ('ROC-AUC:', np.mean(roc_auc))\n    print ('PR-AUC:', np.mean(pr_auc))\n    print ('Precission:', np.mean(precision))\n    print ('Recall:', np.mean(recall))\n    \n    return roc_auc, pr_auc, precision, recall, models","ab9ffba0":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","08c23e29":"# Concatenate training and test sets\ndata = pd.concat([train.drop(['Survived'], axis=1), test])","8360767b":"fig, axarr = plt.subplots(1, 2, figsize=(12,6))\na = sns.countplot(train['Sex'], ax=axarr[0]).set_title('Passengers count by sex')\naxarr[1].set_title('Survival rate by sex')\nb = sns.barplot(x='Sex', y='Survived', data=train, ax=axarr[1]).set_ylabel('Survival rate')","bcb4d177":"# survival rate by class\ntrain.groupby('Pclass').Survived.mean()","f2401094":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\na = sns.countplot(x='Pclass', hue='Survived', data=train, ax=axarr[0]).set_title('Survivors and deads count by class')\naxarr[1].set_title('Survival rate by class')\nb = sns.barplot(x='Pclass', y='Survived', data=train, ax=axarr[1]).set_ylabel('Survival rate')","bbec3c37":"# survival rate by class\ntrain.groupby(['Pclass', 'Sex']).Survived.mean()","79539f06":"plt.title('Survival rate by sex and class')\ng = sns.barplot(x='Pclass', y='Survived', hue='Sex', data=train).set_ylabel('Survival rate')","f491b5dc":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\naxarr[0].set_title('Age distribution')\nf = sns.distplot(train['Age'], color='g', bins=40, ax=axarr[0])\naxarr[1].set_title('Age distribution for the two subpopulations')\ng = sns.kdeplot(train['Age'].loc[train['Survived'] == 1], \n                shade= True, ax=axarr[1], label='Survived').set_xlabel('Age')\ng = sns.kdeplot(train['Age'].loc[train['Survived'] == 0], \n                shade=True, ax=axarr[1], label='Not Survived')","50c5713b":"plt.figure(figsize=(8,5))\ng = sns.swarmplot(y='Sex', x='Age', hue='Survived', data=train).set_title('Survived by age and sex')","63e9e50a":"plt.figure(figsize=(8,5))\nh = sns.swarmplot(x='Pclass', y='Age', hue='Survived', data=train).set_title('Survived by age and class')","96a3403b":"train.Fare.describe()","057a0313":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\nf = sns.distplot(train.Fare, color='g', ax=axarr[0]).set_title('Fare distribution')\nfare_ranges = pd.qcut(train.Fare, 4, labels = ['Low', 'Mid', 'High', 'Very high'])\naxarr[1].set_title('Survival rate by fare category')\ng = sns.barplot(x=fare_ranges, y=train.Survived, ax=axarr[1]).set_ylabel('Survival rate')","e139d0d1":"plt.figure(figsize=(8,5))\n# I excluded the three outliers with fare > 500 from this plot\na = sns.swarmplot(x='Sex', y='Fare', hue='Survived', data=train.loc[train.Fare<500]).set_title('Survived by fare and sex')","fc54c8b9":"# passenger with free ticket or mistake?\ntrain.loc[train.Fare==0]","0ff2ddd5":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\nsns.countplot(train['Embarked'], ax=axarr[0]).set_title('Passengers count by boarding point')\np = sns.countplot(x = 'Embarked', hue = 'Survived', data = train, \n                  ax=axarr[1]).set_title('Survivors and deads count by boarding point')","5d24a8c9":"g = sns.countplot(data=train, x='Embarked', hue='Pclass').set_title('Pclass count by embarking point')","a7eb5c2e":"# Extract Title from Name, store in column and plot barplot\ndata['Title'] = data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\nsns.countplot(x='Title', data=data);\nplt.xticks(rotation=45);","af53aac7":"# Substitute titles and plot barplot\ndata['Title'] = data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\ndata['Title'] = data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\nsns.countplot(x='Title', data=data);\nplt.xticks(rotation=45);","3ebd7d2a":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\na = sns.countplot(train['SibSp'], ax=axarr[0]).set_title('Passengers count by SibSp')\naxarr[1].set_title('Survival rate by SibSp')\nb = sns.barplot(x='SibSp', y='Survived', data=train, ax=axarr[1]).set_ylabel('Survival rate')","2b393f97":"fig, axarr = plt.subplots(1,2,figsize=(12,6))\na = sns.countplot(train['Parch'], ax=axarr[0]).set_title('Passengers count by Parch')\naxarr[1].set_title('Survival rate by Parch')\nb = sns.barplot(x='Parch', y='Survived', data=train, ax=axarr[1]).set_ylabel('Survival rate')","e76b4a64":"# Create column of number of Family members onboard\ndata['Fam_Size'] = data.Parch + data.SibSp + 1\ndata['Fam_type'] = pd.cut(data.Fam_Size, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big'])","63a326c4":"# Extract the first two letters\ndata['Ticket_lett'] = data.Ticket.apply(lambda x: x[:2])\n# Calculate ticket length\ndata['Ticket_len'] = data.Ticket.apply(lambda x: len(x))","33da9880":"# fill NaN\ndata['Age'] = data['Age'].interpolate(limit_direction='both', method='linear')\ndata['Fare'] = data['Fare'].interpolate(limit_direction='both', method='linear')\ndata['Embarked'] = data['Embarked'].fillna(data['Embarked'].value_counts().index[0])","ee52ed79":"# Binning numerical columns\ndata['CatAge'] = pd.qcut(data.Age, q=4, labels=False )","81579c6d":"# remove unneeded columns\ndata = data.drop(['PassengerId', 'Name', 'SibSp', 'Parch',\n       'Ticket', 'Cabin', 'Fam_Size', 'Age'], axis = 1)","3f6e680f":"data.info()","ebc53350":"# Transform into binary variables\ndata_dun = pd.get_dummies(data, drop_first=True)\ndata_dun.shape","fe300ec0":"# extract Y values\ny_train = train['Survived']","b43d7728":"# create Stratified K-Folds cross-validator\nskf = StratifiedKFold(n_splits=5)","6137b1a6":"# split to train and test\ndata_dun_train = data_dun[:891]\ndata_dun_test = data_dun[891:]","c5d174fd":"# random under-sampling\nros = RandomUnderSampler(random_state=0)\nros.fit(data_dun_train, y_train)\nX_resampled, y_resampled = ros.fit_sample(data_dun_train, y_train)","e5ee2e57":"lr = LogisticRegression(max_iter = 10000, penalty=\"l2\")","eaa4ffbe":"grid_values = {'C': [0.001,0.01,0.1,1,10,100,1000]} \nmodel_lr = GridSearchCV(lr, param_grid=grid_values)","2f418305":"# first experiment with under-sampling\nmodel_lr.fit(X_resampled, y_resampled)\nmodel_lr.best_params_","55daf2f8":"lr_1 = LogisticRegression(C = 1, max_iter = 10000, penalty=\"l2\")","439184a8":"roc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(lr_1, skf, X_resampled, y_resampled)","0678bc4f":"# second experiment without under-sampling\nmodel_lr.fit(data_dun_train, y_train)\nmodel_lr.best_params_","c1062337":"lr_2 = LogisticRegression(C = 10, max_iter = 10000, penalty=\"l2\")","19c4cc30":"roc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(lr, skf, data_dun_train, y_train)","d4f0054f":"random_forest = RandomForestClassifier()","45f4da57":"n_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(random_forest, hyperF, cv = 5, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(data_dun_train, y_train)","ded97d68":"gridF.best_params_","9ae2abc1":"random_forest = RandomForestClassifier(max_depth = 25, min_samples_leaf = 1, min_samples_split = 15, n_estimators = 500)\nroc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(random_forest, skf, data_dun_train, y_train)","8e9bb9cd":"params = {\n    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"none\"]\n}\n\nmodel = SGDClassifier(max_iter=5, tol=None)\nclf = GridSearchCV(model, param_grid=params)","c27cb06f":"bestF_sgd = clf.fit(data_dun_train, y_train)","97c03689":"bestF_sgd.best_score_","f712239a":"bestF_sgd.best_estimator_","2f9e4fcf":"sgd_best = SGDClassifier(alpha=0.001, loss='log', max_iter=5, penalty='l1', tol=None)","58c9d885":"roc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc_2(sgd_best, skf, data_dun_train, y_train)","ac657e3a":"X_train, X_test, y_train_2, y_test = train_test_split(data_dun_train, y_train, test_size=0.25)","a245b764":"knn = KNeighborsClassifier() \n\nk_choices = [1, 2, 3, 5, 8, 10, 15, 20, 25, 50]\n\nfor k in k_choices:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train_2)\n    y_pred_prob = knn.predict_proba(X_test)[:,1]\n    #print(k)\n    print('k = ', k, ' ROC-AUC:', roc_auc_score(y_test, y_pred_prob))","00437890":"knn_best = KNeighborsClassifier(n_neighbors=15) \nroc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(knn_best, skf, data_dun_train, y_train)","4988ce82":"gaussian = GaussianNB() \nroc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(gaussian, skf, data_dun_train, y_train)","9952f9f5":"params = {\n    \"max_iter\" : [1, 5, 10, 50, 100, 1000],\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"none\"]\n}\n\nmodel = Perceptron(tol=None)\nper = GridSearchCV(model, param_grid=params)","57965a79":"per_best = per.fit(data_dun_train, y_train)","a25d18d5":"per_best.best_score_","72ca2e77":"per_best.best_params_","964c34f3":"perceptron = Perceptron(alpha=0.001, max_iter=1000, penalty='l1', tol=None)\nroc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc_2(perceptron, skf, data_dun_train, y_train)","977d25b0":"decision_tree = DecisionTreeClassifier() ","e61a1d25":"# Create lists of parameter for Decision Tree Classifier\ncriterion = ['gini', 'entropy']\nmax_depth = [4,6,8,12]\n\n# Create a dictionary of all the parameter options \n# Note has you can access the parameters of steps of a pipeline by using '__\u2019\nparameters = dict(criterion=['gini', 'entropy'],\n                  max_depth = [1,2,4,6,8,12],\n                  min_samples_split = [2, 5, 10, 15, 20, 25, 30, 35, 100],\n                  min_samples_leaf = [1, 2, 5, 10])\n\n# Conduct Parameter Optmization With Pipeline\n# Create a grid search object\nclf_dt = GridSearchCV(decision_tree, parameters)\n\n# Fit the grid search\nclf_dt.fit(data_dun_train, y_train)\n\n# View The Best Parameters\nprint('Best Criterion:', clf_dt.best_estimator_.get_params()['criterion'])\nprint('Best max_depth:', clf_dt.best_estimator_.get_params()['max_depth'])\nprint('Best min samples split:', clf_dt.best_estimator_.get_params()['min_samples_split'])\nprint('Best max depth:', clf_dt.best_estimator_.get_params()['min_samples_leaf'])\n\n# Use Cross Validation To Evaluate Model\nCV_Result = cross_val_score(clf_dt, data_dun_train, y_train, cv=5, n_jobs=-1)\nprint(); print(CV_Result)\nprint(); print(CV_Result.mean())\nprint(); print(CV_Result.std())","368fb64a":"clf_dt.best_params_","11c0836a":"decision_tree_best = DecisionTreeClassifier(criterion='gini',max_depth=4,min_samples_leaf=1,min_samples_split=30) \nroc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(decision_tree_best, skf, data_dun_train, y_train)","2a480ca6":"# Create the parameter grid: gbm_param_grid \ngbm_param_grid = {\n    'n_estimators': range(8, 20),\n    'max_depth': range(6, 10),\n    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'learning_rate': [.4, .45, .5, .55, .6],\n    'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n    'gamma': [0, 0.25, 0.5, 1.0],\n    'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0]\n}\n\n# Instantiate the regressor: gbm\ngbm = XGBClassifier(n_estimators=10)\n\n# Perform random search: grid_mse\nxgb_random = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator = gbm, scoring = \"accuracy\", \n                                    verbose = 1, n_iter = 50, cv = 4)\n\n\n# Fit randomized_mse to the data\nxgb_random.fit(data_dun_train, y_train)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", xgb_random.best_params_)\nprint(\"Best accuracy found: \", xgb_random.best_score_)","acee3f9e":"best_score = xgb_random.best_score_\nbest_params = xgb_random.best_params_\nprint(\"Best score: {}\".format(best_score))\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","93237a5f":"xgb_best = XGBClassifier(colsample_bylevel=1.0, colsample_bytree=0.7,gamma=1.0,learning_rate=0.6,\n                         max_depth=7,min_child_weight=0.5,n_estimators=17,reg_lambda=5.0,\n                         subsample=0.7)","da16fff7":"roc_auc, pr_auc, precision, recall, models = get_cv_roc_pr_rc(xgb_best, skf, data_dun_train, y_train)","24a79d84":"random_forest.fit(data_dun_train, y_train)","1f5b3c09":"y_pred = pd.DataFrame(random_forest.predict(data_dun_test), columns=['Survived'])","fbc57924":"sub = pd.concat([test.PassengerId, y_pred], axis=1)","b7f85135":"sub = sub.set_index(['PassengerId'])","bf489b77":"sub.to_csv('titanic_RF_1.csv')","2d7628b1":"y_pred = pd.DataFrame(xgb_best.predict(data_dun_test), columns=['Survived'])","44bd8651":"sub = pd.concat([test.PassengerId, y_pred], axis=1)","e5d295d6":"sub = sub.set_index(['PassengerId'])","e90fbd9f":"sub.to_csv('titanic_XGB_1.csv')","a62d6d17":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom numpy.random import seed\nimport tensorflow\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier","3249439a":"ss = StandardScaler()\ndata_train_scaler = ss.fit_transform(data_dun_train)\ndata_test_scaler = ss.fit_transform(data_dun_test)","a770360a":"model = Sequential()","699545ec":"len(data_train_scaler[1])","c234ae62":"model.add(Dense(512, activation = 'relu', input_dim = 73))\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))","9a154673":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","9f5f68d9":"model.fit(data_train_scaler, y_train, batch_size = 10, epochs = 100)","1840b6fc":"y_pred = pd.DataFrame(model.predict(data_test_scaler), columns=['Survived'])","69b7fe1b":"sub = pd.concat([test.PassengerId, y_pred], axis=1)","eab8a303":"sub['Survived'].values[sub['Survived'].values > 0.5] = 1\nsub['Survived'].values[sub['Survived'].values <= 0.5] = 0\nsub['Survived'] = sub['Survived'].astype(int)","ccc81af9":"sub = sub.set_index(['PassengerId'])","3eacab59":"sub.to_csv('titanic_keras_2.csv')","9f002334":"data_train_scaler.shape[1]","47ef5faf":"# create model\ntmodel = Sequential()\ntmodel.add(Dense(input_dim=data_train_scaler.shape[1], units=128,\n                 kernel_initializer='normal', bias_initializer='zeros'))\ntmodel.add(Activation('relu'))\n\nfor i in range(0, 8):\n    tmodel.add(Dense(units=64, kernel_initializer='normal',\n                     bias_initializer='zeros'))\n    tmodel.add(Activation('relu'))\n    tmodel.add(Dropout(.25))\n\ntmodel.add(Dense(units=1))\ntmodel.add(Activation('linear'))\n\ntmodel.compile(loss='mean_squared_error', optimizer='rmsprop')","66f34ca6":"tmodel.fit(data_train_scaler, y_train, epochs=600, verbose=2)","a9fd319c":"y_pred = pd.DataFrame(tmodel.predict(data_test_scaler), columns=['Survived'])","a945baff":"sub = pd.concat([test.PassengerId, y_pred], axis=1)","42bf1bfa":"sub['Survived'].values[sub['Survived'].values > 0.5] = 1\nsub['Survived'].values[sub['Survived'].values <= 0.5] = 0\nsub['Survived'] = sub['Survived'].astype(int)","fb5462c5":"sub = sub.set_index(['PassengerId'])","0458c82b":"sub.to_csv('titanic_keras_exp_1.csv')","0f4e9afa":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","30625f6b":"df_all['Fare'] = pd.qcut(df_all['Fare'], 13)","464055f2":"df_all['Age'] = pd.qcut(df_all['Age'], 10)","9b8c22a1":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","3c304974":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","5d747889":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1","2f1fe5ef":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","1771c03d":"import string","814be350":"def extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","96be1f89":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","74346e29":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","a99c1f71":"# 5. Solution","09d60f90":"## 2.2. Pclass","87ff7ef3":"**No undersampling is better**","cfa6a775":"## 5.1 With Random Forest ","059ce226":"## 4.4 K Nearest Neighbor","269e3ffd":"## 5.3 With Keras","cc17283d":"## 2.3 Sex","d50c792d":"### **XGB showed the best result - 0.78708. Keras - 0.78468 and random forest - 0.76555**","25d86985":"## 4.3 Stochastic Gradient Descent (SGD)","cda60d1b":"Naive Bayes doesn't have any hyperparameters to tune","5baf3aa5":"## 2.4 Fare","0518f27b":"## 4.5 Gaussian Naive Bayes","d6c5b1e0":"## 5.2 With XGBClassifier","0b12fda7":"## 4.2 RandomForestClassifier ","a7908a77":"# 2. Plotting","91769bbe":"# 4. Modeling","0e04c347":"## 4.1 LogisticRegression. With under-sampling or no under-sampling","6e0ff436":"## 4.6 Perceptron","ebbeb780":"# 3. Data engineering","c2c78219":"## 2.6 Name","fe05e9ae":"## 4.8 XGBoost","2351d083":"## 2.1 Sex","cf418753":"## 2.7 SibSp","2c40ef29":"## 4.7 Decision Tree","edb79445":"## 2.5 Embarked","4e54e949":"# 1. Functions and get data","d2e762b6":"## 2.8 Parch","a683a092":"# 2 EXP","1550f6ee":"# 1. EXP"}}