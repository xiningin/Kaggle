{"cell_type":{"a76e33c5":"code","ad15c344":"code","9800123b":"code","978c6c97":"code","6dbe86a0":"code","cfa3de84":"code","40d62516":"code","495254ec":"code","5e054dad":"code","55bed351":"code","8a46363d":"code","49cce5ac":"code","7f139398":"code","87a4780a":"code","1e3578ab":"code","84931a7e":"code","63d11849":"code","cef8daba":"markdown","a58ca7ab":"markdown","7f0292ae":"markdown","fb54051e":"markdown","08d7fff1":"markdown","13b66595":"markdown","0e07a3f7":"markdown","fc1ec5d2":"markdown","be317532":"markdown","c4876239":"markdown","ca3db8a7":"markdown","d6541945":"markdown","3a3d74e5":"markdown","086c94ed":"markdown","5f0e3b5b":"markdown","6b3b1068":"markdown","434a4085":"markdown","ee5e0845":"markdown","b39a5395":"markdown"},"source":{"a76e33c5":"import numpy as np\nimport collections\nimport mxnet as mx\nfrom mxnet import nd, autograd\nfrom matplotlib import pyplot as plt\nfrom multiprocessing import cpu_count","ad15c344":"def transform(data, label):\n    return data.astype(np.float32)\/126.0, label.astype(np.float32)","9800123b":"batch_size = 64\n\nX_train = np.load('..\/input\/kuzushiji\/kmnist-train-imgs.npz')['arr_0']\ny_train = np.load('..\/input\/kuzushiji\/kmnist-train-labels.npz')['arr_0']\nX_train, y_train = transform(X_train, y_train)\n\nX_test = np.load('..\/input\/kuzushiji\/kmnist-test-imgs.npz')['arr_0']\ny_test = np.load('..\/input\/kuzushiji\/kmnist-test-labels.npz')['arr_0']\nX_test, y_test = transform(X_test, y_test)\n\ntrain_arr = mx.gluon.data.dataset.ArrayDataset(X_train, y_train)\ntest_arr = mx.gluon.data.dataset.ArrayDataset(X_test, y_test)\n\ntrain_data = mx.gluon.data.DataLoader(train_arr, batch_size, shuffle=True)\ntest_data = mx.gluon.data.DataLoader(test_arr, batch_size, shuffle=False)","978c6c97":"num_train = sum([batch_size for i in train_data])\nnum_batches = num_train \/ batch_size\n\nnum_layers = 2\nnum_hidden = 400\n\nnum_inputs = 784\nnum_outputs = 10","6dbe86a0":"def relu(X):\n    return nd.maximum(X, nd.zeros_like(X))\n\ndef net(X, layer_params):\n    layer_input = X\n    for i in range(len(layer_params) \/\/ 2 - 2):\n        h_linear = nd.dot(layer_input, layer_params[2*i]) + layer_params[2*i + 1]\n        layer_input = relu(h_linear)\n    # last layer without ReLU\n    output = nd.dot(layer_input, layer_params[-2]) + layer_params[-1]\n    return output","cfa3de84":"layer_param_shapes = []\nfor i in range(num_layers + 1):\n    if i == 0: # input layer\n        W_shape = (num_inputs, num_hidden)\n        b_shape = (num_hidden,)\n    elif i == num_layers: # last layer\n        W_shape = (num_hidden, num_outputs)\n        b_shape = (num_outputs,)\n    else: # hidden layers\n        W_shape = (num_hidden, num_hidden)\n        b_shape = (num_hidden,)\n    layer_param_shapes.extend([W_shape, b_shape])","40d62516":"def log_softmax_likelihood(yhat_linear, y):\n    return nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)","495254ec":"log2pi = np.log(2.0 * np.pi)\n\ndef log_gaussian(x, mu, sigma):\n    return -0.5 * log2pi - nd.log(sigma) - (x - mu) ** 2 \/ (2 * sigma ** 2)\n\ndef gaussian_prior(x):\n    sigma_p = nd.array([config['sigma_p']], ctx=ctx)\n    return nd.sum(log_gaussian(x, 0., sigma_p))","5e054dad":"def gaussian(x, mu, sigma):\n    scaling = 1.0 \/ nd.sqrt(2.0 * np.pi * (sigma ** 2))\n    bell = nd.exp(- (x - mu) ** 2 \/ (2.0 * sigma ** 2))\n\n    return scaling * bell\n\ndef scale_mixture_prior(x):\n    sigma_p1 = nd.array([config['sigma_p1']], ctx=ctx)\n    sigma_p2 = nd.array([config['sigma_p2']], ctx=ctx)\n    pi = config['pi']\n\n    first_gaussian = pi * gaussian(x, 0., sigma_p1)\n    second_gaussian = (1 - pi) * gaussian(x, 0., sigma_p2)\n\n    return nd.log(first_gaussian + second_gaussian)","55bed351":"def combined_loss(output, label_one_hot, params, mus, sigmas, log_prior, log_likelihood):\n\n    # Calculate data likelihood\n    log_likelihood_sum = nd.sum(log_likelihood(output, label_one_hot))\n\n    # Calculate prior\n    log_prior_sum = sum([nd.sum(log_prior(param)) for param in params])\n\n    # Calculate variational posterior\n    log_var_posterior_sum = sum([nd.sum(log_gaussian(params[i], mus[i], sigmas[i])) for i in range(len(params))])\n\n    # Calculate total loss\n    return 1.0 \/ num_batches * (log_var_posterior_sum - log_prior_sum) - log_likelihood_sum","8a46363d":"def SGD(params, lr):\n    for param in params:\n        param[:] = param - lr * param.grad","49cce5ac":"def evaluate_accuracy(data_iterator, net, layer_params):\n    numerator = 0.\n    denominator = 0.\n    for i, (data, label) in enumerate(data_iterator):\n        data = data.as_in_context(ctx).reshape((-1, 784))\n        label = label.as_in_context(ctx)\n        output = net(data, layer_params)\n        predictions = nd.argmax(output, axis=1)\n        numerator += nd.sum(predictions == label)\n        denominator += data.shape[0]\n    return (numerator \/ denominator).asscalar()","7f139398":"weight_scale = .1\nrho_offset = -3\n\n# gaussian parameters\nconfig = {\n    \"sigma_p\": 1.0,\n    \"sigma_p1\": 0.75,\n    \"sigma_p2\": 0.1,\n}\n\n# mxnet cpu initialization\nctx = mx.cpu()\n\n# initialize variational parameters; mean and variance for each weight\nmus = []\nrhos = []\n\nfor shape in layer_param_shapes:\n    mu = nd.random_normal(shape=shape, ctx=ctx, scale=weight_scale)\n    rho = rho_offset + nd.zeros(shape=shape, ctx=ctx)\n    mus.append(mu)\n    rhos.append(rho)\n\nvariational_params = mus + rhos\n\nfor param in variational_params:\n    param.attach_grad()","87a4780a":"# 1. Sample \u03f5\u223cN(0,Id)\ndef sample_epsilons(param_shapes):\n    epsilons = [nd.random_normal(shape=shape, loc=0., scale=1.0, ctx=ctx) for shape in param_shapes]\n    return epsilons\n\n# 2. Transform \u03c1 to a postive vector via the softplus function\n# \u03c3 = softplus(\u03c1) = log(1 + exp(\u03c1))\ndef softplus(x):\n    return nd.log(1. + nd.exp(x))\n\ndef transform_rhos(rhos):\n    return [softplus(rho) for rho in rhos]\n\n# 3. Compute element-wise multiplication\n# w: w = \u03bc + \u03c3 \u2218 \u03f5\ndef transform_gaussian_samples(mus, sigmas, epsilons):\n    samples = []\n    for j in range(len(mus)):\n        samples.append(mus[j] + sigmas[j] * epsilons[j])\n    return samples","1e3578ab":"epochs = 10\nlearning_rate = 0.001\nsmoothing_constant = .01\n\ntrain_acc = []\ntest_acc = []","84931a7e":"for e in range(epochs):\n    for i, (data, label) in enumerate(train_data):\n        data = data.as_in_context(ctx).reshape((-1, 784))\n        label = label.as_in_context(ctx)\n        label_one_hot = nd.one_hot(label, 10)\n\n        with autograd.record():\n            # sample epsilons from standard normal\n            epsilons = sample_epsilons(layer_param_shapes)\n\n            # compute softplus for variance\n            sigmas = transform_rhos(rhos)\n\n            # obtain a sample from q(w|theta) by transforming the epsilons\n            layer_params = transform_gaussian_samples(mus, sigmas, epsilons)\n\n            # forward-propagate the batch\n            output = net(data, layer_params)\n\n            # calculate the loss\n            loss = combined_loss(output, label_one_hot, layer_params, mus, sigmas, gaussian_prior, log_softmax_likelihood)\n\n        # backpropagate for gradient calculation\n        loss.backward()\n\n        # apply stochastic gradient descent to variational parameters\n        SGD(variational_params, learning_rate)\n\n        # calculate moving loss for monitoring convergence\n        curr_loss = nd.mean(loss).asscalar()\n        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n\n\n    test_accuracy = evaluate_accuracy(test_data, net, mus)\n    train_accuracy = evaluate_accuracy(train_data, net, mus)\n    train_acc.append(train_accuracy.item())\n    test_acc.append(test_accuracy.item())\n    print(\"Epoch:{:2d}. Loss:{:4.4f}, Train_acc:{:4.4f}, Test_acc:{:4.4f}\"\n          .format(e, moving_loss, train_accuracy, test_accuracy))","63d11849":"_ = plt.figure(figsize=(14,8))\n_ = plt.plot(train_acc, label='Train Accuracy')\n_ = plt.plot(test_acc, label='Test Accuracy')\nplt.legend()\nplt.show()","cef8daba":"## Neural Network","a58ca7ab":"<h1 id=\"optimizer\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Optimizer\n        <a class=\"anchor-link\" href=\"#optimizer\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","7f0292ae":"## Bayes Prior\n\nBayesian treatment for the network - prior over the weights.\n\n![gaussian.png](attachment:bac20f36-e09e-4232-b946-9bce45953d0c.png)","fb54051e":"## Scale mixture prior\n\nInstead of a single Gaussian, we use a scale of mixture prior to P(w).\n\n![mixture.png](attachment:10f502db-250a-4c38-a832-7eba5107255e.png)","08d7fff1":"<h1 id=\"analyze\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","13b66595":"<h1 id=\"training\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","0e07a3f7":"<h1 id=\"accuracy\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Accuracy\n        <a class=\"anchor-link\" href=\"#accuracy\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","fc1ec5d2":"<h1 id=\"nn\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Neural Network \n        <a class=\"anchor-link\" href=\"#nn\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","be317532":"<h1 id=\"parameters\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Parameters\n        <a class=\"anchor-link\" href=\"#parameters\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","c4876239":"## Learnable parameters","ca3db8a7":"## Likelihood","d6541945":"# Combined Loss\n\nLikelihood + Prior + Variational Posterior\n\n![combined.png](attachment:7417955c-fef8-413b-a5b5-d16639ad61c5.png)","3a3d74e5":"<h1 id=\"bayes\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Bayes Prior\/Posterior Lost\n        <a class=\"anchor-link\" href=\"#bayes\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","086c94ed":"<h1 id=\"paramter\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Parameter Initialization\n        <a class=\"anchor-link\" href=\"#paramter\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","5f0e3b5b":"<h1 id=\"gaussian_train\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Gaussian training\n        <a class=\"anchor-link\" href=\"#gaussian_train\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","6b3b1068":"<h1 id=\"reference\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Reference\n        <a class=\"anchor-link\" href=\"#reference\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>\n\n[MXNet GLUON - Bayes Backprop Explanation](https:\/\/gluon.mxnet.io\/chapter18_variational-methods-and-uncertainty\/bayes-by-backprop.html)","434a4085":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/13578\/logos\/header.png\" \/>\n<\/div>","ee5e0845":"![bbb_nn_bayes.png](attachment:9831901f-9148-4337-9d1f-dc43dc0bf479.png)","b39a5395":"<h1 id=\"dataset\" style=\"color:#ad5746; background:#b79677; border:0.5px dotted #b05144;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}