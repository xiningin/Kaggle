{"cell_type":{"d0617a35":"code","2e0b01d8":"code","b0709795":"code","43a93252":"code","39b2e1af":"code","41c58c29":"code","4adb6101":"code","cc49c1e9":"code","58cd069e":"code","83ef3013":"code","33887bc5":"code","b18bba57":"code","729d53e7":"code","9c49cc6c":"code","1498a0fd":"code","01655a9f":"code","277b3452":"code","c5db95ce":"code","ec03c2eb":"code","7ec53460":"code","084eb53a":"code","9e0bdb4f":"code","f992bd45":"code","f156fc0a":"code","7f76972c":"code","4fce94fd":"code","594ad8b8":"code","91037f43":"code","64e42b37":"code","380c7604":"code","c807a506":"code","d3abff6c":"code","cb427556":"code","581073e1":"code","6aa66c01":"code","8ecf8d60":"code","06dd268f":"code","19ba2995":"code","87aaf184":"code","02f6278e":"code","9f21792e":"code","9c5cd9ab":"code","7cab002c":"code","4c30aac2":"code","1c52d9d8":"code","6a5be890":"code","4fe30a55":"code","d73646e1":"code","2866902c":"code","6a385d3c":"code","b7b30a3f":"code","da531623":"code","e6b00f5a":"code","1dbb86f9":"code","180898b4":"code","4cffdb77":"code","503caedb":"code","221bc628":"code","6e6db761":"code","7378c414":"code","a4e12d87":"code","bf3c0ad7":"code","9ab6a832":"code","60fdae5c":"code","bd4a4c9a":"code","5990faee":"code","032ac368":"code","dad40fef":"code","0ff12b2b":"markdown","32805fde":"markdown","ffb9ebb4":"markdown","14e84ae2":"markdown","64e94751":"markdown","82c7f332":"markdown","f19bdc7c":"markdown","fa8aaec5":"markdown","54828b8f":"markdown","aff3e3cb":"markdown","8febcb59":"markdown","74b181f3":"markdown"},"source":{"d0617a35":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.preprocessing import OneHotEncoder\nfrom datetime import date\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","2e0b01d8":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\nGDP_data = pd.read_csv('\/kaggle\/input\/tps-jan22\/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\nholiday_data = pd.read_csv('\/kaggle\/input\/tps-jan22\/Holidays_Finland_Norway_Sweden_2015-2019.csv')","b0709795":"# drop 29th Feb in 2016 (leap year)\ntrain.drop(train[train['date']=='2016-02-29'].index, axis=0, inplace=True)","43a93252":"# Concatinating data in order to simplify processing\ntrain_test_df = train.drop('num_sold',axis=1)\ntrain_test_df = pd.concat([train_test_df,test],axis=0)\ntrain_test_df.tail()","39b2e1af":"# nth day in the dataset(important to learn trend)\ntrain_start_date = date.fromisoformat(train.loc[0,'date'])\ntrain_test_df['date_index'] = train_test_df.date.map(lambda x: (date.fromisoformat(x) - train_start_date).days)","41c58c29":"# Encoding\nhot_encode = OneHotEncoder()\nhot_encode.fit(train_test_df[['country','store','product']])\ntrain_test_df[['Finland','Norway', 'Sweden','KaggleMart','KaggleRama','Kaggle Hat','Kaggle Mug', 'Kaggle Sticker']] = hot_encode.transform(train_test_df[['country','store','product']]).toarray()","4adb6101":"train_test_df['date'] = pd.to_datetime(train_test_df.date)\ntrain_test_df = train_test_df.set_index('date')\ntrain_test_df = train_test_df.to_period('D')","cc49c1e9":"## Validation function\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","58cd069e":"fourier = CalendarFourier(freq=\"A\", order=16)  # 10 sin\/cos pairs for \"A\"nnual seasonality\n\ndp = DeterministicProcess(\n    index=train_test_df.index,\n    constant=True,               # dummy feature for bias (y-intercept)\n    order=1,                     # trend (order 1 means linear)\n    seasonal=True,               # weekly seasonality (indicators)\n    additional_terms=[fourier],  # annual seasonality (fourier)\n    drop=True,                   # drop terms to avoid collinearity\n)\n\nfourier_features_df = dp.in_sample()  # create features for dates in tunnel.index\nfourier_features_df.head(2)","83ef3013":"temp_df = train_test_df.drop(['row_id','country','store','product'],axis=1)\nX_Season = pd.concat([fourier_features_df,temp_df],axis=1)\ny = train['num_sold']","33887bc5":"\nfor country in list(train.country.unique()):\n    for product in list(train['product'].unique()):\n        for store in list(train['store'].unique()):\n            for fourier_feature in list(fourier_features_df.columns):\n                feature_name = country[0:3] +' ' + product.split(' ')[1]+' ' + store[-4:]+ ' ' +fourier_feature \n                #print(feature_name)\n                #print(hot_encoded_features[country]*hot_encoded_features[product]*hot_encoded_features[store]*fourier_features_df[fourier_feature])\n                X_Season[feature_name] =  list(train_test_df[country]*train_test_df[product]*train_test_df[store]*fourier_features_df[fourier_feature])\n                \n       ","b18bba57":"X_Season = X_Season.drop(['Finland','KaggleMart','Kaggle Hat'],axis=1)\nX_test = X_Season.loc['2019-01-01':]\nX = X_Season.loc[:'2018-12-31']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\nX_train.tail(1)","729d53e7":"y = train['num_sold']\nlinear_model_ver1 = LinearRegression(fit_intercept=False)\nlinear_model_ver1.fit(X_train,y_train)\ntrain_y_pred = linear_model_ver1.predict(X)\ntest_y_pred = linear_model_ver1.predict(X_test)\nval_y_pred = linear_model_ver1.predict(X_val)\ntrain['linear_model_ver1'] = train_y_pred\ntest['linear_model_ver1'] = test_y_pred","9c49cc6c":"score_ver1 = SMAPE(y_val, val_y_pred)\nprint('score-',score_ver1)","1498a0fd":"grouped = train.groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\n#fig, ax = plt.subplots(figsize=(20,10))\ncnt = 0\nfor key, grp in grouped:\n    ax = ax_list[cnt]    \n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver1'], label=key)\n    ax.legend()\n \n    cnt = cnt+1","01655a9f":"X_Season['year_bias'] = list(X_Season.index.year-2015)\nX_Season['month_bias'] = X_Season.index.month + 12*X_Season['year_bias']\nfor country in list(train.country.unique()):\n    for product in list(train['product'].unique()):\n        for store in list(train['store'].unique()):\n            feature_name = country[0:3] +' ' + product.split(' ')[1]+' ' + store[-4:]+ ' ' +'year_bias'\n            feature_name_2 = country[0:3] +' ' + product.split(' ')[1]+' ' + store[-4:]+ ' ' +'month_bias'\n            X_Season[feature_name] =  list(train_test_df[country]*train_test_df[product]*train_test_df[store]*X_Season['year_bias'])\n            X_Season[feature_name_2] =  list(train_test_df[country]*train_test_df[product]*train_test_df[store]*X_Season['month_bias'])","277b3452":"X_test = X_Season.loc['2019-01-01':]\nX = X_Season.loc[:'2018-12-31']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\nX_train.tail(1)","c5db95ce":"y = train['num_sold']\nlinear_model_ver2 = LinearRegression(fit_intercept=False)\nlinear_model_ver2.fit(X_train,y_train)\ntrain_y_pred = linear_model_ver2.predict(X)\ntest_y_pred = linear_model_ver2.predict(X_test)\nval_y_pred = linear_model_ver2.predict(X_val)\ntrain['linear_model_ver2'] = train_y_pred\ntest['linear_model_ver2'] = test_y_pred","ec03c2eb":"score_ver2 = SMAPE(y_val, val_y_pred)\nprint('previous_score-',score_ver1,'new_score-',score_ver2,'improvement-',-(score_ver2-score_ver1)\/score_ver1*100,'%')","7ec53460":"grouped = train.groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\ncnt = 0\nfor key, grp in grouped:\n    ax = ax_list[cnt]\n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver1'], label=[key,'ver1'])\n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver2'], label=[key,'ver2'])\n    ax.legend()\n \n    cnt = cnt+1","084eb53a":"\nGDP_data.Finland = GDP_data.Finland.map(lambda x: (x-42802)\/42802)\nGDP_data.Norway = GDP_data.Norway.map(lambda x: (x-74356)\/74356)\nGDP_data.Sweden = GDP_data.Sweden.map(lambda x: (x-51545)\/51545)\nGDP_data.head(5)\n\n\n# log(GDP) didnt give good results for me\n'''\nGDP_data.Finland = GDP_data.Finland.map(lambda x: np.log(x))\nGDP_data.Norway = GDP_data.Norway.map(lambda x: np.log(x))\nGDP_data.Sweden = GDP_data.Sweden.map(lambda x: np.log(x))\nGDP_data.head(5)\n'''","9e0bdb4f":"X_Season['GDP'] = list(train_test_df.reset_index().apply(lambda x:GDP_data[GDP_data['year']==x.date.year][x['country']].values[0],axis=1))\ntrain_test_df['GDP'] = X_Season['GDP']","f992bd45":"X_test = X_Season.loc['2019-01-01':]\nX = X_Season.loc[:'2018-12-31']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\nX_train.tail(1)","f156fc0a":"train['year'] = train.date.map(lambda x: x.split('-')[0])\nnum_sold_grp = train.groupby(['country','store','product','year'])['num_sold'].mean()\n\n\nGDP_based_num_sold_pred = pd.DataFrame(index=['2015','2016','2017','2018','2019'])\nfor country in list(train.country.unique()):\n    for store in list(train['store'].unique()):\n        for product in list(train['product'].unique()):\n            y_dummy = list(num_sold_grp[country][store][product])[:-1]\n            X_dummy = list(GDP_data[country])[:-2]\n            #X_dummy = [np.log(i) for i in X_dummy]\n            y_dummy = np.array(y_dummy,ndmin=2).transpose()\n            y_dummy_true = np.array(list(num_sold_grp[country][store][product]),ndmin=2).transpose()\n            X_dummy = np.array(X_dummy,ndmin=2).transpose()\n            num_sold_mean_model = LinearRegression(fit_intercept=True)\n            num_sold_mean_model.fit(X_dummy,y_dummy)\n            X_dummy_test = np.array(list(GDP_data[country]),ndmin=2).transpose()\n            y_dummy_pred = num_sold_mean_model.predict(X_dummy_test)\n            feature_name = country[0:3]+' ' + store[-4:] +' ' + product.split(' ')[1]\n            GDP_based_num_sold_pred[feature_name] = y_dummy_pred\n            plt.figure(figsize=(10,10))\n            plt.plot(['2015','2016','2017','2018'],y_dummy_true,label=[country,store,product,'True mean of yearly sales'])\n            plt.plot(['2015','2016','2017','2018','2019'],y_dummy_pred, label='Predicted mean yearly sales from GDP')\n            plt.legend()\n            plt.grid()\n            plt.show()","7f76972c":"train_test_df['year'] = train_test_df.index.year\ntrain_test_df['GDP_based_num_sold'] = train_test_df.apply(lambda x: GDP_based_num_sold_pred[x.country[0:3]+' ' + x.store[-4:] +' ' + x['product'].split(' ')[1]][str(x.year)],axis=1)\nX_Season['GDP_based_num_sold'] = train_test_df['GDP_based_num_sold']\nX_Season = X_Season.drop('GDP',axis=1)","4fce94fd":"\nX_test = X_Season.loc['2019-01-01':]\nX = X_Season.loc[:'2018-12-31']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\nX_train.tail(1)","594ad8b8":"y = train['num_sold']\nlinear_model_ver3 = LinearRegression(fit_intercept=False)\nlinear_model_ver3.fit(X_train,y_train)\ntrain_y_pred = linear_model_ver3.predict(X)\ntest_y_pred = linear_model_ver3.predict(X_test)\nval_y_pred = linear_model_ver3.predict(X_val)\ntrain['linear_model_ver3'] = train_y_pred\ntest['linear_model_ver3'] = test_y_pred","91037f43":"grouped = train.groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\ncnt = 0\nfor key, grp in grouped:\n    ax = ax_list[cnt]\n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver1'], label=[key,'ver1'])\n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver3'], label=[key,'ver3'])\n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver2'], label=[key,'ver2'])\n    ax.legend()\n \n    cnt = cnt+1","64e42b37":"score_ver3 = SMAPE(y_val, val_y_pred)\nprint('previous_score-',score_ver2,'new_score-',score_ver3,'improvement-',-(score_ver3-score_ver2)\/score_ver2*100,'%')","380c7604":"\ntrain['year'] = train.date.map(lambda x: int(x.split('-')[0]))\ntest['year'] = test.date.map(lambda x: int(x.split('-')[0]))\ntrain['month'] = train.date.map(lambda x: int(x.split('-')[1]))\ntrain['day'] = train.date.map(lambda x: int(x.split('-')[2]))\ntrain['dayofyear'] = train.date.map(lambda x: pd.to_datetime(x).dayofyear)\n\ngrouped = train[train['year']==2015].groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols*2, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\ncnt = 0\n\n\n# Plot normalized error month wise \nfor key, grp in grouped:\n    ax = ax_list[cnt]\n    max_value = np.max(grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver3'])\n    ax.plot(grp[grp['month']==12]['dayofyear'], (grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver3'])\/max_value, label=[key,'December'])\n    ax.legend() \n    cnt = cnt+1\n    ax = ax_list[cnt]\n    ax.plot(grp[grp['month']==1]['dayofyear'], (grp[grp['month']==1]['num_sold']-grp[grp['month']==1]['linear_model_ver3'])\/max_value, label=[key,'January'])\n    ax.legend() \n    cnt = cnt+1\n","c807a506":"\ndef gaussian(x, mu, sig):\n    return np.exp(-np.power(x - mu, 2.) \/ (2 * np.power(sig, 2.)))\n\nX_Season['dec_jan_holdy'] = X_Season.index.day_of_year.map(lambda x: gaussian(x, 363, 5\/3) if x>355  else(gaussian(x, 0, 5\/3) if x<10  else 0) )\ntrain['dec_jan_holdy'] = train.dayofyear.map(lambda x: gaussian(x, 363, 5\/3) if x>355  else(gaussian(x, 0, 5\/3) if x<10  else 0) )\nX_Season['xmas_pre'] = X_Season.index.day_of_year.map(lambda x: x-350 if ((x>350) & (x<360))  else 0 )\nX_Season['xmas_post'] = X_Season.index.day_of_year.map(lambda x: x-359 if ((x>359) & (x<364))  else 0 )\nX_Season['newyear'] = X_Season.index.day_of_year.map(lambda x: x-363 if (x>363) else (x+2 if x<7  else 0) )\ntrain['dec_jan_pre_holdy'] = train.dayofyear.map(lambda x: x-350 if ((x>350) & (x<360))  else 0 )\n\nfor country in list(train.country.unique()):\n    for product in list(train['product'].unique()):\n        for store in list(train['store'].unique()):\n            feature_name = country[0:3] +' ' + product.split(' ')[1]+' ' + store[-4:]+ ' ' +'dec_holdy'\n            X_Season[feature_name] =  list(train_test_df[country]*train_test_df[product]*train_test_df[store]*X_Season['dec_jan_holdy'])\n","d3abff6c":"grouped = train.groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols*2, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\ncnt = 0\n# Plot normalized error month wise \nfor key, grp in grouped:\n    #print(grp['num_sold'].min(),grp['num_sold'].max())\n    ax = ax_list[cnt]\n    max_value = np.max(grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver3'])\n    ax.scatter(grp[grp['month']==12]['dayofyear'], (grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver3'])\/max_value, label=[key,'December'])\n    ax.scatter(grp[grp['month']==12]['dayofyear'], (grp[grp['month']==12]['dec_jan_holdy']), label=[key,'December'])\n    \n    \n    cnt = cnt+1\n    ax = ax_list[cnt]\n    ax.scatter(grp[grp['month']==1]['dayofyear'], (grp[grp['month']==1]['num_sold']-grp[grp['month']==1]['linear_model_ver3'])\/max_value, label=[key,'Januarary'])\n    ax.scatter(grp[grp['month']==1]['dayofyear'], (grp[grp['month']==1]['dec_jan_holdy']), label=[key,'December'])    \n    ax.legend()\n \n    cnt = cnt+1","cb427556":"import math\nimport dateutil.easter as easter\n\ndef get_holidays(df):\n    # End of year\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"n-dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"f-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                      for d in range(1, 14)}),\n        pd.DataFrame({f\"jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(1, 10)}),\n        pd.DataFrame({f\"s-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                      for d in range(1, 15)})], axis=1)\n    \n    # May\n    df = pd.concat([df, pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                      for d in list(range(1, 10))}),\n        pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in list(range(19, 26))})], axis=1)\n    \n    # June and July\n    df = pd.concat([df, pd.DataFrame({f\"june{d}\":\n                   (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                   for d in list(range(8, 14))})], axis=1)\n    \n    #Swedish Rock Concert\n    #Jun 3, 2015 \u2013 Jun 6, 2015\n    #Jun 8, 2016 \u2013 Jun 11, 2016\n    #Jun 7, 2017 \u2013 Jun 10, 2017\n    #Jun 6, 2018 \u2013 Jun 10, 2018\n    #Jun 5, 2019 \u2013 Jun 8, 2019\n    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n                                         2016: pd.Timestamp(('2016-06-11')),\n                                         2017: pd.Timestamp(('2017-06-10')),\n                                         2018: pd.Timestamp(('2018-06-10')),\n                                         2019: pd.Timestamp(('2019-06-8'))})\n\n    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n                                      for d in list(range(-3, 3))})], axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n                   (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(-4, 6))})], axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\": \n                   (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(0, 9))})], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                   (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                   for d in list(range(6, 14))})], axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n                   (df.date - easter_date == np.timedelta64(d, \"D\"))\n                   for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df\n\ntrain_copy = train.copy()\ntrain_copy['date'] = pd.to_datetime(train_copy.date)\nholidays_train_df = get_holidays(train_copy)\n#holidays_train_df = holidays_train_df.set_index('date')\n\ntest_copy = test.copy()\ntest_copy['date'] = pd.to_datetime(test_copy.date)\nholidays_test_df = get_holidays(test_copy)\n#holidays_test_df = holidays_test_df.set_index('date')\n\nholidays_train_df = holidays_train_df.drop(['row_id', 'date', 'country',  'store',  'product',  'num_sold',\n 'linear_model_ver1',  'linear_model_ver2',  'year',  'linear_model_ver3',  'month',\n 'day',  'dayofyear',  'dec_jan_holdy',  'dec_jan_pre_holdy'],axis=1)\nholidays_test_df = holidays_test_df.drop(['row_id',  'date', 'country',  'store',  'product',  'linear_model_ver1',\n 'linear_model_ver2',  'linear_model_ver3',  'year'],axis=1)","581073e1":"holidays_test_df.columns","6aa66c01":"'''\ndef unofficial_hol(df):\n    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n    \n    # load holiday info.\n    hol_path = '..\/input\/public-and-unofficial-holidays-nor-fin-swe-201519\/holidays.csv'\n    holiday = pd.read_csv(hol_path)\n    \n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df\n\ntrain_copy = train.copy()\ntrain_copy['date'] = pd.to_datetime(train_copy.date)\ntrain_unofficial_hol = unofficial_hol(train_copy)\ntrain_unofficial_hol = get_interactions(train_unofficial_hol)\ntrain_unofficial_hol = get_GDP(train_unofficial_hol)\n\n\n#holidays_train_df = holidays_train_df.set_index('date')\n\ntest_copy = test.copy()\ntest_copy['date'] = pd.to_datetime(test_copy.date)\ntest_unofficial_hol = unofficial_hol(test_copy)\ntest_unofficial_hol = get_interactions(test_unofficial_hol)\ntest_unofficial_hol = get_GDP(test_unofficial_hol)\n#holidays_test_df = holidays_test_df.set_index('date')\n\n\ntrain_unofficial_hol = train_unofficial_hol.drop(['row_id', 'date', 'country',  'store',  'product',  'num_sold',\n 'linear_model_ver1',  'linear_model_ver2',  'year',  'linear_model_ver3',  'month',\n 'day',  'dayofyear',  'dec_jan_holdy',  'dec_jan_pre_holdy'],axis=1)\ntest_unofficial_hol = test_unofficial_hol.drop(['row_id',  'date', 'country',  'store',  'product',  'linear_model_ver1',\n 'linear_model_ver2',  'linear_model_ver3','year'],axis=1)\n \n'''\n","8ecf8d60":"#list(train_unofficial_hol.columns)","06dd268f":"X_test = X_Season.loc['2019-01-01':]\nX = X_Season.loc[:'2018-12-31']\nholidays_train_df =holidays_train_df.set_index(X.index)\nholidays_train_df.astype(np.float32)\n#train_unofficial_hol.astype(np.float32)\n#train_unofficial_hol =train_unofficial_hol.set_index(X.index)\n#X=pd.concat([X,holidays_train_df,train_unofficial_hol],axis=1)\nX=pd.concat([X,holidays_train_df],axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\n\nholidays_test_df =holidays_test_df.set_index(X_test.index)\nholidays_test_df.astype(np.float32)\n#test_unofficial_hol.astype(np.float32)\n#test_unofficial_hol =test_unofficial_hol.set_index(X_test.index)\n#X_test=pd.concat([X_test,holidays_test_df,test_unofficial_hol],axis=1)\nX_test=pd.concat([X_test,holidays_test_df],axis=1)\n\n\ny = train['num_sold']\nlinear_model_ver4 = LinearRegression(fit_intercept=False)\nlinear_model_ver4.fit(X_train,y_train)\ntrain_y_pred = linear_model_ver4.predict(X)\ntest_y_pred = linear_model_ver4.predict(X_test)\nval_y_pred = linear_model_ver4.predict(X_val)\ntrain['linear_model_ver4'] = train_y_pred\ntest['linear_model_ver4'] = test_y_pred","19ba2995":"X_train.head()","87aaf184":"grouped = train[train['year']==2015].groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols*2, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\ncnt = 0\n\n\n# Plot normalized error month wise \nfor key, grp in grouped:\n    ax = ax_list[cnt]\n    max_value = 1 #np.max(grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver4'])\n    max_value_2 = max_value #np.max(grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver3'])\n    ax.plot(grp[grp['month']==12]['dayofyear'], (grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver4'])\/max_value, label=[key,'December ver4'])\n    ax.plot(grp[grp['month']==12]['dayofyear'], (grp[grp['month']==12]['num_sold']-grp[grp['month']==12]['linear_model_ver3'])\/max_value, label=[key,'December ver3'])\n    #ax.scatter(grp[grp['month']==12]['dayofyear'], (grp[grp['month']==12]['dec_jan_holdy']), label=[key,'December'])\n    ax.grid()\n    cnt = cnt+1\n    ax = ax_list[cnt]\n    ax.plot(grp[grp['month']==1]['dayofyear'], (grp[grp['month']==1]['num_sold']-grp[grp['month']==1]['linear_model_ver4'])\/max_value, label=[key,'Januarary ver4'])\n    ax.plot(grp[grp['month']==1]['dayofyear'], (grp[grp['month']==1]['num_sold']-grp[grp['month']==1]['linear_model_ver3'])\/max_value, label=[key,'Januarary ver3'])\n    #ax.scatter(grp[grp['month']==1]['dayofyear'], (grp[grp['month']==1]['dec_jan_holdy']), label=[key,'December'])    \n    ax.legend()\n    ax.grid()\n \n    cnt = cnt+1","02f6278e":"score_ver4 = SMAPE(y_val, val_y_pred)\nprint('previous_score-',score_ver3,'new_score-',score_ver4,'improvement-',-(score_ver4-score_ver3)\/score_ver3*100,'%')","9f21792e":"from sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nlin_reg_pred_X_train = linear_model_ver4.predict(X_train)\nlin_reg_pred_X_val = linear_model_ver4.predict(X_val)\nlin_reg_pred_X_test = linear_model_ver4.predict(X_test)\n\n#lin_reg_pred_X_train = linear_model_ver3.predict(X_train)\n#lin_reg_pred_X_val = linear_model_ver3.predict(X_val)\n#lin_reg_pred_X_test = linear_model_ver3.predict(X_test)\n\n# residuals ==> (true_value - linear_model_prediction)\nresidual_y_train = y_train - lin_reg_pred_X_train\nresidual_y_val = y_val - lin_reg_pred_X_val","9c5cd9ab":"X_Season['month'] = X_Season.index.month\nX_Season['day'] = X_Season.index.day\nX_Season['year'] = X_Season.index.year\n\n\nX_boosting = train_test_df[['Finland','Norway', 'Sweden', 'KaggleMart', 'KaggleRama', 'Kaggle Hat','Kaggle Mug', 'Kaggle Sticker', 'year']]\nX_boosting.loc[:,'day'] = X_Season['day']\nX_boosting.loc[:,'month'] = X_Season.index.month\nX_boosting.loc[:,'day_of_year'] = X_Season.index.day_of_year\nX_boosting.loc[:,'day_of_week'] = X_Season.index.day_of_week\nX_boosting.loc[:,'week']=X_Season.index.weekofyear     # 1 to 53\nX_boosting.loc[:,'week']=X_boosting['week'].astype('int')             # int64\n\nX_boosting.tail(1)\n\n\n\n\n\n","7cab002c":"X_boosting['dec_hol'] = X_Season.dec_jan_holdy","4c30aac2":"X_test = X_boosting.loc['2019-01-01':]\nX = X_boosting.loc[:'2018-12-31']\nholidays_train_df =holidays_train_df.set_index(X.index)\n#train_unofficial_hol =train_unofficial_hol.set_index(X.index)\nholidays_train_df.astype(np.float32)\n#X=pd.concat([X,holidays_train_df],axis=1)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)\nholidays_test_df =holidays_test_df.set_index(X_test.index)\n#test_unofficial_hol =test_unofficial_hol.set_index(X_test.index)\nholidays_test_df.astype(np.float32)\n#X_test=pd.concat([X_test,holidays_test_df],axis=1)\n\n\n","1c52d9d8":"X_test.head()","6a5be890":"gbrt_ver1 = GradientBoostingRegressor(max_depth=10, n_estimators=150,learning_rate=0.1)\ngbrt_ver1.fit(X_train, residual_y_train)\nresidual_y_val_pred = gbrt_ver1.predict(X_val)\nerrors = [mean_squared_error(residual_y_val, residual_y_val_pred) for residual_y_val_pred in gbrt_ver1.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors)\ngbrt_best = GradientBoostingRegressor(max_depth=10,n_estimators=bst_n_estimators)\ngbrt_best.fit(X_train, residual_y_train)","4fe30a55":"plt.plot(errors,'r-', label='Error change with n_estimators')\nplt.grid()","d73646e1":"gbrt_best.predict(X)\ntrain_y_pred = gbrt_best.predict(X)\ntest_y_pred = gbrt_best.predict(X_test)\nval_y_pred = gbrt_best.predict(X_val)\ntrain['residual_for_ver1'] = train_y_pred\n","2866902c":"score_ver5 = SMAPE(y_val, lin_reg_pred_X_val+val_y_pred)\nprint('previous_score-',score_ver4,'new_score-',score_ver5,'improvement-',-(score_ver5-score_ver4)\/score_ver4*100,'%')","6a385d3c":"'''\noutput = pd.DataFrame()\noutput['row_id'] = test.row_id\noutput['num_sold'] = test.linear_model_ver4+test_y_pred\noutput.to_csv('\/kaggle\/working\/submission.csv',index=False)\n'''","b7b30a3f":"\nax = plt.hist(train['num_sold']-train['linear_model_ver4']-train_y_pred,bins=[-200,-100,-75,-50,-25,0,25,50,75,100,200])\n","da531623":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nxgboost_reg = XGBRegressor()\nparameters_grid = [{'max_depth':[12,10,15], 'n_estimators':[100], 'learning_rate':[0.15,0.2,0.25]}]\nxboost_grid_search = GridSearchCV(xgboost_reg,parameters_grid,cv=3)\nxboost_grid_search.fit(X_train, np.array(residual_y_train))\n","e6b00f5a":"xgb = xboost_grid_search.best_estimator_\nxgb","1dbb86f9":"xgb.predict(X)\ntrain_y_pred = xgb.predict(X)\ntest_y_pred = xgb.predict(X_test)\nval_y_pred = xgb.predict(X_val)\ntrain['residual_for_ver2'] = train_y_pred","180898b4":"score_ver6 = SMAPE(y_val, lin_reg_pred_X_val+val_y_pred)\nprint('previous_score-',score_ver5,'new_score-',score_ver6,'improvement-',-(score_ver5-score_ver4)\/score_ver4*100,'%')","4cffdb77":"ax = plt.hist(train['num_sold']-train['linear_model_ver4']-train_y_pred,bins=[-200,-100,-75,-50,-25,0,25,50,75,100,200])","503caedb":"'''\noutput = pd.DataFrame()\noutput['row_id'] = test.row_id\noutput['num_sold'] = test.linear_model_ver4+test_y_pred\noutput.to_csv('\/kaggle\/working\/submission.csv',index=False)\n'''","221bc628":"from xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport time\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nclass HybridModel:\n    def __init__(self, model_1, model_2, grid=None):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.grid=grid\n        \n    def fit(self, X_train_1, X_train_2, y):\n        # Train model 1\n        self.model_1.fit(X_train_1, y)\n        \n        # Predictions from model 1 (trend)\n        y_trend = self.model_1.predict(X_train_1)\n\n        if self.grid:\n            # Grid search\n            tscv = TimeSeriesSplit(n_splits=3)\n            grid_model = GridSearchCV(estimator=self.model_2, cv=tscv, param_grid=self.grid)\n        \n            # Train model 2 on detrended series\n            grid_model.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = grid_model.predict(X_train_2)\n            \n            # Save model\n            self.grid_model=grid_model\n        else:\n            # Train model 2 on residuals\n            self.model_2.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = self.model_2.predict(X_train_2)\n        \n        # Save data\n        self.y_train_trend = y_trend\n        self.y_train_resid = y_resid\n        \n    def predict(self, X_test_1, X_test_2):\n        # Predict trend using model 1\n        y_trend = self.model_1.predict(X_test_1)\n        \n        if self.grid:\n            # Grid model predictions\n            y_resid = self.grid_model.predict(X_test_2)\n        else:\n            # Model 2 predictions\n            y_resid = self.model_2.predict(X_test_2)\n        \n        # Add predictions together\n        y_pred = y_trend + y_resid\n        \n        # Save data\n        self.y_test_trend = y_trend\n        self.y_test_resid = y_resid\n        \n        return y_pred","6e6db761":"X_test = X_Season.loc['2019-01-01':]\nX = X_Season.loc[:'2018-12-31']\nX_train_1=pd.concat([X,holidays_train_df],axis=1)\nX_test_1=pd.concat([X_test,holidays_test_df],axis=1)\n\n## boosting\nX_test_2 = X_boosting.loc['2019-01-01':]\nX_train_2 = X_boosting.loc[:'2018-12-31']\n","7378c414":"model_1=LinearRegression()\nmodels_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0)]\n\n# Parameter grid\nparam_grid = {'n_estimators': [10,30,50,75],\n        'max_depth': [2,3,4, 5],\n        'learning_rate': [0.04,0.06,0.08,0.09]}\n\n# Initialise output vectors\ny_pred=np.zeros(len(X_test_1))\ntrain_preds=np.zeros(len(y))\n\n# Ensemble predictions\nfor model_2 in models_2:\n    # Start timer\n    start = time.time()\n    \n    # Construct hybrid model\n    model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n    model.fit(X_train_1, X_train_2, np.log(y))\n\n    # Save predictions\n    y_pred += np.exp(model.predict(X_test_1,X_test_2))\n    \n    # Training set predictions (for residual analysis)\n    train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n    stop = time.time()\n    \n    print(f'Model_2:{model_2} -- time:{round((stop-start)\/60,2)} mins')\n    \n    if model.grid:\n        print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\ny_pred = y_pred\/len(models_2)\ntrain_preds = train_preds\/len(models_2)","a4e12d87":"score_ver7 = SMAPE(y, train_preds)\nscore_ver7","bf3c0ad7":"print((train['num_sold']-train_preds).mean())\n(train['num_sold']-train_preds).std()","9ab6a832":"print((train['num_sold']-train_preds).mean())\n(train['num_sold']-train_preds).std()","60fdae5c":"\nprint((train['num_sold']-train['linear_model_ver4']-train_y_pred).mean())\n(train['num_sold']-train['linear_model_ver4']-train_y_pred).std()","bd4a4c9a":"ax = plt.hist(train['num_sold']-train_preds,bins=[-200,-100,-75,-50,-25,0,25,50,75,100,200])","5990faee":"ax = plt.hist(train['num_sold']-train_preds,bins=[-200,-100,-75,-50,-25,0,25,50,75,100,200])","032ac368":"train['ensemble_ver_1'] = train_preds\ngrouped = train.groupby(['country','store','product'])\nncols=1\nnrows = int(np.ceil(grouped.ngroups\/ncols))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20,50), sharey=True)\nax_list = axes.flatten()\nprint(axes.flatten)\ncnt = 0\nfor key, grp in grouped:\n    ax = ax_list[cnt]\n    ax.plot(grp['row_id'], grp['num_sold']-grp['ensemble_ver_1'], label='pred')\n    ax.plot(grp['row_id'], grp['num_sold']-grp['linear_model_ver4']+grp['residual_for_ver1'], label='pred gb')\n    #ax.plot(grp['row_id'], grp['num_sold'], label='true')\n    \n    \n\n    ax.legend()\n \n    cnt = cnt+1","dad40fef":"output = pd.DataFrame()\noutput['row_id'] = test.row_id\noutput['num_sold'] = y_pred\noutput.to_csv('\/kaggle\/working\/submission.csv',index=False)","0ff12b2b":"**Model Version 1 - Linear Regression**\n1. Fourier Features + Trend\n2. Fourier Features x [Country,Product,Store] wise\n\n**Model Version 2 - Linear Regression**\n1. Year & Monthly bias feature added\n\n**Model Version 3 - Linear Regression**\n1. GDP based pseudo num_sold included as seperate feature \n(feature added)\n\n**Model Version 4 - Linear Regression + Gradient Boost**\n1. Feature Set for Gradient boosting is smaller subset of that used for Linear Regression \n\n**Model Version 5 - Linear Regression + Gradient Boost**\n1. Added Holiday Feature from @Ambrosm\n2. Removed 29th Feb in 2016","32805fde":"**Version 4 - Gradient Boosting Algorithm to learn the residual errors**","ffb9ebb4":"**Holidays**\n1. Dip just before Xmas,arnd 360th day, shows increase in sales compared to normal sales(zero line).\n2. After Xmas till new year(360 to 365), peak in error represents drop in sales compared to normal sales(zero line) during this period\n3. It becomes important to model before and after effects of holidays, hence, thats the reason most of the topper notebooks have used day-to-holidays as features in the notebook. ","14e84ae2":"**Version 2**","64e94751":"**Importing Libraries**","82c7f332":"**Fourier Feature for each [Country, Product, Store] pair** ","f19bdc7c":"**Version 3**","fa8aaec5":"**Import data**\n- Train & Test data\n- GDP data \n- Holiday data (@AMBROSM)","54828b8f":"**Output Plots from model version 1**","aff3e3cb":"**Fourier Features to capture Seasonality**","8febcb59":"**Pre-processing data**","74b181f3":"**Features to capture the holiday affect including before and after impact on sales**"}}