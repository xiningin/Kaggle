{"cell_type":{"e5ca5ca3":"code","78fa654b":"code","f8210e25":"code","1e42a8a5":"code","5889cb23":"code","49c15744":"code","d530e981":"code","5c87be0a":"code","fad400a9":"code","e353299e":"code","4d5321a3":"code","7497c369":"code","ef25fa47":"code","eee8b46b":"code","46e10eec":"code","cd516ead":"code","01df84f0":"code","5191c87f":"code","4564b628":"code","bd0c11cb":"code","67d68c00":"code","2bb58b95":"code","018ffb4e":"code","a270b241":"code","1622c874":"code","deb6e154":"code","07c0cda7":"code","7c6e5e1f":"code","70e17d66":"code","16529fda":"code","8549400c":"code","340fd136":"code","15bc4714":"code","cf00f161":"code","5600a60e":"code","b8a4ee94":"markdown","7d823426":"markdown","ac62f7b3":"markdown","041c6b1c":"markdown","1189f272":"markdown","061988e8":"markdown","1ecf387e":"markdown","18556815":"markdown","97ef67f5":"markdown","ad7ae4d5":"markdown","d09e62b7":"markdown","897e18e5":"markdown","62a19f81":"markdown","e4ddcae1":"markdown","ef0a633e":"markdown","e9253cfc":"markdown","b1ac75c7":"markdown","e18d857a":"markdown","47a214d0":"markdown","8b8c8dd6":"markdown","c65ed279":"markdown","b3686596":"markdown","99c62b91":"markdown","afc21393":"markdown","7bb7114d":"markdown","5fe7d548":"markdown","ef729cbe":"markdown","15e11021":"markdown","3eab2b2c":"markdown","d9d1b38b":"markdown","5b2ac3bf":"markdown","7cfec8cb":"markdown","3df434b1":"markdown","20640513":"markdown","b684e62a":"markdown","90615576":"markdown","bbb63762":"markdown","9d936eaa":"markdown","ced87234":"markdown","f81f0544":"markdown","e5f0fc05":"markdown","b6ec0bc0":"markdown","141aacd9":"markdown","3371aa3f":"markdown","06a4ee2f":"markdown","65c769c9":"markdown","2918585a":"markdown","6e9358d2":"markdown","76253738":"markdown","8f35a79a":"markdown","cbf45613":"markdown","e23e2132":"markdown","b6307a4b":"markdown","e9e9f533":"markdown","afe508c6":"markdown","50f7139a":"markdown","d2a19726":"markdown","2cd9d4eb":"markdown","18ccb9f4":"markdown","9b10fc20":"markdown"},"source":{"e5ca5ca3":"#Import delle librerie richieste per il funzionamento\nimport itertools\nimport pandas as pd  \nimport numpy as np  \nimport matplotlib.pyplot as plt  \nimport seaborn as seabornInstance \nfrom sklearn import metrics\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\n%matplotlib inline","78fa654b":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ncol_names=['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']\ndataset = pd.read_csv('..\/input\/breast-cancer-csv\/breastCancer.csv', header=0, names = col_names)\narray = dataset.values\n\ndataset.head()","f8210e25":"#Rimozione della feature _Sample code number_\ndataset.drop(['Sample code number'],axis = 1, inplace = True)","1e42a8a5":"dataset.describe()","5889cb23":"dataset.info()","49c15744":"dataset.replace('?',0, inplace=True)","d530e981":"from sklearn.impute import SimpleImputer\n# Conversione del DataFrame in array NumPy per applicare il metodo Imputer().\nvalues = dataset.values\n\nimputer = SimpleImputer()\nimputedData = imputer.fit_transform(values)","5c87be0a":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nnormalizedData = scaler.fit_transform(imputedData)","fad400a9":"#Split del dataset in X e Y\nX = normalizedData[:,0:9] #Esclude la colonna relativa alla classe\nY = normalizedData[:,9] #Considera esclusivamente la classe come feature target\n\n#Impostazione del seed\nseed = 1","e353299e":"names = dataset.columns\n\n# plot correlation matrix\nimport seaborn as sns\n#Using Pearson Correlation\nplt.figure(figsize=(10,10))\ncor = dataset.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.BuGn)\nplt.show()","4d5321a3":"from sklearn.model_selection import train_test_split\n\ny = dataset['Class']\ndataset.drop(['Class'], axis = 1, inplace = True)","7497c369":"from sklearn.preprocessing import StandardScaler\nx = dataset\n\nsc_x = StandardScaler()\nx = sc_x.fit_transform(x)","ef25fa47":"x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=42, shuffle=True)","eee8b46b":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(x_train)","46e10eec":"asse_y = pca.explained_variance_ratio_\nasse_x = np.arange(len(asse_y)) + 1\n\nplt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.bar(asse_x, asse_y)\nplt.plot(asse_x, asse_y, \"ro-\")\nplt.title(\"Variance for each component\")\nplt.xticks(asse_x, [\"Comp.\"+str(i) for i in asse_x], rotation=90)\nplt.ylabel(\"Variance\")\n\ny2 = np.cumsum((pca.explained_variance_ratio_))\n\nplt.subplot(1, 2, 2)\nplt.bar(asse_x,y2)\nplt.plot(asse_x, y2, \"ro-\")\nplt.xticks(asse_x, [\"Comp.\"+str(i) for i in asse_x], rotation=90)\nplt.title(\"Cumulative Variance\")\nplt.ylabel(\"Variance\")\nplt.show()","cd516ead":"pca = PCA(n_components=5)\npca.fit(x_train)\nx_train = pca.transform(x_train)\nx_test = pca.transform(x_test)\n\n#Stampa dei valori della PCA\nprint(pca.components_)\nprint(pca.explained_variance_)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\nprint(pca.singular_values_>=1)","01df84f0":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\ndef plot_accuracy(used_model, best_config_gs, best_config_random_search):\n    model = used_model(**best_config_gs)\n\n    #Suddivisione delle dimensioni del training_set e raccoglimento del training_score e test_score\n    #Impostando il valore di cross_validation = 10\n\n    #Train_sizes: controlla il numero assoluto o relativo di esempi di training utilizzati per generare le curve di apprendimento. \n    #train_sizes = np.linspace (0.1, 1.0, 10) per usare 10 punti equidistanti per le dimensioni del set di dati di allenamento.\n    train_sizes, train_scores, test_scores =\\\n                    learning_curve(estimator=model,\n                                   X=x_train,\n                                   y=y_train,\n                                   train_sizes=np.linspace(0.1, 1.0, 10),\n                                   cv=10,\n                                   n_jobs=-1)\n\n    #Calcolo delle medie e delle deviazioni standard sul training e test\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    #Plot dei valori della GridSearch() sul grafico\n    plt.figure(figsize=(15,5))\n    plt.subplot(1, 2, 1)\n    \n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='Training accuracy')\n\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='Validation accuracy')\n\n    #la funzione fill_between indica la varianza della stima colorando la regione di piano\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.3, 1.03])\n    plt.tight_layout()\n    \n    #RandomSearch e relativo grafico\n    model = used_model(**best_config_random_search)\n\n    #Suddivisione delle dimensioni del training_set e raccoglimento del training_score e test_score\n    #Impostando il valore di cross_validation = 10\n\n    #Train_sizes: controlla il numero assoluto o relativo di esempi di training utilizzati per generare le curve di apprendimento. \n    #train_sizes = np.linspace (0.1, 1.0, 10) per usare 10 punti equidistanti per le dimensioni del set di dati di allenamento.\n    train_sizes, train_scores, test_scores =\\\n                    learning_curve(estimator=model,\n                                   X=x_train,\n                                   y=y_train,\n                                   train_sizes=np.linspace(0.1, 1.0, 10),\n                                   cv=10,\n                                   n_jobs=-1)\n\n    #Calcolo delle medie e delle deviazioni standard sul training e test\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n    plt.subplot(1, 2, 2)\n    #Plot dei valori della GridSearch() sul grafico\n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='Training accuracy')\n\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='Validation accuracy')\n\n    #la funzione fill_between indica la varianza della stima colorando la regione di piano\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.3, 1.03])\n    plt.tight_layout()\n    \n    plt.show()","5191c87f":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n\ngrid={'tol':[0.001,0.01,0.1,1], 'eta0': [0.001,0.01,0.1,1], 'penalty': ['l2','l1','elasticnet']}\n\nppn = Perceptron(random_state=1, n_jobs = -1, max_iter=1000)\nppn_cv=GridSearchCV(ppn,grid,cv=5)\nppn_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",ppn_cv.best_params_)\nbest_config_gs = ppn_cv.best_params_\nprint(\"Accuracy CV:\",ppn_cv.best_score_)\nppn_cv = ppn_cv.best_estimator_\nprint('Test accuracy: %.3f' % ppn_cv.score(x_test, y_test))\n\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(ppn, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(Perceptron, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","4564b628":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n\nx_train_lda = x_train\ny_train_lda = y_train\n\n\ngrid={'solver': ['lsqr', 'eigen'], 'tol':[0.001,0.01,0.1,1]}\n\nlda = LDA()\nlda_cv=GridSearchCV(lda,grid,cv=5)\nlda_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",lda_cv.best_params_)\nbest_config_gs = lda_cv.best_params_\nprint(\"Accuracy CV:\",lda_cv.best_score_)\nlda_cv = lda_cv.best_estimator_\nprint('Test accuracy: %.3f' % lda_cv.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(lda, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(LDA, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","bd0c11cb":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression(n_jobs = -1)\nregr.fit(x_train, y_train)\n\nprint('Accuracy of Linear regression classifier on training set: {:.2f}'\n     .format(regr.score(x_train, y_train)))\nprint('Accuracy of Linear regression classifier on test set: {:.2f}'\n     .format(regr.score(x_test, y_test)))","67d68c00":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_features= PolynomialFeatures(degree=3)\nx_poly_train = polynomial_features.fit_transform(x_train)\nx_poly_test = polynomial_features.fit_transform(x_test)\n\nmodel = LinearRegression(n_jobs = -1)\nmodel.fit(x_poly_train, y_train)\n\nprint('Accuracy of Linear regression classifier on training set: {:.2f}'\n     .format(model.score(x_poly_train, y_train)))\nprint('Accuracy of Linear regression classifier on test set: {:.2f}'\n     .format(model.score(x_poly_test, y_test)))","2bb58b95":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nsolvers = ['newton-cg', 'lbfgs', 'sag', 'saga']\nmulticlass_values = ['auto', 'ovr', 'multinomial']\nC = [x for x in range(10,100+1,20)]\n\ngrid={'solver':solvers, 'multi_class':multiclass_values, 'C':C}\nlogreg=LogisticRegression(random_state=1, max_iter=500)\nlogreg_cv=GridSearchCV(logreg,grid,cv=5)\nlogreg_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",logreg_cv.best_params_)\nbest_config_gs = logreg_cv.best_params_\nprint(\"Accuracy CV:\",logreg_cv.best_score_)\nlogreg_be = logreg_cv.best_estimator_\nprint('Test accuracy: %.3f' % logreg_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(logreg, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(LogisticRegression, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","018ffb4e":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\ngrid={'n_estimators':[x for x in range(1000,2000+1,500)], 'criterion':['mae','mse'], 'max_features':['auto', 'sqrt', 'log2']}\nforest = RandomForestRegressor(random_state=1, n_jobs=-1)\nforest_cv=GridSearchCV(forest,grid,cv=5)\nforest_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",forest_cv.best_params_)\nbest_config_gs = forest_cv.best_params_\nprint(\"Accuracy CV:\",forest_cv.best_score_)\nforest_be = forest_cv.best_estimator_\nprint('Test accuracy: %.3f' % forest_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(forest, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(RandomForestRegressor, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","a270b241":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=1.0)\n\nridge.fit(x_train, y_train)\ny_train_pred = ridge.predict(x_train)\ny_test_pred = ridge.predict(x_test)","1622c874":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","deb6e154":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha=1.0)\n\nlasso.fit(x_train, y_train)\ny_train_pred = lasso.predict(x_train)\ny_test_pred = lasso.predict(x_test)","07c0cda7":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","7c6e5e1f":"from sklearn.linear_model import ElasticNet\nelanet = ElasticNet(alpha=1.0, l1_ratio=0.5)\n\nelanet.fit(x_train, y_train)\ny_train_pred = elanet.predict(x_train)\ny_test_pred = elanet.predict(x_test)","70e17d66":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nprint('MSE train: %.3f, test: %.3f' % (\n        mean_squared_error(y_train, y_train_pred),\n        mean_squared_error(y_test, y_test_pred)))\nprint('R^2 train: %.3f, test: %.3f' % (\n        r2_score(y_train, y_train_pred),\n        r2_score(y_test, y_test_pred)))","16529fda":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ngrid={'criterion':['gini','entropy'], 'splitter':['best', 'random']}\n\nclf = DecisionTreeClassifier()\nclf_cv=GridSearchCV(clf,grid,cv=5)\nclf_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",clf_cv.best_params_)\nbest_config_gs = clf_cv.best_params_\nprint(\"Accuracy CV:\",clf_cv.best_score_)\nclf_be = clf_cv.best_estimator_\nprint('Test accuracy: %.3f' % clf_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 2\nrandom_search = RandomizedSearchCV(clf, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(DecisionTreeClassifier, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","8549400c":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nestimators = [x for x in range(5,50+1,5)]\ncriterion_list = ['gini','entropy']\ngrid={'n_estimators':estimators, 'criterion':criterion_list}\n\nforest = RandomForestClassifier(random_state=1, n_jobs=-1)\nforest_cv=GridSearchCV(forest,grid,cv=5)\nforest_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",forest_cv.best_params_)\nbest_config_gs = forest_cv.best_params_\nprint(\"Accuracy CV:\",forest_cv.best_score_)\nforest_cv = forest_cv.best_estimator_\nprint('Test accuracy: %.3f' % forest_cv.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(forest, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(RandomForestClassifier, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","340fd136":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\ngrid={'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], 'weights':['uniform', 'distance'], 'n_neighbors':[x for x in range(1,20+1)]}\n\nknn = KNeighborsClassifier(n_jobs = -1)\nknn_cv = GridSearchCV(knn,grid,cv=5)\nknn_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",knn_cv.best_params_)\nbest_config_gs = knn_cv.best_params_\nprint(\"Accuracy CV:\",knn_cv.best_score_)\nknn_cv = knn_cv.best_estimator_\nprint('Test accuracy: %.3f' % knn_cv.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 10\nrandom_search = RandomizedSearchCV(knn, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(KNeighborsClassifier, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","15bc4714":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\n\nprint('Accuracy of GNB classifier on training set: {:.2f}'\n     .format(gnb.score(x_train, y_train)))\nprint('Accuracy of GNB classifier on test set: {:.2f}'\n     .format(gnb.score(x_test, y_test)))","cf00f161":"# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\nkernel_list = ['linear', 'poly', 'rbf', 'sigmoid']\n\ngrid={'kernel':kernel_list}\nsvm=SVC()\nsvm_cv=GridSearchCV(svm,grid,cv=5)\nsvm_cv.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"GridSearch():\\n\")\ncombinazioni = 1\nfor x in grid.values():\n    combinazioni *= len(x)\nprint('Per l\\'applicazione della GridSearch ci sono {} combinazioni'.format(combinazioni))\nprint(\"Migliore configurazione: \",svm_cv.best_params_)\nbest_config_gs = svm_cv.best_params_\nprint(\"Accuracy CV:\",svm_cv.best_score_)\nsvm_be = svm_cv.best_estimator_\nprint('Test accuracy: %.3f' % svm_be.score(x_test, y_test))\n\n#RandomizedSearch\nn_iter_search = 2\nrandom_search = RandomizedSearchCV(svm, param_distributions=grid, n_iter=n_iter_search, cv=5, n_jobs =-1)\nrandom_search.fit(x_train,y_train)\n\n#Stampa degli attributi best_score sul modello e i parametri con cui quel 'best_score' \u00e8 stato ottenuto\nprint(\"\\n\\nRandomizedSearch():\\n\")\nprint(\"Migliore configurazione: \",random_search.best_params_)\nbest_config_random_search = random_search.best_params_\nprint(\"Accuracy CV:\",random_search.best_score_)\n\nrandom_search = random_search.best_estimator_\nprint('Test accuracy: %.3f' % random_search.score(x_test, y_test))\n\nplot_accuracy(SVC, best_config_gs, best_config_random_search) #Stampo il grafico dell'accuratezza","5600a60e":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\npred = knn_cv.predict(x_test)\n\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","b8a4ee94":"Implementa un modello che prevede l'utilizzo di un percettrone, il concetto alla base delle reti neurali.\nTale percettrone in fase di training viene inizializzato con dei pesi, i quali vengono utilizzati per fare predizione.\nIn base ai risultati ottenuti, ad ogni iterazione viene calcolata una perdita (loss), che indica il tasso di errore che si sta commettendo nella predizione.\nIn base a tale metrica, vengono poi modificati i pesi in base alla funzione di attivazione che si sta utilizzando, realizzando una tecnica nota come back-propagation.\n\nL'apprendimento termina quando si raggiunge una soglia minima di errore pre-impostata, o quando si raggiunge un numero massimo di iterazioni.\n\nParametri testati:\n- <b>tol<\/b>: Criterio di stop. Le iterazioni terminano quando loss > previous_loss - tol.\n- <b>eta0<\/b>: Costante con la quale gli updates sono moltiplicati\n- <b>penality<\/b>: termine di regolarizzazione da usare:\n    - <b>l2<\/b>\n    - <b>l1<\/b>\n    - <b>elasticnet<\/b>","7d823426":"Un classificatore lineare con un confine di decisione generato dal fitting delle densit\u00e0 condizionali ai dati usando la regola di Bayes.\n\nIl modello fitta una densit\u00e0 Gaussiana per ogni classe, assumendo che tutte le classi condividono la stessa matrice di covarianza.\n\nIl modello fittato pu\u00f2 anche essere usato per ridurre la dimensionalit\u00e0 dell'input proiettando le sue direzioni pi\u00f9 discriminative, utilizzando il metodo transform.\n\nAttraverso LDA si sfrutta la conoscenza delle classi (variabile target del problema). In particolare, si sfrutta la matrice X delle features e le informazioni che si hanno sulle Y.\nSi deve risolvere un problema di classificazione. L'obiettivo \u00e8 di porre un altro vincolo: la proiezione deve tenere le classi separate tra loro.\nL\u2019obiettivo \u00e8 di trovare la direzione w che vada a massimizzare la distanza (favorendo la separazione tra le due classi), minimizzando la varianza (presente al denominatore, massimizzando la coesione all'interno della classe.)\n\u2022  m1 (la proiezione) si ottiene da w e dalla media dell'intera classe 1. \n\u2022  Idem anche su m2\n\u2022  Il denominatore \u00e8 calcolato mediante gli scarti quadratici med\n\nIn contrasto con PCA, LDA tenta di trovare un sottospazio di features che massimizzi la separabilit\u00e0 delle classi.\nLDA fa ipotesi su classi distribuite normalmente e covarianze di classi uguali.\n(La PCA tende a produrre risultati di classificazione migliori in un'attivit\u00e0 di riconoscimento delle immagini se il numero di campioni per una data classe era relativamente piccolo.)\n\nParametri testati:\n- <b>solver<\/b>: risolutore da utilizzare. Si pu\u00f2 scegliere tra:\n    - <b>svd (default)<\/b>:  singular value decomposition. Non viene calcolata la matrice di covarianza, tuttavia \u00e8 necessario modificare questo parametro quando si \u00e8 in presenza di un gran numero di features.\n    - <b>lsqr<\/b>: soluzione ai minimi quadrati\n    - <b>eigen<\/b>: decomposizione di autovalori\n- <b>tol<\/b>: soglia in valore assoluto per i valori singolari da considerare significanti in X. Le dimensioni i cui valori singolari non sono significanti sono scartati. Pu\u00f2 essere utilizzato solo con svd.\n- <b>n_components<\/b>: numero di componenti da tenere in considerazione per la riduzione della dimensionalit\u00e0.","ac62f7b3":"Una volta caricato il dataset, al fine di poter applicare la PCA, si identifica con _y_ la feature target, ovvero _brand_, mentre X rappresenter\u00e0 tutte le restanti features presenti all'interno del dataset.","041c6b1c":"Nel seguente snippet di codice viene utilizzato il modello LinearRegressione della libreria sklearn per trovare la migliore retta di regressione, minimizzando i quadrati dei residui.\nIn questo caso, non si tratta di regressione lineare semplice perch\u00e8 i dati in input sono multidimensionali.","1189f272":"Dal metodo _info()_ si evince che per la feature _Bare Nuclei_ ci sono dei valori che fanno divenire il tipo di dati da int64 a object. Infatti, analizzando il contenuto della feature _Bare Nuclei_ si evince la presenza di valori _?_.\nAl fine di trattare valori mancanti, si utilizzer\u00e0 la _Mean Imputation_ mentre per la gestione di tali caratteri ? verr\u00e0 convertito tale valore con 0.","061988e8":"### Ridge Regression","1ecf387e":"## Regressione Logistica","18556815":"# Utilizzo di metodi regolarizzati per la regressione","97ef67f5":"Al fine di ottenere una descrizione complessiva del Dataframe (e dunque del relativo dataset) caricato, mediante il metodo _info()_ si sono ottenute le seguenti informazioni:\n- <b>#<\/b>: numero di feature presente nel DataFrame\n- <b>Column<\/b>: intestazione delle features nel DataFrame\n- <b>Non-Null Count<\/b>: contatore di valori non nulli per ogni feature presente nel DataFrame\n- <b>Dtype<\/b>: tipo di dato memorizzato per ogni feature presente nel DataFrame","ad7ae4d5":"Gli approcci popolari per i modelli lineare di regressione regolarizzata sono: Ridge Regression, least absolute shrinkage and selection operator (LASSO), and elastic Net.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543418451\/tradeoff_sevifm.png)","d09e62b7":"### Caricamento in memoria del dataset","897e18e5":"### Applicazione della PCA","62a19f81":"## Random Forest","e4ddcae1":"## Regressione Lineare","ef0a633e":"### Normalizzazione delle features\nVista la presenza di dati espressi su un diverso range numerico, si effettua la standardizzazione, mediante apposito metodo StandardScaler().","e9253cfc":"## Analisi esplorativa del dataset","b1ac75c7":"La regressione lineare \u00e8 una tecnica di analisi predittiva di base che utilizza dati raccolti per prevedere una variabile di output. <\/br>L'idea alla base della regressione lineare \u00e8 che se risulta possibile adattare un modello di regressione lineare ai casi osservati, allora \u00e8 possibile utilizzare tale modello per prevedere eventuali valori futuri.\n\nPer semplicit\u00e0 si descrive il funzionamento di un modello di regressione lineare semplice:\n\nIn un modello di regressione lineare esistono due tipi di variabili:\n- <b>Variabile di input<\/b>: sono le variabili che aiutano a prevedere il valore della variabile di output. Viene comunemente indicata con X\n- <b>Variabile di output<\/b>: \u00e8 la variabile che si vuole prevedere. Viene comunemente indicata con Y.\n\nPer stimare Y, usando la regressione lineare, si sfrutta la seguente equazione: ${Y\\_predicted = a0+a1*X}$.<\/br>L'obiettivo \u00e8 trovare valori statisticamente significativi dei parametri $a0$ e $a1$ che minimizzino la differenza tra $Y$ e $Y\\_predicted$ sfruttando una tecnica di minimizzazione dell'errore.<\/br>\nSe si \u00e8 in grado di determinare i valori ottimali per tali parametri, allora si otterr\u00e0 la retta di _best fit_ che \u00e8 possibile utilizzare per prevedere i valori di Y dato X.","e18d857a":"## Support Vector Machine","47a214d0":"## Matrice di confusione per K-nn","8b8c8dd6":"### Split del dataset","c65ed279":"## Gaussian Naive Bayes","b3686596":"L'analisi delle componenti principali (PCA) \u00e8 una tecnica che consente l'estrazione di informazioni rilevanti da un insieme di dati numerici. Consente, inoltre, di rivelare l'esistenza di relazioni lineari (nascoste) in dati multidimensionali.<\/br>\nLa PCA, inoltre, consente di effettuare la _low rank approximization_ di una matrice di dati.\n\nPCA, in presenza di dataset ad alta dimensionalit\u00e0, consente di ridurre quest'ultima in, generalmente, 2 o 3 dimensioni consentendo la rappresentazione di dati multivariati su grafici 2D o 3D.\n\nUn open problem relativo a questa tecnica \u00e8 l'individuazione del numero di componenti principali da considerare: solitamente, attraverso uno scree plot, si individua il gomito del plotting e quest'ultimo indicher\u00e0 il numero delle componenti principali da considerare. Alternativamente, \u00e8 possibile definire una percentuale di varianza spiegata dei dati [70%-90%] che verr\u00e0 raggiunta sommando le varianze di ogni componente principale identificata, poste in ordine decrescente.\n\nUlteriore metodologia, invece, \u00e8 rappresentata dal cerchio delle correlazioni che \u00e8 una rappresentazione grafica utile per avere una idea delle variabili che contribuiscono positivamente o negativamente a spiegare la varianza della PC1 e PC2. Valori di loadings grandi associati a specifiche variabili indicano il contributo di queste ultime rispetto alle PC's.\n\nMediante il metodo di Kaiser, invece, \u00e8 possibile ottenere euristicamente la stima del numero delle PC's da considerare prendendo in considerazioni gli autovalori che presentano un valore numerico maggiore di 1.","99c62b91":"Ottenuto il grafico delle varianze spiegate per ogni singola componente, il numero delle componenti principali da prendere in considerazione \u00e8 fissato a 3 (per le motivazioni sopra riportate).\n\nFissato il numero delle componenti, si effettua il fitting del modello e la trasformazione della matrice X di train (identificata con _x\\_train_) e della matrice X di test (_x\\_test_)\n\nSi procede, successivamente alla stampa delle seguenti informazioni:\n- Componenti identificate dalla PCA\n- Varianza spiegata da ciascuna componente\n- Varianza spiegata in percentuale da ciascuna componente\n- Valori singolari identificati","afc21393":"### Standardizzazione dei dati","7bb7114d":"Per ottenere informazioni statistiche inerenti ciascuna feature a disposizione, mediante il metodo _describe()_ si \u00e8 provveduto al calcolo delle seguenti informazioni:\n- <b>count<\/b>: conteggio del numero di esempi per la feature selezionata\n- <b>mean<\/b>: media aritmetica per la feature selezionata\n- <b>std<\/b>: deviazione standard per la feature selezionata\n- <b>min<\/b>: valore minimo presentato dagli esempi per la feature selezionata\n- <b>25%<\/b>: primo quartile calcolato sugli esempi per la feature selezionata\n- <b>50%<\/b>: secondo quartile calcolato sugli esempi per la feature selezionata\n- <b>75%<\/b>: terzo quartile calcolato sugli esempi per la feature selezionata\n- <b>max<\/b>: valore massimo presentato dagli esempi per la feature selezionata","5fe7d548":"Gli alberi decisionali (DT) sono un metodo di apprendimento supervisionato non parametrico utilizzato per la classificazione e la regressione.\nL'obiettivo \u00e8 creare un modello che preveda il valore di una variabile target apprendendo semplici regole decisionali dedotte dalle caratteristiche dei dati.\n\nAlcuni vantaggi degli alberi decisionali sono:\n- Semplice da capire e da interpretare. Gli alberi possono essere visualizzati.\n- Potrebbe richiedere poca preparazione dei dati. Altre tecniche spesso richiedono la normalizzazione dei dati, la creazione di variabili fittizie e la rimozione dei valori vuoti. Si noti tuttavia che questo modulo non supporta i valori mancanti.\n- Il costo dell'utilizzo dell'albero (ovvero la previsione dei dati) \u00e8 logaritmico nel numero di punti dati utilizzati per addestrare l'albero.\n- In grado di gestire dati sia numerici che categoriali. Altre tecniche sono solitamente specializzate nell'analisi di set di dati che hanno un solo tipo di variabile. Vedi algoritmi per maggiori informazioni.\n- In grado di gestire problemi con pi\u00f9 output.\n- Se una data situazione \u00e8 osservabile in un modello, la spiegazione della condizione \u00e8 facilmente spiegabile dalla logica booleana.\n- Possibilit\u00e0 di validare un modello utilizzando test statistici. Ci\u00f2 consente di tenere conto dell'affidabilit\u00e0 del modello.\n\nGli svantaggi degli alberi decisionali includono:\n- Gli studenti che apprendono l'albero decisionale possono creare alberi troppo complessi che non generalizzano bene i dati. Questo si chiama overfitting. Per evitare questo problema sono necessari meccanismi come la potatura, l'impostazione del numero minimo di campioni richiesto su un nodo fogliare o l'impostazione della profondit\u00e0 massima dell'albero.\n- Gli alberi decisionali possono essere instabili perch\u00e9 piccole variazioni nei dati potrebbero comportare la generazione di un albero completamente diverso. Questo problema viene mitigato utilizzando alberi decisionali all'interno di un insieme.\n- Il problema dell'apprendimento di un albero decisionale ottimale \u00e8 noto per essere NP-completo sotto diversi aspetti dell'ottimalit\u00e0 e anche per concetti semplici. Di conseguenza, gli algoritmi pratici di apprendimento dell'albero decisionale sono basati su algoritmi euristici come l'algoritmo greedy in cui vengono prese decisioni ottimali a livello locale in ogni nodo. Tali algoritmi non possono garantire la restituzione dell'albero decisionale ottimale a livello globale. Ci\u00f2 pu\u00f2 essere mitigato addestrando pi\u00f9 alberi in un gruppo di studenti, in cui le caratteristiche e i campioni vengono campionati in modo casuale con la sostituzione.\n- Ci sono concetti che sono difficili da imparare perch\u00e9 gli alberi decisionali non li esprimono facilmente, come XOR, parit\u00e0 o problemi di multiplexer.\n- Si potrebbero creare alberi sbilanciati. Si consiglia pertanto di bilanciare il set di dati prima di adattarlo all'albero decisionale.\n\nParametri testati:\n- <b>criterion<\/b>: indica la funzione da utilizzare per la misurazione della qualit\u00e0 di uno split. I criteri supportati sono:\n    - <b>gini<\/b>: misura di impurit\u00e0\n    - <b>entropy<\/b>: information gain\n- <b>splitter<\/b>: la strategia usata per scegliere lo split ad ogni nodo. Le strategie supportate sono:\n    - <b>best<\/b>: seleziona il miglior split\n    - <b>random<\/b>: sceglie in modo casuale","ef729cbe":"### Gestione valori nulli","15e11021":"Elastic Net \u00e8 emerso per la prima volta a seguito di una critica alla Lasso Regression, la cui feature selection potrebbe essere troppo dipendente dai dati, rendendo il modello instabile.\n\nLa soluzione \u00e8 quella di combinare le penalit\u00e0 della regressione Ridge e Lasso per ottenere i vantaggi da entrambi i modelli.\n\nSi utilizzano quindi due parametri regolarizzatori, che corrispondono rispettivamente alle funzioni di regolarizzazione di Ridge e Lasso regression.\n\nParametri utilizzati:\n- <b>alpha<\/b>: costante che moltiplica il termine di penalit\u00e0.\n- <b>l1_ratio<\/b>: parametro di mix di elasticNet. Per l1_ratio=0 la penalit\u00e0 \u00e8 la norma l2. Per l1_ratio=1 la penalit\u00e0 \u00e8 la norma l1, altrimenti \u00e8 una combinazione di l1 e l2.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543418448\/eq12_vh6ilt.png)","3eab2b2c":"Dopo aver visionato l'accuratezza di ciascun classificatore, \u00e8 possibile affermare che il K-nearest Neighbor ha restituito la maggiore accuratezza rispetto a tutti i modelli di classificazione testati.\n\nPertanto, per esso \u00e8 definita la matrice di confusione con le relative metriche di valutazione del modello.","d9d1b38b":"Questo modello \u00e8 conosciuto anche come regressione della cresta o regolarizzazione di Tikhonov. Questo stimatore ha un supporto integrato per la regressione multi-variata, cio\u00e8 quando y \u00e8 un array multi-dimensionale.\n\nE' un ottima soluzione da adottare quando si \u00e8 in presenza multi-collinearit\u00e0 tra i vari esempi.\n\nDunque, Ridge aggiunge un fattore di penalizzazione (norma l2) alla cost function. Ci\u00f2 determina la perdita di importanza del valore di una feature, che a seconda della penalit\u00e0 pu\u00f2 essere pi\u00f9 o meno accentuata. La forza della penalit\u00e0 \u00e8 modificabile e controllata da un iperparametro.\n\nParametro utilizzato:\n- <b>alpha<\/b>: forza di regolarizzazione. Migliora il condizionamento del problema. Valori pi\u00f9 grandi specificano una regolarizzazione pi\u00f9 forte.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543418449\/eq7_ylxudw.png)","5b2ac3bf":"La regressione polinomiale utilizza lo stesso metodo della regressione lineare, ma assume che la funzione che meglio descrive l\u2019andamento dei dati non sia una retta (lineare), ma un polinomio (curva). Quindi \u00e8 adatta quando lo scatterplot di una relazione bivariata, ad esempio, mostra una forma diversa da quella della retta, ad esempio una curva.\n\nPer adempiere alla realizzazione del task di regressione polinomiale \u00e8 stato realizzato un apposito snippet:","7cfec8cb":"## Percettrone","3df434b1":"## PCA (Principal Component Analysis)","20640513":"## Linear Discriminant Analysis","b684e62a":"### Matrice di correlazione di Pearson","90615576":"## Descrizione dataset\n\nPer costruire i differenti modelli, verr\u00e0 utilizzato il dataset <a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/breast+cancer+wisconsin+(original)\">Breast Cancer Wisconsin<\/a> riguardante esempi di casi clinici circa il cancro.\n\nDi seguito sono riportate le features presenti:\n 1. Sample code number: id number\n 2. Clump Thickness: 1 - 10\n 3. Uniformity of Cell Size: 1 - 10\n 4. Uniformity of Cell Shape: 1 - 10\n 5. Marginal Adhesion: 1 - 10\n 6. Single Epithelial Cell Size: 1 - 10\n 7. Bare Nuclei: 1 - 10\n 8. Bland Chromatin: 1 - 10\n 9. Normal Nucleoli: 1 - 10\n 10. Mitoses: 1 - 10\n 11. Class: (2 for benign, 4 for malignant)\n   \nPer ciascun algoritmo di tipo Ensemble, si utilizzer\u00e0 al fine della valutazione sui dati non visti una 10-Fold Cross Validation.","bbb63762":"### Dichiarazione della feature target","9d936eaa":"### Pre-processing del dataset","ced87234":"Support Vector Machine (SVM), sono dei modelli di apprendimento supervisionato associati ad algoritmi di apprendimento per la regressione e la classificazione. Dato un insieme di esempi per l'addestramento, ognuno dei quali etichettato con la classe di appartenenza fra le due possibili classi, un algoritmo di addestramento per le SVM costruisce un modello che assegna i nuovi esempi a una delle due classi, ottenendo quindi un classificatore lineare binario non probabilistico. Un modello SVM \u00e8 una rappresentazione degli esempi come punti nello spazio, mappati in modo tale che gli esempi appartenenti alle due diverse categorie siano chiaramente separati da uno spazio il pi\u00f9 possibile ampio. I nuovi esempi sono quindi mappati nello stesso spazio e la predizione della categoria alla quale appartengono viene fatta sulla base del lato nel quale ricade.\n\nOltre alla classificazione lineare \u00e8 possibile fare uso delle SVM per svolgere efficacemente la classificazione non lineare utilizzando il metodo kernel, mappando implicitamente i loro ingressi in uno spazio delle features multi-dimensionale.\n\nQuando gli esempi non sono etichettati \u00e8 impossibile addestrare in modo supervisionato e si rende necessario l'apprendimento non supervisionato: questo approccio cerca d'identificare i naturali gruppi in cui si raggruppano i dati, mappando successivamente i nuovi dati nei gruppi ottenuti. L'algoritmo di raggruppamento a vettori di supporto, applica le statistiche dei vettori di supporto, sviluppate negli algoritmi delle SVM, per classificare dati non etichettati, ed \u00e8 uno degli algoritmi di raggruppamento maggiormente utilizzato nelle applicazioni industriali.\n\nAl fine di testare i differenti Kernel disponibili per SVC, \u00e8 stata implementata una GridSearch con l'ausilio di una 10-Fold Cross Validation.\n\nParametri testati:\n- <b>kernel<\/b>: specifica il tipo di kernel da utilizzare nell'algoritmo. I possibili valori sono:\n    - <b>linear<\/b>\n    - <b>poly<\/b>\n    - <b>rbg (default)<\/b>\n    - <b>sigmoid<\/b>\n    - <b>precoumputed<\/b>","f81f0544":"Per l'applicazione della PCA, si \u00e8 provveduto ad usare il rispettivo metodo _fit()_ che consente il fitting del modello mediante gli esempi contenuti nella matrice X di training, identificata con il nome di _x\\_train_.","e5f0fc05":"Con il seguente comando si effettua il caricamento di quanto contenuto nel dataset _'breast-cancer-wisconsin.data'_:","b6ec0bc0":"Una foresta casuale (in inglese: random forest) \u00e8 un classificatore d'insieme ottenuto dall'aggregazione tramite bagging di alberi di decisione.\nLe foreste casuali si pongono come soluzione che minimizza l'overfitting del training set rispetto agli alberi di decisione.\n\nParametri testati:\n- <b>n_estimators<\/b>: numero di alberi decisionali da utilizzare nella random forest\n- <b>criterion<\/b>: indica la funzione da utilizzare per la misurazione della qualit\u00e0 di uno split. I criteri supportati sono:\n    - <b>gini<\/b>: misura di impurit\u00e0\n    - <b>entropy<\/b>: information gain","141aacd9":"## Decision Tree","3371aa3f":"La regressione logistica \u00e8 uno degli algoritmi di apprendimento automatico pi\u00f9 semplici e comunemente utilizzati per la classificazione a due classi. \u00c8 facile da implementare e pu\u00f2 essere utilizzata come base per qualsiasi problema di classificazione binaria.\n\nLo scopo di questo algoritmo \u00e8 quello di descrivere e stimare la probabilit\u00e0 di una variabile dipendente categorica da una o pi\u00f9 variabili indipendenti. La variabile dipendente \u00e8 una variabile binaria che contiene dati codificati come 1 (s\u00ec, successo, ecc.) o 0 (no, errore, ecc.). In altre parole, il modello di regressione logistica prevede P (Y = 1) come funzione di X.\n\n\nNel caso rappresentato in seguito, si \u00e8 in presenza di Regressione Logistica Multinomiale, in quanto le variabili da predire non sono dicotomiche, ma possono assumere pi\u00f9 di due valori.\nPer testare i molteplici parametri del modello _logreg_ \u00e8 stata utilizzata la GridSearchCV() per ottenere la configurazione migliore del settaggio dei parametri da utilizzare.\n\nParametri testati:\n- <b>solvers<\/b>: indica l'algoritmo da utilizzare nel problema di ottimizzazione\n- <b>multi_class<\/b>:\n    - <b>ovr<\/b>: viene fittato un problema binario per ogni etichetta.\n    - <b>multinomial<\/b>: la loss minimizzata \u00e8 la loss multinomiale fittata su tutta la distribuzione di probabilit\u00f9, anche quando i dati sono binari.\n    - <b>auto (default)<\/b>: viene selezionato il miglior parametro automaticamente\n- <b>C<\/b>: Inverso della potenza di regolarizzazione, inteso come float positivo. Valori pi\u00f9 piccoli indicano una regolarizzazione pi\u00f9 forte.","06a4ee2f":"### Elastic Net","65c769c9":"Al fine di condurre una analisi completa, di seguito \u00e8 riportata la matrice di correlazione di Pearson che prende in considerazione le differenti features presenti all'interno del DataFrame.\n\nI valori presenti all'interno della matrice di correlazione saranno espressi mediante valore decimale nell'intervallo [-1,+1] che rispettivamente indicher\u00e0 la presenza di una correlazione inversa oppure una correlazione diretta.\n\nNel caso in cui il valore calcolato di correlazione sia vicino al valore 0, non \u00e8 possibile definire la presenza di correlazione tra le features considerate.","2918585a":"Onde evitare problemi circa la presenza di dati espressi su un diverso range numerico, come fase di pre-processing \u00e8 stata introdotta la standardizzazione, mediante apposito metodo _StandardScaler()_ applicata su X.","6e9358d2":"Il teorema di Bayes proposto da Thomas Bayes, \u00e8 utilizzato per calcolare la probabilit\u00e0 condizionata di un evento A, sapendo che si \u00e8 verificato un evento B, a partire dalla conoscenza delle probabilit\u00e0 a priori degli eventi A e B e della probabilit\u00e0 condizionata di B noto A.\nIl teorema di Bayes \u00e8 utilizzato in molti campi, come nella diagnosi medica per calcolare la probabilit\u00e0 che un individuo sia affetto da una malattia sapendo che presenti determinati sintomi.\n\nConsiderando un insieme di variabili aleatorie indipendenti $A_1, ..., A_n$ che partizionano l'insieme degli eventi $\\Omega$, la probabilit\u00e0 condizionata \u00e8 definita come:\n$$P(A_i|B) = \\frac{P(B|A_i)P(A_i)}{P(B)} = \\frac{P(B|A_i)P(A_i)}{\\sum_{j=1}^nP(B|A_j)P(A_j)}$$\n\nDove:\n\n- $P(A_i|B)$: probabilit\u00e0 condizionata di $A_i$ noto B. E' anche conosciuta come probabilit\u00e0 a posteriori visto che dipende dallo specifico valore di B.\n- $P(B|A_i)$: probabilit\u00e0 condizionata di B noto $A_i$\n- $P(A_i)$: probabilit\u00e0 a priori di $A_i$.\n- $P(B)$: probabilit\u00e0 a priori di B, e funge da costante di normalizzazione, che permette di ottenre $P(A_i|B) = 1$ al variare di $i$.\n\nL'espressione Naive Bayes (\"ingenuo\") indica il fatto che si fanno forti assunzioni di indipendenza nel modello. Si assume infatti che data una certa classe, ciascuna delle variabili aleatorie (features) siano indipendenti.\nFormalmente quindi dato un vettore $B=(B_1, ..., B_n)$ si assume che sia verificata:\n$$P(B_1, ..., B_n|A) = \\prod_{i=1}^{n}P(B_i|A)$$\nQuesta assunzione di indipendenza per\u00f2 non sempre \u00e8 verificata nella realt\u00e0, motivo per il quale \u00e8 detto \"Naive Bayes\". Nonostante ci\u00f2, i modelli che utilizzano Naive Bayes funzionano sorprendentemente bene.\n\nQuesta particolarit\u00e0 sta a significare che la probabilit\u00e0 a posteriori:\n$$P(A|B_1, ..., B_n) \\propto P(A)P(B_1|A)...P(B_n|A)$$\n\nIl teorema pu\u00f2 essere rivisto in diverse maniere a seconda della distribuzione di probabilit\u00e0 delle varie variabili aleatorie.\n\n\nNel caso esposto nello snippet seguente, si utilizza Gaussian Naive Bayes, perch\u00e8 si assume che le feature di cui si dispone seguano una distribuzione Gaussiana.\n\nPer questo motivo, la probabilit\u00e0 a posteriori pu\u00f2 essere calcolata di volta in volta mediante l'uso dell'equazione Gaussiana.","76253738":"## Plot di Accuracy","8f35a79a":"### Lasso Regression","cbf45613":"Per ottenere il grafico relativo alla varianza spiegata da ogni componente \u00e8 stato sviluppato il seguente codice che consente, a fronte del modello fittato, di ottenere in forma grafica la varianza per ciascuna componente (per identificare il \"gomito\") mentre, a destra, \u00e8 riportato il plot della varianza cumulativa spiegata.\n\nCome \u00e8 possibile notare dal primo grafico, seppure il \"gomito principale\" sia presente sulla terza componente, per le ulteriori vi \u00e8 la presenza di una varianza spiegata non indifferente. Pertanto, mediante prove analitiche svolte, si \u00e8 preferito prendere in considerazione le prime 3 componenti principali che, in ogni caso, vanno a ridurre la dimensionalit\u00e0 del dataset di partenza.<\/br>","e23e2132":"### Random Search\nLa Random Search \u00e8 sorprendentemente efficiente rispetto alla Grid Search. Sebbene la Grid Search alla fine trover\u00e0 il valore ottimale degli iperparametri (supponendo che siano nella griglia), la Random Search di solito trover\u00e0 un valore \"abbastanza vicino\" in molte meno iterazioni.\n\nLa ricerca Grid Search troppo tempo a valutare regioni poco promettenti dello spazio di ricerca dell'iperparametro perch\u00e9 deve valutare ogni singola combinazione nella griglia. La Random Search, al contrario, fa un lavoro migliore nell'esplorare lo spazio di ricerca e quindi di solito pu\u00f2 trovare una buona combinazione di iperparametri in molte meno iterazioni.\n\nLa Random Search dovrebbe probabilmente essere il primo metodo di ottimizzazione degli iperparametri provato per la sua efficacia. Anche se si tratta di un metodo che non si basa sui risultati delle valutazioni precedenti, la Random Search di solito pu\u00f2 ancora trovare valori migliori rispetto a quella predefinita ed \u00e8 semplice da eseguire.","b6307a4b":"## Indice contenuti\n- [Analisi esplorativa del dataset](#Analisi-esplorativa-del-dataset)\n    - [Descrizione del dataset](#Descrizione-del-dataset)\n    - [Caricamento in memoria del dataset](#Caricamento-in-memoria-del-dataset)\n    - [Preprocessing](#Preprocessing)\n    - [Matrice di correlazione di Pearson](#Matrice-di-correlazione-di-Pearson)\n    - [Distribuzione dei dati per feature \"brand\"](#Distribuzione-dei-dati-per-feature-\"brand\")\n- [Grafici](#Grafici)\n    - [Boxplot](#Boxplot)\n    - [Jointplot](#Jointplot)\n    - [Pairplot](#Pairplot)\n- [PCA (Principal Component Analysis)](#PCA-(Principal-Component-Analysis))\n     - [Dichiarazione della feature target](#Dichiarazione-della-feature-target)\n     - [Standardizzazione dei dati](#Standardizzazione-dei-dati)\n     - [Split del dataset](#Split-del-dataset)\n     - [Applicazione della PCA](#Applicazione-della-PCA)\n- [Percettrone](#Percettrone)\n- [Linear Discriminant Analysis](#Linear-Discriminant-Analysis)\n- [Regressione Lineare](#Regressione-Lineare)\n- [Regressione Polinomiale](#Regressione-Polinomiale)\n- [Regressione Logistica](#Regressione-Logistica)\n- [Regressione mediante Random Forest](#Regressione-mediante-Random-forest)\n- [Utilizzo di metodi regolarizzati per la regressione](#Utilizzo-di-metodi-regolarizzati-per-la-regressione)\n    - [Ridge Regression](#Ridge-Regression)\n    - [Lasso Regression](#Lasso-Regression)\n    - [Elastic Net](#Elastic-Net)\n- [Decision Tree](#Decision-Tree)\n- [Random Forest](#Random-Forest)\n- [K-nearest Neighbor](#K-nearest-Neighbor)\n- [Gaussian Naive Bayes](#Gaussian-Naive-Bayes)\n- [Support Vector Machine](#Support-Vector-Machine)\n- [Matrice di confusione per K-nn](#Matrice-di-confusione-per-K-nn)\n     \n<hr>","e9e9f533":"Il k-nearest neighbors (k-NN) \u00e8 un algoritmo che di solito viene utilizzato nel riconoscimento di pattern per la classificazione di oggetti basandosi sulle caratteristiche degli oggetti vicini a quello considerato.\n\nUn oggetto \u00e8 classificato in base alla maggioranza dei voti dei suoi k vicini. k \u00e8 un intero positivo tipicamente non molto grande. In un contesto binario in cui sono presenti esclusivamente due classi \u00e8 opportuno scegliere k dispari per evitare di ritrovarsi in situazioni di parit\u00e0.\nConsiderando solo i voti dei k oggetti vicini c'\u00e8 l'inconveniente dovuto alla predominanza delle classi con pi\u00f9 oggetti. In questo caso pu\u00f2 risultare utile pesare i contributi dei vicini in modo da dare, nel calcolo della media, maggior importanza in base alla distanza dall'oggetto considerato.\n\nLa scelta di k dipende dalle caratteristiche dei dati. Generalmente all'aumentare di k si riduce il rumore che compromette la classificazione, ma il criterio di scelta per la classe diventa pi\u00f9 labile. La scelta pu\u00f2 esser presa attraverso tecniche euristiche, come ad esempio la cross validation.\n\nParametri testati:\n- <b>algorithm<\/b>: algoritmo utilizzato per computare i migliori k vicini\n    - <b>auto<\/b>: seleziona il miglior algoritmo automaticamente\n    - <b>ball_tree<\/b>: utilizza un ballTree\n    - <b>kd_tree<\/b>: utilizza un albero kd\n    - <b>brute<\/b>: ricerca brute force\n- <b>weights<\/b>: funzione di peso da utilizzare per la predizione\n    - <b>uniform<\/b>: ciascun nodo ha lo stesso peso\n    - <b>distance<\/b>: ciascun nodo viene pesato in base all'inverso della distanza con i suoi vicini\n- <b>n_neighbors<\/b>: numero di vicini da tenere in considerazione","afe508c6":"Per valutare le prestazioni del modello sfruttando i dati a disposizione, il dataset di partenza viene suddiviso nel seguente modo:\n- 70%: dataset di training\n- 30%: dataset di test\n\nPoich\u00e8 \u00e8 stata gi\u00e0 definita la feature target (_y_), tale suddivisione avviene anche per tale vettore.\n\nPer evitare, inoltre, un possibile overfitting del modello basato sulla disposizione ingenua dei dati all'interno del dataset, viene effettuato uno shuffle randomico, impostando il parametro _shuffle_=True.","50f7139a":"### Grid Search","d2a19726":"## Regressione mediante Random forest\n\nUna Random Forest \u00e8 un meta stimatore che si adatta ad una serie di alberi decisionali su vari sottocampioni del dataset e utilizza la media per migliorare l'accuratezza predittiva e il controllo dell'overfitting.\n\nLa differenza tra RandomForestClassifier e RandomForestRegressor, sta nel fatto che nel primo caso, viene utilizzato un classificatore per predire un insieme specificato di labels.\n\nNel secondo caso invece, viene utilizzato un regressore per predire valori reali, che possono variare e non appartengono ad un insieme limitato di valori che si possono assumere.\n\nParametri testati:\n- <b>n_estimators<\/b>: numero di alberi nella foresta\n- <b>criterion<\/b>: funzione per misurare la qualit\u00e0 di una divisione. I criteri supportati sono \"mse\" (errore quadratico medio) e \"mae\" (errore medio assoluto)\n- <b>max_features<\/b>: numero di features da considerare quando si cerca la migliore suddivisione.","2cd9d4eb":"Una caratteristica fondamentale della Lasso regression riguarda la gestione delle feature di importanza minore.\nContrariamente alla Ridge regression, che minimizzando il peso di alcune feature ne riduce la contribuzione al modello, la lasso regression effettua una vera e propria feature selection (prendendo in considerazione solo le variabili indipendenti) portando le restanti a zero attraverso un opportuno valore di peso associato generando quindi un modello sparso.\n\nA differenza di Ridge, Lasso penalizza la somma dei valori assoluti, ovvero la L1.\n\nParametro utilizzato:\n- <b>alpha<\/b>: costante che moltiplica il termine l1.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543418448\/eq11_ij4mms.png)","18ccb9f4":"## K-nearest Neighbor","9b10fc20":"## Regressione Polinomiale"}}