{"cell_type":{"693774b2":"code","31cd991e":"code","16595d9e":"code","44b6c651":"code","42bb0fe2":"code","0476c9b0":"code","952b4e89":"code","cd1fe075":"code","667b5def":"code","20a82ef2":"code","c8d6d676":"code","6d1df78d":"code","6dbcc660":"code","776ef49d":"code","c8f77a74":"code","6cf3fc57":"code","4aa46e34":"code","0c4b3329":"code","1a8c9580":"code","29b56c03":"code","b799ad75":"code","161c53fb":"code","5f6433c7":"code","bd66d896":"code","197e9117":"code","b554be09":"code","1eaee652":"code","4b46b13a":"code","717eecbb":"code","c9a29f4d":"code","a7d013ae":"code","1ae611dd":"code","a091da32":"code","72edf812":"code","4e4f7637":"code","a803009c":"code","4edb25bf":"code","8f59cab5":"code","20b5c208":"code","b3f0f3a6":"code","45b543e2":"code","505443d8":"code","08a36ad8":"code","06ece0c9":"code","6397ca10":"code","1e71e3c0":"code","62261a0b":"code","d2a5246d":"code","139e714e":"code","19eb6f69":"code","9cd10cd3":"code","bb8ed75d":"code","d693ddf8":"code","deede04e":"code","701c7cc8":"markdown","51383490":"markdown","fa21a451":"markdown","bd29ff1f":"markdown","cf11a7d3":"markdown","39260d45":"markdown","71c929c1":"markdown","4aa0c1a4":"markdown","6b4216d0":"markdown","eb98e5eb":"markdown","55e3f54b":"markdown","4d92c3c1":"markdown","4a752c21":"markdown","bbe9c651":"markdown","19c1670b":"markdown","bf45a5e5":"markdown","f9b5ebc0":"markdown","839764db":"markdown","7e7d60fd":"markdown","b75b1e1d":"markdown","a26b48fa":"markdown","46611a29":"markdown","7a20fb21":"markdown","ead26b95":"markdown","605ba0b8":"markdown","8f767e50":"markdown","0b21811f":"markdown","1bdc3411":"markdown","f84a9b04":"markdown","03f1bcac":"markdown","c7b8f6bd":"markdown","6755c910":"markdown","3f7073d3":"markdown","ad819a2f":"markdown","17ab46d9":"markdown","3df56c16":"markdown","c4a54789":"markdown","a4ff80ac":"markdown","a414a0ff":"markdown","17e5ce03":"markdown","e03c81f8":"markdown","2b6f9483":"markdown","8c2640d4":"markdown","6acc4753":"markdown","53e7aa8a":"markdown","74c42941":"markdown","3fce884b":"markdown","249f78f3":"markdown","b5f149fa":"markdown","0d3842e3":"markdown","7ff50654":"markdown","8b9f51fa":"markdown"},"source":{"693774b2":"import pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom statsmodels.formula.api import logit\n\n#We are setting the seed to assure you get the same answers on quizzes as we set up\nrandom.seed(42)","31cd991e":"df = pd.read_csv('..\/input\/abtesting\/ab_data.csv')\ndf.head()","16595d9e":"# Know the number of rows in the dataset:\ndf.shape[0]","44b6c651":"# Number of unique users in the dataset using (nuinque):\ndf['user_id'].nunique()","42bb0fe2":"# the probability of the users convereted \nconverted_prop = (df['converted']).mean()\nconverted_prop","0476c9b0":"# the count of numbers that the treatment group doesn't match with the new page\nnot_match = df.query(\"group == 'treatment' and landing_page == 'old_page'\").shape[0]\nnot_match","952b4e89":"df.isnull().sum()","cd1fe075":"# Remove the inaccurate rows, and store the result in a new dataframe df2\n# filtering the dataset with the inaccurate rows that max up the treatment group with old page and the control group with new page\ninaccurate_rows = df.query(\"group == 'treatment' & landing_page !=  'new_page' | group == 'control' & landing_page != 'old_page'\")\ninaccurate_rows.count() ","667b5def":"# adding a new dataframe \ndf2 = df.drop(inaccurate_rows.index, axis = 0)","20a82ef2":"# Double Check all of the incorrect rows were removed from df2 - \n# Output of the statement below should be 0\ndf2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]","c8d6d676":"df2['user_id'].nunique()","6d1df78d":"# using duplicated function to know the repeated user_id\nuser_repeated = df2[df2.duplicated(subset= 'user_id')]\nuser_repeated","6dbcc660":"# more info on the repeated rows\ndf2[df2['user_id'] == 773192]","776ef49d":"# Remove one of the rows with a duplicate user_id..\n# Hint: The dataframe.drop_duplicates() may not work in this case because the rows with duplicate user_id are not entirely identical. \ndf2.drop(1899, inplace= True)","c8f77a74":"# Check again if the row with a duplicate user_id is deleted or not\ndf2[df2.duplicated(subset= 'user_id')]","6cf3fc57":"\np_population = (df2['converted'] == 1).mean()\np_population","4aa46e34":"converted_control = df2.query(\"group == 'control'\").converted.mean()\nconverted_control","0c4b3329":"converted_treatment = df2.query(\"group == 'treatment'\").converted.mean()\nconverted_treatment","1a8c9580":"# Calculate the actual difference (obs_diff) between the conversion rates for the two groups.\nobs_diff = converted_control - converted_treatment\nobs_diff","29b56c03":"# the probability of receiving the new page\n(df['landing_page'] == 'new_page').mean()","b799ad75":"# assuming that the new page equal to the converted under the null hypothesis the new page will be :\np_new = df2['converted'].mean()\np_new","161c53fb":"# assuming that the old page equal to the converted under the null hypothesis the old page will be :\np_old = p_new\np_old","5f6433c7":"# number of the individuals in the treatment group\nn_new = df2.query(\"group=='treatment'\").user_id.nunique()\nn_new","bd66d896":"n_old = df2.query(\"group=='control'\").user_id.nunique()\nn_old","197e9117":"# Simulate a Sample for the treatment Group\nnew_page_converted = np.random.choice([0,1], size=n_new, p=[(1-p_new) ,p_new])\nnew_page_converted.mean()","b554be09":"# Simulate a Sample for the control Group\nold_page_converted=np.random.choice([0,1], size=n_old, p=[(1-p_old), p_old])\nold_page_converted.mean()","1eaee652":"converted_diff =new_page_converted.mean() - old_page_converted.mean()\nconverted_diff","4b46b13a":"# Sampling distribution using numpy built-in fuctions:\n\np_diffss=[]\n\nnew_p_converted = np.random.binomial(n_new, p_new, 10000)\/n_new\n\nold_p_converted = np.random.binomial(n_old, p_old, 10000)\/n_old\n\np_diffss = new_p_converted - old_p_converted ","717eecbb":"# Calculating the confidence interval (95%):\n\nlower_bound = np.percentile(p_diffss, 2.5)\nupper_bound = np.percentile(p_diffss, 97.5)\n\nprint(lower_bound, upper_bound)","c9a29f4d":"old_prob = df2.query(\"group=='control'\")['converted'].mean()\nnew_prob=df2.query(\"group=='treatment'\")['converted'].mean()\nobs_diff=new_prob-old_prob\nobs_diff","a7d013ae":"plt.figure(figsize=(15,5))\n\nsns.histplot(p_diffss, bins= 30)\nplt.axvline(obs_diff, label='obs_diff', c='r', ls='--')\nplt.axvline(x=lower_bound, label='Lower limit', c='k', ls='--')\nplt.axvline(x=upper_bound, label='Upper limit', c='k', ls='--')\n\n\nplt.title('The p.difference between the old page and new page')\nplt.xlabel('p_diffs')\nplt.ylabel('Counts')\n\nplt.legend();","1ae611dd":"# the null distribution \nnull_distribution = np.random.normal(0, np.std(p_diffss), 10000) # Here are 10000 draws from the sampling distribution under the null","a091da32":"plt.figure(figsize=(15,5))\nsns.histplot(x = null_distribution , bins=30)\nplt.axvline(x= obs_diff, c='r', ls='--');","72edf812":"p_value = (null_distribution > obs_diff).mean()\np_value","4e4f7637":"import statsmodels.api as sm\n\n# number of conversions with the old_page\nconvert_old = df2.query(\"landing_page == 'old_page'\")['converted'].sum()\n\n# number of conversions with the new_page\nconvert_new = df2.query(\"landing_page == 'new_page'\")['converted'].sum()\n\n# number of individuals who were shown the old_page\nn_old = df2.query(\"landing_page == 'old_page'\")['user_id'].nunique()\n\n# number of individuals who received new_page\nn_new = df2.query(\"landing_page == 'new_page'\")['user_id'].nunique()\n\nprint('The converted from old page is :', convert_old)\nprint('The converted from new page is :', convert_new)\nprint('The number of individuals who saw the old page :', n_old)\nprint('The number of individuals who saw the new page :', n_new)","a803009c":"# use sm.stats.proportions_ztest() to compute the test statistic and p-value.\nz_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old],alternative='larger')\nprint(\"z_score = \",z_score)\nprint(\"p_value = \", p_value)","4edb25bf":"# making Contingency table to know the test of Independence:\n\nContingency_table = sm.stats.Table.from_data(df2[['landing_page', 'converted']])","8f59cab5":"# Testing for association:\n\nresult = Contingency_table.test_nominal_association()","20b5c208":"# Getting results:\n\nresult.statistic, result.pvalue","b3f0f3a6":"df2.head()","45b543e2":"# Adding an intercept column :\ndf2['intercept'] = 1\n\n# getting dummies :\ndf2['ab_page'] = pd.get_dummies(df2['group'])['treatment'] # to make the treatment 1\n\ndf2.head()","505443d8":"log_model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n\nresult1 = log_model.fit()","08a36ad8":"result1.summary()","06ece0c9":"# Read the countries.csv\ncountries = pd.read_csv('..\/input\/countries\/countries.csv')\ncountries.head()","6397ca10":"# Join with the df2 dataframe\ndf_joined = df2.merge(countries, on = 'user_id')\ndf_joined.head()","1e71e3c0":"df_joined.groupby('country')['user_id'].nunique()","62261a0b":"# Create the necessary dummy variables\ndf_joined[['CA','UK', 'US']] = pd.get_dummies(df_joined['country'])\ndf_joined.head()","d2a5246d":"# Initializing and fitting the model:\nlog_modell = sm.Logit(df_joined['converted'], df_joined[['intercept', 'UK','US']])\n\nresultt = log_modell.fit()","139e714e":"# Getting the model summary:\nresultt.summary()","19eb6f69":"# delete the CA column form the dataset \ndf_joined.drop('CA', inplace= True, axis=1)\n","9cd10cd3":"df_joined.head()","bb8ed75d":"# adding two columns to see the interaction between page and country and if there is a significant effects on conversion\ndf_joined['UK_ab'] = df_joined['ab_page'] * df_joined['UK']\ndf_joined['US_ab'] = df_joined['ab_page'] * df_joined['US']\ndf_joined.head()","d693ddf8":"# Fit your model\n\nlog_model2 = sm.Logit(df_joined['converted'] , df_joined[['intercept', 'ab_page', 'UK','US', 'UK_ab', 'US_ab']])\n\nresults2 = log_model2.fit()","deede04e":"# summarize the results\nresults2.summary()","701c7cc8":"**d.** Remove **one** of the rows with a duplicate **user_id**, from the **df2** dataframe.","51383490":"so here the difference between the old page and the new page with 95% confidence interval is between **-0.002 and 0.002**","fa21a451":"**c.** Display the rows for the duplicate **user_id**? ","bd29ff1f":"**n.** What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?<br><br>","cf11a7d3":"**j.** What proportion of the **p_diffs** are greater than the actual difference observed in the `df2` data?","39260d45":"**e. Simulate Sample for the `treatment` Group**<br> \n\n","71c929c1":"### The sampling distribution of the difference under the null hypothesis:","4aa0c1a4":"\n**a.** What is the probability of an individual converting regardless of the page they receive?<br><br>","6b4216d0":"- I fail to reject the null hypothesis here who indicates that the two variables are independent. \n- the p value = 0.189 is greater than the alpha. ","eb98e5eb":"# Analyze A\/B Test Results \n\nThis project will assure you have mastered the subjects covered in the statistics lessons. We have organized the current notebook into the following sections: \n\n- [Introduction](#intro)\n- [Part I - Probability](#probability)\n- [Part II - A\/B Test](#ab_test)\n- [Part III - Regression](#regression)\n- [Final Check](#finalcheck)\n- [Submission](#submission)\n\n\n<a id='intro'><\/a>\n## Introduction\n\nA\/B tests are very commonly performed by data analysts and data scientists. For this project, you will be working to understand the results of an A\/B test run by an e-commerce website.  Your goal is to work through this notebook to help the company understand if they should:\n\n<a id='probability'><\/a>\n## Part I - Probability\n\nTo get started, let's import our libraries.","55e3f54b":"**b.** The goal is to use **statsmodels** library to fit the regression model you specified in part **a.** above to see if there is a significant difference in conversion based on the page-type a customer receives. However, you first need to create the following two columns in the `df2` dataframe:\n","4d92c3c1":"**d.** What is $n_{old}$, the number of individuals in the control group?","4a752c21":"**There isn't any null values in the dataset**","bbe9c651":"**e.** Consider your results from parts (a) through (d) above, and explain below whether the new `treatment` group users lead to more conversions.","19c1670b":"**b.** Use the cell below to find the number of rows in the dataset.","bf45a5e5":"> **I can't reject the null hypothesis** for many reasons:\n>- The p value for the hypothesis testing is greater than the alpha, So the difference between the old page and the new page is not significant and occured by chance.\n>- In the regression model also the p value is greater than the alpha and the interaction between the countries and the pages are meaningless or don't have effect on the conversion rate.\n\n> It seems that the old page is pretty good and enough ,But I can't say If we should continue with the new page or cancel it\nbecause The dataset don't have more features.\n\n> **I suggest to run the experiment longer to make the decision of whether to implement the new page or not**.","f9b5ebc0":"> The p value of UK_ab and US_ab is greater than the alpha .05 which means that **the interaction of country and ab_page have no significant effect on the conversion rate**.\n\n> **I fail to reject the null**.","839764db":"\n>- The null hypothesis here that there isn't a correlation between the ab_page and the conversion.\n>- The null hypothesis here that there is a correlation between the two of them.\n\n$$H_{0}:P_{new} - P_{old} = 0$$\n$$H_{1}:P_{new} - P_{old} \\neq 0$$\n\n\n> It differs from part || because this is a two-sided t-test compared to a one-sided t-test in part II.\n\n> The p value associated with ab_page is **0.190** which is higher than the alpha .05 . The p value here means that the probability of the correlation between x (ab_page) and y (converted) occurred by chance is 0.190 , SO there isn't a significant difference in conversion based on the page-type a customer receives. **I fail to reject the null**. ","7e7d60fd":"**b.** Given that an individual was in the `control` group, what is the probability they converted?","b75b1e1d":"**e.** The number of times when the \"group\" is `treatment` but \"landing_page\" is not a `new_page`.","a26b48fa":"**h. Sampling distribution** <br>\n","46611a29":">Since I've just calculated that the convereted probability of the control is slightly higher than the converted   probability of the treatment , So the null hypothesis will be that the old page is better than the new page untill I prove the opposite , so the alternative hypothesis will be that the new page is better than the old page with type | error rate of 5%.\n>$$H_{0}:P_{old} \\leq P_{new}$$\n>$$H_{1}:P_{old} > P_{new}$$","7a20fb21":"**d.** Provide the summary of your model below, and use it as necessary to answer the following questions.","ead26b95":"- Adding more features into the model may show us some patterns that is hidden. \n- The disadvantage: if these feautures don't relate to the data.","605ba0b8":"### Plotting the sampling distribution of the difference under the null hypothesis:","8f767e50":">**Logistic Regression** : because the logistic regression model used to predict only two possible outcomes (conversion or no conversion).","0b21811f":"**f.** Now, you are considering other things that might influence whether or not an individual converts.  Discuss why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?","1bdc3411":"<a id='regression'><\/a>\n### Part III - A regression approach\n\n\n**a.** Since each row in the `df2` data is either a conversion or no conversion, what type of regression should you be performing in this case?","f84a9b04":"### Plotting the sampling distribution for p.diffs with 95% confidence:","03f1bcac":"**d.** What is the probability that an individual received the new page?","c7b8f6bd":"**c.** What is $n_{new}$, the number of individuals in the treatment group? <br><br>\n","6755c910":"### using Z-testing for hypothesis testing ","3f7073d3":"**b.** There is one **user_id** repeated in **df2**.  What is it?","ad819a2f":"> The p value for UK and US is greater than the alpha so again **I fail to reject the null**.","17ab46d9":"**g. Adding countries**<br> \n","3df56c16":"**a.** What is the **conversion rate** for $p_{new}$ under the null hypothesis? ","c4a54789":"## Conclusions","a4ff80ac":"> The p value is .9 is greater than the alpha .05 so **I can't reject the null hypothesis**.","a414a0ff":"To utilize two countries form the three, I will drop the least important (CA)","17e5ce03":"**d.** The proportion of users converted.","e03c81f8":"<a id='ab_test'><\/a>\n## Part II - A\/B Test","2b6f9483":"**a.** How many unique **user_id**s are in **df2**?","8c2640d4":"\n>- The conversion probability of the control group is .12 and the treatment group is .11 and the observed differece between them is **.001**\n>- the probability that the individual received the new page is 50% same as the old page.\n\n> **So I don't think there is sufficient evidence to say that the new treatment page leads to more conversions or not.**\n","6acc4753":"**c.** Use **statsmodels** to instantiate your regression model on the two columns you created in part (b). above, then fit the model to predict whether or not an individual converts. \n","53e7aa8a":"\n<center>\n\n|Data columns|Purpose|Valid values|\n| ------------- |:-------------| -----:|\n|user_id|Unique ID|Int64 values|\n|timestamp|Time stamp when the user visited the webpage|-|\n|group|In the current A\/B experiment, the users are categorized into two broad groups. <br>The `control` group users are expected to be served with `old_page`; and `treatment` group users are matched with the `new_page`. <br>However, **some inaccurate rows** are present in the initial data, such as a `control` group user is matched with a `new_page`. |`['control', 'treatment']`|\n|landing_page|It denotes whether the user visited the old or new webpage.|`['old_page', 'new_page']`|\n|converted|It denotes whether the user decided to pay for the company's product. Here, `1` means yes, the user bought the product.|`[0, 1]`|\n<\/center>\nUse your dataframe to answer the questions in Quiz 1 of the classroom.\n\n\n\n**a.** Read in the dataset from the `ab_data.csv` file and take a look at the top few rows here:","74c42941":"**g.** Find the difference in the \"converted\" probability $(p{'}_{new}$ - $p{'}_{old})$ for your simulated samples from the parts (e) and (f) above. ","3fce884b":"**c.** Given that an individual was in the `treatment` group, what is the probability they converted?","249f78f3":"**b.** What is the **conversion rate** for $p_{old}$ under the null hypothesis? ","b5f149fa":"**f. Simulate Sample for the `control` Group** <br>\n","0d3842e3":"**f.** Do any of the rows have missing values?","7ff50654":"**c.** The number of unique users in the dataset.","8b9f51fa":"- The z-score measures the number of standard errors.\n- The findings from the traditional statistical methods agree with the findings before both have p value of = 0.9.\n"}}