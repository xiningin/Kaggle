{"cell_type":{"0132cda5":"code","1c17046b":"code","d94d467b":"code","7aaa25bd":"code","e0dcc12f":"code","09d5e5c9":"code","17a045da":"code","b57c2a8f":"code","13443252":"code","43ad8824":"code","0bfb37e9":"code","7b329bc5":"code","cf39814f":"code","ffcd60c7":"code","091620af":"code","87bfc48a":"code","4f61ec9b":"code","31f479f7":"code","ee45104b":"code","9c33d624":"code","f51fe4f6":"code","9dd394ca":"code","09d6e787":"code","53d8eb82":"markdown","4dc6a2d5":"markdown","54c9a20a":"markdown","5bab3695":"markdown","4a039b5b":"markdown","f3a25490":"markdown","c7f8323b":"markdown","16e0bf06":"markdown","6c1bbfd1":"markdown","2370060b":"markdown","96b7fc1c":"markdown","0c72360d":"markdown","5a7653c7":"markdown","0e322f13":"markdown","8f65783a":"markdown","417d3254":"markdown","d9647214":"markdown","2d1172ca":"markdown","538d1811":"markdown","2e36c916":"markdown","00645aa0":"markdown","de08a1d5":"markdown","c3099920":"markdown","28836a09":"markdown","6f37a1a7":"markdown"},"source":{"0132cda5":"import pandas as pd\n# load the data\nfoldername = '..\/input\/petfinder-pawpularity-score\/'\n# in pandas, \"train\" is called a dataframe (e.g., excel table)\ntrain = pd.read_csv(foldername + 'train.csv')\n\n# print out the data\nprint(train)","1c17046b":"### TODO ###\nprint('#training samples', ???)\nprint('#attributes (excluding target score)', ???)","d94d467b":"### TODO ###\nprint(\"Average score of attributes\", ???)","7aaa25bd":"### TODO\n???","e0dcc12f":"### TODO\nbaseline_pred = ???\n\n# compute the root mean square error (used in the leaderboard)\n# hint: the baseline result is not bad ... the dataset is not balanced!\nnp.sqrt(((train.Pawpularity-baseline_pred)**2).mean())","09d5e5c9":"!pip install ipyplot -qq\nimport ipyplot\nfrom PIL import Image\n\n# popular: Pawpularity > 90\n# okay-ish: 60>Pawpularity > 50\n# not popular: Pawpularity < 10\n\npopular = train.Id[train.Pawpularity > 90].values.tolist()\nnot_popular = train.Id[train.Pawpularity < 10].values.tolist()\n#### TODO\n# Hint: similar to numpy array indexing\n# output: list of image ids\nokayish = ???\n\n\nnum_img = 9\npopular_img = [Image.open(foldername + 'train\/' + x +'.jpg') for x in popular[:num_img]]\nokayish_img = [Image.open(foldername + 'train\/' + x +'.jpg') for x in okayish[:num_img]]\n#### TODO\n# Hint: learn from the example above\nnot_popular_img = ???","17a045da":"print('Popular images')\nipyplot.plot_images(popular_img, max_images=num_img, img_width=120)","b57c2a8f":"print('Okayish images')\nipyplot.plot_images(okayish_img, max_images=num_img, img_width=120)","13443252":"print('Not Popular images')\nipyplot.plot_images(not_popular_img, max_images=num_img, img_width=120)","43ad8824":"metadata_cols = train.columns[1:-1]\n\nimage_paths = []\nlabels = []\ncustom_texts = []\n\nnum_img = 4\n\nfor col in metadata_cols:\n    ### TODO: select the rows with this col value equal to 1\n    tmp_df = ???\n    for i in range(num_img):\n        image_paths.append(foldername + 'train\/'+ tmp_df.iloc[i, 0] + '.jpg')\n        labels.append(col)\n        score = str(tmp_df.iloc[i, -1])\n        meta = tmp_df.iloc[i, :][metadata_cols].values\n        meta = ''.join([f'{col}:{m}, ' for m, col in zip(meta, metadata_cols)])\n        custom_texts.append(f'Pawpularity score: {score} \\n{meta}')","0bfb37e9":"ipyplot.plot_class_tabs(image_paths, labels, custom_texts=custom_texts, force_b64=True, img_width=120)","7b329bc5":"import torch\n#### TODO\n# Hint: reuse the code from pset4\nmodel = ???\nprint(model)","cf39814f":"import torch.nn as nn\n#### TODO\n# Hint: what's the input and output size of the last linear layer\nmodel.fc = ???","ffcd60c7":"import torch.nn.functional as F\ndef criterion(y_gt, y_pred):\n    #### TODO\n    # y_gt is between 0-100 -> scale to 0-1\n    # y_pred is any real number -> add a sigmoid to squash it to 0-1\n    return F.mse_loss(???, ???)","091620af":"import torch.optim as optim\n\n# freeze the weight for all conv layers\n# only learn the last linear layer\nfor name,param in model.named_parameters():\n    if 'fc' in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\n#### TODO\n# Hint: copy it from pset 4\noptimizer = ???","87bfc48a":"# from lab3\nimport numpy as np\nnp.random.seed(123)\n\ndef data_split(N, ratio=[8,2]):\n  # generate a shuffle array\n  shuffle_idx = np.arange(N)\n  np.random.shuffle(shuffle_idx)\n  # divide into train-val-test by the ratio\n  data_split = (np.cumsum(ratio)\/float(sum(ratio))*N).astype(int)\n  out_idx = [None] * len(ratio)\n  out_idx[0] = shuffle_idx[:data_split[0]]\n  for i in range(1,len(ratio)):\n    out_idx[i] = shuffle_idx[data_split[i-1] : data_split[i]]\n  return out_idx  \n\n# split the dataset into train-val split (8:2 ratio)\nsplit_idx = data_split(len(train))\ndf_train = train.loc[split_idx[0]]\n\n#### TODO\n# Hint: understand what is in split_idx\ndf_valid = ???","4f61ec9b":"from torch.utils.data import Dataset\nfrom PIL import Image\n\nmetadata_cols = train.columns[1:-1]\n# make a child class of PyTorch's dataset class\nclass PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        # initialization: called only once during creation\n        self.root_dir = root_dir\n        self.df = df\n        column_names = df.columns\n        self.file_names = df['Id'].values\n        self.meta = df[metadata_cols].values\n        if 'Pawpularity' in df.columns:\n            self.targets = df['Pawpularity'].values\n        else:\n            self.targets = None\n        self.transforms = transforms\n        \n    def __len__(self):\n        # determine how many iterations in one epoch\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        # called every time when the dataloader wants a sample\n        # the dataset has a list of image file names\n        # Input: dataloader provides a random index of the list\n        # Output: corresponding image and meta data\n\n        #### TODO\n        img_path = ???\n        img = Image.open(img_path)\n        if self.transforms:\n            img = self.transforms(img)\n        \n        #### TODO\n        meta = ???\n        \n        if self.targets is None:\n            # during deployment, df doesn't have the target value\n            target = 0            \n        else: \n            # otherwise, return the corresponding target value\n            #### TODO\n            target = self.targets[index]\n\n        return img, meta, target","31f479f7":"from torchvision import transforms\n\nRGB_MEAN = (0.4914, 0.4822, 0.4465)\nRGB_STD = (0.2023, 0.1994, 0.2010)\n\n# unlike pset4 working on the 32x32 images from CIFAR10\n# we here use the transforms for ImageNet challenge\ntransform_train = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(RGB_MEAN, RGB_STD),\n])\n\ntransform_test = transforms.Compose([\n    #### TODO\n    # hint: there are many \"right\" ways to do it\n    # one idea is to take the center crop without randomflip, compared to transform_train\n    ???\n])","ee45104b":"from torch.utils.data import DataLoader\n\ntrain_dataset = PawpularityDataset(foldername + 'train\/', df_train, transforms=transform_train)\n\n#### TODO\nvalid_dataset = ???","9c33d624":"#### nothing to change in this code block ####\n\nclass Config:  \n  def __init__(self, **kwargs):\n    # util\n    self.batch_size = 16\n    self.epochs = 0\n    self.save_model_path = '' # use your google drive path to save the model\n    self.log_interval = 100 # display after number of batches\n    self.criterion = F.cross_entropy # loss for classification\n    self.mode = 'train'\n    for key, value in kwargs.items():\n      setattr(self, key, value)\n   \nclass Trainer:  \n  def __init__(self, model, config, train_data = None, test_data = None):    \n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.epochs = config.epochs\n    self.save_model_path = config.save_model_path\n    self.log_interval = config.log_interval\n    self.mode = config.mode\n\n    self.globaliter = 0\n    self.train_loader = None\n    self.test_loader = None\n    batch_size = config.batch_size\n    if self.mode == 'train': # training mode\n      self.train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n                                          shuffle=True, num_workers=1)      \n      #self.tb = TensorBoardColab()\n      self.optimizer = config.optimizer\n    \n    if test_data is not None: # need evaluation\n      self.test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n                                         shuffle=False, num_workers=1)\n    \n    self.model = model.to(self.device)\n    self.criterion = config.criterion # loss function\n    \n                \n  def train(self, epoch):  \n    self.model.train()\n    for batch_idx, (data, meta, target) in enumerate(self.train_loader):      \n      self.globaliter += 1\n      data, target = data.to(self.device), target.to(self.device)\n\n      self.optimizer.zero_grad()\n      predictions = self.model(data)\n\n      loss = self.criterion(predictions, target)\n      loss.backward()\n      self.optimizer.step()\n\n      if batch_idx % self.log_interval == 0:\n        print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                  epoch, batch_idx * len(data), len(self.train_loader.dataset),\n                  100. * batch_idx \/ len(self.train_loader), loss.item()))\n        #self.tb.save_value('Train Loss', 'train_loss', self.globaliter, loss.item())\n        #self.tb.flush_line('train_loss')\n        \n        \n  def test(self, epoch, do_loss = True, return_pred = False):\n    self.model.eval()\n    test_loss = 0\n    correct = 0\n    pred = []\n    with torch.no_grad():\n      print('Start testing...')\n      for data, meta, target in self.test_loader:\n        data = data.to(self.device)\n        predictions = self.model(data)\n        if return_pred:\n          pred.append(predictions.detach().cpu().numpy())\n        if do_loss:\n            target = target.to(self.device)        \n            test_loss += self.criterion(predictions, target).item()*len(target)\n            prediction = predictions.argmax(dim=1, keepdim=True)\n            correct += prediction.eq(target.view_as(prediction)).sum().item()\n      if do_loss:\n          test_loss \/= len(self.test_loader.dataset)\n          accuracy = 100. * correct \/ len(self.test_loader.dataset)\n          print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n              test_loss, correct, len(self.test_loader.dataset), accuracy))\n      \"\"\"\n      if self.mode == 'train': # add validation data to tensorboard\n        self.tb.save_value('Validation Loss', 'val_loss', self.globaliter, test_loss)\n        self.tb.flush_line('val_loss')\n      \"\"\"\n      if return_pred:\n        return np.hstack(pred)\n  def main(self):\n    pred = []\n    if self.mode == 'train':\n      for epoch in range(1, self.epochs + 1):          \n          self.train(epoch)\n          if self.test_loader is not None:\n            # exist validation data\n            self.test(epoch)\n    if (self.save_model_path != ''):\n        torch.save(self.model.state_dict(), self.save_model_path)\n    elif self.mode == 'test':\n      self.test(0)\n    elif self.mode == 'deploy':          \n      pred = self.test(0, False, True)\n      return pred\n","f51fe4f6":"# set of hyperparameters\ntrain_config = Config(    \n    criterion = criterion,\n    save_model_path = '', # if you like, use your google drive path to save the model (mount google drive first)\n    log_interval = 100, # display after number of batches\n    batch_size = 16,\n    optimizer = optimizer,\n    epochs = 10,\n)\nTrainer(model, train_config, train_dataset, valid_dataset).main()","9dd394ca":"df_test = pd.read_csv(foldername + 'test.csv')\ntest_dataset = PawpularityDataset(foldername + 'test\/', df_test, transforms=transform_test)\n\ntest_config = Config(mode='deploy', batch_size=8)\ntest_pred = Trainer(model, test_config, None, test_dataset).main()","09d6e787":"submission_df = pd.read_csv(foldername + 'sample_submission.csv')\nsubmission_df['Pawpularity'] = test_pred.ravel()\nsubmission_df.to_csv('submission.csv', index = False)\n\n# Summary\nsubmission_df.head(10)","53d8eb82":"## Problem 2.1 Minimum Viable Product (MVP) (14 pts)","4dc6a2d5":"### (b) [1 pt] Visualize images with each of the attribute\nAs expected, the popularity score can be subjective and noisy...","54c9a20a":"### (d4) [1 pt] Build Dataset","5bab3695":"### (f) [1 pt] Create a submission\nYou'll get the point if the code blocks below run through correctly.","4a039b5b":"## Problem 1.2 Look into Images: Visualization with ipyplot (3 pts)\nLet's train our own brain to get some ideas about the task (e.g., pawpularity score).","f3a25490":"### (d) [1 pt] Baseline Result: predit the mean Pawpularity score","c7f8323b":"### (e) [3 pts] Train it!\n\nTo get the point, you need to show that the loss is decreasing after a few epoches. As you experienced in Pset4, here is where you will find out potential bugs in your anwsers to previous questions.","16e0bf06":"### (d3) [1 pt] Build data transform","6c1bbfd1":"# CSCI 3343 Pset 5: Transfer Learning\n\n**Posted:** Wednesday, October 20, 2021\n\n**Due:** Friday, October 29, 2021 (11:59 pm)\n\n__Total Points__: 21\n\n__Name__:\n[Your first name] [Your last name], [Your BC username]\n\n(e.g. Donglai Wei, weidf)\n\n__Submission__: please rename the .ipynb file as __\\<your_username\\>_pset5.ipynb__ before you submit it to canvas. Example: weidf_pset5.ipynb.","2370060b":"# Problem 2. ModelOps (14 pts)","96b7fc1c":"# CONGRATULATIONS!!! You completed the MVP for this Kaggle challenge!!!\n\nThis is a template on how to get started for any CV\/ML project.","0c72360d":"### (a) [2 pts] Visualize \"popular\", \"okay-ish\", and \"not popular\" images\nAs expected, the popularity score can be subjective and noisy...","5a7653c7":"## Problem 1.1 Overview: Dataset Statistics with Pandas (4 pts)\n\nOften the metadata is saved as tables and let's load them with the Pandas library ([Tutorial](https:\/\/www.w3schools.com\/python\/pandas\/pandas_getting_started.asp)).","0e322f13":"### (b) [1 pt] Define loss: Mean-squared error (MSE\/L2 regression)","8f65783a":"### (a1) [1 pt] Download Model: ResNet18","417d3254":"### (d1) [1 pt] Divide the images into train and validation","d9647214":"# Problem 1. DataOps (7 pts)","2d1172ca":"### (c) [1 pt] Plot the histogram of the Target (Pawpularity score)","538d1811":"### (a) [1 pt] Get the number of training images and number of attributes","2e36c916":"Let's kick off the training and hope it works!","00645aa0":"# Introduction\n\nHow to get started for an image understanding task?\nIn the [pset5 writeup](https:\/\/www.dropbox.com\/s\/46gjeg2hqi65u9z\/pset5.pdf?dl=0), we list out some good practices.\n\nHere, we will use this pawpularity challenge as a case study to walk you through the [DataOps](https:\/\/en.wikipedia.org\/wiki\/DataOps) and [ModelOps](https:\/\/en.wikipedia.org\/wiki\/ModelOps).\n\n## WARNING!!!\n\n- You only have 41 hours\/week of GPU usage on Kaggle.\n- Debug your code in CPU and make sure it works before you turn on the GPU mode.\n- Turn off the GPU mode after you are done","de08a1d5":"### (a2) [1 pt] Model surgery: change the last linear layer to predict one number instead","c3099920":"### (c) [1 pt] Define the optimizer","28836a09":"### (d2) [3 pts] Build dataset class","6f37a1a7":"### (b) [1 pt] Get the average score of all attributes (except the id)"}}