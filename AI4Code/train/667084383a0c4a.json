{"cell_type":{"720b8dbc":"code","dd7ad8cf":"code","6fe7e405":"code","9350b9e4":"code","0895d173":"code","662bff7f":"code","35a6b929":"code","42681083":"code","b2d197c0":"code","fa2631a6":"code","e9646543":"code","14dcbdd9":"code","2da5c2cb":"code","9497af88":"code","f2b95938":"code","7142fcbb":"code","740af89f":"code","a798773c":"code","ff799c59":"code","a92df295":"code","b26b4a6e":"markdown","0f2add32":"markdown","75a6c10c":"markdown","77c7b04d":"markdown","9cfecee9":"markdown","0ece3c13":"markdown","dda4e192":"markdown","5f345266":"markdown","29c61063":"markdown","cbd91fea":"markdown","43fb69c5":"markdown","3b9df105":"markdown","2a3554c5":"markdown","24a68493":"markdown","d94ec877":"markdown"},"source":{"720b8dbc":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport seaborn as sns\nimport string\nimport html\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom datetime import datetime\nfrom sklearn.preprocessing import normalize, MinMaxScaler\nfrom scipy.stats import spearmanr, pearsonr\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\" # To see all the outputs in the notebook, not only the last one\npd.set_option('display.max_colwidth', -1) # To see all text in reviews\n\nplt.style.use('ggplot')","dd7ad8cf":"# Reading data \ndata = [\n    pd.read_csv(\"..\/input\/drugsComTest_raw.csv\"),\n    pd.read_csv(\"..\/input\/drugsComTrain_raw.csv\")\n]\n\ndrugs = pd.concat(data, ignore_index=True) # Joining both train and test data","6fe7e405":"drugs.head(2) # Lets see what we have to work with...","9350b9e4":"# Let's delete all of these corrupted rows\nprint(\"Number of Corrupted Reviews: \", len(drugs[drugs.condition.str.contains(\" users found this comment helpful.\",na=False)]))\ndrugs = drugs[~drugs.condition.str.contains(\" users found this comment helpful.\",na=False)]","0895d173":"drugs.describe() # A basic descriptive analysis of the data ","662bff7f":"# We will use this groupings later on...\n\nprint(\"Number of Reviews per Drug\")\n# Number of reviews per drug\nreviews_per_drug = drugs.groupby([\"drugName\"]).agg({\n    \"uniqueID\": pd.Series.nunique\n})\nreviews_per_drug.describe()\n\n\nprint(\"Number of Reviews per Condition\")\n# Number of reviews per condition\nreviews_per_condition = drugs.groupby([\"condition\"]).agg({\n    \"uniqueID\": pd.Series.nunique\n})\n\nreviews_per_condition.describe()","35a6b929":"# Top 10 most reviewed drug names\nplot = drugs.drugName.value_counts().nlargest(10).plot(kind='bar', title=\"Top 10 reviewed drugs\", figsize=(12,6))","42681083":"# Top 10 most suffered condition by reviewers\nplot = drugs.condition.value_counts().nlargest(10).plot(kind='bar', title=\"Top 10 conditions in reviews\", figsize=(12,6))","b2d197c0":"drugs_rating = drugs.groupby('drugName').agg({\n    'rating': np.mean,\n    'uniqueID': pd.Series.nunique\n})\n\nprint(\"Significant number of reviews: More than\", reviews_per_drug.quantile(q=0.75).values[0], \"reviews\")\n\n# We only use the drugs which number of reviews is higher than a threshold\ndrugs_rating = drugs_rating[drugs_rating['uniqueID'] > int(reviews_per_drug.quantile(q=0.75))]\n\n# Top 10\ntop_drugs_rating = drugs_rating.nlargest(20, 'rating')\nplot = top_drugs_rating.plot(y='rating', kind='bar', figsize = (16, 3))\ndummy = plt.title(\"Top 10 'significant' drugs with best rating\") # Assigned to variable to prevent output\ndummy = plt.ylim(9, 10) # Assigned to variable to prevent output\n\n# Bottom 10\nbottom_drugs_rating = drugs_rating.nsmallest(20, 'rating')\nplot = bottom_drugs_rating.plot(y='rating', kind='bar', figsize = (16, 3))\ndummy = plt.title(\"Top 10 'significant' drugs with worst rating\") # Assigned to variable to prevent output\ndummy = plt.ylim(1, 5) # Assigned to variable to prevent output","fa2631a6":"drugs_condition_rating = drugs.groupby(['drugName', 'condition']).agg({\n    'rating': np.mean,\n    'uniqueID': pd.Series.nunique\n})\n\nprint(\"Number of pairs (Drug, Condition):\", len(drugs_condition_rating))\n\nprint(\"Significant number of reviews: More than\", drugs_condition_rating['uniqueID'].quantile(q=0.75), \"reviews\")\n\ndrugs_condition_rating = drugs_condition_rating[drugs_condition_rating['uniqueID'] > int(drugs_condition_rating['uniqueID'].quantile(q=0.75))]\n# drugs_condition_rating.sort_values('rating', ascending=False)\ntop_drugs_condition_rating = drugs_condition_rating.nlargest(20, 'rating')\nplot = top_drugs_condition_rating.plot(y='rating', kind='bar', figsize = (16, 3))\ndummy = plt.title(\"Top 10 (Drug - Condition) with best rating\") # Assigned to variable to prevent output\ndummy = plt.ylim(9.5, 10) # Assigned to variable to prevent output\n\nbottom_drugs_condition_rating = drugs_condition_rating.nsmallest(20, 'rating')\nplot = bottom_drugs_condition_rating.plot(y='rating', kind='bar', figsize = (16, 3))\ndummy = plt.title(\"Top 10 (Drug - Condition) with worst rating\") # Assigned to variable to prevent output\ndummy = plt.ylim(1, 4) # Assigned to variable to prevent output","e9646543":"drugs[\"date_format\"] = drugs[\"date\"].apply( lambda x: datetime.strptime(x, '%d-%b-%y')) # Get date as a date object\ndrugs[\"month\"] = drugs[\"date_format\"].apply(lambda x: x.strftime('%m')) # Extract date month\ndrugs[\"year\"] = drugs[\"date_format\"].apply(lambda x: x.strftime('%Y')) # Extract date year\ndrugs[\"weekday\"] = drugs[\"date_format\"].apply(lambda x: x.strftime('%w')) # Extract date weekday","14dcbdd9":"start_date = drugs[\"date_format\"].min()\nend_date = drugs[\"date_format\"].max()\n\nprint(\"First review date: \", start_date)\nprint(\"Last review date: \", end_date)","2da5c2cb":"days_grouped = drugs.groupby([\"year\", \"month\"]) \ndays_grouped = days_grouped.agg({\n    'rating': np.mean,\n    'usefulCount': np.sum,\n    'uniqueID': pd.Series.nunique\n})\n\ndifferent_months = len(days_grouped)\n\nprint(\"Months on dataset: \", different_months)","9497af88":"MME = MinMaxScaler() # Min-max normalization (0-1) for better visualization\n\ngrouped = days_grouped.reset_index(level=1)\nindex_values = np.unique(grouped.index.values)[1:] # First year (2008) month of January is missing\n\nmonths = pd.DataFrame()\n\nfor year in index_values:\n    months[year] = grouped.loc[year,:][\"rating\"].values # Every column is a year\nmonths_labels = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dic\"]\n\nmonths.iloc[:,:] = MME.fit_transform(months) # Min Max Normalization by columns (year)\n\nplots = months.plot(subplots=True, legend=True, figsize=(6,10), lw=2, title=\"Normalized (min-max) ratings average given in reviews for every month in every year\")\nfor plot in plots:\n    x = plot.set_ylim([-0.05, 1.05]) # Just assigning to variable so there is no output on notebook\nx = plt.xticks(range(0, len(months_labels)), months_labels)","f2b95938":"def encode_reviews(review):\n    return html.unescape(review) # Decode in utf-8 and convert HTML characters\n\nprint(\"Review example: \", drugs.loc[0, 'review']) # Example of review text\n\ndrugs['review'] = drugs['review'].apply(encode_reviews) # Let's clean the text...\n\ndrugs.head(2) # We are ready to go","7142fcbb":"# This might take some time... (3-5 minutes)\nsid = SentimentIntensityAnalyzer() \ndrugs[\"sentiment\"] = drugs[\"review\"].apply(lambda x: sid.polarity_scores(x)[\"compound\"]) # Compound is the overall sentiment score","740af89f":"# Let's see how Vader behaves with some reviews... (check text above)\ndrugs.loc[[215041,215046, 215050, 206473, 215035, 2, 23], [\"review\", \"rating\", \"sentiment\"]]","a798773c":"# Spearman correlation between computed sentiment and given rating\nspearmanr(drugs['sentiment'], drugs['rating']) # Low-moderate correlation can be seen","ff799c59":"# Let's find how Vader performs for the average rating in the most reviewed drugs... \n\ndrugs_sentiment = drugs.groupby([\"drugName\"])\ndrugs_sentiment = drugs_sentiment.agg({\n    'sentiment': np.mean, # drug sentiment average \n    'rating': np.mean,  # drug rating average\n    'uniqueID': pd.Series.nunique\n})\ndrugs_sentiment = drugs_sentiment[drugs_sentiment[\"uniqueID\"] > reviews_per_drug.quantile(q=0.75)[0]]\n\nplot = sns.jointplot(x=\"sentiment\", y=\"rating\", data=drugs_sentiment, kind=\"reg\", height=8, scatter_kws={\"s\": 20})\n\nprint(\"Pearson correlation coefficient\", pearsonr(drugs_sentiment['sentiment'], drugs_sentiment['rating']))","a92df295":"def generate_wordcloud(df, plot_title):\n    \n    stopwords_list = stopwords.words('english') + list(STOPWORDS) # We use both wordcloud and NLTK stopwords list\n    \n    raw_text = \" \".join(df['review'].values)\n    \n    wc = WordCloud(stopwords=stopwords_list, background_color=\"white\", width=1600, height=400).generate(raw_text)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    fig = plt.gcf()\n    fig.set_size_inches(16, 4)\n    plt.title(plot_title)\n    plt.show()\n\nreviews_by_comments = drugs.sort_values(by=\"usefulCount\")\n\ntop_1000_useful_comments = reviews_by_comments.tail(1000)\ntop_100_useful_comments = reviews_by_comments.tail(100)\ntop_1000_unuseful_comments =reviews_by_comments.head(1000)\ntop_100_unuseful_comments =reviews_by_comments.head(100)\n\n\ngenerate_wordcloud(drugs, \"Wordcloud of all reviews\")\ngenerate_wordcloud(top_1000_useful_comments, \"Wordcloud of Top 1000 most useful reviews\")\ngenerate_wordcloud(top_100_useful_comments, \"Wordcloud of Top 100 most useful reviews\")\ngenerate_wordcloud(top_1000_unuseful_comments, \"Wordcloud of Top 1000 most unuseful reviews\")\ngenerate_wordcloud(top_100_unuseful_comments, \"Wordcloud of Top 100 most unuseful reviews\")\n\n\n\n","b26b4a6e":"## Temporal Analysis\n\nBuilding a good prediction model is **no** trivial task. Temporal features ***could*** help a model to perform better if we can foresee some **temporal patterns**. We will analyze monthly patterns over different years of all the reviews.","0f2add32":"## Loading Data\n\nSince we are just doing an exploratory analysis of the data, **we are going to concatenate test and train data into one dataset**.","75a6c10c":"Pending: \n- Top 10 Conditions which have used the most amount of drugs\n- Topic modeling on Reviews\n- Topic Modeling on top 3 drugs reviews.\n- Temporal analysis for one drug (Levonogestrel | Top 1 reviewed). ","77c7b04d":"### Vader Preliminary Results\n\n**First two:** Excellent performance.   \n**Next two: ** Bad performance.   \n**Next one: ** This small text is only positive in the context of medicine. For vader it is neutral.   \n**Last one: ** Fairly good performance.   ","9cfecee9":"### Top 10 Reviewed Drugs","0ece3c13":"### Top 10 conditions undergoing by drug reviewers","dda4e192":"## Text Analysis","5f345266":"### Analyzing Pairs (Drug, Condition) with best and worst ratings\n\nWe theorize that the rating of a drug, is strongly related to the condition that the person who reviewed was suffering. We are going to compute the top 10 best and worst rating of reviews for (Drug, condition) pairs.","29c61063":"### Wordclouds\n\nWordclouds are an useful tool to help us get insights of what kind of language people use in the reviews. In addition to this, we also generate wordclouds with the reviews that **other people** think are useful (and unuseful), in other to check which types of expressions other people think are useful. \n\n- All reviews\n- Top 1000 most useful comments\n- Top 100 most useful comments\n- Top 1000 most unuseful comments\n- Top 100 most unuseful reviews","cbd91fea":"## Sentiment Analysis\n\nWe will analyze the sentiment of the reviews. We theorize that **the sentiment embedded on a review can be a key factor** to determine the review rating, the review impact on people (usefulCount), or how good a drug is for an specific condition. \n\nHowever, reviews can expose many different manifestations, such as: condition symptoms (usually negative), feelings before the drug (usually negative), feelings after the drug (could be positive or negative), side effects (usually negative), symptoms after the drug (could be positive or negative). This mix of manifestations could make any sentiment analysis method to struggle determining a single score for the entire review. Therefore, a aspect-based S.A. could be more viable (such as the ones used by Gr\u00e4\u00dfer et al.).\n\nFor now, we are going to use Vader [1], a rule-based sentiment analysis approach available in Python NLTK. To **obtain a single sentiment score for every review** (-1|negative to 1|positive), and see how it correlates with ratings (spearman coeff.).\n\nBefore doing this, **text** must be processed:\n\n- HTML characters are converted to legible text\n- Punctuation signs are removed\n\n[1] Gilbert, C. H. E. (2014). Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth International Conference on Weblogs and Social Media (ICWSM-14). Available at (20\/04\/16) http:\/\/comp.social.gatech.edu\/papers\/icwsm14.vader.hutto.pdf.","43fb69c5":"## Descriptive Analysis\n","3b9df105":"*No monthly temporal pattern can be seen over the years. More temporal patterns pending...","2a3554c5":"### Top 10 drugs with best and worst rating\n\nWhen doing this analysis for all the drugs, there is a factor which affect the results: **the number of reviews of a drug**. I.e. if we want to compute the drug with best rating, the drug which in **only 1 review** was given a 10 rating would be in TOP 1. For this reason, we only consider drugs with a significant number of reviews. A significant number of reviews is when the number of reviews for a drug is above the 75% of the number of reviews distribution per drug.\n","24a68493":"## Data Cleaning and Data Glimpse\n\nWe are willing to check if this dataset have some corrupted data or non present values. \n\n**condition** is a field which present problems in 1171 registers, probably due to some bug in the data web scraping. The value of those registers is the usefulCount value with an ```span``` HTML tag and the following text: ```users found this comment helpful```. We ignored these registers.","d94ec877":"# Exploratory Data Analysis w\/ Python\n\nThis kernel comprise exploratory data analysis on the provided data, including: \n- Data loading\n- Data cleaning \n- Descriptive Analysis\n- Temporal Analysis\n- Sentiment Analysis\n- Text Analysis\n\n"}}