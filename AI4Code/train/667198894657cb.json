{"cell_type":{"50a00111":"code","426e2143":"code","20e19e22":"code","7ed35ac0":"code","1e7abaa0":"code","1ac74fc1":"code","4d882b2c":"code","1c1e0a7f":"code","c9122915":"code","1424e8ff":"code","03b1aef1":"code","f8824abe":"code","c1cf82d2":"code","228ff966":"code","4684ceaf":"code","8f183a2c":"code","2b18b5c8":"code","e9f958d5":"code","18bd8a9a":"code","4961710e":"code","30466dd9":"code","e0fbcbe9":"code","e9b663e1":"code","c75008c0":"code","f39be022":"code","5826d84f":"code","a6250e2d":"code","e8b435a0":"code","9e9dc712":"code","34fd055b":"code","b68c9885":"code","ee99d84f":"code","30d3fedf":"code","d6220396":"code","4fa13b6b":"code","8debb276":"code","7f1cb6ed":"code","c8a2f998":"code","d57a7b43":"code","1f1aa27a":"code","79316087":"code","c86460a1":"code","38ccdc75":"code","58dd489a":"code","6359bc96":"code","a2fa4e2c":"code","e9b0ea64":"code","d8e7a246":"code","edb6d834":"code","b5238bf3":"markdown","1dc84698":"markdown","8a579de4":"markdown","7b0d2c6c":"markdown","0686b52c":"markdown","aa49214c":"markdown","9846b7c4":"markdown","a125d074":"markdown","0a53ab30":"markdown","93ba2264":"markdown","617529dd":"markdown","b1c5c76b":"markdown","39432e87":"markdown","d9160673":"markdown","4035bffb":"markdown","f0303aa0":"markdown","978c8d09":"markdown","814dacdb":"markdown","590f6dae":"markdown","0b194dc2":"markdown","9ba2d207":"markdown","fce97add":"markdown","41c5339c":"markdown","5831a3db":"markdown","b5c09fc9":"markdown","c5b200ae":"markdown","7c2f8198":"markdown","71a60ea3":"markdown","6ff16ac8":"markdown","688d555f":"markdown","533998d8":"markdown","03b3293d":"markdown","edb99041":"markdown","2eb77666":"markdown","2da5408b":"markdown","6416deef":"markdown","16550676":"markdown","436a38f2":"markdown","c82aba5b":"markdown","983333a9":"markdown","76a9721c":"markdown"},"source":{"50a00111":"!pip install translate\n!pip install folium\n!pip install geopy\n!pip install yellowbrick\n!pip install google_trans_new","426e2143":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom google_trans_new import google_translator\n\nfrom geopy.geocoders import Nominatim\nimport folium\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.text import TSNEVisualizer\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import text \nfrom operator import itemgetter","20e19e22":"sales=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\nsales.head()","7ed35ac0":"sales['item_cnt_day'].describe()","1e7abaa0":"#histogram\nsns.violinplot(sales['item_cnt_day'],split=True)\nplt.title(\"Item Sales Distribution\")","1ac74fc1":"sales['item_cnt_day'].value_counts(bins=3)","4d882b2c":"sales = sales[sales['item_cnt_day'] <= 700]\nsales['item_cnt_day'].value_counts(bins=2)","1c1e0a7f":"sns.distplot(sales['item_cnt_day'],kde=True,bins=10)\nplt.title(\"Item Sales Distribution\")","c9122915":"print(\"Skewness: %f\" % sales['item_cnt_day'].skew())\nprint(\"Kurtosis: %f\" % sales['item_cnt_day'].kurt())","1424e8ff":"sns.violinplot(sales['item_price'])\nplt.title(\"Item Price Distribution\")","03b1aef1":"sales['item_price'].value_counts(bins=6)","f8824abe":"sales = sales[(sales['item_price'] > 0) & (sales['item_price'] < 51000)]\nsns.distplot(sales['item_price'],kde=True,bins=10)\nplt.title(\"Item Price Distribution\")","c1cf82d2":"print(\"Skewness: %f\" % sales['item_price'].skew())\nprint(\"Kurtosis: %f\" % sales['item_price'].kurt())","228ff966":"sales['Revenue']=sales['item_cnt_day']*sales['item_price']\nfig, axes = plt.subplots(2, 1, figsize=(25,15), sharex=True)\ncolors={'item_cnt_day':'green','Revenue':'red'}\nfor name, ax in zip(['item_cnt_day','Revenue'], axes):\n    ax.plot(sales.set_index('date_block_num')[name], marker='.', linestyle='-', linewidth = 0.5, label=name, color=colors[name])\n    #sns.boxplot(data = sales, x='date_block_num', y=name, ax=ax)\n    ax.set_ylabel(\"\",fontsize=20)\n    ax.set_xlabel(\"Month\",fontsize=20)\n    ax.set_title(name,fontsize=20)\n    ax.grid()\n    if ax != axes[-1]:\n        ax.set_xlabel('')","4684ceaf":"sales['month']=sales['date_block_num']%12+1\nsales['month'].unique()","8f183a2c":"print(\"Missing Values in Sales Data:\")\nsales.isna().sum()","2b18b5c8":"shops=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nshops['shop_name'].unique()","e9f958d5":"shops=pd.read_csv(\"..\/input\/1c-translated-shops\/Translated_shops.csv\")\nshops=shops[shops.columns[1:]]\nsales=sales.join(shops.set_index('shop_id'),on='shop_id',how='left')\n\"\"\" \n##Below code is how I completed the translations\n\ntranslator = google_translator()\nimport time\n\nshop_translations = {}\nunique_elements = shops['shop_name'].unique()\nfor element in unique_elements:\n    # add translation to the dictionary    \n    shop_translations[element] = translator.translate(element)\n\nshops['Trans_shops']=shops['shop_name'].map(shop_translations)\nsales['Trans_shops']=sales['shop_name'].map(shop_translations)\n\"\"\"\nsales['Trans_shops'].unique()","18bd8a9a":"print(sales.loc[sales['shop_id']==0,'Trans_shops'].iloc[0],sales.loc[sales['shop_id']==57,'Trans_shops'].iloc[0])\nprint(sales.loc[sales['shop_id']==1,'Trans_shops'].iloc[0],sales.loc[sales['shop_id']==58,'Trans_shops'].iloc[0])\nprint(sales.loc[sales['shop_id']==11,'Trans_shops'].iloc[0],sales.loc[sales['shop_id']==10,'Trans_shops'].iloc[0])\nprint(sales.loc[sales['shop_id']==40,'Trans_shops'].iloc[0],sales.loc[sales['shop_id']==39,'Trans_shops'].iloc[0])","4961710e":"print(sales.loc[sales['shop_id']==0,'shop_name'].iloc[0],sales.loc[sales['shop_id']==57,'shop_name'].iloc[0])\nprint(sales.loc[sales['shop_id']==1,'shop_name'].iloc[0],sales.loc[sales['shop_id']==58,'shop_name'].iloc[0])\nprint(sales.loc[sales['shop_id']==11,'shop_name'].iloc[0],sales.loc[sales['shop_id']==10,'shop_name'].iloc[0])\nprint(sales.loc[sales['shop_id']==40,'shop_name'].iloc[0],sales.loc[sales['shop_id']==39,'shop_name'].iloc[0])","30466dd9":"print(str(sales.loc[sales['shop_id']==0,'date_block_num'].unique()),str(sales.loc[sales['shop_id']==57,'date_block_num'].unique()))\nprint(str(sales.loc[sales['shop_id']==1,'date_block_num'].unique()),str(sales.loc[sales['shop_id']==58,'date_block_num'].unique()))\nprint(str(sales.loc[sales['shop_id']==11,'date_block_num'].unique()),str(sales.loc[sales['shop_id']==10,'date_block_num'].unique()))\nprint(str(sales.loc[sales['shop_id']==40,'date_block_num'].unique()),str(sales.loc[sales['shop_id']==39,'date_block_num'].unique()))","e0fbcbe9":"sales.loc[sales['shop_id']==0,'shop_name']=sales.loc[sales['shop_id']==57,'shop_name'].iloc[0]\nsales.loc[sales['shop_name']==sales.loc[sales['shop_id']==57,'shop_name'].iloc[0],'shop_id']=57\nsales.loc[sales['shop_id']==1,'shop_name']=sales.loc[sales['shop_id']==58,'shop_name'].iloc[0]\nsales.loc[sales['shop_name']==sales.loc[sales['shop_id']==58,'shop_name'].iloc[0],'shop_id']=58\nsales.loc[sales['shop_id']==11,'shop_name']=sales.loc[sales['shop_id']==10,'shop_name'].iloc[0]\nsales.loc[sales['shop_name']==sales.loc[sales['shop_id']==10,'shop_name'].iloc[0],'shop_id']=10\nsales.loc[sales['shop_id']==40,'shop_name']=sales.loc[sales['shop_id']==39,'shop_name'].iloc[0]\nsales.loc[sales['shop_name']==sales.loc[sales['shop_id']==39,'shop_name'].iloc[0],'shop_id']=39\n\nprint(\"New unique shops count: \"+str(sales['shop_id'].nunique()))","e9b663e1":"fig, ax = plt.subplots(1,1, figsize=(30, 15))\nsns.countplot('shop_id', data= sales,\n                 order=sales['shop_id'].value_counts().index[:20],\n                 alpha=0.7)\n#ax[i\/\/3][i%3].set_ylim([0, 250000])\nax.set_xlabel(\"shop_id\",fontsize=20)\nax.set_ylabel(\"count\",fontsize=20)\nax.grid()\nax.set_title('High Frequency shop_ids', fontsize=20)","c75008c0":"fig = plt.figure(figsize=(40, 20))\n\nplt.title(\"Do the shops with most frequent sales have the highest sales?\",fontsize=30)\n\nax1 = fig.add_subplot(2,1,1)\nsns.countplot(data = sales, x = 'shop_id', ax = ax1)\nax1.set_xlabel(\"shop_id\",fontsize=20)\nax1.set_ylabel(\"count\",fontsize=20)\nax1.grid()\n\nax2 = fig.add_subplot(2,1,2)\nsns.boxplot(data = sales, x='shop_id', y='item_cnt_day' , ax = ax2)\nax2.set_xlabel(\"shop_id\",fontsize=20)\nax2.set_ylabel(\"item_cnt_day\",fontsize=20)\nax2.grid()","f39be022":"test=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nprint(\"We have a total of \"+str(test['shop_id'].nunique())+\" shops in our test set.\")","5826d84f":"shopwise_sales=pd.DataFrame(sales.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day'].sum()).reset_index()\n\nshopwise_sales['month_item']=shopwise_sales['date_block_num'].astype('str')+'_'+shopwise_sales['item_id'].astype('str')\ndel shopwise_sales['date_block_num'],shopwise_sales['item_id']\n\nshopwise_sales=shopwise_sales.set_index('month_item').pivot(columns='shop_id',values='item_cnt_day').fillna(0.0)\nshopwise_sales.head()","a6250e2d":"corrmat = shopwise_sales.corr()\nf, ax = plt.subplots(figsize=(16, 12))\nsns.heatmap(corrmat, vmax=.8, square=True);\nplt.title(\"Shop Sales Heatmap\",fontsize=20)","e8b435a0":"print(sales.loc[(sales['shop_id']==9),'Trans_shops'].iloc[0])\nprint(sales.loc[(sales['shop_id']==20),'Trans_shops'].iloc[0])","9e9dc712":"sales=sales[(sales['shop_id']!=9)&(sales['shop_id']!=20)]","34fd055b":"shopwise_sales=pd.DataFrame(sales.groupby(['date_block_num','Trans_shops','item_id'])['item_cnt_day'].sum()).reset_index()\nshopwise_sales['month_item']=shopwise_sales['date_block_num'].astype('str')+'_'+shopwise_sales['item_id'].astype('str')\ndel shopwise_sales['date_block_num'],shopwise_sales['item_id']\nshopwise_sales=shopwise_sales.set_index('month_item').pivot(columns='Trans_shops',values='item_cnt_day').fillna(0.0)\nshopwise_sales['shop']=shopwise_sales.index.str.split('_')\nshopwise_sales['shop']=shopwise_sales['shop'].str[1]\nshopwise_sales.head()","b68c9885":"sns.set(font_scale = 0.75)\ncols = shopwise_sales.columns[:3].tolist()+shopwise_sales.columns[11:13].tolist()\nsns.pairplot(shopwise_sales[cols], size = 2.5, plot_kws= {\"s\":40,\"alpha\":1.0,'lw':0.5,'edgecolor':'k'})\n\nplt.show();","ee99d84f":"sales['city']=sales['Trans_shops'].str.split(' ').str[0]\nsales['city'].unique()","30d3fedf":"sales.loc[sales['city']=='Vorone\u017e','city']='Voronezh'\nsales.loc[sales['city']=='SPb','city']='SPB'\nsales['city']=np.where(sales['city'].str.contains('online'),'Internet',sales['city'])\nsales['city']=np.where(sales['city'].str.contains('Online'),'Internet',sales['city'])\nsales['city']=np.where(sales['city']=='Rostnone','Rostovnadon',sales['city'])\n\nsales.loc[sales['city'].str.contains('Internet'),'shop_id'].unique()","d6220396":"geolocator = Nominatim(user_agent='myapplication')\n\n# Get a basic world map.\nshops_map = folium.Map(location=[60,40], zoom_start=3,tiles=\"Stamen Terrain\")\nfor rus_city in sales['city'].unique():\n    if rus_city=='Sergiev':\n        rus_city= 'Sergiev Posad'\n    location = geolocator.geocode(rus_city)\n    if location!=None:\n        folium.CircleMarker(location=[location.raw['lat'], location.raw['lon']], popup=rus_city,radius=2.5, \n                    color='yellow',\n                    fill=True,\n                    fill_color='blue',\n                    fill_opacity=0.6).add_to(shops_map)\nshops_map","4fa13b6b":"item=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitem_cat=pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ntest=test.join(item.set_index('item_id'),on='item_id',how='left')\ntest=test.join(item_cat.set_index('item_category_id'),on='item_category_id',how='left')\n\nsales=sales.join(item.set_index('item_id'),on='item_id',how='left')\nsales=sales.join(item_cat.set_index('item_category_id'),on='item_category_id',how='left')\n\nitemcat_sales=pd.DataFrame(sales.groupby(['date_block_num','shop_id','item_category_id'])['item_cnt_day'].sum()).reset_index()\nitemcat_sales['month_shop']=itemcat_sales['date_block_num'].astype('str')+'_'+itemcat_sales['shop_id'].astype('str')\ndel itemcat_sales['date_block_num'],itemcat_sales['shop_id']\nitemcat_sales=itemcat_sales.set_index('month_shop').pivot(columns='item_category_id',values='item_cnt_day').fillna(0.0)\nitemcat_sales.head()","8debb276":"print(\"Test set contains \"+str(test['item_category_id'].nunique())+\" item categories\")","7f1cb6ed":"corrmat = itemcat_sales.corr()\nf, ax = plt.subplots(figsize=(16, 16))\nsns.heatmap(corrmat, vmax=.8, square=True);\nplt.title(\"Item Categories Sales Heatmap\",fontsize=20)","c8a2f998":"sales=sales.loc[sales['item_category_id'].isin(test['item_category_id'])]","d57a7b43":"cats=pd.read_csv(\"..\/input\/1c-translated-item-categories\/Translated_item_categories.csv\")\ncats=cats[cats.columns[2:]]\nsales=sales.join(cats.set_index('item_category_id'),on='item_category_id',how='left')\n\"\"\" \n##Below code is how I completed the translations\n\ntranslator = google_translator()\n\ncat_translations = {}\nunique_elements = item_cat['item_category_name'].unique()\nfor element in unique_elements:\n    # add translation to the dictionary\n    cat_translations[element] = translator.translate(element)\n    \nitem_cat['Trans_cat']=item_cat['item_category_name'].map(cat_translations)\nsales['Trans_cats']=sales['item_category_name'].map(cat_translations)\n\n\"\"\"\n\nsales['Trans_cat'].unique()","1f1aa27a":"item_translates=pd.read_csv(\"..\/input\/1c-translated-items\/Completed_item_translations.csv\")\nitem_translates.columns=['item_name','Trans_item']\nitem_translates=dict(item_translates.values)\nsales['Trans_item']=sales['item_name'].map(item_translates)\n\nitem_cat_best=sales.groupby(['Trans_cat','Trans_item'])['item_cnt_day'].sum().sort_values(ascending=False).reset_index()\nitem_cat_best[:10]","79316087":"import plotly.express as px\nfig =px.sunburst(\n    item_cat_best[1:11],path=['Trans_cat','Trans_item'],\n    values='item_cnt_day',title='Top 10 Item Category\/Item name Pairs Sunburst'\n)\nfig.show()","c86460a1":"sales['Broad_cat']=sales['Trans_cat'].str.split('-').str[0]\nsales.loc[sales['Broad_cat']=='PC games ','Broad_cat']='PC Games '\nsales['Broad_cat'].unique()","38ccdc75":"sales['Broad_item']=sales['Trans_item'].str.split(' ').str[0:2].str.join(',').str.replace(',',' ')\nsales['Broad_item'].head()","58dd489a":"fig = plt.subplots(figsize=(16, 8))\n\nplt.subplot(121)\ncustom_colors = sns.color_palette(\"Spectral\")\nbroadcat_sales=sales.groupby('Broad_cat')['item_cnt_day'].sum().sort_values(ascending=False).reset_index()[:10]\nplt.pie(broadcat_sales['item_cnt_day'],labels=broadcat_sales['Broad_cat'],radius = 1.5,\\\n        explode=(0,0,0,0,0,0,0,0,0,0),autopct='%1.1f%%',colors=custom_colors,shadow=True)\ncentre_circle = plt.Circle((0,0),0.65,color='black', fc='white',linewidth=1.25)\nplt.title(\"Top Broad cats donut chart\",fontsize= 30,fontname=\"Times New Roman\",fontweight=\"bold\")\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis('equal')\nfig.tight_layout()\n\nplt.subplot(122)\nimport matplotlib.colors as colors\ncustom_colors = sns.color_palette(\"Paired\")\nbroaditem_sales=sales.groupby('Broad_item')['item_cnt_day'].sum().sort_values(ascending=False).reset_index()[1:11]\nplt.pie(broaditem_sales['item_cnt_day'],labels=broaditem_sales['Broad_item'],radius = 1.5,\\\n        explode=(0.215,0.4,0.15,0.3,0.25,0,0.03,0.1,0.01,0.07),autopct='%1.1f%%',colors=custom_colors,startangle=180)\nplt.title(\"Top Broad items pie chart\",fontsize= 30,fontname=\"Times New Roman\",fontweight=\"bold\")\nplt.axis('equal')\nfig.tight_layout()\nplt.show()","6359bc96":"items_subset = item[['item_id', 'item_name','item_category_id']]\nitems_subset['Trans_item']=items_subset['item_name'].map(item_translates)\n\n##This is the set of tf-idf features I go on to use for the models I train for my final predictions\nfeature_count = 25\ntfidf = TfidfVectorizer(max_features=feature_count)\ntfidf = TfidfVectorizer()\nitems_text_fts = pd.DataFrame(tfidf.fit_transform(items_subset['item_name']).toarray())\n\ncols =items_text_fts.columns\nfor i in range(feature_count):\n    feature_name = 'item_name_tfidf_' + str(i)\n    items_subset[feature_name] = items_text_fts[cols[i]]","a2fa4e2c":"vectorizer = CountVectorizer(stop_words=text.ENGLISH_STOP_WORDS.union(stopwords.words('russian')))\ndocs = vectorizer.fit_transform(items_subset['item_name'])\n\nword_list = vectorizer.get_feature_names()\ncount_list = docs.toarray().sum(axis=0)\nvals=dict(zip(word_list, count_list))\n\nN = 15\nres = dict(sorted(vals.items(), key = itemgetter(1), reverse = True)[:N])\n\nfeatures = vectorizer.get_feature_names()\n\nnew_vals=[]\nrus_vals=[]\nfor element in res.keys():\n    rus_vals.append(element)\n    new_vals.append(element)\n\nfor n, i in enumerate(features):\n    for count in range(N):\n        if i==rus_vals[count]:\n            features[n]=rus_vals[count]   \n            \nvisualizer = FreqDistVisualizer(features=features,n=N,color=custom_colors)\nvisualizer.fit(docs)\nvisualizer.poof()","e9b0ea64":"sample_sales=sales.loc[sales['item_cnt_day']<=1.0].iloc[:500]\nsample_sales=sample_sales.append(sales.loc[(sales['item_cnt_day']>20.0)&(sales['item_cnt_day']<30.0)].iloc[:500])\nsample_sales=sample_sales.append(sales.loc[sales['item_cnt_day']>45.0].iloc[:500])\nsample_sales['binned_item_cnt']=pd.qcut(sample_sales['item_cnt_day'].rank(method='first'),q=3)\n\ntsne = TSNEVisualizer()\n\ntsne.fit(tfidf.fit_transform(sample_sales['item_name']),sample_sales['binned_item_cnt'],color=sns.color_palette(\"Spectral\"),\\\ncmap = ListedColormap(('red', 'green')))\ntsne.poof()","d8e7a246":"sales['num_uniq_items']=sales.groupby('date_block_num')['item_id'].transform(lambda x: x.nunique())\nsales['num_uniq_shops']=sales.groupby('date_block_num')['shop_id'].transform(lambda x: x.nunique())\n\nfig, axes = plt.subplots(2, 1, figsize=(16, 9), sharex=True)\ncolors={'num_uniq_items':'coral','num_uniq_shops':'purple'}\n\nfor name, ax in zip(['num_uniq_items','num_uniq_shops'], axes):\n    ax.plot(sales.set_index('date_block_num')[name], marker='.', linestyle='-', linewidth = 0.5, label=name, color=colors[name])\n    #sns.boxplot(data = sales, x='date_block_num', y=name, ax=ax)\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"Month\")\n    ax.set_title(name)\n    if ax != axes[-1]:\n        ax.set_xlabel('')","edb6d834":"pivoted_item_sales = sales.pivot_table(index=['item_id'],values=['item_cnt_day'], \n                                        columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\npivoted_item_sales.columns = pivoted_item_sales.columns.droplevel().map(str)\npivoted_item_sales = pivoted_item_sales.reset_index(drop=True).rename_axis(None, axis=1)\npivoted_item_sales.columns.values[0] = 'item_id'\n\noutdated_items = pivoted_item_sales[pivoted_item_sales.loc[:,'27':].sum(axis=1)==0]\nprint('Outdated items:', len(outdated_items))\n\nprint('Outdated items in test set:', len(test[test['item_id'].isin(outdated_items['item_id'])]))","b5238bf3":"<font size=\"3\">\n\n>So only 42 of the 56 shops of our train set are present in the test set. This begs a very pivotal question, especially given the already large amount of train data, whether should we really retain the data for those 14 shops absent in the test set? Would they really add value to our forecasting process for the test set? Even if they do add some value, do they justify the vast amount of baggage (large data) they carry?\n>\n>These are some very tricky questions to approach, and as usual the answer lies within our data itself. Let us try to primarily examine for 2 traits within the shops data-;\n>\n> - How much value do these extra 14 shops add to the forecasting of the rest of the 42 shops test data. We need to find a way to quantify this, or even better, visualize it.\n> - How much noise do these 14 shops have in their data?","1dc84698":"<font size=3>\n\n>Armed with the translations of both categories and items now, let's get a bit inventive with how we visualize the top categories and items, and move away from the Seaborn sensation for a little bit.","8a579de4":"<font size=\"3\">\n\n>As we can see from both the English and original Russian shop names (yayyy I can read Russian now too!), this almost certainly is an error, the names are near identical and is probably a data entry typo. But just to be double sure let us examine the time periods these shops are present in to check if they are interspersed.","7b0d2c6c":"<font size=\"3\">\n\n>These seem to be some special \"Sale\" and \"Exit\" shops, that could explain why their transactions data is so unusual, and uncorrelated with the rest of the train data, let's get rid of these shops transactions data already. See I always say more data is not always better!... Ok thats a lie, I love data and I believe I can never have enough of it.\n>\n>Note: To be doubly sure, I did cross validation of my models both with and without these 2 shops as well, and no prizes for guessing, my models without these insolent shop data points fared way better. ","0686b52c":"<font size=\"3\">\n\n>I have cherry picked 5 shops here, 4 of them belonging to the same cities (in 2 pairs) and 1 of them an online shop. Above I have the scatter plots of all these shops' sales data against one another, as we can see the plots confirm that there is indeed use for having a 'city' feature.\n>\n>As we see the Yakutsk city plots (first 2) and Krasnoyarsk city plots (last 2) are the only ones showing correlation in their scatter plots. The Yakutsk shops are uncorrelated with Krasnoyarsk shops and all the shops are uncorrelated with the online shop sales (middle row of plots) where we those L-shaped plots. The L-shaped plots are once again a reminder of how all the data is so skewed towards item sales of very small amounts. Thus we see a lot of promise for the 'city' feature which can so easily be formulated just as the first word of the shop names.","aa49214c":"<font size=\"3\">\n\n>Just as I suspected, the time periods these shops are present in are mutually exclusive (shop nos. 40 and 39 do have some common periods but even for them in the last 10 months, there is only data for shop 39), further strengthening my suspicion. Damn, I would make a good detective, wouldn't you agree? But then again all data scientists like you would too.\n>\n>Let us set these shops to be the same, so that their data can be accurately aggregated and the shop sales are not misrepresented. I am going to set these shops to be equal to the ones which are in the Test Set, for ease in training and then forecasting.","9846b7c4":"<font size=\"3\">\n\n>Obviously not all shops have the same number of transactions, with some shops winning the bulk of the transactions. However the important question we must ask ourselves is does the highest number of transactions translate to highest number of item sales (our target variable, after all)? ","a125d074":"<font size=3>\n   \n>The implication of the above finding is extremely significant! It means that there is a very strong chance those 6972 items in test set (across all shops) are likely to have zero sales for our test month (November), we can simply mark them as 0.0 for our predictions without even running any models! I cross validated the use of this logic for prior months, the most shocking fact I found is even the best models I could train did not make the zero predictions, which when I manually changed to zero, improved my cross validation error significantly. This just goes to show how important preprocessing,visualization,EDA and in general human intuition is, given that it can beat even the most complex models in existence (at least so far it can).\n>\n>And I think that is a very fitting note to end the 1st part of my kernels in, it's truly been extremely fun and joyous putting this together, and I have learned so much while attempting to articulately express my thought process with my work on this competition. I hope you readers have gained at least a little as well from the journey, if not then read it again! Jokes aside, this is my first public kernel, so <b>please do drop feedback on how I can improve it, and what you did\/did not like<\/b>.","0a53ab30":"<font size=\"3\">\n\n>Hmmm 12 and 55, why do they sound so familiar? Yes that's right if you scroll up to our correlation heatmaps, you will see 55 is another of those rogue black shops spreading darkness to our dataset, and 12 is pretty purple too. But since they are unfortunately present in our test set too, we have to stick with them and perhaps find another way to model them. Knowing that both of them belong to the category of Internet shops should help.\n>\n>Anyhow let's see what we can do about getting to those locations of these cities we have generated. I am a visual person and I like it when I am able to perfectly visualize what I am forecasting here.","93ba2264":"<font size=\"3\">\n\n>With the 'city' feature in hand, once again if we scrutinize all the city names, we find that some cities are very similar with minute changes. This is because obviously we cannot rely on perfect translations from the Python Google library (that's right Google, you Suck!! Wait isn't Kaggle owned by Google now? Oh no..)\n>\n>For example, Voronez seems awfully similar to Voronezh and SPB to SPb, even though I have no clue where on Earth these cities are (I will remedy that as well in a bit). Also 'Online' to '1c=online', let's right these wrongs...","617529dd":"<font size=\"3\">\n\n>With that completed, our shops count has dropped and we can be more confident about the data in the context of shops. Now we can suitably scruitinize the shop wise data to make some inferences on the same, let's begin by checking which shops have the highest amount of transactions, and how skewed this distribution is, but this let's add some color to our plots, shall we?","b1c5c76b":"<font size=\"3\">\n<center><h1>Introduction <\/h1><\/center>\n\n> <b>Competition Setting: <\/b>Predicting Future Sales for a Russian software retail company 1C; Given a timeseries starting 2013-2015 Oct for sales of 1C on various items in it's multiple shop outlets, the ask is to predict monthly sales for the various shop-item pairs in Nov 2015.<br>\n> <b>Why is this interesting for 1C: <\/b>This is a crucial question to start any data analytics process with; One of the biggest benefits data science delivers to retail companies is optimizing their inventory and procurement.\nThanks to predictive tools, businesses can use historical data and trend analysis to determine which products they should order-> they can optimize inventory management to emphasize products customers need effectively.\n><center><h1>Dataset <\/h1><\/center>\n\nLet me begin by giving a flavor of the beast we are dealing with here, the very voluminous but relatively easily comprehensible datasets we are to work with-;\n\n> <b>sales_train.csv - <\/b>the training set. Daily historical data from January 2013 to October 2015.<br>\n> <b>test.csv - <\/b>the test set. We need to forecast the sales for these shops and products for November 2015.<br>\n> <b>items.csv - <\/b>supplemental information about the items\/products (item names)<br>\n> <b>item_categories.csv - <\/b>supplemental information about the items categories (item category names)<br>\n> <b>shops.csv - <\/b>supplemental information about the shops (shop names)<br>\n\n<b>Key Data Challenges - <\/b>Aside from the common issues of outliers and missing data (as we will see, the data quality provided here is actually very good, so we don't have to worry about these issues very much), there are 3 unique (or maybe not actually) data challenges-;\n\n> <b>Russian Data - <\/b>Nearly all of the data is in Russian text, however we as data explorer junkies are not to be deterred by this! Here I will show how we can easily translate all of the text to English and the amazing insights this will lead us to, just as all the breathtaking stories only data hows to tell.<br>\n> <b>Large Data - <\/b>The sales_train dataframe comprises of 3 million rows, which obfuscates signal detection to some extent but also obviously provides us with a tremendous opportunity to get more data insights and to pull our big guns out and brag with our more complex data forecasting processes. However for achieving this, we must endeavour to trim the noise from the data as far as possible.<br>\n> <b>Mix of shops and items Data (with some Test data items unseen in Train) - <\/b>We are presented with transactions of various items and various shops in our train data set. Our model must be versatile enough so that it can build a wholesome model that can find ways to consistently predict despite these variations. Moreover it must be able to predict sales for items for which we have no data in our train set too!\n><center><h1>Motivation <\/h1><\/center>\n\n> I very recently embarked on this exciting data science expedition and ever since I have wondered why did I not delve into this intricate world before. This kernel (as part of a 2 kernels series) is intended to showcase (and more importantly to brag - I am <b>currently placed 12\/11789 place (top 0.1%) with 0.821 RMSE score<\/b>) how much I enjoyed working on this data challenge and the unbelievably massive learning I had (will talk more about my learnings in the 2nd kernel); but it is also my hope that I can reveal some really exciting and breathtaking insights about this seemingly run-of-the-mill sales data, so that the reader is engaged and exhilarated enough to want to venture on working on this data themselves too and join in on the data adventures here!<br><br>\n> As is custom, I began working with EDA, in order to invoke ideas for feature engineering, which as we know, is one of the most (if not THE most) significant processes for achieving good forecasting accuracy - here I have presented a snapshot of some of the most beautiful data visualizations which led me to some very unique feature engineering ideas and the final forecasting score that I achieved. Wondrously, this dataset offered the opportunity to leverage a huge variety of feature engineering tools- I have displayed here the many features I developed- <b>geolocation, text vectorization,log transformations,discretization of continuos features and target mean encodings of categorical features<\/b>.<br>\n><center><h1>Acknowledgements <\/h1><\/center>\n\n>I would like to thank the author of the following kernel from whom I learned so much, you must visit it as well-;\n> - https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data\n\n>Without further ado, let us dive into the data analysis as promised!","39432e87":"<font size=\"3\">\n\n>There, the distribution plot is much more reasonable now, however once again we see a significant skew. Again using a log transformation is a good idea when we prepare the data for training our models.All these fancy data distributions, why can't they just be <i>normal<\/i>?\n>\n>Let us now look at how these 2 numeric features have developed over time. But since I am going to be aggregating the data in a monthly time frame (this is the one we are most interested in); instead of plotting item price, I am going to use a factor of item price and sales count, 'Revenue'.","d9160673":"<font size=\"3\">\n\n>This is the primary dataframe we will work with, as aforementioned the item,item category and shop names can all be derived from the other supplemental dataframes, and we will get to that soon enough...Let us first examine the distributions of the two numeric features given to us, the item price and item count day (target).","4035bffb":"<font size=\"3\">\n\n>The skewness and kurtosis are clearly too high, using a log transformation here is a good idea. Let us see if this is the case with our other numeric feature too.","f0303aa0":"<font size=\"3\">\n<center><h1>Importing Libraries..","978c8d09":"<font size=3>\n\n>Wow another unbelieveable method of summarising such a great deal of information in just a circle, I was truly infatuated when I came across this idea of visualizing the various categories as several rings of the circle, starting with the top 10 broader 'item categories' feature here, and on the outer half presenting the top items within each of these categories, and if we hover over any of them, we can even see the exact item sales values for each grouping.\n>\n>This Sunburst once again reiterates my earlier point of having similar categories, as we can see from the 3 'Games' categories in the various platforms: PC,PS3 and Xbox 360. All 3 of these 'Games' categories have 'Grand Theft Auto' as their top grossing product and these games can be expected to exhibit similar characteristics, so let's create a grouping for these similar categories and similar items.","814dacdb":"<font size=\"3\">\n\n>As we can see from the value counts and plot above, the distribution looks a bit more reasonable after removing the outliers.","590f6dae":"<font size=\"3\">\n\n>First observation to make here is that there are negative entries for our target variable here as well, and this is because this represents the items returned. It is a viable option to try to treat these by suitably getting rid of them, however I am actually going to opt to leave them as is since <b>are looking to forecast monthly sales, not daily sales, so when we aggregate these daily sales to monthly (to have a uniform time frame as the test dataset),<\/b>these should automatically be squared off against the original sales. We will do a check at that point, to make sure this happens as we expect it to, of course.\n>\n>We also see the mean>median, so we know this distribution is skewed to the right, perhaps a log transformation would be nice to run here. Let us also visualize the distribution with a violin (yes you read that right).","0b194dc2":"<font size=\"3\">\n\n>Suprise, suprise! The number of transactions seems to be inversely related (loosely speaking) to the number of total item sales by the shops. This is a useful discovery, because going forward (in my 2nd kernel), I use the number of transactions per month for each shop as a feature, and it definitely has significance.\n>\n>An important challenge of our dataset as I mentioned before is that we are presented with transactions for a large amount of shops, are all of these shops present in our test set? Let us investigate.","9ba2d207":"<font size=3>\n\n>There that's lesser than a quarter of the categories we started out with and it looks a lot more cleaner too. I have actually left categories like PC Games, Games, Android Games etc. separated here, but another very interesting paradigm to consider is to club all such categories together (the aforementioned categories could all be grouped under 'Games'), and make an additional feature of the platform the item belongs to (very relevant since this is a software retail company's data we are playing with).","fce97add":"<font size=3>\n\n>Now let's translate all our item category names too, so we can make some sense out of those.","41c5339c":"<font size=\"3\">\n\n>That is a satisfying visualization to leave an imprint of all our shops in our mind, while we train our models. I think the shops in Austria (\"Kolo\") and France (\"Ufa\") are misrepresented here, so we can ignore them, but for the other cities, we can easily tell that even the cross-location distances, for eg. can be a great feature.\n>\n>That's a lot of food for thought from the shops data, let's now leap onto the items' data and direct our detective lens in that direction.","5831a3db":"<font size=3>\n\n>The above timeseries plots are great to demonstrate how 1C shops' sales have developed over time (given the data we have). We see a clear decrease in both the number of item offerings and operating shops for 1C; this raises a very important question, namely how many items are in test set that could be such that they are now outdated,i.e have not been sold by any of the shops since past few months (shout out to kyakovlev for introducing this idea as well)? Let us do some analysis to find out, once again in my favorite dataframe format, the pivoted tables.","b5c09fc9":"<font size=\"3\">\n\n>As for the trends that we can observe for the 2 timeseries plots, we see a general increase in item sales and Revenue over time. The advantage of having the luxury of being able to create both the plots sharing the X-axis, thanks again to Matplotlib, is that we can see a very interesting trend of negative correlation between Revenue and item sales, obviously item price and item sales seem to exhibit some comovement; which is intuitive as well- bigger bulk of products are generally sold at lower prices and vice versa.\n>\n>Also we can clearly observe a tinge of seasonality, indicating an need for taking this into account while training our models. I will do so by creating a very simple 'month' feature, so that each transaction is linked to the month of that transaction.","c5b200ae":"<font size=\"3\">\n\n>Now let us try to see if we can find some meaningful features from the shop names data. Something that immediately catches our eye, is the first word of all the shop names...most of them seem to belong to a city, we can definitely tell that about Moscow. Let us once again get our data in a more friendly format..","7c2f8198":"<font size=3>\n\n>For all you enthusiastic foodies, isn't it so awesome that we can fix our appetite with these data appetizers, served as donuts and pies, I think I am already drooling. Making Broad Item categories has helped us to easily see the most selling items. For instance the games are obviously the biggest sales for 1C, here we see the games just have the first 2 words of their name (making some of them weird but still uniquely identifiable - for all you gamers, \"Need for\" is obviously NFS (Need For Speed), \"Call of \" is obviously \"Call of Duty\"); a fact that would be harder to prove just by seeing the pie charts on individual items, since that would be spread across various platforms.\n>\n>Let's now move across to unsheath another of our big weapons from our data armoury and perform some text analysis.","71a60ea3":"<font size=\"3\">\n\n>To get a good look in, we are going to have to squint a bit, but it's highly rewarding ~ in this one plot, we can examine the correlations between all given shops with each other in terms of their item sales!\n>\n>To start off, we see majority of the shops are light skin-peach-ish colours, indeed indicating a good correlation among shop sales.\n>\n>There are a few red\/purple patches as well, but the worst of them all are the black ones. Let's turn our attention to these black shops, which indeed do seem likely to cast only darkness to our predictive models. If we squint hard enough, we can see there are 4 shops with those black lines - shops 9,20,36 and 55. Unfortunately shops 36 and 55 are present in our test set, so we are going to have to stick with those, however shops 9 and 20 seem to only bring noise in our train data set,let's see their shop names. ","6ff16ac8":"<font size=\"3\">\n\n>Finally let us do a spot check for any missing data in our dataframe.","688d555f":"<font size=3>\n\n>Once again there are 84 item categories and not all of them are present in our test set (which has only 62 of them). So let us revisit our favorite heatmap once again, this time looking at the item categories' sales data pitted against each other.","533998d8":"<font size=\"3\">\n\n>As we can see above, the same problems seem to plague item price as well. But unlike item sales, here there is no reason for us to have negative item prices (I say this but I remember the last time I was shifting apartments, I <b>paid<\/b> a fortune so that somebody would buy and take away that worn-out sofa from me! True story). Let us get rid of the negative and extremely high values. ","03b3293d":"<font size=\"3\">\n\n>From the above pivoted snapshot, we can easily see so many first names to be repeated among shop names, giving voice to my educated guess.\n>\n>I don't know about you readers, but I have little to no idea about the Russian geography, so once again to validate my guess, I am going to again dazzle and overwhelm our senses with a huge amount of data in a small space, so we better have our wits about to draw appropriate inferences!","edb99041":"<font size=\"3\">\n\n>From the shops dataframe, we see that all names are in Russian, so unless we have all native speakers here, we must translate it all to extract some interesting features from the shop names.","2eb77666":"<font size=\"3\">\n\n>There, isn't that so much better, finally we see something that we can use . Already we can make a significant observation here; the 'Yakutsk' shop, seems to be really similar in name. After doing some comprehensive checking of all the shop names, I could find 4 shops that are very identically named-;","2da5408b":"<font size=3>\n\n>I see a lot of similar categories, for eg the various 'Games' categories and obviously they are not the same but they can be expected to exhibit similar behaviour in terms of item sales and Revenue. Thus providing the potential to create a new feature which would perhaps be a broader category clubbing these similar categories together. Let's hold that thought in our minds for a little bit, I will get to it in just a minute. But first let's also translate the item names so it can easily be worked on. Note that I am not going to run the item names translations here because it took me ages to run that on my Personal PC. I will just load it here.","6416deef":"<font size=3>\n\n>Starting out with a simple CountVectorizer, I had to be sure to do the count on Russian words itself (translation always loses some value), remove the stop words of both the English and Russian dictionaries and then convert the words back to English for us; finally thanks to YellowBrick, we have it neatly plotted into a Frequency Distribution visualizer. Once again we see how useful a platform feature will be for our prediction here (will generate it in 2nd kernel), with most keywords playing around pc,ps3,xbox and even MP3!\n>\n>Let's now move across to generating features from the item names text (I found the item names to be the most useful text feature for deriving robust predictive features) using TfIdf Vectorizer from Sklearn. To visualize it, I am going to discretize item sales too.","16550676":"<font size=\"3\">\n\n>Most values are concentrated until ~400. 2 of the values are really extreme->we can be reasonably confident that they are outliers and would not help the forecasts in any way. I mean sales of >700 in a single transaction of an item by a software retail company seems super unlikely. Let's get rid of them.","436a38f2":"<font size=3>\n\n>The binning of the item sales meant that I was able to plot the text features such that we can see whether these features were able to divvy up the binned vector into clusters. But since item sales were heavily right skewed, I had to individually pick a set of sample transactions to ensure there was sufficient dispersion among them while plotting TSNE graph. And we can see some blobs of the 3 colours being formed, providing evidence of the efficacy for using item text features.\n>\n>Finally let us move to a very interesting observation I made with the help of another timeseries plot which can be described as how 1C has grown it's items and shops base with time, which assisted me a great deal in doing some post-processing for improving my model forecasts.","c82aba5b":"<font size=\"3\">\n\n>Like I said earlier, our data is actually extremely clean,now let's move towards analyzing some of the supplemental datasets. That's where I promise our analysis will get so much more creative and amazing!","983333a9":"<font size=\"3\">\n\n>Finally now that our pivoted shopwise dataset is ready in the format I like, I have the license to get the first big weapon out of my Seaborn arsenal! ","76a9721c":"<font size=3>\n\n>Oh my those purple corridors, I think even a truck could pass through them! The item categories' sales data seems much more uncorrelated in general with each other as opposed to that of shops, which intuitively makes a lot of sense. I mean how much can the same product be different in price and amount of sales among different shops really? Maybe some really entreprenuerial shop salesmen can do better than others, but for the same product, it cannot be that much different.\n>\n>Whereas if we talk about different item categories in the same shop, they could be as different as an 'Apple Iphone' and a Ps4 DvD for Witcher 3. One of these rightfully charges a small fortune whereas the other forces it's customers to sell a kidney (and a lung for it's accessories) to pay out a huge fortune (no I'm not going to specify which is which). And thus I really don't see a strong reason to hold on to the item categories' data for which there is no mention of in the test set. And yes once again I did cross validate my models with and without these 22 item categories, finding not a major difference in forecasting accuracy but definitely a boost in forecasting time."}}