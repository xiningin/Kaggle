{"cell_type":{"bbe30e1e":"code","ff3af2d4":"code","8fa765c8":"code","fedf5fb3":"code","b85609a2":"code","ada843a6":"code","2fd5de90":"code","327fc12a":"code","4d87afae":"code","4790fd96":"code","a68f1227":"code","cae66696":"code","1fc5077a":"code","cdf0e5cc":"code","8f93f77c":"code","5a490d04":"code","d8f924f5":"code","95d703d7":"code","89c915b4":"code","eb11ef54":"code","affcd2d9":"code","09f1c1cc":"code","b3f8b713":"code","65144c6d":"code","7cb45227":"code","e9dac913":"code","268ec86b":"code","f0a4ebfc":"code","af5d8fd0":"code","ddc152b9":"code","9e330ce7":"code","0c095363":"code","97bead06":"code","db4f69d0":"code","9b01000f":"markdown","59c6c10d":"markdown","83555521":"markdown","21852e83":"markdown","d5afef5d":"markdown","22cb9201":"markdown","aac0d67b":"markdown","dc9e94bb":"markdown","96363e14":"markdown","4e105d0a":"markdown","bd3598dd":"markdown","f8ac9e91":"markdown","c9dba6f4":"markdown","e039a8d4":"markdown","a5dfe24d":"markdown","f974867b":"markdown","0743ad58":"markdown","cb8ca6ee":"markdown","4df85dc3":"markdown","abfa408d":"markdown","9f6a06ec":"markdown","49a4fa2d":"markdown"},"source":{"bbe30e1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Data visualization\nimport seaborn as sns #Data visualization\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ff3af2d4":"#importing modules and data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model.ridge import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# here we have nine models\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\ntrain_data=pd.read_csv('..\/input\/learn-together\/train.csv',index_col='Id')\ntest_data=pd.read_csv('..\/input\/learn-together\/test.csv',index_col='Id')","8fa765c8":"from sklearn.model_selection import train_test_split\nX,y=train_data.iloc[:,:-1],train_data.iloc[:,-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y)\nmod_val=[]","fedf5fb3":"#Logistic Regression\nmodel=LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Logistics Regression:',mod_val[0])","b85609a2":"#Ridge Classifier\nmodel=RidgeClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Ridge Classifier:',mod_val[1])","ada843a6":"#SupportVectorMachine\n#It takes a long time and also gives poor results\nmodel=SVC()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Support Vector Classifier:',mod_val[2])","2fd5de90":"#Decision tree Regression\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Decision Tree Classifier:',mod_val[3])","327fc12a":"#GradientBoostingClassifier\nmodel=GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Gradient Boosting classifier:',mod_val[4])","4d87afae":"#Adaboost classifier\nmodel=AdaBoostClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Ada boost classifier:',mod_val[5])","4790fd96":"#ExtraTree classifier\nmodel=ExtraTreesClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Extra Tree classifier',mod_val[6])","a68f1227":"#RandomForest Classifier\nmodel=RandomForestClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Random Forest classifier:',mod_val[7])","cae66696":"#K nearest Neighbour classifier\nmodel=KNeighborsClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nmod_val.append(round(accuracy,4))\nprint('Accuracy of Nearest neighbour:',mod_val[8])","1fc5077a":"train_data.describe().T","cdf0e5cc":"print(train_data.info())","8f93f77c":"print(train_data.head().T)","5a490d04":"# Distribution of Data according to Forest cover type\nvalues=train_data.Cover_Type.value_counts()\nplt.bar(values.index,values)\nplt.xlabel('Cover types')\nplt.ylabel('Counts')\nprint(values)","d8f924f5":"#Dropping two columns\ntrain_data.drop(['Soil_Type7','Soil_Type15'],axis=1,inplace=True)\ntrain_data.shape","95d703d7":"# Checking the two features\nprint(train_data.Soil_Type8.value_counts())\nprint(train_data.Soil_Type25.value_counts())","89c915b4":"# Dropping the other two columns as well\ntrain_data.drop(['Soil_Type8','Soil_Type25'],axis=1,inplace=True)\ntrain_data.shape","eb11ef54":"# box plot with the 10 featured column that were noted earlier\nFeatured=['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', \n'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\nplt.figure(figsize=(15,30))\nfor i,feature in enumerate(Featured):\n    plt.subplot(5,2,i+1)\n    sns.boxplot(x='Cover_Type',y=feature,data=train_data).set(title='{} vs Cover_Type'.format(feature))\nplt.tight_layout()","affcd2d9":"#correlation matrix for featured columns vs others\nFeature_Target=pd.concat([train_data[Featured],train_data[['Cover_Type']]],axis=1)\nFeature_Target.columns","09f1c1cc":"corr=Feature_Target.corr()\nplt.figure(figsize=(15,15))\nsns.heatmap(corr,square=True,center=0,linewidths=0.5,annot=True)\nplt.title('Correlation Matrix')","b3f8b713":"#Wilderness\ntrain_data.groupby('Cover_Type')['Wilderness_Area1','Wilderness_Area2','Wilderness_Area3','Wilderness_Area4'].sum()","65144c6d":"#Soil type. We have 36 soil types now and one cover_type. \n#so in total the last 37 soil types can be used for analyzing soil type\ntemp=train_data.iloc[:,-37:]\ntemp.groupby('Cover_Type').sum().T","7cb45227":"# changing dtype of coulmn\nfor col in train_data:\n    if col[:9]=='Soil_Type' or col[:10]=='Wilderness':\n        train_data[col]=train_data[col].astype('category')\ntrain_data['Cover_Type']=train_data['Cover_Type'].astype('category')\ntrain_data.info()","e9dac913":"X,y=train_data.iloc[:,:-1],train_data.iloc[:,-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y)","268ec86b":"def model_build_evaluate(classifier,r_list):\n    model=classifier()\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    accuracy=accuracy_score(y_pred,y_test)\n    r_list.append(round(accuracy,4))\n    print('Accuracy of',classifier, ':',round(accuracy,4))\n    ","f0a4ebfc":"#list of classifiers\nClassifiers=[LogisticRegression,RidgeClassifier,SVC,DecisionTreeClassifier,\n             GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier,\n             RandomForestClassifier,KNeighborsClassifier]\neda_val=[]\nfor classifier in Classifiers:\n    model_build_evaluate(classifier,eda_val)","af5d8fd0":"ax=sns.scatterplot(x=list(range(9)),y=mod_val)\nax=sns.scatterplot(x=list(range(9)),y=eda_val)\nplt.xticks(list(range(9)),Classifiers,rotation=90)\nplt.legend(['With out EDA','After EDA'])","ddc152b9":"#Multinomial Naive Bayes is imported. It is more suited for discrete variables\nfrom sklearn.naive_bayes import MultinomialNB","9e330ce7":"# Naive bayes classifier on categorical data\ncat_data=train_data.iloc[:,-41:]\nprint(cat_data.columns)\nX,y=cat_data.iloc[:,:-1],cat_data.iloc[:,-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y)\nmodel=MultinomialNB()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\naccuracy=accuracy_score(y_pred,y_test)\nprint('Accuracy of Multinomial Naive Bayes classifier',accuracy)","0c095363":"#Preprocessing\nfrom sklearn.preprocessing import StandardScaler\n#Now we have to make use of the whole model\nX,y=train_data.iloc[:,:-1],train_data.iloc[:,-1]\nX_train,X_test,y_train,y_test=train_test_split(X,y)\n#scaling the first 10 features alone\nX_train[Featured]= StandardScaler().fit_transform(X_train[Featured])\nX_test[Featured] = StandardScaler().fit_transform(X_test[Featured])","97bead06":"# classifying again\npp_val=[]\nfor classifier in Classifiers:\n    model_build_evaluate(classifier,pp_val)","db4f69d0":"ax=sns.scatterplot(x=list(range(9)),y=mod_val)\nax=sns.scatterplot(x=list(range(9)),y=eda_val)\nax=sns.scatterplot(x=list(range(9)),y=pp_val)\nplt.xticks(list(range(9)),Classifiers,rotation=90)\nplt.legend(['With out EDA','After EDA','After Preprocessing'])","9b01000f":"# Models without EDA\nThese models are built in a hurry without any EDA!","59c6c10d":"Now its time to see how the data are distributed in the categorical variables wilderness and soil type.\n","83555521":"# Models After EDA","21852e83":"The Data has equal distribution. And so accuracy would also be a better choice for metrics.","d5afef5d":"Here we see there is a significant improvement in SVM classifier from the previous, we can draw scatter plot to further analyze the results.","22cb9201":"Any comments, feedbacks are welcomed!","aac0d67b":"Except for ensembled models other models gave a significant improvement on EDA and preprocessing.\n\nNot to forget it significantly reduced the training time and memory!","dc9e94bb":"# Exploratory Data Analysis\nWe would run the three traditional EDA commands, describe(), info() and head(). We can then note the observations for each comment and then perform analysis according to the noted observations.","96363e14":"There seems to be a little improvement in the models. We must also include preprocessing here. Except for Ada\n\nAnd before that we can construct the bayesian classifier from the catrgorical variable.","4e105d0a":"Here we note that given the forest cover type of 4, the wilderness area will be wilderness area 4 only. \nSimilarly for cover type of 3, the wilderness area would be either 3 or 4.","bd3598dd":"The multinomial Naive Bayes got third place from last! But wait this classification was made just from wilderness area and soil type. \n\nIts time to preprocess and get more closer to the solution.\n\nIn the models we have seen the range of features seems to be different. For elevation the range (diffrence between min and max) is around 2000. 'Horizontal_Distance_To_Roadways' and 'Horizontal_Distance_To_Fire_points' have a range of about 6000 while the range for soil_types are only 1. So there must be a better scaling in the features for better results.","f8ac9e91":"We have only one value of 1 in 'Soil_Type8' and 'Soil_Type25' out of 15119 values. This is too insignificant so we remove this feature also.","c9dba6f4":"Also there are no significant correlation of these columns with the Cover_Type","e039a8d4":"* Wilderness_Area* and Soil_Type* have to be categorical for easy processing.\n* Also the target variable has to be 7 class category.\n* First ten features are continous (Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm,Horizontal_distance_To_Fire_Points) and we can have scatter plots, correlation matrix with target variable to get necessary informations.","a5dfe24d":"These observations implies that bayesian classifier may be of some help.\nWe can build a model of categorical variable with naive-bayes classifier.","f974867b":"We will build the models as before. But there are two difference between the old models and the models to be built now\n1. We have used similar procedure in building the previous models, so we would write a function for model building and evaluation\n\n2. (Very important) We have performed EDA on the data. We have dropped 4 columns!","0743ad58":"Here we could not find a single feature to clearly distinguish the cover type.\nLets see some of the correlation matrix of the featured variables.","cb8ca6ee":"Now we constructed the models and trained the data. We find that the ensemble classifiers had higher accuracy. Extra tree classifier had a maximum accuracy among the classifiers. \n\nThe models built used the data that might include outliers and features that might be irrelevant. We can built an efficient model after EDA.\n\nThere are two things to note here:\n* The dataset did not contain any NaN values else we would have got an error.\n* The metric used here is accuracy. It is valid, if the target variable has equal distribution in all classes","4df85dc3":"Now that we have explored with the three commands we can further process the data with the noted points.","abfa408d":"# Notebook description\n    Role of this notebook would be to see how Exploratory data analysis would affect your model. The first part of the notebook would be model building followed by EDA. We will again fit the model and evaluate both.","9f6a06ec":"* Here we note that there is no null value in any of the column as expected.\n* We have to see how the data is distributed according to target variable.\n* Two features 'Soil_Type7' and 'Soil_Type15' has all the values as 0, since the min=median=max=0. These features are irrelevant and have to be dropped.\n* Two features 'Soil_type8' and 'Soil_type25' have 0.0066% value as 1.(Since the value is categorical). So we need to further analyze these two columns.","49a4fa2d":"Note here that we have reduced the memory space from 6.5 MB to 2.5 MB"}}