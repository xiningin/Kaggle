{"cell_type":{"86541c4c":"code","65f400bf":"code","50b4865c":"code","bba5685d":"code","86da172f":"code","683cb184":"code","bf0918dd":"code","e450d51f":"code","5cbebdca":"code","6a7303b6":"code","b3ec20ac":"code","9a368775":"code","4a2698c9":"code","0a1c2c46":"code","e238ff91":"code","d5b05ba8":"code","51f37042":"code","4ff68591":"code","f26921fd":"code","6527f24f":"code","3b6653a7":"code","9e9cf749":"code","31c514a9":"code","55d5519d":"code","17f74bc6":"code","3ed9517f":"code","5572b8cf":"code","126cf849":"code","71d17062":"code","bfb56e98":"code","bbe2d375":"code","e7862437":"code","69efb7c0":"code","ad1d1113":"code","731e2806":"code","1638b9f6":"code","078ee9a1":"code","13c81434":"code","8226a01a":"code","9a5d3fe6":"code","f8c2f054":"code","19c5432b":"code","588d0274":"code","18c7c333":"code","c466cb9b":"markdown","9bb480fc":"markdown","c0633726":"markdown","1c369e99":"markdown","25ffa7b4":"markdown","cb090e07":"markdown","ec93eeec":"markdown","1df6545e":"markdown","134e9dc5":"markdown","ffeb1639":"markdown","74aa4090":"markdown","6955323d":"markdown","4cc538f5":"markdown","4a03de8a":"markdown","15982cde":"markdown","7610c440":"markdown","5e837211":"markdown","3dc95e41":"markdown","2012301a":"markdown","25dd3380":"markdown","10c14def":"markdown","caef27fc":"markdown","bb02db5c":"markdown","cf47b8a2":"markdown","9abc9dbd":"markdown","3a2af03c":"markdown","2b43961f":"markdown","b6158527":"markdown","c2dd8050":"markdown","73e41caf":"markdown","978a37ec":"markdown","78507749":"markdown","c89f5742":"markdown","c569f273":"markdown","0b7c27ae":"markdown","7b0aaca4":"markdown","cf6338fc":"markdown"},"source":{"86541c4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# pd.set_option('display.max_colwidth', -1)\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport re\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport csv\nfrom matplotlib import rcParams\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.util import ngrams\nstop = stopwords.words('english')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report,plot_confusion_matrix\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","65f400bf":"true = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")\nfalse = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue.head()","50b4865c":"false.head()","bba5685d":"true.subject.value_counts()","86da172f":"rcParams['figure.figsize'] = 15,10\ntrue.subject.value_counts().plot(kind=\"bar\")","683cb184":"rcParams['figure.figsize'] = 15,10\nfalse.subject.value_counts().plot(kind=\"bar\")","bf0918dd":"politics = true[true['subject']==\"politicsNews\"]\nworldnews = true[true['subject']==\"worldnews\"]\nprint(politics.shape)\nprint(worldnews.shape)","e450d51f":"politics_text_len = politics['text'].str.len()\nworldnews_text_len = worldnews['text'].str.len()","5cbebdca":"print(\"The maximum lenght of string in Politcs news is {} words\".format(max(politics_text_len)))\nprint(\"The maximum lenght of string in World news is {} words\".format(max(worldnews_text_len)))","6a7303b6":"def tokenizeandstopwords(text):\n    tokens = nltk.word_tokenize(text)\n    # taken only words (not punctuation)\n    token_words = [w for w in tokens if w.isalpha()]\n    meaningful_words = [w for w in token_words if not w in stop]\n    joined_words = ( \" \".join(meaningful_words))\n    return joined_words","b3ec20ac":"politics['text'] = politics['text'].apply(tokenizeandstopwords)\nworldnews['text'] = worldnews['text'].apply(tokenizeandstopwords)","9a368775":"def generate_word_cloud(text):\n    wordcloud = WordCloud(\n        width = 3000,\n        height = 2000,\n        background_color = 'black').generate(str(text))\n    fig = plt.figure(\n        figsize = (40, 30),\n        facecolor = 'k',\n        edgecolor = 'k')\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.show()\n","4a2698c9":"politics_text = politics.text.values\ngenerate_word_cloud(politics_text)\n","0a1c2c46":"worldnews_text = worldnews.text.values\ngenerate_word_cloud(worldnews_text)","e238ff91":"false.head()","d5b05ba8":"set(false.subject)","51f37042":"Government_News = false[false['subject']==\"Government News\"]\nMiddle_east = false[false['subject']==\"Middle-east\"]\nNews = false[false['subject']==\"News\"]\nUS_News = false[false['subject']==\"US_News\"]\npolitics = false[false['subject']==\"politics\"]","4ff68591":"Government_News['text'] = Government_News['text'].apply(tokenizeandstopwords)\nMiddle_east['text'] = Middle_east['text'].apply(tokenizeandstopwords)\nNews['text'] = News['text'].apply(tokenizeandstopwords)\nUS_News['text'] = US_News['text'].apply(tokenizeandstopwords)\npolitics['text'] = politics['text'].apply(tokenizeandstopwords)","f26921fd":"govertment_news_text = Government_News['text'].values\ngenerate_word_cloud(govertment_news_text)","6527f24f":"middleast_news_text = Middle_east['text'].values\ngenerate_word_cloud(middleast_news_text)","3b6653a7":"news_text = News['text'].values\ngenerate_word_cloud(news_text)","9e9cf749":"usnews_text = US_News['text'].values\ngenerate_word_cloud(usnews_text)","31c514a9":"politicsFake_text = politics['text'].values\ngenerate_word_cloud(politicsFake_text)","55d5519d":"false['target'] = 'fake'\ntrue['target'] = 'true'\nnews = pd.concat([false, true]).reset_index(drop = True)\nnews.head()","17f74bc6":"news.shape","3ed9517f":"news['text'] = news['text'].apply((lambda y:re.sub(\"http:\/\/\\S+\",\" \", y)))\nnews['text'] = news['text'].apply((lambda x:re.sub(\"\\@\", \" \",x.lower())))","5572b8cf":"news.head()","126cf849":"def basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n\n","71d17062":"true_word = basic_clean(''.join(str(true['text'].tolist())))","bfb56e98":"true_bigrams_series = (pd.Series(nltk.ngrams(true_word, 2)).value_counts())[:20]\n","bbe2d375":"true_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","e7862437":"true_trigrams_series = (pd.Series(nltk.ngrams(true_word, 3)).value_counts())[:20]\ntrue_trigrams_series.sort_values().plot.barh(color='red', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","69efb7c0":"false_word = basic_clean(''.join(str(false['text'].tolist())))","ad1d1113":"flase_bigrams_series = (pd.Series(nltk.ngrams(false_word, 2)).value_counts())[:20]","731e2806":"flase_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","1638b9f6":"false_trigrams_series = (pd.Series(nltk.ngrams(false_word, 3)).value_counts())[:20]\nfalse_trigrams_series.sort_values().plot.barh(color='red', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","078ee9a1":"words = basic_clean(''.join(str(news['text'].tolist())))","13c81434":"bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]","8226a01a":"bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Bigrams')\nplt.ylabel('Bigram')\nplt.xlabel('# of Occurances')","9a5d3fe6":"trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]","f8c2f054":"trigrams_series.sort_values().plot.barh(color='red', width=.9, figsize=(12, 8))\nplt.title('20 Most Frequently Occuring Trigrams')\nplt.ylabel('Trigram')\nplt.xlabel('# of Occurances')","19c5432b":"\nx_train,x_test,y_train,y_test = train_test_split(news['text'], news.target, test_size=0.2, random_state=2020)\n\npipe = Pipeline([('vect', CountVectorizer()),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LogisticRegression())])\n\nmodel = pipe.fit(x_train, y_train)\nprediction = model.predict(x_test)\nprint(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))","588d0274":"print(confusion_matrix(y_test, prediction))\nprint(classification_report(y_test, prediction))","18c7c333":"plot_confusion_matrix(model,x_test,y_test)","c466cb9b":"## Word Cloud for Us News Label","9bb480fc":"## Word Cloud for Goverment news Label","c0633726":"## Seperating the dataset into the different dataframe based on the labels","1c369e99":"# False News Dataset Analysis","25ffa7b4":"# N-Gram","cb090e07":"## Stopwords\n>stop words are words which are filtered out before or after processing of natural language data (text).[1] Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list","ec93eeec":"## Word Cloud for Middle east news Label","1df6545e":"# N-gram Analysis - Bigram and Trigram ","134e9dc5":"# Word Cloud for Fake news","ffeb1639":"### since i cannot able to plot this i have just printed the maximum lenght of strin[](http:\/\/)g value in the each labels","74aa4090":"# Full Dataset Analysis","6955323d":"# Exploratory data analysis","4cc538f5":"## Full Data - Bigram","4a03de8a":"## Toekenization\n>In Python tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language.Here we are performing the word tokenization from NLTK Library","15982cde":"## WordCloud\n> A tag cloud (word cloud or wordle or weighted list in visual design) is a novelty visual representation of text data, typically used to depict keyword metadata (tags) on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color.[2] This format is useful for quickly perceiving the most prominent terms to determine its relative prominence. When used as website navigation aids, the terms are hyperlinked to items associated with the tag","7610c440":"# Building a Basic Model","5e837211":"# N-gram for true news","3dc95e41":"## Word Cloud for General News Label","2012301a":"## The above Viz shows that target column is not  equally distributed in False category and News label is more than other labels","25dd3380":"## Word Cloud for Politics Label","10c14def":">Tokenization \n>Stop words removal","caef27fc":"## True News - Trigram","bb02db5c":"## Reading the CSV Files","cf47b8a2":"## False News - Bigram","9abc9dbd":"## The above Viz shows that target column is equally distributed in true category","3a2af03c":"# World Cloud form true News Dataset","2b43961f":"## True News - Bigram","b6158527":"## Full Data - Trigram","c2dd8050":"## Word Cloud for Worldnews Label","73e41caf":">In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles","978a37ec":"## Sepearting the dataset into the different dataframe based on the label column","78507749":"# Merging true and fake news dataset","c89f5742":"# N-Gram -False word Analysis","c569f273":"# Simple Pre-Processing on politics and world news dataset - True Tweets","0b7c27ae":"## Word Cloud for Politics Label in Fake dataset","7b0aaca4":"## False News - Trigram","cf6338fc":"# Defining the word Cloud function to generate the word cloud"}}