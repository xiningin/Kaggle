{"cell_type":{"54722744":"code","0c27d493":"code","9228b2bb":"code","bb47e50a":"code","332908f4":"code","a06a1306":"code","d3c7969b":"code","644e9f4a":"code","65954c26":"code","1b7a97a6":"code","fae30d17":"code","da6fe27c":"code","bfa27990":"code","c691403c":"code","b6223a24":"code","ca4f8bb4":"code","8d6df476":"code","ea074849":"code","fe13ee67":"markdown","787e7615":"markdown","1c548a1c":"markdown","ec143974":"markdown","e7ba6a60":"markdown"},"source":{"54722744":"!pip install gdown","0c27d493":"# Download the model weight if users want to use the\n#  trained model for this task (Fruits 360)\n!gdown  https:\/\/drive.google.com\/uc?id=1YfVZ7aUefO8Zx5UVnyJg8nIuOLyNcx-Y","9228b2bb":"# Pytorch\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\n\n# Build up the pretrained transformers model\nfrom transformers import ViTFeatureExtractor, ViTModel\n\n# Plot tool\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import classification_report","bb47e50a":"# Set up device and training hyperparameters\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbatch_size = 256\nlatent_dim = 256\nepochs = 1\nlearning_rate = 5e-4\n\nprint(device)","332908f4":"# Add the transformation of image\ndata_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224)),\n    transforms.Normalize((0.5,)*3, (0.5,)*3)\n])\n\n# Path for training and validation dataset\ntraining_path = \"..\/input\/fruits\/fruits-360\/Training\"\nvalidating_path = \"..\/input\/fruits\/fruits-360\/Test\"","a06a1306":"# Create train, val dataset\ntraining_dataset = ImageFolder(training_path,\n                               transform=data_transforms)\nvalidating_dataset = ImageFolder(validating_path,\n                                 transform=data_transforms)\n\n# Create train, val dataloader\ntraining_dataloader = DataLoader(training_dataset,\n                                 batch_size=batch_size,\n                                 shuffle=True)\nval_dataloader = DataLoader(validating_dataset,\n                            batch_size=batch_size,\n                            shuffle=True)","d3c7969b":"image_size = 32\n\n# Get the image from dataloader\nshow_images, show_labels = next(iter(training_dataloader))\nshow_images = show_images[:image_size]\nshow_labels = show_labels[:image_size]\n\nncols = 8\nnrows = int(image_size \/ ncols) + 1\n\n# Set up the figure size\nplt.figure(figsize=(12, 12))\n\n\ndef inverse_normalized(image):\n    # Clamp the value to (0.0, 1.0) for floats\n    image = torch.clamp(input=image * 0.5 + 0.5,\n                        min=0.0,\n                        max=1.0)\n    return image\n\n\nfor idx in range(len(show_images)):\n    # Set up the subplot\n    plt.subplot(nrows, ncols, idx + 1)\n    \n    # Close the axis and plot the image\n    plt.axis(\"off\")\n    # Note that the label is change into integer, we should change it back to category\n    plt.title(training_dataset.classes[show_labels[idx].item()])\n    # Note that the image channel of tensor is: (channel, width, height)\n    # We need to change it to (width, height, channel) which we use permute to realize it\n    plt.imshow(inverse_normalized(show_images[idx].permute(1, 2, 0)))\n\n# Avoid overlapping of different titles\nplt.tight_layout()","644e9f4a":"class CategoryClass(nn.Module):\n    def __init__(self, vit, latent_dim, classes_):\n        super(CategoryClass, self).__init__()\n        \n        self.classes_ = classes_\n        \n        # Set up model architecture\n        self.vit = vit\n        self.fc_1 = nn.Linear(768, latent_dim)\n        self.fc_out = nn.Linear(latent_dim, self.classes_)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, in_data):\n        # Type of output `BaseModelOutputWithPooling`\n        vit_outputs = self.vit(in_data)\n        \n        # Shape of pooler_output: (batch_size, hidden_size)\n        pooler_output = vit_outputs.pooler_output\n        \n        # Pass through the linear layout to predict the class\n        # Shape of output: (batch_size, classes_)\n        outputs = torch.relu(self.fc_1(pooler_output))\n        outputs = self.fc_out(self.dropout(outputs))\n        \n        return outputs","65954c26":"# Set up the pretrained model\nvit = ViTModel.from_pretrained('google\/vit-base-patch16-224')","1b7a97a6":"# Freeze the layers in vit exclude the pooler layers\nfor param in vit.parameters():\n    param.requires_grad = False\n\nvit.pooler.dense.weight.requires_grad = True\nvit.pooler.dense.bias.requires_grad = True","fae30d17":"# Create the complete model\nmodel = CategoryClass(vit,\n                      latent_dim,\n                      len(training_dataset.classes)).to(device)","da6fe27c":"# Set up loss function and optimizers\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), learning_rate)","bfa27990":"# Save the training and validating information\ntraining_loss_history =[]\ntraining_accuracy_history = []\nvalidating_loss_history = []\nvalidating_accuracy_history = []\n\n# We only train one epoch\n#  Therefore, we save the batch information\nbatch_loss_history = []\nbatch_accuracy_history = []\n\nfor epoch in range(epochs):\n    # Set to the train mode\n    model.train()\n    \n    train_epoch_loss = 0.0\n    train_epoch_accuracy = 0.0\n    for idx, (images, labels) in enumerate(training_dataloader):\n        # Move the data to the device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Pass through the model\n        outputs = model(images)\n        \n        # Count the loss and update the parameters\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Record the training information\n        train_epoch_loss += loss.item()\n        predict_class = outputs.argmax(dim=-1)\n        accuracy = torch.sum(predict_class == labels).item() \/ labels.shape[0]\n        train_epoch_accuracy += accuracy\n\n        # Save the batch information\n        batch_loss_history.append(loss.item())\n        batch_accuracy_history.append(accuracy)\n\n        # Print batch information\n        print(f\"Batch {idx + 1} in epoch {epoch + 1}\/{epochs}\")\n        print(f\"Average loss: {loss.item()}\")\n        print(f\"Average accuracy {accuracy}\")\n        \n    # Set to the eval mode\n    model.eval()\n    \n    val_epoch_loss = 0.0\n    val_epoch_accuracy = 0.0\n    \n    with torch.no_grad():\n        for val_images, val_labels in val_dataloader:\n            # Move data to the device\n            val_images = val_images.to(device)\n            val_labels = val_labels.to(device)\n            \n            # Pass through model\n            val_outputs = model(val_images)\n            \n            # Count the loss and accuracy\n            val_epoch_loss += criterion(val_outputs, val_labels)\n            val_predict_class = val_outputs.argmax(dim=-1)\n            val_epoch_accuracy += torch.sum(val_predict_class == val_labels).item() \/ val_labels.shape[0]\n            \n    # Save the epoch information\n    training_loss_history.append(train_epoch_loss \/ len(training_dataloader))\n    training_accuracy_history.append(train_epoch_accuracy \/ len(training_dataloader))\n    validating_loss_history.append(val_epoch_loss \/ len(val_dataloader))\n    validating_accuracy_history.append(val_epoch_accuracy \/ len(val_dataloader))\n    \n    # Print the information\n    print(f\"Epoch {epoch + 1}\")\n    print(f\"Average training loss: {training_loss_history[-1]}, Average validation loss: {validating_loss_history[-1]}\")\n    print(f\"Average training accuracy: {training_accuracy_history[-1]}, Average validation accuracy: {validating_accuracy_history[-1]}\")","c691403c":"# Save the model weight\ntorch.save(model.state_dict(), \"model.pt\")","b6223a24":"# Plot the loss and accuracy\nplt.figure(figsize=(18,6))\n\n# Loss\nplt.subplot(1, 2, 1)\nplt.title(\"Loss for batch\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(batch_loss_history)\n\n# Accuracy\nplt.subplot(1, 2, 2)\nplt.title(\"Accuracy for batch\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Accuracy\")\nplt.plot(batch_accuracy_history)","ca4f8bb4":"prediction = []\ntrue_labels = []\n\n# Create the classification report on test dataset\nwith torch.no_grad():\n    for val_images, val_labels in val_dataloader:\n        # Move data to the device\n        val_images = val_images.to(device)\n        val_labels = val_labels.to(device)\n        \n        # Pass through model\n        val_outputs = model(val_images)\n        \n        # Count the loss and accuracy\n        val_predict_class = val_outputs.argmax(dim=-1)\n\n        prediction.extend([predict_class.item() for predict_class in val_predict_class])\n        true_labels.extend([val_label.item() for val_label in val_labels])","8d6df476":"import pandas as pd\n\n# Output dict for making the report\nreport = classification_report(true_labels, prediction,\n                               output_dict=True,\n                               target_names=training_dataset.classes)\nreport_df = pd.DataFrame(report).transpose()","ea074849":"# Show all columns\npd.set_option(\"display.max_rows\", None)\nreport_df.head(134)","fe13ee67":"## Build up dataset and dataloader","787e7615":"## How our data look like?","1c548a1c":"## Build the model","ec143974":"### Download the trained model\n\nI had put the trained weight on Google Drive. Users can download it by running below two blocks","e7ba6a60":"## Introduction\n\n> This notebook used the [Fruit 360](https:\/\/www.kaggle.com\/moltean\/fruits) dataset.\n\nInstead of using CNN block to train the model, I tried to use the Vision Transformer (ViT) which is based on the paper [Image is worth 16 x 16 words](https:\/\/arxiv.org\/abs\/2010.11929). The model has already been build in the [**transformers**](https:\/\/huggingface.co\/transformers\/model_doc\/vit.html) package.\n\nBelow are some resources to help readers to know more about ViT\n* [Paper](https:\/\/www.kaggle.com\/moltean\/fruits)\n* [Video](https:\/\/youtu.be\/TrdevFK_am4)\n* [Implement ViT from scratch using PyTorch](https:\/\/towardsdatascience.com\/implementing-visualttransformer-in-pytorch-184f9f16f632)"}}