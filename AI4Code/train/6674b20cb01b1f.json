{"cell_type":{"7776bba9":"code","04295dee":"code","0a6bb817":"code","7cfb1ec3":"code","d3103a67":"code","e870ef5e":"code","bd1967fe":"code","4921b41e":"markdown","bc98274c":"markdown","27934820":"markdown","bb3a10b2":"markdown","1a0ca730":"markdown","52bcc237":"markdown","44f9b507":"markdown","52c0de1f":"markdown","04460d34":"markdown"},"source":{"7776bba9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","04295dee":"from __future__ import print_function\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.pipeline import make_pipeline\nprint(__doc__)\n\n# Code source: Tyler Lanigan <tylerlanigan@gmail.com>\n#              Sebastian Raschka <mail@sebastianraschka.com>\n\n# License: BSD 3 clause\n\nRANDOM_STATE = 42\nFIG_SIZE = (10, 7)\n\n\nfeatures, target = load_wine(return_X_y=True)\n\n# Make a train\/test split using 30% test size\nX_train, X_test, y_train, y_test = train_test_split(features, target,\n                                                    test_size=0.30,\n                                                    random_state=RANDOM_STATE)\n\n# Fit to data and predict using pipelined GNB and PCA.\nunscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\nunscaled_clf.fit(X_train, y_train)\npred_test = unscaled_clf.predict(X_test)\n\n# Fit to data and predict using pipelined scaling, GNB and PCA.\nstd_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\nstd_clf.fit(X_train, y_train)\npred_test_std = std_clf.predict(X_test)\n\n# Show prediction accuracies in scaled and unscaled data.\nprint('\\nPrediction accuracy for the normal test dataset with PCA')\nprint('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n\nprint('\\nPrediction accuracy for the standardized test dataset with PCA')\nprint('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))\n\n# Extract PCA from pipeline\npca = unscaled_clf.named_steps['pca']\npca_std = std_clf.named_steps['pca']\n\n# Show first principal components\nprint('\\nPC 1 without scaling:\\n', pca.components_[0])\nprint('\\nPC 1 with scaling:\\n', pca_std.components_[0])\n\n# Scale and use PCA on X_train data for visualization.\nscaler = std_clf.named_steps['standardscaler']\nX_train_std = pca_std.transform(scaler.transform(X_train))\n\n# visualize standardized vs. untouched dataset with PCA performed\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n\n\nfor l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n    ax1.scatter(X_train[y_train == l, 0], X_train[y_train == l, 1],\n                color=c,\n                label='class %s' % l,\n                alpha=0.5,\n                marker=m\n                )\n\nfor l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n    ax2.scatter(X_train_std[y_train == l, 0], X_train_std[y_train == l, 1],\n                color=c,\n                label='class %s' % l,\n                alpha=0.5,\n                marker=m\n                )\n\nax1.set_title('Training dataset after PCA')\nax2.set_title('Standardized training dataset after PCA')\n\nfor ax in (ax1, ax2):\n    ax.set_xlabel('1st principal component')\n    ax.set_ylabel('2nd principal component')\n    ax.legend(loc='upper right')\n    ax.grid()\n\nplt.tight_layout()\n\nplt.show()","0a6bb817":"# Perform the necessary imports\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nimport pandas as pd\n\n# Reading grains data\ngrains = pd.read_csv('..\/input\/seeds-width-vs-length.csv')\n# Assign the 0th column of grains: width\nwidth = grains.iloc[:,0]\n\n# Assign the 1st column of grains: length\nlength = grains.iloc[:,1]\n\n# Scatter plot width vs length\nplt.scatter(width, length)\nplt.xlabel('Width')\nplt.ylabel('Length')\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation\ncorrelation, pvalue = pearsonr(width,length)\n\n# Display the correlation\nprint(correlation)\n","7cfb1ec3":"# Import PCA\nfrom sklearn.decomposition import PCA\n\n# Create PCA instance: model\nmodel = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(grains)\n\n# Assign 0th column of pca_features: xs\nxs = pca_features[:,0]\n\n# Assign 1st column of pca_features: ys\nys = pca_features[:,1]\n\n# Scatter plot xs vs ys\nplt.scatter(xs, ys)\nplt.xlabel('xs')\nplt.ylabel('ys')\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation of xs and ys\ncorrelation, pvalue = pearsonr(xs, ys)\n\n# Display the correlation\nprint(\"Correlation : \",correlation)\n","d3103a67":"# Make a scatter plot of the untransformed points\nplt.scatter(grains.iloc[:,0], grains.iloc[:,1])\n\n# Create a PCA instance: model\nmodel = PCA()\n\n# Fit model to points\nmodel.fit(grains)\n\n# Get the mean of the grain samples: mean\nmean = model.mean_\n\n# Get the first principal component: first_pc\nfirst_pc = model.components_[0,:]\n\n# Plot first_pc as an arrow, starting at mean\nplt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n\n# Keep axes on same scale\nplt.axis('equal')\nplt.xlabel('First Principal Component 1')\nplt.ylabel('First Principal Component 2')\nplt.show()\n","e870ef5e":"# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\n\n\n# Reading grains data\ngrains = pd.read_csv('..\/input\/seeds.csv')\nsamples = grains.iloc[:,1:]\nprint(\"Samples Info \",samples.info())\n# Preprocessing : Changing type of last column from int to float\ns1=samples.iloc[:,-1]\ns1=s1.astype(float)\nsamples.iloc[:,-1] = s1\nprint(\"Samples Info Updated \",samples.info())\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler,pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(samples)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('Variance')\nplt.xticks(features)\nplt.show()\n","bd1967fe":"# Import PCA\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# scaled samples\nscaler = StandardScaler()\nscaled_samples = scaler.fit_transform(samples)\nprint('Scaled Samples \\n',scaled_samples[1:5,:])\n\n# Create a PCA model with 2 components: pca\npca = PCA(n_components=2)\n\n# Fit the PCA instance to the scaled samples\npca.fit(scaled_samples)\n\n# Transform the scaled samples: pca_features\npca_features = pca.transform(scaled_samples)\n\n# Print the shape of pca_features\nprint(\"PCA Features Shape \\n\",pca_features.shape)\n\nprint(\"PCA Features \\n\", pca_features[1:10,:])\n","4921b41e":"### Please Upvote and Provide Feedback\/Comments","bc98274c":"**Analyzing First Principal Component**\n\nThe first principal component of the data is the direction in which the data varies the most. In this exercise, your job is to use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot.\n\nThe array\u00a0grains\u00a0gives the length and width of the grain samples. PyPlot (plt) and\u00a0PCA\u00a0have already been imported for you.\n","27934820":"**De-Correlating the Feature Measurements with PCA**\n\nYou observed in the previous exercise that the width and length measurements of the grain are correlated. \nNow, you will use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.\n","bb3a10b2":"**Conclusion**\n\nHere, I have shared several examples of PCA for dimensionality reduction and improving performance of machine learning algorithms.   While PCA does improve performance in many cases, but it is no silver bullet as pointed out by Andrew Ng, \"Don't assume you need to do PCA. Try your full machine learning algorithm without PCA first. Then use PCA if you find that you need it.\"\nNote : Some of the code examples have been taken from https:\/\/www.datacamp.com\/home\n","1a0ca730":"**Introduction**\n\nPrincipal component analysis(PCA) is considered as one of the most popular technique for linearly independent feature extractiond and dimensionality reduction. We sometimes have machine learning problems in which input features have very high dimensions, which complicates machine learning, increasing processing and reducing accuracy. \nSo, our first task is to reduce high dimensional input feature space, to a lower dimensional space which is more effective in machine learning task. \nSo PCA has several benefits including data compression, improved visualization, increasing performance, simplifying machine learning models etc.\n\nIt is important to note that it is not only possible to reduce dimensionality of input feature space using PCA while retaining most of the variability of the data, but it is also possible to reconstruct the the original data through back projection techniques. \n","52bcc237":"**Correlated Data in Nature**\n\nYou are given an array\u00a0grains\u00a0giving the width and length of samples of grain. You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation.","44f9b507":"**Dimensionsionality Reduction of the Seed Measurements**\n\nIn a previous exercise, you saw that\u00a04\u00a0was a reasonable choice for the \"intrinsic dimension\" of the seed measurements. Now use PCA for dimensionality reduction of the seed measurements, retaining only the 4 most important components.\n","52c0de1f":"**Importance of Feature Scaling**\n\nFeature scaling through standardization (or Z-score normalization) can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n\nWhile many algorithms (such as SVM, K-nearest neighbors, and logistic regression) require features to be normalized, intuitively we can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the \u2018weight\u2019 axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.\nReference : [http:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html]\n","04460d34":"**Feature Selection by Analyzing Variance of the PCA Features**\n\nThe seed dataset is multi-dimensional. But what is its\u00a0intrinsic\u00a0dimension? Make a plot of the variances of the PCA features to find out. As before,\u00a0samples\u00a0is a 2D array, where each row represents a crop. You'll need to standardize the features first.\n"}}