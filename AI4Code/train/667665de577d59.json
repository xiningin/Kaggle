{"cell_type":{"9cfbc49e":"code","be425e16":"code","5df1292c":"code","982cf50f":"code","a363db1e":"code","89cbd15f":"code","724d63d2":"code","29b6e489":"code","458bcc3a":"code","76f18f00":"code","162368b2":"code","9e564180":"code","4dfc6049":"code","df6101d2":"code","7c6b59b2":"code","a2871d02":"code","95d29c59":"code","33319976":"code","fb5effcb":"code","e52f5207":"code","04ff3a00":"code","628c4f75":"code","b928b042":"code","c59fd99d":"code","b369d60f":"code","705d707d":"code","f0ea1327":"code","74bafc5e":"code","01f3f1a6":"markdown","a7d7a03d":"markdown","b8d81a0f":"markdown","f65fce5d":"markdown","d29ca1b2":"markdown","a2a0d6ec":"markdown","c6f2b733":"markdown","475b4f0a":"markdown","ef27eeef":"markdown","e97fd9f0":"markdown"},"source":{"9cfbc49e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler","be425e16":"#Let's load the dataset into the kernel\ndata = load_wine()\ndata.keys()","5df1292c":"#Let's gain some statistical and useful information about our dataset by executing the code below:\nprint(data.DESCR)","982cf50f":"#Let's create a dataframe \nX = pd.DataFrame(data.data)\ny = data.target","a363db1e":"X.columns = data.feature_names\nX.head()","89cbd15f":"X.isnull().sum()","724d63d2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","29b6e489":"sfs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),\n          k_features = 7,\n          forward= True,\n          floating = False,\n          verbose= 2,\n          scoring= 'accuracy',\n          cv = 5,\n          n_jobs= -1\n         ).fit(X_train, y_train)\n\n\n# 8 concurrent workers for my PC\n# you can check for all features!","458bcc3a":"sfs.k_feature_names_","76f18f00":"sfs.k_feature_idx_","162368b2":"sfs.k_score_","9e564180":"pd.DataFrame.from_dict(sfs.get_metric_dict()).T","4dfc6049":"sfs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),\n         k_features = (1, 8),\n          forward= True,\n          floating = False,\n          verbose= 2,\n          scoring= 'accuracy',\n          cv = 5,\n          n_jobs= -1\n         ).fit(X_train, y_train)\n","df6101d2":"sfs.k_score_","7c6b59b2":"sfs.k_feature_names_","a2871d02":"sfs.k_feature_idx_\n# (0, 4, 6, 9, 11) = check out above table for max acc (best combination)","95d29c59":"sfs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),\n         k_features = (1, 8),\n          forward= False,\n          floating = False,\n          verbose= 2,\n          scoring= 'accuracy',\n          cv = 5,\n          n_jobs= -1\n         ).fit(X_train, y_train)","33319976":"sbs = sfs\nsbs.k_score_","fb5effcb":"sbs.k_feature_names_\n\n# check out this accuracy from above warning\n# this 8 features may be change to another feature count","e52f5207":"from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS","04ff3a00":"efs = EFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1),\n         min_features= 4,\n          max_features= 5,\n          scoring='accuracy',\n          cv = None,\n          n_jobs=-1\n         ).fit(X_train, y_train)","628c4f75":"715 + 1287","b928b042":"help(efs)","c59fd99d":"efs.best_score_","b369d60f":"efs.best_feature_names_","705d707d":"efs.best_idx_","f0ea1327":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs","74bafc5e":"plot_sfs(efs.get_metric_dict(), kind='std_dev')\nplt.title('Performance of the EFS algorithm with changing number of features')\nplt.show()","01f3f1a6":"# Feature Combination\n* Gaining the best combination of the features based on their avg-score","a7d7a03d":"### Exhaustive Feature Selection (EFS): the most expensive Feature Selection\n\nIt will start with the subset of minimum features to maximum subset of features.","b8d81a0f":"**Use of mlxtend library in Wrapper Method**\n\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.","f65fce5d":"**1- Forward Step Selection:** In this wrapping method, it selects one best feature every time and finally it combines all the best features for the best accuracy.\n\n**2- Backward Step Selection:** It is reverse process of Forward Step Selection method, intially it takes all the features and remove one by one every time. Finally it left with required number of features for the best accuracy.\n\n**3- Exhaustive Feature Selection:** It is also called as subset selection method and fits the model with each possible combinations of N features.It requires massive computational power and uses test error to evaluate model performance\n\n( y = B0, y = B0 + B1.X1, y = C0 + C1.X2 )\n\n","d29ca1b2":"# Step Forward Selection (SFS) Analysis","a2a0d6ec":"### Step Forward Selection (SFS)","c6f2b733":"* The number of features combination that execute in this method\n* C(13, 4) + C(13, 5) = 715 + 1287","475b4f0a":"Here, we are using `SequentialFeatureSelector()` and passing `Random Forest Classifier` in this we are passing number of estimators, random_state and number of jobs.\n\n`k` number of features are the required number of features.\n\nIn this case, since it is forward step method, forward is equal to `True`.\n\nFor verbose it is for log here we are using 2.\n\nCross validation set,here we are choosing as 4.\n\nNumber of jobs means how many cores we will use, here -1 means use all the available core in this system.","ef27eeef":"### Step Backward Selection (SBS)","e97fd9f0":"for more information please check:\nhttp:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/"}}