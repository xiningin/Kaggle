{"cell_type":{"f685c803":"code","58d45b46":"code","3f982e68":"code","f5a97319":"code","d6efd91e":"code","7348bb2c":"code","16b0e710":"code","0ad55f67":"code","e429c44d":"code","d5d7c6a7":"code","5d8cebb7":"code","1803f7a2":"code","f4eafc37":"code","d31128e4":"code","5cedfa8e":"code","365ff53b":"code","68f7a3e4":"code","14d774d1":"code","b4358b58":"code","cba57316":"code","ad5ab631":"code","0648fe83":"code","d00c2cf9":"code","f36787b9":"code","95001e75":"code","a884cb96":"code","b119f41f":"markdown","86a16cc3":"markdown","68a4b949":"markdown","7cbb3316":"markdown","33a94985":"markdown","2d88fbaa":"markdown","049151a9":"markdown","fe2ef2b2":"markdown","cd560e4d":"markdown","d1c87c16":"markdown","405b5f92":"markdown","2223962d":"markdown","72302ef5":"markdown","b9fefc04":"markdown","cf8c8354":"markdown","a2d13961":"markdown","be6b9d4c":"markdown","81b37839":"markdown","29dd2746":"markdown","2267b045":"markdown","f2514dcc":"markdown"},"source":{"f685c803":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58d45b46":"# Reading the training data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain.info()","3f982e68":"train = train.drop(['Cabin', 'Ticket'], axis=1)\ntrain = train.dropna().reset_index(drop='index')\ntrain.info()","f5a97319":"train.head()","d6efd91e":"train.Embarked.unique()","7348bb2c":"train['Sex'] = train['Sex'].apply(lambda x: 1 if x=='male' else 0)\ntrain['Embarked'] = train['Embarked'].apply(lambda x:-1 if x=='S' else 0 if x=='C' else 1)\ntrain.head()","16b0e710":"sns.heatmap(train.corr(), annot=True, linewidths=0.5)\nplt.show()","0ad55f67":"train = train[['Survived', 'Pclass', 'Sex', 'Embarked']]\ntrain.head()","e429c44d":"# SKlearn imports\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler","d5d7c6a7":"# Splitting up that data\ndata = train.drop(['Survived'], axis=1)\nlabels = train['Survived']\nx_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.3,random_state=42)\n# Model\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n\nmse = mean_squared_error(y_test, model.predict(x_test)) \nrmse = np.sqrt(mse) \n# Results\nprint('Score:',model.score(x_test, y_test))\nprint('Model Intercept:',model.intercept_)\nprint('Model Coef:',model.coef_)\nprint('RMSE:',rmse)","5d8cebb7":"from sklearn.neighbors import NearestCentroid\nfrom sklearn.neighbors import KNeighborsClassifier\n# Splitting up that data\ndata = train.drop(['Survived'], axis=1)\nlabels = train['Survived']\nx_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.4,random_state=42)\n# Model\nclf = KNeighborsClassifier(55) \nclf.fit(x_train, y_train)\n# Results\nknn_weight = clf.score(x_test, y_test)\nprint('Score:',clf.score(x_test, y_test))","1803f7a2":"from sklearn.naive_bayes import GaussianNB\n# Splitting up that data\ndata = train.drop(['Survived'], axis=1)\nlabels = train['Survived']\nx_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.4,random_state=42)\n# Model\ngnb = GaussianNB()\ngnb.fit(x_train, y_train)\nnb_weight = gnb.score(x_test, y_test)\n# Results\nprint('Score:',gnb.score(x_test, y_test))","f4eafc37":"from sklearn.tree import DecisionTreeClassifier\n# Splitting up that data\ndata = train.drop(['Survived'], axis=1)\nlabels = train['Survived']\nx_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.4,random_state=42)\n# Model\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train, y_train)\ndt_weight = dtc.score(x_test, y_test)\n# Results\nprint('Score:',dtc.score(x_test, y_test))","d31128e4":"from tensorflow import keras\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Sequential\nfrom keras.layers import Dense","5cedfa8e":"# Building the model\nmodel = Sequential()\nmodel.add(Input(shape=(3,)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(2, activation='relu'))\nmodel.summary()","365ff53b":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n            metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=1)","68f7a3e4":"\ndef eval(x_t, knn=clf, nb=gnb, dt=dtc):\n    weight_sum = knn_weight+nb_weight+dt_weight\n    knn_pred = knn.predict(x_t)\n    nb_pred = nb.predict(x_t)\n    dt_pred = dt.predict(x_t)\n    pred = []\n    for i in range(len(x_t)):\n        y = knn_pred[i]*knn_weight+nb_pred[i]*nb_weight+dt_pred[i]*dt_weight\n        y = y \/ weight_sum\n        if y > 0.5:\n            pred.append(1)\n        else:\n            pred.append(0)\n    return pred\n    ","14d774d1":"test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest.info()","b4358b58":"test = test[['Pclass', 'Sex', 'Embarked']]\ntest['Sex'] = test['Sex'].apply(lambda x: 1 if x=='male' else 0)\ntest.head()","cba57316":"from sklearn.neighbors import NearestCentroid\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\ntcopy = test.copy()\ntcopy = tcopy.dropna().reset_index(drop='index')\n# Splitting up that data\ndata = tcopy.drop(['Embarked'], axis=1)\nlabels = tcopy['Embarked']\nx_train1, x_test1, y_train1, y_test1 = train_test_split(data, labels, test_size=0.4,random_state=42)\n# Model\nknn_ev = KNeighborsClassifier(55)\ngnb_ev = GaussianNB()\ndt_ev = DecisionTreeClassifier()\nknn_ev.fit(x_train1, y_train1)\ngnb_ev.fit(x_train1, y_train1)\ndt_ev.fit(x_train1, y_train1)\n# Results\nprint('Score:',knn_ev.score(x_test1, y_test1))\nprint('Score:',gnb_ev.score(x_test1, y_test1))\nprint('Score:',dt_ev.score(x_test1, y_test1))","ad5ab631":"test_na = test[test['Embarked'].isna() == True].reset_index(drop='index')\ntest_na.info()","0648fe83":"x_testna = test_na.drop(['Embarked'], axis=1)\nemb_na = knn_ev.predict(x_testna)","d00c2cf9":"embarked = []\ntem = test['Embarked']\nj = 0\nfor i in range(len(test['Sex'])):\n    if tem[i] in ['S', 'C', 'Q']:\n        embarked.append(tem[i])\n    else:\n        embarked.append(emb_na[j])\n        j += 1\n\ntest['Embarked'] = embarked\ntest['Embarked'] = test['Embarked'].apply(lambda x:-1 if x=='S' else 0 if x=='C' else 1)","f36787b9":"test.info()","95001e75":"model_survived = eval(test)\nmodel_survived[:10]","a884cb96":"model_results = pd.DataFrame({'PassengerId':np.arange(10**5,10**5+len(knn_survived),1), 'Survived':model_survived})\nmodel_results.to_csv('tpsapr21_pipe.csv', index=False)","b119f41f":"### Evaluating\nNaive Bayes is as good as KNN. Its score is only 0.6% lower, meaning both models would work just as well with predicting the test data.","86a16cc3":"## Naive Bayes","68a4b949":"All the models are as good as eachother.","7cbb3316":"## Conclusion\nThe best three models were KNN, Decision trees and Naive Bayes. I'll make my model a combination of these three.","33a94985":"### Evaluation\nThe Decision tree scores 76% which is about the same as the other two classification algorithms.","2d88fbaa":"# Test Data","049151a9":"## K-Nearest Neighbours","fe2ef2b2":"Now all the null values have been filled in, its time to create the model","cd560e4d":"The variables that are most strongly correlated with survivability are Pclass (-0.29), Sex (-0.51), and Embarked(0.32)","d1c87c16":"Since there is lots of data, we will just remove any rows with null values. I will also remove the Cabin and Ticket columns because only a third of the Cabin column was filled in and any relevant details from the Ticket should be embeded in the other variables.","405b5f92":"### Evaluation\nNeural networks didn't perform as well as I thought they would. The model only trained on 2% of the data but I doubt training on more data would improve the model by much. ","2223962d":"## Decision Tree","72302ef5":"## Neural Network\nNeural networks are only really worth trying if you have a large amount of data. Fortunately we have 96,000 data points to work with, which should be more than enough. Here's an example from the Keras documentation I will be using as a guide: https:\/\/keras.io\/examples\/structured_data\/structured_data_classification_from_scratch\/#introduction.","b9fefc04":"## Inspection","cf8c8354":"### Evaluation\nThe multilinear model isn't very good. It is only able to correctly predict 30% of the results. This isn't too surprising, multilinear regression tends to be good for making predicitons with continuous variables. Our data is very discrete, hence we shall try other models.","a2d13961":"### Evaluation\nK-Nearest Neighbours was signficantly better than multilinear regression, correctly classifying 76% of the data.  ","be6b9d4c":"## Predictions","81b37839":"## Multilinear Regression","29dd2746":"# Making Predictions","2267b045":"There are 277 null values in the Embarked column. To fill these in I will make a new classifier that predicts where they embarked based on the other two variables","f2514dcc":"I will change the Sex and embarked columns to numbers so they can evaluated in the heatmap below."}}