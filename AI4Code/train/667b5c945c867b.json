{"cell_type":{"607f3340":"code","e074e9de":"code","6ca86062":"code","d6c118e6":"code","cd65d136":"code","c18b9c51":"code","ed4d14fa":"code","8acb1d53":"code","cfc4026f":"code","fadb277a":"code","46ada3c4":"code","ab3a9b11":"code","85c865c0":"code","ff7bca11":"code","e332e815":"code","d2dfb2e3":"code","0b9567ad":"code","9708dada":"code","f8b59b2f":"code","b76ce1a7":"code","e4ca2044":"code","acb41381":"code","abdb0846":"code","6f42a3f0":"code","e2089cd4":"code","921d9081":"code","b89ac7d7":"code","40dc91fa":"code","c74c204c":"code","d1151d7d":"code","791279a8":"code","62d2135a":"code","546ca4f8":"code","35516e29":"code","d101f2af":"code","5023c4f5":"code","6a480d31":"code","e4cb2532":"code","d276e4a8":"code","09e99780":"code","d0f293ec":"code","8faa2986":"code","eed8b804":"code","14e613b2":"code","015c0024":"code","937e133f":"code","10b6c349":"code","f28e1c0e":"code","9f9d1883":"code","a299ed30":"code","f4c5f4c3":"code","df5c0c44":"code","6ac45c47":"code","04d08486":"code","5352476e":"code","a87f4720":"code","4746d061":"code","db7e8a30":"code","2a1a418d":"code","8578e831":"code","bac0eeb3":"code","e00920be":"code","0859c518":"code","11d7977f":"code","4cad05ce":"code","343ec01a":"code","8cad6fc5":"code","ac2a0199":"code","f7bd9cff":"code","e3576252":"code","52c2b357":"code","14945dab":"code","f1f44373":"code","17531bd9":"code","8a35612b":"code","d76dcfa2":"code","87ecde35":"code","2f2ca22d":"code","3ddad1f7":"code","f52ba158":"code","b4ea7981":"code","7f3e1857":"code","83051935":"code","eeac6a03":"code","acee28df":"code","4a5d9261":"code","bd7986b0":"code","969eb825":"code","575f0018":"code","e57724d7":"code","931e1d89":"code","2707f59d":"code","de83815a":"code","d13908c6":"code","729ab31d":"code","8cff407e":"markdown","1df83837":"markdown","4f68a7f3":"markdown","40767623":"markdown","b0973673":"markdown","393734a5":"markdown","453afb77":"markdown","b99965c6":"markdown","3dcbccc8":"markdown","f290dda3":"markdown","3fa4d0c1":"markdown","6a7cf05e":"markdown","e64e4412":"markdown","da5122b4":"markdown","3bb85c54":"markdown","76f86e4f":"markdown","14c793c2":"markdown","74c1c32b":"markdown","37058d4d":"markdown","ee37ab81":"markdown","0dd89fb1":"markdown","ad148a12":"markdown","464868ae":"markdown","624d1e35":"markdown","ec25b4d9":"markdown","3d7933df":"markdown","fdd462fb":"markdown","c71d519d":"markdown"},"source":{"607f3340":"# for data manipulation and analysis\nimport pandas as pd\nimport numpy as np\n\n# for plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits import mplot3d\nsns.set_style('darkgrid')\n\n# Silhouette analysis\nfrom sklearn.metrics import silhouette_score\n\n# To perform KMeans clustering \nfrom sklearn.cluster import KMeans\n\n# for scaling\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","e074e9de":"pip install openpyxl","6ca86062":"pip install xlrd","d6c118e6":"# loading the dataset\ndf = pd.read_excel('\/kaggle\/input\/online-retail-data-set-from-uci-ml-repo\/Online Retail.xlsx')\n\n# looking at top 5 rows\ndf.head()","cd65d136":"#pip install openpyxl","c18b9c51":"# checking the number of rows and columns\ndf.shape","ed4d14fa":"# looking at the overall picture\ndf.info()","8acb1d53":"# checking the number of missing values in each column\ndf.isnull().sum()","cfc4026f":"# count of duplicated rows in the data\ndf.duplicated().sum()","fadb277a":"# removing the duplicate rows\ndf = df[~df.duplicated()]\ndf.shape","46ada3c4":"# these are the transactions that have negative quantity which indicates returned or cancelled orders\ndf[df['InvoiceNo'].str.startswith('C')==True]","ab3a9b11":"# removing all the invoice number who starts with 'C' as they are returned orders\ndf = df[df['InvoiceNo'].str.startswith('C')!=True]\ndf.shape","85c865c0":"# checking the number of unique transactions\n# though there are more than 5 lakh entries but the number of transaction happened is 21892\ndf.InvoiceNo.nunique()","ff7bca11":"# checking the unique stock ids in the data or number of unqiue item sold by retailer\ndf.StockCode.nunique()","e332e815":"# top 10 stock ids that sold the most\ndf.StockCode.value_counts().head(10)","d2dfb2e3":"# looking at the distribution of the quantity\n# we seen that there is negative value which might indicate return orders\ndf.Quantity.describe()","0b9567ad":"# looking at the data where quantity is negative and possible explanation is these are return orders or cancelled order\ndf[df['Quantity']<0]","9708dada":"# keeping only those transactions that have successfully ordered\ndf = df[df['Quantity']>=0]\ndf.shape","f8b59b2f":"print('The minimum date is:',df.InvoiceDate.min())\nprint('The maximum date is:',df.InvoiceDate.max())","b76ce1a7":"# checking the distribution of unit price\ndf.UnitPrice.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","e4ca2044":"# we see that more than 90% have country as UK which is obvious as the retailer is UK based\ndf.Country.value_counts(normalize=True)","acb41381":"# putting UK as one country and combine rest countries into one category\ndf['Country'] = df['Country'].apply(lambda x:'United Kingdom' if x=='United Kingdom' else 'Others')\ndf.Country.value_counts(normalize=True)","abdb0846":"# checking the number of unique item list\ndf.Description.nunique()","6f42a3f0":"# top 10 item sold\ndf.Description.value_counts().head(10)","e2089cd4":"# there are cases where the descriptions contains some code\/name which are not directly refers to sales\n# checking the data where description = ? and it is noted that customerid is NaN and unit price is 0\ndf[df['Description'].str.startswith('?')==True]","921d9081":"# removing all the above entries\ndf = df[df['Description'].str.startswith('?')!=True]\ndf.shape","b89ac7d7":"# checking the data where description = * and it is noted that customerid is NaN\ndf[df['Description'].str.startswith('*')==True]","40dc91fa":"# replacing with appropriate name\ndf['Description'] = df['Description'].replace(('*Boombox Ipod Classic','*USB Office Mirror Ball'),\n                                             ('BOOMBOX IPOD CLASSIC','USB OFFICE MIRROR BALL'))","c74c204c":"# Description have actual entries in uppercase words and those who don't have are some of the noises in the dataset\ndf[df['Description'].str.islower()==True]['Description'].value_counts()","d1151d7d":"# removing all the above noises\ndf = df[df['Description'].str.islower()!=True]\ndf.shape","791279a8":"# Description have actual entries in uppercase words and those who don't have are some of the noises in the dataset\ndf[df['Description'].str.istitle()==True]['Description'].value_counts()","62d2135a":"# removing all the above listed noises\ndf = df[df['Description'].str.istitle()!=True]\ndf.shape","546ca4f8":"df['Description'] = df['Description'].str.strip()","35516e29":"# count of unique customer\ndf.CustomerID.nunique()","d101f2af":"# checking where customer id is null\ndf[df.CustomerID.isnull()]","5023c4f5":"# removing entries where customer id is null\ndf = df[~df.CustomerID.isnull()]\ndf.shape","6a480d31":"df.info()","e4cb2532":"df.isnull().sum()","d276e4a8":"# checking random 5 rows from data\ndf.sample(5)","09e99780":"# creating some columns for exploratory\n\ndf['Amount'] = df['Quantity']*df['UnitPrice']\ndf['year'] = df['InvoiceDate'].dt.year\ndf['month'] = df['InvoiceDate'].dt.month\ndf['day'] = df['InvoiceDate'].dt.day\ndf['hour'] = df['InvoiceDate'].dt.hour\ndf['day_of_week'] = df['InvoiceDate'].dt.dayofweek","d0f293ec":"df.head()","8faa2986":"column = ['InvoiceNo','Amount']\n\nplt.figure(figsize=(15,5))\nfor i,j in enumerate(column):\n    plt.subplot(1,2,i+1)\n    sns.barplot(x = df[df['Country']=='United Kingdom'].groupby('Description')[j].nunique().sort_values(ascending=False).head(10).values,\n                y = df[df['Country']=='United Kingdom'].groupby('Description')[j].nunique().sort_values(ascending=False).head(10).index,\n                color='yellow')\n    plt.ylabel('')\n    if i==0:\n        plt.xlabel('Sum of quantity')\n        plt.title('Top 10 products purchased by customers in UK',size=15)\n    else:\n        plt.xlabel('Total Sales')\n        plt.title('Top 10 products with most sales in UK',size=15)\n        \nplt.tight_layout()\nplt.show()","eed8b804":"column = ['Others','United Kingdom']\n\nplt.figure(figsize=(15,5))\nfor i,j in enumerate(column):\n    plt.subplot(1,2,i+1)\n    sns.barplot(x = df[df['Country']==j].groupby('Description')['UnitPrice'].mean().sort_values(ascending=False).head(10).values,\n                y = df[df['Country']==j].groupby('Description')['UnitPrice'].mean().sort_values(ascending=False).head(10).index,\n                color='yellow')\n    plt.ylabel('')\n    if i==0:\n        plt.xlabel('Unit Price')\n        plt.title('Top 10 high value products outside UK',size=15)\n    else:\n        plt.xlabel('Unit Price')\n        plt.title('Top 10 high value products in UK',size=15)\n        \nplt.tight_layout()\nplt.show()","14e613b2":"# Looking the distribution of column Quantity\nplt.figure(figsize=(10,7))\n\nskewness = round(df.Quantity.skew(),2)\nkurtosis = round(df.Quantity.kurtosis(),2)\nmean = round(np.mean(df.Quantity),0)\nmedian = np.median(df.Quantity)\n\nplt.subplot(2,2,1)\nsns.boxplot(y=df.Quantity)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.boxplot(y=df[df.Quantity<5000]['Quantity'])\nplt.title('Distribution when Quantity<5000')\n\nplt.subplot(2,2,3)\nsns.boxplot(y=df[df.Quantity<200]['Quantity'])\nplt.title('Distribution when Quantity<200')\n\nplt.subplot(2,2,4)\nsns.boxplot(y=df[df.Quantity<50]['Quantity'])\nplt.title('Distribution when Quantity<50')\n\nplt.show()","015c0024":"# removing the expectional case where quantity > 70000\ndf = df[df['Quantity']<70000]","937e133f":"# Looking the distribution of column Unit Price\nplt.figure(figsize=(10,7))\n\nskewness = round(df.UnitPrice.skew(),2)\nkurtosis = round(df.UnitPrice.kurtosis(),2)\nmean = round(np.mean(df.UnitPrice),0)\nmedian = np.median(df.UnitPrice)\n\nplt.subplot(2,2,1)\nsns.boxplot(y=df.UnitPrice)\nplt.title('Boxplot\\n Mean:{}\\n Median:{}\\n Skewness:{}\\n Kurtosis:{}'.format(mean,median,skewness,kurtosis))\n\nplt.subplot(2,2,2)\nsns.boxplot(y=df[df.UnitPrice<300]['UnitPrice'])\nplt.title('Distribution when Unit Price<300')\n\nplt.subplot(2,2,3)\nsns.boxplot(y=df[df.UnitPrice<50]['UnitPrice'])\nplt.title('Distribution when Unit Price<50')\n\nplt.subplot(2,2,4)\nsns.boxplot(y=df[df.UnitPrice<10]['UnitPrice'])\nplt.title('Distribution when Unit Price<10')\n\nplt.show()","10b6c349":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.barplot(y = df[df['Country']=='United Kingdom'].groupby('CustomerID')['Amount'].sum().sort_values(ascending=False).head(10).values,\n            x = df[df['Country']=='United Kingdom'].groupby('CustomerID')['Amount'].sum().sort_values(ascending=False).head(10).index, \n            color='green')\nplt.ylabel('Sales')\nplt.xlabel('Customer IDs')\nplt.xticks(rotation=45)\nplt.title('Top 10 customers in terms of sales in UK',size=15)\n\nplt.subplot(1,2,2)\nsns.barplot(y = df[df['Country']=='United Kingdom'].groupby('CustomerID')['InvoiceNo'].nunique().sort_values(ascending=False).head(10).values,\n            x = df[df['Country']=='United Kingdom'].groupby('CustomerID')['InvoiceNo'].nunique().sort_values(ascending=False).head(10).index, \n            color='green')\nplt.ylabel('Number of visits')\nplt.xlabel('Customer IDs')\nplt.xticks(rotation=45)\nplt.title('Top 10 customers in terms of frequency in UK',size=15)\n\nplt.show()","f28e1c0e":"plt.figure(figsize=(12,5))\ndf[df['Country']=='United Kingdom'].groupby(['year','month'])['Amount'].sum().plot(kind='line',label='UK',color='blue')\ndf[df['Country']=='Others'].groupby(['year','month'])['Amount'].sum().plot(kind='line',label='Other',color='grey')\nplt.xlabel('Year-Month',size=12)\nplt.ylabel('Total Sales', size=12)\nplt.title('Sales in each month for an year', size=15)\nplt.legend(fontsize=12)\nplt.show()","9f9d1883":"plt.figure(figsize=(12,5))\ndf[df['Country']=='United Kingdom'].groupby(['day'])['Amount'].sum().plot(kind='line',label='UK',color='blue')\ndf[df['Country']=='Others'].groupby(['day'])['Amount'].sum().plot(kind='line',label='Other',color='grey')\nplt.xlabel('Day',size=12)\nplt.ylabel('Total Sales', size=12)\nplt.title('Sales on each day of a month', size=15)\nplt.legend(fontsize=12)\nplt.show()","a299ed30":"plt.figure(figsize=(12,5))\ndf[df['Country']=='United Kingdom'].groupby(['hour'])['Amount'].sum().plot(kind='line',label='UK',color='blue')\ndf[df['Country']=='Others'].groupby(['hour'])['Amount'].sum().plot(kind='line',label='Other',color='grey')\nplt.xlabel('Hours',size=12)\nplt.ylabel('Total Sales', size=12)\nplt.title('Sales in each hour in a day', size=15)\nplt.legend(fontsize=12)\nplt.show()","f4c5f4c3":"# copying the data into new df\ndf_cohort = df.copy()\n# select only limited columns\ndf_cohort = df_cohort.iloc[:,:9]\ndf_cohort.head()","df5c0c44":"# creating the first variable 'Invoice Month'\n# extracting only year-month from Invoice Date and day will be 1 automatically\n\ndf_cohort['InvoiceMonth'] = df_cohort['InvoiceDate'].dt.strftime('%Y-%m')\n# converting the variable to datetime format\ndf_cohort['InvoiceMonth'] = pd.to_datetime(df_cohort['InvoiceMonth'])","6ac45c47":"# creating the second variable 'Cohort Month'\n# getting the first time purchase date for each customer\n\ndf_cohort['CohortMonth'] = df_cohort.groupby('CustomerID')['InvoiceMonth'].transform('min')\n# converting the variable to datetime format\ndf_cohort['CohortMonth'] = pd.to_datetime(df_cohort['CohortMonth'])","04d08486":"df_cohort.info()","5352476e":"# creating the third variable 'Cohort Period'\n# for this we create a function which calculates the number of month between their first purchase date and Invoice date\n\ndef diff_month(d1, d2):\n    return((d1.dt.year - d2.dt.year) * 12 + d1.dt.month - d2.dt.month)\n\ndf_cohort['CohortPeriod'] = diff_month(df_cohort['InvoiceMonth'], df_cohort['CohortMonth'])","a87f4720":"df_cohort.sample(5)","4746d061":"customer_cohort = df_cohort.pivot_table(index='CohortMonth', columns='CohortPeriod', values='CustomerID', aggfunc='nunique')\ncustomer_cohort","db7e8a30":"# Retention table\n\ncohort_size = customer_cohort.iloc[:,0]\nretention = customer_cohort.divide(cohort_size,axis=0) #axis=0 to ensure the divide along the row axis\nretention.index = pd.to_datetime(retention.index).date\nretention.round(3) * 100 #to show the number as percentage","2a1a418d":"#Build the heatmap or pictorial representation of above table\n\nplt.figure(figsize=(15, 8))\nplt.title('Retention Rates(in %) over one year period', size=15)\nsns.heatmap(data=retention, annot = True, fmt = '.0%', cmap=\"summer_r\")\nplt.show()","8578e831":"amount_cohort = df_cohort.pivot_table(index='CohortMonth', columns='CohortPeriod', values='Amount', aggfunc='mean').round(2)\namount_cohort","bac0eeb3":"#Build the heatmap or pictorial representation of above table\n\namount_cohort.index = pd.to_datetime(amount_cohort.index).date\nplt.figure(figsize=(15, 8))\nplt.title('Average Spending Over Time', size=15)\nsns.heatmap(data = amount_cohort, annot = True, cmap=\"summer_r\")\nplt.show()","e00920be":"# copying the data in other df\ndf_rfm = df.copy()\n# keeping only desired columns\ndf_rfm = df_rfm.iloc[:,:9]\ndf_rfm.head()","0859c518":"# extracting the RECENCY\n\nrecency = pd.DataFrame(df_rfm.groupby('CustomerID')['InvoiceDate'].max().reset_index())\nrecency['InvoiceDate'] = pd.to_datetime(recency['InvoiceDate']).dt.date\nrecency['MaxDate'] = recency['InvoiceDate'].max()\nrecency['recency'] = (recency['MaxDate'] - recency['InvoiceDate']).dt.days + 1\nrecency = recency[['CustomerID','recency']]\nrecency.head()","11d7977f":"# extracting the FREQUENCY\n\nfrequency = pd.DataFrame(df_rfm.groupby('CustomerID')['InvoiceNo'].nunique().reset_index())\nfrequency.columns = ['fCustomerID','frequency']\nfrequency.head()","4cad05ce":"# extracting the MONETARY\n\nmonetary = pd.DataFrame(df_rfm.groupby('CustomerID')['Amount'].sum().reset_index())\nmonetary.columns = ['mCustomerID','monetary']\nmonetary.head()","343ec01a":"# combining the three into one table\n\nrfm = pd.concat([recency,frequency,monetary], axis=1)\nrfm.drop(['fCustomerID','mCustomerID'], axis=1, inplace=True)\nrfm.head(10)","8cad6fc5":"# extracting SPEED OF VISIT\n\ncustomer_list = list(df_rfm.CustomerID.unique())\nc = []\nv = []\nfor ids in customer_list:\n    sov = df_rfm[df_rfm['CustomerID']==ids].groupby('InvoiceDate')['InvoiceNo'].count().reset_index()\n    if sov.shape[0]>3:\n        sov['InvoiceDate1'] = sov['InvoiceDate'].shift(1)\n        sov['Difference'] = (sov['InvoiceDate']-sov['InvoiceDate1']).dt.days\n        mean_days = round(sov.Difference.mean(),0)\n        c.append(ids)\n        v.append(mean_days)\n    else:\n        c.append(ids)\n        v.append(0)\nspeed_of_visit = pd.DataFrame()\nspeed_of_visit['sCustomerID'] = c\nspeed_of_visit['sov'] = v\nspeed_of_visit = speed_of_visit.sort_values('sCustomerID').reset_index(drop=True)\nspeed_of_visit.head()","ac2a0199":"# checking the overall highlights. The number of distinct customers are 4334\nrfm.info()","f7bd9cff":"# checking the summary\nrfm.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","e3576252":"# assigning the numbers to RFM values. The better the RFM value higher the number\n# note that this process is reverse for R score as lower the value the better it is\n\nrfm['recency_score'] = pd.cut(rfm['recency'], bins=[0,18,51,143,264,375], labels=[5,4,3,2,1])\nrfm['recency_score'] = rfm['recency_score'].astype('int')\nrfm['frequency_score'] = pd.cut(rfm['frequency'], bins=[0,1,2,5,9,210], labels=[1,2,3,4,5])\nrfm['frequency_score'] = rfm['frequency_score'].astype('int')\nrfm['monetary_score'] = pd.cut(rfm['monetary'], bins=[-1,306,667,1650,3614,290000], labels=[1,2,3,4,5])\nrfm['monetary_score'] = rfm['monetary_score'].astype('int')","52c2b357":"rfm.info()","14945dab":"# summing the R,F,M score to make a one single column that has value range from 3-15\n\ndef score_rfm(x) : return (x['recency_score']) + (x['frequency_score']) + (x['monetary_score'])\nrfm['score'] = rfm.apply(score_rfm,axis=1 )\nrfm.head()","f1f44373":"rfm.score.describe(percentiles=[0.25,0.5,0.75,0.9,0.95,0.99])","17531bd9":"# assigning the customers into one of the category Bad, Bronze, Silver, Gold and Platinum based upon the score they get\n# we make cuts using percentiles. It can be done in many other ways\n\nrfm['customer_type'] = pd.cut(rfm['score'], bins=[0,6,8,11,13,16], labels=['Bad','Bronze','Silver','Gold','Platinum'])\nrfm.head()","8a35612b":"round(rfm.customer_type.value_counts(normalize=True)*100,0)","d76dcfa2":"# looking the RFM value for each of the category\nrfm.groupby('customer_type')['recency','frequency','monetary'].mean().round(0)","87ecde35":"column = ['recency','frequency','monetary']\nplt.figure(figsize=(15,4))\nfor i,j in enumerate(column):\n    plt.subplot(1,3,i+1)\n    rfm.groupby('customer_type')[j].mean().round(0).plot(kind='bar', color='pink')\n    plt.title('What is the {} of each customer type'.format(j), size=13)\n    plt.xlabel('')\n    plt.xticks(rotation=45)\n\nplt.show()","2f2ca22d":"# copying the data into new variable\ndf_kmeans = rfm.copy()\n# taking only relevant columns\ndf_kmeans = df_kmeans.iloc[:,:4]\ndf_kmeans.head()","3ddad1f7":"plt.figure(figsize=(15,5))\nplt.subplot(1,3,1)\nplt.scatter(df_kmeans.recency, df_kmeans.frequency, color='grey', alpha=0.3)\nplt.title('Recency vs Frequency', size=15)\nplt.subplot(1,3,2)\nplt.scatter(df_kmeans.monetary, df_kmeans.frequency, color='grey', alpha=0.3)\nplt.title('Monetary vs Frequency', size=15)\nplt.subplot(1,3,3)\nplt.scatter(df_kmeans.recency, df_kmeans.monetary, color='grey', alpha=0.3)\nplt.title('Recency vs Monetary', size=15)\nplt.show()","f52ba158":"# checking the distribution of the variables\n\ncolumn = ['recency','frequency','monetary']\nplt.figure(figsize=(15,5))\nfor i,j in enumerate(column):\n    plt.subplot(1,3,i+1)\n    sns.boxplot(df_kmeans[j], color='skyblue')\n    plt.xlabel('')\n    plt.title('{}'.format(j.upper()), size=13)\nplt.show()","b4ea7981":"# Removing outliers for Monetary\nQ1 = df_kmeans.monetary.quantile(0.05)\nQ3 = df_kmeans.monetary.quantile(0.95)\nIQR = Q3 - Q1\ndf_kmeans = df_kmeans[(df_kmeans.monetary >= Q1 - 1.5*IQR) & (df_kmeans.monetary <= Q3 + 1.5*IQR)]\n\n# Removing outliers for Recency\nQ1 = df_kmeans.recency.quantile(0.05)\nQ3 = df_kmeans.recency.quantile(0.95)\nIQR = Q3 - Q1\ndf_kmeans = df_kmeans[(df_kmeans.recency >= Q1 - 1.5*IQR) & (df_kmeans.recency <= Q3 + 1.5*IQR)]\n\n# Removing outliers for Frequency\nQ1 = df_kmeans.frequency.quantile(0.05)\nQ3 = df_kmeans.frequency.quantile(0.95)\nIQR = Q3 - Q1\ndf_kmeans = df_kmeans[(df_kmeans.frequency >= Q1 - 1.5*IQR) & (df_kmeans.frequency <= Q3 + 1.5*IQR)]","7f3e1857":"# resetting the index\ndf_kmeans = df_kmeans.reset_index(drop=True)\ndf_kmeans.info()","83051935":"# looking at random 5 rows\ndf_kmeans.sample(5)","eeac6a03":"# removing customer id as it will not used in making cluster\ndf_kmeans = df_kmeans.iloc[:,1:]\n\n# scaling the variables and store it in different df\nstandard_scaler = StandardScaler()\ndf_kmeans_norm = standard_scaler.fit_transform(df_kmeans)\n\n# converting it into dataframe\ndf_kmeans_norm = pd.DataFrame(df_kmeans_norm)\ndf_kmeans_norm.columns = ['recency','frequency','monetary']\ndf_kmeans_norm.head()","acee28df":"# Kmeans with K=5\n\nmodel_clus5 = KMeans(n_clusters = 5)\nmodel_clus5.fit(df_kmeans_norm)","4a5d9261":"# checking the labels\nmodel_clus5.labels_","bd7986b0":"df_kmeans['clusters'] = model_clus5.labels_\ndf_kmeans.head()","969eb825":"df_kmeans.groupby('clusters').mean().round(0)","575f0018":"# Elbow-curve\/SSD\n\nssd = []\nfor num_clusters in list(range(1,11)):\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(df_kmeans_norm)\n    ssd.append(model_clus.inertia_)\n    \n# plot the SSDs for each n_clusters\nplt.figure(figsize=(10,5))\nplt.plot(np.arange(1,11,1), ssd)\nplt.xlabel('Number of cluster', size=12)\nplt.ylabel('Sum of Square Distance(SSD)', size=12)\nplt.title('Elbow Curve for deciding K', size=15)\nplt.show()","e57724d7":"# Silhouette analysis\n\nfor num_clusters in list(range(2,11)):\n    # intialise kmeans\n    model_clus = KMeans(n_clusters = num_clusters, max_iter=50)\n    model_clus.fit(df_kmeans_norm)\n    \n    cluster_labels = model_clus.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_kmeans_norm, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","931e1d89":"# Kmeans with K=3\nmodel_clus3 = KMeans(n_clusters = 3)\nmodel_clus3.fit(df_kmeans_norm)","2707f59d":"df_kmeans['clusters'] = model_clus3.labels_\ndf_kmeans.head()","de83815a":"df_kmeans.groupby('clusters').mean().round(0)","d13908c6":"column = ['recency','frequency','monetary']\nplt.figure(figsize=(15,4))\nfor i,j in enumerate(column):\n    plt.subplot(1,3,i+1)\n    sns.boxplot(y=df_kmeans[j], x=df_kmeans['clusters'], palette='spring')\n    plt.title('{} wrt clusters'.format(j.upper()), size=13)\n    plt.ylabel('')\n    plt.xlabel('')\n\nplt.show()","729ab31d":"# Creating figure\nfig = plt.figure(figsize = (8, 5))\nax = plt.axes(projection =\"3d\")\n\n# Creating plot\nax.scatter3D(df_kmeans.recency, df_kmeans.frequency, df_kmeans.monetary, c=df_kmeans.clusters, cmap='Accent')\nax.set_xlabel('Recency')\nax.set_ylabel('Frequency')\nax.set_zlabel('Monetary')\nplt.title('RFM in 3D with Clusters', size=15)\nax.set(facecolor='white')\nplt.show()","8cff407e":"## EDA","1df83837":"## Basic Cleaning","4f68a7f3":"##### Invoice Date","40767623":"##### For cohort analysis, there are a few labels that we have to create:\n- `Invoice Month`: A string representation of the year and month of a single transaction\/invoice.\n- `Cohort Month`:\u200aA string representation of the the year and month of a customer\u2019s first purchase. This label is common across all invoices for a particular customer.\n- `Cohort period`:\u200aA integer representation a customer\u2019s stage in its \u201clifetime\u201d. The number represents the number of months passed since the first purchase.","b0973673":"##### Observations:\n- We see that around 9% of customers are in platinum category and these are the customers who score is best in all the three RFM. Combining with the gold 19% customers are those who are genuine and honest with the business.\n- Silver category are those where the business can target to convert them into gold category by rolling out offers and new strategies for them.\n- Bad category are those who are less concerned for the business and does not put much efforts to bring them back.","393734a5":"---\n## III. k-Means Clustering","453afb77":"## I. Cohort Analysis\n![image.png](attachment:image.png)\nAn analytical techniques that focuses on analyzing the behavior of a group of users\/customers over time, thereby uncovering insights about the experiences of those customers, and what companies can do to better those experiences.\n\n##### Types of cohorts:\n1. `Time Cohorts` are customers who signed up for a product or service during a particular time frame. Analyzing these cohorts shows the customers\u2019 behavior depending on the time they started using the company\u2019s products or services. The time may be monthly or quarterly even daily.\n2. `Behavior cohorts` are customers who purchased a product or subscribed to a service in the past. It groups customers by the type of product or service they signed up. Customers who signed up for basic level services might have different needs than those who signed up for advanced services. Understaning the needs of the various cohorts can help a company design custom-made services or products for particular segments.\n3. `Size cohorts` refer to the various sizes of customers who purchase company\u2019s products or services. This categorization can be based on the amount of spending in some periodic time after acquisition or the product type that the customer spent most of their order amount in some period of time","b99965c6":"##### Observations:\n- The above table show `retention` and `acquistion` of customers.\n- Vertically i.e. the first column '0' tells how many new customers the business acquired in a particular month. ex: 884 is the number of customers business acquired in Dec'2010, 415 is the number of customers(different from previous month) business acquired in Jan'2011, and so on. \n- Horizontally i.e the first row tells the number of customers who is continuing to be part of business since their first purchase i.e. Dec'2010. ex: 323 is the number of customers out of 884 that continue to purchase one month after their first purchase, 286 is the number of customers that continue to purchase two months after their first purchase, and so on.","3dcbccc8":"##### Unit Price","f290dda3":"#### Finding the Optimal Number of Clusters\n##### Elbow Curve to get the right number of Clusters\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.","3fa4d0c1":"- `Recency`: How much time has elapsed since a customer\u2019s last activity or transaction with the brand? Activity is usually a purchase, although variations are sometimes used, e.g., the last visit to a website or use of a mobile app. In most cases, the more recently a customer has interacted or transacted with a brand, the more likely that customer will be responsive to communications from the brand.\n- `Frequency`: How often has a customer transacted or interacted with the brand during a particular period of time? Clearly, customers with frequent activities are more engaged, and probably more loyal, than customers who rarely do so. And one-time-only customers are in a class of their own.\n- `Monetary`: Also referred to as \u201cmonetary value,\u201d this factor reflects how much a customer has spent with the brand during a particular period of time. Big spenders should usually be treated differently than customers who spend little. Looking at monetary divided by frequency indicates the average purchase amount \u2013 an important secondary factor to consider when segmenting customers.","6a7cf05e":"##### Observations:\n- In the above 3D graph, I put all the three variable into 3 axis and added the cluster variable to differentiate the points.\n- Grey points is the group of customers whose Recency is high, Frequency is low and Monetary value is also low.\n- Green points are the group of customers whose Recency is low, Frequency is better than grey ones and Monetary is good.\n- Blue points are the group of customers whose Recency is low(that is good), Frequency is better than the other two and Monetary is high.","e64e4412":"## What's in this Notebook\nThe data in this notebook is of *Online Retail of some UK-based*. The data is at transactional level. Here I covered some customer segmentation technique - \n1. Cohort Analysis\n2. RFM Analysis\n3. k-Means Clustering\n\nAlso data pre-processing and cleaning is done first and then all the above discussed technqiue is implemented.<br>\nAll the necessary steps and explaination is provided with commenting on the code as well.<br>\n`If you like this notebook UPVOTE it and for any reviews and doubts please leave a comment`.","da5122b4":"##### Observation:\n- The above table shows the average amount spent by the group of customers over the period of time.\n- ex: For the group of customers of Jan'2011 they initially spent 19.79 but after one month they spent 24.47 higher than the previous, then they spend 20.98 after two months and so on.","3bb85c54":"- From the elbow curve we observe the elbow at cluster 3 and cluster 4.\n- Also from Silhouette analysis we see the value is better when number of cluster will be 3 rather than 4.\n- **So we now categorize the data into 3 clusters and check their RFM values and its distribution.**","76f86e4f":"### Have you ever thought ??\nWe have extracted the three pillars of RFM technique, but have you ever think is there any other technique beside RFM or any alternative technique. Have you ever think of the fourth pillar that can be added in RFM. Do you think there can be any other variable beside RFM?\n\nNo, that is the obvious answer. Most of us accept RFM as what it is. Give a hard thought what could be the possible fourth pillar\/dimension that can be added to RFM which maybe useful to segment the customers better.\n\n<font color='blue'>Here, I created a new variable **Speed of Visit** which basically tells in how many days, on an average, the customer revisit. Example: Suppose a customer visit a store 10 times in a year. Its frequency is 10. He\/She buys some items worth Rs3000 in its 10 visits. The monetary value is 3000. Now with 10 visits we have 10 dates. What we do is subtract the date from the previous date to get after how many days he\/she visit again. We get 9 values from 10 dates. We took the mean of those 9 values and that would be our speed of visit. Note that I haven't used this in segmenting the customer.","14c793c2":"#### Silhouette Analysis\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of<br>\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n- The value of the silhouette score range lies between -1 to 1. \n- A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n- A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","74c1c32b":"##### Country","37058d4d":"##### Stock Code","ee37ab81":"##### Description","0dd89fb1":"##### InvoiceNo","ad148a12":"##### Customer ID","464868ae":"##### Quantity","624d1e35":"---\n## II. RFM Analysis\n![recency%20frequency%20monetary%20value.png](attachment:recency%20frequency%20monetary%20value.png)","ec25b4d9":"K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.\n\nThe algorithm works as follows:\n- First we initialize k points, called means, randomly.\n- We categorize each item to its closest mean and we update the mean\u2019s coordinates, which are the averages of the items categorized in that mean so far.\n- We repeat the process for a given number of iterations and at the end, we have our clusters.","3d7933df":"##### Approach:\n1. As we got RFM for each customer, now the objective is to label or categorize the customer into different category based upon their value.\n2. For this first we categorize the R,F,M values into the labels 1-5. 1 being the low and 5 being the higest. Here we make cuts at 25%, 50%, 75% and 90% to distribute them into 5 categories.\n3. After that we sum these values for each row and label the column 'score' which have values range from 3-15.\n4. After that we make cuts on 'score' at 25%, 50%, 75% and 90% to categorize the customers into Bad, Bronze, Silver, Gold and Platinum.\n5. These cuts can be done in many ways like by discussion with the client, according to expert advise, business understanding or quartiles cuts(which we did in this) etc.","fdd462fb":"- Initially without any knowledge we are clustering the data into 5 clusters. The only intution to do is as in RFM we categorize the data into 5 categories.\n- Later we look different methods to decide the optimal value for **k**.","c71d519d":"##### Observations:\n- The above table is nothing but showing value in percentages.\n- We can see that over the period of time how the customer interact with the business. ex- In Jan'2011 the business acquire some new customers but after one month only 21.9% are retained or say revisit again. Then the number rise to 26.7% which means some customers back and purchase again and the reason could be an invitation\/offers is sent to those group of customers."}}