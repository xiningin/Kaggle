{"cell_type":{"85487a74":"code","60da1681":"code","e8824c57":"code","96ba226f":"code","940ab3ad":"code","cef756af":"code","45621c0a":"code","bd993352":"code","00a548bd":"code","70d33078":"code","0b9f79ca":"code","76d1cd24":"code","e7fa6dad":"code","240b86cd":"code","bbe2c749":"code","3f446b83":"code","ce57070d":"code","cf488e80":"code","31e28d45":"code","f6bd0989":"code","34bb9de7":"code","6d230a73":"code","03c23849":"code","4ea98920":"code","e98ac0a5":"code","233c4019":"code","48409a64":"code","2efd5da8":"code","bcb9b2fe":"code","2c406384":"code","78fb2d09":"code","1d6c1055":"code","6cd313ff":"code","9659f906":"code","51ae52fa":"code","768f504c":"code","5116eb52":"code","011fad6f":"code","567ed90a":"code","f940d621":"code","0b2566fb":"code","b632f181":"code","13e725b0":"code","e105cb59":"code","6c32fde7":"code","ba07a927":"code","7c919242":"code","dec4afb1":"code","4ff9bb6b":"code","bd203628":"code","843cd04c":"code","e6cca67a":"code","8a4c7106":"code","6ba29199":"code","e46a4645":"code","07024438":"code","b4cdb3ac":"code","2af5a01d":"code","1646102e":"code","21e28861":"code","18899450":"code","9bd114f2":"code","e70362ae":"code","2222462d":"code","2905f530":"code","3ea8d42d":"code","d17a77ac":"code","615572ff":"code","702bf468":"code","7efd11be":"code","80c9f1a2":"code","7f5ca817":"code","f2a69d9d":"code","1f720f35":"code","435f5f83":"code","3e4e28c9":"code","9d3a2044":"code","73029cb7":"code","1de4ce2a":"code","f27fa1fa":"code","0f2c4d81":"code","3935befa":"markdown","5ad3a906":"markdown","71eb8111":"markdown","ce4a3e9b":"markdown","524fa56a":"markdown","76358598":"markdown","d323c8fa":"markdown","2b054942":"markdown","cad487d7":"markdown","243d8288":"markdown","f8b43b06":"markdown","d73a929a":"markdown","38884180":"markdown","74a87541":"markdown","40de5b22":"markdown","138f7d4a":"markdown","479067f4":"markdown","0ff0d220":"markdown","cb82b3ee":"markdown","735506e9":"markdown","1b56f367":"markdown","0946b496":"markdown","755b5b44":"markdown","75f2d64d":"markdown","9277fc8c":"markdown","b40af877":"markdown","254a582f":"markdown","d7d03c42":"markdown","616a70e0":"markdown","9d898d3c":"markdown","0d47092a":"markdown","8cf2aa7b":"markdown","ecf67d27":"markdown","46e1ce64":"markdown","fa6173e5":"markdown","398ffec9":"markdown","050487f7":"markdown","b189ca12":"markdown","b956bb44":"markdown","d4b74c4b":"markdown","075c5516":"markdown","5c39c16d":"markdown","e5b3c228":"markdown","129d726c":"markdown","d8d58c06":"markdown","47fb1385":"markdown","4ff477fd":"markdown","4ca1d61b":"markdown","0c7f4c8a":"markdown","b1dc3ac9":"markdown","93e1855f":"markdown","6ae4194b":"markdown","8a0ee63a":"markdown","96eab91b":"markdown","d5bcd6d8":"markdown","3cfdd202":"markdown","26902dbd":"markdown","0ad0ea74":"markdown","d5e07bef":"markdown","015a5984":"markdown","1550c14b":"markdown","a704465a":"markdown","744eaa98":"markdown","47ce402e":"markdown","87d25412":"markdown","7bf2cc69":"markdown","e93b9a3a":"markdown","7d8eed5d":"markdown","eeb0ebd9":"markdown","b8d0b7f3":"markdown","d4c0374b":"markdown","0e851c8d":"markdown","ae82e439":"markdown","660cf1a4":"markdown","32123db5":"markdown","1a6e8a16":"markdown","0e46a551":"markdown","a7f75006":"markdown","1043ae6c":"markdown","ab5723e6":"markdown","9376aaa3":"markdown","79546a2f":"markdown","1a64a702":"markdown","3beec81a":"markdown","b9a65588":"markdown","7ce464b4":"markdown","48f3659e":"markdown","504be575":"markdown","b2ef8bdc":"markdown","6f298c78":"markdown","8fd97ead":"markdown","2373fc1e":"markdown","4f0ecb68":"markdown","ed8dcadd":"markdown","0459d4ff":"markdown","e9301f13":"markdown","d32e4a13":"markdown","902d5cbb":"markdown","b20bab2e":"markdown","d43f538d":"markdown","9aa1e59b":"markdown","a86c5a7d":"markdown","8abb79af":"markdown","289677b0":"markdown","58eed2bd":"markdown","d2772666":"markdown","628c257c":"markdown","1f53b3a1":"markdown","8f1d25a1":"markdown","cd8ba3a5":"markdown","8dd03c74":"markdown"},"source":{"85487a74":"#pip install catboost --no-cache-dir","60da1681":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport itertools\n\nimport time\n\n# used to supress display of warnings\nimport warnings\n\n# ols library\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport missingno as mno\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import OPTICS\n\n# import zscore for scaling the data\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, RobustScaler\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\n# pre-processing methods\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.compose import TransformedTargetRegressor\n\n# the regression models \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# cross-validation methods\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn import metrics\n\nfrom sklearn.pipeline import Pipeline\n\n# feature-selection methods\nfrom sklearn.feature_selection import SelectFromModel\n\n# bootstrap sampling\nfrom sklearn.utils import resample","e8824c57":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","96ba226f":"# Reading Concrete data\nconcrete_df = pd.read_csv(\"..\/input\/cement-manufacturing-concrete-dataset\/concrete.csv\")","940ab3ad":"# Get the top 5 rows\nconcrete_df.head()","cef756af":"# Get the shape of Concrete data\nconcrete_df.shape","45621c0a":"print(\"Number of rows = {0} and Number of Columns = {1} in Data frame\".format(concrete_df.shape[0],concrete_df.shape[1]))","bd993352":"# Check datatypes\nconcrete_df.dtypes","00a548bd":"# Check Data frame info\nconcrete_df.info()","70d33078":"# Column names of Data frame\nconcrete_df.columns","0b9f79ca":"# Check duplicates in a data frame\nconcrete_df.duplicated().sum()","76d1cd24":"# View the duplicate records\nduplicates = concrete_df.duplicated()\n\nconcrete_df[duplicates]","e7fa6dad":"# Delete duplicate rows\nconcrete_df.drop_duplicates(inplace=True)","240b86cd":"# Get the shape of Concrete data\nconcrete_df.shape","bbe2c749":"# Create a boxplot for all the continuous features\nconcrete_df.boxplot(column = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength'], rot=45, figsize = (20,10));","3f446b83":"concrete_df_outliers = pd.DataFrame(concrete_df.loc[:,])\n\n# Calculate IQR\nQ1 = concrete_df_outliers.quantile(0.25)\nQ3 = concrete_df_outliers.quantile(0.75)\nIQR = Q3 - Q1\n\nprint(IQR)","ce57070d":"concrete_df.columns","cf488e80":"# We can use IQR score to filter out the outliers by keeping only valid values\n\n# Replace every outlier on the upper side by the upper whisker - for 'water', 'superplastic', \n# 'fineagg', 'age' and 'strength' columns\nfor i, j in zip(np.where(concrete_df_outliers > Q3 + 1.5 * IQR)[0], np.where(concrete_df_outliers > Q3 + 1.5 * IQR)[1]):\n    \n    whisker  = Q3 + 1.5 * IQR\n    concrete_df_outliers.iloc[i,j] = whisker[j]\n    \n# Replace every outlier on the lower side by the lower whisker - for 'water' column\nfor i, j in zip(np.where(concrete_df_outliers < Q1 - 1.5 * IQR)[0], np.where(concrete_df_outliers < Q1 - 1.5 * IQR)[1]): \n    \n    whisker  = Q1 - 1.5 * IQR\n    concrete_df_outliers.iloc[i,j] = whisker[j]","31e28d45":"# Remove outliers columns - 'water', 'superplastic', 'fineagg', 'age', 'water' and 'strength'\nconcrete_df.drop(columns = concrete_df.loc[:,], inplace = True)","f6bd0989":"# Add 'water', 'superplastic', 'fineagg', 'age', 'water' and 'strength' with no outliers from concrete_df_outliers to \n# concrete_df\nconcrete_df = pd.concat([concrete_df, concrete_df_outliers], axis = 1)","34bb9de7":"# Create a boxplot for all the continuous features\nconcrete_df.boxplot(column = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength'], rot=45, figsize = (20,10));","6d230a73":"# Check the presence of missing values\nconcrete_df.isnull().sum()","03c23849":"# Check the presence of missing values\nconcrete_df_missval = concrete_df.copy()   # Make a copy of the dataframe\nisduplicates = False\n\nfor x in concrete_df_missval.columns:\n    concrete_df_missval[x] = concrete_df_missval[x].astype(str).str.replace(\".\", \"\")\n    result = concrete_df_missval[x].astype(str).str.isalnum() # Check whether all characters are alphanumeric\n    if False in result.unique():\n        isduplicates = True\n        print('For column \"{}\" unique values are {}'.format(x, concrete_df_missval[x].unique()))\n        print('\\n')\n        \nif not isduplicates:\n    print('No duplicates in this dataset')","4ea98920":"# Visualize missing values\nmno.matrix(concrete_df, figsize = (20, 6));","e98ac0a5":"# Summary statistics\nconcrete_df.describe().T","233c4019":"cols = [i for i in concrete_df.columns if i not in 'strength']\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\"]\nfig = plt.figure(figsize=(13,25))\n\nfor i,j,k in itertools.zip_longest(cols,range(length),cs):\n    plt.subplot(4,2,j+1)\n    ax = sns.distplot(concrete_df[i],color=k,rug=True)\n    ax.set_facecolor(\"w\")\n    plt.axvline(concrete_df[i].mean(),linestyle=\"dashed\",label=\"mean\",color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","48409a64":"#for x in concrete_df:\n    #sns.distplot(concrete_df[x]);\n    #plt.title(\"{} distribution\".format(x))\n    #plt.show()    ","2efd5da8":"plt.figure(figsize=(13,6))\nsns.distplot(concrete_df[\"strength\"],color=\"b\",rug=True)\nplt.axvline(concrete_df[\"strength\"].mean(), linestyle=\"dashed\",color=\"k\", label='mean',linewidth=2)\nplt.legend(loc=\"best\",prop={\"size\":14})\nplt.title(\"Concrete compressivee strength distribution\")\nplt.show()","bcb9b2fe":"# Summary statistics\nconcrete_df.describe().T","2c406384":"sns.pairplot(concrete_df, diag_kind = 'kde', corner = True);","78fb2d09":"# Check the Correlation\nconcrete_df.corr()","1d6c1055":"sns.pairplot(concrete_df[['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age', 'strength']], kind = 'reg', corner = True);","6cd313ff":"corr = abs(concrete_df.corr()) # correlation matrix\nlower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\nmask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\nplt.figure(figsize = (12,10))\nsns.heatmap(lower_triangle, center = 0.5, cmap = 'coolwarm', annot= True, xticklabels = corr.index, yticklabels = corr.columns,\n            cbar= True, linewidths= 1, mask = mask)   # Da Heatmap\nplt.show()","9659f906":"import matplotlib.gridspec as gridspec\n\n# sns styling figures\nsns.set(style='white')\nsns.set(style='whitegrid',color_codes=True)","51ae52fa":"X = concrete_df.drop(['strength'], axis = 1) # Considering all Predictors\ny = concrete_df['strength']","768f504c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)","5116eb52":"print('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","011fad6f":"def train_test_model(model, method, X_train, X_test, y_train, y_test, of_type, index, scale):\n    \n    print (model)\n    print (\"***************************************************************************\")\n    \n    if scale == 'yes':\n        # prepare the model with input scaling\n        pipeline = Pipeline([('scaler', PowerTransformer()), ('model', model)])\n    elif scale == 'no':\n        # prepare the model with input scaling\n        pipeline = Pipeline([('model', model)])\n\n    pipeline.fit(X_train, y_train) # Fit the model on Training set\n    prediction = pipeline.predict(X_test) # Predict on Test set\n\n    r2 = metrics.r2_score(y_test, prediction) # Calculate the r squared value on the Test set\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction)) # Root mean squared error\n    \n    if of_type == \"coef\":\n        # Intercept and Coefficients\n        print(\"The intercept for our model is {}\".format(model.intercept_), \"\\n\")\n        \n        for idx, col_name in enumerate(X_train.columns):\n            print(\"The coefficient for {} is {}\".format(col_name, model.coef_.ravel()[idx]))\n    \n    # Accuracy of Training data set\n    train_accuracy_score = pipeline.score(X_train, y_train)\n    \n    # Accuracy of Test data set\n    test_accuracy_score = pipeline.score(X_test, y_test)\n    \n    print (\"***************************************************************************\")\n    \n    if of_type == \"coef\":\n        \n        # FEATURE IMPORTANCES plot\n        plt.figure(figsize=(13,12))\n        plt.subplot(211)\n        print(model.coef_)\n        coef = pd.DataFrame(np.sort(model.coef_)[::-1].ravel())\n        coef[\"feat\"] = X_train.columns\n        ax1 = sns.barplot(coef[\"feat\"],coef[0],palette=\"jet_r\", linewidth=2)\n        ax1.set_facecolor(\"lightgrey\")\n        ax1.axhline(0,color=\"k\",linewidth=2)\n        plt.ylabel(\"coefficients\")\n        plt.xlabel(\"features\")\n        plt.title(method + ' ' + 'FEATURE IMPORTANCES')\n    \n    elif of_type == \"feat\":\n        \n        # FEATURE IMPORTANCES plot\n        plt.figure(figsize=(13,12))\n        plt.subplot(211)\n        coef = pd.DataFrame(np.sort(model.feature_importances_)[::-1])\n        coef[\"feat\"] = X_train.columns\n        ax2 = sns.barplot(coef[\"feat\"], coef[0],palette=\"jet_r\", linewidth=2)\n        ax2.set_facecolor(\"lightgrey\")\n        ax2.axhline(0,color=\"k\",linewidth=2)\n        plt.ylabel(\"coefficients\")\n        plt.xlabel(\"features\")\n        plt.title(method + ' ' + 'FEATURE IMPORTANCES')\n    \n    # Store the accuracy results for each model in a dataframe for final comparison\n    resultsDf = pd.DataFrame({'Method': method, 'R Squared': r2, 'RMSE': rmse, 'Train Accuracy': train_accuracy_score, \n                              'Test Accuracy': test_accuracy_score}, index=[index])\n    \n    return resultsDf  # return all the metrics along with predictions","567ed90a":"def train_test_allmodels(X_train_common, X_test_common, y_train, y_test, scale):\n    # define regressor models\n    models=[['LinearRegression',LinearRegression()],\n        ['Ridge',Ridge(random_state = 1)],\n        ['Lasso',Lasso(random_state = 1)],\n        ['KNeighborsRegressor',KNeighborsRegressor(n_neighbors = 3)],\n        ['SVR',SVR(kernel = 'linear')],\n        ['RandomForestRegressor',RandomForestRegressor(random_state = 1)],\n        ['BaggingRegressor',BaggingRegressor(random_state = 1)],\n        ['ExtraTreesRegressor',ExtraTreesRegressor(random_state = 1)],\n        ['AdaBoostRegressor',AdaBoostRegressor(random_state = 1)],\n        ['GradientBoostingRegressor',GradientBoostingRegressor(random_state = 1)],\n        ['CatBoostRegressor',CatBoostRegressor(random_state = 1, verbose=False)],\n        ['XGBRegressor',XGBRegressor()]\n    ]\n\n    resultsDf_common = pd.DataFrame()\n    i = 1\n    for name, regressor in models:\n        # Train and Test the model\n        reg_resultsDf = train_test_model(regressor, name, X_train_common, X_test_common, y_train, y_test, 'none', i, scale)\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf_common = pd.concat([resultsDf_common, reg_resultsDf])\n        i = i+1\n\n    return resultsDf_common","f940d621":"def hyperparameterstune_model(name, model, X_train, y_train, param_grid):\n    \n    start = time.time()  # note the start time \n    \n    # define grid search\n    cv = KFold(n_splits=10, random_state=1)\n    #grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, n_jobs=-1, cv=cv, \n                                     #scoring = 'neg_root_mean_squared_error', error_score=0)\n    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, \n                                     scoring = 'neg_root_mean_squared_error', error_score=0)\n    model_grid_result = grid_search.fit(X_train, y_train)\n\n    # summarize results\n    print(name, \"- Least: RMSE %f using %s\" % (model_grid_result.best_score_ * (-1), model_grid_result.best_params_))\n    \n    end = time.time()  # note the end time\n    duration = end - start  # calculate the total duration\n    print(\"Total duration\" , duration, \"\\n\")\n    \n    return model_grid_result.best_estimator_","0b2566fb":"# Building a Linear Regression model\nlr = LinearRegression()\n                                                     \n# Train and Test the model\nresultsDf = train_test_model(lr, 'LinearRegression', X_train, X_test, y_train, y_test, 'none', 1, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf","b632f181":"# R^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no \n# influence on the predicted variable. Instead we use adjusted R^2 which removes the statistical chance that improves R^2\n\n# OLS library expects the X and y to be given in one single dataframe\nconcrete_df_train = pd.concat([X_train, y_train], axis=1)\nconcrete_df_train.head()\n\nlr_ols = smf.ols(formula= 'strength ~ cement + slag + ash + water + superplastic + coarseagg + fineagg + age', \n              data = concrete_df_train).fit()\n\nprint(lr_ols.summary())  # Inferential statistics","13e725b0":"# Building a Ridge Regression model\nrr = Ridge(random_state = 1)\n\n# Train and Test the model\nrr_resultsDf = train_test_model(rr, 'Ridge', X_train, X_test, y_train, y_test, 'coef', 2, 'yes')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rr_resultsDf])\nresultsDf","e105cb59":"# Building a Lasso Regression model\nlasso = Lasso(random_state = 1)\n\n# Train and Test the model\nlasso_resultsDf = train_test_model(lasso, 'Lasso', X_train, X_test, y_train, y_test, 'coef', 3, 'yes')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, lasso_resultsDf])\nresultsDf","6c32fde7":"# Transfom X_train and X_test to polynomial features\npipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = 2, interaction_only=True))])\nX_train_poly2 = pd.DataFrame(pipe.fit_transform(X_train))\nX_test_poly2 = pd.DataFrame(pipe.fit_transform(X_test))","ba07a927":"# Train and Test the model\nlr_resultsDf = train_test_model(lr, 'Linear Regression with interaction features', X_train_poly2, X_test_poly2, y_train, y_test, \n                                'none', 4, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,lr_resultsDf])\nresultsDf","7c919242":"# Building a Ridge Regression model\nrr = Ridge(random_state = 1)\n\n# Train and Test the model\nrr_resultsDf = train_test_model(rr, 'Ridge with interaction features', X_train_poly2, X_test_poly2, y_train, y_test, 'coef', 5, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rr_resultsDf])\nresultsDf","dec4afb1":"# Building a Lasso Regression model\nlasso = Lasso(random_state = 1)\n\n# Train and Test the model\nlasso_resultsDf = train_test_model(lasso, 'Lasso with interaction features', X_train_poly2, X_test_poly2, y_train, y_test, 'coef', 6, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, lasso_resultsDf])\nresultsDf","4ff9bb6b":"for i in range(1,6):\n    pipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = i)), \n                 ('model', LinearRegression())])\n    pipe.fit(X_train, y_train) # Fit the model on Training set\n    prediction = pipe.predict(X_test) # Predict on Test set\n\n    r2 = metrics.r2_score(y_test, prediction) # Calculate the r squared value on the Test set\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, prediction)) # Root mean squared error\n    \n    print (\"R-Squared for {0} degree polynomial is {1}\".format(i, r2))\n    print (\"ROOT MEAN SQUARED ERROR for {0} degree polynomial features is {1}\".format(i, rmse),\"\\n\")","bd203628":"pipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = 2)), \n                 ('model', LinearRegression())])\n    \npipe.fit(X_train, y_train) # Fit the model on Training set\nprediction = pipe.predict(X_test) # Predict on Test set\n        \nr2 = metrics.r2_score(y_test, prediction) # Calculate the r squared value on the Test set\nrmse = np.sqrt(metrics.mean_squared_error(y_test, prediction)) # Root mean squared error\n\nprint (\"R-Squared :\", r2)\nprint (\"ROOT MEAN SQUARED ERROR :\", rmse)\n\n# Accuracy of Training data set\nprint(\"Accuracy of Training data set: {0:.4f} %\".format(pipe.score(X_train, y_train)))\n\n# Accuracy of Test data set\naccuracy_score = pipe.score(X_test, y_test)\nprint(\"Accuracy of Test data set: {0:.4f} %\".format(accuracy_score))","843cd04c":"# Store the accuracy results for each model in a dataframe for final comparison\npoly_resultsDf = pd.DataFrame({'Method': 'Linear Regression with Polynomial features', 'R Squared': r2, 'RMSE': rmse, 'Train Accuracy': pipe.score(X_train, y_train), \n                          'Test Accuracy': accuracy_score}, index=[7])\nresultsDf = pd.concat([resultsDf, poly_resultsDf])\nresultsDf","e6cca67a":"# Transfom X_train and X_test to polynomial features\npipe = Pipeline([('scaler', PowerTransformer()), ('polynomial', PolynomialFeatures(degree = 2))])\nX_train_poly_2 = pd.DataFrame(pipe.fit_transform(X_train))\nX_test_poly_2 = pd.DataFrame(pipe.fit_transform(X_test))","8a4c7106":"# Building a Ridge Regression model\nrr = Ridge(random_state = 1)\n\n# Train and Test the model\nrr_resultsDf = train_test_model(rr, 'Ridge with polynomial features', X_train_poly_2, X_test_poly_2, y_train, y_test, 'coef', 8, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rr_resultsDf])\nresultsDf","6ba29199":"# Building a Lasso Regression model\nlasso = Lasso(random_state = 1)\n\n# Train and Test the model\nlasso_resultsDf = train_test_model(lasso, 'Lasso with polynomial features', X_train_poly_2, X_test_poly_2, y_train, y_test, 'coef', 9, 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, lasso_resultsDf])\nresultsDf","e46a4645":"# Scale the data using PowerTransformer\nscale = PowerTransformer()\nconcrete_df_scaled = pd.DataFrame(scale.fit_transform(concrete_df))","07024438":"cluster_range = range(1, 15)  \ncluster_errors = []\nfor num_clusters in cluster_range:\n    clusters = KMeans(n_clusters = num_clusters, n_init = 5, random_state = 1)\n    clusters.fit(concrete_df_scaled)\n    \n    labels = clusters.labels_\n    centroids = clusters.cluster_centers_\n    \n    cluster_errors.append(clusters.inertia_ )\n\nclusters_df = pd.DataFrame({ \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","b4cdb3ac":"# Elbow plot\nplt.figure(figsize=(12,6))\nplt.plot(clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" );","2af5a01d":"# k = 6\ncluster = KMeans(n_clusters = 6, random_state = 1)\ncluster.fit(concrete_df_scaled)","1646102e":"# Creating a new column \"GROUP\" which will hold the cluster id of each record\nprediction=cluster.predict(concrete_df_scaled)\nconcrete_df_scaled[\"GROUP\"] = prediction","21e28861":"centroids = cluster.cluster_centers_\ncentroids","18899450":"centroid_df = pd.DataFrame(centroids, columns = list(concrete_df))\ncentroid_df","9bd114f2":"## Instead of interpreting the neumerical values of the centroids, let us do a visual analysis by converting the \n## centroids and the data in the cluster into box plots.\nconcrete_df_scaled.boxplot(by = 'GROUP',  layout=(3,3), figsize=(15, 10));","e70362ae":"def train_test_transform(X_train, X_test):\n    scale = PowerTransformer()\n    \n    X_train_scaled = pd.DataFrame(scale.fit_transform(X_train))\n    X_test_scaled = pd.DataFrame(scale.fit_transform(X_test))\n    \n    return X_train_scaled, X_test_scaled","2222462d":"# empty list that will hold error\nerror = []\n\nX_train_scaled, X_test_scaled = train_test_transform(X_train, X_test)\n\n# perform error metrics for values from 1,2,3....29\nfor k in range(1,30):\n    \n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test_scaled)\n    error.append(np.mean(y_pred != y_test))","2905f530":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30), error, color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","3ea8d42d":"# Building a KNN Regression model\nknn = KNeighborsRegressor(n_neighbors = 2)\n\n# Train and Test the model\nknn_resultsDf = train_test_model(knn, 'KNeighborsRegressor', X_train, X_test, y_train, y_test, 'none', 10, 'yes')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf, knn_resultsDf])\nresultsDf","d17a77ac":"# define regressor models\nmodels=[\n    ['SVR',SVR(kernel='linear')],\n    ['DecisionTreeRegressor', DecisionTreeRegressor(random_state = 1)],\n    ['RandomForestRegressor',RandomForestRegressor(random_state = 1)],\n    ['BaggingRegressor',BaggingRegressor(random_state = 1)],\n    ['ExtraTreesRegressor',ExtraTreesRegressor(random_state = 1)],\n    ['AdaBoostRegressor',AdaBoostRegressor(random_state = 1)],\n    ['GradientBoostingRegressor',GradientBoostingRegressor(random_state = 1)],\n    ['CatBoostRegressor',CatBoostRegressor(random_state = 1, verbose=False)],\n    ['XGBRegressor',XGBRegressor()]\n]\n\n\ni = 11\nfor name, regressor in models:\n    if name == 'SVR':\n        # Train and Test the model\n        svr_resultsDf = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'coef', i, 'yes')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf = pd.concat([resultsDf, svr_resultsDf])\n    elif name == 'BaggingRegressor':\n        # Train and Test the model\n        bag_resultsDf = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'none', i, 'yes')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf = pd.concat([resultsDf, bag_resultsDf])\n    else:\n        # Train and Test the model\n        ensemble_resultsDf = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'feat', i, 'yes')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf = pd.concat([resultsDf, ensemble_resultsDf])\n    i = i+1","615572ff":"# Show results dataframe\nresultsDf","702bf468":"# Selecting features using Lasso regularisation using SelectFromModel\nsel_ = SelectFromModel(Lasso(random_state = 1))\nsel_.fit(X_train, y_train)\n\n# Visualising features that were kept by the lasso regularisation\nsel_.get_support()\n\n# Make a list of with the selected features\nlasso_selected_feat = X_train.columns[(sel_.get_support())]\n\n# Prepare train and test data\nX_train_lasso = X_train[lasso_selected_feat]\nX_test_lasso = X_test[lasso_selected_feat]\n\n# Lasso with it's important features\nX_train_lasso.columns","7efd11be":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_lasso, X_test_lasso, y_train, y_test, 'no')","80c9f1a2":"# Selecting features using Lasso regularisation using SelectFromModel\nsel_ = SelectFromModel(Lasso(random_state = 1))\nsel_.fit(X_train_poly_2, y_train)\n\n# Visualising features that were kept by the lasso regularisation\nsel_.get_support()\n\n# Make a list of with the selected features\nlasso_poly_selected_feat = X_train_poly_2.columns[(sel_.get_support())]\n\n# Prepare train and test data\nX_train_lasso_poly = X_train_poly_2[lasso_poly_selected_feat]\nX_test_lasso_poly = X_test_poly_2[lasso_poly_selected_feat]\n\n# Lasso with it's polynomial important features\nX_train_lasso_poly.columns","7f5ca817":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_lasso_poly, X_test_lasso_poly, y_train, y_test, 'no')","f2a69d9d":"# Select top 5 important features\nX_feat = concrete_df[['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'age']]\ny = concrete_df['strength']\n\n# Split data into train and test set\nX_train_feat, X_test_feat, y_train, y_test = train_test_split(X_feat, y, test_size = 0.30, random_state = 1)","1f720f35":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_feat, X_test_feat, y_train, y_test, 'yes')","435f5f83":"# define regressor models\nmodels=[['Ridge',Ridge()],\n    #['Lasso',Lasso()],\n    #['KNeighborsRegressor',KNeighborsRegressor()],\n    ['SVR',SVR()]\n    #['RandomForestRegressor',RandomForestRegressor()],\n    #['BaggingRegressor',BaggingRegressor()],\n    #['ExtraTreesRegressor',ExtraTreesRegressor()],\n    #['AdaBoostRegressor',AdaBoostRegressor()],\n    #['GradientBoostingRegressor',GradientBoostingRegressor()],\n    #['CatBoostRegressor',CatBoostRegressor(verbose=False)],\n    #['XGBRegressor',XGBRegressor()]\n]\n\n# define model parameters\nridge_param_grid = {'alpha': [1,0.1,0.01,0.001,0.0001,0]}\nlasso_param_grid = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\nknn_param_grid = {'n_neighbors': range(3, 21, 2),\n                 'weights': ['uniform', 'distance'],\n                 'metric': ['euclidean', 'manhattan', 'minkowski']}\nsvr_param_grid = {'kernel': ['poly', 'rbf', 'sigmoid'],\n                 'C': [50, 10, 1.0, 0.1, 0.01],\n                 'gamma': ['scale']}\nrf_param_grid = {'n_estimators': [10, 100, 1000],\n                 'max_features': ['auto', 'sqrt', 'log2']}\nbag_param_grid = {'n_estimators': [10, 100, 1000],\n                 'max_samples': np.arange(0.7, 0.8, 0.05)}\net_param_grid = {'n_estimators': np.arange(10,100,10),\n                 'max_features': ['auto', 'sqrt', 'log2'],\n                 'min_samples_split': np.arange(2,15,1)}\nadb_param_grid = {'n_estimators': np.arange(30,100,10),\n                 'learning_rate': np.arange(0.1,1,0.5)}\ngb_param_grid = {'n_estimators': np.arange(30,100,10),\n                 'learning_rate': np.arange(0.1,1,0.5)}\ncatb_param_grid = {'depth': [4, 7, 10],\n                  'learning_rate' : [0.03, 0.1, 0.15],\n                  'l2_leaf_reg': [1,4,9],\n                  'iterations': [300]}\nxgb_param_grid = {'learning_rate': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n                  'max_depth' : [ 3, 4, 5, 6, 8, 10, 12, 15],\n                  'min_child_weight': [ 1, 3, 5, 7],\n                  'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4],\n                  'colsample_bytree': [ 0.3, 0.4, 0.5 , 0.7]}\n\n\nfor name, regressor in models:\n    if name == 'Ridge':\n        ridge_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, ridge_param_grid)\n    elif name == 'Lasso':\n        lasso_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, lasso_param_grid)\n    elif name == 'KNeighborsRegressor':\n        knn_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, knn_param_grid)\n    elif name == 'SVR':\n        svr_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, svr_param_grid)\n    elif name == 'RandomForestRegressor':\n        rf_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, rf_param_grid)\n    elif name == 'BaggingRegressor':\n        bag_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, bag_param_grid)\n    elif name == 'ExtraTreesRegressor':\n        et_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, et_param_grid)\n    elif name == 'AdaBoostRegressor':\n        adb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, adb_param_grid)\n    elif name == 'GradientBoostingRegressor':\n        gb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, gb_param_grid)\n    elif name == 'CatBoostRegressor':\n        catb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, catb_param_grid)\n    elif name == 'XGBRegressor':\n        xgb_best_estimator = hyperparameterstune_model(name, regressor, X_train, y_train, xgb_param_grid)","3e4e28c9":"# define regressor models\nmodels=[['Ridge', ridge_best_estimator],\n    #['Lasso', lasso_best_estimator],\n    #['KNeighborsRegressor', knn_best_estimator],\n    ['SVR', svr_best_estimator],\n    #['RandomForestRegressor', rf_best_estimator],\n    #['BaggingRegressor', bag_best_estimator],\n    #['ExtraTreesRegressor',et_best_estimator],\n    #['AdaBoostRegressor', adb_best_estimator],\n    #['GradientBoostingRegressor', gb_best_estimator],\n    #['CatBoostRegressor', catb_best_estimator],\n    #['XGBRegressor', xgb_best_estimator]\n]\n\nresultsDf_hp = pd.DataFrame()\ni = 1\nfor name, regressor in models:\n    # Train and Test the model\n    resultsDf_hp_ind = train_test_model(regressor, name, X_train, X_test, y_train, y_test, 'none', i, 'yes')\n\n    # Store the accuracy results for each model in a dataframe for final comparison\n    resultsDf_hp = pd.concat([resultsDf_hp, resultsDf_hp_ind])\n    i = i+1\n\n# Show results dataframe\nresultsDf_hp","9d3a2044":"# Drop K-means cluster group from concrete_df_scaled dataset\nconcrete_df_scaled.drop(columns=['GROUP'], axis=1, inplace=True)","73029cb7":"values = concrete_df_scaled.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(concrete_df_scaled) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\ngbm_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n\n     # fit model\n    gbmTree = GradientBoostingRegressor(n_estimators=50)\n\n    # fit against independent variables and corresponding target values\n    gbmTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbmTree.score(test[:, :-1] , y_bs_test)\n    predictions = gbmTree.predict(test[:, :-1])  \n\n    gbm_stats.append(score)","1de4ce2a":"# plot scores\nplt.hist(gbm_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(gbm_stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(gbm_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","f27fa1fa":"values = concrete_df_scaled.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(concrete_df_scaled) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nrf_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n     # fit model\n    rfTree = RandomForestRegressor(n_estimators=100)\n\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_bs_test)\n    predictions = rfTree.predict(test[:, :-1]) \n\n    rf_stats.append(score)","0f2c4d81":"# plot scores\nplt.hist(rf_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(rf_stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(rf_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","3935befa":"<a id = '5.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.3 Study Summary Statistics <\/strong><\/p> ","5ad3a906":"### Domain:\n\nCement manufacturing","71eb8111":"<a id = '9.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.1 Hyper Parameter Tuning <\/strong><\/p>","ce4a3e9b":"<a id = '4.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.2 Drop Duplicates <\/strong><\/p> ","524fa56a":"#### Concrete compressive strength distribution","76358598":"<a id = '5.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.4 Multivariate Analysis <\/strong><\/p> ","d323c8fa":"<p style = \"font-size:20px; color: #007580 \"><strong> Train and test model <\/strong><\/p> ","2b054942":"* **The above output prints the important summary statistics of all the numeric variables like the mean, median (50%), minimum, and maximum values, along with the standard deviation.**\n\n* **cement column - Right skewed distribution -- cement is skewed to higher values**\n* **slag column - Right skewed distribution -- slag is skewed to higher values and there are two gaussians**\n* **ash column - Right skewed distribution -- ash is skewed to higher values and there are two gaussians**\n* **water column - Moderately left skewed distribution**\n* **superplastic column - Right skewed distribution -- superplastic is skewed to higher values and there are two gaussians**\n* **coarseagg column - Moderately left skewed distribution**\n* **fineagg column - Moderately left skewed distribution**\n* **age column - Right skewed distribution -- age is skewed to higher values and there are five gaussians**\n* **strength column - Moderately right skewed distribution**","cad487d7":"<p style = \"font-size:20px; color: #007580 \"><strong> Data type of each attribute <\/strong><\/p> ","243d8288":"<p style = \"font-size:20px; color: #007580 \"><strong> Ridge Regression <\/strong><\/p> ","f8b43b06":"<a id = '7.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.3 Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and presenting my findings in terms of the independent attributes and their suitability to predict strength <\/strong><\/p> ","d73a929a":"**Lasso with it's important features - observation: All these models performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.**","38884180":"**Observation: This model performs better on training set as well as test set and RMSE is als reduced to 6.77**\n\n","74a87541":"#### Numerical columns - 'Cement', 'Slag', 'Ash', 'Water', 'Superplastic', 'Coarseagg', 'Fineagg', 'Age' and 'Strength'","40de5b22":"**Note:** The **first array contains the list of row numbers** and **second array respective column numbers** in concrete_df_outliers data frame","138f7d4a":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","479067f4":"<p style = \"font-size:20px; color: #007580 \"><strong> Adding Interaction Terms - Linear Regression <\/strong><\/p>","0ff0d220":"* **Strength column seems to be uniformly distributed**","cb82b3ee":"<p style = \"font-size:20px; color: #007580 \"><strong> Importance features from ensemble models - DecisionTree, ExtraTree, AdaBoost, GradientBoost, CatBoost and XGBoost <\/strong><\/p>\n\n**As mentioned in earlier steps, top 5 important features from ensemble models are - cement, slag, ash, water and superplastic**","735506e9":"<a id = '5.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. EDA (Data Analysis and Preparation) <\/h2> ","1b56f367":"<a id = '8.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.3 Comparison of with and without feature selection methods <\/strong><\/p>\n\n1. Best models without feature selection methods:\n\n    a. Linear Regression with Polynomial features - Test accuracy = 86.94% with RMSE = 5.50\n\n    b. Ridge regression with original features - Test accuracy = 80.24% with RMSE = 6.77\n\n    c. SVR with original features - Test accuracy = 80.03% with RMSE = 6.81\n\n\n2. Best models with ensemble's feature selection methods:\n\n    a. Linear regression - Test accuracy = 79.50% with RMSE = 6.90\n\n    b. Ridge regression - Test accuracy = 79.49% with RMSE = 6.90\n\n    c. SVR - Test accuracy = 79.34% with RMSE = 6.92\n\n**By comparing both options, I see the best models which suits this project are from without feature selection methods, I mean with original features.**","0946b496":"1. I am able to predict the concrete compressive strength using original features with an accuracy of 86.94% on test data with RMSE = 5.50\n\n2. If we look at the above results from various methods then we got the best accuracy from original features and followed below steps to gain that much of accuracy.\n\n    a. As mentioned in Multi-variate analysis, there are some non-linear(curvy-linear) relatioship within independent features as well as with target variable hence I have tried with polynomial features.\n\n    b. Simple linear regression with polynomial features with degree = 2 performs better on both training and test set with 1% difference.\n\n3. We had 25 duplicate instances in dataset and dropped those duplicates.\n\n4. We had outliers in 'Water', 'Superplastic', 'Fineagg', 'Age' and 'Strength' column also, handled these outliers by replacing every outlier with upper and lower side of the whisker.\n\n5. Except 'Cement', 'Water', 'Superplastic' and 'Age' features, all other features are having very weak relationship with concrete 'Strength' feature and does not account for making statistical decision (of correlation).\n\n6. Range of clusters in this dataset is 2 to 6\n\n7. No missing values in dataset.\n\n8. Standardization of data using PowerTransformer improves accuracy slightly.\n\n9. Bootstrap sampling with GradientBoostingRegressor model performance is between 84.8% - 89.0% is better than other Regression algorithms.\n\n10. Bootstrap sampling with RandomForestRegressor model performance is between 86.8% - 91.6% is better than other Regression algorithms.\n\n11. **Finally Bootstrap sampling with RandomForestRegressor model with an accuracy of 86.6% - 91.6% is our best model.**\n\n<p style = \"font-size:30px; color: #007580 \"><strong> Thanks for reading.<\/strong><\/p>","755b5b44":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**\n\n","75f2d64d":"<p style = \"font-size:20px; color: #007580 \"><strong> Lasso Regression <\/strong><\/p>","9277fc8c":"<a id = '5.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.2 Univariate Analysis <\/strong><\/p> ","b40af877":"<p style = \"font-size:20px; color: #007580 \"><strong> Model with Hyperparameter Tuning <\/strong><\/p> ","254a582f":"<a id = '1.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Overview <\/h2> ","d7d03c42":"<a id = '5.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.5 Study Correlation <\/strong><\/p> ","616a70e0":"### Objective\n\n**Modeling of strength of high performance concrete using Machine Learning.**","9d898d3c":"<p style = \"font-size:20px; color: #007580 \"><strong> Polynomial Linear Regression <\/strong><\/p>","0d47092a":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on polynomial features - Lasso Regression <\/strong><\/p>","8cf2aa7b":"<a id = '6.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Feature Engineering <\/h2> ","ecf67d27":"**Observation: This model performs better on training set and performance drops on test set which shows that it's an overfitting and very complex model.**","46e1ce64":"### Attribute Information:\n    \n* **Cement** : measured in kg in a m3 mixture\n* **Blast** : measured in kg in a m3 mixture\n* **Fly ash** : measured in kg in a m3 mixture\n* **Water** : measured in kg in a m3 mixture\n* **Superplasticizer** : measured in kg in a m3 mixture\n* **Coarse Aggregate** : measured in kg in a m3 mixture\n* **Fine Aggregate** : measured in kg in a m3 mixture\n* **Age** : day (1~365)\n* **Concrete compressive strength** measured in MPa","fa6173e5":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on polynomial features - Ridge Regression <\/strong><\/p>","398ffec9":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on interaction terms - Lasso Regression <\/strong><\/p>","050487f7":"<a id = '8.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.2 Overall summary - after feature selection <\/strong><\/p>\n\nI am able to predict the concrete compressive strength using few ingradients with below details.\n\nI have tried three different feature importance methods and please find the below results.\n\n1. Lasso with it's important features are as follows:\n ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age']\n\n I have built all models with above features and all of them performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.\n\n2. Lasso with it's polynomial important features are as follows:\n [1, 2, 4, 5, 7, 8, 26, 44]\n\n I have built all models with above features and all of them performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.\n\n3. All models with ensemble's important features\n ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'age']\n\n I have built all models with above features and all of them performs better on training set and poorly on test set except below models which shows that it's an overfitting and very complex models.\n\n    a. Linear regression - Test accuracy = 79.50% with RMSE = 6.90\n\n    b. Ridge regression - Test accuracy = 79.49% with RMSE = 6.90\n\n    c. SVR - Test accuracy = 79.34% with RMSE = 6.92","b189ca12":"<p style = \"font-size:20px; color: #007580 \"><strong> RandomForestRegressor <\/strong><\/p>","b956bb44":"**Observation: Notice that test accuracy is better than Linear regression with interaction features. This model performs better on training set and performance drops on test set which shows that it's an overfitting and very complex model.**","d4b74c4b":"<p style = \"font-size:20px; color: #007580 \"><strong> Fit a simple non regularized linear model on interaction terms - Ridge Regression <\/strong><\/p>","075c5516":"<a id = '4.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.3 Check Outliers <\/strong><\/p> ","5c39c16d":"<a id = '8.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 8. Feature Selection Methods <\/h2> ","e5b3c228":"**After adding interaction terms we have 66 columns which includes original 8 columns and others which are created from original columns.**\n\n**Notice that by adding interaction terms, RMSE decreased from 8.65 to 6.20**\n\n**When we have more columns are less rows then we are likely to be in overfit zone because model coefficients is having high values. Hence let's try with non-regularized models.**","129d726c":"<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks, please have a look and definitely you will find it useful. Happy reading :)<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/industrial-safety-complete-solution\">Industrial Safety - Complete Solution<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/eda-statistical-analysis-hypothesis-testing\">EDA - Statistical Analysis - Hypothesis Testing<\/a><\/li>\n<\/ol>","d8d58c06":"<a id = '3.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Collection <\/h2> ","47fb1385":"* **Target variable:** 'Strength'\n* **Predictors (Input varibles):** 'Cement', 'Slag', 'Ash', 'Water', 'Superplastic', 'Coarseagg', 'Fineagg', 'Age'","4ff477fd":"<p style = \"font-size:20px; color: #007580 \"><strong> Ridge and SVR models - Hyperparameter tuning with original features <\/strong><\/p>","4ca1d61b":"#### Transform original data","0c7f4c8a":"### Data Description:\n\nThe actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not scaled). The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).","b1dc3ac9":"**Univariate analysis:**\n* **Cement column - Right skewed distribution -- cement is skewed to higher values**\n* **Slag column - Right skewed distribution -- slag is skewed to higher values and there are two gaussians**\n* **Ash column - Right skewed distribution -- ash is skewed to higher values and there are two gaussians**\n* **Water column - Moderately left skewed distribution**\n* **Superplastic column - Right skewed distribution -- superplastic is skewed to higher values and there are two gaussians**\n* **Coarseagg column - Moderately left skewed distribution**\n* **Fineagg column - Moderately left skewed distribution**\n* **Age column - Right skewed distribution -- age is skewed to higher values and there are five gaussians**","93e1855f":"<p style = \"font-size:20px; color: #007580 \"><strong> Ridge and SVR models with Hyperparameters <\/strong><\/p>","6ae4194b":"#### From the above output, we see that except for the column 'age' all our columns datatype is float64.\n\n#### The data has 8 quantitative input variables and 1 quantitative output variable - Strength","8a0ee63a":"<a id = '2.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Import the necessary libraries <\/h2> ","96eab91b":"**By looking at the above results, RMSE is start increasing from 1 degree polynomial which has 6.77 RMSE and RMSE came down to 5.50 for 2 degree polynomial features. Again from 3 degree polynomial RMSE is starts increasing hence optimal degree of polynomial is 2 degree polynomial.**\n\n**Let's try 2-degree polynomial model on the same data**","d5bcd6d8":"<a id = '4.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. Data Cleaning <\/h2> ","3cfdd202":"<a id = '7.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 7. Model Building and Validation <\/h2> ","26902dbd":"<a id = '7.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.1 Sampling Techniques - Create Training and Test Set <\/strong><\/p> ","0ad0ea74":"<p style = \"font-size:20px; color: #007580 \"><strong> Train and test all models <\/strong><\/p> ","d5e07bef":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","015a5984":"<p style = \"font-size:20px; color: #007580 \"><strong> K Means Clustering <\/strong><\/p>","1550c14b":"#### Heatmap for checking the Correlation","a704465a":"<a id = '5.6'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.6 EDA (Exploratory Data Analysis) Summary <\/strong><\/p> ","744eaa98":"<p style = \"font-size:20px; color: #007580 \"><strong> Build SupportVectorRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, CatBoostRegressor and XGBRegressor models <\/strong><\/p>","47ce402e":"<a id = '4.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.5 Check Outliers after correction <\/strong><\/p> ","87d25412":"* **Looking at the above plot, there are no more outliers in concrete data set**","7bf2cc69":"<a id = '4.7'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.7 Data Cleaning Summary <\/strong><\/p> ","e93b9a3a":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","7d8eed5d":"<a id = '7.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.4 Overall Summary - Before feature selection <\/strong><\/p>\n\nI am able to predict the Concrete compressive strength using few ingradients with below details.\n\nRefer the above table:\n\n1. I have tried with simple linear regression which is overfit model hence I moved on to non-regularized models.\n\n    a. Ridge performs better on both training and test set.\n\n    b. Lasso performs better on training set and poorly on test set.\n2. As mentioned in Multi-variate analysis, there is some interaction between independent features hence I have tried with simple linear regression and non-regularized models (Ridge and Lasso) and all of them turned out to be overfit models.\n\n3. As mentioned in Multi-variate analysis, there are some non-linear(curvy-linear) relatioship within independent features as well as with target variable hence I have tried with polynomial features.\n\n    a. Simple linear regression with polynomial features with degree = 2 performs better on both training and test set with 1% difference.\n\n    b. Ridge and Lasso with polynomial features turned out to be overfit models.\n\n4. I have tried with Support Vector Regressor and it performs better on both training and test set.\n\n5. I have tried with KNeighborsRegressor, DecisionTreeRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor, CatBoostRegressor and XGBRegressor models, sad news is all these models turned out to be overfit models.\n\n6. Best models are as follows:\n\n    a. Linear Regression with Polynomial features - Test accuracy = 86.94% with RMSE = 5.50\n\n    b. Ridge regression with original features - Test accuracy = 80.24% with RMSE = 6.77\n\n    c. SVR with original features - Test accuracy = 80.03% with RMSE = 6.81","eeb0ebd9":"* Here, None of the dimensions are good predictor of target variable.\n* For all the dimensions (variables) every cluster have a similar range of values except in one case.\n* We can see that the body of the cluster are overlapping.\n* So in k means, though, there are clusters in datasets on different dimensions. But we can not see any distinct characteristics of these clusters which tell us to break data into different clusters and build separate models for them.","b8d0b7f3":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n1. [Overview](#1.0)\n2. [Import the necessary libraries](#2.0)\n3. [Data Collection](#3.0)\n4. [Data Cleaning](#4.0)\n\t- [4.1 Check Duplicates](#4.1)\n\t- [4.2 Drop Duplicates](#4.2)\n\t- [4.3 Check Outliers](#4.3)\n\t- [4.4 Working with Outliers: Correcting, Removing](#4.4)\n\t- [4.5 Check Outliers after correction](#4.5)\n\t- [4.6 Check Missing Values](#4.6)\n5. [EDA (Data Analysis and Preparation)](#5.0)\n\t- [5.1 Variable Identification](#5.1)\n\t- [5.2 Univariate Analysis](#5.2)\n\t- [5.3 Study Summary Statistics](#5.3)\n\t- [5.4 Multivariate Analysis](#5.4)\n\t- [5.5 Study Correlation](#5.5)\n\t- [5.6 EDA (Exploratory Data Analysis) Summary](#5.6)\n6. [Feature Engineering](#6.0)\n\t- [6.1 Variable Creation](#6.1)\n7. [Model Building and Validation](#7.0)\n\t- [7.1 Sampling Techniques - Create Training and Test Set](#7.1)\n\t- [7.2 Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help](#7.2)\n\t- [7.3 Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and presenting my findings in terms of the independent attributes and their suitability to predict strength](#7.3)\n\t- [7.4 Overall Summary - Before feature selection](#7.4)\n8. [Feature Selection Methods](#8.0)\n\t- [8.1 Feature Importance](#8.1)\n\t- [8.2 Overall summary - after feature selection](#8.2)\n\t- [8.3 Comparison of with and without feature selection methods](#8.3)\n9. [Optimization](#9.0)\n\t- [9.1 Hyper Parameter Tuning](#9.1)\n\t- [9.2 Bootstrap Sampling - Model performance range at 95% confidence level](#9.2)\n10. [Conclusion](#10.0)","d4c0374b":"* **Looking at the plot above; Stag, Water, Superplastic, Fineagg, Age and Strength columns have outliers and we need to treat those outliers.**","0e851c8d":"**Model Statistical Outputs:**\n\n* R-squared and Adj. R-squared are very close, it is sign that all predictors are relevant to the overall model.\n\n* F-statistic = 240.8 is large value of F-statistic and p-value = 1.64e-194 is very close to 0 and also it is less than 0.05 hence we can reject null hypothesis. That means there is evidence that there is good amount of linear relationship between target variable (Strength) and all predictors.\n\n**Parameters Estimates and the Associated Statistical Tests:**\n\n* By looking into OLS summary coefficients column results, they are same as sklearn linear model coefficients and even intercept is same.\n\n* By looking into OLS summary t-test columns results: so for constant variable ie -0.564, we have a p-value = 0.573 which is greater than 0.05 then we accept the null hypothesis.\n\n* By looking into OLS summary t-test columns results: Cement, Slag, Ash, Water, Coarseagg and Fineagg  and Superplastic are having p-value < 0.05 becuase we are testing t-test at 95% confidence interval, so we reject null hypothesis and accepth alternate hypothesis. That means that there is evidence that these predictors are having good amount of linear relationship with target variable.\n\n* By looking into OLS summary t-test columns results: Coarseagg and Fineagg  and Superplastic are having p-value > 0.05 becuase we are testing t-test at 95% confidence interval, so we accept null hypothesis. That means that there is evidence these predictors are not having good amount of linear relationship with target variable.\n\n* By looking into OLS summary t-test columns results: std err reflects the level of accuracy of the coefficients. std err values are very close to 0 except intercept that means the level of accuracy is high.\n\n**Residul Tests Results:**\n\n* Skew: 0.076, there is small tail to left in the residuals distribution.\n* Kurtosis: 3.285, there is a peak in the residuals distribution.\n* Prob(Omnibus): 0.225, Prob(JB): 0.216 - indicates that p-value > 0.05 meaning it's not siginificant and data is normally distributed.\n* The condition number is large, 1.05e+05. This indicates that some of the features are collinear.","ae82e439":"<p style = \"font-size:20px; color: #007580 \"><strong> Lasso with it's polynomial important features <\/strong><\/p>","660cf1a4":"#### There is no need to worry about preserving the data; it is already a part of the concrete dataset and we can merely remove or drop these rows from your cleaned data","32123db5":"<p style = \"font-size:20px; color: #007580 \"><strong> KNN Regressor <\/strong><\/p>","1a6e8a16":"<a id = '8.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 8.1 Feature Importance <\/strong><\/p> ","0e46a551":"<a id = '4.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.4 Working with Outliers: Correcting, Removing <\/strong><\/p> ","a7f75006":"<a id = '6.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 6.1 Variable Creation <\/strong><\/p> ","1043ae6c":"#### Setting Options","ab5723e6":"<p style = \"font-size:20px; color: #007580 \"><strong> All models - Lasso with it's important features <\/strong><\/p> ","9376aaa3":"<p style = \"font-size:20px; color: #007580 \"><strong> Lasso with it's important features <\/strong><\/p> ","79546a2f":"<a id = '5.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.1 Variable Identification <\/strong><\/p> ","1a64a702":"**As mentioned in above comparisons, we can hypertune the parameters for Ridge and Support Vector Regressor algorithms.**","3beec81a":"<p style = \"font-size:20px; color: #007580 \"><strong> All models with ensemble's important features <\/strong><\/p>","b9a65588":"<a id = '10.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 10. Conclusion <\/h2> ","7ce464b4":"<a id = '7.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 7.2 Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help <\/strong><\/p> ","48f3659e":"<a id = '4.6'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.6 Check Missing Values <\/strong><\/p> ","504be575":"#### Observations:\n\n* **Looking at the Correlation table; 'Cement', 'Water', 'Superplastic' and 'Age' features are influencing the concrete strength.**\n\n\n* **Concrete strength feature is having Moderate Positive Correlation with Cement feature.**\n* **Concrete strength feature is having Low Positive Correlation with Superplastic and Age features**\n* **Concrete strength feature is having Low Positive Correlation with Water features**\n* **Concrete strength feature is having negligible Correlation with Slag, Ash, Coarseagg and Fineagg features**\n\n\n* **Water feature is having Moderate Positive Correlation with Superplastic feature**\n\n\n* **Concrete cement feature is having Low Positive Correlation with Slag and Ash features**\n\n\n* **Concrete fineagg feature is having Low Positive Correlation with Water feature**\n\n* **Concrete ash feature is having Low Positive Correlation with Superplastic feature**","b2ef8bdc":"1. We had 25 duplicate instances in dataset and dropped those duplicates.\n2. We had outliers in 'Water', 'Superplastic', 'Fineagg', 'Age' and 'Strength' column also, handled these outliers by replacing every outlier with upper side of the whisker.\n3. We had outliers in 'Water' column also, handled these outliers by replacing every outlier with lower side of the whisker.\n4. No missing values in dataset.","6f298c78":"#### Pairplot for checking the Correlation","8fd97ead":"<a id = '9.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 9.2 Bootstrap Sampling - Model performance range at 95% confidence level<\/strong><\/p>","2373fc1e":"### Identify opportunities (if any) to create a composite feature, drop a feature etc.","4f0ecb68":"<p style = \"font-size:20px; color: #007580 \"><strong> Shape of the data <\/strong><\/p> ","ed8dcadd":"**By looking at above results, 2-degree polynomial model is performs better on training set and even on test set with 1% difference which shows that it's like to be sweet spot. Henc let's try with non-regularized models.**","0459d4ff":"<p style = \"font-size:20px; color: #007580 \"><strong> GradientBoostingRegressor<\/strong><\/p>","e9301f13":"**Observation: This model performs better on training set and poorly on test set which shows that it's an overfitting and very complex model.**","d32e4a13":"<a id = '9.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 9. Optimization <\/h2> ","902d5cbb":"<a id = '4.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.1 Check Duplicates <\/strong><\/p> ","b20bab2e":"* As mentioned in EDA summary.\n\n**Independent features are influencing concrete strength - 'Cement', 'Water', 'Superplastic' and 'Age'**\n\n**Composite features are influencing concrete strength - cement + slag, cement + ash and water + fineagg. We can create these composite featues because these features are having some relationship within them.** \n\n**Note: Before concluding anything we can try with feature selection methods and then compare the resutls.**","d43f538d":"<p style = \"font-size:20px; color: #007580 \"><strong> All models - Lasso with it's polynomial important features <\/strong><\/p>","9aa1e59b":"**All models with ensemble's important features - observation:**\n\n1. Linear regression - Test accuracy = 79.50% with RMSE = 6.90\n2. Ridge regression - Test accuracy = 79.49% with RMSE = 6.90\n3. SVR - Test accuracy = 79.34% with RMSE = 6.92\n\nRest all models performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.","a86c5a7d":"### Context\n\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.","8abb79af":"**Let's try polynomial model on the same data from 1 to 5 degree polynomial features**","289677b0":"**Lasso with it's polynomial important features - observation: All these models performs better on training set and poorly on test set which shows that it's an overfitting and very complex models.**","58eed2bd":"<br>\n<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Cement Manufacturing Project<\/h2> \n<br>","d2772666":"<p style = \"font-size:20px; color: #007580 \"><strong> Modelling - Linear Regression <\/strong><\/p> ","628c257c":"#### Optimal value of K is 2","1f53b3a1":"#### By looking at above feature importnce from ensemble models:\n**Cement, Slag, Ash, Water, Superplastic, Coarsegg and fineagg are top important features**","8f1d25a1":"<p style = \"font-size:20px; color: #007580 \"><strong> Linear Regression using Statsmodels <\/strong><\/p> ","cd8ba3a5":"1. Except 'Cement', 'Water', 'Superplastic' and 'Age' features, all other features are having very weak relationship with concrete 'Strength' feature and does not account for making statistical decision (of correlation).\n\n2. Concrete Cement feature is having Low Positive Correlation with Slag and Ash features, perhaps we can create additional features like (cement + slag) and (cement + ash) to predict the concrete strength.\n\n3. Concrete Fineagg feature is having Low Positive Correlation with Water feature, perhaps we can create additional features like  (water + fineagg) to predict the concrete strength.\n\n4. Concrete Ash feature is having Low Positive Correlation with Superplastic feature, perhaps we can create additional features like  (ash + Superplastic) to predict the concrete strength.\n\n5. Range of clusters in this dataset is 2 to 6.","8dd03c74":"#### Diagonals Analysis\n\n* If we look at KDE diagonal plots, there are at least 2 Gaussians (2 peaks) in Slag, Ash, Superplastic and Age, even though it's not unsupervised learning but in this dataset there are at least 2 clusters and there may be more.\n\n* Range of clusters in this dataset is 2 to 6.\n\n* The diagonal analysis give same insights as we got from univariate analysis.\n\n#### Off Diagonal Analysis: Relationship between indpendent attributes\n##### Scatter plots\n\n* Cement vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Slag vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Ash vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Water vs other independent attributes: This attribute have negative curvy-linear relationship with Fineagg, Coarseagg and Superplastic, as Water content increases means Fineagg, Coarseagg and Superplastic are reducing. It does not have any significant relationship with other independent atributes.\n\n* Superplastic vs other independent attributes:This attribute have negative linear relationship with water only. It does not have any significant relationship with other independent attributes.\n\n* Coarseagg vs other independent attributes: This attribute does not have any significant relation with other independent features. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\n* Fineagg vs other independent attributes: It has negative linear relationship with water. It does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n\nThe reason why we are doing all this analysis is if we find any kind of dimensions which are very strongly correlated i.e. r value close to 1 or -1 such dimensions are giving same information to your algorithms, its a redundant dimension. So in such cases we may want to keep one and drop the other which we should keep and which we should drop depends on again your domain expertise, which one of the dimension is more prone to errors.I would like to drop that dimension. Or we have a choice to combine these dimensions and create a composite dimension out of it.\n\n\n#### Strength attribute : Relationship between dependent and independent attributes\n\n* Strength vs Cement: It is having curvy-linear relationship with concrete cement and it is good predictor of concrete strength.\n* Strength vs Slag: It is having very weak relationship with concrete slag because there are cloud of points(rectangular shape).\n* Strength vs Ash: It is having weak relationship with concrete ash because there are cloud of points(rectangular shape).\n* Strength vs Water: It is having curvy-linear relationship with water and it is good predictor of concrete strength.\n* Strength vs Superplastic: It is having weak relationship with superplastic because there are cloud of points(ballon shape) and it might be a good predictor of concrete strength..\n* Strength vs Coarseagg: It is having very weak relationship with concrete coarseagg because there are cloud of points(rectangular shape).\n* Strength vs Fineagg: It is having very weak relationship with concrete fineagg because there are cloud of points(rectangular shape).\n* Strength vs Age: It is having curvy-linear relationship with concrete age and it might be a good predictor of concrete strength.\n\n\n* **Finally Cement, Water, Superplastic and Age can be good predictors of concrete strength.**"}}