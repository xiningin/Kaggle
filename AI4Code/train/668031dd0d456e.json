{"cell_type":{"f2c50815":"code","3fc0b3c5":"code","9156436f":"code","003fb0f2":"code","28afa822":"code","6a357a4a":"code","7ca464b0":"code","9cba8acf":"code","262b0e06":"code","047864d5":"code","841cf77d":"code","be75f3d4":"code","11fcf025":"code","9f30c9ad":"code","40fbcb4a":"code","6b23a50d":"code","b5cb83c1":"markdown","38c99511":"markdown","c3c6604c":"markdown","0a7bc373":"markdown","5b653f56":"markdown","6aff1a1a":"markdown","ec3258f3":"markdown","34f090bd":"markdown"},"source":{"f2c50815":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","3fc0b3c5":"data = pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\ndata","9156436f":"data = data.drop('Unnamed: 0', axis=1)","003fb0f2":"print(f\"Cuts:{data['cut'].unique()}\")\nprint(f\"Colors: {data['color'].unique()}\")\nprint(f\"Clarities: {data['clarity'].unique()}\")","28afa822":"# Cut\nfig,ax=plt.subplots(1,2, figsize = (18,6))\nfig.suptitle('Diamonds by Cut', fontsize=20)\ng1=sns.countplot(ax=ax[0],x=\"cut\", data=data,order=['Fair','Good','Very Good','Premium','Ideal'])\ng1.set(xlabel=None)\ng1.tick_params(labelrotation=45)\ng2=sns.boxplot(ax=ax[1],x=\"cut\", y=\"price\", data=data, order=['Fair','Good','Very Good','Premium','Ideal'])\ng2.set(xlabel=None)\ng2.tick_params(labelrotation=45)\n\nplt.show()","6a357a4a":"# Color\nfig,ax=plt.subplots(1,2, figsize = (18,6))\nfig.suptitle('Diamonds by Color', fontsize=20)\ng1=sns.countplot(ax=ax[0],x=\"color\", data=data.sort_values(by=['color'],ascending=False))\ng1.set(xlabel=None)\ng2=sns.boxplot(ax=ax[1],x=\"color\", y=\"price\", data=data.sort_values(by=['color'],ascending=False))\ng2.set(xlabel=None)\n\nplt.show()","7ca464b0":"# Clarity\nfig,ax=plt.subplots(1,2, figsize = (18,6))\nfig.suptitle('Diamonds by Clarity', fontsize=20)\ng1=sns.countplot(ax=ax[0],x=\"clarity\", data=data.sort_values(by=['clarity']))\ng1.set(xlabel=None)\ng1.tick_params(labelrotation=45)\ng2=sns.boxplot(ax=ax[1],x=\"clarity\", y=\"price\", data=data.sort_values(by=['clarity']))\ng2.set(xlabel=None)\ng2.tick_params(labelrotation=45)\nplt.show() ","9cba8acf":"data1 = data.copy()\n\ndata1 = pd.get_dummies(data1)\ndata1","262b0e06":"corr1 = data1.corr(method='pearson')\nplt.figure(figsize=(15,10))\nsns.heatmap(corr1, annot=True, fmt='.2f', cmap='coolwarm')\nplt.show()","047864d5":"X1 = data1.drop('price',axis=1)\ny1 = data['price']\n\nscaler = StandardScaler()\nX1 = scaler.fit_transform(X1)\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2,random_state=0)","841cf77d":"data2 = data.copy()\n\nencoder = LabelEncoder()\n\ncolumns = ['cut', 'color', 'clarity']\n\nfor col in columns:\n    data2[col] = encoder.fit_transform(data2[col])\n\ndata2","be75f3d4":"corr2 = data2.corr(method='pearson')\nplt.figure(figsize=(11,8))\nsns.heatmap(corr2, annot=True, fmt='.2f', cmap='coolwarm')\nplt.show()","11fcf025":"X2 = data2.drop('price',axis=1)\ny2 = data['price']\n\nscaler2 = StandardScaler()\nX2 = scaler2.fit_transform(X2)\n\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2,random_state=0)","9f30c9ad":"std_model = LinearRegression()\nl1_model = Lasso(alpha=1)\nl2_model = Ridge(alpha=1)\ntre_model = DecisionTreeRegressor()\nrandom_model = RandomForestRegressor()","40fbcb4a":"\nstd_1 = std_model.fit(X1_train, y1_train)\nl1_1 = l1_model.fit(X1_train, y1_train)\nl2_1 = l2_model.fit(X1_train, y1_train)\ntree_1 = tre_model.fit(X1_train, y1_train) \nrandom_1 = random_model.fit(X1_train, y1_train)\n\nprint(\"Feature Enginnering 1: get_dummies\")\nprint(f\"---Without regularization: {std_1.score(X1_test, y1_test)}\")\nprint(f\"Lasso (L1) regularization: {l1_1.score(X1_test, y1_test)}\")\nprint(f\"Ridge (L2) regularization: {l2_1.score(X1_test, y1_test)}\")\nprint(f\"Decision Tree: {tree_1.score(X1_test, y1_test)}\")\nprint(f\"Random Florest: {random_1.score(X1_test, y1_test)}\")","6b23a50d":"std_2 = std_model.fit(X2_train, y2_train)\nl1_2 = l1_model.fit(X2_train, y2_train)\nl2_2 = l2_model.fit(X2_train, y2_train)\ntree_2 = tre_model.fit(X2_train, y2_train) \nrandom_2 = random_model.fit(X2_train, y2_train)\n\nprint(\"Feature Enginnering 2: LabelEncoder\")\nprint(f\"---Without regularization: {std_2.score(X2_test, y2_test)}\")\nprint(f\"Lasso (L1) regularization: {l1_2.score(X2_test, y2_test)}\")\nprint(f\"Ridge (L2) regularization: {l2_2.score(X2_test, y2_test)}\")\nprint(f\"Decision Tree: {tree_2.score(X2_test, y2_test)}\")\nprint(f\"Random Florest: {random_2.score(X2_test, y2_test)}\")","b5cb83c1":"## Categorical Data Visualization","38c99511":"**Conclusion: Linear regression models were influenced by the technique used to convert categorical data into numeric ones. The ensemble (tree) models, on the other hand, did not suffer significant influence**","c3c6604c":"## Feature Engineering 1: apply get_dummies","0a7bc373":"Most diamonds are in colour G, E, F, repectively\nMost expensive colour are J, I, respectively, G and H they have similar price.","5b653f56":"## Models Building","6aff1a1a":"Most diamonds in the dataset has Ideal cut, and the Premium cut is most expensive.","ec3258f3":"The most diamonds has SI1 and VS2.\nThe most expensive diamonds are VS1 and VS2 clarity with similar price.","34f090bd":"## Feature Engineering 2: apply LabelEncoder"}}