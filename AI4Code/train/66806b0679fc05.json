{"cell_type":{"9be911c3":"code","fe8267b4":"code","f350db76":"code","7f447c5e":"code","3cb68771":"code","7b4d90bc":"code","333d6f61":"code","ec9feee4":"code","5c8afed4":"code","9084a6b5":"code","a11c0fd6":"code","7111d998":"code","4d1f353b":"code","4c125fdc":"code","4758fa23":"code","1955ec02":"code","63338d6e":"code","794984f9":"code","cfec793d":"code","f60edf4b":"code","19a13ebd":"code","bed42665":"code","d3afec00":"code","c72c84af":"code","8d9293f2":"code","21fd39a7":"code","5224f3dc":"code","0842ad28":"code","dcf6d0c0":"code","fe9369f0":"code","96acdd01":"code","ccfe3378":"code","f83f929f":"code","764c3274":"code","514636db":"markdown","bbd26117":"markdown","f6cf2671":"markdown","571aa951":"markdown","14986604":"markdown","cb7a1913":"markdown","7abfadce":"markdown"},"source":{"9be911c3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format\n\n","fe8267b4":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain_ID = train.Id\ntest_ID = test.Id\n\nn_target = train.SalePrice\n\n_=train.pop('Id')\n_=test.pop('Id')","f350db76":"def show_dist(x):\n    sns.distplot(x, fit=norm)\n    (mu, sigma) = norm.fit(x)\n\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})'.format(mu,sigma)], loc='best')\n    plt.ylabel('Frequency')\n    plt.title('SalePrice distribution')\n\n    fig = plt.figure()\n    res = stats.probplot(x, plot=plt)\n    plt.show()\n    print(\"Skewness : %.2f\" % x.skew())\n    print(\"Kurtosis : %.2f\" % x.kurt())\n    return","7f447c5e":"show_dist(n_target)","3cb68771":"target = np.log(n_target)\nshow_dist(target)","7b4d90bc":"def na_count(df):\n    total = df.isna().sum().sort_values(ascending=False)\n    percent = 100*(total\/df.shape[0])\n    return pd.concat([total, percent], axis=1, keys=['Total NA', '%'])","333d6f61":"na_count(train).head(10)","ec9feee4":"def reg_on_na(ser,target, p=0.25):\n    n_missing = ser.isna().sum()\n    percent = n_missing\/ser.shape[0]\n    if n_missing == 0:\n        print(\"no missing values in :\"+str(ser.name))\n    else:    \n        if percent < p :\n            # missing value index\n            m_id = ser[ser.isna()].index\n            # labels : non-NA values of our missing series\n            Y = np.array(ser.drop(m_id)).reshape(-1,1)\n            # target values (salerice) that we will train to predict missing feature\n            # Single feature data must be reshaped before training\n            X = np.array(target.drop(m_id)).reshape(-1,1)\n            # Missing saleprices upon which we will make prediction\n            Xm = np.array(target[m_id]).reshape(-1,1)\n            reg = LassoCV(cv=5, random_state=0).fit(X,Y)\n            ser[m_id] = reg.predict(Xm)\n        else :\n            print(\"You should drop :\"+str(ser.name))   \n    return ser\n\ndef class_on_na(ser,target, p=0.25):\n    n_missing = ser.isna().sum()\n    percent = n_missing\/ser.shape[0]\n    if n_missing == 0:\n        print(\"no missing values in :\"+str(ser.name))\n    else:    \n        if percent < p :\n            # missing value index\n            m_id = ser[ser.isna()].index\n            # labels : non-NA values of our missing series\n            Y = np.array(ser.drop(m_id)).reshape(-1,1)\n            # target values (salerice) that we will train to predict missing feature\n            # Single feature data must be reshaped before training\n            X = np.array(target.drop(m_id)).reshape(-1,1)\n            # Missing saleprices upon which we will make prediction\n            Xm = np.array(target[m_id]).reshape(-1,1)\n            clas = SVC(gamma=2, C=1).fit(X,Y)\n            ser[m_id] = clas.predict(Xm)\n        else :\n            print(\"You should drop :\"+str(ser.name))   \n    return ser","5c8afed4":"def fill_missing(df, target=target):\n    num_cols = df.select_dtypes([np.number]).columns\n    cat_cols = df.select_dtypes([np.object]).columns\n    dfnum = df[num_cols].copy()\n    dfcat = df[cat_cols].copy()\n    num_cols_miss = dfnum.isna().sum()[dfnum.isna().sum()>0].index\n    cat_cols_miss = dfcat.isna().sum()[dfcat.isna().sum()>0].index\n    df[num_cols_miss] = df[num_cols_miss].apply(lambda x: reg_on_na(x, target))\n    df[cat_cols_miss] = df[cat_cols_miss].apply(lambda x: class_on_na(x, target))\n    return df","9084a6b5":"train = fill_missing(train,target)\n\ntest.Utilities.fillna('AllPub', inplace=True)\ntest = fill_missing(test, test.GrLivArea)\n","a11c0fd6":"def outliers(x, y=n_target, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx\n    ","7111d998":"outs = outliers(train['GrLivArea'], top=5)\ntrain = train.drop(outs)\ntarget = target.drop(outs)\nn_target = n_target.drop(outs)\nntrain = train.shape[0]\nntest = test.shape[0]\nalldata = pd.concat([train, test]).reset_index(drop=True)\n\nalldata.drop(['SalePrice','Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)\nprint(alldata.shape)\n","4d1f353b":"show_dist(target)","4c125fdc":"# All the feature engeneering i've got\nalldata['TotalSF'] = alldata['TotalBsmtSF'] + alldata['1stFlrSF'] + alldata['2ndFlrSF']\nalldata['Area_Qual'] = alldata['TotalSF']*alldata['OverallQual']\n","4758fa23":"numeric_feats = alldata.select_dtypes([np.number]).columns\n\n# Check the skew of all numerical features\nskewed_feats = alldata[numeric_feats].apply(skew).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","1955ec02":"skewness = skewness[abs(skewness) > 0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.10\n_skewed = alldata[numeric_feats].apply(lambda x: boxcox1p(x,lam)).apply(skew).sort_values(ascending=False)\nskewness['boxed'] = pd.Series(_skewed)\nalldata[numeric_feats] = alldata[numeric_feats].apply(lambda x: boxcox1p(x,lam))\nskewness.head(10)","63338d6e":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, Ridge, SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR, LinearSVR \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport time","794984f9":"alldata = pd.get_dummies(alldata)\nalldata = RobustScaler().fit_transform(alldata) \nalldata = PCA(n_components=0.999).fit_transform(alldata) \ntrain = alldata[:ntrain]\ntest = alldata[ntrain:]\ny_train = n_target.values\nprint(train.shape)\nprint(test.shape)","cfec793d":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train)\n    rmse= np.sqrt(-cross_val_score(model, train, target.values, scoring=\"neg_mean_squared_error\", cv = kf))\n    return rmse","f60edf4b":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train)\n        grid_search = GridSearchCV(self.model,param_grid,cv=kf, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","19a13ebd":"pd.options.display.float_format = '{:.4f}'.format\ngrid( Lasso(random_state=1)).grid_get(train,target.values,{'alpha': [0.0004,0.0005,0.0007,0.0009],'max_iter':[10000, 15000]})","bed42665":"# grid(lgb.LGBMRegressor(objective='regression',\n#                               max_bin = 55, bagging_fraction = 0.8,\n#                               bagging_freq = 5, feature_fraction = 0.2319,\n#                               feature_fraction_seed=9, bagging_seed=9,\n#                               min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)).grid_get(train,target.values,{'num_leaves': [5, 10 ,15],\n#                                                                                                                 'learning_rate': [0.05, 0.01, 0.001],\n#                                                                                                                'n_estimators': [500, 720, 900]})","d3afec00":"grid(ElasticNet(random_state=3)).grid_get(train,target.values,{'alpha': [0.0004,0.0005,0.0007,0.001], 'l1_ratio': [0.3, 0.6, 0.9]})","c72c84af":"grid(KernelRidge()).grid_get(train,target.values,{'alpha': [0.1,0.3,0.6,0.9], 'kernel':['linear','polynomial'], 'degree':[2,3,4], 'coef0':[0.01,2.5,5]})","8d9293f2":"grid(Ridge(random_state=5)).grid_get(train,target.values,{'alpha': [5,10,20,30]})","21fd39a7":"grid(SVR()).grid_get(train,target.values,{'gamma': [0.00001,0.00005,0.0009], 'epsilon': [0.001,0.005,0.009]})","5224f3dc":"models = [ Lasso(alpha =0.0005, random_state=1), \n         ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3), \n         KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5), \n         Ridge(alpha=20), \n         SVR(gamma= 0.0009,kernel='rbf',C=13,epsilon=0.005),\n         BayesianRidge(),\n         LinearSVR(),\n         SGDRegressor(max_iter=1000,tol=1e-3)\n         ]\nnames = ['Lasso', 'ElasticNet', 'KernelRidge', 'Ridge', 'SVR', 'BayesianRidge', 'LinearSVR', 'SGDRegressor']","0842ad28":"for name, model in zip(names, models):\n    start = time.time()\n    score = rmsle_cv(model)\n    end = time.time()\n    print(\"{}: {:.6f}, {:.4f} in {:.3f} s\".format(name,score.mean(),score.std(),end-start))","dcf6d0c0":"class AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","fe9369f0":"sel_models = [Lasso(alpha =0.0005, random_state=1, max_iter=10000), \n             ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3), \n             KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5), \n             Ridge(alpha=20), \n             SVR(gamma= 0.0009,kernel='rbf',C=13,epsilon=0.005),\n             BayesianRidge(),\n             LinearSVR(),\n              ]\n\n ","96acdd01":"np.random.seed(42)\nstack = StackingCVRegressor(regressors=sel_models,\n                            meta_regressor=SVR(gamma= 0.0009,kernel='rbf',C=13,epsilon=0.005),\n                            use_features_in_secondary=True)\nstart = time.time()\nscore = rmsle_cv(stack)\nend = time.time()\nprint(\"Stacked : {:.6f}, (+\/-) {:.4f} in {:.3f} s\".format(score.mean(),score.std(),end-start))","ccfe3378":"sel_models = [\n             KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5), \n             Ridge(alpha=20), \n             SVR(gamma= 0.0009,kernel='rbf',C=13,epsilon=0.005),\n             BayesianRidge(),\n             stack\n              ]\n\nweights = [0.2,0.15,0.2,0.15,0.3]\n\nstart = time.time()\nblended = AverageWeight(mod = sel_models ,weight=weights)\nscore = rmsle_cv(blended)\nend = time.time()\nprint(\"Weighted avg: {:.6f}, {:.4f} in {:.3f} s\".format(score.mean(),score.std(),end-start))","f83f929f":"blended.fit(train, target.values)","764c3274":"pred = np.exp(blended.predict(test))\nresults = pd.DataFrame({'Id':test_ID, 'SalePrice':pred})\nq1 = results['SalePrice'].quantile(0.0042)\nq2 = results['SalePrice'].quantile(0.99)\n\nresults['SalePrice'] = results['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nresults['SalePrice'] = results['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nresults.to_csv(\"submission.csv\",index=False)","514636db":"# 4 - Feature engeneering","bbd26117":"# Kernel (V.02) Summary\n\n\n   In this Kernel we wil predict the saleprice based on a quick EDA, trying as much as possible to deal with the features as unknown variables, like we usually encounter in  KAGGLE competitions, and some intermediate regressors manipulation.\n\n  The missing values will be predicted with a regressor for numerical features and classifier for the categorical ones using the Saleprice as training input for the general train data and the most correlated feature (GrivLiv) for the remaining test data.\n  \n  The outliers will be spotted with the sklearn's LOF function that gives an outlying factor for all 2-D datapoints then ploted for visual confirmation.\n  \n  The skewness of numerical columns will be delt with using the BoxCox transformation trying to reduce it as much as possible for better predictions. (The Log transformation is a specific case of the BoxCox : lambda=0).\n  \n  Basic one-hot encoding for categoricals, Standard Scaler and PCA to feed the data to our models.\n  \n  The modeling part will rely on basic sklearn regressors, we will staring by performing grid_search on 5-fold cross validation, do some blending  and finally stack the blended models.\n  \n  That's all folks !  :)\n  \n  \n### Forthcoming improvement ( hoping it will improve though...)\n\n\n\n*   I'm trying to use neural nets , wrap them in sklearn, perform gridcv and blend\/stack, the problem is I achevied once a good rmse score but when i started tweaking the model, I lost it ( going 5-10 times higher) even if I went back to the prior configuration. **WIP** . It will be very helpfull if somebody can help with this issue.\n*   I've tried an Auto-encoder for dimensiality reduction, then concatenate with original data but no improvement.\n*   Hoping the denoising auto-encoder will do better (using gaussian and swap noise). **WIP**\n*  As I skipped a lot of the feature engeneering part, I want to find a way to use a variational aito-encoder to generate sampled features  from the latent space. I steel need to develop that\n\n\n\n\n\n\n\n\n\n\n\n","f6cf2671":"# Content :\n1- Load the data and target analysis\n\n2- Fill missing values with regression\n\n3- Automatic detection of outliers\n\n4- Feature selection\n\n5- Modeling (PCA, Gridsearch, Blending and stacking)","571aa951":"# 3 - Removing outliers","14986604":"# 2 - Missing data","cb7a1913":"# 5 - Modeling","7abfadce":"# 1 - Load the data and target analysis "}}