{"cell_type":{"ee3d644b":"code","ca30da53":"code","53cd8932":"code","cdb59d16":"code","118951f7":"code","d9c5f480":"code","a4b206c6":"code","b1f3f44f":"code","7aa80aaa":"code","27e324bc":"code","e37c3141":"code","3b4d4950":"code","e39ee22e":"code","07b6b4fe":"code","8d2e5144":"code","2fd18bbc":"code","9d23f967":"code","99cf7504":"code","e89f0deb":"code","3c7fe787":"code","cc0003e6":"code","6d224fc6":"markdown","a6cefdfe":"markdown","5e37eea7":"markdown","44421fb4":"markdown","ce6c4b50":"markdown","cca49347":"markdown","ffb639b5":"markdown","7c1ce288":"markdown","81a95776":"markdown"},"source":{"ee3d644b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import model_selection\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ca30da53":"# we use Breast Cancer Dataset\ndf =pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","53cd8932":"###  Independent And dependent features\nX=df.iloc[:,2:]\ny=df.iloc[:,1]","cdb59d16":"X.head()","118951f7":"X=X.dropna(axis=1)","d9c5f480":"X.head()","a4b206c6":"y.value_counts()","b1f3f44f":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4)\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nresult = model.score(X_test, y_test)\nprint(result)","7aa80aaa":"\nfrom sklearn.model_selection import KFold\nmodel=DecisionTreeClassifier()\nkfold_validation=KFold(10)","27e324bc":"\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nresults=cross_val_score(model,X,y,cv=kfold_validation)\nprint(results)\nprint(np.mean(results))","e37c3141":"from sklearn.model_selection import StratifiedKFold\nskfold=StratifiedKFold(n_splits=5)\nmodel=DecisionTreeClassifier()\nscores=cross_val_score(model,X,y,cv=skfold)\nprint(np.mean(scores))","3b4d4950":"scores","e39ee22e":"from sklearn.model_selection import LeaveOneOut\nmodel=DecisionTreeClassifier()\nleave_validation=LeaveOneOut()\nresults=cross_val_score(model,X,y,cv=leave_validation)","07b6b4fe":"\nresults","8d2e5144":"print(np.mean(results))","2fd18bbc":"#from sklearn.model_selection import LeavePOut\n#model=DecisionTreeClassifier()\n#leave_validation=LeavePOut(2)\n#p_results=cross_val_score(model,X,y,cv=leave_validation)","9d23f967":"#p_results","99cf7504":"#print(np.mean(p_results))","e89f0deb":"from sklearn.model_selection import ShuffleSplit\nmodel=DecisionTreeClassifier()\nssplit=ShuffleSplit(n_splits=10,test_size=0.30)\nresults=cross_val_score(model,X,y,cv=ssplit)","3c7fe787":"results","cc0003e6":"\nnp.mean(results)","6d224fc6":"## Leave One Out Cross Validation(LOOCV)\n**In leave on out cross validation each time a single record will take out and use for testing and It will continue untill the iteration completes and the record select randomly on each iteration.Because of these iteration it is one of a most expensive computation cost technique.So it is not used in large dataset.**\n\n![image.png](attachment:b069a447-64a8-461f-b30d-1f38e49516f7.png)","a6cefdfe":"# Cross Validation\n**If you are Begineer than this Notebook will be very helpful for you.\nWhenever we have to train our model it is necessary to divide our dataset into train and validation.for dividing the dataset we use cross validation techniques.Cross validation techniques are not only limited to divide the dataset.It is also plays important role in Hyperparameter Tuning.**\n\n\n\n\n\n\n**Today we will Talk about some famous Cross validation technique:**\n*  **HoldOut Validation Approach- Train And Test Split**\n*  **K Fold Cross Validation**\n*  **Stratified K-fold Cross Validation**\n*  **Leave One Out Cross Validation(LOOCV)**\n*  **Leave P Out Cross Validation(LPOCV)**\n*  **Repeated Random Test-Train Splits**","5e37eea7":"## K Fold Cross Validation\n**In K fold Cross Validation,we divide our dataset into some number of folds.for example we take folds as 5 then it create 5 splits of train and test as in fig below.our model will train on each split.after that we get different different score for each and every split.we will take mean of all the scores for getting final result**\n\n\n![image.png](attachment:467cd427-edb8-4883-a7ff-0c95dd8abfc6.png))","44421fb4":"## Leave P Out Cross Validation(LPOCV)\n**In Leave P Out Cross Validation, P number of Validation records take out for testing the model.On each and every iteration the P records take out randomly.It is one of most expensive in computational cost more than LeaveOneOut Cross validation. For example we take p=3 then the splits like given below in figure**\n\n\n![image.png](attachment:0d274825-a54b-48eb-b641-3f183bde6ccd.png)","ce6c4b50":"**If you like this notebook then please Upvote it.**\n\n**Keep on learning.All the best**","cca49347":"## HoldOut Validation Approach:-\n**It is also known as Train and test split.Mostly Holdout Validation is used in machine learning.In HoldOut Validation we take some percentage(as per dataset size) of dataset for validation purpose.Basically it divides your whole Input Features into train and test(validation). After model building this test or validation is used for testing our model.**\n\n\n**Disadvantage:**\n- **Their is one disadvantage of Holdout Validation approach.whenever we change the value of random_state the output get affected.It may actually lead to overfitting as well as underfitting also.**\n\n![image.png](attachment:d4a4ef1f-b351-4ef9-87cf-06d21c552ce8.png)","ffb639b5":"**We can see there are some nan values so we dont want that nan value disturb our model so we remove them**","7c1ce288":"## Repeated Random Test-Train Splits\n**This technique is a hybrid of traditional train-test splitting and the k-fold cross-validation method. In this technique, we create random splits of the data in the training-test set manner and then repeat the process of splitting and evaluating the algorithm multiple times, just like the cross-validation method.**\n\n\n![image.png](attachment:f88638fd-6966-4847-8f44-61f37b4f7b67.png)","81a95776":"## Stratified K-fold Cross Validation\n**What will be do if you have an Imbalanced Dataset.Their are lots of ways to deal with imbalanced dataset one of them are  Stratified K-fold cross validation.It will be very helpful in that situation when we have imbalanced datset.\nStratified k-fold make folds in a manner that the folds will make with approx equal amount of both classes.**\n\n![image.png](attachment:c0ee5fad-e9d4-4a56-9ae9-83e8f919917e.png)"}}