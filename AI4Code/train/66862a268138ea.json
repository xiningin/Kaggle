{"cell_type":{"073d0832":"code","71dfec95":"code","bad8167e":"code","32b445fe":"code","904d1cc9":"code","f5e7a845":"code","483ddeca":"code","600bcde5":"code","f14c6d49":"code","8d3970a9":"code","6ca59134":"code","39cbee80":"code","2fbec79d":"code","f196c548":"code","e0751da3":"code","3d9d6bbe":"code","b12e4a7e":"markdown","592df067":"markdown","9eb7b130":"markdown","00be399e":"markdown"},"source":{"073d0832":"# PROBLEM STATMENT 1 | LANGUAGE MODELING\n# This is a kaggle kernel notebook using \n# DATASET SOURCE: Amazon Fine Food Review dataset: https:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\n# I have made this notebook public for authentication purpose here: \n\nimport pandas as pd # just to input and output CSV file I\/O (e.g. pd.read_csv)\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","71dfec95":"data = pd.read_csv('\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndata.head()","bad8167e":"data = data[['Text']][:1000000]\ndata.head()","32b445fe":"print(\"We will use only top 5000 reviews to prcess out model\", data.shape)","904d1cc9":"data['Text'][2]","f5e7a845":"sentences = []\nfor review in data['Text']:\n    # split sentences\n    x = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', review)     \n    \n    # marking <eos> as full stop\n    x = ' . '.join(x)\n    \n    #collection of these clean sentences\n    sentences.append(x)\n    \n# create a df column\ndata['separate_sentences'] = sentences\ndata.head()\n","483ddeca":"# Reference: https:\/\/stackoverflow.com\/a\/47091490\/4084039\n# replace slangs with full words\n# e.g. replace [ can't   ->   can not ]\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","600bcde5":"# common function to pre process whole data\n\npreprocessed_reviews = []\n\nfor sentance in data['separate_sentences'].values:\n    \n    #remove url\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    \n    #remove tags \n    sentance = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', sentance)\n    \n    #convert short forms into full forms\n    sentance = decontracted(sentance)\n    \n    #remove words with numbers\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    \n    #remove spacial character EXCEPT <eos> i.e. fullstop (.):\n    sentance = re.sub('[^A-Za-z.]+', ' ', sentance)\n    \n    sentance = ' '.join(e.lower() for e in sentance.split())\n    preprocessed_reviews.append(sentance.strip())","f14c6d49":"print(\"Original Text: \\n\\n\", data['Text'][2])\nprint(\"-\"*50)\nprint(\"In sentence format: \\n\\n\", data['separate_sentences'][2])\nprint(\"-\"*50)\nprint(\"Preprocessed: \\n\\n\",preprocessed_reviews[2])","8d3970a9":"data['separate_sentences'] = preprocessed_reviews\ndata.head()","6ca59134":"# dictionary to catch frequency of each word or say create a bag of words\nbow = {}\n\nfor sentence in data['separate_sentences']:\n    temp = sentence.split()\n    for token in range(len(temp[:-1])):\n        if temp[token]+' '+temp[token+1] in bow.keys():\n            bow[str(temp[token]+' '+temp[token+1])]+=1\n        else:\n            bow[str(temp[token]+' '+temp[token+1])]=1\n            \nprint(\" \\\"FEW\\\" BiGram BoW showing frequencies of bigrams : \\n\\n\")\ntemp=0\nfor key,value in bow.items():\n    print(key, \":\", value)\n    temp+=1\n    if temp >30:\n        break","39cbee80":"# dictionary to catch frequency of each word or say create a bag of words\nbow_unigram = {}\n\nfor sentence in data['separate_sentences']:\n    temp = sentence.split()\n    for token in temp:\n        if token in bow_unigram.keys():\n            bow_unigram[token]+=1\n        else:\n            bow_unigram[token]=1\n\n\nbow_unigram['.'] = 0\nprint(\" \\\"FEW\\\" UniGram BoW showing frequencies of unigrams : \\n\\n\")\ntemp=0\nfor key,value in bow_unigram.items():\n    print(key, \":\", value)\n    temp+=1\n    if temp >30:\n        break","2fbec79d":"#Probability Formulae\n\ndef prob(w_dash,w):    \n    #variable to store COUNT(W,W')\n    count_bigram = 0\n    #check occurence of bigram\n    bigram = str(str(w.strip())+' '+str(w_dash.strip()))\n    if bigram in bow.keys():\n        count_bigram = bow[bigram]\n    \n    #variable to store unigram count\n    count_uni = 0\n    #check ocurences of unigram\n    unigram = str(w.strip())\n    if unigram in bow_unigram.keys():\n        count_uni = bow_unigram[unigram]\n       \n    #make sure denominator is not zero\n    if count_uni:\n        #return probability as mentioned in problem statement formulae.\n        return (count_bigram\/count_uni)\n        #ALSO WHILE IMPLEMENTING NAIVE BAYES, ALPHA (LAPLACE SMOOTHING) IS DONE TO REMOVE THE PROBLEM OF DIVISION BY 0.\n        # This could be understood by the practical case as when \n        # \"IN TEST DATA\" a word is encountered which has not been appeared yet in train data then division by 0 problem will occur\n    return 0 ","f196c548":"#function to find next word by comparing all the possible bigrams and occurences and fethcing the bigram with maximum probability\n\ndef find_next(word):\n    l = [(key,bow[key]) for key in bow.keys() if word.lower()+' ' in key]\n    #var to store max probability for the w_dash being the new word. \n    max_prob = 0\n    #var to store next word\n    next_word = None\n    \n    for each in l:        \n        \n        w, w_dash = each[0].split()\n        \n        #finding probability\n        p = prob(w_dash, w)\n        \n        #only update next word if the new probability is more than the max prob\n        if p>max_prob:\n            max_prob = p\n            next_word = w_dash\n    \n    #return next word\n    return next_word","e0751da3":"#take first word as input from user\na = input()","3d9d6bbe":"#predicted sentence is formed in this variable\npredicted_sentence = \"\"+a\nnext_word = a\n\n#maximum length is set to 10 but if a fullstop is encountered then it will break in mid way...\nwhile(len(predicted_sentence.split())<10):\n    next_word = find_next(next_word)\n    predicted_sentence=predicted_sentence+' '+next_word.strip()\n    print(predicted_sentence)\n    \n    #break if <eos> is encountered\n    if next_word.strip() is '.':\n        break\n    ","b12e4a7e":"## Creating custom Bag of Words (BIGRAM) vectoriser","592df067":"## Creating custom Bag of Words (UNIGRAM) vectoriser","9eb7b130":"Now lets input a word and check ","00be399e":"we can see there are a lot of sentences in a single text row of dataframe. We need to mark the beginning and ending of each sentence with some tags"}}