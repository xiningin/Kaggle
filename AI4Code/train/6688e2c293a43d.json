{"cell_type":{"46e28443":"code","c422bfec":"code","2fbf3c19":"code","2d6b8787":"code","7eb8a42a":"code","27a9da37":"code","40485279":"code","c4d40ba6":"code","06ff6c28":"code","27b969d1":"code","e0e2950b":"code","2dbb0d1d":"code","9aae2f92":"code","affb3461":"code","362cebef":"code","5ffd20fa":"code","9b3f843e":"code","83d56d80":"code","397f1319":"code","307d6d0d":"code","501793e6":"code","11f45f27":"code","0c33bf0f":"code","804c9da6":"code","3f4948d5":"code","cb14584b":"code","e06d5e4f":"code","9d58fad3":"code","26001b09":"code","f6c0ad16":"code","bb721207":"code","18523bcb":"code","8dd69930":"code","cb0dc9a9":"code","d6cc184b":"code","0b50ad34":"code","2626ed5a":"code","f6b254e7":"code","e02f3d38":"code","977709e3":"code","f86ed242":"code","136162c7":"code","a09f5f97":"code","67e6cc15":"code","69e47343":"code","2e6b1d0d":"code","891bac51":"code","9efcd90a":"code","1b06ee92":"code","92b5220f":"code","c0506cd2":"code","66f33870":"code","7d6b346a":"markdown","9337e856":"markdown","8757442b":"markdown","05c52094":"markdown","e056c3db":"markdown","4e634e0c":"markdown","15c62211":"markdown","cb77f6b1":"markdown","dc0a698b":"markdown","2b35a908":"markdown"},"source":{"46e28443":"import catalyst\nfrom catalyst import dl, metrics, utils\ncatalyst.__version__","c422bfec":"import pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain.head()","2fbf3c19":"import torch\nfrom torch.nn import functional as F\n\nclass CustomRunner(dl.Runner):\n    \n    def predict_batch(self, batch):       \n        input_ids = batch['input_ids'].T.to(self.device)\n        #token_type_ids = batch['token_type_ids'].to(self.device)\n        attention_mask = batch['attention_mask'].T.to(self.device)\n        return self.model(input_ids, attention_mask)  #, token_type_ids\n    \n    def on_loader_start(self, runner):\n        super().on_loader_start(runner)\n        self.meters = {\n            key: metrics.AdditiveValueMetric(compute_on_call=False)\n            for key in [\"loss\", \"mae\"]\n        }\n\n    def handle_batch(self, batch):\n        \n        input_ids = batch['input_ids']\n        #token_type_ids = batch['token_type_ids']\n        attention_mask = batch['attention_mask']\n        \n        y = batch['label'].view(-1, 1).float()\n\n        y_pred = self.model(input_ids, attention_mask).view(-1, 1).float() #, token_type_ids\n        \n        self.batch = {'logits': y_pred, 'target': y}\n        \n        loss = F.mse_loss(y_pred.view(-1), y.view(-1))\n\n        self.batch_metrics.update({\"loss\": loss**0.5, \"mae\": F.l1_loss(y_pred, y)})\n        for key in [\"loss\", \"mae\"]:\n            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)\n\n        if self.is_train_loader:\n            loss.backward(retain_graph=True)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n    \n    def on_loader_end(self, runner):\n        for key in [\"loss\", \"mae\"]:\n            self.loader_metrics[key] = self.meters[key].compute()[0]\n        super().on_loader_end(runner)","2d6b8787":"from transformers import AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\n\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\nclass TextData(Dataset):\n    def __init__(self, text, labels, max_len=250):\n        self.text = text\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        tokenized_text = tokenizer(\n            self.text[item].replace('\\n', ''), max_length=self.max_len, truncation=True, \n            return_attention_mask=True, return_token_type_ids=True)\n        \n        padding_length = self.max_len - len(tokenized_text['input_ids'])\n        \n        return {\n            'input_ids':torch.tensor(tokenized_text['input_ids'] + ([0] * padding_length), dtype=torch.long),\n            #'token_type_ids':torch.tensor(tokenized_text['token_type_ids'] + ([0] * padding_length), dtype=torch.long),\n            'attention_mask':torch.tensor(tokenized_text['attention_mask'] + ([0] * padding_length), dtype=torch.long),\n            'label':torch.tensor(self.labels[item], dtype=torch.double),\n        }","7eb8a42a":"from transformers import AutoModel\n\nclass RegressionModel(torch.nn.Module):\n    \n    def __init__(self):\n        super(RegressionModel, self).__init__()\n        self.bert = AutoModel.from_pretrained('roberta-base', output_hidden_states=False)\n        self.dropout = torch.nn.Dropout(0.1)\n        self.regressor = torch.nn.Linear(768, 1)\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            #token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]#.last_hidden_state[-1, :]#[1]\n        \n        logits = self.regressor(self.dropout(sequence_output))\n\n        return logits ","27a9da37":"import numpy as np\nimport torch\n\ntrain_dataset = TextData(train.loc[:2000, 'excerpt'].values, train.loc[:2000, 'target'].values)\nvalid_dataset = TextData(train.loc[2000:, 'excerpt'].values, train.loc[2000:, 'target'].values)\n\nloaders = {\n    \"train\": DataLoader(train_dataset, shuffle=True, batch_size=8), \n    \"valid\": DataLoader(valid_dataset, batch_size=8)\n}\n\nmodel = RegressionModel()","40485279":"model","c4d40ba6":"for param in model.bert.embeddings.parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[0].parameters():\n    param.requires_grad = False \n\nfor param in model.bert.encoder.layer[1].parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[2].parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[3].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[4].parameters():\n    param.requires_grad = False\n\nfor param in model.bert.encoder.layer[5].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[6].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[7].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[8].parameters():\n    param.requires_grad = False\n    \nfor param in model.bert.encoder.layer[9].parameters():\n    param.requires_grad = False","06ff6c28":"criterion = torch.nn.MSELoss()\n#optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)\n\noptimizer = torch.optim.AdamW([\n    {'params': model.bert.parameters(), 'lr': 0.00001},\n    {'params': model.regressor.parameters(), 'lr': 0.001}\n])\n\nrunner = CustomRunner()\n\nrunner.train(\n  model=model, \n  optimizer=optimizer, \n  loaders=loaders, \n  logdir=\"logs\",\n  valid_loader=\"valid\",\n  valid_metric=\"loss\",\n  num_epochs=10,\n  minimize_valid_metric=True,\n  verbose=True,\n  timeit=False,\n)","27b969d1":"inp = next(iter(loaders['valid']))\n\ninp.pop('label')\n\nprint(inp)","e0e2950b":"runner.model.to('cpu')(**inp)","2dbb0d1d":"preds = []\n\nfor prediction in runner.predict_loader(loader=loaders['valid']):\n    preds.extend(prediction.detach().cpu())\n    \npreds = np.array(preds) \npreds","9aae2f92":"runner.predict_batch(next(iter(loaders['valid'])))","affb3461":"torch.save(runner.model.state_dict(), \"n_model.pth\")","362cebef":"torch.save(utils.quantize_model(model=runner.model).state_dict(), \"q_model.pth\")","5ffd20fa":"!ls -al #--block_size=1M","9b3f843e":"features_batch = next(iter(loaders[\"valid\"]))#.to(runner.device)\n\n#features_batch = [features_batch['input_ids'].to(runner.device),\n#    features_batch['attention_mask'].to(runner.device)]\n\nfeatures_batch = {\n    'input_ids': features_batch['input_ids'].to(runner.device),\n    'attention_mask': features_batch['attention_mask'].to(runner.device)\n}","83d56d80":"features_batch","397f1319":"model.load_state_dict(utils.get_averaged_weights_by_path_mask(logdir=\".\/logs\", path_mask=\"*.pth\"))","307d6d0d":"%%timeit\n\nrunner.model(**features_batch)","501793e6":"traced_model = utils.trace_model(model=runner.model.cuda(), batch=[features_batch['input_ids'], features_batch['attention_mask']])","11f45f27":"torch.jit.save(traced_model, \"traced_roberta.pt\")","0c33bf0f":"loaded_model = torch.jit.load(\"traced_roberta.pt\")\nloaded_model.eval()","804c9da6":"loaded_model(**features_batch)","3f4948d5":"runner.model.cuda().eval()","cb14584b":"%%timeit\n\nrunner.model(**features_batch)","e06d5e4f":"%%timeit\n\nloaded_model(**features_batch)","9d58fad3":"list(runner.model.bert.encoder.layer[0].parameters())[0].dtype","26001b09":"model_quantized = utils.quantize_model(model=runner.model.cpu())\nmodel_quantized","f6c0ad16":"#list(runner.model.bert.encoder.layer[0].parameters())","bb721207":"#list(model_quantized.bert.encoder.layer[0].parameters())#[0].dtype","18523bcb":"%%timeit\n\nmodel_quantized.cpu()(features_batch['input_ids'].cpu(), features_batch['attention_mask'].cpu())","8dd69930":"model_prune = runner.model","cb0dc9a9":"utils.prune_model(model = model_prune, pruning_fn=\"l1_unstructured\", amount=0.8)","d6cc184b":"model_prune","0b50ad34":"torch.save(model_prune.state_dict(), \"pruned_model.pth\")","2626ed5a":"features_batch","f6b254e7":"runner.model(**features_batch)","e02f3d38":"model_prune.cuda()(**features_batch)","977709e3":"loaded_model(**features_batch)","f86ed242":"%%timeit\n\nrunner.model(**features_batch)","136162c7":"features_batch","a09f5f97":"torch.onnx.export(runner.model,               # model being run\n                  (features_batch['input_ids'], features_batch['attention_mask']),                         # model input (or a tuple for multiple inputs)\n                  \"roberta.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=11,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input_ids', 'attention_mask'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={'input_ids' : {0 : 'batch_size'},    # variable length axes\n                                'attention_mask' : {0 : 'batch_size'},\n                                'output' : {0 : 'batch_size'}})","67e6cc15":"import onnx\n\nonnx_model = onnx.load(\"roberta.onnx\")\nonnx.checker.check_model(onnx_model)","69e47343":"onnx_model ","2e6b1d0d":"!pip install onnxruntime","891bac51":"import onnxruntime\n\nort_session = onnxruntime.InferenceSession(\"roberta.onnx\")","9efcd90a":"def to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()","1b06ee92":"ort_session.get_inputs()[0].name, ort_session.get_inputs()[1].name","92b5220f":"to_numpy(features_batch['input_ids'])","c0506cd2":"ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(features_batch['input_ids']), \n             ort_session.get_inputs()[1].name: to_numpy(features_batch['attention_mask'])}\nort_outs = ort_session.run(None, ort_inputs)","66f33870":"ort_outs","7d6b346a":"### model pruning","9337e856":"### ONNX","8757442b":"## Run","05c52094":"# Catalyst","e056c3db":"### model tracing","4e634e0c":"### stochastic weight averaging","15c62211":"## Optimization","cb77f6b1":"## Class","dc0a698b":"## Dataset","2b35a908":"### quantization"}}