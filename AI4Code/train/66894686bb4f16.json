{"cell_type":{"2acac082":"code","903ef9f0":"code","0c236206":"code","685149e7":"code","2bbf2447":"code","96d57df0":"code","990a1a72":"code","df945155":"code","b8da8ef3":"code","c4f527ce":"code","b403542b":"code","917f8934":"code","ff6828ff":"code","19dd813c":"code","ab7b204a":"code","9b00a349":"code","6d3ba055":"code","70896cbf":"code","cd547f2c":"code","4f50e53f":"code","289f37ca":"code","efc356d9":"code","8fa79b9c":"markdown","1d929a45":"markdown","85c535b6":"markdown","6b0b3130":"markdown","d7fa5900":"markdown","b315acf3":"markdown","972c304b":"markdown","51a702a0":"markdown","c5b4951e":"markdown","7b6a1dc1":"markdown","d8d4d1ed":"markdown","c65087de":"markdown","65179bb5":"markdown","efc398c1":"markdown","a20a0bac":"markdown","ec9259a5":"markdown","e99a67ed":"markdown","152cccc4":"markdown","e831d582":"markdown","fa10a90e":"markdown","6014456d":"markdown","6f30033a":"markdown","fb687c89":"markdown","0c93f2e6":"markdown","eaafb45e":"markdown","a16b95d3":"markdown","135f268f":"markdown"},"source":{"2acac082":"from IPython.display import Image\nImage(\"\/kaggle\/input\/image1\/Annotation 2020-05-17 135651.png\")","903ef9f0":"import warnings\nimport math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools as itr\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n\nplt.style.use('ggplot')","0c236206":"INPUT_DIR = '..\/input\/m5-forecasting-accuracy'\ncal = pd.read_csv(f'{INPUT_DIR}\/calendar.csv')\nstv = pd.read_csv(f'{INPUT_DIR}\/sales_train_validation.csv')\n\nstv.head()","685149e7":"#select all the columns that contain d_\nd_cols = [c for c in stv.columns if 'd_' in c] # sales data columns\n\nstv.loc[stv['id'] == 'FOODS_2_317_TX_3_validation'][d_cols].T \\\n    .plot(figsize=(20, 5),\n          title='FOODS_2_317_TX_3 sales by \"d\" number')\nplt.legend('')\nplt.show()","2bbf2447":"all_data = stv[d_cols] \\\n    .sum(axis=0) \\\n    .T \\\n    .reset_index()\n\nall_data.columns = ['d','sales']\n\nprint(all_data)","96d57df0":"all_data.plot(figsize=(20, 5),\n          title='unit sales of all products by \"d\" number')\nplt.legend('')\nplt.show()","990a1a72":"cal.head()","df945155":"print(cal['event_name_1'].unique())\nprint(cal['event_name_2'].unique())","b8da8ef3":"# Merge calendar on our items' data\n\nall_data_merged = all_data.merge(cal, how='left', validate='1:1')\nall_data_merged.head()","c4f527ce":"#y is the sales data we are gonna work with from now on\ny = all_data_merged.set_index('date')['sales']\n\n#Detect days that have either event_1 or event_2\nplaces = all_data_merged.loc[~(all_data_merged['event_name_1'].isna()) | ~(all_data_merged['event_name_2'].isna())]['d']\n\nchange = list(all_data_merged.d.isin(list(places)))\nfor i in range(len(change)):\n    if change[i] == True:\n        y.iloc[i] = (y.iloc[i-1] + y.iloc[i+1]) \/ 2\n\ny.plot(figsize=(20, 5),\n          title='cleaned unit sales of all products by day')\nplt.legend('')\nplt.show()","b403542b":"y.iloc[-30:,].plot(figsize=(20, 5),\n          title='cleaned unit sales of all products by week')\nplt.legend('')\nplt.show()","917f8934":"y_month = y.reset_index()\ny_month['date'] = pd.to_datetime(y_month['date'])\ny_month = y_month.set_index('date')\ny_month = y_month.resample('W').mean()\n\ny_month.iloc[-49:,].plot(figsize=(20, 5),\n          title='cleaned unit sales of all products by week')\nplt.legend('')\nplt.show()","ff6828ff":"y_year = y.reset_index()\ny_year['date'] = pd.to_datetime(y_year['date'])\ny_year = y_year.set_index('date')\ny_year = y_year.resample('M').mean()\n\ny_year.iloc[-40:,].plot(figsize=(20, 5),\n          title='cleaned unit sales of all products by month')\nplt.legend('')\nplt.show()","19dd813c":"y_sales = y.reset_index().drop(['date'],axis=1)\n\n#Time scale\npredic1 = range(1913)\n\n#Applying the Fourier series to the time scale\npredic_annual_cos = list(map(lambda x: math.cos(2*math.pi*x\/365), predic1))\npredic_annual_sin = list(map(lambda x: math.sin(2*math.pi*x\/365), predic1))\n\npredic_month_cos = list(map(lambda x: math.cos(2*math.pi*x\/30), predic1))\npredic_month_sin = list(map(lambda x: math.sin(2*math.pi*x\/30), predic1))\n\npredic_week_cos = list(map(lambda x: math.cos(2*math.pi*x\/7), predic1))\npredic_week_sin = list(map(lambda x: math.sin(2*math.pi*x\/7), predic1))\n\n#assembling the regressors\nreg = pd.DataFrame(list(zip(predic1, predic_annual_cos, predic_annual_sin, predic_month_cos, predic_month_sin, predic_week_cos, predic_week_sin)), \n               columns =['predic1', 'predic_annual_cos', 'predic_annual_sin', 'predic_month_cos', 'predic_month_sin', 'predic_week_cos', 'predic_week_sin']) \n\n#Model\nmodel = LinearRegression().fit(reg, y_sales)\n\n#The estimated parameters\nr2 = model.score(reg, y_sales)\nprint('coefficient of determination:', r2)","ab7b204a":"trend = model.intercept_ + model.coef_[0][0]*np.array(predic1)\nseas_annual = model.coef_[0][1]*np.array(predic_annual_cos) + model.coef_[0][2]*np.array(predic_annual_sin)\nseas_month = model.coef_[0][3]*np.array(predic_month_cos) + model.coef_[0][4]*np.array(predic_month_sin)\nseas_week = model.coef_[0][5]*np.array(predic_week_cos) + model.coef_[0][6]*np.array(predic_week_sin)\n\ntrend_seas = trend + seas_annual + seas_month + seas_week\n\nax = pd.DataFrame(trend_seas, columns=['trend+seasonalities']).plot(figsize=(20,8))\ny_sales.plot(ax=ax,alpha=0.7)","9b00a349":"y_adjusted = np.array(list(y_sales['sales'])) - trend_seas\ny_adjusted = pd.DataFrame(y_adjusted, columns=['noise'])\ny_adjusted.plot(figsize=(20,8))","6d3ba055":"y_train = y_adjusted.iloc[:-28,]\ny_test = y_adjusted.iloc[-28:,]","70896cbf":"# Define the p and q parameters to take any value between 0 and 4, d between 0 and 1\np = q = range(0, 6)\nd = [0,1,2]\n\n# Generate all different combinations of p, q and q triplets\npdq = list(itr.product(p, d, q))\n\n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [[0,0,0,0]]\n\nwarnings.filterwarnings(\"ignore\") # specify to ignore warning messages\nminimum = 500000 #initialize the minimum AIC variable with a high enough value\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y_train,\n                                            order=param,\n                                            seasonal_order=param_seasonal)\n\n            results = mod.fit()\n            \n            if results.aic < minimum:\n                minimum = results.aic\n                param_ideal = param\n                \n\n            print('ARIMA{} - AIC:{}'.format(param, results.aic))\n            \n        except:\n            print('none')\n            continue\n            \nprint('And the result is ARIMA{} - AIC:{}'.format(param_ideal, minimum))","cd547f2c":"mod = sm.tsa.statespace.SARIMAX(y_train,\n                                order=(3, 1, 5),\n                                seasonal_order=(0, 0, 0, 0))\nresults = mod.fit()\n\nprint(results.summary().tables[1])\n\nresults.plot_diagnostics(figsize=(15, 12))\nplt.show()","4f50e53f":"pred = results.get_forecast(steps=28)\n\nax = y_test.plot(figsize=(20, 10))\n\npd.DataFrame(pred.predicted_mean, columns=['forecast']).plot(ax=ax)\n\nax.fill_between(range(1885,1913), pred.conf_int()['lower noise'], pred.conf_int()['upper noise'], color='b', alpha=.04)\n\nplt.legend()\nplt.show()","289f37ca":"ax = pd.DataFrame(np.array(list(y_test['noise']))+trend_seas[1885:],index = range(1885,1913), columns=['sales']).plot(figsize=(20, 10))\n\npd.DataFrame(pred.predicted_mean+trend_seas[1885:], columns=['forecast']).plot(ax=ax)\n\nax.fill_between(range(1885,1913), pred.conf_int()['lower noise']+trend_seas[1885:], pred.conf_int()['upper noise']+trend_seas[1885:], color='b', alpha=.04)\n\nplt.legend()\nplt.show","efc356d9":"pred_all = results.predict(start=0, end=1912)\n\nax = y_sales.plot(figsize=(20, 10))\n\npd.DataFrame(pred_all + trend_seas, columns=['model']).plot(ax=ax)\n\nplt.legend()\nplt.show()","8fa79b9c":"The first remark we can claim is the numerous days where 0 items were sold. Furthermore, this serie doesn't show any clear trend or seasonality thus for an illustrative purpose it's not the best to use in our approach. \n\nOur approach is based on a simple univariate model ignoring all the clear dependencies with daily prices, snap days and event days.\n\n### Unit sales of all products, aggregated for all stores\/states","1d929a45":"Let's visualize the real forecasts now addind the trend and the seasonality we have extracted.","85c535b6":"A SARIMAX model then will be used on this final dataset to predict the noise in the sales serie before addind up the trand and seasonalities again for real forecasts.\n\n### Training and test datasets\n\nWe will test our model on 28 steps ahead just like the submission instructions.","6b0b3130":"# Some visualizations\n\nThe notebook doesn't contain a full explanatory analysis, but still some visualisations are essential so that we understand the nature of the data we are dealing with.\n### Importing necessary libraries","d7fa5900":"This is a sample of the historic sales data in the sales_train_validation dataset.\n\n* For each product in each store and department, we are given the number of sales for days d_1 to d_1913.\n* days d_1914 - d_1941 represents the validation rows which we will predict in stage 1.\n* days d_1942 - d_1969 represents the evaluation rows which we will predict for the final submission.","b315acf3":"This notebook was made in collaboration with: [Adil El Hakouni](https:\/\/www.kaggle.com\/adilelhakouni)\n\nHi everyone and welcome to our very first notebook ^^\n\nFirst of all I would like to highlight that there are many time series that we can consider in this competition, see the image below:\n","972c304b":"# Introduction","51a702a0":"Which is visually not bad. Now that we have extracted the serie's trend and seasonality, let's extract them from the original serie so that only noise is left to estimate.","c5b4951e":"### Replacing the events' values","7b6a1dc1":"The result is satisfying and our model is precisely following the original dataset with some important gaps at the peaks. This is also confirmed by the next plot of both the training and the test periods:","d8d4d1ed":"Bingo! There is a clear increase in sales during summer comparing to winter (disregarding the trend).\n\n# Removing trend and seasonalities\n\nStarting from an additive model hypothesis [1], the idea is fitting a linear regression model over the fourier series corresponding to the different seasonalities that we have  [2]. \n\nIndeed, we will be able to extract the trend and the seasonality components leaving only the noise to predict with the SARIMAX model, see the formulas below:\n\n$$y_{sales} = Constant + Trend + Seasonality + Noise \\space \\space \\space [1]$$","c65087de":"# Cleaning the data\n\nWe will consider that only the events' days are outliers (This is a strong hypothesis and is far from being an exhaustive selection of outliers since snap days and low prices' days might be part of it too), and we will replace their values with the mean value of the previous and the next day.\n\n### Merging the stv dataset and the calendar dataset","65179bb5":"### The data","efc398c1":"### Some external resources:\n\n* [Linear Regression in Python](https:\/\/realpython.com\/linear-regression-in-python\/) by Mirko Stojiljkovic\n\n* [A Gentle Introduction to SARIMA for Time Series Forecasting in Python](https:\/\/machinelearningmastery.com\/sarima-for-time-series-forecasting-in-python\/) by Jason Brownlee","a20a0bac":"This is a month of data and it shows a clear sinusoidal attitude of 4 periods which corresponds to the weekly seasonality.\n\n### Monthly seasonality","ec9259a5":"This time serie is more adapted to the SARIMAX model we want to use.\n\nHowever, the outliers might be nuisible to our model as they are clearly distinquished from the plot (like the christmas day where the stores are closed).\n\ncalendar.csv provide us with the data about the events that have occured during both the training period and the test period, let's look at a sample: ","e99a67ed":"Here again we can assume that every month the number of sales has a peak at the beginning of the month and then a trough, which demonstrates the existence of a monthly seasonality.\n\n### Yearly seasonality","152cccc4":"* There are many levels of aggregation for a total of 42,840 time series.\n* The nature of these time series is not the same as we will see in the first part of the notebook, hence I decided to treat the up level series with the less number of zero values with a **univariate** (that does not include external variables) statistical model as the title demonstrates.\n* The following notebook is a general guide to multi-seasonal time series forecasting using **SARIMAX** model.","e831d582":"### Evaluating the model\n\nNow that we have set all the model's hyperparameters, let's evaluate its performance.","fa10a90e":"# SARIMAX model\n\nThe Seasonal Autoregressive Integrated Moving Average, or SARIMA, model is an approach for modeling univariate time series data that may contain trend and seasonal components. This model has hyperparameters that control the nature of the model performed for the series:\n* order: A tuple p, d, and q parameters for the modeling of the trend\n* seasonal order: A tuple of P, D, Q, and s parameters for the modeling of the seasonality\n\n*p*, *d* and *q* stand for the number of times or steps to consider for the Autoregressive, the differenciation and the moving average operators, although *s* is the period of the seasonality we want to include.\n\n### Tuning the model\n\nIn our case, since we have already excluded the trend and the seasonalities, the parameter P, D, Q and s will be set to zero (I could have used an ARIMA model, I'm just used to the SARIMAX one).\n\nIn order to decide of the adequate values of the remaining hyperparameters, a grid search is done with the following code (The selection criterion is the [Akaike information criterion (AIC)](https:\/\/en.wikipedia.org\/wiki\/Akaike_information_criterion)):","6014456d":"# Conclusion\n\nWith all the strong hypothesis made throughout the notebook, the final forecast are very good and describe accurately the serie, tracks of improvement to this method could be:\n\n* Detecting more periods to the seasonality and include more Fourier series to the linear regression model\n* Use the seasonal components of the SARIMAX model if any was left after the extraction phase\n* Include the external variables available in order to boost the accuracy\n\nPlease feel free to share your opinion about my notebook or any inconsistency in the approach.\nThank you for reading.","6f30033a":"The standardized residuals are the error between the model's predictions and the actual observed values, two criterion need to be verified for the adequacy of our SARIMAX model: \n* The residuals should follow a Gaussian distribution centered on zero (which is almost the case from the Histogram plus estimated density plot and the Q-Q plot of theoretical quantiles)\n* There should be no correlations between the residuals which would mean that there is still significant information to use in computing forecasts (which is verified from the correlogram)\n\n### Testing the model","fb687c89":"Here are the many event days that exist:","0c93f2e6":"$$y_{sales} = b_{0} + b_{1}t + b_{2}\\cos(\\frac{2\\pi t}{365}) + b_{3}\\sin(\\frac{2\\pi t}{365}) + b_{4}\\cos(\\frac{2\\pi t}{30}) + b_{5}\\sin(\\frac{2\\pi t}{30}) + b_{6}\\cos(\\frac{2\\pi t}{7}) + b_{7}\\sin(\\frac{2\\pi t}{7}) + \\epsilon \\space \\space \\space[2]$$\n\n\nThe performance of the linear regression model will be evaluated using the **coefficient of determination**, denoted as *R\u00b2*. it tells us which amount of variation in y_sales can be explained by the dependence on the periodic functions we used as regressors.","eaafb45e":"# Selecting seasonalities\n\nIt is clear that this final dataset will have multiple seasonalities. It is also logical that the sales would follow a pattern in a period of a week (people will have the tendency to purchase more on weekends), of a month (depends on which time of the month the majority of the population get their salary in) and also a year (summer is never like winter when it comes to purchasing).\n\nThe detection of the many seasonalities is a very important step in the deployment of our model, let's make sure that the frequencies that we have mentionned really exist.\n\n### weekly seasonality","a16b95d3":"### Unit sales for a random product in a random store ","135f268f":"The more *R\u00b2* is close to 1 the more our model is accurate. Here with almost 70% of the variance explained by our model, we are sure that the seasonalities we included are relevant to the serie.\n\nLet's extract the trend and the seasonalities estimated and visualize them:"}}