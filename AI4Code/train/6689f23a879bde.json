{"cell_type":{"849ab88c":"code","22e62013":"code","9d581405":"code","0a38b44e":"code","666f1593":"code","58b082bd":"code","b16a1585":"code","821f3fb9":"code","919b5f16":"code","7be1be8c":"code","4ad7b991":"code","f5274ec6":"code","f9d6573a":"code","84b25061":"code","fbdd6c44":"code","c0cf6893":"code","8a2772bf":"code","2026ec15":"markdown","c24ac883":"markdown","78274c02":"markdown","f1af19a8":"markdown","737016a0":"markdown","f1a96fd3":"markdown","366b1c92":"markdown","f5150358":"markdown","b416ad5f":"markdown","7ef99647":"markdown"},"source":{"849ab88c":"import tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\nfrom sklearn.utils import class_weight\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n#!pip install imbalanced-learn\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\nfrom tensorflow.keras.losses import CategoricalCrossentropy\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","22e62013":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\ntrain.head()","9d581405":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n# counts each type of Class\nsorted(train['target'].value_counts())","0a38b44e":"for i in range(50):\n        #mean, std = train[f'feature_{i}'].mean(), train[f'feature_{i}'].std()\n        #train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : (x-mean)\/std)\n        max = train[f'feature_{i}'].max()\n        train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : x\/max)\n        \ntrain.head()","666f1593":"# transform target column into four columns, one for each class\n\nlabel_dict = {val:idx for idx, val in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(label_dict)\n\ntarget = train['target']\ntrain.drop(['target'], inplace=True, axis=1)","58b082bd":"train = train.values\ntarget = target.values\ntarget = to_categorical(target)","b16a1585":"oversample = SMOTE()\ntrainS, targetS = oversample.fit_resample(train, target)\nnp.sum(targetS, axis = 0)","821f3fb9":"X_train, X_val, y_train, y_val = train_test_split(trainS, targetS, test_size = 0.20, \n                                                  random_state = 2021, stratify = targetS)","919b5f16":"num_features = 50\nnum_classes = 4","7be1be8c":"model = Sequential([\n        Dense(528, input_dim = num_features, kernel_initializer='normal', activation='relu'),\n        Dropout(0.3),\n        BatchNormalization(),\n        Dense(256, activation='relu'),\n        Dropout(0.3),\n        BatchNormalization(),\n        Dense(128, activation='relu'),\n        Dropout(0.2),\n        BatchNormalization(),\n        Dense(num_classes, activation = 'softmax')\n    ])\n\nmodel.summary()","4ad7b991":"model.compile(loss = CategoricalCrossentropy(label_smoothing = 0.001),\n              optimizer = keras.optimizers.Adam(), \n              metrics = 'categorical_accuracy')","f5274ec6":"history = model.fit(X_train, y_train,\n            batch_size = 512, epochs = 60, verbose = 2,\n            validation_data = (X_val, y_val))","f9d6573a":"score = model.evaluate(X_val, y_val, verbose = 0)\nscore = model.evaluate(train, target, verbose = 0)\nprint('Val loss: {}%'.format(score[0]))\nprint('Val score: {}%'.format(score[1] * 100))\nprint(\"MLP Error: %.2f%%\" % (100*(1 - score[1])))","84b25061":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.legend(['train', 'test'], loc='best')\nplt.show()","fbdd6c44":"# Method 2\npred = model.predict(train).argmax(axis=1)\nfig, ax = plt.subplots(figsize=(9, 9))\nsns.heatmap(confusion_matrix(target.argmax(axis=1), pred), cmap ='Blues', \n            annot = True, cbar = False, fmt ='d', square = True, linewidth = 0.4, ax = ax)\n\nax.set_xlabel('Pred', fontweight='bold')\nax.set_ylabel('True', fontweight='bold')\n\nplt.show()","c0cf6893":"for i in range(50):\n        #mean, std = test[f'feature_{i}'].mean(), test[f'feature_{i}'].std()\n        #test[f'feature_{i}'] = test[f'feature_{i}'].apply(lambda x : (x-mean)\/std)\n        max = test[f'feature_{i}'].max()\n        test[f'feature_{i}'] = test[f'feature_{i}'].apply(lambda x : x\/max)\n        \ntest.head()","8a2772bf":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = model.predict(test)\n\nsample_submission.to_csv('my_submission.csv',index = False)\nsample_submission.head(20)","2026ec15":"### Evaluate","c24ac883":"## Simple Keras Pipeline\n\n- EDA : https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda","78274c02":"The structure of the model can be changed freely, and the model is an MLP model using only Dense, Batchnormalization, Dropout.","f1af19a8":"## Result visualization","737016a0":"### Compile","f1a96fd3":"### Confusion Matrix","366b1c92":"## Model (Keras)","f5150358":"## Output","b416ad5f":"## Normalization","7ef99647":"## Split Data"}}