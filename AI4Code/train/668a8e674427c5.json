{"cell_type":{"abff0fce":"code","a0de3ccf":"code","2f13f29b":"code","fc2d9a45":"code","be25ee7f":"code","f94b4a79":"code","72d190a3":"code","c17974bf":"code","ec89ed72":"code","2ea5ec4a":"code","7a65e1c3":"code","f898100b":"code","39e003db":"code","bc200220":"code","86b7aed4":"code","dacafed2":"code","4487be00":"code","7eff60a4":"markdown","7dfb7dbb":"markdown","f58e1a64":"markdown","301078ad":"markdown","cd37f901":"markdown","e1c07dc3":"markdown","d967a8e1":"markdown","142b794f":"markdown","a5ac1f59":"markdown","b98cd6ae":"markdown","d4c15a44":"markdown","3701e361":"markdown","4688d4e7":"markdown","9c09f82f":"markdown","f7cbe8c3":"markdown","50f15569":"markdown"},"source":{"abff0fce":"import numpy as np # lineer cebir\nimport pandas as pd # veri i\u015fleme\nimport matplotlib.pyplot as plt #g\u00f6rselle\u015ftirme\nfrom sklearn.model_selection import train_test_split","a0de3ccf":"data = pd.read_csv('..\/input\/framingham-heart-study-dataset\/framingham.csv')","2f13f29b":"data.info()","fc2d9a45":"data.drop(['education','cigsPerDay','BPMeds','totChol','heartRate','glucose','BMI'],axis=1,inplace=True)\ndata.head(10) # ilk 10 \u00f6rne\u011fi g\u00f6ster","be25ee7f":"y = data.TenYearCHD.values\nx_data = data.drop(['TenYearCHD'], axis=1)","f94b4a79":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values # values: numppy array'e \u00e7evirmek i\u00e7in\nx.head()","72d190a3":"print(data.info())","c17974bf":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","ec89ed72":"# sat\u0131r ve s\u00fctunlar\u0131n yerlerini de\u011fi\u015ftiriyorum\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape) #14 tane feature, 3392 tane sample var\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","2ea5ec4a":"def initialize_weights_and_bias(dimension): # dimension: ka\u00e7 tane feature varsa o kadar weights olu\u015ftur\n    w = np.full((dimension,1),0.01) # weigths'in t\u00fcm de\u011ferlerine 0.01 verdik\n    b = 0.0 \n    return w,b","7a65e1c3":"def sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","f898100b":"print(sigmoid(0), sigmoid(10))","39e003db":"def forward_back_propagation(w, b, x_train, y_train):\n    # forward propagation\n    z = np.dot(w.T, x_train) + b # matris \u00e7arp\u0131m\u0131 i\u00e7in w'in transpozu al\u0131nd\u0131\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head) #hata fonksiyonu form\u00fcl\u00fc\n    cost = (np.sum(loss)) \/ x_train.shape[1] #hata fonksiyonlar\u0131n\u0131 topla, x_train.shape[1] ile scale et\n    \n    # back propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T))) \/ x_train.shape[1] #weight de\u011ferlerinin t\u00fcrevini al\n    derivative_bias = np.sum(y_head-y_train) \/ x_train.shape[1] #bias de\u011ferlerinin t\u00fcrevini al\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias} #parametreleri depolamak i\u00e7in (dictionary)\n    \n    return cost, gradients #gradients: weight'in cost'a g\u00f6re t\u00fcrevi","bc200220":"#parametrelerin g\u00fcncellenmesi\ndef update(w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    #number_of_iteration kadar parametreleri g\u00fcncelle\n    for i in range(number_of_iteration):\n        #forward ve back propagation ile cost ve gradients'leri bulal\u0131m\n        cost, gradients = forward_back_propagation(w, b, x_train, y_train)\n        cost_list.append(cost) #t\u00fcm cost'lar\u0131 depola\n        #g\u00fcncelleme\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0: \n            cost_list2.append(cost) \n            index.append(i)\n            print(\"G\u00fcncellemeden sonra cost %i: %f\" %(i, cost))\n            \n    #weights ve bias'\u0131 parameters i\u00e7erisinde depola\n    parameters = {\"weight\": w,\"bias\": b}\n        \n    #g\u00f6rselle\u015ftirme\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation=\"vertical\")\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","86b7aed4":"#tahmin\ndef predict(w, b, x_test):\n    #Forward Propagation i\u00e7in x_test giri\u015f de\u011ferimiz\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    \n    #z 0.5'ten b\u00fcy\u00fckse tahmin sonucu 1 (y_head = 1)\n    #z 0.5'ten k\u00fc\u00e7\u00fckse tahmin sonucu 0 (y_head = 0)\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction\n    ","dacafed2":"#logistic regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    #de\u011fi\u015fkenleri olu\u015ftur\n    dimension = x_train.shape[0] #3392 tane\n    w,b = initialize_weights_and_bias(dimension)\n    \n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    \n    #train\/test errors g\u00f6ster\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n\nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, num_iterations = 200)","4487be00":"#Sklearn ile LR\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nlr.fit(x_train.T, y_train.T) #sample & feature\nprint(\"test accuracy: {}\".format(lr.score(x_test.T, y_test.T)))","7eff60a4":"Sigmoid fonksiyonunu test edelim.","7dfb7dbb":"# Train Test Split<a id=\"3\"><\/a>\nVeri setimizi e\u011fitim ve test verileri olarak ikiye b\u00f6l\u00fcyoruz. \n\n","f58e1a64":"# Normalization<a id=\"2\"><\/a>\nNormalizasyonun amac\u0131 \u00f6rneklerdeki t\u00fcm de\u011ferleri 0 ve 1 say\u0131lar\u0131 aras\u0131na alarak birbirleri ile aralar\u0131ndaki fark\u0131 kapatmak b\u00f6ylece \u00e7ok y\u00fcksek bir de\u011fer \u00e7ok d\u00fc\u015f\u00fck bir de\u011feri ihmal ettirmemi\u015f olacakt\u0131r.","301078ad":"# Parametre De\u011ferlerini Girme<a id=\"4\"><\/a>\n\nModeli ilk olu\u015fturdu\u011fumuzda parametre de\u011ferlerini biz belirliyoruz.","cd37f901":"Veri setimizde 4240 tane \u00f6rnek var, bu \u00f6rnekler DataFrame i\u00e7erisinde bulunuyor. 15 tane feature ve bir tane de class'\u0131m\u0131z var. \n\nclass = TenYearCHD\n\n7 adet feature float de\u011ferli oldu\u011fu i\u00e7in bunlar\u0131 data'dan \u00e7\u0131karal\u0131m.","e1c07dc3":"# Implementing Logistic Regression<a id=\"9\"><\/a>\n\nLojistik Regresyon Uygulamas\u0131nda\n\ne\u011fitimi yapacak bir x_train\n\nbunu kar\u015f\u0131la\u015ft\u0131rma yapabilece\u011fimiz y_train\n\nx_test data's\u0131yla olu\u015fturaca\u011f\u0131m\u0131z Y_prediction\n\nx_text data's\u0131n\u0131n etiketlerinin tutuldu\u011fu y_test\n\ntahminin do\u011frulu\u011funa bakmak i\u00e7in Y_prediction ile y_test'in kar\u015f\u0131la\u015ft\u0131r\u0131lmas\u0131na ihtiyac\u0131m\u0131z var.\n\nlearning_rate; \u00f6\u011frenme h\u0131z\u0131n\u0131n belirlenmesi\n\nnumber_of_iterations; Forward ve Back Propagation ka\u00e7 defa olacak.\n","d967a8e1":"# SkLearn K\u00fct\u00fcphanesi ile Lojistik Regresyon<a id=\"10\"><\/a>\nSklearn veya Scikit-learn; regresyon, k\u00fcmeleme veya s\u0131n\u0131fland\u0131rma i\u015flemleri yapabilmek i\u00e7in kullan\u0131lan Makine \u00d6\u011frenmesi k\u00fct\u00fchanelerinden bir tanesidir.","142b794f":"# \u0130\u00c7\u0130NDEK\u0130LER\n* [Veri Seti Hakk\u0131nda](#1)\n* [Normalization](#2)\n* [Train Test Split](#3)\n* [Parametre De\u011ferlerini Girme](#4)\n* [Sigmoid Fonksiyonu](#5)\n* [Forward - Back Propagation](#6)\n* [Parametrelerin G\u00fcncellenmesi](#7)\n* [Prediction](#8)\n* [Implementing Logistic Regression](#9)\n* [SkLearn K\u00fct\u00fcphanesi ile Lojistik Regresyon](#10)\n\n# G\u0130R\u0130\u015e\n\nMakine \u00f6\u011freniminde 3 farkl\u0131 \u00f6\u011frenim \u00e7e\u015fidi bulunmaktad\u0131r.\n- Supervised Learning (G\u00f6zetimli \u00d6\u011frenme)\n- Unsupervised Learning (G\u00f6zetimsiz \u00d6\u011frenme)\n- Reinforcement Learning (Peki\u015ftirmeli \u00d6\u011frenme)\n\nG\u00f6zetimli \u00d6\u011frenme: Giri\u015f verilerinden (etiketlenmi\u015f veri) \u00e7\u0131k\u0131\u015f verileri \u00fcreten bir makine \u00f6\u011frenmesi tekni\u011fidir. E\u011fitim verisi girdilerden ve \u00e7\u0131kt\u0131lardan olu\u015fur, bu verilerden bir fonksiyon \u00fcretir. Bu fonksiyonu **classification** veya **regression** algoritmalar\u0131 ile belirleyebiliriz. Biz bu \u00e7al\u0131\u015fmada s\u0131n\u0131fland\u0131rma algoritmalar\u0131ndan biri olan Lojistik Regresyonu inceleyece\u011fiz.\n\n**Logistic Regression**\n* Makine \u00d6\u011frenmesi algoritmalar\u0131ndan bir tanesidir ayn\u0131 zamanda Derin \u00d6\u011frenmenin en basit sinir a\u011flar\u0131 modellerindendir, genelde Derin \u00d6\u011frenmeye Giri\u015fte Lojistik Regresyon anlat\u0131l\u0131r.\n* \u0130kili s\u0131n\u0131fland\u0131rma (0 yada 1, kedi ya da k\u00f6pek, var ya da yok) ile \u00e7al\u0131\u015fmaktad\u0131r.\n\n![res1.jpg](attachment:res1.jpg)\n\nLojistik Regresyonun nas\u0131l \u00e7al\u0131\u015ft\u0131\u011f\u0131n\u0131 yukar\u0131daki grafik \u00fczerinden anlatmaya \u00e7al\u0131\u015fal\u0131m.\n* Elimizde 4096 pixel boyutuna sahip g\u00f6rseller ve bu g\u00f6rsellerde de farkl\u0131 \u00e7izimlerde 0 ve 1 de\u011ferleri olsun. \n* Her bir pixel bizim giri\u015f de\u011ferlerimizi olu\u015fturuyor, bunlar px1, px2 ... px4096'ya kadar gidiyor.\n* Her bir giri\u015f de\u011ferini weights de\u011ferleri ile \u00e7arp\u0131yoruz. \n    * (px1*w1 + px2*w2 + ... px4096*w4096)\n* Elde etti\u011fimiz de\u011feri bias ile topluyoruz ve bir z de\u011feri elde ediyoruz. \n    * z = (px1*w1 + px2+w2 + ... px4096*w4096) + b\n    * Weights: A\u011f\u0131rl\u0131k veya her pikselin katsay\u0131s\u0131, Bias: Kesme de\u011feri\n* Elde edilen z de\u011ferini Sigmoid Fonksiyonuna tabi tutuyoruz. Sigmoid Fonksiyonu bize 0 ile 1 aras\u0131nda de\u011ferler \u00fcretir.\n* Art\u0131k Sigmoid i\u015flemide bitti elimizde model taraf\u0131ndan tahmin (predict) edilen bir de\u011fer var. Buna y_head diyoruz.\n* E\u011fer y_head 0.5'ten b\u00fcy\u00fckse sonu\u00e7 1'dir de\u011filse 0'd\u0131r.\n* Biz modele 0 ve 1'leri tan\u0131mas\u0131n\u0131 isterken bize yanl\u0131\u015f tahminler d\u00f6nd\u00fcrd\u00fc\u011f\u00fcnde bir loss function (hata oran\u0131) elde ediyoruz.\n* T\u00fcm loss function'lar\u0131 toplad\u0131\u011f\u0131m\u0131zda ise cost(maliyet) de\u011ferini elde ediyoruz.\n* \u0130lk ad\u0131mdan ba\u015flayarak son ad\u0131ma kadar ki i\u015flemlere Forward Propagation diyoruz.\n* Tamam, \u015fimdi modeli olu\u015fturduk ama \u00e7ok fazla hatam\u0131z var. Neden? \u00c7\u00fcnk\u00fc ba\u015fta ilk parametre (weights ve bias) de\u011ferlerini biz atad\u0131k ve cost de\u011feri sa\u00e7ma veya y\u00fcksek bir de\u011fer \u00e7\u0131kt\u0131. \n* Cost de\u011ferini minimuma indirgemeliyiz ki model en iyi performans\u0131yla \u00e7al\u0131\u015fs\u0131n. Bunun i\u00e7in ne yap\u0131yoruz? Back Propagation.\n* Geriye yay\u0131l\u0131m algoritmas\u0131 ile art\u0131k cost de\u011ferine g\u00f6re weights ve bias'lar belirleniyor. Sonra ayn\u0131 i\u015flemleri tekrar yap\u0131yoruz. Ta ki cost de\u011feri minimum de\u011fere ula\u015ft\u0131\u011f\u0131nda, bunuda kendini tekrar etmeye ba\u015flad\u0131\u011f\u0131nda anl\u0131yoruz yani 0'a yakla\u015ft\u0131\u011f\u0131nda.","a5ac1f59":"# Veri Seti Hakk\u0131nda<a id=\"1\"><\/a>\nBu ara\u015ft\u0131rma, kalp hastal\u0131\u011f\u0131n\u0131n risk fakt\u00f6rlerini saptamay\u0131 ve lojistik regresyon kullanarak genel riski tahmin etmeyi ama\u00e7lamaktad\u0131r. \n\n**De\u011fi\u015fkenler (Variables):**\n\nHer \u00f6zellik potansiyel bir risk fakt\u00f6r\u00fcd\u00fcr. Hem demografik hem davran\u0131\u015fsal hem de t\u0131bbi risk fakt\u00f6rleri vard\u0131r.\n\n**Demografik:**\nsex: erkek veya kad\u0131n; (Nominal)\n\nage: hastan\u0131n ya\u015f\u0131; (Continuous)\n\n**Davran\u0131\u015fsal:**\n\ncurrentSmoker: hastan\u0131n halen sigara i\u00e7ip i\u00e7medi\u011fi (Nominal)\n\ncigsPerDay: Ki\u015finin ortalama bir g\u00fcnde i\u00e7ti\u011fi sigara say\u0131s\u0131. (Continuous)\n\n**T\u0131bbi Ge\u00e7mi\u015f:**\n\nBPMeds: hastan\u0131n tansiyon ilac\u0131 kullan\u0131p kullanmad\u0131\u011f\u0131 (Nominal)\n\nprevalentStroke: hastan\u0131n daha \u00f6nce inme ge\u00e7irip ge\u00e7irmedi\u011fi (Nominal)\n\nprevalentHyp: hastan\u0131n hipertansif olup olmad\u0131\u011f\u0131 (Nominal)\n\ndiabetes: hastan\u0131n diyabeti olup olmad\u0131\u011f\u0131 (Nominal)\n\n**Medikal (G\u00fcncel):**\n\ntotChol: toplam kolesterol seviyesi (Continuous)\n\nsysBP: sistolik kan bas\u0131nc\u0131 (Continuous)\n\ndiaBP: diyastolik kan bas\u0131nc\u0131 (Continuous)\n\nBMI: V\u00fccut Kitle \u0130ndeksi (Continuous)\n\nheartRate: kalp at\u0131\u015f h\u0131z\u0131 (Continuous)\n\nglucose: glikoz seviyesi (Continuous)\n\n**Predict (\u0130stenen Hedef):**\n\n10 y\u0131ll\u0131k koroner kalp hastal\u0131\u011f\u0131 riski KKH (binary: \"1\": \"Evet\", \"0\": \"Hay\u0131r\" anlam\u0131na gelir).\n\n*Nominal: As\u0131l anlam\u0131 yerine say\u0131sal de\u011ferli ifade kullan\u0131m\u0131. Mesela erkek yerine 1 kad\u0131n yerine 0 \u00f6rne\u011fi.*\n\n*Continuous: Olas\u0131 de\u011ferlerin ifade edilmesine s\u00fcrekli diyoruz.*","b98cd6ae":"# Parametrelerin G\u00fcncellenmesi<a id=\"7\"><\/a>\n\nModelin e\u011fitimi demek weight ve bias'lar\u0131n g\u00fcncellenmesi demektir. G\u00fcncelleme giri\u015f de\u011ferlerimiz \u00fczerinden yap\u0131ld\u0131\u011f\u0131 i\u00e7in x_train ve y_train'i de al\u0131yoruz.\n\n**weights'in g\u00fcncellenmesi (ayn\u0131s\u0131 bias i\u00e7inde ge\u00e7erli)**\n![res7.jpg](attachment:res7.jpg)\nweight = weight - learning_rate * (weights'in cost'a g\u00f6re t\u00fcrevi)\n\nw = w - learning_rate * gradients[\"derivative_weight\"]\n\n\n\n**Hiperparametreler**\n* **learning_rate**\n\u00d6\u011frenme h\u0131z\u0131 veya ad\u0131m b\u00fcy\u00fckl\u00fc\u011f\u00fc olarak ifade edebiliriz. learning_rate kullan\u0131rken se\u00e7ilirken \u00e7ok k\u00fc\u00e7\u00fck olursa model yava\u015f \u00f6\u011frenir \u00e7ok y\u00fcksek olursa istenilen noktaya var\u0131lamaz.\n\n* **number_of_iteration**\nKa\u00e7 kez Forward ve Back Probagation yapaca\u011f\u0131m\u0131z\u0131 deneyerek belirliyoruz. Her Forward ve Back Probagation a\u015famas\u0131nda bir number_of_iteration ger\u00e7ekle\u015fir.","d4c15a44":"**x ve y eksenlerimizi belirleyelim.**","3701e361":"# Prediction<a id=\"8\"><\/a>\n\npredict metodumuzu olu\u015fturaca\u011f\u0131z. Bunun i\u00e7in parametrelerimizi ve tahmin edece\u011fimiz veriyi al\u0131yoruz, bunlar neler?\n\nweights, bias, x_test.","4688d4e7":"**Veri setimizin neler i\u00e7erdi\u011fine bakal\u0131m.**","9c09f82f":"# Forward - Back Propagation<a id=\"6\"><\/a>\n\n**Loss Function**\n![res5.jpg](attachment:res5.jpg)\nx_train.shape[1]: 3392","f7cbe8c3":"y: etiketlenen veriler, x: \u00f6zelliklerin bulundu\u011fu veriler\n\nVeriyi e\u011fitim i\u00e7in %80, test i\u00e7inde %20 olarak b\u00f6ld\u00fck\n\nx_test ve y_test ile modelin do\u011frulu\u011funu test edece\u011fiz\n\nModel her seferinde farkl\u0131 de\u011ferler olu\u015fturmamas\u0131, tutarl\u0131l\u0131\u011f\u0131n bozulmamas\u0131 i\u00e7in random_state kullan\u0131yoruz b\u00f6ylece modelin ba\u015far\u0131 sonucu de\u011fi\u015fkenlik g\u00f6stermeyecektir.","50f15569":"# Sigmoid Fonksiyonu<a id=\"5\"><\/a>\nSigmoid fonksiyonuna hangi say\u0131 girerse girsin bize 0 ve 1 aras\u0131nda \u00e7\u0131kt\u0131lar \u00fcretir.\n![res4.png](attachment:res4.png)"}}