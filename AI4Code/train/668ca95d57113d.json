{"cell_type":{"5ba966d1":"code","f82341e9":"code","2cf3397e":"code","c9ce1121":"code","cda812ab":"code","8759b2b5":"code","933ad4bd":"code","edf8d28c":"code","3d2f8b4a":"code","d4199af7":"code","77431294":"code","0a353255":"code","5b13d0b6":"code","080777f8":"code","725fcad4":"code","80cfff13":"code","915c1fb0":"code","557fee4e":"code","2ed398ea":"code","bba80d42":"code","64d16592":"code","51cf3812":"code","b3fab1fa":"code","92155bd6":"code","8c6c8914":"code","e0a3831d":"code","b39a076c":"code","08ce4ac4":"code","775093ea":"code","722346f9":"code","981e25ec":"code","ad11358a":"code","e7d800e5":"code","eeee6bf5":"code","37ee3ae6":"code","21ca1c4a":"code","31239b4a":"code","3ac82ce9":"code","84655060":"code","f1ce62ab":"code","6b73a99a":"code","b30d18f6":"code","6fe984ba":"code","d0c15ae8":"code","2e5c5a9d":"code","7bf49e61":"code","307ada0e":"code","ac8ad478":"code","c26ce773":"code","baec95cf":"code","d46463fc":"code","8ebfd59b":"code","89e4d534":"code","d215eb39":"code","c3ba7cf1":"code","1fff9eee":"code","5e8fc653":"code","2aade444":"code","ee8ffd8d":"code","070d2883":"markdown","b533c908":"markdown","6cfa1f61":"markdown"},"source":{"5ba966d1":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\n# Standard plotly imports\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\n","f82341e9":"# Read train and test data with pd.read_csv():\ntrain_id= pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntest_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ntrain_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")","2cf3397e":"train_id.head()\n","c9ce1121":"test_id.head()","cda812ab":"train_tr.head()","8759b2b5":"test_tr.head()","933ad4bd":"train_id.info()","edf8d28c":"test_id.info()","3d2f8b4a":"train_tr.head()\n","d4199af7":"train_tr.info()","77431294":"test_tr.info()","0a353255":"def data_und_cat(data,df):\n    target=data[\"isFraud\"]\n    \n    print(\"describtion of data = \",df.describe())\n    print(\"value counts= \",df.value_counts())\n    print(\"total uniq values= \",df.value_counts().count())\n    print(\"total missing values = \",df.isnull().sum())\n    print(\"% missing values = \",df.isnull().sum()\/df.value_counts().sum())\n    df.value_counts().plot.barh().set_title(\"Class Frequencies of Variable\");\n    return data    \n    \n        \n        \n        \n    \n   \n","5b13d0b6":"def data_und_num(data,df):\n    target=data[\"isFraud\"]\n    print(\"describtion of data = \",df.describe())\n    print(\"total missing values = \",df.isnull().sum())\n    print(\"% missing values = \",df.isnull().sum()\/df.value_counts().sum())\n    return data    ","080777f8":"missing_values_count = train_tr.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_tr.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","725fcad4":"missing_values_count = train_id.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_id.shape)\ntotal_missing = missing_values_count.sum()\nprint(\"total_missing = \",missing_values_count.sum())\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","80cfff13":"train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntrain.head()","915c1fb0":"test=pd.merge(test_tr, test_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntest.head()","557fee4e":"train['Trans_min_mean'] = (train['TransactionAmt'] - train['TransactionAmt'].mean())\ntrain['Trans_min_std'] = train['Trans_min_mean'] \/ train['TransactionAmt'].std()\ntest['Trans_min_mean'] = test['TransactionAmt'] - test['TransactionAmt'].mean()\ntest['Trans_min_std'] = test['Trans_min_mean'] \/ test['TransactionAmt'].std()\n\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\ntest['id_02_to_mean_card1'] = test['id-02'] \/ test.groupby(['card1'])['id-02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id-02'] \/ test.groupby(['card4'])['id-02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id-02'] \/ test.groupby(['card1'])['id-02'].transform('std')\ntest['id_02_to_std_card4'] = test['id-02'] \/ test.groupby(['card4'])['id-02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\n\n\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['TransactionAmt'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt'] = np.log(test['TransactionAmt'])","2ed398ea":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","bba80d42":"del train_id, train_tr, test_id, test_tr","64d16592":"Y = train[\"isFraud\"]\ntrain_TransactionID= train[\"TransactionID\"]\ntest_TransactionID=test[\"TransactionID\"]\ntrain = train.drop([\"TransactionID\"], axis=1)\ntest=test.drop([\"TransactionID\"],axis=1)","51cf3812":"train_cat = train.select_dtypes(include=['object'])\ntrain_cat_columns=train_cat.columns\ntrain_cat_columns","b3fab1fa":"test_cat = test.select_dtypes(include=['object'])\ntest_cat_columns=test_cat.columns\ndel test_cat, train_cat","92155bd6":"train[train_cat_columns]=train[train_cat_columns].astype('category')\ntest[test_cat_columns]=test[test_cat_columns].astype(\"category\")","8c6c8914":"from sklearn import preprocessing\nfor i in train_cat_columns: \n    lbe=preprocessing.LabelEncoder()\n    train[i]=lbe.fit_transform(train[i].astype(str))","e0a3831d":"for i in test_cat_columns:    \n    test[i]=lbe.fit_transform(test[i].astype(str))","b39a076c":"#train.fillna(-1,inplace=True)\n#test.fillna(-1,inplace=True)\n#train=train.dropna(axis=1)\n#test=test.dropna(axis=1)","08ce4ac4":"train.head()","775093ea":"#test[i].value_counts().sum()","722346f9":"#train_columns=train.columns\n#train_columns=train_columns.drop(\"isFraud\")\n#test.columns=train_columns\n#test.columns\n#for i in train_cat_columns:\n    #if test[i].value_counts()==train[i].value_counts():\n           # test = pd.get_dummies(test, columns = [i])\n            #train=pd.get_dummies(train, columns = [i])\n            \n\n            \n        \n    ","981e25ec":"for i in train:\n    train[i] = train[i].fillna((train[i].min() - 2))","ad11358a":"train.isnull().sum().sum()","e7d800e5":"correlat=train.corrwith( train['isFraud'], method= 'spearman')\ncorrelat1=pd.DataFrame(correlat,columns=[\"corr\"])\ncorrelat1.reset_index()\ncor_features=correlat1.loc[(correlat1.loc[:,\"corr\"] > 0.07)|(correlat1.loc[:,\"corr\"]<-0.07)]\ncor_features=cor_features.T\ncor_features=cor_features.columns\ntrain=train[cor_features]\ncor_features=cor_features.drop(\"isFraud\")\ntest=test[cor_features]","eeee6bf5":"test=test[cor_features]","37ee3ae6":"train = train.drop(['isFraud'], axis=1)\ntrain.shape","21ca1c4a":"#from sklearn.preprocessing import StandardScaler\n#train = StandardScaler().fit_transform(train)\n#train","31239b4a":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","3ac82ce9":"%%time\ntrain = reduce_mem_usage2(train)\ntest = reduce_mem_usage2(test)","84655060":"#from sklearn.decomposition import PCA\n#optimum bilese sayisi\n#pca = PCA().fit(train)\n#plt.plot(np.cumsum(pca.explained_variance_ratio_))\n#plt.xlabel(\"Bile\u015fen Say\u0131s\u0131n\u0131\")\n#plt.ylabel(\"K\u00fcm\u00fclatif Varyans Oran\u0131\");","f1ce62ab":"#final\n#pca = PCA(n_components = 200)\n#train = pca.fit_transform(train)\n#test = pca.fit_transform(test)","6b73a99a":"#train = pd.DataFrame(data = train)\n#test = pd.DataFrame(data = test)","b30d18f6":"#pca.explained_variance_ratio_.sum()","6fe984ba":"train.head()","d0c15ae8":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","2e5c5a9d":"names = [\"LogisticRegression\",\"GBM\",\"LightGBM\"]\n    \n    \nclassifiers = [LogisticRegression(), GradientBoostingClassifier(),LGBMClassifier()]","7bf49e61":"def base_models(train):\n    \n  \n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n   \n    #Y = train[\"isFraud\"]\n    #X = train.drop(['isFraud'], axis=1)\n\n    X_train, X_test, y_train, y_test = train_test_split(train, Y, \n                                                    test_size = 0.20, \n                                                    random_state = 42)\n    \n    #results = []\n    \n    names = [\"LogisticRegression\",\"GBM\",\"LightGBM\"]\n    \n    \n    classifiers = [LogisticRegression(), GradientBoostingClassifier(),LGBMClassifier()]\n    \n    \n    for name, clf in zip(names, classifiers):\n\n        model = clf.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        acc = accuracy_score(y_test, y_pred)\n        msg = \"%s: %f\" % (name, acc)\n        print(msg)","307ada0e":"base_models(train)","ac8ad478":"#Y = train[\"isFraud\"]\n#X = train.drop(['isFraud'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(train, Y, \n                                                    test_size = 0.20, \n                                                    random_state = 42)","c26ce773":"\nfrom lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier(\n          num_leaves= 256,\n          min_child_samples= 79,\n          objective= 'binary',\n          max_depth= 13,\n          learning_rate= 0.03,\n          boosting_type= \"gbdt\",\n          subsample_freq= 3,\n          subsample= 0.9,\n          bagging_seed= 11,\n          metric= 'auc',\n          verbosity= -1,\n          reg_alpha= 0.3,\n          reg_lambda= 0.3,\n          colsample_bytree= 0.9).fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\naccuracy_score(y_test, y_pred)\n","baec95cf":"print(lgbm_model.feature_importances_)","d46463fc":"%%time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgbm_model.feature_importances_,X_train.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(30, 80))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","8ebfd59b":"ids = test_TransactionID\npredictions = lgbm_model.predict(test)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ \"TransactionID\" : ids, \"isFraud\": predictions })\noutput.to_csv('submission_son.csv', index=False)","89e4d534":"columns=pd.DataFrame(train.columns)","d215eb39":"feature_imp.to_excel(\"output.xlsx\")","c3ba7cf1":"columns.to_excel(\"columns.xlsx\")","1fff9eee":"feature_imp.head()","5e8fc653":"columns.columns=[\"Feature\"]\ncolumns.head()","2aade444":"proje=pd.merge(columns,feature_imp, on=\"Feature\",how= \"left\")","ee8ffd8d":"proje.to_excel(\"columns.xlsx\")","070d2883":"*  Missing Values in train_tr DataFrame. % 41 percent data are missing ","b533c908":"# Missing Values","6cfa1f61":"Missing Values in train_id DataFrame. % 35 percent data are missing "}}