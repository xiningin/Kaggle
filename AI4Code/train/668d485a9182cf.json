{"cell_type":{"f8f1cd60":"code","5706d222":"code","50862eae":"code","0297b7b5":"code","af3eb20f":"code","d6490a6f":"code","357c08ce":"code","0a69710f":"code","b34ce892":"code","03ed606a":"code","afb34b4e":"code","3e4221c9":"code","ef74dc1b":"code","d95a0782":"code","379f34ea":"code","65285605":"code","82d42683":"code","a0a69788":"code","2653672f":"code","fb52fcf8":"code","566bbda3":"code","eaeea62d":"code","b23a3224":"code","21a861e0":"code","b4870920":"code","d648bec6":"code","a9106a38":"code","d9f65737":"code","35c64431":"code","8a2cd24d":"code","13fd5d90":"code","a68e6d6b":"code","f52b5e7b":"code","94396723":"code","dd0a04f5":"code","6e88aa8e":"code","7b231f32":"code","8ad1cda3":"code","2aa765d8":"code","28c85cf4":"code","ea126c7d":"code","c2680659":"code","226c1c20":"code","ca4a5bfb":"markdown","6d6eed53":"markdown","034f4600":"markdown","b7fd67fd":"markdown","89dbeba3":"markdown","737bb6f4":"markdown","5f104ba4":"markdown","6dfaaa4f":"markdown","98765528":"markdown","190cfeb1":"markdown","a52ec655":"markdown","0a10aa68":"markdown","83739fa7":"markdown","fdafa5e4":"markdown","dde5c2fe":"markdown","2b9f4f46":"markdown","03aab174":"markdown","96ac1156":"markdown","d8005456":"markdown","fb3d83e2":"markdown","7c9e0043":"markdown","d096ee12":"markdown","6cdf3857":"markdown","0721d999":"markdown","5cdca175":"markdown","94d57646":"markdown","d1efb20a":"markdown"},"source":{"f8f1cd60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5706d222":"train_data = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/test.csv\")","50862eae":"train_data","0297b7b5":"train_data.info()","af3eb20f":"train_data.dropna(inplace = True)\ntrain_data","d6490a6f":"toxic_data = train_data[train_data.target == 1]\nnon_toxic_data = train_data[train_data.target == 0]","357c08ce":"for d in non_toxic_data.question_text.sample(10):\n  print(d)","0a69710f":"for d in toxic_data.question_text.sample(10):\n  print(d)","b34ce892":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nprint(toxic_data.shape, non_toxic_data.shape)\nprint(\"--------------------------\")\n\nsns.countplot(x='target', data=train_data)","03ed606a":"print( \"D\u1eef li\u1ec7u Insincere chi\u1ebfm\", toxic_data.shape[0] \/ train_data.shape[0])\nprint( \"D\u1eef li\u1ec7u Sincere chi\u1ebfm\", non_toxic_data.shape[0] \/ train_data.shape[0])","afb34b4e":"import re\nimport nltk\nimport string\nfrom unidecode import unidecode\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\ncontraction_dict = {\"dont\": \"do not\", \"aint\": \"is not\", \"isnt\": \"is not\", \"doesnt\": \"does not\"\n, \"cant\": \"cannot\", \"mustnt\": \"must not\", \"ll\":\"will\" , \"re\": \"are\" ,\"ll\": \"will\", \"wont\": \"will not\" ,\"hasnt\": \"has not\"\n, \"havent\": \"have not\", \"arent\": \"are not\", \"ain't\": \"is not\", \"aren't\": \"are not\"\n,\"can't\": \"cannot\", \"\u2018cause\": \"because\", \"could've\": \"could have\"\n, \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\"\n, \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\"\n,\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\"\n, \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\"\n, \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"Iam\": \"I am\", \"I've\": \"I have\"\n, \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\"\n,\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\"\n, \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\"\n, \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\"\n,\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\"\n, \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\"\n, \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\"\n, \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\"\n, \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\"\n, \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\"\n, \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\"\n, \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n\"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\"\n, \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n\"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n\"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \n\"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n\"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n\"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n\"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \n\"you'll\": \"you will\", \"youll\":\"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk_stopwords = stopwords.words('english')\n\nnltk_stopwords.remove('not')\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","3e4221c9":"def clean(text):        \n    # chuy\u1ec3n t\u1eeb v\u1ec1 d\u1ea1ng unicode \n    text = unidecode(text).encode(\"ascii\")\n    text = str(text, \"ascii\")\n\n    # chuy\u1ec3n v\u1ec1 d\u1ea1ng ch\u1eef th\u01b0\u1eddng, b\u1ecf c\u00e1c link, k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t, ch\u1eef s\u1ed1.\n    text = text.lower()\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  \n    text = re.sub('\\n', '', text)\n    text = re.sub('[\u2019\u201c\u201d\u2026]', ' ', text)  \n    text = ''.join(i for i in text if not i.isdigit())\n\n    # Chuy\u1ec3n c\u00e1c t\u1eeb vi\u1ebft t\u1eaft trong t\u1eeb \u0111i\u1ec3n v\u1ec1 d\u1ea1ng th\u01b0\u1eddng\n    tokens = word_tokenize(text)\n    tokens = [contraction_dict.get(token) if (contraction_dict.get(token) != None) else token for token in tokens]\n    text = \" \".join(tokens)\n\n    # B\u1ecf c\u00e1c t\u1eeb ch\u1ee9a \u1edf trong stop-words   \n    tokens = word_tokenize(text)\n    tokens_without_sw = [word for word in tokens if not word in nltk_stopwords]\n\n    # Chuy\u1ec3n t\u1eeb v\u1ec1 d\u1ea1ng s\u1ed1 nhi\u1ec1u v\u1ec1 d\u1ea1ng th\u01b0\u1eddng\n    text = [lemmatizer.lemmatize(word) for word in tokens_without_sw ] \n    text = \" \".join(text)\n\n    return text","ef74dc1b":"word = clean(\"\u00e6\u0153\u2030\u00e6\u00af\u2019 they're you'll aren't mean in chinese takes taking taked\")\nprint(word)","d95a0782":"train_data['clean_questions'] = train_data['question_text'].apply(clean)\ntrain_data","379f34ea":"X = train_data['clean_questions']\ny = train_data['target']\n\nX.shape,y.shape","65285605":"train_data['target'].value_counts()","82d42683":"non_toxic_data = X[y == 0]\ntoxic_data = X[y == 1]\n\nprint(\"D\u1eef li\u1ec7u Label 0 (Ti\u1ec1n x\u1eed l\u00fd):\\n \", non_toxic_data[:10])\nprint('\\n---------------------------\\n')\nprint(\"D\u1eef li\u1ec7u Label 1 (Ti\u1ec1n x\u1eed l\u00fd):\\n \",toxic_data[:10])","a0a69788":"from collections import Counter\n\n# top 30 words in a non toxic data\np = Counter(\" \".join(non_toxic_data).split()).most_common(30)\nrslt = pd.DataFrame(p, columns=['Word', 'Frequency'])\n\nrslt.plot(x='Word',kind = \"barh\", figsize=(12,10), title=\"Top 30 t\u1eeb s\u1eed d\u1ee5ng nhi\u1ec1u nh\u1ea5t c\u1ee7a label 1\")","2653672f":"# top 30 words in a toxic data\np = Counter(\" \".join(toxic_data).split()).most_common(30)\nrslt = pd.DataFrame(p, columns=['Word', 'Frequency'])\n\nrslt.plot(x='Word',kind = \"barh\", figsize=(12,10), title=\"Top 30 t\u1eeb s\u1eed d\u1ee5ng nhi\u1ec1u nh\u1ea5t c\u1ee7a label 1\")","fb52fcf8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\nprint(X_test.shape, y_test.shape)\nprint(X_train.shape, y_train.shape)","566bbda3":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1,3))","eaeea62d":"# Count Vectorizer\ncount_train = count_vectorizer.fit(X)\n\nX_train_vec = count_train.transform(X_train)\nX_test_vec = count_train.transform(X_test)","b23a3224":"X_test","21a861e0":"X_train","b4870920":"# Th\u01b0 vi\u1ec7n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom imblearn.over_sampling import RandomOverSampler\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score, recall_score ,f1_score\nfrom sklearn.metrics import classification_report","d648bec6":"lr = LogisticRegression(class_weight = 'balanced', random_state=0, solver='liblinear')\nlr.fit(X_train_vec, y_train)\ny_pred = lr.predict(X_test_vec)","a9106a38":"def get_evaluation(predictions, test):\n    print('Recall: ', recall_score(predictions, test))\n    print('F1 score :', f1_score(predictions, test), '\\n')\n    print(classification_report(test, predictions))","d9f65737":"print(\"Logistic Regression with Count Vectorizer\\n\")\nget_evaluation(y_pred, y_test)","35c64431":"lsvc = LinearSVC(random_state=3, tol=0.01, loss='hinge', C=1, verbose=2, class_weight='balanced')\n\nlsvc.fit(X_train_vec, y_train) \ny_pred2 = lsvc.predict(X_test_vec)","8a2cd24d":"print('-----------\\n')\nprint(\"SVC with Count Vectorizer\\n\")\nget_evaluation(y_pred2, y_test)","13fd5d90":"# define oversampling + undersampling strategy\noversample = RandomOverSampler(sampling_strategy=0.5)\n\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(X_train_vec, y_train)","a68e6d6b":"from collections import Counter\n\nprint(Counter(y_over))\ny_over.shape","f52b5e7b":"lr2 = LogisticRegression(class_weight='balanced', solver='liblinear')\nlsvc2 = LinearSVC(class_weight='balanced')\n\nlr2.fit(X_over, y_over)\ny_pred3 = lr2.predict(X_test_vec)","94396723":"lsvc2.fit(X_over, y_over)\ny_pred4 = lsvc2.predict(X_test_vec)","dd0a04f5":"print('Logistic Regression - Count Vectorizer - Oversampling')\nget_evaluation(y_pred3, y_test)","6e88aa8e":"print('Linear SVC - Count Vectorizer - Oversampling')\nget_evaluation(y_pred4, y_test)","7b231f32":"print(\"Logistic Regression with Count Vectorizer\\n\")\nget_evaluation(y_pred, y_test)\nprint('-------------\\n')\nprint(\"SVC with Count Vectorizer\\n\")\nget_evaluation(y_pred2, y_test)\nprint('-------------\\n')\nprint('Logistic Regression - Count Vectorizer - Oversampling')\nget_evaluation(y_pred3, y_test)\nprint('-------------\\n')\nprint('Linear SVC - Count Vectorizer - Oversampling')\nget_evaluation(y_pred4, y_test)","8ad1cda3":"test_data['clean_questions'] = test_data['question_text'].apply(clean)\ntest_data","2aa765d8":"X = test_data['clean_questions']\n\nX.shape","28c85cf4":"X_vec_test = count_vectorizer.transform(X)","ea126c7d":"predictions = lr.predict(X_vec_test)","c2680659":"predictions.shape","226c1c20":"test_data['prediction'] = predictions\n\nresults = test_data[['qid', 'prediction']]\nresults.to_csv('submission.csv', index=False)","ca4a5bfb":"**2.3. HU\u1ea4N LUY\u1ec6N M\u00d4 H\u00ccNH**","6d6eed53":"**M\u00f4 h\u00ecnh SVC**\n* Count Vectorizer v\u1edbi N-grams = [1,3]","034f4600":"**B\u00c1O C\u00c1O B\u00c0I T\u1eacP L\u1edaN H\u1eccC M\u00c1Y CU\u1ed0I K\u1ef2**\n\n**M\u00e3 L\u1edbp**: INT3405_1\n\n**H\u1ecd v\u00e0 t\u00ean**: Nguy\u1ec5n Di\u1ec7p Y\u1ebfn\n\n**MSSV**:18021455","b7fd67fd":"**3. T\u1ea0O SUBMISSION**","89dbeba3":"**1. V\u1ea4N \u0110\u1ec0 B\u00c0I TO\u00c1N: Quora Insincere Classification**\n>Quora l\u00e0 n\u1ec1n t\u1ea3ng n\u1ed5i ti\u1ebfng n\u01a1i m\u1ecdi ng\u01b0\u1eddi \u0111\u1eb7t cho nhau nh\u1eefng c\u00e2u h\u1ecfi \u0111\u1ec3 h\u1ecdc h\u1ecfi th\u00eam nhi\u1ec1u th\u1ee9. Tuy nhi\u00ean c\u00f3 nh\u1eefng c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u1eb7t ra mang t\u00ednh nh\u1ea1y c\u1ea3m cho ng\u01b0\u1eddi \u0111\u1ecdc. Ta ph\u1ea3i ph\u00e2n lo\u1ea1i ra nh\u1eefng c\u00e2u h\u1ecfi mang t\u00ednh ti\u00eau c\u1ef1c, kh\u00f4ng \u0111\u00fang \u0111\u1eafn.\n* **Input** : C\u00e2u h\u1ecfi c\u1ee7a Quora (d\u1eef li\u1ec7u text)\n* **Output**: Label Sincere (0), Insincere (1)","737bb6f4":"**1.1. PH\u00c2N T\u00cdCH D\u1eee LI\u1ec6U**\n* D\u1eef li\u1ec7u c\u00f3 3 thu\u1ed9c t\u00ednh: qid, question_text, target\n* Khi ph\u00e2n lo\u1ea1i, ta d\u00f9ng thu\u1ed9c t\u00ednh question_text l\u00e0 \u0111\u1ea7u v\u00e0o X, target l\u00e0 label y","5f104ba4":"=> T\u1eeb d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cho, ta th\u1ea5y:\n* 1306122 c\u00e2u h\u1ecfi question_text ch\u1ee9a gi\u00e1 tr\u1ecb text data.\n* 1306122 label target ch\u1ee9a gi\u00e1 tr\u1ecb 0, 1.\n* D\u1eef li\u1ec7u kh\u00f4ng ch\u1ee9a gi\u00e1 tr\u1ecb NA\n","6dfaaa4f":"**H\u00ecnh 1 - B\u1ea3ng \u0111\u1ebfm s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u label 0 v\u00e0 label 1**","98765528":"**M\u00f4 h\u00ecnh Logistic Regression**\n>* Count Vectorizer v\u1edbi N-grams = [1,3]","190cfeb1":"**2.2. VECTOR H\u00d3A D\u1eee LI\u1ec6U**","a52ec655":"**2.1. TI\u1ec0N X\u1eec L\u00dd**\n\n(Code \u1edf \u0111\u00e2y em t\u1ea3i v\u1ec1 b\u1ed9 t\u1eeb \u0111i\u1ec3n \u0111\u1ecbnh ngh\u0129a s\u1eb5n trong NLTK)","0a10aa68":"T\u00e1ch d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh, d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3\n\n* D\u1eef li\u1ec7u train = 80%\n* D\u1eef li\u1ec7u test = 20%","83739fa7":"**Nh\u1eefng c\u00e2u h\u1ecfi Insincere c\u1ee7a Quora ( label target c\u00f3 gi\u00e1 tr\u1ecb 1 )**","fdafa5e4":"* **Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u** (B\u1ecf d\u1ea5u, ch\u1eef s\u1ed1, t\u1eeb d\u1eebng, r\u00fat g\u1ecdn, t\u00e1ch t\u1eeb)\nUnicode: Chuy\u1ec3n v\u1ec1 d\u1ea1ng unicode\nLowercase : Chuy\u1ec3n v\u1ec1 d\u1ea1ng in th\u01b0\u1eddng.\nPunctuation, Remove Number: B\u1ecf d\u1ea5u, ch\u1eef s\u1ed1.\nTokenize, Stopwords: T\u00e1ch t\u1eeb, C\u00e1c t\u1eeb d\u1eebng.\nLemmatizers: R\u00fat g\u1ecdn t\u1eeb v\u1ec1 d\u1ea1ng ng\u1eafn g\u1ecdn.\n=> Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb kh\u00f4ng \u00fd ngh\u0129a, gi\u00fap ta x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c c\u00e1c t\u1eeb c\u1ea7n thi\u1ebft \u0111\u1ec3 ph\u00e2n lo\u1ea1i t\u1eeb \u0111\u00f3 thu\u1ed9c Label 1 hay 0.\n\n* **Tr\u00edch xu\u1ea5t d\u1eef li\u1ec7u**\nS\u1eed d\u1ee5ng CountVectorizer \u0111\u1ec3 tr\u00edch xu\u1ea5t c\u00e1c t\u1eeb, bi\u1ebfn words th\u00e0nh d\u1ea1ng vectors \u1edf d\u1ea1ng Bag-of-Words b\u1eb1ng c\u00e1ch \u0111\u1ebfm s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c t\u1eeb trong b\u1ed9 d\u1eef li\u1ec7u.\n\n* **Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh**\n\u00c1p d\u1ee5ng m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y Logistic Regression, LinearSVC \u0111\u1ec3 hu\u1ea5n luy\u1ec7n, d\u1ef1 \u0111o\u00e1n. S\u1eed d\u1ee5ng OverSampling th\u1eed xem k\u1ebft qu\u1ea3 nh\u01b0 th\u1ebf n\u00e0o v\u1edbi m\u00f4 h\u00ecnh tr\u00ean.","dde5c2fe":"Ch\u1ecdn m\u00f4 h\u00ecnh Logistic Regression \u0111\u1ea7u ti\u00ean \u0111\u1ec3 ki\u1ec3m tra.\n","2b9f4f46":"**So s\u00e1nh m\u00f4 h\u00ecnh**\n* Nh\u1eadn x\u00e9t: K\u1ebft qu\u1ea3 m\u00f4 h\u00ecnh kh\u00f4ng thay \u0111\u1ed5i sau l\u1ea7n OverSampling","03aab174":"**1.2. NH\u1eacN X\u00c9T V\u1ec0 D\u1eee LI\u1ec6U**","96ac1156":"-- **Topic: Classify toxic question** --","d8005456":"**B\u1ecf \u0111i c\u00e1c d\u1eef li\u1ec7u NA, kh\u00f4ng t\u1ed3n t\u1ea1i trong b\u1ea3ng.**","fb3d83e2":"Th\u00f4ng th\u01b0\u1eddng CountVectorizer ch\u1ec9 c\u1ea7n bigram l\u00e0 \u0111\u1ee7.","7c9e0043":"**V\u1ea5n \u0111\u1ec1**\n>Trong d\u1eef li\u1ec7u train, c\u00f3 80810 d\u1eef li\u1ec7u insincere (label 1) , 1225312 d\u1eef li\u1ec7u sincere (label 0).\n\n  * \u1ede \u0111\u00e2y ta th\u1ea5y d\u1eef li\u1ec7u insincere ch\u1ec9 chi\u1ebfm t\u1ec9 l\u1ec7 0.062\n  * C\u00f2n so v\u1edbi d\u1eef li\u1ec7u sincere chi\u1ebfm t\u1ec9 l\u1ec7 0.94 => D\u1eef li\u1ec7u m\u1ea5t c\u00e2n b\u1eb1ng\n![1234.png](attachment:ffd73c8c-bea2-455b-a0f6-d432fdeca461.png)\n\n=> \u0110\u1ed9 \u0111o s\u1eed d\u1ee5ng l\u00e0 F1-Score","d096ee12":"(\u0110\u1ecbnh ngh\u0129a h\u00e0m clean() \u0111\u1ec3 ti\u1ec1n x\u1eed l\u00fd c\u00e1c d\u1eef li\u1ec7u theo c\u00e1c b\u01b0\u1edbc \u0111\u00e3 n\u00eau \u1edf tr\u00ean)","6cdf3857":"* CountVectorizer N-gram = (1,3)\n>=> S\u1eed d\u1ee5ng CountVectorizer v\u1edbi n-gram trong kho\u1ea3ng [1,3] \u0111\u1ec3 tr\u00edch xu\u1ea5t \u0111\u01b0\u1ee3c trong kho\u1ea3ng 2 t\u1eeb v\u00e0 3 t\u1eeb li\u00ean ti\u1ebfp nhau.\n\n* Tr\u00edch xu\u1ea5t th\u00eam d\u1eef li\u1ec7u \u0111\u1ec3 hi\u1ec3u c\u00e2u t\u1eeb trong ng\u1eef c\u1ea3nh n\u00e0o h\u01a1n.\n>Gi\u1ea3 s\u1eed nh\u01b0: ta c\u00f3 t\u1eeb germany th\u00ec s\u1ebd ch\u01b0a hi\u1ec3u n\u00f3 l\u00e0 thu\u1ed9c label n\u00e0o.\nNh\u01b0ng n\u1ebfu m\u00e0 ta tr\u00edch \u0111\u01b0\u1ee3c germany hitler => kh\u1ea3 n\u0103ng cao c\u00e2u h\u1ecfi thu\u1ed9c label 1.","0721d999":"**Gi\u1edd ta s\u1ebd xem s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa d\u1eef li\u1ec7u label 1, 0**","5cdca175":"**M\u00f4 h\u00ecnh SVC + LR + Oversampling**\n* Count Vectorizer v\u1edbi N-grams = [1,3]","94d57646":"**2. \u0110\u1ec0 XU\u1ea4T H\u01af\u1edaNG L\u00c0M**","d1efb20a":"**Nh\u1eefng c\u00e2u h\u1ecfi Sincere c\u1ee7a Quora ( label target c\u00f3 gi\u00e1 tr\u1ecb 0 )**"}}