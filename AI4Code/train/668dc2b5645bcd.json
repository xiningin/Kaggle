{"cell_type":{"52493566":"code","f09a1196":"code","41712b94":"code","e72644cd":"code","d819ee70":"code","e3247b93":"code","401357c7":"code","7b75857c":"code","2121c537":"code","7f14bd56":"code","d6000870":"code","19727e4d":"code","96eeb796":"code","9461892d":"code","a74e0d52":"code","d2a26a01":"code","45d76954":"code","eddd82d0":"code","150bea05":"code","768a4842":"code","b9ae55f3":"code","c790d4b3":"code","c0addeea":"code","e5670a39":"code","f07f5873":"code","1f690209":"code","46c21bb3":"code","e2b840b0":"code","596dc446":"code","43db1eb4":"code","5010df32":"code","c40eaef0":"code","4ceebe7c":"code","94a9d2aa":"code","bba715c8":"code","750f32b2":"code","ed9f53f6":"markdown","bebf204d":"markdown","b90a13dc":"markdown","d21732b8":"markdown","cb1ace5f":"markdown","c1bb63b8":"markdown","f1c2555a":"markdown","f0fb7268":"markdown","28775fe8":"markdown","7a529ebc":"markdown","cff3fbf1":"markdown"},"source":{"52493566":"!wget https:\/\/github.com\/Chaogan-Yan\/DPABI\/raw\/master\/Templates\/ch2better.nii","f09a1196":"import numpy as np # linear algebra\nimport nilearn as nl\nimport nilearn.plotting as nlplt\nimport nibabel as nib\nimport h5py\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nmask_filename = '..\/input\/trends-assessment-prediction\/fMRI_mask.nii'\nsubject_filename = '..\/input\/trends-assessment-prediction\/fMRI_train\/10004.mat'\nsmri_filename = 'ch2better.nii'\nmask_niimg = nl.image.load_img(mask_filename)\n\ndef load_subject(filename, mask_niimg):\n    \"\"\"\n    Load a subject saved in .mat format with\n        the version 7.3 flag. Return the subject\n        niimg, using a mask niimg as a template\n        for nifti headers.\n        \n    Args:\n        filename    <str>            the .mat filename for the subject data\n        mask_niimg  niimg object     the mask niimg object used for nifti headers\n    \"\"\"\n    subject_data = None\n    with h5py.File(subject_filename, 'r') as f:\n        subject_data = f['SM_feature'][()]\n    # It's necessary to reorient the axes, since h5py flips axis order\n    subject_data = np.moveaxis(subject_data, [0,1,2,3], [3,2,1,0])\n    subject_niimg = nl.image.new_img_like(mask_niimg, subject_data, affine=mask_niimg.affine, copy_header=True)\n    return subject_niimg\nsubject_niimg = load_subject(subject_filename, mask_niimg)\nprint(\"Image shape is %s\" % (str(subject_niimg.shape)))\nnum_components = subject_niimg.shape[-1]\nprint(\"Detected {num_components} spatial maps\".format(num_components=num_components))","41712b94":"nlplt.plot_prob_atlas(subject_niimg, bg_img=smri_filename, view_type='filled_contours', draw_cross=False, title='All %d spatial maps' % num_components, threshold='auto')\n","e72644cd":"grid_size = int(np.ceil(np.sqrt(num_components)))\nfig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*10, grid_size*10))\n[axi.set_axis_off() for axi in axes.ravel()]\nrow = -1\nfor i, cur_img in enumerate(nl.image.iter_img(subject_niimg)):\n    col = i % grid_size\n    if col == 0:\n        row += 1\n    nlplt.plot_stat_map(cur_img, bg_img=smri_filename, title=\"IC %d\" % i, axes=axes[row, col], threshold=3, colorbar=False)\n","d819ee70":"import matplotlib.pyplot as plt\n\nfrom nilearn import datasets\nhaxby_dataset = datasets.fetch_haxby()   # load dataset\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\n# Build the mean image because we have no anatomic data\nfrom nilearn import image\nfunc_filename = haxby_dataset.func[0]\nmean_img = image.mean_img(func_filename)\n\nz_slice = -14\n\nfig = plt.figure(figsize=(4, 5.4), facecolor='k')\n\nfrom nilearn.plotting import plot_anat, show\ndisplay = plot_anat(mean_img, display_mode='z', cut_coords=[z_slice],\n                    figure=fig)\nmask_vt_filename = haxby_dataset.mask_vt[0]\nmask_house_filename = haxby_dataset.mask_house[0]\nmask_face_filename = haxby_dataset.mask_face[0]\ndisplay.add_contours(mask_vt_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['red'])\ndisplay.add_contours(mask_house_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['blue'])\ndisplay.add_contours(mask_face_filename, contours=1, antialiased=False,\n                     linewidths=4., levels=[0], colors=['limegreen'])\n\n# We generate a legend using the trick described on\n# http:\/\/matplotlib.sourceforge.net\/users\/legend_guide.httpml#using-proxy-artist\nfrom matplotlib.patches import Rectangle\np_v = Rectangle((0, 0), 1, 1, fc=\"red\")\np_h = Rectangle((0, 0), 1, 1, fc=\"blue\")\np_f = Rectangle((0, 0), 1, 1, fc=\"limegreen\")\nplt.legend([p_v, p_h, p_f], [\"vt\", \"house\", \"face\"])\n\nshow()","e3247b93":"from nilearn import datasets\n\n# haxby dataset to have EPI images and masks\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is at: %s' %\n      haxby_dataset.func[0])  # 4D data\n\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# one motor contrast map from NeuroVault\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","401357c7":"from nilearn import plotting\n\n# Visualizing t-map image on EPI template with manual\n# positioning of coordinates using cut_coords given as a list\nplotting.plot_stat_map(stat_img,\n                       threshold=3, title=\"plot_stat_map\",\n                       cut_coords=[36, -27, 66])","7b75857c":"view = plotting.view_img(stat_img, threshold=3)\n# In a Jupyter notebook, if ``view`` is the output of a cell, it will\n# be displayed below the cell\nview","2121c537":"plotting.plot_glass_brain(stat_img, title='plot_glass_brain',\n                          threshold=3)","7f14bd56":"plotting.plot_anat(haxby_anat_filename, title=\"plot_anat\")","d6000870":"plotting.plot_roi(haxby_mask_filename, bg_img=haxby_anat_filename,\n                  title=\"plot_roi\")","19727e4d":"# Import image processing tool\nfrom nilearn import image\n\n# Compute the voxel_wise mean of functional images across time.\n# Basically reducing the functional image from 4D to 3D\nmean_haxby_img = image.mean_img(haxby_func_filename)\n\n# Visualizing mean image (3D)\nplotting.plot_epi(mean_haxby_img, title=\"plot_epi\")","96eeb796":"\n# haxby dataset to have anatomical image, EPI images and masks\n#haxby_dataset = datasets.fetch_haxby()\nhaxby_anat_filename = haxby_dataset.anat[0]\nhaxby_mask_filename = haxby_dataset.mask_vt[0]\nhaxby_func_filename = haxby_dataset.func[0]\n\n# localizer dataset to have contrast maps\nmotor_images = datasets.fetch_neurovault_motor_task()\nstat_img = motor_images.images[0]","9461892d":"plotting.plot_stat_map(stat_img, display_mode='ortho',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='ortho', cut_coords=[36, -27, 60]\")\n\n","a74e0d52":"plotting.plot_stat_map(stat_img, display_mode='z', cut_coords=5,\n                       title=\"display_mode='z', cut_coords=5\")","d2a26a01":"plotting.plot_stat_map(stat_img, display_mode='x',\n                       cut_coords=[-36, 36],\n                       title=\"display_mode='x', cut_coords=[-36, 36]\")","45d76954":"plotting.plot_stat_map(stat_img, display_mode='y', cut_coords=1,\n                       title=\"display_mode='y', cut_coords=1\")","eddd82d0":"plotting.plot_stat_map(stat_img, display_mode='z',\n                       cut_coords=1, colorbar=False,\n                       title=\"display_mode='z', cut_coords=1, colorbar=False\")","150bea05":"plotting.plot_stat_map(stat_img, display_mode='xz',\n                       cut_coords=[36, 60],\n                       title=\"display_mode='xz', cut_coords=[36, 60]\")","768a4842":"plotting.plot_stat_map(stat_img, display_mode='yx',\n                       cut_coords=[-27, 36],\n                       title=\"display_mode='yx', cut_coords=[-27, 36]\")","b9ae55f3":"plotting.plot_stat_map(stat_img, display_mode='yz',\n                       cut_coords=[-27, 60],\n                       title=\"display_mode='yz', cut_coords=[-27, 60]\")","c790d4b3":"plotting.plot_stat_map(stat_img, display_mode='tiled',\n                       cut_coords=[36, -27, 60],\n                       title=\"display_mode='tiled'\")","c0addeea":"from nilearn import image\n\n# Compute voxel-wise mean functional image across time dimension. Now we have\n# functional image in 3D assigned in mean_haxby_img\nmean_haxby_img = image.mean_img(haxby_func_filename)","e5670a39":"display = plotting.plot_anat(mean_haxby_img, title=\"add_edges\")\n\n# We are now able to use add_edges method inherited in plotting object named as\n# display. First argument - anatomical image  and by default edges will be\n# displayed as red 'r', to choose different colors green 'g' and  blue 'b'.\ndisplay.add_edges(haxby_anat_filename)","f07f5873":"# As seen before, we call the `plot_anat` function with a background image\n# as first argument, in this case again the mean fMRI image and argument\n# `cut_coords` as list for manual cut with coordinates pointing at masked\n# brain regions\ndisplay = plotting.plot_anat(mean_haxby_img, title=\"add_contours\",\n                             cut_coords=[-34, -39, -9])\n# Now use `add_contours` in display object with the path to a mask image from\n# the Haxby dataset as first argument and argument `levels` given as list\n# of values to select particular level in the contour to display and argument\n# `colors` specified as red 'r' to see edges as red in color.\n# See help on matplotlib.pyplot.contour to use more options with this method\ndisplay.add_contours(haxby_mask_filename, levels=[0.5], colors='r')\n\n","1f690209":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"add_contours with filled=True\",\n                             cut_coords=[-34, -39, -9])\n\n# By default, no color fillings will be shown using `add_contours`. To see\n# contours with color fillings use argument filled=True. contour colors are\n# changed to blue 'b' with alpha=0.7 sets the transparency of color fillings.\n# See help on matplotlib.pyplot.contourf to use more options given that filled\n# should be True\ndisplay.add_contours(haxby_mask_filename, filled=True, alpha=0.7,\n                     levels=[0.5], colors='b')","46c21bb3":"display = plotting.plot_anat(mean_haxby_img, title=\"add_markers\",\n                             cut_coords=[-34, -39, -9])\n\n# Coordinates of seed regions should be specified in first argument and second\n# argument `marker_color` denotes color of the sphere in this case yellow 'y'\n# and third argument `marker_size` denotes size of the sphere\ncoords = [(-34, -39, -9)]\ndisplay.add_markers(coords, marker_color='y', marker_size=100)","e2b840b0":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True)","596dc446":"display = plotting.plot_anat(mean_haxby_img,\n                             title=\"adding a scale bar\",\n                             cut_coords=[-34, -39, -9])\ndisplay.annotate(scalebar=True, scale_size=25, scale_units='mm')","43db1eb4":"from nilearn import datasets\n\nrest_dataset = datasets.fetch_development_fmri(n_subjects=20)\nfunc_filenames = rest_dataset.func\nconfounds = rest_dataset.confounds\n","5010df32":"# Import dictionary learning algorithm from decomposition module and call the\n# object and fit the model to the functional datasets\nfrom nilearn.decomposition import DictLearning\n\n# Initialize DictLearning object\ndict_learn = DictLearning(n_components=8, smoothing_fwhm=6.,\n                          memory=\"nilearn_cache\", memory_level=2,\n                          random_state=0)\n# Fit to the data\ndict_learn.fit(func_filenames)\n# Resting state networks\/maps in attribute `components_img_`\n# Note that this attribute is implemented from version 0.4.1.\n# For older versions, see the note section above for details.\ncomponents_img = dict_learn.components_img_\n\n# Visualization of functional networks\n# Show networks using plotting utilities\nfrom nilearn import plotting\n\nplotting.plot_prob_atlas(components_img, view_type='filled_contours',\n                         title='Dictionary Learning maps')","c40eaef0":"# Import Region Extractor algorithm from regions module\n# threshold=0.5 indicates that we keep nominal of amount nonzero voxels across all\n# maps, less the threshold means that more intense non-voxels will be survived.\nfrom nilearn.regions import RegionExtractor\n\nextractor = RegionExtractor(components_img, threshold=0.5,\n                            thresholding_strategy='ratio_n_voxels',\n                            extractor='local_regions',\n                            standardize=True, min_region_size=1350)\n# Just call fit() to process for regions extraction\nextractor.fit()\n# Extracted regions are stored in regions_img_\nregions_extracted_img = extractor.regions_img_\n# Each region index is stored in index_\nregions_index = extractor.index_\n# Total number of regions extracted\nn_regions_extracted = regions_extracted_img.shape[-1]\n\n# Visualization of region extraction results\ntitle = ('%d regions are extracted from %d components.'\n         '\\nEach separate color of region indicates extracted region'\n         % (n_regions_extracted, 8))\nplotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n                         title=title)","4ceebe7c":"# First we need to do subjects timeseries signals extraction and then estimating\n# correlation matrices on those signals.\n# To extract timeseries signals, we call transform() from RegionExtractor object\n# onto each subject functional data stored in func_filenames.\n# To estimate correlation matrices we import connectome utilities from nilearn\nfrom nilearn.connectome import ConnectivityMeasure\n\ncorrelations = []\n# Initializing ConnectivityMeasure object with kind='correlation'\nconnectome_measure = ConnectivityMeasure(kind='correlation')\nfor filename, confound in zip(func_filenames, confounds):\n    # call transform from RegionExtractor object to extract timeseries signals\n    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n    # call fit_transform from ConnectivityMeasure object\n    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n    # saving each subject correlation to correlations\n    correlations.append(correlation)\n\n# Mean of all correlations\nimport numpy as np\nmean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n                                                          n_regions_extracted)","94a9d2aa":"title = 'Correlation between %d regions' % n_regions_extracted\n\n# First plot the matrix\ndisplay = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n                               colorbar=True, title=title)\n\n# Then find the center of the regions and plot a connectome\nregions_img = regions_extracted_img\ncoords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n\nplotting.plot_connectome(mean_correlations, coords_connectome,\n                         edge_threshold='90%', title=title)","bba715c8":"# First, we plot a network of index=4 without region extraction (left plot)\nfrom nilearn import image\n\nimg = image.index_img(components_img, 4)\ncoords = plotting.find_xyz_cut_coords(img)\ndisplay = plotting.plot_stat_map(img, cut_coords=coords, colorbar=False,\n                                 title='Showing one specific network')","750f32b2":"# For this, we take the indices of the all regions extracted related to original\n# network given as 4.\nregions_indices_of_map3 = np.where(np.array(regions_index) == 4)\n\ndisplay = plotting.plot_anat(cut_coords=coords,\n                             title='Regions from this network')\n\n# Add as an overlay all the regions of index 4\ncolors = 'rgbcmyk'\nfor each_index_of_map3, color in zip(regions_indices_of_map3[0], colors):\n    display.add_overlay(image.index_img(regions_extracted_img, each_index_of_map3),\n                        cmap=plotting.cm.alpha_cmap(color))\n\nplotting.show()","ed9f53f6":"In this competition, you will predict multiple assessments plus age from multimodal brain MRI features. You will be working from existing results from other data scientists, doing the important work of validating the utility of multimodal features in a normative population of unaffected subjects. Due to the complexity of the brain and differences between scanners, generalized approaches will be essential to effectively propel multimodal neuroimaging research forward.","bebf204d":"In this challenge, participants will predict age and assessment values from two domains using features derived from brain MRI images as inputs.\n\nModels are expected to generalize on data from a different scanner\/site (site 2). All subjects from site 2 were assigned to the test set, so their scores are not available. While there are fewer site 2 subjects than site 1 subjects in the test set, the total number of subjects from site 2 will not be revealed until after the end of the competition. To make it more interesting, the IDs of some site 2 subjects have been revealed below. Use this to inform your models about site effects. Site effects are a form of bias. To generalize well, models should learn features that are not related to or driven by site effects.","b90a13dc":"![image.png](attachment:image.png)","d21732b8":"# Brain Development Functional Datasets","cb1ace5f":"# Modeling","c1bb63b8":"# Nilearn Visualizations","f1c2555a":"# Intoduction","f0fb7268":"Modeling will be done soon ","28775fe8":"Be stay tuned and upvote if you like this notebook ","7a529ebc":"Human brain research is among the most complex areas of study for scientists. We know that age and other factors can affect its function and structure, but more research is needed into what specifically occurs within the brain. With much of the research using MRI scans, data scientists are well positioned to support future insights. In particular, neuroimaging specialists look for measurable markers of behavior, health, or disorder to help identify relevant brain regions and their contribution to typical or symptomatic effects.","cff3fbf1":"# Importing all necessary packages"}}