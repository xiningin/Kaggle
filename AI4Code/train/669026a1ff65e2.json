{"cell_type":{"5d5e8a9e":"code","58179195":"code","eebd9fa0":"code","fc7e2cec":"code","dbc6b80d":"code","7ffa2264":"code","71cae59b":"code","319e20d0":"code","b0835360":"code","c8cf542f":"code","98eeaba6":"code","af693787":"code","de07c979":"code","b53de98f":"code","6012a10f":"code","7f684ddf":"code","255de259":"code","d490b1e9":"code","cbfb0f38":"code","f712814a":"code","20affaa9":"code","3821e164":"code","894a08f6":"code","25621ac6":"code","6ece57d4":"code","d5125438":"code","ad18cf22":"code","e997d0d8":"code","254c3933":"code","13ed7bf2":"code","d67b6e5e":"code","52b6cd0e":"code","d317b326":"code","5c3b8f66":"code","c51c04f5":"code","3f72313f":"code","0644150f":"code","c107ed6b":"code","986880cd":"code","05202c25":"code","e1f96dce":"code","e5373331":"code","6c299931":"code","12ced4e0":"code","11ca5ad4":"code","1d4bd24d":"code","a856ff57":"code","3327ab18":"markdown","82e93262":"markdown","625ea957":"markdown","ce0cff07":"markdown","feff5dc2":"markdown","fe55a41c":"markdown","ae33a7c4":"markdown","0b621606":"markdown","4bb47bfe":"markdown","6684d87d":"markdown","067ae643":"markdown","cc6170a6":"markdown","92b82f91":"markdown","0bef0581":"markdown","8ff02e2c":"markdown","bc401f28":"markdown","cbfcd9be":"markdown","b23d1039":"markdown","ff78b98b":"markdown","d2c06fa3":"markdown","0b37e0d0":"markdown","db1041c3":"markdown","9080c929":"markdown","f16797ef":"markdown","68353d01":"markdown","1508f966":"markdown","93e8669d":"markdown"},"source":{"5d5e8a9e":"!pip install jcopml","58179195":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom jcopml.pipeline import num_pipe, cat_pipe\n# from jcopml.utils import save_model, load_model\nfrom jcopml.plot import plot_missing_value\nfrom jcopml.feature_importance import mean_score_decrease","eebd9fa0":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation","fc7e2cec":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf.head()\n\n# why \"id\" is not set as index? because it is treated as list in the phase of model prediction to test dataset","dbc6b80d":"# plot missing value\nplot_missing_value(df, return_df=True)","7ffa2264":"\n# drop too missing values data\ndf.drop(columns=[\"keyword\", \"location\"], inplace=True)","71cae59b":"plot_missing_value(df, return_df=True)","319e20d0":"print('the data is ready to be engineered')\ndf.head(10)","b0835360":"# total of words of old df\ntotal_words_old_df = df.text.apply(lambda x: len(x.split(\" \"))).sum()\nprint(total_words_old_df)","c8cf542f":"import nltk \nimport re\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom string import punctuation","98eeaba6":"for i in range(len(df.text)):\n    print(df.text[i])\n    if i == 10:\n        break","af693787":"print(\"The length of dataframe is\", len(df), \"rows\")","de07c979":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(\"\\n\", \" \", text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = text.split()\n    text = \" \".join(text)\n    return text\n\ndf1 = df.text.apply(str).apply(lambda x:clean_text(x))","b53de98f":"for i in range(len(df1)):\n    print(df1[i])\n    if i == 10:\n        break","6012a10f":"print(\"The length of dataframe is\", len(df1), \"rows\")","7f684ddf":"df1_clean_text = []\nfor i in range(len(df1)):\n    x = df1[i]\n    # x_sent_token = sent_tokenize(x)\n    # x_sent_token\n    x_word_tokens = word_tokenize(x)\n    # x_word_tokens\n#     print(x)\n    \n#     print(df1[i])\n    \n    # punctuation removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens if w not in punctuation]\n#     print(x_word_tokens_removed_punctuations, \"punctuation\")\n    \n    # numeric removal\n    x_word_tokens_removed_punctuations = [w for w in x_word_tokens_removed_punctuations if w.isalpha()]\n#     print(x_word_tokens_removed_punctuations, \"numeric\")\n    \n    # stopwords removal\n    x_word_tokens_removed_punctuation_removed_sw = [w for w in x_word_tokens_removed_punctuations if w not in stopwords.words('english')]\n#     print(x_word_tokens_removed_punctuation_removed_sw, \"stopwords\")\n\n    # rejoining the words into one string\/sentence as inputted before being tokenized\n    x_word_tokens_removed_punctuation_removed_sw = \" \".join(x_word_tokens_removed_punctuation_removed_sw)\n#     print(x_word_tokens_removed_punctuation_removed_sw)\n    \n    df1_clean_text.append(x_word_tokens_removed_punctuation_removed_sw)","255de259":"# text vs processed text\nfor i,j in zip(df1[0:10], df1_clean_text[0:10]):\n    print(i)\n    print(j)\n    print()","d490b1e9":"# list (df1_clean_text) to series (df1_clean_text_series)\n\n# list\nprint(type(df1_clean_text))\nprint(len(df1_clean_text))\n\n# converting list to pandas series\ndf1_clean_text_series = pd.Series(df1_clean_text)\n\nprint(type(df1_clean_text_series))\nprint(len(df1_clean_text_series))","cbfb0f38":"# new series\ndf1_clean_text_series.head()","f712814a":"# new df\ndf['text'] = df1_clean_text_series\ndf.head(10)","20affaa9":"# total of words of old df vs new df \n\ntotal_words_new_df = df.text.apply(lambda x: len(x.split(\" \"))).sum()\n\nprint(\"old df: \", total_words_old_df, \"words\")\nprint(\"new df: \", total_words_new_df, \"words\")\nprint(\"text processing has reduced the number of words by\", round((total_words_old_df-total_words_new_df)\/total_words_old_df*100), \"%\")","3821e164":"X = df.text\ny = df.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","894a08f6":"# Check the imbalance dataset\ny_train.value_counts() \/ len(y_train) *100\n\n# the train data is more or less balance","25621ac6":"X_train.head(), X_test.head(), y_train.head(), y_test.head()","6ece57d4":"X_train.head(11)","d5125438":"import seaborn as sns\nimport matplotlib.pyplot as plt","ad18cf22":"y_train.shape, y_test.shape","e997d0d8":"sns.set(style=\"darkgrid\")\nsns.countplot(x=y_train)\nplt.title(\"y_train\");","254c3933":"sns.set(style=\"darkgrid\")\nsns.countplot(x=y_test)\nplt.title(\"y_test\");","13ed7bf2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer ","d67b6e5e":"pipeline = Pipeline([\n    ('prep', TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42))\n])\n\nmodel_logreg_tfidf = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=5, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel_logreg_tfidf.fit(X_train, y_train)\n\nprint(model_logreg_tfidf.best_params_)\nprint(model_logreg_tfidf.score(X_train, y_train), model_logreg_tfidf.best_score_, model_logreg_tfidf.score(X_test, y_test))","52b6cd0e":"pipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42))\n])\n\n# model = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=5, n_iter=50, n_jobs=-1, verbose=1, random_state=42)\nmodel_logreg_bow = RandomizedSearchCV(pipeline, rsp.logreg_params, cv=5, n_jobs=-1, verbose=1)\nmodel_logreg_bow.fit(X_train, y_train)\n\nprint(model_logreg_bow.best_params_)\nprint(model_logreg_bow.score(X_train, y_train), model_logreg_bow.best_score_, model_logreg_bow.score(X_test, y_test))","d317b326":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom jcopml.tuning import random_search_params as rsp\nfrom jcopml.tuning import random_search_params as gsp\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer ","5c3b8f66":"pipeline = Pipeline([\n    ('prep', CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [7, 8, 9, 10, 11, 12, 13, 14, 15],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\n# model_sgd_bow = GridSearchCV(pipeline, parameter, cv=5, n_jobs=-1, verbose=1)\nmodel_sgd_bow = RandomizedSearchCV(pipeline, parameter, cv=5, n_jobs=-1, verbose=1, random_state=42)\nmodel_sgd_bow.fit(X_train, y_train)\n\n\nprint(model_sgd_bow.best_params_)\nprint(model_sgd_bow.score(X_train, y_train), model_sgd_bow.best_score_, model_sgd_bow.score(X_test, y_test))","c51c04f5":"pipeline = Pipeline([\n    ('prep', TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3))),\n    ('algo', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n])\n\n\nparameter = {\n    'algo__loss': ['hinge', 'log', 'modified_huber', 'perceptron'],\n    'algo__penalty': ['l2', 'l1', 'elasticnet'],\n    'algo__alpha': [0.0001, 0.0002, 0.0003], \n    'algo__max_iter': [7, 8, 9, 10, 11, 12, 13, 14, 15],\n    'algo__tol': [0.0001, 0.0002, 0.0003]\n}\nmodel_sgd_tfidf = RandomizedSearchCV(pipeline, parameter, cv=5, n_jobs=-1, verbose=1, random_state=42)\n# modelt_sgd_fidf = GridSearchCV(pipeline, parameter, cv=5, n_jobs=-1, verbose=1)\nmodel_sgd_tfidf.fit(X_train, y_train)\n\n\nprint(model_sgd_tfidf.best_params_)\nprint(model_sgd_tfidf.score(X_train, y_train), model_sgd_tfidf.best_score_, model_sgd_tfidf.score(X_test, y_test))","3f72313f":"from jcopml.utils import save_model","0644150f":"# save_model(model, \"disaster_tweet_v1.pkl\")\n# save_model(model, \"disaster_tweet_v2.pkl\")\n# save_model(model, \"disaster_tweet_v3.pkl\")\n# save_model(model, \"disaster_tweet_v4.pkl\")\n# save_model(model, \"disaster_tweet_v5.pkl\")\n# save_model(model, \"disaster_tweet_v6.pkl\")\n# save_model(model, \"disaster_tweet_v7.pkl\")\n# save_model(model, \"disaster_tweet_v8.pkl\")\n# save_model(model, \"disaster_tweet_v9.pkl\")\n# save_model(model, \"disaster_tweet_v10.pkl\")\nsave_model(model, \"disaster_tweet_v11.pkl\")","c107ed6b":"df_submit = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndf_submit.drop(columns=[\"keyword\", \"location\"], inplace=True)\ndf_submit_id_list = df_submit.id.values.tolist()\ndf_submit_text_list = df_submit.text.values.tolist()\ndf_submit.head()","986880cd":"print(\"df_submit.id: \", len(df_submit.id))\nprint(\"df_submit.text: \", len(df_submit.text))\nprint(\"df_submit_id_list: \", len(df_submit_id_list))\nprint(\"df_submit_text_list: \", len(df_submit_text_list))","05202c25":"target = modeltfidf.predict(df_submit_text_list)\ntarget","e1f96dce":"print(type(target))\nprint(len(target))","e5373331":"df_submit_final = pd.DataFrame({\n    \"id\": df_submit_id_list,\n    \"target\": target\n})","6c299931":"# df_submit_final.set_index('id', inplace=True)\ndf_submit_final.head()","12ced4e0":"# df_submit_final.to_csv(\"disaster_tweet_v1.csv\")\n# df_submit_final.to_csv(\"disaster_tweet_v2.csv\")\n# df_submit_final.to_csv(\"disaster_tweet_v3.csv\")\n# df_submit_final.to_csv(\"disaster_tweet_v4.csv\")\n# df_submit_final.to_csv(\"disaster_tweet_v5.csv\")\n# df_submit_final.to_csv(\"disaster_tweet_v6.csv\") #numeric removal, tfidf\n# df_submit_final.to_csv(\"disaster_tweet_v7.csv\") #numeric removal, bow, n_iter default, random_state default\n# df_submit_final.to_csv(\"disaster_tweet_v8.csv\") #SGD Classifier\n# df_submit_final.to_csv(\"disaster_tweet_v9.csv\") #SGD Classifier\n# df_submit_final.to_csv(\"disaster_tweet_v10.csv\") #SGD Classifier\n# df_submit_final.to_csv(\"disaster_tweet_v11.csv\") #SGD Classifier\ndf_submit_final.to_csv(\"disaster_tweet_v12.csv\") #SGD Classifier + tfidf","11ca5ad4":"import pickle","1d4bd24d":"pickle.dump(model_logreg_tfidf, open(\"model_logreg_tfidf.pkl\", 'wb'))\npickle.dump(model_logreg_bow, open(\"model_logreg_bow.pkl\", 'wb'))\npickle.dump(model_sgd_tfidf, open(\"model_sgd_tfidf.pkl\", 'wb'))\npickle.dump(model_sgd_bow, open(\"model_sgd_bow.pkl\", 'wb'))","a856ff57":"# model = pickle.load(open(\"knn.pkl\", 'rb'))","3327ab18":"## Feature Engineering - Basic Text Processing","82e93262":"##### y_train dataset","625ea957":"##### 1) Normalization to Lower Case & Removing \"https: ...\"","ce0cff07":"##### resource:\n\n- Lemmatization: https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/\n- Stemming: https:\/\/www.geeksforgeeks.org\/python-stemming-words-with-nltk\/ <br>  https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/\n- Replacing values in DataFrame: https:\/\/www.python-course.eu\/pandas_replacing_values.php\n- Converting list into pandas series: https:\/\/www.geeksforgeeks.org\/creating-a-pandas-series-from-lists\/","feff5dc2":"# Conclusion","fe55a41c":"## Save & Submit Model with JCOPML","ae33a7c4":"##### Save Model","0b621606":"### Text Processing","4bb47bfe":"##### 2) Sentence & Word Tokenization; Punctuation and Words Removal ","6684d87d":"### Linear SVM is the best algorithm for classification in the case \"Disaster Tweets\n#### Parameter\n- 'algo__alpha': 0.0003, \n- 'algo__loss': 'log', \n- 'algo__max_iter': 8, \n- 'algo__penalty': 'elasticnet', \n- 'algo__tol': 0.0001}\n\n#### Score\n- train: 0.960919540229885 \n- 0.7876847290640395 \n- test: 0.8214051214707814\n\n#### Further Steps\n1. Use RandomizedSearchCV to once again crosscheck whether the score can be better by tuning the parameter\n2. Try to apply LSTM to improve the score","067ae643":"##### Submit Model","cc6170a6":"##### Display text samples","92b82f91":"# Linear SVM\n\nsource: https:\/\/towardsdatascience.com\/multi-class-text-classification-model-comparison-and-selection-5eb066197568#:~:text=Linear%20Support%20Vector%20Machine%20is,the%20best%20text%20classification%20algorithms.&text=We%20achieve%20a%20higher%20accuracy,5%25%20improvement%20over%20Naive%20Bayes.","0bef0581":"##### Sample Text","8ff02e2c":"##### y_test dataset","bc401f28":"##### BoW","cbfcd9be":"# Training","b23d1039":"##### Display text samples","ff78b98b":"## Logistic Regression","d2c06fa3":"# Import Data","0b37e0d0":"#### Bag of Words - CountVectorizer","db1041c3":"# Save & Submit Model with Pickle","9080c929":"##### TFIDF","f16797ef":"# Dataset Splitting","68353d01":"#### TF-IDF","1508f966":"##### joining the series into dataframe","93e8669d":"# Visualize the Target Label"}}