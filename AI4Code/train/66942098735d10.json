{"cell_type":{"8a453fb8":"code","01b4f698":"code","93b3bbed":"code","188343bc":"code","0b93c30b":"code","d461e2f7":"code","41a46cac":"code","9511b2ad":"code","7928c7b1":"code","f43cdbc0":"code","eb11ee63":"code","be9811ad":"code","78e9e7bc":"code","f4fa89a2":"code","055c0e03":"code","a5958f00":"code","441b8ddc":"code","9bd71df1":"code","c0cf7030":"code","6c6455a5":"code","29efdf8b":"code","631fd9cf":"code","494c5ced":"code","de1e7d89":"code","e1f9fecd":"code","55b88ab1":"code","60004096":"code","3723a006":"markdown","013cd678":"markdown","1695be52":"markdown","844a718a":"markdown","16c53f78":"markdown","52bf0c47":"markdown","e9939c6e":"markdown","9300eaa2":"markdown","b60072f6":"markdown","e6bd0fea":"markdown","5c794362":"markdown","0ab6bd70":"markdown","be5540e6":"markdown","ed6344a7":"markdown","2e4fa81a":"markdown"},"source":{"8a453fb8":"#Importing basic libaries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport datetime","01b4f698":"# reading the data from csv file\nWORKING_DIR  = \"\/kaggle\/input\/nba2k20-player-dataset\/nba2k20-full.csv\"\nnba_full_data = pd.read_csv(WORKING_DIR)","93b3bbed":"# Displaying the first five players data\nnba_full_data.head()","188343bc":"nba_full_data.info()","0b93c30b":"#convert columns to integers\nfeatures = ['draft_round','draft_peak']\nnba_full_data[features] = nba_full_data[features].replace('Undrafted','0')\nnba_full_data[features] = nba_full_data[features].astype(int)","d461e2f7":"## Creating Age and Experience by using current year\nnba_full_data_copy = nba_full_data.copy()\n\n# Removing the un-necessary data in the dataset\nnba_full_data_copy['salary'] = nba_full_data_copy['salary'].apply(lambda x: float(x.replace('$','')))\nnba_full_data_copy['height'] = nba_full_data_copy['height'].apply(lambda x: float(x.split('\/')[1]))\nnba_full_data_copy['weight'] = nba_full_data_copy['weight'].apply(lambda x: float(x.split('\/')[1].split('kg')[0]))","41a46cac":"# Creating new column for Age\nfor index,value in enumerate(nba_full_data['b_day']):\n    nba_full_data_copy.loc[index,'Age']= int( (datetime.datetime.today() - datetime.datetime.strptime(value,'%m\/%d\/%y')).days \/ 365.25)\n \n# Creating new column for Experience\nfor index,value in enumerate(nba_full_data['draft_year']):\n    nba_full_data_copy.loc[index,'Experience']= int( (datetime.datetime.today() - datetime.datetime.strptime(str(value),'%Y')).days \/ 365.25)","9511b2ad":"# Creating new column for BMI by using Weight and Height of the player\nnba_full_data_copy['bmi'] = nba_full_data_copy['weight'] \/ (nba_full_data_copy['height']**2)","7928c7b1":"nba_full_data_copy.corr()","f43cdbc0":"nba_full_data_copy.hist(bins=30,figsize=(20,15))\nplt.show()","eb11ee63":"# Finding the useful features to predict the salary\ncorre = nba_full_data_copy.corr()\ncorre['salary'].sort_values(ascending=False)","be9811ad":"# Players who are not in the team, assigning the value to No team\nnba_full_data_copy.fillna('no team',inplace=True)","78e9e7bc":"nba_full_data_copy['position'].value_counts()\n\n# Creating a new column for Team salary by using grouping team and salary columns\n\nsalary = nba_full_data_copy[['salary', 'team']]\nnew_sal = salary.groupby('team').mean().reset_index()\nboundaries = [np.NINF, 7E+6, 7.6E+6, 8.1E+6, 9E+6, 9.5E+6, np.Inf]\nlabels = [1,2,3,4,5,6]\nnew_sal['team_salary'] = pd.cut(salary.groupby('team').mean().\\\n                                reset_index()['salary'], bins=boundaries,labels=labels)\nnew_sal.drop(['salary'], axis = 1, inplace = True)\n#merging this categories to data\nnba_full_data_copy = nba_full_data_copy.merge(new_sal, on = 'team', how = 'left')","f4fa89a2":"# Grouping the players by using Country , this will help how many players are from outside USA\n# Changing the values of the country field which is outside of USA , since only 25% players are from Outside USA, so we are going \n# to have only two categories\nnba_full_data_copy.loc[nba_full_data_copy['country'] != 'USA','country'] = 'not USA'","055c0e03":"##Dropping non-useful features\ndrop_features = ['full_name','jersey','team','college','b_day','height','weight']\nnba_full_data_copy = nba_full_data_copy.drop(drop_features,axis=1)","a5958f00":"nba_full_data_copy.info()","441b8ddc":"X = nba_full_data_copy.drop(['salary'],axis=1)\ny = nba_full_data_copy['salary']","9bd71df1":"from sklearn.preprocessing import OrdinalEncoder,LabelEncoder,OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\n# determine categorical and numerical features\ncategorical_ix = X.select_dtypes(include=['object', 'bool']).columns\nnumerical_ix = X.select_dtypes(include=['int64','int32', 'float64']).columns\n\nct = ColumnTransformer([(\"ordinal\", OrdinalEncoder(),categorical_ix),\n      (\"scaler\", StandardScaler(),numerical_ix)])\n\nx_scaled =  ct.fit_transform(X) ","c0cf7030":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n## we have used number of folds as 11 for Cross validation , since we have 429 rows of datasets, its better to split \n## to equal number of folds. \n## Benefits: \n    # 1. At end of the loop we will take average of the prediction of 11 models which will give better accuracy\n    # 2. The more model train on the better accuracy we will get\n## Disadvantages:\n    # 1. Since we have set 11 folds , so we will have 11 models at the end. It will take more time to train on and need more \n    # computational power\nnumber_of_folds = 11\n\nstr_fold = StratifiedKFold(n_splits=11, random_state=None, shuffle=False)\n\nregressor = RandomForestRegressor()\nparam_grid = [{\n    'n_estimators':[300,500,700,800],\n    'max_features':[2,4,6,8],\n    'random_state':[0]\n}]\n\ngrid_search = GridSearchCV(regressor,param_grid,cv=str_fold,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(x_scaled,y)","6c6455a5":"grid_search.best_estimator_","29efdf8b":"grid_search.best_estimator_.feature_importances_","631fd9cf":"from sklearn.metrics import mean_squared_error\n\n\ny_predict_models = 0\nmse_list = []\n\nregressor = RandomForestRegressor(max_features=4, n_estimators=500, random_state=0)\n\nfor train_index,test_index in str_fold.split(X,y):\n    X_train_folds,X_test_folds = x_scaled[train_index],x_scaled[test_index]\n    y_train_folds,y_test_folds = y[train_index],y[test_index]\n    \n    print(X_train_folds.shape,y_train_folds.shape) # printing shape of train and test of each folds\n     \n    regressor.fit(X_train_folds,y_train_folds)\n    y_predict = regressor.predict(X_test_folds)\n    \n    mse = mean_squared_error(y_test_folds,y_predict)\n    mse_list.append(np.sqrt(mse)) # storing all the Mean squared error of the each model\n    y_predict_models += (y_predict) # adding the current model prediction with the previous model\n    \n    \n","494c5ced":"## Calculating MSE of the model\ny_predict_final = y_predict_models\/\/number_of_folds\nmse = mean_squared_error(y_test_folds,y_predict_final)\n\nprint(f'The model prdiction final mse {np.sqrt(mse)}')\nfinal_mse = sum(mse_list)\/\/number_of_folds\nprint(f'The model prdiction final mse from the list {final_mse}')","de1e7d89":"print('Accuracy: ', round(regressor.score(x_scaled, y)*100, 2))","e1f9fecd":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(regressor.predict(x_scaled)))), y=regressor.predict(x_scaled),\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y))), y=y,\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","55b88ab1":"mse = mean_squared_error(y_test_folds,y_predict_final)\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_predict_final))), y=y_predict_final,\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test_folds))), y=y_test_folds,\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","60004096":"# Setting confidence level as 95% for the model\nfrom scipy import stats\nconfidence = 0.95\nsquared_errors = (regressor.predict(x_scaled)-y)**2\nnp.sqrt(stats.t.interval(confidence,len(squared_errors)-1,loc=squared_errors.mean(),scale=stats.sem(squared_errors)))","3723a006":"## Feature Engineering","013cd678":"## Feature Scaling","1695be52":"## Calculating performance of the model","844a718a":"## Creating Features and Target columns","16c53f78":"## Cleaning the data","52bf0c47":"## Importing Data and libaries","e9939c6e":"# NBA2020 Predict Salary\n\nPlease find the project summary and datasets from below Kaggle link\nhttps:\/\/www.kaggle.com\/isaienkov\/nba2k20-player-dataset\n\n\n**Description:** By using the player rating, age and few attributes we are trying to predict the salary of the NBA player in 2020. In this notebook we will be doing Data preparation, cleaning, Feature Engineering and scaling and Cross validation. Finally we will built a model to predict the salary of the player. Here we have used *GridSearchCV* to find out the best model.\n","9300eaa2":"## Feature Selection","b60072f6":"## Visualizing final output of the model","e6bd0fea":"## Getting one with data","5c794362":"## Data Visualization","0ab6bd70":"## Model Selection","be5540e6":"### Random Forest Regressor\n### StratifiedKFold Cross validation","ed6344a7":"### Feature Extraction","2e4fa81a":"## Model Training"}}