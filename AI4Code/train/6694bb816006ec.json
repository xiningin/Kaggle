{"cell_type":{"0aa68751":"code","106f9e5c":"code","1f542ed5":"code","d4ae9164":"code","e12a6f35":"code","75a3abc5":"code","8fd09ba9":"code","0d6b03b6":"code","d182da01":"code","7e69a833":"code","e89d6a57":"code","674f29e1":"code","e575e876":"code","fbe70946":"code","4e9409c2":"code","daea71f5":"code","2da8cf6c":"code","5f0c396f":"code","9894a1ba":"code","e9301616":"code","f4beeeb1":"code","2d5b0c9f":"code","6466551c":"code","3914df2b":"code","6d22ec73":"code","1e12b7a9":"code","93fd5f33":"code","7b20a909":"code","f3590619":"code","b2f08f73":"code","add5f195":"code","9e8707e3":"code","16982ac4":"code","43102d1a":"code","fd161f08":"code","c5ca0c67":"code","85bab6e8":"code","6eb12115":"code","b0265ae9":"code","80e8670c":"code","591a32e5":"code","6b90ebf4":"code","862ba462":"code","689e6120":"code","bd323f2b":"code","4dffc74c":"code","dc5fcb77":"code","77bc4438":"code","fdb660e5":"code","d53d2135":"code","ed94de8f":"code","ec2a7e7f":"code","0cb52498":"code","cc1b225d":"code","aff7fbd5":"code","c451bfa3":"code","2a82021e":"code","7a7ffcd8":"code","22df10e3":"code","5f747835":"code","538c540a":"code","b954059f":"code","9364767d":"code","f1df125f":"code","b6a36f27":"code","2a289ac2":"code","250449e9":"code","68c5bff5":"code","6c7534ba":"code","bdf61318":"code","cbc6f32c":"code","d3dd6502":"code","8ab1568e":"code","9a43044f":"code","7c3b97c1":"code","824ec5eb":"code","4501a9e1":"code","93a72d58":"code","2330f14b":"code","6f801d1e":"code","faee00af":"code","af456c7a":"code","6bcfcb0b":"code","8428808d":"code","e7356467":"code","b8902edb":"code","cf58d4b6":"code","04e525a5":"code","1eaa31e1":"code","4d1af728":"code","2de68c7d":"code","b5e79b6e":"code","016761fa":"code","2639735f":"code","d96ab653":"code","f7120cff":"code","fcaf38ef":"code","3f0bb324":"code","6431439f":"code","34f61af8":"code","bdf66faf":"code","4eac4dd4":"code","85ca7034":"code","b13a8fc2":"code","5be8a607":"code","64e7b8fb":"code","d6b1efd3":"code","b6c8ecc5":"code","97b55d49":"code","a832a558":"code","0300e85c":"code","0ad8b1f3":"code","ca4f54f9":"code","2677c0a4":"code","3075d6e1":"code","3073c500":"code","841a38b6":"code","98152e03":"code","ab33032e":"code","359a3bb9":"code","5848f5bd":"code","08737a6d":"code","00d48824":"code","3368642e":"code","3d4c2b3b":"code","1e7b220e":"code","98a25468":"code","218178b4":"code","47bea74e":"code","836a3a01":"code","3e2b7afe":"code","bf2d030b":"code","cd136893":"code","1768d3f4":"code","8f800e34":"code","b860baf6":"code","c7afd9bd":"code","b679bd26":"code","7254b675":"code","606e4275":"code","e036137e":"code","9869373f":"code","ab071e8c":"code","c7f7cf2e":"code","d5bd15bf":"code","3427aa4f":"code","92c0dac0":"code","915ec624":"code","9c2757a4":"code","ee027e14":"code","1806f6d7":"code","ca657be9":"code","43c3cda3":"code","e9ba1024":"code","44321da7":"code","926c36e1":"code","e998487a":"code","66e558a6":"code","a8cdab24":"code","bf4b4ca1":"code","344a703c":"code","3d38ef38":"code","dd7ff0de":"code","ac5ecce4":"code","95864a6c":"code","a1ab442a":"code","2e5f905a":"code","dcba41ad":"code","73163fcb":"code","c3f7124c":"code","86b8c561":"code","d3acd645":"code","2db93984":"code","16472e71":"code","4f95ed08":"code","f6fd190b":"markdown","69d13993":"markdown","e36b5925":"markdown","dc69231e":"markdown","f45c4979":"markdown","9b72600f":"markdown","4528a0da":"markdown","76b25d10":"markdown","e15aab2a":"markdown","c84394f0":"markdown","1dee8e6c":"markdown","b712fb3d":"markdown","999dacb4":"markdown","18c08540":"markdown","a30be822":"markdown","1b5af5d3":"markdown","3188b85b":"markdown","56358651":"markdown","b48515af":"markdown","382be25b":"markdown","e5ef4eb1":"markdown","87454946":"markdown","95d74e6f":"markdown","6758d5eb":"markdown","b9c23161":"markdown","b512d432":"markdown","1a94d466":"markdown","ebf52e64":"markdown","34078cfd":"markdown","359d1c6b":"markdown","7de12c3e":"markdown","424c8218":"markdown","46520167":"markdown","5bdb1184":"markdown","793ce93e":"markdown","e08a6729":"markdown","c566683b":"markdown","0da9608f":"markdown","bdfe14c6":"markdown","53c4d2d3":"markdown","5d610a29":"markdown","8d0b7b7c":"markdown","ed55f195":"markdown","e65e3fbe":"markdown","eab2cb0f":"markdown","7bbfdebe":"markdown","033d448e":"markdown","55995a9a":"markdown","f7bf1454":"markdown","638fb846":"markdown","4455d33e":"markdown","844301a4":"markdown","90e675d7":"markdown","d323421a":"markdown","63c19d4f":"markdown","2c70d1db":"markdown","e5c721b7":"markdown","f26f11ce":"markdown","272c0521":"markdown","fe7b4f12":"markdown","e1d401d6":"markdown","cd153fbd":"markdown","48a0b653":"markdown","312935f5":"markdown","d9217c7c":"markdown","13adeeac":"markdown","b9452dc0":"markdown","54c64272":"markdown","87af8545":"markdown","03bb1356":"markdown","91fce80a":"markdown","113e4b3e":"markdown","3961ca09":"markdown","8b49574d":"markdown","2bdc4b44":"markdown","e9aa56bb":"markdown","50b274ab":"markdown","05a5905a":"markdown","a5ed7a7c":"markdown","38e39986":"markdown","af45630f":"markdown","adab4ba0":"markdown","cbbf388d":"markdown","46752def":"markdown","c392406d":"markdown","1d46eec1":"markdown","c37786c9":"markdown","a32eceb1":"markdown","ffeb6fe6":"markdown","dec34ac3":"markdown","947dc1ee":"markdown","053f6286":"markdown","42b18dfc":"markdown","fb3eb795":"markdown","f1cc1411":"markdown","8a5bd0ef":"markdown","2afcb7cb":"markdown","69fdf251":"markdown","a33f86ee":"markdown","69ccefaa":"markdown","65cf2a94":"markdown","2370d67c":"markdown","fddbdbd0":"markdown","1f541212":"markdown","7d8c1536":"markdown","a8a5bc59":"markdown","d2f70fc0":"markdown","2129c292":"markdown","20d8639c":"markdown","89b9c0c4":"markdown","318d08cf":"markdown","4287ef57":"markdown","96120b21":"markdown","c8ba099a":"markdown","e13933ca":"markdown","abc00923":"markdown","f3295fc0":"markdown","91308cb6":"markdown","c8f56d0d":"markdown","99b13ae5":"markdown","68b38c7a":"markdown","58463f76":"markdown","c8db56ae":"markdown","849b25e1":"markdown","d81e273f":"markdown","f99ecdfd":"markdown","89c3b920":"markdown","7bfdff50":"markdown","39631707":"markdown","595db2ef":"markdown","c70fb7de":"markdown"},"source":{"0aa68751":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","106f9e5c":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1f542ed5":"# Importing lead dataset\nlead_data = pd.read_csv(\"..\/input\/leads-data\/Leads.csv\")\nlead_data.head()","d4ae9164":"# checking the shape of the data \nlead_data.shape","e12a6f35":"# checking non null count and datatype of the variables\nlead_data.info()","75a3abc5":"# Describing data\nlead_data.describe()","8fd09ba9":"# Converting 'Select' values to NaN.\nlead_data = lead_data.replace('Select', np.nan)","0d6b03b6":"# checking the columns for null values\nlead_data.isnull().sum()","d182da01":"# Finding the null percentages across columns\nround(lead_data.isnull().sum()\/len(lead_data.index),2)*100","7e69a833":"# dropping the columns with missing values greater than or equal to 40% .\nlead_data=lead_data.drop(columns=['How did you hear about X Education','Lead Quality','Lead Profile',\n                                  'Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score',\n                                 'Asymmetrique Profile Score'])","e89d6a57":"# Finding the null percentages across columns after removing the above columns\nround(lead_data.isnull().sum()\/len(lead_data.index),2)*100","674f29e1":"plt.figure(figsize=(17,5))\nsns.countplot(lead_data['Specialization'])\nplt.xticks(rotation=90)","e575e876":"# Creating a separate category called 'Others' for this \nlead_data['Specialization'] = lead_data['Specialization'].replace(np.nan, 'Others')","fbe70946":"# Visualizing Tags column\nplt.figure(figsize=(10,7))\nsns.countplot(lead_data['Tags'])\nplt.xticks(rotation=90)","4e9409c2":"# Imputing the missing data in the tags column with 'Will revert after reading the email'\nlead_data['Tags']=lead_data['Tags'].replace(np.nan,'Will revert after reading the email')","daea71f5":"# Visualizing this column\nsns.countplot(lead_data['What matters most to you in choosing a course'])\nplt.xticks(rotation=45)","2da8cf6c":"# Finding the percentage of the different categories of this column:\nround(lead_data['What matters most to you in choosing a course'].value_counts(normalize=True),2)*100","5f0c396f":"# Dropping this column \nlead_data=lead_data.drop('What matters most to you in choosing a course',axis=1)","9894a1ba":"sns.countplot(lead_data['What is your current occupation'])\nplt.xticks(rotation=45)","e9301616":"# Finding the percentage of the different categories of this column:\nround(lead_data['What is your current occupation'].value_counts(normalize=True),2)*100","f4beeeb1":"# Imputing the missing data in the 'What is your current occupation' column with 'Unemployed'\nlead_data['What is your current occupation']=lead_data['What is your current occupation'].replace(np.nan,'Unemployed')","2d5b0c9f":"plt.figure(figsize=(17,5))\nsns.countplot(lead_data['Country'])\nplt.xticks(rotation=90)","6466551c":"# Imputing the missing data in the 'Country' column with 'India'\nlead_data['Country']=lead_data['Country'].replace(np.nan,'India')","3914df2b":"plt.figure(figsize=(10,5))\nsns.countplot(lead_data['City'])\nplt.xticks(rotation=90)","6d22ec73":"# Finding the percentage of the different categories of this column:\nround(lead_data['City'].value_counts(normalize=True),2)*100","1e12b7a9":"# Imputing the missing data in the 'City' column with 'Mumbai'\nlead_data['City']=lead_data['City'].replace(np.nan,'Mumbai')","93fd5f33":"# Finding the null percentages across columns after removing the above columns\nround(lead_data.isnull().sum()\/len(lead_data.index),2)*100","7b20a909":"# Dropping the rows with null values\nlead_data.dropna(inplace = True)","f3590619":"# Finding the null percentages across columns after removing the above columns\nround(lead_data.isnull().sum()\/len(lead_data.index),2)*100","b2f08f73":"# Percentage of rows retained \n(len(lead_data.index)\/9240)*100","add5f195":"lead_data[lead_data.duplicated()]","9e8707e3":"Converted = (sum(lead_data['Converted'])\/len(lead_data['Converted'].index))*100\nConverted","16982ac4":"plt.figure(figsize=(10,5))\nsns.countplot(x = \"Lead Origin\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 45)","43102d1a":"plt.figure(figsize=(13,5))\nsns.countplot(x = \"Lead Source\", hue = \"Converted\", data = lead_data, palette='Set1')\nplt.xticks(rotation = 90)","fd161f08":"# Need to replace 'google' with 'Google'\nlead_data['Lead Source'] = lead_data['Lead Source'].replace(['google'], 'Google')","c5ca0c67":"# Creating a new category 'Others' for some of the Lead Sources which do not have much values.\nlead_data['Lead Source'] = lead_data['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',\n  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Others')\n","85bab6e8":"# Visualizing again\nplt.figure(figsize=(10,5))\nsns.countplot(x = \"Lead Source\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","6eb12115":"sns.countplot(x = \"Do Not Email\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","b0265ae9":"sns.countplot(x = \"Do Not Call\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","80e8670c":"lead_data['TotalVisits'].describe(percentiles=[0.05,.25, .5, .75, .90, .95, .99])","591a32e5":"sns.boxplot(lead_data['TotalVisits'],orient='vert',palette='Set1')","6b90ebf4":"percentiles = lead_data['TotalVisits'].quantile([0.05,0.95]).values\nlead_data['TotalVisits'][lead_data['TotalVisits'] <= percentiles[0]] = percentiles[0]\nlead_data['TotalVisits'][lead_data['TotalVisits'] >= percentiles[1]] = percentiles[1]","862ba462":"# Visualizing again\nsns.boxplot(lead_data['TotalVisits'],orient='vert',palette='Set1')","689e6120":"sns.boxplot(y = 'TotalVisits', x = 'Converted', data = lead_data,palette='Set1')","bd323f2b":"lead_data['Total Time Spent on Website'].describe()","4dffc74c":"sns.boxplot(lead_data['Total Time Spent on Website'],orient='vert',palette='Set1')","dc5fcb77":"sns.boxplot(y = 'Total Time Spent on Website', x = 'Converted', data = lead_data,palette='Set1')","77bc4438":"lead_data['Page Views Per Visit'].describe()","fdb660e5":"sns.boxplot(lead_data['Page Views Per Visit'],orient='vert',palette='Set1')","d53d2135":"percentiles = lead_data['Page Views Per Visit'].quantile([0.05,0.95]).values\nlead_data['Page Views Per Visit'][lead_data['Page Views Per Visit'] <= percentiles[0]] = percentiles[0]\nlead_data['Page Views Per Visit'][lead_data['Page Views Per Visit'] >= percentiles[1]] = percentiles[1]\n","ed94de8f":"# Visualizing again\nsns.boxplot(lead_data['Page Views Per Visit'],palette='Set1',orient='vert')","ec2a7e7f":"sns.boxplot(y = 'Page Views Per Visit', x = 'Converted', data =lead_data,palette='Set1')","0cb52498":"lead_data['Last Activity'].describe()","cc1b225d":"plt.figure(figsize=(15,6))\nsns.countplot(x = \"Last Activity\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","aff7fbd5":"# We can club the last activities to \"Other_Activity\" which are having less data.\nlead_data['Last Activity'] = lead_data['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked', \n                                                       'Visited Booth in Tradeshow', 'Approached upfront',\n                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'], 'Other_Activity')","c451bfa3":"# Visualizing again\nplt.figure(figsize=(15,6))\nsns.countplot(x = \"Last Activity\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","2a82021e":"plt.figure(figsize=(15,6))\nsns.countplot(x = \"Country\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","7a7ffcd8":"plt.figure(figsize=(15,6))\nsns.countplot(x = \"Specialization\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","22df10e3":"plt.figure(figsize=(15,6))\nsns.countplot(x = \"What is your current occupation\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","5f747835":"sns.countplot(x = \"Search\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","538c540a":"sns.countplot(x = \"Magazine\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","b954059f":"sns.countplot(x = \"Newspaper Article\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","9364767d":"sns.countplot(x = \"X Education Forums\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","f1df125f":"sns.countplot(x = \"Newspaper\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","b6a36f27":"sns.countplot(x = \"Digital Advertisement\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","2a289ac2":"sns.countplot(x = \"Through Recommendations\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","250449e9":"sns.countplot(x = \"Receive More Updates About Our Courses\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","68c5bff5":"plt.figure(figsize=(15,6))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","6c7534ba":"sns.countplot(x = \"Update me on Supply Chain Content\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","bdf61318":"sns.countplot(x = \"Get updates on DM Content\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","cbc6f32c":"plt.figure(figsize=(15,5))\nsns.countplot(x = \"City\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","d3dd6502":"sns.countplot(x = \"I agree to pay the amount through cheque\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","8ab1568e":"sns.countplot(x = \"A free copy of Mastering The Interview\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","9a43044f":"plt.figure(figsize=(15,5))\nsns.countplot(x = \"Last Notable Activity\", hue = \"Converted\", data = lead_data,palette='Set1')\nplt.xticks(rotation = 90)","7c3b97c1":"lead_data = lead_data.drop(['Lead Number','Tags','Country','Search','Magazine','Newspaper Article','X Education Forums',\n                            'Newspaper','Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses',\n                            'Update me on Supply Chain Content','Get updates on DM Content','I agree to pay the amount through cheque',\n                            'A free copy of Mastering The Interview'],1)","824ec5eb":"lead_data.shape","4501a9e1":"lead_data.info()","93a72d58":"vars =  ['Do Not Email', 'Do Not Call']\n\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\nlead_data[vars] = lead_data[vars].apply(binary_map)","2330f14b":"# Creating a dummy variable for the categorical variables and dropping the first one.\ndummy_data = pd.get_dummies(lead_data[['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation',\n                             'City','Last Notable Activity']], drop_first=True)\ndummy_data.head()","6f801d1e":"# Concatenating the dummy_data to the lead_data dataframe\nlead_data = pd.concat([lead_data, dummy_data], axis=1)\nlead_data.head()","faee00af":"lead_data = lead_data.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation',\n                             'City','Last Notable Activity'], axis = 1)","af456c7a":"lead_data.head()","6bcfcb0b":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = lead_data.drop(['Prospect ID','Converted'], axis=1)\nX.head()","8428808d":"# Putting target variable to y\ny = lead_data['Converted']\n\ny.head()","e7356467":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","b8902edb":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","cf58d4b6":"# Checking the Lead Conversion rate\nConverted = (sum(lead_data['Converted'])\/len(lead_data['Converted'].index))*100\nConverted","04e525a5":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)             # running RFE with 20 variables as output\nrfe = rfe.fit(X_train, y_train)","1eaa31e1":"rfe.support_","4d1af728":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","2de68c7d":"# Viewing columns selected by RFE\ncols = X_train.columns[rfe.support_]\ncols","b5e79b6e":"import statsmodels.api as sm","016761fa":"X_train_sm = sm.add_constant(X_train[cols])\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nresult = logm1.fit()\nresult.summary()","2639735f":"# Dropping the column 'What is your current occupation_Housewife'\ncol1 = cols.drop('What is your current occupation_Housewife')","d96ab653":"X_train_sm = sm.add_constant(X_train[col1])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","f7120cff":"col1 = col1.drop('Last Notable Activity_Had a Phone Conversation')","fcaf38ef":"X_train_sm = sm.add_constant(X_train[col1])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","3f0bb324":"col1 = col1.drop('What is your current occupation_Student')","6431439f":"X_train_sm = sm.add_constant(X_train[col1])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","34f61af8":"\ncol1 = col1.drop('Lead Origin_Lead Add Form')","bdf66faf":"X_train_sm = sm.add_constant(X_train[col1])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","4eac4dd4":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col1].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","85ca7034":"# Dropping the column  'What is your current occupation_Unemployed' because it has high VIF\ncol1 = col1.drop('What is your current occupation_Unemployed')","b13a8fc2":"X_train_sm = sm.add_constant(X_train[col1])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","5be8a607":"# Dropping the column  'Lead Origin_Lead Import' because it has high Pvalue\ncol1 = col1.drop('Lead Origin_Lead Import')","64e7b8fb":"X_train_sm = sm.add_constant(X_train[col1])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","d6b1efd3":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col1].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b6c8ecc5":"# Dropping the column  'Last Activity_Unsubscribed' to reduce the variables\ncol1 = col1.drop('Last Activity_Unsubscribed')","97b55d49":"X_train_sm = sm.add_constant(X_train[col1])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","a832a558":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col1].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0300e85c":"# Dropping the column  'Last Notable Activity_Unreachable' to reduce the variables\ncol1 = col1.drop('Last Notable Activity_Unreachable')","0ad8b1f3":"X_train_sm = sm.add_constant(X_train[col1])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","ca4f54f9":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col1].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col1].values, i) for i in range(X_train[col1].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2677c0a4":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","3075d6e1":"# Reshaping into an array\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","3073c500":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","841a38b6":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","98152e03":"from sklearn import metrics\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","ab33032e":"# The confusion matrix indicates as below\n# Predicted     not_converted    converted\n# Actual\n# not_converted        3461      444\n# converted            719       1727  ","359a3bb9":"# Let's check the overall accuracy.\nprint('Accuracy :',metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","5848f5bd":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","08737a6d":"# Sensitivity of our logistic regression model\nprint(\"Sensitivity : \",TP \/ float(TP+FN))","00d48824":"# Let us calculate specificity\nprint(\"Specificity : \",TN \/ float(TN+FP))","3368642e":"# Calculate false postive rate - predicting converted lead when the lead actually was not converted\nprint(\"False Positive Rate :\",FP\/ float(TN+FP))","3d4c2b3b":"# positive predictive value \nprint(\"Positive Predictive Value :\",TP \/ float(TP+FP))","1e7b220e":"# Negative predictive value\nprint (\"Negative predictive value :\",TN \/ float(TN+ FN))","98a25468":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","218178b4":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_prob, drop_intermediate = False )","47bea74e":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","836a3a01":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","3e2b7afe":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","bf2d030b":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","cd136893":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.34 else 0)\n\ny_train_pred_final.head()","1768d3f4":"y_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_prob.map( lambda x: round(x*100))\n\ny_train_pred_final.head()","8f800e34":"# Let's check the overall accuracy.\nprint(\"Accuracy :\",metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted))","b860baf6":"# Confusion matrix\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","c7afd9bd":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","b679bd26":"# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity : \",TP \/ float(TP+FN))","7254b675":"# Let us calculate specificity\nprint(\"Specificity :\",TN \/ float(TN+FP))","606e4275":"# Calculate false postive rate - predicting converted lead when the lead was actually not have converted\nprint(\"False Positive rate : \",FP\/ float(TN+FP))","e036137e":"# Positive predictive value \nprint(\"Positive Predictive Value :\",TP \/ float(TP+FP))","9869373f":"# Negative predictive value\nprint(\"Negative Predictive Value : \",TN \/ float(TN+ FN))","ab071e8c":"#Looking at the confusion matrix again\n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nconfusion","c7f7cf2e":"# Precision\nTP \/ TP + FP\n\nprint(\"Precision : \",confusion[1,1]\/(confusion[0,1]+confusion[1,1]))","d5bd15bf":"# Recall\nTP \/ TP + FN\n\nprint(\"Recall :\",confusion[1,1]\/(confusion[1,0]+confusion[1,1]))","3427aa4f":"from sklearn.metrics import precision_score, recall_score","92c0dac0":"print(\"Precision :\",precision_score(y_train_pred_final.Converted , y_train_pred_final.predicted))","915ec624":"print(\"Recall :\",recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","9c2757a4":"from sklearn.metrics import precision_recall_curve\n\ny_train_pred_final.Converted, y_train_pred_final.predicted","ee027e14":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","1806f6d7":"# plotting a trade-off curve between precision and recall\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","ca657be9":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits',\n                                                                                                        'Total Time Spent on Website',\n                                                                                                        'Page Views Per Visit']])","43c3cda3":"# Assigning the columns selected by the final model to the X_test \nX_test = X_test[col1]\nX_test.head()","e9ba1024":"# Adding a const\nX_test_sm = sm.add_constant(X_test)\n\n# Making predictions on the test set\ny_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","44321da7":"# Converting y_test_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","926c36e1":"# Let's see the head\ny_pred_1.head()","e998487a":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","66e558a6":"# Putting Prospect ID to index\ny_test_df['Prospect ID'] = y_test_df.index","a8cdab24":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","bf4b4ca1":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","344a703c":"y_pred_final.head()","3d38ef38":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_prob'})","dd7ff0de":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(columns=['Prospect ID','Converted','Converted_prob'])","ac5ecce4":"# Let's see the head of y_pred_final\ny_pred_final.head()\n","95864a6c":"y_pred_final['final_predicted'] = y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.34 else 0)","a1ab442a":"y_pred_final.head()","2e5f905a":"# Let's check the overall accuracy.\nprint(\"Accuracy :\",metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted))","dcba41ad":"# Making the confusion matrix\nconfusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","73163fcb":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","c3f7124c":"# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity :\",TP \/ float(TP+FN))","86b8c561":"# Let us calculate specificity\nprint(\"Specificity :\",TN \/ float(TN+FP))","d3acd645":"y_pred_final['Lead_Score'] = y_pred_final.Converted_prob.map( lambda x: round(x*100))\n\ny_pred_final.head()","2db93984":"hot_leads=y_pred_final.loc[y_pred_final[\"Lead_Score\"]>=85]\nhot_leads","16472e71":"print(\"The Prospect ID of the customers which should be contacted are :\")\n\nhot_leads_ids = hot_leads[\"Prospect ID\"].values.reshape(-1)\nhot_leads_ids","4f95ed08":"res.params.sort_values(ascending=False)","f6fd190b":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","69d13993":"### 16) X Education Forums","e36b5925":"### Inference\n**Focus should be more on the Specialization with high conversion rate.**","dc69231e":"# Exploratory Data Anaysis","f45c4979":"### Checking for VIF values:","9b72600f":"#### 2) Tags column\n\n'Tags' column has 36% missing values","4528a0da":"We can see that this is highly skewed column but it is an important information w.r.t. to the lead. Since most values are 'India' , we can impute missing values in this column with this value.","76b25d10":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","e15aab2a":"### 10) Country","c84394f0":"Since the  most values are 'Unemployed' , we can impute missing values in this column with this value. ","1dee8e6c":"### 2) Lead Origin","b712fb3d":"### Checking for VIF values:","999dacb4":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","18c08540":"### 4) Do not Email ","a30be822":"## Data Preparation","1b5af5d3":"#### We found out that our specificity was good (~88%) but our sensitivity was only 70%. Hence, this needed to be taken care of.\n#### We have got sensitivity of 70% and this was mainly because of the cut-off point of 0.5 that we had arbitrarily chosen. Now, this cut-off point had to be optimised in order to get a decent value of sensitivity and for this we will use the ROC curve.","3188b85b":"#### 6)  Column: 'City'\n\nThis column has 40% missing values","56358651":"### 9) Last Activity","b48515af":"### 12) What is your current occupation","382be25b":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","e5ef4eb1":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","87454946":"**Since we have higher (0.89) area under the ROC curve , therefore our model is a good one.**\n\n### Finding Optimal Cutoff Point\n\n**Above we had chosen an arbitrary cut-off value of 0.5. We need to determine the best cut-off value and the below section deals with that. Optimal cutoff probability is that prob where we get balanced sensitivity and specificity**\n","95d74e6f":"### 19) Through Recommendations","6758d5eb":"### Checking for VIF values:","b9c23161":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","b512d432":"### 20) Receive More Updates About Our Courses","1a94d466":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","ebf52e64":"## Precision and Recall","34078cfd":"* **Precision = Also known as Positive Predictive Value, it refers to the percentage of the results which are relevant.**\n* **Recall = Also known as Sensitivity , it refers to the percentage of total relevant results correctly classified by the algorithm.**","359d1c6b":"Since Pvalue of 'What is your current occupation_Student' is very high, we can drop this column.","7de12c3e":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","424c8218":"### 24) City","46520167":"### Assigning Lead Score to the Testing data","5bdb1184":"### 1) Converting some binary variables (Yes\/No) to 1\/0","793ce93e":"### 2) Creating Dummy variables for the categorical features:\n'Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City','Last Notable Activity'","e08a6729":"#### 5)  Column: 'Country'\n\nThis column has 27% missing values","c566683b":"### 21) Tags","0da9608f":"### Inference\n* Median for converted and unconverted leads is the same.\n\n**Nothing can be said specifically for lead conversion from Page Views Per Visit**","bdfe14c6":"### 22) Update me on Supply Chain Content","53c4d2d3":"### Inference\n* Median for converted and not converted leads are the same.\n\nNothing can be concluded on the basis of Total Visits.","5d610a29":"## Model Evaluation ","8d0b7b7c":"### Model-7","ed55f195":"We have almost 38% lead conversion rate.","e65e3fbe":"We have 9240 rows and 37 columns in our leads dataset.","eab2cb0f":"### Model-2","7bbfdebe":"### Model-9","033d448e":"### 3) Splitting the data into  train and test set.","55995a9a":"**As we can see there are a number of outliers in the data.\nWe will cap the outliers to 95% value for analysis.**","f7bf1454":"### 5) Do not call","638fb846":"### 27) Last Notable Activity","4455d33e":"### Metrics beyond simply accuracy","844301a4":"Since most values are 'Mumbai' , we can impute missing values in this column with this value. ","90e675d7":"### Making Prediction on the Train set","d323421a":"### Model-5","63c19d4f":"### Precision and recall tradeoff\u00b6","2c70d1db":"### 23) Get updates on DM Content","e5c721b7":"### 7) Total Time Spent on Website","f26f11ce":"#### 1)  Column: 'Specialization'\n\nThis column has 37% missing values","272c0521":"## Making predictions on the test set\n\n### Scaling the test data","fe7b4f12":"### Inference\n* Leads spending more time on the weblise are more likely to be converted.\n\n**Website should be made more engaging to make leads spend more time.**","e1d401d6":"### 8) Page Views Per Visit","cd153fbd":"## Logistic Regression Case Study on -\n\n## Lead Scoring\n\n### Problem Statement\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses.\n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos.\nWhen these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals.\nOnce these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%.\n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, **the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019**.\nIf they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. A typical lead conversion process can be represented using the following funnel:\n\n<img src=\"https:\/\/cdn.upgrad.com\/UpGrad\/temp\/189f213d-fade-4fe4-b506-865f1840a25a\/XNote_201901081613670.jpg\">\n\n                  \n                                \n                               Lead Conversion Process - Demonstrated as a funnel\nAs you can see, there are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom.\nIn the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.\n\nX Education has appointed you to help them select the most promising leads, i.e. the leads that are most likely to convert into paying customers.The company requires you to build a model wherein you need to assign a lead score to each of the leads such that the customers with higher lead score have a higher conversion chance and the customers with lower lead score have a lower conversion chance.**The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.**\n\n### Data\n\nYou have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column \u2018Converted\u2019 which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn\u2019t converted.\n\nAnother thing that you also need to check out for are the levels present in the categorical variables.\n\nMany of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value.\n\n### Goal\n\nThere are quite a few goals for this case study.\n\n* **Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.**\n","48a0b653":"### 13) Search","312935f5":"### Model-6","d9217c7c":"### Checking for duplicates:","13adeeac":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","b9452dc0":"## Univariate Analysis and Bivariate Analysis\n\n### 1) Converted \n#### Converted is the target variable, Indicates whether a lead has been successfully converted (1) or not (0)","54c64272":"Since most values are 'Will revert after reading the email' , we can impute missing values in this column with this value. ","87af8545":"### Choosing an arbitrary cut-off probability point of 0.5 to find the predicted labels \n\n**Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0**","03bb1356":"#### 4)  Column: 'What is your current occupation'\n\nthis column has 29% missing values","91fce80a":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","113e4b3e":"#### 3)  Column: 'What matters most to you in choosing a course'\n\nthis column has 29% missing values","3961ca09":"### 6) TotalVisits","8b49574d":"### Inference\n1. Most of the lead have their Email opened as their last activity.\n2. Conversion rate for leads with last activity as SMS Sent is almost 60%.","2bdc4b44":"### Results\n**Based on the univariate analysis we have seen that many columns are not adding any information to the model, hence we can drop them for further analysis**","e9aa56bb":"### 25) I agree to pay the amount through cheque","50b274ab":"## Recommendations:\n\n* The company **should make calls** to the leads coming from the `lead sources \"Welingak Websites\"`  and `\"Reference\"` as these are more likely to get converted.\n* The company **should make calls** to the leads who are the `\"working professionals\"` as they are more likely to get converted.\n* The company **should make calls** to the leads who spent `\"more time on the websites\"` as these are more likely to get converted.\n* The company **should make calls** to the leads coming from the `lead sources \"Olark Chat\"` as these are more likely to get converted.\n* The company **should make calls** to the leads whose `last activity` was `SMS Sent` as they are more likely to get converted.\n\n* The company **should not make calls** to the leads whose `last activity` was `\"Olark Chat Conversation\"` as they are not likely to get converted.\n* The company **should not make calls** to the leads whose `lead origin` is `\"Landing Page Submission\"` as they are not likely to get converted.\n* The company **should not make calls** to the leads whose `Specialization` was `\"Others\"` as they are not likely to get converted.\n* The company **should not make calls** to the leads who chose the option of `\"Do not Email\" as \"yes\"` as they are not likely to get converted.","05a5905a":"## Observations:\nAfter running the model on the Test Data , we obtain:\n\n* **Accuracy : 80.4 %**\n* **Sensitivity : 80.4 %**\n* **Specificity : 80.5 %**","a5ed7a7c":"### 4)  Scaling the features","38e39986":"**Creating a dataframe with the actual Converted flag and the predicted probabilities**","af45630f":"**Dropping the columns for which dummies were created**","adab4ba0":"**Since the Pvalues of all variables is 0 and VIF values are low for all the variables, model-9 is our final model. We have 12 variables in our final model.**","cbbf388d":"### Inference\n1. Google and Direct traffic generates maximum number of leads.\n2. Conversion Rate of reference leads and leads through welingak website is high.\n\n**To improve overall lead conversion rate, focus should be on improving lead converion of olark chat, organic search, direct traffic, and google leads and generate more leads from reference and welingak website.**","46752def":"### Model-3","c392406d":"### Inference\n**Most values are 'India' no such inference can be drawn**","1d46eec1":"## Model Building","c37786c9":"Since Pvalue of 'Last Notable Activity_Had a Phone Conversation' is very high, we can drop this column.","a32eceb1":"**As we can see there are a number of outliers in the data. We will cap the outliers to 95% value for analysis.**","ffeb6fe6":"From above description about counts, we can see that there are missing values present in our data. \n\n## Data Cleaning\n\n### 1)Handling the 'Select' level that is present in many of the categorical variables.\n\nWe observe that there are 'Select' values in many columns.It may be because the customer did not select any option from the  list, hence it shows 'Select'.'Select' values are as good as NULL. So we can convert these values to null values.","dec34ac3":"We see there are no duplicate records in our lead dataset.","947dc1ee":"We see that for some columns we have high percentage of missing values. We can drop the columns with missing values greater than 40% .","053f6286":"### Making the Confusion matrix","42b18dfc":"## Results :\n\n### 1) Comparing the values obtained for Train & Test:\n\n#### Train Data: \n\n* **Accuracy : 81.0 %**\n* **Sensitivity : 81.7 %**\n* **Specificity : 80.6 %**\n\n#### Test Data: \n\n* **Accuracy : 80.4 %**\n* **Sensitivity : 80.4 %**\n* **Specificity : 80.5 %**\n\n**Thus we have achieved our goal of getting a ballpark of the target lead conversion rate to be around 80% . The Model seems to predict the Conversion Rate very well and we should be able to give the CEO confidence in making good calls based on this model to get a higher lead conversion rate of 80%.**","fb3eb795":"### Model-8","f1cc1411":"Now we don't have any missing value in the dataset. \n### We can find the percentage of rows retained.","8a5bd0ef":"### 18) Digital Advertisement","2afcb7cb":"**The above graph shows the trade-off between the Precision and Recall .","69fdf251":"### 11) Specialization","a33f86ee":"Using sklearn utilities for the same","69ccefaa":"#### Rest missing values are under 2% so we can drop these rows.\n","65cf2a94":"### Assigning Lead Score to the Training data\n","2370d67c":"**So there are 368 leads which can be contacted and have a high chance of getting converted.  The Prospect ID of the customers to be contacted are :**","fddbdbd0":"Since Pvalue of 'What is your current occupation_Housewife' is very high, we can drop this column.","1f541212":"## Plotting the ROC Curve\n\nAn ROC curve demonstrates several things:\n\n* It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n* The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n* The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","7d8c1536":"#### All the dataypes of the variables are in correct format.","a8a5bc59":"### Inference\n1. Working Professionals going for the course have high chances of joining it.\n2. Unemployed leads are the most in numbers but has around 30-35% conversion rate.","d2f70fc0":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","2129c292":"### Inference\n**Most leads are from mumbai with around 50% conversion rate.**","20d8639c":"### 2) Finding out the leads which should be contacted:\n#### The customers which should be contacted are the customers whose \"Lead Score\" is equal to or greater than 85. They can be termed as 'Hot Leads'.","89b9c0c4":"### Checking for VIF values:","318d08cf":"There is 37% missing values present in the Specialization column .It may be possible that the lead may leave this column blank if he may be a student or not having any specialization or his specialization is not there in the options given. So we can create a another category 'Others' for this.","4287ef57":"### 3) Finding out the Important Features  from our final model:","96120b21":"### 14) Magazine","c8ba099a":"### Assessing the model with StatsModels\n\n### Model-1","e13933ca":"### 3) Lead Source","abc00923":"The lead conversion rate is 38%.","f3295fc0":"#### We have retained 98% of the rows after cleaning the data . ","91308cb6":"### Inference\nSince this is a column which is generated by the sales team for their analysis , so this is not available for model building . So we will need to remove this column before building the model.","c8f56d0d":"#### From the curve above, 0.34 is the optimum point to take it as a cutoff probability.","99b13ae5":"### 17) Newspaper","68b38c7a":"## Feature Selection Using RFE","58463f76":"Since Pvalue of 'Lead Origin_Lead Add Form' is very high, we can drop this column.\n","c8db56ae":"### Model-4","849b25e1":"### 26) A free copy of Mastering The Interview","d81e273f":"## Data Inspection","f99ecdfd":"### Inference :\n1. API and Landing Page Submission have 30-35% conversion rate but count of lead originated from them are considerable.\n2. Lead Add Form has more than 90% conversion rate but count of lead are not very high.\n3. Lead Import are very less in count.\n\n**To improve overall lead conversion rate, we need to focus more on improving lead converion of API and Landing Page Submission origin and generate more leads from Lead Add Form.**","89c3b920":"### 15) Newspaper Article","7bfdff50":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","39631707":"We can see that this is highly skewed column so we can remove this column.","595db2ef":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter.","c70fb7de":"### Inference\nMost entries are 'No'. No Inference can be drawn with this parameter."}}