{"cell_type":{"a15d3a03":"code","55612abc":"code","edbcb0eb":"code","5aac0c19":"code","d8afa631":"code","c662ae12":"code","51ec1934":"code","1eb49cec":"code","51847e44":"code","9cc40534":"code","a20c9384":"markdown","e0c665ef":"markdown","0f8e204c":"markdown","e699fae2":"markdown","b1d0cf51":"markdown","7a5928a5":"markdown","94a3ac3b":"markdown"},"source":{"a15d3a03":"# Imported Libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom scipy import interp\nfrom sklearn import metrics\nfrom sklearn.metrics import auc, roc_curve, average_precision_score, f1_score, precision_recall_curve, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\nfrom imblearn.pipeline import make_pipeline\n","55612abc":"# Load engineered dataset from EDA section\n\ndf  = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')    \n\n\n# data columns will be all other columns except class\ndata_cols = list(df.columns[ df.columns != 'Class' ])\nlabel_cols = ['Class']\n\nprint(data_cols)\nprint('# of data columns: ',len(data_cols))\n\ndf.head()","edbcb0eb":"# 284315 normal transactions (class 0)\n# 492 fraud transactions (class 1)\n\ndf.groupby('Class')['Class'].count()","5aac0c19":"\n# Duplicates? Yes\ndata = df.copy()\nnormal_duplicates = sum( data.loc[ data.Class==0 ].duplicated() )\nfraud_duplicates = sum( data.loc[ data.Class==1 ].duplicated() )\ntotal_duplicates = normal_duplicates + fraud_duplicates\n\nprint( 'Normal duplicates', normal_duplicates )\nprint( 'Fraud duplicates', fraud_duplicates )\nprint( 'Total duplicates', total_duplicates )\nprint( 'Fraction duplicated', total_duplicates \/ len(data) )","d8afa631":"# 'Time' is seconds from first transaction in set\n# 48 hours worth of data\n# Let's convert time to time of day, in hours\n\n\ndata['Time'] = ( data['Time'].values \/ 3600 ) % 24\n\nplt.hist( [ data.loc[ data['Class']==0, 'Time'], data.loc[ data['Class']==1, 'Time'] ], density =True, label=['normal','fraud'], bins=np.linspace(0,24,30))\nplt.legend()\nplt.show()\n\n# Looks like normal transactions have a bias towards 8am to midnight\n# Fraud has spikes at 2-3am and noon","c662ae12":"\n# Log transform amount values to give more normal distribution\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nplt.hist(data['Amount'], bins=40)\nplt.title('Original Amount Distribution')\n\nplt.subplot(1,2,2)\nd0 = np.log10( data['Amount'].values + 1 )\n# d0 = np.log1p( data['Amount'].values ) \/ np.log(10)\nplt.hist( d0, bins=40 )\nplt.title('Log10(x+1) Transformed Amount Distribution')\nplt.show()\n\n\n# Use log transformed data\n\ndata['Amount'] = d0","51ec1934":"\n# Plot the data by each feature\n\naxarr = [[]]*len(data_cols)\ncolumns = 4\nrows = int( np.ceil( len(data_cols) \/ columns ) )\nf, fig = plt.subplots( figsize=(columns*3.5, rows*2) )\n\nf.suptitle('Data Distributions by Feature and Class', size=16)\n\nfor i, col in enumerate(data_cols[:]):\n    axarr[i] = plt.subplot2grid( (int(rows), int(columns)), (int(i\/\/columns), int(i%columns)) )\n    axarr[i].hist( [ data.loc[ data.Class == 0, col ], data.loc[ data.Class == 1, col ] ], label=['normal','fraud'], \n                          bins=np.linspace( np.percentile(data[col],0.1), np.percentile(data[col],99.9), 30 ),\n                          density=True )\n    axarr[i].set_xlabel(col, size=12)\n    axarr[i].set_ylim([0,0.8])\n    axarr[i].tick_params(axis='both', labelsize=10)\n    if i == 0: \n        legend = axarr[i].legend()\n        legend.get_frame().set_facecolor('white')\n    if i%4 != 0 : \n        axarr[i].tick_params(axis='y', left='off', labelleft='off')\n    else:\n        axarr[i].set_ylabel('Fraction',size=12)\n\nplt.tight_layout(rect=[0,0,1,0.95]) # xmin, ymin, xmax, ymax\n# plt.savefig('plots\/Engineered_Data_Distributions.png')\nplt.show()","1eb49cec":"X = data.drop('Class', axis=1)\ny = data['Class']\n\nSK= StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\nfor train_index, test_index in SK.split(X, y):\n    Xtrain, Xtest = X.iloc[train_index], X.iloc[test_index]\n    ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n\ntrain_unique_label, train_counts_label = np.unique(ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(ytest, return_counts=True)\nprint('Label Distributions in ytrain and ytest: \\n')\nprint(train_counts_label\/len(ytrain))\nprint(test_counts_label\/len(ytest))\n","51847e44":"from xgboost.sklearn import XGBClassifier\n\nLW = 2\nRANDOM_STATE = 42\n\nclass DummySampler:\n    def sample(self, X, y):\n        return X, y\n\n    def fit(self, X, y):\n        return self\n\n    def fit_resample(self, X, y):\n        return self.sample(X, y)\n\n\nclf_xgb = XGBClassifier(objective=\"binary:logistic\", learning_rate=0.5, reg_alpha= 5, reg_lambda= 0.1,)\n                   \n\nsamplers = [\n    ['Standard', DummySampler()],\n    ['ADASYN', ADASYN( )],\n    ['ROS', RandomOverSampler()],\n    ['SMOTE', SMOTE()],]\n\npipelines = [['{}'.format(sampler[0]),make_pipeline(sampler[1], clf_xgb )] for sampler in samplers ]\n\nreport_list = pd.DataFrame( index = ['f1', 'precision', 'recall',' average_precision' ],columns =[sampler[0] for sampler in samplers])\n\nfig, axs = plt.subplots(1, 2, figsize=(15,10))\nfig1, axs1 = plt.subplots(2, 2, figsize=(22,12))\ntitle_cm= [sampler[0] for sampler in samplers]\n\nfor (idx, (name, pipeline)), ax1 in zip(enumerate(pipelines,0), axs1.flat):\n    clf = pipeline.fit(Xtrain.values, ytrain.values)\n    yhat= clf.predict_proba(Xtest.values)\n#    ypred0 = best_clf.decision_function(Xtest)\n    ypred = clf.predict(Xtest.values)\n    fpr, tpr, thresholds = roc_curve(ytest, yhat[:,1])\n    roc_auc = auc(fpr, tpr)\n    f1 = f1_score(ytest,ypred)\n    pr, re, thresholds = precision_recall_curve(ytest, yhat[:,1])\n    pr_auc = auc(re, pr)\n    report_list.iloc[[0],[idx]] = [f1]\n    report_list.iloc[[1],[idx]] = [metrics.precision_score(ytest, ypred)]\n    report_list.iloc[[2],[idx]] = [metrics.recall_score(ytest, ypred)]\n    report_list.iloc[[3],[idx]] = [average_precision_score(ytest, yhat[:,1])]    \n    axs[1].plot(pr, re, linestyle='-', label=r'%s (area = %0.3f )' % (name, pr_auc),lw=LW)\n    axs[0].plot(fpr, tpr, label='{} (area = %0.3f)'.format(name) % roc_auc, lw=LW)\n# confusion_matrix\n    cm_nn = confusion_matrix(ytest,ypred)\n    sns.heatmap(cm_nn, ax=ax1,annot=True,robust=True,fmt='g' ,cmap=\"Reds\", cbar=False)\n    ax1.set_title(title_cm[idx], fontsize=14)\n\naxs[0].plot([0, 1], [0, 1], linestyle='--', lw=LW, color='k', label='Luck')\n# make nice plotting\nxlabel= ['False Positive Rate', 'Recall']\nylabel= ['True Positive Rate', 'Precision']\ntitle = ['Receiver operating characteristisc (ROC)', 'Precision-Recall Curve ']\nfor i, ax in  enumerate(axs.flat, 0):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines['left'].set_position(('outward', 10))\n    ax.spines['bottom'].set_position(('outward', 10))\n    ax.axis(xmin=0,xmax=1)\n    ax.axis(ymin=0,ymax=1)\n    ax.set_xlabel(xlabel[i])\n    ax.set_ylabel(ylabel[i])\n    ax.set_title(title[i])\n    ax.legend(loc=\"lower right\")\n\nplt.show()\n","9cc40534":"report_list","a20c9384":"Now, spit the data into train and test data. Why?\n\nFor testing purposes, although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.","e0c665ef":"* Data Distributions by Feature and Class","0f8e204c":"The following figure plots the PR curve for our XGBoost classifier along with Random Oversampling, SMOTE, ADASYN to compare their overall classification performances. The top right corner of the PR plane represents perfect classification, i.e. precision = recall = 1, and the closer a PR curve gets to this point the better.","e699fae2":"It is clear from the result of the PR curve for XGBoost whit RandomOverSampler performs considerably better than all other Oversampling methods. \n\n\nXGBoost works considerably better than the other algorithm implemented in the Kernel on 'Benchmarking data sampling techniques for Credit Card Fraud Detection': https:\/\/www.kaggle.com\/jaddiabdelaziz\/fraud-detection-benchmarking-sampling-techniques .","b1d0cf51":"If you found this Kernel helpful please up vote it since it will keep me motivated to perform more in-depth reserach towards this subject. If you have some feedback and question don't forget to comment below.","7a5928a5":"# Detecting Credit Card Fraud using XGBoost and Oversampling techniques\n\nIf you found this Kernel helpful please up vote it since it will keep me motivated to perform more in-depth reserach towards this subject. \nIf you have some feedback and question don't forget to comment below.\n\n## Summary\nIn this project I use the Extreme Gradient Boosting (XGBoost) algorithm to detect fradulent credit card transactions in a real-world (anonymized) dataset of european credit card transactions. But, he main challenge in fraud detection is the extreme class imbalance in the data which makes it difficult for many classification algorithms to effectively separate the two classes. Only 0.172% of transactions are labeled as fradulent in this dataset.  I address the class imbalance by reweighting the data before training XGBoost, I used  Data Oversampling techniques (Random Oversampling, SMOTE, ADASYN)\n\nFor testing purposes, although we are splitting the data when implementing Random OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques.\n\n\n### Table of Contents:\n\n1. Exploratory Data Analysis (EDA)\n2. xgboost fraud detection whit Oversampling techniques","94a3ac3b":"## xgboost fraud detection whit Oversampling techniques\n\nOversampling methods involves duplicating examples of the minority class or synthesizing new examples from the minority class from existing examples.\n\nI will attempt to perform 3 methods of Oversampling to resolve this imbalanced dataset issue:\n* Random Oversampling\n* SMOTE\n* ADASYN\n"}}