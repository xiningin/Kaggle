{"cell_type":{"7f15c7e7":"code","e3e0c6ac":"code","a0ade8f1":"code","7fc120bf":"code","d72a3f1d":"code","551b9ff5":"code","aeabd603":"code","a7d29f95":"code","6a904137":"code","5e65e0bf":"code","effe35dc":"code","4598dc04":"code","a686ebbf":"code","4602a991":"code","c2cab23a":"code","422016ae":"code","04ae885a":"code","8161f3ee":"code","25488340":"code","350d6b4d":"code","65f81b38":"code","90f241bc":"code","133fc0ab":"code","660f0a28":"code","26ca493f":"code","a49961df":"code","0ccc9b5c":"code","218cd900":"code","de2932c2":"code","66eaaae0":"code","6e386ecb":"code","abc184e0":"code","aeb6d11c":"code","2f591b85":"code","c4d2cc3a":"code","d3461881":"code","234ebff3":"code","65cffc3b":"code","025e6c5e":"code","8dc7e5c6":"code","285a34d2":"code","293f2593":"code","0eebe4ad":"code","ae267619":"code","e30b4d6c":"code","c97b1bb0":"code","db29cee4":"code","b1f278b8":"code","2f755f98":"code","e0c70b42":"code","3694fc75":"code","49ff35d9":"code","fe403c2d":"code","41396592":"code","04606ffe":"code","bd729c1b":"code","1c43ad6a":"code","1c6fab4f":"code","4b6b323f":"code","dcff5a89":"code","d5e302ac":"code","7b66412d":"code","ba6166e0":"code","8c9f2348":"code","fcbbd365":"markdown","d574177e":"markdown","4819113f":"markdown","17e07262":"markdown","ff9fce9e":"markdown"},"source":{"7f15c7e7":"# Data Link-- https:\/\/www.kaggle.com\/arkhoshghalb\/twitter-sentiment-analysis-hatred-speech","e3e0c6ac":"import numpy as np\nimport pandas as pd","a0ade8f1":"#load csv file\nTrain = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ndata=Train\ndata.head()","7fc120bf":"test_data = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")\ntest_data.head()","d72a3f1d":"data.label.value_counts()","551b9ff5":"data.shape","aeabd603":"data['label'].value_counts()","a7d29f95":"import seaborn as sns\nax=sns.countplot(data.label)","6a904137":"# removing usernames from tweet\n\ndata['new_tweet'] = data.tweet.str.replace('@user', '')\ndata.head()","5e65e0bf":"# removing usernames from test_data\n\ntest_data['new_tweet'] = test_data.tweet.str.replace('@user', '')\ntest_data.head()\nids = test_data['id']","effe35dc":"#Removing Punctuations, Numbers, and Special Characters\n#[a-zA-Z] = Any single character in the range a-z or A-Z\n# ^ = Start of line \n# $ = End of line \n\ndata['new_tweet'] = data['new_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ndata['new_tweet'] = data['new_tweet'].str.replace(\"#\", \"\")\ndata.head()\n","4598dc04":"test_data['new_tweet'] = test_data['new_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntest_data['new_tweet'] = test_data['new_tweet'].str.replace(\"#\", \"\")\ntest_data.head()","a686ebbf":"# get most common words in training dataset\nfrom collections import Counter \nall_words = []\nfor line in list(data['new_tweet']):\n    words = line.split()\n    for word in words:\n        all_words.append(word.lower())\n    \n    \na=Counter(all_words).most_common(10)\na","4602a991":"#tokenization\ndata['new_tweet'] = data['new_tweet'].apply(lambda x: x.split())\ndata.head()","c2cab23a":"test_data['new_tweet'] = test_data['new_tweet'].apply(lambda x: x.split())\ntest_data.head()","422016ae":"#stemmer\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\ndata['new_tweet']= data['new_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\ndata.head()","04ae885a":"test_data['new_tweet']= test_data['new_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\ntest_data.head()","8161f3ee":"import nltk\nnltk.download('stopwords')","25488340":"from nltk.corpus import stopwords\n\nfrom nltk.tokenize import word_tokenize\n\n#stopwords = set(stopwords.words('english'))\nstopwords = nltk.corpus.stopwords.words('english')\n\n","350d6b4d":"newStopWords = ['u','go','got','via','or','ur','us','in','i','let','the','to','is','amp','make','one','day','days','get']\nstopwords.extend(newStopWords)\n","65f81b38":"import string \n\ndef process(text):\n    # Check characters to see if they are in punctuation\n    nopunc = set(char for char in list(text) if char not in string.punctuation)\n    # Join the characters to form the string.\n    nopunc = \" \".join(nopunc)\n    # remove any stopwords if present\n    return [word for word in nopunc.lower().split() if word.lower() not in stopwords]\n    ","90f241bc":"data['new_tweet'] = data['new_tweet'].apply(process) \ndata.head()","133fc0ab":"# test_data\ntest_data['new_tweet'] = test_data['new_tweet'].apply(process) \ntest_data.head()","660f0a28":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n\n# split sentences to get individual words\nwords = []\nfor line in data['new_tweet']: \n    words.extend(line)\n    \n# create a word frequency dictionary\nwordfreq = Counter(words)\n# draw a Word Cloud with word frequencies\nwordcloud = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords\n   ).generate_from_frequencies(wordfreq)\nplt.figure(figsize=(10,9))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","26ca493f":"def string (text):\n    to_return=\"\"\n    for i in list(text):\n        to_return += str(i) + \" \"\n    to_return = to_return[:-1]\n    \n    return to_return\n    \n       \ndata['new_tweet'] = data['new_tweet'].apply(string)\ndata.head()  ","a49961df":"test_data['new_tweet'] = test_data['new_tweet'].apply(string)\ntest_data.head()","0ccc9b5c":"positive = [r for r in data['new_tweet'][data['label']==0]]\npos = ''.join(positive)\n\n# draw a Word Cloud with word frequencies\nwordcloud = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords\n   ).generate(pos)\nplt.figure(figsize=(10,9))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","218cd900":"negative = [r for r in data['new_tweet'][data['label']==1]]\nneg = ''.join(negative)\n\n# draw a Word Cloud with word frequencies\nwordcloud = WordCloud(\n    background_color='black',\n    max_words=2000,\n    stopwords=stopwords\n   ).generate(neg)\nplt.figure(figsize=(10,9))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","de2932c2":"data.drop([\"id\",\"tweet\" ],axis=1,inplace=True)\ndata.head()","66eaaae0":"test_data.drop([\"id\",\"tweet\" ],axis=1,inplace=True)\ntest_data.head()","6e386ecb":"#Split data into training and testing sets \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data[\"new_tweet\"], \n                                                    data[\"label\"], test_size = 0.2, random_state = 42)\n\nprint(\"training set :\",x_train.shape,y_train.shape)\nprint(\"testing set :\",x_test.shape,y_test.shape)","abc184e0":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\ncount_vect = CountVectorizer(stop_words='english')\ntransformer = TfidfTransformer(norm='l2',sublinear_tf=True)\n","aeb6d11c":"test_x = test_data['new_tweet']\ntest_x","2f591b85":"x_train_counts = count_vect.fit_transform(x_train)\nx_train_tfidf = transformer.fit_transform(x_train_counts)\n\nprint(x_train_counts.shape)\nprint(x_train_tfidf.shape)","c4d2cc3a":"test_x_counts = count_vect.transform(test_x)\ntest_x_tfidf = transformer.transform(test_x_counts)\n\nprint(test_x_counts.shape)\nprint(test_x_tfidf.shape)","d3461881":"x_test_counts = count_vect.transform(x_test)\nx_test_tfidf = transformer.transform(x_test_counts)\n\nprint(x_test_counts.shape)\nprint(x_test_tfidf.shape)","234ebff3":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=200)\nmodel.fit(x_train_tfidf,y_train)","65cffc3b":"predictions = model.predict(x_test_tfidf)","025e6c5e":"submission  = model.predict(test_x_tfidf)","8dc7e5c6":"#Accuracy_score\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,predictions)*100","285a34d2":"from sklearn.metrics import confusion_matrix,f1_score\nconfusion_matrix(y_test,predictions)","293f2593":"#f1-score\nf1_score(y_test,predictions)","0eebe4ad":"from __future__ import print_function\n\n\n# Special END separator\nEND = '0e8ed89a-47ba-4cdb-938e-b8af8e084d5c'\n\n# Text attributes\nALL_OFF = '\\033[0m'\nBOLD = '\\033[1m'\nUNDERSCORE = '\\033[4m'\nBLINK = '\\033[5m'\nREVERSE = '\\033[7m'\nCONCEALED = '\\033[7m'\n\n# Foreground colors\nFG_BLACK = '\\033[30m'\nFG_RED = '\\033[31m'\nFG_GREEN = '\\033[32m'\nFG_YELLOW = '\\033[33m'\nFG_BLUE = '\\033[34m'\nFG_MAGENTA = '\\033[35m'\nFG_CYAN = '\\033[36m'\nFG_WHITE = '\\033[37m'\n\n# Background colors\nBG_BLACK = '\\033[40m'\nBG_RED = '\\033[41m'\nBG_GREEN = '\\033[42m'\nBG_YELLOW = '\\033[43m'\nBG_BLUE = '\\033[44m'\nBG_MAGENTA = '\\033[45m'\nBG_CYAN = '\\033[46m'\nBG_WHITE = '\\033[47m'\n\n\nclass pretty_output():\n    '''\n    Context manager for pretty terminal prints\n    '''\n\n    def __init__(self, *attr):\n        self.attributes = attr\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        pass\n\n    def write(self, msg):\n        style = ''.join(self.attributes)\n        print('{}{}{}'.format(style, msg.replace(END, ALL_OFF + style), ALL_OFF))\n\n\nif __name__ == '__main__':\n\n    with pretty_output(FG_RED) as out:\n        out.write('This is a test in RED')\n\n    with pretty_output(FG_BLUE) as out:\n        out.write('This is a test in BLUE')\n\n    with pretty_output(BOLD, FG_GREEN) as out:\n        out.write('This is a bold text in green')\n\n    with pretty_output(BOLD, BG_GREEN) as out:\n        out.write('This is a text with green background')\n\n    with pretty_output(FG_GREEN) as out:\n        out.write('This is a green text with ' + BOLD + 'bold' + END + ' text included')\n\n    with pretty_output() as out:\n        out.write(BOLD + 'Use this' + END + ' even with ' + BOLD + FG_RED + 'no parameters' + END + ' in the with statement')","ae267619":"from sklearn.metrics import classification_report\ndf = pd.DataFrame(classification_report(predictions, \n                                        y_test, digits=2,\n                                        output_dict=True)).T\n\nwith pretty_output(BOLD, FG_GREEN) as out:\n    out.write('                 RANDOM FOREST                 ')\ndf.style.background_gradient(cmap='viridis',\n                             subset=pd.IndexSlice['0':'1', :'f1-score'],)\n","e30b4d6c":"def color_negative_red(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for negative\n    strings, black otherwise.\n    \"\"\"\n    color = 'red' if val == 'NEG' else 'green'\n    return 'color: %s' % color","c97b1bb0":"tweets = pd.DataFrame(columns=['Tweets','Prediction','Label'])\ntweets['Tweets'] = x_test\ntweets['Prediction'] = predictions\ntweets['Label'] = y_test\ntweets.replace([0,1],['POS', 'NEG'], inplace=True)\nwith pretty_output(BOLD, FG_GREEN) as out:\n    out.write('                 RANDOM FOREST Tweets Prediction                ')\n(tweets.sample(10).style.applymap(color_negative_red, subset=['Prediction', 'Label']))","db29cee4":"predictions","b1f278b8":"test_data.head()","2f755f98":"submission_df = pd.DataFrame(columns=['id', 'label'])\nsubmission_df['id'] = ids\nsubmission_df['label'] = submission\nsubmission_df.head(5)","e0c70b42":"submission_df.to_csv('submission.csv',index=False)","3694fc75":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression(random_state=400 )\nlogmodel.fit(x_train_tfidf,y_train)","49ff35d9":"log_predictions = logmodel.predict(x_test_tfidf)","fe403c2d":"from sklearn.metrics import confusion_matrix,f1_score\nconfusion_matrix(y_test,log_predictions)","41396592":"f1_score(y_test,log_predictions)","04606ffe":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,log_predictions)*100","bd729c1b":"submission  = model.predict(test_x_tfidf)\nsubmission_df = pd.DataFrame(columns=['id', 'label'])\nsubmission_df['id'] = ids\nsubmission_df['label'] = submission\nsubmission_df.to_csv('submission-log.csv',index=False)\nsubmission_df.head(5)","1c43ad6a":"from sklearn.ensemble import GradientBoostingRegressor\n\nalg= GradientBoostingRegressor(n_estimators= 550, learning_rate= 0.1, max_depth= 3)\nalg.fit(x_train_tfidf,y_train)","1c6fab4f":"alg_predictions = logmodel.predict(x_test_tfidf)","4b6b323f":"from sklearn.metrics import confusion_matrix,f1_score\nconfusion_matrix(y_test,alg_predictions)","dcff5a89":"f1_score(y_test,alg_predictions)","d5e302ac":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,alg_predictions)*100","7b66412d":"import seaborn as sns","ba6166e0":"models = [\"Naive Bayes\", 'Random Forest', 'XGBoost']\nacc = [94, 96, 95]\nplt.style.use('ggplot')\nplt.figure(figsize=(6, 4))\nsns.barplot(models, acc, );\n# Get current axis on current figure\nax = plt.gca()\n\n# ylim max value to be set\ny_max = 110 \nax.set_ylim([0, y_max])\n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/2., p.get_height(), '%d' % int(p.get_height()), \n            fontsize=12, color='red', ha='center', va='bottom')\nplt.title('Accuracy of classifiers')\nplt.show()","8c9f2348":"submission  = model.predict(test_x_tfidf)\nsubmission_df = pd.DataFrame(columns=['id', 'label'])\nsubmission_df['id'] = ids\nsubmission_df['label'] = submission\nsubmission_df.to_csv('submission_gb.csv',index=False)\nsubmission_df.head(5)","fcbbd365":"#### Random Forest Classifier","d574177e":"### Logistic Regression","4819113f":"### Gradient Boosting Regressor","17e07262":"### Model building","ff9fce9e":"### Bag of words "}}