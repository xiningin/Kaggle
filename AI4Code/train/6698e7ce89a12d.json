{"cell_type":{"c0994ba7":"code","dbde518c":"code","f0a6ac15":"code","acd65e6e":"code","ff5fda26":"code","63ca02dd":"code","52715620":"code","4d5d3764":"code","4b41a29e":"code","5b342ddf":"code","683a9885":"code","231dec31":"code","2307ed07":"code","eacddcd4":"code","f850f123":"code","a0c504c5":"code","1dc01bc0":"code","ed3a3677":"code","b1039256":"code","cfc4c339":"code","5baeb30a":"code","2899c683":"code","e44b2f38":"code","3b5fdbe6":"code","4a60f1ab":"code","18fb2e5b":"code","fb0ad449":"code","09ba52c0":"code","fd7a1ec8":"code","faec7cda":"code","61233ccb":"code","7addaf55":"code","66cbe552":"code","d2fb6638":"code","808288c4":"code","ad85aebb":"code","809fb2e1":"code","61319b54":"code","0553b8cb":"code","355b4c19":"code","0ab3e0c0":"code","9bd130a1":"code","e05210e2":"code","c16dc19d":"code","725e01b6":"code","81fbc1ef":"code","2a15c3a1":"code","57760f3e":"code","ae67f5fd":"code","51920748":"code","5c33e37f":"code","bd727fb4":"code","c6a4c558":"code","3d8b25da":"code","173b099c":"code","2d7ec8a2":"code","d9a8cdab":"code","de7e0c75":"code","4eca88d9":"code","175e16a1":"code","8743a1e5":"markdown","f2d0ff4b":"markdown","20408b83":"markdown","31eeb4a9":"markdown","f9a72ef1":"markdown","b4f96a3a":"markdown","822b1ccc":"markdown","71c60505":"markdown","4f181f3c":"markdown","9c40bf5f":"markdown","f8627ed1":"markdown","91830c62":"markdown","e44f3f3f":"markdown","8f7a5971":"markdown","de583ea7":"markdown","a92a89d6":"markdown","29c49ada":"markdown","4cd7a577":"markdown","b5e4123b":"markdown","448ffd87":"markdown","31e42e05":"markdown","0c1ad3b0":"markdown","3bdcc362":"markdown","2e4f8853":"markdown","2de95eb6":"markdown","672de25b":"markdown","1815d6cb":"markdown","0906e62d":"markdown","bcf93efe":"markdown","17c43936":"markdown","0baec1a5":"markdown","7d1bda73":"markdown","add25d58":"markdown"},"source":{"c0994ba7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.output_result { max-width:100% !important; }<\/style>\"))\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)\nplt.style.use('fivethirtyeight')\nimport seaborn as sns","dbde518c":"credit= pd.read_csv('..\/input\/credit-card-data\/Data.csv')","f0a6ac15":"#Understanding the features\nc= credit.columns\nprint(c[0],': Customer Id')\nprint(c[1],': Money the customer owe to the credit card company')\nprint(c[2],': Ratio of last 12 month with balance ')\nprint(c[3],': total  purchase amount spent during last 12 month')\nprint(c[4],': total amount paid without any continuous history of payments or ontime payments')\nprint(c[5],': total amount paid using installments')\nprint(c[6],': total money taken out using ATM,etc')\nprint(c[7],': percent of month with atleast one purchase')\nprint(c[8],': percent of month with atleast one oneoff purchase')\nprint(c[9],': percent of month with atleast one purchase using installments')\nprint(c[10],': percent of month with atleast one cash_advance')\nprint(c[11],': average amount used during a cash advance transaction')\nprint(c[12],': average amout used during a purchase')\nprint(c[13],': credit limit on the card')\nprint(c[14],': Due ammount paid by the customer to decrease their statement balance')\nprint(c[15],': Total minimum payment amount paid')\nprint(c[16],': Percentage of months with full payment of the due statement balance')\nprint(c[17],': Tenure')","acd65e6e":"#found some missing values in the dataset\nna_values= credit.isna().sum()\nna_values[na_values>0]","ff5fda26":"#Filling missing values with median\n\ncredit['CREDIT_LIMIT']= credit['CREDIT_LIMIT'].fillna(credit['CREDIT_LIMIT'].median())\ncredit['MINIMUM_PAYMENTS']= credit['MINIMUM_PAYMENTS'].fillna(credit['MINIMUM_PAYMENTS'].median())","63ca02dd":"credit['MONTHLY_AVG_PURCHASE']=credit['PURCHASES']\/credit['TENURE']\ncredit['MONTHLY_CASH_ADVANCE']=credit['CASH_ADVANCE']\/credit['TENURE']","52715620":"credit[['MONTHLY_AVG_PURCHASE','MONTHLY_CASH_ADVANCE']]","4d5d3764":"o_p= 'ONEOFF_PURCHASES'\ni_p= 'INSTALLMENTS_PURCHASES'\nprint('\\033[1mPurchase on \\033[0m :')\nprint('\\t Only oneoff  :',credit[(credit[o_p]>0) & (credit[i_p]==0)].shape[0])\nprint('\\t only  installments :',credit[(credit[o_p]==0) & (credit[i_p]>0)].shape[0])\nprint('\\t installments and oneoff :',credit[(credit[o_p]>0) & (credit[i_p]>0)].shape[0])\nprint('\\t None :',credit[(credit[o_p]==0) & (credit[i_p]==0)].shape[0])","4b41a29e":"def purchase_type(credit):\n    if (credit['ONEOFF_PURCHASES']==0) & (credit['INSTALLMENTS_PURCHASES']==0):\n        return 'none'\n    if (credit['ONEOFF_PURCHASES']>0) & (credit['INSTALLMENTS_PURCHASES']>0):\n         return 'both_oneoff_installment'\n    if (credit['ONEOFF_PURCHASES']>0) & (credit['INSTALLMENTS_PURCHASES']==0):\n        return 'one_off'\n    if (credit['ONEOFF_PURCHASES']==0) & (credit['INSTALLMENTS_PURCHASES']>0):\n        return 'istallment'\ncredit['PURCHASE_TYPE']=credit.apply(purchase_type,axis=1)","5b342ddf":"vc_Ptype= credit['PURCHASE_TYPE'].value_counts()\nvc_Ptype= vc_Ptype\/np.sum(vc_Ptype) *100\nvc_Ptype.plot(kind='barh')\n\nfor i,val in enumerate(vc_Ptype.values):\n    plt.annotate(s=str(np.round(val,1)),xy=(val,i))\n\nplt.xlim(0,100)\nplt.yticks([0,1,2,3],['Both','installment','None','One Off'])\nplt.title('Purchase % by type')","683a9885":"print('The average amount per purchase is :\\033[1m',credit['PURCHASES'].mean(),'\\033[0m')\nprint('The Average cash advance per transaction is \\033[1m:', credit['CASH_ADVANCE'].mean(),'\\033[0m')","231dec31":"#higher number means high debt or credit card utilization\ncredit['LIMIT_USAGE']=credit.apply(lambda x: (x['BALANCE']\/x['CREDIT_LIMIT']), axis=1)\nlow,medium,high= credit['LIMIT_USAGE'].describe(percentiles=[0.35,0.6,1.0])[['35%','60%','100%']].values","2307ed07":"low_count= len(credit[credit['LIMIT_USAGE']<low])\nmed_count= len(credit[(credit['LIMIT_USAGE']>=low) & (credit['LIMIT_USAGE']<=medium)])\nhigh_count= len(credit[credit['LIMIT_USAGE']>medium])\n\nplt.bar(x=[0,1,0,2,0,3],height= [0,low_count,0,med_count,0,high_count],width=0.4)\nplt.xticks([0,1,2,3,4],['','low CCU','medium CCU','high CCU',''])\nplt.ylim(0,4000)\nplt.title('Credit Card Utilization of customers')","eacddcd4":"#Found no missing values\n\ndisplay(credit['PAYMENTS'].isna().sum())\ndisplay(credit['MINIMUM_PAYMENTS'].isna().sum())","f850f123":"credit['PAYMENT_MINPAY']=credit.apply(lambda x:x['PAYMENTS']\/x['MINIMUM_PAYMENTS'],axis=1)","a0c504c5":"print('Average credit the company gives to its customer is : \\033[1m',(credit['CREDIT_LIMIT']\/credit['TENURE']).mean())","1dc01bc0":"per= len(credit[(credit['BALANCE']- 390.783)>0])\/len(credit) *100\nprint('\\033[1m {}% \\033[0m  customers have exhausted their credit limit on the current period'.format(np.round(per,2)))","ed3a3677":"types= credit.groupby('PURCHASE_TYPE').groups.keys()\nmin_pay=[]\nfor group in types:\n    min_pay.append(credit.groupby('PURCHASE_TYPE').get_group(group)['PAYMENT_MINPAY'].mean())\n    ","b1039256":"plt.bar(x=['Both','only installment','None','only oneoff'],height= min_pay,width=0.6)\nplt.ylim(0,19)\nplt.xlim(-1,3.5)\nplt.title('Minimum payment comparision of diff purchase type')","cfc4c339":"types","5baeb30a":"cash_adv=[]\nfor group in types:\n    cash_adv.append(credit.groupby('PURCHASE_TYPE').get_group(group)['CASH_ADVANCE'].mean())\nplt.bar(x=['Both','only installment','None','only oneoff'],height= cash_adv,width=0.6)\nplt.title('Average cash advance taken by customers of different Purchase type ')","2899c683":"credit_limit=[]\nfor group in types:\n    credit_limit.append(credit.groupby('PURCHASE_TYPE').get_group(group)['LIMIT_USAGE'].mean())\nplt.bar(x=['Both','only installment','None','only oneoff'],height= credit_limit,width=0.6)\nplt.title('Credit Limit Usage according to different purchase type')","e44b2f38":"dummy_ptype= pd.get_dummies(credit['PURCHASE_TYPE'])","3b5fdbe6":"credit_pro= pd.concat([credit,dummy_ptype],axis=1).drop('PURCHASE_TYPE',axis=1)","4a60f1ab":"#well this didnt turned out as expected\ncmap = cmap=sns.diverging_palette(5, 200, as_cmap=True)\ncredit_pro.corr().style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '10px', 'font-size': '10pt'})","18fb2e5b":"plt.figure(figsize=(16,7))\nsns.heatmap(credit_pro.corr()[credit_pro.corr()>0.4],linewidths=0.1,linecolor='black')","fb0ad449":"from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()","09ba52c0":"#columns which required normalization\ncol= ['PURCHASES','CASH_ADVANCE','CREDIT_LIMIT','PAYMENTS','MINIMUM_PAYMENTS','PRC_FULL_PAYMENT','LIMIT_USAGE','BALANCE_FREQUENCY', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'MONTHLY_AVG_PURCHASE',\n       'MONTHLY_CASH_ADVANCE', 'LIMIT_USAGE', 'PAYMENT_MINPAY',\n       'both_oneoff_installment', 'istallment', 'none', 'one_off']\n\nprint('Removing Columns which cannot be considered a behavioural atribute: ')\n\nfor c in credit_pro.columns:\n    if c not in  col:\n        print('\\t \\t \\t \\t \\t \\t \\t \\t  ',c)\n\nprint('Scalling the following columns :')     \ncredit_pro= credit_pro[col].copy()\nscaled_credit= sc.fit_transform(credit_pro)\nprint('\\n',col)\n","fd7a1ec8":"scaled_credit.shape","faec7cda":"from sklearn.decomposition import PCA\npca=PCA(n_components= 23)\ncredit_pca= pca.fit(scaled_credit)","61233ccb":"#it seem PCA can explain 100% variance\nnp.sum(credit_pca.explained_variance_ratio_)","7addaf55":"#now let's find least number of dimension from which PCA could explained maximum variance\nvar_ratio={}\nfor n in range(2,23):\n    pca=PCA(n_components=n)\n    cr_pca=pca.fit(scaled_credit)\n    var_ratio[n]=sum(cr_pca.explained_variance_ratio_)\nvar_ratio","66cbe552":"plt.figure(figsize=(12,4))\nx= list(var_ratio.keys())\ny= list(var_ratio.values())\nplt.plot(x,y)\nx= np.arange(2,23,step=2)\ny= [var_ratio[n] for n in x]\nplt.bar(x,y,width=0.04)\nfor i in x:\n    plt.annotate(s=(str(np.round(var_ratio[i],2)*100)+'%'),xy=(i,var_ratio[i]))\nplt.xticks(np.arange(0,24))\nplt.title('Explained Variance ratio with outliers')\nplt.xlabel('number of components')\nplt.ylabel('Variance explained')","d2fb6638":"#number of components which can explained atleast 95% of variance\npca=PCA(n_components=0.95)\ncr_pca=pca.fit(scaled_credit)\nsum(cr_pca.explained_variance_ratio_)","808288c4":"#Their are some columns in the dataset which contain large value\noutlier_col= ['CUST_ID',\n 'BALANCE',\n 'PURCHASES',\n 'CASH_ADVANCE',\n 'CREDIT_LIMIT',\n 'PAYMENTS',\n 'MINIMUM_PAYMENTS',\n 'PRC_FULL_PAYMENT',\n 'TENURE',\n]\n\ncol= ['BALANCE_FREQUENCY', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'MONTHLY_AVG_PURCHASE',\n       'MONTHLY_CASH_ADVANCE', 'LIMIT_USAGE', 'PAYMENT_MINPAY',\n       'both_oneoff_installment', 'istallment', 'none', 'one_off']\n\na= sc.fit_transform(credit_pro[col])\n\nvar_ratio={}\nfor n in range(2,17):\n    pca=PCA(n_components=n)\n    cr_pca=pca.fit(a)\n    var_ratio[n]=sum(cr_pca.explained_variance_ratio_)\nvar_ratio","ad85aebb":"plt.figure(figsize=(12,4))\nx= list(var_ratio.keys())\ny= list(var_ratio.values())\nplt.plot(x,y)\nx= np.arange(2,17,step=2)\ny= [var_ratio[n] for n in x]\nplt.bar(x,y,width=0.04)\nfor i in x:\n    plt.annotate(s=(str(np.round(var_ratio[i],2)*100)+'%'),xy=(i,var_ratio[i]))\nplt.xticks(np.arange(0,17))\nplt.title('Explained Variance ratio without outliers')\nplt.xlabel('number of components')\nplt.ylabel('Variance explained')","809fb2e1":"pca=PCA(n_components=0.95)\ncr_pca=pca.fit(a)\nsum(cr_pca.explained_variance_ratio_)\nprint(len(pca.components_))","61319b54":"pca_final= PCA(n_components=0.9)\npca_final.fit(scaled_credit)\nreduced_credit= pca_final.fit_transform(scaled_credit)","0553b8cb":"#chose 12 components\nprint(scaled_credit.shape)\nprint(reduced_credit.shape)","355b4c19":"dd= pd.DataFrame(pca_final.components_)\ndd.columns= credit_pro.columns\ndd.T","0ab3e0c0":"from sklearn.cluster import KMeans\nkm_4=KMeans(n_clusters=4,random_state=123)","9bd130a1":"km_4.fit(reduced_credit)","e05210e2":"km_4.labels_","c16dc19d":"pd.Series(km_4.labels_).value_counts()","725e01b6":"cluster_range = range( 1, 21 )\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans( num_clusters )\n    clusters.fit( reduced_credit )\n    cluster_errors.append( clusters.inertia_ )# clusters.inertia_ is basically cluster error here.","81fbc1ef":"clusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:21]","2a15c3a1":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\",linewidth=0.6)\nplt.title('Cluster errors with respect to number of cluster')","57760f3e":"from sklearn import metrics\n# calculate SC for K=3 through K=12\nk_range = range(2, 21)\nscores = []\nfor k in k_range:\n    km = KMeans(n_clusters=k, random_state=1)\n    km.fit(reduced_credit)\n    scores.append(metrics.silhouette_score(reduced_credit, km.labels_))","ae67f5fd":"scores","51920748":"plt.plot(k_range, scores,linewidth=0.7)\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Coefficient')\nplt.grid(True)","5c33e37f":"color_map={0:'r',1:'b',2:'g',3:'black'}\nlabel_color=[color_map[l] for l in km_4.labels_]\nplt.figure(figsize=(7,7))\nplt.scatter(reduced_credit[:,0],reduced_credit[:,1],c=label_color,cmap='Spectral',alpha=0.1)\nplt.legend()\n","bd727fb4":"plt.ylim(-10,10)\nplt.scatter(reduced_credit[:,0],reduced_credit[:,2],c=label_color,cmap='Spectral',alpha=0.1)","c6a4c558":"plt.ylim(-5,10)\ndisplay(plt.scatter(reduced_credit[:,0],reduced_credit[:,3],c=label_color,cmap='Spectral',alpha=0.1))","3d8b25da":"df_pair_plot=pd.DataFrame(reduced_credit,columns=['PC_' +str(i) for i in range(12)])\ndf_pair_plot['Cluster']=km_4.labels_","173b099c":"df_pair_plot","2d7ec8a2":"sns.pairplot(df_pair_plot,hue='Cluster', palette= 'Dark2', diag_kind='kde',size=1.85)","d9a8cdab":"# Factor Analysis : variance explained by each component- \npd.Series(pca_final.explained_variance_ratio_*100,index=['PC_'+ str(i) for i in range(12)])","de7e0c75":"credit_pro['cluster']= km_4.labels_","4eca88d9":"credit_pro =credit_pro.loc[:,~credit_pro.columns.duplicated()]\ncredit_pro.columns","175e16a1":"# cols=['PURCHASES','CASH_ADVANCE_TRX','both_oneoff_installment','one_off','istallment','none','cluster']\ncols=['MONTHLY_CASH_ADVANCE','LIMIT_USAGE','MONTHLY_AVG_PURCHASE','PAYMENT_MINPAY','istallment','ONEOFF_PURCHASES','cluster']\ncluster= credit_pro[cols].groupby('cluster')\nnp.log(cluster.mean()).plot(kind='bar',figsize=(18,9))\nplt.xticks([0,1,2,3],['Cluster1','Cluster2','Cluster3','Cluster4'])\nplt.title('Overview of different clusters')","8743a1e5":"So above data gave us eigen vector for each component we had all eigen vector value very small we can remove those variable bur in our case its not.\n## We have succefully reduced the dataset into 12 dimension\/ features which can explained about 90% of the variance","f2d0ff4b":"## 90% variance explained should be enough for this problem ","20408b83":"4. Limit_usage (balance to credit limit ratio ) credit card utilization <br>\n    Lower value implies cutomers are maintaing thier balance properly. Lower value means good credit score\n\nNote: Credit Card Utilization ratio is the comparision of total credit used to total credit available","31eeb4a9":"1. Monthly average purchase and cash advance amount","f9a72ef1":"In the above graph we can see that most customers have high Credit Card Utilization value, which means most customers are using using money till their credit limit","b4f96a3a":"## Insight from  KPI","822b1ccc":"## Customer who do not do any kind of purchase i.e oneoff or in installements tend to take out cash through various means like atm, cheque,etc.\n\nNow the next question which comes after consuming above graph is Does those custome take out money to their credit limit?","71c60505":"It is very difficult to draw iddividual plot for cluster, so we will use pair plot which will provide us all graph in one shot. To do that we need to take following steps","4f181f3c":"## Here we didn't knew k value so we will find the K. To do that we need to take a cluster range between 1 and 21.\n","9c40bf5f":"## Clustering\n\nBased on the intuition on type of purchases made by customers and their distinctive behavior exhibited based on the purchase_type (as visualized above in Insights from KPI) , I am starting with 4 clusters.\n","f8627ed1":"##  Implementing Principal component analysis which basically reduces data into lower dimensions or into lower features \nit uses the concept of  eigen vector and eigen values( Vectors which do not change their behavious while a linear transformation is applied)","91830c62":"<br>\n<br>\n<br>\n\n## Cash advance are good for the company, since the company can put tax for the service. So, In the following experiment, let's find average cash advance ratio with respect to purchase type","e44f3f3f":"## 11 Components which can explained 95 % variance without outlier","8f7a5971":"## Okay, From the above figure it can be said that customes buying stuffs through installment are more like to pay a minimum payment. So, for the company it is more feasible to make customer buy stuff on installments.","de583ea7":"As per above detail we found out that there are 4 types of purchase behaviour in the data set. So we need to derive a categorical variable based on their behaviour","a92a89d6":"## Choosing k=4 i.e 0,1,2,3","29c49ada":"Just out of curiosity, I would like to see how much Credit Limit does a person gets on average","4cd7a577":"- Heat map shows that many features are co-related so applying dimensionality reduction will help negating multi-colinearity in data\n\n    Before applying PCA we will standardize data to avoid effect of scale on our result. Centering and Scaling will make all features with equal weight.\n","b5e4123b":"our dataset contain features which contain outliers, first i will find how much components are required to explained 95% variance along with features contain outlier and then i will try to find variance without those columns and then we will compare the results","448ffd87":"## With Outlier columns","31e42e05":"## it is requiring 14 components are explaining 95% of variance, let's choose 14 features","0c1ad3b0":"\n\nPerformance metrics also suggest that K-means with 4 cluster is able to show distinguished characteristics of each cluster.\n\nInsights with 4 Clusters\n\n    cluster 1 is rarely takes it to the credit limit and is paying comparatively higher minimum payment and poor on installments This group is about 23% of the total customer base\n    Cluster 2 is the group of customers who have highest monthly cash advance purchases and doing both installment as well as one_off purchases, have comparatively good limit usage. This group is about 31% of the total customer base\n    Cluster 3 customers are doing maximum One_Off transactions and least purchase on installment and limit usage is on lower side This group is about 21% of the total customer base\n\n    Cluster 4 customers have maximum one off purchase and are paying dues and are doing maximum Monthly average purchases. This group is about 25% of the total customer base\n\nMarketing Strategy Suggested:\na. Group 2\n\n    They are potential target customers who are paying dues and doing purchases and maintaining comparatively good credit score ) -- we can increase credit limit or can lower down interest rate -- Can be given premium card \/loyality cards to increase transactions\n\nb. Group 1\n\n    They have poor credit score and taking only cash on advance. We can target them by providing less interest rate on purchase transaction\n\nc. Group 0\n\n    This group is has minimum paying ratio and using card for just oneoff transactions (may be for utility bills only). This group seems to be risky group.\n\nd. Group 3\n\n    This group is performing best among all as cutomers are maintaining good credit score and paying dues on time. -- Giving rewards point will make them perform more purchases.\n\n","3bdcc362":"Some other KPI's","2e4f8853":"## From above graph we will find elbow range. here it is 4,5,6","2de95eb6":"##  from the above pairplot and above table , it can be seen that component 0 and 1 are classifying the clusters\nIt shows that first two components are able to indentify clusters\nNow we have done here with principle component now we need to come bring our original data frame and we will merge the cluster with them.\n\nTo interprate result we need to use our data frame\n","672de25b":"## What is the average minimum payment ratio of each purchase type?","1815d6cb":"## Deriving a new KPI( Key performance index)","0906e62d":"2- Purchases by type (one-off, installments) <br>\n   -To find what type of purchases customers are making on credit card\n\n","bcf93efe":"## Customers who don't do either of purchase i.e Oneoff or through installment tends to take more Cash Advance and exhaust their credit limit more frequently","17c43936":"3- What is the Average amount per purchase and cash advance per transaction?","0baec1a5":"5- Payments to minimum payments ratio etc.","7d1bda73":"How many people have exhausted this limit in the current period","add25d58":"##  Preparing data for machine learning algorithm"}}