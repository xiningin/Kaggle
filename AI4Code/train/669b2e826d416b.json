{"cell_type":{"46742009":"code","1c642fbe":"code","bbb73397":"code","40f3a69a":"code","d95caa1e":"code","aaccf01d":"code","abcab71a":"code","16ad7c81":"code","9c6d8f53":"code","554e5455":"code","d2618ef2":"code","6267000e":"code","f6b64715":"code","5b9329ea":"code","4c7bfdd0":"code","91d01796":"code","638297c8":"code","892ead30":"code","8aa4fe7c":"code","55e4ed56":"code","ea718886":"code","690e9bc6":"code","187de08a":"code","1143a4de":"code","a680a992":"code","70c67c0c":"code","b84c3fd8":"code","26b14ef5":"code","fbd20bce":"code","ed8f60cf":"code","d3456d53":"code","57ce67f7":"code","e84e3dd8":"code","a7e3caf1":"code","06fea479":"code","6edb58d2":"code","377d879e":"code","bdf86fbc":"code","d7df96a7":"code","4917e736":"code","3d1ee630":"code","9cbde459":"code","5f791eaf":"code","8e6ea267":"code","593e9651":"markdown","f33915ed":"markdown","745e1434":"markdown","1ef482cb":"markdown","dc2b57a9":"markdown","11532c35":"markdown","14215e55":"markdown","60b2977a":"markdown","eb39af9d":"markdown","20dd7bd7":"markdown","6dc5fcea":"markdown","0f24ed79":"markdown","2cb356eb":"markdown","7d0c9a7b":"markdown","6ef13190":"markdown","fc1c2a59":"markdown","0d64de72":"markdown"},"source":{"46742009":"# necessary imports\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()\n%matplotlib inline","1c642fbe":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv') # loading data","bbb73397":"df.head()","40f3a69a":"df.info()","d95caa1e":"df.describe()","aaccf01d":"df.isna().sum()","abcab71a":"# let's see how data is distributed for every column\n\nplt.figure(figsize = (25, 20))\nplotnumber = 1\n\nfor col in df:\n    if plotnumber <= 12:\n        ax = plt.subplot(4, 3, plotnumber)\n        sns.distplot(df[col])\n        plt.xlabel(col, fontsize = 15)\n        \n    plotnumber += 1\n    \nplt.tight_layout()\nplt.show()","16ad7c81":"# heatmap\n\nplt.figure(figsize = (16, 7))\n\ncorr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr, mask = mask, annot = True, fmt = '0.2g', linewidths = 1)\nplt.show()","9c6d8f53":"# creating X and y\n\nX = df.drop('quality', axis = 1)\ny = df['quality']","554e5455":"# splitting data into training and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y)","d2618ef2":"# scaling data\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6267000e":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')\nlr.fit(X_train, y_train)","f6b64715":"# accuracy score\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlr_acc = accuracy_score(y_test, lr.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, lr.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {lr_acc}\\n\")","5b9329ea":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","4c7bfdd0":"# accuracy score\n\nknn_acc = accuracy_score(y_test, knn.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, knn.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {knn_acc}\\n\")","91d01796":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)","638297c8":"# accuracy score\n\nsvc_acc = accuracy_score(y_test, svc.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, svc.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {svc_acc}\\n\")","892ead30":"from sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nsgd = SGDClassifier()\nparameters = {\n    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n    'loss' : ['hinge', 'log'],\n    'penalty' : ['l1', 'l2']\n}\n\ngrid_search = GridSearchCV(sgd, parameters, cv = 10, n_jobs = -1)\ngrid_search.fit(X_train, y_train)","8aa4fe7c":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","55e4ed56":"sgd = SGDClassifier(alpha = 0.01, loss = 'log', penalty = 'l2')\nsgd.fit(X_train, y_train)\n\ny_pred = sgd.predict(X_test)\n\nprint(accuracy_score(y_train, sgd.predict(X_train)))\n\nsgd_acc = accuracy_score(y_test, sgd.predict(X_test))\nprint(sgd_acc)","ea718886":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)","690e9bc6":"# accuracy score\n\ndtc_acc = accuracy_score(y_test, dtc.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, dtc.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {dtc_acc}\\n\")","187de08a":"# dtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 11, min_samples_leaf = 1, min_samples_split = 2, splitter = 'best')\n# dtc.fit(X_train, y_train)","1143a4de":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","a680a992":"# accuracy score\n\nrf_acc = accuracy_score(y_test, rf.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, rf.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {rf_acc}\\n\")","70c67c0c":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\nada.fit(X_train, y_train)","b84c3fd8":"# accuracy score\n\nada_acc = accuracy_score(y_test, ada.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, ada.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {ada_acc}\\n\")","26b14ef5":"# hyper parameter tuning using grid search cv\n\ngrid_param = {\n    'n_estimators' : [40, 50, 70, 80, 100],\n    'learning_rate' : [0.01, 0.1, 0.05, 0.5, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, grid_param, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","fbd20bce":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","ed8f60cf":"ada = AdaBoostClassifier(base_estimator = ada, algorithm = 'SAMME.R', learning_rate = 1, n_estimators = 100)\nada.fit(X_train, y_train)","d3456d53":"# accuracy score\n\nada_acc = accuracy_score(y_test, ada.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, ada.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {ada_acc}\\n\")","57ce67f7":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","e84e3dd8":"# accuracy score\n\ngb_acc = accuracy_score(y_test, gb.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, gb.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {gb_acc}\\n\")","a7e3caf1":"sgb = GradientBoostingClassifier(subsample = 0.9, max_features = 0.8)\nsgb.fit(X_train, y_train)","06fea479":"# accuracy score\n\nsgb_acc = accuracy_score(y_test, sgb.predict(X_test))\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, sgb.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {sgb_acc}\\n\")","6edb58d2":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(learning_rate = 0.1, loss = 'deviance', n_estimators = 100)\nxgb.fit(X_train, y_train)","377d879e":"# accuracy score\n\nxgb_acc = accuracy_score(y_test, xgb.predict(X_test))\n\nprint(f\"Accuracy Score of Training Data is {accuracy_score(y_train, xgb.predict(X_train))}\")\nprint(f\"Accuracy Score of Training Data is {xgb_acc}\\n\")","bdf86fbc":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier()\nlgbm.fit(X_train, y_train)\n\nlgbm_acc = accuracy_score(y_test, lgbm.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, lgbm.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {lgbm_acc} \\n\")","d7df96a7":"from catboost import CatBoostClassifier\n\ncat = CatBoostClassifier()\ncat.fit(X_train, y_train)","4917e736":"cat_acc = accuracy_score(y_test, cat.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, cat.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {cat_acc} \\n\")","3d1ee630":"from sklearn.ensemble import ExtraTreesClassifier\n\netc = ExtraTreesClassifier()\netc.fit(X_train, y_train)","9cbde459":"etc_acc = accuracy_score(y_test, etc.predict(X_test))\n\nprint(f\"Training Accuracy of Decision Tree Classifier is {accuracy_score(y_train, etc.predict(X_train))}\")\nprint(f\"Test Accuracy of Decision Tree Classifier is {etc_acc} \\n\")","5f791eaf":"models = pd.DataFrame({\n    'Model' : ['Logistic Regression', 'KNN', 'SVC', 'SGD',  'Decision Tree', 'Random Forest','Ada Boost',\n             'Gradient Boosting', 'SGB', 'XgBoost', 'LGBM', 'Cat Boost', 'Extra Tree'],\n    'Score' : [lr_acc, knn_acc, svc_acc, sgd_acc, dtc_acc, rf_acc, ada_acc, gb_acc, sgb_acc, xgb_acc, lgbm_acc, cat_acc, etc_acc]\n})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","8e6ea267":"plt.figure(figsize = (20, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.ylim(0.45, 0.75)\nplt.show()","593e9651":"### XgBoost","f33915ed":"### Extra Trees Classifier","745e1434":"#### If you like my kernel, please do upvote","1ef482cb":"### Light Gradient Boosting Classifier","dc2b57a9":"### SGD Classifier","11532c35":"### Gradient Boosting Classifier","14215e55":"### Decision Tree","60b2977a":"### Thank You","eb39af9d":"### Cat Boost Classifier","20dd7bd7":"It looks like there are no missing values.","6dc5fcea":"### Stochastic Gradient Boosting (SGB)","0f24ed79":"### Ada Boost Classifier","2cb356eb":"### KNN","7d0c9a7b":"### SVC","6ef13190":"### Logistic Regression","fc1c2a59":"### Random Forest","0d64de72":"# Red Wine Classification"}}