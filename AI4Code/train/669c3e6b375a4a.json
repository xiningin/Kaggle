{"cell_type":{"a987973b":"code","e617c691":"code","67745984":"code","ebd35c49":"code","6bda6e8d":"code","5026f97e":"code","2208c23b":"code","a9196f2a":"code","a369364a":"code","8f526c07":"code","9935e53d":"code","c5ccc1e5":"markdown","0c0ad8ce":"markdown","2c9764c3":"markdown","104ebb59":"markdown","14d9d90c":"markdown","3275892a":"markdown","263a8bab":"markdown","6c70c89f":"markdown","9910b19c":"markdown","acbd50fc":"markdown"},"source":{"a987973b":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","e617c691":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()","67745984":"market_df, _ = env.get_training_data()\nmarket_df['time'] = market_df.time.dt.date\nmarket_df.shape","ebd35c49":"df = market_df.pivot(index='time', columns='assetCode', values='universe')\ntoRemove = df.count()[df.count().values < 100].index.values\ntoRemove = np.append(toRemove, df.mean()[df.mean().values < .05].index.values)\ndf = market_df.groupby('assetCode').agg({'time': 'max'}).reset_index()\ntoRemove = np.append(toRemove, df[df.time < datetime.date(2016, 12, 1)].assetCode.values)\ntoRemove = np.unique(toRemove)\nmarket_df = market_df[~market_df.assetCode.isin(toRemove)]\nmarket_df.shape","6bda6e8d":"market_df = market_df.sort_values(['assetCode', 'time'])\nmarket_df['sma5'] = market_df['close'].rolling(5, min_periods=1).mean()\ndf = market_df.pivot(index='time', columns='assetCode', values='sma5').reset_index()\ndf = df[df.time > datetime.date(2016,1,1)]\ndf = df.set_index('time')\ndf = df.dropna(axis=1)\ncorr = df.corr()\ndf.index.name = 'time'\ndf","5026f97e":"size = 7\nfig, ax = plt.subplots(figsize=(size, size))\nax.matshow(corr,cmap=cm.get_cmap('coolwarm'), vmin=0,vmax=1)","2208c23b":"from scipy.cluster.hierarchy import dendrogram, linkage\nZ = linkage(corr, 'average')\nfrom scipy.cluster.hierarchy import cophenet\nfrom scipy.spatial.distance import pdist\nimport pylab\nc, coph_dists = cophenet(Z, pdist(corr))\nc","a9196f2a":"fig = plt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram (truncated)')\nplt.xlabel('sample index or (cluster size)')\nplt.ylabel('distance')\ndendrogram(\n    Z,\n    truncate_mode='lastp',  # show only the last p merged clusters\n    p=100,  # show only the last p merged clusters\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,  # to get a distribution impression in truncated branches\n)\nplt.show()","a369364a":"from scipy.cluster.hierarchy import fcluster\nmax_d = 10\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters.max()","8f526c07":"df = df.transpose()\ndf['cluster'] = clusters\ndef norm(x):\n    y = x.transpose()\n    for col in y.columns:\n        y[col] = (y[col] - y[col][:-1].min()) \/ (y[col][:-1].max() - y[col][:-1].min())\n    return y.transpose()\nnormed_df = norm(df)\n\ndata = normed_df[df['cluster'] == 1]\ndata = data.transpose()\nf, (ax1) = plt.subplots(1, 1, figsize=(20,8), sharey=True)\nplt.subplots_adjust(wspace=0.05)\n\nt = data.drop('cluster')\ntickers = data.columns.values\nfor tix in tickers:\n    ax1.plot(t[tix],label=tix)\nax1.set_title('Stock Correlation')\nax1.set_ylabel('Close prices')\nax1.legend(loc='upper left',prop={'size':8})\nplt.setp(ax1.get_xticklabels(), rotation=70);","9935e53d":"dff = df.sort_values('cluster').transpose()\n# dff = df.groupby('cluster').mean().transpose()\nnew_corr = dff.corr()\nsize = 7\nfig, ax = plt.subplots(figsize=(size, size))\nax.matshow(new_corr,cmap=cm.get_cmap('coolwarm'), vmin=0,vmax=1)","c5ccc1e5":"So we are starting off with 4 million rows, which is way too much for the given memory! But there are lots tickers that either don't have enough entries to train or their \"universe\" label is mostly off, so they wouldn't have any significant effect on the final score anyways! Besides, we are interested only in the stocks that are still alive by the end of 2016.","0c0ad8ce":"I keep seeing people truncate the Market_train_df to the last couple of years to manage the memory. Not sure if that's their actual solution or they are keeping the big hammer in their private kernels. Anyways, I just wanted to contribute to the community by showing how to properly manage the training data by clustering the similar stocks together through the following steps.\n* **Data Preparation**: We'll start off with the usual suspects.","2c9764c3":"* **Clustering:**\nThere are lots of red spots that represent high correlation. Now we can use this correlation function as a distance metrics to cluster the stocks.","104ebb59":"As the scientists say, \"without the loss of generality\" we have reduced the size of the dataframe by 35%. But that's not enough! \n* **Correlation:**\nThere are many stocks that have very similar trends. So we can use hierarchical clustering (or whatever you are comfortable with) to combine these stocks together, based on the correlation of their 5 day moving average for the last year:","14d9d90c":"Now, let's take a look at the correlation matrix after sorting the data based on the clusters","3275892a":"So, now we have only 18 different indices! Compare it to the original 3000+ stocks!\nFinally, let's see how the stocks in the same cluster look!","263a8bab":"Let's take a look at the correlation matrix:","6c70c89f":"I have used \"average\" as my metric, seen it's showing one of the highest cophentic correlation distance. But feel free to explore your options!\nNow let's take a look at the last 100 iterations:","9910b19c":"***A brief tutorial on clustering the stocks***","acbd50fc":"The values of the x-axis show the number of stocks clustered together. Now it's time to pick a cut of value and be done with our clustering. Using the maximum distance, I'm clustering the stocks with distance of 10 or lower, which is too extreme but it's a good start for our training purposes."}}