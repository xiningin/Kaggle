{"cell_type":{"f4faf007":"code","292cc080":"code","eddc632f":"code","d6a98454":"code","834db855":"code","0b7b7b89":"code","29399873":"code","8b593f19":"code","028fb3f3":"code","66b160c9":"code","5852490e":"code","389b1441":"code","76310710":"code","d970a156":"code","a08ffe45":"code","cb286d79":"code","c73e7b05":"code","35c2fb34":"code","ad9a9f46":"code","7d5e2fb6":"markdown","3ded6bee":"markdown","ed7af0c9":"markdown","2758fd7d":"markdown","ddc557e4":"markdown","98792579":"markdown","c0c456d8":"markdown","7f494fe1":"markdown","e62d6b08":"markdown","c07253a4":"markdown","e4fd462d":"markdown","98b78fa0":"markdown"},"source":{"f4faf007":"from sklearn.datasets import make_circles\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np","292cc080":"X, y = make_circles(n_samples=1000, noise=.05, factor=.5)","eddc632f":"plt.scatter(X[:,0],X[:,1],c=y,)","d6a98454":"col = np.sum(np.square(X),axis=1)","834db855":"Z  = np.hstack([X,col.reshape(-1,1)])","0b7b7b89":"from mpl_toolkits.mplot3d import Axes3D","29399873":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter3D(xs=Z[:,0],ys=Z[:,1],zs=Z[:,2],c=y)","8b593f19":"from sklearn.datasets import load_iris\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","028fb3f3":"iris_data = load_iris()","66b160c9":"plt.scatter(iris_data.data[:,2], iris_data.data[:,3],c=iris_data.target)","5852490e":"from sklearn.svm import SVC","389b1441":"svc = SVC(kernel='linear')","76310710":"svc.fit(iris_data.data[:,[2,3]], iris_data.target)","d970a156":"X = iris_data.data[:,[2,3]]","a08ffe45":"y = iris_data.target","cb286d79":"h = .02\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))","c73e7b05":"Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])","35c2fb34":"Z = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:,0],X[:,1],c=y,s=10)\nplt.scatter(svc.support_vectors_[:,0], svc.support_vectors_[:,1], c='r',s=20)","ad9a9f46":"svc.support_vectors_","7d5e2fb6":"### 2. Maximal Margin Classifier\n* Objective is to find a line( or hyperplane ) which maximizes the separation between data of different classes\n* The distance between the line and the closest data points is referred to as the margin.\n* The best or optimal line or hyper-plane is that can separate the two classes is the line that as the largest margin.\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQkOQ1Q4Xr1J-J8qbwOZxbV9HmwOs2o2iHo2dLRxVxjq1R0MibU0A\">\n* Hyperplane is learnt from training data & objective is to maximize the margin","3ded6bee":"<h3> What we are going to do here is to separate this non-linearly separable data into linearly separable <h3\/>","ed7af0c9":"### Understanding Kernels\n* Kernels are integral part of SVM which is responsible of transforming non-linearly separable data to higher dimension such that they are linearly seprable\n* Different types of kernels are supported by SVM\n* Every kernel has different way to transforming data into higher dimension\n* Linear, Polynomial, RBF, Sigmoid & your own custom kernel","2758fd7d":"### 4. Support Vector Machine\n\n\n* For data linearly separable, SVM works like above mentioned Soft Margin Classifier\n* Data which is non-linearly separable \n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*C3j5m3E3KviEApHKleILZQ.png\" width=\"300px\">\n\n* SVM supports configurable kernel functions which has ability to transform data to higher dimension.\n* Check if decision boundry exists now to separate the classes in higher dimension.\n* Transform the decision boundry to original dimension\n    ","ddc557e4":"### 3. Soft Margin Classifier\n* In real world, data is always messed up & there is no clear separation of boundries\n* With Soft Margin Classifier, few data points are allowed to be misclassified\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Catarina_Moreira2\/publication\/260283043\/figure\/fig12\/AS:297261608259590@1447884098130\/Figure-A14-Soft-margin-linear-SVM-classifier.png\" width=\"300px\">\n\n* The C parameter trades off correct classification of training examples against maximization of the decision function\u2019s margin. \n* For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. \n* A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. \n* In other words``C`` behaves as a regularization parameter in the SVM.","98792579":"### 1. Introduction to Support Vector Machines\nA support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they\u2019re able to categorize new text.\n\nCompared to newer algorithms like neural networks, they have two main advantages: higher speed and better performance with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems, where it\u2019s common to have access to a dataset of at most a couple of thousands of tagged samples.\n\n* Supervised Learning method used for Classification, Regression & Outlier Detection\n* SVMs' are effective in high dimension spaces\n* Uses subset of training data, so memory efficient\n* Effective in higher dimensional data\n* Also, in situations where dimension is greater than rows","c0c456d8":"#### Linear Kernels\n* Always take simple to complex approach.\n* If linear separation is possible between classes, linear kernels would work & not otherwise","7f494fe1":"## Support Vector Machines\n\n","e62d6b08":"* Now, we can see a possible plane separating the data\n* SVM does all these thing using libraries","c07253a4":"<h3> Let us generate an example here <h3\/>","e4fd462d":"### Doing transformation manually","98b78fa0":"* We can see linear separators"}}