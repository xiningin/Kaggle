{"cell_type":{"02cf37e8":"code","782df1c5":"code","743c4546":"code","bd2cdcb3":"code","6684cef4":"code","0d094563":"code","1bbfd8a3":"code","4db93f19":"code","7e78ab3b":"code","9450db21":"code","d3064cfb":"code","5ae1c94b":"code","3038ae1f":"code","c7d3f961":"code","21f20b76":"code","016b59fe":"code","cd9a7526":"markdown","c4112fd7":"markdown","82b00660":"markdown","a73f517c":"markdown","5f3552e1":"markdown","e6e843e3":"markdown","ff138fa5":"markdown","047afc12":"markdown","255c417b":"markdown","d449a791":"markdown"},"source":{"02cf37e8":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport random\n\n#the basics\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nSEED = 2020\n\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n\nseed_everything(SEED)","782df1c5":"from torch import nn\n\n\nclass RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n        return loss\n\n\nclass MCRMSELoss(nn.Module):\n    def __init__(self, num_scored=3):\n        super().__init__()\n        self.rmse = RMSELoss()\n        self.num_scored = num_scored\n\n    def forward(self, yhat, y):\n        score = 0\n        for i in range(self.num_scored):\n            score += self.rmse(yhat[:, :, i], y[:, :, i]) \/ self.num_scored\n\n        return score","743c4546":"import pandas as pd\n\n\ndef load_json(path):\n    return pd.read_json(path, lines=True)\n\ndf = load_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json')\ndf_test = load_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json')\nsample_sub = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')\ndf = df[df.SN_filter == 1]","bd2cdcb3":"\ndf.head()","6684cef4":"target_cols = [\"reactivity\", \"deg_Mg_pH10\", \"deg_Mg_50C\"]\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n\ntrain_inputs = torch.tensor(preprocess_inputs(df)).to(device)\nprint(\"input shape: \", train_inputs.shape)\ntrain_labels = torch.tensor(\n    np.array(df[target_cols].values.tolist()).transpose(0, 2, 1)\n).float().to(device)","0d094563":"class LSTM_model(nn.Module):\n    def __init__(\n        self, seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128, hidden_layers=3\n    ):\n        super(LSTM_model, self).__init__()\n        self.pred_len = pred_len\n\n        self.embeding = nn.Embedding(num_embeddings=len(token2int), embedding_dim=embed_dim)\n        self.gru = nn.LSTM(\n            input_size=embed_dim * 3,\n            hidden_size=hidden_dim,\n            num_layers=hidden_layers,\n            dropout=dropout,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.linear = nn.Linear(hidden_dim * 2, len(target_cols))\n\n    def forward(self, seqs):\n        embed = self.embeding(seqs)\n        reshaped = torch.reshape(embed, (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        output, hidden = self.gru(reshaped)\n        truncated = output[:, : self.pred_len, :]\n        out = self.linear(truncated)\n        return out\n\ncriterion = MCRMSELoss(len(target_cols))\n\ndef compute_loss(batch_X, batch_Y, model, optimizer=None, is_train=True):\n    model.train(is_train)\n\n    pred_Y = model(batch_X)\n\n    loss = criterion(pred_Y, batch_Y)\n\n    if is_train:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return loss.item()","1bbfd8a3":"FOLDS = 4\nEPOCHS = 90\nBATCH_SIZE = 64\nVERBOSE = 2\nLR = 0.01","4db93f19":"#get different test sets and process each\npublic_df = df_test.query(\"seq_length == 107\").copy()\nprivate_df = df_test.query(\"seq_length == 130\").copy()\n\npublic_inputs = torch.tensor(preprocess_inputs(public_df)).to(device)\nprivate_inputs = torch.tensor(preprocess_inputs(private_df)).to(device)\n\npublic_loader = DataLoader(TensorDataset(public_inputs), shuffle=False, batch_size=BATCH_SIZE)\nprivate_loader = DataLoader(TensorDataset(private_inputs), shuffle=False, batch_size=BATCH_SIZE)","7e78ab3b":"lstm_histories = []\nlstm_private_preds = np.zeros((private_df.shape[0], 130, len(target_cols)))\nlstm_public_preds = np.zeros((public_df.shape[0], 107, len(target_cols)))\n\ncriterion = MCRMSELoss()\n\nkfold = KFold(FOLDS, shuffle=True, random_state=2020)\n\nfor k, (train_index, val_index) in enumerate(kfold.split(train_inputs)):\n    train_dataset = TensorDataset(train_inputs[train_index], train_labels[train_index])\n    val_dataset = TensorDataset(train_inputs[val_index], train_labels[val_index])\n\n    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n\n    model = LSTM_model().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=LR)\n\n    train_losses = []\n    val_losses = []\n    for epoch in tqdm(range(EPOCHS)):\n        train_losses_batch = []\n        val_losses_batch = []\n        for (batch_X, batch_Y) in train_loader:\n            train_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=True)\n            train_losses_batch.append(train_loss)\n        for (batch_X, batch_Y) in val_loader:\n            val_loss = compute_loss(batch_X, batch_Y, model, optimizer=optimizer, is_train=False)\n            val_losses_batch.append(val_loss)\n        train_losses.append(np.mean(train_losses_batch))\n        val_losses.append(np.mean(val_losses_batch))\n    model_state = model.state_dict()\n    del model\n            \n    lstm_histories.append({'train_loss': train_losses, 'val_loss': val_losses})\n\n\n    lstm_short = LSTM_model(seq_len=107, pred_len=107).to(device)\n    lstm_short.load_state_dict(model_state)\n    lstm_short.eval()\n    lstm_public_pred = np.ndarray((0, 107, len(target_cols)))\n    for batch in public_loader:\n        batch_X = batch[0]\n        pred = lstm_short(batch_X).detach().cpu().numpy()\n        lstm_public_pred = np.concatenate([lstm_public_pred, pred], axis=0)\n    lstm_public_preds += lstm_public_pred \/ FOLDS\n\n    lstm_long = LSTM_model(seq_len=130, pred_len=130).to(device)\n    lstm_long.load_state_dict(model_state)\n    lstm_long.eval()\n    lstm_private_pred = np.ndarray((0, 130, len(target_cols)))\n    for batch in private_loader:\n        batch_X = batch[0]\n        pred = lstm_long(batch_X).detach().cpu().numpy()\n        lstm_private_pred = np.concatenate([lstm_private_pred, pred], axis=0)\n    lstm_private_preds += lstm_private_pred \/ FOLDS\n    \n    del lstm_short, lstm_long","9450db21":"\nfig, ax = plt.subplots(1, 1, figsize = (20, 10))\n\nfor history in lstm_histories:\n    ax.plot(history['train_loss'], 'b')\n    ax.plot(history['val_loss'], 'r')\n\nax.set_title('LSTM')\n\nax.legend(['train', 'validation'], loc = 'upper right')\n\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch');\n\n","d3064cfb":"public_df = df_test.query(\"seq_length == 107\").copy()\nprivate_df = df_test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","5ae1c94b":"preds_lstm = []\n\nfor df, preds in [(public_df, lstm_public_preds), (private_df, lstm_private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_lstm.append(single_df)\n\npreds_lstm_df = pd.concat(preds_lstm)\npreds_lstm_df.head()","3038ae1f":"submission = sample_sub[['id_seqpos']].merge(preds_lstm_df, on=['id_seqpos'])","c7d3f961":"submission['deg_pH10'] = 0\nsubmission['deg_50C'] = 0","21f20b76":"submission.head()","016b59fe":"submission.to_csv('submission.csv', index=False)\nprint('Submission saved')\n","cd9a7526":"### Dataload","c4112fd7":"just as we use the loss,\n```python\ncriterion = MCRMSELoss()\npredictions = model(data)\nloss = criterion(predictions, targets)\n```","82b00660":"### KFold Training and Inference","a73f517c":"## Model","5f3552e1":"### preprocess","e6e843e3":"## MCRMSELoss","ff138fa5":"# Implementation of MCRMSELoss\n\nIn this notebook, MCRMSE is implemented according to [this resource](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/overview\/evaluation) and some examples are shown.\nExample section is based on the [@hiroshun's notebook](https:\/\/www.kaggle.com\/hiroshun\/pytorch-implementation-gru-lstm). Thank you for publishing a good implementation.\n\nUsing this MCRMSELoss and `SN_filter` `df = df[df.SN_filter == 1]`, you can simulate the LB score. However you got to be carefull that the private score does not use `SN_filter`.","047afc12":"if you use `SN_filter`, you can get the LB-like score. \nLet's see how it works.","255c417b":"### Submission","d449a791":"## usage"}}