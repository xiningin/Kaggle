{"cell_type":{"b45341aa":"code","4af9a551":"code","95eb9b39":"code","8ad52394":"code","92ee4d01":"code","828fe453":"code","b875d933":"code","ac6d2cf2":"code","54f1567a":"code","89733988":"code","282a9087":"code","745a384d":"code","ee41c10a":"code","01968d50":"code","9cd938c1":"code","8bcd01bd":"code","61aec463":"code","ff9b4b4a":"code","3cb99a55":"code","47345f6f":"code","580d95d8":"code","73e68bad":"code","ca14977c":"code","9587e064":"code","cab54c87":"code","16a0f732":"code","555f8e68":"code","1ecd9307":"code","45c3498f":"code","db595ffa":"code","71b7c0fb":"code","fa0e6efb":"code","f36e9a6a":"code","af41fe18":"markdown","85f764bf":"markdown","4a7207f7":"markdown","41303d25":"markdown","a5c9c073":"markdown","9460c6d9":"markdown","e264c29d":"markdown","c1e6acad":"markdown","aa822bc9":"markdown","143db5dd":"markdown","f817c6d6":"markdown","84c87f0c":"markdown","a1649c68":"markdown","2e34b437":"markdown","cdf4d39c":"markdown"},"source":{"b45341aa":"VOCABULARY_SIZE = 4095\nVALIDATION_SPLIT = 0.25\nSPACY_MODEL = 'en_core_web_lg'","4af9a551":"# basic imports\nimport os\n\nimport IPython\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport keras\nfrom keras import layers\nfrom keras.regularizers import l1, l2\n\nfrom sklearn.model_selection import train_test_split","95eb9b39":"# an NLP library\n# get a model with pretrained embeddings and load it\n\n# if GPU is on, Internet connection is needed in Kaggle kernels!\n!python -m spacy download $SPACY_MODEL\n\nimport spacy\nnlp = spacy.load(SPACY_MODEL)","8ad52394":"# convenience settings\n\n### plotting ###\n# prettier and larger basic graphs\nsns.set(rc={\n    'figure.figsize':(18,8),\n    'axes.titlesize':14,\n})\n\n### pandas ###\n# make the tables more compact vertically, too\npd.options.display.max_rows = 20","92ee4d01":"print(os.listdir(\"..\/input\"))","828fe453":"# load and preview the training set\nreviews_df = pd.read_csv('..\/input\/drugsComTrain_raw.csv')\nreviews_df","b875d933":"# once more for the test set\nreviews_test_df = pd.read_csv('..\/input\/drugsComTest_raw.csv')\nreviews_test_df","ac6d2cf2":"# sanitizing review text\n\nfrom html import unescape\n\ndef clean_review(text):\n    \"\"\"Replace HTML escaped characters, byte order mark\n    and strip outer quotes and spaces\"\"\"\n    return unescape(text.strip(' \"\\'')).replace('\\ufeff1', '')\n\nreviews_df.review = reviews_df.review.apply(clean_review)\nreviews_test_df.review = reviews_test_df.review.apply(clean_review)","54f1567a":"from wordcloud import WordCloud\n\nwordcloud = WordCloud(\n    width=1600,\n    height=900,\n    random_state=2019,\n    background_color='white',\n    max_words=400,\n    colormap='bone'\n)\n\nwordcloud.generate('\\n'.join(reviews_df.review))\nplt.imshow(wordcloud, interpolation='lanczos')\nplt.axis('off')\nplt.show()","89733988":"markdown = '### A peek at some reviews\\n'\nfor idx, row in reviews_df.sample(10).iterrows():\n    markdown += f'\\n\\nReview {row.uniqueID}\\n\\n> '\n    markdown += '\\n> '.join(row.review.splitlines())\n    \nIPython.display.Markdown(markdown)","282a9087":"IPython.display.Markdown(\n    f\"I will use spaCy's `{SPACY_MODEL}` model for tokenizing reviews and the embeddings, \"\n    f\"the model has {len(nlp.vocab)} lexemes in its vocabulary \"\n    f\"and {len(nlp.vocab.vectors)} have embeddings of length {nlp.vocab.vectors_length}.\"\n)","745a384d":"# sequencing review text\n\ndef text2tokens(text):\n    \"\"\"text -> tokenize -> lemmatize\/normalize\"\"\"\n    \n    tokens = nlp(\n        # also split on \"\/\"\n        text.replace('\/', ' \/ '),\n        \n        # we only need tokenizer and lemmas, so disable the rest\n        disable=['tagger', 'parser', 'ner']\n    )\n    \n    lexemes = []\n    for token in tokens:\n        \n        # sometimes whitespace gets recognized as a token...\n        if token.text.isspace():\n            continue\n            \n        # prefer more general representations\n        # but only if they have an embedding\n        if nlp.vocab[token.lemma_.lower()].has_vector:\n            lexeme = token.lemma_.lower()\n        elif nlp.vocab[token.norm_.lower()].has_vector:\n            lexeme = token.norm_.lower()\n        else:\n            lexeme = token.lower_\n        \n        lexemes.append(lexeme)\n        \n    return lexemes\n\nreviews_word_seq = [text2tokens(review) for review in reviews_df.review]\nreviews_test_word_seq = [text2tokens(review) for review in reviews_test_df.review]","ee41c10a":"# count occurences of each word\/token\n\nword_count = {}\nfor lemmas in reviews_word_seq:\n    for lemma in lemmas:\n        word_count[lemma] = word_count.get(lemma, 0) + 1\nword_count = dict(sorted(word_count.items(), key=lambda pair: pair[1], reverse=True))","01968d50":"# display a table of the most frequent words\n\n# keep it within our vocabulary\nshow_words = min(1000, VOCABULARY_SIZE)\ncolumns = 5\n\nmarkdown_rows = []\nmarkdown_rows.append(f'#### Top {show_words} most frequent tokens\\n')\nmarkdown_rows.append('| Token | Count '*columns + '|')\nmarkdown_rows.append('| ---: | :---- '*columns + '|')\n\nrow = ''\nfor index, (word, count) in enumerate(word_count.items()):\n    if index >= show_words: break\n    if index%columns == 0 and row:\n        markdown_rows.append(row + '|')\n        row = ''\n    row += f'| {word} | {count} '\nif row:\n    markdown_rows.append(row + '|')\n    \nIPython.display.Markdown('\\n'.join(markdown_rows))","9cd938c1":"plt.title('Vocabulary Distribution')\n\nwc_vals = list(word_count.values())\n\nplt.plot(wc_vals, 'o', label='Word count')\nplt.plot(sum(wc_vals)-np.cumsum(wc_vals), '.', label='Uncaptured words')\nplt.axvline(x=VOCABULARY_SIZE, color='yellowgreen', label=f'Current vocabulary size ({VOCABULARY_SIZE})')\n\nplt.gca().set_yscale('log')\n# plt.gca().set_xscale('log')\n\nplt.xlabel('Vocabulary Size')\nplt.ylabel('Amount')\nplt.legend()\n\nplt.show()","8bcd01bd":"# word sequences to indexed sequences\n\nvocab = list(word_count)[:VOCABULARY_SIZE]\nword2index = {word:index for index, word in enumerate(vocab, start=1)}\n\nword_seq2indexed = lambda reviews: [\n    [\n        word2index.get(word, 0)\n        for word in review\n    ]\n    for review in reviews\n]\n\nreviews_seq = word_seq2indexed(reviews_word_seq)\nreviews_test_seq = word_seq2indexed(reviews_test_word_seq)\n\n# no need for these anymore\ndel reviews_word_seq, reviews_test_word_seq","61aec463":"# set max sequence length so that it captures 99% full reviews\nreview_lengths = np.array(list(map(len, reviews_seq)))\nsequence_cutoff_legth = int(np.quantile(review_lengths, 0.99))","ff9b4b4a":"plt.title('Distribution of Review Lengths')\n\nsns.distplot(\n    review_lengths,\n    hist_kws=dict(label='Normalized histogram'),\n    kde=True,\n    kde_kws=dict(label='Kernel density'),\n    rug=True,\n    norm_hist=False,\n    rug_kws=dict(color='orangered', label='Review'),\n    axlabel='Sequence Length',\n)\nplt.axvline(\n    x=sequence_cutoff_legth,\n    color='yellowgreen',\n    label=f'Sequence cutoff length ({sequence_cutoff_legth})'\n)\n\nplt.gca().set_xscale('log')\n\nplt.xlabel('Review Length')\nplt.ylabel('Density')\nplt.legend()\n\nplt.show()","3cb99a55":"# pad the sequences\n\nreviews_seq = keras.preprocessing.sequence.pad_sequences(\n    reviews_seq,\n    maxlen=sequence_cutoff_legth\n)\n\nreviews_test_seq = keras.preprocessing.sequence.pad_sequences(\n    reviews_test_seq,\n    maxlen=sequence_cutoff_legth\n)","47345f6f":"# extract embeddings for our vocabulary\n\nembedding_weights = np.zeros((\n    VOCABULARY_SIZE+1, # indices\/hashes\n    nlp.vocab.vectors_length # embedding dimmension\n))\n\nfor word, index in word2index.items():\n    embedding_weights[index] = nlp.vocab[word].vector","580d95d8":"# here it goes :-)\n\ninput_reviews = layers.Input(shape=(sequence_cutoff_legth,), dtype='int32')\n\n\nembedding = layers.Embedding(\n    *embedding_weights.shape,\n    weights=[embedding_weights],\n    input_length=sequence_cutoff_legth,\n    trainable=True,\n)(input_reviews)\nembedding = layers.GaussianNoise(0.15)(embedding)\n\n\nbranch_a = layers.CuDNNLSTM(128)(embedding)\nbranch_a = layers.BatchNormalization()(branch_a)\nbranch_a = layers.Dropout(0.3)(branch_a)\nbranch_a = layers.GaussianNoise(0.3)(branch_a)\n\nbranch_a = layers.Dense(10, activation='relu')(branch_a)\nbranch_a = layers.BatchNormalization()(branch_a)\nbranch_a = layers.GaussianNoise(0.3)(branch_a)\n\n\nbranch_b = layers.Conv1D(16, 3, padding='same', activation='relu')(embedding)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPool1D(2)(branch_b)\n\nbranch_b = layers.Conv1D(32, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPool1D(2)(branch_b)\n\nbranch_b = layers.Conv1D(64, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPool1D(2)(branch_b)\n\nbranch_b = layers.Conv1D(128, 3, padding='same', activation='relu')(branch_b)\nbranch_b = layers.Dropout(0.3)(branch_b)\n\nbranch_b = layers.GlobalAvgPool1D()(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.Dropout(0.3)(branch_b)\nbranch_b = layers.GaussianNoise(0.3)(branch_b)\n\nbranch_b = layers.Dense(10, activation='relu')(branch_b)\nbranch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.GaussianNoise(0.3)(branch_b)\n\n\ninput_useful_count = layers.Input(shape=(1,))\n\n\nintermediate = layers.concatenate([branch_a, branch_b, input_useful_count])\nintermediate = layers.Dropout(0.3)(intermediate)\n\nintermediate = layers.Dense(32, activation='relu')(intermediate)\nintermediate = layers.BatchNormalization()(intermediate)\nintermediate = layers.Dropout(0.3)(intermediate)\nintermediate = layers.GaussianNoise(0.3)(intermediate)\n\n\nregression_top = layers.Dense(10, activation='relu')(intermediate)\nregression_top = layers.BatchNormalization()(regression_top)\nregression_top = layers.GaussianNoise(0.1)(regression_top)\n\nregression_top = layers.Dense(1, activation='sigmoid', name='regressor')(regression_top)\n\n\nclassification_top = layers.Dense(10, activation='relu', activity_regularizer=l2(2e-4))(intermediate)\nclassification_top = layers.BatchNormalization()(classification_top)\nclassification_top = layers.Dropout(0.2)(classification_top)\nclassification_top = layers.GaussianNoise(0.2)(classification_top)\n\nclassification_top = layers.Dense(10, activation='softmax', name='classifier')(classification_top)\n\n\nmodel = keras.models.Model(\n    inputs=[input_reviews, input_useful_count],\n    outputs=[regression_top, classification_top],\n)\nmodel.summary()","73e68bad":"# visualize the model graph\n\nmodel_viz = keras.utils.vis_utils.model_to_dot(model)\nIPython.display.SVG(model_viz.create(prog='dot', format='svg'))","ca14977c":"# standard scaling of the \"useful count\" attribute\n\nuseful_count_mean = reviews_df.usefulCount.mean()\nuseful_count_std = reviews_df.usefulCount.std()\n\nreviews_df.usefulCount += useful_count_mean\nreviews_df.usefulCount \/= useful_count_std\n\nreviews_test_df.usefulCount += useful_count_mean\nreviews_test_df.usefulCount \/= useful_count_std","9587e064":"# training\/validation split and creation\n\nrating = reviews_df.rating.values\n\n(\n    y_regr_train, y_regr_valid,\n    y_clas_train, y_clas_valid,\n    x1_train, x1_valid,\n    x2_train, x2_valid,\n) = train_test_split(\n    # reggression output between 0.1 and 1.0\n    rating \/ 10,\n    # classification between 0 and 9 and to categorical\n    keras.utils.to_categorical(rating-1, num_classes=10),\n    \n    reviews_seq, # main input\n    reviews_df.usefulCount.values, # aux. input\n    \n    # options\n    test_size=VALIDATION_SPLIT,\n    stratify=reviews_df.rating.values\n)\n\nx_train = [x1_train, x2_train]\ny_train = [y_regr_train, y_clas_train]\n\nx_valid = [x1_valid, x2_valid]\ny_valid = [y_regr_valid, y_clas_valid]","cab54c87":"model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=['mae', 'categorical_crossentropy'],\n    metrics={'classifier':'accuracy', 'regressor':'mae'},\n    # put more weight to the regressor\n    loss_weights=[2,1],\n)","16a0f732":"# let's give it some work out!\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    validation_data=(x_valid, y_valid),\n    batch_size=256,\n    epochs=300,\n    verbose=0,\n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            'model'\n            '-epoch_{epoch:02d}'\n            '-regr_mae_{val_regressor_loss:.2f}'\n            '-clas_acc_{val_classifier_acc:.2f}.hdf5',\n            monitor='val_regressor_loss',\n            verbose=0,\n            save_best_only=True,\n            save_weights_only=False,\n            mode='auto',\n            period=1,\n        ),\n    ],\n)\n\n\nmodel.save('model-last_epoch.hdf5')","555f8e68":"def plot_history(history, skip_first_n_epochs=0):\n    \"\"\"Show information about the training\"\"\"\n    \n    # plot every train-valid metric pair separately\n    for metric in history:\n        if not metric.startswith('val_'):\n            x = np.arange(len(history[metric]))+1\n\n            y_train = history[metric][skip_first_n_epochs:]\n            y_valid = history['val_'+metric][skip_first_n_epochs:]\n\n            plt.plot(x, y_train)\n            plt.plot(x, y_valid)\n\n            plt.legend([metric, 'val_'+metric], fontsize='large')\n\n            plt.title(\n                f'{metric.upper()} - '\n                f'min\/max [train: {min(y_train):.3f}\/{max(y_train):.3f}, '\n                f'valid: {min(y_valid):.3f}\/{max(y_valid):.3f}]'\n            )\n            \n            plt.xlabel('epoch')\n            plt.show()\n            \nplot_history(history.history)","1ecd9307":"# evaluate the model\n\ndef y2original(y):\n    \"\"\"Convert Y result to the original score scale\"\"\"\n    y_regr, y_clas = y\n    y_regr = 10*y_regr\n    y_clas = np.argmax(y_clas, axis=1) + 1\n    return y_regr, y_clas\n\nx_test = [reviews_test_seq, reviews_test_df.usefulCount.values]\n\ny_test = reviews_test_df.rating.values\ny_train = y2original(y_train)[0]\ny_valid = y2original(y_valid)[0]\n\n# evaluate model and convert output\nyx_train_regr, yx_train_clas = y2original(model.predict(x_train))\nyx_valid_regr, yx_valid_clas = y2original(model.predict(x_valid))\nyx_test_regr,  yx_test_clas  = y2original(model.predict(x_test))","45c3498f":"from sklearn.metrics import cohen_kappa_score, mean_absolute_error, accuracy_score\n\ndef show_metrics(title, y_true, y_regr, y_clas):\n    y_regr_closest = np.round(y_regr)\n    \n    fmt = '{:<16} | {:>8} | {:>8} | {:>8}'.format\n    nums2str = lambda *nums: (f'{n:.3f}' for n in nums)\n    \n    print(fmt(title, 'MAE', 'KAPPA', 'ACCURACY'))\n    print(fmt(' regression', *nums2str( \n            mean_absolute_error(y_true, y_regr),\n            cohen_kappa_score(y_true, y_regr_closest),\n            accuracy_score(y_true, y_regr_closest)\n    )))\n    \n    print(fmt(' classification', *nums2str( \n            mean_absolute_error(y_true, y_clas),\n            cohen_kappa_score(y_true, y_clas),\n            accuracy_score(y_true, y_clas)\n    )))\n    print()\n    ","db595ffa":"show_metrics('TRAINING SET', y_train, yx_train_regr, yx_train_clas)\nshow_metrics('VALIDATION SET', y_valid, yx_valid_regr, yx_valid_clas)\nshow_metrics('TEST SET', y_test, yx_test_regr, yx_test_clas)","71b7c0fb":"# rearange so that true labels lump together\n# (sort is stable so within a rating level train-valid-test order holds)\n\nY = np.concatenate([y_train, y_valid, y_test])\nYX_REGR = np.concatenate([yx_train_regr, yx_valid_regr, yx_test_regr])\nYX_CLAS = np.concatenate([yx_train_clas, yx_valid_clas, yx_test_clas])\n_INDEX = np.arange(len(Y))\n\nquadratuplets = zip(Y, YX_REGR, YX_CLAS, _INDEX)\nrearanged = sorted(quadratuplets, key=lambda q: q[0])\n\ny, yx_regr, yx_clas, index = zip(*rearanged)","fa0e6efb":"# separate them again, so that we can add colors\n\n# alternative would be to make an array with colors already earlier\n# and add it to the sorting above, then having it as an argument\n# for scatter plot, it was, however, slow and not so pretty :-)\n\n# for axis indices on the plot\naxis = np.arange(len(y))\n\nindex_train = []\nindex_valid = []\nindex_test = []\nt1, t2 = len(y_train), len(y_train) + len(y_valid)\nfor i, a in zip(index, axis):\n    if i < t1:\n        index_train.append(a)\n    elif i < t2:\n        index_valid.append(a)\n    else:\n        index_test.append(a)\n\naxis_train = axis[index_train]\naxis_valid = axis[index_valid]\naxis_test = axis[index_test]\n\nyx_train_regr = np.asarray(yx_regr)[index_train]\nyx_valid_regr = np.asarray(yx_regr)[index_valid]\nyx_test_regr  = np.asarray(yx_regr)[index_test]\n\nyx_train_clas = np.asarray(yx_clas)[index_train]\nyx_valid_clas = np.asarray(yx_clas)[index_valid]\nyx_test_clas  = np.asarray(yx_clas)[index_test]","f36e9a6a":"# the plot finally\ncm = plt.cm.viridis\n\nplt.plot(axis_train, yx_train_regr, '.', alpha=0.1, color=cm(0.75))\nplt.plot(axis_valid, yx_valid_regr, '.', alpha=0.1, color=cm(0.8))\nplt.plot(axis_test,  yx_test_regr , '.', alpha=0.1, color=cm(0.9))\n\nplt.plot(axis_train, yx_train_clas, 'o', alpha=0.1, color=cm(0.1))\nplt.plot(axis_valid, yx_valid_clas, 'o', alpha=0.1, color=cm(0.17))\nplt.plot(axis_test,  yx_test_clas , 'o', alpha=0.1, color=cm(0.3))\n\nplt.plot(axis, y, 'o', c='#88888801', markersize=12)\n\nlegend_label = lambda c, l: plt.Line2D(\n    [0], [0], linewidth=0, marker='o', color=c, label=l)\n\nlegend_elements = [\n    legend_label('#888888', 'Actual score'),\n    legend_label(cm(0.1), 'Classification (training)'),\n    legend_label(cm(0.17), 'Classification (validation)'),\n    legend_label(cm(0.3), 'Classification (test)'),\n    legend_label(cm(0.75), 'Regression (training)'),\n    legend_label(cm(0.8), 'Regression (validation)'),\n    legend_label(cm(0.9), 'Regression (test)'),\n]\n\nplt.legend(handles=legend_elements)\nplt.xticks([])\nplt.xlabel('Reviews')\nplt.ylabel('Rating')\nplt.show()","af41fe18":"### *What do they talk about?*","85f764bf":"## Model\n\nIf you prefer to see a graphical representation rather than the code, scroll below.\n\nThe model is based on a merge of two models which I found to perform relatively well\non the regression task, both had about 1.0 - 1.2 mean absolute validation error\n(when measured against original 1 - 10 integer labels).\nAt first, I only made an ensemble of the two\n(the recurrent branch having its own embeddings and the recurrent part having spaCy's pretrained embeddings).\nMany iterative changes to the architecture were inconclusive - such as having both embeddings trainable,\nhaving only single common embedding (for simplicity and speed, I decided to go with this option),\nlarger model on top of the branches...\nHowever, I managed to improve slightly on the previous results\nand the model was consistently better than my original [benchmark (S. Prikhodko et al.)](https:\/\/www.kaggle.com\/stasian\/predicting-review-scores-using-neural-networks).\nI also added the \"useful count\" attribute as an auxiliary input which delivered another slight boost, depending on the architecture.\n\nI was interested in repurposing the model for the classification problem introduced in the [original paper](https:\/\/dl.acm.org\/citation.cfm?id=3194677).\nThe model was strongly overfitting and it proved to be very challenging to regularize.\nAdding the regressive output layer helped to alleviate the overfitting,\nalbeit the the regressive branch started underfitting...\nThat trade inspired me to modify the classifier for the original 10 classes,\nin belief that the two outputs will support and regularize each other when they share a common distribution of targets.\nThe results are positive (although the classifier still overfits but still achieves great performance).","4a7207f7":"## A bit of insight","41303d25":"The reviews are quite unhealthy - they contain bad stuff for tokenizers such as `&#039;` *(HTML code for apostrophe)*,\nlet's take some remedy before going further...","a5c9c073":"## What is this?\n\nThis is an assignment for *Cognitive Systems for Health Technology Applications* course.\n\nThe goal was to design a model for **drug rating prediction based on its review** in the [UCI Drug Review dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Drug+Review+Dataset+%28Drugs.com%29).\n\n### Brief overview of the results\nThe model was trained to predict original labels from reviews,\nboth using a discrete classifier and a single variable regression.\nBased on a validation set,\nit reaches about 0.85 mean average error,\n0.54 Cohen's kappa score, 0.62 accuracy.\nThe full results can be found at the bottom.\n\n\n### You can find here\n * brief overview of the dataset\n * text (pre)processing\n * decision supporting analysis\n * my final model's architecture, training and evaluation\n * comments along the way\n\nI decided not to include exploratory analysis of the dataset,\nas there already are many good resources available, for instance:\n - [Exploratory Data Analysis w\/ Python](https:\/\/www.kaggle.com\/lkuffo\/exploratory-data-analysis-w-python)\n - [Kaggle Starter](https:\/\/www.kaggle.com\/kerneler\/starter-uci-ml-drug-review-dataset-b99672ef-0)\n - [Data analysis and first classification experiments](https:\/\/github.com\/sakluk\/cognitive-systems-for-health-technology\/blob\/master\/Week%206.%20Case%203%20-%20First%20classification%20experiments.ipynb)\n\nOther resources:\n - [this dataset on Kaggle](https:\/\/www.kaggle.com\/jessicali9530\/kuc-hackathon-winter-2018)\n - [original paper](https:\/\/dl.acm.org\/citation.cfm?id=3194677)\n - [similar kernel](https:\/\/www.kaggle.com\/stasian\/predicting-review-scores-using-neural-networks)","9460c6d9":"### Building the model","e264c29d":"# Case 3. Drug Reviews\n\n* Author: Martin Roznovjak\n* Last edited: 2019-03-21\n* Organization: [Metropolia University of Applied Sciences](https:\/\/www.metropolia.fi\/)","c1e6acad":"### Training","aa822bc9":"### Visualizing predictions\n\n*(slightly lengthy, sorry...)*","143db5dd":"## Conclusion\n\nThis is a really nice dataset to play with, not only for text regression.\n\nIt seems that the vocabulary size (tried between 1000 and 8000)\nand text preprocessing (punctuation\/numbers, stop words, lemmas, indexing\/hashing)\nfor this model are near the sweet spot.\n\nThe model still overfits.\nRegularization of weights turned out to be hard - as it either had barely any effect\nor it rendered the model unable to learn.\nOther hyperparameters can, of course, be still improved, however,\nI doubt it would deliver much improvement over smarter architecture and preprocessing.\n\nTechniques such as bidirectional or multilayer RNN and attention were not fruitful, although,\nI have not put a lot of effort into optimizing them...\n\nNevertheless, considering the general dificulty of understanding language\nand amount of noise in the data (like sarkasm, subjective preferences),\nI think the results are great.","f817c6d6":"## Loading the data\n\nI have been training models mostly in Kaggle and mirrored my local directory structure accordingly.","84c87f0c":"## Results\n\n*Convention: `y_...` denotes the true score, `yx_...` is the predicted*","a1649c68":"## Let's begin!","2e34b437":"**MAE**<br\/>\nMean Absolute Error shows how much off the predictions are from the true rating on average (measured against the original 1 to 10 rating).\n\n**KAPPA**<br\/>\n[Cohen's Kappa](https:\/\/www.statisticshowto.datasciencecentral.com\/cohens-kappa-statistic\/)\nmeasures how much better the classifier is compared to guessing (it considers the rating distribution).\n\n**ACCURACY**<br\/>\nAccuracy gives the plain fraction of the correct predictions.","cdf4d39c":"## Text preprocessing\n\nI was trying out different styles of preprocessing and as expected embeddings turned out to be the most helpful\nand using pretrained embeddings sped up the trainings.\n\nStop words, punctuation and, surprisingly, even numbers were benefiacial when\nbeing included in the vocabulary for this model's performance (both for training and validation).\n\nOn the other hand, dropping out-of-vocabulary words or replacing them with a reserved index\nbrought positive results for some architectures, negative for others,\nin case of the model in this notebook it appears that generalization is slightly better\nwith out-of-vocabulary words being kept and replaced with a common index (0)."}}