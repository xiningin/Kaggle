{"cell_type":{"adf9eb13":"code","470869be":"code","94e2f334":"code","f4c12e1f":"code","a36e1036":"code","80a76e6d":"code","bf537af9":"code","7d6fe8bb":"code","4fb69c0b":"code","6802b6ca":"code","cb3c6276":"code","775131d9":"code","f80e6c8a":"code","9053ea9b":"code","6b7df903":"code","d60fe9a0":"code","f6cf31ff":"code","eaa13218":"code","24ca771f":"code","6b06c84c":"code","7de3ba59":"code","f2b8b219":"code","7fe3c9a1":"code","ce493f10":"code","83c4cfcf":"code","708e348d":"code","9d253cfc":"code","dafa8211":"code","f9389b58":"code","f3fac826":"code","9fc02e8e":"code","270988ab":"code","d479a2ee":"code","aa20d412":"code","0e0ea51d":"code","1817b387":"markdown","f0e31164":"markdown","3be56968":"markdown","d3ca25f9":"markdown","a3e9af21":"markdown","76382e53":"markdown","f696b85d":"markdown","1eae0dd6":"markdown","f9d10b01":"markdown","0b363194":"markdown","474adc97":"markdown","48dcd663":"markdown","72a09693":"markdown","e07be65a":"markdown","104a1af1":"markdown"},"source":{"adf9eb13":"# importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","470869be":"# getting the dataset\ndataset = pd.read_csv('..\/input\/sentiment-analysis-on-financial-tweets\/stockerbot-export1.csv')","94e2f334":"dataset.head()","f4c12e1f":"dataset = dataset.drop('id',axis=1)","a36e1036":"dataset.isnull().sum()","80a76e6d":"# rather than removing the null url values, I just replace them with http:\/\/www.NULL.com\n\ndataset['url'] = dataset['url'].fillna('http:\/\/www.NULL.com')","bf537af9":"plt.figure(figsize=(15,6))\ndataset['source'].value_counts()[:10].plot(kind='barh',color=sns.color_palette('summer',30))\nplt.title('Source with most number of tweets')","7d6fe8bb":"plt.figure(figsize=(15,6))\ndataset['url'].value_counts()[:10].plot(kind='barh',color=sns.color_palette('summer',30))","4fb69c0b":"plt.figure(figsize=(15,6))\ndataset['company_names'].value_counts()[:30].plot(kind='bar',color=sns.color_palette('summer',30))","6802b6ca":"pat1 = r'@[A-Za-z0-9]+' # this is to remove any text with @....\npat2 = r'https?:\/\/[A-Za-z0-9.\/]+'  # this is to remove the urls\ncombined_pat = r'|'.join((pat1, pat2)) \npat3 = r'[^a-zA-Z]' # to remove every other character except a-z & A-Z\ncombined_pat2 = r'|'.join((combined_pat,pat3)) # we combine pat1, pat2 and pat3 to pass it in the cleaning steps","cb3c6276":"len(dataset['text'])","775131d9":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncleaned_tweets = []\n\nfor i in range(0, len(dataset['text'])) :\n    tweets = re.sub(combined_pat2,' ',dataset['text'][i])\n    tweets = tweets.lower()\n    tweets = tweets.split()\n    tweets = [ps.stem(word) for word in tweets if not word in set(stopwords.words('english'))]\n    tweets = ' '.join(tweets)\n    cleaned_tweets.append(tweets)","f80e6c8a":"cleaned_tweets[:10]","9053ea9b":"dataset.columns","6b7df903":"dataset['cleaned_tweets'] = cleaned_tweets","d60fe9a0":"#nltk.download('vader_lexicon')","f6cf31ff":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\nfor tweet in cleaned_tweets[:10]:\n    print(tweet)\n    s = sia.polarity_scores(tweet)\n    for k in sorted(s):\n        print('{0}: {1}, '.format(k, s[k]), end='')\n        print()","eaa13218":"def findpolarity(data):\n    sid = SentimentIntensityAnalyzer()\n    polarity = sid.polarity_scores(data)\n    if(polarity['compound'] >= 0.2):  \n        sentiment = 1\n    if(polarity['compound'] <= -0.2):\n        sentiment = -1 \n    if(polarity['compound'] < 0.2 and polarity['compound'] >-0.2):\n        sentiment = 0     \n    return(sentiment)","24ca771f":"findpolarity(cleaned_tweets[0])","6b06c84c":"sentiment = []\nfor i in range(0, len(cleaned_tweets)):\n    s = findpolarity(cleaned_tweets[i])\n    sentiment.append(s)","7de3ba59":"len(sentiment)","f2b8b219":"len(cleaned_tweets)","7fe3c9a1":"tweet_sentiment = pd.DataFrame()\ntweet_sentiment['cleaned_tweets'] = cleaned_tweets\ntweet_sentiment['sentiment'] = sentiment","ce493f10":"tweet_sentiment.to_csv('tweet_sentiment.csv', index=False)","83c4cfcf":"tweet_sentiment.shape[0]","708e348d":"positive_tweet = []\nnegative_tweet = []\nneutral_tweet = []\n\nfor i in range(0, tweet_sentiment.shape[0]):\n    if tweet_sentiment['sentiment'][i] == 0:\n        neutral_tweet.append(tweet_sentiment['cleaned_tweets'][i])\n    elif tweet_sentiment['sentiment'][i] == 1:\n        positive_tweet.append(tweet_sentiment['cleaned_tweets'][i])\n    elif tweet_sentiment['sentiment'][i] == -1:\n        negative_tweet.append(tweet_sentiment['cleaned_tweets'][i])","9d253cfc":"negative_tweet[:10]","dafa8211":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(positive_tweet)\nshow_wordcloud(neutral_tweet)\nshow_wordcloud(negative_tweet)\n","f9389b58":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(tweet_sentiment['cleaned_tweets']).toarray()\ny = tweet_sentiment['sentiment']","f3fac826":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)","9fc02e8e":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","270988ab":"y_pred = classifier.predict(X_test)","d479a2ee":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nscore = accuracy_score(y_test, y_pred)","aa20d412":"cm","0e0ea51d":"score","1817b387":"**Plotting the top 30 talked about companies in the tweets**","f0e31164":"**Plotting the top 10 url with the most tweets**","3be56968":"**This is an attempt to sentiment analysis on the financial tweets.**\n\nOur goal is :\n1. Clean the text from the file stockerbot-export1.csv \n2. Find the **polarity** for the cleaned text (i.e. **Positive(1)**, **Neutral(0)**, **Negative(-1)**)\n3. Create **Word Cloud** \n4. Creating a **sparse matrix** of all the unique words\n5. Using **Naive Bayes** for classification and prediction","d3ca25f9":"To create word clouds, we will first have to install word cloud (If using Jupyter Notebook)\n\n**pip install wordcloud**","a3e9af21":"**Finding Polarity**\n\nTo find the polarity we use the **SentimentIntensityAnalyzer** from **nltk.sentiment.vader**","76382e53":"Now, based on the 'compound' polarity score and the knowledge of the data, we can choose which tweet falls in the categories of Positive, Negative and Neutral","f696b85d":"**This is the first version of the analysis and there are a lot of possibilities for improvement. I will try to do the best I can in updating this notebook as frequently as possible. **\n\n**Any feedback, support and comments will be highly appreciated. **","1eae0dd6":"For the classification we will use the **Naive Bayes** classifier","f9d10b01":"re.sub() will clean up the text\n\ntweets.lower() - converting text to lowercase\n\ntweets.split() - splits the sentence by each word\n\nps.stem() - converts the words to lowest degree and we also remove all the stopwords from the text (for example: this, that, etc)\n\n' '.join(tweets) - joins back the words to a sentence and separates them with a space","0b363194":"The following code will give us the polarity scores for each of the cleaned tweet \n\nFor example:\n\ncompound: 0.0, \n    neg: 0.0, \n    neu: 1.0, \n    pos: 0.0, ","474adc97":"We create a new dataframe to store the cleaned tweets and their respective polarities and save them to a .csv file","48dcd663":"**Word Cloud**\n\nTo create word clouds of different sentiments, I created three different lists each for positive, negative and neutral tweets","72a09693":"**Plotting the top 10 sources with the most financial tweets**","e07be65a":"****Cleaning the Tweets****\n\nWe will use the NLTK and re libraries to clean up the text","104a1af1":"Now we will use **CountVectorizer** to create a sparse matrix from the cleaned tweets and define the DV and IV for the classification"}}