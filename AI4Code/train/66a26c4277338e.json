{"cell_type":{"defde69d":"code","5b6dd3ad":"code","c41d65da":"code","89b136a4":"code","ba72df54":"code","1e7778f8":"code","0713b8b7":"code","3bfe328d":"code","8aa59284":"code","e13be09b":"code","dca975e7":"code","c5bb2b91":"code","0a84187c":"code","00ee1a63":"code","5f02fca6":"code","35088259":"code","e0b18f96":"code","e297c0c7":"code","0255c2ab":"code","3f5803fc":"code","6085bacd":"code","0958d2c9":"code","af70010e":"code","3d746263":"code","7756c1ff":"code","4c174679":"code","6518201b":"code","97b8c9eb":"code","a4e125ec":"code","5cab5421":"code","e8e48653":"code","3834516e":"code","089b1860":"code","8eec6a0e":"code","ac72d7b9":"code","2fac70a5":"code","226d66db":"code","02a01f0a":"code","8b6f6162":"code","3a018f4c":"code","6e107a30":"code","3bbe8764":"code","2457329a":"code","a474ae31":"code","92c57571":"code","721148c9":"code","e6f08b9e":"code","68332d2e":"code","8d12ba19":"code","4fe073ab":"code","e18d2901":"code","766bdc3d":"code","40ed52f4":"markdown","cceae3ce":"markdown","0889aecd":"markdown","f202dd44":"markdown","8e265152":"markdown","342ab2dc":"markdown","addf01f4":"markdown","eedc85b7":"markdown","51d268b1":"markdown","ae87eab9":"markdown","1b518e00":"markdown","6162b882":"markdown","e043e4d2":"markdown","31cee3e1":"markdown","6973d374":"markdown","e6b28fc0":"markdown","253e0744":"markdown","2f9b4f2b":"markdown","d907f086":"markdown","590e71a9":"markdown","daa50af0":"markdown","f9da091b":"markdown","ca2c56b8":"markdown","d6418db2":"markdown"},"source":{"defde69d":"!pip install beautifulsoup4 stylecloud -q -q","5b6dd3ad":"import re, os, sys\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nimport stylecloud\nfrom pathlib import Path\nfrom IPython.display import HTML, Image, display\nfrom bs4 import BeautifulSoup\nfrom collections import Counter\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport spacy\n\nMK = Path(f'..\/input\/meta-kaggle')\n\nglove_file = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt'\n\nFONT_PATH = '\/usr\/share\/fonts\/truetype\/dejavu\/DejaVuSans.ttf'\n\ndef log_log_plot():\n    plt.yscale('log')\n    plt.xscale('log')\n    plt.ylim(bottom=0.5)\n    plt.xlim(left=0.5)\n\ndef parse_html(r):\n    bs = BeautifulSoup(r, 'html')\n    for block in bs('code'):\n        block.decompose()\n    txt = bs.get_text()\n    txt = re.sub(r'\\[quote.*\\[\/quote\\]', ' ', txt, flags=re.S)\n    txt = re.sub(r'\\s+', ' ', txt)\n    txt = txt.strip()\n    return txt","c41d65da":"# Thanks to Sam Stoltenberg: https:\/\/skelouse.github.io\/styling_a_jupyter_notebook\ndisplay(HTML(\"\"\"<style>.container { max-width:100% !important; }\n.output_result { max-width:100% !important; }\n.output_area { max-width:100% !important; }\n.input_area { max-width:100% !important; }\nh1 {\n  border: 3px solid #333;\n  padding: 8px 12px;\n  background-image: linear-gradient(180deg, rgb(160, 147, 147), #fff);\n  position: static;\n}\n<\/style>\"\"\"))\n\nplt.rcParams[\"figure.figsize\"] = (12, 9)\nplt.rcParams[\"figure.facecolor\"] = \"#FFFFFF\"\nplt.rcParams[\"axes.facecolor\"] = \"#E0E0E0\"\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = \"--\"","89b136a4":"topics = pd.read_csv(MK \/ 'ForumTopics.csv')\ntopics = topics.dropna(subset=['Title'])\ntopics = topics.set_index('Id')\ntopics.shape","ba72df54":"msgs = pd.read_csv(MK \/ 'ForumMessages.csv')\nmsgs = msgs.dropna(subset=['Message'])\nmsgs = msgs.set_index('Id')\nmsgs.shape","1e7778f8":"%%time\ntext1 = ('<html>' + msgs['Message'].str.lower() + '<\/html>').apply(parse_html)\ntext2 = ('<html>' + topics['Title'].str.lower() + '<\/html>').apply(parse_html)\ntext = text1.append(text2)","0713b8b7":"embedding_dict = {}\nwith open(glove_file, 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors","3bfe328d":"words = list(embedding_dict.keys())\nwords[:10]","8aa59284":"len(words)","e13be09b":"word_set = set(words)","dca975e7":"nlp = spacy.load('en', disable=['parser', 'ner', 'tagger'])","c5bb2b91":"counts = Counter()","0a84187c":"%%time\ndocs = nlp.pipe(text, n_threads=2)\nfor doc in docs:\n    counts.update([w.text for w in doc])","00ee1a63":"counts.most_common(10)","5f02fca6":"oom = pd.Series({w:c for w, c in counts.most_common() if w not in word_set})\noom.index.name = 'word'\noom = oom.to_frame('count')\noom = oom.reset_index()\noom.shape","35088259":"df = pd.DataFrame(index=pd.Index(words, name='word'))","e0b18f96":"df['count'] = pd.Series(counts)","e297c0c7":"df.insert(0, 'index', 1 + np.arange(len(df)))","0255c2ab":"df.head()","3f5803fc":"df.describe().T","6085bacd":"df = df.reset_index()","0958d2c9":"df.corr(method='spearman').style.background_gradient()","af70010e":"df['count'].isnull().mean()","3d746263":"df.plot.scatter('index',\n                'count',\n                logy=True,\n                alpha=0.05,\n                title='GloVe word counts within Kaggle Forums');","7756c1ff":"df.plot.scatter('index',\n                'count',\n                alpha=0.3,\n                s=2,\n                c='r',\n                title='GloVe word counts within Kaggle Forums - log\/log')\nlog_log_plot();","4c174679":"df['count'].fillna(0).cumsum().plot(logx=True, title='Cumulative GloVe word counts within Kaggle Forums')\nplt.ylabel('Cumulative Count')\nplt.xlabel('Word Index');","6518201b":"basis = df['index'].values\ntargets = df['count'].fillna(0).values","97b8c9eb":"%%time\nr = IsotonicRegression(increasing=False)\nr.fit(basis, targets)\npreds = r.predict(basis)","a4e125ec":"def set_predictions(p):\n    df['expected'] = p\n    # avoid zeros\n    was_zero = (df['expected'] == 0)\n    min_nonzero = df.loc[~was_zero, 'expected'].min()\n    df.loc[was_zero, 'expected'] = min_nonzero\n    print('Zeros', was_zero.sum(), 'Min', min_nonzero)\n    df['ratio'] = df['count'] \/ df['expected']","5cab5421":"set_predictions(preds)","e8e48653":"df.describe().T","3834516e":"df.plot.scatter('count',\n                'expected',\n                alpha=0.1,\n                title='Word counts vs prediction');\ncoords = np.arange(df['count'].max())\nplt.plot(coords, coords, linestyle='--', linewidth=1, color='k', alpha=0.5)\nlog_log_plot();","089b1860":"df.plot.scatter('index',\n                'count',\n                alpha=0.2,\n                s=2,\n                c='r',\n                title='GloVe word counts within Kaggle Forums - log\/log')\nplt.plot(df['index'], df['expected'])\nlog_log_plot();","8eec6a0e":"N_SHOW = 500\n\n\ndef fmt_exp(x):\n    return ('<mark title=\"#{rank}\\n'\n            'Expected: {expected:.6f}\">'\n            '{word}'\n            '<\/mark>').format(**x)\n\ndef fmt_count(x):\n    return ('<mark title=\"#{rank}\\n'\n            'Count: {count:.0f}\">'\n            '{word}'\n            '<\/mark>').format(**x)\n\ndef fmt_full(x):\n    return ('<mark title=\"#{rank}\\n'\n            'Count: {count:.0f}\\n'\n            'Expected: {expected:.6f}\\n'\n            'Ratio: {ratio:.6f}\" '\n            'id=\"{word}\">'\n            '{word}'\n            '<\/mark>').format(**x)\n\ndef html_report(df, formatter):\n    rank = range(1, len(df) + 1)\n    df = df.assign(rank=rank)\n    return ', '.join(df.apply(formatter, 1))","ac72d7b9":"oom.head()","2fac70a5":"HTML(html_report(oom.head(N_SHOW), fmt_count))","226d66db":"stylecloud.gen_stylecloud(\n    text=oom.head(200).set_index('word')['count'].to_dict(),\n    icon_name='fas fa-comment-alt',\n    background_color='#303030',\n    colors=[\"#2ECB99\", \"#00BFF9\", \"#9A5289\", \"#FF6337\", \"#DFA848\"],\n    size=600,\n    font_path=FONT_PATH,\n    output_name='stylecloud_unknown.png',\n    stopwords=False,\n    random_state=42)\n\nImage('stylecloud_unknown.png')","02a01f0a":"HTML(html_report(oom[oom.word.str.len() == 1], fmt_count))","8b6f6162":"HTML(html_report(oom[oom.word.str.startswith('@')].head(100), fmt_count))","3a018f4c":"top = df.sort_values('ratio', ascending=False)\ntop.head()","6e107a30":"HTML(html_report(top.head(N_SHOW), fmt_full))","3bbe8764":"stylecloud.gen_stylecloud(\n    text=top.head(200).set_index('word')['count'].to_dict(),\n    icon_name='fas fa-comment-alt',\n    background_color='#303030',\n    colors=[\"#2ECB99\", \"#00BFF9\", \"#9A5289\", \"#FF6337\", \"#DFA848\"],\n    size=600,\n    font_path=FONT_PATH,\n    output_name='stylecloud_overrepresented.png',\n    stopwords=False,\n    random_state=42)\n\nImage('stylecloud_overrepresented.png')","2457329a":"top = df.sort_values('ratio', ascending=True)\ntop.head()","a474ae31":"HTML(html_report(top.head(N_SHOW), fmt_full))","92c57571":"top = df.loc[df['count'].isnull()].sort_values('expected', ascending=False)\ntop.head()","721148c9":"HTML(html_report(top.head(N_SHOW), fmt_exp))","e6f08b9e":"top = top.head(1000)\nx = np.vstack([embedding_dict[w] for w in top.word])\nx.shape","68332d2e":"tsne = TSNE(metric='cosine')\nx2 = tsne.fit_transform(x)","8d12ba19":"km = KMeans(n_clusters=20)\nkm.fit(x)","4fe073ab":"pdf = pd.DataFrame(x2, index=top.word)\npdf = pdf.add_prefix('tsne')\npdf['cluster'] = km.labels_\npdf['cluster'] = 'C' + pdf['cluster'].astype(str)\npdf = pdf.reset_index()","e18d2901":"fig = px.scatter(pdf,\n                 title='Unseen Words',\n                 x='tsne0',\n                 y='tsne1',\n                 hover_name='word',\n                 color='cluster')\nfig.update_layout(showlegend=False)","766bdc3d":"df.to_csv('glove_word_forum_counts.csv', index=False)\noom.to_csv('out_of_vocabulary_words.csv', index=False)","40ed52f4":"# Regression\n\nThere are many ways to fit a line through that data, here's just one of them (chosen as it's quick and easy and does not require tuning).\n\nUsing [IsotonicRegression][1] we can train a line from the index to the count value, a line that is non-increasing, so as the index goes up the line always stays the same or goes down to a lower count.\n\n\n [1]: https:\/\/en.wikipedia.org\/wiki\/Isotonic_regression\n","cceae3ce":"# Words That Do Not Appear\n\nThis was the original idea: which words are not there at all?\n\nThe top words are caused by using a different parser to GloVe, perhaps words with punctuation would be better off removed.\n\nI can see definite politics \/ warfare \/ tennis themes.\n\nIt may be that these words have appeared but the post was reported\/deleted.\nIt could also be the case that messages in Meta Kaggle are filtered, from: https:\/\/www.kaggle.com\/kaggle\/meta-kaggle\n - *Please note: This data is not a complete dump of our database. Rows, columns, and tables have been filtered out and transformed.*\n\nSeeing\n<a href=\"#iranians\"><mark>iranians<\/mark><\/a> and\n<a href=\"#cuba\"><mark>cuba<\/mark><\/a>\nin the list jogged my memory about the general competition rules:\n\n<blockquote>\nCOMPETITIONS ARE OPEN TO RESIDENTS OF THE UNITED STATES AND WORLDWIDE, EXCEPT THAT IF YOU ARE A RESIDENT OF CRIMEA, <b>CUBA, IRAN<\/b>, SYRIA, NORTH KOREA, SUDAN, OR ARE SUBJECT TO U.S. EXPORT CONTROLS OR SANCTIONS, YOU MAY NOT ENTER THE COMPETITION.\n\n...\n<\/blockquote>\n","0889aecd":"# Under Represented\n\n\nWords that have appeared but with far less frequency than \"expected\".","f202dd44":"# Read Forums\n\nTopic titles and messages are stored in separate tables.","8e265152":"# Don't Mention It!?\n\n____\n\nDon't mention what? In other words: which words *never* appear on the Kaggle Forums?\n\nTo answer, we need\n1. some external idea of words and how often they *should* appear\n2. to scrape all the Kaggle forums!\n\nFor #1 I will use the [GloVe: Global Vectors for Word Representation](rtatman\/glove-global-vectors-for-word-representation) dataset, and #2 has been done for us!\nKaggle include the HTML for their forums in the [Meta Kaggle](https:\/\/www.kaggle.com\/kaggle\/meta-kaggle) dataset :-)\n\n____\n\n\n### Alternative Titles\n\n- What Are You ***Not*** Talking About? [\\*](https:\/\/www.kaggle.com\/jtrotman\/what-are-you-talking-about)\n- Kagglers Don't Say \u201c\u274b\u274b\u274b\u274b\u274b\u274b\u201d?\n- Best Left Unsaid\n- Words That Do Not Appear\n\n____\n\n## Contents\n\n * [Read Forums](#Read-Forums)\n * [Language Model - GloVe](#Language-Model---GloVe)\n * [Parse Forums - Spacy](#Parse-Forums---Spacy)\n * [Plot Word Counts](#Plot-Word-Counts)\n * [Zipf's Law - Log Rank vs Log Count](#Zipf's-Law---Log-Rank-vs-Log-Count)\n * [Regression](#Regression)\n * [Regression Result](#Regression-Result)\n * [Unknown Words](#Unknown-Words)\n * [The Most @mentioned Users!](#The-Most-@mentioned-Users!)\n * [Over Represented](#Over-Represented)\n * [Under Represented](#Under-Represented)\n * [Words That Do Not Appear](#Words-That-Do-Not-Appear)\n * [Scatter Plot Embeddings](#Scatter-Plot-Embeddings)\n * [Conclusions](#Conclusions)\n * [See Also](#See-Also)","342ab2dc":"# Unknown Words\n\nFirst a table, then the top words: **hover over them to see the appearance count**.\n\nThe most prevalent words in the forum that are not in the GloVe language model. We won't see these in the later sections.\n\nIt's surprising that <mark>kaggle<\/mark> did not make the cut, but the model is from 2014...\nIt's not surprising that <mark>pytorch<\/mark> is not there because it was released in 2016 :)\n\n[1]: https:\/\/en.wikipedia.org\/w\/index.php?title=Kaggle&oldid=427204425","addf01f4":"Check stats...","eedc85b7":"## Word Cloud","51d268b1":"# Regression Result\n\nHere is our line through the data.\nNot a lovely smooth mathematical line but a wiggly approximation - good enough, as an engineer would say :D\n\nWe just want an approximation to the original distribution of the Wikipedia word counts.\nClearly some points are way over and some way under - which is the result of interest.","ae87eab9":"# Plot Word Counts\n\nThe early words with a low index appear much more frequently.","1b518e00":"About 80% of the 400k tokens are yet to appear on Kaggle forums","6162b882":"# Over Represented\n\nFirst a table, then the top words: **hover over them to see the count, the expected, and the ratio**.\n\nWords appearing on Kaggle more often than we'd expect based on the GloVe data.\nDo these words conjure up the 'essence' of Kaggle in *your* mind?\n\nJust some brief highlights...\n\n<a href=\"#quora\"><mark>quora<\/mark><\/a>\nand\n<a href=\"#lyft\"><mark>lyft<\/mark><\/a>\ndeserve to be high up in the list having both run two competitions.\n\nKaggle\/ML legends:\n<a href=\"#giba\"><mark>giba<\/mark><\/a>,\n<a href=\"#srk\"><mark>srk<\/mark><\/a>,\n<a href=\"#tatman\"><mark>tatman<\/mark><\/a>,\n<a href=\"#scirpus\"><mark>scirpus<\/mark><\/a>,\n<a href=\"#triskelion\"><mark>triskelion<\/mark><\/a>,\n<a href=\"#mar\u00edlia\"><mark>mar\u00edlia<\/mark><\/a>,\n<a href=\"#shivam\"><mark>shivam<\/mark><\/a>,\n<a href=\"#serigne\"><mark>serigne<\/mark><\/a> and\n<a href=\"#chollet\"><mark>chollet<\/mark><\/a>\n\n#### Kaggle is a Happy Place #2\n\nThe majority of emoticons are happy ones and the first to appear:\n<mark>:-)<\/mark>\n<mark>:)<\/mark>\n<mark>;)<\/mark>\n<mark>:P<\/mark>\n\nAre ranked higher than the first sad one:\n<mark>:(<\/mark>\n","e043e4d2":"### Let the game commence :)\n\nThere are many words listed.\nWhat insights did I miss out above?\nHmmm.\nIf you comment under this notebook that you are surprised a word does not appear - it will then be on the forums, it will go into [Meta Kaggle](https:\/\/www.kaggle.com\/kaggle\/meta-kaggle) within 24 hours and so ***disappear*** from this list the next time I run the notebook \ud83d\ude02\n\n____\n\nMaybe there are a few words you might think are genuinely underrepresented but worthy topics.\nYou could fork the notebook and show a much bigger list.\nIs there an appetite out there for more tennis stats and discussion?\nPerhaps this will inspire some new datasets?\n\nThe counts are saved below in case you want to dig deeper!","31cee3e1":"# Language Model - GloVe\n____\n\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/\n\nThe words are frequency ordered, most common first.\n\nThe words come from:\n -  <a href=\"http:\/\/dumps.wikimedia.org\/enwiki\/20140102\/\">Wikipedia 2014<\/a> + <a href=\"https:\/\/catalog.ldc.upenn.edu\/LDC2011T07\">Gigaword 5<\/a> (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, &amp; 300d vectors, 822 MB download): <a href=\"http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\">glove.6B.zip<\/a>\n\nWhy use an old model? See the conclusion for details...","6973d374":"Create a DataFrame called `oom` which stands for *out of model*","e6b28fc0":"# The Most @mentioned Users!\n\nThis is quite correlated to the discussion rankings, but note how @cpmp would be #1 or #2 but *many* times people use his display name instead of his user name.\nSimilarly for @heng instead of @hengck23 and others.","253e0744":"# Zipf's Law - Log Rank vs Log Count\n\nIt's best seen on a log-log plot.\nIn log-log space it looks like a linear relationship: this effect is known as [Zipf's law][1].\nWe don't have the original counts of words from the training (Wikipedia) corpus, but we could approximate them.\n\nIf we fit a line to the data shown here, then predictions of *count* (using the line) compared to the real *count* tell you if the word is over or under represented.\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Zipf%27s_law\n","2f9b4f2b":"## Word Cloud","d907f086":"# Scatter Plot Embeddings\n\nWe can use the word embeddings for the words that do not appear: reduce them down to two dimensions with TSNE, then visualise them to see the natural clusters which are essentially topics that have *never* arisen amongst the Kaggle crowd in the 10 years the site has been running.","590e71a9":"# Conclusions\n\nWhy did I use GloVe 6B?\nI tried different options, but with larger vocabulary models on different training texts the ***unseen*** words were nearly all obscenities!\n(You can fork this notebook and try the [glove.840B.300d.txt](https:\/\/www.kaggle.com\/takuok\/glove840b300dtxt) embeddings to see the effect - but ***please do not make it public!***)\n\nSo above are *some* words we are not seeing on the forums, but step back a level: in a very *meta* way, we are not seeing *these* words in this Notebook:\n\n<mark>a&#42;&#42;&#42;&#42;&#42;&#42;s<\/mark>,\n<mark>d&#42;&#42;&#42;&#42;e<\/mark>,\n<mark>f&#42;&#42;&#42;s<\/mark>,\n<mark>t&#42;&#42;s<\/mark>,\n<mark>t&#42;&#42;&#42;&#42;o<\/mark>,\n<mark>y&#42;&#42;&#42;e<\/mark>,\n<mark>w&#42;&#42;&#42;&#42;&#42;t<\/mark>,\n<mark>w&#42;&#42;&#42;t<\/mark>,\n<mark>d&#42;&#42;&#42;s<\/mark>,\n<mark>b&#42;&#42;&#42;&#42;&#42;s<\/mark>,\n<mark>b&#42;&#42;&#42;r<\/mark>,\n<mark>m&#42;&#42;&#42;&#42;&#42;&#42;&#42;&#42;&#42;&#42;r<\/mark>,\n<mark>c&#42;&#42;&#42;&#42;m<\/mark>,\n<mark>w&#42;&#42;&#42;&#42;&#42;s<\/mark>,\n<mark>c&#42;&#42;&#42;&#42;&#42;s<\/mark>,\n<mark>t&#42;&#42;&#42;&#42;&#42;s<\/mark>,\n<mark>o&#42;&#42;&#42;&#42;m<\/mark>,\netc\netc\n\nPerhaps that's the\u00a0main result of interest; Kaggle is overwhelmingly polite, decent and upbeat \ud83d\ude04 \ud83d\ude0a \ud83d\ude03 (if a little vote obsessed)\n\nRemember, our ***expected*** word count comes from Wikipedia in 2014.\nThis is a kind of cross-tabulation of word frequencies between Kaggle and that corpus.\nIt's not like these missing words *should* appear, let's stick to data science and machine learning ;)\n\n<!--\nComing in version 2? plotting the word vectors?\n-->\n\n# See Also\n\n[Don't upvote this, seriously][1] - a forum EDA by @lucabasa that sets out to answer good questions, among them: is there too much pleading for upvotes?\n\n\n\n[1]: https:\/\/www.kaggle.com\/lucabasa\/don-t-upvote-this-seriously\n","daa50af0":"Plot real counts against predictions of count","f9da091b":"Main DataFrame for storing results - count just the words from the language model.\nThe `index` column is the order they appear in the Glove embeddings.","ca2c56b8":"### Single characters league table!\n\n***Kaggle is a Happy Place #1*** : The most frequent emoticons are overwhelmingly cheery!\n\nGold is the rarer medal, bronze the commonest but in # of mentions: \ud83c\udf89>\ud83d\udd25>\ud83c\udfc6>\ud83e\udd47>\ud83e\udd48>\ud83e\udd49>\ud83c\udf69, but nothing beats a \ud83d\udc4d","d6418db2":"# Parse Forums - Spacy\n\nSimply count all words."}}