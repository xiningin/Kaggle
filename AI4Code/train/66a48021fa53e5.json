{"cell_type":{"bcc5ac84":"code","62cedd43":"code","98df4942":"code","94a24e29":"code","e717d5ff":"code","bf752bfd":"code","151a3653":"code","6822e8fc":"code","32a70956":"code","787d959f":"code","59726d7b":"code","aebf608c":"code","52a7c9b1":"code","d6653f71":"code","124b9a46":"code","042d26f0":"code","0ae53e25":"code","1ea4adf5":"code","a90193a2":"code","6f98462f":"code","b1d8c64f":"code","4367c654":"code","be4907a6":"code","bcb96fb4":"code","47264b1b":"code","1c824f12":"code","f0a5a2fc":"code","a0c34ebd":"code","2a8e205a":"code","17364276":"code","4c83dde5":"code","30d48f47":"code","027a28e6":"code","fdfa1afc":"markdown","b21189a3":"markdown","2c20aa8a":"markdown","24850be3":"markdown","9d6b4357":"markdown","6cd28350":"markdown","3e590335":"markdown","81fc70bb":"markdown","968d156f":"markdown"},"source":{"bcc5ac84":"import os \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","62cedd43":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport time\n\nfrom sklearn.svm import SVC\nfrom sklearn import datasets \nfrom sklearn.tree import plot_tree\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold\nfrom sklearn.metrics import (\n    classification_report, \n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_curve, auc, precision_recall_curve\n)","98df4942":"#reading dataset\nX = pd.read_excel(\"\/kaggle\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx\")","94a24e29":"#crating a copy\nx = X.copy()\nprint(x.shape)","e717d5ff":"x.head()","bf752bfd":"x.describe()","151a3653":"#find any categorical feature\ncat_features = list(x.select_dtypes(exclude=['number']).columns)\nprint(cat_features)","6822e8fc":"#unique values from the column\nset(x[\"AGE_PERCENTIL\"].values)","32a70956":"#function to transform the percentil column values to int\ndef agr_perc_to_int(percentil):\n    if percentil == \"Above 90th\":\n        return(100)\n    else:\n        return(int(\"\".join(c for c in str(percentil) if c.isdigit())))","787d959f":"#result of function applied\nx[\"AGE_PERCENTIL\"] = x.AGE_PERCENTIL.apply(lambda x: agr_perc_to_int(x))\nset(x[\"AGE_PERCENTIL\"].values)","59726d7b":"#unique values from the column\nset(x[\"WINDOW\"].values)","aebf608c":"#function to transform the window column values to int\ndef window_to_int(window):\n    if window == \"ABOVE_12\":\n        return(13)\n    else:\n        return(int((window.split(\"-\")[1])))","52a7c9b1":"#result of function applied\nx[\"WINDOW\"] = x.WINDOW.apply(lambda x: window_to_int(x))\nset(x[\"WINDOW\"].values)","d6653f71":"#columns with nan values\nprint(x.isna().sum()[x.isna().sum()>0])","124b9a46":"# lines with nan values\nx[x.isna().any(axis=1)].shape","042d26f0":"#gruping the data by pacient and creatin a boolean column for right ICU value\nx_grouped = x.groupby([\"PATIENT_VISIT_IDENTIFIER\"]).sum()\nx_grouped.rename(columns={'ICU': 'ICUSUM'}, inplace = True)","0ae53e25":"#function to calculate right aICU value\ndef bool_icu(icusum):\n    if icusum > 0:\n        return(1)\n    else:\n        return(0)\nx_grouped[\"ICU\"] = x_grouped.ICUSUM.apply(lambda x: bool_icu(x))","1ea4adf5":"#we dont have more nan\nprint(x_grouped.isna().sum()[x_grouped.isna().sum()>0])","a90193a2":"#defining the targets and excluding them from data\ny = x_grouped.ICU\ny2 = x_grouped.ICUSUM\nx_grouped.drop(['ICU','ICUSUM'], inplace=True, axis=1)","6f98462f":"#we have a balanced dataset\ny.describe()","b1d8c64f":"target_names=[\"0\",\"1\"]\nsc = StandardScaler()\nX_train, X_test, y_train, y_test = train_test_split(x_grouped, y, test_size=0.2)\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\n\ny_pred = rf_clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\nprint(classification_report(y_true=y_test, y_pred=y_pred, target_names=target_names, zero_division=0))","4367c654":"# defining hyperparameters\n\n# trees in the forest\nn_estimators = [int(x) for x in np.linspace(start = 1, stop = 50, num = 10)]\n# features by split\nmax_features = ['auto', 'sqrt']\n# trees level\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# minimum sample division per node\nmin_samples_split = [2, 5, 10]\n# minimum sample per node\nmin_samples_leaf = [1, 2, 4]\n# try bootstrap\nbootstrap = [True, False]\n# creating a random seach grid\ntuned_parameters = {'n_estimators': n_estimators,\n                    'max_features': max_features,\n                    'max_depth': max_depth,\n                    'min_samples_split': min_samples_split,\n                    'min_samples_leaf': min_samples_leaf,\n                    'bootstrap': bootstrap}\nprint(tuned_parameters)","be4907a6":"#setting hyperparameters into model\n\nscores = ['precision', 'recall']\n\nscore = scores[0]\nprint(\"# Ajuste dos hyper-par\u00e2metros (%s)\" % score)\nprint()\n\nrf_clf_cv = RandomizedSearchCV(\n    RandomForestClassifier(), tuned_parameters, cv=5, scoring='%s_macro' % score\n)\nrf_clf_cv.fit(X_train, y_train)","bcb96fb4":"#best hyperparameters\nprint(rf_clf_cv.best_params_)","47264b1b":"#collecting data for plotting roc curve\n\ny_proba = rf_clf_cv.predict_proba(X_test)\ny_test_ohe = OneHotEncoder(sparse=False).fit_transform(y_test.values.reshape(-1, 1))\n\n# calculate roc curve for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(target_names)):\n    fpr[i], tpr[i], _ = roc_curve(y_test_ohe[:, i], y_proba[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# calculate micro-average\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_ohe.ravel(), y_proba.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","1c824f12":"#plotting roc curve\nplt.figure()\nlw = 2\nplt.plot(fpr[1], tpr[1], color='darkorange',\n         lw=lw, label='ROC curve (auc = %0.2f)' % roc_auc[1])\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.show()","f0a5a2fc":"#plotting roc curve and auc  for each class\n#https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\nfrom numpy import interp\nfrom itertools import cycle\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(target_names))]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(len(target_names)):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= len(target_names)\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label=f'micro-average ROC curve (auc = {roc_auc[\"micro\"]:0.2f})',\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label=f'macro-average ROC curve (auc = {roc_auc[\"macro\"]:0.2f})',\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color, name in zip(range(len(target_names)), colors, target_names):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label=f'ROC curve of class {name} (auc = {roc_auc[i]:0.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=\"lower right\")\nplt.show()","a0c34ebd":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nN = 200\nk = 5\n\nskf = StratifiedKFold(n_splits=k, random_state=42)\nthresholds = np.linspace(0, 1, N, endpoint=True)\n\nmetrics = {'accuracy':{}, 'precision':{}, 'recall':{}, 'f1':{}}\nsc = StandardScaler()\nfor train_index, test_index in skf.split(x_grouped, y):\n    X_train, X_test = x_grouped.iloc[train_index], x_grouped.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    rf_clf = RandomForestClassifier()\n    rf_clf.fit(X_train, y_train)\n\n    y_pred = rf_clf.predict_proba(X_test)\n    \n    \n    for threshold in thresholds:\n        metrics['accuracy'].setdefault(threshold,[]).append(accuracy_score(y_test==1, y_pred[:,1]>threshold))\n        metrics['precision'].setdefault(threshold,[]).append(precision_score(y_test==1, y_pred[:,1]>threshold))\n        metrics['recall'].setdefault(threshold,[]).append(recall_score(y_test==1, y_pred[:,1]>threshold))\n        metrics['f1'].setdefault(threshold,[]).append(f1_score(y_test==1, y_pred[:,1]>threshold))","2a8e205a":"#metrics for class \"1\"\nplt.figure(figsize=(13, 13))\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Score\")\nax = plt.gca()\nax.set_xlim(0.0, 1)\nax.set_ylim(0.0, 1.2)\n\nfor metric, color in zip(metrics.keys(), ['k', 'g', 'b', 'r']):\n    metric_df = pd.DataFrame.from_dict(metrics[metric])\n    ax.fill_between(thresholds, metric_df.mean() - metric_df.std(),\n                    metric_df.mean() + metric_df.std(),\n                    alpha=0.1, color=color)\n    ax.plot(thresholds, metric_df.mean(), '-', color=color,\n            alpha=1,\n            label=metric)\n\nplt.legend(loc=\"best\")\nplt.show()","17364276":"\n# LDA\nX_train, X_test, y_train, y_test = train_test_split(x_grouped, y, test_size=0.2)\nlda_clf = LinearDiscriminantAnalysis(n_components=1, store_covariance=True)\nlda_clf.fit(X_train, y_train)\n\ny_pred = lda_clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\nprint(classification_report(y_true=y_test, y_pred=y_pred, target_names=target_names))","4c83dde5":"kf = KFold(n_splits=3, shuffle=True)\nlabels = np.array(target_names)\nn_components = min(x_grouped.shape[1],labels.shape[0])\naccuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor n in range(1,(n_components+1)):\n    acc = []\n    prec=[]\n    rec = []\n    f1_aux=[]\n    for train_index, validation_index in kf.split(x_grouped):\n\n        # Standardize data\n        X_train = x_grouped.iloc[train_index]\n        X_validation = x_grouped.iloc[validation_index]\n\n\n        lda_clf = LinearDiscriminantAnalysis(n_components=1, )\n        lda_clf.fit(X_train, y.iloc[train_index])\n\n        y_validation = lda_clf.predict(X_validation)\n\n        acc.append(accuracy_score(y.iloc[validation_index], y_validation))\n        prec.append(precision_score(y.iloc[validation_index], y_validation, average=None))\n        rec.append(recall_score(y.iloc[validation_index], y_validation, average=None))\n        f1_aux.append(f1_score(y.iloc[validation_index], y_validation, average=None))\n    accuracy.append(acc) \n    precision.append(prec)\n    recall.append(rec)\n    f1.append(f1_aux)","30d48f47":"#accuracy for different number of components\naccuracy = np.array(accuracy)\nmean = accuracy.mean(axis=1)\nstd = accuracy.std(axis=1)\nplt.figure()\nplt.errorbar(range(1,n_components+1), mean, std, marker='^')\nplt.xlabel('# Componets')\nplt.ylabel('Accuracy')\nplt.show()","027a28e6":"# metrics for different number of components\nfig, axs = plt.subplots(1,3, figsize=(21, 6))\nmetrics = [precision, recall, f1]\nmetrics_names = ['Precision', 'Recall', 'F1 Score']\n\nfor i in range(len(metrics)):\n    metric = np.array(metrics[i])\n    mean = metric.mean(axis=1)\n    std = metric.std(axis=1)\n\n    \n    for classes in range(len(labels)):\n            axs[i].errorbar(range(1,n_components+1), \n                        mean[:,classes], \n                        std[:,classes], \n                        label=labels[classes], \n                        marker='^')\n    \n    axs[i].set_xlabel('# Components')\n    axs[i].set_ylabel(metrics_names[i])\n    axs[i].legend()","fdfa1afc":"# LDA","b21189a3":"## Training Random Forest Classifier","2c20aa8a":"## LDA Cross Validation","24850be3":"## Imports","9d6b4357":"## Traning LDA","6cd28350":"## Random Forest Cross Validation","3e590335":"# Conclusion\nThe accuracy for the Random forest Classifier(aprox. 82%) is a little low when compared to the Linear Discriminant Analisys model(aprox 89%). As long as the data is well distributed and has good linear characteristics LDA will have a (depending on the point of view)\"satisfatory\" performance both in predicting who will need ICU and who won't too.\n\n\nPs: it's my first Kaggle's submission, hope you enjoy it and feel free to comment!","81fc70bb":"## Read","968d156f":"## Feature Engineering"}}