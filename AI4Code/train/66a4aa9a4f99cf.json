{"cell_type":{"99f48027":"code","9631901c":"code","7b567d74":"code","30e94643":"code","18961996":"code","ba29f83b":"code","103b763f":"code","5a9fea9a":"code","2cb79d80":"code","12f2fadf":"code","9ce18913":"markdown","b8373740":"markdown","c5367e67":"markdown","ba922f4e":"markdown","8a197827":"markdown","23fe76a6":"markdown","167eff1a":"markdown","839a3c2c":"markdown","c44e9923":"markdown","b7b6ad40":"markdown","699bcb26":"markdown","e463c2c5":"markdown","e9214c78":"markdown","1ede30c8":"markdown","732102a8":"markdown","663a0629":"markdown"},"source":{"99f48027":"import scipy.special as sps\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9631901c":"def exactSolution(beta,n,j):\n    zExact = 0.0;\n    for m in range(0,n):\n        zExact += 2*sps.binom(n-1,m)*np.exp(-beta*j*(n-1-2*m));\n\n    return -np.log(zExact)\/(beta*n);","7b567d74":"print(\"At beta = 0.3, N = 20, J = 1, the free energy pr spin is F =\")\nprint(exactSolution(0.3,20,1))","30e94643":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nfrom random import random as rnd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","18961996":"class MaskedLinear(nn.Linear):\n#This is a very inefficient implementation.\n#We have a fully connected layer which we mask so that some weights\n#are effectively zero.\n    def __init__(self, n, in_channels, out_channels, exclusive, bias=True):\n        super(MaskedLinear,self).__init__(n*in_channels, n*out_channels, bias)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \n        self.exclusive = exclusive\n        self.mask = torch.ones(n,n)\n        if self.exclusive:\n            self.mask = torch.tril(self.mask,-1)\n        else:\n            self.mask = torch.tril(self.mask)\n        self.mask = torch.cat([self.mask] * self.in_channels, dim = 1)\n        self.mask = torch.cat([self.mask] * self.out_channels, dim = 0)\n        \n        self.weight.data *= self.mask\n        self.weight.data *= torch.sqrt(self.mask.numel()\/self.mask.sum())\n        \n        if torch.cuda.is_available():\n            self.mask = self.mask.to(device)\n        \n        self.reset_parameters()\n        \n    def forward(self, x):\n        return F.linear(x, self.mask * self.weight, self.bias)\n\n    def extra_repr(self):\n        return (super(MaskedLinear, self).extra_repr() +\n                ', exclusive={exclusive}'.format(**self.__dict__))","ba29f83b":"class Net(nn.Module):\n    def __init__(self,nspins):\n        super(Net, self).__init__()\n        self.nspins = nspins\n        self.fc1 = MaskedLinear(self.nspins,1,20,True,False)\n        self.fc2 = MaskedLinear(self.nspins,20,20,False,False)\n        self.fc3 = MaskedLinear(self.nspins,20,1,False,False)\n        #Here we have two hidden layer and the output layer.\n        \n    def forward(self,sample):\n        batchsize = sample.size()[0]\n        with torch.no_grad():\n            for i in range(0,self.nspins):\n                x = torch.sigmoid(self.fc1(sample))\n                x = torch.sigmoid(self.fc2(x))\n                x = torch.sigmoid(self.fc3(x))\n                for j in range(0,batchsize):\n                    if x[j,i] >= rnd():\n                        sample[j,i] = 1\n                    else:\n                        sample[j,i] = -1\n        #This is also very inefficient. In lieu of an actual ARNN implementation\n        #in Pytorch we have fully connected layers. Here we actually evaluate\n        #the net nspins more times than we have to!\n        #If this wasn't a demonstration notebook I would change it, please don't\n        #use this for anything important.\n        x = torch.sigmoid(self.fc1(sample))\n        x = torch.sigmoid(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        \n        return x","103b763f":"import torch.optim as optim\n\nclass Jorge():\n#This class will train our network.\n    def __init__(self,jj,beta,nspins,batchsize):\n        self.jj = jj\n        self.beta = beta\n        self.nspins = nspins\n        self.batchsize = batchsize\n        self.net = Net(self.nspins)\n        self.f = torch.zeros(self.batchsize,requires_grad=False)\n        self.logprob = torch.zeros(self.batchsize)\n        self.sample = torch.zeros(self.batchsize,self.nspins)\n        if torch.cuda.is_available():\n            self.net = self.net.to(device)\n            self.f = self.f.to(device)\n            self.logprob = self.logprob.to(device)\n            self.sample = self.sample.to(device)\n        self.optimizer = optim.Adam(self.net.parameters(), lr = 1e-3, betas = (0.9, 0.99), eps = 1e-8)\n    \n    def objectives(self,x):\n        qi = self.sample*(x-1\/2) + 1\/2\n        q = torch.prod(qi,1)\n        self.logprob = torch.log(q)\n        \n        with torch.no_grad():\n            for j in range(0,self.batchsize):\n                self.f[j] = 0\n                for i in range(0,self.nspins-1):\n                    self.f[j] = self.f[j] + self.sample[j,i]*self.sample[j,i+1]\n                self.f[j] = self.f[j]*self.jj*self.beta + self.logprob[j]\n    \n    def step(self):\n        self.optimizer.zero_grad()\n        x = self.net(self.sample)\n        self.objectives(x)\n        self.loss = (self.f - self.f.mean()) * self.logprob\n        self.loss_reinforce = self.loss.mean()\n        self.loss_reinforce.backward()\n        self.optimizer.step()\n    \n    def train(self,epochs,verbose=False):\n        self.f_mean = torch.zeros(epochs,requires_grad=False)\n        self.f_var = torch.zeros(epochs,requires_grad=False)\n        self.loss_mean = torch.zeros(epochs,requires_grad=False)\n        self.loss_var= torch.zeros(epochs,requires_grad=False)\n        \n        epoch = np.linspace(0,epochs-1,epochs,dtype=int)\n        temps = np.linspace(0,self.beta,epochs)\n        for i in epoch:\n            self.step()\n            with torch.no_grad():\n                self.f_mean[i] = self.f.mean()\/(self.nspins*self.beta)\n                self.f_var[i] = self.f.var()\/(self.nspins*self.beta)**2\n                self.loss_mean[i] = self.loss.mean()\n                self.loss_var[i] = self.loss.var()\n                \n            \n        if verbose:\n            plt.errorbar(epoch,self.f_mean.numpy(),np.sqrt(self.f_var.numpy()),None,'k',errorevery = 50)\n            \n            plt.xlabel('Epochs')\n            plt.ylabel('F')\n            \n            plt.title('Machine Convergence')","5a9fea9a":"beta = 0.3\nn = 10\nj = 1\nbatchsize = 1000\n\nprint(\"At beta = 0.3, N = 20, J = 1, the analytical free energy pr spin is F =\")\nprint(exactSolution(beta,n,j))\n\ntrainer = Jorge(j,beta,n,batchsize)\nepochs = 1000\n\ntrainer.train(epochs,verbose=True)","2cb79d80":"beta_min = 1\nbeta_max = 10\nbeta_step = 3\nnpoints = int(1+np.round((beta_max-beta_min)\/beta_step))\nbeta_array = np.linspace(beta_min,beta_max,npoints)\nf_mean_array = np.zeros(npoints)\nf_std_array = np.zeros(npoints)\nloss_mean_array = np.zeros(npoints)\nloss_std_array = np.zeros(npoints)\nf_exact = np.zeros(npoints)\n\nn = 20\nj = 1\nbatchsize = 1000\nepochs = 2000\n\nfor i in range(0,npoints):\n    beta = beta_array[i]\n    trainer = Jorge(j,beta,n,batchsize)\n    trainer.train(epochs,verbose=False)\n    f_exact[i] = exactSolution(beta,n,j)\n    f_mean_array[i] = trainer.f_mean[-1]\n    f_std_array[i] = np.sqrt(trainer.f_var[-1])\n    loss_mean_array[i] = trainer.loss_mean[-1]\n    loss_std_array[i] = np.sqrt(trainer.loss_var[-1])\n","12f2fadf":"plt.figure(figsize=(9,9))\n\nplt.subplot(311)\nplt.errorbar(beta_array,f_mean_array,f_std_array,None,'k^')\nplt.errorbar(beta_array,f_exact,None,None,'k')\nplt.xlabel('beta')\nplt.ylabel('F')\nplt.legend(['ARNN','Exact'])\nplt.subplot(312)\nplt.errorbar(beta_array,loss_mean_array,loss_std_array,None,'k^')\nplt.xlabel('beta')\nplt.ylabel('loss')\nplt.subplot(313)\nplt.semilogy(beta_array,np.abs(f_mean_array-f_exact),'k^')\nplt.xlabel('beta')\nplt.ylabel('error')\n\nplt.show()","9ce18913":"$$ \n\\begin{bmatrix}\n\\hat{s}_1 \\\\\n\\hat{s}_2 \\\\\n\\vdots \\\\\n\\hat{s}_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\sigma ( b_1 ) \\\\\n\\sigma ( w_{2,1} s_1 + b_2) \\\\\n\\vdots \\\\\n\\sigma ( \\sum_i w_{n,i} s_i + b_n)\n\\end{bmatrix}\n$$","b8373740":"The Ising model is one of the most important models in physics. It is simple, but sufficient to demonstrate ubiquitous phenomena in statistical mechanics. We formulate it in the following way for the one-dimensional Ising spin chain.\n\nSuppose you have $n$ spins spaced equally apart each with a magnetic moment $s_i = \\pm 1$. The interaction energy of a pair of neighboring spins is $E_i = J s_i s_{i+1}$. Here $J$ is a constant term which gives the interaction energy. The total energy of the system is then\n$$\nH = \\sum_{i=1}^{n-1} J s_i s_{i+1}.\n$$\n\n\n","c5367e67":"In the above analytical calculation we used the Boltzman distribution to describe the system at equilibrium. Thus we were able to calculate the free energy exactly.\n\nHowever, in most situations that are relevant to contemporary research we cannot easily calculate $p(\\mathbf{s})$, let alone $Z_\\beta$. The only reason it works in the Ising model is because the number of distinct energy states grows linearly with the system size, so we can make the kinds of simplifications that make analytical calculations possible. In most cases of interest we do not know the analytical solutions.\n\nHowever, if we can just generate the right probability distributions $p(\\mathbf{s})$, then we can always calculate the free energy functional using those. It doesn't matter how we calculate them either as long as we can minimize the free energy we will be fine.\n\nWe will here use the following very general approach. We can decompose the probabilities as\n$$\np(s_n,s_{n-1},\\ldots,s_1)\\\\\n=p(s_n|s_{n-1},s_{n-2},\\ldots,s_1) p(s_{n-1},\\ldots,s_1)\\\\\n=\\ldots\\\\\n=p(s_n|s_{n-1},s_{n-2},\\ldots,s_1) p(s_{n-1}|s_{n-2},\\ldots,s_1)\\ldots p(s_1).\n$$\nWhat the above equation shows you is that the probabilitiy function can be decomposed as a product of conditional probabilities. This now gives us an approximate way to evaluate quantities like the free energy that does not require us to evaluate the probability of every single spin configuration.\n\nInstead we can sample a number of spin configurations and use those to take expectation values. We start by sampling $s_1$ using $p(s_1)$. When this is done we can sample $s_2$ using $p(s_2|s_1)$ and so on. This approach is able to express correlation very well since the conditional probabilities contain enough information about how spins correlate even at long ranges.\n\nThis sampling procedure can be continued until a sufficient number of spin configurations have been generated and the probability of each of them has been evaluated. This allows a calculation of the free energy and other system quantities.\n\nAll we need now is the right machinery to generate good probability functions $q_\\theta(s_i|s_{i-1},\\ldots)$ based on variational parameters $\\theta$, and a way to optimize them to minimize the free energy.","ba922f4e":"# Statistical mechanics with ARNN","8a197827":"### Solving the same model with machine learning","23fe76a6":"The Ising model in one dimension is pretty easy to solve. I might add the analytical solution to the bottom of this notebook later, but here I will just make some notes.\n\nIf you look at the expresseion for the energy of a single pair of spins, i.e. $E_i$, you may notice something. Since the spins $s_i$ can only take the values $\\pm 1$, if I reverse the sign of each spin, the energy is unchanged. It turns out that this is true for the total energy $H$ as well. This is a kind of symmetry of our system, and it's called $\\mathcal{z}_2$ symmetry. The implication of this symmetry is that every state of the system has a partner with the same energy, which can be found by reversing the sign of each spin.\n\nThe lowest energy state of the system therefore has two solutions. If $J$ is negative they will be $s_i = 1$ for all spins or $s_i = -1$ for all spins. Higher energy levels are spaced apart in steps of 2 J and each energy level has a degeneracy that can be expressed as a binomial coefficient doubled due to $\\mathcal{z}_2$ symmetry. We can quantize the energy in terms of some number $m$ and write $H_m = -(N-1) J + 2 J m$.\n\nWe can now calculate the analytical free energy. At equilibrium the system follows the Boltzman distribution with\n$$\np_\\beta(\\mathbf{s}) = \\frac{e^{-\\beta H(\\mathbf{s})}}{Z_\\beta}.\n$$\nHere $\\beta = 1\/k_b T$ and\n$$\nZ_\\beta = \\sum_{\\mathbf{s}} e^{-\\beta H(\\mathbf{s})}\\\\\n        = \\sum_{m=0}^{N-1} 2 \\binom{m}{N-1} e^{-\\beta H_m}.\n$$\nTo get to the last equation we have simply taken the sum over the energy levels instead of each possible spin-configuration. We can do this since we have a simple expression for the degeneracy.\n\nThe free energy is\n$$\nF_\\beta = -\\beta^{-1} \\ln Z_\\beta \n$$\nLet's code it up.","167eff1a":"with $\\sigma(x) = \\frac{e^{x}}{e^{x}+1}$ the sigmoid function. For the moment let's appreciate the above equation just so that we can get an intuition for what our machine can do without including hidden layers or anything else.\n\nLet's say we train our machine in such a way that all the weights $w_{i,j} = 0$ and all the biases $0 \\neq b_{i} = b_{j}$ $\\forall i,j$. Well, in that case, we lose all correlations and are back to the naive mean-field theory. This is what we should expect to happen if we apply a strong magnetic field. We would have to add a term to the model that breaks $\\mathcal{Z}_2$ symmetry.\n\nA machine like that can only express one type of order though, it cannot express a probability function that is symmetric and therefore cannot express spontaneous order.\n\nOn the other hand, we can express correlation using the weights $w_{i,j}$. Since each $\\hat{s}_i$ is the variational probability that $s_i=+1$, then if $w_{i,j}$ is positive then spins $s_i$ and $s_j$ have a higher likelyhood of being the same value, i.e. they are correlated. If the weights are negative they are likewise anti-correlated. Thus, near the critical point we should expect the weights to play a significant role in expressing the correct probability distribution.\n\nSince we are using a model that does not include an applied field, we will switch off the biases $b_i$ for now.\n\nWe will of course use hidden layers too, as the probability functions we can express without them are severely limited.","839a3c2c":"We utilze Adam to train our network. This is a stochastic gradient descent method, which means that it uses the gradient of the objective function (in this case the free energy) with respect to the weights evaluated at some sample that is generated by our network. We therefore need to know the derivative of our probability distribution. Luckily this is an easy calculation to do since we're using sigmoids. \n$$\n\\sigma(x)' = \\frac{ e^x}{(e^x+1)^2}.\n$$\nThe free energy is\n$$\n F_\\theta = \\frac{1}{\\beta} \\mathcal{E}_{\\mathbf{s} \\leftarrow q_\\theta}\\{\\beta E(\\mathbf{s})+\\ln  q_{\\theta}(\\mathbf{s})\\}.\n$$\nThe above expectation value is taken by generating samples $\\mathbf{s}$ using the probability distribution $q_{\\theta}(\\mathbf{s})$. We use the following stochastic gradient\n$$\n\\nabla_\\theta F_\\theta = \\frac{1}{\\beta} \\mathcal{E}_{\\mathbf{s} \\leftarrow q_\\theta}\\{[\\beta E(\\mathbf{s})+\\ln  q_{\\theta}(\\mathbf{s})]\\nabla_\\theta \\ln  q_{\\theta}(\\mathbf{s})\\}.\n$$\nThe above gradient is evaluated by taking samples of $\\mathbf{s}$ using the $q_{\\theta}(\\mathbf{s})$, just like we do for the free energy. Now the procedure is the following. We write up a vector that contains all our weights and biases, and at each step in the training scheme we update this vector by going in the direction of the negative gradient in $\\theta$ space. Thus we have the following update rule\n$$\n\\theta^{(i)} = \\theta^{(i-1)} - c^{(i)} \\nabla F_\\theta.\n$$\nHere $c^{(i)}$ is a step-size modifier that is updated at each iteration. The implementation of $c^{(i)}$ is what sets Adam apart from other popular stochastic gradient methods. Here we use the second moment of the gradient to better estimate the correct stepsize.\n\nBefore we are completely done, we have to change our cost function slightly. It is perfectly fine to use $F_\\theta$, but during convergence Adam tries to minimize the second moment of the gradient as well. In order for this to go to zero, we have to subtract the mean from $F_\\theta$, which we do simply by evaluating the baseline\n$$\nb = \\mathcal{E}_{\\mathbf{s} \\leftarrow q_\\theta} \\beta \\{E(\\mathbf{s})+\\ln  q_{\\theta}(\\mathbf{s})\\}.\n$$\nWe then have the gradient\n$$\n\\nabla_\\theta F_\\theta = \\frac{1}{\\beta} \\mathcal{E}_{\\mathbf{s} \\leftarrow q_\\theta}\\{[\\beta E(\\mathbf{s})+\\ln  q_{\\theta}(\\mathbf{s}) - b]\\nabla_\\theta \\ln  q_{\\theta}(\\mathbf{s})\\}.\n$$\n\nThere is enough literature on Adam that I wont write here how it works in detail. Both pytorch and tensorflow implement it as an optimization procedure, and I believe this captures the points we have to be mindful of.\n\nPytorch automatically calculates the gradients for us using the built-in .backward() method of the tensor class. Thus we avoid having to manually do backwards propagation (calculating gradients by going backwards in the network).","c44e9923":"Let's now try to see what happens when we plot the solution found by our machine after training it, and compare with the analytical solution.","b7b6ad40":"### Solving the Ising model in 1D analytically for a finite number of spins","699bcb26":"For this we use an autoregressive neural network (ARNN) [1]. These networks generate outputs which they then use successively to correct future outputs. They are therefore well-suited for the task we have in mind.\n\nWe define $\\hat{s}_i$ as the probability that $s_i = 1$, which we train our network to calculate. We will then construct a network which calculates each $\\hat{s}_i$ based on the previously sampled $s_{i-1},s_{i-2},\\ldots,s_{1},$. Then we can calculate each $q_\\theta (s_i|s_{i-1,\\ldots}) = (s_i(\\hat{s}_i - 1\/2) + 1\/2)$. The $\\theta$ are in this case the weights and biases of our model. In the simplest direct case what we calculate is the following.","e463c2c5":"In physics there is a direct relationship between how matter behaves at an everyday level, where we can measure things like heat capacity, work and pressure, and the microscopic level where we account for the interaction between particles and their energy levels. In order to derive the macroscopic properties of matter from interactions at a microscopic level, we have to count the number of energy levels of the system and assign a probability to each of them.\n\nAs you might imagine, counting energy levels of systems consisting of an avogadro's number of atoms (about $6\\times10^{23}$) can be a very difficult task. Luckily, this is the problem that statistical mechanics was invented to solve.\n\nIn statistical mechanics we describe a state $ \\mathbf{x} $ in terms of the probability of this state occuring in that particular system. This $\\mathbf{x}$ could be a vector containing the velocities of each gas molecule of an ideal gas, it could be the spacing of individual atoms relative to some rest position inside a metal or as in this notebook the magnetic moments of a group of particles arranged in a one-dimensional chain. \n\nIf we are able to calculate the probability $p(\\mathbf{x})$ of each possible state then we can calculate every macroscopic quantity associated with that material. That's really neat, but the problem is of course that this is hard to do in practice due to the sheer enormity of available states. In a very simple example we can use $\\mathbf{x} = [s_1,s_2,\\ldots,s_n]$ to describe the magnetic moment, or the spin $s_i$ of each atom in a one-dimensional chain. The very simplest model describing this, the Ising model, assigns either $+1$ or $-1$ to each spin, thus giving a number of states $2^n$ for $n$ spins.\n\nEven for this simple example, it seems that we have to know $2^n$ different probabilities in the most general case (although in practice the problem is symmetric enough that we can make simplifications). Physicists tend to deal with intractable situations like this by introducing approximations. In this case one of the simplest is\n$$\np(s_1,s_2,\\ldots,s_n) \\approx \\prod_i p(s_i).\n$$\nHere each spin is assigned a static probability of being in either state $+1$ or $-1$ independent of the other spins in the chain. Thus we have reduced the complexity of the calculation since we now have to consider only $n$ probabilities instead of $2^n$. But we lose all information about the interactions the spins have with each other, since the probabilities are completely independent. In other words, we are not able to express \\emph{correlations} between spins in any way.\n\nIn reality there are certain phases of matter that are highly correlated, and these are an important topic of modern research in solid state and condensed matter physics. But how can we hope to go beyond this naive approximation without making the problem intractable?\n\nIn this notebook we will see how autoregressive neural networks allow us to express correlation while still enabling efficient computation of system variables [1]. We will also go through how to train and optimize these networks by minimizing a free energy functional using stochastic gradient descent and the Adam algorithm [2].\n\nTo read the original paper on this method please see [1] and to read the original paper on the Adam algorithm, please see [2]. The results presented in this notebook are meant as a demonstration of both of these original works.","e9214c78":"This seems to work remarkably well. Let's test it for a range of parameters.","1ede30c8":"We have plotted the solutions for various values of $\\beta$ and $n=10$, $j=1$. In each case 1000 batches were used and training took 1000 epochs.\n\nThe top figure shows the free energy found by the ARNN alongside the analytical solution. The middle figure shows the value of the loss-function which was used to train the network. The bottom figure shows the calculated error, i.e. the difference between the exact free energy and the one approximated by the network.\n\nAt high temperatures this shallow network seems to work quite well. Perhaps deeper networks are needed at lower temperatures where interactions are important in order to properly express the conditional probabilities.","732102a8":"[1] Wu, D., Wang, L., & Zhang, P. (2019). Solving statistical mechanics using variational autoregressive networks. Physical review letters, 122(8), 080602.\n\n[2] Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.","663a0629":"## Solving the Ising model and beyond"}}