{"cell_type":{"099e91e1":"code","f1fc7a17":"code","4fe2f909":"code","5dbeff8f":"code","7577327a":"code","ec1b28f0":"code","2221fb6a":"code","fa8f21e7":"code","d8e95c8c":"code","ea354855":"code","86e69711":"code","39695d1d":"code","ec9fb606":"code","b1075773":"code","5a627411":"code","9fec937d":"code","b87b87de":"markdown","272a6baf":"markdown","4dd81b24":"markdown","45e9a81a":"markdown","afe02004":"markdown"},"source":{"099e91e1":"import numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport catboost as cb\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix","f1fc7a17":"%%time\ntrain = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use","4fe2f909":"#\u041d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0441\u0434\u0435\u043b\u043a\u0438 \u0441 \u043d\u0443\u043b\u0435\u0432\u044b\u043c \u0432\u0435\u0441\u043e\u043c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u0438\u0445 \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u0443\u0435\u043c\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain.shape","5dbeff8f":"#\u0414\u0430\u043d\u043d\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u0441 86 \u0434\u043d\u044f\ntrain = train.query('date > 85').reset_index(drop = True)\ntrain.shape","7577327a":"#\u0417\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0438\u043c \ntrain.fillna(train.mean(),inplace=True)","ec1b28f0":"#\u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f 0 \u0438\u043b\u0438 1 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 resp \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0438\u0445 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0435 'action'\ntrain['action'] = (train['resp'] > 0 ).astype('int')","2221fb6a":"resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']","fa8f21e7":"features_train_data  = train.iloc[:,7:137]","d8e95c8c":"# \u041d\u0430\u0439\u0434\u0435\u043c \u043f\u0430\u0440\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0435\u0439 > |0.9|\ndef corrFilter(x: pd.DataFrame, bound: float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\n\nhigh_correlations=corrFilter(features_train_data, .9).to_frame()","ea354855":"all_drop_cols = set(high_correlations.index.get_level_values(0))","86e69711":"features = features_train_data.columns.tolist()","39695d1d":"# for i in all_drop_cols:\n#     features.remove(i)","ec9fb606":"f_mean = np.mean(train[features].values,axis=0)","b1075773":"X = train.loc[:, features].values\ny = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T","5a627411":"models = [] # \u0441\u043f\u0438\u0441\u043e\u043a \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c\n\nfor i in tqdm(range(y.shape[1])):\n    x_tr,x_val,y_tr,y_val = train_test_split(X ,y[:,i],test_size=0.2,stratify=y[:,i])\n    model = CatBoostClassifier(iterations = 5000,\n                          depth=10,\n                          learning_rate = 0.1,\n                          random_seed = 42,\n                          eval_metric='Accuracy',\n                          custom_metric=['Logloss', 'AUC'],\n                          od_wait=500,\n                          task_type='GPU',\n                         )\n    model.fit(x_tr, y_tr,\n         eval_set=(x_val, y_val),\n         verbose_eval=100,\n         use_best_model=True,\n         #plot=True\n         )\n    \n    nom_fich = \"weights_target_\" + resp_cols[i]\n    \n    model.save_model(nom_fich)\n    \n    models.append(model)","9fec937d":"f = np.median\nth = 0.5000\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt.sum()):\n            x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean\n        \n        pred = f(np.stack([model.predict(x_tt) for model in models]),axis=0).T\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","b87b87de":"# Importing Data \ud83d\udcda\n\n","272a6baf":"# Creating Train and Test DataFrame ","4dd81b24":"# Submission","45e9a81a":"# Import Libraries \ud83d\udcc2","afe02004":"# Preparing Data"}}