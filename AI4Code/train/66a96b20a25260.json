{"cell_type":{"22f55e25":"code","16258764":"code","2273fb56":"code","a46514b4":"code","807be8eb":"code","1cf426c9":"code","050d9c8f":"code","a5697bd3":"code","52caabc9":"code","e9bb31c0":"code","c2ef3e89":"code","db3a4cc5":"code","e0af32b4":"code","03a6c91b":"code","ef2d3f72":"code","b8fb8522":"code","72d4748e":"code","1b586146":"code","3ee0ea00":"code","aa6fb4cf":"code","483fd775":"code","0e94a8a8":"code","3dd36278":"code","8b96949b":"code","17558071":"code","774f6534":"code","b3fa854a":"code","e8fa2ffb":"code","5a4e6510":"code","b6c6f55a":"code","a767a88f":"code","c0715868":"code","633527f0":"code","b40a2cc4":"code","ec9bbcf8":"code","3bb84707":"code","0c4494f5":"code","0c932e30":"code","cae7e885":"code","d827c09f":"code","33153a92":"code","42b657da":"code","9772b770":"code","cddf669c":"code","0ba433e3":"code","41ffd095":"code","ff28e081":"code","917f3192":"code","93723cfb":"code","3dae9175":"code","a2b680d1":"code","f0cf5f40":"code","249a38d3":"code","cd2888cf":"code","2f913b7b":"code","de1fa841":"code","f1fb826c":"code","0aadd46d":"code","c8c42249":"code","fadd676d":"code","aa252537":"code","6d4a0484":"code","433c882c":"code","7cb8dd77":"code","6789b18b":"code","1782e1f3":"code","9185dac7":"code","23e7a831":"code","b733eb5f":"code","8d698028":"code","cdbd3fd5":"code","bf62f67c":"code","27f81640":"code","5dec4dde":"markdown","56877d58":"markdown","f896c1e2":"markdown","2ca82958":"markdown","f0b67a97":"markdown"},"source":{"22f55e25":"# Import the libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport re","16258764":"# Loading the Datasets:\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntrain_data.head()","2273fb56":"# Let's first check if we have NA values:\ntrain_data.isnull().sum()","a46514b4":"# Let's explore the age:\nsns.heatmap(train_data[[\"Age\"]].isnull(), cmap = sns.color_palette(\"crest\"))","807be8eb":"# Let's think of how to fill out the NAs in Age:\nsns.displot(data = train_data, col = \"Pclass\", row = \"Sex\", x = \"Age\")\n# We will use the mean of ages grouped by sex and Pclass.","1cf426c9":"# Let's check the cabin column:\ntrain_data.Cabin.dropna().head()","050d9c8f":"sns.heatmap(data = train_data[[\"Cabin\"]].isnull(), cmap = sns.color_palette(\"flare\"))","a5697bd3":"# Let's see the unique values of cabin\ntrain_data.Cabin.dropna().unique()","52caabc9":"# Let's remove the numbers from the cabin and let's only use the alphabets:\npattern  = r'[0-9]*'\nregex = re.compile(pattern)\ntempList = []\nfor cabin in train_data.Cabin.dropna():\n    tempList.append(regex.sub(\"\",cabin))\ntempList[:10]","e9bb31c0":"# It looks like the data is not so clear since there are cabins with several alphabets. Let's simplify the data:\nfor idx in range(len(tempList)):\n    tempList[idx] = tempList[idx][0]\ntempList[:10]","c2ef3e89":"# Let's assign this to the main data:\ntrain_data.loc[train_data.Cabin.notnull(), \"Cabin\"] = pd.Series(tempList)","db3a4cc5":"# Let's check if it changed:\ntrain_data.Cabin.unique()","e0af32b4":"# Let's change the nan to str so that we can plot:\nfor cabin in train_data.Cabin:\n    if cabin == np.nan:\n        cabin = \"NA\"","03a6c91b":"# Let's see how the cabins are distributed across Pclass:\nsns.catplot(data = train_data, x = \"Cabin\", kind = \"count\", col = \"Pclass\")\n# Let's just discard this column","ef2d3f72":"# Let's see if our data is balanced:\ntrain_data.groupby(\"Survived\").size().plot.bar()","b8fb8522":"# Let's fill out the missing values in Age:\ntrain_data.groupby([\"Sex\",\"Pclass\"], as_index=False).mean()","72d4748e":"# Let's fill out the missing values in Age:\ntempDict = {idx:val for idx,val in enumerate(train_data.groupby([\"Sex\",\"Pclass\"], as_index=False).mean().Age)}\ntempDict","1b586146":"# Let's give the dictionary better names:\nnewKeys = [\"F1\", \"F2\", \"F3\", \"M1\", \"M2\", \"M3\"]\nfor idx in range(len(newKeys)):\n    tempDict[newKeys[idx]] = tempDict.pop(idx) #This should be run once.","3ee0ea00":"tempDict","aa6fb4cf":"train_data[train_data.Age.isnull()]","483fd775":"# Function to impute the missing values from the dictionary:\ndef impute(df, dictionary=tempDict):\n    ageCol = df[\"Age\"]\n    pclassCol = df[\"Pclass\"]\n    sexCol = df[\"Sex\"]\n    \n    if pd.isnull(ageCol):\n        if pclassCol == 1 and sexCol == \"female\":\n            return dictionary[\"F1\"]\n        elif pclassCol == 2 and sexCol == \"female\":\n            return dictionary[\"F2\"]\n        elif pclassCol == 3 and sexCol == \"female\":\n            return dictionary[\"F3\"]\n        elif pclassCol == 1 and sexCol == \"male\":\n            return dictionary[\"M1\"]\n        elif pclassCol == 2 and sexCol == \"male\":\n            return dictionary[\"M2\"]\n        elif pclassCol == 3 and sexCol == \"male\":\n            return dictionary[\"M3\"]\n    else:\n        return(ageCol)","0e94a8a8":"# Apply the function:\ntrain_data[\"Age\"] = train_data.apply(impute, axis =1)\ntest_data[\"Age\"] = test_data.apply(impute, axis =1)","3dd36278":"# Let's check:\nfig, ax = plt.subplots(1,2)\nsns.heatmap(train_data[[\"Age\"]].isnull(), cmap = sns.color_palette(\"crest\"), ax = ax[0])\nsns.heatmap(test_data[[\"Age\"]].isnull(), cmap = sns.color_palette(\"crest\"), ax = ax[1])","8b96949b":"# We will discard the columns: Cabin, Ticket\ntrain_data.drop([\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntest_data.drop([\"Cabin\", \"Ticket\"], axis = 1, inplace = True)","17558071":"# Let's dummify the Embarked column\ntrain_Embarked_Dummy = pd.get_dummies(train_data.Embarked, drop_first = True)\ntrain_Embarked_Dummy","774f6534":"# Let's merge this with the main dataframe:\ntrain_data_pre = pd.concat([train_data,train_Embarked_Dummy], axis = 1)\ntrain_data_pre.drop(\"Embarked\", axis =1, inplace = True) # Removing the Embarked column\ntrain_data_pre","b3fa854a":"# Apply the same thing to test data\ntest_Embarked_Dummy = pd.get_dummies(test_data.Embarked, drop_first = True)\ntest_data_pre = pd.concat([test_data,test_Embarked_Dummy], axis = 1)\ntest_data_pre.drop(\"Embarked\", axis =1, inplace = True) # Removing the Embarked column\ntest_data_pre","e8fa2ffb":"# Let's dummify the Sex column too:\ntrain_Sex_Dummy = pd.get_dummies(train_data.Sex, drop_first= True)\ntrain_Sex_Dummy","5a4e6510":"# Merge it with the main preprocessed data set\ntrain_data_pre = pd.concat([train_data_pre, train_Sex_Dummy], axis = 1)\ntrain_data_pre.drop(\"Sex\", axis = 1, inplace = True) # Drop the Sex column","b6c6f55a":"# Apply the same steps for the test data:\ntest_Sex_Dummy = pd.get_dummies(test_data.Sex, drop_first= True)\ntest_data_pre = pd.concat([test_data_pre, test_Sex_Dummy], axis = 1)\ntest_data_pre.drop(\"Sex\", axis = 1, inplace = True) # Drop the Sex column","a767a88f":"test_data_pre","c0715868":"# It looks like we have a missing value for Fare column in the test dataset:\nsns.heatmap(test_data_pre[[\"Fare\"]].isnull(), cmap = sns.color_palette(\"crest\"))","633527f0":"# Let's look at the statistics of the Fare column in test dataset:\ntest_data_pre.describe()","b40a2cc4":"# Let's fill the missing value with the test_data's mean value of Fare:\ntest_data_pre[\"Fare\"].fillna(35.627188, inplace = True)","ec9bbcf8":"# Check:\nsns.heatmap(test_data_pre[[\"Fare\"]].isnull(), cmap = sns.color_palette(\"crest\"))","3bb84707":"# We need to scale the Fare column and Age Column:\n#def Scale_min_max(data, col):\n#    return (data[col]-data[col].min())\/(data[col].max() - data[col].min())","0c4494f5":"def normalizer(data, col):\n    return (data[col]-data[col].mean())\/(data[col].std())","0c932e30":"train_data_pre.Fare = normalizer(train_data_pre, \"Fare\")\ntrain_data_pre.Age = normalizer(train_data_pre, \"Age\")","cae7e885":"train_data_pre","d827c09f":"# Let's scale the test data too:\ntest_data_pre.Fare = normalizer(test_data_pre, \"Fare\")\ntest_data_pre.Age = normalizer(test_data_pre, \"Age\")","33153a92":"# Let's dummify the Pclass:\ntrain_Pclass_dummy = pd.get_dummies(train_data.Pclass, drop_first=True)\ntrain_Pclass_dummy","42b657da":"# Let's add this to the train_data_pre dataframe:\ntrain_data_pre = pd.concat([train_data_pre, train_Pclass_dummy], axis = 1)","9772b770":"#Let's now remove the columns that we don't want:\ntrain_data_pre.drop([\"PassengerId\", \"Pclass\"], axis = 1, inplace= True)","cddf669c":"train_data_pre","0ba433e3":"# Let's apply the same steps to the test_data_pre\ntest_Pclass_dummy = pd.get_dummies(test_data.Pclass, drop_first=True)\ntest_data_pre = pd.concat([test_data_pre, test_Pclass_dummy], axis = 1)\ntest_data_pre.drop([\"PassengerId\", \"Pclass\"], axis = 1, inplace= True)","41ffd095":"# Let's extract some important patterns\/titles from the Name:\npattern = r' ([a-z]+)\\.'\nregex = re.compile(pattern, flags=re.IGNORECASE)","ff28e081":"# Let's append this to the dataframe:\ntrain_data_pre[\"Titles\"] = train_data_pre.Name.str.extract(regex, expand = False)\ntrain_data_pre.Titles","917f3192":"# Let's see how the titles are related in terms of Gender\npd.crosstab(train_data_pre[\"Titles\"], train_data.Sex)","93723cfb":"pd.crosstab(train_data_pre[\"Titles\"], train_data.Sex).plot.bar()","3dae9175":"# Let's have a look at the title and the survived\npd.crosstab(train_data_pre[\"Titles\"], train_data.Survived)","a2b680d1":"pd.crosstab(train_data_pre[\"Titles\"], train_data.Survived).plot.bar(stacked=True)","f0cf5f40":"# Let's simplify the categories of the title:\ntempDict = {\"Capt\" : \"Mr\", \"Col\" : \"Mr\", \"Countess\" : \"Miss\", \"Don\" : \"Mr\", \"Jonkheer\" : \"Mr\", \"Lady\" : \"Miss\", \"Major\" : \"Mr\", \"Mlle\" : \"Miss\", \"Mme\" : \"Miss\", \"Ms\" : \"Miss\", \"Sir\" : \"Mr\", \"Rev\" : \"Mr\"}","249a38d3":"# Let's also check if the test_data has some new titles:\ntest_data_pre.Name.str.extract(regex, expand=False).unique()","cd2888cf":"# Let's append Dona to the Dictionary\ntempDict[\"Dona\"] = \"Mrs\"","2f913b7b":"tempDict","de1fa841":"# Let's replace the titles:\ntrain_data_pre[\"Titles\"] = train_data_pre.Titles.replace(tempDict)\ntrain_data_pre.Titles.value_counts()","f1fb826c":"# Let's dummify the Titles:\ntrain_titles_dummy = pd.get_dummies(train_data_pre[\"Titles\"], drop_first = True)\ntrain_titles_dummy","0aadd46d":"#Let's add this to the main data:\ntrain_data_pre = pd.concat([train_data_pre, train_titles_dummy], axis = 1)\ntrain_data_pre","c8c42249":"# Let's drop the useless columns\ntrain_data_pre.drop([\"Name\", \"Titles\"], axis = 1, inplace= True)","fadd676d":"train_data_pre","aa252537":"# Apply the same methods to test_data_pre:\ntest_data_pre[\"Titles\"] = test_data_pre.Name.str.extract(regex, expand = False)\ntest_data_pre[\"Titles\"] = test_data_pre.Titles.replace(tempDict)\ntest_titles_dummy = pd.get_dummies(test_data_pre[\"Titles\"], drop_first = True)\ntest_data_pre = pd.concat([test_data_pre, test_titles_dummy], axis = 1)\ntest_data_pre.drop([\"Name\", \"Titles\"], axis = 1, inplace= True)","6d4a0484":"train_data_pre","433c882c":"test_data_pre","7cb8dd77":"# Let's extract the feature set and the labels\nX = train_data_pre.drop(\"Survived\", axis = 1)\ny = train_data_pre.Survived","6789b18b":"# Import Models:\nfrom sklearn import model_selection, svm\nfrom sklearn.ensemble import RandomForestClassifier","1782e1f3":"#Split the data so that we can see how our model works:\n#X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3)","9185dac7":"# Random Forest gave me the highest prediction score for now\nrf = RandomForestClassifier()\nrf.fit(X,y)","23e7a831":"rf.score(X,y)","b733eb5f":"# SVM:\n#clfsvm = svm.SVC(kernel = \"rbf\")\n#clfsvm.fit(X, y)","8d698028":"#clfsvm.score(X, y)","cdbd3fd5":"#Predicting the outcome:\nprediction = rf.predict(test_data_pre)\nprediction","bf62f67c":"# Assigning prediction to submission:\nsubmission.Survived = prediction","27f81640":"#Saving it:\nsubmission.to_csv('submission_final.csv', index=False)","5dec4dde":"\n# `Loading the datasets:`","56877d58":"# Analyzing Titanic Dataset and predicting with SVM and RandomForest model\n\n#### Please let me know if you see any further improvements I could make!\n","f896c1e2":"# *`EDA`*:","2ca82958":"## *Cleaning:*\n### `Fill NAs:`","f0b67a97":"## Training the model:\n"}}