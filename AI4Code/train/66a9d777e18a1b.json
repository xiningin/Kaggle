{"cell_type":{"cf4f588c":"code","c83b884e":"code","962b2608":"code","801e8ef6":"code","7232d0b0":"code","74813595":"code","380240a1":"code","bb13eea6":"code","5c65638f":"code","20e7aa4c":"code","983747d7":"code","dde77eb3":"code","8624bbbc":"code","1fcb8faa":"code","59ae9e2e":"code","013d9272":"code","9e140bfe":"code","af9b1dcb":"code","36892350":"code","1a320143":"code","e8befbdb":"code","91043bb5":"code","1fa56774":"code","4127f8c7":"code","e82eae6c":"code","bf6f78da":"code","ee900973":"code","52e6ef21":"code","cea34288":"code","4ff89f76":"code","390cc8be":"code","4443b950":"code","499f6920":"code","c0c913e3":"code","14828f11":"code","7dcb95ba":"code","623339de":"markdown","0295b3f5":"markdown","bea81e5e":"markdown","0e4c54df":"markdown","7249e637":"markdown","f9a97666":"markdown","749dc845":"markdown"},"source":{"cf4f588c":"import os \nimport numpy as np \nimport cv2 as cv \nimport matplotlib.pyplot as plt \nimport PIL \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom colorama import Fore,Style\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix , classification_report\nimport seaborn as sns\nimport math\nimport pickle\nb__ = Fore.BLUE\nst__ = Style.RESET_ALL","c83b884e":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential ,Model\nfrom tensorflow.keras.layers import Input,Dense,BatchNormalization,Dropout,Conv2D,MaxPool2D,\\\nFlatten , GlobalAveragePooling2D\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import Sequence\nimport tensorflow as tf ","962b2608":"config = {\n   \"basic_path\" : \"..\/input\/fer2013\",\n   \"classes\" : [\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"],\n   \"batch_size\" : 64,\n    \"epochs\" : 200\n}","801e8ef6":"fig , ax = plt.subplots(1,len(config[\"classes\"]),figsize=(35,10))\nfor i in range(len(config[\"classes\"])) :\n    emotion = config[\"classes\"][i]\n    dest = os.path.join(config[\"basic_path\"],\"train\",emotion)\n    des_img = os.listdir(dest)[0]\n    img = os.path.join(dest,des_img)\n    img = cv.imread(img)\n    ax[i].imshow(img,cmap=\"gray\")\n    ax[i].set_title(emotion,size=18,color=\"#355\")","7232d0b0":"val_datagen = ImageDataGenerator(rescale=1\/255)\ntrain_datagen = ImageDataGenerator(rescale=1\/255,\n                                  rotation_range=30,\n                                  shear_range=0.3,\n                                  zoom_range=0.3,\n                                  horizontal_flip=True,\n                                  fill_mode=\"nearest\")\ntrain_generator = train_datagen.flow_from_directory(os.path.join(config[\"basic_path\"],\"train\"),\\\n                                                   batch_size = config[\"batch_size\"],\\\n                                                   target_size =(48,48),\n                                                   color_mode = \"grayscale\",\n                                                   class_mode = \"categorical\")\nval_generator = val_datagen.flow_from_directory(os.path.join(config[\"basic_path\"],\"test\"),\\\n                                               batch_size=config[\"batch_size\"],\\\n                                               target_size=(48,48),\\\n                                               class_mode = \"categorical\",\n                                               color_mode = \"grayscale\")","74813595":"def create_model() :\n    md = Sequential()\n    md.add(Conv2D(32,kernel_size=(3,3),activation=\"relu\",input_shape=(48,48,1),\\\n                  kernel_regularizer=regularizers.l2(0.0001)))\n    md.add(MaxPool2D(pool_size=(2,2)))\n    md.add(Conv2D(64,kernel_size=(3,3),activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n    md.add(MaxPool2D(pool_size=(2,2)))\n    md.add(Conv2D(128,kernel_size=(3,3),activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n    md.add(MaxPool2D(pool_size=(2,2)))\n    md.add(Conv2D(256,kernel_size=(3,3),activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001)))\n    md.add(MaxPool2D(pool_size=(2,2)))\n    md.add(Flatten())\n    md.add(BatchNormalization())\n    md.add(Dropout(0.2))\n    md.add(Dense(128,activation=\"relu\"))\n    md.add(BatchNormalization())\n    md.add(Dropout(0.2))\n    md.add(Dense(64,activation=\"relu\"))\n    md.add(BatchNormalization())\n    md.add(Dropout(0.2))\n    md.add(Dense(32,activation=\"relu\"))\n    md.add(BatchNormalization())\n    md.add(Dropout(0.2))\n    md.add(Dense(len(config[\"classes\"]),activation=\"softmax\"))\n    \n    md.compile(optimizer=Adam(lr=5e-5),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n    \n    return md","380240a1":"model = create_model()","bb13eea6":"filepath = \"emotion_detector_models.hdf5\"\ncheckpoint = ModelCheckpoint(filepath,monitor=\"val_accuracy\",verbose=1,save_best_only=True,\\\n                             mode=\"max\")","5c65638f":"nb_train_samples = 28709\nnb_validation_samples = 3589","20e7aa4c":"history = model.fit(train_generator,steps_per_epoch=nb_train_samples\/\/config[\"batch_size\"],validation_data=val_generator,\\\n          callbacks=[checkpoint],validation_steps=nb_validation_samples\/\/config[\"batch_size\"],\\\n          epochs=config[\"epochs\"])","983747d7":"print(history.history.keys())\nplt.plot(range(config[\"epochs\"]),history.history[\"val_loss\"],label=\"validation\")\nplt.plot(range(config[\"epochs\"]),history.history[\"loss\"],label=\"train\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Train and Valid Loss over epochs\",size=15,color=\"#DED\")\nplt.show()","dde77eb3":"val_generator = val_datagen.flow_from_directory(os.path.join(config[\"basic_path\"],\"test\"),\\\n                                               batch_size=config[\"batch_size\"],\\\n                                               target_size=(48,48),\\\n                                               shuffle = False,\n                                               class_mode = \"categorical\",\n                                               color_mode = \"grayscale\")","8624bbbc":"y_true = val_generator.classes\nclasses = list(val_generator.class_indices.keys())","1fcb8faa":"val_generator.classes","59ae9e2e":"model = tf.keras.models.load_model(\".\/emotion_detector_models.hdf5\")","013d9272":"#model.load_weights(filepath)\ny_pred = model.predict(val_generator)","9e140bfe":"y_pred = np.argmax(y_pred,axis=1)","af9b1dcb":"print(f\"{b__}\" ,\" \" * 35 ,f\" Confusion Matrix {st__}\")\ncf = pd.DataFrame(confusion_matrix(y_true,y_pred),columns=classes,index=classes)\nfig = plt.figure(figsize=(35,10))\nsns.heatmap(cf,square=True,annot=True)","36892350":"print(f\"{b__}\" ,\" \" * 35 ,f\"Classification Report {st__}\")\ncf = classification_report(y_true,y_pred,target_names=classes)\nfig = plt.figure(figsize=(35,10))\nprint(cf)\n#sns.heatmap(cf,square=True,annot=True)","1a320143":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","e8befbdb":"def arc_margin_model(model,N_classes=len(classes),training=True):\n    margin =  ArcMarginProduct(\n            n_classes = N_classes, \n            s = 30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n            )\n    inp_img = Input(shape=(48,48,1))\n    md = Conv2D(32,kernel_size=(3,3),activation=\"relu\",\\\n                kernel_regularizer=regularizers.l2(0.0001))(inp_img)\n    md = MaxPool2D(pool_size=(2,2))(md)\n    md = Conv2D(64,kernel_size=(3,3),activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001))(md)\n    md = MaxPool2D(pool_size=(2,2))(md)\n    md = Conv2D(128,kernel_size=(3,3),activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001))(md)\n    md = MaxPool2D(pool_size=(2,2))(md)\n    md = Conv2D(256,kernel_size=(3,3),activation=\"relu\",kernel_regularizer=regularizers.l2(0.0001))(md)\n    md = MaxPool2D(pool_size=(2,2))(md)\n    #md.add(Flatten())\n    md = GlobalAveragePooling2D()(md)\n    md = BatchNormalization()(md)\n    md = Dropout(0.2)(md)\n    md = Dense(128,activation=\"relu\")(md)\n    md = BatchNormalization()(md)\n    md = Dropout(0.2)(md)\n    md = Dense(64,activation=\"relu\")(md)\n    md = BatchNormalization()(md)\n    md = Dropout(0.2)(md)\n    md = Dense(32,activation=\"relu\")(md)\n    md = BatchNormalization()(md)\n    \n    if training :\n       y = Input(shape=(),name=\"labels\")\n       logits = margin([md,y])\n       logits = tf.keras.layers.Softmax()(logits)\n       model = Model([inp_img,y],logits)\n    else :\n        #logits = tf.keras.layers.Dense(N_classes,activation=\"softmax\")(md)\n        model = Model(inp_img,md)\n    \n    \n    model.compile(optimizer=Adam(lr=5e-5),loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\\\n                  metrics=[\"accuracy\"])\n    \n    return model","91043bb5":"def model_prediction(arc_model,N_classes=len(classes)):\n    inp = Input(shape=(48,48,1))\n    md = Model(inputs=arc_model.layers[0].input,outputs=arc_model.layers[-4].output)\n    x = md(inp)\n    out = Dense(N_classes,activation=\"softmax\")(x)\n    model = Model(inp,out)\n    for layer in md.layers :\n        layer.trainable = False\n    model.compile(optimizer=Adam(lr=0.05),loss= \"categorical_crossentropy\",metrics=[\"accuracy\"])\n    return model","1fa56774":"class Dataset(Sequence) :\n    \n    def __init__(self,generator) :\n        self.generator = generator\n    def __len__(self) :\n        return len(self.generator)\n    def __getitem__(self,idx) :\n        x = self.generator[idx] [0]\n        y = np.argmax(self.generator[idx] [1],axis=1)\n        return (x,y),y\n        ","4127f8c7":"margin_model = arc_margin_model(model)","e82eae6c":"margin_model.summary()","bf6f78da":"train_data = Dataset(train_generator)\nval_data = Dataset(val_generator)","ee900973":"def get_lr_callback():\n    lr_start   = 0.000001\n    lr_max     = 0.000005 * config[\"batch_size\"]\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max    \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    return lr_callback","52e6ef21":"arc_file_model = \"margin_model.hdf5\"\ncheck = ModelCheckpoint(arc_file_model,monitor=\"val_accuracy\",mode=\"max\",save_best_only=True,\\\n                       save_weights_only=True,verbose=1)\nhis = margin_model.fit(train_data,callbacks=[check,get_lr_callback()],validation_data=\\\n                       val_data,epochs=config[\"epochs\"])","cea34288":"margin_model.load_weights(\".\/margin_model.hdf5\")","4ff89f76":"margin_arc_face = model_prediction(margin_model)","390cc8be":"path = \"margin_fine_tunned.hdf5\"\nmargin_check = ModelCheckpoint(path,mode=max,save_best_only=True,save_weights_only=True)","4443b950":"margin_arc_face.fit(train_generator,validation_data=val_generator,callbacks=[margin_check],\\\n                    epochs=25)","499f6920":"print(classification_report(y_true,np.argmax(margin_arc_face.predict(val_generator),axis=1),target_names=classes))\n","c0c913e3":"margin_arc_face.save(\"margin_arc_face_model\")","14828f11":"correspondances = {val:key for (key,val) in val_generator.class_indices.items()}","7dcb95ba":"with open (\"classes\",\"wb\") as f :\n    pickle.dump(correspondances,f)","623339de":"## 3. Modeling :","0295b3f5":"## 2. Data processing :","bea81e5e":"## Test on real webcam :","0e4c54df":"## 1. EDA:","7249e637":"## Margin Arc face ","f9a97666":"# <center> <font color=\"blue\"> Learn facial expressions from an image <\/font>\n<font><center>![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQcYU6F4gXS6VAnfr2O0mJiFFEPgBK1cR7J_Q&usqp=CAU)   ","749dc845":"## 0. Import Libraories :"}}