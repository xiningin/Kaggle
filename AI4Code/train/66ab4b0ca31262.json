{"cell_type":{"46d12933":"code","b8d9e890":"code","24b1351c":"code","dcf89adc":"code","9f1ca09e":"code","34569f28":"code","27d291dc":"code","8f6b9446":"code","343aa7c6":"code","9f7e9878":"code","03bc5903":"markdown","b068ccf8":"markdown","16da0f3d":"markdown","8db292e4":"markdown","fe269071":"markdown","b204448c":"markdown"},"source":{"46d12933":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8d9e890":"from transformers import pipeline, set_seed\n","24b1351c":"generator = pipeline('text-generation', model='gpt2')\ngenerator(\"Hello, I like to play cricket,\", max_length=60, num_return_sequences=7)\n","dcf89adc":"generator(\"The Indian man worked as a\", max_length=10, num_return_sequences=5)","9f1ca09e":"# Allocate a pipeline for sentiment-analysis\nclassifier = pipeline('sentiment-analysis')\nclassifier('The secret of getting ahead is getting started.')","34569f28":"nlp = pipeline(\"question-answering\")\n\ncontext = r\"\"\"\nMicrosoft was founded by Bill Gates and Paul Allen in 1975.\nThe property of being prime (or not) is called primality.\nA simple but slow method of verifying the primality of a given number n is known as trial division.\nIt consists of testing whether n is a multiple of any integer between 2 and itself.\nAlgorithms much more efficient than trial division have been devised to test the primality of large numbers.\nThese include the Miller-Rabin primality test, which is fast but has a small probability of error, and the AKS primality test, which always produces the correct answer in polynomial time but is too slow to be practical.\nParticularly fast methods are available for numbers of special forms, such as Mersenne numbers.\nAs of January 2016, the largest known prime number has 22,338,618 decimal digits.\n\"\"\"\n\n#Question 1\nresult = nlp(question=\"What is a simple method to verify primality?\", context=context)\n\nprint(f\"Answer 1: '{result['answer']}'\")\n\n#Question 2\nresult = nlp(question=\"When did Bill gates founded Microsoft?\", context=context)\n\nprint(f\"Answer 2: '{result['answer']}'\")","27d291dc":"unmasker = pipeline('fill-mask', model='bert-base-cased')\nunmasker(\"Hello, My name is [MASK].\")","8f6b9446":"#Summarization is currently supported by Bart and T5.\nsummarizer = pipeline(\"summarization\")\n\nARTICLE = \"\"\"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972.\nFirst conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,\nApollo was later dedicated to President John F. Kennedy's national goal of \"landing a man on the Moon and returning him safely to the Earth\" by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress.\nProject Mercury was followed by the two-man Project Gemini (1962-66).\nThe first manned flight of Apollo was in 1968.\nApollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966.\nGemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions.\nApollo used Saturn family rockets as launch vehicles.\nApollo\/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\n\"\"\"\n\nsummary=summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)[0]\n\nprint(summary['summary_text'])","343aa7c6":"# English to German\ntranslator_ger = pipeline(\"translation_en_to_de\")\nprint(\"German: \",translator_ger(\"Joe Biden became the 46th president of U.S.A.\", max_length=40)[0]['translation_text'])\n\n# English to French\ntranslator_fr = pipeline('translation_en_to_fr')\nprint(\"French: \",translator_fr(\"Joe Biden became the 46th president of U.S.A\",  max_length=40)[0]['translation_text'])","9f7e9878":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft\/DialoGPT-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft\/DialoGPT-medium\")\n\n# Let's chat for 5 lines\nfor step in range(5):\n   # encode the new user input, add the eos_token and return a tensor in Pytorch\n   new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n   # append the new user input tokens to the chat history\n   bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n   # generated a response while limiting the total chat history to 1000 tokens,\n   chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n\n   # pretty print last output tokens from bot\n   print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","03bc5903":"## Text Summarization","b068ccf8":"## English to German Translation\n","16da0f3d":"## Text Prediction using BERT\n\nBERT (Bidirectional Encoder Representations from Transformers) makes use of a Transformer, which learns contextual relations between words in a text. In its vanilla form, Transformer includes two separate mechanisms\u200a\u2014\u200aan encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT\u2019s goal is to generate a language model, only the encoder mechanism is used. So BERT is just transformer encoders stacked above each other.","8db292e4":"## Text Generation","fe269071":"## Conversation","b204448c":"## Sentiment Analysis"}}