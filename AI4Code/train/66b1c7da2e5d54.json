{"cell_type":{"e60e57c7":"code","3f8187f8":"code","e79b152b":"code","57fab9d6":"code","efee3852":"code","6dc15be1":"code","6d97271d":"code","bc5cc732":"code","0a1c2e79":"code","d5384034":"code","4f8b7db9":"code","892e5fa4":"code","631003c1":"code","ef381484":"code","a03c6862":"code","5005259d":"code","db73ff5d":"code","83e47d9f":"code","c5e7879c":"markdown","410e1dc9":"markdown","9182574f":"markdown","4ebfe985":"markdown","6541aa38":"markdown","2602915b":"markdown","534750fe":"markdown","85ce20d1":"markdown","51c59113":"markdown","92e632b9":"markdown","94ea5b1b":"markdown","36abc3ac":"markdown","6e14671a":"markdown","6102fed0":"markdown","1a245486":"markdown"},"source":{"e60e57c7":"import pandas as pd\nimport numpy as np\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.porter import PorterStemmer\nfrom gensim.models import Word2Vec\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3f8187f8":"# Read dataset\ntrain_df = pd.read_csv(\"..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Train.csv\")","e79b152b":"# first few lines\ntrain_df.head()","57fab9d6":"# shape of dataset\ntrain_df.shape","efee3852":"sns.countplot(train_df['label'])","6dc15be1":"train_df['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in train_df['text']]","6d97271d":"porter_stemmer = PorterStemmer()\ntrain_df['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in train_df['tokenized_text']]\ntrain_df","bc5cc732":"\n# Train Test Split Function\ndef split_train_test(train_df, test_size=0.3, shuffle_state=True):\n    X_train, X_test, Y_train, Y_test = train_test_split(train_df[['stemmed_tokens']], \n                                                        train_df['label'], \n                                                        shuffle=shuffle_state,\n                                                        test_size=test_size, \n                                                        random_state=15)\n    print(\"Value counts for Train sentiments\")\n    print(Y_train.value_counts())\n    print(\"Value counts for Test sentiments\")\n    print(Y_test.value_counts())\n    print(type(X_train))\n    print(type(Y_train))\n    X_train = X_train.reset_index()\n    X_test = X_test.reset_index()\n    Y_train = Y_train.to_frame()\n    Y_train = Y_train.reset_index()\n    Y_test = Y_test.to_frame()\n    Y_test = Y_test.reset_index()\n    print(X_train.head())\n    return X_train, X_test, Y_train, Y_test\n\n# Call the train_test_split\nX_train, X_test, Y_train, Y_test = split_train_test(train_df)","0a1c2e79":"size = 500\nwindow = 3\nmin_count = 1\nworkers = 3\n# 0 for CBOW, 1 for skip-gram\nsg = 0\nOUTPUT_FOLDER = '\/kaggle\/working\/'\n# Function to train word2vec model\ndef make_word2vec_model(train_df, padding, sg, min_count, size, workers, window):\n    if  padding:\n        #print(len(train))\n        temp_df = pd.Series(train_df['stemmed_tokens']).values\n        temp_df = list(temp_df)\n        temp_df.append(['pad'])\n        #print(str(size))\n        word2vec_file = OUTPUT_FOLDER + '2ata' + '_PAD.model'\n    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n\n    w2v_model.save(word2vec_file)\n    return w2v_model, word2vec_file\n\n# Train Word2vec model\nw2vmodel, word2vec_file = make_word2vec_model(train_df, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)","d5384034":"max_sen_len = train_df.stemmed_tokens.map(len).max()\n\npadding_idx = w2vmodel.wv.vocab['pad'].index\nprint(padding_idx)\ndef make_word2vec_vector_cnn(sentence):\n    padded_X = [padding_idx for i in range(max_sen_len)]\n    i = 0\n    for word in sentence:\n        if word not in w2vmodel.wv.vocab:\n            padded_X[i] = 0\n        else:\n            padded_X[i] = w2vmodel.wv.vocab[word].index\n        i += 1\n    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)","4f8b7db9":"max_sen_len","892e5fa4":"padding_idx","631003c1":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","ef381484":"EMBEDDING_SIZE = 500\nNUM_FILTERS = 10\n\n#torch.nn.Conv2d(in_channels: int, out_channels: int, kernel_size: Union[T, Tuple[T, T]], \n#stride: Union[T, Tuple[T, T]] = 1, padding: Union[T, Tuple[T, T]] = 0, \n#dilation: Union[T, Tuple[T, T]] = 1, groups: int = 1, bias: bool = True, padding_mode: str = 'zeros')\n\nclass CnnTextClassifier(nn.Module):\n    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n        super(CnnTextClassifier, self).__init__()\n        w2vmodel = gensim.models.KeyedVectors.load(OUTPUT_FOLDER + '2ata_PAD.model')\n        weights = w2vmodel.wv\n        # With pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n        \n        # like a python list, it was designed to store any desired number of nn.Module\n        self.convs = nn.ModuleList([\n                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n                                   for window_size in window_sizes\n        ])\n    \n        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x) # [B, T, E]\n\n        # Apply a convolution + max_pool layer for each window size\n        x = torch.unsqueeze(x, 1)\n        xs = []\n        for conv in self.convs:\n            x2 = torch.tanh(conv(x))\n            x2 = torch.squeeze(x2, -1)\n            x2 = F.max_pool1d(x2, x2.size(2))\n            xs.append(x2)\n        x = torch.cat(xs, 2)\n\n        # FC\n        x = x.view(x.size(0), -1)\n        logits = self.fc(x)\n\n        probs = F.softmax(logits, dim = 1)\n\n        return probs","a03c6862":"def make_target(label):\n    if label == 0:\n        return torch.tensor([0], dtype=torch.long, device=device)\n    elif label == 1:\n        return torch.tensor([1], dtype=torch.long, device=device)","5005259d":"NUM_CLASSES = 2\nVOCAB_SIZE = len(w2vmodel.wv.vocab)\nprint(VOCAB_SIZE)\ncnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\ncnn_model.to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)\nnum_epochs = 5","db73ff5d":"# Open the file for writing loss\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nf = open(loss_file_name,'w')\nf.write('iter, loss')\nf.write('\\n')\nlosses = []\n\ncnn_model.train()\nfor epoch in range(num_epochs):\n    print(\"Epoch\" + str(epoch + 1))\n    train_loss = 0\n\n    for index, row in X_train.iterrows():\n        # Clearing the accumulated gradients\n        cnn_model.zero_grad()\n\n        # Make the bag of words vector for stemmed tokens \n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n       \n        # Forward pass to get output\n        probs = cnn_model(bow_vec)\n\n        # Get the target label\n        #print(Y_train['label'][index])\n        target = make_target(Y_train['label'][index])\n\n        # Calculate Loss: softmax --> cross entropy loss\n        loss = loss_function(probs, target)\n        train_loss += loss.item()\n        \n\n        # Getting gradients w.r.t. parameters\n        loss.backward()\n\n        # Updating parameters\n        optimizer.step()\n\n    print(f'train_loss : {train_loss \/ len(X_train)}')\n    print(\"Epoch ran :\"+ str(epoch+1))\n    \n    f.write(str((epoch+1)) + \",\" + str(train_loss \/ len(X_train)))\n    f.write('\\n')\n    train_loss = 0\n\ntorch.save(cnn_model, OUTPUT_FOLDER + 'cnn_big_model_500_with_padding.pth')\n\nf.close()\nprint(\"Input vector\")\nprint(\"Probs\")\nprint(probs)\nprint(torch.argmax(probs, dim=1).cpu().numpy()[0])","83e47d9f":"bow_cnn_predictions = []\noriginal_lables_cnn_bow = []\ncnn_model.eval()\nloss_df = pd.read_csv(OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv')\nprint(loss_df.columns)\n# loss_df.plot('loss')\n\ny_pred_list = []\ny_true_list = []\n\nwith torch.no_grad():\n    for index, row in X_test.iterrows():\n        #print(row['stemmed_tokens'])\n        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n        #print(bow_vec)\n        probs = cnn_model(bow_vec)\n        #print(probs.data)\n        _, predicted = torch.max(probs.data,  1)\n        \n        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n        original_lables_cnn_bow.append(make_target(Y_test['label'][index]).cpu().numpy()[0])\n\nprint(confusion_matrix(original_lables_cnn_bow, bow_cnn_predictions))\n#print(original_lables_cnn_bow)\nprint(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\nloss_file_name = OUTPUT_FOLDER + '1cnn_class_big_loss_with_padding.csv'\nloss_df = pd.read_csv(loss_file_name)\nprint(loss_df.columns)\nplt_500_padding_30_epochs = loss_df[' loss'].plot()\nfig = plt_500_padding_30_epochs.get_figure()\nfig.savefig(OUTPUT_FOLDER + '1loss_plt_500_padding_30_epochs.pdf')","c5e7879c":"**Tokenization** is the process of tokenizing or splitting a string, text into a list of tokens.  ","410e1dc9":"## Model Test","9182574f":"## Stemming","4ebfe985":"## Import Libraries","6541aa38":"**Stemming** is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.The input to the stemmer is tokenized words.\nA stemming algorithm reduces the words \u201cchocolates\u201d, \u201cchocolatey\u201d, \u201cchoco\u201d to the root word, \u201cchocolate\u201d   ","2602915b":"**word2vec** algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. ","534750fe":"## Padding","85ce20d1":"## Word2Vec Model Creation","51c59113":"## CNN model ","92e632b9":"## Tokenization","94ea5b1b":"Let's see weather our dataset is balanced or imbalanced","36abc3ac":"From above graph we can see that our dataset is balanced dataset.So that's great","6e14671a":"All the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary.\n**Padding** will make all sentences of same length by inserting 0 in the end or bigenning of the sentences","6102fed0":"## Train-Test Split","1a245486":"## Train the model"}}