{"cell_type":{"0a1803ee":"code","745bd061":"code","be405c5f":"code","d639aa46":"code","5e12282d":"code","3d4606d8":"code","12c8b2f8":"code","c9db2cde":"code","0413dd28":"code","a19d73dc":"code","2727058d":"code","55266b32":"code","6ce0a158":"code","78464448":"code","6b416bd0":"code","a0f3dd9b":"code","386797f0":"code","a23fb78a":"code","fbeb20ef":"code","c7becc28":"code","e1ab32f4":"markdown","930c5a62":"markdown","d8e7f72b":"markdown","dd6a8a12":"markdown","290cdee6":"markdown","cdda6901":"markdown","13a21d62":"markdown"},"source":{"0a1803ee":"import cupy as cp\nimport numpy as np","745bd061":"import pandas as pd\nfrom tqdm import tqdm\nimport os\nimport cv2\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom cupyx.scipy.ndimage.filters import convolve\nimport skimage.measure","be405c5f":"path = '..\/input\/natural-images\/natural_images\/'","d639aa46":"idx_elements = { k:v for (k,v) in enumerate(list(os.walk(path))[0][1])}\nelements_idx = { v:k for (k,v) in enumerate(list(os.walk(path))[0][1])}","5e12282d":"features = []\nlabels = []\n\nfor folder in list(os.walk(path))[1:]:\n    feature = folder[0].split('\/')[-1]\n    for img_path in folder[2]:\n        features.append(cv2.resize(cv2.imread(path + feature + \"\/\" + img_path), (28, 28)))\n        one_hot = cp.zeros(len(idx_elements))\n        one_hot[elements_idx[feature]] = 1\n\n        labels.append(one_hot)\n\nfeatures = cp.array(features)\nlabels = cp.array(labels)","3d4606d8":"print(features.shape)","12c8b2f8":"# normalize\nfeatures = features \/ 255.0","c9db2cde":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)","0413dd28":"def relu(x):\n    mask = (x>0) * 1.0 \n    return x * mask\n\ndef drelu(x):\n    mask = (x>0) * 1.0 \n    return  mask","a19d73dc":"def sigmoid(z):\n  return 1.0 \/ (1 + cp.exp(-z))\n\ndef dsigmoid(z):\n  return sigmoid(z) * (1-sigmoid(z))","2727058d":"def softmax(s): \n    exps = cp.exp(s - cp.max(s, axis=1, keepdims=True))\n    return exps\/cp.sum(exps, axis=1, keepdims=True)\n\ndef cross_entropy(pred, real):\n    n_samples = real.shape[0]\n    res = pred - real\n    return res\/n_samples","55266b32":"def error(pred, real):\n    n_samples = real.shape[0]\n    logp = - cp.log(pred[cp.arange(n_samples), real.argmax(axis=1)])\n    loss = cp.sum(logp)\/n_samples\n    return loss","6ce0a158":"def forward(x, theta):\n    k, w1, w2, b1, b2 = theta\n\n    m = cp.asarray([convolve(input=x[i], weights=k) for i in range(len(x))])\n    n = relu(m)\n    o = cp.asarray([skimage.measure.block_reduce(cp.asnumpy(n[i]), (2,2,1), np.max) for i in range(len(m))])\n    f = o.reshape(n.shape[0], 588)\n    p = f.dot(w1) + b1\n    q = sigmoid(p)\n    r = q.dot(w2) + b2\n    s = softmax(r)\n\n    return m, n, o, f, p, q, r, s","78464448":"def backward(x, y, theta):\n    k, w1, w2, b1, b2 = theta\n    m, n, o, f, p, q, r, s = forward(x, theta)\n\n    ds = cross_entropy(s, y)\n    dr = ds.dot(w2.T)\n    dq = dr * dsigmoid(p)\n    dp = dq.dot(w1.T)\n\n    db2 = cp.mean(ds, axis=0)\n    dw2 = ds.T.dot(q).T\n\n    db1 = cp.mean(dq, axis=0)\n    dw1 = dq.T.dot(f).T\n\n    masks = cp.asarray([cp.equal(n[i], o[i].repeat(2, axis=0).repeat(2, axis=1)).astype(int) for i in range(len(n))])\n    windows = cp.asarray([(masks[i] * dp[i].reshape(14, 14, 3).repeat(2, axis=0).repeat(2, axis=1)) for i in range(len(masks))])\n\n    dk = cp.mean(cp.array([cp.rot90(convolve(x[i], cp.rot90(windows[i] * n[i],2 )),2) for i in range(len(windows))]))\n\n    return dk, dw1, dw2, db1, db2","6b416bd0":"def optimize(grads, theta, lr=0.1):\n    theta = tuple([theta[i] - (grads[i] * lr) for i in range(len(theta))])\n    return theta","a0f3dd9b":"cp.random.seed(35435345353)\n\nk = cp.random.uniform(size=(28, 28, 3))\n\nw1 = cp.random.uniform(size=(588, 64))\nb1 = cp.random.uniform(size=(1, 64))\n\nw2 = cp.random.uniform(size=(64, 8))\nb2 = cp.random.uniform(size=(1, 8))\n\ntheta = k, w1, w2, b1, b2","386797f0":"# mini batches\nx_batches = cp.array_split(X_train, math.ceil(len(X_train) \/ 500))\ny_batches = cp.array_split(y_train, math.ceil(len(y_train) \/ 500))","a23fb78a":"errors = []","fbeb20ef":"for epoch in range(301):\n    for i,x_batch in enumerate(x_batches):\n        grads = backward(x_batch, y_batches[i], theta)\n        theta = optimize(grads, theta, 0.005)\n\n    if(epoch % 25 == 0):\n        e = error(forward(x_batches[0], theta)[-1], y_batches[0])\n        errors.append(e)\n        print('Epoch:{0}, Error:{1}'.format(epoch, e))","c7becc28":"plt.plot(errors)","e1ab32f4":"# Training","930c5a62":"# Activation functions","d8e7f72b":"# Forward Propagation","dd6a8a12":"# Backward Propagation","290cdee6":"# CNN CuPy Natural Images","cdda6901":"# Error function","13a21d62":"# Data Preparation"}}