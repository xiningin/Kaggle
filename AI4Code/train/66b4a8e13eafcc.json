{"cell_type":{"c6e3ea0a":"code","ea3e3da2":"code","0f1ff6a2":"code","cdc6dafc":"code","19a2d9d1":"code","bd41735b":"code","553a1ac9":"code","deb0b25f":"code","9c5e459c":"code","829e1e3c":"code","f9cdf97c":"code","05474e9b":"code","60a25fc5":"code","f0050e0e":"code","d9b95a87":"code","6d2c1a49":"code","bae002b4":"code","cd9c91ff":"code","6ec061fb":"code","a7287cde":"code","4e51d76d":"code","6b55f4c6":"code","dee0cca3":"code","3b78e941":"code","964ecd62":"code","836d9b2e":"code","b76249e7":"markdown","02afd58b":"markdown","c04c6ea6":"markdown","7a1b75d5":"markdown","50d35c9e":"markdown","dca74d02":"markdown","dde1aacc":"markdown","0bcf37e3":"markdown","513c731d":"markdown","93a21ac6":"markdown","dd7bb4a0":"markdown","93c5e77e":"markdown","cc2c947f":"markdown","1c458b2d":"markdown","1017e613":"markdown","71279953":"markdown","12d42fcb":"markdown","c49824be":"markdown","d0a94a84":"markdown","6f4e55d2":"markdown","2dc0f35d":"markdown"},"source":{"c6e3ea0a":"!pip install tensorflow==2.0.0-alpha0\n\nfrom tensorflow.python.ops import control_flow_util\ncontrol_flow_util.ENABLE_CONTROL_FLOW_V2 = True","ea3e3da2":"import tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.model_selection import train_test_split\n\nfrom os import listdir","0f1ff6a2":"train_dir = '..\/input\/train\/train\/'\ntrain_imgs = listdir(train_dir)\nnr_train_images = len(train_imgs)\nnr_train_images","cdc6dafc":"train_lables_df = pd.read_csv('..\/input\/train.csv', index_col='id')\nprint('Total entries: ' + str(train_lables_df.size))\nprint(train_lables_df.head(10))","19a2d9d1":"train_lables_df['has_cactus'].value_counts()","bd41735b":"def get_test_image_path(id):\n    return train_dir + id\n\ndef draw_cactus_image(id, ax):\n    path = get_test_image_path(id)\n    img = mpimg.imread(path)\n    plt.imshow(img)\n    ax.set_title('Label: ' + str(train_lables_df.loc[id]['has_cactus']))\n\nfig = plt.figure(figsize=(20,20))\nfor i in range(12):\n    ax = fig.add_subplot(3, 4, i + 1)\n    draw_cactus_image(train_imgs[i], ax)","553a1ac9":"train_image_paths = [train_dir + ti for ti in train_imgs]\ntrain_image_labels = [train_lables_df.loc[ti]['has_cactus'] for ti in train_imgs]\n\nfor i in range(10):\n    print(train_image_paths[i], train_image_labels[i])","deb0b25f":"def img_to_tensor(img_path):\n    img_tensor = tf.cast(tf.image.decode_image(tf.io.read_file(img_path)), tf.float32)\n    img_tensor \/= 255.0 # normalized to [0,1]\n    return img_tensor\n\nimg_to_tensor(train_image_paths[0])","9c5e459c":"X_train, X_valid, y_train, y_valid = train_test_split(train_image_paths, train_image_labels, test_size=0.2)\n\ndef process_image_in_record(path, label):\n    return img_to_tensor(path), label\n\ndef build_training_dataset(paths, labels, batch_size = 32):\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n    ds = ds.map(process_image_in_record)\n    ds = ds.shuffle(buffer_size = len(paths))\n    ds = ds.repeat()\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef build_validation_dataset(paths, labels, batch_size = 32):\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n    ds = ds.map(process_image_in_record)\n    ds = ds.batch(batch_size)\n    return ds\n\ntrain_ds = build_training_dataset(X_train, y_train)\nvalidation_ds = build_validation_dataset(X_valid, y_valid)","829e1e3c":"mini_train_ds = build_training_dataset(X_train[:5], y_train[:5], batch_size=2)\n# Fetch and print the first batch of 2 images\nfor images, labels in mini_train_ds.take(1):\n    print(images)\n    print(labels)","f9cdf97c":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","05474e9b":"history = model.fit(train_ds, epochs=20, steps_per_epoch=400, validation_data=validation_ds)","60a25fc5":"def plot_accuracies_and_losses(history):\n    plt.title('Accuracy')\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.legend(['training', 'validation'], loc='upper left')\n    plt.show()\n    \n    plt.title('Cross-entropy loss')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.legend(['training', 'validation'], loc='upper left')\n    plt.show()\n\nplot_accuracies_and_losses(history)","f0050e0e":"cnn_model = tf.keras.Sequential()\n\ncnn_model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\ncnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\ncnn_model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\ncnn_model.add(tf.keras.layers.MaxPooling2D((2,2)))\ncnn_model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n\ncnn_model.add(tf.keras.layers.Flatten())\ncnn_model.add(tf.keras.layers.Dense(64, activation='relu'))\ncnn_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\ncnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","d9b95a87":"cnn_model.summary()","6d2c1a49":"history = cnn_model.fit(train_ds, epochs=20, steps_per_epoch=400, validation_data=validation_ds)","bae002b4":"plot_accuracies_and_losses(history)","cd9c91ff":"# Following the example in https:\/\/www.kaggle.com\/amarjeet007\/visualize-cnn-with-keras\nlayer_outputs = [layer.output for layer in cnn_model.layers]\nactivation_model = tf.keras.Model(inputs=cnn_model.input, outputs=layer_outputs)\n\ndef print_example_and_activations(example, col_size, row_size, act_index): \n    example_array = img_to_tensor(example).numpy()\n    plt.imshow(example_array)\n    activations = activation_model.predict(example_array.reshape(1,32,32,3)) # batch of 1 - just the example array\n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray') # image for the first (and only) element in the batch at activation_index\n            activation_index += 1","6ec061fb":"# Visualizing layer 1\nprint_example_and_activations(X_train[0], 8, 4, 1)","a7287cde":"# Visualizing layer 3\nprint_example_and_activations(X_train[0], 8, 4, 3)","4e51d76d":"test_dir = '..\/input\/test\/test\/'\ntest_imgs = listdir(test_dir)\nprint(len(test_imgs))\ntest_imgs[:5]","6b55f4c6":"def path_to_numpy_array(path):\n    tensor = img_to_tensor(path)\n    array = tensor.numpy()\n    return array\n\ntest_image_paths = [test_dir + ti for ti in test_imgs]\ntest_instances = np.asarray([path_to_numpy_array(tip) for tip in test_image_paths])\n\ntest_instances[:2]","dee0cca3":"predictions = cnn_model.predict(test_instances)\nprint(len(predictions))","3b78e941":"submission_data = pd.DataFrame({'id': test_imgs, 'has_cactus': predictions.flatten()})\nsubmission_data.head(20)","964ecd62":"submission_data.to_csv('submission.csv', index=False)","836d9b2e":"!head submission.csv","b76249e7":"A first thing to look at is how many positive vs. negative examples we have:","02afd58b":"And take care of all the other imports that we will need here:","c04c6ea6":"The training images reside in the `..\/input\/train\/train\/` directory.","7a1b75d5":"Just to get the ball rolling, let's create a simple feed-forward network with a somewhat arbitrary choice of hyperparameters: after flattening the input, I will add two hidden layers with `ReLu` activations having 256 and 64 units, respectively and a layer with a single unit and `Sigmoid` activation for the output, since we're in a classic binary classification setup.","50d35c9e":"Well, I certainly can't tell much from these pictures. Let's try implementing a simple model to see how it does.","dca74d02":"First and foremost, this is an important sanity check: we can see that our model is gradually learning and getting better on both the training and validation sets. This gives us some confidence that there should be no glaring mishaps in the way we set up the data pipeline or the model.\n\nAnd second, while the graphs look a little noisy, the curves seem to be improving over time for both the training and validation sets, so there are no signs of overfitting. It therefore seems worth exploring a more complex model in the hopes of getting a better score.","dde1aacc":"### Checking out the data","0bcf37e3":"Since we'll be using TensorFlow, let's construct a `tf.data.Dataset` of the training data: images and labels. First, let's match up the image paths and the labels in a more convenient data structure:","513c731d":"### Visualizing what the CNN learns\n\nWhile the few pictures we looked at in the beginning didn't seem very clear, the `cnn_model` is certainly able to figure out where those cacti are. The nice thing about CNNs is that we can actually visualize what is being learnt. Let's do that next.","93a21ac6":"Now, we need some tensors to flow. To get from the image paths above to actual `tf.Tensor`s we will combine `tf.io.read_file` and `tf.image.decode_image`. This very simple process is illustrated below for the first example:","dd7bb4a0":"### Building a simple model with the tf.keras Sequential API","93c5e77e":"So we have 17500 training images. Their labels are in the `train.csv` file.","cc2c947f":"### Loading the data","1c458b2d":"### Using convolutions\n\nConvolutional Neural Networks are the go-to type of neural network architectures for tackling tasks on image data, so let's try applying such a model to our problem.","1017e613":"That's what you want to see!\n\nWe clearly could have stopped training quite a bit earlier, but again we don't seem to be overfitting. So there's potential room for improvement here by choosing a more complex model.\n\nBut most importantly, the convolutional network gave us a big boost: going from 90% to a solid 98-99% accuracy on the validation set. With literally 0 effort put into tuning the model. Will do for now. Time to make a submission.","71279953":"Let's see this in action on a small subset of the data:","12d42fcb":"### Making a submission","c49824be":"First, let's install the alpha version of TensorFlow 2.0.","d0a94a84":"Ok, looks like this simple model is learning *something* and we reach about 90% accuracy on the validation set. Let's plot the losses and accuracies over time to see a bit more clearly how our network learnt.","6f4e55d2":"So the dataset does look somewhat imbalanced: 3 times more positive examples than negative. Let's look at a few examples.","2dc0f35d":"So that's how we can easily convert our training examples to tensors of shape (32, 32, 3). Now let's construct our training and validation sets from the images in the train directory."}}