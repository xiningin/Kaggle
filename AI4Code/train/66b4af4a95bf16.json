{"cell_type":{"aa70846b":"code","68c7f9f6":"code","9023f802":"code","dfcf2eba":"code","913e4498":"code","5e29e3f3":"code","8365b1b9":"code","b2e10a7d":"code","ec5f5743":"code","a7a0abe9":"code","d776ff7a":"code","100e28b5":"code","4c2d180f":"code","2b20e306":"markdown","fd520237":"markdown","b6fcff42":"markdown","bdace43a":"markdown","d972acc0":"markdown","49ca90e0":"markdown","c263a976":"markdown","9456d210":"markdown","0b7a44e8":"markdown","8fcfddac":"markdown","3018b9d9":"markdown","d7b43bf5":"markdown"},"source":{"aa70846b":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n#LodeStar has addded the option below\npd.set_option('display.max_columns', 100)\n\n# Read the data\nX_train_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id') # Train data contain 1460 rows\nX_test_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id') # Test data contain 1459 rows\n\n# print(X_train_full.head()) # Return all column with first 5 values\n# print(\"X_train_full data row and col:\", X_test_full.shape) # (1460, 80)\n# X_train_full data contain SalePrice, while X_test_full not contain SalePrice which need to predict","68c7f9f6":"# Remove rows with missing target, separate target from predictors\nX_train_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n# print(X_train_full.shape) # (1460, 80) i.e no value missing\n\nY_train_full = X_train_full.SalePrice\n# print(Y_train_real.shape) # (1460,) i.e only target data, single column\n\nX_train_full.drop(['SalePrice'], axis=1, inplace=True)\n# print(X_train_full.shape) # (1460, 79) i.e only train data","9023f802":"# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n# print(len(cols_with_missing)) # Return 19, as 19 column contain missing values\nX_train_full.drop(cols_with_missing, axis=1, inplace=True)\nX_test_full.drop(cols_with_missing, axis=1, inplace=True) # Also removing from test data to test with model\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, Y_train_full, train_size=0.8, test_size=0.2, random_state=0)\n\n# print(X_train.shape) # (1168, 60)\n# X_train.head() # return all numerical as well as categorical columns","dfcf2eba":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","913e4498":"# First approach drop column with categorical data\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\n\nprint(\"MAE from Approach 1 (Drop categorical variables):\")\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid)) # 17837.82570776256","5e29e3f3":"# First lets investigatge one column from dataset which is contain categorical data, column: Condition2\nprint(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\nprint(\"\\nUnique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())","8365b1b9":"# Fetch all categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_valid[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that should be ordinal encoded:', good_label_cols)\nprint('\\n')\nprint('Categorical columns that should be dropped from the dataset:', bad_label_cols)","b2e10a7d":"from sklearn.preprocessing import OrdinalEncoder\n\n# Drop categorical columns that will not be encoded as treated as bad columns\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\nlabel_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\nlabel_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])\n\n# print(label_X_train.shape) # (1168, 57) as 3 columns (bad_label_cols) are dropped\n# print(X_train.head)\n# print(\"==================================================================================\")\n# print(label_X_train.head)\n# Uncomment above print values to see difference as how categorical columns are changed to numeric values\n\nprint(\"MAE from Approach 2 (Ordinal Encoding):\") \nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid)) # 17098.01649543379","ec5f5743":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])\n# Street have two type of unique values, where Neighborhood has 25 types of unique values","a7a0abe9":"# Now lets drop high cardinality (grater than 10) columns as it will generate lots of columns, which is not feasible\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","d776ff7a":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","100e28b5":"print(\"MAE from Approach 3 (One-Hot Encoding):\") \nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","4c2d180f":"# Fetch all categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_test_full[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Drop categorical columns that will not be encoded as treated as bad columns\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_test = X_test_full.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\nlabel_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\nlabel_X_test[good_label_cols] = ordinal_encoder.transform(X_test_full[good_label_cols])\n\n# Impute Nan with mean value\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer(strategy='median') \nlabel_X_train = pd.DataFrame(my_imputer.fit_transform(label_X_train))\nlabel_X_test = pd.DataFrame(my_imputer.transform(label_X_test)) \n\nmodel1 = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel1.fit(label_X_train, y_train)\npreds = model1.predict(label_X_test)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_full.index,\n                       'SalePrice': preds})\noutput.to_csv('home-data-for-ml-course-handle-categorical-variables-submission.csv', index=False)\n# Download file and submit, Its gave me MAE: 16349.40468 for real sumbited result at, https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\n","2b20e306":"# Lets collect data for train and test","fd520237":"# Create common function to get Mean absolute error","b6fcff42":"> Fitting an ordinal encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In the case that the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them. Notice that the 'Condition2' column in the validation data contains the values 'RRAn' and 'RRNn', but these don't appear in the training data -- thus, if we try to use an ordinal encoder with scikit-learn, the code will throw an error.","bdace43a":"# Approach 1 : Drop column with categorical data","d972acc0":"# Approach 2 : Ordinal encoding","49ca90e0":"This is a common problem that you'll encounter with real-world data, and there are many approaches to fixing this issue. For instance, you can write a custom ordinal encoder to deal with new categories. The simplest approach, however, is to drop the problematic categorical columns.\n\nRun the code cell below to save the problematic columns to a Python list **bad_label_cols**. Likewise, columns that can be safely ordinal encoded are stored in **good_label_cols**.","c263a976":"# **So lets predict by test data set and submit to competition**","9456d210":"# As seen above, Approach 2 : Ordinal encoding giving best result, as lowest MAE 17098.02","0b7a44e8":"# **Simple explanation of how to handle categorical variables**\n\nWe refer below competition for tutorial,\nhttps:\/\/www.kaggle.com\/c\/home-data-for-ml-course\n\nCover below course topic,\nhttps:\/\/www.kaggle.com\/alexisbcook\/categorical-variables\n\n","8fcfddac":"![Lets start](https:\/\/i.ibb.co\/y4RRqQS\/lets-start.jpg)","3018b9d9":"![Thank you](https:\/\/i.ibb.co\/7vRbVHd\/thank-you-ribbon-sticker-thank-you-sign-thank-you-banner.jpg)","d7b43bf5":"# Approach 3 : One-hot encoding"}}