{"cell_type":{"61f5e3fd":"code","92a81456":"code","3810e7f8":"code","fe206051":"code","8e44b4c8":"code","47ea2096":"code","aba3c58c":"code","7b9c4931":"markdown","681c12fe":"markdown","a723365c":"markdown","c9268d70":"markdown","ae39609a":"markdown","4e782a72":"markdown","5db0b9ee":"markdown","a498c763":"markdown"},"source":{"61f5e3fd":"# To store data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To create models\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.models import LdaModel, CoherenceModel\nfrom gensim import corpora\n\n# To search directories\nimport os\n\n# To use regex\nimport re\n\n# To get punctuation\nimport string\n\n# To parse html\nfrom bs4 import BeautifulSoup\n\n# To get progression bars\nfrom tqdm import tqdm\n\n# To measure time\nfrom time import time\n\n# To get simple counters\nfrom collections import Counter\n\n# To process natural language\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\n# To use sparse matrices\nfrom scipy.sparse import csr_matrix\n\n# To create plots\nimport matplotlib.pyplot as plt","92a81456":"# Path to the data\npath = '..\/input\/'\n\n# Create file lists\nfiles_comments = [os.path.join(path, file) for file in os.listdir(path) if file.startswith('C')]\nfiles_articles = [os.path.join(path, file) for file in os.listdir(path) if file.startswith('A')]\n\n# Load data\ncomments = []\nfor file in files_comments[:1]:\n    comments.extend(pd.read_csv(file, low_memory=False)['commentBody'].dropna().values)\n    \nprint('Loaded Comments: {}'.format(len(comments)))","3810e7f8":"# Number of comments to use in the LDA\nn = 5000\n\n# To remove punctuation\nre_punctuation = re.compile('['+string.punctuation+']')\n\n# To tokenize the comments\ntokenizer = RegexpTokenizer('\\w+')\n\n# Get stopwords\nstop = stopwords.words('english')\n\n\n# Iterate over all comments\npreprocessed_comments = []\nfor comment in tqdm(np.random.choice(comments, n)):\n    # Remove html\n    comment = BeautifulSoup(comment, 'lxml').get_text().lower()\n    \n    # Remove punctuation\n    comment = re_punctuation.sub(' ', comment)\n    \n    # Tokenize comments\n    comment = tokenizer.tokenize(comment)\n    \n    # Remove stopwords\n    comment = [word for word in comment if word not in stop]\n    preprocessed_comments.append(comment)\n    \n    \n# Count overall word frequency\nwordFrequency = Counter()\nfor comment in preprocessed_comments:\n    wordFrequency.update(comment)\nprint('Unique Words In Comments: {}'.format(len(wordFrequency)))\n\n\n# Remove rare words\nminimumWordOccurrences = 5\ntexts = [[word for word in comment if wordFrequency[word] > minimumWordOccurrences] for comment in preprocessed_comments]\n\n\n# Create word dictionary\ndictionary = corpora.Dictionary(texts)\nvocabulary = [dictionary[i] for i in dictionary.keys()]\nprint('Documents\/Comments: {}'.format(len(texts)))\n\n\n# Create corpus\ncorpus = [dictionary.doc2bow(doc) for doc in texts]\n\n\n# Create sparse matrix\ndef makesparse(mycorpus, ncolumns):\n    data, row, col = [], [], []\n    for cc, doc in enumerate(mycorpus):\n        for word in doc:\n            row.append(cc)\n            col.append(word[0])\n            data.append(word[1])\n    X = csr_matrix((np.array(data), (np.array(row), np.array(col))), shape=(cc+1, ncolumns))\n    return X\n\n\n# Create sparse matrix\nX = makesparse(corpus, len(dictionary))\nprint('Train Shape:\\t{}'.format(X.shape))","fe206051":"# Set topic number\nnumberTopics = 20\nprint('Number of topics:\\t{}'.format(numberTopics))","8e44b4c8":"# Create the model\nmodel_sklearn = LatentDirichletAllocation(n_components=numberTopics, \n                                          learning_method='online',\n                                          n_jobs=16,\n                                          max_iter = 1,\n                                          total_samples = 10000,\n                                          batch_size = 20)\n\nperplexity_sklearn = []\ntimestamps_sklearn = []\nstart = time()\nfor _ in tqdm(range(100)):\n    model_sklearn.partial_fit(X)\n    # Append the models metric\n    perplexity_sklearn.append(model_sklearn.perplexity(X))\n    timestamps_sklearn.append(time()-start)\n    \n# Plot the topics\nfor i, topic in enumerate(model_sklearn.components_.argsort(axis=1)[:, -10:][:, ::-1], 1):\n    print('Topic {}: {}'.format(i, ' '.join([vocabulary[id] for id in topic])))","47ea2096":"# Create the model\nmodel_gensim = LdaModel(num_topics=numberTopics,\n                        id2word=dictionary,\n                        iterations=10,\n                        passes=1,\n                        chunksize=50,\n                        alpha='auto',\n                        eta='auto',\n                        update_every=1)\n\n\nperplexity_gensim = []\ntimestamps_gensim = []\nstart = time()\nfor _ in tqdm(range(100)):\n    # Online update of the model\n    model_gensim.update(corpus)\n    # To compare sklearn and gensim the perplexity has to be transformed by np.exp(-1*x)\n    perplexity_gensim.append(np.exp(-1 * model_gensim.log_perplexity(corpus)))\n    timestamps_gensim.append(time() - start)\n    \n    \n    \n# Plot the topics\nfor i, topic in enumerate(model_gensim.get_topics().argsort(axis=1)[:, -10:][:, ::-1], 1):\n    print('Topic {}: {}'.format(i, ' '.join([vocabulary[id] for id in topic])))","aba3c58c":"plt.figure(figsize=(15,8))\nplt.plot(timestamps_sklearn, perplexity_sklearn, '-o', label='sklearn', c='g')\nplt.plot(timestamps_gensim, perplexity_gensim, '-o', label='gensim', c='b')\nplt.title('Perplexity sklearn & gensim')\nplt.xlabel('Duration [s]')\nplt.ylabel('Perplexity')\nplt.legend()\nplt.show()","7b9c4931":"## Preprocess Data","681c12fe":"## Load Data\n\nA dataset of comments on articles from the [New York Times](https:\/\/www.kaggle.com\/aashita\/nyt-comments\/home) will be used. To reduce the computation time only a subset of comments will be used for the LDA.","a723365c":"## Conclusion\n\nBoth libraries were successful and differ in their capabilities. While sklearn only supports multicore processing gensim enables the user to employ GPUs even on distributed systems.    \nIt has to be mentioned that sklearn seems to have a pretty good algorithm for convergence while gensim needs to be optimised and regulated properly.","c9268d70":"# Compare LDA (Topic Modeling) In [Sklearn](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.LatentDirichletAllocation.html) And [Gensim](https:\/\/radimrehurek.com\/gensim\/models\/ldamodel.html)\n\nIn this notebook I will compare the implementation of [Latent Dirichlet Allocation](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation) in the libraries sklearn and gensim. \n\n## Import Libraries","ae39609a":"### sklearn LDA","4e782a72":"## Compare Results","5db0b9ee":"### gensim LDA","a498c763":"## Compute LDAs"}}