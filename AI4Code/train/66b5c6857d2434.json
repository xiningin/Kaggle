{"cell_type":{"726919be":"code","aa2149bb":"code","017a695d":"code","9beea1be":"code","e17449da":"code","b17b6fd0":"code","30fdf924":"code","247919d7":"code","0f23ea19":"code","af24d14b":"code","50225dea":"code","ddbae298":"code","1fb5446b":"code","02823919":"code","97080beb":"code","db93a3f2":"code","30246df0":"code","1cda1033":"code","cd0774ae":"code","dd52bb75":"code","e5666d54":"code","3435daae":"code","32aaf524":"code","fdad0195":"code","9c9818f5":"code","682b1c58":"code","520f0e49":"code","c3b04947":"code","a1498e7b":"code","08ac27d8":"code","e626ef05":"code","d9f843bb":"code","17d87be4":"code","b5c9e4f7":"code","f16847e8":"code","1220bd9a":"code","72d97772":"code","1c8228c3":"code","32f7b4fa":"markdown","12cddfe7":"markdown"},"source":{"726919be":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","aa2149bb":"df = pd.read_csv(\"..\/input\/Amazon_Unlocked_Mobile.csv\")","017a695d":"df.head()\n# Review votes in the table indicates number of people found the review helpful","9beea1be":"df.info()","e17449da":"df.describe()","b17b6fd0":"df['Brand Name'].value_counts().head()","30fdf924":"df.dropna(inplace=True) # drop any rows with missing values","247919d7":"# assuming rating with 3 are neutral reviews\n# so drop rows with rating = 3 (by chosing all the rows with rating!=3)\n\ndf = df[df['Rating']!=3]","0f23ea19":"# assuming rating with greater than 3 are rated as postive\n# so we assign 1 to Positively rated and 0 to those are not\n# if Rating > 3, then 'Positively Rated' = 1, else 'Positively Rated' = 0\n\ndf['Positively Rated'] = np.where(df['Rating']>3, 1, 0) ","af24d14b":"df.head()","50225dea":"df['Positively Rated'].mean()","ddbae298":"X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],\n                                                    df['Positively Rated'],\n                                                    random_state=0)","1fb5446b":"y_train[0], X_train[0]","02823919":"X_train.shape","97080beb":"# We'll need to convert text into a numeric so that scikit-learn can use\n# The bag-of-words approach ignores structure and only counts how often each word occurs\n# CountVectorizer use the bag-of-words by converting text into a matrix of token counts.","db93a3f2":"# First, we instantiate the CountVectorizer and fit it to our training data.\n\n# Fitting the CountVectorizer consists of the \n#     tokenization of the trained data and \n#     building of the vocabulary\n\n# Fitting the CountVectorizer \n#     tokenizes each document by finding \n#         all sequences of characters of \n#             at least two letters or \n#             numbers separated by word boundaries. \n# Converts everything to \n#     lowercase and \n#     builds a vocabulary using these tokens.","30246df0":"vect = CountVectorizer().fit(X_train)\nvect","1cda1033":"len(vect.get_feature_names())","cd0774ae":"vect.get_feature_names()[0:10]","dd52bb75":"vect.get_feature_names()[::4000]","e5666d54":"# We use transform method to transform X_train to a document term matrix\n# giving us the bag-of-word representation of X_train\n\n# This representation is stored in a SciPy sparse matrix where \n#     each row corresponds to a document and \n#     each column a word from our training vocabulary.\n\n# The entries in this matrix are the number of times each word appears in each document.\n\n# Because the number of words in the vocabulary is so much larger \n# than the number of words that might appear in a single review, \n# most entries of this matrix are zero.\n\n# and the shape will be \n#     number of document\/rows(here in dataframe)\/reviews(in this case) *\n#     number of words in the vocabulary\/tokens","3435daae":"# Here's a trivial example ... Let's suppose we have 3 documents:\n\n#     Doc1: Hello, World, the sun is shining\n#     Doc2: Hello world, the weather is nice\n#     Doc3: Hello world, the wind is cold\n\n\n# Then, our vocabulary would look like this (using 1-grams without stop word removal):\n\n#     Vocabulary: [hello, world, the, wind, weather, sun, is, shining, nice, cold]\n\n\n# The corresponding, binary feature vectors are:\n\n#     Doc1: [1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n#     Doc2: [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n#     Doc3: [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n\n\n# Which we use to construct the dense matrix \/ document term matrix:\n\n#     [[1, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n#      [1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n#      [1, 1, 1, 1, 0, 0, 1, 0, 0, 1]]","32aaf524":"X_train_vectorized = vect.transform(X_train)\nX_train_vectorized","fdad0195":"X_train_vectorized.shape","9c9818f5":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)","682b1c58":"pred = model.predict(vect.transform(X_test))\nprint('roc accuracy score ', roc_auc_score(y_test, pred)\n\n# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","520f0e49":"print(model.predict(vect.transform(['not an issue, phone is working', \n                                    'an issue, phone is not working'])))","c3b04947":"# Tf\u2013idf, or Term frequency-inverse document frequency\n# allows us to weight terms based on how important they are to a document.\n# high weight is given to terms that appear often in a particular document, \n# but don't appear often in the corpus. \n\n# Features with low tf\u2013idf are either commonly used across all documents \n# or rarely used and only occur in long documents.\n\n# Features with high tf\u2013idf are frequently used within specific documents, \n# but rarely used across all documents.","a1498e7b":"# Similar to how we used CountVectorizer, \n# we'll instantiate the tf\u2013idf vectorizer and fit it to our training data.\n\n# mindf, which allows us to specify a minimum number of documents \n# in which a token needs to appear to become part of the vocabulary","08ac27d8":"vect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","e626ef05":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npred = model.predict(vect.transform(X_test))\n\nroc_auc_score(y_test, pred)","d9f843bb":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","17d87be4":"print(model.predict(vect.transform(['not an issue, phone is working', \n                                    'an issue, phone is not working'])))","b5c9e4f7":"# One way we can add some context is by adding sequences of word features known as n-grams. \n\n# For example, bigrams, which count pairs of adjacent words, \n# could give us features such as is working versus not working. \n# And trigrams, which give us triplets of adjacent words, \n# could give us features such as not an issue.\n\n# To create these n-gram features, \n# we'll pass in a tuple to the parameter ngram_range, \n# where the values correspond to the minimum length and maximum lengths of sequences.\n\n# For example, if I pass in the tuple, 1, 2, \n# CountVectorizer will create features using the individual words, \n# as well as the bigrams.","f16847e8":"vect = TfidfVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nlen(vect.get_feature_names())","1220bd9a":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npred = model.predict(vect.transform(X_test))\n\nroc_auc_score(y_test, pred)","72d97772":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))","1c8228c3":"print(model.predict(vect.transform(['not an issue, phone is working', \n                                    'an issue, phone is not working'])))","32f7b4fa":"# n-gram","12cddfe7":"# TF IDF"}}