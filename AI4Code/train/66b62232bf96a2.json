{"cell_type":{"23ba57e1":"code","1d89801b":"code","b9d16a03":"code","247cede1":"code","39d46fa3":"code","a4b73506":"code","48f7e642":"code","85ce60ef":"code","3fe92da9":"code","57adcb76":"code","9e7cebae":"code","f2c496eb":"code","118699fe":"code","2cb3e156":"code","41323260":"code","0dcf9a06":"code","4786eb7a":"code","584cb68b":"code","ca02a8fd":"code","7b35e574":"code","aa8549d1":"code","831bed6d":"code","7468f104":"code","413ef8ee":"code","01be494a":"code","b1b51e04":"code","767b47f6":"code","3fa927aa":"code","9cf3e064":"code","f265ea15":"code","85100ec2":"code","4d85fe9f":"markdown","507e745d":"markdown","0e34245f":"markdown","6a3af512":"markdown","81e53c2b":"markdown","6345d897":"markdown","68054994":"markdown","b7840603":"markdown","6aa62edc":"markdown","3c1e44e3":"markdown","36b805a6":"markdown","757ec749":"markdown","05a541cb":"markdown"},"source":{"23ba57e1":"!pip install PySastrawi","1d89801b":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nimport numpy as np\nimport pandas as pd\nimport warnings\nimport random\nimport torch\nimport os\nimport time\nwarnings.filterwarnings('ignore')\nimport sys, re, csv, codecs\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n\nRANDOM_STATE = 1\ndef get_cv():\n    return RepeatedStratifiedKFold(\n        n_splits=10,\n        n_repeats=10,\n        random_state=RANDOM_STATE\n    )\n\n\ndef set_seed(seed=1):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    \nset_seed(seed=RANDOM_STATE)\nembed_size = 100 # Word2Vec embedding dimension\nDIR_DATA_A = \"..\/input\/ukara-test-phase\/\"\nDIR_DATA_B = \"..\/input\/ukara-test-phase\/\"\nDIR_DATA_MISC = \"..\/input\/word2vec-100-indonesian\" # directory to word2vec\nDIR_DATA_FINAL = \".\"","b9d16a03":"import nltk\nimport itertools\nimport re\ndef normalizing_words(review):\n    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(review))\n\ndef preprocess(text):\n    text = text.strip()\n    text = text.lower()\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n    text = re.sub(' +', ' ', text).strip()\n    return text","247cede1":"data_A_train = pd.read_csv(\"{}\/data_train_A.csv\".format(DIR_DATA_A))\ndata_A_dev = pd.read_csv(\"{}\/data_dev_A.csv\".format(DIR_DATA_A))\ndata_A_test = pd.read_csv(\"{}\/data_test_A.csv\".format(DIR_DATA_A))\n\ndata_B_train = pd.read_csv(\"{}\/data_train_B.csv\".format(DIR_DATA_B))\ndata_B_dev = pd.read_csv(\"{}\/data_dev_B.csv\".format(DIR_DATA_B))\ndata_B_test = pd.read_csv(\"{}\/data_test_B.csv\".format(DIR_DATA_B))\n\ndata_A_train['RESPONSE'] = data_A_train['RESPONSE'].apply(lambda x: preprocess(x))\ndata_A_dev['RESPONSE'] = data_A_dev['RESPONSE'].apply(lambda x: preprocess(x))\ndata_A_test['RESPONSE'] = data_A_test['RESPONSE'].apply(lambda x: preprocess(str(x)))\n\nstart_preprocess1 = time.time()\ndata_B_train['RESPONSE'] = data_B_train['RESPONSE'].apply(lambda x: preprocess(x))\ndata_B_dev['RESPONSE'] = data_B_dev['RESPONSE'].apply(lambda x: preprocess(x))\ndata_B_test['RESPONSE'] = data_B_test['RESPONSE'].apply(lambda x: preprocess(str(x)))\nend_preprocess1 = time.time()\n\nstimulus_a = [\"Pemanasan global terjadi karena peningkatan produksi karbon dioksida yang dihasilkan oleh pembakaran fosil dan konsumsi bahan bakar yang tinggi.\",\n\"Salah satu akibat adalah mencairnya es abadi di kutub utara dan selatan yang menimbulkan naiknya ketinggian air laut.\",\n\"kenaikan air laut akan terjadi terus menerus meskipun dalam hitungan centimeter akan mengakibatkan perubahan yang signifikan.\",\n\"Film \u201cWaterworld\u201d, adalah film fiksi ilmiah yang menunjukkan akibat adanya pemanasan global yang sangat besar sehingga menyebabkan bumi menjadi tertutup oleh lautan.\",\n\"Negara-negara dan daratan yang dulunya kering menjadi tengelamn karena terjadi kenaikan permukaan air laut.\",\n\"Penduduk yang dulunya bisa berkehidupan bebas menjadi terpaksa mengungsi ke daratan yang lebih tinggi atau tinggal diatas air.\",\n\"Apa yang akan menjadi tantangan bagi suatu penduduk ketika terjadi situasi daratan tidak dapat ditinggali kembali karena tengelam oleh naiknya air laut.\"]\n\nstimulus_b = [\"Sebuah toko baju berkonsep self-service menawarkan promosi dua buah baju bertema tahun baru seharga Rp50.000,00. sebelum baju bertema tahun baru dibagikan kepada pembeli, sebuah layar akan menampilkan tampilan gambar yang menampilkan kondisi kerja di dalam sebuah pabrik konveksi\/pembuatan baju. \",\n\"Kemudian pembeli diberi program pilihan untuk menyelesaikan pembeliannya atau menyumpangkan Rp50.000,00 untuk dijadikan donasi pembagian baju musim dingin di suatu daerah yang membutuhkan.\",\n\"Delapan dari sepuluh pembeli memilih untuk memberikan donasi.\",\n\"Menurut anda mengapa banyak dari pembeli yang memilih berdonasi?\"]\n\ndata_stimulus = []\n\nfor text in stimulus_a:\n    data_stimulus.append(preprocess(text))\n    \nfor text in stimulus_b:\n    data_stimulus.append(preprocess(text))\n    \ndata_stimulus.extend(data_A_train['RESPONSE'].values)\ndata_stimulus.extend(data_A_dev['RESPONSE'].values)\ndata_stimulus.extend(data_A_test['RESPONSE'].values)\ndata_stimulus.extend(data_B_train['RESPONSE'].values)\ndata_stimulus.extend(data_B_dev['RESPONSE'].values)\ndata_stimulus.extend(data_B_test['RESPONSE'].values)","39d46fa3":"print(len(data_stimulus))\ndata_stimulus[0:3]","a4b73506":"unique_string = set()\nfor x in data_stimulus:\n    for y in x.split():\n        unique_string.add(y)\n        \nprint(len(unique_string))","48f7e642":"len_data = [len(x.split()) for x in data_stimulus]\nprint(np.mean(len_data))\nprint(np.median(len_data))\nprint(np.std(len_data))\nprint(np.min(len_data))\nprint(np.max(len_data))\nprint(np.percentile(len_data, 98))","85ce60ef":"max_features = 3000 # how many unique words to use (since the total of unique word is only 2816)\nmaxlen = 43 # max number of words in a text to use (from 90th percentile)\n\n\nstart_preprocess2 = time.time()\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(data_stimulus)\nlist_tokenized_train = tokenizer.texts_to_sequences(data_B_train[\"RESPONSE\"].values)\nlist_tokenized_dev = tokenizer.texts_to_sequences(data_B_dev[\"RESPONSE\"].values)\nlist_tokenized_test = tokenizer.texts_to_sequences(data_B_test[\"RESPONSE\"].values)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_dev = pad_sequences(list_tokenized_dev, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\nend_preprocess2 = time.time()\n","3fe92da9":"X_t[0]","57adcb76":"import gensim\npath = '{}\/idwiki_word2vec_100.model'.format(DIR_DATA_MISC)\nid_w2v = gensim.models.word2vec.Word2Vec.load(path)\nprint(id_w2v.most_similar('makan'))","9e7cebae":"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()","f2c496eb":"index2word_set = set(id_w2v.wv.index2word)","118699fe":"start_preprocess3 = time.time()\n\ntotal_known_word = 0\ntotal_unknown_word = 0\ndict_known_word = {}\ndict_unknown_word = {}\n\nword_index = tokenizer.word_index\nnb_words = max_features\nembedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\nunknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\nfor word, i in word_index.items():\n    cur = word\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        \n        if cur in dict_known_word:\n            dict_known_word[cur] += 1\n        else:\n            dict_known_word[cur] = 1\n        \n        total_known_word += 1\n        continue\n        \n    cur = stemmer.stem(word)\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        \n        if cur in dict_known_word:\n            dict_known_word[cur] += 1\n        else:\n            dict_known_word[cur] = 1\n        \n        total_known_word += 1\n        continue\n    \n    cur = normalizing_words(word)\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        \n        if cur in dict_known_word:\n            dict_known_word[cur] += 1\n        else:\n            dict_known_word[cur] = 1\n            \n        total_known_word += 1\n        continue\n        \n    cur = stemmer.stem(cur)\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        \n        if cur in dict_known_word:\n            dict_known_word[cur] += 1\n        else:\n            dict_known_word[cur] = 1\n            \n        total_known_word += 1\n        continue\n    \n    embedding_matrix[i] = unknown_vector\n    if cur in dict_unknown_word:\n        dict_unknown_word[cur] += 1\n    else:\n        dict_unknown_word[cur] = 1\n        \n    total_unknown_word += 1\n    \nend_preprocess3 = time.time()","2cb3e156":"print(total_unknown_word)\nprint(total_known_word)\nimport operator\nsorted_known = sorted(dict_known_word.items(), key=operator.itemgetter(1), reverse=True)\nprint(sorted_known[0:3])\n\nwith open('word_frequency_known.txt', 'w') as f:\n    for item in sorted_known:\n        f.write('{} {}\\n'.format(item[0], item[1]))\n        \nsorted_unknown = sorted(dict_unknown_word.items(), key=operator.itemgetter(1), reverse=True)\nprint(sorted_unknown[0:3])\n\nwith open('word_frequency_unknown.txt', 'w') as f:\n    for item in sorted_unknown:\n        f.write('{} {}\\n'.format(item[0], item[1]))","41323260":"from tensorflow.keras import callbacks\n\nfrom tensorflow.keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef get_model():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    return model\n","0dcf9a06":"X = X_t\ny = data_B_train[\"LABEL\"].values\n\nf1_list = np.array([])\nprecision_list = np.array([])\nrecall_list = np.array([])\n\ntraining_time_list = np.array([])\ninference_time_list = np.array([])\n\npred_cv = np.zeros(len(y))\npred_dev = np.zeros(len(X_dev))\npred_test = np.zeros(len(X_te))\n\ncount = 0\n\nfor train_index, test_index in get_cv().split(X, y):\n    \n    \n    count += 1\n    print(count, end='')\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    es = callbacks.EarlyStopping(monitor='val_f1', min_delta=0.0001, patience=8,\n                                             verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n    \n    model = get_model()\n    start_training = time.time()\n    model.fit(X_train, \n             y_train, batch_size=16, epochs=50,\n             validation_data=(X_test, y_test),\n             callbacks=[es, rlr],\n             verbose=0)\n    end_training = time.time()\n    training_time_list = np.append(training_time_list, end_training-start_training)\n    \n    model.save(\"model_{}.h5\".format(count))\n    \n    prediction_test_proba = model.predict(X_test)[:,0]\n    prediction_test = [1 if x>=0.5 else 0 for x in prediction_test_proba]\n    f1_list = np.append(f1_list, f1_score(y_test, prediction_test))\n    precision_list = np.append(precision_list, precision_score(y_test, prediction_test))\n    recall_list = np.append(recall_list, recall_score(y_test, prediction_test))\n    \n    pred_cv[[test_index]] += prediction_test\n    \n    start_test = time.time()\n    pred_test += model.predict(X_te)[:,0]\n    end_test = time.time()\n    inference_time_list = np.append(inference_time_list, end_test-start_test)\n    \n    pred_dev += model.predict(X_dev)[:,0]\n    \n    \n","4786eb7a":"print(\"F1: {} (\u00b1 {})\".format(np.mean(f1_list), np.std(f1_list)))\nprint(\"Precision: {} (\u00b1 {})\".format(np.mean(precision_list), np.std(precision_list)))\nprint(\"Recall: {} (\u00b1 {})\".format(np.mean(recall_list), np.std(recall_list)))","584cb68b":"prediksi_CV = {'RES_ID': data_B_train['RES_ID'],\n                'LABEL': np.array(pred_cv)\/10\n               }\ndf_final_CV = pd.DataFrame(prediksi_CV, columns= ['RES_ID', 'LABEL'])\ndf_final_CV.head()","ca02a8fd":"df_final_CV.to_csv('{}\/df_final_B_cv.csv'.format(DIR_DATA_FINAL), index=False)","7b35e574":"pred_cv = np.array(pred_cv)\/10\npred_cv[0:5]","aa8549d1":"bin_pred_cv = [1 if x>=0.50 else 0 for x in pred_cv]\nbin_pred_cv[0:5]","831bed6d":"f1_score(y, bin_pred_cv)","7468f104":"prediksi_data_B_dev = {'RES_ID': data_B_dev['RES_ID'],\n                       'LABEL': np.array(pred_dev)\/100\n                      }","413ef8ee":"df_final_B_dev = pd.DataFrame(prediksi_data_B_dev, columns= ['RES_ID', 'LABEL'])\ndf_final_B_dev.head()","01be494a":"df_final_B_dev.to_csv('{}\/df_final_B_dev.csv'.format(DIR_DATA_FINAL), index=False)","b1b51e04":"prediksi_data_B = {'RES_ID': data_B_test['RES_ID'],\n                   'LABEL': np.array(pred_test)\/100\n                  }","767b47f6":"df_final_B = pd.DataFrame(prediksi_data_B, columns= ['RES_ID', 'LABEL'])\ndf_final_B.head()","3fa927aa":"df_final_B.to_csv('{}\/df_final_B_test.csv'.format(DIR_DATA_FINAL), index=False)","9cf3e064":"print(\"Preprocessing1 (initial text preprocessing): {} second\".format(end_preprocess1-start_preprocess1))\nprint(\"Preprocessing2 (tokenizing and padding): {} second\".format(end_preprocess2-start_preprocess2))\nprint(\"Preprocessing3 (building embedding): {} second\".format(end_preprocess3-start_preprocess3))\nprint(\"Training time: sum:{}\\tavg:{} (\u00b1 {}) second\".format(np.sum(training_time_list), np.mean(training_time_list), np.std(training_time_list)))\nprint(\"Inference time: sum:{}\\tavg:{} (\u00b1 {}) second\".format(np.sum(inference_time_list), np.mean(inference_time_list), np.std(inference_time_list)))","f265ea15":"!ls '.'","85100ec2":"import subprocess\nfrom ast import literal_eval\n\ndef run(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    print(out.decode('utf-8').strip())\n          \nprint('# CPU')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^model name\"')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^cpu MHz\"')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^cpu cores\"')\n          \nprint('# RAM')\nrun('cat \/proc\/meminfo | egrep \"^MemTotal\"')\n          \nprint('# OS')\nrun('uname -a')","4d85fe9f":"## Preprocess Text","507e745d":"## Find max feature","0e34245f":"## Save Result\n### Cross validation","6a3af512":"## Training","81e53c2b":"## Tokenizing and Padding","6345d897":"# Time Elapsed","68054994":"## Building Embedding","b7840603":"## Initialization\nImporting libraries and setting contant variable","6aa62edc":"# UKARA: Training Bi-LSTM with Word2Vec for Data B\n\nThis notebook produced the result for Data B in my Ukara NLP Challenge submission. For more information, check the repository.  \n\nRepository: [https:\/\/github.com\/ilhamfp\/ukara-1.0-challenge](https:\/\/github.com\/ilhamfp\/ukara-1.0-challenge)","3c1e44e3":"### Dev","36b805a6":"## Find max len","757ec749":"# Hardware Information","05a541cb":"### Test"}}