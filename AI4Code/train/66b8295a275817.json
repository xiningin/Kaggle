{"cell_type":{"c4cea0b2":"code","689c2a49":"code","ed8078cd":"code","9b29297c":"code","c9e10428":"code","39422da2":"code","61c6aef1":"code","1e9b14d4":"code","3aef6233":"code","9373b5fc":"code","ae4a013d":"code","46e40e5e":"code","a19df889":"code","663b8cea":"code","96914f4f":"code","4f6558d0":"code","5108995e":"code","111bf96d":"code","4a6f914a":"code","d5c86c93":"code","c960dc0d":"code","d9bbc179":"code","f5b9f4e7":"code","0ac3c4ea":"code","9f3ac47c":"code","51b1d905":"code","0bd98038":"code","911bdeb0":"code","3c5b6cbf":"code","7d22652c":"code","20f12d91":"code","88b83c69":"code","ab0110df":"code","268678d6":"code","d38bde9b":"code","178b99ea":"code","c76218c5":"code","2d817fa3":"code","a44d2444":"code","16450d7a":"code","45c31426":"code","4f04b0ca":"code","c64b1f54":"code","4470dfc6":"code","2fc5d9b9":"code","e3b8c680":"code","0817e0d9":"code","abd28d2f":"code","39e27f8c":"code","e39bab23":"code","72e92be6":"code","e65734ac":"code","e256120c":"markdown","0ac2be77":"markdown","7eba77c4":"markdown","24fcb9c9":"markdown","db26eef4":"markdown","5dc09d1a":"markdown","93a1de71":"markdown","5882ead2":"markdown","8f8965f8":"markdown","1144dcad":"markdown","d61af6ae":"markdown","240acbf3":"markdown","356ca8d8":"markdown","ee6db74e":"markdown","7198367a":"markdown","bf373123":"markdown","b433bfcf":"markdown","1769e13e":"markdown","95b41b46":"markdown","8f392770":"markdown","837bc429":"markdown","7a224d21":"markdown","e67904d7":"markdown","9bbe99b6":"markdown","baa411dc":"markdown","f7f27b4a":"markdown","3a9a3a99":"markdown","a96366ab":"markdown","5869b87f":"markdown","0ce4d017":"markdown","74ab1788":"markdown","b71a2f0f":"markdown","e0f1a68e":"markdown","37ca659a":"markdown","de48481b":"markdown","b92e03f6":"markdown","09269c76":"markdown","6c029a12":"markdown","40c4e21a":"markdown","58d6d188":"markdown","039d6ca1":"markdown","ed495f28":"markdown","6148e664":"markdown","a7e0247d":"markdown","0f7d6342":"markdown","dcc2838d":"markdown","e35d8177":"markdown","1aad4f1f":"markdown","5f3ea9be":"markdown","29edffef":"markdown","26f1d1fe":"markdown","7bcad72a":"markdown","644aad55":"markdown","709826f1":"markdown","aa622c92":"markdown","ba311e99":"markdown","974a7dc4":"markdown"},"source":{"c4cea0b2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, roc_auc_score, roc_curve, plot_roc_curve, auc, precision_recall_curve","689c2a49":"dataset = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')","ed8078cd":"dataset","9b29297c":"missingno.bar(dataset,color=(0.44, 0.61, 0.74), sort=\"ascending\", figsize=(8,4), fontsize=11)","c9e10428":"#barplot on categorial cols\nn=1\nplt.figure(figsize=[20,30])\nfor col in ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level',\n           'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job',\n           'target']:\n    plt.subplot(6,2,n)\n    sns.countplot(x=col, edgecolor=\"black\", alpha=0.7,data=dataset)\n    sns.despine()\n    plt.title(\"Countplot of {}\".format(col))\n    n=n+1\n#plt.tight_layout()\nplt.show()\n\n#histogram on numeric cols\nn=1\nplt.figure(figsize=[20,12])\nfor col in ['enrollee_id', 'city_development_index', 'training_hours']:\n    plt.subplot(2,2,n)\n    sns.histplot(x=col, edgecolor=\"black\", alpha=0.7,data=dataset)\n    sns.despine()\n    plt.title(\"Histogram of {}\".format(col))\n    n=n+1\n#plt.tight_layout()\nplt.show()","39422da2":"def stackplot(groupbyvariable, stackedvariable, figsize, haslegend):\n    fig, ax = plt.subplots(figsize=figsize)\n    df = dataset.groupby([groupbyvariable])[stackedvariable].value_counts(normalize=True).rename('percentage').reset_index().sort_values(groupbyvariable)\n    labelsofgroupbyvariable = list(df[df[stackedvariable]==df[stackedvariable].unique()[0]][groupbyvariable])\n    labels = list(map(str, labelsofgroupbyvariable))\n    labels = list(map(str, df[groupbyvariable].unique()))\n    legend = df[stackedvariable].unique()\n    previous_bottom = np.zeros(len(labels))\n    \n    for x in legend:\n        extracted_df = df[df[stackedvariable]==x]\n        perofx = extracted_df['percentage']\n        the_bars = ax.bar(labels, perofx.values, bottom=previous_bottom, label=x)\n        previous_bottom = previous_bottom + perofx.values\n        perodx_diff = (perofx.max() - perofx.min())\/perofx.min()\n        if perodx_diff > 0.5:\n            #print('{}: the max and min {}% difference by {} is {}'.format(x, stackedvariable, groupbyvariable, perodx_diff))\n            #turn it on to see detailed percetage difference\n            ax.bar_label(the_bars, label_type='center', labels=list(map(lambda _: x, labels)))\n    ax.set_ylabel('{}% within the same {}'.format(stackedvariable, groupbyvariable))\n    ax.set_xlabel('{}'.format(groupbyvariable))\n    ax.set_title(\"{}% by {}\".format(stackedvariable, groupbyvariable))\n    if haslegend:\n        ax.legend(loc='center left', bbox_to_anchor=(1.04, 1))\n    fig.tight_layout()\n    plt.show()","61c6aef1":"stackplot('target', 'city',(3,20), False)\n#the legend is not shown due to limited size\n#you can set the false to true if you want to see the legend","1e9b14d4":"stackplot('target', 'city_development_index', (3,20), False)\n#the same 'city_development_index' is usually matched with the same 'city', so we can treat 'city_development_index' as categorical feature here.","3aef6233":"stackplot('target', 'gender', (3,5), True)","9373b5fc":"stackplot('target', 'relevent_experience',(5,5), True)","ae4a013d":"stackplot('target', 'experience',(3,20), False)","46e40e5e":"stackplot('target', 'enrolled_university', (5,5), True)","a19df889":"stackplot('target', 'education_level', (5,10), True)","663b8cea":"stackplot('target', 'major_discipline', (5,10), True)","96914f4f":"stackplot('target', 'company_size', (5,10), True)","4f6558d0":"stackplot('target', 'company_type', (5,10), True)","5108995e":"stackplot('target', 'last_new_job', (5,10), True)","111bf96d":"sns.catplot(x=\"target\", y=\"city_development_index\",\n            kind=\"violin\", split=True, data=dataset)","4a6f914a":"sns.catplot(x=\"target\", y=\"training_hours\",\n            kind=\"violin\", split=True, data=dataset)","d5c86c93":"sns.catplot(x=\"target\", y=\"enrollee_id\",\n            kind=\"violin\", split=True, data=dataset)","c960dc0d":"corr=dataset.corr()[\"target\"]\ncorr[np.argsort(corr, axis=0)[:-1]]","d9bbc179":"sns.catplot(x=\"gender\", y=\"city_development_index\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)\n#The area of each violin is set to the same by 'scale=\"area\", scale_hue=False'","f5b9f4e7":"sns.catplot(x=\"gender\", y=\"training_hours\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","0ac3c4ea":"sns.catplot(x=\"gender\", y=\"enrollee_id\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","9f3ac47c":"sns.catplot(x=\"relevent_experience\", y=\"city_development_index\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","51b1d905":"sns.catplot(x=\"relevent_experience\", y=\"training_hours\", hue='target',\n            kind=\"violin\", split=True, data=dataset, scale=\"area\", scale_hue=False)","0bd98038":"sns.catplot(x=\"relevent_experience\", y=\"enrollee_id\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","911bdeb0":"sns.catplot(x=\"enrolled_university\", y=\"city_development_index\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)\n#sns.violinplot(x=\"enrolled_university\", y=\"city_development_index\", hue='target', split=False, data=dataset, scale=\"count\")","3c5b6cbf":"sns.catplot(x=\"enrolled_university\", y=\"training_hours\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","7d22652c":"sns.catplot(x=\"enrolled_university\", y=\"enrollee_id\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","20f12d91":"sns.catplot(x=\"education_level\", y=\"city_development_index\", hue='target',\n            kind=\"violin\", split=False, data=dataset, scale=\"area\", scale_hue=False)","88b83c69":"sns.catplot(x=\"education_level\", y=\"training_hours\", hue='target',\n            kind=\"violin\", split=True, data=dataset, scale=\"area\", scale_hue=False)","ab0110df":"sns.catplot(x=\"education_level\", y=\"enrollee_id\", hue='target',\n            kind=\"violin\", split=True, data=dataset, scale=\"area\", scale_hue=False)","268678d6":"x = dataset.iloc[:, 0:13].copy()#\ny = dataset.iloc[:, -1]","d38bde9b":"import seaborn as sns\nsns.set_style('whitegrid')\nsns.countplot(y, edgecolor = 'black')","178b99ea":"from sklearn.preprocessing import LabelEncoder\nlist_of_lbl_enc=[]\n\nfor col in x[['city','gender', 'relevent_experience', 'enrolled_university', 'education_level',\n             'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']]:\n    lbl_enc = LabelEncoder()\n    #print(col)\n    col_level_series_transformed = pd.Series(\n        lbl_enc.fit_transform(x[col][x[col].notnull()]),\n        index=x[col][x[col].notnull()].index)\n    x[col] = col_level_series_transformed\n    list_of_lbl_enc.append(lbl_enc)\n    #print(col, list(lbl_enc.classes_))\n    #print()\n    #print(x[col].value_counts())\n#print(list_of_lbl_enc)","c76218c5":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1) ","2d817fa3":"from sklearn.impute import KNNImputer\nknn_imputer = KNNImputer(n_neighbors=3) \nx_train = knn_imputer.fit_transform(x_train)\nx_test = knn_imputer.transform(x_test)","a44d2444":"x_train = pd.DataFrame(x_train)\nx_test = pd.DataFrame(x_test)","16450d7a":"#round imputed categorical features\nx_train[[1,3,4,5,6,7,8,9,10,11]] = np.rint(x_train.iloc[:, [1,3,4,5,6,7,8,9,10,11]]).astype(int)\nx_test[[1,3,4,5,6,7,8,9,10,11]] = np.rint(x_test.iloc[:, [1,3,4,5,6,7,8,9,10,11]]).astype(int)","45c31426":"x_train = pd.DataFrame(x_train)\nx_test = pd.DataFrame(x_test)","4f04b0ca":"i=0\nfor col in [1,3,4,5,6,7,8,9,10,11]:\n    lbl_enc = list_of_lbl_enc[i]\n    x_train.iloc[:,col] = lbl_enc.inverse_transform(x_train.iloc[:,col])\n    x_test.iloc[:,col] = lbl_enc.inverse_transform(x_test.iloc[:,col])\n    i=i+1\nprint(x_train)","c64b1f54":"#generate more data\nfrom imblearn.over_sampling import SMOTENC\nsmote_nc = SMOTENC(categorical_features=[1,3,4,5,6,7,8,9,10,11], random_state = 0)\nx_smote, y_smote = smote_nc.fit_resample(x_train,y_train)\nsns.countplot(y_smote, edgecolor = 'black')","4470dfc6":"x_train = x_smote\ny_train = y_smote","2fc5d9b9":"from catboost import CatBoostClassifier, Pool\ncat_features = [1,3,4,5,6,7,8,9,10,11]\nclassifier_catboost = CatBoostClassifier(iterations=1000, use_best_model=True, eval_metric='AUC')\neval_dataset = Pool(x_test,\n                    y_test,cat_features)\nclassifier_catboost.fit(x_train,\n          y_train,cat_features,\n          eval_set=eval_dataset,\n          verbose=True)\nprint(classifier_catboost.get_best_iteration())","e3b8c680":"def pipeline(params, param_names, x_train, y_train, x_test, y_test):\n    # convert params to dictionary\n    params = dict(zip(param_names, params))\n\n    catboost_param_names = ['learning_rate', 'depth', 'l2_leaf_reg', 'random_strength', 'bagging_temperature', 'border_count']\n    catboost_params = {param_name: params[param_name] for param_name in params if param_name in catboost_param_names}\n    knn_params = {param_name: params[param_name] for param_name in params if param_name not in catboost_param_names}\n    \n    # initialize model with current parameters\n    knn_imputer = KNNImputer(**knn_params)\n    classifier_catboost = CatBoostClassifier(iterations=32, verbose=False, **catboost_params)\n    \n    x_train = knn_imputer.fit_transform(x_train)\n    x_test = knn_imputer.transform(x_test)\n    \n    x_train = pd.DataFrame(x_train)\n    x_test = pd.DataFrame(x_test)\n    \n    x_train[[1,3,4,5,6,7,8,9,10,11]] = np.rint(x_train.iloc[:, [1,3,4,5,6,7,8,9,10,11]]).astype(int)\n    x_test[[1,3,4,5,6,7,8,9,10,11]] = np.rint(x_test.iloc[:, [1,3,4,5,6,7,8,9,10,11]]).astype(int)\n        \n    x_train = pd.DataFrame(x_train)\n    x_test = pd.DataFrame(x_test)\n    \n    a=0\n    for col in [1,3,4,5,6,7,8,9,10,11]:\n        lbl_enc = list_of_lbl_enc[a]\n        x_train.iloc[:,col] = lbl_enc.inverse_transform(x_train.iloc[:,col])\n        x_test.iloc[:,col] = lbl_enc.inverse_transform(x_test.iloc[:,col])\n        a=a+1\n\n    smote_nc = SMOTENC(categorical_features=[1,3,4,5,6,7,8,9,10,11], random_state = 0)\n    x_train_smote, y_train_smote = smote_nc.fit_resample(x_train,y_train)\n\n\n    #fit model\n    cat_features = [1,3,4,5,6,7,8,9,10,11]\n    classifier_catboost.fit(x_train_smote,y_train_smote,cat_features)\n\n    y_pred_train = classifier_catboost.predict_proba(x_train)\n    y_pred_train_pos = y_pred_train[:,1]\n    \n    y_pred_test = classifier_catboost.predict_proba(x_test)\n    y_pred_test_pos = y_pred_test[:,1]\n    \n    return y_pred_train_pos, classifier_catboost.predict(x_train), y_pred_test_pos, classifier_catboost.predict(x_test)","0817e0d9":"#Bayesian optimization algorithm need a function they can optimize. \n#Most of the time, it\u2019s about the minimization of this function, like we minimize loss.\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, roc_auc_score, roc_curve, plot_roc_curve, auc, precision_recall_curve\n\ndef optimize(params, param_names, x, y):\n    # initialize stratified k fold\n    kf = StratifiedKFold(n_splits = 5)\n    \n    i = 0\n    \n    # initialize auc scores list\n    auc_scores = []\n    \n    #loop over all folds\n    for index in kf.split(X = x, y = y): \n        #split generate indices to split data into training and test set.\n        train_index, test_index = index[0], index[1]\n        \n        x_train = x.iloc[train_index,:]\n        y_train = y[train_index]\n        \n        x_test = x.iloc[test_index,:]\n        y_test = y[test_index]\n        \n        _, _, y_pred_test_pos, _ = pipeline(params, param_names, x_train, y_train, x_test, y_test)\n        \n        auc = roc_auc_score(y_test, y_pred_test_pos)\n        print(f'Current parameters of fold number {i} -> {params}')\n        print(f'AUC score of test {i} f {auc}')\n\n        i = i+1\n        auc_scores.append(auc)\n        \n    return -1 * np.mean(auc_scores)\n    ","abd28d2f":"#define a parameter space\n# from skopt import space, gp_minimize\n# from functools import partial\n# param_spaces = [space.Real(0.0001, 0.47, name = 'learning_rate'),\n#                 space.Integer(3, 11, name = 'depth'),\n#                 space.Real(0, 10, name = 'l2_leaf_reg'),\n#                 space.Real(-10,10, name = 'random_strength'),\n#                 space.Real(0,2, name = 'bagging_temperature'),\n#                 space.Integer(254, 300, name = 'border_count'),\n#                 space.Integer(1, 10, name = 'n_neighbors'),\n#                 space.Categorical(['uniform', 'distance'], transform='identity', name='weights')\n# ]\n\n# # make a list of param names this has to be same order as the search space inside the main function\n#param_names = ['learning_rate', 'depth', 'l2_leaf_reg', 'random_strength', 'bagging_temperature', 'border_count', 'n_neighbors', 'weights']\n\n# # by using functools partial, i am creating a new function which has same parameters as the optimize function except \n# # for the fact that only one param, i.e. the \"params\" parameter is required. \n# # This is how gp_minimize expects the optimization function to be. \n\n# optimize_function = partial(optimize, param_names = param_names, x = x, y = y)\n# result = gp_minimize(optimize_function, dimensions = param_spaces, n_calls = 20, n_random_starts = 5, verbose = True)","39e27f8c":"best_params = {'learning_rate': 0.26303264462846715, 'depth': 3, 'l2_leaf_reg': 10.0, 'random_strength': -10.0, 'bagging_temperature': 0.1968690337194276, 'border_count': 254, 'n_neighbors': 8, 'weights': 'uniform'}\nparam_names = ['learning_rate', 'depth', 'l2_leaf_reg', 'random_strength', 'bagging_temperature', 'border_count', 'n_neighbors', 'weights']\nresult_x = [best_params[pn] for pn in param_names]","e39bab23":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1) \n\ny_pred_train_pos, y_pred_train, y_pred_test_pos, y_pred_test = pipeline(result_x, param_names, x_train, y_train, x_test, y_test)","72e92be6":"def plot_auc_curve(fpr, tpr, auc, title_prefix=''):\n    plt.figure(figsize = (16,6))\n    plt.plot(fpr,tpr,'b+',linestyle = '-')\n    plt.fill_between(fpr, tpr, alpha = 0.5)\n    plt.ylabel('True Postive Rate')\n    plt.xlabel('False Postive Rate')\n    plt.title(f'{title_prefix}ROC Curve Having AUC = {auc}')","e65734ac":"# Training ROC\nauc_train = roc_auc_score(y_train, y_pred_train_pos)\nfpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train_pos)\nplot_auc_curve(fpr_train, tpr_train, auc_train, 'Training ')\n\n# Testing ROC\nauc_test = roc_auc_score(y_test, y_pred_test_pos)\nfpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_pos)\nplot_auc_curve(fpr_test, tpr_test, auc_test, 'Testing ')\n\n\n# Training confusion matrix\ncm_train = confusion_matrix(y_train, y_pred_train)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm_train)\ndisp.plot()\ndisp.ax_.set_title('Training set. Accuracy score = {:.2%}'.format(accuracy_score(y_train, y_pred_train)))\nplt.show()\n\n# Testing confusion matrix\ncm_test = confusion_matrix(y_test, y_pred_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm_test)\ndisp.plot()\ndisp.ax_.set_title('Testing set. Accuracy score = {:.2%}'.format(accuracy_score(y_test, y_pred_test)))\nplt.show()","e256120c":"# Load Dataset","0ac2be77":"No significant correlation within the same 'last_new_job' when compared to different targets.","7eba77c4":"Feature Description\n\n- enrollee_id: Unique ID for candidate\n- city: City code\n- city_development_index: Developement index of the city (scaled)\n- gender: Gender of candidate\n- relevent_experience: Relevant experience of candidate\n- enrolled_university: Type of University course enrolled if any\n- education_level: Education level of candidate\n- major_discipline: Education major discipline of candidate\n- experience: Candidate total experience in years\n- company_size: No of employees in current employer's company\n- company_type: Type of current employer\n- lastnewjob: Difference in years between previous job and current job\n- training_hours: training hours completed\n- target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","24fcb9c9":"## Impute missing data using KNNImputer","db26eef4":"### Correlations between numerical features and target","5dc09d1a":"# Data Visualization","93a1de71":"### Correlations between categorical features and target","5882ead2":"### education_level, enrollee_id, target","8f8965f8":"> A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, in which all of the plot components correspond to actual datapoints, the violin plot features a kernel density estimation of the underlying distribution.\n\n(See [seaborn violin plot doc](https:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html#seaborn.violinplot)).","1144dcad":"Note not to read into the different distributions among the violins too much, due to the influence of sample size.","d61af6ae":"### education_level, training_hours, target","240acbf3":"The result is shown not overfitted. ","356ca8d8":"## Countplot and Histogram plot of individual feature","ee6db74e":"## Hyperparam Tuning (Bayesian optimization with gaussian process)","7198367a":"## Correlations between features and target","bf373123":"### enrolled_university, enrollee_id, target","b433bfcf":"To generate more data, I will use SMOTENC which takes in categorical features, so the data need to be inverse transformed into categorical data","1769e13e":"No significant correlation within the same gender when compared to different targets.","95b41b46":"The data should be split before doing any KNN imputation or SMOTE, so it doesn't introduce any 'made up' data into the testing set.","8f392770":"### relevent_experience, enrollee_id, target","837bc429":"## Label encode categorical data","7a224d21":"No significant correlation within the same 'major_discipline' when compared to different targets.","e67904d7":"## Split the dataset into training set and testing set","9bbe99b6":"city_development_index that are positively correlated to target 0:\n0.926(city_114), 0.91(city_16), 0.897(city_136), 0.939(city_75), 0.855(city_67), 0.924(city_104), 0.884(city_71)\n\ncity_development_index that are positively correlated to target 1:\n0.55(city_11), 0.579, 0.624(city_21)\n\nIt's easy to tell high CDI is positively related to target0, whereas low CDI is positively related to target1. We can also tell city and CDI are highly correlated. We don't need to get rid of one col as catboost can handle multicollinearity within the features fine.","baa411dc":"For a first takeaway, we can tell from the 'target' countplot that the data is imbalanced. Next let's see if we can find correlations between each feature and 'target'.","f7f27b4a":"I also tried doing some dimensionality reduction including PCA, KPCA, LDA, but they don't seem to help much, so I didn't keep the code.","3a9a3a99":"### relevent_experience, training_hours, target","a96366ab":"Here I tried to measure correlation in data using Correlation coefficients.\n\nCorrelation coefficients are used to measure how strong a relationship is between two variables.  The coefficients are values between -1 and 1, where:\n \n* 1 indicates a strong positive relationship.\n* -1 indicates a strong negative relationship.\n* A result of zero indicates no relationship at all.","5869b87f":"#### Visulize the correlations using Violin Plot","0ce4d017":"### gender, cdi, target","74ab1788":"The correlation coefficients match the violin plots above","b71a2f0f":"### gender, enrollee_id, target","e0f1a68e":"## Missing Data","37ca659a":"Following the Catboost documentation on [number of trees](https:\/\/catboost.ai\/docs\/concepts\/parameter-tuning.html#trees-number)(\"It is recommended to check that there is no obvious underfitting or overfitting before tuning any other parameters. In order to do this it is necessary to analyze the metric value on the validation dataset and select the appropriate number of iterations.\"), I set the eval_metric to 'AUC' to manually find the best iterations.","de48481b":"Let's play with the data a little bit. See if we can get some insights from it.","b92e03f6":"I turned off the training bc it took hours to train, but the hyperparam combo which gives the best test AUC score is : {'learning_rate': 0.26303264462846715, 'depth': 3, 'l2_leaf_reg': 10.0, 'random_strength': -10.0, 'bagging_temperature': 0.1968690337194276, 'border_count': 254, 'n_neighbors': 8, 'weights': 'uniform'}. \n\nI only did n_calls = 20, we can keep going further to set the n_calls to be a bigger number, but I will keep going with the best I got due to limited time.","09269c76":"I made a function to plot the stackplot to visulize correlations between variables, since seaborn doesn't have a stackplot. \n\nThe stackplot controls for the bias introduced by imbalanced data, because it shows groups as percentages of each target label, rather than as raw counts. Keep in mind that we still have a somewhat small sample size for target=1, so we should be cautious to make conclusions especially when interescting with other imbalanced groups (e.g. gender), because the sample size would cumulatively be quite small for some combinations (e.g. target=1 & gender=other).\n\nFor labeling, I set the threshold to a relative difference of 50%(0.5), so that labels for groups with small differences won't clutter up the plot. If you want to see more detail, you can change the if statement in the cell. This filtering will however still label groups which have\na large relative different but very small absolute size, which practically speaking may be noise. To keep the code simple, we won't filter those out, but we can just choose to ignore them when discussing correlations.","6c029a12":"## Visualize correlations between more than one feature and target using Violin Plot","40c4e21a":"No significant correlation within the same 'company_size' when compared to different targets.","58d6d188":"### enrolled_university, training_hours, target","039d6ca1":"People with education level Phd or Primary School are more likely not looking for a job change(target 0)","ed495f28":"experience(in years) that are positively correlated to target 0:\n15, 16, 17, 18, 19, >20\n\nexperience(in years) that are positively correlated to target 1:\n<1, 1, 3\n\nMore experienced people are more likely to not change the job(target 0), whereas less experienced people are more likely to change the job(target 1), which also makes sense.","6148e664":"We can tell the data is very imbalanced here, so we will use SMOTE to generate more data. Before using SMOTE, I want to use KNN to impute missing data. Before doing KNN, we need to label encode the categorical data, as KNN only takes numeric data.","a7e0247d":"cities that are positively correlated to target 0: \ncity_16, city_114, city_136, city_67, city_75, city_104, city_102, city_71\n\ncities that are positively correlated to target 1: \ncity_21, city_11","0f7d6342":"## Generate data using SMOTENC","dcc2838d":"People enrolled in 'full time course' are more likely looking for a job change(target 1).","e35d8177":"### education_level, cdi, target","1aad4f1f":"Turns out the best is 372, so we will use iterations = 372 for the following tuning.","5f3ea9be":"# Training with Catboost and hyperparameter tuning using Bayesian optimization with gaussian process","29edffef":"### gender, training_hours, target","26f1d1fe":"### enrolled_university, cdi, target","7bcad72a":"# Data Preprocessing","644aad55":"### relevent_experience, cdi, target","709826f1":"The chart below shows the percentage of non-missing data within each feature. The missing data percentage is quite significant with the max missing data up to 30%.","aa622c92":"People with 'No relevent experience' are more likely to be target '1'(Looking for a job change). That makes sense because people with no experience want to learn enough skills from the company then apply for a better position elsewhere.","ba311e99":"No significant correlation within the same 'company_type' when compared to different targets.","974a7dc4":"#### Calculate the correlation"}}