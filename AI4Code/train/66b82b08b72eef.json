{"cell_type":{"94b6b98b":"code","098dc7a2":"code","16ccc0ee":"code","dd336206":"code","98fd6647":"code","4a10e61b":"code","a6e812d1":"code","e718c69a":"code","622627af":"code","8f3f2ade":"code","982c9fec":"code","19faa215":"code","39934dff":"code","69a28605":"code","6430ad31":"code","8dd3be2e":"code","74c3231e":"code","97796d1c":"code","651af359":"code","8561a179":"code","3237f882":"code","ec9012c2":"code","277c9f07":"code","b0c72974":"code","b21a22f2":"code","41b23307":"code","6118d88f":"code","53235d7e":"code","81fc1a2e":"code","049740f4":"code","33b15d96":"code","0ada086d":"code","437baf88":"code","fe1f56f0":"code","75340600":"code","62a35466":"code","aaca861d":"code","29edc831":"code","d97852dc":"code","9ff28d10":"code","13c30a48":"code","5ce409ff":"code","80acf3b5":"code","9fc00180":"code","a0253afa":"code","89655686":"code","9cc09f8f":"code","90071e23":"code","45e3cc0b":"code","5a6ec9c6":"code","a457c278":"code","7c26f2ab":"markdown","753ff561":"markdown","18f6139a":"markdown","260d25cb":"markdown","5b472a58":"markdown","48b2b6c2":"markdown","3479beaa":"markdown","14eccc67":"markdown","33b2e5e5":"markdown","fe6f38d9":"markdown","127f2c95":"markdown","e8318603":"markdown","5a8f81e7":"markdown","2d8e4164":"markdown","1087ebdf":"markdown","5711ee37":"markdown","91136019":"markdown","68256ed4":"markdown","38f39733":"markdown","856e22ab":"markdown","894edc73":"markdown","5967db78":"markdown","7d217e4a":"markdown","74f40c39":"markdown","f5f8b3e1":"markdown","0c55433c":"markdown","4246665a":"markdown","eb7f3320":"markdown","5b8221b4":"markdown","c50edac9":"markdown","87a70293":"markdown","38e35bbe":"markdown","6e98df0f":"markdown","b42b58d7":"markdown","36d1ad4c":"markdown","56e478b1":"markdown","f043d3d2":"markdown","8a60a070":"markdown","5a558610":"markdown","0d14a3a3":"markdown"},"source":{"94b6b98b":"# importing important libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n## display all columns and rows\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',None)","098dc7a2":"t=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nt.head()","16ccc0ee":"plt.hist(t.SalePrice)","dd336206":"t['SalePrice']=np.log(t['SalePrice'])\nplt.hist(t.SalePrice)","98fd6647":"t.info()","4a10e61b":"# creating seperate ID dataframe\nt_id=t['Id']\n# setting ID as index\nt.set_index('Id', inplace=True)","a6e812d1":"t_saleprice = pd.DataFrame(t, columns=['SalePrice'])\nt=t.drop(columns=['SalePrice'])","e718c69a":"categorical=t.select_dtypes(include='O')\ncategorical.head()","622627af":"for feature in categorical:\n    sns.catplot(x=feature , data=categorical, kind='count')\n","8f3f2ade":"integervalues=t.select_dtypes(exclude='O')\nintegervalues.head()","982c9fec":"# continous features\ncontinous=[feature for feature in integervalues\n         if len(t[feature].unique()) > 10]\n\ncontinous_num=pd.DataFrame(t, columns=continous)\n\n\n\n# discrete features\ndiscrete=[feature for feature in integervalues \n         if len(t[feature].unique()) <= 10]\ndiscrete_num=pd.DataFrame(t, columns=discrete)","19faa215":"plt.style.use(\"ggplot\")\ncontinous_num.hist(bins=50,figsize=(30,30))\nplt.show()","39934dff":"for feature in discrete_num:\n   \n    sns.catplot(x=feature , data=discrete_num, kind='count')\n   ","69a28605":"print('Categorical Features')\nprint('---------------------')\nprint(categorical.isnull().sum())\nprint('----------------------------------')\nprint('Continous Features')\nprint('-------------------')\nprint(continous_num.isnull().sum())\nprint('------------------------------------')\nprint('Discrete Features')\nprint('-------------------')\nprint(discrete_num.isnull().sum())","6430ad31":"categorical['Alley'].fillna('None', inplace=True)\ncategorical['MasVnrType'].fillna('None', inplace=True)\ncategorical['BsmtQual'].fillna('None', inplace=True)\ncategorical['BsmtCond'].fillna('None', inplace=True)\ncategorical['BsmtExposure'].fillna('None', inplace=True)\ncategorical['BsmtFinType1'].fillna('None', inplace=True)\ncategorical['BsmtFinType2'].fillna('None', inplace=True)\ncategorical['FireplaceQu'].fillna('None', inplace=True)\ncategorical['GarageType'].fillna('None', inplace=True)\ncategorical['GarageFinish'].fillna('None', inplace=True)\ncategorical['GarageQual'].fillna('None', inplace=True)\ncategorical['GarageCond'].fillna('None', inplace=True)\ncategorical['PoolQC'].fillna('None', inplace=True)\ncategorical['Fence'].fillna('None', inplace=True)\ncategorical['MiscFeature'].fillna('None', inplace=True)\n\n# Filling null value in electrical column with the most occuring value in that feature.\n\ncategorical['Electrical'].fillna(t['Electrical'].mode()[0], inplace=True)","8dd3be2e":"## since Masonry veneer area has very few null values we can fill these places with most occuring value.\n\ncontinous_num['MasVnrArea'].fillna(continous_num['MasVnrArea'].mode()[0], inplace=True)\n\n## we can fill the lot frontage and Garage year built with median, since there distribution is not normal\ncontinous_num['LotFrontage'].fillna(continous_num['LotFrontage'].median(), inplace=True)\ncontinous_num['GarageYrBlt'].fillna(continous_num['GarageYrBlt'].median(),inplace=True)","74c3231e":"#  list of names in continous feature\nconti_columns=continous_num.columns.tolist()\n\n# interquartile range function\ndef iqr_trimming(df, cols):\n    \n    drop_outliers = np.array([])\n    \n    for col in cols:\n        \n        q1 = df[col].quantile(0.01)\n        q3 = df[col].quantile(0.99)\n        # If we reduce the IQR range, a large number of rows gets dropped.\n        \n        s = df[col]\n        \n        indexes = s[(s > q3 )| (s < q1)].index\n        \n        drop_outliers = np.append(drop_outliers, indexes)\n    \n    return drop_outliers","97796d1c":"dropped = np.unique(iqr_trimming(continous_num, conti_columns))","651af359":"## using OrdinalEncoder for encoding the Ordinal features with numeric values.\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# the OrdinalEncoder takes the distinct values of the features to assign them numeric values in order.\n\nUtilities=['ELO','NoSeWa','NoSe','NoSewr','AllPub']\nExterQual=['Po','Fa','TA','Gd','Ex']\nExterCond=['Po','Fa','TA','Gd','Ex']\nBsmtQual=['None','Po','Fa','TA','Gd','Ex']\nBsmtCond=['None','Po','Fa','TA','Gd','Ex']\nBsmtExposure=['None','No','Mn','Av','Gd']\nBsmtFinType1=['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ']\nBsmtFinType2=['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ']\nHeatingQC=['Po','Fa','TA','Gd','Ex']\nKitchenQual=['Po','Fa','TA','Gd','Ex']\nFunctional=['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ']\nFireplaceQu=['None','Po','Fa','TA','Gd','Ex']\nGarageFinish=['None','Unf','RFn','Fin']\nGarageQual=['None','Po','Fa','TA','Gd','Ex']\nGarageCond=['None','Po','Fa','TA','Gd','Ex']\nPoolQC=['None','Fa','TA','Gd','Ex']\nFence=['None','MnWw','GdWo','MnPrv','GdPrv']","8561a179":"# Creating object\nordi= OrdinalEncoder(categories=[Utilities,ExterQual,ExterCond,BsmtQual,BsmtCond,BsmtExposure,\n                              BsmtFinType1,BsmtFinType2,HeatingQC,KitchenQual,Functional,FireplaceQu,\n                               GarageFinish,GarageQual,GarageCond, PoolQC,Fence])\n\n# Fitting the values\nordi.fit(categorical[['Utilities','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\n                              'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu',\n                               'GarageFinish','GarageQual','GarageCond','PoolQC','Fence']])\n\n# Transforming the values\nordinalc=ordi.transform(categorical[['Utilities','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\n                              'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu',\n                               'GarageFinish','GarageQual','GarageCond','PoolQC','Fence']])\n\n","3237f882":"# creating dataframe with ordinal features\n\nordinal=pd.DataFrame(ordinalc, columns=['Utilities','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\n                              'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu',\n                               'GarageFinish','GarageQual','GarageCond','PoolQC','Fence'])","ec9012c2":"# To start the index with 1\nordinal.index +=1\nordinal.head()","277c9f07":"nominal=categorical.drop(columns=['Utilities','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\n                              'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu',\n                               'GarageFinish','GarageQual','GarageCond','PoolQC','Fence'])","b0c72974":"# using pandas dummies to treat nominal features\nnominal_dummies=pd.get_dummies(nominal)\nnominal_dummies.head()","b21a22f2":"final_t=pd.concat([continous_num,discrete_num,nominal_dummies,ordinal,t_saleprice['SalePrice']], axis=1)","41b23307":"final_t.drop(dropped, inplace=True)","6118d88f":"final_t.head()","53235d7e":"final_t.shape","81fc1a2e":"correlation=final_t.corrwith(t_saleprice['SalePrice']).sort_values(ascending=False)\nstrong_correlation=correlation[abs(correlation)>= 0.5]\nstrong_correlation","049740f4":"# creating a dataframe of highly correlated features\\\na=['OverallQual','GrLivArea','ExterQual','KitchenQual','GarageCars','YearBuilt','GarageArea','BsmtQual',\n           'GarageFinish','FullBath','TotalBsmtSF','YearRemodAdd','Foundation_PConc','GarageYrBlt','1stFlrSF','TotRmsAbvGrd','SalePrice']\n\ndf=final_t[['OverallQual','GrLivArea','ExterQual','KitchenQual','GarageCars','YearBuilt','GarageArea','BsmtQual',\n           'GarageFinish','FullBath','TotalBsmtSF','YearRemodAdd','Foundation_PConc','GarageYrBlt','1stFlrSF','TotRmsAbvGrd','SalePrice']]\n\ndf.head()","33b15d96":"# using standard Scaler to standardized our dataset\n\nfrom sklearn.preprocessing import StandardScaler\nSS=StandardScaler()\nscaled=SS.fit_transform(df)","0ada086d":"scaled_df = pd.DataFrame(scaled,columns=a)\nscaled_df.head()","437baf88":"# seperating our target feature\n\nx=scaled_df.drop(columns=['SalePrice'])\ny=scaled_df['SalePrice']","fe1f56f0":"## splitting our dataset into training and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test =train_test_split(x,y,test_size=0.3)","75340600":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nmodel=lr.fit(X_train,y_train)\ny_pred=model.predict(X_test)\n","62a35466":"# evaluating our model \nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score","aaca861d":"print('r2_score is: ',r2_score(y_test,y_pred))\nprint('mean squared error is: ',mean_squared_error(y_test,y_pred))\nprint('mean_absolute_error is: ',mean_absolute_error(y_test,y_pred))","29edc831":"from sklearn.tree import DecisionTreeRegressor\ndtr= DecisionTreeRegressor()\ndtr.fit(X_train,y_train)\ny_pred = dtr.predict(X_test)","d97852dc":"from sklearn.model_selection import GridSearchCV","9ff28d10":"param_grid = {\"criterion\": [\"mse\", \"mae\"],\n              \"min_samples_split\": [10, 20, 40],\n              \"max_depth\": [2, 6, 8],\n              \"min_samples_leaf\": [20, 40, 100],\n              \"max_leaf_nodes\": [5, 20, 100],\n              }\n\ngrid_cv_dtm = GridSearchCV(dtr, param_grid, cv=5)\ngrid_cv_dtm.fit(X_train, y_train)","13c30a48":"grid_cv_dtm.best_score_","5ce409ff":"grid_cv_dtm.best_params_","80acf3b5":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor()\nrfr.fit(X_train, y_train)\ny_pred=rfr.predict(X_test)","9fc00180":"from sklearn.model_selection import RandomizedSearchCV","a0253afa":"random_grid = {'n_estimators':[100,300,500,700,1000,1200],\n               'max_features': ['auto','sqrt'],\n               'max_depth': [10,20,30,40,50],\n               'min_samples_split':[2,5,7,10,15,20] ,\n               'min_samples_leaf': [1,2,5,10]}\nrfr_random = RandomizedSearchCV(estimator = rfr, param_distributions = random_grid, cv=5)\nrfr_random.fit(X_train, y_train)","89655686":"rfr_random.best_score_","9cc09f8f":"rfr_random.best_params_","90071e23":"from sklearn.ensemble import GradientBoostingRegressor\ngbr =GradientBoostingRegressor()\ngbr.fit(X_train,y_train)\ny_pred=gbr.predict(X_test)\n","45e3cc0b":"random_gbr = {  'learning_rate': [0.01,0.02,0.1,0.2],\n                'subsample': [0.7,0.8,0.9,1],\n               'n_estimators':[100,300,500,700,1000,1200],\n               'max_features': ['auto','sqrt'],\n               'max_depth': [20,30,40,50,60,70],\n               'min_samples_split':[7,10,15,20] ,\n               'min_samples_leaf': [2,5,10]}\n\ngbr_random=RandomizedSearchCV(estimator = gbr, param_distributions = random_gbr, cv=5)\n\ngbr_random.fit(X_train, y_train)","5a6ec9c6":"gbr_random.best_score_","a457c278":"gbr_random.best_params_","7c26f2ab":"#### 2.2 Loading data","753ff561":"**Categorical**<br>\n\nLooking at tha data description file, we will find that in most of these features which have NaN in the columns simply means\nthat feature is not available.\nwe can fill these null values with **'None'**.","18f6139a":"In this section we will perform the following:\n * Missing Value Imputation\n * Outlier Treatment\n * Feature Transformation\n * Feature Scaling","260d25cb":"##### Nominal features","5b472a58":"***We have deal with categorical and numerical features seperately. Now we will merge all our features***","48b2b6c2":"## 6. Model Evaluation","3479beaa":"## 4. Feature Engineering","14eccc67":"**These are the indexes that will be dropped once we merge all the features.**","33b2e5e5":" For Numerical features having more than 10 unique value, we will consider it as continous features and the rest will be discrete features.","fe6f38d9":"## 3.EDA\nIn this we will see:\n* Target Value\n* Basic Data information\n* Missing Values\n* Outliers\n* Relation between target value and features","127f2c95":"### 7.2 Decision Tree Model","e8318603":"## 5. Feature Selection","5a8f81e7":"## 1. Introduction\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nThe *Ames Housing dataset* was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n<br><br>\n\n**Problem Statement**<br>\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa,we have to predict the final price of each home.","2d8e4164":"##### continous features","1087ebdf":"### 4.2 Outlier Treatment","5711ee37":"**we have 80 features including Id**<br>\n In features we can have the following types of data.\n \n 1. Categorical features\n     \n     i. **Nominal** - Nominal categories are those data which do not have any order among them. Ex-colours (red, green, blue, etc.)<br>\n     ii. **Ordinal** - Ordinal categories are those which have orders among them. Ex- rank(first, second, third and so on)\n\n\n 2. Numerical features\n \n     i. **Continous** - Continous values are values that represnt the quantity or measurement of that feature. Ex- weight of a person.<br>\n     ii. **Discrete** - Numerical features may contain discrete values which have some order among them.<br>\n     \n\n#### We will seperate all these features and deal with them seperately.","91136019":" We will seperate the nominal and ordinal categories by looking at data description provided to us.","68256ed4":"We will remove the outliers in continous feature","38f39733":"### 4.1 Missing Value Imputation\n","856e22ab":"Most of the model cannot interpret the string value (Categorical). Features which contain string value needs to be converted into numerical value.<br>\nAs we have seen earlier, categorical features can be either Nominal or Ordinal. And we will transform them separately.\n","894edc73":"#### Numerical Features","5967db78":"**Lets have a look at the sales price varable**","7d217e4a":"***Hyperparameter tunning fro random forest model***","74f40c39":"We will create seperate dataframe for sales price and drop it from the main data","f5f8b3e1":"#### Categorical Features","0c55433c":"### 4.3 Feature Transformation","4246665a":"###### Hyperparameter tunning for decision tree","eb7f3320":" ### 7.1 Linear Regression Model","5b8221b4":"##### Ordinal features\nWe need to look at the description file to know which feature is Ordinal.","c50edac9":"#### Missing Values","87a70293":"## 2. Data Loading\n#### 2.1 Importing Libraries","38e35bbe":"**Numerical**<br>\n","6e98df0f":"# Ames house prices prediction\n\n#### In this notebook we will perform Linear regression, Decision Tree, Random forest and Gradient Boosting Machine with Hyperparameter Tuning.\n#### Basic steps:\n  1. Introduction\n  2. Data Loading\n  3. Exploratory data analysis (EDA)\n  4. Feature Engineering\n  5. Feature Selection\n  6. Model Evaluation\n  7. Model Building\n  8. Evaluation Metrics","b42b58d7":"But first we will set the Id column as index so that when we merge all the features the integrity of the data is not compromised.","36d1ad4c":"### 7.3 Random Forest Model","56e478b1":"## 7. Model Building & Evaluation","f043d3d2":"The target variable is right skewed. Linear models perform well when data is **normally distributed**. So we can transform this variable and make it more normally distributed.","8a60a070":"Before we use these features to build our model these features need to be scaled first","5a558610":"##### Discrete Features","0d14a3a3":"### 7.4 Gradient boosting "}}