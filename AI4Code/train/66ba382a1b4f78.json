{"cell_type":{"db4fb33b":"code","1d4cfa2d":"code","d0616d14":"code","e0ecefc3":"code","429b5055":"code","231db715":"code","9ef1a06a":"code","a40a1210":"code","c9be04ab":"code","c2316bc5":"code","c44e3ed9":"code","756289f4":"code","e8b9eeb3":"code","2b112cb8":"code","cac3ff83":"code","2b509e3d":"code","6be13319":"code","ee9ca68f":"code","81f2310f":"code","938c8885":"code","e928e116":"code","b7d6bb8c":"code","0e633fec":"code","e6d0fe88":"code","61e2bef9":"code","43dd589b":"code","5d6b3ea3":"code","b510f22a":"code","9d42d76f":"code","00b68658":"code","1b00b41d":"code","89eaf784":"code","22c427f1":"code","a5da0676":"code","1e021721":"code","228f77ed":"code","7c9130b0":"code","628e0bc2":"code","cfd7a014":"code","fa7dd331":"code","274bd7f5":"code","ebaa3ca7":"code","7942b919":"code","0a27ff04":"code","2fb4c444":"code","4179a420":"code","fac8e00c":"code","20c20efb":"code","fbed3457":"code","9f2ea433":"code","760620eb":"code","76173393":"code","0799ca3e":"code","74f9b4b0":"code","62a6cb84":"code","ab280485":"code","fcd6e873":"code","da0688cb":"code","3ffee752":"markdown","aeffc375":"markdown","68ef2adf":"markdown","474d036d":"markdown","cd141fce":"markdown","7cd35167":"markdown","e6fce694":"markdown","70bed604":"markdown","27a1f7e8":"markdown","0ab44c0f":"markdown","d4873db2":"markdown","1147f90b":"markdown","9dac2060":"markdown","6070f5f7":"markdown","68a1e322":"markdown","6cdc5f53":"markdown","641669a0":"markdown","6512412b":"markdown","ddafb5ae":"markdown","f8c767e9":"markdown","9097ad89":"markdown","c65f0960":"markdown","d31a5797":"markdown","d51f487a":"markdown","d6ec55c7":"markdown","82e784f0":"markdown","5ba56820":"markdown","3c41280d":"markdown","eebdab1d":"markdown","3934b78c":"markdown"},"source":{"db4fb33b":"!conda install -c conda-forge pydotplus -y\n!conda install -c conda-forge python-graphviz -y","1d4cfa2d":"!pip install --upgrade scikit-learn==0.20.3","d0616d14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\nimport matplotlib.image as mpimg\n%matplotlib inline \n\nimport itertools\n\n\n#EVALUATION ALGORITHMS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn import metrics\n# from sklearn.metrics import jaccard_score\nfrom sklearn.externals.six import StringIO\nfrom sklearn import tree\n\nimport pydotplus\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0ecefc3":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","429b5055":"df.head()","231db715":"df.shape","9ef1a06a":"df.info()","a40a1210":"df.describe()","c9be04ab":"# SHAPE OF FEATURE DATASET\n\ndf.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\nY = df.diagnosis\nX = df.drop('diagnosis', axis=1)\nX.shape","c2316bc5":"#DATA STANDARDIZATION\nX_std = (X - X.mean()) \/ (X.std())","c44e3ed9":"#VISUALIZE NUMBER OF BENIGN AND MALIGNANT CASES\n\nsns.countplot(df['diagnosis'], label='Count')\nplt.show()\n\nB, M = df['diagnosis'].value_counts()\nprint('Benign: ',B)\nprint('Malignant : ',M)","756289f4":"# SPLIT DATASET INTO TWO SETS OF 15 FEATURES EACH\n\ndf_set1 = pd.concat([Y, X_std.iloc[:, 0:15]], axis=1)\ndf_set2 = pd.concat([Y, X_std.iloc[:, 15:30]], axis=1)\n\n\n# TRANSFORM DATA INTO 3 COLUMN DATA FRAME W\/ ALL FEATURES IN ONE COLUMN\n\ndf_melt1 = pd.melt(df_set1, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\ndf_melt2 = pd.melt(df_set2, id_vars=\"diagnosis\", var_name=\"features\", value_name='value')\ndf_melt1.head()","e8b9eeb3":"# VIOLIN PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nplt.figure(figsize=(15,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt1, split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","2b112cb8":"# SWARM PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nsns.set(style='whitegrid', palette='muted')\n\nplt.figure(figsize=(15,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt1)\nplt.xticks(rotation=90)\nplt.show()","cac3ff83":"# VIOLIN PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nplt.figure(figsize=(15,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt2, split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","2b509e3d":"# SWARM PLOT TO VISUALIZE BENIGN AND MALIGNANT FEATURE CORRELATIONS\n\nsns.set(style='whitegrid', palette='muted')\n\nplt.figure(figsize=(15,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df_melt2)\nplt.xticks(rotation=90)\nplt.show()","6be13319":"# CREATE HEATMAP TO VISUALIZE DATA CORRELATIONS\n\nplt.figure(figsize=(16,16))\nsns.heatmap(df.corr(), cbar = True,  square = True, annot=True, fmt= '.1f', annot_kws={'size': 12},\n           xticklabels=X.columns, yticklabels=X.columns,\n           cmap= 'YlGnBu')\nplt.title('FEATURE VARIABLE CORRELATIONS')\nplt.show()","ee9ca68f":"# CREATE NEW FEATURE SET \n\nfeatures = ['area_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean',\n        'area_se', 'texture_se', 'smoothness_se', 'compactness_se', 'symmetry_se', 'fractal_dimension_se',\n        'area_worst', 'texture_worst', 'smoothness_worst', 'compactness_worst', 'symmetry_worst', 'fractal_dimension_worst']\nX_1 = X[features]\n","81f2310f":"from sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\nimport itertools","938c8885":"X_train, X_test, y_train, y_test = train_test_split(X_1, Y, test_size=0.3, random_state=1)\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\nyhat_rf = clf_rf.predict(X_test)\nyhat_proba_rf = clf_rf.predict_proba(X_test)\n\nac_rf = accuracy_score(y_test, yhat_rf)\nprint('Accuracy Score: ', ac_rf)\n\ncm_rf = confusion_matrix(y_test, yhat_rf)\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Random Forest')\n\nplt.show()","e928e116":"f1_rf = f1_score(y_test, yhat_rf, average='weighted') \nprint('F1 Score: ', f1_rf)","b7d6bb8c":"log_loss_rf = log_loss(y_test, yhat_proba_rf)\nlog_loss_rf","0e633fec":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","e6d0fe88":"y_train","61e2bef9":"select_features = SelectKBest(chi2, k=9).fit(X_train, y_train)\nX_train.columns[select_features.get_support()]","43dd589b":"feature_scores = pd.DataFrame(X_train.columns, columns=['Features'])\nfeature_scores['scores'] = select_features.scores_\nfeature_scores = feature_scores.sort_values(by='scores', ascending=False)","5d6b3ea3":"X_train_2 = select_features.transform(X_train)\nX_test_2 = select_features.transform(X_test)\n\nclf_rf2 = RandomForestClassifier()\nclf_rf2.fit(X_train_2, y_train)\nyhat_rf2 = clf_rf2.predict(X_test_2)\nyhat_proba_rf2 = clf_rf2.predict_proba(X_test_2)\n\nac_rf2 = accuracy_score(y_test, yhat_rf2)\nprint('Accuracy Score: ', ac_rf2)\n\ncm_rf_kbest = confusion_matrix(y_test, yhat_rf2)\nsns.heatmap(cm_rf_kbest, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Random Forest 2')\nplt.show()","b510f22a":"f1_rf2 = f1_score(y_test, yhat_rf2, average='weighted')\nprint('F1 Score: ', f1_rf2)","9d42d76f":"log_loss_rf2 = log_loss(y_test, yhat_proba_rf2)\nlog_loss_rf2","00b68658":"clf_svm = svm.SVC(kernel='rbf', probability=True)\nclf_svm.fit(X_train, y_train)\nyhat_svm = clf_svm.predict(X_test)\nyhat_proba_svm = clf_svm.predict_proba(X_test)","1b00b41d":"ac_svm = accuracy_score(y_test, yhat_svm)\nprint('Accuracy Score: ', ac_svm)\n\ncm_svm = confusion_matrix(y_test, yhat_svm)\nsns.heatmap(cm_svm, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Support Vector Machine')\nplt.show()","89eaf784":"f1_svm = f1_score(y_test, yhat_svm, average='weighted')\nprint('F1 Score: ', f1_svm)","22c427f1":"log_loss_svm = log_loss(y_test, yhat_proba_svm)\nlog_loss_svm","a5da0676":"from sklearn.linear_model import LogisticRegression","1e021721":"clf_lr = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nclf_lr","228f77ed":"clf_lr.fit(X_train, y_train)\nyhat_lr = clf_lr.predict(X_test)\nyhat_proba_lr = clf_lr.predict_proba(X_test)","7c9130b0":"ac_lr = accuracy_score(y_test, yhat_lr)\nprint('Accuracy Score: ', ac_lr)\n\ncm_lr = confusion_matrix(y_test, yhat_lr)\nsns.heatmap(cm_lr, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Logistic Regression')\nplt.show()","628e0bc2":"f1_lr = f1_score(y_test, yhat_lr, average='weighted') \nprint('F1 Score: ', f1_lr)","cfd7a014":"print (classification_report(y_test, yhat_lr))","fa7dd331":"log_loss_lr = log_loss(y_test, yhat_proba_lr)\nlog_loss_lr","274bd7f5":"from sklearn.tree import DecisionTreeClassifier","ebaa3ca7":"clf_dt = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\nclf_dt.fit(X_train, y_train)\nyhat_dt = clf_dt.predict(X_test)\nyhat_proba_dt = clf_dt.predict_proba(X_test)","7942b919":"metrics.accuracy_score(yhat_dt, y_test)\nac_dt = metrics.accuracy_score(yhat_dt, y_test)\nprint(\"DecisionTrees's Accuracy: \", ac_dt)","0a27ff04":"cm_dt = confusion_matrix(y_test, yhat_dt)\nsns.heatmap(cm_dt, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - Decision Tree')\nplt.show()","2fb4c444":"f1_dt = f1_score(y_test, yhat_dt, average='weighted') \nprint('F1 Score: ', f1_dt)","4179a420":"log_loss_dt = log_loss(y_test, yhat_proba_dt)\nlog_loss_dt","fac8e00c":"dot_data = StringIO()\nfilename = \"clf_dt.png\"\nfeatureNames = X_train.columns[0:18]\ntargetNames = Y.unique().tolist()\nout=tree.export_graphviz(clf_dt,feature_names=featureNames, out_file=dot_data, class_names= np.unique(y_train), filled=True,  special_characters=True,rotate=False)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(filename)\nimg = mpimg.imread(filename)\nplt.figure(figsize=(100, 200))\nplt.imshow(img,interpolation='nearest')\nplt.show()","20c20efb":"from sklearn.neighbors import KNeighborsClassifier","fbed3457":"k = 7\nclf_knn = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat_knn = clf_knn.predict(X_test)\nyhat_proba_knn = clf_knn.predict_proba(X_test)","9f2ea433":"ac_knn = metrics.accuracy_score(y_test, yhat_knn)\n\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, clf_knn.predict(X_train)))\nprint(\"Test set Accuracy: \", ac_knn)","760620eb":"Ks = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n# ConfusionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    clf_knn = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat_knn=clf_knn.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat_knn)\n\n    \n    std_acc[n-1]=np.std(yhat_knn==y_test)\/np.sqrt(yhat_knn.shape[0])\n\nmean_acc","76173393":"plt.plot(range(1,Ks),mean_acc,'g')\nplt.fill_between(range(1,Ks), mean_acc - 1 * std_acc, mean_acc + 1 * std_acc, alpha=0.10)\nplt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","0799ca3e":"print( \"The best accuracy was with\", mean_acc.max(), \"with k =\", mean_acc.argmax()+1) \n\ncm_knn = confusion_matrix(y_test, yhat_knn)\nsns.heatmap(cm_knn, annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nplt.xlabel('Predicted Values - Y Hat')\nplt.ylabel('Actual Values - Y')\nplt.title('Confusion Matrix - K Nearest Neighbors')\nplt.show()","74f9b4b0":"f1_knn = f1_score(y_test, yhat_knn, average='weighted') \nprint('F1 Score: ', f1_knn)","62a6cb84":"log_loss_knn = log_loss(y_test, yhat_proba_knn)\nlog_loss_knn","ab280485":"# CREATE NEW DATAFRAME WITH THE ALGORITHM AND EACH ACCURACY MEASUREMENT. \n\nd = {'Algorithm' : ['Random Forest', 'Random Forest w\/ KBest', 'Support Vector Machine', 'Logistic Regression', 'Decision Tree', 'K Nearest Neighbor'],\n     'Accuracy_Score' : [ac_rf, ac_rf2, ac_svm, ac_lr, ac_dt, ac_knn],\n    'F1_Score' : [f1_rf, f1_rf2, f1_svm, f1_lr, f1_dt, f1_knn],\n    'Log_Loss' : [log_loss_rf, log_loss_rf2, log_loss_svm, log_loss_lr, log_loss_dt, log_loss_knn]}\ndf_accuracy = pd.DataFrame(data=d)\ndf_accuracy\n","fcd6e873":"# CREATE BAR CHART TO VISUALIZE EACH ALGORITHM'S ACCURACY MEASUREMENT. \n\nfig = go.Figure(data=[go.Bar(name='Accuracy_Score', x=df_accuracy['Algorithm'], y=df_accuracy['Accuracy_Score']),\n                      go.Bar(name='F1_Score', x=df_accuracy['Algorithm'], y=df_accuracy['F1_Score']),\n                      go.Bar(name='Log_Loss', x=df_accuracy['Algorithm'], y=df_accuracy['Log_Loss']),\n                     ])\n\n# Change the bar mode\nfig.update_layout(barmode='group', title_text='Classification Scores')\nfig.show()","da0688cb":"# CREATE SUBPLOTS WITH ALL CONFUSIION MATRICES\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8), sharey=True)\nfig.suptitle(\"CONFUSION MATRICES\", fontsize=16)\nfig.text(0.5, 0.04, 'PREDICTED VALUES (YHAT)', ha='center', va='center')\nfig.text(0.06, 0.5, 'ACTUAL VALUES (Y)', ha='center', va='center', rotation='vertical')\n\nax1 = sns.heatmap(cm_rf, ax=axes[0, 0], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax2 = sns.heatmap(cm_rf_kbest,ax=axes[0, 1], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax3 = sns.heatmap(cm_svm, ax=axes[0, 2], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax4 = sns.heatmap(cm_lr, ax=axes[1, 0], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax5 = sns.heatmap(cm_dt, ax=axes[1, 1], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\nax6 = sns.heatmap(cm_knn, ax=axes[1, 2], annot=True, fmt='d', cmap='YlGnBu', xticklabels='BM', yticklabels='BM')\n\nax1.set_title('Random Forest')\nax2.set_title('Random Forest - KBest')\nax3.set_title('Support Vector Machine')\nax4.set_title('Logistic Regression')\nax5.set_title('Decision Tree')\nax6.set_title('K Neareest Neighbors')\n\n\nplt.show()","3ffee752":"<a id=\"conclusion\"><\/a>\n## CONCLUSION\n\nIt appears that the Random Forest algorithm gives us the best chance at accuracy within our dataset with an accuracy score of 93% and Log Loss of 16%. \n\nThank you for stopping by! I'd love to recieve some feedback or suggestions on what I could do to improve this kernel. Please leave a comment below. \n\n-Milton","aeffc375":"#### LOG LOSS","68ef2adf":"#### F1 SCORE","474d036d":"#### LOG LOSS","cd141fce":"# <a id='1'>I. LOAD LIBRARIES & PACKAGES<\/a>","7cd35167":"<a id=\"rf\"><\/a>\n### RANDOM FOREST CLASSIFICATION","e6fce694":"Several features have a 100% correlation. For instance, **radius_mean**, **perimeter_mean**, and **area_mean** are all 100% correlated so we can keep one and eliminate the rest. We'll keep **area_mean**. \n\nThis step will be repeated for the other features until we have a feature set that is narrowed down to the most essential features. ","70bed604":"#### LOG LOSS","27a1f7e8":"#### F1 SCORE","0ab44c0f":"# <a id='3'>III. DATA WRANGLING<\/a>","d4873db2":"# <a id='2'>II. DATA OVERVIEW AND INSIGHTS<\/a>","1147f90b":"#### F1 SCORE","9dac2060":"<a id=\"svm\"><\/a>\n### SUPPORT VECTOR MACHINE","6070f5f7":"#### LOG LOSS","68a1e322":"<a id=\"lr\"><\/a>\n### LOGISTIC REGRESSION","6cdc5f53":"#### LOG LOSS","641669a0":"<a id=\"knn\"><\/a>\n### K NEAREST NEIGHBORS","6512412b":"# <a id='4'>IV. EXPLORATORY DATA ANALYSIS<\/a>\n\nThis section will use visualization techniques to get an overview of the data and the correlation between each feature and the target variable. \n\nUsing violin and swarm plots we'll be able to see what kind of distinctions there are between benign and malignant cases and their respective feature variables. ","ddafb5ae":"<p  style=\"text-align: center;\"><font size=\"10\"><b>PREDICTING BREAST CANCER IN WISCONSIN<\/b><\/font><\/p>\n\n\nUsing data from a digitized images of a brest mass in the state of Wisconsin, this notebook will use feature selection and model building using several different algorithms to attempt to predict whether a breast mass is benign or malignant. ","f8c767e9":"<a id=\"comparison\"><\/a>\n## CLASSIFICATION ACCURACY COMPARISON\n\nWe will do a side by side comparison and a visualization of each algorithm's **accuracy_score**, **f1_score**, and **log loss** to determine which model yielded the best results. ","9097ad89":"#### VISUALIZE DECISION TREE","c65f0960":"<hr>","d31a5797":"<a id=\"rfkbest\"><\/a>\n### SELECT K BEST AND RANDOM FOREST","d51f487a":"# <a id='VI'>VI. MODEL BUILDING<\/a>\n\nUsing our new feature set we will build several models to determine which method is best for predicting the outcome of our target variable, diagnosis. \n\nWe will then evaluate the accuracy of each model using **accuracy_score**, **F1_Score**, **Confusion Matrix**, and **Log Loss**\n\n### MODELS USED:\n1. RANDOM FOREST\n2. RANDOM FOREST USING SELECT K BEST FEATURES\n3. SUPPORT VECTOR MACHINE\n4. LOGISTIC REGRESSION\n5. DECISION TREE\n6. K NEAREST NEIGHBOR","d6ec55c7":"# <a id='5'>FEATURE SELECTION<\/a>\n\nWe will narrow down our features by using a heatmap to visualize the correlation between variables and eliminating those features that are fully correlated. ","82e784f0":"#### F1_SCORE","5ba56820":"#### F1 SCORE","3c41280d":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n\n* <a href='#1'>I. LOAD LIBRARIES & PACKAGES<\/a>\n* <a href='#2'>II. LOAD LIBRARIES & PACKAGES<\/a>\n* <a href='#3'>III. LOAD LIBRARIES & PACKAGES<\/a>\n* <a href='#4'>IV. LOAD LIBRARIES & PACKAGES<\/a>\n* <a href='#5'>V. LOAD LIBRARIES & PACKAGES<\/a>\n* <a href='#6'>VI. LOAD LIBRARIES & PACKAGES<\/a>\n\n2. [Initial Insights](#insights)\n3. [Data Preprocessing & Feature Engineering](#preprocessing)\n4. [Data Exploration & Visualization](#exploration)\n5. [Feature Selection](#features)  \n6. [Model Building](#models)  \n    A. [Random Forest](#rf)  \n    B. [Random Forest w Select K Best](#rfkbest)  \n    C. [Support Vector Machine](#svm)  \n    D. [Logistic Regression](#lr)  \n    E. [Decision Tree](#dt)  \n    F. [K Nearest Neighbors](#knn) \n7. [Algorithm Comparison](#comparison)\n8. [Conclusion](#conclusion) ","eebdab1d":"<a id=\"dt\"><\/a>\n### DECISION TREE","3934b78c":"#### F1_SCORE"}}