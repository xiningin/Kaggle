{"cell_type":{"1fb7f368":"code","1a45549e":"code","17efa56b":"code","3535612e":"code","466b115d":"markdown","1365daf3":"markdown","9459745a":"markdown","bf7c0b78":"markdown","a517a0c8":"markdown"},"source":{"1fb7f368":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n%matplotlib inline\npd.set_option('display.max_rows', 200)\npd.set_option('display.max_columns', 200)\nplt.style.use('ggplot')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a45549e":"#Import the data from CSV file\ndf = pd.read_csv('..\/input\/us-accidents\/US_Accidents_June20.csv')\nprint(\"Shape of input DataFrame is - \",df.shape,\"\\n\")\nprint(\"List of Columns in the dataset are:\\n\",df.columns)","17efa56b":"df[\"State\"].value_counts().plot(kind='bar', figsize=(10, 6))","3535612e":"df_CA=df[(df[\"State\"]=='CA')].copy(deep=True)\ndf_CA.shape","466b115d":"## 1. Import all the necceary libararies for this Analysis##\n\nFor this analysis we will need numpy, pandas, seaborn, matplotlib and scikit-learn libraries","1365daf3":"# Work in Progress...#","9459745a":"## 3. Statwise Distribution of the accidents##\nLet's first understand the accidents distribution per state. We will use Bar graph to visually understand this.","bf7c0b78":"## 4. Choosing CA state for analysis\nFrom the graph above, it is very clear that number of accidents reported in the state of \"California\" are high compared with other states. So we will focus our analysis on CA state. Let's filter out accident data for California State from input data. For other states, similar approach can be followed for analysis.","a517a0c8":"## 2. Data for Analysis\nThis is a countrywide car accident dataset, which covers 49 states of the USA. The accident data are collected from February 2016 to June 2020, using two APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 3.5 million accident records in this dataset. Check here to learn more about this dataset.\n\n**Acknowledgements**\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, and Rajiv Ramnath. \u201cA Countrywide Traffic Accident Dataset.\u201d, 2019.\n\nMoosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. \"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\" In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019.\n\n**Author**\nRonghui Zhou, zhou.uf@gmail.com https:\/\/github.com\/RonghuiZhou\n\n"}}