{"cell_type":{"fd607ccf":"code","5751d836":"code","912cc3c2":"code","ecf4f91f":"code","78b46965":"code","7da044e3":"code","09ba77f9":"code","ab62c307":"code","dfb7c398":"code","60e9a3e5":"code","489a9fc4":"code","3218a857":"code","547d9447":"code","cd92b71a":"code","632fae69":"code","b04eb65e":"code","2e942124":"code","c50dfd8a":"code","587aea61":"code","de352dd6":"code","c957b65a":"code","59b1b66d":"code","a3b27a47":"code","7db7cdc8":"code","a82457af":"code","c794e9f7":"code","ec0fd93f":"code","8743569c":"code","5793ed1d":"code","10eb9bf7":"code","4033c978":"markdown","aca1ba71":"markdown","7d83c232":"markdown","56535fab":"markdown","03673679":"markdown","f8a5ab8a":"markdown","6db3dc97":"markdown","dd3d26fb":"markdown","9d8e398d":"markdown","5fe9003b":"markdown","7064c103":"markdown","285d8cb8":"markdown","722d2d55":"markdown","440871a2":"markdown","ff4378a8":"markdown","c7d46eac":"markdown","d9fe6355":"markdown","bc313ad2":"markdown","050d967e":"markdown","1f5b55b2":"markdown","ac484a34":"markdown","ceb39b08":"markdown","ea489778":"markdown","3ef6ab6d":"markdown","97948dae":"markdown","3bce55d6":"markdown","0bbc0034":"markdown","7cc488c8":"markdown","33b89220":"markdown","b21d233f":"markdown","aa7f2743":"markdown","443120b8":"markdown","c02a3dde":"markdown","fa77523a":"markdown","66a0a7df":"markdown","cb3dadcc":"markdown","fc39dbcf":"markdown","8575d08f":"markdown","e852419d":"markdown"},"source":{"fd607ccf":"#importing librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\nimport time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n","5751d836":"data = pd.read_csv('..\/input\/breast-cancer-data\/data.csv')\n","912cc3c2":"data.head()  # head method show only first 5 rows\n","ecf4f91f":"null_feat = pd.DataFrame(len(data['id']) - data.isnull().sum(), columns = ['Count'])\n\ntrace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'lightgrey', line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  \"Missing Values\")\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","78b46965":"# feature names as a list\ncol = data.columns      \n# .columns gives columns names in data \nprint(col)","7da044e3":"# y includes our labels and x includes our features\ny = data.diagnosis                          # M or B \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )\nx.head()","09ba77f9":"ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","ab62c307":"x.describe()\n","dfb7c398":"# first ten features\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\n","60e9a3e5":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","489a9fc4":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","3218a857":"# As an alternative of violin plot, box plot can be used\n# box plots are also useful in terms of seeing outliers\n# I do not visualize all features with box plot\nplt.figure(figsize=(10,10))\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","547d9447":"sns.jointplot(x.loc[:,'concavity_worst'], x.loc[:,'concave points_worst'], kind=\"reg\", color=\"#ce1414\")\n","cd92b71a":"sns.set(style=\"white\")\ndf = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","632fae69":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","b04eb65e":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","2e942124":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\ntoc = time.time()\nplt.xticks(rotation=90)\nprint(\"swarm plot time: \", toc-tic ,\" s\")","c50dfd8a":"#correlation map\nf,ax = plt.subplots(figsize=(30, 30))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.0%',ax=ax,)","587aea61":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()","de352dd6":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.0%',ax=ax)","c957b65a":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\n#n_estimators is the number of trees in the forest.\n\n\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","59b1b66d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","a3b27a47":"print('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)\n","7db7cdc8":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")","a82457af":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","c794e9f7":"print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])","ec0fd93f":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","8743569c":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","5793ed1d":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","10eb9bf7":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())\/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())\/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\nplt.figure(1, figsize=(14, 13))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')","4033c978":"* **Conclusion**\nShortly, I tried to show importance of feature selection and data visualization. Default data includes 33 feature but after feature selection we drop this number from 33 to 5 with accuracy 95%.","aca1ba71":"4) Recursive feature elimination with cross validation and random forest classification\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html Now we will not only find best features but we also find how many features do we need for best accuracy.","7d83c232":"we can see the 16 best features and the 5 features with the highest value\n","56535fab":"import data ","03673679":"# **Feature Selection and Random Forest Classification**","f8a5ab8a":"After drop correlated features there are no more correlated features","6db3dc97":"***INTRODUCTION**\n\n*In this data analysis report,the will be feature selection with correlation, univariate feature selection, recursive feature elimination, recursive feature elimination with cross validation and tree based feature selection methods are used with random forest classification. Apart from these, principle component analysis are used to observe number of components.","dd3d26fb":"Four things caught my attention: \n1) There is an ID that cannot be used for classification\n\n2)Diagnosis is our class label \n\n3) Unnamed: 32 functions include NaN, so we don\u2019t need it","9d8e398d":"2) Univariate feature selection and random forest classification\n\nUnivariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the output variable.\n\n\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features. \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest","5fe9003b":"In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? . \nI do not try all combinations but I only choose k = 5 and find best 5 features","7064c103":"# **Visualization**\n\nIn order to visualizate data we are going to use seaborn plots\nand here, we are our trying to know data\nBefore violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.","285d8cb8":"Feature Extraction\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html We will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize (bring all the variables to the same range) data for better performance of PCA.\n![image.png](attachment:image.png)","722d2d55":"See if the choosing the new feature is correct thus we use model selection ","440871a2":"Therefore, drop these unnecessary features","ff4378a8":"Correlation mesaures between 3 features radius_worst, perimeter_worst and area_worst","c7d46eac":"In this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict.","d9fe6355":"Before making anything like feature selection,feature extraction and classification, firstly we start with basic data analysis. Lets look at features of data","bc313ad2":"**What is data standardization exactly?**\n\n\nData standardization is about making sure that data is internally consistent;\n that is, each data type has the same content and format.\n like the z score we use in the Central limit theorem TCL\n \n\n![image.png](attachment:image.png)\n\nZ\t=\tstandard score\n\nx\t=\tobserved value\n\n\\mu\t=\tmean of the sample\n\n\\sigma\t=\tstandard deviation of the sample\n\n\n","050d967e":"According to variance ration, 3 component can be chosen.\n\n","1f5b55b2":"In order to compare two features deeper, lets use joint plot.\n* pearsonr correlation coefficient measures the linear relationship \n\nPearsonr value 1 is the highest. \nTherefore, 0.86 is looks enough to say that they are correlated.\n","ac484a34":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","ceb39b08":"area_worst in last swarm plot looks like M and B are seprated not totaly but mostly so they can be usefull for the classification.\n\n\nsmoothness_se in swarm plot 2 looks like M and B are mixed so it is hard to classfy while using this feature.\n","ea489778":"Finally, we find best 11 features that are texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst for best classification. Lets look at best accuracy with plot.","3ef6ab6d":"variable of concavity_worst and concave point_worst looks like similar but how can we decide whether they are correlated with each other or not. (Not always true but, basically if the features are correlated with each other we can drop one of them)","97948dae":"Well, we choose our features but did we choose correctly ? Lets use random forest and find accuracy according to chosen features.","3bce55d6":"now we will see a description for our data \ntill now we dont know what each attribute or feature means.\nso we will see the variance standart deviation , count (number of sample) , min and max\nthese type of information helps to understand about what is going on data.\n\n","0bbc0034":"5) Tree based feature selection and random forest classification\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html In random forest classification method there is a featureimportances attributes that is the feature importances (the higher, the more important the feature). !!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.","7cc488c8":"Best 5 feature to classify is that area_mean, area_se, texture_mean, concavity_worst and concavity_mean. So lets se what happens if we use only these best scored 5 feature.","33b89220":"As it can be seen in map heat figure radius_mean, perimeter_mean and area_mean are correlated with each other so we will use only area_mean.\nIn the swarm plot area_mean looks like clear. \nSo lets find other correlated features and look accuracy with random forest classifier.\n\n","b21d233f":"texture_mean feature, median of the Malignant and Benign looks like separated so it can be good for classification. \nHowever, in fractal_dimension_mean feature, median of the Malignant and Benign does not looks like separated so it does not gives good information for classification.","aa7f2743":"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar. Now lets see other feature selection methods to find better results.","443120b8":"> correlation between features","c02a3dde":"1) Feature selection with correlation and random forest classification","fa77523a":"* Data Analysis","66a0a7df":"Chosen 5 best features by rfe is texture_mean, area_mean, concavity_mean, area_se, concavity_worst. They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is 5. Maybe if we use best 2 or best 15 feature we will have better accuracy. Therefore lets see how many feature we need to use with rfecv method.","cb3dadcc":"swarm plot to viz our data\neasier than violin","fc39dbcf":"since we have 1 dataset we will split it into 2 one used for the training or the other fot the test \nhere 70 % for the training and the other for testing the accuracy","8575d08f":"Compactness_mean, concavity_mean and concave points_mean are correlated with each other.Therefore I only choose concavity_mean. \nradius_se, perimeter_se and area_se are correlated and I only use area_se. radius_worst, perimeter_worst and area_worst are correlated so I use area_worst. \nCompactness_worst, concavity_worst and concave points_worst so I use concavity_worst. \nCompactness_se, concavity_se and concave points_se so I use concavity_se. texture_mean and texture_worst are correlated and I use texture_mean. area_worst and area_mean are correlated, I use area_mean.","e852419d":"3) Recursive feature elimination (RFE) with random forest\n\n(RFE) is a feature selection method that fits a model and removes the weakest feature\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html Basically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features\n\nLike previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method."}}