{"cell_type":{"b12c3628":"code","f265b7f7":"code","91b3433f":"code","198cd86a":"code","1211bf22":"code","a9ba7fda":"code","d3b34478":"code","2709d57a":"code","7049f4b0":"code","8046fe23":"code","c978cf7e":"code","337c3c77":"code","db83f8f1":"markdown","3f41241c":"markdown","915eaa5b":"markdown","d2b70e65":"markdown","d89639d3":"markdown"},"source":{"b12c3628":"import json\nimport torch\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport numpy.ma as ma\n\nfrom pathlib import Path\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import cross_val_score","f265b7f7":"COMPETITION_DATA_PATH = Path('..\/input\/commonlitreadabilityprize')\nTRAIN_DATA_PATH = COMPETITION_DATA_PATH \/ 'train.csv'\nTEST_DATA_PATH = COMPETITION_DATA_PATH \/ 'test.csv'","91b3433f":"BATCH_SIZE = 32\nRANDOM_STATE = 41","198cd86a":"train_data = pd.read_csv(TRAIN_DATA_PATH)\ntest_data = pd.read_csv(TEST_DATA_PATH)\n# Remove these lines before submission\n# train_data = train_data.sample(n=40, random_state=RANDOM_STATE)\ntrain_data = train_data.sort_values(by='excerpt', key=lambda x: x.str.len())\nprint(f'Length of train data: {len(train_data)}')\nprint(f'Length of test data: {len(test_data)}')","1211bf22":"class TrainDataset(Dataset):\n    def __init__(self, text_excerpts, targets):\n        self.text_excerpts = text_excerpts\n        self.targets = targets\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        return {'text_excerpt': self.text_excerpts[idx], 'target': self.targets[idx]}\n    \nclass PredictionDataset(Dataset):\n    def __init__(self, text_excerpts):\n        self.text_excerpts = text_excerpts\n    \n    def __len__(self):\n        return len(self.text_excerpts)\n    \n    def __getitem__(self, idx):\n        return {'text_excerpt': self.text_excerpts[idx]}","a9ba7fda":"train_dataset = TrainDataset(text_excerpts=train_data['excerpt'].tolist(),\n                             targets=train_data['target'].tolist())\ntest_dataset = PredictionDataset(text_excerpts=test_data['excerpt'].tolist())\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","d3b34478":"%%capture\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')","2709d57a":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel.eval()\nmodel = model.to(device)","7049f4b0":"def derive_cls_hidden_state(hidden_state):\n    return np.asarray(hidden_state)[:,0,:]\n\ndef mask_and_compute_mean_hidden_state(hidden_state, attention_mask):\n    vector_size = hidden_state.shape[-1]\n    attention_mask = np.expand_dims(attention_mask, axis=2)\n    attention_mask = attention_mask == 0\n    tiled_attention_mask = np.tile(attention_mask, reps=[1, 1, vector_size])\n    masked_hidden_state = ma.array(hidden_state, mask=tiled_attention_mask)\n    mean_masked_hidden_state = masked_hidden_state.mean(axis=1).data\n    return mean_masked_hidden_state\n\ndef extract_and_derive_features(dataloader):\n    features = {'pooler_output': [],\n                'mean_last_hidden_state': [],\n                'mean_last_but_one_hidden_state': [],\n                'mean_last_but_two_hidden_state': [],\n                'cls_last_hidden_state': [],\n                'cls_last_but_one_hidden_state': [],\n                'cls_last_but_two_hidden_state': []}\n    \n    for batch_num, batch in enumerate(dataloader):\n        text_excerpts_batch = batch['text_excerpt']\n        with torch.no_grad():\n            inputs = tokenizer(text_excerpts_batch, return_tensors='pt', padding=True, truncation=True)\n            inputs = inputs.to(device)\n            outputs = model(**inputs, output_attentions=False, output_hidden_states=True)\n            attention_mask = inputs['attention_mask'].detach().cpu().numpy()\n            # Extract features\n            pooler_output = outputs['pooler_output'].detach().cpu().numpy()\n            last_but_two_hidden_state, last_but_one_hidden_state,  last_hidden_state = [hidden_state.detach().cpu().numpy() for hidden_state in outputs['hidden_states'][-3:]]\n            # Derive features\n            mean_last_hidden_state = mask_and_compute_mean_hidden_state(last_hidden_state, attention_mask)\n            mean_last_but_one_hidden_state = mask_and_compute_mean_hidden_state(last_but_one_hidden_state, attention_mask)\n            mean_last_but_two_hidden_state = mask_and_compute_mean_hidden_state(last_but_two_hidden_state, attention_mask)\n            cls_last_hidden_state = derive_cls_hidden_state(last_hidden_state)\n            cls_last_but_one_hidden_state = derive_cls_hidden_state(last_but_one_hidden_state)\n            cls_last_but_two_hidden_state = derive_cls_hidden_state(last_but_two_hidden_state)\n            # Append features\n            features['pooler_output'].extend(pooler_output.tolist())\n            features['mean_last_hidden_state'].extend(mean_last_hidden_state.tolist())\n            features['mean_last_but_one_hidden_state'].extend(mean_last_but_one_hidden_state.tolist())\n            features['mean_last_but_two_hidden_state'].extend(mean_last_but_two_hidden_state.tolist())\n            features['cls_last_hidden_state'].extend(cls_last_hidden_state.tolist())\n            features['cls_last_but_one_hidden_state'].extend(cls_last_but_one_hidden_state.tolist())\n            features['cls_last_but_two_hidden_state'].extend(cls_last_but_two_hidden_state.tolist())       \n    features = {key: np.asarray(value) for key, value in features.items()}\n    return features","8046fe23":"%%time\ntrain_features = extract_and_derive_features(train_dataloader)\ntest_features = extract_and_derive_features(test_dataloader)\n\ntrain_targets = [batch['target'].detach().cpu().tolist() for batch in train_dataloader]\ntrain_targets = list(itertools.chain(*train_targets))","c978cf7e":"for key in train_features.keys():\n    regressor = SVR(C=10, kernel='rbf', gamma='auto')\n#     regressor = Ridge(fit_intercept=True, normalize=False)\n    scores = cross_val_score(regressor, train_features[key], train_targets, cv=5, scoring='neg_root_mean_squared_error')\n    print(f'{key}: Average Root mean squared error: {np.abs(np.mean(scores))}')","337c3c77":"best_feature = 'mean_last_hidden_state'\nregressor = regressor.fit(train_features[best_feature], train_targets)\n\ntest_data = pd.read_csv(TEST_DATA_PATH)\ntest_data['target'] = regressor.predict(test_features[best_feature])\ntest_data[['id','target']].to_csv('submission.csv', index=False)","db83f8f1":"# Fit regressor","3f41241c":"# Context:\nThe extra-ordinary power of Transformer models comes from the fact that they can be used a feature extractor. Therefore I wanted to figure out what are the possible features one could extract from a transformer model and on top of it figure out what are all the additional features one could derive from it. To illustrate this, from a simple pretrained BERT base model I do the following:\n* Extract the following features\n    1. Pooler output\n    2. Hidden states of the transformer. (In the case of BERT base there are 12 hidden states)  \n    \n    \n    \n* Derive the following features from the hidden states:\n    1. Mean of the last hidden state (from n<sup>th<\/sup> layer)\n    2. Mean of the last but one hidden state (from n-1<sup>st<\/sup> layer)\n    3. Mean of the last but two hidden state (from n-2<sup>nd<\/sup> layer)\n    4. Hidden state corresponding to the CLS token of last hidden state (from n<sup>th<\/sup> layer)\n    5. Hidden state corresponding to the CLS token of last but one hidden state (from n-1<sup>st<\/sup> layer)\n    6. Hidden state corresponding to the CLS token of last but two hidden state (from n-2<sup>nd<\/sup> layer)\n    \n  \n# Objectives:\n1. To extract various features from a transformer model\n2. To derive many more features from the extracted features\n3. Evaluate the performance of the features on the given task\n\nA pre-trained BERT model is used to demonstrate this pipeline\n\n# Steps:\n1. Read train and test data\n2. Define Dataset and DataLoader\n3. Define the model\n4. Make sure the entire notebook can execute on CPU or CUDA\n5. Iterate through the train_dataloader and extract features\n6. Derive additional features from the extracted features\n7. Build a regressor and evaluate the various features","915eaa5b":"# Extract and Derive features\nThe mean of the hidden state has to be calculated carefully after masking the hidden states corresponding to the PAD token","d2b70e65":"# Dataset and DataLoader creation","d89639d3":"# Model definition"}}