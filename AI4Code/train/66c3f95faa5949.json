{"cell_type":{"1a30d9f8":"code","88e8e7dc":"code","3a6c1685":"code","0cf4c944":"code","c4e739b1":"code","1b70c1a8":"code","d9651b85":"code","e7918273":"code","03f08a79":"code","67c7b003":"code","1f646a7d":"code","1e1fc2a4":"code","57583f0f":"code","6c0e2843":"code","72009f23":"code","1d54bbdc":"code","46bd5dd1":"code","53577f9a":"code","55df212e":"code","877a7232":"code","8e691aa3":"code","5b206ec6":"code","139e5a2d":"code","7238667e":"code","03228d5c":"code","745f9973":"code","edcb56eb":"code","77a78008":"code","36cd751e":"code","553c9e9f":"code","8cc326ee":"code","6bb64e55":"code","591b46ec":"code","f4742490":"code","d2f67a6a":"code","9b5dfe29":"code","e40cd1e4":"code","4fd44047":"code","f8828519":"code","f3349131":"code","b064d9ce":"code","9128967e":"code","1e6d6c37":"code","5b5ad5ee":"code","434e089f":"code","fa774bf8":"markdown","338e8778":"markdown","d1b3f305":"markdown","cc667d7c":"markdown","aafb1d10":"markdown","f9106ae0":"markdown","f6b9bf93":"markdown","e916898d":"markdown","88ec84b2":"markdown","6587cdfe":"markdown","915cdff9":"markdown","e42a9c2c":"markdown","6a313912":"markdown","6b763ae3":"markdown","f8d3aebe":"markdown","8f4898d6":"markdown","88914fcd":"markdown","8f13377b":"markdown"},"source":{"1a30d9f8":"!pip install -q efficientnet\n!pip install tf-nightly\n!pip install -q git+https:\/\/github.com\/tensorflow\/examples.git","88e8e7dc":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport time\nimport pandas as pd\nimport numpy as np\n\nimport cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom IPython import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport tensorflow.keras.layers as L\nimport efficientnet.tfkeras as efn\nimport tensorflow as tf\nfrom tensorflow_examples.models.pix2pix import pix2pix\n\nfrom tqdm import tqdm as tqdm\nimport gc\n# Data access\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","3a6c1685":"def InitializeSession():\n    tf.compat.v1.keras.backend.clear_session()\n    tf.compat.v1.reset_default_graph()\n    tf.compat.v1.set_random_seed(123)\n    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n    tf.compat.v1.keras.backend.set_session(sess)","0cf4c944":"InitializeSession()\nEPOCHS = 600\nSAMPLE_LEN = 100\nBUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\n\nIMAGE_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/images\/\"\nTEST_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/test.csv\"\nTRAIN_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/train.csv\"\nSUB_PATH = \"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\"\n\nsub = pd.read_csv(SUB_PATH)\ntest_data = pd.read_csv(TEST_PATH)\ntrain_data = pd.read_csv(TRAIN_PATH)\nmd_gan = []","c4e739b1":"def format_path(st):\n    return GCS_DS_PATH + '\/images\/' + st + '.jpg'","1b70c1a8":"train_data.head()","d9651b85":"healthy = train_data[train_data['healthy']>0]\nmultiple_diseases = train_data[train_data['multiple_diseases']>0]\nrust = train_data[train_data['rust']>0]\nscab = train_data[train_data['scab']>0]","e7918273":"labels = 'healthy', 'multiple diseases', 'rust', 'scab'\nsizes = [len(healthy), len(multiple_diseases), len(rust), len(scab)]\ncolors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue']\nexplode = (0.1, 0, 0, 0)  # explode 1st slice\n\n# Plot\nplt.rcParams.update({'font.size': 22})\nplt.figure(figsize=(10,10))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","03f08a79":"def load_image(image_id, label=None, image_size=(IMG_WIDTH, IMG_HEIGHT)):\n    \n    if image_id.numpy().decode(\"utf-8\").split('_')[0]=='gan' and len(image_id.numpy().decode(\"utf-8\").split('_'))==2:\n        image_id = int(image_id.numpy().decode(\"utf-8\").split('_')[1])\n        return md_gan[image_id], [0,1,0,0]\n    else:        \n        bits = tf.io.read_file(image_id)\n        image = tf.image.decode_jpeg(bits, channels=3)\n        image = tf.cast(image, tf.float32) \/ 255.0\n        image = tf.image.resize(image, image_size)\n        if label is None:\n            return image\n        else:\n            return image, label","67c7b003":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","1f646a7d":"def data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","1e1fc2a4":"# normalizing the images to [-1, 1]\ndef normalize(image):\n    image = tf.cast(image, tf.float32)\n    image = (image \/ 127.5) - 1\n    return image","57583f0f":"def random_jitter(image):\n    # resizing to 256 x 256 x 3\n    image = tf.image.resize(image, [256,256],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    # random mirroring\n    image = tf.image.random_flip_left_right(image)\n\n    return image","6c0e2843":"def preprocess_image_train(image, label):\n    bits = tf.io.read_file(image)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    \n    image = random_jitter(image)\n    image = normalize(image)\n    return image","72009f23":"def preprocess_image_test(image, label):\n    image = normalize(image)\n    return image","1d54bbdc":"healthy_imgs = healthy.image_id.apply(format_path).values\nmd_imgs = multiple_diseases.image_id.apply(format_path).values","46bd5dd1":"train_healthy = (\n    tf.data.Dataset\n    .from_tensor_slices((healthy_imgs, ['healthy_train']*len(healthy_imgs)))\n    .map(preprocess_image_train, num_parallel_calls=AUTOTUNE)\n    .cache()\n    .shuffle(BUFFER_SIZE)\n    .batch(1)\n)\ntrain_md = (\n    tf.data.Dataset\n    .from_tensor_slices((md_imgs, ['multiple_diseases_train']*len(md_imgs)))\n    .map(preprocess_image_train, num_parallel_calls=AUTOTUNE)\n    .cache()\n    .shuffle(BUFFER_SIZE)\n    .batch(1)\n)","53577f9a":"OUTPUT_CHANNELS = 3\n\ngenerator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\ngenerator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\ndiscriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\ndiscriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)","55df212e":"sample_healthy = next(iter(train_healthy))\nsample_md = next(iter(train_md))","877a7232":"to_md = generator_g(sample_healthy)\nto_healthy = generator_f(sample_md)\nplt.rcParams.update({'font.size': 8})\nplt.figure(figsize=(8, 8))\ncontrast = 8\nimgs = [sample_healthy, to_md, sample_md, to_healthy]\ntitle = ['healthy', 'to multiple diseases', 'multiple diseases', 'to healthy']\n\nfor i in range(len(imgs)):\n    plt.subplot(2, 2, i+1)\n    plt.title(title[i])\n    if i % 2 == 0:\n        plt.imshow(imgs[i][0] * 0.5 + 0.5)\n    else:\n        plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()\n","8e691aa3":"plt.figure(figsize=(8, 8))\n\nplt.subplot(121)\nplt.title('Is a real multiple disiase leaf?')\nplt.imshow(discriminator_y(sample_md)[0, ..., -1], cmap='RdBu_r')\n\nplt.subplot(122)\nplt.title('Is a real healthy leaf?')\nplt.imshow(discriminator_x(sample_healthy)[0, ..., -1], cmap='RdBu_r')\n\nplt.show()","5b206ec6":"LAMBDA = 10\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","139e5a2d":"def discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n    total_disc_loss = real_loss + generated_loss\n    \n    return total_disc_loss * 0.5","7238667e":"def generator_loss(generated):\n    return loss_obj(tf.ones_like(generated), generated)","03228d5c":"def calc_cycle_loss(real_image, cycled_image):\n    loss1 = tf.reduce_mean(tf.abs(real_image-cycled_image))\n    return LAMBDA * 0.5 * loss1","745f9973":"def identity_loss(real_image, same_image):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return LAMBDA * 0.5 * loss","edcb56eb":"generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","77a78008":"checkpoint_path = \".\/checkpoints\/train\"\n\nckpt = tf.train.Checkpoint(\n    generator_g = generator_g,\n    generator_f = generator_f,\n    discriminator_x = discriminator_x,\n    discriminator_y = discriminator_y,\n    generator_g_optimizer = generator_g_optimizer,\n    generator_f_optimizer = generator_f_optimizer,\n    discriminator_x_optimizer=discriminator_x_optimizer,\n    discriminator_y_optimizer=discriminator_y_optimizer\n)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')","36cd751e":"def generate_images(model, test_input):\n    prediction = model(test_input)\n    \n    plt.figure(figsize=(12, 12))\n    display_list = [test_input[0], prediction[0]]\n    title = ['Input Image', 'Predicted Image']\n    \n    for i in range(2):\n        plt.subplot(1, 2, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()\n","553c9e9f":"@tf.function\ndef train_step(real_x, real_y):\n    # persistent is set to True because the tape is used more than\n    # once to calculate the gradients.\n    with tf.GradientTape(persistent=True) as tape:\n        # Generator G translates X -> Y\n        # Generator F translates Y -> X.\n        fake_y = generator_g(real_x, training=True)\n        cycled_x = generator_f(fake_y, training=True)\n        \n        fake_x = generator_f(real_y, training=True)\n        cycled_y = generator_g(fake_x, training=True)\n\n        # same_x and same_y are used for identity loss.\n        same_x = generator_f(real_x, training=True)\n        same_y = generator_g(real_y, training=True)\n        \n        disc_real_x = discriminator_x(real_x, training=True)\n        disc_real_y = discriminator_y(real_y, training=True)\n        \n        disc_fake_x = discriminator_x(fake_x, training=True)\n        disc_fake_y = discriminator_y(fake_y, training=True)\n        \n        # calculate the loss\n        gen_g_loss = generator_loss(disc_fake_y)\n        gen_f_loss = generator_loss(disc_fake_x)\n        \n        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n        \n        # Total generator loss = adversarial loss + cycle loss\n        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n        \n        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n        \n    # Calculate the gradients for generator and discriminator\n    generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                          generator_g.trainable_variables)\n    generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                          generator_f.trainable_variables)\n    discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                          discriminator_x.trainable_variables)\n    discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n    # Apply the gradients to the optimizer\n    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n\n    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n\n    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","8cc326ee":"for epoch in range(EPOCHS):\n    start = time.time()\n\n    n = 0\n    for image_x, image_y in tf.data.Dataset.zip((train_healthy, train_md)):\n        train_step(image_x, image_y)\n        if n % 10 == 0:\n            print ('.', end='')\n        n+=1\n\n    clear_output(wait=True)\n    # Using a consistent image (sample_horse) so that the progress of the model\n    # is clearly visible.\n    generate_images(generator_g, sample_healthy)\n\n    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                              time.time()-start))\nckpt_save_path = ckpt_manager.save()\n#print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n#                                                 ckpt_save_path))","6bb64e55":"disp_img=[]\nfor image_ in tf.data.Dataset.zip(train_healthy):\n    disp_img.append(image_)\n    prediction_ = generator_g(image_)\n    md_gan.append(prediction_[0].numpy() * 0.5 + 0.5)","591b46ec":"fig, axis =plt.subplots(3, 5, figsize=(15, 10))\nfor i, ax in enumerate(axis.flat):\n    # getting the pixel values between [0, 1] to plot it.\n    ax.imshow(md_gan[i])\n    ax.axis('off')\n    ","f4742490":"for i in range(7):\n    generate_images(generator_g, disp_img[i])","d2f67a6a":"del train_healthy, train_md, generator_g, generator_g_optimizer, generator_f_optimizer, discriminator_x_optimizer\ndel discriminator_y_optimizer, healthy, multiple_diseases, rust, scab, disp_img, generator_f,discriminator_x\ndel discriminator_y \ngc.collect()","9b5dfe29":"InitializeSession()\nAUTO = tf.data.experimental.AUTOTUNE\ngpu = tf.distribute.cluster_resolver.TFConfigClusterResolver()\ntf.config.experimental_connect_to_cluster(gpu)\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n# Configuration\nEPOCHS = 50\nBATCH_SIZE = 10","e40cd1e4":"train_paths = train_data.image_id.apply(format_path).values\ntest_paths = train_data.image_id.apply(format_path).values\ntrain_labels = train_data.loc[:, 'healthy':].values\n\ntrain_paths = list(train_paths)\ntrain_labels = list(train_labels)\n\nfor x in range(len(md_gan)):\n    train_paths.append('gan_'+str(x))\n    train_labels.append([0,1,0,0])\n    \ntrain_paths = np.asarray(train_paths)\ntrain_labels = np.asarray(train_labels)\n\ntrain_imgs, valid_imgs, train_labels, valid_labels = train_test_split(\n    train_paths, \n    train_labels, \n    test_size=0.15,\n    random_state=42,\n)\n","4fd44047":"def load_image_wrapper(file, labels):\n   \n    return tf.py_function(load_image, [file, labels], [tf.float32, tf.int64])","f8828519":"def build_LrFunction(lr_start=0.00001, lr_max=0.000075, \n               lr_min=0.000001, lr_rampup_epochs=20, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","f3349131":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_imgs, train_labels))\n    .map(load_image_wrapper, num_parallel_calls=AUTOTUNE)\n    .cache()\n    .map(data_augment, num_parallel_calls=AUTOTUNE)\n    .shuffle(3)\n    .batch(BATCH_SIZE)\n    .repeat(EPOCHS)\n    .prefetch(AUTOTUNE)\n)\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_imgs, valid_labels))\n    .map(load_image_wrapper, num_parallel_calls=AUTOTUNE)\n    .cache()\n    .repeat(EPOCHS)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths,)\n    .map(decode_image, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n)","b064d9ce":"with strategy.scope():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB7(\n            input_shape=(256,256, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        L.GlobalAveragePooling2D(),\n        L.Dense(train_labels.shape[1], activation='softmax')\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss = 'categorical_crossentropy',\n        metrics=['categorical_accuracy']\n    )\n    model.summary()","9128967e":"lrfn = build_LrFunction()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\nSTEPS_PER_EPOCH = train_labels.shape[0] \/\/ BATCH_SIZE\n\nhistory = model.fit(\n    train_dataset.as_numpy_iterator(), \n    epochs=EPOCHS, \n    callbacks=[lr_schedule],\n    steps_per_epoch=STEPS_PER_EPOCH,\n    #validation_data=valid_dataset.as_numpy_iterator()\n)","1e6d6c37":"def display_training_curves(training, title, subplot):\n    \"\"\"\n    Source: https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(20,15), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    #ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","5b5ad5ee":"display_training_curves(\n    history.history['loss'], \n     \n    'loss', 211)\ndisplay_training_curves(\n    history.history['categorical_accuracy'], \n    \n    'accuracy', 212)","434e089f":"probs = model.predict(test_dataset, verbose=1)\nsub.loc[:, 'healthy':] = probs\nsub.to_csv('submission.csv', index=False)\nsub.head()","fa774bf8":"## Note\nI came across an error while fitting the model that is **Out of range: End of sequence**.<br>\nThe problem was occurring while validating the validation dataset. \nI rendered the entire internet to solve this, all of them suggest to increase the size of the validation set or to create a try and except image generator function where we catch \"OutofRange error\" and \"pass\" it. I have probably tried all of them but yet the error remains. Therefore I have removed the validation data while training the model. I hope you would find a solution to correct this error.","338e8778":"## Credits\n* [Plant Pathology: Very Concise TPU EfficientNet](https:\/\/www.kaggle.com\/xhlulu\/plant-pathology-very-concise-tpu-efficientnet)\n* [LeafGAN: An Effective Data Augmentation Method for Practical Plant Disease Diagnosis](https:\/\/arxiv.org\/pdf\/2002.10100.pdf)\n* [Understanding and Implementing CycleGAN in TensorFlow](https:\/\/hardikbansal.github.io\/CycleGANBlog\/)\n* [Tensorflow\/Cycle gan](https:\/\/www.tensorflow.org\/tutorials\/generative\/cyclegan)","d1b3f305":"## Visualize Generated Images","cc667d7c":"As shown above, generator **G** is responsible for translating image **X** to image **Y**. Identity loss says that, if you fed image **Y** to generator **G**, it should yield the real image **Y** or something close to image **Y**.<br><br>\nIdentity Loss = |G(Y) - Y| + |F(X) - X|","aafb1d10":"## Loss functions\nIn CycleGAN, there is no paired data to train on, hence there is no guarantee that the input x and the target y pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.","f9106ae0":"## Import and reuse the Pix2Pix models\nThe generatior model could also be a res-net model but for easy implementation we would be importing the pix2pix generator and discriminator  where the generator is a U-net.Cyclegan uses instance normalization instead of batch normalization.<br>","f6b9bf93":"## What is cycle GAN?\nThe Cycle GAN is an improved version of Pix2Pix GAN where CycleGAN works without paired examples of transformation from source to target domain, while Pix2Pix requires paired example.<br>\nThe code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data. CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. \n![cyclic](https:\/\/www.tensorflow.org\/tutorials\/generative\/images\/horse2zebra_1.png)","e916898d":"## Visualize Distribution","88ec84b2":"### Please UpVote and share if you like this notebook or if this notebook was informative to you by any means. Also, let me know your opinions and suggestions in the comment section below.","6587cdfe":"## Future Scopes\nI came across this paper named **\"LeafGAN: An Effective Data Augmentation Method for Practical Plant Disease Diagnosis\"**. <br>Link to the paper - [https:\/\/arxiv.org\/pdf\/2002.10100.pdf](https:\/\/arxiv.org\/pdf\/2002.10100.pdf)\n<br>\nIt explains how leaf gan which is an improved version of cycle gan that segments out the leaf using heatmap from the background and uses it to generate better quality diseased leaf images to balance the dataset. Thereby increasing the classification accuracy and model performance. This model could also be implemented as a scope to solve this problem.","915cdff9":"## Our objective\n> As we can see that the multiple disease class is imbalanced compared to other classes in the dataset. So here we would be implementing **cycle gan** to generate multiple diseased leaves from healthy leaves to balance the dataset.","e42a9c2c":"## Thank YOU","6a313912":"## Adversarial Loss\nAccording to the 2014 GAN model we can define the adversarial error of two mappings. For the mapping (G:X->Y) and its corresponding discriminator Dy, the loss function can be defined as\n![](https:\/\/www.zhihu.com\/equation?tex=%5Cmathcal%7BL%7D%28G%2CD_Y%2CX%2CY%29%3D%7B%5Cmathbb%7BE%7D%7D_%7By%7B%5Csim%7Dp_%7Bdata%7D%28y%29%7D%5B%5Clog%7BD_Y%28y%29%7D%5D%2B%7B%5Cmathbb%7BE%7D%7D_%7Bx%7B%5Csim%7Dp_%7Bdata%7D%28x%29%7D%5B%5Clog%7B1-D_Y%28G%28x%29%29%7D%5D+~%28%2A%29)\nIn other words, when the mapping **G** generates a fake but seemingly true picture **G(x)**. The goal of the discriminator **Dx** is to identify the false **Gx** and the real **y0**. For the generator **G** its goal is opposite. The only revolutionary task is to minimize the objective function of **Dx** (*). (ps: **Dx** aims to maximize), i.e.\n![](https:\/\/www.zhihu.com\/equation?tex=%5Cbegin%7Bequation%7D+%5Cmin_%7BG%7D+%5Cmax_%7BD_Y%7D+%5Cmathcal%7BL%7D_%7BGAN%7D%28G%2C+D_Y%2CX%2CY%29.+%5Cend%7Bequation%7D)\n\n## Cycle Consistency Loss\nCycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the original sentence.<br>\nIn cycle consistency loss, \n* Image X is passed via generator G that yields generated image Y'.\n* Generated image Y' is passed via generator F that yields cycled image X'.\n* Mean absolute error is calculated between X and X'.\n* forward cycle consistency loss: X -> G(X) -> F(G(X) -> X'\n* backward cycle consistency loss: Y -> F(X) -> G(F(Y) -> Y'\n\nThe objective function is given directly below:<br>\n![](https:\/\/www.zhihu.com\/equation?tex=%5Cmathcal%7BL%7D_%7Bcyc%7D%28G%2CF%29%3D%7B%5Cmathbb%7BE%7D%7D_%7Bx%7B%5Csim%7Dp_%7Bdata%7D%28x%29%7D%5B%5C%7CF%28G%28x%29%29-x%5C%7C_1%5D+%2B%7B%5Cmathbb%7BE%7D%7D_%7By%7B%5Csim%7Dp_%7Bdata%7D%28y%29%7D%5B%5C%7CG%28F%28y%29%29-y%5C%7C_1%5D)\n\n![](https:\/\/www.tensorflow.org\/tutorials\/generative\/images\/cycle_loss.png)\n\n## Full Objective\nSo, we can get the total objective function as follows:<br>\n![](https:\/\/www.zhihu.com\/equation?tex=%5Cmathcal%7BL%7D%28G%2CF%2C+D_X%2CD_Y%29%3D%5Cmathcal%7BL%7D_%7BGAN%7D%28G%2C+D_Y%2CX%2CY%29%2B%5Cmathcal%7BL%7D_%7BGAN%7D%28F%2C+D_X%2CY%2CX%29%2B%5Clambda%5Cmathcal%7BL%7D_%7Bcyc%7D%28G%2CF%29)\n<br>Among them is the importance of the **\u03bb** used to control both losses. That is our goal:\n![](https:\/\/www.zhihu.com\/equation?tex=%5Cbegin%7Bequation%7D+G%5E%2A%2CF%5E%2A%3D%5Carg%5Cmin_%7BG%2CF%7D%5Cmax_%7BD_X%2CD_Y%7D%5Cmathcal%7BL%7D%28G%2CF%2CD_X%2CD_Y%29.+%5Cend%7Bequation%7D)","6b763ae3":"## Cycle GAN implementation","f8d3aebe":"## Adding the generated image with the original dataset","8f4898d6":"## Cycle GAN to balance imbalanced diseased leaves\nImages of healthy leaves could be easily found or gathered but in the case of diseased leaves gathering, proper data becomes a bit challenging. Hence we observe an imbalance in the dataset causing undesirable model performance.<br>\nThe main objective of this notebook is to demonstrate the use of cycle gan to generate diseased leaves from healthy ones. But along with that, it will also serve the purpose of balancing the dataset. ","88914fcd":"## What is Pix2Pix GAN?<br>\nPix2Pix GAN is an algorithm for image to image translation using conditional GAN's. Using this technique we can colorize black and white photos, convert google maps to google earth, etc. Here, we convert building facades to real buildings.<br>\nLink to the paper - [Image-to-Image Translation with Conditional Adversarial Networks](https:\/\/arxiv.org\/abs\/1611.07004)<br>\n![pix2pix](https:\/\/www.tensorflow.org\/images\/gan\/pix2pix_2.png)","8f13377b":"There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. <br>\n* Generator G learns to transform image X to image Y.(G: X->Y)\n* Generator F learns to transform image Y to image X.(F: Y->X)\n* Discriminator D_X learns to differentiate between image X and generated image X (F(Y)).\n* Discriminator D_Y learns to differentiate between image Y and generated image Y (G(X)).\n![](https:\/\/www.tensorflow.org\/tutorials\/generative\/images\/cyclegan_model.png)"}}