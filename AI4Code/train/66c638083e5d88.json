{"cell_type":{"694c4e6b":"code","d3d44914":"code","7320c938":"code","c4396917":"code","3c2f91fb":"code","35cdde4b":"code","e508cd82":"code","6fdccb5d":"code","433863d5":"code","0a65b417":"code","8ee0595e":"code","00babdef":"code","f9cfeb00":"code","e9ac279c":"code","707aad38":"code","4c1103e9":"code","6688f587":"code","c8a84c0f":"code","13eb54cf":"code","6885e814":"code","5def8227":"code","10a22d46":"code","341aea54":"code","e5690bd0":"code","bda32de9":"code","dcfab769":"code","26020f13":"code","ba93deae":"code","d4f982ea":"code","5cefdc86":"code","897913a3":"code","5613cb2a":"code","e3ebbc27":"code","5076c4ed":"code","eacef434":"code","db56d947":"code","83d1d955":"code","fad54a71":"code","85fcb1d5":"code","ce77a40f":"code","2de35c78":"code","6de29894":"code","722c0a94":"code","ed7dedaa":"code","adb39f23":"code","93c24066":"code","abec007e":"code","f6ce4d09":"code","9d609104":"code","cbcb77f8":"code","9dab5506":"code","e9741617":"code","f2db2dd5":"code","93687ba2":"code","020e75ce":"code","5f421a7c":"code","0f314139":"code","a29696e9":"code","2bbc7ba2":"code","02a9167d":"code","f58e731c":"code","d16680ee":"code","d8c645da":"code","01e12f78":"code","9c089a77":"code","32f3ca19":"code","19f17cfc":"code","22ac9bdc":"code","52bd0884":"code","d072fdef":"code","73dd1d03":"code","6e22a817":"code","8411040b":"code","f87bfb9e":"code","8452695f":"code","c2978e9d":"code","00be3297":"code","4a0d2474":"code","3e9ab886":"code","b6b1e0a3":"code","6e9ce6e3":"code","f4bc037f":"code","236ee7fe":"code","34a40b83":"code","5d6359ff":"code","bc61f0a9":"code","d136af82":"code","8efdbc96":"code","af8dfd27":"code","7b754b52":"code","940b4539":"code","bf8256bb":"code","ef1ae6e9":"code","55ff011f":"code","3c3289af":"code","ee3b2677":"code","191a85b1":"code","0a25e3e2":"code","7fc2b232":"code","0fd94621":"code","860c0a36":"code","538d91ea":"code","b30307f4":"code","17c9bd22":"code","f6ad17a3":"code","3c5e44a7":"code","87a09221":"code","d510f51b":"code","56239ee7":"code","4d438c7a":"code","19f5160f":"code","d9ecb462":"code","ea5d7df7":"code","e5f24dbe":"code","6f30626e":"code","5b89ae29":"code","f5819ee2":"code","fe629efd":"code","25790319":"code","c202bde6":"code","cee2bd92":"code","21f914e7":"code","47a18a61":"code","4db7fe4d":"code","12c3f82e":"code","077c65fe":"code","750f3dad":"code","3adca5cf":"code","d4c1d032":"code","4423ee95":"code","e8e3b944":"code","ba001fda":"code","c7a66e45":"code","f83366df":"code","29ad6e3b":"code","098f2fb4":"code","2c9eebc9":"code","c4e30b99":"code","21b82309":"code","48595e5f":"code","f983426b":"code","7d5738a9":"code","8bf877d1":"code","6f7a1688":"code","51016056":"code","a146b554":"code","bef818e6":"code","d60a7474":"code","b53246e8":"code","65b23e62":"code","2857216a":"code","35fa345b":"code","207b0562":"code","0740cdb2":"code","de005e6b":"code","3a56a0ae":"code","da97864c":"code","27ec999d":"code","b8d3a57b":"markdown","fd0ad17f":"markdown","2d8942d1":"markdown","d40981a2":"markdown","c82be44d":"markdown","7558c58a":"markdown","b2e39fb6":"markdown","1560392d":"markdown","396c801a":"markdown","3ea407e6":"markdown","291efb84":"markdown","24feb5f2":"markdown","7948f4c3":"markdown","67c13097":"markdown","0f21183c":"markdown","1ac332f3":"markdown","08df87f3":"markdown","de9dc169":"markdown","d3079eb6":"markdown","0f2c4f4e":"markdown","0b772dbd":"markdown","9cf6b4a4":"markdown","0ed2aca0":"markdown","c60388e7":"markdown","653e95e5":"markdown","2194d9ae":"markdown","c0599e02":"markdown","4f45dd20":"markdown","9e7b6098":"markdown","87132f90":"markdown","2f1d5e62":"markdown","0d5f9e4c":"markdown","2a6f1d7e":"markdown","fad7bde6":"markdown","f0682a85":"markdown","12a8282f":"markdown","e051594c":"markdown","28b4f37e":"markdown","c7f60735":"markdown","ebb5e033":"markdown","35a7de5c":"markdown","7c4bbbf4":"markdown","137bc58b":"markdown","55b16711":"markdown","c9e47ee7":"markdown","17159014":"markdown","c9fc4ecd":"markdown","5df8e13c":"markdown","18fc7a9a":"markdown","23a17035":"markdown","dec8f2d9":"markdown","7ab60edc":"markdown","56a1debf":"markdown","9e9993e1":"markdown","8180152a":"markdown","4b34522d":"markdown","6fb41311":"markdown","d44f2045":"markdown","6ac45b1c":"markdown","b006138e":"markdown","ceae21d1":"markdown"},"source":{"694c4e6b":"# lets import the relevat files first\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d3d44914":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7320c938":"train_original = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_original = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","c4396917":"train = train_original.copy()\ntest = test_original.copy()","3c2f91fb":"train.head(10)","35cdde4b":"train.info()","e508cd82":"# MSSubClass is actually a categorical variable, hence converting it to the categorical one and dropping ID\ntrain['MSSubClass'] = train['MSSubClass'].astype('object')\ntrain.drop(['Id'],axis=1, inplace=True)\ntest['MSSubClass'] = test['MSSubClass'].astype('object')\ntest.drop(['Id'],axis=1, inplace=True)\n\ntrain['MoSold'] = train['MoSold'].astype('object')\ntest['MoSold'] = test['MoSold'].astype('object')\n\ntrain['YrSold'] = train['YrSold'].astype('object')\ntest['YrSold'] = test['YrSold'].astype('object')\n# Simultaneously we will make a note of this in a separate notebook, so that we can make use of it later on if required.","6fdccb5d":"cat_cols_train = []\ncont_cols_train = []\n\nfor i in train.columns:\n    if train[i].dtypes == 'object':\n        cat_cols_train.append(i)\n    else:\n        cont_cols_train.append(i)","433863d5":"cat_cols_test = []\ncont_cols_test = []\n\nfor i in test.columns:\n    if test[i].dtypes == 'object':\n        cat_cols_test.append(i)\n    else:\n        cont_cols_test.append(i)","0a65b417":"sns.boxplot(train['SalePrice'])\nplt.show()","8ee0595e":"from scipy.stats import norm\n(avge, std_dev) = norm.fit(train['SalePrice'])\nplt.figure(figsize = (20,10))\nsns.distplot(a=train['SalePrice'],hist=True,kde=True,fit=norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel('Sale Price in US$')\nplt.legend(['Sale Price ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(avge, std_dev)],\n            loc='best')\nplt.show()","00babdef":"# qq plot for SalePrice\n# qq plots can be imported from the statsmodels library\nimport statsmodels.api as sm \nsm.qqplot(train['SalePrice'], line='s')\nplt.show()","f9cfeb00":"# We can also draw a probability plot to check the same\n# probplot can be imported from scipy.stats\nimport scipy.stats as stats\nimport pylab\nstats.probplot(train['SalePrice'], dist='norm', plot=pylab)\npylab.show()","e9ac279c":"train['SalePrice'] = np.log(train['SalePrice'])","707aad38":"train['SalePrice'].head()","4c1103e9":"(avge, std_dev) = norm.fit(train['SalePrice'])\nplt.figure(figsize = (20,10))\nsns.distplot(a=train['SalePrice'],hist=True,kde=True,fit=norm)\nplt.title('SalePrice distribution after log vs Normal Distribution', fontsize = 13)\nplt.xlabel('Sale Price in US$')\nplt.legend(['Sale Price ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(avge, std_dev)],\n            loc='best')\nplt.show()","6688f587":"# qqplot\nsm.qqplot(train['SalePrice'],line='s')\nplt.show()","c8a84c0f":"#Probplot\nstats.probplot(train['SalePrice'],dist='norm', plot=pylab)\npylab.show()","13eb54cf":"sns.boxplot(train['SalePrice'], orient='v')\nplt.show()","6885e814":"def outliers(variable):\n  sorted(train[variable])\n  Q1,Q3 = np.percentile(train[variable],[25,75])\n  IQR = Q3-Q1\n  lr = Q1 - (1.5*IQR)\n  ur = Q3 + (1.5*IQR)\n  return ur,lr","5def8227":"ur,lr = outliers('SalePrice')","10a22d46":"train = train.drop(train[(train['SalePrice']<lr ) | (train['SalePrice']>ur)].index)","341aea54":"train.shape","e5690bd0":"sns.boxplot(train['SalePrice'], orient='v')\nplt.show()","bda32de9":"#Probplot\nstats.probplot(train['SalePrice'],dist='norm', plot=pylab)\npylab.show()","dcfab769":"(avge, std_dev) = norm.fit(train['SalePrice'])\nplt.figure(figsize = (20,10))\nsns.distplot(a=train['SalePrice'],hist=True,kde=True,fit=norm)\nplt.title('SalePrice distribution after log vs Normal Distribution', fontsize = 13)\nplt.xlabel('Sale Price in US$')\nplt.legend(['Sale Price ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(avge, std_dev)],\n            loc='best')\nplt.show()","26020f13":"train[cont_cols_train].hist(figsize=(20,20))\nplt.show()","ba93deae":"list1=['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','LowQualFinSF','BsmtHalfBath','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\ndesc1 = train[list1].describe().transpose()\ndesc1['coeff_of_var'] = desc1['std']\/desc1['mean']","d4f982ea":"desc1","5cefdc86":"desc1[desc1['coeff_of_var']>3].T.columns","897913a3":"dropped_columns = ['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', '3SsnPorch','ScreenPorch', 'PoolArea', 'MiscVal']\n\ntrain.drop(dropped_columns,axis=1, inplace=True)\ntest.drop(dropped_columns,axis=1, inplace=True)","5613cb2a":"cat_cols= []\ncont_cols = []\n\nfor i in test.columns:\n    if test[i].dtypes == 'object':\n        cat_cols.append(i)\n    else:\n        cont_cols.append(i)","e3ebbc27":"cat_cols","5076c4ed":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\ncorr_matrix = train.corr('pearson')\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ncmap = sns.diverging_palette(300, 50, as_cmap=True)\nsns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","eacef434":"for i in train.columns:\n    if train[i].isnull().sum()>0:\n        if train[i].dtypes == 'object':\n            print(i)\n            print('Total null values:',train[i].isnull().sum())\n            print('Null values as a % of total:',round((train[i].isnull().sum()*100)\/train['SalePrice'].count(),1)) \n            print()","db56d947":"for i in test.columns:\n    if test[i].isnull().sum()>0:\n        if test[i].dtypes == 'object':\n            print(i)\n            print('Total null values:',test[i].isnull().sum())\n            print('Null values as a % of total:',round((test[i].isnull().sum()*100)\/train['SalePrice'].count(),1)) \n            print()","83d1d955":"f, axes = plt.subplots(12, 4, figsize=(20, 40))\nfor ax, col in zip(axes.ravel(), cat_cols):\n    y = train[col].value_counts()\n    ax.bar(y.index, y)\n    ax.set_title(col)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(45)","fad54a71":"for i in train.columns:\n    if train[i].dtypes == 'object':\n        if train[i].isnull().sum()>0:\n            missing_val_perc = round((train[i].isnull().sum()*100)\/train['SalePrice'].count(),1)\n            if missing_val_perc > 40:\n                print(i)\n                print(train[i].value_counts())\n                print('Total null values:',train[i].isnull().sum())\n                print('Null values as a % of total:',round((train[i].isnull().sum()*100)\/train['SalePrice'].count(),1)) \n                print()","85fcb1d5":"train.drop('PoolQC',axis=1,inplace=True)\ntest.drop('PoolQC',axis=1,inplace=True)\ndropped_columns.append('PoolQC')","ce77a40f":"cat_cols= []\ncont_cols = []\n\nfor i in test.columns:\n    if test[i].dtypes == 'object':\n        cat_cols.append(i)\n    else:\n        cont_cols.append(i)","2de35c78":"list2 =['Alley','FireplaceQu','Fence','MiscFeature']\n\nfor i in list2:\n    train[i].fillna('Not_Applicable', inplace=True)\n    test[i].fillna('Not_Applicable', inplace=True)","6de29894":"for i in cat_cols:\n    if train[i].isnull().sum()>0:\n        train[i].fillna(train[i].value_counts().index[0], inplace=True)","722c0a94":"for i in cat_cols:\n    if test[i].isnull().sum()>0:\n        test[i].fillna(train[i].value_counts().index[0], inplace=True)","ed7dedaa":"train[cat_cols].isnull().sum()","adb39f23":"test[cat_cols].isnull().sum()","93c24066":"for i in train.columns:\n    if train[i].isnull().sum()>0:\n        if train[i].dtypes != 'object':\n            print(i)\n            print('Total null values:',train[i].isnull().sum())\n            print('Null values as a % of total:',round((train[i].isnull().sum()*100)\/train['SalePrice'].count(),1)) \n            print()","abec007e":"for i in test.columns:\n    if test[i].isnull().sum()>0:\n        if test[i].dtypes != 'object':\n            print(i)\n            print('Total null values:',test[i].isnull().sum())\n            print('Null values as a % of total:',round((test[i].isnull().sum()*100)\/train['SalePrice'].count(),1)) \n            print()","f6ce4d09":"for i in cont_cols:\n    if train[i].isnull().sum()>0:\n        train[i].fillna(train[i].median(), inplace=True)","9d609104":"# lets check for the missing values again\n\ntrain.isnull().sum()","cbcb77f8":"for i in cont_cols:\n    if test[i].isnull().sum()>0:\n        test[i].fillna(test[i].median(), inplace=True)","9dab5506":"# lets check for the missing values again\n\ntrain.isnull().sum()","e9741617":"for i in cont_cols:\n  plt.figure(figsize=(10,5))\n  sns.scatterplot(x=train['SalePrice'], y=train[i])\n  plt.show()","f2db2dd5":"len(cat_cols)","93687ba2":"for i in cat_cols:\n  plt.figure(figsize=(15,5))\n  f = sns.stripplot(x=train[i], y=train['SalePrice'])\n  f.set_xticklabels(f.get_xticklabels(),rotation=45)\n  plt.show()\n  plt.figure(figsize=(15,5))\n  g = sns.boxplot(x=train[i], y=train['SalePrice'])\n  g.set_xticklabels(g.get_xticklabels(),rotation=45)\n  plt.show()","020e75ce":"plt.figure(figsize=(15,5))\nf = sns.stripplot(x=train['OverallQual'], y=train['SalePrice'])\nf.set_xticklabels(f.get_xticklabels(),rotation=45)\nplt.show()\nplt.figure(figsize=(15,5))\ng = sns.boxplot(x=train['OverallQual'], y=train['SalePrice'])\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\nplt.show()","5f421a7c":"list1 =['BsmtFinType1','BsmtFinType2','BsmtQual','ExterCond','ExterQual','Fence','FireplaceQu','Functional','GarageCond','GarageQual','SaleCondition','KitchenQual']","0f314139":"# defining a function for ordinal encoding of the certain variables\ndef ordinal_encoding(variable):\n  df = train[[variable,'SalePrice']]\n  df1 = df.groupby(by=variable,axis=0).median()\n  df1 = df1.sort_values(by='SalePrice', axis=0, ascending=True)\n  df1 = df1.reset_index()\n  df1[variable+'_codes'] = df1['SalePrice'].astype('category').cat.codes\n  df1[variable+'_codes'].astype('int')\n  df1[[variable+'_codes']] = df1[[variable+'_codes']]+1\n  df1.drop(['SalePrice'],axis=1,inplace=True)\n  df2 = train.merge(df1, on=variable, how='left')\n  return df2","a29696e9":"# adding the ordinal variables to the train dataframe\ntrain_final = ordinal_encoding('BsmtCond')\nfor i in list1:\n  df1 = ordinal_encoding(i)\n  j=i+'_codes'\n  df2 = df1[j]\n  train_final = pd.concat([train_final,df2],axis=1)","2bbc7ba2":"train_final","02a9167d":"# lets do the same for the test dataset as well\ndef ordinal_encoding_test(variable):\n  df = train_final[variable+'_codes'].groupby([train_final[variable]]).mean().sort_values()\n  df = df.reset_index()\n  df2 = test.merge(df, on=variable, how='left')\n  return df2","f58e731c":"# adding the ordinal variables to the test dataframe\ntest_final = ordinal_encoding_test('BsmtCond')\nfor i in list1:\n  df1 = ordinal_encoding_test(i)\n  j=i+'_codes'\n  df2 = df1[j]\n  test_final = pd.concat([test_final,df2],axis=1)","d16680ee":"test_final.head()","d8c645da":"# Since we have added the ordinal variables for the certain variables, lets remove the original \n# from both test and train datasets\n# list1.append('BsmtCond')\nfor i in list1:\n  train_final.drop([i],axis=1,inplace=True)\n  test_final.drop([i],axis=1, inplace=True)\n  dropped_columns.append(i)","01e12f78":"dropped_columns","9c089a77":"# lets check if our operation is successful\ntrain_final.head()","32f3ca19":"test_final.head()","19f17cfc":"# taking squarefeet per room\ntrain_final[\"SqFtPerRoom\"] = train_final[\"GrLivArea\"] \/ (train_final[\"TotRmsAbvGrd\"] + \n                                         train_final[\"FullBath\"] +\n                                         train_final[\"HalfBath\"] + \n                                         train_final[\"KitchenAbvGr\"])\n\n# taking the total number of bathrooms in the house\ntrain_final['Total_Bathrooms'] = (train_final['FullBath'] + \n                                  (0.5 * train_final['HalfBath']) +\n                                  train_final['BsmtFullBath'])\n\n# Similarly doing the same for the test dataset\n\n# taking squarefeet per room\ntest_final[\"SqFtPerRoom\"] = test_final[\"GrLivArea\"] \/ (test_final[\"TotRmsAbvGrd\"] + \n                                         test_final[\"FullBath\"] +\n                                         test_final[\"HalfBath\"] + \n                                         test_final[\"KitchenAbvGr\"])\n\n# taking the total number of bathrooms in the house\ntest_final['Total_Bathrooms'] = (test_final['FullBath'] + \n                                 (0.5 * test_final['HalfBath']) +\n                                 test_final['BsmtFullBath'])","22ac9bdc":"plt.figure(figsize = (30,30))\nsns.heatmap(train_final.corr(),annot=True)\nplt.show()","52bd0884":"# lets see which are those variables which have low correlation with the SalePrice\n# its better to remove them since these are not good predictors of the SalePrice and most likely will add noise\ndf4 = train_final.corr()\ndf4.loc['SalePrice'][df4['SalePrice']<.2]","d072fdef":"list3=['OverallCond','BedroomAbvGr','KitchenAbvGr','EnclosedPorch','BsmtCond_codes','BsmtFinType2_codes','ExterCond_codes','Functional_codes','GarageCond_codes']\ntrain_final.drop(list3,axis=1,inplace=True)\ntest_final.drop(list3,axis=1,inplace=True)\nfor i in list3:\n  dropped_columns.append(i)","73dd1d03":"# lets check the heatmap once again\nplt.figure(figsize = (30,30))\nsns.heatmap(train_final.corr(),annot=True)\nplt.show()","6e22a817":"# finding those pairs where correlation is >0.6, to identify and remove multicollinearity\nfor i in train_final.corr().columns:\n  for j in train_final.corr().columns:\n    train_corr= train_final[[i,j]].corr()\n    x=train_corr.iloc[0,1]\n    if (x >.6)& (x<1):\n      sns.pairplot(train_final[[i,j]])\n      plt.show()\n      print(\"(\",i,\",\",j,\")\")\n      print('correlation value is',x)\n      print()","8411040b":"dropped_columns","f87bfb9e":"list4=['BsmtQual_codes','KitchenQual_codes','ExterQual_codes','GarageYrBlt','BsmtFullBath','1stFlrSF','2ndFlrSF','FullBath','TotRmsAbvGrd','SqFtPerRoom','GarageArea',]\ntrain_final.drop(list4,axis=1,inplace=True)\ntest_final.drop(list4,axis=1,inplace=True)\nfor i in list4:\n  dropped_columns.append(i)","8452695f":"# we missed Fireplaces lets drop that variable as well.\ntrain_final.drop(['Fireplaces'],axis=1,inplace=True)\ntest_final.drop(['Fireplaces'],axis=1,inplace=True)\ndropped_columns.append('Fireplaces')","c2978e9d":"# lets check the correlation heatmap once again\nplt.figure(figsize = (30,30))\nsns.heatmap(train_final.corr(),annot=True)\nplt.show()","00be3297":"# now we are ready for next step which is feature scaling and train test split\n# but before that lets make sure that everything is in order\ntrain_final.head()","4a0d2474":"train_final.shape","3e9ab886":"test_final.shape","b6b1e0a3":"test_final.info()","6e9ce6e3":"train_final.info()","f4bc037f":"list5 = train_final.columns.drop(['SalePrice'])","236ee7fe":"cat_cols=[]\ncont_cols=[]\nfor i in list5:\n  if train_final[i].dtypes =='object':\n    cat_cols.append(i)\n  else:\n    cont_cols.append(i)","34a40b83":"#importing StandardScaler from SciKit Learn\nfrom sklearn.preprocessing import StandardScaler","5d6359ff":"scaler = StandardScaler()","bc61f0a9":"scaled_features = train_final.copy()\nscaled_train = scaled_features[cont_cols]\nscaled_train = scaler.fit_transform(scaled_train)","d136af82":"df_tr=train_final.copy()\ndf_tr[cont_cols]=scaled_train","8efdbc96":"scaled_features_test = test_final.copy()\nscaled_test = scaled_features_test[cont_cols]\nscaled_test = scaler.fit_transform(scaled_test)","af8dfd27":"df_test=test_final.copy()\ndf_test[cont_cols]=scaled_test","7b754b52":"df_tr_encoded = pd.get_dummies(df_tr, drop_first = True, columns = cat_cols )","940b4539":"df_test_encoded = pd.get_dummies(df_test, drop_first = True, columns = cat_cols )","bf8256bb":"df_tr_encoded.head()","ef1ae6e9":"df_tr_encoded.shape","55ff011f":"df_test_encoded.shape","3c3289af":"df_tr_encoded.columns = df_tr_encoded.columns.str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '').str.replace('%', 'perc').str.replace('\/', '_').str.replace('-', '_').str.replace('.', 'p').str.replace('[', '_').str.replace(']', '').str.replace('&', '').str.replace('$', '').str.replace('#', '')\ndf_test_encoded.columns = df_test_encoded.columns.str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '').str.replace('%', 'perc').str.replace('\/', '_').str.replace('-', '_').str.replace('.', 'p').str.replace('[', '_').str.replace(']', '').str.replace('&', '').str.replace('$', '').str.replace('#', '')","ee3b2677":"train_columns = df_tr_encoded.columns\ntrain_columns","191a85b1":"test_columns = df_test_encoded.columns\ntest_columns","0a25e3e2":"train_col_list = df_tr_encoded.columns.sort_values()\ntest_col_list = df_test_encoded.columns.sort_values()","7fc2b232":"compatible_list = set(train_col_list).intersection(test_col_list)","0fd94621":"df_tr_encoded_2 = df_tr_encoded[compatible_list]","860c0a36":"df_tr_encoded_2.head()","538d91ea":"df_tr_encoded_2.shape","b30307f4":"df_test_encoded_2 = df_test_encoded[compatible_list]","17c9bd22":"df_test_encoded_2.head()","f6ad17a3":"df_test_encoded_2.shape","3c5e44a7":"import statsmodels.api as sm","87a09221":"# copying all predictor variables into X and Target variable in Y\nX = df_tr_encoded_2\nY = df_tr_encoded['SalePrice']","d510f51b":"X.head()","56239ee7":"Y.head()","4d438c7a":"from sklearn.model_selection import train_test_split","19f5160f":"train_X, test_X, train_Y, test_Y = train_test_split(X,Y, test_size = 0.2, random_state=42) ","d9ecb462":"# invoking the LinearRegression function and find the bestfit model on training data\nfrom sklearn.linear_model import LinearRegression\nregression_model = LinearRegression()\nregression_model.fit(train_X, train_Y)","ea5d7df7":"regression_model.coef_","e5f24dbe":"# Let us explore the coefficients for each of the independent attributes\n\nfor i, col_name in enumerate(train_X.columns):\n    print(\"The coefficient for\",col_name, \"is\", regression_model.coef_[i])","6f30626e":"# Let us check the intercept for the model\n\nintercept = regression_model.intercept_\n\nprint(\"The intercept for our model is\", intercept)","5b89ae29":"regression_model.score(train_X, train_Y)","f5819ee2":"regression_model.score(test_X, test_Y)","fe629efd":"# finding RSME\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(train_Y, regression_model.predict(train_X)))","25790319":"np.sqrt(mean_squared_error(test_Y, regression_model.predict(test_X)))","c202bde6":"data_train = pd.concat([train_X, train_Y], axis=1)\ndata_train.head()","cee2bd92":"data_train.columns","21f914e7":"reg_expression = 'SalePrice ~ MSZoning_RH+HouseStyle_SFoyer+MSZoning_FV+Neighborhood_Blueste+Exterior1st_Stucco+MSZoning_RL+MSSubClass_85+Exterior1st_HdBoard+LotShape_IR3+BldgType_Twnhs+LotConfig_CulDSac+Neighborhood_NridgHt+RoofStyle_Shed+Neighborhood_NoRidge+GarageType_BuiltIn+SaleType_WD+MoSold_10+SaleType_Oth+Neighborhood_BrkSide+Neighborhood_Somerst+Condition2_Norm+MSSubClass_60+Exterior2nd_Stone+Condition1_RRNn+WoodDeckSF+MiscFeature_Othr+Neighborhood_StoneBr+BsmtCond_Po+MSSubClass_45+MasVnrType_None+Neighborhood_Sawyer+LotConfig_Inside+MoSold_12+GarageFinish_RFn+Exterior2nd_Plywood+Neighborhood_Crawfor+Alley_Pave+Foundation_PConc+YearRemodAdd+MSSubClass_50+LotConfig_FR2+Neighborhood_ClearCr+BsmtFinSF1+Neighborhood_NPkVill+Electrical_FuseF+SaleType_CWD+YrSold_2010+TotalBsmtSF+GarageType_Basment+RoofStyle_Hip+Neighborhood_MeadowV+MSSubClass_90+Condition1_RRAe+CentralAir_Y+MoSold_9+Condition1_PosN+MSSubClass_40+Condition1_RRAn+Electrical_FuseP+Exterior2nd_Wd_Shng+Electrical_SBrkr+Foundation_CBlock+Heating_Grav+MSSubClass_80+Neighborhood_Edwards+LandSlope_Mod+Neighborhood_Timber+MasVnrType_Stone+HouseStyle_1Story+OverallQual+SaleType_Con+Foundation_Stone+FireplaceQu_codes+Neighborhood_NAmes+Total_Bathrooms+Exterior1st_CBlock+Exterior1st_MetalSd+Neighborhood_BrDale+YearBuilt+HeatingQC_TA+PavedDrive_P+Fence_codes+Neighborhood_CollgCr+HouseStyle_2Story+Condition2_PosA+Foundation_Slab+SaleType_New+MSSubClass_75+LandSlope_Sev+Condition1_PosA+MoSold_5+Heating_Wall+OpenPorchSF+LotFrontage+GrLivArea+HeatingQC_Po+Neighborhood_SawyerW+BsmtExposure_Mn+LotArea+GarageType_Attchd+Condition2_PosN+BsmtFinType1_codes+BldgType_2fmCon+BldgType_TwnhsE+Exterior2nd_CmentBd+GarageType_CarPort+RoofMatl_TarGrv+LotConfig_FR3+LotShape_IR2+Exterior1st_Plywood+MoSold_4+RoofMatl_WdShngl+RoofStyle_Gable+HalfBath+LandContour_Lvl+Neighborhood_Mitchel+Exterior1st_WdShing+Exterior2nd_Wd_Sdng+Foundation_Wood+Condition1_Norm+MSSubClass_180+MasVnrType_BrkFace+YrSold_2007+GarageType_Detchd+Alley_Not_Applicable+Exterior2nd_HdBoard+Exterior2nd_ImStucc+BsmtExposure_No+MiscFeature_Not_Applicable+SaleType_ConLD+SaleType_ConLw+Exterior2nd_Brk_Cmn+Street_Pave+Neighborhood_OldTown+MiscFeature_Shed+GarageFinish_Unf+RoofMatl_WdShake+Exterior2nd_Stucco+Neighborhood_Veenker+BsmtCond_Gd+Exterior1st_Wd_Sdng+LotShape_Reg+MoSold_11+MSZoning_RM+LandContour_HLS+MSSubClass_70+Exterior2nd_MetalSd+GarageCars+MoSold_6+LandContour_Low+Heating_GasW+MasVnrArea+YrSold_2008+Exterior2nd_VinylSd+MSSubClass_30+Exterior2nd_AsphShn+BldgType_Duplex+Exterior2nd_CBlock+HouseStyle_2p5Unf+RoofStyle_Gambrel+PavedDrive_Y+Neighborhood_SWISU+HeatingQC_Fa+MoSold_3+Exterior1st_BrkFace+HouseStyle_1p5Unf+MoSold_7+MSSubClass_160+Neighborhood_Gilbert+Neighborhood_NWAmes+Exterior1st_BrkComm+MoSold_8+BsmtExposure_Gd+Condition1_Feedr+YrSold_2009+Exterior1st_AsphShn+SaleType_ConLI+HeatingQC_Gd+Exterior1st_VinylSd+HouseStyle_SLvl+MSSubClass_120+BsmtCond_TA+Exterior2nd_BrkFace+Neighborhood_IDOTRR+BsmtUnfSF+RoofStyle_Mansard+MSSubClass_190+SaleCondition_codes+Exterior1st_CemntBd+Condition1_RRNe+Condition2_Feedr+GarageQual_codes+MoSold_2'","47a18a61":"import statsmodels.formula.api as smf\nmodel1 = smf.ols(formula=reg_expression, data=data_train).fit()\n# displaying first 5 parameters\nmodel1.params.head()","4db7fe4d":"print(model1.summary())","12c3f82e":"# calculating the Mean square error\nmse = np.mean((model1.predict(data_train.drop('SalePrice',axis=1))- data_train['SalePrice'])**2)","077c65fe":"np.sqrt(mse)","750f3dad":"data_test = pd.concat([test_X, test_Y], axis=1)\ndata_test.head()","3adca5cf":"# calculating the Mean square error\nmse_test = np.mean((model1.predict(data_test.drop('SalePrice',axis=1))- data_test['SalePrice'])**2)","d4c1d032":"# RMSE for the test data\nnp.sqrt(mse_test)","4423ee95":"reg_expression2 = 'SalePrice ~ Neighborhood_Blueste+LotShape_IR3+Neighborhood_NridgHt+Neighborhood_NoRidge+GarageType_BuiltIn+WoodDeckSF+Neighborhood_StoneBr+Neighborhood_Crawfor+YearRemodAdd+MSSubClass_50+GarageType_Basment+Neighborhood_MeadowV+CentralAir_Y+Condition1_PosN+Condition1_RRAn+Electrical_SBrkr+Neighborhood_Edwards+HouseStyle_1Story+OverallQual+FireplaceQu_codes+Total_Bathrooms+HeatingQC_TA+HouseStyle_2Story+Condition2_PosA+Foundation_Slab+LandSlope_Sev+Condition1_PosA+MoSold_5+LotFrontage+GrLivArea+LotArea+GarageType_Attchd+Condition2_PosN+MoSold_4+LandContour_Lvl+Condition1_Norm+GarageType_Detchd+BsmtCond_Gd+LandContour_HLS+GarageCars+MoSold_6+MSSubClass_30+HouseStyle_1p5Unf+MoSold_7+BsmtExposure_Gd+HeatingQC_Gd+BsmtCond_TA+SaleCondition_codes+GarageQual_codes'","e8e3b944":"model2 = smf.ols(formula=reg_expression2,data=data_train).fit()\n# Displaying top 5 parameters\nmodel2.params.head()","ba001fda":"print(model2.summary())","c7a66e45":"# Calculating MSE\nMSE2 = np.mean((model2.predict(data_train.drop(['SalePrice'],axis=1))- data_train['SalePrice'])**2)","f83366df":"#RMSE\nnp.sqrt(MSE2)","29ad6e3b":"# MSE on the test data\nMSE2_test = np.mean((model2.predict(data_test.drop(['SalePrice'],axis=1))- data_test['SalePrice'])**2)","098f2fb4":"# RMSE on the test Data\nnp.sqrt(MSE2_test)","2c9eebc9":"reg_expression3 = 'SalePrice ~ Neighborhood_Blueste+LotShape_IR3+Neighborhood_NridgHt+Neighborhood_NoRidge+GarageType_BuiltIn+WoodDeckSF+Neighborhood_StoneBr+Neighborhood_Crawfor+YearRemodAdd+GarageType_Basment+Neighborhood_MeadowV+CentralAir_Y+Condition1_PosN+Neighborhood_Edwards+HouseStyle_1Story+OverallQual+FireplaceQu_codes+Total_Bathrooms+HeatingQC_TA+Foundation_Slab+MoSold_5+GrLivArea+LotArea+GarageType_Attchd+Condition2_PosN+MoSold_4+LandContour_Lvl+Condition1_Norm+GarageType_Detchd+BsmtCond_Gd+GarageCars+MoSold_6+MSSubClass_30+MoSold_7+BsmtExposure_Gd+HeatingQC_Gd+BsmtCond_TA+SaleCondition_codes+GarageQual_codes'","c4e30b99":"model3 = smf.ols(formula=reg_expression3,data=data_train).fit()\nmodel3.params.head()","21b82309":"print(model3.summary())","48595e5f":"# Calculating MSE\nMSE3 = np.mean((model3.predict(data_train.drop(['SalePrice'],axis=1))- data_train['SalePrice'])**2)","f983426b":"#RMSE\nnp.sqrt(MSE3)","7d5738a9":"# MSE on the test data\nMSE3_test = np.mean((model3.predict(data_test.drop(['SalePrice'],axis=1))- data_test['SalePrice'])**2)","8bf877d1":"# RMSE on the test Data\nnp.sqrt(MSE3_test)","6f7a1688":"# lets drop one more variable where the p value is greater than 0.05 and see if the RMSE further improves:\nreg_expression4 = 'SalePrice ~ Neighborhood_Blueste+LotShape_IR3+Neighborhood_NridgHt+Neighborhood_NoRidge+GarageType_BuiltIn+WoodDeckSF+Neighborhood_StoneBr+Neighborhood_Crawfor+YearRemodAdd+GarageType_Basment+Neighborhood_MeadowV+CentralAir_Y+Condition1_PosN+Neighborhood_Edwards+HouseStyle_1Story+OverallQual+FireplaceQu_codes+Total_Bathrooms+HeatingQC_TA+Foundation_Slab+MoSold_5+GrLivArea+LotArea+GarageType_Attchd+Condition2_PosN+MoSold_4+LandContour_Lvl+Condition1_Norm+GarageType_Detchd+BsmtCond_Gd+GarageCars+MSSubClass_30+MoSold_7+BsmtExposure_Gd+HeatingQC_Gd+BsmtCond_TA+SaleCondition_codes+GarageQual_codes'","51016056":"model4 = smf.ols(formula=reg_expression4,data=data_train).fit()\nmodel4.params.head()","a146b554":"print(model4.summary())","bef818e6":"RMSE4 = np.sqrt(np.mean((model4.predict(data_train.drop(['SalePrice'],axis=1))- data_train['SalePrice'])**2))\nRMSE4","d60a7474":"RMSE4_test = np.sqrt(np.mean((model4.predict(data_test.drop(['SalePrice'],axis=1))- data_test['SalePrice'])**2))\nRMSE4_test","b53246e8":"# Import linear models\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\n# Create lasso and ridge objects\nlasso = linear_model.Lasso()\nridge = linear_model.Ridge()\n# Fit the models\nlasso.fit(train_X, train_Y)\nridge.fit(train_X, train_Y)\n# Print scores, MSE, and coefficients\nprint(\"lasso score:\", lasso.score(train_X, train_Y))\nprint(\"ridge score:\",ridge.score(train_X, train_Y))\nprint(\"lasso RMSE:\", np.sqrt(mean_squared_error(test_Y, lasso.predict(test_X))))\nprint(\"ridge RMSE:\", np.sqrt(mean_squared_error(test_Y, ridge.predict(test_X))))\nprint(\"lasso coef:\", lasso.coef_)\nprint(\"ridge coef:\", ridge.coef_)","65b23e62":"# Import library for visualization\nimport matplotlib.pyplot as plt\ncoefsLasso = []\ncoefsRidge = []\n# Build Ridge and Lasso for 200 values of alpha and write the coefficients into array\nalphasLasso = np.arange (0, 25, 0.1)\nalphasRidge = np.arange (0, 250, 1)\nfor i in range(250):\n    lasso = linear_model.Lasso(alpha=alphasLasso[i])\n    lasso.fit(train_X, train_Y)\n    coefsLasso.append(lasso.coef_)\n    ridge = linear_model.Ridge(alpha=alphasRidge[i])\n    ridge.fit(train_X, train_Y)\n    coefsRidge.append(ridge.coef_[0])\n\n# Build Lasso and Ridge coefficient plots\nplt.figure(figsize = (16,7))\n\nplt.subplot(121)\nplt.plot(alphasLasso, coefsLasso)\nplt.title('Lasso coefficients')\nplt.xlabel('alpha')\nplt.ylabel('coefs')\n\nplt.subplot(122)\nplt.plot(alphasRidge, coefsRidge)\nplt.title('Ridge coefficients')\nplt.xlabel('alpha')\nplt.ylabel('coefs')\n\nplt.show()","2857216a":"# model1 predicts the best RMSE scores for OLS method\ntest_predicted_ols = model1.predict(df_test_encoded_2)\ntest_predicted_ols","35fa345b":"test_predicted_ridge = ridge.predict(df_test_encoded_2)\ntest_predicted_ridge","207b0562":"test_pred = test_predicted_ols.copy()","0740cdb2":"test_pred","de005e6b":"sns.distplot(data_train['SalePrice'],color = 'blue', label='train')\nsns.distplot(test_predicted_ols,color = 'red', label='test')\nsns.distplot(test_predicted_ridge,color = 'green', label='test')\nplt.show()","3a56a0ae":"submission = test_original['Id']\ntest_pred = np.expm1(test_pred)\ntest_pred = pd.DataFrame(test_pred)\nsubmission = pd.concat([submission,test_pred],axis=1)\nsubmission.rename({0:'SalePrice'},axis=1,inplace=True)","da97864c":"submission.head()","27ec999d":"submission.to_csv(\"result.csv\", index = False, header = True)","b8d3a57b":"Lets check the coefficients of the variables in the regression equation","fd0ad17f":"Here we can see that there are several variables, where the coefficient of Variation (std\/mean) is extremely high accompanied by very few non zero values. These data in these variables have very high variability.\n","2d8942d1":"We have the following variables for which the values are heavily right skewed.\n\n'LoTArea',  'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtHalfBath', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'\n\nLets check the descriptive values for these variables","d40981a2":"Testing the model on the test data","c82be44d":"Well it seems that we have a decent prediction. The distribution of OLS predicion SalePrices looks closer to the train dataset.","7558c58a":"## Lets check the categorical values first","b2e39fb6":"For 'Alley','FireplaceQu','Fence','MiscFeature', lets fill the missing values by Not_Applicable for others lets fill the missing values by most frequently occurring values","1560392d":"Here we can see that there are a few variables where there are an overwhelming number of missing values. Lets check for those variables where missing values exceed 40% ","396c801a":"Lets differentiate between the categorical and continuous variables and store these in separate lists. This will come handy later","3ea407e6":"In the above we can notice the following:\n\n1. for most continuous variables, the scales vary widely, hence we will need to standardise the data.\n1. Variables such as YearBuilt, GarageYrBlt are left skewed, but still more and more houses are build in the recent years and more and more garages are built in the later years. Hence we will not check or treat any ourliers for these variables.\n1. Similarly, variables like EnclosedPorch, OpenPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal have overwhelming number of values close to 0. This means they actually may be significant for determining the Sale Price.\n1. However before dropping these variables we will look at their value counts as well as correlation martix.","291efb84":"As we can see that the SalePrice is not normal. Lets further check this by way of qq plots","24feb5f2":"Lets create 2 more variables -> Squarefeet per room -> this is indicative of the fact that properties with bigger rooms fetch larger prices. \n\nHowever for a standard number of rooms, bathrooms and kitchen this should correlate with the Total Living Area.\n\nLets do this and see. We can easily drop it later if there is high correlation between this variable and GrLivArea","7948f4c3":"# Feature Engineering","67c13097":"**Goal: Predict the sales price for each house in the test set. Main evaluation metric is Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.**\n\nThis notebook will be specially helpful to those who are from a non coding background. I have used simple visualizations, to objectively understand the data for the purposes of modelling.","0f21183c":"Now lets check the pairs one by one:\n1. OverallQual-> we would like to keep this variable since it is highly correlated with the sale prices. Hence we would be better off removing the following:\n>* BsmtQual_codes\n>* KitchenQual_codes\n>* ExterQual_codes\n\n2. Yearbuilt: We would like to keep this in the model since its correlation with the SalePrice is high. Hence we would be better off removing:\n>* GarageYrBlt\n\nSimilarly we will be removing the following variables as well\n\n3. BsmtFullBath\n4. 1stFlrSF\n5. 2ndFlrSF\n6. FullBath\n7. TotRmsAbvGrd\n8. SqFtPerRoom\n9. GarageArea\n\n\n\n","1ac332f3":"We will be dropping ['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', '3SsnPorch','ScreenPorch', 'PoolArea', 'MiscVal'].from the train dataset and well as the test dataset, and store this in a list","08df87f3":"* There are a Categorical variables where we can make \n\n* the comparison among the values. Example MSSubClass, MSZoning,  ExterQual, FireplaceQual, GarageCond, Condition of Sale etc. \n\n* Then we know that around the world, the prices vary as per the location and hence we can say that one type of location in the city is better than others. \n\n* Similarly we can say that proximity to main road will fetch higher price(which again may or may not depend on the neighborhood)\n\n* So we will need to create continuous variables for all those which can be compared.\n\n* Also we will need to find a proxy for location.\n\nTo sum it up we need to do the following:\n\n1. House Quality -> we have several variables but none of them tend to show the overall house quality. SO first we will translate all the quality and conditions variables into ordinal ones and then see if we need further feature engineering.\n1. Creating a Location variable:\n> 1. The problems with having a dummy of each\nneighbourhood are: \n> 1. there are only a handful of observations for some neighbourhoods, with less than 30 for 8 neighbourhoods, and less than 100 for the majority of them;\n> 1. there would be significant multicollinearity between certain neighbourhoods that share similar characteristics.\n1. To do this a very simplistic approach would be to assign ordinal values to the neighbourhoods based on the mean saleprice of each locality but again the main idea behind ranking localities is their desirability. Hence we also need to take into account the quality, condition, proximity to the main road\/railroad etc.\n\nHowever we will not create this location variable immediately lets first convert the others into their ordinal codes and then check the correlation.","de9dc169":"# One hot encoding or dummy encoding for the categorical variables","d3079eb6":"Lets check the correlation between various variables","0f2c4f4e":"## lets draw the histograms to understand the data distribution in the continuous variables","0b772dbd":"We have seen that the RMSE has not improved, instead, it has become worse. However, from the regression equation above lets further remove those Variables where P values exceed 0.05, and then see if the values improve. Else we will select model1\n\n","9cf6b4a4":"# Regularisation using Ridge and Lasso\n\nLets go for regularisation to further improve the regression models\nwe will be doing:\nL1 regularisation: also called Lasso\nL2 regularisation: also called Ridge","0ed2aca0":"## Insights from EDA 2.0","c60388e7":"Lets Now check the variability of the SalePrice with respect to Categorical Variables","653e95e5":"# Lets check for the missing values","2194d9ae":"We get the best RMSE scores from the model 1 hence we will be using model 1 ","c0599e02":"Among the 3 i.e., OLS, Ridge and Lasso, Ridge has the best RMSE scores. However after submission with ridge, the score is 0.16153\nhence choosing to go with OLS score now.","4f45dd20":"# EDA 1.0\n\nthe purpose of EDA is to identify the following:\n1. Understand the business context\n1. See if any feature engineering might be required\n1. check for the skewness in the data.\n1. Identify if the outliers and missing values are genuine and whether they should be treated\n1. Parameters to treat the outliers and the missing values\n1. Whether scaling of the data will be required\n1. If there's any multicollinearity present among the variables and whether some variables should be dropped.\n","9e7b6098":"Lets convert this into the natural log and then see the distribution","87132f90":"For the other variables, where there are missing values lets fill the missing values by median or most frequent, whereever applicable ","2f1d5e62":"**importing the test and train datasets**\n\nWe will be importing both the test and train datasets.\n\nAfter importing both the test and the train datasets, we will process them for missing values, and other data hygiene\n\nThe processing on the test data set will be the dropping those columns from the test, which have been dropped from the train, so as to keep the columns of both the datasets aligned, as well as filling the missing values in the test","0d5f9e4c":"# Linear Regression using Statsmodels\n\n* using statsmodels.formula.api => this does not require us to add a constant to the train values\n\nR^2 is not a reliable metric as it always increases with addition of more attributes even if the attributes have no influence on the predicted variable. \n\nInstead we use adjusted R^2 which removes the statistical chance that improves R^2. \n\nScikit does not provide a facility for adjusted R^2, so we use statsmodel, a library that gives results similar to what you obtain in R language. This library expects the X and Y to be given in one single dataframe","2a6f1d7e":"## Checking the target variable\n\n","fad7bde6":"Continuous Variables:\n1. LotFrontage and LotArea do not show any significant correlation with the Target Variable\n1. However OverQual and the yearbuilt and YearRemodadd show considerable correlation. It will be an interesting thing to see if the age of the house has any thing to do with the sale price and locality.\n1. Features related to Basement shows some correlation with the SalePrice. Lets try to do some Feature engineering to see if the basement related features have a significant impact on the Sale Price\n1. Similarly, Greater liv area, 1st floor area and the second floor area too have significant impact on the SalePrice.\n1. Similarly, total rooms and the garage related variables seem to have good correlation on the SalePrice.\n\nCategorical variables:\n1. Variables such as MsSubClass, Neighborhood, MSzoning, Condition, building type, House style, Exterior, Foundation, Heating, Central Airconditioning, kitchen quality, Garage Quality, Garage condition, Saletype seem to have significant variation with the saleprice. Lets try to capture these in the correlation matrix via feature engineering","f0682a85":"# Calculating the Mean Square Error","12a8282f":"Lets check for the barplot of the categorical variables","e051594c":"lets create ordinal values for the following variables:\n\n['BsmtCond','BsmtFinType1','BsmtFinType2','BsmtQual','ExterCond','ExterQual','Fence','FireplaceQu','Functional','GarageCond','GarageType','SaleCondition'\n]","28b4f37e":"# Checking compatibility between the test and the train data\n\nHere since we have different sets of data for both train and test set, we need to see if the values in categorical variables in the train and test are same. If not, this will cause the model to not run on the test dataset.\n\nexample.\n\nsuppose there is a Variable named Payment_methods. The unique values in train dataset are:\n\n1. credit_card\n1. debit_card\n1. cod\n1. Wallet_paytm\n\nafter one hot \/ dummy encoding, this will transform into 4 variables:\n\n1. Payment_methods:credit_card\n1. Payment_methods:debit_card\n1. Payment_methods:cod\n1. Payment_methods:Wallet_paytm\n\nNow lets say that the test dataset has the following values\n\n1. credit_card\n1. debit_card\n1. cod\n1. Wallet_freecharge\n\nafter one hot \/ dummy encoding, this will transform into 4 variables:\n\n1. Payment_methods:credit_card\n1. Payment_methods:debit_card\n1. Payment_methods:cod\n1. Payment_methods:Wallet_freecharge.\n\nSo we can see above that there will be a mismatch and the regression wont run on the test dataset. for this purpose we would like to see which all variables are there in the test and not in train and viceversa.","c7f60735":"Now here we can see that there are a lot of variables in the train which are not there in the test and vice versa could also be possible.","ebb5e033":"Lets now proceed with the EDA 1.0","35a7de5c":"Here we can see that RMSE values for the test and train samples are close. However there are variables where p-values for a lot of coefficients are very high hence lets see if by removing them the RMSE gets better.\n\nHence we will remove those variables from the linear regression expression where the P value is greater than 0.05","7c4bbbf4":"# Data Preprocessing\n\nLets check the data that we have read,in the respective dataframes. \nWe will basically check for number of rows and columns, and basic data hygiene","137bc58b":"# Train Test Split","55b16711":"# Lets now check for the continuous variables","c9e47ee7":"While we will typically include data with high variability, but in this case we will ignore those variables where the upper quantile is also 0 and the cofeeicient of variation is above 3. Its very mych like having the missing values","17159014":"# Submission","c9fc4ecd":"In the above we have 2 scenarios\n1. either the houses dont have these attributes hence their value has been left out\n2. Or the houses have these attributes but their values have not been filled.\n\nHowever it seems really rare that all the houses will have all the 50+ attributes. Hence we can say that these missing values actually correspond to Not Applicable, except PoolQC, where we actually have the Pool are available. So we can drop PoolQC\n","5df8e13c":"Lets make the list of columns which are present in both the test and the train dataset ","18fc7a9a":"# Importing the relevant libraries","23a17035":"# Scaling the continuous variables","dec8f2d9":"We can fill missing values in the train dataset by their median since a very few of them are missing values","7ab60edc":"From EDA we learnt that there are different scales of various features. \n\nNot scaling these features might result in serious biases in the final model.\n\nSo lets go ahead and scale the features using standard scaler.\n\nIn this we will be able to scale only those features which are not of object type so before that lets update the lists of categorical and continuous variables","56a1debf":"Lets remove special characters from the column names and make them conducive for analysis","9e9993e1":"## Outliers in the target variable\nlets check for the outliers in the target variable.","8180152a":"# EDA 2.0 \nLets Check how independent variables Vary with the Log SalePrice","4b34522d":"# Creating A feature set (X) and Outcome Variable (Y)","6fb41311":"In the above , though have the coefficients of the  regression variables, we dont know if these coefficients are significant or not. So lets print the Model summary. Here id the P values are greater than 0.05 that would mean that the coefficient is not significant in predicting the target variable.\n\nHence we would drop such variables, this will be reflected in the decrease of Mean absolute error and the RMSE","d44f2045":"Lets do the same for the test dataset. We are doing this to see if there is any discrepancy between the two datasets. In case there is no discrepancy between the two data sets, we can then combine the two datasets","6ac45b1c":"lets remove these outliers as these can really have a detrimental effect on the linear regression models that we are trying to build.","b006138e":"Now from the above heatmap, i would want to drop those variables:\n1. which do not seem to be a good predictor of the target variable\n1. Which are highly correlated with other variables","ceae21d1":"Now the test and train Dataframes are perfectly compatible. We will now proceed for creating a feature set and the outcome variable on the train dataset"}}