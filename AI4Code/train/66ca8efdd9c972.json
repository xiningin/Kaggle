{"cell_type":{"e51f1f27":"code","3a48c774":"code","60e193a8":"code","5cc52f6a":"code","db9a3004":"code","4daf32d4":"code","b99ed1c1":"code","1db620cd":"code","8ced78a7":"code","1565e683":"code","aa28d30e":"code","8d0b81a8":"code","b53ecb2e":"code","b3a17a09":"code","14a2db8f":"code","7c31dca9":"code","0e0eb077":"code","0a12435f":"code","5b73ce1b":"code","1c46d890":"code","d091957a":"code","90e8ac08":"code","611bfe0c":"code","74778b15":"code","bb2717de":"code","481cec2e":"markdown","a53dbb10":"markdown","138a4d8e":"markdown","b4551061":"markdown","15a6f134":"markdown","e5d7f81c":"markdown","cfadf4cd":"markdown","7c5aee43":"markdown","01bd330d":"markdown","5161c354":"markdown","862fad5f":"markdown","91f6f3fb":"markdown","2751c5c0":"markdown","c1536d1c":"markdown","105f2071":"markdown","f1e52fe0":"markdown"},"source":{"e51f1f27":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_json('..\/input\/basic-scraperdataset-1\/Scrape_Babelio_Site\/Babelio_reviews\/spiders\/datababelio.json')\n\ndf.head()","3a48c774":"len(df)","60e193a8":"# We Examine the null values\ndf.isnull().sum()","5cc52f6a":"# We get the indexes of the rows with nan values.\nindex_with_nan = df.index[df.isnull().any(axis=1)]\nprint(index_with_nan)","db9a3004":"# We delete rows with nan values\ndf.drop(index_with_nan, axis=0, inplace=True)","4daf32d4":"len(df)","b99ed1c1":"# We\u00a0count\u00a0the\u00a0values\u00a0in\u00a0each\u00a0rating\u00a0category.\ndf['rating'].value_counts()","1db620cd":"# We\u00a0convert\u00a0the\u00a0string\u00a0values of rating column\u00a0into\u00a0float\u00a0ones.\ndf['rating'] = df['rating'].map(lambda x: float(x))","8ced78a7":"# I just want\u00a0to\u00a0examine\u00a0the\u00a03\u00a0values\u00a0that\u00a0have\u00a05.5!\u00a0They\u00a0look\u00a0like weird values.\ndf_top= df[df['rating']==5.5]\ndf_top","1565e683":"# The\u00a0first\u00a0review\u00a0that\u00a0has a 5.5\u00a0rating\u00a0\n# I\u00a0confirm\u00a0that\u00a0the\u00a0review\u00a0is\u00a0positive\u00a0(it's a top\u00a0review).\ndf_top['review'].iloc[0]","aa28d30e":"# The\u00a0second\u00a0review\u00a0that\u00a0has a 5.5\u00a0rating\u00a0\n# I\u00a0confirm\u00a0that\u00a0the\u00a0review\u00a0is\u00a0positive\ndf_top['review'].iloc[1]","8d0b81a8":"# The\u00a0third review\u00a0that\u00a0has a 5.5\u00a0rating\u00a0\n# I\u00a0confirm\u00a0that\u00a0the\u00a0review\u00a0is\u00a0positive\ndf_top['review'].iloc[2]","b53ecb2e":"# We add the label Column \n# We\u00a0will\u00a0classify\u00a0all\u00a0the\u00a0reviews\u00a0with a rating\u00a0<= 2.5\u00a0as\u00a0negative\u00a0(that\u00a0includes\u00a00.5,\u00a01.0,\u00a01.5,\u00a02.0,\u00a02.5 values).\n# And the reviews with rating of 3.0 and 3.5 as neutral\n# And the reviews with rating >= 4.0 as positive (that includes 4.0, 4.5 and 5 values).\ndf.loc[:,'label'] = np.nan","b3a17a09":"# We update the new column \"label\", with the negative values\ndf.loc[df['rating']<=2.5,'label'] = -1\n\n# We update the new column \"label\", with the positive values\ndf.loc[df['rating']>= 4.0,'label'] = 1\n\n# We update the new column \"label\", with the neutral values\ndf.loc[np.isnan(df['label']),'label'] = 0","14a2db8f":"df['label']= pd.to_numeric(df['label'], downcast='integer')\ndf.head()","7c31dca9":"df['label'].value_counts()","0e0eb077":"# Producing the final processed dataset of Babelio reviews\ndf.to_json('babelio_reviews.json', orient='records', force_ascii=False)","0a12435f":"df_data = pd.read_json('..\/input\/crawler-datase-2\/Scrape_Site_Critiques_libres\/critiques_libres_scraping\/spiders\/data_critiques_libres.json')\n\ndf_data.head()","5b73ce1b":"# We truncate the rating column (to the digit)\ndf_data['rating'] = df_data['rating'].apply(lambda x: x[:-8])","1c46d890":"df_data['rating'].value_counts()","d091957a":"# We convert the rating column to integer type\ndf_data['rating']= df_data['rating'].map(lambda x: int(x))\n\n# To match the first dataset, we convert the rating from over 10 to 5.\ndf_data['rating'] = df_data['rating'].map(lambda x: float(x\/2))\n\ndf_data.head()","90e8ac08":"# We add the label Column \n# We\u00a0will\u00a0classify\u00a0all\u00a0the\u00a0reviews\u00a0with a rating\u00a0<= 2.5\u00a0as\u00a0negative\u00a0(that\u00a0includes\u00a00.5,\u00a01.0,\u00a01.5,\u00a02.0,\u00a02.5 values).\n# And the reviews with rating of 3.0 and 3.5 as neutral\n# And the reviews with rating >= 4.0 as positive (that includes 4.0, 4.5 and 5 values).\ndf_data.loc[:,'label'] = np.nan","611bfe0c":"# We update the new column \"label\", with the negative values\ndf_data.loc[df_data['rating']<=2.5,'label'] = -1\n\n# We update the new column \"label\", with the positive values\ndf_data.loc[df_data['rating']>= 4.0,'label'] = 1\n\n# We update the new column \"label\", with the neutral values\ndf_data.loc[np.isnan(df_data['label']),'label'] = 0\n\n# We convert the data type of label column to integer (instead of float)\ndf_data['label']= pd.to_numeric(df_data['label'], downcast='integer')\n\ndf_data.head(3)","74778b15":"df_data['label'].value_counts()","bb2717de":"# We produce the final processed second dataset (having the same scheme as the first dataset).\ndf_data.to_json('critiques_libres_reviews.json', orient='records', force_ascii=False)","481cec2e":"### **I-2 Building a Basic Spider to Scrape the 1st book reviews site**  \n\n\n- The original source of data of the **first dataset** is the [Babelio](https:\/\/www.babelio.com\/) site.\n\n- **About Babelio**: is both a website and a mobile application dedicated to **French literature** and a **social network** intended to register personal libraries, which can then be shared and commented on by other users. The site operates on the principle of a social cataloging web application.\n\nYou find **all the Scrapy project files** attached to the notebook (in the data section). The **path of the folder** is: ..\/input\/basic-scraperdataset-1  \n\nBelow I add the **commented code** of the **first spider: a basic spider** (that you can also find on this path : ..\/input\/basic-scraperdataset-1\/Scrape_Babelio_Site\/Babelio_reviews\/spiders\/abirspider.py)7\n```python\nimport scrapy\n\nclass AbirspiderSpider(scrapy.Spider):\n    name = 'abirspider'\n    start_urls = ['https:\/\/www.babelio.com\/dernierescritiques.php?p=2']\n   \n\n    def parse(self, response):\n        \n        # I retrieved all the xpahs of the fields to extract\n        \n        block_reviews = response.xpath('\/\/div[@class=\"post_con\"]')\n        for block in block_reviews:\n            \n            book_title = block.xpath('.\/\/a[@class =\"titre1\"]\/text()').get()\n            author = block.xpath('.\/\/a[@class=\"libelle\"]\/text()').get()\n            review = block.xpath('normalize-space(.\/\/div[@class=\"text row\"]\/div\/text())').get()\n            rating = block.xpath('.\/\/td\/div[@data-rateit-mode= \"font\"]\/@data-rateit-value').get()\n\n            yield{\n                'book_title':book_title,\n                'author': author,\n                'review': review,\n                'rating' : rating\n            }\n        # Check if the next button exists (to re-chain the parse function) for all the next pages.\n        next_button = response.xpath('\/\/a[@class=\"fleche icon-next\"]\/@href').get()\n        if next_button:\n            new_url = response.urljoin(next_button)\n            yield scrapy.Request(url=new_url, callback=self.parse, dont_filter=True)\n```","a53dbb10":"#### In this **first section**, I will explain (with words and code) the **two spiders** (one **basic spider** and one **web crawler**) that I developed to collect data on book reviews available on French websites. For this purpose, my spiders iterated several pages and subpages to create a complete dataset.","138a4d8e":"**Then, when opening the Scrapy project you've just created, you will see all these files added.**   \nScrapy has created every file needed for your scraper. **The structure is the following** :\n\n![2.jpg](attachment:89324c58-f1f4-48b1-b93a-dac76bb19ff8.jpg)","b4551061":"# **II- Post-Processing The first Dataset**\n### **II-1 Loading the 1st scraped dataset (Babelio Book reviews)**  ","15a6f134":"In fact, this **structure** mentioned above is illustrated by the **following diagram**, which you can find in the [scrapy documentation](https:\/\/docs.scrapy.org\/en\/latest\/index.html)  \n \nThis link [scrapy Project components](https:\/\/docs.scrapy.org\/en\/latest\/topics\/architecture.html) provides a brief **Description** of the various components (of a Scrapy Project).\n\n![scrapy_architecture_02.png](attachment:12e41fb0-61f1-43de-8bbe-dfeabc149af0.png)","e5d7f81c":"### **I-3 Building a Crawler to Scrape the 2nd book reviews site**  \n\n- The original source of data of the **second dataset** is the [Critiques Libres](http:\/\/www.critiqueslibres.com\/) site.\n\n- **About Critiques Libres**: Like Wikipedia, this French web site is made possible by the contributions of volunteers who use the Internet to share their knowledge and reading experiences. \n\nYou find **all the Scrapy project files** attached to the notebook (in the data section). The **path of the folder** is: ..\/input\/crawler-datase-2\n\nBelow I add the code of the **second spider : a crawler** (that you can also find on this path : ..\/input\/crawler-datase-2\/Scrape_Site_Critiques_libres\/critiques_libres_scraping\/spiders\/crawlerbooks.py\n```python\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom ..items import AbirItem\n\n\nclass CrawlerbooksSpider(CrawlSpider):\n\n    name = 'crawlerbooks'\n\n    start_urls = ['http:\/\/www.critiqueslibres.com\/i.php\/list\/newcrit']\\\n    +[f'http:\/\/www.critiqueslibres.com\/i.php\/list\/newcrit\/?p={i}' for i in range(2,8)]\\\n    +['http:\/\/www.critiqueslibres.com\/i.php\/list\/newecl\/']\\\n    +[f'http:\/\/www.critiqueslibres.com\/i.php\/list\/newecl?p={i}' for i in range(2,8)]\\\n    +['http:\/\/www.critiqueslibres.com\/i.php\/list\/topstar\/']\\\n    +[f'http:\/\/www.critiqueslibres.com\/i.php\/list\/topstar\/?p={i}' for i in range(2,8)]\\\n    +['http:\/\/www.critiqueslibres.com\/i.php\/list\/newparu\/']\\\n    +[f'http:\/\/www.critiqueslibres.com\/i.php\/list\/newparu\/?p={i}' for i in range(2,8)]\n    \n    # We add the rules for the crawler, specifying the xpaths of links to follow.\n    # Here I used the param restrict_paths =..., but there are other prams: allow = ..., deny = ..\n    # and restrict_css = ... (If you are more comfortable with CSS selectors)\n\n    rules = (\n        Rule(LinkExtractor(restrict_xpaths= '\/\/div[@class =\"media-body\"]\/h4\/a[@class =\"lientexte\"]'), callback='parse_item', follow=True),)\n\n    # We define the parse_item function of the callback (included in the rule).\n    # In this function, we define all the xpaths of the elements we want to extract\n    def parse_item(self, response):\n        block_book = response.xpath('\/\/div[@class= \"panel-body\"]')[0]\n        title = block_book.xpath('normalize-space(.\/\/div[@class=\"media-body\"]\/strong\/text())').get()\n        author = response.xpath('normalize-space(\/\/h3\/a\/text())').get()\n        rating = response.xpath('\/\/div[@id= \"summary\"]\/img\/@alt').get()\n        review = response.xpath('normalize-space(\/\/p[@class =\"critic\"]\/text())').get()\n\n        # We instantiate an object from the AbirItem class (defined in the items.py file).\n        # See items.py file\n        # Note: we have to import this class here in the crawler file (see above in the import section).\n        french_reviews = AbirItem()\n\n        # we define each item\n        french_reviews['book_title'] = title\n        french_reviews['author'] = author\n        french_reviews['reader_review'] = review\n        french_reviews['rating'] = rating\n        \n        return french_reviews\n```\n\n**And I add the code of the class  used from items.py**\n\n```python\n\n# Define here the models for your scraped items\n# See documentation in:\n# https:\/\/docs.scrapy.org\/en\/latest\/topics\/items.html\n\nimport scrapy\n\nclass AbirItem(scrapy.Item):\n    book_title = scrapy.Field()\n    author = scrapy.Field()\n    reader_review = scrapy.Field()\n    rating = scrapy.Field()\n\n```","cfadf4cd":"# **IV- Aggregating the two Datasets**","7c5aee43":"### **II-2 Dataset Cleaning & Processing**","01bd330d":"- The  dataset called **datababelio.json** is the raw dataset that I created with web scraping detailed in the previous first section \"Collecting Data with Scrapy | Basic Spider & Web Crawler\"\n\n- **For recall**: The original source of data is the [Babelio](https:\/\/www.babelio.com\/) site.\n\n- **For recall**: Babelio is both a website and a mobile application dedicated to **French literature** and a **social network** intended to register personal libraries, which can then be shared and commented on by other users. The site operates on the principle of a social cataloging web application.","5161c354":"**==> In this screenshot, I demonstrate how the Scrapy Project is launched using terminal commands mentioned above.**\n\n![initializing Scrapy Project](attachment:81101322-91e1-4655-b2bd-b1d47e8dda6d.jpg)","862fad5f":"**Since the two datasets have the same scheme, it is possible to aggregate them into one unified dataset. I called it french_book_reviews, and you can find it here (with a detailed description). [french_book_reviews](https:\/\/www.kaggle.com\/abireltaief\/books-reviews)**","91f6f3fb":"- The  dataset called **data_critiques_libres.json** is the raw dataset that I created with web scraping detailed in the first dection \"Collecting Data with Scrapy | Basic Spider & Web Crawler\"\n\n- **For recall**: The original source of data is the [Critiques Libres](http:\/\/www.critiqueslibres.com\/) site.\n\n- **For recall**: Like Wikipedia, this French web site is made possible by the contributions of volunteers who use the Internet to share their knowledge and reading experiences. ","2751c5c0":"### **I-1 What is Scrapy and how can you create your first project?**\n\n**Scrapy** is an **open framework** for extracting the data we need from websites.\nScrapy was created to build web spiders that can crawl the web on their own and save data. It is not a library but a whole framework that is enough by itself to build a robust web scraping project, while other web scraping tools are simple libraries that will often need to import additional libraries to achieve the scope of the Scrapy features.\n\n**To Build your first dataset** with **Scrapy framework**, you need to:\n- **Install Scrapy**: you have to write the following command in the terminal: **pip install scrapy** (or in the Anaconda's command prompt: **conda install scrapy**)\n- **Start a Project** (still in the Terminal), you write: **scrapy startproject name_project**\n- **Create a basic spider**: (still on the Terminal), you write: **scrapy genspider name_spider address_site_to_scrape**\n- **Create a crawler**: (still in the terminal), you write: **scrapy genspider -t crawl name_crawler address_site_to_scrape**","c1536d1c":"# **I- Collecting Data with Scrapy | Basic Spider & Web Crawler**","105f2071":"# **III- Post-Processing The second Dataset**\n### **III-1 Loading the 2nd scraped dataset (\"Critiques libres\" reviews)**\n","f1e52fe0":"### **III-2 Dataset Cleaning & Processing**"}}