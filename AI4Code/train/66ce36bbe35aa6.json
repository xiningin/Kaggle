{"cell_type":{"6ae80580":"code","58d3ee26":"code","41bd5d43":"code","e05ed797":"code","ed95dd45":"code","6012b452":"code","2a328ced":"code","3eb1d60f":"code","d769ee3a":"code","498d16a7":"code","d3a054c8":"code","47e8b37f":"code","19f74aa4":"code","26590788":"code","60462c9c":"code","542745c9":"code","f228516f":"code","ccbe44de":"code","10837574":"code","a1e50060":"code","3d53aac9":"code","55afce1d":"code","87579b6d":"code","91cc032e":"code","6d85578f":"code","5fe63f9c":"code","90cef113":"code","7d32c358":"code","9b9e5bc8":"code","713b8a90":"code","ae335571":"code","1cdf3358":"code","213f2436":"code","a42ead96":"code","6afa082f":"code","8fd00bdf":"code","fffa30bc":"code","9db0a0ab":"code","c35168aa":"code","6e1d5236":"code","6dab1379":"code","5dfe0537":"code","1f3390ca":"code","9b27f240":"code","184044fe":"code","d73b21d1":"code","03654e00":"code","d0653f17":"code","354a3b59":"code","85def5a6":"code","12a18971":"code","3624d012":"code","fbeec22a":"markdown","33d7152e":"markdown","8adccb7f":"markdown","a93f960a":"markdown","a13ce82c":"markdown","11a0f51d":"markdown","887aaa4e":"markdown","265b72b3":"markdown","864e2c4f":"markdown","03f21da0":"markdown","7766e942":"markdown","bd0f4840":"markdown","e7958ca9":"markdown","1fb62c61":"markdown","a4691bba":"markdown","e3994619":"markdown","1d95c6a3":"markdown","f8dc82c1":"markdown","2f24df7b":"markdown","672ae15a":"markdown","987260b7":"markdown","ef80e7b8":"markdown","f630df70":"markdown","2b65d39b":"markdown","07e43436":"markdown"},"source":{"6ae80580":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58d3ee26":"# Taking an example string\ns = ['hot', 'warm', 'cold', 'freezing', 'hot', 'hot', 'warm', 'freezing', 'warm', 'warm']\npd.get_dummies(s)","41bd5d43":"from sklearn.feature_extraction.text import CountVectorizer\ntext=['This is introduction to NLP','It is useful to people',\n        'Machine learning is the new electicity',\n        'There would be less hype around AI going forward',\n        'python is the best',\n        'R is good language',\n        'I like this book',\n        'I want more books like this']","e05ed797":"vectorizer = CountVectorizer()\nvector = vectorizer.fit_transform(text)","ed95dd45":"print(vectorizer.vocabulary_)","6012b452":"print(vector.toarray())","2a328ced":"text[0]","3eb1d60f":"from nltk.util import ngrams\nn_gram_list = []\nfor i in text:\n    n_gram_list.extend(list(ngrams(i.split(), 2)))","d769ee3a":"n_gram_list","498d16a7":"text","d3a054c8":"vectorizer_ngram = CountVectorizer(ngram_range=(2, 2)) # start at bigrams and end at bigrams\nvector_ngram = vectorizer_ngram.fit_transform(text)","47e8b37f":"vectorizer_ngram.vocabulary_","19f74aa4":"vector_ngram.toarray()","26590788":"from nltk import bigrams\nimport itertools\nimport nltk","60462c9c":"def co_occurrence_matrix(corpus):\n    vocab = set(corpus)\n    vocab = list(vocab)\n    vocab_to_index = { word:i for i, word in enumerate(vocab) }\n    # Create bigrams from all words in corpus\n    bi_grams = list(bigrams(corpus))\n    # Frequency distribution of bigrams ((word1, word2),num_occurrences)\n    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n    # Initialise co-occurrence matrix\n    # co_occurrence_matrix[current][previous]\n    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n    # Loop through the bigrams taking the current and previous word,\n    # and the number of occurrences of the bigram.\n    for bigram in bigram_freq:\n        current = bigram[0][1]\n        previous = bigram[0][0]\n        count = bigram[1]\n        pos_current = vocab_to_index[current]\n        pos_previous = vocab_to_index[previous]\n        co_occurrence_matrix[pos_current][pos_previous] = count\n    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n    # return the matrix and the index\n    return co_occurrence_matrix,vocab_to_index","542745c9":"sample = [x.split() for x in text]","f228516f":"sample","ccbe44de":"# merging all lists together for a usable corpus\nmerged = list(itertools.chain.from_iterable(sample))\nmerged","10837574":"matrix, vocab_to_index = co_occurrence_matrix(merged)","a1e50060":"CoMatrixFinal = pd.DataFrame(matrix, index=vocab_to_index.keys(), columns=vocab_to_index.keys())","3d53aac9":"CoMatrixFinal","55afce1d":"from sklearn.feature_extraction.text import HashingVectorizer\ntext","87579b6d":"# n_features refers to the length of the row we require\nvectorizer = HashingVectorizer(n_features = 10)\nvector = vectorizer.fit_transform(text)","91cc032e":"print(vector.shape)","6d85578f":"print(vector.toarray())","5fe63f9c":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit_transform(text)","90cef113":"print(vectorizer.vocabulary_)\nprint(vectorizer.idf_)","7d32c358":"len(set(merged))","9b9e5bc8":"sample","713b8a90":"import gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot","ae335571":"# Sg - 1 for skipgram\nskipgram = Word2Vec(sample, vector_size=50, window=2, min_count=1, sg=1)\nprint(skipgram)","1cdf3358":"# The trained word vectors are stored in a KeyedVectors instance, as model.wv:\nprint(skipgram.wv['This'])","213f2436":"skipgram.wv.vectors","a42ead96":"skipgram.wv.vectors.shape","6afa082f":"X = skipgram.wv.vectors\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(skipgram.wv.index_to_key)\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","8fd00bdf":"skipgram = Word2Vec(sample, vector_size=50, window=2, min_count=1, sg=0)\nprint(skipgram)","fffa30bc":"skipgram.wv.vectors","9db0a0ab":"skipgram.wv.vectors.shape","c35168aa":"X = skipgram.wv.vectors\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(skipgram.wv.index_to_key)\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","6e1d5236":"model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)","6dab1379":"print(model.similarity('this', 'is'))","5dfe0537":"print(model.similarity('male', 'female'))","1f3390ca":"# Finding the odd one out\nmodel.doesnt_match('car husband wife child'.split())","9b27f240":"# Finding relations\nmodel.most_similar(positive=['woman', 'king'], negative=['man'])","184044fe":"# Extracting a single vector\ns = model['household']\ns.shape","d73b21d1":"from gensim.models import FastText\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot","03654e00":"sample","d0653f17":"fast = FastText(sample,vector_size=20, window=1, min_count=1,\nworkers=5, min_n=1, max_n=2)","354a3b59":"print(fast.wv['This'])","85def5a6":"print(fast.wv['Car'])","12a18971":"list(fast.wv.index_to_key)","3624d012":"X = fast.wv.vectors\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n\npyplot.scatter(result[:, 0], result[:, 1])\nwords = list(fast.wv.index_to_key)\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","fbeec22a":"<h1>Encoding text into features","33d7152e":"<h2>Co-occurence matrices<\/h2>\n\nIt counts occurrence of words together instead of individual words.","8adccb7f":"The next code block is essentially performing the folllowing operation\n\n`King + Woman - Man = ?`","a93f960a":"<h2>TF-IDF vectorizer<\/h2>\n\nThe whole idea of having TF-IDF is to reflect on how\nimportant a word is to a document in a collection, and\nhence normalizing words appeared frequently in all the\ndocuments.","a13ce82c":"**Using Word2vec** - word2vec is the deep learning Google framework to train\nword embeddings. It will use all the words of the whole corpus and predict\nthe nearby words. It will create a vector for all the words present in the\ncorpus in a way so that the context is captured. It also outperforms any\nother methodologies in the space of word similarity and word analogies.\nThere are mainly 2 types in word2vec.\n\n* Skip-Gram\n* Continuous Bag of Words (CBOW)","11a0f51d":"<h2>N-gram based feature vector<\/h2>","887aaa4e":"We can do some pretty neat things with the model, such as checking similarity","265b72b3":"As fastText is built on character levels, we can get results for words that weren't even in the training. ","864e2c4f":"<h2>Implementing fastText<\/h2>\n\nIt is a deep learning framework developed by Facebook to capture context and meaning.","03f21da0":"This section assumes that you have a working knowledge of how a neural\nnetwork works and the mechanisms by which weights in the neural\nnetwork are updated. We've looked at bag-of-words models for now, frequency-based embeddings or features. We will now look at semantic and prediction-based embeddings.\n\nWord embedding is the feature learning technique where words from\nthe vocabulary are mapped to vectors of real numbers capturing the\ncontextual hierarchy","7766e942":"<h2>Loading the Word2Vec Google model<\/h2>\n\nThis model has been pre-trained with 100 billion words.","bd0f4840":"This is usually done for categorical data, we do not use this much in NLP tasks, due to the high dimensionality dataframe it creates.","e7958ca9":"<h2>Continuous Bag-of-Words model","1fb62c61":"<h2>N-gram generation<\/h2>\n\nEach word is considered a separate feature, we don't consider any words in front, or behind the current word. This can lead to losing information. For example: consider the word \u201cnot bad.\u201d If this is split into individual\nwords, then it will lose out on conveying \u201cgood\u201d \u2013 which is what this word\nactually means. As we saw, we might lose potential information or insight because a\nlot of words make sense once they are put together. This problem can be\nsolved by N-grams.\nN-grams are the fusion of multiple letters or multiple words. They are\nformed in such a way that even the previous and next words are captured.\n\n* Unigrams are the unique words present in the sentence. \n* Bigram is the combination of 2 words.\n* Trigram is 3 words and so on.\n\n\nFor example: \u201cI am learning NLP\u201d\n\n**Unigrams**: \u201cI\u201d, \u201cam\u201d, \u201c learning\u201d, \u201cNLP\u201d\n\n**Bigrams**: \u201cI am\u201d, \u201cam learning\u201d, \u201clearning NLP\u201d\n\n**Trigrams**: \u201cI am learning\u201d, \u201cam learning NLP\u201d","a4691bba":"**Term frequency (TF):** Term frequency is simply the ratio of the count of a\nword present in a sentence, to the length of the sentence.\nTF is basically capturing the importance of the word irrespective of the\nlength of the document. For example, a word with the frequency of 3 with\nthe length of sentence being 10 is not the same as when the word length of\nsentence is 100 words. It should get more importance in the first scenario;\nthat is what TF does.\n\n**Inverse Document Frequency (IDF):** IDF of each word is the log of\nthe ratio of the total number of rows to the number of rows in a particular\ndocument in which that word is present.\n\n`IDF = log(N\/n), where N is the total number of rows and n is the number of rows in which the word was present.`\n\nIDF will measure the rareness of a term. Words like \u201ca,\u201d and \u201cthe\u201d show\nup in all the documents of the corpus, but rare words will not be there\nin all the documents. So, if a word is appearing in almost all documents,\nthen that word is of no use to us since it is not helping to classify or in\ninformation retrieval. IDF will nullify this problem.\n\n**TF-IDF is the simple product of TF and IDF so that both of the\ndrawbacks are addressed, which makes predictions and information\nretrieval relevant.**","e3994619":"<h2>One-hot encoding<\/h2>\n\nIt is a traditional method used in feature engineering which involves converting categorical variables into features and columns and then coding in a binary value for the presence of that particular category. ","1d95c6a3":"<h2>Hash Vectorizing<\/h2>\n\nThe above methods of co-occurrence matrices and count vectorizers fall short if the vocabulary becomes too large. We can use hash vectorizers for improved efficiency. Hash Vectorizer is memory efficient and instead of storing the tokens\nas strings, the vectorizer applies the hashing trick to encode them as\nnumerical indexes. The downside is that it\u2019s one way and once vectorized,\nthe features cannot be retrieved.","f8dc82c1":"**Skip-gram** - The skip-gram model (Mikolov et al., 2013)1\n is used to predict the\nprobabilities of a word given the context of word or words.\nLet us take a small sentence and understand how it actually works.\nEach sentence will generate a target word and context, which are the words\nnearby. The number of words to be considered around the target variable\nis called the window size. The table below shows all the possible target\nand context variables for window size 2. Window size needs to be selected\nbased on data and the resources at your disposal. The larger the window\nsize, the higher the computing power.\n\n![image.png](attachment:b07afbd5-b362-4f32-847d-af20aeb2570b.png)","2f24df7b":"<h2>Word embeddings<\/h2>","672ae15a":"It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=\u2019l1\u2019 or projected on the euclidean unit sphere if norm=\u2019l2\u2019.\n\nThis text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.","987260b7":"Even though all previous methods solve most of the problems, once\nwe get into more complicated problems where we want to capture the\nsemantic relation between the words, these methods fail to perform.\nBelow are the challenges:\n\n* All these techniques fail to capture the context and meaning of the words. All the methods discussed so far basically depend on the appearance or frequency of the words. But we need to look at how to capture the context or semantic relations: that is, how frequently the words are appearing close by.\n\n* For a problem like a document classification (book classification in the library), a document is really huge and there are a humongous number of tokens generated. In these scenarios, your number of features can get out of control (wherein) thus hampering the accuracy and performance.","ef80e7b8":"This will a notebook highlighting and working with various techniques to convert text into features. These features can then be passed into an assortment of machine learning\/deep learning models such as:\n* Logistic classifiers\n* SVMs\n* Recurrent neural networks and LSTMs etc.","f630df70":"Plotting the results","2b65d39b":"<h2>Count vectorizer<\/h2>\n\nThis counts the frequency of each word present in the document. The index pertains to the sentence, the columns pertain to the total vocabulary, and the values are the frequency of the particular word in the index.","07e43436":"This summarises some of the popular and not-so-popular techniques for feature encoding, We'll be looking at some more techniques such as GloVe while working on real-world problems."}}