{"cell_type":{"6d71b2e9":"code","521a4ec1":"code","667070e0":"code","851bce8d":"code","c8c5fa22":"code","20924002":"code","720750d3":"code","476852fc":"code","eae8467e":"code","43f5f246":"code","dcb8d2fb":"code","25794b91":"code","386326a9":"code","319eea4d":"code","be73290f":"code","d76042ff":"code","e2fa64bc":"code","a15b49bb":"code","8e734e3b":"code","25b28969":"code","7e6fe953":"code","2c23b0de":"code","ae954fe1":"code","7028dba5":"code","6cb2ce66":"code","d668e7ba":"code","86b8471c":"code","4d9790a1":"code","16c1666b":"code","4d09a825":"code","3354b333":"code","c925e88c":"code","8b222080":"markdown","0c158a6a":"markdown","90f9e26a":"markdown","75363ea9":"markdown","d38ae224":"markdown","55cd2161":"markdown","7fd1e13c":"markdown","79f918dc":"markdown","987f1197":"markdown","934e9d87":"markdown","722164d8":"markdown","b0a2be9b":"markdown","699653f4":"markdown","afde7e72":"markdown","faf51090":"markdown","e024b591":"markdown","ed673e2f":"markdown","f437488f":"markdown","b7be76f0":"markdown","26eb45fa":"markdown","979bdef4":"markdown","00ec6b5f":"markdown","7ee864c6":"markdown","a6359556":"markdown","b7a2b243":"markdown","c24516ef":"markdown","34174051":"markdown","3bf79f64":"markdown","24f591e5":"markdown"},"source":{"6d71b2e9":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport h2o\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom h2o.frame import H2OFrame\nfrom h2o.automl import H2OAutoML, get_leaderboard\nfrom IPython.display import FileLink\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', 200)\n# nnlm_embed = hub.load(\"https:\/\/tfhub.dev\/google\/nnlm-en-dim128\/2\")\nuse_embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\")","521a4ec1":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","667070e0":"train_df.head()","851bce8d":"train_df.info()","c8c5fa22":"test_df.info()","20924002":"train_df[train_df['location'].isnull()].head(100)","720750d3":"train_df[(train_df['target'] == 0) & (train_df['keyword'].isnull())]","476852fc":"train_df[(train_df['target'] == 1) & (train_df['keyword'].isnull())]","eae8467e":"print(f\"No. of unique keywords in class 0: {train_df[(train_df['target'] == 0)]['keyword'].value_counts().shape[0]}\")\nprint(f\"No. of unique keywords in class 1: {train_df[(train_df['target'] == 1)]['keyword'].value_counts().shape[0]}\")\nprint(f\"Total no. of unique keywords: {train_df['keyword'].value_counts().shape[0]}\")","43f5f246":"fig = px.bar(x=['Non-disaster tweet', 'Disaster tweet'], y=train_df['target'].value_counts().values)\nfig.update_layout(title_text='Distribution of tweets', xaxis_title='Tweet type', yaxis_title='Count')","dcb8d2fb":"URL_PATTERN = re.compile(r\"(https:\\\/\\\/\\S+)|(http:\\\/\\\/\\S+)|(www\\.\\S+)\")\nHTML_TAGS_PATTERN = re.compile(r'<.*>')\nALPHA_NUMERIC_PATTERN = re.compile(r\"\\w*[:,-]*(\\d+[:,-]*)+\\d*\\w*\")\nPUNCTUATION_PATTERN = re.compile(r'[^a-zA-Z ]')\nMENTIONS_PATTERN = re.compile(r'@[\\w]*')\nHASH_TAGS_PATTERN = re.compile(r'#\\S+')\nUNWANTED_WORDS_PATTERN = re.compile(r'&amp;|RT: \\S+:|RT \\S+:|FYI|CAD|RT |GMT|UTC|JST|\\s[b-zB-Z]\\s|ST|nsfw')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef get_punctuations(text):\n    \n    return PUNCTUATION_PATTERN.findall(text)\n\npunctuations = train_df.apply(lambda x: get_punctuations(x['text']), axis=1)\npunctuations = punctuations[punctuations.notnull()].explode().value_counts()\n\nfig = px.bar(x=punctuations.index, y=punctuations.values)\nfig.update_layout(title_text='Punctuation count', xaxis_title='Punctuations', yaxis_title='Count')","25794b91":"def has_unwanted_words(text):\n    \n    return 'Tweets with unwanted words' if re.search(UNWANTED_WORDS_PATTERN, text)\\\n                                        else 'Tweets without unwanted words'\n\nhas_unwanted_words = train_df.apply(lambda x: has_unwanted_words(x['text']), axis=1).to_frame()\nhas_unwanted_words.columns = ['Unwanted words'] \nhas_unwanted_words['target'] = train_df['target']\n\npx.histogram(has_unwanted_words, x='Unwanted words', color='target', barmode='group')","386326a9":"def has_html_tags(text):\n    \n    return 'Tweets with HTML tags' if re.search(HTML_TAGS_PATTERN, text) else 'Tweets without HTML tags'\n\nhas_tags = train_df.apply(lambda x: has_html_tags(x['text']), axis=1).to_frame()\nhas_tags.columns = ['HTML tags'] \nhas_tags['target'] = train_df['target']\n\npx.histogram(has_tags, x='HTML tags', color='target', barmode='group')","319eea4d":"def has_urls(text):\n    \n    return 'Tweets with URLs' if re.search(URL_PATTERN, text) else 'Tweets without URLs'\n\nhas_urls = train_df.apply(lambda x: has_urls(x['text']), axis=1).to_frame()\nhas_urls.columns = ['URLs'] \nhas_urls['target'] = train_df['target']\n\npx.histogram(has_urls, x='URLs', color='target', barmode='group')","be73290f":"def has_alnums(text):\n    \n    return 'Tweets with Alphanumeric words' if re.search(ALPHA_NUMERIC_PATTERN, text) else 'Tweets without Alphanumeric words'\n\nhas_alnums = train_df.apply(lambda x: has_alnums(x['text']), axis=1).to_frame()\nhas_alnums.columns = ['Alnums'] \nhas_alnums['target'] = train_df['target']\n\npx.histogram(has_alnums, x='Alnums', color='target', barmode='group')","d76042ff":"def has_mentions(text):\n    \n    return 'Tweets with mentions' if re.search(MENTIONS_PATTERN, text) else 'Tweets without mentions'\n    \nhas_mentions = train_df.apply(lambda x: has_mentions(x['text']), axis=1).to_frame()\nhas_mentions.columns = ['Mentions'] \nhas_mentions['target'] = train_df['target']\n\npx.histogram(has_mentions, x='Mentions', color='target', barmode='group')","e2fa64bc":"def has_hash_tags(text):\n    \n    return 'Tweets with hash tags' if re.search(HASH_TAGS_PATTERN, text) else 'Tweets without hash tags'\n    \nhas_hash_tags = train_df.apply(lambda x: has_hash_tags(x['text']), axis=1).to_frame()\nhas_hash_tags.columns = ['Hash tags'] \nhas_hash_tags['target'] = train_df['target']\n\npx.histogram(has_hash_tags, x='Hash tags', color='target', barmode='group')","a15b49bb":"tweets = train_df[['text']].apply(lambda x: \" \".join(x))['text'].lower()\nwordcloud = WordCloud(max_words=200, width=1000, height=600, background_color='white').generate(tweets)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","8e734e3b":"# Source: https:\/\/www.kaggle.com\/prachichitnis\/stack-tfidf-embedding-xgboost\n\ndef replace_contractions(tweet):\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)\n    \n    return tweet","25b28969":"class TweetTransformer(BaseEstimator, TransformerMixin):\n    \n    \n    def __init__(self, remove_stopwords=False):\n        \"\"\" decides whether to remove the stopwords or not\n        \"\"\"\n        self.remove_stopwords_ = remove_stopwords\n        \n    def replace_contractions(self, tweet):\n        \"\"\" replaces the contractions in the tweet\n        \"\"\"\n        \n        return replace_contractions(tweet)\n\n    def replace_urls(self, tweet):\n        \"\"\" replaces the URLs in the tweet\n        \"\"\"\n\n        return URL_PATTERN.sub('', tweet)\n    \n    def replace_unwanted_words(self, tweet):\n        \"\"\" replaces the unwanted words in the tweet\n        \"\"\"\n        \n        return UNWANTED_WORDS_PATTERN.sub(' ', tweet)\n\n    def replace_mentions(self, tweet):\n        \"\"\" replaces the mentions in the tweet\n        \"\"\"\n\n        return MENTIONS_PATTERN.sub('', tweet)\n\n    def replace_alpha_nums(self, tweet):\n        \"\"\" replaces the alphanumeric words in the tweet\n        \"\"\"\n\n        return ALPHA_NUMERIC_PATTERN.sub('', tweet)\n\n    def remove_emoji_characters(self, tweet):\n        \"\"\" removes the emoticons from the tweet \n        \"\"\"\n\n        return tweet.encode('ascii', 'ignore').decode('ascii')\n\n    def replace_punctuations(self, tweet):\n        \"\"\" replaces the punctuations in the tweet\n        \"\"\"\n\n        return PUNCTUATION_PATTERN.sub(' ', tweet)\n    \n    def modify_empty_and_rare_utterances(self, tweet):\n        \"\"\" handles empty tweets and other rare cases\n        \"\"\"\n        \n        if tweet.strip() == '':\n            tweet = 'no text'\n            \n        tweet = tweet.lower()\n        tweet = re.sub('lo+l', 'laughing out loud', tweet)\n        tweet = re.sub('coo+l', 'cool', tweet)\n        tweet = re.sub('(calif|cal)', 'california', tweet)\n        tweet = tweet.replace('rd', 'road')\n        tweet = tweet.replace('nyc', 'new york city')\n        tweet = tweet.replace('sismo', 'earthquake')\n        tweet = tweet.replace('detactado', 'detected')\n        tweet = re.sub(' +', ' ', tweet)\n        tweet = re.sub('^ +', '', tweet)\n        \n        return tweet\n    \n    def remove_stopwords(self, tweet):\n        \"\"\" removes the stopwords from the tweet\n        \"\"\"\n        \n        tokens = word_tokenize(tweet)\n        for i, token in enumerate(tokens):\n            if token in STOPWORDS:\n                del tokens[i]\n\n        return \" \".join(tokens)\n    \n    def transform_helper(self, tweet):\n        \"\"\" makes a call to the preprocessing functions\n        \"\"\"\n        \n        tweet = self.replace_contractions(tweet)\n        tweet = self.replace_urls(tweet)\n        tweet = self.replace_unwanted_words(tweet)\n        tweet = self.replace_mentions(tweet)\n        tweet = self.replace_alpha_nums(tweet)\n        tweet = self.remove_emoji_characters(tweet)\n        tweet = self.replace_punctuations(tweet)\n        tweet = self.modify_empty_and_rare_utterances(tweet)\n        \n        if self.remove_stopwords_:\n            tweet = self.remove_stopwords(tweet)\n        \n        return tweet\n    \n    def fit(self, X=None):\n        \n        return self\n    \n    def transform(self, X):\n        \"\"\" returns a pd.Series of transformed tweets\n        \"\"\"\n        \n        return X.map(lambda x: self.transform_helper(x))","7e6fe953":"class NNLMEmbeddingsTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X=None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        return nnlm_embed(X.values)","2c23b0de":"class USEEmbeddingsTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X=None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        return use_embed(X.values)","ae954fe1":"# Switching to USE embeddings as it helps to capture the context across a sentence rather than individual words\n\n# nnlm_embeddings_pipeline = Pipeline([\n#     ('tweet_transformer', TweetTransformer()),\n#     ('nnlm_embeddings_transformer', NNLMEmbeddingsTransformer())\n# ])\n\n# USE embeddings pipeline\nuse_embeddings_pipeline = Pipeline([\n    ('tweet_transformer', TweetTransformer()),\n    ('use_embeddings_transformer', USEEmbeddingsTransformer())\n])\n\n# tf-idf pipeline\ntfidf_pipeline = Pipeline([\n    # using remove_stopwords parameter with True for tf-idf\n    # as it doesn't take the context into account\n    \n    ('tweet_transformer', TweetTransformer(remove_stopwords=True)),\n    ('tfidf_vectorizer', TfidfVectorizer())\n])\n\n# Combining the USE and tf-idf pipelines\ndata_prep_pipeline = FeatureUnion([\n    ('use_embeddings', use_embeddings_pipeline),\n    ('tfidf', tfidf_pipeline)\n])\n\n# Transforming the training data\nX_train = data_prep_pipeline.fit_transform(train_df['text'])\n# Target labels\ny_train = train_df['target']\n# Transforming the test data\nX_test = data_prep_pipeline.transform(test_df['text'])","7028dba5":"# Checking the shape of the transformed data\nX_train.shape","6cb2ce66":"h2o.init()","d668e7ba":"train_h2o = H2OFrame(X_train.todense())\nx = train_h2o.columns\ny = 'target'\ntrain_h2o[y] = H2OFrame(y_train.values).asfactor()\nX_test_h2o = H2OFrame(X_test.todense())","86b8471c":"# Checking the shape of the training data\n\ntrain_h2o.shape","4d9790a1":"# Creating 10 models with a maximum traning time of 10 hours\n# and training them on the train data\n\naml = H2OAutoML(max_models=20, max_runtime_secs=18000, seed=1)\naml.train(x=x, y=y, training_frame=train_h2o)","16c1666b":"# Displaying the leaderboard\n\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)","4d09a825":"# Predicting the test data\n\npredictions = aml.predict(X_test_h2o)","3354b333":"# Generating the submission file\n\nsubmission_df['target'] = predictions['predict'].as_data_frame().values\nsubmission_df.to_csv('submissions.csv', index=False)\nFileLink('submissions.csv')","c925e88c":"# Checking the submission data frame\n\nsubmission_df.head()","8b222080":"**Visualizing the distribution of mentions across the different classes of the tweets**","0c158a6a":"## Real or Not? NLP with Disaster Tweets\n\nThis kernel uses **Plotly** to visualize the distribution of tweets with respect to their characteristics, and it uses some simple natural language cleaning techniques to purify the tweets. I combined the **tf-idf** word-based and **Google's Universal Sentence Encoder's** 512-dimension features as it improved the model's accuracy than being individually used. Finally, I used **h20.ai's AutoML** module to fit the transformed data and was able to achieve a **F1 score** of **82.413%** on the test data. If you find this kernel useful please **upvote** and share your valuable feedback.\n\nMy other kernels (Please **upvote** if you like the implementation):\n\n* [House Sales Price Prediction](https:\/\/www.kaggle.com\/gauthampughazh\/house-sales-price-prediction-svr)\n* [Titanic Survival Prediction](https:\/\/www.kaggle.com\/gauthampughazh\/titanic-survival-prediction-pandas-plotly-keras)","90f9e26a":"## Exploratory Data Analysis","75363ea9":"**Using a word cloud to view the top 200 most frequently occurred words**","d38ae224":"**Creating a custom scikit-learn transformer to generate Google's NNLM 128-dimension embeddings**","55cd2161":"**Visualizing the distribution of URLs across the different classes of the tweets**","7fd1e13c":"Checking top 100 tweets with missing ***location*** feature","79f918dc":"**Creating a custom sklearn transformer to perform basic cleaning operations on the tweets**","987f1197":"Checking the tweets in class 1 with missing ***keyword*** feature","934e9d87":"## Modelling","722164d8":"**Visualizing the distribution of HTML across the different classes of the tweets**","b0a2be9b":"**Creating a custom scikit-learn transformer to generate Google's USE 512-dimension embeddings**","699653f4":"**Submission**","afde7e72":"**Conclusion**: Keywords present in class 1 are also present in class 0","faf51090":"**Visualizing the distribution of unwanted words across the different classes of the tweets**","e024b591":"**Displaying the model results**","ed673e2f":"**Initilaize H2O**","f437488f":"**Making Predictions**","b7be76f0":"**Replacing the contractions**","26eb45fa":"## Cleaning the tweets","979bdef4":"**Visualizing the distribution of hashtags across the different classes of the tweets**","00ec6b5f":"**Convert the transformed data into H2OFrames**","7ee864c6":"**Peeking the data**","a6359556":"**Visualizing the distribution of punctuations of the tweets**","b7a2b243":"**Traning the models**","c24516ef":"Checking the tweets in class 0 with missing ***keyword*** feature","34174051":"**Creating custom scikit-learn pipelines to transform the tweets**","3bf79f64":"**Visualizing the distribution of alphanumeric words across the different classes of the tweets**","24f591e5":"**Checking the distribution of tweets among the classes**"}}