{"cell_type":{"6bbf2d3d":"code","8dc0a46c":"code","b0e1f3db":"code","b24de558":"code","1dc4759e":"code","80a8b5b0":"code","01afd989":"code","50b4247a":"code","38bf293b":"code","e272fae7":"code","a97a2537":"code","54054c12":"code","2dde476a":"code","bfa72381":"markdown","5fdc44e2":"markdown","876c8e4e":"markdown","9866ad79":"markdown","520bcc00":"markdown","e5a83317":"markdown","180ef730":"markdown","03488ce2":"markdown","b62ec0aa":"markdown","912b870d":"markdown","2f8c6313":"markdown","9a7432e8":"markdown"},"source":{"6bbf2d3d":"from IPython.display import clear_output\n!pip install imutils\n!pip install tensorflow-gpu==2.0.0-beta1\n\nclear_output()","8dc0a46c":"# Set everything up\nimport os\n\nimport cv2\nimport imutils as imutils\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf # machine learning\nfrom tqdm import tqdm # make your loops show a smart progress meter \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sn\n\nRANDOM_SEED = 1\nIMG_SIZE = (224, 224) # size of vgg16 input\nIMG_PATH = \"..\/input\/brain-tumor-images-dataset\/brain tumor images dataset\/Brain Tumor Images Dataset\/\"\n\nprint(os.listdir(IMG_PATH))","b0e1f3db":"Test_Path = IMG_PATH + \"test_set\/\"\nTraining_Path = IMG_PATH + \"training_set\/\"\nValidation_Path = IMG_PATH + \"validation_set\/\"","b24de558":"def number_of_imgs(path):\n    print(path)\n    for value in os.listdir(path):\n        print(value, \"has\", len(os.listdir(path + value)), \"imgs\")\n    print('\\n')\n    \n    \nnumber_of_imgs(Test_Path)\nnumber_of_imgs(Training_Path)\nnumber_of_imgs(Validation_Path)","1dc4759e":"def create_dataframe(path):\n    data = []\n    for value in os.listdir(path):\n        for image in os.listdir(path + value + \"\/\"):\n            file_path = path + value + \"\/\" + image\n            # if hemmorhage than set if to 1 else 0\n            hemmorhage = 1 if value.lower() == \"hemmorhage_data\" else 0\n            data.append({\"path\": file_path, 'hemmorhage': hemmorhage})\n            \n    df = pd.DataFrame(data=data).sample(frac=1).reset_index(drop=True)\n\n    return df","80a8b5b0":"%matplotlib inline\ndef plot_imgs(title, paths):\n    fig = plt.figure(figsize=(14, 8), dpi=72)\n    fig.suptitle(title, fontsize=24, y=1.05)\n    for i, row in paths.iterrows():\n        img=mpimg.imread(row['path'])\n        plt.subplot(3, 5, i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        subtitle = 'YES' if row['hemmorhage'] == 1 else 'NO'\n        plt.title(subtitle)\n        plt.imshow(img)\n    plt.tight_layout()\n    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None)\n    plt.show()\n    \n    \nplot_imgs(\"Test\", create_dataframe(Test_Path).sample(15).reset_index(drop=True))\nplot_imgs(\"Train\", create_dataframe(Training_Path).sample(15).reset_index(drop=True))\nplot_imgs(\"Validation\", create_dataframe(Validation_Path).sample(15).reset_index(drop=True))","01afd989":"def crop_imgs(set_name, add_pixels_value=0):\n    \"\"\"Finds the extreme points on the image and crops the rectangular out of them\"\"\"\n    set_new = []\n    for img in set_name:\n        img = cv2.imread(img)\n        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # threshold the image, then perform a series of erosions +\n        # dilations to remove any small regions of noise\n        thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n        thresh = cv2.erode(thresh, None, iterations=2)\n        thresh = cv2.dilate(thresh, None, iterations=2)\n\n        # find contours in thresholded image, then grab the largest one\n        cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cnts = imutils.grab_contours(cnts)\n        c = max(cnts, key=cv2.contourArea)\n\n        # find the extreme points\n        ext_left = tuple(c[c[:, :, 0].argmin()][0])\n        ext_right = tuple(c[c[:, :, 0].argmax()][0])\n        ext_top = tuple(c[c[:, :, 1].argmin()][0])\n        ext_bot = tuple(c[c[:, :, 1].argmax()][0])\n\n        add_pixels = add_pixels_value\n        new_img = img[ext_top[1] - add_pixels:ext_bot[1] + add_pixels\n        , ext_left[0] - add_pixels:ext_right[0] + add_pixels].copy()\n        set_new.append(new_img)\n        \n    return np.array(set_new)\n\ncrop_imgs(create_dataframe(Test_Path)['path'])\ncrop_imgs(create_dataframe(Training_Path)['path'])\ncrop_imgs(create_dataframe(Validation_Path)['path'])\n\nclear_output()","50b4247a":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.25],\n    horizontal_flip=True,\n    vertical_flip=True,\n    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n)\n\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n    Training_Path,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=32,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)\n\n\nvalidation_generator = test_datagen.flow_from_directory(\n    Validation_Path,\n    color_mode='rgb',\n    target_size=IMG_SIZE,\n    batch_size=16,\n    class_mode='binary',\n    seed=RANDOM_SEED\n)","38bf293b":"vgg16_weight_path = '..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nbase_model = tf.keras.applications.VGG16(\n    weights=vgg16_weight_path,\n    include_top=False,\n    input_shape=IMG_SIZE + (3,)\n)\n\nmodel = tf.keras.models.Sequential()\nmodel.add(base_model)\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.layers[0].trainable = False\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=['accuracy']\n)\n\nmodel.summary()","e272fae7":"EPOCHS = 25\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    patience=6\n)\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=50,\n    epochs=EPOCHS,\n    validation_data=validation_generator,\n    validation_steps=25,\n    callbacks=[early_stopping]\n)\n\nprint(\"Training Done\")\nmodel.save(\"model.h5\")","a97a2537":"def preprocess_imgs(path, img_size):\n    set_new = []\n    for value in os.listdir(path):\n        for img in os.listdir(path + value):\n            img = cv2.imread(path + value + \"\/\" + img)\n            img = cv2.resize(\n                img,\n                dsize=img_size,\n                interpolation=cv2.INTER_CUBIC\n            )\n            set_new.append(tf.keras.applications.vgg16.preprocess_input(img))\n    \n    return np.array(set_new)\n\ntest_data = preprocess_imgs(Test_Path, img_size=IMG_SIZE)\n\nreality = []\nfor value in os.listdir(Test_Path):\n    for img in os.listdir(Test_Path + value):\n        reality.append(1) if value.lower() == \"hemmorhage_data\" else reality.append(0)\n        \npredictions = model.predict(test_data)\npredictions = [0 if x > 0.5 else 1 for x in predictions]\n\naccuracy = accuracy_score(reality, predictions)\nprint(\"Test Accuracy:\", accuracy)","54054c12":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(1, len(history.epoch) + 1)\n\n\nplt.figure(figsize=(10,5))\n\nplt.plot(epochs_range, acc, label='Training')\nplt.plot(epochs_range, val_acc, label='Validation')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Model Accuracy')\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10,5))\n\nplt.plot(epochs_range, loss, label='Training')\nplt.plot(epochs_range, val_loss, label='Validation')\nplt.legend(loc=\"best\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Model Loss')\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\nplt.tight_layout()\nplt.show()\n","2dde476a":"confusion_mtx = confusion_matrix(reality, predictions)\n\nax = plt.axes()\nsn.heatmap(confusion_mtx, annot=True,annot_kws={\"size\": 25}, cmap=\"Blues\", ax = ax)\nax.set_title('Test Accuracy', size=14)\nplt.show()","bfa72381":"So for testing, we have 20, for training 140 and for validation 40 images. This is not really great for training and validation.<br\/><br\/>\nBut first, let's look at them.   \nFor that, we first need a dataframe.","5fdc44e2":"## 5. Accuracy <a name=\"accuracy\"><\/a>\nFor testing its accuracy we will use the test data.  <br\/><br\/>\nFor that, we will need to preprocess them as the vgg16 model needs it.","876c8e4e":"Now we can plot the accuracy and loss over time.","9866ad79":"## 3. Data Augmentation <a name=\"dataaug\"><\/a>\nAs I have already mentioned 140 training images is not really enough in order to get a satisfactory result. <br\/>   \nTherefore I will use something called [Data Augmentation](https:\/\/medium.com\/nanonets\/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced) (basically making new data from existing one by turning it, zooming in or out, changing brightness, etc.).  <br\/>  \nIn tensorflow one can use a ImageDataGenerator.","520bcc00":"We can see that there are 3 folders ('training_set', 'validation_set' and 'test_set') each with a positive and a negative folder full of images.","e5a83317":"Let's see how many images each folder contains.","180ef730":"## 4. Model <a name=\"model\"><\/a>\nNow let's load the VGG16 model and tweak it to work for our purpose.","03488ce2":"author: Raul C. S\u00eempetru <br\/>\n## 1. Project Information <a name=\"projectinfo\"><\/a> \nThis is the first machine learning project I have ever done without a tutorial to follow. I hope you will enjoy it.  <br\/><br\/>\nFor this project, I will be using the [VGG16](https:\/\/neurohive.io\/en\/popular-networks\/vgg16\/) model for image classification.  <br\/><br\/>\nThe main idea of the project is to create a NN which is capable of detecting tumors.  \nI will also use [data augmentation](https:\/\/medium.com\/nanonets\/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced) and will try to auto-crop the images to remove redundant information.","b62ec0aa":"Now let's plot 15 random images for each folder\/set.","912b870d":"A confusion matrix is also very nice for illustrating our success.","2f8c6313":"Now let's train the model and then save it.","9a7432e8":"## 2. Auto-Cropping <a name=\"autocrop\"><\/a>\nNot all the pictures are perfectly aligned in the center so I will try to auto-crop the redundant information out by using the algorithm I found [here](https:\/\/www.pyimagesearch.com\/2016\/04\/11\/finding-extreme-points-in-contours-with-opencv\/). <br\/><br\/>\nThis is a picture that shows what the script is supposed to do.\n![The picture should have been here => link is broken](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2016\/04\/extreme_points_header.jpg)"}}