{"cell_type":{"e89cdf19":"code","492a23f1":"code","78a88a29":"code","02057f39":"code","946a0866":"code","a2dfa691":"code","e7c8fb6b":"code","20d7dae0":"code","a0107729":"code","c8d65b7f":"code","a944ce53":"code","7b503979":"code","883553d2":"code","31bb5154":"code","45cb376b":"code","c7f71e3f":"code","ec853e40":"code","6486955a":"code","c854e90a":"code","19682954":"code","62559521":"code","17bb8a08":"code","7ffcc678":"code","d3ed4ee5":"code","606f1f5f":"code","26af415c":"code","b03b3ecc":"code","b42ac746":"code","a21c3627":"code","55a52706":"code","09f1f7e9":"code","579e184c":"code","088550a0":"markdown","ca1db2c9":"markdown","7c88bf29":"markdown","34a7b6ab":"markdown","64f1f3ce":"markdown","666ebecd":"markdown","f87cab82":"markdown","59799aaf":"markdown","ed6187c4":"markdown","073d376e":"markdown","94b552c1":"markdown","406fe849":"markdown","4923cdad":"markdown","3c8a20af":"markdown","43de7a48":"markdown","5296bce2":"markdown","7212e7ea":"markdown","cccf30c4":"markdown","7727d2b4":"markdown","3c2c0b81":"markdown","e0e41c39":"markdown"},"source":{"e89cdf19":"import logging\nfrom xml.etree import ElementTree\nfrom urllib.request import urlopen\n\nimport pandas as pd\n\n\ndef sitemap_to_df(sitemap_url):\n    xml_text = urlopen(sitemap_url)\n    tree = ElementTree.parse(xml_text)\n    root = tree.getroot()\n        \n    xml_df = pd.DataFrame()\n\n    if root.tag.split('}')[-1] == 'sitemapindex':\n        for elem in root:\n            for el in elem:\n                if el.text.endswith('xml'):\n                    try:\n                        xml_df = xml_df.append(sitemap_to_df(el.text), ignore_index=True)\n                        logging.info(msg='Getting ' + el.text)\n                    except Exception as e:\n                        logging.warning(msg=str(e) + el.text)\n                        xml_df = xml_df.append(pd.DataFrame(dict(sitemap=el.text),\n                                                            index=range(1)), ignore_index=True)\n\n    else:\n        logging.info(msg='Getting ' + sitemap_url)\n        for elem in root:\n            d = {}\n            for el in elem:\n                tag = el.tag\n                name = tag.split('}', 1)[1] if '}' in tag else tag\n\n                if name == 'link':\n                    if 'href' in el.attrib:\n                        d.setdefault('alternate', []).append(el.get('href'))\n                else:\n                    d[name] = el.text.strip() if el.text else ''\n            d['sitemap'] = sitemap_url\n            xml_df = xml_df.append(pd.DataFrame(d, index=range(1)), ignore_index=True)\n    if 'lastmod' in xml_df:\n        xml_df['lastmod'] = pd.to_datetime(xml_df['lastmod'], utc=True)\n    if 'priority' in xml_df:\n        xml_df['priority'] = xml_df['priority'].astype(float)\n    return xml_df\n\n    ","492a23f1":"bloomberg = pd.read_csv('..\/input\/bloomberg-business-articles-urls\/bloomberg_biz_sitemap.csv',\n                        parse_dates=['lastmod'], index_col='lastmod',\n                        low_memory=False)\nbloomberg['priority'] = bloomberg['priority'].astype(float)\nprint(bloomberg.shape)\nbloomberg","78a88a29":"bloomberg[bloomberg.index.isna()]['sitemap'].str.contains('video').mean()","02057f39":"bloomberg[bloomberg.index.notna()]['sitemap'].str.contains('video').mean()","946a0866":"by_year_count = bloomberg.resample('A')['loc'].count()\nby_year_count","a2dfa691":"by_month_count = bloomberg.resample('M')['loc'].count()\nby_month_count","e7c8fb6b":"import plotly.graph_objects as go\nfig = go.Figure()\nfig.add_bar(x=by_year_count.index, y=by_year_count.values, showlegend=False)\nfig.layout.template = 'none'\nfig.layout.title = 'Bloomberg Business Articles Published per Year 1991 - 2020'\nfig.layout.xaxis.tickvals = by_year_count.index.date\nfig.layout.xaxis.ticktext = by_year_count.index.year\nfig.layout.yaxis.title = 'Number of articles'\nfig","20d7dae0":"fig = go.Figure()\nfig.add_bar(x=by_month_count.index, y=by_month_count.values, showlegend=False)\nfig.layout.template = 'none'\nfig.layout.title = 'Bloomberg Business Articles Published per Month 1991 - 2020'\nfig.layout.yaxis.title = 'Number of articles'\nfig","a0107729":"(bloomberg\n .groupby(bloomberg.index.weekday)['loc']\n .count().to_frame()\n .rename(columns=dict(loc='count'))\n .assign(day=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n .style.bar().format(dict(count='{:,}'))\n)","c8d65b7f":"bloomberg[:1]['loc'].values[0], bloomberg[:1]['loc'].index","a944ce53":"bloomberg['loc'].str.split('\/').str[-3].value_counts()","7b503979":"bloomberg['loc'].str.split('\/').str[-4].value_counts()","883553d2":"bloomberg['loc'].str.split('\/').str[-1]","31bb5154":"!pip install advertools","45cb376b":"import advertools as adv\nadv.word_frequency(bloomberg['loc'].dropna().str.split('\/').str[-1].str.replace('-', ' ')).iloc[:20, :2]","c7f71e3f":"adv.word_frequency(bloomberg['loc'].dropna().str.split('\/').str[-1].str.replace('-', ' '),phrase_len=2).iloc[:20, :2]","ec853e40":"bloomberg['loc'].dropna()[bloomberg['loc'].dropna().str.contains('-dot')].str.split('\/').str[-1][:10].values","6486955a":"by_year_count_china = bloomberg[bloomberg['loc'].fillna('').str.contains('china', case=False)].resample('A')['loc'].count()","c854e90a":"fig = go.Figure()\nfig.add_scatter(x=by_year_count.index,\n                y=by_year_count.values,\n                name='All Articles',\n                yaxis='y', \n                mode='lines+markers',\n                marker=dict(size=10))\nfig.add_scatter(x=by_year_count_china.index,\n                y=by_year_count_china.values,\n                name='China Articles',\n                yaxis='y2',\n                mode='lines+markers',\n                marker=dict(size=10))\nfig.layout.template = 'none'\nfig.layout.title = 'BusinessWeek Articles Published per Year 2001 - 2020 (China vs All Topics)'\nfig.layout.xaxis.tickvals = by_year_count.index.date\nfig.layout.xaxis.ticktext = by_year_count.index.year\nfig.layout.yaxis.title = 'All articles'\nfig.layout.yaxis2 = dict(title='China', overlaying='y', side=\"right\", position=1, anchor='free')\n\n\nfig","19682954":"def plot_topic_vs_all(*topics, include_all=True, log_y=True):\n    \n    fig = go.Figure()\n    if include_all:\n        fig.add_scatter(x=by_year_count.index,\n                        y=by_year_count.values,\n                        name='All Articles',\n                        yaxis='y', \n                        mode='lines+markers',\n                        marker=dict(size=10))\n    for topic in topics:\n        topic_df = bloomberg[bloomberg['loc'].fillna('').str.contains(topic, case=False)].resample('A')['loc'].count()\n        fig.add_scatter(x=topic_df.index,\n                        y=topic_df.values,\n                        name=topic + ' Articles',\n                        mode='lines+markers',\n                        yaxis='y2',\n                        marker=dict(size=10))\n    fig.layout.template = 'none'\n    all_topics = ' vs. All Topics)' if include_all else ')'\n    fig.layout.title = f'BusinessWeek Articles Published per Year 2001 - 2020 ({\", \".join(topics)}' + all_topics\n    fig.layout.xaxis.tickvals = topic_df.index.date\n    fig.layout.xaxis.ticktext = topic_df.index.year\n    fig.layout.yaxis.title = 'All articles'\n    fig.layout.yaxis2 = dict(title=f\"'{topics}' articles\", overlaying='y',\n                             side=\"right\", position=1, anchor='free')\n    if log_y:\n        fig.layout.yaxis.type = 'log'\n        fig.layout.yaxis2.type = 'log'\n\n\n    return fig","62559521":"plot_topic_vs_all('oil', 'china', include_all=True, log_y=True)","17bb8a08":"plot_topic_vs_all('trump', 'china', include_all=False, log_y=True)","7ffcc678":"plot_topic_vs_all('trump', 'huawei', 'china', include_all=False)","d3ed4ee5":"plot_topic_vs_all('trump', 'fed', include_all=False)","606f1f5f":"bloomberg[bloomberg['loc'].str.contains('articles\/1000')].iloc[:10, :1]['loc'].str[30:]","26af415c":"bloomberg['loc'] = bloomberg['loc'].str.replace('articles\/1000-', 'articles\/2000-')","b03b3ecc":"bloomberg['pub_date'] = pd.to_datetime(bloomberg['loc'].str.extract('(\\d{4}-\\d{2}-\\d{2})')[0])\nbloomberg['pub_date']","b42ac746":"bloomberg['dates_equal'] = bloomberg['pub_date'].dt.date == bloomberg.index.date","a21c3627":"bloomberg['dates_equal'].mean()","55a52706":"bloomberg['days_to_update'] = bloomberg['pub_date'].dt.date - bloomberg.index.date","09f1f7e9":"bloomberg.iloc[:5, [0, 1, 2, 3, 4, -3, -2, -1]]","579e184c":"bloomberg['days_to_update'].value_counts(normalize=True)[:15]","088550a0":"## Word Frequency \/ Text Analysis\n\nFor more information on the [`word_frequency`](https:\/\/advertools.readthedocs.io\/en\/master\/advertools.word_frequency.html) function, you can check out my article on [text analysis for online marketing](https:\/\/www.semrush.com\/blog\/text-analysis-for-online-marketers\/).","ca1db2c9":"It seems only 24.7% of the values are equal. Let's create a new column that counts the difference between those two, for a better view and see if there are large differences.","7c88bf29":"`dates_equal` compares the inferred `pub_date` with the `lastmod` tag.","34a7b6ab":"## Compare `lastmod` to Publishing Date\n\nArticle URLs contain the date of publishing. These can be compared to the `lastmod` tag value to see whether or not they match. This can serve three purposes: \n1. Check to see how often stories are updated. In a news website like this I don't expect stories to be updated a lot because they have a short shelf life. In other sites that provide less time-sensitive content (like recipes or exercise for example) you might expect them to do more updates. \n2. Check if there are big discrepancies in the data, as we will do for this case. \n3. When there is no `lastmod` like in the video URLs, this date can serve that purpose, and you can do the same exercise for video articles.\nLooking at some of the dates included in the URLs I realized some were dated in the year one thousand. ","64f1f3ce":"Now we can extract those numbers, put them in a new column `pub_date` as datetime objects.","666ebecd":"The dates in `lastmod` seem to be correct, but not in the URLs. Let's change those to 2000","f87cab82":"## Publishing Trends\nThe first question I usually ask, is how often do they publish?  \nAs this is a large time frame, we can first take a look at articles published per year, and then zoom in further to monthly trends. ","59799aaf":"I would expect Monday to be the busiest day, but the difference seems massive.\n\n## URL Analysis\nNow let's take a look at the URLs and try to get some information from them. First, a quick look at a URL to see their structure:","ed6187c4":"There are clear and very interesting trends in their publishing. It slowed down in 1996 after gowing gradually up. It shot up almost four times in 2001, then maintained a certain publishing level. The sharp drop after 2009 could be attributed to the financial crisis, as many financial institutions were having a difficult time. But Bloomberg? I mean Mike just spent half a billion on his presidential campaign...  \nThe sudden drop in 2015, followed by a massive spike in 2019 are strange. It's not clear to me why. Maybe they just decided to focus much more on their website and publish more. There might be issues with the data. I'll try to check using dates in the URLs below, which indicates that the data might be valid, but it's still worth investigating, as the articles in 2019 were almost eighteen times more than 2018, and 2020 is on track to top that.  \nI'll assume that the data are correct, as this is just an exploration on what you can extract from sitemaps. \nWe can visualize the same trend, on a monthly basis and see: ","073d376e":"300k URLs seems like a lot, which makes it more interesting.  \nSome of the values of the index are `NaT` which is the missing date\/time representation. After manually checking, I realized many of them were for video URLs. Let's quantify this:","94b552c1":"Now the year 2019 looks really strange. If you zoom you'll see a jump from 159 articles in March, and then 2,360 articles the following month. It's highly unlikely that the editorial team grew that fast and immediately. Especially that they maintained that level of publishing throughout the year. They might have hired a huge number of people all of a sudden, made a large acquisition, or something like that. \nMarch 2020, the month of Corona, shows the highest publishing level, which makes sense.  \nWe can also see their weekday trends: ","406fe849":"# XML Sitemaps Analysis with Python\n\nThis is a quick tutorial on how to get and parse XML sitemaps into a `pandas` `DataFrame`, and how they can be analyzed.  \n\nThe `lastmod` tag provides the ability to check the publishing frequency of the website, and how it changed across years, months, days, or any time unit.  \nThe `loc` tag which is basically the URL, can provide some limited information on the content, depending on what the website includes in the URL. The richer the URLs the more the information. URLs like http:\/\/example.com\/article\/12345 provide nothing interesting.\n\nWe first define a function to parse the sitemaps (it is actually taken from [scrapy's code](https:\/\/github.com\/scrapy\/scrapy\/blob\/master\/scrapy\/utils\/sitemap.py) with many modifications). \nThis function takes a sitemap URL, goes through all the elements and puts them in a `DataFrame`. It would typically have the columns `\"loc\", \"lastmod\", \"changefreq\", \"priority\", \"alternate\"`, but not necessarily all of them.  \nIf the sitemap happens to be a sitemap index, then it goes through all of the inclcuded sitemaps one by one, and returns everything in one `DataFrame`.\n","4923cdad":"In some cases it makes sense, like the title that contains \"dot-com era\" for example. But in the others, they have decided to replaces dots \".\" with the word \"dot\". For example \"Amazon vs. Jet.com\" was transformed to \"amazon-vs-dot-jet-dot-com\". Strange. \n\nNow let's extract all URLs that contain \"china\", resample annually, and plot.","3c8a20af":"# Bloomberg Business Articles Sitemaps\n\nAs an example I'll go through the sitemaps of Bloomberg business articles from 1991 to March 2020.   \nA quick look at their [sitemap index](https:\/\/www.bloomberg.com\/feeds\/businessweek\/sitemap_index.xml) reveals that they have a template for their sitemaps as follows: \n`https:\/\/www.bloomberg.com\/feeds\/businessweek\/sitemap_{year}_{month}.xml`  \nIt seems that they have a sitemap for each month. There are also video sitemaps.\n\nI specified the column `lastmod` as the index, and parsed it as a date object, which makes the DataFrame a timeseries. This makes it easy to resample it to any required time unit.","43de7a48":"Most of the words are too generic. \"China\", \"Fed\", and \"Trump\" might be useful.  \nWe can also run the same function by specifying `phrase_len` to be two or more. This counts the phrases that contain two words, where the previous function implicitly specified one.","5296bce2":"It seems this is the pattern that they use:  \n`bloomberg.com\/news\/{type}\/{YYY-MM-DD}\/slug-containing-important-keywords`  \nSo let's see how many `type`s of articles they have: ","7212e7ea":"Pretty much all of the URLs are `news`, so we can ignore that.  \nThe last part of the URL is the one that contains the most important information. We can now replace dashes with spaces, and count the words in those slugs.","cccf30c4":"What the dot?!\n","7727d2b4":"99.8% of the sitemap URLs with a missing date contain the word \"video\", which means it is right. We can also check from the other direction (how many sitemap URLs with missing date and time correspond to URLs with \"video\"):","3c2c0b81":"In most cases, the trend of publishing stories about China seems to be in line with all articles, as you would expect for such an important topic. Note that the scales are different for each of Y axes.  \nIt's a bit misleading to see the big spike in articles on China, but the problem is mainly due to the big spike from 2018 to 2019 in all articles. In these cases it might be better to plot them on a log scale, so we can see percentage changes more clearly, which is what we actually want.  \nThe above script is generalized below to a function that takes an arbitrary number of words, if you want to compare three or four or more. It optionally allows you to include the trend for all the articles, for perspective on how the topic(s) compare to the total, and specifying whether or not you want to have the Y-axis on a log scale: ","e0e41c39":"The majority of the updates seem to happen one or zero days after publishing, which means not much changes happen after publishing.  \n\nThere are many other text mining and topic modeling techniques that can be run on this data. The fact that it has dates makes it even more interesting, because you can also compare the changes in the topics and keywords across time, and see if you can get better insights. "}}