{"cell_type":{"9bfc3680":"code","9e959ce3":"code","90fcd3ec":"code","71d61321":"code","7515175c":"code","9321c83c":"code","21c1c380":"code","09f7f02c":"code","99ae2baa":"code","4df1188a":"code","a25a8e46":"code","5c61c728":"code","d59cab37":"code","f8bd456b":"code","adb6a5c9":"code","1eb46c8a":"code","d41b4008":"code","f6efe0bf":"markdown"},"source":{"9bfc3680":"import nltk\n# nltk.download()\nfrom nltk.tokenize import word_tokenize","9e959ce3":"import pandas as pd\nimport numpy as np\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n\npd.set_option('display.max_colwidth', -1)","90fcd3ec":"data = pd.read_csv(\"..\/input\/twitter-text-emotions\/text_emotion.csv\")\nprint(data.head(2))\nprint(data['content'][:10])","71d61321":"#data cleaning\nclean_data = []\n\n# start cleaning data\nfor text in data['content']:\n#     print('\\nmain text=> ',text)\n    tokens = word_tokenize(text) # split the tokens\n    tokens = [w.lower() for w in tokens] # convert to lower case\n#     print('tokens=> ',tokens)\n    # remove punctuation from each word\n    import string\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    words = [word for word in stripped if word.isalpha()] # filter all the punciations\n#     print('words=> ', words)\n    \n    # filtering stop words\n    from nltk.corpus import stopwords\n    stop_words = stopwords.words('english')\n#     print(stop_words)\n    words = [w for w in words if not w in stop_words]\n#     print('words after stop words=> ', words)\n    \n    # stemming of words\n    from nltk.stem.porter import PorterStemmer\n    porter = PorterStemmer()\n    stemmed_words = [porter.stem(word) for word in words]\n#     print('stemmed words=> ', stemmed_words)\n    clean_text = ''\n    for w in stemmed_words:\n        clean_text += ' ' + w\n#     print(clean_text)\n    clean_data.append(clean_text)\n\nprint('Clean data created')\ndata['clean_tweet'] = clean_data\n# print(data['clean_tweet'])\ndata['tweet_len'] = data['clean_tweet'].apply(len)\n# print(data['tweet_len'])  \n    \n","7515175c":"# data.groupby(['tweet_len', 'sentiment']).size().unstack().plot(kind='line', stacked=False)\ndata['sentiment'].value_counts().plot(kind='bar', stacked=False)","9321c83c":"# tokenization\nmax_words = 2000\ntokenizer = Tokenizer(num_words=max_words, split=' ')\ntokenizer.fit_on_texts(data['clean_tweet'].values)\nX = tokenizer.texts_to_sequences(data['clean_tweet'].values)\n# print(data['clean_tweet'][:5])\n# print('Tokenized sentences', X[:5])\nX = pad_sequences(X, maxlen=32)\n# print(X[:5])\nprint(X.shape[1])\n","21c1c380":"enbedding_out_dim = 256\nlstm_out_dim = 256\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, enbedding_out_dim,input_length = X.shape[1]))\nmodel.add(LSTM(lstm_out_dim+1))\nmodel.add(Dense(13,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","09f7f02c":"# data set to train\ndummies = pd.get_dummies(data['sentiment'])\nY = dummies.values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 50)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","99ae2baa":"# print(data['sentiment'][:10], '->', Y[:10])\ndict_emotion = {}\ndict_label = {}\nfor i in range(len(Y)):\n    dict_emotion[data['sentiment'][i]] = np.argmax(Y[i])\n    dict_label[np.argmax(Y[i])] = data['sentiment'][i]\n    if len(dict_emotion) == 13:\n        print('Break at: ', i)\n        break\n#     print(data['sentiment'][i], '->', Y[i])\nprint(dict_emotion, dict_label)","4df1188a":"X_val = X_train[:500]\nY_val = Y_train[:500]\npartial_X_train = X_train[500:]\npartial_Y_train = Y_train[500:]\n","a25a8e46":"# train the net\nbatch_size = 512\nhistory = model.fit(X_train,Y_train, \n                    epochs = 50, \n                    batch_size=batch_size,\n                    validation_data=(X_val, Y_val))\n","5c61c728":"import matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d59cab37":"# validation\ntotal, correct, false = 0, 0, 0\n# print(len(X_val))\nfor x in range(len(X_val)):\n    total += 1\n#     print(x)\n\n    result = model.predict(X_val[x].reshape(1, X_test.shape[1]), batch_size=1)[0]\n#     print(np.argmax(result), np.argmax(Y_val[x]))\n\n    if np.argmax(result) == np.argmax(Y_val[x]):\n        correct += 1\n\n    else:\n        false += 1\nprint(\"accuracy\", correct \/ total * 100, \"%\")\n# print(\"negative accuracy\", neg_correct \/ negative_count * 100, \"%\")","f8bd456b":"covid_data = pd.read_csv(\"..\/input\/twitter-text-emotions\/20200410_134114_covid_19_tweets.csv\", usecols=[0,5], nrows=1000)\n# print(covid_data.head(2))\nprint(covid_data['tweet_text'][:10])","adb6a5c9":"#data cleaning\nclean_tweet_data = []\n# filtering stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport string\nfrom nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n\n# start cleaning data\nfor text in covid_data['tweet_text']:\n#     print('\\nmain text=> ',text)\n    tokens = word_tokenize(text) # split the tokens\n    tokens = [w.lower() for w in tokens] # convert to lower case\n#     print('tokens=> ',tokens)\n    # remove punctuation from each word\n    \n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    words = [word for word in stripped if word.isalpha()] # filter all the punciations\n#     print('words=> ', words)\n    \n    \n#     print(stop_words)\n    words = [w for w in words if not w in stop_words]\n#     print('words after stop words=> ', words)\n    \n    # stemming of words\n    \n    stemmed_words = [porter.stem(word) for word in words]\n#     print('stemmed words=> ', stemmed_words)\n    clean_text = ''\n    for w in stemmed_words:\n        clean_text += ' ' + w\n#     print(clean_text)\n    clean_tweet_data.append(clean_text)\n\nprint('Clean data created')\ncovid_data['clean_tweet'] = clean_tweet_data\n# print(data['clean_tweet'])\ncovid_data['tweet_len'] = covid_data['clean_tweet'].apply(len)\n# print(covid_data['tweet_len'])  \n    \n","1eb46c8a":"# tokenization\nmax_words = 2000\ntokenizer = Tokenizer(num_words=max_words, split=' ')\ntokenizer.fit_on_texts(covid_data['clean_tweet'].values)\nX = tokenizer.texts_to_sequences(covid_data['clean_tweet'].values)\n# print(covid_data['clean_tweet'][:5])\n# print('Tokenized sentences', X[:5])\n# print(X.shape[1])\nX = pad_sequences(X, maxlen=32)\n# print(X[:5])\n\n# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 50)\nprint(X.shape[1])\n# print(X_test.shape,Y_test.shape)","d41b4008":"total, correct, false = 0, 0, 0\n# print(len(X))\ncount = {}\nfor i in range(13):\n    count[i] = 0\n    \nfor i in range(len(X)):\n#     print(i, '->', X[i])\n    total += 1\n\n    result = model.predict(X[i].reshape(1,X_test.shape[1]), batch_size=1)[0]\n    emotion_value = np.argmax(result)\n    count[emotion_value] += 1\n    emotion = dict_label[emotion_value]\n#     print(covid_data['tweet_text'][i], '->', emotion)\n\n#     if np.argmax(result) == np.argmax(Y_val[x]):\n#         correct += 1\n\n#     else:\n#         false += 1\nfor i in range(13):\n    print('number of status with emotion', \"'\" + dict_label[i]+\"'\",': ', count[i])\n# print(\"accuracy\", correct \/ total * 100, \"%\")\n# print(\"negative accuracy\", neg_correct \/ negative_count * 100, \"%\")","f6efe0bf":"# Start classification\nIn this part I'll import some tweets about COVID-19 and will classify them "}}