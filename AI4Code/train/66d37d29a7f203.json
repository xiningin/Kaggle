{"cell_type":{"171403fd":"code","5a9b0a80":"code","be1a1d45":"code","c5e4e291":"code","f682bb3d":"code","5aaa6a3c":"code","138514f6":"code","c03787ac":"code","1c7855b1":"code","fe240e90":"code","07316d3a":"code","f59e79cc":"code","329f2d76":"code","6d8bbc8e":"code","ed894fd5":"code","7d1d9537":"code","d905db77":"code","f2020a3e":"code","70316b5b":"code","03392f23":"code","ec5054b3":"code","752081e5":"code","1eab8d47":"code","e77303e0":"code","5eb034ff":"code","b3912959":"code","ee5e1b19":"markdown"},"source":{"171403fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a9b0a80":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.layers import Conv2D,MaxPooling2D,UpSampling2D,Input,BatchNormalization,LeakyReLU\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\n#from tensorflow import set_random_seed\n#set_random_seed(123)\n#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n#tf.keras.backend.set_session(sess)\n#set_random_seed(2)\nnp.random.seed(1)\n\nprint(os.listdir(\"\/kaggle\/input\"))","be1a1d45":"#ImagePath=\"\/kaggle\/input\/architecture-colorization\/Tudor Revival architecture\/\"\nImagePath=\"\/kaggle\/input\/architecture-dataset\/arcdataset\/arcDataset\/Greek Revival architecture\/\"","c5e4e291":"img = cv2.imread(ImagePath+\"196_800px-Harold_E._Stearns_House%2C_Montreal_03.jpg\")\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nimg = cv2.resize(img, (150, 150))\nplt.imshow(img)\nimg.shape","f682bb3d":"X_img=[]\ny_img=[]\nX_img1=[]\ny_img1=[]","5aaa6a3c":"#LAB\nHEIGHT=224\nWIDTH=224\n#ImagePath=\"..\/input\/dataset\/dataset_updated\/training_set\/painting\/\"\n\ndef ExtractInput(path,X_img,y_img):\n#     X_img=[]\n#     y_img=[]\n    for imageDir in os.listdir(ImagePath):\n        try:\n            img = cv2.imread(ImagePath + imageDir)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    \n            img = img.astype(np.float32)\n            img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n            #Convert the rgb values of the input image to the range of 0 to 1\n            #1.0\/255 indicates that we are using a 24-bit RGB color space.\n            #It means that we are using numbers between 0\u2013255 for each color channel\n            #img_lab = 1.0\/225*img_lab\n            # resize the lightness channel to network input size \n            img_lab_rs = cv2.resize(img_lab, (WIDTH, HEIGHT)) # resize image to network input size\n            img_l = img_lab_rs[:,:,0] # pull out L channel\n            #img_l -= 50 # subtract 50 for mean-centering\n            img_ab = img_lab_rs[:,:,1:]#Extracting the ab channel\n            img_ab = img_ab\/128\n            #The true color values range between -128 and 128. This is the default interval \n            #in the Lab color space. By dividing them by 128, they too fall within the -1 to 1 interval.\n            X_img.append(img_l)\n            y_img.append(img_ab)\n        except:\n            pass\n    X_img = np.array(X_img)\n    y_img = np.array(y_img)\n    \n    return X_img,y_img","138514f6":"#RGB-Gray\n#LAB\nHEIGHT=224\nWIDTH=224\n#ImagePath=\"..\/input\/dataset\/dataset_updated\/training_set\/painting\/\"\n\ndef ExtractInput_rgb(path,X_img,y_img):\n#     X_img=[]\n#     y_img=[]\n    for imageDir in os.listdir(ImagePath):\n        try:\n            img = cv2.imread(ImagePath + imageDir)\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n\n    \n            img_rgb = img_rgb.astype(np.float32)\/255\n            img_gray = img_gray.astype(np.float32)\/255  \n          # img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\n            #Convert the rgb values of the input image to the range of 0 to 1\n            #1.0\/255 indicates that we are using a 24-bit RGB color space.\n            #It means that we are using numbers between 0\u2013255 for each color channel\n            #img_lab = 1.0\/225*img_lab\n            # resize the lightness channel to network input size \n            img_rgb = cv2.resize(img_rgb, (WIDTH, HEIGHT)) # resize image to network input size\n            img_gray = cv2.resize(img_gray, (WIDTH, HEIGHT))\n\n            X_img.append(img_rgb)\n            y_img.append(img_gray)\n        except:\n            pass\n    X_img = np.array(X_img)\n    y_img = np.array(y_img)\n    \n    return X_img,y_img","c03787ac":"#ImagePath=\"\/kaggle\/input\/architecture-colorization\/Tudor Revival architecture\/\"\nImagePath=\"\/kaggle\/input\/architecture-dataset\/arcdataset\/arcDataset\/Greek Revival architecture\/\"","1c7855b1":"X_,y_ = ExtractInput(ImagePath,X_img,y_img) # Data-preprocessing\nX_1,y_1 = ExtractInput_rgb(ImagePath,X_img1,y_img1) # Data-preprocessing","fe240e90":"print(X_.shape) #LAB\nprint(y_.shape)\nprint(X_1.shape)#RGB\nprint(y_1.shape)","07316d3a":"X_[1].shape","f59e79cc":"X_1[1].shape","329f2d76":"import matplotlib.pyplot as plt\nplt.imshow(X_1[100],cmap=plt.cm.gray)","6d8bbc8e":"np.save(\"Architecture-L\",X_)","ed894fd5":"np.save(\"Architecture-AB\",y_)","7d1d9537":"np.save(\"Architecture-RGB\",X_1)","d905db77":"np.save(\"Architecture-Gray\",y_1)","f2020a3e":"K.clear_session()\ndef InstantiateModel(in_):\n    model_ = Conv2D(16,(3,3),padding='same',strides=1)(in_)\n    model_ = LeakyReLU()(model_)\n    #model_ = Conv2D(64,(3,3), activation='relu',strides=1)(model_)\n    model_ = Conv2D(32,(3,3),padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    model_ = BatchNormalization()(model_)\n    model_ = MaxPooling2D(pool_size=(2,2),padding='same')(model_)\n    \n    model_ = Conv2D(64,(3,3),padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    model_ = BatchNormalization()(model_)\n    model_ = MaxPooling2D(pool_size=(2,2),padding='same')(model_)\n    \n    model_ = Conv2D(128,(3,3),padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    model_ = BatchNormalization()(model_)\n    \n    model_ = Conv2D(256,(3,3),padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    model_ = BatchNormalization()(model_)\n    \n    model_ = UpSampling2D((2, 2))(model_)\n    model_ = Conv2D(128,(3,3),padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    model_ = BatchNormalization()(model_)\n    \n    model_ = UpSampling2D((2, 2))(model_)\n    model_ = Conv2D(64,(3,3), padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    #model_ = BatchNormalization()(model_)\n    \n    concat_ = concatenate([model_, in_]) \n    \n    model_ = Conv2D(64,(3,3), padding='same',strides=1)(concat_)\n    model_ = LeakyReLU()(model_)\n    model_ = BatchNormalization()(model_)\n    \n    model_ = Conv2D(32,(3,3),padding='same',strides=1)(model_)\n    model_ = LeakyReLU()(model_)\n    #model_ = BatchNormalization()(model_)\n    \n    model_ = Conv2D(2,(3,3), activation='tanh',padding='same',strides=1)(model_)\n\n    return model_","70316b5b":"Input_Sample = Input(shape=(HEIGHT, WIDTH,1))\nOutput_ = InstantiateModel(Input_Sample)\nModel_Colourization = Model(inputs=Input_Sample, outputs=Output_)","03392f23":"LEARNING_RATE = 0.0001\nModel_Colourization.compile(optimizer=Adam(lr=LEARNING_RATE),\n                            loss='mean_squared_error')\nModel_Colourization.summary()","ec5054b3":"def GenerateInputs(X_,y_):\n    for i in range(len(X_)):\n        X_input = X_[i].reshape(1,224,224,1)\n        y_input = y_[i].reshape(1,224,224,2)\n        yield (X_input,y_input)\nModel_Colourization.fit_generator(GenerateInputs(X_,y_),epochs=60,verbose=1,steps_per_epoch=30,shuffle=True)#,validation_data=GenerateInputs(X_val, y_val))","752081e5":"#TestImagePath=\"\/kaggle\/input\/architecture-colorization\/Tudor Revival architecture\/\"\nTestImagePath=\"\/kaggle\/input\/architecture-dataset\/arcdataset\/arcDataset\/Queen Anne architecture\/\"","1eab8d47":"def ExtractTestInput(ImagePath):\n    img = cv2.imread(ImagePath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    img_ = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    img_ = cv2.cvtColor(img_, cv2.COLOR_RGB2Lab)\n    img_=img_.astype(np.float32)\n    img_lab_rs = cv2.resize(img_, (WIDTH, HEIGHT)) # resize image to network input size\n    img_l = img_lab_rs[:,:,0] # pull out L channel\n    #img_l -= 50\n    img_l_reshaped = img_l.reshape(1,224,224,1)\n    \n    return img_l_reshaped","e77303e0":"ImagePath=TestImagePath+\"1129_800px-USA-San_Jose-318_North_Sixth_Street-2.jpg\"\nimage_for_test = ExtractTestInput(ImagePath)\nPrediction = Model_Colourization.predict(image_for_test)\nPrediction = Prediction*128\nPrediction=Prediction.reshape(224,224,2)\n#..\/input\/architecture-dataset\/arcDataset\/Queen Anne architecture\/1129_800px-USA-San_Jose-318_North_Sixth_Street-2.jpg","5eb034ff":"plt.figure(figsize=(30,20))\nplt.subplot(5,5,1)\nimg = cv2.imread(TestImagePath+\"1129_800px-USA-San_Jose-318_North_Sixth_Street-2.jpg\")\nimg_1 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nimg = cv2.cvtColor(img_1, cv2.COLOR_RGB2GRAY)\nimg = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\nimg = cv2.resize(img, (224, 224))\nplt.imshow(img)\n\nplt.subplot(5,5,1+1)\nimg_ = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\nimg_[:,:,1:] = Prediction\nimg_ = cv2.cvtColor(img_, cv2.COLOR_Lab2RGB)\nplt.title(\"Predicted Image\")\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nplt.title(\"Ground truth\")\nplt.imshow(img_1)","b3912959":"plt.figure(figsize=(30,20))\nplt.subplot(5,5,1)\nimg = cv2.imread(TestImagePath+\"1129_800px-USA-San_Jose-318_North_Sixth_Street-2.jpg\")\nimg_1 = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nimg = cv2.cvtColor(img_1, cv2.COLOR_RGB2GRAY)\nimg = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\nimg = cv2.resize(img, (224, 224))\nplt.imshow(img)\n\nplt.subplot(5,5,1+1)\nimg_ = cv2.cvtColor(img, cv2.COLOR_RGB2Lab)\nimg_[:,:,1:] = Prediction\nimg_ = cv2.cvtColor(img_, cv2.COLOR_Lab2RGB)\nplt.title(\"Predicted Image\")\nplt.imshow(img_)\n\nplt.subplot(5,5,1+2)\nplt.title(\"Ground truth\")\nplt.imshow(img_1)","ee5e1b19":"1Tudor Revival architecture.\n2.Queen Anne architecture\n3.American Foursquare architecture\n4.American craftsman style\n5.Bauhaus architecture\n6.Georgian architecture\n7.Greek Revival architecture"}}