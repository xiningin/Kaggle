{"cell_type":{"3b9d9585":"code","ed7fbc8e":"code","d685baef":"code","e4f10d8a":"code","3eb0aa69":"code","9e0ee64a":"code","1ff5429c":"code","4a599891":"code","0a54dc43":"code","91b28cc9":"code","f7590649":"code","b207eae6":"code","dd1b1580":"code","2ddc3c7a":"code","35332f84":"code","aedd780a":"code","fa0c21b0":"code","ffc9b4e2":"code","0c1b864e":"code","ceab5591":"code","c477caec":"code","ca65ea1c":"code","b62cefea":"code","79c0def9":"code","5b803813":"code","5a66bd66":"code","d05b536d":"code","f506e7c2":"code","8fb1c3ee":"code","c0ae4597":"code","14215592":"code","85f3ec42":"code","6c89c860":"code","ba2302ee":"code","7e6d98a3":"code","b10359f3":"code","e7dd192a":"code","faed6b42":"code","76b1a2bc":"code","ea39b02e":"code","931425e2":"code","8acc7097":"markdown","1edb017b":"markdown","be663670":"markdown","fce6b4f4":"markdown","36cbedcd":"markdown","17d8938a":"markdown","f20d1eb7":"markdown","a007073f":"markdown"},"source":{"3b9d9585":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed7fbc8e":"import keras\nfrom keras.preprocessing import image","d685baef":"from keras import applications as keras_applications","e4f10d8a":"!pip install keras_applications\n!pip install git+https:\/\/github.com\/rcmalli\/keras-vggface.git","3eb0aa69":"from keras_vggface.vggface import VGGFace\nfrom keras_vggface import utils","9e0ee64a":"os.system('tar -xf \/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/fer2013.tar.gz');\ndata = pd.read_csv('fer2013\/fer2013.csv')\ndata","1ff5429c":"data.Usage.unique()","4a599891":"train_data = data[data.Usage=='Training']\nval_data = data[data.Usage=='PublicTest']\ntest_data = data[data.Usage=='PrivateTest']","0a54dc43":"train_data.shape, val_data.shape, test_data.shape","91b28cc9":"import collections\nimport imblearn\noversampler = imblearn.over_sampling.RandomOverSampler()","f7590649":"collections.Counter(train_data.emotion)","b207eae6":"from keras.utils import to_categorical\n\nx_train, y_train = oversampler.fit_resample(train_data.pixels.values.reshape(-1,1),train_data.emotion.values)\n\n# x_train = train_data.pixels.values.reshape(-1,1)\n# y_train = train_data.emotion.values\n\nx_val = val_data.pixels.values.reshape(-1,1)\ny_val = val_data.emotion.values\n\nx_test = test_data.pixels.values.reshape(-1,1)\ny_test = test_data.emotion.values","dd1b1580":"collections.Counter(y_train)","2ddc3c7a":"x_train = list(x_train)\nx_val   = list(x_val)\nx_test  = list(x_test)\n\nfor i,item in enumerate(x_train):\n    x_train[i] = np.fromstring(item[0],sep=' ').reshape(48,48,1)\nfor i,item in enumerate(x_val):\n    x_val[i] = np.fromstring(item[0],sep=' ').reshape(48,48,1)\nfor i,item in enumerate(x_test):\n    x_test[i] = np.fromstring(item[0],sep=' ').reshape(48,48,1)\n    \nx_train = np.vstack(x_train).reshape(-1,48,48,1)\nx_val = np.vstack(x_val).reshape(-1,48,48,1)\nx_test = np.vstack(x_test).reshape(-1,48,48,1)","35332f84":"y_train = to_categorical(y_train,num_classes=7)\ny_val   = to_categorical(y_val  ,num_classes=7)\ny_test  = to_categorical(y_test ,num_classes=7)","aedd780a":"from copy import deepcopy as copy\n\ndef smooth_labels(y, smooth_factor):\n    '''Convert a matrix of one-hot row-vector labels into smoothed versions.\n\n    # Arguments\n        y: matrix of one-hot row-vector labels to be smoothed\n        smooth_factor: label smoothing factor (between 0 and 1)\n\n    # Returns\n        A matrix of smoothed labels.\n    '''\n    assert len(y.shape) == 2, 'input should be a batch of one-hot-encoded data'\n    y2 = copy(y)\n    if 0 <= smooth_factor <= 1:\n        # label smoothing ref: https:\/\/www.robots.ox.ac.uk\/~vgg\/rg\/papers\/reinception.pdf\n        y2 *= 1 - smooth_factor\n        y2 += smooth_factor \/ y.shape[1]\n    else:\n        raise Exception(\n            'Invalid label smoothing factor: ' + str(smooth_factor))\n    return y2","fa0c21b0":"from keras.utils import Sequence\nfrom keras.utils import to_categorical\nimport cv2\nfrom math import floor\n\nclass data_sequence(Sequence):\n    '''\n      yield sequence of data\n      features -- list of features\n      labels -- list of labels\n      target_channels {int} -- 1 (gray) or 3(RGB)\n    '''\n    def __init__(self, features, labels, batch_size=128, target_dim=(224,224), \n                 n_classes=7, shuffle=True, smooth=0.0):\n        'Initialization'\n        assert len(features)==len(labels), 'number of feature and labels not consistent'\n        self.features = features\n        self.labels = labels\n        self.batch_size = batch_size\n        self.target_dim = target_dim\n        self.target_channels = 3\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.smooth = smooth\n        self.sample_count = len(labels)\n        self.indexes = np.arange(self.sample_count)\n        self.on_epoch_end()\n#         self.verbose = verbose\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return floor(self.sample_count \/ self.batch_size)\n\n    def __gray2RGB__(self,x):\n      if len(x.shape)==2:\n        return np.stack((x,x,x),-1)\n      else:\n        assert len(x.shape)==3\n        if len(x[0,0,:]) == 1:\n          return np.stack((x[:,:,0],x[:,:,0],x[:,:,0]),-1)\n        else:\n          assert len(x[0,0,:])==self.target_channels\n      return x\n\n\n    def __getitem__(self, idx):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n        X = np.empty((self.batch_size, *self.target_dim, self.target_channels))\n        Y = np.empty((self.batch_size, self.n_classes))\n        for i,ind in enumerate(indexes):\n          x = self.features[ind]\n          # resize image to the target size \n          x = cv2.resize(x,self.target_dim,interpolation=cv2.INTER_CUBIC)\n          x = self.__gray2RGB__(x)\n          X[i] = utils.preprocess_input(x, version=2) # or version=2 for VGGFace2 ResNet50  \n          y = self.labels[ind]\n          if isinstance(y,int):\n            Y[i]=to_categorical(y,7)\n          else:\n            assert len(y)==self.n_classes\n            Y[i]=y\n        X = np.array(X)\n        Y = np.array(Y)\n        if self.smooth > 0.0:\n          smooth_labels(Y, self.smooth)\n        return X,Y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)","ffc9b4e2":"train_sequence = data_sequence(x_train,y_train,batch_size=16,target_dim=(224,224),n_classes=7,shuffle=False)\nfeature,lable = train_sequence.__getitem__(0)","0c1b864e":"emotion_dict = {0: 'Angry', 1:'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6:'Neutral'}\n\nimport matplotlib.pyplot as plt\nplt.imshow(feature[0,:,:,:])\nplt.title(emotion_dict[np.argmax(lable[0])]);","ceab5591":"vggface = VGGFace(model='resnet50', include_top=False, input_shape = (224,224,3))\nvggface.trainable = False\nvggface.summary()","c477caec":"from keras.models import Sequential\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\n\n# model = Sequential([vggface,\n#                     Flatten(),\n#                     Dropout(0.5),\n#                     BatchNormalization(),\n#                     Dense(128, activation='relu'),\n#                     Dropout(0.5),\n#                     BatchNormalization(),\n#                     Dense(len(emotion_dict), activation='softmax', name = 'classifer')])\nmodel = Sequential([vggface,\n                    Flatten(),\n                    Dropout(0.25),\n                    Dense(2048, activation='relu'),\n                    Dropout(0.25),\n                    Dense(1024, activation='relu'),\n                    Dense(7, activation='softmax', name = 'classifer')])\nmodel.summary()","ca65ea1c":"train_sequence = data_sequence(x_train,y_train,batch_size=64,target_dim=(224,224),n_classes=7,shuffle=True,smooth=0.07)\nval_sequence   = data_sequence(x_val,  y_val,  batch_size=64,target_dim=(224,224),n_classes=7,shuffle=True,smooth=0.0)\ntest_sequence  = data_sequence(x_test, y_test, batch_size=64,target_dim=(224,224),n_classes=7,shuffle=True,smooth=0.0)","b62cefea":"model.compile(optimizer = keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\nhist = model.fit_generator(generator = train_sequence,\n                           validation_data = val_sequence,\n                           epochs = 20)","79c0def9":"plt.figure(figsize=(8,3))\nplt.subplot(1,2,1)\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.subplot(1,2,2)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])","5b803813":"for layer in model.layers[0].layers:\n    if 'bn' not in layer.name:\n        layer.trainable = True","5a66bd66":"model.compile(optimizer = keras.optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\nhist2 = model.fit_generator(generator = train_sequence,\n                            validation_data = val_sequence,\n                            epochs = 10)","d05b536d":"for key in ['accuracy','val_accuracy','loss','val_loss']:\n    hist.history[key] = hist.history[key] + hist2.history[key]","f506e7c2":"plt.figure(figsize=(8,3))\nplt.subplot(1,2,1)\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.subplot(1,2,2)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])","8fb1c3ee":"from keras.callbacks import ModelCheckpoint\ncheck_point = ModelCheckpoint('VGGfaceTransfer_dropout_smoothing_keras_model', \n                              monitor='val_acc', verbose=0, save_best_only=True,\n                              save_weights_only=False, mode='auto', period=1)\n\n\nmodel.compile(optimizer = keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\nhist3 = model.fit_generator(generator = train_sequence,\n                            validation_data = val_sequence,\n                            epochs = 10,\n                            callbacks=[check_point])","c0ae4597":"ls","14215592":"for key in ['accuracy','val_accuracy','loss','val_loss']:\n    hist.history[key] = hist.history[key] + hist3.history[key]","85f3ec42":"plt.figure(figsize=(8,3))\nplt.subplot(1,2,1)\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.subplot(1,2,2)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])","6c89c860":"model.evaluate(test_sequence)","ba2302ee":"model.save('VGGfaceTransfer_dropout_smoothing_keras_model')","7e6d98a3":"!tar -czvf VGGfaceTransfer_dropout_smoothing_keras_model.tar.gz VGGfaceTransfer_dropout_smoothing_keras_model","b10359f3":"from IPython.display import FileLink\nFileLink(r'VGGfaceTransfer_dropout_smoothing_keras_model.tar.gz')","e7dd192a":"import tensorflow as tf\ny_pred = []\ny_true = []\nfor x,y in test_sequence:\n    y_pred = y_pred + list(model.predict_classes(x))\n    y_true = y_true + list(np.argmax(y,axis=1))","faed6b42":"emotion_dict.values()","76b1a2bc":"emotion_dict","ea39b02e":"confusion_matrix = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()\nconfusion_matrix = np.around(confusion_matrix.astype('float') \/ confusion_matrix.sum(axis=1)[:, np.newaxis], decimals=2)\n\nconfusion_matrix = pd.DataFrame(confusion_matrix,\n                                index = emotion_dict.values(), \n                                columns = emotion_dict.values())","931425e2":"import seaborn as sns\nfigure = plt.figure(figsize=(8, 8))\nsns.heatmap(confusion_matrix, annot=True,cmap=plt.cm.Blues)\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","8acc7097":"# model test","1edb017b":"### finetune further all, lr=1e-5","be663670":"### train only top layers","fce6b4f4":"### finetune all layer, lr=1e-4","36cbedcd":"### Confusion matrix","17d8938a":"# Data Preprocess","f20d1eb7":"# Challenges in Representation Learning: Facial Expression Recognition Challenge\n\nhttps:\/\/www.kaggle.com\/c\/challenges-in-representation-learning-facial-expression-recognition-challenge","a007073f":"# Model"}}