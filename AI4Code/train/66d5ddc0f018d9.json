{"cell_type":{"f93bbca9":"code","1a00365e":"code","eb759c7a":"code","e2c6ea15":"code","f8b05c55":"code","9dc2596c":"code","c2449169":"code","d352e45b":"code","296f40e1":"code","27aa3b07":"code","a4e74e26":"code","54d83975":"code","6d9350ef":"code","cb5636b7":"code","4203d798":"code","6eed54f0":"code","aaeaa0d6":"code","0d67dc99":"code","7318b266":"code","ca679371":"code","daee2c16":"code","77a80aca":"code","0d5adfd8":"code","4a842cb5":"code","70290ca8":"code","480a4764":"code","9d3c83e5":"code","435ec4a0":"code","a1db2bd8":"code","c3208d63":"code","caf7108e":"code","31b299f0":"code","1cfc10d4":"code","602dcb39":"markdown","751fcae0":"markdown","66fd1576":"markdown","c925d8a2":"markdown","45c5eed9":"markdown","3088f82b":"markdown","e267e3ad":"markdown","76c825ed":"markdown","14c4224e":"markdown","d2407f81":"markdown","97da868e":"markdown","5c9690cc":"markdown","7a4d72b2":"markdown","e3b421f7":"markdown","be7d44a7":"markdown","d7983e15":"markdown","5581d0f1":"markdown","e56ade7c":"markdown","bf93d1d4":"markdown"},"source":{"f93bbca9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1a00365e":"import pandas as pd\r\nimport numpy as np \r\nimport matplotlib.pyplot as plt","eb759c7a":"# Reading the CSV file \ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","e2c6ea15":"df.head(5)","f8b05c55":"df.describe()","9dc2596c":"df.shape","c2449169":"df.columns","d352e45b":"df.dtypes","296f40e1":"df.info()","27aa3b07":"df_target = df.groupby(\"target\").size()\r\ndf_target","a4e74e26":"# \"target 0\" - absence,\"target 1\"- presence\r\nplt.pie(df_target.values,labels=[\"target 0\",\"target 1\"], autopct=\"%1.1f%%\",radius=1,\r\ntextprops={\"fontsize\":16},explode=[0.01,0])\r\nplt.show()","54d83975":"df_sex = df.groupby(['sex', 'target']).size()\r\nprint(df_sex)\r\n\r\nplt.pie(df_sex.values,labels=[\"sex_0,target_0\", \"sex_0,target_1\", \r\n\"sex_1,target_0\", \"sex_1,target_1\"], \r\nautopct=\"%1.1f%%\",radius=1,\r\ntextprops={\"fontsize\":16})\r\nplt.show()","6d9350ef":"plt.hist([df[df.target==0].age, df[df.target==1].age],bins=20,alpha=0.5\r\n,label = [\"no_heart_disease\", \"with_heart_disease\"])\r\nplt.xlabel(\"age\")\r\nplt.ylabel(\"percentage\")\r\nplt.legend(loc=\"best\")\r\nplt.show()","cb5636b7":"plt.hist([df[df.target==0].chol, df[df.target==1].chol],bins=20,alpha=0.5\r\n,label = [\"no_heart_disease\", \"with_heart_disease\"])\r\nplt.xlabel(\"chol\")\r\nplt.ylabel(\"percentage\")\r\nplt.legend(loc=\"best\")\r\nplt.show()","4203d798":"df_cp = df.groupby(['cp', 'target']).size()\r\nprint(df_cp)\r\n\r\nplt.pie(df_cp.values,labels=[\"cp_0,target_0\", \"cp_0,target_1\", \r\n\"cp_1,target_0\", \"cp_1,target_1\", \"cp_2,target_0\", \"cp_2,target_1\",\"cp_3,target_0\", \"cp_3,target_1\"], \r\nautopct=\"%1.1f%%\",radius=1,\r\ntextprops={\"fontsize\":9})\r\nplt.title(\"Chest Pain pie chart\")\r\nplt.show()","6eed54f0":"plt.hist([df[df.cp==0].age, df[df.cp==1].age, df[df.cp==2].age, df[df.cp==3].age],bins=10,alpha=0.8\r\n,label = [\"chest_pain_0\", \"chest_pain_1\",\"chest_pain_2\",\"chest_pain_3\"])\r\nplt.xlabel(\"age\")\r\nplt.ylabel(\"percentage\")\r\nplt.legend(loc=\"best\")\r\nplt.show()","aaeaa0d6":"df_cp_sex = df.groupby(['sex', 'cp']).size()\r\nprint(df_cp_sex)\r\n\r\nplt.pie(df_cp_sex.values,labels=[\"sex_0,cp_0\", \"sex_0,cp_1\",\"sex_0,cp_2\",\"sex_0,cp_3\",\r\n\"sex_1,cp_0\", \"sex_1,cp_1\",\"sex_1,cp_2\",\"sex_1,cp_3\", ], \r\nautopct=\"%1.1f%%\",radius=1,\r\ntextprops={\"fontsize\":9})\r\nplt.title(\"Chest Pain pie chart by sex\")\r\nplt.show()","0d67dc99":"plt.hist([df[df.target==0].trestbps, df[df.target==1].trestbps],bins=20,alpha=0.5\r\n,label = [\"no_heart_disease\", \"with_heart_disease\"])\r\nplt.xlabel(\"trestbps\")\r\nplt.ylabel(\"percentage\")\r\nplt.legend(loc=\"best\")\r\nplt.show()","7318b266":"plt.hist([df[df.target==0].thalach, df[df.target==1].thalach],bins=20,alpha=0.5\r\n,label = [\"no_heart_disease\", \"with_heart_disease\"])\r\nplt.xlabel(\"thalach\")\r\nplt.ylabel(\"percentage\")\r\nplt.legend(loc=\"best\")\r\nplt.show()","ca679371":"# Import relevant libraries\r\n\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\r\nimport xgboost as xgb\r\nfrom sklearn.tree import export_graphviz as eg\r\nfrom sklearn.model_selection import train_test_split,GridSearchCV,StratifiedShuffleSplit\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n","daee2c16":"X = df.drop([\"target\"], axis=1)\r\ny = df.target\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,train_size=.8, random_state=10)\r\n\r\nprint(\"This is X variable: \\n{}\\n\".format(X[:5]))\r\nprint(\"This is y variable: \\n{}\".format(y[:5]))\r\nprint(\"This is X_train variable: \\n{}\\n\".format(X_train[:5]))\r\nprint(\"This is X_test variable: \\n{}\".format(X_test[:5]))\r\nprint(\"This is y_train variable: \\n{}\\n\".format(y_train[:5]))\r\nprint(\"This is y_test variable: \\n{}\".format(y_test[:5]))","77a80aca":"# Creating the pipleline\r\n\r\npipe  = Pipeline([(\"preprocessing\", StandardScaler()),(\"classifier\",KNeighborsClassifier())])\r\n\r\n# Creating the Parameter grid\r\n\r\nparams_grid = [\r\n    {   \"preprocessing\": [StandardScaler(),None], \r\n        \"classifier\":[KNeighborsClassifier()],\r\n        \"classifier__n_neighbors\":[1,2,3,4,5]     \r\n    },\r\n    {\r\n        \"classifier\":[DecisionTreeClassifier(),RandomForestClassifier()],\r\n        # \"classifier__n_estimators\":[100,None],\r\n        \"classifier__max_depth\":[3,None],\r\n        \"preprocessing\": [None]\r\n    },\r\n\r\n    {\r\n        \"classifier\":[GradientBoostingClassifier(), xgb.XGBClassifier()],\r\n        # \"classifier__n_estimators\":[100,None],\r\n        \"classifier__max_depth\":[3,None],\r\n        \"classifier__learning_rate\":[0.001,0.01, 0.1],\r\n        \"preprocessing\": [None]\r\n    }\r\n\r\n    ]\r\n\r\n\r\nStratified_Shuffle_Split = StratifiedShuffleSplit(random_state=10)","0d5adfd8":"params_grid","4a842cb5":"## Making use of the Grid search and fitting to the training set\r\ngrid = GridSearchCV(pipe,param_grid=params_grid, cv=Stratified_Shuffle_Split)\r\ngrid.fit(X_train,y_train)","70290ca8":"## Printing the values\r\n\r\nprint(\"Best params:\\n {}\".format(grid.best_params_))\r\nprint(\"Best estimator:\\n {}\".format(grid.best_estimator_))\r\nprint(\"Best score:\\n {}\".format(grid.best_score_))\r\n\r\n# Displaying using the pandas format\r\nresults = pd.DataFrame(grid.cv_results_)\r\ndisplay(results.T)","480a4764":"rf = RandomForestClassifier(max_depth=3)\r\nrf.fit(X_train,y_train)\r\nprint(\"Test data score: \\n{}\\n\".format(rf.score(X_test,y_test)))","9d3c83e5":"y_rf_pred = rf.predict(X_test)\r\nprint(\"Predictions are \\n{}\\n\".format(y_rf_pred))","435ec4a0":"def feature_importances(X,model):\r\n    importance = model.feature_importances_\r\n    importance_sort = np.argsort(importance)[::-1]\r\n    print(\"Feature importances: \\n{}\".format(importance))\r\n    print(\"Feature importance sorted indices: \\n{}\".format(importance_sort))\r\n    print(\"Feature importance sorted: \\n{}\".format(importance[importance_sort]))\r\n    feature_dict = {}\r\n    for index in importance_sort:\r\n        feature_dict[list(X)[index]] = float(importance[index])\r\n    print(\"The features with their names:\")\r\n    return feature_dict\r\n\r\nfeature_importances(X,rf)","a1db2bd8":"def plot_feature_importances(model):\r\n    n_features = X.shape[1]\r\n    plt.barh(range(n_features), model.feature_importances_, align='center')\r\n    plt.yticks(np.arange(n_features), X.columns)\r\n    plt.xlabel(\"Feature importance ratio\")\r\n    plt.ylabel(\"Feature\")\r\n    plt.title(\"Feature importance\")\r\nplot_feature_importances(rf)","c3208d63":"# rf classification report\r\nfrom sklearn.metrics import  classification_report,plot_confusion_matrix,confusion_matrix\r\ny_test = y_test\r\ntarget_names = [\"target_0\",\"target_1\"]\r\n\r\ndef class_report(model,y_test,pred,target_names):\r\n    print(classification_report(y_test,pred, target_names=target_names))\r\n    disp = plot_confusion_matrix(model, X_test, y_test,labels=[1,0])\r\n    disp.figure_.suptitle(\"Confusion Matrix\")\r\n    print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\r\n    plt.show()\r\n\r\n\r\nclass_report(rf,y_test,y_rf_pred,target_names)","caf7108e":"tp,fn,fp,tn = confusion_matrix(y_test, y_rf_pred, labels=[1,0]).ravel()\nprint(\"The tp,tn,fp,fn respectively: \\n{} {} {} {}\\n\".format(tp,tn,fp,fn))\nprint(\"TNR (Specificity) is: \\n{}\\n\".format({tn\/(tn+fp)}))\nprint(\"TPR (Sensitivity) is: \\n{}\\n\".format({tp\/(tp+fn)}))","31b299f0":"from sklearn.metrics import average_precision_score,roc_auc_score, plot_precision_recall_curve,plot_roc_curve\r\n\r\ny_predict_proba = rf.predict_proba(X_test)\r\nprint(\"Checking Uncertainties: \\n{}\\n\".format(y_predict_proba[:5]))\r\n\r\naverage_precision = average_precision_score(y_test,rf.predict_proba(X_test)[:,1])\r\nprint(\"Average Precision: {:.2f}\".format(average_precision))\r\ndisp = plot_precision_recall_curve(rf,X_test,y_test)\r\nplt.title(\"2-class Precision-Recall curve:\")","1cfc10d4":"roc_scoring = roc_auc_score(y_test,rf.predict_proba(X_test)[:,1])\r\nprint(\"AUC score: {:.2f}\".format(roc_scoring))\r\ndisp = plot_roc_curve(rf,X_test,y_test)\r\nplt.title(\"2-class ROC curve:\")\r\nplt.show()","602dcb39":"## Precision-recall curve","751fcae0":"chest pain type (cp) is the feature that ranks highest while fasting blood sugar &gt; 120 mg\/dl (fbs) ranks last","66fd1576":"resting blood pressure (in mm Hg on admission to the hospital)\r\n\r\nIt seems people within the range of (120-140)mmHg have a high percentage of heart disease","c925d8a2":"## Building the model","45c5eed9":"## Conclusion\nChest pain is the main symptom of heart disease based on the feature importance shown by the Randomforest Classifier. Moreover, males have a higher chance of having chest pain which can lead to heart disease, also people with chest_pain_2 have a higher chance of having heart disease. \nThe algorithm has a sensitivity of 0.88, which enables the prediction of people with heart disease. Precautions should be taken to reduce chest_pain_2.","3088f82b":"### Feature importance\r\nwe look into the feature importance of the data and sort it to see which feature ranks higher than the other.","e267e3ad":"The ratio gets higher over the age of forty. That is, people who are over forty, are under high risk of heart disease.","76c825ed":"Males have a high percentage of cp_2 chest pain from the pie chart above.","14c4224e":"People with cp_2 chest pain, has a high chance of having heart disease from the pie chart above","d2407f81":"target_1 has a high recall of 0.88, while target_0 has a high precision of 0.90.\nMoreover, the Sensitivity of the algorithm is 0.88 and the Specificity is 0.74.\nBecause the prediction of true positive heart disease is of high importance. The alogrithm can be considered, based on it's sensitivity.","97da868e":"## THE MODEL\r\nSo the best model is RandomForestClassifier with a max_depth of 3, so we build the model itself and apply it to the test data.","5c9690cc":"## DATA SETUP","7a4d72b2":"## Further Analysis\nFor further analysis we shall look into the \n1.) confusion matrix, \n2.) class report, \n3.) Precision-recall curve\n4.) AUC curve","e3b421f7":"This shows that people within the cholesterol range of 200 - 300 have a high chance of high disease","be7d44a7":"To determine the best model for this we shall be using a pipeline with a grid search.","d7983e15":"Plotting the feature importance ","5581d0f1":"maximum heart rate achieved\r\nWith a mean of 150, it shows that people within the range of 160-180, people have an estimated amount of 25% to have heart disease","e56ade7c":"### Specificity or True Negative Rate (TNR)\r\nTNR (ranges from 0 to 1, higher is better) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition).\r\nTNR = TN\/(TN+FP)\r\n\r\n### Precision, Positive Predictive Value (PPV)\r\nPPV (ranges from 0 to 1, higher is better) is the ratio of true positives over all true and false positives:\r\nPrecision = TP\/(TP+FP)\r\nHigh precision means that an algorithm returned substantially more relevant results than irrelevant ones, or in other words the more likely everything it returns is right, but it does not mean it may get all the right results that are out there.\r\n\r\n\r\n### Recall, Sensitivity, Hit Rate or True Positive Rate (TPR)\r\nTPR (ranges from 0 to 1, higher is better) is the ratio of true positives over the sum of true positives and false negatives:\r\nRecall = TP \/ (TP+FN)\r\nHigh recall means that an algorithm returned most of the relevant results, \r\nRecall is used as performance metric when we need to identify all positive samples; that is, when it is important to avoid false negatives.","bf93d1d4":"## ROC curve"}}