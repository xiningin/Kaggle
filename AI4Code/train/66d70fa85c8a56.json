{"cell_type":{"d91284c7":"code","26f84a38":"code","b7f13074":"code","f5c5b0a7":"code","4202f535":"code","81201d72":"code","b1e478ec":"code","7e680faa":"code","285bdff4":"code","f8117e38":"code","53bb638d":"code","82dcd0c5":"code","04a6b057":"code","67c13583":"code","dc610a5a":"code","c7800ad5":"code","b923f954":"code","f8a47a63":"code","8de77ea9":"code","6e39716a":"code","318ab17c":"code","0368820e":"code","16d63ede":"code","4ee9f2a5":"code","51215f57":"code","24446a82":"code","93cd30a9":"code","158ba228":"code","28e84558":"code","25aabe45":"code","cdb1c19a":"code","eaaab0b0":"code","66fa8c6a":"code","91530e36":"code","d3951fd8":"code","f7f82ced":"code","a1d8edc2":"code","f0254042":"code","962fa9c1":"code","84eb3d7c":"markdown","aaa46d70":"markdown","77663eed":"markdown","47d74b32":"markdown","1a1ab585":"markdown","9a23c9af":"markdown","b99329b7":"markdown"},"source":{"d91284c7":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime \nimport category_encoders as ce\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nrandom.seed(20)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\nfrom statistics import mean, median,variance,stdev\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","26f84a38":"#df = pd.read_csv('..\/input\/train.csv')\n#df = df.sample(n=30000, random_state=20)\n#df_train, df_test = train_test_split(df, test_size=0.2, random_state=20)","b7f13074":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","f5c5b0a7":"df_gdp = pd.read_csv(\"..\/input\/US_GDP_by_State.csv\")\ndf_state = pd.read_csv(\"..\/input\/statelatlong.csv\")\ndf_spi = pd.read_csv(\"..\/input\/spi.csv\")","4202f535":"year = []\nfor y in list(df_train[\"issue_d\"]):\n    year.append(int(y[4:8]))\ndf_train[\"year\"] = year\n\nyear_test = []\nfor y in list(df_test[\"issue_d\"]):\n    year_test.append(int(y[4:8]))\ndf_test[\"year\"] = year_test","81201d72":"years = [2007, 2008, 2009, 2010, 2011, 2012]\nadd_list = []\nfor y in years:\n    for state in set(df_gdp[\"State\"]):\n        add = []\n        add.append(state)\n        add.append(int(df_gdp[\"State & Local Spending\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Gross State Product\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Real State Growth %\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Population (million)\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(y)\n        add_list.append(add)\n        df_gdp.append(pd.DataFrame(add_list, columns=[\"State\", \"State & Local Spending\", \"Gross State Product\", \"Real State Growth %\", \"Population (million)\", \"year\"]))\n\ndf_gdp = pd.merge(df_gdp, df_state, left_on = \"State\", right_on = \"City\", how='left')\n\ndate_spi = []\nfor d in list(df_spi['date']):\n    date_temp = datetime.strptime(d, '%d-%b-%y')\n    date_spi.append(date_temp.strftime('%b-%Y'))\ndf_spi[\"date\"] = date_spi\ndf_spi = df_spi.groupby(\"date\").mean()\n\ndf_train = pd.merge(df_train, df_gdp, left_on=['addr_state','year'], right_on = ['State_y','year'], how ='left')\ndf_test = pd.merge(df_test, df_gdp, left_on=['addr_state','year'], right_on = ['State_y','year'], how ='left')","b1e478ec":"df_train = pd.merge(df_train, df_spi, left_on='issue_d', right_on ='date', how = 'left')\ndf_test = pd.merge(df_test, df_spi, left_on='issue_d', right_on ='date', how = 'left')","7e680faa":"#y_train = df_train.loan_condition\n#y_test = df_test.loan_condition\n#df_train = df_train.drop([\"loan_condition\", \"issue_d\"], axis=1)\n#df_test = df_test.drop([\"issue_d\"], axis=1)","285bdff4":"df_train[\"loan_sal_ratio\"] = df_train[\"loan_amnt\"]\/df_train[\"annual_inc\"]\ndf_test[\"loan_sal_ratio\"] = df_test[\"loan_amnt\"]\/df_test[\"annual_inc\"]\ndf_train[\"loan_sal_ratio\"] = df_train[\"loan_sal_ratio\"].replace(np.inf, 1)\ndf_test[\"loan_sal_ratio\"] = df_test[\"loan_sal_ratio\"].replace(np.inf, 1)","f8117e38":"df_train[\"collections_12_mths_ex_med*mths_since_last_major_derog\"] = df_train[\"collections_12_mths_ex_med\"]*df_train[\"mths_since_last_major_derog\"]\ndf_train[\"mths_since_last_major_derog\/open_acc\"] = df_train[\"mths_since_last_major_derog\"]\/df_train[\"open_acc\"]\ndf_train[\"loan_amnt*dti\"] = df_train[\"loan_amnt\"]*df_train[\"dti\"]\ndf_train[\"pub_rec\/revol_util\"] = df_train[\"pub_rec\"]\/df_train[\"revol_util\"]\ndf_train[\"pub_rec\/installment\"] = df_train[\"pub_rec\"]\/df_train[\"installment\"]\ndf_train[\"pub_rec\/loan_amnt\"] = df_train[\"pub_rec\"]\/df_train[\"loan_amnt\"]\ndf_train[\"pub_rec\/total_acc\"] = df_train[\"pub_rec\"]\/df_train[\"total_acc\"]\ndf_train[\"dti*installment\"] = df_train[\"dti\"]*df_train[\"installment\"]\ndf_train[\"total_acc*pub_rec\"] = df_train[\"total_acc\"]*df_train[\"pub_rec\"]\ndf_train[\"collections_12_mths_ex_med*mths_since_last_major_derog\"] = df_train[\"collections_12_mths_ex_med*mths_since_last_major_derog\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"mths_since_last_major_derog\/open_acc\"] = df_train[\"mths_since_last_major_derog\/open_acc\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"loan_amnt*dti\"] = df_train[\"loan_amnt*dti\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"pub_rec\/revol_util\"] = df_train[\"pub_rec\/revol_util\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"pub_rec\/installment\"] = df_train[\"pub_rec\/installment\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"pub_rec\/loan_amnt\"] = df_train[\"pub_rec\/loan_amnt\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"pub_rec\/total_acc\"] = df_train[\"pub_rec\/total_acc\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"dti*installment\"] = df_train[\"dti*installment\"].replace(np.inf,1).replace(-np.inf,1)\ndf_train[\"total_acc*pub_rec\"] = df_train[\"total_acc*pub_rec\"].replace(np.inf,1).replace(-np.inf,1)","53bb638d":"df_test[\"collections_12_mths_ex_med*mths_since_last_major_derog\"] = df_test[\"collections_12_mths_ex_med\"]*df_test[\"mths_since_last_major_derog\"]\ndf_test[\"mths_since_last_major_derog\/open_acc\"] = df_test[\"mths_since_last_major_derog\"]\/df_test[\"open_acc\"]\ndf_test[\"loan_amnt*dti\"] = df_test[\"loan_amnt\"]*df_test[\"dti\"]\ndf_test[\"pub_rec\/revol_util\"] = df_test[\"pub_rec\"]\/df_test[\"revol_util\"]\ndf_test[\"pub_rec\/installment\"] = df_test[\"pub_rec\"]\/df_test[\"installment\"]\ndf_test[\"pub_rec\/loan_amnt\"] = df_test[\"pub_rec\"]\/df_test[\"loan_amnt\"]\ndf_test[\"pub_rec\/total_acc\"] = df_test[\"pub_rec\"]\/df_test[\"total_acc\"]\ndf_test[\"dti*installment\"] = df_test[\"dti\"]*df_test[\"installment\"]\ndf_test[\"total_acc*pub_rec\"] = df_test[\"total_acc\"]*df_test[\"pub_rec\"]\ndf_test[\"collections_12_mths_ex_med*mths_since_last_major_derog\"] = df_test[\"collections_12_mths_ex_med*mths_since_last_major_derog\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"mths_since_last_major_derog\/open_acc\"] = df_test[\"mths_since_last_major_derog\/open_acc\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"loan_amnt*dti\"] = df_test[\"loan_amnt*dti\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"pub_rec\/revol_util\"] = df_test[\"pub_rec\/revol_util\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"pub_rec\/installment\"] = df_test[\"pub_rec\/installment\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"pub_rec\/loan_amnt\"] = df_test[\"pub_rec\/loan_amnt\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"pub_rec\/total_acc\"] = df_test[\"pub_rec\/total_acc\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"dti*installment\"] = df_test[\"dti*installment\"].replace(np.inf,1).replace(-np.inf,1)\ndf_test[\"total_acc*pub_rec\"] = df_test[\"total_acc*pub_rec\"].replace(np.inf,1).replace(-np.inf,1)","82dcd0c5":"columns_raw = df_train.columns\ncolumns_raw_test = df_test.columns","04a6b057":"df_train['fold'] = df_train.loan_amnt.apply( lambda x: random.randint(0,5))","67c13583":"columns = df_train.columns\ncolumns_hasnull = []\nfor col in columns:\n    if df_train[col].isnull().sum() > 0:\n        columns_hasnull.append(col)","dc610a5a":"def MissingColumns(train, test, columns):\n    trainout = train.copy()\n    testout = test.copy()\n    for col in columns:\n        name = col + \"_Miss\"\n        trainout[name] = list(train[col].isnull())\n        trainout = trainout.drop(col,axis=1)\n        testout[name] = list(testout[col].isnull())\n        testout = testout.drop(col, axis=1)\n    return trainout, testout","c7800ad5":"col_miss = [\"emp_length\", \"title\", \"emp_title\"]\ntrain_Miss, test_Miss = MissingColumns(df_train[col_miss], df_test[col_miss],col_miss)","b923f954":"for i in columns_hasnull:\n    if df_train[i].dtype == \"float64\":\n        df_train[i].fillna(df_train[i].median(), inplace=True)\n        df_test[i].fillna(df_train[i].median(), inplace=True)\n    if df_train[i].dtype == \"object\":\n        df_train[i].fillna(\"Kuhaku\", inplace=True)\n        df_test[i].fillna(\"Kuhaku\", inplace=True)","f8a47a63":"df_train = df_train.join(train_Miss)\ndf_test = df_test.join(test_Miss)","8de77ea9":"def OrdinalEncoder(traindf, testdf, category):\n    oe = ce.OrdinalEncoder(cols=category, return_df=True)\n    names = list(map(lambda x: x + \"_OrdinalEncoder\", category))\n    trainout = oe.fit_transform(traindf[category])\n    testout = oe.transform(testdf[category])\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef OneHotEncoder(traindf, testdf, category):\n    ohe = ce.OneHotEncoder(cols=category, handle_unknown='impute')\n    trainout = ohe.fit_transform(traindf[category])\n    testout = ohe.transform(testdf[category])\n    return trainout, testout\n\ndef CountEncoder(traindf, testdf, category):\n    trainout = pd.DataFrame()\n    testout = pd.DataFrame()\n    for val in category:\n        newname = val + \"_CountEncoder\"\n        count = traindf.groupby(val)[val].count()\n        trainout[newname] = traindf.groupby(val)[val].transform('count')\n        testout[newname] = traindf.groupby(val)[val].transform('count')\n        trainout[newname].fillna(trainout[newname].median(), inplace=True)\n        testout[newname].fillna(trainout[newname].median(), inplace=True)\n    return trainout, testout\n\ndef TargetEncoder(traindf, testdf, category, target):\n    trainout = pd.DataFrame()\n    testout = pd.DataFrame()\n    for col in category:\n        trainout_temp = pd.DataFrame()\n        testout_temp = pd.DataFrame()\n        n= 0\n        name = col + \"_TargetEncoder\"\n        # training\n        for i in set(traindf[\"fold\"]):\n            label_mean = traindf[traindf[\"fold\"] != i].groupby(col)[target].mean()\n            if n == 0:\n                trainout_temp = traindf[traindf[\"fold\"] == i][col].map(label_mean)\n            else:\n                trainout_temp = trainout_temp.append(traindf[traindf[\"fold\"] == i][col].map(label_mean))\n            n = n + 1\n        trainout[name] = trainout_temp\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        \n        # test\n        label_mean = traindf.groupby(col)[target].mean()\n        testout[name] = testdf[col].map(label_mean)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout","6e39716a":"col_CE = [\n#\"earliest_cr_line\",\n]\ntrain_CE, test_CE = CountEncoder(df_train, df_test, col_CE)","318ab17c":"col_OE = [\n#    \"initial_list_status\",\n#    \"zip_code\",\n#    \"earliest_cr_line\",\n]\ntrain_OE, test_OE = OrdinalEncoder(df_train, df_test, col_OE)","0368820e":"col_TE = [\n        \"initial_list_status\",\n    \"zip_code\",\n    \"earliest_cr_line\",\n\"title\",\n\"grade\",\n\"sub_grade\",\n\"emp_length\",\n\"home_ownership\",\n\"purpose\",\n\"addr_state\"\n]\ntrain_TE, test_TE = TargetEncoder(df_train, df_test, col_TE, \"loan_condition\")","16d63ede":"for i in list(df_train.columns):\n    if df_train[i].dtype!=\"object\":\n        print(i)","4ee9f2a5":"def StandardScale(train, test, columns):\n    scaler = StandardScaler()\n    scaler.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = scaler.transform(train[columns])\n    testout[columns] = scaler.transform(test[columns])\n    names = list(map(lambda x: x + \"_StandardScale\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef MinMaxScale(train, test, columns):\n    scaler = MinMaxScaler()\n    scaler.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = scaler.transform(trainout[columns])\n    testout[columns] = scaler.transform(testout[columns])\n    names = list(map(lambda x: x + \"_MinMaxScale\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef LogTransform(train, test, columns):\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    for col in columns:\n        logval = []\n        for i in list(train[col]):\n            tes = np.sign(i)*np.log(abs(i))\n            logval.append(tes)\n        trainout[col] = logval\n        \n        logval_test = []\n        for i in list(test[col]):\n            tes = np.sign(i)*np.log(abs(i))\n            logval_test.append(tes)\n        testout[col] = logval_test\n    names = list(map(lambda x: x + \"_Log\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef BoxCoxTransform(train, test, columns):\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = pt.transform(train[columns])\n    testout[columns] = pt.transform(test[columns])\n    names = list(map(lambda x: x + \"_BoxCox\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout","51215f57":"col_SS = [\n    \"loan_amnt\",\n    \"installment\",\n    \"dti\",\n    \"delinq_2yrs\",\n    \"inq_last_6mths\",\n    \"pub_rec\",\n    \"revol_util\",\n    \"State & Local Spending\",\n    \"Gross State Product\",\n    \"Population (million)\",\n    \"close\",\n    \"tot_coll_amt\",\n    \"loan_sal_ratio\",\n    \"mths_since_last_major_derog\",\n    \"collections_12_mths_ex_med*mths_since_last_major_derog\",\n    #\"mths_since_last_major_derog\/open_acc\",\n    #\"loan_amnt*dti\",\n    #\"pub_rec\/revol_util\",\n    #\"pub_rec\/installment\",\n    #\"pub_rec\/loan_amnt\",\n    #\"pub_rec\/total_acc\",\n    #\"dti*installment\",\n    #\"total_acc*pub_rec\",\n]\ntrain_SS, test_SS = StandardScale(df_train, df_test, col_SS)","24446a82":"col_log = [\n    \"annual_inc\",\n    \"open_acc\",\n    \"revol_bal\",\n    \"total_acc\",\n    \"tot_cur_bal\",\n    \"Real State Growth %\"\n]\ntrain_log, test_log = LogTransform(df_train, df_test, col_log)","93cd30a9":"col_MinMax = [\n    \"mths_since_last_record\"\n]\ntrain_MinMax, test_MinMax = MinMaxScale(df_train, df_test, col_MinMax)","158ba228":"col_BoxCox = [\n \"mths_since_last_delinq\"\n]\ntrain_BoxCox, test_BoxCox = BoxCoxTransform(df_train, df_test, col_BoxCox)","28e84558":"col_text = [\n    \"emp_title\"\n]","25aabe45":"def TFIDF(trainlist, testlist):\n    features = 30\n    vec = TfidfVectorizer(min_df = 1, max_features = features)\n    alltxt = trainlist + testlist\n    vec.fit(alltxt)\n    trainout = pd.DataFrame(vec.transform(trainlist).toarray())\n    testout = pd.DataFrame(vec.transform(testlist).toarray())\n    names = []\n    for f in range(features):\n        txt = \"txt_\" + str(f)\n        names.append(txt)\n    trainout.columns = names\n    testout.columns = names\n    return trainout, testout","cdb1c19a":"train_TXT, test_TXT = TFIDF(list(df_train[\"emp_title\"]), list(df_test[\"emp_title\"]))","eaaab0b0":"df_train.columns","66fa8c6a":"df_train = df_train.join([\n    train_CE,\n    train_OE,\n    train_TE,\n    train_SS,\n    train_log,\n    train_MinMax,\n    train_BoxCox,\n    train_TXT\n])\ndf_test = df_test.join([\n    test_CE,\n    test_OE,\n    test_TE,\n    test_SS,\n    test_log,\n    test_MinMax,\n    test_BoxCox,\n    test_TXT\n])","91530e36":"X_train = df_train.drop(columns_raw, axis = 1).drop(\"fold\", axis = 1)\ny_train = df_train.loan_condition\nX_test = df_test.drop(columns_raw_test, axis = 1)","d3951fd8":"X = pd.concat([X_train, X_test])\ny = pd.DataFrame(np.concatenate([np.zeros(len(X_train)), np.ones(len(X_test))]))\nskf = StratifiedKFold(n_splits=3, random_state=71, shuffle=True)\ndf_split = pd.DataFrame\nn = 0\nfor train_ix, test_ix in skf.split(X,y):\n    X_train_, y_train_ = X.iloc[train_ix], y.iloc[train_ix]\n    X_val, y_val = X.iloc[test_ix], y.iloc[test_ix]\n    \n    clf = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=71, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\n    clf.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])\n    y_pred = clf.predict_proba(X_val)[:,1]\n\n    df_temp = X_val.copy()\n    df_temp[\"y_val\"] = list(y_val[0])\n    df_temp[\"y_pred\"] = list(y_pred)\n    print(\"test pred \", roc_auc_score(y_val, y_pred))\n    if n == 0:\n        df_split = df_temp\n    else:\n        df_split = pd.concat([df_split, df_temp])\n    n = n+1","f7f82ced":"train_index = df_split[df_split[\"y_val\"]==0].drop(\"y_val\", axis=1).sort_values(by=[\"y_pred\"], ascending=False)[:round(len(df_split)*0.25)].index","a1d8edc2":"X_train = df_train.drop(train_index).drop(columns_raw, axis =1).drop(\"fold\", axis=1)\ny_train = df_train.drop(train_index).loan_condition\nX_val = df_train.iloc[train_index].drop(columns_raw, axis = 1).drop(\"fold\", axis=1)\ny_val = df_train.iloc[train_index].loan_condition","f0254042":"clf = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=71, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\n#X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=42)\nclf.fit(X_train, y_train, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","962fa9c1":"y_pred = clf.predict_proba(X_test)[:,1]\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.loan_condition = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","84eb3d7c":"# \u6570\u5024\u51e6\u7406","aaa46d70":"# \u8ffd\u52a0\u7279\u5fb4\u91cf\u4f5c\u6210","77663eed":"# \u30ab\u30c6\u30b4\u30ea\u30fc\u51e6\u7406","47d74b32":"# \u30c6\u30ad\u30b9\u30c8","1a1ab585":"# NULL\u51e6\u7406","9a23c9af":"# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","b99329b7":"# \u30c7\u30fc\u30bf\u6e96\u5099"}}