{"cell_type":{"f664b7b5":"code","7ddf4564":"code","45c19f45":"code","ae276c25":"code","6553976d":"code","90210eb5":"code","7c0e30fb":"code","9eeb6432":"code","cadab66b":"code","e0663e28":"code","3b8186bf":"code","e849fa4b":"code","d8d06c8a":"code","db8dc153":"code","d7aa9c4e":"code","9b37d078":"code","5d9f98f3":"code","f61a1d8d":"code","6fd49f4b":"code","3e20a203":"code","3d6d9029":"code","cd5c4a32":"code","0c083fb3":"code","1a2b237d":"code","d32cbf41":"code","54c69b7a":"code","93e4aa91":"code","8221255e":"code","b56c5316":"code","c755df0c":"code","f9e9646a":"code","4aa31ae5":"code","85efb947":"code","cb3ad2fc":"code","2f4160ca":"code","fcbbcbd2":"code","931cd680":"code","9a1c977d":"code","e8e3d7be":"code","b1d929d8":"code","4898ae93":"code","96ae75cc":"code","c4d69c30":"code","8043cc89":"code","41c89a28":"code","80f9a6ad":"code","9c86c9c9":"code","58a72b41":"code","6b460017":"code","529eb4c8":"code","1db3124a":"code","a020a478":"code","c045baa5":"code","14cf6ab5":"code","d3156ebd":"code","c6fc4287":"code","e5af00a2":"code","e0c72ded":"code","faf2ff8f":"code","229b0e26":"code","0d653ba4":"code","a9276da3":"code","63c0b52f":"code","544e2a96":"code","048d96d7":"code","094354f3":"code","404e687c":"code","9e32b171":"code","40b42f24":"code","f73a3d82":"code","89a4d862":"code","005da697":"code","0549ccaf":"code","4f2bccf9":"code","8e7b1571":"code","932c5871":"code","44ae4e58":"code","0c3525b7":"code","cc344756":"code","e89ac633":"code","2a589035":"code","09c81001":"code","5c386599":"code","ff5f2e8e":"code","e0759c6a":"code","2498d5ac":"code","6049882d":"code","8ee1af0a":"code","4c83f871":"code","4413ce44":"code","a166b1a6":"code","a3e9c2df":"code","d36b3f7d":"code","0bcc5e41":"markdown","30090e29":"markdown","c50fc933":"markdown","a3cac31e":"markdown","55632473":"markdown","3047cb82":"markdown","c2b5d0e2":"markdown","b9160cda":"markdown","00947f04":"markdown","c0fd29e1":"markdown","df4eacdb":"markdown","e6adbcac":"markdown","e4dc9972":"markdown","b786287a":"markdown","a5da98e2":"markdown","78af3b9e":"markdown","e1ab015e":"markdown","e0c186a7":"markdown","605fea9d":"markdown","3491d204":"markdown","f4c8b4bc":"markdown","314585c4":"markdown","c1900267":"markdown","c7f3204e":"markdown","92a4fd15":"markdown","0e0e56c3":"markdown","62ad9b2d":"markdown","6845b23c":"markdown","a069168e":"markdown","615056bc":"markdown","0211a141":"markdown","6f97f47d":"markdown","2e48d36c":"markdown","16c521d8":"markdown","b415e63b":"markdown","e2c45109":"markdown","4c0e6676":"markdown","f9efeddb":"markdown","a1035594":"markdown","3c728618":"markdown","e2bc34a1":"markdown","8bb05cc1":"markdown","dc006a69":"markdown"},"source":{"f664b7b5":"# You can install it if you haven't install yet\n#!pip install plotly","7ddf4564":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\nfrom PIL import Image\nimport requests\n","45c19f45":"from io import BytesIO\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly import tools\nimport plotly.figure_factory as ff","ae276c25":"ratings = pd.read_csv('BX-Book-Ratings.csv',sep=\";\",error_bad_lines=False, encoding='latin-1')\nratings.head(10)","6553976d":"ratings.info()","90210eb5":"ratings.shape","7c0e30fb":"rating = ratings['Book-Rating']\nrating_mean = rating.mean() \nrating_mean","9eeb6432":"ratings[ratings == 0].count()","cadab66b":"ratings.drop_duplicates(inplace=True, keep='first') \n\nprint(ratings.shape)","e0663e28":"ratings = ratings.dropna()\nprint(ratings.shape)","3b8186bf":"ratings['Book-Rating'].mean()","e849fa4b":"ratings = ratings[ratings['Book-Rating'] != 0]\nratings.info()","d8d06c8a":"#rating_clean.to_csv(\"rating_clean.csv\")","db8dc153":"books = pd.read_csv('BX_Books.csv',sep=\";\",error_bad_lines=False, encoding='latin-1')\nbooks.head(3)","d7aa9c4e":"users = pd.read_csv('BX-Users.csv',sep=\";\",error_bad_lines=False, encoding='latin-1')\nusers","9b37d078":"#users_df['User-ID'].describe()","5d9f98f3":"print(users_df['User-ID'].isnull().sum())","f61a1d8d":"users_df0 = users_df.dropna()","6fd49f4b":"users_df0['User-ID'].astype(np.int64)","3e20a203":"rating_clean.head(3)","3d6d9029":"B1 = pd.merge(rating_clean, users_df0, on='User-ID', how='left')\nB1","cd5c4a32":"B2 = pd.merge(B1, bookings_df, on='ISBN', how='left')\nB2","0c083fb3":"#B2.to_csv(\"B2.csv\")","1a2b237d":"B3 = B2.dropna()\nprint(B3.shape)","d32cbf41":"#B3","54c69b7a":"#ratings1 = B3['Book-Rating']\n#ratings1_mean = ratings1.mean() \n#ratings1_mean","93e4aa91":"B3.rename(columns={\n    'User-ID': 'User_ID', \n    'Book-Rating': 'Book_Rating', \n    'Book-Title': 'Book_Title',\n    'Book-Author': 'Book_Author',\n    'Year-Of-Publication': 'Year_Of_Publication'\n}, inplace=True)\n","8221255e":"#B3.head(2)","b56c5316":"#B3['Country'] = B3['Country'].apply(lambda x:x[:-1])\nB3.head(3)","c755df0c":"#B3.to_csv(\"B3.csv\")","f9e9646a":"B3.info()","4aa31ae5":"#B3.describe()","85efb947":"bn = B3[\"Book_Title\"].value_counts()\nbn","cb3ad2fc":"#B3[\"Book_Title\"].describe()\n","2f4160ca":"B3[\"User_ID\"].value_counts()\n","fcbbcbd2":"user = B3['User_ID'].astype(\"str\")\nuser.describe()","931cd680":"B4 = B3.drop(B3[B3['Age'] >= 80].index)\nB4.shape","9a1c977d":"B4 = B4.drop(B4[B4['Age'] <= 10].index)\nB4.shape","e8e3d7be":"B4 = B4.drop(B4[B4['Year_Of_Publication'] >= 2010].index)\nB4.shape","b1d929d8":"B4['Year_Of_Publication'].describe()","4898ae93":"B4 = B4.drop(B4[B4['Year_Of_Publication'] <= 1200].index)\nB4.shape","96ae75cc":"bn = B4[\"Book_Title\"].value_counts()\nbn","c4d69c30":"user = B4['User_ID']\nuser.drop_duplicates(inplace=True, keep='first') \n\nuser = pd.merge(user, B4, on='User_ID', how='left')\nuser['Age'].plot(kind='hist', title='Age Distribution',)\nB4['Book_Rating'].plot(kind='hist', title='Book_Rating Distribution',)","8043cc89":"from matplotlib import pyplot as plt\nfrom matplotlib import font_manager\n\ndata1 = B4.groupby(by=\"Book_Title\").count().sort_values(by=\"Book_Rating\", ascending=False)[:5][\"Book_Rating\"]\n_x = data1.index\n_y = data1.values\n\n\nplt.figure(figsize=(29,8), dpi=100)\nplt.bar(range(len(_x)), _y, width=0.5)\n\nplt.xticks(range(len(_x)), _x)\nplt.xlabel(\"Book Title\")\nplt.ylabel(\"Num Counts\")\nplt.title(\"Top Rated Books\")\nplt.show()\n","41c89a28":"#use B4 as our primary dataset\nB4.info()","80f9a6ad":"user = B4['User_ID'].astype(\"str\")","9c86c9c9":"B4[\"User_ID\"].value_counts()","58a72b41":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nlangs = ['98391', '153662', '235105 ', '16795', '171118']\nstudents = [5689,1833,1017,956,954]\nax.bar(langs,students)\nplt.xlabel(\"User ID\")\nplt.ylabel(\"Num Counts\")\nplt.title(\"Top5 Rating Users\")\nplt.show()","6b460017":"#user = B4['User_ID'].astype(\"str\")\n#user.describe()","529eb4c8":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport nxviz as nv\nfrom nxviz import CircosPlot\nfrom networkx.algorithms import bipartite","1db3124a":"#import and set sample size to 2000\ndata = pd.read_csv('B4.csv')\ndata=data.head(5000)\n#data.info()\ndata['ISBN']=pd.to_numeric(data['ISBN'],errors='coerce')\ndata.dropna(inplace=True)\ndata.head()\ndata.info()\n","a020a478":"data.drop(['Unnamed: 0','Image-URL-S','Image-URL-M','Image-URL-L'],axis=1,inplace=True)\n#data = data[data['Book_Rating']==10]\ndata.head()\ndata.info()","c045baa5":"G.adj","14cf6ab5":"print(nx.adjacency_matrix(G).todense())","d3156ebd":"G = nx.Graph()\nm=list(data['User_ID'])\nn=list(data['Book_Title'])\nzip_list=list(zip(m,n))\n# Add nodes with the node attribute \"bipartite\"\nG.add_nodes_from(m, bipartite=0)\nG.add_nodes_from(n, bipartite=1)\nG.add_edges_from(list(zip(m,n))) \n    \nbipartite.is_bipartite(G)","c6fc4287":"pdd=pd.DataFrame(zip_list,columns=['source','target'])\npdd.head()\npdd.to_csv('edgelist.csv')","e5af00a2":"top_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0}\nbottom_nodes = set(G) - top_nodes","e0c72ded":"nodes = G.nodes()\ndegree = G.degree()\ncolors = [degree[n] for n in nodes]\n\npos = nx.bipartite_layout(G,top_nodes)\ncmap = plt.cm.viridis_r\n#cmap = plt.cm.Greys\n\nvmin = min(colors)\nvmax = max(colors)\n\nfig = plt.figure(figsize = (15,15), dpi=100)\n\nnx.draw(G,pos,alpha = 0.8, nodelist = nodes, node_color = 'r', node_size = 10, with_labels= True,font_size = 6,font_color='b', width = 0.2, cmap = cmap, edge_color ='blue')\n#fig.set_facecolor('#0B243B')\n\nplt.show()","faf2ff8f":"c = CircosPlot(G,node_color='bipartite',node_grouping='bipartite')\nc.draw()\nplt.show()\n","229b0e26":"#write to gexf file for further development and optimization\nnx.write_gexf(G,'bi-network.gexf')\nprint('success!!')","0d653ba4":"cent = nx.degree_centrality(G)\nname = []\ncentrality = []\n\nfor key, value in cent.items():\n    name.append(key)\n    centrality.append(value)","a9276da3":"cent = pd.DataFrame()    \ncent['name'] = name\ncent['centrality'] = centrality\ncent = cent.sort_values(by='centrality', ascending=False)","63c0b52f":"plt.figure(figsize=(10, 25))\nbb = sns.barplot(x='centrality', y='name', data=cent[:15], orient='h')\nbb = plt.xlabel('Degree Centrality')\nbb = plt.ylabel('Correspondent')\nbb = plt.title('Top 15 Degree Centrality Scores in Enron Email Network')\nplt.show()","544e2a96":"between = nx.betweenness_centrality(G)\nname = []\nbetweenness = []","048d96d7":"for key, value in between.items():\n    name.append(key)\n    betweenness.append(value)\n\nbet = pd.DataFrame()\nbet['name'] = name\nbet['betweenness'] = betweenness\nbet = bet.sort_values(by='betweenness', ascending=False)","094354f3":"plt.figure(figsize=(10, 25))\naa = sns.barplot(x='betweenness', y='name', data=bet[:10], orient='h')\naa = plt.xlabel('Degree Betweenness Centrality')\naa = plt.ylabel('Correspondent')\naa = plt.title('Top 10 Betweenness Centrality Scores in Hillary Clinton Email Network')\nplt.show()","404e687c":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA","9e32b171":"#import and set sample size to 2000\ndata = pd.read_csv('B4.csv')\ndata=data.head(5000)\n#data.info()\ndata['ISBN']=pd.to_numeric(data['ISBN'],errors='coerce')\ndata.dropna(inplace=True)\ndata1=data.copy()\ndata.drop(['Book_Title','Publisher','Unnamed: 0','Location','Image-URL-S','Image-URL-M','Image-URL-L'],axis=1,inplace=True)\ndata.head()\ndata.info()\n\n#use get_dummies function to change those qualitative columns into binary ones\ndata_encoded = pd.get_dummies(data)\ndata_encoded\ndata1.info()","40b42f24":"# minmax scaler (this part is referred from HW2)\nscaler = MinMaxScaler()\ntrain_X,test_X = train_test_split(data_encoded, test_size=0.3, random_state=930)\nX_train = scaler.fit_transform(train_X)\nX_test = scaler.transform(test_X)\n\nX = scaler.transform(data_encoded)\n","f73a3d82":"# KMeans\n# choose k value with elbow method\nK = range(1, 20)\nmeanDispersions = []\nfor k in K:\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X_train)\n\n    meanDispersions.append(kmeans.inertia_)\n\nplt.plot(K, meanDispersions, 'rx-')\nplt.xlabel('k')\nplt.ylabel('Average Dispersion')\nplt.title('Selecting k with the Elbow Method')\nplt.show() \n\n","89a4d862":"# From the graph I would select k=2 as the optimal number of clusters\n\n# cluster\nkmeans = KMeans(n_clusters=2)\n\ny1 = kmeans.fit_predict(X_train)\ny2 = kmeans.predict(X_test)\nwhole_data = kmeans.predict(X)\n\n#generate two subsets with data generated from last step \ntrain = pd.DataFrame(train_X,columns = data_encoded.columns)\ntest = pd.DataFrame(test_X,columns = data_encoded.columns)\n#then add the prediction of clustering to these data\ntrain['Cluster'] = y1\ntest['Cluster'] = y2\ndata1['Cluster'] = whole_data\n#data.to_csv('clustered.csv')","005da697":"# check the outcomes of each cluster\ngroupby1 = data1.groupby(by='Cluster').mean()\ngroupby1","0549ccaf":"data.head(5)","4f2bccf9":"books = pd.read_csv('BX_Books.csv',sep=';',error_bad_lines=False, encoding='latin-1')\nbooks.drop(['Image-URL-S','Image-URL-M','Image-URL-L'],axis=1,inplace=True)\nbooks.head()\nbooks.info()","8e7b1571":"books.rename(columns={'Book-Title':'book_name','Book-Author':'author','Year-Of-Publication':'year','Publisher':'publisher'},inplace=True)\nbooks.head()","932c5871":"#change the column names to make life easier\nusers.rename(columns={'User-ID':'user_id','Location':'location','Age':'age'},inplace=True)\nusers.head()","44ae4e58":"ratings.rename(columns={'User-ID':'user_id','ISBN':'ISBN','Book-Rating':'book-rating'},inplace=True)","0c3525b7":"#check the data structure\n\nprint(books.info())\nprint(ratings.info())\nprint(users.info())","cc344756":"# reduce the magnitude of data by filtering those users who have reviewed more than 30 books (frequent users)\nx = ratings['user_id'].value_counts()>30\nx.shape","e89ac633":"#filtered out ratings that frequent users have made.\nindex1 = x.index\nratings = ratings[ratings['user_id'].isin(index1)]\nratings.head()\nratings.info()","2a589035":"merged = ratings.merge(books, on = 'ISBN')\nmerged.head()\nmerged.info()","09c81001":"#merge the data with number of ratings\nmerged_groupby=merged.groupby('book_name')['book-rating'].count().reset_index()\nmerged_groupby.rename(columns={'book-rating':'number_of_ratings'},inplace=True)\n#filter books with more than 30 reviews \nmerged_groupby=merged_groupby[merged_groupby['number_of_ratings']>30]\nmerged_groupby.head()","5c386599":"#merge the above two files together to get an integrated book review data with total review count for each book;then remove the duplicates\nintegrated_merged=merged.merge(merged_groupby, on='book_name')\nintegrated_merged.drop_duplicates(['user_id','book_name'],inplace=True)\nintegrated_merged.head()\nintegrated_merged.info()","ff5f2e8e":"pivot=pd.pivot_table(integrated_merged, columns='user_id',index='book_name',fill_value=0,values='book-rating')\npivot.shape\npivot","e0759c6a":"pivot_csr=csr_matrix(pivot)\npivot_csr","2498d5ac":"#construct kNN models\nmodel=NearestNeighbors(algorithm='brute')\nmodel.fit(pivot_csr)","6049882d":"pivot.iloc[:,:].values.reshape(1,-1)\n#example of k neighbors \ndistances,suggestions=model.kneighbors(pivot.iloc[55,:].values.reshape(1,-1))","8ee1af0a":"distances","4c83f871":"suggestions","4413ce44":"#test the kNN collaborative filtering model\nfor i in range(len(suggestions)):\n    print(pivot.index[suggestions[i]])\n    print(suggestions[i])","a166b1a6":"list1=list(books['book_name'])\nlist1","a3e9c2df":"#final recommender system function building\ndef book_recommend(book_name):\n    if book_name in list1:\n        book_id = np.where(pivot.index == book_name)[0][0]\n        distances, recommendations = model.kneighbors(pivot.iloc[book_id,:].values.reshape(1,-1))\n        print('begin to recommend all books similar to this book!!!')\n        for i in range(len(recommendations)):\n            if i == 0:\n                print(f\"For book \\\"{book_name}\\\" we would recommend the following:\")\n            if not i:\n                list2=pivot.index[recommendations[i]]\n                for j in range(len(list2)):\n                    print(list2[j])\n    else:\n        raise ValueError","d36b3f7d":"name=input('Please Input a book name: ')\nbook_recommend(name)","0bcc5e41":"The datasets we used are from Kaggle website, which contains about 278 thousands anonymized users providing over 1 million ratings about 271 thousands books. After dropped duplicate, missing and abnormal values, the merged dataset now looks like this on the right side, and it contains about 262 thousands ratings now.\n\nIt shows that users are mainly from 20 to 50 years old in this dataset\uff0cand we also get a pretty right skewed book rating distribution, that most of the rating scores are from 5 to 10.\n\nAdditionally, we can see the top 5 most rated books are \u2018wild animals\u2019,\u2019The lovely bones\u2019,\u2019The Da Vinci Code\u2019,\u2019The secret life of bees\u2019, and \u2018Bridget Jone\u2019s Diary. We also get the top 5 readers who gave the most ratings on different books.\n\nNext, we use user Id and book title as nodes and ratings of the users towards books as edges to build an affiliate network. Now nodes are get connected, and we can already know the centrality and the betweenness in this graph.\n\nTo be more clear, this graph shows weighted relationships between nodes. We can easily tell which the most popular books are.","30090e29":"# 1. Revised introduction of your problem","c50fc933":"## 5.2 import books data and users data","a3cac31e":"### 5.2.5 columns renaming","55632473":"### 5.4.3 Calculate centrality","3047cb82":"### 5.2.1 books","c2b5d0e2":"## 6.1 Building the recommender system","b9160cda":"### 5.1.3 Deleting duplicate columns","00947f04":"In this notebook, we are going to build a book review recommender system that uses book ratings to recommend similar books that a reader may like according to the books he\/she has read on the platform before. There are many pieces of research that analyzed the recommendation systems used in many popular book retailing platforms online, such as Amazon.com or Douban (Leino & R\u00e4ih\u00e4, 2007). Leino (2007) stated that the purpose of any recommender system is to direct the users to the items that best satisfy them, but he did not mention how to avoid users to be recommended with books they dislike. In another prior research,  the author had researched on the recommender systems of various online sales platform in India, where he stated that various techniques like Collaborative Filtering, Content-based, and Demographic have been adopted for the recommendation but there are several drawbacks causing these techniques to fail in providing effective recommendations (Chandak et al., 2015).   \n\nHowever, our book review recommender system is different from those as we use various attributes to control the contents that we are going to recommend, including user-based and item-based collaborative filtering as well as demographic and geographic recommendation techniques. Besides, we aim to emphasize the impact of negative ratings when making recommendations. In this way, we would improve the efficiency as well as the effectiveness of our system.\n","c0fd29e1":"### 5.4.2 Graphing and Visualizing","df4eacdb":"# 6 Book Review Recommender System","e6adbcac":"### 5.1.5 remove rows that rating = 0","e4dc9972":"### 5.5.3 constructing a pivot table","b786287a":"### 5.2.2 users","a5da98e2":"### 5.3.2 Year_Of_Publication","78af3b9e":"#### 5.4.1.1 preprocessing","e1ab015e":"## 5.4.1 Affiliation Network and visualization","e0c186a7":"## 5.1 Preprocessing","605fea9d":"### 5.4.4 Calculate betweenness","3491d204":"## 5.5 Collaborative Filtering","f4c8b4bc":"### 5.2.3 merging dataset","314585c4":"# Please Upvote if you think it helps!","c1900267":"# 7 Conclusion and Business Insights","c7f3204e":"### 5.1.4 missing value","92a4fd15":"## 5.5.4 construct kNN models","0e0e56c3":"# 5. Coding Part","62ad9b2d":"### 5.5.1 Further data cleaning and merging","6845b23c":"### 5.5.2 collaborative filtering","a069168e":"### 5.4.5 K-means Clustering","615056bc":"# 3. Background and business inspirations","0211a141":"# 2. Review of the relevant prior work\n","6f97f47d":"#### 5.4.1.2 Adjacency Matrix","2e48d36c":"The kNN model is the core algorithm of our recommender system, whose mechanism is finding the target node by finding out k nearest neighbors of it. We also use SVD to reduce dimensions, as well as using pivot table to speed up the calculation process.","16c521d8":"### 5.1.1 Install Packages","b415e63b":"## 5.3 summary statistics","e2c45109":"#### Author: Morgan Fang & Lingyu Guo. All rights Reserved.","4c0e6676":"The initial inspiration is that we are confused by some current book review recommender systems because sometimes they cannot understand what we like and dislike. In other words, when it recommends books that I dislike, it would not make sense to induce us to read more. Therefore, our aim is to build a hybrid recommender system that induces book readers to buy more books, in order to promote sales for online bookselling platforms. For example, as a very quick search of \u201capple\u201d on amazon.com could lead to completely different outcomes. Therefore, what we are going to do is to predict what the users really want to find using collaborative filtering.  In addition, we would expect the recommendation system would bring convenience to the online book users, especially in China, many readers would read before they buy, so the referral from peers (including ratings, reviews etc.) would be of vital importance. Thus, the platforms could be more sociable using our recommender system.","f9efeddb":"### 5.2.4 cleaning","a1035594":"### 5.1.2 import rating data","3c728618":"The main problem we are solving is designing a recommendation system to both recommend good books that users may like and introduce friends to people with the same taste in books. This idea has been widely addressed before, for example, from YouTube, Facebook, and many other apps.  YouTube always guesses what customers like to watch and show those videos on users\u2019 main page, and it also gives users an option to subscribe to a channel or a YouTuber that is related to the videos he or she recently watched. What we want to do is pretty similar to these platforms. Although recommendation methods exist in many prior works to help users make decisions like selecting movies, music, products and dishes, those systems are not always completely based on the preferences of users themselves. The preferences of other users and what is popular now have great impacts on the recommendation as well. Therefore, it is desirable for us  to have a method of recommending books to users based on the readers\u2019 own preferences and more in a network relationship way. \n\nTo test our recommendation system, we will use the SVD algorithm of Surprise library to predict the user's rating of the movie as a baseline. Then, we will observe two indicators, RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) to evaluate our prediction results. We hope this research can help readers to find the books they like faster and encourage them to read more. Besides the convenience of the users, it will build connections between similar readers as well, providing a more active and sociable reading environment for users to share their opinions.\n","e2bc34a1":"# 4. Major Algorithms and methods\n\n4.1 k Nearest Neighbors (kNN) algorithm\n\nkNN algorithm is an unsupervised learning algorithm, where the function is only approximated locally and all computation is deferred until function evaluation. For both regression and classification purposes, kNN could help a lot in predicting the neighbors of the users conveniently because it does not need results to feed its learning process.\n\n4.2 Collaborative Filtering\n\nThe motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Also, we would use Singular Value Decomposition (SVD) in our recommendation system to find the books that they did not give ratings, and then project them via SVD to predict their ratings to similar books. Besides, we will test whether our system is making sense by checking whether some of the recommendations are actually the books they gave high ratings.\n\n4.3 Affiliation Network\nAffiliation networks would help the recommender system focus on not only the actors in the social network but also the societies. It will focus on the subsets or groups of readers instead of the direct ties between them. In this project, we would draw an affiliation network graph to show an outline of the structure of the dataset we used.\n","8bb05cc1":"## 5.4 Network and Clustering ","dc006a69":"### 5.3.1 age\n\nFurther Cleaning"}}