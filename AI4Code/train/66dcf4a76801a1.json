{"cell_type":{"d2a0d98a":"code","c713b86a":"code","d1c8ad56":"code","1ea26285":"code","02323094":"code","57ff06b0":"code","713665be":"code","df232c58":"code","0fe1a041":"code","4dc6fea1":"code","fe4dbc1c":"code","c99b91a7":"code","ca603bbd":"code","3a57d580":"code","a2f7494e":"code","62d0edf2":"code","b9134bc3":"code","e97c67ed":"code","451d056b":"code","97c87e5b":"code","e7efcfbc":"code","370b289e":"code","8aef8848":"code","7d766b6c":"code","0c83ab75":"code","6017b0fc":"code","d2692deb":"code","e2be1687":"code","22ee5194":"markdown","a35cd986":"markdown","943a9c91":"markdown","38b117bb":"markdown","3ac4ee0a":"markdown","4a11363b":"markdown","317357e5":"markdown","ae3d981e":"markdown","a792769a":"markdown","f3392151":"markdown","b087421c":"markdown"},"source":{"d2a0d98a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os","c713b86a":"from IPython.display import Image\nImage(filename='\/kaggle\/input\/skin-cancer-malignant-vs-benign\/train\/benign\/775.jpg')","d1c8ad56":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Activation, Dropout\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils","1ea26285":"np.random.seed(42)","02323094":"data_root = '..\/input\/skin-cancer-malignant-vs-benign\/train' # \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u043c \u043f\u0430\u043f\u043a\u0438\nprint(os.listdir(data_root)) ","57ff06b0":"binign = '..\/input\/skin-cancer-malignant-vs-benign\/train\/benign'\nmalignant = '..\/input\/skin-cancer-malignant-vs-benign\/train\/malignant'","713665be":"from keras.preprocessing import image","df232c58":"X = []\ny = []\nfilename='\/kaggle\/input\/skin-cancer-malignant-vs-benign\/train\/'\nsize = (224,224)\ni = 0\nfor name_dir in [binign, malignant]: \n    for picture in next(os.walk(name_dir))[2]:\n        x = image.load_img(str(name_dir) + '\/' + picture, target_size=(224,224))\n        x = image.img_to_array(x, data_format='channels_last')\n        x = x.astype('float32')\n        x \/= 255\n        X.append(x)\n        y.append(int(i))\n    i = i + 1","0fe1a041":"X = np.array(X)","4dc6fea1":"X[0].shape","fe4dbc1c":"type(X)","c99b91a7":"y = np_utils.to_categorical(y, 2)","ca603bbd":"type(y)","3a57d580":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=(224,224,3), data_format='channels_last'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","a2f7494e":"model.fit(X, y, batch_size=9, epochs=25, validation_split=0.2, shuffle=True, verbose=1)","62d0edf2":"X_test = []\ny_test = []\nbinign = '..\/input\/skin-cancer-malignant-vs-benign\/test\/benign'\nmalignant = '..\/input\/skin-cancer-malignant-vs-benign\/test\/malignant'\nfilename='\/kaggle\/input\/skin-cancer-malignant-vs-benign\/test\/'\nsize = (224,224)\ni = 0\nfor name_dir in [binign, malignant]: \n    for picture in next(os.walk(name_dir))[2]:\n        x = image.load_img(str(name_dir) + '\/' + picture, target_size=(224,224))\n        x = image.img_to_array(x, data_format='channels_last')\n        x = x.astype('float32')\n        x \/= 255\n        X_test.append(x)\n        y_test.append(int(i))\n    i = i + 1","b9134bc3":"X_test = np.array(X_test)\ny_test = np_utils.to_categorical(y_test, 2)","e97c67ed":"preds = model.predict(X_test)\npreds # 73% \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 (accuracy)","451d056b":"preds[0] ","97c87e5b":"y_pred = []\nfor pred in preds:\n    if pred[0] > pred[1]:\n        y_pred.append(0)\n    else: \n        y_pred.append(1)","e7efcfbc":"y_pred = np_utils.to_categorical(y_pred, 2)","370b289e":"sum_true = 0\nfor i in range(len(y_pred)):\n    if y_pred[i][0] == y_test[i][0]:\n        sum_true += 1\n\nsum_true \/ len(y_pred) * 100 # \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043d\u0430 \u0442\u0435\u0441\u0442\u0435","8aef8848":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.models import Model","7d766b6c":"model_vgg16 = VGG16(weights='imagenet') \n\nx = model_vgg16.layers[-1].output\nx = Dense(512,activation='relu')(x)\npredictions = Dense(2,activation='sigmoid')(x)\n\nmodel_2 = Model(inputs=model_vgg16.input, outputs=predictions)\n\nmodel_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","0c83ab75":"model_2.fit(X, y, batch_size=1, epochs=2, shuffle=True, verbose=1)","6017b0fc":"preds = model_2.predict(X_test)\npreds","d2692deb":"y_pred = []\nfor pred in preds:\n    if pred[0] > pred[1]:\n        y_pred.append(0)\n    else: \n        y_pred.append(1)\ny_pred = np_utils.to_categorical(y_pred, 2)","e2be1687":"sum_true = 0\nfor i in range(len(y_pred)):\n    if y_pred[i][0] == y_test[i][0]:\n        sum_true += 1\n\nsum_true \/ len(y_pred) * 100 # 54.54545454545454 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043d\u0430 \u0442\u0435\u0441\u0442\u0435","22ee5194":"## Model VGG16","a35cd986":"## Fitting","943a9c91":"\u0414\u043b\u044f \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0438\u043c\u043e\u0441\u0442\u0438 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0437\u0430\u0434\u0430\u0442\u044c seed ","38b117bb":"# \u0427\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u043e \u0432 \u0434\u0430\u043d\u043d\u043e\u043c notebook? (What will be implemented in this notebook)\n\n1. \u0420\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 (\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0441 \u0443\u0447\u0438\u0442\u0435\u043b\u0435\u043c, \u0431\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f). (Object recognition into pictures (supervised learning, binary classification))\n2. \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u043e\u0439 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u043a\u0435 (VGG16 - Visual Geometry Group (16 layers)) (Training on a popular pretrained neural network)","3ac4ee0a":"## Fitting","4a11363b":"# #1\n\n\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 (Importing libraries)","317357e5":"**VGG16** \u2014 \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438, \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u043d\u0430\u044f K. Simonyan \u0438 A. Zisserman \u0438\u0437 \u041e\u043a\u0441\u0444\u043e\u0440\u0434\u0441\u043a\u043e\u0433\u043e \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0438\u0442\u0435\u0442\u0430 \u0432 \u0441\u0442\u0430\u0442\u044c\u0435 \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. \u041c\u043e\u0434\u0435\u043b\u044c \u0434\u043e\u0441\u0442\u0438\u0433\u0430\u0435\u0442 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 92.7% \u2014 \u0442\u043e\u043f-5, \u043f\u0440\u0438 \u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u043d\u0430 ImageNet \u0432 \u0437\u0430\u0434\u0430\u0447\u0435 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043d\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438. \u042d\u0442\u043e\u0442 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u043c 14 \u043c\u0438\u043b\u043b\u0438\u043e\u043d\u043e\u0432 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0430\u0449\u0438\u0445 \u043a 1000 \u043a\u043b\u0430\u0441\u0441\u0430\u043c.","ae3d981e":"## Test","a792769a":"\u0413\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","f3392151":"## Model (\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c)","b087421c":"# #2"}}