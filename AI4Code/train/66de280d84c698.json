{"cell_type":{"65db6e66":"code","4f9cc68d":"code","1f2de215":"code","3a6ca6f3":"code","bb7db94f":"code","19d40010":"code","cdb6adb9":"code","628d47d5":"code","ff0b397e":"code","0ee5cacf":"code","6689e977":"code","b65244cf":"code","42c7a456":"code","1e0fe265":"code","931293f8":"code","a98a4756":"code","582beac1":"code","31562337":"code","aadf96a0":"code","ba952cfd":"code","aabc0084":"code","955f7a55":"code","aaf87768":"code","33bf0a77":"code","2bb8ac9f":"markdown","5286d4cc":"markdown","e6fc4bfc":"markdown","4a49746c":"markdown","6fd88f8e":"markdown","7dd2a51f":"markdown","db92457b":"markdown","288a413c":"markdown","dc92b72e":"markdown","1d9d268b":"markdown","2a162144":"markdown","dfe476aa":"markdown","3b504b9e":"markdown","fce36466":"markdown","2af2c58d":"markdown","d8a72fd6":"markdown","6e0813e5":"markdown","c43b82d0":"markdown","d8b4a1ee":"markdown"},"source":{"65db6e66":"import numpy as np # linear algebra\nimport pandas as pd # data processing","4f9cc68d":"df = pd.read_csv('..\/input\/train.csv')\ndf.head(10)","1f2de215":"train_pos = df[df['label'] == 0]\ntrain_neg = df[df['label'] == 1]","3a6ca6f3":"def clean_word(data):\n    words = \" \".join(data['tweet'])\n    \n    cleaned_words = \" \".join([word for word in words.split() \n                             if 'http' not in word\n                             and not word.startswith('@')\n                             and not word.startswith('#')\n                             and word != 'RT'])\n    return cleaned_words","bb7db94f":"pos_clean_words = clean_word(train_pos)\nneg_clean_words = clean_word(train_neg)","19d40010":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud,STOPWORDS","cdb6adb9":"def wcloud(cleaned_words):\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                         background_color='black',\n                         width=3000,\n                          height=2500\n                         ).generate(cleaned_words)\n    return wordcloud","628d47d5":"pos_wcloud = wcloud(pos_clean_words)\nneg_wcloud = wcloud(neg_clean_words)","ff0b397e":"print('Non racist tweets')\n\nplt.figure(1,figsize=(12,12))\nplt.imshow(pos_wcloud)\nplt.axis('off')\nplt.show()","0ee5cacf":"print('Racist tweets')\n\nplt.figure(1,figsize=(12,12))\nplt.imshow(neg_wcloud)\nplt.axis('off')\nplt.show()","6689e977":"df['label'].value_counts(normalize = True).plot.bar()","b65244cf":"import re\nimport nltk\nfrom nltk.corpus import stopwords","42c7a456":"def clean_tweet_words(tweet):\n    alpha_only = re.sub(\"[^a-zA-Z]\",' ',tweet) #\"[^a-zA-Z]\" this regex will remove any non-alphabetical char as they are not significant\n    words = alpha_only.lower().split()\n    stop = set(stopwords.words('english'))\n    #from the dataframe we can see 'user' word is quite common in the tweets, which is basically used for tagging someone in the tweet\n    #so I will be removing that\n    stop.add('user')\n    sig_words = [word for word in words if not word in stop]\n    return(\" \".join(sig_words))","1e0fe265":"df['clean_tweet']  = df['tweet'].apply(lambda tweet: clean_tweet_words(tweet))\n\ndf.head(10)","931293f8":"from sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(df,test_size = 0.2,random_state=0)","a98a4756":"train_clean_tweet = []\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet = []\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","582beac1":"from sklearn.feature_extraction.text import TfidfVectorizer","31562337":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline","aadf96a0":"svc_pipe = Pipeline([('tfidf',TfidfVectorizer()),('svc', LinearSVC(random_state=0,max_iter=5000))])\nnb_pipe = Pipeline([('tfidf',TfidfVectorizer()),('nb', MultinomialNB())])","ba952cfd":"svc_pipe.fit(train_clean_tweet,train['label'])\nnb_pipe.fit(train_clean_tweet,train['label'])","aabc0084":"pred_svc = svc_pipe.predict(test_clean_tweet)\npred_nb = nb_pipe.predict(test_clean_tweet)","955f7a55":"from sklearn.metrics import accuracy_score, confusion_matrix","aaf87768":"print('SVC')\nprint(accuracy_score(test['label'],pred_svc))\nprint('\\n')\nprint(confusion_matrix(test['label'],pred_svc))\nprint('\\n')","33bf0a77":"print('Naive Bayes Classifier')\nprint(accuracy_score(test['label'],pred_nb))\nprint('\\n')\nprint(confusion_matrix(test['label'],pred_nb))\nprint('\\n')","2bb8ac9f":"Until now we are done with the basic preprocessing steps, but one **crucial thing is still left** our data is still in the form of text and we can't feed it directly to classifier.\n\nThere are two approaches to solve this problem:\n\n* Use sklearn CountVectorizer\n* Or we can use sklearn TfidfVectorizer\n\n# CountVectorizer\nCountVectorizer converts the all the text in the document to form [DTM(Document Term Matrix)](https:\/\/en.wikipedia.org\/wiki\/Document-term_matrix)\n\n# TfidfVectorizer\n\nTfidfVectorizer first prepares A DTM and then prepares a [tf-idf matrix](https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)\n\nFrom the exploratory data analysis we have seen words such as 'amp',etc. whose occurrence frequency is quite high in positive as well as in negative tweets, tf-idf penalises such words with high occurring frequency and give importance to less frequently occuring words as they can give more meaning to a sentence.\n\nSo we will be using TfidfVectorizer for our solution\n\nFeel free to do experiments with CountVectorizer.\n","5286d4cc":"Now lets  clean the tweets from hashtags, mentions and links","e6fc4bfc":"<h3>Let's start our solution with some basic exploratory data analysis and try do dig some insights from the dataset.<\/h3>\n","4a49746c":"We can see our dataset is highly skewed, we will have to take care of this.\n\nBut we won't be going in much details of on how to deal with skewed data as the main motive of this kernel is to give you a starting point for performing sentiment analysis.\n\n[You can check out this link for more information regarding handling of skewed data](https:\/\/medium.com\/@TheDataGyan\/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55)","6fd88f8e":"Now we are ready for a WordCloud visualization which shows only the most emphatic words of the Positive and Negative tweets.","7dd2a51f":"Stopwords are some of the most frequently occuring words e.g. 'a','an','the'. They does not give any significant information regarding the content and context of text.\n\n[Check out this link for more information.](https:\/\/en.wikipedia.org\/wiki\/Stop_words)","db92457b":"Now our dataset contains a column with cleaned tweets","288a413c":"We can see that for the positive tweets most commonly occuring words are **'amp','happy','day','love',** etc. and most of this word doesn't conveys any racist sentiments","dc92b72e":"We can see that the negative tweets has most common occuring words such as 'racist','black','libtard' even 'trump'(:p which is funny though :P)\n\n**One important insight that we can draw from the visualization is that the word 'amp' has a significant presence in positive as well as in negative tweets, hence we have to take care of this as it may confuse our classifier**","1d9d268b":"One last thing that we would be exploring is distribution of our dataset","2a162144":"Now I'll be splitting the dataset into train and test set using skelearn's train_test_split\n\n**#suggestion: Rather using train_test_split() you should use [sklearn.model_selection.StratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html) this will make sure that the distribution in train and test set remains similar**","dfe476aa":"Our Support Vector Classifer is showing good results, but it can be improved further by doing some feature engineering such as adding a new feature like length of tweet, etc. also switiching to a different model such as Adaboost or Random Forest might give a better result.\n\nSo I leave it on you to explore and learn and most important to have fun!! :)","3b504b9e":"Now here's a hack rather instantiating TfidfVectorizer directly and then using .fit_trasform() and .transform() method separately on train and test set we will add this to our ML pipeline so that we don't have to do it every time for train, cross-validation and test sets.\n\nFor the sake of simplicity we will be using [LinearSVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html) and [MultinomialNB](hhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html) model for our solution as they are believed to deliver a robust performance on text data.\n\nFeel free to experiment with other models and share you results :)\n","fce36466":"Our classifier is doing good on +ve tweets but some work still needs to be done so as to improve its performance on -ve tweets.","2af2c58d":"# Dataset \n\nDataset that I have used contains variety of tweets and our objective is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist\/sexist and label '0' denotes the tweet is not racist\/sexist.\n\n# Motivation\n\nHate  speech  is  an  unfortunately  common  occurrence  on  the  Internet.  Often social media sites like Facebook and Twitter face the problem of identifying and censoring  problematic  posts  while weighing the right to freedom of speech. The  importance  of  detecting  and  moderating hate  speech  is  evident  from  the  strong  connection between hate speech and actual hate crimes. Early identification of users promoting  hate  speech  could  enable  outreach  programs that attempt to prevent an escalation from speech to action.","d8a72fd6":"**According to Wikipedia**, Opinion mining (sometimes known as sentiment analysis or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n\n![sentiments](https:\/\/s3.amazonaws.com\/com.twilio.prod.twilio-docs\/images\/SentimentAnalysis.width-800.png)\n\nSentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. Apart for the perspective of marketing, sentiment analysis can also be used for tagging online literature that conveys wrong message or tries to spread hatred in the community.\n\nSentiment Analysis is one of the core part of Natural Language Processing and just like any other domain of Data Science and Machine Learning it's quite challenging for the beginners to find an appropriate starting point.\n\nIn this kernel I have tried to address some of the problems that I faced when I first entered in field of NLP.\nIn my upcoming kernels I'll be sharing some common guidelines and information about the tools that will make your life a bit easier during your beginning.\n\n# Let's Begin\n","6e0813e5":"First let's divide our dataset into two part, first one contains positive tweets and the other one containing negative tweets i.e. tweets with racist\/ sexist remarks","c43b82d0":"We will using [Regular Expression](https:\/\/docs.python.org\/3\/howto\/regex.html) for removing special characters eg. '@','#', etc.","d8b4a1ee":"Now we done with our exploratory data analysis.\n\n# It's time for us to perform some data preprocessing before we can use it for training our classifier.\n"}}