{"cell_type":{"2911959d":"code","dbb6c388":"code","831935d2":"code","48d12985":"code","d54097f2":"code","40a2abab":"code","4a135d0d":"code","6b262b64":"code","3b217f61":"code","41c3e7ae":"code","55f2aae2":"code","1e0cff91":"code","38d912a6":"code","0fa023ee":"code","d3332ec0":"code","9f5679f7":"code","f894c454":"code","58393d7e":"code","47a35a73":"code","077f160b":"code","7d91752c":"code","18fdfc6d":"code","f7735c9f":"code","81e765a6":"code","e98475d3":"code","d479884a":"code","1e9326f9":"code","1d4113db":"code","eeded789":"code","abf93790":"code","c7580fe6":"code","77c75559":"code","565c06bb":"code","b65436b9":"code","aa26fde1":"code","0e28ef39":"code","4208643d":"code","e233c977":"code","d8384e88":"code","d3ff9e80":"code","7ea2b26c":"code","f732d1f6":"code","457ce838":"code","5d938ed0":"code","8a6f2cbb":"code","b64f776a":"code","cc428790":"code","b23b9269":"code","6e9427b8":"code","e1943569":"code","8853b0a2":"code","dd452e89":"code","fbaa0308":"code","6a4bafec":"code","9fb76e17":"code","931179cb":"code","d4be1b6a":"code","f4d5848e":"code","8639d2db":"code","9bbae996":"code","2a74d5e3":"code","23a14090":"code","b17f6b9a":"code","2309098d":"code","78dd4e3f":"code","b85e5803":"code","b1b108cc":"code","f21e7cb3":"code","6ba3f0bc":"code","2dac4bd3":"code","47dea04a":"code","2efd31ec":"code","200e18fa":"code","54bfea5b":"markdown","04accb0f":"markdown","7138804b":"markdown","be62ccc4":"markdown","ed07fd7e":"markdown","dbd77ad6":"markdown","e9f2e845":"markdown","39f51649":"markdown","7899b0ff":"markdown","da7bcad2":"markdown","280809f5":"markdown","bd11d368":"markdown","05eb631d":"markdown","f82bc3f5":"markdown","722cee74":"markdown","96a0b5ec":"markdown","67250dce":"markdown","c3cc48b9":"markdown","bcf7d2aa":"markdown","1817e4db":"markdown","ada30fc5":"markdown","81c28e45":"markdown","ef9de8f5":"markdown","9c677a9b":"markdown"},"source":{"2911959d":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n","dbb6c388":"from albumentations import *\nimport cv2\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#!pip install pretrainedmodels\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision.models import *\n#import pretrainedmodels\n\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\nfrom fastai.callbacks import * \n\n#from utils import *\nimport sys","831935d2":"path = \"..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\"","48d12985":"import numpy as np\nimport pandas as pd","d54097f2":"#trainAV = pd.read_csv(\"..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/train.csv\")\ntrainAV = pd.read_csv(\"..\/input\/cleantrain\/clean_tr.csv\")\ntestAV = pd.read_csv(\"..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/test_vc2kHdQ.csv\")","40a2abab":"trainAV.shape","4a135d0d":"trainAV.info()","6b262b64":"trainAV.head()","3b217f61":"tfms = get_transforms(max_rotate=90.0, max_zoom=1.3, max_lighting=0.4, max_warp=0.4,\n                      p_affine=1.0, p_lighting=1.)","41c3e7ae":"bs = 32 #with image size 299, bs=48 & above will allocate more memory\nsz = 299","55f2aae2":"np.random.seed(42)\ndata = ImageDataBunch.from_csv('..\/input', folder = 'emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/images', csv_labels = 'cleantrain\/clean_tr.csv',\n                               valid_pct=0.10,size = sz, ds_tfms = tfms,bs=bs)\ndata.normalize(imagenet_stats)","1e0cff91":"data.show_batch(rows = 3)","38d912a6":"data.train_ds","0fa023ee":"def _plot(i,j,ax):\n    x,y = data.train_ds[3]\n    x.show(ax, y=y)\n\nplot_multi(_plot, 3, 3, figsize=(8,8))","d3332ec0":"print(data.classes); data.c","9f5679f7":"gc.collect()\nlearnResnet34 = cnn_learner(data, models.resnet34, metrics=error_rate, bn_final=True)","f894c454":"learnResnet34.fit_one_cycle(1)","58393d7e":"gc.collect()\nlearnResnet50 = cnn_learner(data, models.resnet50, metrics=error_rate, bn_final=True)","47a35a73":"learnResnet50.fit_one_cycle(1)","077f160b":"gc.collect()\nlearnResnet152 = cnn_learner(data, models.resnet152, metrics=accuracy, bn_final=True)#error_rate","7d91752c":"learnResnet152.fit_one_cycle(1)","18fdfc6d":"learnResnet152.model_dir = \"\/kaggle\/working\/models\"\nlearnResnet152.save(\"stage-152-1\")","f7735c9f":"learnResnet152.unfreeze()\nlearnResnet152.fit_one_cycle(1)","81e765a6":"learnResnet152.model_dir = \"\/kaggle\/working\/models\"\nlearnResnet152.save(\"stage-152-2\")","e98475d3":"learnResnet152.load('stage-152-2');\nlearnResnet152.lr_find()\n","d479884a":"learnResnet152.recorder.plot()","1e9326f9":"learnResnet152.load('stage-152-2');","1d4113db":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","eeded789":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","abf93790":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","c7580fe6":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","77c75559":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","565c06bb":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","b65436b9":"learnResnet152.save('stage-152-fin1');","aa26fde1":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","0e28ef39":"learnResnet152.fit_one_cycle(1, max_lr=slice(3e-7,1e-3))","4208643d":"learnResnet152.fit_one_cycle(1, max_lr=slice(3e-7,1e-3))","e233c977":"learnResnet152.load('stage-152-fin1');","d8384e88":"learnResnet152.freeze()","d3ff9e80":"learnResnet152.fit_one_cycle(3, max_lr=slice(1e-3))","7ea2b26c":"learnResnet152.fit_one_cycle(3, max_lr=slice(1e-3))","f732d1f6":"learnResnet152.fit_one_cycle(3, max_lr=slice(3e-7,1e-3))","457ce838":"learnResnet152.fit_one_cycle(1, max_lr=slice(1e-3))","5d938ed0":"learnResnet152.unfreeze()","8a6f2cbb":"learnResnet152.fit_one_cycle(3, max_lr=slice(1e-4))","b64f776a":"learnResnet152.fit_one_cycle(1, max_lr=slice(1e-4))","cc428790":"learnResnet152.fit_one_cycle(1, max_lr=slice(1e-5))","b23b9269":"learnResnet152.fit_one_cycle(1, max_lr=slice(1e-5))","6e9427b8":"learnResnet152.fit_one_cycle(1, max_lr=slice(1e-5))","e1943569":"learnResnet152.fit_one_cycle(5, max_lr=slice(1e-5))","8853b0a2":"learnResnet152.load('stage-152-fin1');","dd452e89":"interp = ClassificationInterpretation.from_learner(learnResnet152)\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)\ninterp.plot_top_losses(4, figsize=(15,11))","fbaa0308":"interp.plot_confusion_matrix()","6a4bafec":"learnVGG = cnn_learner(data, models.vgg19_bn, metrics=accuracy, bn_final=True)#error_rate\n#learnVGG =  VGG16()\n#ConvLearner.pretrained","9fb76e17":"learnVGG.fit_one_cycle(3)","931179cb":"learnVGG.model_dir = \"\/kaggle\/working\/models\"\nlearnVGG.save(\"stage-vgg-1\")","d4be1b6a":"learnVGG.unfreeze()\nlearnVGG.fit_one_cycle(1)","f4d5848e":"learnVGG.save(\"stage-vgg-2\")","8639d2db":"learnVGG.lr_find()\nlearnVGG.recorder.plot()","9bbae996":"learnVGG.fit_one_cycle(6,slice(1e-6,1e-5))","2a74d5e3":"interpvg = ClassificationInterpretation.from_learner(learnVGG)\nlosses,idxs = interpvg.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)\ninterpvg.plot_top_losses(9, figsize=(15,11))","23a14090":"interpvg.plot_confusion_matrix()","b17f6b9a":"dft = pd.read_csv('..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/test_vc2kHdQ.csv')\ndft.head()","2309098d":"dft.shape","78dd4e3f":"dt_test = ImageList.from_df(dft, '..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/images')","b85e5803":"# To get image from images folder based on name from test.csv\n# str('..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/images\/'+dft[\"image_names\"][1])","b1b108cc":"img = open_image(str('..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/images\/'+dft[\"image_names\"][1]))\npred_class,pred_idx,outputs = learnResnet152.predict(img)\npred_class\n","f21e7cb3":"defaults.device = torch.device('cpu')\nlabl =[]\nfor i in range(dft.shape[0]): #\n    img = open_image(str('..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/images\/'+dft[\"image_names\"][i]))\n    #img = img.normalize(imagenet_stats)\n    pred_class,pred_idx,outputs = learnResnet152.predict(img)\n    labl.append(pred_class)","6ba3f0bc":"defaults.device = torch.device('cpu')\nlabl =[]\nfor i in range(dft.shape[0]): #\n    img = open_image(str('..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train_SOaYf6m\/images\/'+dft[\"image_names\"][i]))\n    #img = img.normalize(imagenet_stats)\n    pred_class,pred_idx,outputs = learnVGG.predict(img)\n    labl.append(pred_class)","2dac4bd3":"len(labl)","47dea04a":"sample = pd.read_csv('..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/sample_submission_yxjOnvz.csv')","2efd31ec":"sample.head()","200e18fa":"#create datafarme for submission & export\nsample['image_names'] = testAV['image_names']\nsample['emergency_or_not'] = labl\nsample.to_csv('submit141.csv', index=False)","54bfea5b":"### Results\nLet's see what results we have got.\n\nWe will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly.\n\nFurthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.","04accb0f":"## Let's plot & see","7138804b":"### Predict from vgg19","be62ccc4":"## Apply different Fast-AI ResNet Models & train it","ed07fd7e":"### Load neccessary libraries","dbd77ad6":"# Thank you","e9f2e845":"### Trained model with various validation sets & finally for 90% train & 10% test, this will give highest model accuracy","39f51649":"### Predict for target images\n### from Resnet152","7899b0ff":"trainKg = pd.read_csv(\"..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/train.csv\")\ntestKg = pd.read_csv(\"..\/input\/emergency-vs-nonemergency-vehicle-classification\/dataset\/test_vc2kHdQ.csv\")","da7bcad2":"### Let's unload pre-trained set, get learning rate & train model for training data we have","280809f5":"## Resnet-152","bd11d368":"With pre-trained model we get 90.24% accuracy","05eb631d":"We get 93.9% accuracy with resnet34 model","f82bc3f5":"VGG19 modle gives 0.9513 score on AV","722cee74":"### Export submission file","96a0b5ec":"We get 92.7% accuracy with resnet50 model","67250dce":"# Computer Vision Hackathon AV with fast-ai","c3cc48b9":"## Predict for given Test data","bcf7d2aa":"## Resnet-50","1817e4db":"## Apply vgg19_bn models","ada30fc5":"Clean_tr is file with updated mislabelled images","81c28e45":"### Results\nLet's see what results we have got.\n\nWe will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not. In this case the mistakes look reasonable (none of the mistakes seems obviously naive). This is an indicator that our classifier is working correctly.\n\nFurthermore, when we plot the confusion matrix, we can see that the distribution is heavily skewed: the model makes the same mistakes over and over again but it rarely confuses other categories. This suggests that it just finds it difficult to distinguish some specific categories between each other; this is normal behaviour.","ef9de8f5":"differential learning rates to alter the lower layers as well. The lower layers want to be altered less, so it is good practice to set each learning rate to be 10 times lower than the last:","9c677a9b":"## Resnet-34"}}