{"cell_type":{"6402517f":"code","be6310ab":"code","513a0096":"code","843f3b1f":"code","957e1aca":"code","dbe2c080":"code","070730ea":"code","7f4960e5":"code","4b0ceaa9":"code","c797a922":"code","45fe0740":"code","b331e8c3":"code","6f3f9a8f":"code","41e62953":"code","67e632f4":"code","397fa19b":"code","9fcc8f05":"code","dfac9e73":"code","5adb2dc3":"code","9b87b921":"code","5f9b560c":"code","51162c8a":"code","54a8fec0":"markdown","53f9fdf3":"markdown","df36d792":"markdown","2a21bf52":"markdown","62a3feb9":"markdown","59b01d23":"markdown","dc0b264b":"markdown","8717796b":"markdown","d0f09d21":"markdown","75a9cb70":"markdown","b5d57930":"markdown","3a15a7b9":"markdown","fa381115":"markdown","10895d66":"markdown","f35a008f":"markdown","0cbc0e8d":"markdown","5a5381cb":"markdown","4f6e23ef":"markdown","5f43ba20":"markdown","340a2032":"markdown","ba21544e":"markdown","d0dbd549":"markdown","14fbdd5c":"markdown","edc23364":"markdown","aec544c8":"markdown","27bd49f7":"markdown","9dbaf822":"markdown"},"source":{"6402517f":"%%time\n!pip install tensorflow==1.15\n!pip install bert-serving-server==1.10.0\n!pip install bert-serving-client==1.10.0\n\n!cp \/kaggle\/input\/biobert-pretrained \/kaggle\/working -r\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.index \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.index\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.data-00000-of-00001 \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.data-00000-of-00001\n%mv \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/model.ckpt-1000000.meta \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed\/bert_model.ckpt.meta\n\n!pip install transformers\n!pip install sentence-transformers\n!pip install rake-nltk\n\nprint('installation done')","be6310ab":"import subprocess\nimport pickle as pkl\nimport pandas as pd\nimport numpy as np \nfrom sentence_transformers import SentenceTransformer\nfrom transformers import BertTokenizer, BertModel\nfrom ipywidgets import interact, widgets # this is what makes the dataframe interactive\nfrom scipy.spatial.distance import cdist\nfrom scipy.spatial.distance import jensenshannon\nfrom IPython.display import HTML, display\nimport matplotlib.pyplot as plt\nfrom os import path\nfrom PIL import Image\nfrom textblob import TextBlob\nimport pyLDAvis.gensim\nimport pyLDAvis\nimport gensim\nimport spacy\nimport os\nfrom scipy import spatial\n\nplt.style.use(\"dark_background\")\n","513a0096":"df = pkl.load(open('..\/input\/bertbiobertdataframe\/BERT-BioBERT-dataframe.pkl', \"rb\"))","843f3b1f":"meta_df=pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\nmeta_df = meta_df.dropna(subset=['url'])","957e1aca":"from collections import Counter\nmeta_df = meta_df.dropna(subset=['journal'])\njournals=meta_df['journal'].tolist()\n\ncount = Counter(journals)\nfreq=count.most_common(10)\n\npaper_count=pd.DataFrame(freq,columns=['journals','number of papers'])\npaper_count.sort_values('number of papers', ascending=False).set_index('journals')[:20].sort_values('number of papers', ascending=True).plot(kind='barh')","dbe2c080":"# df['abstract_word_count'] = df['abstract'].apply(lambda x: len(x.strip().split()))\n# df['body_word_count'] = df['body_text'].apply(lambda x: len(x.strip().split()))\n# df['title_word_count'] = df['title'].apply(lambda x: len(x.strip().split()))\n# df[['abstract_word_count', 'body_word_count','title_word_count']].plot(kind='box', title='Boxplt of Word Count', figsize=(10,6))\n# plt.show()\n","070730ea":"import seaborn as sns\n\nheadline_length=df['title'].str.len()\nsns.distplot(headline_length)\nplt.show()\nheadline_length=df['abstract'].str.len()\nsns.distplot(headline_length)\nplt.show()\nheadline_length=df['body_text'].str.len()\nsns.distplot(headline_length)\nplt.show()","7f4960e5":"df['polarity'] = df['abstract'].map(lambda text: TextBlob(text).sentiment.polarity)\ndf['abstract_len'] = df['abstract'].astype(str).apply(len)\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\ndf['polarity'].iplot(kind='hist',bins=50,xTitle='polarity',linecolor='black',yTitle='count',title='Sentiment Polarity Distribution')\n","4b0ceaa9":"def Text(list):  \n    str1 = \"\"    \n    for element in list:  \n        str1 += element \n    return str1\ndef getwordcloud(text): \n    wordcloud = WordCloud(max_font_size=256, max_words=500, background_color=\"white\",stopwords = set(STOPWORDS),random_state=42, width=500, height=500).generate(text)\n    plt.figure()\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n   ","c797a922":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nabstracts=Text(df['abstract'][:10].tolist())\ncommon_abstract_word=getwordcloud(abstracts)","45fe0740":"df2=df\ndf2['title']=df2['title'].astype(str)\ntitle=Text(df2['title'].tolist())\ncommon_abstract_word=getwordcloud(title)","b331e8c3":"#Bert\nbert_client = SentenceTransformer('bert-base-nli-max-tokens')\n\n#Biobert\nbio_bert_command = 'bert-serving-start -model_dir \/kaggle\/working\/biobert-pretrained\/biobert_v1.1_pubmed -max_seq_len=None -max_batch_size=32 -num_worker=2'\nprocess = subprocess.Popen(bio_bert_command.split(), stdout=subprocess.PIPE)\nfrom bert_serving.client import BertClient\nbiobert_client = BertClient(ignore_all_checks=True)","6f3f9a8f":"bio_vectors = np.array(df.biobert_vector.tolist())\nbert_vectors = np.array(df.bert_vector.tolist())","41e62953":"from scipy import spatial\ndef score(model_vectors,model_encode,size):\n    score = []\n    for i in range(size):\n        result = 1 - spatial.distance.cosine(model_vectors[i],model_encode)\n        score.append(result)\n    return score","67e632f4":"def avrage (model1_score,model2_score):\n    l=[sum(n) for n in zip(*[model1_score,model2_score])]\n    final_score = [x * 0.5 for x in l]\n    return final_score ","397fa19b":"default_question = 'Neonates and pregnant women'\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('max_colwidth', 180)\n\nresults=[]\ntotal_docs=df.shape[0]\n@interact\ndef search_articles(\n    query=default_question,\n    num_results=[10, 25, 100],show_scores=[False, True],score_type=['Cosine']):\n\n    bio_encode =biobert_client.encode([query])\n    bert_encode = bert_client.encode([query])\n   \n    if score_type is 'Cosine':\n        bert_score=score(bert_vectors,bert_encode,total_docs)\n        bio_score=score(bio_vectors,bio_encode,total_docs)\n        f_score=avrage(bert_score,bio_score)\n        df[\"score\"] = f_score\n        select_cols = ['title', 'abstract', 'authors', 'score','url']\n        results = df[select_cols].sort_values(by=['score'], ascending=False).head(num_results)\n        results = results.dropna(subset=['title'])\n        \n#     print(\"results : {}\".format(results[20703\t]))\n    if (len(results.index) == 0):\n        print('NO RESULTS')\n        \n        return None\n    else:\n        \n\n        top_row = results.iloc[0]\n\n        print('TOP RESULT OUT OF ' + str(total_docs) + ' DOCS FOR QUESTION:\\n' + query + '\\n')\n        print('TITLE: ' + str(top_row['title']) + '\\n')\n        print('ABSTRACT: ' + top_row['abstract'] + '\\n')\n        #print('PREDICTED TOPIC: ' + topic_list[int(top_row['best_topic'].replace('topic_', ''))])\n\n        print('\\nAUTHORS: ' + str(top_row['authors']))\n\n        select_cols.remove('authors')\n        \n        return results[select_cols]","9fcc8f05":"df_LDA = pkl.load(open('..\/input\/lda-pkl\/lda_output_final.pkl', \"rb\"))\ndf['license']=meta_df['license']","dfac9e73":"def relevant_articles_lda(query, k,show_LDA,show_Bert ):\n    \n    bert_encode = bert_client.encode([query])    \n    bert_vectors = np.array(df.bert_vector.tolist())\n    \n    LDA_encode = pkl.load(open('..\/input\/lda-pkl\/'+str(query)+ '.pkl', \"rb\"))\n    LDA_vectors = np.array(df_LDA)\n    \n    bert_score = score(bert_vectors,bert_encode,df_LDA.shape[0])\n    LDA_score = score(LDA_vectors,LDA_encode,df_LDA.shape[0])\n       \n    df[\"bert_scores\"]=bert_score\n    df[\"LDA_scores\"]=LDA_score\n    df[\"LDA_scores\"] = df[\"LDA_scores\"].apply(lambda x: x*15)\n    \n    scores = pd.DataFrame({'bert_scores':bert_score,'LDA_scores': df[\"LDA_scores\"]})\n    df['avg_scores'] = scores.mean(axis=1)\n    \n    out_avg=df.sort_values(by=['avg_scores'],ascending=False)\n    \n    out_LDA=df.sort_values(by=['LDA_scores'],ascending=False)\n    out_bert=df.sort_values(by=['bert_scores'],ascending=False)\n    \n    h = '<br\/>'.join(['<a href=\"' + str(l) + '\" target=\"_blank\">'+ str(n) + '<\/a>' for l, n in out_avg[['url','title']].head(k).values])\n    display(HTML(h))\n    \n    if(show_LDA):\n        h = '<br\/>'.join(['<a href=\"' + str(l) + '\" target=\"_blank\">'+ str(n) + '<\/a>' for l, n in out_LDA[['url','title']].head(k).values])\n        display(HTML(h))\n   \n    if(show_Bert):\n        h = '<br\/>'.join(['<a href=\"' + str(l) + '\" target=\"_blank\">'+ str(n) + '<\/a>' for l, n in out_bert[['url','title']].head(k).values])\n        display(HTML(h))\n","5adb2dc3":"query_list=Risk_factors_list=[\"cerebrovascular\",\"Age\" ,\"Cancer\",\"Chronic\",\"Diabetes\",\"Drinking\",\"Heart disease\",\"Hypertension\",\"Obese\",\"Respiratory\",\"smoke\",\"Liver\",\"Male gender\",\"COPD\"]\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('max_colwidth', 180)\n\n@interact\ndef search_articles(\n    query=query_list,num_results=[5, 10, 25],show_LDA=[False,True],show_Bert=[False,True]):\n    \n    relevant_articles_lda(query,num_results,show_LDA,show_Bert)","9b87b921":"license=df.pivot_table(index=['license'], aggfunc='size')\nprint(len(license))\nlicense","5f9b560c":"out_avg=df.sort_values(by=['avg_scores'],ascending=False)\nlicense_list=[\"bronze-oa\",\"cc-by\",\"cc-by-nc\",\"gold-oa\",\"green-oa\"]\nlicense_color=[\"bronze\",   \"red\",    \"pink\",  \"gold\",  \"green\"]\n\ndef highlight_license(s):\n    for i in range(len(license_list)):\n        if s.license == license_list[i]:\n            return ['background-color:'+ str(license_color[i])]*2\n    return ['background-color:white']*2","51162c8a":"out_colored=out_avg[[\"license\",\"title\"]].iloc[70:100]\nout_colored.style.apply(highlight_license, axis=1)\n","54a8fec0":"# Pros. \n1. You can try many models:  Biobert, Bert and LDA.\n   Then by avraging the scores together -as in the main kernel-, you can get more accurate output.\n2. The body is used on some methods and the abstract with the title on others, so as to get as much information as possible when mixing the methods.\n3. The output of all the methods is saved as pkl files, so the user can use it quickly.\n4.  Very interactive GUI.\n5. Visualization of data.\n","53f9fdf3":"## Get avrage Score:","df36d792":"### 1.Data","2a21bf52":"[CC-BY](http:\/\/openaccess.ox.ac.uk\/2013\/06\/13\/cc-by-what-does-it-mean-for-scholarly-articles-3\/) <span style=\"color:red\">Creative Commons 'Attribution' licence <\/span><br\/>\n[cc-by-nc](https:\/\/en.wikipedia.org\/wiki\/Creative_Commons_NonCommercial_license) <span style=\"color:pink\">Creative Commons NonCommercial license <\/span><br\/>\n[gold-oa](https:\/\/www.rvc.ac.uk\/research\/about\/open-access-publishing\/gold-open-access) <span style=\"color:gold\">Gold open access<\/span><br\/>\n[green-oa](https:\/\/www.rvc.ac.uk\/research\/about\/open-access-publishing\/gold-open-access) <span style=\"color:green\">Green open access <\/span><br\/>","62a3feb9":"### Resources:\n* [BioBERT_BERT Encoding notebook](https:\/\/www.kaggle.com\/jdparsons\/biobert-corex-topic-search)\n* [Topic Modeling notebook](https:\/\/www.kaggle.com\/danielwolffram\/topic-modeling-finding-related-articles)\n* [BERT Word Embeddings Tutorial](https:\/\/l.facebook.com\/l.php?u=https%3A%2F%2Fmccormickml.com%2F2019%2F05%2F14%2FBERT-word-embeddings-tutorial%2F%3Ffbclid%3DIwAR0Tszxw2niNjbWOYvm9K3NV6syx4kP2AsbFvttIUArZxn0sJ_zGEOIaEF4&h=AT3C0KI7RcUlmwdtb-YKUvyBzhdXo9zIjTM3dwBreUm3XmyVyepLMqwTKnzbj_rmoH_FJa1x64is1L11hGHfDHInnidkbHzimnZyh3Zx4Z4vJQueXowbNHBWLnrkq-zo5hyXGw)","59b01d23":"3.Distribution of length for Both Body and Abstract3","dc0b264b":"# LDA-Risk Factors-","8717796b":"4.Visualisation of The distribution of abstract sentiment polarity score","d0f09d21":"***","75a9cb70":" 2.Word Count for Both Body and Abstract","b5d57930":"# Cons\n* Running code is very slow for the first time for new dataset.","3a15a7b9":"5.Word Cloud For Common Word in Title and Abstract","fa381115":"1.The following plot show you the top ten journals participated to share COVID_19 papers","10895d66":"# Data load","f35a008f":"***","0cbc0e8d":"# Methodology for LDA's risk factor tester\n* Here we used LDA model to ask queries about our task -Risk Factors-.\n* Each query is fitted on our main kernel and Pkl files are uploaded here for fast use.","5a5381cb":"### 3.Search","4f6e23ef":"# 4.GUI","5f43ba20":"# License","340a2032":"# Models","ba21544e":"# Data visualization","d0dbd549":"## Cosine function to get models Score:","14fbdd5c":"***","edc23364":"# Mixer GUI","aec544c8":"# Methodology for mixer\n*  we use the output vectors from [our notebook](https:\/\/www.kaggle.com\/fatma98\/biobert-bert-encoding), at which we make BERT and BioBERT vectorization model, and here we will avraging the score of BERT model based on BERT ecoding on the paper body_txt and BioBERT model  based on BioBERT encoding on papers title and abstract.","27bd49f7":"# Notebook purpose:\n* At this notebook you can get the nearest articles relevant to your questions about COVID_19.\n* You can get nearest article for each risk factor.\n","9dbaf822":"# Imports & Installation"}}