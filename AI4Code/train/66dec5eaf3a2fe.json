{"cell_type":{"99c5e900":"code","5ec7e9eb":"code","b7079bf7":"code","e19d0c6a":"code","ee8e4141":"code","3a1c8319":"code","305eaf7f":"code","0c5d6c19":"code","61f4cd0f":"code","d1b9778a":"code","88b9d544":"code","c0ebb07d":"code","0e2cce54":"code","1670d2a0":"code","dd792f3d":"code","47e426e4":"code","ec4c39a4":"code","f8287f80":"code","965313a8":"code","c1a19e1f":"code","66e2c907":"code","d3c58767":"code","ba07711f":"code","5ae800f7":"code","22ce0dca":"code","ab2291a4":"code","ab8e2af5":"code","787e6ba4":"code","c5193a29":"code","f7441d13":"code","e439edc3":"code","48ae5e1f":"code","ebb148d8":"code","47b2abe5":"code","410baab3":"code","422e3f5a":"code","d814eaa3":"code","322a343b":"code","df2a372d":"code","44ddea27":"code","7582b8a8":"code","bdf166e6":"code","1e52439d":"code","6253c9eb":"code","bb22d378":"code","c1d196b0":"code","bf8d9ab0":"code","1f84862c":"code","fd57c07e":"code","6ab3ef54":"code","3e1ab904":"code","52da2bc2":"code","3416436b":"code","cae0d4da":"code","3f71aa94":"code","0000fe3d":"code","2d4c0efe":"code","cb22a1a3":"code","0282d7bf":"code","fecebe1f":"code","7ebc814c":"code","3831b7b0":"code","94db5682":"code","45146705":"code","041ab599":"code","6b04d330":"code","d95886f8":"code","d04623d0":"code","7a0711ec":"code","178b793a":"code","b538afbb":"code","aa61333e":"code","8156446c":"code","e07815f7":"code","3bcf3f57":"code","f6c136ec":"code","50dd06c4":"code","96e430e5":"code","554e3879":"code","48355f6e":"code","38cd5521":"code","1165771c":"code","b16063e6":"code","b60a7e3b":"code","cdcd903f":"code","e2217f29":"code","5049352b":"code","84bcebb6":"code","eae92081":"code","806d6a02":"code","528e7135":"code","0798ce21":"code","b6a01210":"code","e340062b":"code","10134f88":"code","690a2f9c":"code","0abbad0e":"code","c542cf4a":"code","e01fc4a8":"code","8b1c40c5":"code","529ff147":"code","eb543d51":"code","e562db26":"code","2849366e":"markdown","29069c83":"markdown","409ccdbb":"markdown","a4dcc662":"markdown","9ca36d19":"markdown","8c164498":"markdown","49e1fcd0":"markdown","3d21e469":"markdown","980c4e59":"markdown","d724a6b0":"markdown","949f5a0d":"markdown","0a75a8d9":"markdown","774f9b38":"markdown","221d264b":"markdown","3cc6a99f":"markdown","3c06b4be":"markdown","aa847617":"markdown","4c08715e":"markdown","2b5264ff":"markdown","1c5bc8ec":"markdown","3a7f8445":"markdown","6d5d015a":"markdown","56e1b24e":"markdown","a59274ff":"markdown","1902e416":"markdown","5ae77a68":"markdown","bdac653c":"markdown","20ddd27c":"markdown","46eb02db":"markdown","b1f7c28c":"markdown","7aa4cb5f":"markdown","c5a47896":"markdown","998835b9":"markdown","b3da4853":"markdown","8b139e7c":"markdown","73271552":"markdown","b043d0e0":"markdown","5a88a727":"markdown","f1fdf85a":"markdown","e43079ae":"markdown","1bec09af":"markdown","eaaa89eb":"markdown","a6214a55":"markdown","e8d349a0":"markdown","5b80a55c":"markdown","51a94329":"markdown","500c7a42":"markdown","f32a5e0a":"markdown","aab78c11":"markdown","17baa481":"markdown","3f2daa88":"markdown","22ef141e":"markdown","ef52c63f":"markdown","b040301f":"markdown","ab686776":"markdown","81a22596":"markdown","8ae45ca6":"markdown","822a55ef":"markdown","ab684614":"markdown","44eb536d":"markdown","cba2552c":"markdown","a97618a2":"markdown","c74e1d68":"markdown","8e39ed49":"markdown","2d73c20b":"markdown","2a91b30c":"markdown","df476dd1":"markdown","0f05617d":"markdown","bd167a8a":"markdown","2376cef8":"markdown","fd4f2af0":"markdown","8a28d629":"markdown","5e31ae37":"markdown","4d0988f7":"markdown","52d10284":"markdown"},"source":{"99c5e900":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression ,Ridge,Lasso\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,median_absolute_error,r2_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n","5ec7e9eb":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","b7079bf7":"data_train = train.copy()\ndata_train.head()\n","e19d0c6a":"data_train_Id = data_train.drop('Id',axis=1,inplace = True)\ntest_Id = test['Id']\ntest.drop('Id',axis=1,inplace = True)\ndata_train.info()","ee8e4141":"data_train.describe()","3a1c8319":"#missing data\ntotal = data_train.isnull().sum().sort_values(ascending=False)\npercent = (data_train.isnull().sum()\/data_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent*100], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","305eaf7f":"#correlation matrix\ncorr_matrix = data_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_matrix, vmax=.8, square=True);","0c5d6c19":"corr_matrix['SalePrice'].sort_values(ascending=False)","61f4cd0f":"#Visualizing the most correlated features\ncols = ['SalePrice','OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF',\n        'FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd','GarageYrBlt','MasVnrArea','Fireplaces']\nsns.pairplot(data_train[cols],diag_kind='kde')","d1b9778a":"def remove_bad_features(data):\n    return data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'],axis=1,inplace=True)\nremove_bad_features(data_train)","88b9d544":"fig, ax = plt.subplots() \nax.scatter(x = data_train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","c0ebb07d":"def remove_outlier(data):\n    data = data[data['GrLivArea']< 5000]\n    data.reset_index(drop= True,inplace=True)\n    return\nremove_outlier(data_train)\ndata_train.info()","0e2cce54":"def remove_corr_features(data):\n    features = ['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageArea']\n    return data.drop(features,axis=1,inplace=True)\nremove_corr_features(data_train)","1670d2a0":"data_train[['LotFrontage','MasVnrType','MasVnrArea','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical',\n            'GarageType','GarageFinish','GarageQual','GarageCond']].head(20)","dd792f3d":"# Group by neighborhood and fill in missing value by the median of all the neighborhood\ndef imputing_missing_values(data):\n        data['LotFrontage'] = data.groupby(\"Neighborhood\")['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n        data['MasVnrArea'].fillna(0,inplace=True)\n        data.fillna('None',inplace=True)\n        return\nimputing_missing_values(data_train)\ndata_train.info()","47e426e4":"cat_data = data_train.select_dtypes(object).copy()\ncat_data.head()","ec4c39a4":"for col in cat_data.columns:\n    print(cat_data[col].unique())","f8287f80":"def transform_categorical(data):\n    global data_train\n    data = pd.get_dummies(data,columns=[col for col in cat_data.columns])\n    data_train = data.copy()\n    return \ntransform_categorical(data_train)\ntotal_columns = data_train.copy()\ntotal_columns =total_columns.columns\ndata_train.shape","965313a8":"cols = ['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF',\n        'FullBath','YearBuilt','YearRemodAdd','MasVnrArea','Fireplaces']\nfor col in cols:\n    fig = plt.figure()\n\n    sns.distplot(data_train[col], fit=norm);\n    fig = plt.figure()\n    stats.probplot(data_train[col], plot=plt)\n","c1a19e1f":"# applying log transformation\ncolumns = ['SalePrice','GrLivArea','TotalBsmtSF','YearBuilt','YearRemodAdd','MasVnrArea']\nfor col in columns :\n    data_train[col+'temp'] = np.log(data_train[col].replace(0,1))\n\nfor col in columns:\n    fig = plt.figure()\n    sns.distplot(data_train[col+'temp'], fit=norm)\n    fig = plt.figure()\n    stats.probplot(data_train[col+'temp'], plot=plt)","66e2c907":"columns_1 = ['TotalBsmtSF','YearBuilt','YearRemodAdd','MasVnrArea']\nfor col in columns_1 :\n    data_train[col+'temp'] = np.sqrt(data_train[col].replace(0,1))\n\nfor col in columns_1:\n    fig = plt.figure()\n    sns.distplot(data_train[col+'temp'], fit=norm)\n    fig = plt.figure()\n    stats.probplot(data_train[col+'temp'], plot=plt)","d3c58767":"# SQRT not doing well \nfor col in columns:\n    data_train.drop([col+'temp'],axis=1,inplace = True)\ndef transform_features(data):\n    columns = ['SalePrice','GrLivArea']\n    for col in columns:\n        data[col] = np.log(data[col].replace(0,1))\n    return\ntransform_features(data_train)\nsns.distplot(data_train['SalePrice'], fit=norm)","ba07711f":"std_scaler = StandardScaler()\n\ndef feature_scaling(data):\n    global data_train\n    global std_scaler\n    y= data['SalePrice']\n    data.drop('SalePrice',axis=1)\n    data_train_array = std_scaler.fit_transform(data)\n    data_ = pd.DataFrame(data_train_array , columns = data.columns , index = data.index)\n    data_['SalePrice'] = y\n    data_train = data_.copy()\n    return\nfeature_scaling(data_train)\ndata_train.head(10)","5ae800f7":"# splitting\ny = data_train['SalePrice']\ndata_train.drop('SalePrice', axis = 1,inplace=True)\nX = data_train\nX_train, X_test ,y_train, y_test = train_test_split(X.values,y.values, test_size=0.2,shuffle =True ,random_state=0)","22ce0dca":"print(X_train.shape,X_test.shape)","ab2291a4":"reg_model = LinearRegression()\nreg_model.fit(X_train,y_train)\ny_pred_reg = reg_model.predict(X_train)\n","ab8e2af5":"print('the training score = ',reg_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_reg)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","787e6ba4":"def dispaly_scores(scores):\n    print('scores : ', scores)\n    print('mean = ', scores.mean())\n    print('standard deviation = ',scores.std())\n","c5193a29":"reg_score = cross_val_score(reg_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nreg_rmse_score = np.sqrt(-reg_score)\ndispaly_scores(reg_rmse_score)","f7441d13":"ridge_model = Ridge(random_state=42 , alpha=0.0005)\nridge_model.fit(X_train,y_train)\ny_pred_ridge = ridge_model.predict(X_train)","e439edc3":"print('the training score = ',ridge_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_ridge)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","48ae5e1f":"ridge_score = cross_val_score(ridge_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nridge_rmse_score = np.sqrt(-ridge_score)\ndispaly_scores(ridge_rmse_score)","ebb148d8":"lasso_model = Lasso(random_state=42 , alpha=0.0005)\nlasso_model.fit(X_train,y_train)\ny_pred_lasso = ridge_model.predict(X_train)","47b2abe5":"print('the training score = ',lasso_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_lasso)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","410baab3":"lasso_score = cross_val_score(lasso_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nlasso_rmse_score = np.sqrt(-lasso_score)\ndispaly_scores(lasso_rmse_score)","422e3f5a":"tree_model = DecisionTreeRegressor(random_state=42)\ntree_model.fit(X_train,y_train)\ny_pred_tree = tree_model.predict(X_train)","d814eaa3":"print('the training score = ',tree_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_tree)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","322a343b":"tree_score = cross_val_score(tree_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\ntree_rmse_score = np.sqrt(-tree_score)\ndispaly_scores(tree_rmse_score)","df2a372d":"forest_model = RandomForestRegressor(random_state=42)\nforest_model.fit(X_train,y_train)\ny_pred_random = forest_model.predict(X_train)","44ddea27":"print('the training score = ',forest_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_random)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","7582b8a8":"forest_score = cross_val_score(forest_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nforest_rmse_score = np.sqrt(-forest_score)\ndispaly_scores(forest_rmse_score)","bdf166e6":"svr_model = SVR(kernel='rbf',C= 1)\nsvr_model.fit(X_train,y_train)\ny_pred_svr = svr_model.predict(X_train)","1e52439d":"print('the training score = ',svr_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_svr)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","6253c9eb":"svr_score = cross_val_score(svr_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nsvr_rmse_score = np.sqrt(-svr_score)\ndispaly_scores(svr_rmse_score)","bb22d378":"gbr_model = GradientBoostingRegressor(learning_rate=1, n_estimators=1000, random_state=42)\ngbr_model.fit(X_train,y_train)\ny_pred_gbr = gbr_model.predict(X_train)","c1d196b0":"print('the training score = ',gbr_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_gbr)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","bf8d9ab0":"gbr_score = cross_val_score(gbr_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\ngbr_rmse_score = np.sqrt(-gbr_score)\ndispaly_scores(gbr_rmse_score)","1f84862c":"xgb_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1)\nxgb_model.fit(X_train,y_train)\ny_pred_xgb = xgb_model.predict(X_train)","fd57c07e":"print('the training score = ',xgb_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_xgb)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","6ab3ef54":"xgb_score = cross_val_score(xgb_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nxgb_rmse_score = np.sqrt(-xgb_score)\ndispaly_scores(xgb_rmse_score)","3e1ab904":"lgb_model = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nlgb_model.fit(X_train,y_train)\ny_pred_lgb = lgb_model.predict(X_train)","52da2bc2":"print('the training score = ',lgb_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred_lgb)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","3416436b":"lgb_score = cross_val_score(lgb_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nlgb_rmse_score = np.sqrt(-lgb_score)\ndispaly_scores(lgb_rmse_score)","cae0d4da":"tune_pipeline_lasso = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',Lasso(random_state = 42))])\n\ngrid_search_lasso = GridSearchCV( estimator = tune_pipeline_lasso, param_grid = {'selector__k':[200,276] , \n  'model__alpha':[0.03,0.05]}, n_jobs=-1,scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],\n                                 refit = 'neg_mean_squared_error', cv=10, verbose=3)\n","3f71aa94":"grid_search_lasso.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_lasso.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_lasso.best_score_))","0000fe3d":"grid_search_lasso.best_estimator_.score(X_train,y_train)","2d4c0efe":"tune_pipeline_tree = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',DecisionTreeRegressor(random_state = 42))])\n\ngrid_search_tree = GridSearchCV( estimator = tune_pipeline_tree, param_grid = {'selector__k':[200,276] , \n          'model__max_depth':[8,7]}, n_jobs=-1,\n                scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_squared_error', cv=10, verbose=3)\n","cb22a1a3":"grid_search_tree.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_tree.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_tree.best_score_))","0282d7bf":"grid_search_tree.best_estimator_.score(X_train,y_train)","fecebe1f":"tune_pipeline_random = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',RandomForestRegressor(random_state = 42))])\n\ngrid_search_random = GridSearchCV( estimator = tune_pipeline_random, param_grid = {'selector__k':[200,276] , \n  'model__n_estimators':np.arange(200,301,50),'model__max_depth':[15,20]}, n_jobs=-1,\n                                  scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_squared_error', cv=10, verbose=3)\n","7ebc814c":"grid_search_random.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_random.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_random.best_score_))","3831b7b0":"grid_search_random.best_estimator_.score(X_train,y_train)","94db5682":"tune_pipeline_svr = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',SVR())])\n\ngrid_search_svr = GridSearchCV( estimator = tune_pipeline_svr, param_grid = {'selector__k':[200,276] , \n  'model__kernel':['linear','rbf'],'model__C':[20,100],\n                                    'model__epsilon':[0.3,3]}, n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","45146705":"grid_search_svr.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_svr.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_svr.best_score_))","041ab599":"grid_search_svr.best_estimator_.score(X_train,y_train)","6b04d330":"tune_pipeline_gbr = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',GradientBoostingRegressor(random_state=42))])\n\ngrid_search_gbr = GridSearchCV( estimator = tune_pipeline_gbr, param_grid = {'selector__k':[276] , \n  'model__loss':['huber'],'model__max_depth':[3,5],'model__learning_rate':[0.05,0.07],'model__n_estimators':[500]}, n_jobs=-1, \n                               scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_squared_error', cv=5, verbose=3)","d95886f8":"grid_search_gbr.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_gbr.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_gbr.best_score_))","d04623d0":"grid_search_gbr.best_estimator_.score(X_train,y_train)","7a0711ec":"tune_pipeline_ridge = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',Ridge(random_state=42))])\n\ngrid_search_ridge = GridSearchCV( estimator = tune_pipeline_ridge, param_grid = {'selector__k':[276] , \n  'model__alpha':[400,500]}, n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","178b793a":"grid_search_ridge.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_ridge.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_ridge.best_score_))","b538afbb":"grid_search_ridge.best_estimator_.score(X_train,y_train)","aa61333e":"tune_pipeline_xgb = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',xgb.XGBRegressor(random_state=42))])\n\ngrid_search_xgb = GridSearchCV( estimator = tune_pipeline_xgb, param_grid = {'selector__k':[276] , \n  'model__learning_rate':[0.05,1],'model__n_estimators':[1500,1800],'model__max_depth':[3,5],'model__colsample_bytree':[0.3]},\n                               n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","8156446c":"grid_search_xgb.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_xgb.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_xgb.best_score_))","e07815f7":"grid_search_xgb.best_estimator_.score(X_train,y_train)","3bcf3f57":"tune_pipeline_lgb = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',lgb.LGBMRegressor(random_state=42,objective='regression',\n                              bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11))])\n\ngrid_search_lgb = GridSearchCV( estimator = tune_pipeline_lgb, param_grid = {'selector__k':[276] , \n  'model__learning_rate':[0.005,0.01],'model__num_iterations':[10000],'model__n_estimators':[500],\n                            'model__max_bin':[100],'model__num_leaves':[25,30]},\n                               n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","f6c136ec":"grid_search_lgb.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_lgb.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_lgb.best_score_))","50dd06c4":"grid_search_lgb.best_estimator_.score(X_train,y_train)","96e430e5":"lasso_final_model = grid_search_lasso.best_estimator_\nlasso_y_pred = lasso_final_model.predict(X_test)\nlasso_final_model.score(X_test,y_test)","554e3879":"rmse_lasso = np.sqrt(mean_squared_error(y_test,lasso_y_pred))\nmae_lasso = mean_absolute_error(y_test,lasso_y_pred)\nmedian_ae_lasso = median_absolute_error(y_test,lasso_y_pred)\nprint(rmse_lasso)\nprint(mae_lasso)\nprint(median_ae_lasso)","48355f6e":"tree_final_model = grid_search_tree.best_estimator_\ntree_y_pred = tree_final_model.predict(X_test)\ntree_final_model.score(X_test,y_test)","38cd5521":"rmse_tree = np.sqrt(mean_squared_error(y_test,tree_y_pred))\nmae_tree = mean_absolute_error(y_test,tree_y_pred)\nmedian_ae_tree = median_absolute_error(y_test,tree_y_pred)\nprint(rmse_tree)\nprint(mae_tree)\nprint(median_ae_tree)","1165771c":"random_final_model = grid_search_random.best_estimator_\nrandom_y_pred = random_final_model.predict(X_test)\nrandom_final_model.score(X_test,y_test)","b16063e6":"rmse_random = np.sqrt(mean_squared_error(y_test,random_y_pred))\nmae_random = mean_absolute_error(y_test,random_y_pred)\nmedian_ae_random = median_absolute_error(y_test,random_y_pred)\nprint(rmse_random)\nprint(mae_random)\nprint(median_ae_random)","b60a7e3b":"gbr_final_model = grid_search_gbr.best_estimator_\ngbr_y_pred = gbr_final_model.predict(X_test)\ngbr_final_model.score(X_test,y_test)","cdcd903f":"rmse_gbr = np.sqrt(mean_squared_error(y_test,gbr_y_pred))\nmae_gbr = mean_absolute_error(y_test,gbr_y_pred)\nmedian_ae_gbr = median_absolute_error(y_test,gbr_y_pred)\nprint(rmse_gbr)\nprint(mae_gbr)\nprint(median_ae_gbr)","e2217f29":"ridge_final_model = grid_search_ridge.best_estimator_\nridge_y_pred = ridge_final_model.predict(X_test)\nridge_final_model.score(X_test,y_test)","5049352b":"rmse_ridge = np.sqrt(mean_squared_error(y_test,ridge_y_pred))\nmae_ridge = mean_absolute_error(y_test,ridge_y_pred)\nmedian_ae_ridge = median_absolute_error(y_test,ridge_y_pred)\nprint(rmse_ridge)\nprint(mae_ridge)\nprint(median_ae_ridge)","84bcebb6":"xgb_final_model = grid_search_xgb.best_estimator_\nxgb_y_pred = xgb_final_model.predict(X_test)\nxgb_final_model.score(X_test,y_test)","eae92081":"rmse_xgb = np.sqrt(mean_squared_error(y_test,xgb_y_pred))\nmae_xgb = mean_absolute_error(y_test,xgb_y_pred)\nmedian_ae_xgb = median_absolute_error(y_test,xgb_y_pred)\nprint(rmse_xgb)\nprint(mae_xgb)\nprint(median_ae_xgb)","806d6a02":"lgb_final_model = grid_search_lgb.best_estimator_\nlgb_y_pred = lgb_final_model.predict(X_test)\nlgb_final_model.score(X_test,y_test)","528e7135":"rmse_lgb = np.sqrt(mean_squared_error(y_test,lgb_y_pred))\nmae_lgb = mean_absolute_error(y_test,lgb_y_pred)\nmedian_ae_lgb = median_absolute_error(y_test,lgb_y_pred)\nprint(rmse_lgb)\nprint(mae_lgb)\nprint(median_ae_lgb)","0798ce21":"remove_bad_features(test)","b6a01210":"remove_corr_features(test)","e340062b":"def imputing_missing_values(data):\n        data['LotFrontage'] = data.groupby(\"Neighborhood\")['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n        coll = data.select_dtypes('float64','int64').columns.values\n        for col in coll:\n            test[col].fillna(0,inplace=True)\n        data.replace(np.nan,'None',inplace=True)\n        return\nimputing_missing_values(test)","10134f88":"cat_test_data = test.select_dtypes(object).copy()\ndef transform_categorical(data):\n    global test\n    data = pd.get_dummies(data,columns=[col for col in cat_test_data.columns])\n    test = data.copy()\n    return \ntransform_categorical(test)","690a2f9c":"def transform_features(data):\n    columns = ['GrLivArea']\n    for col in columns:\n        data[col] = np.log(data[col].replace(0,1))\n    return\ntransform_features(test)","0abbad0e":"test.info()","c542cf4a":"# Get missing columns in the training test\nmissing_cols = set( total_columns.values ) - set( test.columns.values )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\ntest = test[total_columns.values]\n\nextra_cols =  set( test.columns.values ) - set( total_columns.values )\nfor c in extra_cols:\n    test.drop(c,axis=1 ,inplace = True)\ntest = test[total_columns.values]\n    \n","e01fc4a8":"def feature_scaling(data):\n    global test\n    data_test_array = std_scaler.transform(data)\n    data_ = pd.DataFrame(data_test_array , columns = data.columns , index = data.index)\n    test = data_.copy()\n    return\nfeature_scaling(test)","8b1c40c5":"test.drop('SalePrice',axis=1,inplace=True)\ntest.info()","529ff147":"gbr_y_test_pred = np.exp(gbr_final_model.predict(test))\nxgb_y_test_pred = np.exp(gbr_final_model.predict(test))\nlgb_y_test_pred = np.exp(lgb_final_model.predict(test))","eb543d51":"prediction = gbr_y_test_pred*0.10 + xgb_y_test_pred*0.5 + lgb_y_test_pred*0.4","e562db26":"sub = pd.DataFrame()\nsub['Id'] = test_Id\nsub['SalePrice'] = prediction\nsub.to_csv('submission.csv',index=False)","2849366e":"### 8.XGBoost Model","29069c83":"## Feature Engineering","409ccdbb":"#### Decision Tree","a4dcc662":"#### LightGBM Model","9ca36d19":"#### SVR ","8c164498":"# FINE TUNE THE MODEL \n\n","49e1fcd0":"### 3.Lasso Model","3d21e469":"### 4.Decision Tree Model","980c4e59":"## Data Cleaning","d724a6b0":"So here we have some features that have null values more than 50% [PoolQC, MiscFeature, Alley, Fence, FireplaceQu].\nthese features should be removed to avoid missleading.","949f5a0d":"#### Feature scaling","0a75a8d9":"# IMPORT","774f9b38":"###### Cross validation Score","221d264b":"# Explore the Data ","3cc6a99f":"###### Cross validation Score","3c06b4be":"#### drop correlated features \n","aa847617":"#### Random Forest Regressor","4c08715e":"#### Ridge Regression Model","2b5264ff":"# Submission","1c5bc8ec":"We tried to ensemble some models as 50% XGBoost || 40% LightGBM || 10% GradientBoostingRegressor ","3a7f8445":"###### Model Evaluation","6d5d015a":"###### Model Evaluation","56e1b24e":"#### Imputing missing values\n","a59274ff":"###### Model Evaluation","1902e416":"# Prepare the Data \n","5ae77a68":"###### Model Evaluation","bdac653c":"### 1.LinearRegression Model","20ddd27c":"###### Model Evaluation","46eb02db":"#### Decision Tree","b1f7c28c":"#### Ridge Regression Model","7aa4cb5f":"# MACHINE LEARNING MODELS ","c5a47896":"#### XGBoost Model","998835b9":"it`s more bad than Linear Regression.\nthe model has badly overfit the data.\n\nlet`s try random forest model ","b3da4853":"## Visualization & Correlation","8b139e7c":"# EVALUATE THE MODEL ON THE TEST SET\n\n","73271552":"#### Lasso","b043d0e0":"#### Transform features (Normality)","5a88a727":"#### Random Forest Regressor","f1fdf85a":"###### Model Evaluation","e43079ae":"#### Gradient Boosting Regressor","1bec09af":"###### Cross validation Score","eaaa89eb":"-------------the Random Forest Regressor is better than  SVR model-------------------\n\nlet`s try \n","a6214a55":"### 5.Random Forest Regressor Model","e8d349a0":"### Working on Test data processing","5b80a55c":"###### Cross validation Score","51a94329":"###### Cross validation Score","500c7a42":"# LOAD DATA\n","f32a5e0a":"###### Model Evaluation","aab78c11":"technique to mix feature selection and hyperparameter tuning in the same procedure, considering the feature set as a hyperparameter itself. ","17baa481":"### 7.Gradient Boosting Regressor Model","3f2daa88":"### 2.Ridge Regression Model","22ef141e":"###### Model Evaluation","ef52c63f":"### Prediction","b040301f":"## EDA summary:\n1. the data should be scaled (Standard Scaler)\n2. remove PoolQC, MiscFeature, Alley, Fence, FireplaceQu and fill missing values\n3. feature selection\n4. outlier removal \n5. Gaussian transformation\n\n","ab686776":"From evaluations we have noticed that the most promising models are GradientBoostingRegressor , XGBoost & LightGBM model","81a22596":"## Processing Summary:\n* data cleaning:\n    * remove_bad_features()\n    * remove_outlier()\n    * remove_corr_features()\n    * imputing_missing_values()\n* feature engineering:\n    * transform_categorical()                                                          *** need edit***\n    * transform_features()              > for *** SalePrice & GrLivArea ***   \n    * feature_scaling()                 > for all except SalePrice(target variable)    *** need edit***\n","8ae45ca6":"#### Removing uneffective features\n","822a55ef":"###### Cross validation Score","ab684614":"#### Lasso","44eb536d":"###### Cross validation Score","cba2552c":"The log transformation do well with only the 1st and 2nd features (SalePrice & GrLivArea)","a97618a2":"the model seems it has no error!!!!!!\nI think it has badly overfitting.\nSo we have to evaluate it using cross validation test.","c74e1d68":"#### Gradient Boosting Regressor","8e39ed49":"### 9.LightGBM  Model","2d73c20b":"###### Model Evaluation","2a91b30c":"# SPLITTING DATA ","df476dd1":"#### LightGBM Model","0f05617d":"#### Handling Categorical attributes","bd167a8a":"I think there are 2 or 3 outlier points should be removed","2376cef8":"### 6.SVR Model","fd4f2af0":"# Final Model for Test set","8a28d629":"#### XGBoost Model","5e31ae37":"###### Cross validation Score","4d0988f7":"###### Cross validation Score","52d10284":"#### Removing Outliers\n"}}