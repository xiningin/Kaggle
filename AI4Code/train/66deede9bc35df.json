{"cell_type":{"d6f3ce2e":"code","c7dfabd3":"code","0291b0d8":"code","870e885b":"code","f26bd50d":"code","d08eca2f":"code","8b106fdd":"code","1d63373f":"code","2c691a6e":"code","df780ee7":"code","9cea7748":"code","e0c616bb":"code","4792f84d":"code","96a920ba":"code","7c972828":"code","933753bf":"code","f30f47d1":"code","d54235b9":"code","8b9ed420":"code","0831754e":"code","d0a717ae":"code","96d5d0d3":"code","c2cd1068":"markdown","f13f8839":"markdown","64ceb905":"markdown","9f9d8770":"markdown","42446fba":"markdown","dee77f94":"markdown","8a2f4bdc":"markdown","745a1b5e":"markdown","da635a94":"markdown","846c1467":"markdown","a41aa479":"markdown","d00abb91":"markdown","8cca3fd5":"markdown","9a8e90e7":"markdown","51533961":"markdown"},"source":{"d6f3ce2e":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","c7dfabd3":"def plot_accs(values):\n    plt.plot(range(len(values)), values, '-ko')\n    sns.despine(offset=15)\n    \ndef train_with_range(train_function, X_train, X_test, y_train, y_test):\n    accs = []\n    for value in range(2,25):\n        accs.append(train_function(value, X_train, X_test, y_train, y_test))\n    return accs","0291b0d8":"breast_cancer = load_breast_cancer()\ndata = pd.DataFrame(breast_cancer.data,columns=breast_cancer.feature_names)\ndata['target'] = pd.Series(breast_cancer.target)\ndata.head()","870e885b":"data.info()","f26bd50d":"data.describe()","d08eca2f":"col = data.columns       # .columns gives columns names in data \nprint(col)","8b106fdd":"ax = sns.countplot(data.target,label=\"Count\")\nB, M = data.target.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","1d63373f":"featureMeans = list(data.columns[:-1])\nfeatureMeans","2c691a6e":"X = data.loc[:,featureMeans]\ny = data.loc[:, 'target']","df780ee7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.95, random_state = 42)","9cea7748":"def train_logistic(X_train, X_test, y_train, y_test):\n    model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=2000)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print('Acur\u00e1cia:',acc )\n    return acc","e0c616bb":"def use_PCA(pca_value,X_train,X_test):\n    pca = PCA(pca_value)\n    X_train_pca = pca.fit_transform(X_train)\n    X_test_pca = pca.transform(X_test)    \n    return X_train_pca, X_test_pca","4792f84d":"def train_logistic_with_pca(pca_value, X_train, X_test, y_train, y_test):\n    X_train_pca, X_test_pca = use_PCA(pca_value,X_train,X_test)\n    return train_logistic(X_train_pca, X_test_pca, y_train, y_test)","96a920ba":"accs = train_with_range(train_logistic_with_pca, X_train, X_test, y_train, y_test)","7c972828":"plot_accs(accs)","933753bf":"def use_KBest(kbest_value, X_train, X_test, y_train, y_test):\n    fvalue_selector = SelectKBest(f_classif, k=kbest_value)\n    X_train_kbest = fvalue_selector.fit_transform(X_train, y_train)\n    X_test_kbest = fvalue_selector.transform(X_test)    \n    return X_train_kbest, X_test_kbest","f30f47d1":"def train_logistic_with_kbest(kbest_value, X_train, X_test, y_train, y_test):\n    X_train_kbest, X_test_kbest = use_KBest(kbest_value, X_train, X_test, y_train, y_test)\n    return train_logistic(X_train_kbest, X_test_kbest, y_train, y_test)","d54235b9":"accs = train_with_range(train_logistic_with_kbest, X_train, X_test, y_train, y_test)","8b9ed420":"plot_accs(accs)","0831754e":"def train_ensemble_models(X, y):\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=10, random_state=1)\n    clf3 = GaussianNB()\n    eclf_x2 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='hard')\n    eclf_x3 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb',clf3)], voting='hard')\n\n    for clf, label in zip([clf1, clf2, clf3, eclf_x2, eclf_x3], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble X2', 'Ensemble X3']):\n        execute_pipeline(clf, X, y, label)\n    ","d0a717ae":"def execute_pipeline(clf, X, y, title):\n\n    pipe = Pipeline([\n        ('reduce_dim', 'passthrough'),\n        ('classify', clf)\n    ])\n\n    N_FEATURES_OPTIONS = [2, 4, 10, 20]\n    \n    param_grid = [\n        {\n            'reduce_dim': [PCA()],\n            'reduce_dim__n_components': N_FEATURES_OPTIONS\n        },\n        {\n            'reduce_dim': [SelectKBest()],\n            'reduce_dim__k': N_FEATURES_OPTIONS\n        },\n    ]\n    reducer_labels = ['PCA', 'KBest']\n\n    grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid,return_train_score=True)\n    grid.fit(X, y)\n\n    mean_train_scores = np.array(grid.cv_results_['mean_train_score'])\n    mean_scores = np.array(grid.cv_results_['mean_test_score'])\n    mean_scores = mean_scores.reshape(2, len(N_FEATURES_OPTIONS))\n    bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + .5)\n\n    plt.figure()\n    COLORS = 'bgrcmyk'\n    for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n        plt.bar(bar_offsets + i, mean_train_scores[i], label='{} train'.format(label),alpha=.7)\n        plt.bar(bar_offsets + i, reducer_scores, label='{} test'.format(label), color=COLORS[i])\n\n    plt.title(title)\n    plt.xlabel('Number of features')\n    plt.xticks(bar_offsets + len(reducer_labels) \/ 2, N_FEATURES_OPTIONS)\n    plt.ylabel('Classification accuracy')\n    plt.ylim((0.87, 1))\n    plt.legend(loc='upper left')\n\n    plt.show()","96d5d0d3":"train_ensemble_models(X, y)","c2cd1068":"## Conclusion","f13f8839":"## Data Analysis","64ceb905":"## Data Visualization","9f9d8770":"## Clean Dataset","42446fba":"### Dimensionality Reduction","dee77f94":"## Models","8a2f4bdc":"### Ensemble Models","745a1b5e":"This database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29","da635a94":"### Feature Selection","846c1467":"* mean\n* standard error \n* worst (mean of the three largest values) \n\nOf these features were computed for each image, resulting in 30 features. \n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none","a41aa479":"## Feature Engineering Notebook","d00abb91":"# Breast Cancer Dataset","8cca3fd5":"## imports","9a8e90e7":"## Read dataset","51533961":"## Data Preprocessing"}}