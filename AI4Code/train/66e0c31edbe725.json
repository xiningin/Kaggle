{"cell_type":{"aa388394":"code","34d7c31d":"code","03e662ed":"code","72537b29":"code","1275d466":"code","19322c08":"code","06be7f4e":"code","2fed0370":"code","8aaa3e92":"code","a17402a0":"code","c229e135":"code","f0cbd5aa":"code","49d54e4a":"code","9fac8524":"code","d7544b93":"code","1c255f19":"code","2bae9821":"code","91159593":"code","e22913b5":"code","a6f754ed":"code","0b74c111":"code","f972fd52":"code","7e7f2bd3":"code","150e0f7d":"code","e782d44b":"code","ddd83922":"code","c0252414":"code","78005df8":"code","1c7f91b4":"code","38971552":"code","77aff2cb":"code","4d2d9ce2":"code","a24ee21c":"code","5e566b2a":"code","b7658a8b":"code","875aeb7d":"code","7792efc0":"code","e8ca8d58":"code","3fac7bb3":"code","4340c9b0":"code","9ce11cb6":"code","f82db469":"code","49ac2903":"code","f5e93a45":"code","de3c2006":"code","13a00a65":"code","5ea0ffac":"code","0ec01ab3":"code","5c76f078":"markdown","db9e743f":"markdown","6e10cdf5":"markdown","02a199f4":"markdown","c1db3a4e":"markdown","dbbbcdcf":"markdown","d36a9826":"markdown","9f26e056":"markdown","d0abcd36":"markdown","80f520dd":"markdown","985c5da1":"markdown","703a5328":"markdown","8c62982d":"markdown","8e938c3e":"markdown","18ff1403":"markdown","11a9d09e":"markdown","081ca970":"markdown","05749c5b":"markdown","a430e435":"markdown","00d8066a":"markdown","1d014ff3":"markdown","b230c955":"markdown","37cc8f26":"markdown","a7eeed0d":"markdown","0351f5a6":"markdown","59e8736e":"markdown","1c2d8925":"markdown","61c6f38d":"markdown","f8c65e48":"markdown","a280a073":"markdown","e2773780":"markdown","e5a67f73":"markdown","9a51b293":"markdown","d2b8bad3":"markdown","0d2e654a":"markdown","acdef982":"markdown","d8466830":"markdown","1104115a":"markdown","e506e832":"markdown","634cbe69":"markdown"},"source":{"aa388394":"import numpy as np \nimport pandas as pd \nimport re\nimport nltk \nimport matplotlib.pyplot as plt\n%matplotlib inline","34d7c31d":"data_source_url = r\"C:\\Users\\ASUS\\Desktop\\ai and ml\\data\\Tweets.csv\"\nairline_tweets = pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")","03e662ed":"airline_tweets.head()","72537b29":"airline_tweets.shape","1275d466":"airline_tweets.airline_sentiment.value_counts()","19322c08":"plot_size = plt.rcParams[\"figure.figsize\"] \nprint(plot_size[0]) \nprint(plot_size[1])\n\nplot_size[0] = 8\nplot_size[1] = 6\nplt.rcParams[\"figure.figsize\"] = plot_size ","06be7f4e":"airline_tweets.airline.value_counts().plot(kind='pie', autopct='%1.0f%%')","2fed0370":"airline_tweets.airline_sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\", \"yellow\", \"green\"])","8aaa3e92":"airline_sentiment = airline_tweets.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack()\nairline_sentiment.plot(kind='bar')","a17402a0":"import seaborn as sns\n\nsns.barplot(x='airline_sentiment', y='airline_sentiment_confidence' , data=airline_tweets)","c229e135":"def plot_sub_sentiment(Airline):\n    pdf = airline_tweets[airline_tweets['airline']==Airline]\n    count = pdf['airline_sentiment'].value_counts()\n    Index = [1,2,3]\n    color=sns.color_palette(\"husl\", 10)\n    plt.bar(Index,count,width=0.5,color=color)\n    plt.xticks(Index,['Negative','Neutral','Positive'])\n    plt.title('Sentiment Summary of' + \" \" + Airline)\n\nairline_name = airline_tweets['airline'].unique()\nplt.figure(1,figsize=(12,12))\nfor i in range(6):\n    plt.subplot(3,2,i+1)\n    plot_sub_sentiment(airline_name[i])","f0cbd5aa":"#counting the total number of negative reasons\nairline_tweets.negativereason.value_counts()","49d54e4a":"#Plotting all the negative reasons \ncolor=sns.color_palette(\"husl\", 10)\npd.Series(airline_tweets[\"negativereason\"]).value_counts().plot(kind = \"bar\",\n                        color=color,figsize=(8,6),title = \"Total Negative Reasons\")\nplt.xlabel('Negative Reasons', fontsize=10)\nplt.ylabel('No. of Tweets', fontsize=10)","9fac8524":"from wordcloud import WordCloud,STOPWORDS","d7544b93":"airline_tweets=airline_tweets [airline_tweets ['airline_sentiment']=='negative']\nwords = ' '.join(airline_tweets ['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])","1c255f19":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)","2bae9821":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","91159593":"airline_tweets=airline_tweets [airline_tweets ['airline_sentiment']=='positive']\nwords = ' '.join(airline_tweets ['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])","e22913b5":"wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)","a6f754ed":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","0b74c111":"# Calculate highest frequency words in positive tweets\ndef freq(str): \n  \n    # break the string into list of words  \n    str = str.split()          \n    str2 = [] \n  \n    # loop till string values present in list str \n    for i in str:              \n  \n        # checking for the duplicacy \n        if i not in str2: \n  \n            # insert value in str2 \n            str2.append(i)  \n              \n    for i in range(0, len(str2)): \n        if(str.count(str2[i])>50): \n            print('Frequency of', str2[i], 'is :', str.count(str2[i]))\n        \nprint(freq(cleaned_word))","f972fd52":"air_senti=pd.crosstab(airline_tweets.airline, airline_tweets.airline_sentiment)\nair_senti","7e7f2bd3":"percent=air_senti.apply(lambda a: a \/ a.sum() * 100, axis=1)\npercent","150e0f7d":"pd.crosstab(index = airline_tweets[\"airline\"],columns = airline_tweets[\"airline_sentiment\"]).plot(kind='bar',\n                figsize=(10, 6),alpha=0.5,rot=0,stacked=True,title=\"Airline Sentiment\")","e782d44b":"airline_tweets['tweet_created'] = pd.to_datetime(airline_tweets['tweet_created'])\nairline_tweets[\"date_created\"] = airline_tweets[\"tweet_created\"].dt.date","ddd83922":"airline_tweets[\"date_created\"]","c0252414":"df = airline_tweets.groupby(['date_created','airline'])\ndf = df.airline_sentiment.value_counts()\ndf.unstack()","78005df8":"features = airline_tweets.iloc[:, 10].values\nlabels = airline_tweets.iloc[:, 1].values","1c7f91b4":"features","38971552":"labels","77aff2cb":"processed_features = []\n\nfor sentence in range(0, len(features)):\n    # Remove all the special characters\n    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n\n    # remove all single characters\n    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n\n    # Remove single characters from the start\n    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n\n    # Substituting multiple spaces with single space\n    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n\n    # Removing prefixed 'b'\n    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n\n    # Converting to Lowercase\n    processed_feature = processed_feature.lower()\n\n    processed_features.append(processed_feature)","4d2d9ce2":"processed_features","a24ee21c":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\nprocessed_features = vectorizer.fit_transform(processed_features).toarray()","5e566b2a":"processed_features","b7658a8b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)","875aeb7d":"from sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\ntext_classifier.fit(X_train, y_train)","7792efc0":"predictions = text_classifier.predict(X_test)","e8ca8d58":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint(accuracy_score(y_test, predictions))","3fac7bb3":"from sklearn.neighbors import KNeighborsClassifier\ntext_classifier2 = KNeighborsClassifier(n_neighbors = 5)#no of neighbors is hpyer parameter\ntext_classifier2.fit(X_train, y_train)","4340c9b0":"predictions2 = text_classifier2.predict(X_test)","9ce11cb6":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions2))\nprint(classification_report(y_test,predictions2))\nprint(accuracy_score(y_test, predictions2))","f82db469":"from sklearn.linear_model import LogisticRegression\nmodel =LogisticRegression()","49ac2903":"model.fit(X_train, y_train)","f5e93a45":"predictions3 = model.predict(X_test)","de3c2006":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions3))\nprint(classification_report(y_test,predictions3))\nprint(accuracy_score(y_test, predictions3))","13a00a65":"from sklearn.tree import DecisionTreeClassifier\nmodel3= DecisionTreeClassifier(criterion=\"gini\")\n#here we are facing the problem of overfitting\n#train the model\nmodel3.fit(X_train, y_train)","5ea0ffac":"predictions4 = model3.predict(X_test)","0ec01ab3":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions4))\nprint(classification_report(y_test,predictions4))\nprint(accuracy_score(y_test, predictions4))","5c76f078":"To evaluate the performance of the machine learning models, we can use classification metrics such as a confusion metrix, F1 measure, accuracy, etc.","db9e743f":"# Making Predictions and Evaluating the Model","6e10cdf5":"In the code above, we define that the max_features should be 2500, which means that it only uses the 2500 most frequently occurring words to create a bag of words feature vector. Words that occur less frequently are not very useful for classification.\n\nSimilarly, max_df specifies that only use those words that occur in a maximum of 80% of the documents. Words that occur in all documents are too common and are not very useful for classification. Similarly, min-df is set to 7 which shows that include words that occur in at least 7 documents.","02a199f4":"**Please upvote me if you found valuable**","c1db3a4e":"# We can find that the Tweets with negative moods are frequently involved some words like cancelled, flight ,customer or hour. People might guess that customer tends to complain when they are waiting for the delayed flights.","dbbbcdcf":"Given tweets about six US airlines, the task is to predict whether a tweet contains positive, negative, or neutral sentiment about the airline. This is a typical supervised learning task where given a text string, we have to categorize the text string into predefined categories.","d36a9826":"# TF-IDF","9f26e056":"# Data Analysis","d0abcd36":"n the previous section, we converted the data into the numeric form. As the last step before we train our algorithms, we need to divide our data into training and testing sets. The training set will be used to train the algorithm while the test set will be used to evaluate the performance of the machine learning model.","80f520dd":"# Representing Text in Numeric Form","985c5da1":"# Word Cloud for the negative Tweets","703a5328":"# Importing the Required Libraries","8c62982d":"In the bag of words approach, each word has the same weight. The idea behind the TF-IDF approach is that the words that occur less in all the documents and more in individual document contribute more towards classification.\n\nTF-IDF is a combination of two terms. Term frequency and Inverse Document frequency. They can be calculated as:","8e938c3e":"Once data is split into training and test set, machine learning algorithms can be used to learn from the training data. \nThe sklearn.ensemble module contains the RandomForestClassifier class that can be used to train the machine learning model using the random forest algorithm. To do so, we need to call the fit method on the RandomForestClassifier class and pass it our training features and labels, as parameters.","18ff1403":"The code for getting positive sentiments is completely same with the one for negative sentiments. Just replace negative with positive in the first line. Easy, right!","11a9d09e":"Enough of the exploratory data analysis, our next step is to perform some preprocessing on the data and then convert the numeric data into text data as shown below.","081ca970":"Tweets contain many slang words and punctuation marks. We need to clean our tweets before they can be used for training the machine learning model. However, before cleaning the tweets, let's divide our dataset into feature and label sets.\n\nOur feature set will consist of tweets only. If we look at our dataset, the 11th column contains the tweet text. Note that the index of the column will be 10 since pandas columns follow zero-based indexing scheme where the first column is called 0th column. Our label set will consist of the sentiment of the tweet that we have to predict. The sentiment of the tweet is in the second column (index 1). To create a feature and a label set, we can use the iloc method off the pandas data frame.","05749c5b":"To make statistical algorithms work with text, we first have to convert text to numbers. To do so, three main approaches exist i.e. Bag of Words, TF-IDF and Word2Vec. ","a430e435":"TF  = (Frequency of a word in the document)\/(Total words in the document)\n\nIDF = Log((Total number of docs)\/(Number of docs containing the word))","00d8066a":"# Decision Tree Algorithim","1d014ff3":"# Training the Model","b230c955":"To solve this problem, we will follow the typical machine learning pipeline. We will first import the required libraries and the dataset. We will then do exploratory data analysis to see if we can find any trends in the dataset. Next, we will perform text preprocessing to convert textual data to numeric data that can be used by a machine learning algorithm. Finally, we will use machine learning algorithms to train and test our sentiment analysis models.","37cc8f26":"# Problem Definition","a7eeed0d":"let's use the Seaborn library to view the average confidence level for the tweets belonging to three sentiment categories. ","0351f5a6":"Let's now see the distribution of sentiments across all the tweets. ","59e8736e":"# Dividing Data into Training and Test Sets","1c2d8925":"# Wordcloud for positive reasons","61c6f38d":"# Logistic Regression","f8c65e48":"From the output, you can see that the confidence level for negative tweets is higher compared to positive and neutral tweets.","a280a073":"# KNN ALGO","e2773780":"The first step as always is to import the required libraries:","e5a67f73":"**American,US Airways , United have more negative tweets**","9a51b293":"The last step is to make predictions on the model. To do so, we need to call the predict method on the object of the RandomForestClassifier class that we used for training.","d2b8bad3":"From the output, you can see that the majority of the tweets are negative (63%), followed by neutral tweets (21%), and then the positive tweets (16%).\n\nNext, let's see the distribution of sentiment for each individual airline,","0d2e654a":"Let's explore the dataset a bit to see if we can find any trends. ","acdef982":"In the output, you can see the percentage of public tweets for each airline. United Airline has the highest number of tweets i.e. 26%, followed by US Airways (20%).\n\n","d8466830":"Once we divide the data into features and training set, we can preprocess data in order to clean it. To do so, we will use regular expressions. ","1104115a":"It is evident from the output that for almost all the airlines, the majority of the tweets are negative, followed by neutral and positive tweets. Virgin America is probably the only airline where the ratio of the three sentiments is somewhat similar.","e506e832":"# Solution","634cbe69":"# Data Cleaning"}}