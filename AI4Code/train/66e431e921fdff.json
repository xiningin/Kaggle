{"cell_type":{"bb4aa8fc":"code","096b789b":"code","beca7e09":"code","c6855e2a":"code","4d46eb11":"code","9b95a92d":"code","a14178d3":"code","03a9cd79":"code","7fc7f634":"code","8bc34057":"code","53db594e":"code","67223e6a":"code","fd1eb723":"code","9ec7a53d":"code","659b6e67":"code","6a95c6af":"code","2d0cab92":"code","223cf867":"code","76f2e1e3":"code","90a4bacd":"code","f38b81c2":"code","4f2d1cda":"code","602a42b3":"code","70f66b32":"code","9bff5db0":"code","669505cb":"code","599348b8":"code","a0197491":"code","2f2e1c33":"code","f0ca0efe":"code","1aba3109":"code","4a9e24e8":"code","bd043135":"code","923b88e3":"code","e2fb3456":"code","f3f3e632":"code","17347169":"code","d4837a9d":"markdown","3e0d0576":"markdown","c0c6197a":"markdown","53a9207e":"markdown","1335cb25":"markdown","13990058":"markdown","799a3a2a":"markdown"},"source":{"bb4aa8fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","096b789b":"import pandas as pd\ntest = pd.read_csv(\"..\/input\/advertsuccess\/Test.csv\")\ntrain = pd.read_csv(\"..\/input\/advertsuccess\/Train.csv\")","beca7e09":"# check for null values in dataset\ntrain.isnull().sum()\n# no null values so no column needs to be ommitted","c6855e2a":"train.head()","4d46eb11":"le1 = preprocessing.LabelEncoder()\nle1.fit(train['realtionship_status'])\nlist(le1.classes_)\ntrain['realtionship_status'] = le1.transform(train['realtionship_status'])\ntrain.head()","9b95a92d":"le2 = preprocessing.LabelEncoder()\nle2.fit(train['industry'])\nlist(le2.classes_)\ntrain['industry'] = le2.transform(train['industry']) \ntrain.head()","a14178d3":"le3 = preprocessing.LabelEncoder()\nle3.fit(train['genre'])\nlist(le3.classes_)\ntrain['genre'] = le3.transform(train['genre']) \ntrain.head()","03a9cd79":"le4 = preprocessing.LabelEncoder()\nle4.fit(train['targeted_sex'])\nlist(le4.classes_)\ntrain['targeted_sex'] = le4.transform(train['targeted_sex']) \ntrain.head()","7fc7f634":"le5 = preprocessing.LabelEncoder()\nle5.fit(train['airtime'])\nlist(le5.classes_)\ntrain['airtime'] = le5.transform(train['airtime']) \ntrain.head()","8bc34057":"le6 = preprocessing.LabelEncoder()\nle6.fit(train['airlocation'])\nlist(le6.classes_)\ntrain['airlocation'] = le6.transform(train['airlocation']) \ntrain.head()","53db594e":"le7 = preprocessing.LabelEncoder()\nle7.fit(train['expensive'])\nlist(le7.classes_)\ntrain['expensive'] = le7.transform(train['expensive'])\ntrain.head()","67223e6a":"le8 = preprocessing.LabelEncoder()\nle8.fit(train['money_back_guarantee'])\nlist(le8.classes_)\ntrain['money_back_guarantee'] = le8.transform(train['money_back_guarantee'])\ntrain.head()","fd1eb723":"train.head()","9ec7a53d":"le9 = preprocessing.LabelEncoder()\nle9.fit(train['netgain'])\nlist(le9.classes_)\ntrain['netgain'] = le9.transform(train['netgain'])\ntrain.head()","659b6e67":"#Considering all available features for decision tree classifier\nfeatures = ['realtionship_status','industry','genre','targeted_sex','average_runtime(minutes_per_week)','airtime','airlocation','ratings','expensive','money_back_guarantee']\nX = train[features]\ny = train['netgain']","6a95c6af":"# Decision tree classifier and model evaluation using kFold cross validation\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\n\nkfold_mae_train=0\nkfold_mae_test=0\nkfold_f_imp_dic = 0\n\nno_of_folds = 5\n\nkf = KFold(no_of_folds,True,1)\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n    \n    dt_classifier = DecisionTreeClassifier(random_state=1)\n    dt_classifier.fit(X_train,y_train)\n    \n    mae_train = mean_absolute_error(dt_classifier.predict(X_train),y_train)\n    kfold_mae_train=(kfold_mae_train+mae_train)\n    \n    mae_test = mean_absolute_error(dt_classifier.predict(X_test),y_test)\n    kfold_dt_mae_test = (kfold_mae_test+mae_test)\n    \n    kfold_f_imp_dic = kfold_f_imp_dic + dt_classifier.feature_importances_\n    \nprint('Decision Tree Regressor train set mean absolute error =',kfold_mae_train\/no_of_folds)\nprint('Decision Tree Regressor test set mean absolute error  =',kfold_dt_mae_test\/no_of_folds)","2d0cab92":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nf_importance_dic = dict(zip(features,kfold_f_imp_dic\/no_of_folds))\ndf_imp_features = pd.DataFrame(list(f_importance_dic.items()),columns=['feature','score'])\n\nplt.figure(figsize=(30,10))\nplt.bar(df_imp_features['feature'], df_imp_features['score'],color='green',align='center', alpha=0.5)\nplt.xlabel('Mobile features', fontsize=20)\nplt.ylabel('Relative feature score',fontsize=20)\nplt.title('Relative Feature importance in determining price',fontsize=30)","223cf867":"df = pd.DataFrame(test)","76f2e1e3":"le1 = preprocessing.LabelEncoder()\nle1.fit(test['realtionship_status'])\nlist(le1.classes_)\ntest['realtionship_status'] = le1.transform(test['realtionship_status'])\n# df['realtionship_status'] = list(le.inverse_transform())\ntest.head()","90a4bacd":"le2 = preprocessing.LabelEncoder()\nle2.fit(test['industry'])\nlist(le2.classes_)\ntest['industry'] = le2.transform(test['industry'])\ntest.head()","f38b81c2":"le3 = preprocessing.LabelEncoder()\nle3.fit(test['genre'])\nlist(le3.classes_)\ntest['genre'] = le3.transform(test['genre'])\ntest.head()","4f2d1cda":"le4 = preprocessing.LabelEncoder()\nle4.fit(test['targeted_sex'])\nlist(le4.classes_)\ntest['targeted_sex'] = le4.transform(test['targeted_sex'])\ntest.head()","602a42b3":"le5.fit(test['airtime'])\nlist(le5.classes_)\ntest['airtime'] = le5.transform(test['airtime'])\ntest.head()","70f66b32":"le6.fit(test['airlocation'])\nlist(le6.classes_)\ntest['airlocation'] = le6.transform(test['airlocation'])\ntest.head()","9bff5db0":"le7.fit(test['expensive'])\nlist(le7.classes_)\ntest['expensive'] = le7.transform(test['expensive'])\ntest.head()","669505cb":"le8.fit(test['money_back_guarantee'])\nlist(le8.classes_)\ntest['money_back_guarantee'] = le8.transform(test['money_back_guarantee'])\ntest.head()","599348b8":"#Prediction on test data\ntest['netgain'] = dt_classifier.predict(test[features])\ntest.head(5)","a0197491":"test['netgain'] = le9.inverse_transform(test['netgain'])\ntest.head(5)","2f2e1c33":"test['money_back_guarantee'] = le8.inverse_transform(test['money_back_guarantee'])\ntest.head(5)","f0ca0efe":"test['expensive'] = le7.inverse_transform(test['expensive'])\ntest.head(5)","1aba3109":"test['airlocation'] = le6.inverse_transform(test['airlocation'])\ntest.head(5)","4a9e24e8":"test['airtime'] = le5.inverse_transform(test['airtime'])\ntest.head(5)","bd043135":"test['targeted_sex'] = le4.inverse_transform(test['targeted_sex'])\ntest.head(5)","923b88e3":"test['genre'] = le3.inverse_transform(test['genre'])\ntest.head(5)","e2fb3456":"test['industry'] = le2.inverse_transform(test['industry'])\ntest.head(5)","f3f3e632":"test['realtionship_status'] = le1.inverse_transform(test['realtionship_status'])\ntest.head(5)","17347169":"test.head(40)","d4837a9d":"**We start reverse labeling the categorical values.**","3e0d0576":"**It is not recommended to use the same label encoder for all the features in the data set. It is safe to create a label encoder for each column because each feature varies in terms of the values.**","c0c6197a":"**Now, we implement the decision tree method for prediction.**","53a9207e":"**We encode each of the categorical columns using label encoding.**","1335cb25":"**Now we perform the same label encoding operation on the test dataset.**","13990058":"**Now, all of our categorical features are transformed into numerical values using label encoding**","799a3a2a":"**The final test dataset is as followed:**"}}