{"cell_type":{"811ad641":"code","c519a5dc":"code","c232cf6f":"code","92ea03cf":"code","df668017":"code","0fb383cd":"code","d4e27f74":"code","e07f6d51":"code","dd143840":"code","e26a1207":"code","d97b17e3":"code","402a918a":"code","51efe44f":"code","7436bef8":"code","bbe6da68":"code","6b08ef6a":"code","d68b0df2":"code","5c18311a":"code","b0e7d592":"code","2c96c75f":"code","fbb41b6a":"code","f7408387":"code","2155062f":"code","98c7c613":"code","ff8a482e":"code","e1525f38":"code","46f97d9b":"code","1d9199c2":"code","ae91aeec":"code","77b2838f":"code","843766e4":"code","7a2b464d":"code","12e8654c":"code","e0de4969":"code","e38f7cbd":"code","ca92feb9":"code","60406bff":"code","9839bb60":"code","5b1e2d78":"code","4aa0f41c":"code","de57aae2":"code","38ecb7ff":"code","68a8b6ec":"code","74eeb95a":"code","04398510":"code","6a20d505":"markdown","b1fb07a9":"markdown","794abd40":"markdown","00746142":"markdown","0c1d8bd6":"markdown","6b8220c6":"markdown","6b7cabef":"markdown"},"source":{"811ad641":"import seaborn as sns\nimport pandas as pd","c519a5dc":"import warnings\nwarnings.filterwarnings(\"ignore\")","c232cf6f":"df = pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.head()","92ea03cf":"df.shape","df668017":"df.dtypes","0fb383cd":"df.nunique()","d4e27f74":"df.Species.value_counts()","e07f6d51":"df[\"Species\"] = df[\"Species\"].astype(\"category\")","dd143840":"df.dtypes","e26a1207":"df.isnull().sum()","d97b17e3":"#nombre de fleurs pour chaque esp\u00e8ce\nsns.countplot(df.Species)","402a918a":"#Afficher la variable sepal_length selon l'esp\u00e8ce du fleur (species)\nsns.scatterplot(y=df.SepalLengthCm,x=range(len(df)),hue='Species',data=df)","51efe44f":"sns.displot(x='SepalLengthCm',  kind='kde',data=df,hue='Species')","7436bef8":"#Afficher la variable sepal_width selon l'esp\u00e8ce du fleur (species)\nsns.scatterplot(y=df.SepalWidthCm,x=range(len(df)),hue='Species',data=df)","bbe6da68":"sns.displot(x='SepalWidthCm',  kind='kde',data=df,hue='Species')","6b08ef6a":"#Afficher la variable petal_length selon l'esp\u00e8ce du fleur (species)\nsns.scatterplot(y=df.PetalLengthCm,x=range(len(df)),hue='Species',data=df)","d68b0df2":"sns.displot(x='PetalLengthCm',  kind='kde',data=df,hue='Species')","5c18311a":"#Afficher la variable petal_length selon l'esp\u00e8ce du fleur (species)\nsns.scatterplot(y=df.PetalWidthCm,x=range(len(df)),hue='Species',data=df)","b0e7d592":"sns.displot(x='PetalWidthCm',  kind='kde',data=df,hue='Species')","2c96c75f":"from sklearn.model_selection import train_test_split\nY=df.Species\nX=df.drop(['Species','Id'], axis=1)\n# Split train \/ test data :\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)","fbb41b6a":"from sklearn import tree\ntree_model = tree.DecisionTreeClassifier()\n#tree_model = tree.DecisionTreeClassifier(max_depth = 2)\ntree_model=tree_model.fit(X_train, Y_train)","f7408387":"Y_train.value_counts()","2155062f":"import matplotlib.pyplot as plt\nplt.figure(figsize=(15,10))\nnames = ['Setosa', 'Versicolor', 'Virginica']\ntree.plot_tree(tree_model,feature_names = X.columns, \n               class_names=names,\n               filled = True)","98c7c613":"Y_predict=tree_model.predict(X_test)","ff8a482e":"# Plot the Confusion Matrix :\nfrom sklearn.metrics import accuracy_score, confusion_matrix \nmat = confusion_matrix(Y_predict, Y_test)\nprint(mat)","e1525f38":"#plt.figure(figsize=(15,5))\nsns.heatmap(mat, annot=True,  xticklabels=names, yticklabels=names)\nplt.xlabel('Test')\nplt.ylabel('Predicted')","46f97d9b":"# Accuracy:\na_CART = accuracy_score(Y_test,Y_predict)\nprint(\"L'accuracy score du mod\u00e8le CART est de : \",a_CART)","1d9199c2":"(16+17+8)\/(16+18+11)","ae91aeec":"from sklearn.model_selection import GridSearchCV\nimport numpy as np","77b2838f":"np.arange(1, 21,2)","843766e4":"num_leafs = [2,4,6,8,10] # nombre minimum d'observation pour chaque split\ndepths = np.arange(1, 21,2) # The maximum depth of the tree\nnum_features = np.arange(1,X.shape[1]) # candidates for split \n\n\nparam_grid = [{'max_depth':depths,  'min_samples_leaf':num_leafs, 'max_features':num_features}]","7a2b464d":"grid_tree= GridSearchCV(estimator=tree.DecisionTreeClassifier(),param_grid=param_grid,scoring='accuracy',cv=10)\ngrid_tree.fit(X_train, Y_train)\nbest_model_tree = grid_tree.best_estimator_","12e8654c":"Y_grid=best_model_tree.predict(X_test)\n\n# Accuracy:\naccuracy_score(Y_test, Y_grid)","e0de4969":"grid_tree.best_params_","e38f7cbd":"from sklearn.ensemble import RandomForestClassifier","ca92feb9":"Rf_model = RandomForestClassifier()\nRf_model=Rf_model.fit(X_train, Y_train)\n","60406bff":"Y_predict=Rf_model.predict(X_test)","9839bb60":"# Accuracy:\na_CART = accuracy_score(Y_test,Y_predict)\nprint(\"L'accuracy score du mod\u00e8le RF est de : \",a_CART)","5b1e2d78":"mat = confusion_matrix(Y_predict, Y_test)\nsns.heatmap(mat, annot=True,  xticklabels=names, yticklabels=names)\nplt.xlabel('Test')\nplt.ylabel('Predicted')","4aa0f41c":"from sklearn.neighbors import KNeighborsClassifier","de57aae2":"clf = KNeighborsClassifier(n_neighbors=1)\nclf.fit(X_train, Y_train)\ny_pred=clf.predict(X_test)","38ecb7ff":"accuracy_score(Y_test,y_pred)","68a8b6ec":"parameters = {'n_neighbors':range(1,20)}\nknn=KNeighborsClassifier()\n#Fit the model\nmodel_knn = GridSearchCV(knn, param_grid=parameters)\nmodel_knn.fit(X_train,Y_train)","74eeb95a":"best_model_knn = model_knn.best_estimator_\nY_grid_Knn=best_model_knn.predict(X_test)\n\n# Accuracy:\naccuracy_score(Y_test, Y_grid_Knn)","04398510":"model_knn.best_params_","6a20d505":"## Evaluation","b1fb07a9":"## For\u00eat d'arbres d\u00e9cisionnels (Random forest)\nSi on a un nombre important de variables explicatives (features). on utilise la For\u00eat d'arbres qui fonctionne comme le suivant:\n- on prend des sous ensembles de donn\u00e9es et des sous ensembles de variables explicatives.\n- on applique l'Arbre de d\u00e9cision sur chaque sous ensemble.\n- la pr\u00e9diction de la for\u00eat al\u00e9atoire est alors un simple vote majoritaire des arbes construites.","794abd40":"## Arbre de d\u00e9cision (decision tree)\nUn arbre de d\u00e9cision est un arbre orient\u00e9 dont les noeuds sont \u00e9tiquet\u00e9s par un test et les arcs contiennent les r\u00e9sultats du test. On choisit de faire un test sur la variable qui disperse le mieux les classes. Pour cela, on calcule le **coefficient de Gini** qui mesure l'impurit\u00e9 d'un sous ensemble S de donn\u00e9e:\n$$Gini(S)=1-\\sum _{i}P(c_i\/S)^2$$\no\u00f9 $P(c_i\/S)$ est la probabilit\u00e9 de la classe $c_i$ dans le sous ensemble S. Pour un exemple de 2 classes: \n- le coefficient de Gini est null si l'ensemble est homeg\u00e9ne (il y a une seule classe) \n- le coefficient de Gini est maximale (vaut 0.5) pour un ensemble avec autant d'\u00e9l\u00e9ments de chaque classe\nOn choisit alors la variable et le test qui minimise le coefficient de Gini.","00746142":"## KNN \nhttps:\/\/www.datacamp.com\/community\/tutorials\/k-nearest-neighbor-classification-scikit-learn","0c1d8bd6":"# Classification\n\nQuand la variable cible (celle qu'on veut predire) est qualitative, on parle de classification. Le type de la variable dans ce cas est **category**.","6b8220c6":"Changer le type de la colonne **species** ","6b7cabef":"## Gridsearch"}}