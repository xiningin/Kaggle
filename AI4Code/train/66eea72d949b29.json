{"cell_type":{"55d004d3":"code","835bddaa":"code","f25edabe":"code","95fdbc32":"code","9cb6ba3e":"code","cb9cdad5":"code","d788a1c5":"code","e5792fbf":"code","c2168564":"code","a2f4cd16":"code","7d9af631":"code","e8093af8":"code","b0fa0d13":"code","1e3ca311":"code","c5ecaea7":"code","2d9309d3":"code","c2ffeb6a":"code","35a7d912":"code","1b484904":"code","ea094a23":"code","80552342":"code","e139d648":"code","603affdf":"code","52aeb780":"code","92be4c39":"code","1301a5b0":"code","40a0d1c6":"code","02ee9c12":"code","aa0fedd4":"code","922ed475":"code","b4b793cc":"code","5dd1bc67":"code","0790e577":"code","a9a95d8f":"code","05fd1f1e":"code","7757d912":"markdown","19a9f008":"markdown","c37d9754":"markdown","2cec13c0":"markdown","c8369b1f":"markdown","db4fe720":"markdown","afac1223":"markdown","2903354c":"markdown","0674588e":"markdown","87366582":"markdown","01d18c9e":"markdown","c8d8be8a":"markdown","b84c19c2":"markdown","07de3851":"markdown","560b4217":"markdown","ada54dd0":"markdown","b88e6f26":"markdown","2ac86952":"markdown","7fa07b71":"markdown","15d1267b":"markdown"},"source":{"55d004d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\nfrom IPython.core.display import display, HTML\nfrom plotly.offline import init_notebook_mode, iplot\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom plotly import tools\nimport plotly.offline as py\nfrom bokeh.layouts import gridplot\nfrom bokeh.plotting import figure, show, output_file\nfrom bokeh.io import output_notebook\nimport re\nfrom nltk.tokenize import word_tokenize \nfrom PIL import Image\noutput_notebook()\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda,BatchNormalization\nfrom keras.optimizers import Adam,Adadelta\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom tqdm import tqdm_notebook\nimport keras\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim import utils\nfrom keras.optimizers import SGD\nfrom gensim.models import Doc2Vec\n\nimport torch\n# Any results you write to the current directory are saved as output.","835bddaa":"#Distilbert for embeddings\nimport sys\n!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\nimport transformers","f25edabe":"inputpath='..\/input\/google-quest-challenge'\n\nprint(\"Reading the data\")\ntraindata=pd.read_csv(inputpath+'\/train.csv')\ntestdata=pd.read_csv(inputpath+'\/test.csv')\nsubmission=pd.read_csv(inputpath+'\/sample_submission.csv')","95fdbc32":"nrows = traindata.shape[0]\nncols=traindata.shape[1]\n\nnrows1 = testdata.shape[0]\nncols1=testdata.shape[1]\n\ncategories = traindata[\"category\"].nunique()\ntarget_labels=[i for i in list(set(traindata.columns).intersection(submission.columns)) if i!='qa_id']\n\ndisplay(HTML(f\"\"\"<br>Number of rows in the training dataset: {nrows:,}<\/br>\n                 <br>Number of rows in the test dataset: {nrows1:,}<\/br>\n                 <br>Number of cols in the training dataset: {ncols:,}<\/br>\n                 <br>Number of cols in the test dataset: {ncols1:,}<\/br>\n                 <br>Number of unique categories in the training dataset: {categories:,}<\/br>\n                  <br>Number of target labels: {len(target_labels):,}<\/br>\n             \"\"\"))","9cb6ba3e":"traindata.head(2)","cb9cdad5":"traindata['sourcename']=traindata['host'].apply(lambda x: x.split('.')[0])\ntestdata['sourcename']=testdata['host'].apply(lambda x: x.split('.')[0])\n","d788a1c5":"cnt_srs = traindata[\"sourcename\"].value_counts()\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"#1E90FF\",\n    ),\n)\n\nlayout = go.Layout(\n    title=go.layout.Title(\n        text=\"Different sources - Count\",\n        x=0.5\n    ),\n    font=dict(size=14),\n    width=1000,\n    height=500,\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"sources\")\n","e5792fbf":"cnt_srs = traindata[\"category\"].value_counts()\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"#1E90FF\",\n    ),\n)\n\nlayout = go.Layout(\n    title=go.layout.Title(\n        text=\"Different Categories - Count\",\n        x=0.5\n    ),\n    font=dict(size=14),\n    width=1000,\n    height=500,\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"categories\")\n","c2168564":"#Reference source: https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-ashrae","a2f4cd16":"\ndef make_plot(title, hist, edges, xlabel):\n    p = figure(title=title, tools='', background_fill_color=\"#fafafa\")\n    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n           fill_color=\"#1E90FF\", line_color=\"white\", alpha=0.5)\n\n    p.y_range.start = 0\n    p.xaxis.axis_label = f'{xlabel}'\n    p.yaxis.axis_label = 'Distribution'\n    p.grid.grid_line_color=\"white\"\n    return p\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"answer_type_reason_explanation\"].values, density=True, bins=10)\np1 = make_plot(\"Answer type reason\", hist, edges, \"answer\")\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"answer_well_written\"].values, density=True, bins=10)\np2 = make_plot(\"Answer well written\", hist, edges, 'answer')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"answer_helpful\"].values, density=True, bins=10)\np3 = make_plot(\"Answer helpful\", hist, edges, 'answer')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"answer_relevance\"].values, density=True, bins=10)\np4 = make_plot(\"Answer relevant\", hist, edges, 'answer')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"answer_level_of_information\"].values, density=True, bins=10)\np5 = make_plot(\"Answer level of information\", hist, edges, 'answer')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"answer_plausible\"].values, density=True, bins=10)\np6 = make_plot(\"Answer plausible\", hist, edges, 'answer')\n\nshow(gridplot([p1,p2,p3,p4,p5,p6], ncols=3, plot_width=400, plot_height=400, toolbar_location=None))\n\ndel p1,p2,p3,p4,p5,p6","7d9af631":"\ndef make_plot(title, hist, edges, xlabel):\n    p = figure(title=title, tools='', background_fill_color=\"#fafafa\")\n    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n           fill_color=\"#1E90FF\", line_color=\"white\", alpha=0.5)\n\n    p.y_range.start = 0\n    p.xaxis.axis_label = f'{xlabel}'\n    p.yaxis.axis_label = 'Distribution'\n    p.grid.grid_line_color=\"white\"\n    return p\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"question_well_written\"].values, density=True, bins=10)\np1 = make_plot(\"Question well written\", hist, edges, \"Question\")\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"question_type_entity\"].values, density=True, bins=10)\np2 = make_plot(\"Question type entity\", hist, edges, 'Question')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"question_type_choice\"].values, density=True, bins=10)\np3 = make_plot(\"Question type choice\", hist, edges, 'Question')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"question_fact_seeking\"].values, density=True, bins=10)\np4 = make_plot(\"Question fact seeking\", hist, edges, 'Question')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"question_not_really_a_question\"].values, density=True, bins=10)\np5 = make_plot(\"Question?\", hist, edges, 'Question')\n\ntemp_df = traindata[traindata[\"category\"]=='STACKOVERFLOW']\nhist, edges = np.histogram(temp_df[\"question_multi_intent\"].values, density=True, bins=10)\np6 = make_plot(\"Question multi intent\", hist, edges, 'Question')\n\nshow(gridplot([p1,p2,p3,p4,p5,p6], ncols=3, plot_width=400, plot_height=400, toolbar_location=None))\n\ndel p1,p2,p3,p4,p5,p6","e8093af8":"plt.figure(figsize=(16,12))\ncorr = traindata[target_labels].corr()\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3,\n     center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","b0fa0d13":"\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    \n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    \n    stops  = set(STOPWORDS)\n    text = [w for w in word_tokenize(x) if w not in stops]    \n    text = \" \".join(text)\n    \n    return text\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","1e3ca311":"input_columns = ['question_title','question_body','answer']\n\ntraindata = clean_data(traindata, input_columns)\ntestdata = clean_data(testdata, input_columns)","c5ecaea7":"traindata.head(2)","2d9309d3":"def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    \n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nd = '..\/input\/masks\/masks-wordclouds\/'\n","c2ffeb6a":"comments_text = str(traindata.question_body)\ncomments_mask = np.array(Image.open(d + 'upvote.png'))\nplot_wordcloud(comments_text, comments_mask, max_words=2000, max_font_size=300, \n               title = 'Most common words in all of the questions body', title_size=30)","35a7d912":"comments_text = str(traindata.answer)\ncomments_mask = np.array(Image.open(d + 'upvote.png'))\nplot_wordcloud(comments_text, comments_mask, max_words=2000, max_font_size=300, \n               title = 'Most common words in all of the answers', title_size=30)","1b484904":"#Credits: https:\/\/www.kaggle.com\/abazdyrev\/use-features-oof\n\nfeatures = ['sourcename', 'category']\nmerged = pd.concat([traindata[features], testdata[features]])\nohe = OneHotEncoder()\nohe.fit(merged)\n\nfeatures_train = ohe.transform(traindata[features]).toarray()\nfeatures_test = ohe.transform(testdata[features]).toarray()","ea094a23":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","80552342":"def fetch_vectors(string_list, batch_size=64):\n    # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in tqdm_notebook(chunks(string_list, batch_size)):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:200])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n        \n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","e139d648":"gc.collect()\n\ntrain_question_body_dense = fetch_vectors(traindata.question_body.values)\ntrain_answer_dense = fetch_vectors(traindata.answer.values)\n\ntest_question_body_dense = fetch_vectors(testdata.question_body.values)\ntest_answer_dense = fetch_vectors(testdata.answer.values)","603affdf":"module_url = \"..\/input\/universalsentenceencoderlarge4\/\"\nembed = hub.load(module_url)","52aeb780":"embeddings_test = {}\nembeddings_train = {}\n\nfor text in input_columns:\n    print(text)\n    train_text = traindata[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = testdata[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    \n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \ndel embed,curr_train_emb,curr_test_emb\nK.clear_session()\ngc.collect()","92be4c39":"# tfidf = TfidfVectorizer(ngram_range=(1, 3))\n# tsvd = TruncatedSVD(n_components = 128, n_iter=5)\n\n# tfquestion_title = tfidf.fit_transform(traindata[\"question_title\"].values)\n# tfquestion_title_test = tfidf.transform(testdata[\"question_title\"].values)\n# tfquestion_title = tsvd.fit_transform(tfquestion_title)\n# tfquestion_title_test = tsvd.transform(tfquestion_title_test)\n\n# tfquestion_body = tfidf.fit_transform(traindata[\"question_body\"].values)\n# tfquestion_body_test = tfidf.transform(testdata[\"question_body\"].values)\n# tfquestion_body = tsvd.fit_transform(tfquestion_body)\n# tfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\n# tfanswer = tfidf.fit_transform(traindata[\"answer\"].values)\n# tfanswer_test = tfidf.transform(testdata[\"answer\"].values)\n# tfanswer = tsvd.fit_transform(tfanswer)\n# tfanswer_test = tsvd.transform(tfanswer_test)\n\n# del tfidf,tsvd","1301a5b0":"def constructLabeledSentences(data):\n    sentences=[]\n    for index, row in data.iteritems():\n        sentences.append(TaggedDocument(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n    return sentences\n\n\ntrain_question_body_sentences = constructLabeledSentences(traindata['question_body'])\ntrain_question_title_sentences = constructLabeledSentences(traindata['question_title'])\ntrain_answer_sentences = constructLabeledSentences(traindata['answer'])\n\ntest_question_body_sentences = constructLabeledSentences(testdata['question_body'])\ntest_question_title_sentences = constructLabeledSentences(testdata['question_title'])\ntest_answer_sentences = constructLabeledSentences(testdata['answer'])","40a0d1c6":"all_sentences = train_question_body_sentences + \\\n                train_answer_sentences + \\\n                test_question_body_sentences + \\\n                test_answer_sentences\n\nText_INPUT_DIM=128\ntext_model = Doc2Vec(min_count=1, window=5, vector_size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, epochs=5,seed=1)\ntext_model.build_vocab(all_sentences)\ntext_model.train(all_sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)","02ee9c12":"def infer_vec(df, columns: list):\n    for col in columns:\n        df[col+'_vec'] = df[col].apply(lambda x: np.array(text_model.infer_vector([x])))\n\n    return df\n\ntraindata = infer_vec(traindata, input_columns)\ntestdata = infer_vec(testdata, input_columns)","aa0fedd4":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n]).T\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T\n\nX_train = np.hstack([item for k, item in embeddings_train.items()]+ [features_train, dist_features_train])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])\ny_train = traindata[target_labels].values","922ed475":"X_train = np.concatenate((X_train,train_question_body_dense,train_answer_dense,np.array(traindata['question_title_vec'].tolist())\n                    ,np.array(traindata['question_body_vec'].tolist()),np.array(traindata['answer_vec'].tolist())),axis=1)\nX_test = np.concatenate((X_test,test_question_body_dense,test_answer_dense,np.array(testdata['question_title_vec'].tolist())\n                  ,np.array(testdata['question_body_vec'].tolist()),np.array(testdata['answer_vec'].tolist())),axis=1)","b4b793cc":"class SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n            self.model.save_weights(self.model_name)\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","5dd1bc67":"def swish(x):\n    return K.sigmoid(x) * x\n\ndef mish(x):\n    return x * keras.backend.tanh(keras.backend.softplus(x))\n\n\ndef create_model():\n    inps = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation=swish)(inps)\n    x = Dropout(0.2)(x)\n    x = Dense(y_train.shape[1], activation='sigmoid')(x)\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer=Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n    model.summary()\n    return model\n\ndef create_model2():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation=mish\n              )(input1)\n    x = Dropout(0.2)(x)\n    output = Dense(len(target_labels),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    optimizer = Adadelta()\n    \n    model.compile(\n        optimizer=optimizer,\n        loss=['binary_crossentropy']\n    )\n    model.summary()\n    return model\n\ndef create_model3():\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation=swish)(input1)\n    x = Dropout(0.2)(x)\n    x = Dense(256, activation=swish,\n              kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    x = Dropout(0.2)(x)\n    output = Dense(len(target_labels),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    model.compile(\n        optimizer=Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n    model.summary()\n    return model\n","0790e577":"all_predictions = []\n\nkf = MultilabelStratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\nfor ind, (tr, val) in enumerate(kf.split(X_train,y_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=8, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=u'best_model_batch.h5')]\n    )\n    model.load_weights('best_model_batch.h5')\n    all_predictions.append(model.predict(X_test))\n    \n    os.remove('best_model_batch.h5')\n    \nmodel = create_model3()\nmodel.fit(X_train, y_train, epochs=30, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n    \nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model2()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=64, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=u'best_model_batch.h5')]\n    )\n    model.load_weights('best_model_batch.h5')\n    all_predictions.append(model.predict(X_test))\n    \n    os.remove('best_model_batch.h5')\n    \n    \n# model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n# model.fit(X_train, y_train)\n# all_predictions.append(model.predict(X_test))\n\nmodel = create_model3()\nmodel.fit(X_train, y_train, epochs=30, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n\nmodel = create_model3()\nmodel.fit(X_train, y_train, epochs=20, batch_size=8, verbose=False)\nall_predictions.append(model.predict(X_test))","a9a95d8f":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds\/max_val + 1e-12","05fd1f1e":"submission = pd.read_csv(path_join(inputpath, 'sample_submission.csv'))\nsubmission[target_labels] = test_preds\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","7757d912":"To be continued!!","19a9f008":"Now let's do basic preprocessing and see the word clouds","c37d9754":"Looks like most of the questions pairs were collected from stackoverflow","2cec13c0":"<font color='#088a5a' size=4>Baseline<\/font><br>","c8369b1f":"<font color='#088a5a' size=4>Data glimpse<\/font><br>","db4fe720":"> <font color='#088a5a' size=3>Universal sentence encoder<\/font>","afac1223":"> <font color='#088a5a' size=3>Other features<\/font>","2903354c":"Most of the answers from stack-overflow seems to have convincing score","0674588e":"<img src='https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/7968\/logos\/thumb76_76.png' width=300>\n<div align=\"center\"><font size=\"2\">Source: Google<\/font><\/div>","87366582":"> <font color='#088a5a' size=3>Doc2vec<\/font>","01d18c9e":"<p>The focus of the competition is to bring advancement in the development of machine intelligent systems specifically focussing on <b>Question and Answering systems<\/b> which is one of the hot topics of natural understanding tasks. In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts<\/p>\n\n<p> Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.<\/p>","c8d8be8a":"> Change status:\n* Update_V7: Adding doc2vec to the input vectors and added Multilabel stratified kfold for CV\n* Update_V6: Adding swish  and mish activatin  function and reducing the no of layers and removing tf-idf features\n* Update_V5: Adding stack models with different activation functions","b84c19c2":"> <font color='#088a5a' size=3>Distilbert features<\/font>","07de3851":"If the intent of the question is well captured then it means the question is also well written. Questions and answers well written are highly correlated","560b4217":"> <font color='#088a5a' size=3>TF-IDF<\/font>","ada54dd0":"Majority of questions comes under the category of technology","b88e6f26":"<font color='#088a5a' size=4>Data Understanding<\/font><br>","2ac86952":"<font color='#088a5a' size=4>Feature engineering<\/font><br>","7fa07b71":"The metrics for  this competition is spearman correlation, hence implicitly labels should be correlated with each other and let's check the same","15d1267b":"<font color='#088a5a' size=3>Kindly upvote the kernel if you like it!<\/font><br>"}}