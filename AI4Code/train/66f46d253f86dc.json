{"cell_type":{"43c1a81b":"code","616bfde8":"code","2399459d":"code","d45034b3":"code","3c845c2c":"code","740a226c":"code","d1b6e4e3":"code","acbdfbc2":"code","8766c387":"code","41c2fe7f":"code","b9a26425":"code","8e73d1ff":"code","59233117":"code","df3979f4":"code","17594c8c":"code","ed25f884":"code","9eab1b80":"code","a078211f":"code","0470c358":"code","fc6ffb7a":"code","86b08364":"code","3f9da763":"code","ec38561b":"code","2f0dfc41":"code","2b91c94f":"code","edb677ff":"code","27579ead":"code","95ca157c":"code","3b98713b":"code","e8944791":"code","34cdf88e":"code","e25f0130":"code","27a04a3b":"code","4f0b0022":"code","e3edfd81":"code","2fa70918":"code","93521f49":"code","7e0f3fa4":"code","a5a0b356":"code","04ba150c":"code","88d6013e":"code","e65d162c":"code","4249d29c":"code","0f0916f3":"code","c259f280":"code","3c82dc16":"code","853cb271":"code","77daf062":"code","599e22d8":"code","2350b4c0":"code","d15e0b7e":"code","eebe259b":"code","fef782f7":"code","59ce1d99":"code","2408451b":"code","427767bc":"code","852230b3":"code","4eb7e148":"code","41aa7994":"code","22b8ec4e":"code","ec04a709":"code","1c440782":"code","bf65617b":"code","894a37b8":"code","fc3aa25e":"code","1bfad594":"code","5a9c4827":"code","dadf4837":"code","8e323030":"code","f906e65c":"markdown","32be9605":"markdown","9be14c4c":"markdown","92eebc98":"markdown","27666a9c":"markdown","045a051f":"markdown","530419d2":"markdown","c9fd7c56":"markdown","1c4d416b":"markdown","fa368cb5":"markdown"},"source":{"43c1a81b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nimport xgboost as xgb\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","616bfde8":"df_titanic = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\") # Importing training dataset\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\") # Importing test datset","2399459d":"df_titanic.columns   # Columns of training dataset","d45034b3":"df_titanic.head()   #Top 5 results of training Dataset","3c845c2c":"df_titanic.info()   #Information about training Dataset","740a226c":"df_titanic.shape   #Checking rows and columns of training dataset","d1b6e4e3":"df_titanic.describe()   #Statistics of training Dataframe","acbdfbc2":"#Checking missing values & percentage in training Dataset\nmissing_percent=df_titanic.isnull().sum()\/891*100\nmissing_count=df_titanic.isnull().sum()\ndf_null_train= pd.DataFrame(data={'missing_percent_train': missing_percent, 'missing_count_train': missing_count},\n                            index=test.columns) \ndf_null_train.sort_values(by='missing_percent_train', ascending=False)","8766c387":"test.head()  # Columns of test dataset","41c2fe7f":"test.shape   # No. of Rows and Columns of test dataset","b9a26425":"test.info()  # Information about test dataset","8e73d1ff":"test.isnull().sum()   # Missing values in test dataset","59233117":"#Checking missing values & percentage in test Dataset\nmissing_percent_test=test.isnull().sum()\/418*100\nmissing_count_test=test.isnull().sum()\ndf_null_test= pd.DataFrame(data={'missing_percent_test': missing_percent_test, 'missing_count_test': missing_count_test},\n                      index=test.columns) \ndf_null_test.sort_values(by='missing_percent_test', ascending=False)","df3979f4":"#Replacing missing values in training dataset\ndf_titanic['Embarked'].fillna(df_titanic['Embarked'].dropna().mode(), inplace=True)\ndf_titanic['Embarked'].fillna(df_titanic['Embarked'].value_counts()[0], inplace=True)\ndf_titanic['Age'].fillna(df_titanic['Age'].dropna().mean(), inplace=True)\n\n\n#from sklearn.impute import SimpleImputer\n#imp = SimpleImputer(strategy='most_frequent')\n#temp = imp.fit_transform(df_titanic)","17594c8c":"#Replacing missing values in test dataset\ntest['Age'].fillna(test['Age'].dropna().mean(), inplace=True)\ntest['Fare'].fillna(test['Fare'].dropna().mean(), inplace=True)","ed25f884":"df_titanic.isnull().sum()  #Checking missing value in training dataset again","9eab1b80":"test.isnull().sum()  #Checking missing value in test dataset again","a078211f":"df_titanic.head()","0470c358":"#Creating new feature from SibSp and Parch\ndf = df_titanic.copy()   #Copying training dataset into new dataframe\ndf_test = test.copy()    #Copying test dataset into new dataframe\n\ndf['TotalFamily'] = df['SibSp'] + df['Parch']\ndf['FamilyBucket'] = 'FamilyBucket'\ndf.loc[df['TotalFamily'] == 0, 'FamilyBucket'] = 'Single'\ndf.loc[(df['TotalFamily']>=1) & (df['TotalFamily']<=3), 'FamilyBucket'] = 'SmallFamily'\ndf.loc[df['TotalFamily']>3, 'FamilyBucket'] = 'LargeFamily'\n\ndf_test['TotalFamily'] = df_test['SibSp'] + df_test['Parch']\ndf_test['FamilyBucket'] = 'FamilyBucket'\ndf_test.loc[df_test['TotalFamily'] == 0, 'FamilyBucket'] = 'Single'\ndf_test.loc[(df_test['TotalFamily']>=1) & (df_test['TotalFamily']<=3), 'FamilyBucket'] = 'SmallFamily'\ndf_test.loc[df_test['TotalFamily']>3, 'FamilyBucket'] = 'LargeFamily'\n\n\n#Creating new feature from Age\ndf['AgeGroup'] = 'agegroup'\ndf.loc[df['Age']<=1, 'AgeGroup'] = 'Infant'\ndf.loc[(df['Age']>1) & (df['Age']<=5), 'AgeGroup'] = 'Child'\ndf.loc[(df['Age']>5) & (df['Age']<=10), 'AgeGroup'] = 'YoungChild'\ndf.loc[(df['Age']>10) & (df['Age']<=50), 'AgeGroup'] = 'Adult'\ndf.loc[df['Age']>50, 'AgeGroup'] = 'SeniorCitizen'\n\ndf_test['AgeGroup'] = 'agegroup'\ndf_test.loc[df_test['Age']<=1, 'AgeGroup'] = 'Infant'\ndf_test.loc[(df_test['Age']>1) & (df_test['Age']<=5), 'AgeGroup'] = 'Child'\ndf_test.loc[(df_test['Age']>5) & (df_test['Age']<=10), 'AgeGroup'] = 'YoungChild'\ndf_test.loc[(df_test['Age']>10) & (df_test['Age']<=50), 'AgeGroup'] = 'Adult'\ndf_test.loc[df_test['Age']>50, 'AgeGroup'] = 'SeniorCitizen'\n","fc6ffb7a":"#Encoding object values in training dataset\nDAgeGroup = pd.get_dummies(df['AgeGroup'], prefix='AgeGroup')\ndf = pd.concat([df, DAgeGroup], axis=1)\nDFamilyBucket = pd.get_dummies(df['FamilyBucket'], prefix='FamilyBucket')\ndf = pd.concat([df, DFamilyBucket], axis=1)\nDEmbarked = pd.get_dummies(df['Embarked'], prefix='Embarked')\ndf = pd.concat([df, DEmbarked], axis=1)\nDSex = pd.get_dummies(df['Sex'], prefix = 'Sex')\ndf = pd.concat([df, DSex], axis=1)\n\n#Encoding object values in test dataset\nDAgeGroup_test = pd.get_dummies(df_test['AgeGroup'], prefix='AgeGroup')\ndf_test = pd.concat([df_test, DAgeGroup_test], axis=1)\nDFamilyBucket_test = pd.get_dummies(df_test['FamilyBucket'], prefix='FamilyBucket')\ndf_test = pd.concat([df_test, DFamilyBucket_test], axis=1)\nDEmbarked_test = pd.get_dummies(df_test['Embarked'], prefix='Embarked')\ndf_test = pd.concat([df_test, DEmbarked_test], axis=1)\nDSex_test = pd.get_dummies(df_test['Sex'], prefix = 'Sex')\ndf_test = pd.concat([df_test, DSex_test], axis=1)","86b08364":"#Removing Name, Ticket & Cabin from training dataset & test dataset\ndf = df.drop(columns=['Name', 'Ticket', 'Cabin'], axis=1)\ndf_test = df_test.drop(columns=['Name', 'Ticket', 'Cabin'], axis=1)","3f9da763":"df.head()","ec38561b":"df_test.head()","2f0dfc41":"#Checking Correlation matrix with Heatmap in training dataset\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), annot=True,cmap='cubehelix_r')\nplt.show()","2b91c94f":"X_feat_sel = df.drop(columns=['PassengerId', 'Survived', 'FamilyBucket', 'AgeGroup', 'Embarked', 'Sex'], axis=1)\ny_feat_sel = df['Survived']\n\n#Applying SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X_feat_sel,y_feat_sel)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_feat_sel.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","edb677ff":"#Using inbuilt class feature_importances of tree based classifiers\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X_feat_sel,y_feat_sel)\nprint(model.feature_importances_) \n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X_feat_sel.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","27579ead":"temp1=df.groupby(['Survived'])['AgeGroup'].value_counts()\nplt.figure(figsize=(20,6))\nsns.barplot(temp1.index, temp1.values)\nplt.title('Survived Vs Age group count')\nplt.xlabel('Survived, Age group')\nplt.ylabel('Age group count')\nplt.show()","95ca157c":"#Checking distribution of Fare\nfig,ax=plt.subplots(figsize=(8,4))\nsns.distplot(df['Fare'],hist=True,kde=True,color='g')\nplt.show()","3b98713b":"#Checking distribution of Gender between Survived and Fare using strip plot\nfig,ax=plt.subplots(figsize=(8,4))\nsns.stripplot(x='Survived', y='Fare', hue='Sex', data=df)\nplt.show()","e8944791":"#Checking distribution of Age Group between Survived and Fare using swarm plot\nfig, ax=plt.subplots(figsize=(10,6))\nsns.swarmplot(x='Survived', y='Fare', hue='AgeGroup', data=df)\nplt.show()","34cdf88e":"#Checking distribution of Family Group between Survived and Fare using swarm plot\nfig, ax=plt.subplots(figsize=(10,6))\nsns.swarmplot(x='Survived', y='Fare', hue='FamilyBucket', data=df)\nplt.show()","e25f0130":"#Pair plot of Survived, Pclass, Age, Fare, TotalFamily & Sex\nsns.pairplot(df[['Sex','Survived','Pclass','Age', 'TotalFamily','Fare']],hue='Sex')\nplt.show()","27a04a3b":"#Plotting Bar plot of Survived Vs Avg. Fare\ntemp11=df.groupby(['Survived'])['Fare'].mean()\nsns.barplot(x=temp11.index,y=temp11.values, data=df_titanic, color='c')\nplt.ylabel('Avg. Fare')\nplt.title('Survived Vs Avg. Fare')\nplt.show()","4f0b0022":"#Selecting dependent and independent variables\n#X = df.drop(columns=['PassengerId', 'Survived', 'FamilyBucket', 'AgeGroup', 'Embarked', 'Sex'], axis=1)\nX = df[['Fare', 'Sex_female', 'Sex_male', 'FamilyBucket_SmallFamily', 'Pclass', 'Age', 'Embarked_C', 'FamilyBucket_Single', 'AgeGroup_Infant', 'FamilyBucket_LargeFamily']]\ny = df['Survived']","e3edfd81":"#Splitting Training dataset\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22)","2fa70918":"#Using Random Forest Classifier\n\nrfc = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=2, min_samples_split=2, min_samples_leaf=1, max_features='auto', bootstrap=True)\nacc_rfc_cv=cross_val_score(estimator=rfc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Random Forest Classifier using K-fold cross validation is :\",np.mean(acc_rfc_cv))\n\nrfc.fit(X_train, y_train)\ny_pred_rfc = rfc.predict(X_test)\nacc_rfc = metrics.accuracy_score(y_pred_rfc, y_test)\nprint('Accuracy of test Random Forest Classifier is: ', metrics.accuracy_score(y_pred_rfc, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_rfc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_rfc))","93521f49":"#Using Grid search to get best parameters for Random Forest classifier\nparam_grid = { \n    'n_estimators': [10, 20, 30, 40, 50],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [2, 3, 4, 5, 6],\n    'bootstrap': [True, False],\n    'criterion' :['gini', 'entropy'],\n    'min_samples_leaf' : [5, 10, 15, 20]\n}\n\nCV_rfc = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_rfc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_rfc.best_score_)","7e0f3fa4":"#Using Random Forest Classifier with Gridsearch best parameters\n\nrfc2 = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=5, min_samples_split=2, min_samples_leaf=5, max_features='log2', bootstrap=False)\nacc_rfc_cv2=cross_val_score(estimator=rfc2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Random Forest Classifier using K-fold cross validation is :\",np.mean(acc_rfc_cv2))\n\nrfc2.fit(X_train, y_train)\ny_pred_rfc2 = rfc2.predict(X_test)\nacc_rfc2 = metrics.accuracy_score(y_pred_rfc2, y_test)\nprint('Accuracy of test Random Forest Classifier is: ', metrics.accuracy_score(y_pred_rfc2, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_rfc2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_rfc2))","a5a0b356":"#Using Decision Tree classifier\n\ndtc = DecisionTreeClassifier(criterion='entropy', max_depth=2, min_samples_split=2, min_samples_leaf=1)\nacc_dtc_cv=cross_val_score(estimator=dtc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Decision Classifier using K-fold cross validation is :\",np.mean(acc_dtc_cv))\n\ndtc.fit(X_train, y_train)\ny_pred_dtc = dtc.predict(X_test)\nacc_dtc = metrics.accuracy_score(y_pred_dtc, y_test)\nprint('Accuracy of test Decision Tree Classifier is: ', metrics.accuracy_score(y_pred_dtc, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_dtc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_dtc))\n","04ba150c":"#Choosing best parameters of Decision Tree using Grid search\ngrid = {'criterion' : ['gini', 'entropy'],\n       'max_depth' : np.arange(1,10),\n       'min_samples_split' : np.arange(2,10),\n       'max_features' : ['auto', 'sqrt', 'log2']}\n\nCV_dtc = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=grid, cv=5)\nCV_dtc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_dtc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_dtc.best_score_)","88d6013e":"#Using Decision tree Classifier with Best parameter from Grid Search\n\ndtc2 = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=6, max_features='log2',min_samples_leaf=1)\nacc_dtc_cv2 = cross_val_score(estimator=dtc2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Decision Classifier using K-fold cross validation is :\",np.mean(acc_dtc_cv2))\n\ndtc2.fit(X_train, y_train)\ny_pred_dtc2 = dtc2.predict(X_test)\nacc_dtc2 = metrics.accuracy_score(y_pred_dtc2, y_test)\nprint('Accuracy of test Decision Tree Classifier is: ', metrics.accuracy_score(y_pred_dtc2, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_dtc2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_dtc2))","e65d162c":"#Using Logistic Regression\n\nlr = LogisticRegression(penalty='l2', C=1.0, max_iter=100)\nacc_lr_cv=cross_val_score(estimator=lr,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Logistic Regression using K-fold cross validation is :\",np.mean(acc_lr_cv))\n\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\nacc_lr = metrics.accuracy_score(y_pred_lr, y_test)\nprint('Accuracy of Logistic Regression is: ', metrics.accuracy_score(y_pred_lr, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_lr))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_lr))","4249d29c":"#Choosing best parameters of Logistic egression using Grid search\ngrid = {'penalty': ['l1', 'l2'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n\nCV_lr = GridSearchCV(estimator=LogisticRegression(), param_grid=grid, cv= 5)\nCV_lr.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_lr.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_lr.best_score_)","0f0916f3":"#Using Logistic Regression with best parameters as per Grid search\n\nlr2 = LogisticRegression(penalty='l1', C=1, max_iter=100)\nacc_lr_cv2=cross_val_score(estimator=lr2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Logistic Regression using K-fold cross validation is :\",np.mean(acc_lr_cv2))\n\nlr2.fit(X_train, y_train)\ny_pred_lr2 = lr2.predict(X_test)\nacc_lr2 = metrics.accuracy_score(y_pred_lr2, y_test)\nprint('Accuracy of Logistic Regression is: ', metrics.accuracy_score(y_pred_lr2, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_lr2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_lr2))","c259f280":"#Using KNN classifier\n\nknc = KNeighborsClassifier(n_neighbors=5)\nacc_knc_cv=cross_val_score(estimator=knc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of KNN classifier using K-fold cross validation is :\",np.mean(acc_knc_cv))\n\nknc.fit(X_train, y_train)\ny_pred_knc = knc.predict(X_test)\nacc_knc = metrics.accuracy_score(y_pred_knc, y_test)\nprint('Accuracy of KNN classifier is: ', metrics.accuracy_score(y_pred_knc, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_knc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_knc))","3c82dc16":"#Choosing best parameters of KNN using Grid search\ngrid ={\"n_neighbors\":np.arange(1,50)}\nCV_knc=GridSearchCV(KNeighborsClassifier(),grid,cv=10)#K=10 \nCV_knc.fit(X_train,y_train)\nprint(\"tuned hyperparameter K:\",CV_knc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_knc.best_score_)","853cb271":"#Using KNN classifier again with Gridsearch best parameters\n\nknc2 = KNeighborsClassifier(n_neighbors=7)\nacc_knc_cv2 = cross_val_score(estimator=knc2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of KNN classifier using K-fold cross validation is :\",np.mean(acc_knc_cv2))\n\nknc2.fit(X_train, y_train)\ny_pred_knc2 = knc2.predict(X_test)\nacc_knc2 = metrics.accuracy_score(y_pred_knc2, y_test)\nprint('Accuracy of KNN classifier is: ', metrics.accuracy_score(y_pred_knc2, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_knc2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_knc2))","77daf062":"#Using SVM classifier\n\nsvc=SVC(C=1,kernel='linear',degree=3,gamma=1)\nacc_svc_cv=cross_val_score(estimator=svc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of SVM classifier using K-fold cross validation is :\",np.mean(acc_svc_cv))\n\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\nacc_svm = metrics.accuracy_score(y_pred_knc, y_test)\nprint('Accuracy of SVM classifier is: ', metrics.accuracy_score(y_pred_svc, y_test))\nprint('Classification report: ', classification_report(y_test, y_pred_svc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_svc))","599e22d8":"#Using Bagging classifier\n\nbagclf=BaggingClassifier(n_estimators=100, bootstrap_features=True)\nacc_bagclf_cv=cross_val_score(estimator=bagclf,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Bagging classifier using K-fold cross validation is :\",np.mean(acc_bagclf_cv))\n\nbagclf.fit(X_train, y_train)\ny_pred_bagclf = bagclf.predict(X_test)\nacc_bagclf = accuracy_score(y_test, y_pred_bagclf)\nprint('Accuracy of Bagging classifier is: ', accuracy_score(y_test, y_pred_bagclf))\nprint('Classification report: ', classification_report(y_test, y_pred_bagclf))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_bagclf))\n","2350b4c0":"#Choosing best parameters of Bagging classifier using Grid search\ngrid = {'n_estimators' : np.arange(10,100),\n       'bootstrap' : ['True', 'False'],\n       'bootstrap_features' : ['True', 'False']}\n\nCV_bagclf = GridSearchCV(estimator=BaggingClassifier(), param_grid=grid, cv=5)\nCV_bagclf.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_bagclf.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_bagclf.best_score_)","d15e0b7e":"#Using Bagging classifier with best parameters from Grid search\n\nbagclf2 = BaggingClassifier(n_estimators=27, bootstrap=True, bootstrap_features=True)\nacc_bagclf_cv2 = cross_val_score(estimator=bagclf2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of Bagging classifier using K-fold cross validation is :\",np.mean(acc_bagclf_cv2))\n\nbagclf2.fit(X_train, y_train)\ny_pred_bagclf2 = bagclf2.predict(X_test)\nacc_bagclf2 = accuracy_score(y_test, y_pred_bagclf2)\nprint('Accuracy of Bagging classifier is: ', accuracy_score(y_test, y_pred_bagclf2))\nprint('Classification report: ', classification_report(y_test, y_pred_bagclf2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_bagclf2))","eebe259b":"#Using AdaBoost classifier\n\nabc = AdaBoostClassifier()\nacc_abc_cv = cross_val_score(estimator=abc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of AdaBoost classifier using K-fold cross validation is :\",np.mean(acc_abc_cv))\n\nabc.fit(X_train, y_train)\ny_pred_abc = abc.predict(X_test)\nacc_abc = accuracy_score(y_test, y_pred_abc)\nprint('Accuracy of AdaBoost classifier is: ', accuracy_score(y_test, y_pred_abc))\nprint('Classification report: ', classification_report(y_test, y_pred_abc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_abc))","fef782f7":"#Choosing best parameters of AdaBoost classifier using Grid search\ngrid = {'n_estimators' : np.arange(10,100)}\nCV_abc = GridSearchCV(estimator=AdaBoostClassifier(),param_grid=grid, cv=5)\nCV_abc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_abc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_abc.best_score_)","59ce1d99":"#Using AdaBoost classifier with best parameters from Grid search\n\nabc2 = AdaBoostClassifier(n_estimators=18)\nacc_abc_cv2 = cross_val_score(estimator=abc2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of AdaBoost classifier using K-fold cross validation is :\",np.mean(acc_abc_cv2))\n\nabc2.fit(X_train, y_train)\ny_pred_abc2 = abc2.predict(X_test)\nacc_abc2 = accuracy_score(y_test, y_pred_abc2)\nprint('Accuracy of AdaBoost classifier is: ', accuracy_score(y_test, y_pred_abc2))\nprint('Classification report: ', classification_report(y_test, y_pred_abc2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_abc2))","2408451b":"#Using Gradient Boosting classifier\n\ngbc = GradientBoostingClassifier()\nacc_gbc_cv = cross_val_score(estimator=gbc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of  Gradient Boosting classifier using K-fold cross validation is :\",np.mean(acc_gbc_cv))\n\ngbc.fit(X_train, y_train)\ny_pred_gbc = gbc.predict(X_test)\nacc_gbc = accuracy_score(y_test, y_pred_gbc)\nprint('Accuracy of Gradient Boosting classifier is: ', accuracy_score(y_test, y_pred_gbc))\nprint('Classification report: ', classification_report(y_test, y_pred_gbc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_gbc))","427767bc":"#Choosing best parameters of Gradient Boosting classifier using Grid search\ngrid = {'n_estimators' : np.arange(10,100),\n       'loss': ['deviance', 'exponential'],\n       'learning_rate' : [0.001, 0.01, 0.1]}\nCV_gbc = GridSearchCV(estimator=GradientBoostingClassifier(),param_grid=grid, cv=5)\nCV_gbc.fit(X_train, y_train)\nprint(\"tuned hyperparameters :\",CV_gbc.best_params_)\nprint(\"tuned parameter accuracy (best score):\",CV_gbc.best_score_)","852230b3":"#Using Gradient Boosting classifier with best parameters from Grid search\n\ngbc2 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=41, loss='exponential')\nacc_gbc_cv2 = cross_val_score(estimator=gbc2,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of  Gradient Boosting classifier using K-fold cross validation is :\",np.mean(acc_gbc_cv2))\n\ngbc2.fit(X_train, y_train)\ny_pred_gbc2 = gbc2.predict(X_test)\nacc_gbc2 = accuracy_score(y_test, y_pred_gbc2)\nprint('Accuracy of Gradient Boosting classifier is: ', accuracy_score(y_test, y_pred_gbc2))\nprint('Classification report: ', classification_report(y_test, y_pred_gbc2))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_gbc2))","4eb7e148":"#Using XGBoost classifier\n\n\nxbc = xgb.XGBClassifier(random_state=1,learning_rate=0.01)\nacc_xbc_cv = cross_val_score(estimator=xbc,X=X_train,y=y_train,cv=10)  #K=10\nprint(\"Average accuracy of XGBoost classifier using K-fold cross validation is :\",np.mean(acc_xbc_cv))\n\nxbc.fit(X_train, y_train)\ny_pred_xbc = xbc.predict(X_test)\nacc_xbc = accuracy_score(y_test, y_pred_xbc)\nprint('Accuracy of XGBoost classifier is: ', accuracy_score(y_test, y_pred_xbc))\nprint('Classification report: ', classification_report(y_test, y_pred_xbc))\nprint('Confusion matrix: ', confusion_matrix(y_test, y_pred_xbc))\n","41aa7994":"#Comparing Accuracy of each model\nmodels = pd.DataFrame({'Model' : ['RandomForest', 'DecisionTree', 'LogisticRegression', 'KNN', 'SVM', 'BaggingClassifier', 'AdaBoost', 'GradientBoost', 'XgBoost'], \n                      'Score' : [acc_rfc, acc_dtc, acc_lr, acc_knc, acc_svm, acc_bagclf, acc_abc, acc_gbc, acc_xbc]})\nmodels.sort_values(by='Score', ascending=False)\nfig, ax=plt.subplots(figsize=(15,6))\nsns.barplot(x='Model', y='Score', data=models)\nax.set_xlabel('Classifiers')\nax.set_ylabel('Accuracy Score')\nax.set_title('Classifiers Vs Accuracy score')\nax.set_ylim([0.6, 0.9])\nplt.show()","22b8ec4e":"#Comparing Accuracy of each model post Grid search\nmodels2 = pd.DataFrame({'Model' : ['RandomForest', 'DecisionTree', 'LogisticRegression', 'KNN', 'SVM', 'BaggingClassifier', 'AdaBoost', 'GradientBoost'], \n                      'Score' : [acc_rfc2, acc_dtc2, acc_lr2, acc_knc2, acc_svm, acc_bagclf2, acc_abc2, acc_gbc2]})\nmodels2.sort_values(by='Score', ascending=False)\nfig, ax=plt.subplots(figsize=(15,6))\nsns.barplot(x='Model', y='Score', data=models)\nax.set_xlabel('Classifiers')\nax.set_ylabel('Accuracy Score')\nax.set_title('Classifiers Vs Accuracy score post Grid search')\nax.set_ylim([0.6, 0.9])\nplt.show()","ec04a709":"#Plotting ROC curve using Random Forest\n\ny_pred_rfc_prob = rfc.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_rfc_prob)\nplt.title('ROC curve for test samples using Random Forest')\nplt.show()\n","1c440782":"#Plotting ROC curve using Decision Tree\n\ny_pred_dtc_prob = dtc.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_dtc_prob)\nplt.title('ROC curve for test samples using Decision Tree')\nplt.show()","bf65617b":"#Plotting ROC curve using Logistic Regression\n\ny_pred_lr_prob = lr.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_lr_prob)\nplt.title('ROC curve for test samples using Logistic regressor')\nplt.show()","894a37b8":"#Plotting ROC curve using KNN\n\ny_pred_knc_prob = knc.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_knc_prob)\nplt.title('ROC curve for test samples using KNN')\nplt.show()","fc3aa25e":"#Plotting ROC curve using Bagging Classifier\n\ny_pred_bagclf_prob = bagclf.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_bagclf_prob)\nplt.title('ROC curve for test samples using Bagging Classifier')\nplt.show()","1bfad594":"#Plotting ROC curve using AdaBoost Classifier\n\ny_pred_abc_prob = abc.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_abc_prob)\nplt.title('ROC curve for test samples using AdaBoost Classifier')\nplt.show()","5a9c4827":"#Plotting ROC curve using GradientBoosting Classifier\n\ny_pred_gbc_prob = gbc.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_gbc_prob)\nplt.title('ROC curve for test samples using GradientBoosting Classifier')\nplt.show()","dadf4837":"#Plotting ROC curve using XgBoost Classifier\n\ny_pred_xbc_prob = xbc.predict_proba(X_test)\nskplt.metrics.plot_roc(y_test, y_pred_xbc_prob)\nplt.title('ROC curve for test samples using XgBoost Classifier')\nplt.show()","8e323030":"#Setting Id as PassengerId and predicting survival \n\ndf_test['Embarked_644']=0   # Adding one column which is missing in test dataset but available in training model\nId = df_test['PassengerId']\n#predictions = gbc.predict(df_test.drop(columns=['PassengerId','Sex','Embarked','FamilyBucket','AgeGroup'], axis=1))\npredictions = gbc.predict(df_test[['Fare', 'Sex_female', 'Sex_male', 'FamilyBucket_SmallFamily', 'Pclass', 'Age', 'Embarked_C', 'FamilyBucket_Single', 'AgeGroup_Infant', 'FamilyBucket_LargeFamily']])\n\n\n#Converting output dataframe to csv file named \"submission.csv\"\noutput = pd.DataFrame({ 'PassengerId' : Id, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","f906e65c":"> **Submission of prediction file**","32be9605":"> **Using different models for training the dataset and checking accuracy**","9be14c4c":"> **Exploratory Data Analysis using different graphs**","92eebc98":"**Data cleaning**","27666a9c":"> **Adding New Features**","045a051f":"> **ROC curve for different models**","530419d2":"**Feature selection analysis**","c9fd7c56":"> ****Encoding Object values to numeric****","1c4d416b":"> **Selecting Independent and Dependent variables for model**","fa368cb5":"**Exploratory Data Analysis**"}}