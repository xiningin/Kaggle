{"cell_type":{"f1940821":"code","bdb6593f":"code","1ff63198":"code","5b600ec9":"code","71b840c9":"code","31b97bb8":"code","7e1daae5":"code","f5556a66":"code","6fdec939":"code","77619ee0":"code","88693e13":"code","9ea9cac2":"code","b6908b1c":"code","ef8b6d75":"code","670e4cd8":"code","0b0480df":"code","dec8e87e":"code","fa299ebb":"code","204315fc":"markdown","dbeae251":"markdown"},"source":{"f1940821":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bdb6593f":"%matplotlib inline","1ff63198":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nimport shap\nimport xgboost as xgb\nprint(xgb.__version__)\nimport warnings\nwarnings.filterwarnings('ignore')\n# load JS visualization code to notebook\nshap.initjs()","5b600ec9":"train = pd.read_csv('..\/input\/widsdatathon2022\/train.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2022\/test.csv')\ndel train['id']\ndel test['id']\n\nsub = pd.read_csv('..\/input\/widsdatathon2022\/sample_solution.csv')\ntrain.head()","71b840c9":"test.head()","31b97bb8":"columns = test.columns\ncolumns","7e1daae5":"target = train['site_eui'].values\ndel train['site_eui']","f5556a66":"train_oof = np.zeros((train.shape[0],))\ntest_preds = 0\ntrain_oof_shap = np.zeros((train.shape[0],train.shape[1]-1))\ntest_preds_shap = 0\ntrain_oof.shape","6fdec939":"xgb_params= {\n        \"objective\": \"reg:squarederror\",\n        \"max_depth\": 4,\n        \"learning_rate\": 0.01,\n        \"colsample_bytree\": 0.4,\n        \"subsample\": 0.8,\n        \"reg_alpha\" : 0.4,\n        \"reg_lambda\" : 2e-08,\n        \"n_jobs\": 4,\n        \"seed\": 2021,\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n        'predictor': 'gpu_predictor'\n    }","77619ee0":"# Convert the Categorical variables to one-hoe encoded features...\n# It will help in the training process\nCATEGORICAL = ['State_Factor', 'building_class', 'facility_type']\ndef create_one_hot(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Creates one_hot encoded fields for the specified categorical columns...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    df = pd.get_dummies(df, columns=CATEGORICAL)\n    return df\n\n\ndef encode_categ_features(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Use the label encoder to encode categorical features...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    le = LabelEncoder()\n    for col in categ_colums:\n        df[col] = le.fit_transform(df[col])\n    return df\n\ntrain = encode_categ_features(train, categ_colums = CATEGORICAL)\ntest = encode_categ_features(test, categ_colums = CATEGORICAL)    ","88693e13":"train.shape,test.shape","9ea9cac2":"test = xgb.DMatrix(test)","b6908b1c":"NUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 25000)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test)\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS      \n        print(mean_squared_error(temp_oof, val_target, squared=False))","ef8b6d75":"mean_squared_error(train_oof, target, squared=False)","670e4cd8":"np.save('train_oof', train_oof)\nnp.save('test_preds', test_preds)\nnp.save('train_oof_shap', train_oof_shap)\nnp.save('test_preds_shap', test_preds_shap)","0b0480df":"train = pd.read_csv('..\/input\/widsdatathon2022\/train.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2022\/test.csv')","dec8e87e":"sub['site_eui'] = test_preds\nsub.to_csv('submission.csv', index=False)","fa299ebb":"sub.head()","204315fc":"In this notebook we'll explore feature importance using SHAP values. SHAP values are the most mathematically consistent way for getting feature importances, and they work particulalry nicely with the tree-based models. Unfortunately, calculating SHAP values is an extremely resource intensive process. However, starting with XGBoost 1.3 it is possible to calcualte these values on GPUs, whcih speeds up the process by a factor of 20X - 50X compared to calculating the same on a CPU. Furthermore, it is also possible to calculate SHAP values for feature interactions. The GPU speedup for those is even more dramatic - it takes a few minutes, as opposed to days or even longer on a CPU.","dbeae251":"## Thanks to up)vote original notebook from @tunguz : https:\/\/www.kaggle.com\/tunguz\/tps-08-21-feature-importance-with-xgboost-and-shap"}}