{"cell_type":{"f9b082ac":"code","4ff157cf":"code","93ba891c":"code","eca5cb88":"code","ea793f19":"code","1c35ae4b":"code","c67d9a6b":"code","539bfd15":"code","35d60deb":"code","27da48e5":"code","7143041f":"code","8016b7d8":"code","7e7e3684":"code","c988eb29":"code","fa131268":"code","7025a052":"code","cec8702c":"code","57ee392f":"code","69c5d5e3":"code","355a58af":"code","529f381c":"code","e3c51b8e":"code","5cb905d5":"code","7987ddca":"code","1612d28a":"code","aec3d785":"code","6ed52463":"code","73b651ac":"markdown","b2afae1a":"markdown","06150e22":"markdown","457d9e2e":"markdown","3266a697":"markdown","ea60070b":"markdown","2a888610":"markdown","98324d61":"markdown","84414bc7":"markdown","680844c5":"markdown"},"source":{"f9b082ac":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport time\n\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression","4ff157cf":"#import data from keras datasets\nfashion_mnist = keras.datasets.fashion_mnist","93ba891c":"#load data\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()","eca5cb88":"#create validation set\n#will train neural net using gradient descent; scale input features (scale the pixel intensities down to the 0-1 range by dividing by 255.0)\n\nX_valid, X_train = X_train_full[:5000] \/ 255.0, X_train_full[5000:] \/ 255.0\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n\nX_test = X_test \/ 255.0","ea793f19":"#create label names\nclass_names = [\"T-shirt\/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]","1c35ae4b":"#show training images with their labels\nfig, ax = plt.subplots(1,10,figsize=(15,15))\n\nfor i in range(10):\n    ax[i].imshow(X_train[i])\n    ax[i].set_title(class_names[y_train[i]])","c67d9a6b":"#build classification MLP (Multilayer perceptron) with two hidden layers\n\n#create sequential model; simplest model for neural networks composed of a single stack of layers connected sequentially; Sequential API\nmodel = keras.models.Sequential()\n\n#build the first layer; Flatten layer which converts each input image into 1D array, computes X.reshape(-1,1); preprocessing layer\nmodel.add(keras.layers.Flatten(input_shape=[28,28]))\n\n#add dense hidden layer with 300 neurons use ReLU activation function\n#each dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\n\n#add second dense layer with 100 neurons, also using ReLU activation function\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\n\n#add dense output layer with 10 neurons (one per class) using the softmax activation function\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\n","539bfd15":"#alternative method; pass a list of layers in the sequential model\nmodel = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28,28]),\n    keras.layers.Dense(300, activation=\"relu\"),\n    keras.layers.Dense(100, activation=\"relu\"),\n    keras.layers.Dense(10, activation=\"softmax\")\n])","35d60deb":"#show model's layers\nmodel.summary()","27da48e5":"#generate image of the model\nkeras.utils.plot_model(model)","7143041f":"#show list of layers in the model\nmodel.layers","8016b7d8":"for i in  model.layers:\n    print(i.name)","7e7e3684":"#show parameters of layers\nweights, biases = hidden1.get_weights()\nweights","c988eb29":"print(weights.shape)\nprint(biases.shape)","fa131268":"#specify the loss function and the optimizer to use\n#use sparse_categorical_crossentropy because we have sparse labels (because we have a target clas index from 0 to 9) and classes are exclusive; we would use categorical_crossentropy if we had one target probability per class\n#\"sgd\" means we train hte model with stochastic gradient descent\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])","7025a052":"#train the model\n#pass input features (X_train) and target class (y_train) as well as number of epoch to train\n#pass validation set (optional)\n#keras will measure the loss and the extra metrics on this set at the end of each epoch, which is useful to see how the model really performs\n#model is probably overfitting if performance on training set is much better on training than on validation set\n\nhistory = model.fit(X_train, y_train, epochs=30,\n                   validation_data=(X_valid, y_valid))","cec8702c":"#show learning curves\n#mean training loss and accuracy measured over each epoch\n#mean validation loss and accuracy measured at the end of each epoch\n\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1) # set the vertical range to [0-1]\nplt.show()","57ee392f":"#use the model to make predictions on test data\ny_proba = model.predict(X_test)\ny_proba.round(2)","69c5d5e3":"#use predict_classes() to find the class with the highest probability\ny_pred = model.predict_classes(X_test)\ny_pred","355a58af":"print(\"Performance Summary of Sequential Neural Network on test data:\")\n\n#show classification report\nprint(metrics.classification_report(y_test, y_pred))\n\n#show confusion matrix\nprint(metrics.confusion_matrix(y_test, y_pred))","529f381c":"#show first 5 test images with their predicted labels and probabilities\nfig, ax = plt.subplots(1,5,figsize=(20,20))\n\nfor i in range(5):\n    ax[i].imshow(X_test[i])\n    ax[i].set_title(\"{} with a probability of {}\".format(class_names[y_pred[i]], str(round(max(y_proba[i]),2))))","e3c51b8e":"#reshape image array to fit to sklearn\nX_train2 = X_train.reshape(55000, 784)\nX_valid2 = X_valid.reshape(5000, 784)\nX_test2 = X_test.reshape(10000, 784)\n\nprint(\"original shape of training data is {}\".format(X_train.shape))\nprint(\"new shape of training data is {}\".format(X_train2.shape))\nprint('\\n')\nprint(\"original shape of validation data is {}\".format(X_valid.shape))\nprint(\"new shape of validation data is {}\".format(X_valid2.shape))\nprint('\\n')\nprint(\"original shape of test data is {}\".format(X_test.shape))\nprint(\"new shape of test data is {}\".format(X_test2.shape))","5cb905d5":"#train, fit, predict model; print performance measurements\ndef create_model(classifier, X_train, X_test, y_train, y_test):\n    \n    #set start time\n    tic = time.time()\n    \n    #create and fit model\n    model = classifier\n    model.fit(X_train, y_train)\n\n    #make predictions\n    y_pred = model.predict(X_test)\n\n    #set end time\n    toc = time.time()\n    \n    #show score\n    score = model.score(X_test, y_test)\n    print('the score is : {}'.format(score))\n    \n    #show classification report\n    print(metrics.classification_report(y_test, y_pred))\n    \n    #show confusion matrix\n    print(metrics.confusion_matrix(y_test, y_pred))\n    \n    print('Elapsed time is %f seconds \\n' % float(toc - tic))","7987ddca":"#train and compare performance to the validation data\ncreate_model(GaussianNB(var_smoothing = 0.001), X_train2, X_valid2, y_train, y_valid)","1612d28a":"#train and compare performance to the test data\ncreate_model(GaussianNB(var_smoothing = 0.001), X_train2, X_test2, y_train, y_test)","aec3d785":"#train and compare performance to the validation data\ncreate_model(LogisticRegression(), X_train2, X_valid2, y_train, y_valid)","6ed52463":"#train and compare performance to the test data\ncreate_model(LogisticRegression(), X_train2, X_test2, y_train, y_test)","73b651ac":"at each epoch during training, Keras displays the number of instances processed so far (along with the progress bar), the mean training time per sample, and the loss and accuracy (or any other extra metrics) on both the training set and hte validation set\n\nthe training loss went down which is a good sign\n\nvalidation accuracy reached 88.96% after 30 epochs which is not too far from the training accuracy, so there does not seem to be much overfitting going on","b2afae1a":"Use model to predict from test data","06150e22":"Gaussian Naive Bayes","457d9e2e":"model estimates one probability per class from class 0 to 9. ","3266a697":"Logistic Regression","ea60070b":"Here we compare the performance of the sequential neural network model vs. other machine learning models","2a888610":"training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease","98324d61":"# Fashion MNIST","84414bc7":"dense layers often have a lot of parameters.\n\nfirst hidden layer has 784 x 300 connection weights + 300 bias terms = 235,500 parameters\n\nthis gives a lot of flexibility to fit the training data, but also means that hte model runs the risk of overfitting, especially when you don't have a lot of training data","680844c5":"## Compare classification performance with other models "}}