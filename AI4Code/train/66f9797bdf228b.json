{"cell_type":{"3ce71401":"code","6421c7b8":"code","9a6d5400":"code","13ae4c29":"code","47b4c198":"code","cb737b12":"code","0bbf50a7":"code","c95a37b8":"code","b9f908f1":"code","ff67777e":"code","7c6d925f":"code","015ec62d":"code","356c0176":"code","21cb9553":"code","bf38b0bd":"code","1c880c7c":"code","1cb42c38":"code","8c5c0432":"code","0ac716f3":"code","97da61ef":"code","eb41567c":"code","e858878b":"code","af6fd168":"markdown","1174a159":"markdown","aae70429":"markdown","d69d00eb":"markdown","5e0ddd34":"markdown","57526b88":"markdown","f6dedcab":"markdown","c17a2014":"markdown","30d3a929":"markdown","37260a6c":"markdown","cd12e246":"markdown","e2628b2a":"markdown","5fb33a35":"markdown","04c05887":"markdown","5ddeed17":"markdown","180f1297":"markdown","dd3602e1":"markdown"},"source":{"3ce71401":"_ = !pip install fuggle","6421c7b8":"from fuggle import setup\nsetup(is_lab=True)","9a6d5400":"from typing import List, Any, Dict, Iterable\n\ndef concat(rows:Iterable[List[Any]]) -> Iterable[List[Any]]:\n    for row in rows:\n        nn = [x for x in row if x is not None]\n        if len(nn)>=2:\n            yield row + [\"\".join(nn)]\n            \nlist(concat([\n    [\"+\",\"-\",\"*\",\"\/\"],\n    [\"+\",None,\"*\",\"\/\"],\n    [\"+\",None,\"*\",None],\n    [None,None,\"*\",None],\n    [None,None,None,None],\n]))","13ae4c29":"import pandas as pd\nimport numpy as np\n\n_CHOICE = list(\"xyz\")+ [None]\n\ndef create_sample(n=10):\n    np.random.seed(0)\n    return pd.DataFrame(dict(\n        a=np.random.choice(_CHOICE, n),\n        b=np.random.choice(_CHOICE, n),\n        c=np.random.choice(_CHOICE, n),\n        d=np.random.choice(_CHOICE, n),\n    ))\n\npd_sample = create_sample(100)\npd_sample","47b4c198":"data = pd_sample.values.tolist()\ndf = pd.DataFrame(concat(data), columns=list(\"abcde\"))\ndf","cb737b12":"from fugue import transform\n\ntransform(pd_sample, concat, schema=\"*,e:str\")","0bbf50a7":"from pyspark.sql import SparkSession\n\nspark_session = SparkSession.builder.getOrCreate()","c95a37b8":"from pyspark.sql import Row\n\ndef wrapper(rows:Iterable[Row]) -> Iterable[List[Any]]:\n    def convert():\n        for row in rows:\n            yield [row.a, row.b, row.c, row.d]\n\n    yield from concat(convert())\n\nsdf = spark_session.createDataFrame(pd_sample)\nmapped = sdf.rdd.mapPartitions(wrapper)\nresult = spark_session.createDataFrame(mapped, \"a string, b string, c string, d string, e string\")\nresult.show()\nprint(result.count())","b9f908f1":"result = transform(sdf, concat, schema=\"*,e:str\", engine=spark_session)\nresult.show()","ff67777e":"result = transform(pd_sample, concat, schema=\"*,e:str\", engine=spark_session)\nresult.show()","7c6d925f":"pd_sample2 = pd_sample.iloc[:, :-1]\npd_sample2","015ec62d":"sdf = spark_session.createDataFrame(pd_sample2)\nmapped = sdf.rdd.mapPartitions(wrapper)\nresult = spark_session.createDataFrame(mapped, \"a string, b string, c string, e string\")\nresult.show()","356c0176":"def wrapper2(rows:Iterable[Row]) -> Iterable[List[Any]]:\n    def convert():\n        for row in rows:\n            yield [row.a, row.b, row.c]\n\n    yield from concat(convert())\n    \nmapped = sdf.rdd.mapPartitions(wrapper2)\nresult = spark_session.createDataFrame(mapped, \"a string, b string, c string, e string\")\nresult.show()","21cb9553":"result = transform(pd_sample2, concat, schema=\"*,e:str\", engine=spark_session)\nresult.show()","bf38b0bd":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\ndef generate_train_test(cat, n=1000, train_size=0.1):\n    def generate():\n        for i in range(len(cat)):\n            x, y = make_classification(n, 5, random_state=i)\n            df = pd.DataFrame(x, columns = list(\"abcde\"))\n            yield df.assign(y=y, cat=cat[i])\n            \n    df = pd.concat(generate())\n    df[\"train\"] = np.random.rand(df.shape[0]) < train_size \n    return df.reset_index(drop=True)\n    \ndata = generate_train_test(list(\"abcd\"))\ntrain = data[data.train]\ntest = data[~data.train]\n\nprint(train.shape, test.shape)\ndata","1c880c7c":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression().fit(train[list(\"abcde\")],train.y)","1cb42c38":"def predict(df: pd.DataFrame, model: LogisticRegression) -> pd.DataFrame:\n    return df.assign(pred=model.predict(df[list(\"abcde\")]))\n\npredict(test, lr)","8c5c0432":"from typing import Iterator, Any, Union\nfrom pyspark.sql.types import StructType, StructField, IntegerType\nfrom pyspark.sql import DataFrame, SparkSession\nimport pyspark.sql.functions as sf\n\ndef predict_wrapper(dfs: Iterator[pd.DataFrame], model):\n    for df in dfs:\n        yield predict(df, model)\n\nsdf = spark_session.createDataFrame(test)\nschema = StructType(list(sdf.schema.fields))\nschema.add(StructField(\"pred\", IntegerType()))\nresult = sdf.mapInPandas(lambda dfs: predict_wrapper(dfs, lr), schema=schema)\nresult.select((sf.sum(result.y*result.pred)\/sf.sum(result.pred)).alias(\"precision\")).show()","0ac716f3":"result = transform(test, predict, schema=\"*,pred:int\", params=dict(model=lr), engine=spark_session)\nresult.select((sf.sum(result.y*result.pred)\/sf.sum(result.pred)).alias(\"precision\")).show()","97da61ef":"import pickle\n\ndef train_cat(df:pd.DataFrame) -> Iterable[Dict[str, Any]]:\n    yield dict(\n        cat = df.cat.iloc[0],\n        model = pickle.dumps(LogisticRegression().fit(df[list(\"abcde\")],df.y))\n    )\n    \ndef predict_cat(model:List[Dict[str,Any]], df:pd.DataFrame) -> pd.DataFrame:\n    assert len(model) == 1\n    m = pickle.loads(model[0][\"model\"])\n    return df.assign(pred=m.predict(df[list(\"abcde\")]))","eb41567c":"from fugue import FugueWorkflow\nfrom fugue.column import col, functions as ff\n\ndag = FugueWorkflow()\nmodels = dag.df(train).partition_by(\"cat\").transform(train_cat, schema=\"cat:str,model:binary\")\nresult = models.zip(dag.df(test)).transform(predict_cat, schema=\"y:int,pred:int\")\nresult.select((ff.sum(col(\"y\")*col(\"pred\"))\/ff.sum(col(\"pred\"))).alias(\"precision\")).show()\n\ndag.run()","e858878b":"%%fsql\ntrain = SELECT * FROM data WHERE train\n\nmodels = \n    TRANSFORM PREPARTITION BY cat\n    USING train_cat SCHEMA cat:str,model:binary\n            \nZIP models, data\nTRANSFORM USING predict_cat SCHEMA y:int,pred:int,train:int\n\nSELECT SUM(y*pred)\/SUM(pred) AS precision GROUP BY train\nPRINT","af6fd168":"### Spark","1174a159":"## Train and predict by category","aae70429":"### Python","d69d00eb":"### Fugue","5e0ddd34":"### Fugue","57526b88":"### Python","f6dedcab":"### Fugue","c17a2014":"## Business requirement changed\n\nThe input will no longer have column d, other things remain unchanged.","30d3a929":"### Fugue SQL","37260a6c":"### Fugue","cd12e246":"### Pandas","e2628b2a":"# Example 1\n\nYou are processing a dataframe, here is the business logic:\n\n1. 4 columns a,b,c,d, all strings\n2. For any given row if the number of non-null values < 2, drop the row\n3. Otherwise, concatenate the non-null columns in order, output in e column, and also keep the original columns\n\n","5fb33a35":"### Fugue","04c05887":"### Python","5ddeed17":"### Spark","180f1297":"### Spark","dd3602e1":"# Example 2\n\nWe have a pair of train and test dataset with partition key cat. We want to train a linear regression model on training set, and do distributed inference on test set."}}