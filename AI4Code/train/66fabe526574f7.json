{"cell_type":{"170271c7":"code","08195ddb":"code","d66586c1":"code","36648d47":"code","9daee979":"code","57c4cf8f":"code","89a5d9eb":"code","dca72fe5":"code","bb2e8b37":"code","346a087f":"code","87c04f3e":"code","c204efd5":"code","3af9ee5d":"code","274e20be":"code","1912016d":"code","b56fdcc9":"code","66ce641c":"code","7fb385ee":"code","902ddcc8":"code","0889e913":"code","186c9c6e":"code","5e69af0b":"code","560af44d":"code","d5ceaa6c":"code","c46d26f4":"code","7c00b9d5":"code","891bc1b7":"code","3b727096":"code","2caa7d17":"code","5e0f0d32":"code","76f3ede3":"code","e759dcf8":"markdown","b5f62630":"markdown","c4ed2385":"markdown","3e8d37ec":"markdown","41010cc5":"markdown","045b3701":"markdown","2ac5216b":"markdown","17366d4c":"markdown","3d4731f3":"markdown","b0efea1b":"markdown","7b0c1861":"markdown","1707b6de":"markdown","18db354a":"markdown","cf5006ca":"markdown","9b03e2db":"markdown","4c7d0439":"markdown","9c7eecea":"markdown","8498cae9":"markdown","668a078e":"markdown","9acf933d":"markdown","db951c2b":"markdown"},"source":{"170271c7":"!pip install sweetviz\n!pip install ppscore","08195ddb":"import pandas as pd\nimport numpy as np\nimport sweetviz as sv\nimport ppscore as pps\nimport seaborn as sns\nfrom pandas import read_csv\nfrom numpy import set_printoptions\nfrom matplotlib import pyplot\n%matplotlib inline\nsns.set_palette('husl')\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import  DecisionTreeClassifier\nfrom sklearn.linear_model import ElasticNet \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings('ignore')","d66586c1":"data = pd.read_csv(\"..\/input\/bankruptcy\/bankruptcy-prevention.csv\",sep=';')","36648d47":"data","9daee979":"data.tail(10)","57c4cf8f":"type(data)\ndata.shape","89a5d9eb":"#data types\ndata.dtypes","dca72fe5":"data.isnull().sum()","bb2e8b37":"data.info()","346a087f":"labelencoder = LabelEncoder()\ndata.iloc[:, -1] = labelencoder.fit_transform(data.iloc[:,-1])","87c04f3e":"data","c204efd5":"\n# Create the default pairplot\nsns.pairplot(data)","3af9ee5d":"#Correlation\n\ndata.corr()","274e20be":"\n# training the model\nclf = IsolationForest(random_state=10,contamination=.01)\nclf.fit(data)\ny_pred_outliers = clf.predict(data)\ny_pred_outliers","1912016d":"data['scores']=clf.decision_function(data)\ndata['anomaly']=clf.predict(data.iloc[:,0:7])\ndata","b56fdcc9":"#print the anomaly\ndata[data['anomaly']==-1]","66ce641c":"data = data.drop(data.index[[27, 72, 192]], axis=0)\ndata.reset_index(drop=True,inplace = True)\ndata","7fb385ee":"data1=data.iloc[0:,:6]\ndata1.hist(figsize=(14,10))\npyplot.show","902ddcc8":"array = data.values\nX = array[:,0:6]\nY = array[:,-3]\n# feature extraction\nmodel = LogisticRegression(max_iter=400)\nrfe = RFE(model, 4)\nfit = rfe.fit(X, Y)","0889e913":"fit.support_","186c9c6e":"fit.ranking_","5e69af0b":"data","560af44d":"data2=data.iloc[:,0:7]","d5ceaa6c":"#calculate the whole PPS matrix\npps.matrix(data2)","c46d26f4":"sweet_report = sv.analyze(data1)\nsweet_report.show_notebook(layout='vertical' , w=880, h=700, scale=0.8)","7c00b9d5":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=1)","891bc1b7":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('SVM', SVC(gamma='auto')))\n# evaluate each model in turn\nresults = []\nmodel_names = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=18, random_state=1, shuffle=True)\ncv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\nresults.append(cv_results)\nmodel_names.append(name)\nprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","3b727096":"model = SVC(gamma='auto')\nmodel.fit(X_train, Y_train)\nprediction = model.predict(X_test)","2caa7d17":"print(f'Test Accuracy: {accuracy_score(Y_test, prediction)}')\nprint(f'Classification Report: \\n {classification_report(Y_test, prediction)}')","5e0f0d32":"e_net = ElasticNet(alpha = .4,l1_ratio=.5) \ne_net.fit(X_train, Y_train)","76f3ede3":"y_pred_elastic = e_net.predict(X_test) \nmean_squared_error = np.mean((y_pred_elastic - Y_test)**2) \nprint(\"Mean Squared Error on test set\", mean_squared_error) \n\ne_net_coeff = pd.DataFrame() \ne_net_coeff[\"Columns\"] = X_train.columns \ne_net_coeff['Coefficient Estimate'] = pd.Series(e_net.coef_) \ne_net_coeff ","e759dcf8":"**The data set includes the following variables:\nindustrial_risk: 0=low risk, 0.5=medium risk, 1=high risk.\nmanagement_risk: 0=low risk, 0.5=medium risk, 1=high risk.\nfinancial flexibility: 0=low flexibility, 0.5=medium flexibility, 1=high flexibility.\ncredibility: 0=low credibility, 0.5=medium credibility, 1=high credibility.\ncompetitiveness: 0=low competitiveness, 0.5=medium competitiveness, 1=high competitiveness.\noperating_risk: 0=low risk, 0.5=medium risk, 1=high risk.\nclass: bankruptcy, non-bankruptcy (target variable).\n**","b5f62630":"**Here is no need to change any datatype as all of them are of float 64**","c4ed2385":"**Recursive Feature Elimination**","3e8d37ec":"**OUTLIER DETECTION USING ISOLATION FOREST**","41010cc5":"**Train Test split Splitting our dataset into train and test using train_test_split(), what we are doing here is taking 80% of data to train our model, and 20% that we will hold back as a validation dataset:**","045b3701":"**Here all the columns **","2ac5216b":"**250rows and 7 columns including one target variable**","17366d4c":"**as we can see that competitiveness ,credibility and financial flexibility are more important with respect to our target variable class**","3d4731f3":"**These are the outliers.\nNow we will drop them and reset the index**","b0efea1b":"**Importing Dataset**","7b0c1861":"**OUTLIER DETECTION AND DATA VISUALIZATION**","1707b6de":"**PPS SCORE**","18db354a":"**Univariate Histogram**","cf5006ca":"**Importing Libraries**","9b03e2db":"**Using Label Encoder at a categorical column to get binary value as its a classification project\n**","4c7d0439":"* Our algoritm can suffer a lot we dont remove the outliers and to avoid this problem we will drop the values**","9c7eecea":"**as we can see all here are non-null values hence we dont require coerce**","8498cae9":"**IN this correlation matrix we can see no columns are correlated to each other neither input variables**","668a078e":"By Looking at this histogram we can say that we have higher risks than credibility and competitiveness","9acf933d":"**We need to have LOW(industrial risk,management risk,operating risk) and HIGH(financial flexibility,credibility,competitiveness) in ordere to avoid Bankruptcy**","db951c2b":"**By above results we can see that column no-0 and 5 less important as compared to other features but as per our domain knowledge we cant drop those columns.**"}}