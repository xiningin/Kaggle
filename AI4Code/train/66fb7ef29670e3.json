{"cell_type":{"38d8defd":"code","5c812332":"code","5df68025":"code","9785984c":"code","4756aa79":"code","e535026a":"code","2e1cf299":"code","7fe1e667":"code","c15e0b66":"code","f33f8e08":"code","3778bcad":"code","5642b1fb":"code","bbd50faa":"code","ee499257":"code","ab88307c":"code","f0a965e6":"code","15f0aab4":"markdown","d8b2e9e9":"markdown","5e637d90":"markdown","b521bcc3":"markdown","4c814c19":"markdown","1aa11354":"markdown"},"source":{"38d8defd":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c812332":"import os\nimport pandas as pd\nimport numpy as np\nimport json\nimport re\nfrom nltk.tokenize import sent_tokenize \nfrom transformers import BertTokenizer, AutoTokenizer\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom tqdm import tqdm\nimport glob\n\nimport warnings\nwarnings.filterwarnings('ignore')","5df68025":"platform = 'Kaggle'\nmodel_name = 'epoch_14_model_sage_bert_base_uncased.bin'\n\nif platform == 'Kaggle':\n    bert_path = '..\/input\/huggingface-bert\/bert-base-uncased'\n    train_path = '\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/*'\n    test_path = '\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/*'\n    model_path = '..\/input\/d\/thanish\/coleridgemodels\/'\n\nconfig = {'MAX_LEN': 128,\n          'tokenizer': AutoTokenizer.from_pretrained(bert_path , do_lower_case=True),\n          'batch_size': 20,\n          'Epoch': 10,\n          'train_path': train_path, \n          'test_path': test_path, \n          'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n          'bert_path': bert_path,\n          'model_path': model_path,\n          'model_name': model_name\n         }","9785984c":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","4756aa79":"# def data_joining(data_dict_id):\n#     '''\n#     This function is to join all the text data from different sections in the json to a single\n#     text file. \n#     '''\n#     data_length = len(data_dict_id)\n\n#     temp = [data_dict_id[i]['text'] for i in range(data_length)]\n# #     temp = [data_dict_id[i]['text'] for i in range(0, 1)]\n#     temp = '. '.join(temp)\n    \n#     return temp\n\n\ndef data_joining(data_dict_id):\n    '''\n    This function is to join all the text data from different sections in the json to a single\n    text file. \n    '''\n\n    sent_list = []\n    for i in range(len(data_dict_id)):\n        text = data_dict_id[i]['text']\n        text = clean_text(text).strip()\n        text = re.sub(' +', ' ', text)\n            \n#         if len(text.split(\" \"))>15: #If the text is greater than 10 words.\n#             temp = [text if any(word in text.lower() for word in ['data', 'study']) else '']\n#             sent_list.append(temp[-1])\n\n        if len(text.split(\" \"))>15: #If the text is greater than 20 words.\n            sent_list.append(text)\n            \n    sent_list = list(set(sent_list))\n    final_sentence = '. '.join(sent_list)\n            \n    return final_sentence","e535026a":"# %%time\n# for id in test_data_dict.keys():\n#     print(id)\n#     print(len(data_joining(test_data_dict[id]).split(\" \")))\n#     print(len(data_joining_2(test_data_dict[id]).split(\" \")))\n    \n    \n# # 8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60\n# # 7540\n# # 7689\n# # 2100032a-7c33-4bff-97ef-690822c43466\n# # 3652\n# # 3128\n# # 2f392438-e215-4169-bebf-21ac4ff253e1\n# # 28673\n# # 23368\n# # 3f316b38-1a24-45a9-8d8c-4e05a42257c6\n# # 10671\n# # 9868\n# # CPU times: user 67.9 ms, sys: 3.32 ms, total: 71.2 ms\n# # Wall time: 69.4 ms","2e1cf299":"def read_test_json(test_data_folder):\n    '''\n    This function reads all the json input files and return a dictionary containing the id as the key\n    and all the contents of the json as values\n    '''\n\n    test_text_data = {}\n    total_files = len(glob.glob(test_data_folder))\n    \n    for i, test_json_loc in enumerate(glob.glob(test_data_folder)):\n        filename = test_json_loc.split(\"\/\")[-1][:-5]\n\n        with open(test_json_loc, 'r') as f:\n            test_text_data[filename] = json.load(f)\n\n        if (i%1000) == 0:\n            print(f\"Completed {i}\/{total_files}\")\n\n    print(\"All files read\")\n    return test_text_data","7fe1e667":"test_data_dict = read_test_json(test_data_folder=config['test_path'])","c15e0b66":"# initializing the model\nmodel = transformers.BertForTokenClassification.from_pretrained(config['bert_path'],  num_labels = 3)\nmodel = nn.DataParallel(model)\n\n# Reading the trained checkpoint model\ntrained_model_name = config['model_path'] + config['model_name']\nprint(\"Trained model checkpoint:\", trained_model_name)\ncheckpoint = torch.load(trained_model_name, map_location = config['device'])\nprint(\"Checkpoint loaded\")\n\n# Matching the trained checkpoint model to the initialized model\nmodel.load_state_dict(checkpoint)\nprint(\"Model loaded with all keys matching with the checkpoint\")","f33f8e08":"# Prediction\ndef prediction_fn(tokenized_sub_sentence):\n\n    tkns = tokenized_sub_sentence\n    indexed_tokens = config['tokenizer'].convert_tokens_to_ids(tkns)\n    segments_ids = [0] * len(indexed_tokens)\n\n    tokens_tensor = torch.tensor([indexed_tokens]).to(config['device'])\n    segments_tensors = torch.tensor([segments_ids]).to(config['device'])\n    \n    model.eval()\n    with torch.no_grad():\n        logit = model(tokens_tensor, \n                      token_type_ids=None,\n                      attention_mask=segments_tensors)\n\n        logit_new = logit[0].argmax(2).detach().cpu().numpy().tolist()\n        prediction = logit_new[0]\n\n#         print(tkns)\n#         print(prediction)\n        \n        kword = ''\n        kword_list = []\n\n        for k, j in enumerate(prediction):\n            if (len(prediction)>1):\n\n                if (j!=0) & (k==0):\n                    #if it's the first word in the first position\n                    #print('At begin first word')\n                    begin = tkns[k]\n                    kword = begin\n\n                elif (j!=0) & (k>=1) & (prediction[k-1]==0):\n                    #begin word is in the middle of the sentence\n                    begin = tkns[k]\n                    previous = tkns[k-1]\n\n                    if begin.startswith('##'):\n                        kword = previous + begin[2:]\n                    else:\n                        kword = begin\n\n                    if k == (len(prediction) - 1):\n                        #print('begin and end word is the last word of the sentence')\n                        kword_list.append(kword.rstrip().lstrip())\n\n                elif (j!=0) & (k>=1) & (prediction[k-1]!=0):\n                    # intermediate word of the same keyword\n                    inter = tkns[k]\n\n                    if inter.startswith('##'):\n                        kword = kword + \"\" + inter[2:]\n                    else:\n                        kword = kword + \" \" + inter\n\n\n                    if k == (len(prediction) - 1):\n                        #print('begin and end')\n                        kword_list.append(kword.rstrip().lstrip())\n\n                elif (j==0) & (k>=1) & (prediction[k-1] !=0):\n                    # End of a keywords but not end of sentence.\n                    kword_list.append(kword.rstrip().lstrip())\n                    kword = ''\n                    inter = ''\n            else:\n                if (j!=0):\n                    begin = tkns[k]\n                    kword = begin\n                    kword_list.append(kword.rstrip().lstrip())\n#         print(kword_list)\n#         print(\"\")\n    return kword_list\n","3778bcad":"def long_sent_split(long_tokens):\n    '''\n    If the token length is >the max length this function splits it into mutiple list of specified smaller max_length\n    '''\n    \n    start = 0\n    end = len(long_tokens)\n    max_length = 64\n\n    final_long_tok_split = []\n    for i in range(start, end, max_length):\n        temp = long_tokens[i: (i + max_length)]\n        final_long_tok_split.append(temp)\n    return final_long_tok_split","5642b1fb":"def get_predictions(data_dict):\n    \n    results = {}\n\n    for i, Id in enumerate(data_dict.keys()):\n        current_id_predictions = []\n        \n#         print(Id)\n        sentences = data_joining(data_dict[Id])\n        sentence_tokens = sent_tokenize(sentences)\n        \n        for sub_sentence in sentence_tokens:\n            cleaned_sub_sentence = clean_text(sub_sentence)\n        \n            # Tokenize the sentence\n            tokenized_sub_sentence = config['tokenizer'].tokenize(cleaned_sub_sentence)\n            \n            if len(tokenized_sub_sentence) == 0:\n                # If the tokenized sentence are empty\n                sub_sentence_prediction_kword_list = []\n                \n            elif len(tokenized_sub_sentence) <= 512:\n                # If the tokenized sentence are less than 512\n                sub_sentence_prediction_kword_list = prediction_fn(tokenized_sub_sentence)\n\n            else:\n                # If the tokenized sentence are >512 which is long sentences\n                long_sent_kword_list = []\n                \n                tokenized_sub_sentence_tok_split = long_sent_split(tokenized_sub_sentence)\n                for i, sent_tok in enumerate(tokenized_sub_sentence_tok_split):\n                    if len(sent_tok) != 0:\n                        kword_list = prediction_fn(sent_tok)\n                        long_sent_kword_list.append(kword_list)\n                flat_long_sent_kword = [item for sublist in long_sent_kword_list for item in sublist]\n                sub_sentence_prediction_kword_list = flat_long_sent_kword\n                            \n            if len(sub_sentence_prediction_kword_list) !=0:\n                current_id_predictions = current_id_predictions + sub_sentence_prediction_kword_list\n\n        results[Id] = list(set(current_id_predictions))\n                \n    print(\"All predictions completed\")\n    \n    return results","bbd50faa":"%%time\nresults = get_predictions(data_dict = test_data_dict)\n# results","ee499257":"def remove_few_word_prediction(prediction_dict):\n    final_result = {}\n    for ID in prediction_dict.keys():\n        temp = []\n\n        for pred in prediction_dict[ID]:\n            pred_split = pred.split(\" \")\n#             print(ID, pred_split)\n            condition1 = len(pred_split)<=2\n            condition2 = 'adni' not in pred\n            condition3 = 'cccsl' not in pred\n            condition4 = 'ibtracs' not in pred\n            condition5 = 'slosh model' not in pred\n            \n            if condition1 & condition2 & condition3 & condition4 & condition5:\n                pass\n            else:\n                temp.append(pred)\n        final_result[ID] = temp\n        \n    return final_result\n\nresults = remove_few_word_prediction(prediction_dict=results)\nresults","ab88307c":"sub_df = pd.DataFrame({'Id': list(results.keys()),\n                       'PredictionString': list(results.values())})\nsub_df.PredictionString = sub_df.PredictionString.apply(lambda x : \"|\".join(x))\nsub_df","f0a965e6":"sub_df.to_csv(\"submission.csv\", index=False)","15f0aab4":"# ------------------------------------------- Consider upvoting if you like it :) ------------------------------------------- ","d8b2e9e9":"# Reading the test data","5e637d90":"### This notebook is the continuation from my Notebook on [Bert for Token Classification - Training](http:\/\/https:\/\/www.kaggle.com\/thanish\/bert-for-token-classification-training). \nPlease do check it out for the training code","b521bcc3":"# Config ","4c814c19":"# Prediction function","1aa11354":"# Loading the saved model"}}