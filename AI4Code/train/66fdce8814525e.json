{"cell_type":{"e006891b":"code","69e9c64f":"code","4985a0dd":"code","6264ee47":"code","ac17863a":"code","31028058":"code","7facdd6a":"code","54528f5f":"code","6c658a03":"code","1de5c8d4":"code","a90cc2b9":"code","44d4011e":"code","ba5c24fd":"code","8bba8e83":"code","d9bd5ce6":"code","b086d160":"code","fa80411b":"code","89de456c":"code","087b6e45":"code","4c8e959c":"markdown","9ef328b5":"markdown","770d8054":"markdown","71cb41d3":"markdown","bb899221":"markdown","17ee6895":"markdown","4454df4e":"markdown","174e1925":"markdown","2f755c8d":"markdown","c4651fb5":"markdown","b511e97d":"markdown","d90ef5c9":"markdown","39ac935b":"markdown","e0e9c5d2":"markdown","edead01e":"markdown","e76af3e7":"markdown","dc588ad0":"markdown","5fe8a067":"markdown","709f71b0":"markdown","8a7b40c8":"markdown","adea9e63":"markdown","58e857a1":"markdown","31560fcf":"markdown","8bb9f4a6":"markdown","f73b8f2b":"markdown","6d7a4326":"markdown"},"source":{"e006891b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","69e9c64f":"# First we need to load the data into a dataframe\n\ndf = pd.read_csv(\"..\/input\/train.csv\")","4985a0dd":"df.head()","6264ee47":"# Lets also have a look at the test data\n\ntdf = pd.read_csv(\"..\/input\/test.csv\")\nprint(tdf.head())\ndel(tdf)","ac17863a":"# Let's first have a look at some of the comments whose scores were above 0.0\n\nrandom_indices = np.random.choice([i for i in range(len(df)) if df[\"target\"][i] > 0.], 5)\nfor i in random_indices:\n    print(\"Text: \", df[\"comment_text\"][i])\n    print(\"Score: \", df[\"target\"][i])","31028058":"# Now let's have a look at completely non toxic comments\n\nrandom_indices = np.random.choice([i for i in range(len(df)) if df[\"target\"][i] == 0.], 5)\nfor i in random_indices:\n    print(\"Text: \", df[\"comment_text\"][i])\n    print(\"Score: \", df[\"target\"][i])","7facdd6a":"# Now lets have a look at very toxic comments: target > 0.75\n\nrandom_indices = np.random.choice([i for i in range(len(df)) if df[\"target\"][i] >= .75], 5)\nfor i in random_indices:\n    print(\"Text: \", df[\"comment_text\"][i])\n    print(\"Score: \", df[\"target\"][i])","54528f5f":"import matplotlib.pyplot as plt\nplt.figure(figsize=(20, 5))\nplt.hist(df['target'], bins = 100)\nplt.show()","6c658a03":"plt.figure(figsize=(20, 5))\nplt.hist(df[df['target'] > 0.0]['target'], bins = 100)\nplt.show()","1de5c8d4":"tdf = df.loc[:30000, [\"comment_text\", \"target\"]]\ntdf.head()","a90cc2b9":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(tdf[\"comment_text\"], tdf[\"target\"], test_size = .10)","44d4011e":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","ba5c24fd":"cvect = CountVectorizer(min_df = 0.1, ngram_range=(1, 3), analyzer=\"word\").fit(X_train)\nX_trcv = cvect.transform(X_train)\nX_tscv = cvect.transform(X_test)\n\n# In order to convert the coninuous values of the target to binary, as logistic regression can accept only binary values (0 or 1) as the target values\n# Here we choose 0.1 as a cutoff as we want to classify even slightly toxic comments as toxic\ny_train_lg = np.array(y_train > 0.1, dtype=np.float)\ny_test_lg = np.array(y_test > 0.1, dtype=np.float)\n\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression().fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","8bba8e83":"from sklearn.dummy import DummyClassifier\n\ndclf = DummyClassifier(strategy=\"most_frequent\").fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(dclf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(dclf.score(X_tscv, y_test_lg)))\npredicted = dclf.predict(X_tscv)\n\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","d9bd5ce6":"from sklearn.naive_bayes import BernoulliNB\n\nclf = BernoulliNB().fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","b086d160":"from sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB().fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","fa80411b":"tfvect = TfidfVectorizer().fit(X_train)\n\nX_trtf = tfvect.transform(X_train)\nX_tstf = tfvect.transform(X_test)\n\nclf = MultinomialNB().fit(X_trtf, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trtf, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tstf, y_test_lg)))\npredicted = clf.predict(X_tstf)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","89de456c":"from sklearn.svm import SVC\n\nclf = SVC(kernel=\"linear\").fit(X_trtf, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trtf, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tstf, y_test_lg)))\npredicted = clf.predict(X_tstf)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","087b6e45":"clf = SVC(kernel=\"linear\").fit(X_trcv, y_train_lg)\nprint(\"Training Accuracy: {}\".format(clf.score(X_trcv, y_train_lg)))\nprint(\"Testing Accuracy: {}\".format(clf.score(X_tscv, y_test_lg)))\npredicted = clf.predict(X_tscv)\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"Test Precision: {}\".format(precision_score(y_test_lg, predicted)))\nprint(\"Test Recall: {}\".format(recall_score(y_test_lg, predicted)))","4c8e959c":"Again the above classifier underperformed. It is almost always impossible to say which classifier will give the best results. Its better to try different classifiers and go with the one with the best performance.","9ef328b5":"### Lets have a look at some random comment texts and their labels to better understand the bias mentioned in the competition description","770d8054":"### Non toxic comments","71cb41d3":"#### SVM Classifier with a linear kernel and TFIDF vectors","bb899221":"### Improvement\nWe can see that the precision and recall scores have gone up, but the total accuracy score has gone down.\nNext we will run:\n#### Multinomial NB with TFIDF and Count Vector features","17ee6895":"#### Training and Testing Data","4454df4e":"### Performance:\nTherefore, we see that our classifier is not much better than a simple baseline model which just predicts all outputs to be the most frequent class.\nTherefore, we need better models. Hence we will explore other models better suited for text classification purposes viz Naive Bayes Classifier and Support Vector Machines","174e1925":"So there are very few comments with high scores, therefore, we should count all the comments with low scores (e.g., target > 0.10) as toxic","2f755c8d":"### Dummy Classifier\nWe can see that the accuracy scores are not that bad. However, the precision and recall scores are just unacceptable. This is because of the imbalanced targets, as most of the targets have label 0 and few have label 1. So even a dumb classifier which always predicts the most common class would give a respectable accuracy score. So we need to compare our classifier's performance with once such most-common-class-classifier. Lets see how to do that...","c4651fb5":"### End\nUnfortunately I have to end this notebook here. However, if this is helpful, please comment. I will do further analysis using simple machine learning algorithms and share with you.\n\nWithin a few weeks I will publish a notebook on how to train an LSTM Classifier for this task. Stay tuned.","b511e97d":"## Feature Extraction\n\n#### First I am going to import Count Vectorizer and TFIDF Vectorizers which are used to convert the texts into feature vector forms which can be used as by the machine learning algorithm. \nFor more information please visit https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\nand https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html\n\nThese are used to convert the text data from strings which cannot be used directly by the machine learning algorithms into floats which can be used as features.\nIn short these algorithms count the occurence of every word token in the texts and convert the word scores for each text into a float. For more details refer to the abovementioned links.","d90ef5c9":"#### SVM Classifier with linear kernel and Count Vectors\nNow lets try the last classifier on our list. ","39ac935b":"### Severely toxic comments","e0e9c5d2":"#### Bernoulli Naive Bayes using Count Vectors as features","edead01e":"So the test data is composed of only the comments, and our algorithm has to predict the toxicity score of the text between 0 and 1, o being the lowest and 1 being the highest.","e76af3e7":"## This notebook is primarily for exploration purposes and training simple predictive models on the data. \n### PS: You wont find any high end DNNs like LSTMs or GRUs in this notebook\n\nThis notebook is for people who are just starting out in data sciences and text mining. Although Deep Learning models such as LSTMs are currently dominating the NLP landscape, it might be difficult for beginners to get a grasp over them without understanding the basics of text mining, also beginners wont totally appreciate the modern NN based architectures if they are not fully aware of the different classical machine learning methodologies and their pitfalls.\n\nIn this notebook, I am going to explain in simple steps how to go about exploring text data and how to make simple models like simple Logistic regression, SVM and Naiive Bayes Classifiers.","dc588ad0":"It seems that the majority of the comments are non toxic (score == 0.0). Therefore, we sould have a look at the toxic ones separately","5fe8a067":"## Logistic Regression with Count Vectoriser","709f71b0":"### Neutral\/ slightly toxic comments","8a7b40c8":"#### Train Test Split","adea9e63":"### Now lets have a look at the distribution of the target","58e857a1":"## Making the model\nNow we should start making a model to predict the target in unseen texts","31560fcf":"### Improvement:\nIn this model the precision and accuracy have improved, however, the recall is very low. Lets now play around with SVM Models, which are also used very often in text classification","8bb9f4a6":"## Model\nNow I will build a simple model for binary text classification. The two classes being toxic and non toxic.\nI will explore different classic machine learning algorithms for this purpose. \nHowever I wont go into RNNs in this notebook. Very soon I will release another notebook with LSTMs, word embeddings etc.","f73b8f2b":"The precision and recall scores again went down. But the accuracy increased and became equal to the Dummy classifier\nLets use TFIDF features now","6d7a4326":"### Major Improvement\nThe above classifier has major improvements in terms of test accuracy, precision and recall."}}