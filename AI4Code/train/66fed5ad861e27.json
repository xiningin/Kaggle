{"cell_type":{"2800bb61":"code","6735c8cf":"code","5fed44c3":"code","725e2865":"code","541751b9":"code","73eb684e":"code","a6e163f6":"code","7c7d1dc3":"code","bea72ef2":"code","8b7c57d1":"code","265952ca":"markdown","686c3e2b":"markdown","9a2924ca":"markdown","bb38fc2a":"markdown","2f1c6aed":"markdown","ac655ce9":"markdown","c9520c5a":"markdown"},"source":{"2800bb61":"! pip install tensorflow-gpu==2.0.0-rc","6735c8cf":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import get_file\nimport numpy as np","5fed44c3":"# get data\n\n(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\n\nprint(train_data[0], train_labels[0])\nprint('Number of training instances: {0}, number of testing instances: {1}'.format(train_data.shape[0], test_data.shape[0]))","725e2865":"# get vocab\nword_to_id = keras.datasets.imdb.get_word_index()\nindex_from=3\nword_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nword_to_id[\"<UNK>\"] = 2\nid_to_word = {value:key for key,value in word_to_id.items()}\n\nprint(' '.join([id_to_word[i] for i in train_data[0]]))","541751b9":"train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_to_id[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_to_id[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)","73eb684e":"train_data.shape","a6e163f6":"# Problem: Use tf.data to implement input pipeline\n\n# placeholder for implementing dataset\ndef create_dataset_from_tensor_slices(X, y):\n    return tf.data.Dataset.from_tensor_slices((np.array(X), np.array(y)))\n\ndef create_dataset_from_generator(X, y):\n    def create_gen():\n        for single_x, single_y in zip(X, y):\n            yield (single_x, single_y)\n    output_types = (tf.int32, tf.int32)\n    output_shapes = ([256], [])\n    return tf.data.Dataset.from_generator(create_gen, output_types=output_types, output_shapes=output_shapes)\n\ndef create_dataset_tfrecord(X, y, mode='train'):\n    file_name = '{0}.tfrecord'.format(mode)\n    \n    # serialize features\n    # WARNING: DO NOT WRITE MULTITPLE TIMES IN PRACTICE!!! IT'S SLOW!!!\n    def _int64_list_feature(value):\n        \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n    def _int64_feature(value):\n        \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n    def serialize_fn(single_x, single_y):\n        feature_tuples = {'feature': _int64_list_feature(single_x), 'label': _int64_feature(single_y)}\n        example_proto = tf.train.Example(\n            features=tf.train.Features(feature=feature_tuples))\n        return example_proto.SerializeToString()\n    # write to file\n    with tf.io.TFRecordWriter(file_name) as writer:\n        for single_x, single_y in zip(X, y):\n            example = serialize_fn(single_x, single_y)\n            writer.write(example)\n            \n    # read file\n    dataset = tf.data.TFRecordDataset(file_name)\n    def parse_fn(example_proto):\n        feature_description = {'feature': tf.io.FixedLenFeature([256], tf.int64), 'label': tf.io.FixedLenFeature([], tf.int64)}\n        feature_tuple = tf.io.parse_single_example(\n            example_proto, feature_description)\n        return feature_tuple['feature'], feature_tuple['label']\n    dataset = dataset.map(parse_fn)\n    return dataset\n\n# train_dataset = create_dataset_from_generator(train_data, train_labels)\n# test_dataset = create_dataset_from_generator(test_data, test_labels)\n\ntrain_dataset = create_dataset_tfrecord(train_data, train_labels)\ntest_dataset = create_dataset_tfrecord(test_data, test_labels, mode='test')\n\ntrain_dataset = train_dataset.shuffle(10000).batch(256).prefetch(100).repeat()\ntest_dataset = test_dataset.batch(256).prefetch(100)\n    ","7c7d1dc3":"# Problem: Implement a custom keras layer which has the identical effects of dense, but print the mean\n#   of the variables if the mean value is greater than zero. Print for maximum 10 times.\n\n# placeholder for implementing using Functional API or Model Subclassing\nclass WeirdDense(tf.keras.layers.Layer):\n\n    def __init__(self, output_dim, activation):\n        super(WeirdDense, self).__init__()\n        self.output_dim = output_dim\n        self.activation = activation\n        self.print_times = tf.Variable(0, dtype=tf.int32, trainable=False)\n        \n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.w = self.add_weight(shape=(input_shape[-1], self.output_dim),\n                                 initializer='random_normal',\n                                 trainable=True)\n        self.b = self.add_weight(shape=(self.output_dim,),\n                                 initializer='random_normal',\n                                 trainable=True)\n    @tf.function\n    def call(self, x):\n        mean_val = tf.reduce_mean(self.w)\n        if tf.greater(mean_val, 0):\n            if tf.less_equal(self.print_times, 10):\n                tf.print(mean_val)\n                self.print_times.assign_add(1)\n\n        return_tensor = self.activation(tf.matmul(x, self.w) + self.b)\n        return return_tensor\n            \n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)","bea72ef2":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(WeirdDense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()","8b7c57d1":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(train_dataset,\n                    epochs=1,\n                    steps_per_epoch=100,\n                    validation_data=test_dataset,\n                    validation_steps=100,\n                    verbose=1)","265952ca":"# \u6784\u5efa\u6a21\u578b\n\n\u4e0b\u9762\u5c31\u662f\u6fc0\u52a8\u4eba\u5fc3\u7684\u65f6\u5019\u4e86: \u5199\u4e00\u4e2a\u6587\u672c\u5206\u7c7b\u6a21\u578b!\n\n\u4f60\u9700\u8981\u6539\u5199\u4e00\u4e0b\u4e0b\u9762\u7684\u6a21\u578b,\u8ba9\u5176\u51c6\u786e\u7387\u66f4\u9ad8\n\n\u4f60\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528Dropout, CudnnGRU\u7b49\u66f4\u52a0fancy\u7684\u65b9\u6cd5","686c3e2b":"\u5982\u679c\u8981\u4f7f\u7528Keras\u63d0\u4f9b\u7684\u8bad\u7ec3\u3001\u9884\u6d4bAPI, \u4f60\u9700\u8981\u5148compile\u6a21\u578b, \u7136\u540e\u8c03\u7528\u8be5API","9a2924ca":"# Colab Practice\n\u8fd9\u4e2anotebook\u5927\u90e8\u5206\u4ee3\u7801\u6765\u81ea: https:\/\/www.tensorflow.org\/tutorials\/keras\/basic_text_classification","bb38fc2a":"\u63a5\u4e0b\u6765\u6211\u4eec\u8981\u5bf9\u8f93\u5165\u8fdb\u884c\u8865\u5168(padding)\n\n\u8865\u5168\u4f1a\u5bfc\u81f4\u4e00\u90e8\u5206\u65e0\u7528\u8ba1\u7b97, \u4f46\u662f\u66f4\u52a0\u65b9\u4fbf\u5904\u7406(\u601d\u8003\u9898: \u600e\u6837\u51cf\u5c11\u65e0\u7528\u8ba1\u7b97?)","2f1c6aed":"# import\u76f8\u5173\u5e93\n","ac655ce9":"# \u4e0b\u8f7dIMDB\u7535\u5f71\u8bc4\u8bba\u6570\u636e","c9520c5a":"\u6211\u4eec\u770b\u5230\u7684\u662f\u8bcd\u7d22\u5f15, \u60f3\u8981\u770b\u5230\u539f\u672c\u7684\u8bcd\u9700\u8981\u7528\u8bcd\u8868\u627e\u56de\u539f\u6765\u7684\u8bcd\u8bed"}}