{"cell_type":{"3794be28":"code","1600a427":"code","5136e16f":"code","c036ac4e":"code","ff46516b":"code","d2b23d78":"code","5647e5f2":"code","2c1f10ef":"code","2f9ff1c9":"code","569fe13d":"code","16a82b5a":"code","752d4eaa":"code","3852025e":"code","4137321d":"code","0e608fa4":"code","ac060b1e":"code","0bf633df":"code","ea723a02":"code","c6eae369":"code","8fafe6e1":"code","e94b9ac1":"code","4d37c754":"code","f73a583a":"code","c6c8b15c":"code","83c7ce6a":"code","a174f20f":"code","25dcb61b":"code","6a539e62":"code","5ef98662":"code","006f0cae":"code","b1b8dc3d":"code","6d3a7286":"code","070c3e53":"code","af4cddb9":"code","854801db":"code","47f46600":"code","7e18dea1":"code","3d531cf2":"code","d9b9fd02":"code","226dd1cf":"code","46d65eaf":"code","d06bfd75":"code","ff5a0462":"code","ee64b5cf":"code","e3fcbd0e":"code","66e6e05b":"code","40c7bceb":"markdown","944716a9":"markdown","a9e50f61":"markdown","28d3bb1b":"markdown"},"source":{"3794be28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1600a427":"import tensorflow as tf","5136e16f":"#Data Exploration\nfilepath = '\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv'\ndf = pd.read_csv(filepath, nrows = 1000)","c036ac4e":"df.head()","ff46516b":"df.isna().sum()","d2b23d78":"len(df)","5647e5f2":"ds_raw = tf.data.experimental.make_csv_dataset(filepath, batch_size = 5, label_name = 'status', num_epochs = 1, shuffle = False)","2c1f10ef":"for features, labels in ds_raw.take(1):\n    for key, values in features.items():\n        print(f'{key:20}: {values}')\n    print(labels.numpy())","2f9ff1c9":"#First let's encode the status label\nds = ds_raw.map(lambda features, label: (features, tf.map_fn(lambda x: 1 if x == 'Placed' else 0, label, dtype = tf.int32)))","569fe13d":"for features, labels in ds.take(1):\n    for key, values in features.items():\n        print(f'{key:20}: {values}')\n    print(labels.numpy())","16a82b5a":"#Before we go into the next step, we would like to deal with NaN values, and only salary feature has lots of NaNs. \n#All NaNs were loaded as 0 by default in tf.data.experimental.make_csv_dataset\n#we would just drop salary feature because of the amount of NaNs","752d4eaa":"def popping(input_dict, feature_name):\n    input_dict.pop(feature_name)\n    return input_dict","3852025e":"ds = ds.map(lambda features, label: (popping(features, 'salary'), label))\nds = ds.map(lambda features, label: (popping(features, 'sl_no'), label)) #drop Serial Number, too","4137321d":"#Next we would like to get the numeric columns apart from categorical columns\ncolumn_dtypes = {}\nfor key, value in ds.element_spec[0].items():\n    column_dtypes[key] = value.dtype","0e608fa4":"column_dtypes","ac060b1e":"numeric_features = [column for column, dtype in column_dtypes.items() if dtype in [tf.int32, tf.float32]]\ncategorical_features = [column for column, dtype in column_dtypes.items() if dtype == tf.string]","0bf633df":"#For numeric features, we need to standardize, here we need to go back to the original Pandas dataframe and grab the mean and std\nmetadata = df.drop(['salary', 'sl_no'], axis = 1).describe().T\nmetadata","ea723a02":"#Next we pack all the numeric features\nclass PackNumericFeatures():\n    def __init__(self, names):\n        self.names = names\n        \n    def __call__(self, features, label):\n        numeric = [features.pop(name) for name in self.names]\n        numeric = [tf.cast(item, tf.float32) for item in numeric]\n        numeric = tf.stack(numeric, -1)\n        features['numeric'] = numeric\n        \n        return features, label","c6eae369":"ds = ds.map(PackNumericFeatures(numeric_features))","8fafe6e1":"for features, labels in ds.take(1):\n    for key, values in features.items():\n        print(f'{key:20}: {values}')\n    print(labels.numpy())","e94b9ac1":"#Now we can actually perform train test split\nprint(f'There are {len(df)} records.')\n\n#The reason we only take 40 below is because each batch contains 5 records\nds_train = ds.take(40)\nds_test = ds.skip(40)","4d37c754":"ds_test.reduce(0, lambda x, _: x + 1).numpy()","f73a583a":"#Next, we need a Standardization function for the numeric columns\ndef Standardize(tensor, mean, std):\n    return (tensor - mean)\/std\n\nmean = metadata['mean'].to_numpy()\nstd = metadata['std'].to_numpy()","c6c8b15c":"mean","83c7ce6a":"import functools\n\nStandardize_partial = functools.partial(Standardize, mean = mean, std = std)","a174f20f":"#for numeric columns\nnumeric_column = tf.feature_column.numeric_column('numeric', shape = (len(numeric_features), ), normalizer_fn = Standardize_partial)\nnumeric_columns = [numeric_column]","25dcb61b":"#for categorical columns\nprint(categorical_features)\n\ncategories = {'gender': ['M', 'F'],\n             'ssc_b': ['Central', 'Others'],\n              'hsc_b': ['Central', 'Others'],\n              'hsc_s': ['Commerce', 'Science', 'Arts'],\n              'degree_t': ['Comm&Mgmt', 'Sci&Tech', 'Others'],\n              'workex': ['Yes', 'No'],\n              'specialisation': ['Mkt&Fin', 'Mkt&HR']\n             }\n\ncategorical_columns = [tf.feature_column.categorical_column_with_vocabulary_list(key, values) for key, values in categories.items()]\ncategorical_columns = [tf.feature_column.indicator_column(column) for column in categorical_columns]","6a539e62":"feature_columns = numeric_columns + categorical_columns","5ef98662":"estimator = tf.estimator.DNNClassifier(hidden_units = [32, 32, 32], feature_columns = feature_columns, model_dir = '\/estimator', n_classes = 2, activation_fn = 'relu')","006f0cae":"def input_fn(ds_train):\n    return ds_train.unbatch().shuffle(1000).batch(5).repeat()","b1b8dc3d":"#estimator.train(input_fn = lambda: input_fn(ds_train)) #does not work","6d3a7286":"#So we probably need to create the EagerTensor within the function, so let's try the alternative to define a function that creates the EagerTensor starting from the beginning\ndef input_fn(numeric_features, train = True, batch_size = 5, num_epochs = 1, shuffle = False, steps = 40):\n    ds_raw = tf.data.experimental.make_csv_dataset(filepath, batch_size = batch_size, label_name = 'status', num_epochs = num_epochs, shuffle = shuffle)\n    ds = ds_raw.map(lambda features, label: (features, tf.map_fn(lambda x: 1 if x == 'Placed' else 0, label, dtype = tf.int32)))\n    \n    ds = ds.map(lambda features, label: (popping(features, 'salary'), label))\n    ds = ds.map(lambda features, label: (popping(features, 'sl_no'), label)) #drop Serial Number, too\n    \n    ds = ds.map(PackNumericFeatures(numeric_features))\n    \n    if train:\n        ds = ds.take(steps)\n        return ds.unbatch().shuffle(1000).batch(batch_size).repeat()        \n    else:\n        ds = ds.skip(steps)\n        return ds.unbatch().shuffle(1000).batch(batch_size)\n\n","070c3e53":"estimator.train(input_fn = lambda: input_fn(numeric_features, train = True), steps = 400)","af4cddb9":"eval_result = estimator.evaluate(input_fn = lambda: input_fn(numeric_features, train = False))","854801db":"eval_result","47f46600":"#The accuracy of 1.0 seems too good to be true...","7e18dea1":"input_layer = tf.keras.layers.DenseFeatures(feature_columns)","3d531cf2":"model = tf.keras.Sequential()\n\nmodel.add(input_layer)\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(32, activation = 'relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])","d9b9fd02":"input_train = ds_train.unbatch().shuffle(1000).batch(5)\nmodel.fit(ds_train, epochs = 20)","226dd1cf":"eval_results2 = model.evaluate(ds_test)","46d65eaf":"#So this time we have a lower test accuracy, we can still predict the result\npredictions = model.predict(ds_test)","d06bfd75":"def Sigmoid(x):\n    return 1\/(1+np.exp(-x))","ff5a0462":"Sigmoid(20)","ee64b5cf":"pred = predictions.ravel()\npred_prob = Sigmoid(pred)","e3fcbd0e":"actual = np.array([])\nfor batch in ds_test:\n    actual = np.append(actual, batch[1].numpy())","66e6e05b":"for p, a in zip(pred_prob, actual):\n    print(f'The predicted Placement probability is {p:.2%}, the actual result of the placement is {\"Placed\" if a == 1 else \"Not Placed\"}.')","40c7bceb":"# We would like to predict Placement","944716a9":"# Next we would train the model","a9e50f61":"# Manually building a Keras Model","28d3bb1b":"# Next we would build feature columns"}}