{"cell_type":{"828ca277":"code","43034d48":"code","9831c850":"code","c597c73b":"code","cf501fc3":"code","21a58f12":"code","3c7f2f60":"code","8f29ae7b":"code","0ddb0d86":"code","af0c1208":"code","374acfad":"code","4fc96b63":"code","c230ac6a":"code","16af8ee5":"code","70137423":"code","1eb38704":"code","0d412f85":"code","8aaf2841":"code","1724a3e8":"code","370ccd5b":"code","7dab947a":"code","83caae86":"code","27e51381":"code","0758ebf6":"code","64f0922d":"code","a2dd317e":"code","b05c6409":"code","3ebb3d8d":"code","b1f53a55":"code","4df690b0":"code","7be93692":"code","35f47065":"code","613eaee0":"code","88c6ba98":"code","0936849f":"code","f99b8512":"code","64133d11":"code","3fa81354":"code","9fa7a457":"markdown","4b2280a0":"markdown","2009f0a5":"markdown","a159d548":"markdown","eb3513be":"markdown","f0543b0a":"markdown","2311a03f":"markdown","31b820cc":"markdown","51d2ad9d":"markdown","1498af1b":"markdown","8ba5d06b":"markdown","b1321f97":"markdown","a475d718":"markdown","4e687a2e":"markdown","6d637fbd":"markdown","a8a8f181":"markdown","a274c9e6":"markdown","56d466c9":"markdown","ee12a6a0":"markdown","3a94878d":"markdown","eb7b822f":"markdown","8a9011a6":"markdown","a24094a1":"markdown","383189cc":"markdown","765b56b5":"markdown","757e9f2e":"markdown","63a2429f":"markdown","c525ae93":"markdown","9e6f06ff":"markdown","eae2eb01":"markdown","90d50598":"markdown","d9345850":"markdown","6967b942":"markdown","338e8486":"markdown","ff6562ce":"markdown","bc13ecd3":"markdown","b1d9be48":"markdown","0ae9d5e7":"markdown","a7d7aa1e":"markdown","bec2a78f":"markdown","ce36a958":"markdown","6341c1d1":"markdown","0392891b":"markdown","c38a11e1":"markdown","d4f5a276":"markdown","665e5f22":"markdown","563b8fed":"markdown","1ff446a5":"markdown","e9d2fe71":"markdown","3daf7185":"markdown"},"source":{"828ca277":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# data analysis\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\n\n# modeling\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport sklearn.metrics as metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","43034d48":"sns.set(style=\"ticks\", color_codes=True)\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.3f}'.format\npd.options.display.max_colwidth = -1","9831c850":"insurance = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")\ninsurance.head()","c597c73b":"insurance.info()","cf501fc3":"insurance[['sex', 'smoker', 'region']] = insurance[['sex', 'smoker', 'region']].astype('category')\ninsurance.dtypes","21a58f12":"for col in insurance.select_dtypes('category').columns:\n    print(col, \":\", insurance[col].cat.categories)","3c7f2f60":"pair_plot = sns.pairplot(insurance, diag_kind = \"kde\", corner = True, markers = '+',\n                         kind = \"reg\")\npair_plot.fig.suptitle(\"Pair Plot of Numerical Variables\", size = 25, y = 1.05)\npair_plot","8f29ae7b":"fig, axes = plt.subplots(1, 3, figsize=(15,5))\nfor ax, col in zip(axes, insurance.select_dtypes('category').columns):\n    sns.violinplot(x = col, y = \"charges\", data = insurance, ax = ax)\nplt.tight_layout()\nfig.suptitle(\"Violin Plot of Categorical Variables\", size = 28, y = 1.05)\nplt.show()","0ddb0d86":"fig, axes = plt.subplots(3, 3, figsize=(15,15))\n\nfor row, cat in enumerate(insurance.select_dtypes('category').columns):\n    for col, num in enumerate(insurance.select_dtypes(np.number).columns[:-1]):\n        sns.scatterplot(x = num, y = \"charges\", hue = cat, data = insurance,\n                        alpha = 0.6, ax = axes[row][col])\n    \nplt.tight_layout()\nfig.suptitle(\"Scatter Plot of Each Numerical and Categorical Variables\", size = 28, y = 1.025)\nplt.show()","af0c1208":"y_boxplot = sns.boxplot(insurance['charges'])\ny_boxplot.set_title(\"Boxplot of Charges\")\ny_boxplot","374acfad":"insurance_wo_outlier = insurance.copy()\nwhile True:\n    y_boxplot = plt.boxplot(insurance_wo_outlier['charges'])\n    lower_whisker, upper_whisker = [item.get_ydata()[1] for item in y_boxplot['whiskers']]\n    outlier_flag = (insurance_wo_outlier['charges'] < lower_whisker) | (insurance_wo_outlier['charges'] > upper_whisker)\n    num_outlier = sum(outlier_flag)\n    if num_outlier == 0:\n        before = insurance.shape[0]\n        after = insurance_wo_outlier.shape[0]\n        print(\"Total Outlier Removed: {}\/{} ({}%)\".format(before-after, before,\n                                                          round(100*(before-after)\/before, 3)))\n        print(\"Final Range: ({}, {})\".format(lower_whisker, upper_whisker))\n        \n        break\n    print(\"Remove Outlier: {}\/{} ({}%)\".format(num_outlier, insurance_wo_outlier.shape[0],\n                                               round(100*num_outlier\/insurance_wo_outlier.shape[0], 3)))\n    plt.show()\n    insurance_wo_outlier = insurance_wo_outlier[-outlier_flag]","4fc96b63":"corr_heatmap = sns.heatmap(insurance.corr(method = \"pearson\"),\n                           annot = True, fmt='.3f', linewidths = 5, cmap = \"Reds\")\ncorr_heatmap.set_title(\"Pearson Correlation\", size = 25)\ncorr_heatmap","c230ac6a":"def check_linearity(data, target_var, SL = 0.05):\n    cor_test_list = []\n    for col in data.drop(target_var, axis = 1).columns:\n        if col in data.select_dtypes('category').columns:\n            cor_test = stats.spearmanr(data[col], data[target_var])\n            cor_type = \"Spearman\"\n        else:\n            cor_test = stats.pearsonr(data[col], data[target_var])\n            cor_type = \"Pearson\"\n        cor_dict = {\"Predictor\": col,\n                    \"Type\": cor_type,\n                    \"Correlation\": cor_test[0],\n                    \"P-Value\": cor_test[1],\n                    \"Conclusion\": \"significant\" if cor_test[1] < SL else \"not significant\"}\n        cor_test_list.append(cor_dict)\n    return pd.DataFrame(cor_test_list)\n\ncheck_linearity(insurance, \"charges\")","16af8ee5":"X_raw = insurance.drop([\"charges\"], axis = 1)\ny = insurance.charges.values","70137423":"X = pd.get_dummies(X_raw, columns = insurance.select_dtypes('category').columns, drop_first = True)\nX.head()","1eb38704":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 333)\nprint(\"X Train:\", X_train.shape)\nprint(\"X Test:\", X_test.shape)\nprint(\"y Train:\", y_train.shape)\nprint(\"y Test:\", y_test.shape)","0d412f85":"model_all = sm.OLS(y_train, sm.add_constant(X_train))\nresult_all = model_all.fit()\nprint(result_all.summary())","8aaf2841":"def backwardEliminationByAIC(X, y, show_iter = True):\n    X_step = X.copy()\n    num_iter = 1\n    drop_list = []\n    while True:\n        res_list = []\n        for col in ['none'] + list(X_step.columns):\n            X_curr = X_step.drop(col, axis = 1) if col != \"none\" else X_step\n            model = sm.OLS(y, sm.add_constant(X_curr)).fit()\n            res_list.append({\"drop\": col, \"aic\": model.aic})\n\n        curr_res = pd.DataFrame(res_list).sort_values(\"aic\")\n        col_to_be_removed = list(curr_res[\"drop\"])[0]\n    \n        if show_iter:\n            print(\"Iteration {}: Drop {}\".format(num_iter, col_to_be_removed))\n            display(HTML(curr_res.to_html(index=False)))\n\n        if col_to_be_removed == \"none\":\n            break \n        else:\n            drop_list.append(col_to_be_removed)\n            X_step = X_step.drop(col_to_be_removed, axis = 1)\n        num_iter += 1\n    X_back = X.drop(drop_list, axis = 1)\n    model_back = sm.OLS(y, sm.add_constant(X_back))\n    return model_back\n\nmodel_back = backwardEliminationByAIC(X, y)\npredictor_back = model_back.exog_names[1:]\npredictor_back","1724a3e8":"def forwardSelectionByAIC(X, y, show_iter = True):\n    X_step = pd.DataFrame(sm.add_constant(X)['const'])\n    num_iter = 1\n    add_list = []\n\n    while True:\n        res_list = [{\"add\": \"none\", \"aic\": sm.OLS(y, sm.add_constant(X_step)).fit().aic}]\n        for col in list(set(X.columns) - set(add_list)):\n            X_curr = X[add_list + [col]]\n            model = sm.OLS(y, sm.add_constant(X_curr)).fit()\n            res_list.append({\"add\": col, \"aic\": model.aic})\n\n        curr_res = pd.DataFrame(res_list).sort_values(\"aic\")\n        col_to_be_added = list(curr_res[\"add\"])[0]\n\n        if show_iter:\n            print(\"Iteration {}: Add {}\".format(num_iter, col_to_be_added))\n            display(HTML(curr_res.to_html(index=False)))\n\n        if col_to_be_added == \"none\":\n            break \n        else:\n            add_list.append(col_to_be_added)\n            X_step = X[add_list]\n        num_iter += 1\n    X_forward = X[add_list]\n    model_forward = sm.OLS(y, sm.add_constant(X_forward))\n    return model_forward\n\nmodel_forward = forwardSelectionByAIC(X, y)\npredictor_forward = model_forward.exog_names[1:]\npredictor_forward","370ccd5b":"set(predictor_back) == set(predictor_forward)","7dab947a":"def experimentModel(X_train, X_test, y_train, ignore_var=[]):\n    X_train_new = X_train.drop(ignore_var, axis = 1)\n    X_test_new = X_test.drop(ignore_var, axis = 1)\n    model_new = sm.OLS(y_train, sm.add_constant(X_train_new))\n    result_new = model_new.fit()\n    return X_test_new, result_new","83caae86":"# 2. Model without predictor sex\nX_test_wo_sex, result_wo_sex = experimentModel(X_train, X_test, y_train,\n                                               ignore_var = ['sex_male'])\n\n# 3. Model without predictor sex and region\nX_test_wo_sex_region, result_wo_sex_region = experimentModel(X_train, X_test, y_train,\n                                                             ignore_var = ['sex_male', 'region_northwest', 'region_southeast', 'region_southwest'])\n","27e51381":"X_wo_outlier = X.iloc[insurance_wo_outlier.index]\ny_wo_outlier = insurance_wo_outlier.charges.values\nX_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier, y_test_wo_outlier = train_test_split(X_wo_outlier, y_wo_outlier, test_size = 0.2, random_state = 333)\nprint(\"X Train:\", X_train_wo_outlier.shape)\nprint(\"X Test:\", X_test_wo_outlier.shape)\nprint(\"y Train:\", y_train_wo_outlier.shape)\nprint(\"y Test:\", y_test_wo_outlier.shape)","0758ebf6":"X_test_wo_outlier_all, result_wo_outlier_all = experimentModel(X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier)\nX_test_wo_outlier_wo_sex, result_wo_outlier_wo_sex = experimentModel(X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier,\n                                                                     ignore_var = ['sex_male'])\nX_test_wo_outlier_wo_sex_region, result_wo_outlier_wo_sex_region = experimentModel(X_train_wo_outlier, X_test_wo_outlier, y_train_wo_outlier,\n                                                                                   ignore_var = ['sex_male', 'region_northwest', 'region_southeast', 'region_southwest'])","64f0922d":"def evalRegression(model, X_true, y_true, outlier = True, decimal = 5):\n    y_pred = model.predict(sm.add_constant(X_true))\n    \n    metric = {\n        \"Predictor\": sorted(model.model.exog_names[1:]),\n        \"Outlier\": \"Included\" if outlier else \"Excluded\",\n        \"R-sq\": round(model.rsquared, decimal),\n        \"Adj. R-sq\": round(model.rsquared_adj, decimal),\n        \"RMSE\": round(np.sqrt(metrics.mean_squared_error(y_true, y_pred)), decimal),\n        \"MAPE\": round(np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100, decimal)\n    }\n    \n    return metric","a2dd317e":"eval_df = pd.DataFrame([evalRegression(result_all, X_test, y_test),\n              evalRegression(result_wo_sex, X_test_wo_sex, y_test),\n              evalRegression(result_wo_sex_region, X_test_wo_sex_region, y_test),\n              evalRegression(result_wo_outlier_all, X_test_wo_outlier_all, y_test_wo_outlier, outlier = False),\n              evalRegression(result_wo_outlier_wo_sex, X_test_wo_outlier_wo_sex, y_test_wo_outlier, outlier = False),\n              evalRegression(result_wo_outlier_wo_sex_region, X_test_wo_outlier_wo_sex_region, y_test_wo_outlier, outlier = False)],\n              index = range(1, 7))\neval_df.index.name = \"Model\"\neval_df","b05c6409":"# Model 5\nfinal_X_test = X_test_wo_outlier_wo_sex\nfinal_y_test = y_test_wo_outlier\nfinal_result = result_wo_outlier_wo_sex","3ebb3d8d":"def evalRegularizedRegression(model_result, X_test, y_test):\n    model = model_result.model\n    eval_regularized_list = []\n    for alpha in np.linspace(0, 10, 101):\n        for fit_type in [0, 1]:\n            result_regularized = model.fit_regularized(alpha = round(alpha, 2),\n                                                       L1_wt = fit_type,\n                                                       start_params = model_result.params)\n            final = sm.regression.linear_model.OLSResults(model, \n                                                          result_regularized.params,\n                                                          model.normalized_cov_params)\n            metric = {}\n            metric[\"alpha\"] = alpha\n            metric[\"Fit Type\"] = \"Ridge\" if fit_type == 0 else \"Lasso\"\n            metric.update(evalRegression(final, X_test, y_test, outlier = False))\n            metric.pop(\"Predictor\")\n\n            eval_regularized_list.append(metric)\n    return pd.DataFrame(eval_regularized_list)\n\neval_regularized = evalRegularizedRegression(final_result, final_X_test, final_y_test)","b1f53a55":"for metric in [\"Adj. R-sq\", \"MAPE\"]:\n    facet = sns.FacetGrid(eval_regularized, col = \"Fit Type\")\n    facet = facet.map(plt.plot, \"alpha\", metric)\n    facet.set_axis_labels(\"Penalty Weight\")\n    facet.fig.suptitle(\"Evaluate Regularized Linear Regression by {}\".format(metric), y = 1.05)","4df690b0":"eval_regularized[(eval_regularized[\"Adj. R-sq\"] == max(eval_regularized[\"Adj. R-sq\"])) &\n                 (eval_regularized[\"MAPE\"] == min(eval_regularized[\"MAPE\"]))]","7be93692":"print(final_result.summary())","35f47065":"print(final_result.params)","613eaee0":"y_pred = final_result.predict(sm.add_constant(final_X_test))\nresidual = final_y_test - y_pred\nresidual.describe()","88c6ba98":"def plotResidualNormality(residual):\n    residual_histogram = sns.distplot(residual)\n    residual_histogram.set_xlabel(\"Residual\")\n    residual_histogram.set_ylabel(\"Relative Frequency\")\n    residual_histogram.set_title(\"Distribution of Residuals\", size = 25)\n    return residual_histogram\n\nplotResidualNormality(residual)","0936849f":"def statResidualNormality(residual):\n    W, pvalue = stats.shapiro(residual)\n    print(\"Shapiro-Wilk Normality Test\")\n    print(\"W: {}, p-value: {}\".format(W, pvalue))\n    \nstatResidualNormality(residual)","f99b8512":"def plotResidualHomoscedasticity(y_true, y_pred):\n    residual = y_true - y_pred\n    residual_scatter = sns.scatterplot(y_pred, residual)\n    residual_scatter.axhline(0, ls = '--', c = \"red\")\n    residual_scatter.set_xlabel(\"Fitted Value\")\n    residual_scatter.set_ylabel(\"Residual\")\n    residual_scatter.set_title(\"Scatter Plot of Residuals\", size = 25)\n    return residual_scatter\n\nplotResidualHomoscedasticity(final_y_test, y_pred)","64133d11":"def statResidualHomoscedasticity(X, residual):\n    lm, lm_pvalue, fvalue, f_pvalue = sm.stats.diagnostic.het_breuschpagan(residual,\n                                                                           np.array(sm.add_constant(X)))\n\n    print(\"Studentized Breusch-Pagan Test\")\n    print(\"Lagrange Multiplier: {}, p-value: {}\".format(lm, lm_pvalue))\n    print(\"F: {}, p-value: {}\".format(fvalue, f_pvalue))\n    \nstatResidualHomoscedasticity(final_X_test, residual)","3fa81354":"vif_list = []\nfor idx, col in enumerate(final_result.model.exog_names[1:]):\n    vif_dict = {\"Variable\": col,\n                \"VIF\":  variance_inflation_factor(final_result.model.exog, idx+1)}\n    vif_list.append(vif_dict)\n    \npd.DataFrame(vif_list)","9fa7a457":"### Models without Outliers\nRecall that 16.592% of our target variable are outliers. Next, we try to build the same three models above with the outliers being removed:\n1. Model with all predictors\n2. Model without predictor `sex`\n3. Model without predictor `sex` and `region`","4b2280a0":"## Visualization\nWe visualize the numerical variables using pair plot. Plots on the diagonal represent the distribution of each variables, and the rest will be plotted as scatter plot.","2009f0a5":"From the correlation test above, we can conclude that `sex` and `region` will not have significant correlation to `charges`. But is this statement also hold if we construct a Linear model from the data? We'll see in the next section.","a159d548":"# Model Interpretation\nWe obtained our final model as follows:","eb3513be":"## Homoscedasticity\nThe residuals have to distributed homogeneously, doesn't form a pattern. We can visualize it using a scatter plot.","f0543b0a":"# Conclusion\nLinear regression is highly interpretable, meaning we can understand and account the predictors that are being included and excluded from the model. From the Multiple Linear Regression model, we know that being a `smoker` will cause a significant increase in beneficiary's `charges` than those who aren't.\n\nIn the end, we choose the model which excludes the outlier data since it gives us a better prediction (lower error performance, but the trade-off is a lower Adjusted R-squared. Using regularization worsen the overall model performance.\n\nThe normality of residual assumption is not fulfilled. If we only focus on the model performance, we can use another regression method which is more robust such as Random Forest Regressor.","2311a03f":"Based on the result, it recommends us to penalize the parameters using $\\alpha = 0$ which means no regularization is needed. Unfortunately, we couldn't improve the model performance by this technique. So we stick with the previous **model 5**.","31b820cc":"## Variable Assumption: Linearity\nWe also have to perform statistical significancy test to check the linearity of all candidate predictors to our target variable, including the categorical variables. Spearman's rank correlation is a non-parametric test which can be used to measure the degree of association between categorical and numerical variables.\n\nHere is the hypothesis of the test:\n- Null Hypothesis ($H_0$): Correlation is not significant\n- Alternative Hypothesis ($H_1$): Correlation is significant","51d2ad9d":"## Regularized Linear Regression\nHere, we try to improve the **model 5** performance by using regularized linear regression. There are two simple techniques to reduce model complexity and prevent overfitting:\n1. Ridge Regression, the cost function is added by a penalty weight equivalent to the square of the coefficients. This shrinks the coefficients and helps to reduce the model complexity and multicollinearity.\n2. Lasso (Least Absolute Shrinkage and Selection Operator) Regression, which helps reducing overfitting and also used as feature selection.","1498af1b":"From the pair plot above, we couldn't get any interesting insight. How about the categorical variables? Let's us plot them with violin plot, which tell us the distribution of `charges` for categorical variable in each levels.","8ba5d06b":"# Introduction\n## Business Problem\nHi, in this notebook we will try to analyze the factor of medical cost for personal insurance (charges) using Multiple Linear Regression. This use case is provided on [Kaggle](https:\/\/www.kaggle.com\/mirichoi0218\/insurance) and the dataset is available on [Github](https:\/\/github.com\/stedy\/Machine-Learning-with-R-datasets).","b1321f97":"## Normality of Residual\nThe residuals have to distribute around mean 0 and some variance, which follows a standard normal distribution model. We plot a histogram to visualize the distribution of the residuals.","a475d718":"Another way is to statistically test the homoscedasticity using Breusch-Pagan test.\n- Null Hypothesis ($H_0$): Homoscedasticity is present\n- Alternative Hypothesis ($H_1$): Homoscedasticity is not present (Heteroscedasticity)","4e687a2e":"The Pearson correlation between `charges` and the other numerical variables is positively correlated but not very high, means they have a weak linear relationship in the same direction.","6d637fbd":"## Correlation\nIn order to apply Linear Regression to our data, we have to check the correlation between each candidate predictors and `charges` as the target variable. Variable will not be a good predictor if it doesn't significantly correlated with the target variable.\n\nCorrelation is a statistical measure on how strong a linear relationship between two variables. The values ranged between -1.0 and 1.0, where:\n- Positive value = positive correlation, means one variable increases as the other variable increases, or vice versa. The closer to 1, the stronger the positive relationship.\n- Negative value = negative correlation, means one variable decreases as the other variable increases, or vice versa. The closer to -1, the stronger the negative relationship.\n- Zero = no correlation, means that a variable has nothing to do with the other variable.\n\nPearson's Correlation Coefficient is one type of correlation which measures the linear relationship between two quantitative variables.","a8a8f181":"# Check Model Assumption\nIt is necessary for us to check whether the model fulfill the following assumption:\n1. Normality of Residual\n2. Heteroscedasticity\n3. Multicollinearity","a274c9e6":"### Evaluation\nIn order to choose the best model, we are going to use several metrics:\n- R-squared, the higher the better\n- Adjusted R-squared, the higher the better\n- Root Mean Squared Error (RMSE), the lower the better\n- Mean Absolute Percentage Error (MAPE), the lower the better","56d466c9":"# Modeling\nFrom previous section, we can summarize:\n- Beneficiary who is a `smoker` have relatively high `charges`\n- Numerical candidate predictors (`age`, `bmi`, `children`) have positive correlation to `charges`\n- `sex` and `region` have no significant correlation to `charges`\n- We have to keep in mind about the outliers of `charges`\n\nIn this section, we try to construct a linear model to further analyze and interpret the result.","ee12a6a0":"Three plots on the second column show us an interesting pattern. Just like the violin plot before, beneficiary who is a `smoker` will relatively have high `charges` no matter the values of `age`, `bmi`, or `children`.\n\nSo far, we only focus on the candidate predictors, but how about the distribution of the target variables itself?","3a94878d":"From the result above, we can conclude that there is only little multicollinearity exist in our model.","eb7b822f":"### Backward Elimination","8a9011a6":"# Exploratory Data Analysis (EDA)\nLet's us explore to better understand the provided data, by doing some data preparation and visualization of the data.","a24094a1":"From the test above, we reject the null hypothesis since the p-value < 0.05 and conclude that the residuals are not normally distributed.","383189cc":"## Multicollinearity\nWe expect the model to have little to no multicollinearity. It is a condition where at least two predictors have a strong linear relationship. Multicollinearity exists if the Variance Inflation Factor (VIF) value is greater than 10.","765b56b5":"A total of 16.592% data is considered as outliers. Keep in mind, we have to take into consideration about these outliers during the modeling process.","757e9f2e":"From the violin plot above, we can clearly see the difference of charges distribution based on `smoker`. The hypothesis is that beneficiary who is a `smoker` will have relatively high `charges` than those who aren't. Is this statistically significant? We will try to analyze this on the modeling section.\n\nTo explore the data more, we create scatter plot which represent the distribution of numerical variables based on `charges`, distinguished by each categorical variables.","63a2429f":"## Feature Selection\nHow we can improve our model? One way is by feature selection using step-wise regression, spesifically backward elimination and forward selection. On each step, we remove\/select the predictors which results a smaller AIC. Besides that, we also have to really consider the business case also. This plays a role in making decision about which predictors are going to be used in our model.","c525ae93":"# MEDICAL COST ANALYSIS: SMOKING IS BAD FOR YOU (FOR YOUR INSURANCE CHARGES) <a class='tocSkip'>","9e6f06ff":"## Model Comparison\nIn this section, we are going to compare the following models:\n1. Model with all predictors (created on section 3.2)\n2. Model without predictor `sex`\n3. Model without predictor `sex` and `region`\n4. All of the above models with outlier removed","eae2eb01":"We investigate the levels for each categorical data","90d50598":"Wow, there's a lot of information and numbers! Here are some important terms:\n***\nTop Section\n***\n- R-squared tells about the goodness of the fit, ranges between 0 and 1. The closer the value to 1, the better it explains the dependent variables variation in the model. However, it is biased in a way that it never decreases when we add new variables.\n- Adj. R-squared has a penalising factor. It decreases or stays identical to the previous value as the number of predictors increases. If the value keeps increasing on removing the unnecessary parameters go ahead with the model or stop and revert.\n- F-statistic used to compare two variances and the value is always greater than 0. In regression, it is the ratio of the explained to the unexplained variance of the model.\n- AIC stands for Akaike\u2019s information criterion. It estimates the relative amount of information lost by a given model. The lower the AIC, the higher the quality of that model.\n\n***\nMid Section\n***\n- coef is the coefficient\/estimate value of intercept and slope.\n- $P>|t|$ refers to the p-value of partial tests with the null hypothesis $H_0$ that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that the predictor has significant effect to the target variable.\n\n***\nBottom Section\n***\n- Omnibus - D\u2019Angostino\u2019s test. It provides a combined statistical test for the presence of skewness and kurtosis.\n- Skew informs about the data symmetry about the mean.\n- Kurtosis measures the shape of the distribution (i.e. the amount of data close to the mean than far away from the mean).","d9345850":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Business-Problem\" data-toc-modified-id=\"Business-Problem-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Business Problem<\/a><\/span><\/li><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Import Packages<\/a><\/span><\/li><li><span><a href=\"#Set-Notebook-Options\" data-toc-modified-id=\"Set-Notebook-Options-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Set Notebook Options<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis-(EDA)\" data-toc-modified-id=\"Exploratory-Data-Analysis-(EDA)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Exploratory Data Analysis (EDA)<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Data Preparation<\/a><\/span><\/li><li><span><a href=\"#Visualization\" data-toc-modified-id=\"Visualization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Visualization<\/a><\/span><\/li><li><span><a href=\"#Correlation\" data-toc-modified-id=\"Correlation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Correlation<\/a><\/span><\/li><li><span><a href=\"#Variable-Assumption:-Linearity\" data-toc-modified-id=\"Variable-Assumption:-Linearity-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Variable Assumption: Linearity<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Modeling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Data Preparation<\/a><\/span><\/li><li><span><a href=\"#Linear-model-with-all-predictors\" data-toc-modified-id=\"Linear-model-with-all-predictors-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Linear model with all predictors<\/a><\/span><\/li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Backward-Elimination\" data-toc-modified-id=\"Backward-Elimination-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;<\/span>Backward Elimination<\/a><\/span><\/li><li><span><a href=\"#Forward-Selection\" data-toc-modified-id=\"Forward-Selection-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;<\/span>Forward Selection<\/a><\/span><\/li><li><span><a href=\"#Business-Case\" data-toc-modified-id=\"Business-Case-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;<\/span>Business Case<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Comparison\" data-toc-modified-id=\"Model-Comparison-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Model Comparison<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Second-and-Third-Model\" data-toc-modified-id=\"Second-and-Third-Model-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;<\/span>Second and Third Model<\/a><\/span><\/li><li><span><a href=\"#Models-without-Outliers\" data-toc-modified-id=\"Models-without-Outliers-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;<\/span>Models without Outliers<\/a><\/span><\/li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Regularized-Linear-Regression\" data-toc-modified-id=\"Regularized-Linear-Regression-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;<\/span>Regularized Linear Regression<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Interpretation\" data-toc-modified-id=\"Model-Interpretation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Model Interpretation<\/a><\/span><\/li><li><span><a href=\"#Check-Model-Assumption\" data-toc-modified-id=\"Check-Model-Assumption-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Check Model Assumption<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Normality-of-Residual\" data-toc-modified-id=\"Normality-of-Residual-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Normality of Residual<\/a><\/span><\/li><li><span><a href=\"#Homoscedasticity\" data-toc-modified-id=\"Homoscedasticity-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Homoscedasticity<\/a><\/span><\/li><li><span><a href=\"#Multicollinearity\" data-toc-modified-id=\"Multicollinearity-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Multicollinearity<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","6967b942":"Another way is to statistically test the normality using Shapiro-Wilk test.\n- Null Hypothesis ($H_0$): Residuals are normally distributed\n- Alternative Hypothesis ($H_1$): Residuals are not normally distributed","338e8486":"We can interpret the coefficient of model summary as follows:\n* One unit increase in `age`, `bmi`, and `children` will increase insurance `charges` by 234.3, 33.8, 391.3 respectively\n* Being a `smoker` will cost 12785.84 more `charges` than those who aren't (what a significant increase!)\n* If two beneficiary have the same `age`, `bmi`, `children`, and `smoker`, then people who are living in southeast will have the smallest `charges` than other `region`. On the other hand, people who are living in northeast will have the largest `charges`.","ff6562ce":"## Import Packages\nFirst of all, let's us import packages used in this analysis, mainly for data analysis, visualization, and modeling.","bc13ecd3":"From the test above, we fail to reject the null hypothesis since the p-value > 0.05 and conclude that the homoscedasticity is present.","b1d9be48":"`insurance` is a DataFrame object with 1338 rows and 7 columns. There is no missing value present in our data. Here are the explanation for each columns:\n\n- age: age of primary beneficiary (in years)\n- sex: gender of insurance contractor, either female or male\n- bmi: Body Mass Index which provides an understanding of a body by using a number expressing the ratio of body weight (in kilograms) to height squared (in meters). The value of bmi is ideally between 18.5 and 24.9\n- children: number of children\/dependents covered by health insurance\n- smoker: whether the primary beneficiary smoking or not\n- region: the beneficiary's residential area in the US, either northeast, southeast, southwest, or northwest\n- charges: Individual medical costs billed by health insurance\n\nWe will use `charges` as our target variable and the rest as the candidate predictors.\n\nBased on the explanation above, we have to convert the data type of column `sex`, `smoker`, and `region` into categorical data.","0ae9d5e7":"## Data Preparation\nWe have to do some data preparation spesifically for modeling:\n- Separate the target variable from the predictors\n- Create dummy variables for the categorical predictors\n- Train test split in order to evaluate our model, with 80% train and 20% test","a7d7aa1e":"### Second and Third Model","bec2a78f":"### Forward Selection","ce36a958":"We are going to select the best model out of the six models, here are the thought process:\n1. Compare the first three models (with outlier) with the last three models (without outlier). We can see that models with outlier have better R-squared and Adjusted R-squared, but worst in terms of error. In this case, we prefer models **without outlier**, since the drop of MAPE is quite significant.\n2. Out of the three models without outlier, the model with all predictors has the highest Adj. R-sq and the lowest RMSE. But according to the statistical test and business case, we should remove `sex` predictor. On the other hand, `region` must be included due to the business case even though it is not significant statistically. So, in this case we choose **model 5**, having the lowest MAPE","6341c1d1":"Our data is now ready, let's us visualize it and analyze it deeper.","0392891b":"## Data Preparation\nBefore we jump into further step, we have to make sure our data is ready to be analyze. We import `insurance.csv` and analyze the data structure.","c38a11e1":"## Set Notebook Options\n- Set color of plot to be contrast\n- Suppress warning of pandas chaining assignment\n- Change float format to three decimal places\n- Display all content in a `pandas` column","d4f5a276":"The predictors that backward elimination method recommends are: \n* `age`\n* `bmi`\n* `children`\n* `smoker_yes`\n* `region_southeast`\n* `region_southwest`","665e5f22":"From the summary result above, we can conclude:\n- Performance of model: 73.8% (Adj. R-squared)\n- There are two insignificant predictors: `sex` and `region`, which align with the statement on EDA section.","563b8fed":"The predictors that forward selection method recommends are just the **same** as backward selection recommends: \n* `age`\n* `bmi`\n* `children`\n* `smoker_yes`\n* `region_southeast`\n* `region_southwest`","1ff446a5":"## Linear model with all predictors\nThis is the equation of Multiple Linear Regression that we have to estimate:\n$\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n$\nwhere:\n- $\\hat{Y}$ is the predicted value of target variable\n- $X_1, X_2, ..., X_n$ are the predictors\n- $\\beta_0$ is the intercept \/ constant\n- $\\beta_1, \\beta_2, ..., \\beta_n$ are the slope of respective predictors $X_1, X_2, ..., X_n$\n- $n$ is the number of predictors\n\nAs a starting point, let's us fit a linear model with all existing predictors and see how it goes.","e9d2fe71":"The dots which lie outside the whiskers are called as an outlier. They are data points which significantly differs from the other observations. Next, we iteratively remove these outliers from the data and calculate exactly how many data points are considered as an outlier.","3daf7185":"### Business Case\nWhen building a model, we have to take into consideration about the business case also. According to [healthcare.gov](https:\/\/www.healthcare.gov\/how-plans-set-your-premiums\/), five factors that can affect a plan\u2019s premium are location, age, tobacco use, plan category, and whether the plan covers dependents. Insurance companies can\u2019t charge women and men different prices for the same plan.\n\nSo based on the step-wise regression and business case, we should have the following predictors:\n* `age`\n* `bmi`\n* `children`\n* `smoker`\n* `region`"}}