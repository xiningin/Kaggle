{"cell_type":{"5e27a0e3":"code","2d7b3aea":"code","598a422b":"code","4e6ddb5b":"code","e21dbfbb":"code","c10af3f4":"code","f6b06f6d":"code","40ef972d":"code","5b3d645c":"code","c497714e":"code","170751e9":"code","0f20ce34":"code","7a7fdb0d":"code","6c178e57":"code","33a62649":"code","2f035457":"code","8fa83217":"code","6e275547":"code","b99cf040":"code","e1a741c0":"code","5ffaaeb3":"code","9ad8b6c6":"markdown","5059da46":"markdown","a81e026c":"markdown","bf0e0650":"markdown","f84f2f86":"markdown","d3e91aa5":"markdown","995d4997":"markdown","92f9d1b7":"markdown"},"source":{"5e27a0e3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, Convolution2D, MaxPool2D, MaxPooling2D, ZeroPadding2D, BatchNormalization\nfrom numpy.random import permutation\nfrom keras import optimizers\nfrom keras.optimizers import SGD, Adam\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.utils import to_categorical, np_utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, plot_confusion_matrix, roc_auc_score, roc_curve\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV","2d7b3aea":"def load_data(keras_datasets, first_layer=\"dense\", channels=1, plot_images=False, class_names=[]):\n    (x_train, y_train), (x_test, y_test) = keras_datasets.load_data()\n    print('Before reshape - X_train.shape:', x_train.shape)\n    print('Before reshape - X_test.shape:', x_test.shape)\n    height=x_train.shape[1]\n    width=x_train.shape[2]        \n    # flatten 28*28 images to a 784 vector for each image\n    num_pixels = height * width\n    \n    #plot images\n    if plot_images:\n        plot_images_with_labels(x_train, y_train, height, width, class_names, 25)\n    \n    if first_layer == \"dense\":\n        # convert shape of x_train from (60000, 28, 28) to (60000, 784) - 784 columns per row        \n        X_train = x_train.reshape((x_train.shape[0], num_pixels)).astype('float32')\n        X_test = x_test.reshape((x_test.shape[0], num_pixels)).astype('float32')        \n    elif first_layer == \"conv2d\":\n        # Select class 6 images (class 6)\n#         x_train = x_train[y_train.flatten() == 6]\n        X_train = x_train.reshape((x_train.shape[0], height, width, channels)).astype('float32')\n        X_test = x_test.reshape((x_test.shape[0], height, width, channels)).astype('float32')        \n        print((x_train.shape[0],) + (height, width, channels))        \n        \n    print('After reshape - X_train.shape:', X_train.shape)\n    print('After reshape - X_test.shape:', X_test.shape)\n    print('Before rescaling:', X_train[0])\n    #normalize the values between 0 and 1\n    X_train = (X_train.astype(np.float32))\/255\n    X_test = (X_test.astype(np.float32))\/255\n    print('After rescaling:', X_train[0])\n              \n    #convert labels to categorical\/dummy encoding so that we can use simple \"categorical_crossentropy\" as loss.\n    print('Class label of first image before converting to categorical:', y_train[0])\n    # one hot encode outputs\n    y_train = np_utils.to_categorical(y_train)\n    y_test = np_utils.to_categorical(y_test)\n    num_classes = y_test.shape[1]\n    print('Total number of classes:', num_classes)\n    print('Class label of first image after converting to categorical:', y_train[0])\n              \n    return (X_train, y_train, X_test, y_test, height, width)\n\ndef plot_images_with_labels(X, y, img_height, img_width, class_names, nb_count=25):\n    plt.figure(figsize=(10, 10))\n    for i in range(nb_count):\n        plt.subplot(np.sqrt(nb_count), np.sqrt(nb_count), i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(X[i].reshape((img_height,img_width)), cmap=plt.get_cmap('gray'))        \n        label_index = int(y[i])\n        plt.title(class_names[label_index])\n    plt.show()\n\ndef train_model(model, X_train, y_train, X_valid=None, y_valid=None, validation_split=0.20, data_aug = False, best_model_name='best_model.h5', epochs=50, batch_size=512,verbose=1):\n    er = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=verbose)\n    cp = ModelCheckpoint(filepath = best_model_name, save_best_only = True,verbose=verbose)\n#     lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_delta=0.0001)\n    callbacks = [cp, er]\n    \n    if not data_aug and X_valid is not None:  \n        print('Training without data augmentation...')\n        history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,verbose=verbose, callbacks=callbacks, validation_data=(X_valid,y_valid))\n        return history\n    elif not data_aug and X_valid is None:\n        print('Training without data augmentation...')\n        history = model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs, verbose=verbose, shuffle=True, callbacks=callbacks, validation_split=validation_split)\n        return history\n    else:\n        print('Training with data augmentation...')\n        train_datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n        train_set_ae = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n\n        validation_datagen = ImageDataGenerator()\n        validation_set_ae = validation_datagen.flow(X_valid, y_valid, batch_size=batch_size)\n        \n        history = model.fit_generator(train_set_ae,\n                                           epochs=epochs,\n                                           steps_per_epoch=np.ceil(X_train.shape[0]\/batch_size),\n                                           verbose=verbose, callbacks=callbacks,\n                                           validation_data=(validation_set_ae),\n                                           validation_steps=np.ceil(X_valid.shape[0]\/batch_size))\n        \n        return history\n    \ndef plot_loss_and_metrics(history, plot_loss_only= False, metrics=['acc']):\n    fig, axes = plt.subplots(nrows=1, ncols=len(metrics)+1, figsize=(20, 4))\n    axes[0].plot(history.history['loss'])\n    axes[0].plot(history.history['val_loss'])\n    axes[0].set_title('Model Loss')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].legend(['Train', 'Val'], loc='lower right')    \n        \n    if not plot_loss_only:\n        axes[1].plot(history.history['acc'])\n        axes[1].plot(history.history['val_acc'])\n        axes[1].set_title('Model Accuracy')\n        axes[1].set_ylabel('Accuracy')\n        axes[1].set_xlabel('Epoch')\n        axes[1].legend(['Train', 'Val'], loc='lower right')  \n        \n        if 'mae' in metrics:\n            axes[2].plot(history.history['mae'])\n            axes[2].plot(history.history['val_mae'])\n            axes[2].set_title('Model Mean Absolute Error')\n            axes[2].set_ylabel('Mean Absolute Error')\n            axes[2].set_xlabel('Epoch')\n            axes[2].legend(['Train', 'Val'], loc='lower right') \n        if 'mse' in metrics:\n            axes[3].plot(history.history['mse'])\n            axes[3].plot(history.history['val_mse'])\n            axes[3].set_title('Model Mean Squared Error')\n            axes[3].set_ylabel('Mean Squared Error')\n            axes[3].set_xlabel('Epoch')\n            axes[3].legend(['Train', 'Val'], loc='lower right')\n            \n    plt.show()\n    \ndef plot_roc_curve(fpr,tpr): \n  import matplotlib.pyplot as plt\n  plt.plot(fpr,tpr) \n  plt.axis([0,1,0,1]) \n  plt.xlabel('False Positive Rate') \n  plt.ylabel('True Positive Rate') \n  plt.show()  \n    \ndef load_evaluate_predict(fileName, X_test, y_test, nb_round=0, print_first=1, metrics=['acc']):\n    #load best model, evaluate and predict on unseen data    \n    best_model = load_model(fileName)\n    results = best_model.evaluate(X_test, y_test)    \n    print('Test loss = {}'.format(np.round(results[0], 2)))\n    print('Test accuracy = {}'.format(np.round(results[1], 2)))\n    if len(metrics)>1:\n        print('Test ' + metrics[1] + '= {}'.format(np.round(results[2], 2)))\n        print('Test ' + metrics[2] + '= {}'.format(np.round(results[3], 2)))\n\n    y_pred_proba = best_model.predict(X_test)\n    for i in range(print_first):\n        print('')\n        print(\"   Actual:\", y_test[i])\n        print('Predicted:', np.round(y_pred_proba[i], nb_round))\n    \n    return best_model, y_pred_proba\n\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import itertools\n    \n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=12)\n    plt.yticks(tick_marks, classes, fontsize=12)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)\n    \ndef report_metrics(y_test, y_pred, y_pred_proba, classes, multiclass=False):\n    #confusion matrix\n    cnf_matrix = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(6,6))\n    plot_confusion_matrix(cnf_matrix, classes=classes, title=\"Confusion matrix\")\n    plt.show()\n\n    #classification report\n    print('classification report:\\n', classification_report(y_test, y_pred))\n    \n    if not multiclass:\n        #calculate the roc auc score\n        auc = roc_auc_score(y_test, y_pred_proba)\n        print('AUC: %.3f' % auc)\n    \n        #plot the roc curve\n        fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_proba)\n        print('ROC curve:\\n')\n        plot_roc_curve(fpr_keras, tpr_keras)","598a422b":"NUM_CLASSES=10\nCLASS_NAMES = [\n    \"T-shirt\/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\"\n]\nEPOCHS=50\nBATCH_SIZE=1000\nCHANNELS=1\nVERBOSE=1\nMETRICS=['acc']","4e6ddb5b":"X_train, y_train, X_test, y_test, IMG_HEIGHT, IMG_WIDTH = load_data(keras.datasets.fashion_mnist, first_layer=\"dense\", channels=1, plot_images=True, class_names=CLASS_NAMES)","e21dbfbb":"X_train_cnn, y_train_cnn, X_test_cnn, y_test_cnn, IMG_HEIGHT, IMG_WIDTH = load_data(keras.datasets.fashion_mnist, first_layer=\"conv2d\", channels=1, plot_images=False)","c10af3f4":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\nX_train_cnn, X_valid_cnn, y_train_cnn, y_valid_cnn = train_test_split(X_train_cnn, y_train_cnn, test_size=0.20, random_state=42)","f6b06f6d":"#create single layer network called perceptron\ndef build_slp_model(height, width, nb_classes, metrics):\n    model = keras.models.Sequential()\n#     model.add(keras.layers.Flatten(input_shape=(height, width)))\n    model.add(keras.layers.Dense(nb_classes, input_dim=(height* width), use_bias=False, activation='softmax')) #no hidden layers, all inputs connected to all outputs\n    model.compile(loss = 'categorical_crossentropy',\n              optimizer=keras.optimizers.Adam(lr=.0001),#optimizer='adam',              \n              metrics=metrics)\n    return model\n\nmodel_slp = build_slp_model(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES, METRICS)\nmodel_slp.summary()","40ef972d":"history_slp = train_model(model_slp, X_train, y_train, X_valid=X_valid, y_valid=y_valid, data_aug = False, \n            best_model_name='best_model_slp.h5', epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)","5b3d645c":"# print the loss and accuracy\nplot_loss_and_metrics(history_slp)\n_, y_pred_proba_slp = load_evaluate_predict('best_model_slp.h5', X_test, y_test, nb_round=0, print_first=1, metrics=METRICS)\n\nprint(y_pred_proba_slp[0])\nprint(y_pred_proba_slp[0].shape)\n## Get most likely class\ny_pred_slp = np.argmax(y_pred_proba_slp, axis=1)\nprint(y_pred_slp)\n\n#Confusion Matrix, Classification report, ROC curve\nreport_metrics(np.argmax(y_test, axis=1), y_pred_slp, y_pred_proba_slp, CLASS_NAMES, multiclass=True)","c497714e":"#predictions on unseen test data\nX_new = X_test[:3]\ny_proba = model_slp.predict(X_new)\nprint(y_proba.round(2))\n\ny_pred = model_slp.predict_classes(X_new)\nprint(y_pred)\n\nprint(np.array(CLASS_NAMES)[y_pred])","170751e9":"def build_mlp_model(height, width, nb_classes, metrics):\n    #create the multilayer preceptron model with 4 hidden layers\n    model = keras.models.Sequential()\n#     model.add(keras.layers.Flatten(input_shape=(height, width)))\n    model.add(keras.layers.Dense(256, input_dim=(height* width), activation='relu'))\n    model.add(keras.layers.Dense(256, activation='relu'))\n    model.add(keras.layers.Dense(256, activation='relu'))\n    model.add(keras.layers.Dense(256, activation='relu'))\n    model.add(keras.layers.Dense(nb_classes, activation='softmax'))\n    model.compile(loss = 'categorical_crossentropy',\n              optimizer='rmsprop',                           #optimizer=keras.optimizers.SGD(lr=.001),#optimizer='sgd',              \n              metrics=metrics)\n    return model\n\nmodel_mlp = build_mlp_model(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES, METRICS)\nmodel_mlp.summary()","0f20ce34":"model_mlp.layers","7a7fdb0d":"hidden1 = model_mlp.layers[1]\nweights, biases = hidden1.get_weights()\nprint(weights.shape)\nprint(biases.shape)","6c178e57":"history_mlp = train_model(model_mlp, X_train, y_train, X_valid=X_valid, y_valid=y_valid, data_aug = False, \n            best_model_name='best_model_mlp.h5', epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)","33a62649":"# print the loss and accuracy\nplot_loss_and_metrics(history_mlp)\n_, y_pred_proba_mlp = load_evaluate_predict('best_model_mlp.h5', X_test, y_test, nb_round=0, print_first=1, metrics=METRICS)\n\nprint(y_pred_proba_mlp[0])\nprint(y_pred_proba_mlp[0].shape)\n## Get most likely class\ny_pred_mlp = np.argmax(y_pred_proba_mlp, axis=1)\nprint(y_pred_mlp)\n\n#Confusion Matrix, Classification report, ROC curve\nreport_metrics(np.argmax(y_test, axis=1), y_pred_mlp, y_pred_proba_mlp, CLASS_NAMES, multiclass=True)","2f035457":"#predictions on unseen test data\nX_new = X_test[:3]\ny_proba = model_mlp.predict(X_new)\nprint(y_proba.round(2))\n\ny_pred = model_mlp.predict_classes(X_new)\nprint(y_pred)\n\nprint(np.array(CLASS_NAMES)[y_pred])","8fa83217":"def create_simple_conv_model(image_height=IMG_HEIGHT, image_width=IMG_WIDTH, channels=CHANNELS, nb_classes=NUM_CLASSES, metrics=METRICS, optimizer='adam'):    \n    # number of convolutional filters to use\n    nb_filters = 32   \n    # convolution kernel size\n    nb_conv = 3\n     # size of pooling area for max pooling\n    nb_pool = 2\n    model = Sequential()\n    model.add(Conv2D(filters=nb_filters, kernel_size=(nb_conv,nb_conv), strides=(1, 1), activation='relu', input_shape=(image_height, image_width, channels)))  \n    model.add(BatchNormalization())    \n    model.add(MaxPool2D(pool_size=(nb_pool,nb_pool)))   \n#     model.add(Dropout(0.5))   \n\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))    \n#     model.add(Dropout(0.5))\n    \n    model.add(Dense(nb_classes, activation='softmax'))   \n        \n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=metrics)\n    return model\n\nmodel_cnn = create_simple_conv_model(IMG_HEIGHT, IMG_WIDTH, CHANNELS, NUM_CLASSES, METRICS)\nmodel_cnn.summary()","6e275547":"history_cnn = train_model(model_cnn, X_train_cnn, y_train_cnn, X_valid=X_valid_cnn, y_valid=y_valid_cnn, data_aug = False, \n            best_model_name='best_model_cnn.h5', epochs=EPOCHS, batch_size=BATCH_SIZE,verbose=VERBOSE)","b99cf040":"# print the loss and accuracy\nplot_loss_and_metrics(history_cnn)\n_, y_pred_proba_cnn=  load_evaluate_predict('best_model_cnn.h5', X_test_cnn, y_test, nb_round=0, print_first=1, metrics=METRICS)\n\nprint(y_pred_proba_cnn[0])\nprint(y_pred_proba_cnn[0].shape)\n## Get most likely class\ny_pred_cnn = np.argmax(y_pred_proba_cnn, axis=1)\nprint(y_pred_cnn)\n\n#Confusion Matrix, Classification report, ROC curve\nreport_metrics(np.argmax(y_test, axis=1), y_pred_cnn, y_pred_proba_cnn, CLASS_NAMES, multiclass=True)","e1a741c0":"#predictions on unseen test data\nX_new = X_test_cnn[:3]\ny_proba = model_cnn.predict(X_new)\nprint(y_proba.round(2))\n\ny_pred = model_cnn.predict_classes(X_new)\nprint(y_pred)\n\nprint(np.array(CLASS_NAMES)[y_pred])","5ffaaeb3":"# create model\nmodel = KerasClassifier(build_fn=create_simple_conv_model, verbose=0)\n\n# grid search epochs, batch size and optimizer\noptimizers = ['adam', 'rmsprop', 'SGD']\n# init = ['glorot_uniform', 'normal', 'uniform']\nepochs = [10, 30, 50]\nbatches = [500, 1000, 5000]\nparam_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid_result = grid.fit(X_train_cnn, y_train_cnn)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","9ad8b6c6":"## Multi Layer Perceptron(MLP)\n\nWrite a multi layer perceptron using tensorflow\/keras for fashion MNIST data to classify all 10 classes. Solve for each questions:\nTake epoch= 2000\n\nDefine a function that creates 4 layers using TensorFlow\/keras layers.","5059da46":"* https:\/\/victorzhou.com\/blog\/keras-neural-network-tutorial\/\n* https:\/\/www.kaggle.com\/gpreda\/cnn-with-tensorflow-keras-for-fashion-mnist\n* https:\/\/www.kaggle.com\/yufengg\/fashion-mnist\n* https:\/\/www.kaggle.com\/ankurshukla03\/cnn-for-fashion-mnist\n* https:\/\/machinelearningmastery.com\/implement-perceptron-algorithm-scratch-python\/\n* https:\/\/www.kaggle.com\/jyotiprasadpal\/deep-learning-singlelayer-perceptron?scriptVersionId=31873120","a81e026c":"## Single Layer Perceptron (SLP)\n\n\nDefine a function that can create single layer network called perceptron on Fashion MNIST.\n\n\u2022 Learning rate: 0.0001\n\n\u2022 Epochs: 5000","bf0e0650":"## Convolutional Neural Network","f84f2f86":"## Some necessary functions","d3e91aa5":"## Problem Statement: \nThe dataset is similar to MNIST, but includes images of certain clothing and accessory. The objective is to classify images into specific classes using single layer perceptron, multilayer perceptron and CNN.\n\n## Dataset: \nTotal Images :- 70,000 \n\nTrain Images :- 60,000 \n\nTest Images :- 10,000 \n\nImage Size :- 28 X 28 \n\nDifferent Classes: Classes: 'T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot'","995d4997":"### Question 1: \nA) Reshape and Normalize the data:\n\nReshape the input image \n\n* train; 60000,28 * 28 * 1 \n* test; 10000, 28 * 28 * 1 \n\nChange the type of data to float32 \n\nNormalize the data by dividing with 255 \n\nB) Convert the y_train and y_test to categorical by using keras to_categorical function and define num_classes=10","92f9d1b7":"### Write an experiment that can perform multiple parameters training. Save the intermediate output of each experiment in the dictionary. The key for each experiment will be string combination of (optimizer+Epoch+BatchSize).\n\nHint:- - Epochs = [10,30,50] - Batch Size = [500,1000, 5000] - Optimizer = [Adam, RmsProp, SGD]\n\nUse the standard CNN architecture using above parameters and check the accuracy and performance of various models."}}