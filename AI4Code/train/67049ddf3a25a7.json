{"cell_type":{"67e7b082":"code","a296d86f":"code","25f7fe34":"code","2ee7d32b":"code","2103df13":"code","b8cda73a":"code","96ef9324":"code","590268a5":"code","2f19edfa":"code","2e3b833f":"code","d766f8f2":"markdown","59d39c80":"markdown"},"source":{"67e7b082":"!find ..\/input\/fork-of-simulations-episode-scraper-match-download\/ -name \"*.json\" -print0 | xargs -0 -I {} cp {} .\/","a296d86f":"import pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport collections","25f7fe34":"## You should configure these to your needs. Choose one of ...\n# 'hungry-geese', 'rock-paper-scissors', santa-2020', 'halite', 'google-football'\nCOMP = 'hungry-geese'\nMAX_CALLS_PER_DAY = 3600 # Kaggle says don't do more than 3600 per day and 1 per second\nLOWEST_SCORE_THRESH = 1220","2ee7d32b":"ROOT =\"..\/working\/\"\nMETA = \"..\/input\/meta-kaggle\/\"\nMATCH_DIR = '..\/working\/'\nbase_url = \"https:\/\/www.kaggle.com\/requests\/EpisodeService\/\"\nget_url = base_url + \"GetEpisodeReplay\"\nBUFFER = 1\nCOMPETITIONS = {\n    'hungry-geese': 25401,\n    'rock-paper-scissors': 22838,\n    'santa-2020': 24539,\n    'halite': 18011,\n    'google-football': 21723\n}","2103df13":"# Load Episodes\nepisodes_df = pd.read_csv(META + \"Episodes.csv\")\n\n# Load EpisodeAgents\nepagents_df = pd.read_csv(META + \"EpisodeAgents.csv\")\n\nprint(f'Episodes.csv: {len(episodes_df)} rows')\nprint(f'EpisodeAgents.csv: {len(epagents_df)} rows')","b8cda73a":"# Get top scoring submissions# Get top scoring submissions\nmax_df = (epagents_df.sort_values(by=['EpisodeId'], ascending=False).groupby('SubmissionId').head(1).drop_duplicates().reset_index(drop=True))\nmax_df = max_df[max_df.UpdatedScore>=LOWEST_SCORE_THRESH]\nmax_df = pd.merge(left=episodes_df, right=max_df, left_on='Id', right_on='EpisodeId')\nsub_to_score_top = pd.Series(max_df.UpdatedScore.values,index=max_df.SubmissionId).to_dict()\nprint(f'{len(sub_to_score_top)} submissions with score over {LOWEST_SCORE_THRESH}')","96ef9324":"# Get episodes for these submissions\nsub_to_episodes = collections.defaultdict(list)\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    eps = sorted(epagents_df[epagents_df['SubmissionId'].isin([key])]['EpisodeId'].values,reverse=True)\n    sub_to_episodes[key] = eps\ncandidates = len(set([item for sublist in sub_to_episodes.values() for item in sublist]))\nprint(f'{candidates} episodes for these {len(sub_to_score_top)} submissions')","590268a5":"global num_api_calls_today\nnum_api_calls_today = 0\nall_files = []\nfor root, dirs, files in os.walk(MATCH_DIR, topdown=False):\n    all_files.extend(files)\nseen_episodes = [int(f.split('.')[0]) for f in all_files \n                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']\nremaining = np.setdiff1d([item for sublist in sub_to_episodes.values() for item in sublist],seen_episodes)\nprint(f'{len(remaining)} of these {candidates} episodes not yet saved')\nprint('Total of {} games in existing library'.format(len(seen_episodes)))","2f19edfa":"def saveEpisode(epid):\n    # request\n    re = requests.post(get_url, json = {\"EpisodeId\": int(epid)})\n        \n    # save replay\n    with open(MATCH_DIR + '{}.json'.format(epid), 'w') as f:\n        f.write(re.json()['result']['replay'])","2e3b833f":"r = BUFFER;\n\nstart_time = datetime.datetime.now()\nse=0\nfor key, value in sorted(sub_to_score_top.items(), key=lambda kv: kv[1], reverse=True):\n    if num_api_calls_today<=MAX_CALLS_PER_DAY:\n        print('')\n        remaining = sorted(np.setdiff1d(sub_to_episodes[key],seen_episodes), reverse=True)\n        print(f'submission={key}, LB={\"{:.0f}\".format(value)}, matches={len(set(sub_to_episodes[key]))}, still to save={len(remaining)}')\n        \n        for epid in remaining:\n            if epid not in seen_episodes and num_api_calls_today<=MAX_CALLS_PER_DAY:\n                saveEpisode(epid); \n                r+=1;\n                se+=1\n                try:\n                    size = os.path.getsize(MATCH_DIR+'{}.json'.format(epid)) \/ 1e6\n                    print(str(num_api_calls_today) + f': saved episode #{epid}')\n                    seen_episodes.append(epid)\n                    num_api_calls_today+=1\n                except:\n                    print('  file {}.json did not seem to save'.format(epid))    \n                if r > (datetime.datetime.now() - start_time).seconds:\n                    time.sleep( r - (datetime.datetime.now() - start_time).seconds)\n            if num_api_calls_today>(min(3600,MAX_CALLS_PER_DAY)):\n                break\nprint('')\nprint(f'Episodes saved: {se}')","d766f8f2":"# Simulations Episode Scraper Match Downloader","59d39c80":"This notebook downloads episodes using Kaggle's GetEpisodeReplay API and the [Meta Kaggle](https:\/\/www.kaggle.com\/kaggle\/meta-kaggle) dataset.\n\nMeta Kaggle is refreshed daily, and sometimes fails a daily refresh. That's OK, Goose keeps well for 24hr.\n\nWhy download replays?\n- Train your ML\/RL model\n- Inspect the performance of yours and others agents\n- To add to your ever growing json collection \n\nOnly one scraping strategy is implemented: For each top scoring submission, download all missing matches, move on to next submission.\n\nOther scraping strategies can be implemented, but not here. Like download max X matches per submission or per team per day, or ignore certain teams or ignore where some scores < X, or only download some teams.\n\nPlease let me know of any bugs. It's new, and my goose may be cooked.\n\nTodo:\n- Add teamid's once meta kaggle add them (a few days away)"}}