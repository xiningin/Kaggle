{"cell_type":{"7012bc9d":"code","52b8cbd8":"code","e1d5bd87":"code","fb28f56e":"code","8f736ff0":"code","eee1d126":"code","d45317eb":"code","f56218ed":"code","cfe0eead":"code","10a8ed69":"code","35174223":"code","dfb315f9":"code","af094d28":"code","9a83c059":"code","4fe2c73f":"code","fac7a481":"code","3d85e8a4":"code","1d3338a8":"code","0315d92e":"code","f052f1e1":"code","e5bc1d8b":"code","e83ac54f":"code","c562168b":"code","18002fb3":"code","5fd9a9fb":"code","c9fb9860":"code","d9aad1a8":"code","3ebd8e9a":"code","abe340e5":"code","d387ac01":"markdown","7758ad7e":"markdown","f435ee13":"markdown","492d92c0":"markdown","bdcf8fd5":"markdown","7449639f":"markdown","de42ef6e":"markdown","b9b5f015":"markdown","1633ca51":"markdown","ee962e72":"markdown","c32eb284":"markdown","3c2223ee":"markdown","69271242":"markdown","e06a12eb":"markdown","0bd6cf65":"markdown","29a6cd09":"markdown","bbeeb950":"markdown","ddb413e6":"markdown","bd5fcbe8":"markdown","c150056b":"markdown","e60c9d74":"markdown","bd7babb4":"markdown","e5d53429":"markdown","ab136a87":"markdown","21644f72":"markdown","42d35726":"markdown","a0bf5eb6":"markdown","9fce7232":"markdown","14b0adb0":"markdown","ce1151ea":"markdown","83eb1a6f":"markdown","a8126de1":"markdown","dba90b1c":"markdown","ed78f0b4":"markdown","129f8e5c":"markdown"},"source":{"7012bc9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport sklearn.model_selection\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport warnings\nwarnings.filterwarnings('ignore')  # change once to ignore when publishing \n","52b8cbd8":"from sklearn.metrics import accuracy_score\n\nrituximab = pd.read_csv(\"..\/input\/rituximab.csv\")\nrit=rituximab.copy()\nrit.columns=['FSCH','SSCH','FL1H','FL2H','FL3H','FL1A','FL1W','Time','Gate']\nrit.head(5)\n","e1d5bd87":"\nrit1=rit.drop('Time',axis=1)\nrit1=rit1[rit1['Gate']!=-1]\nrit1.loc[:, \"Gate\"] = rit1.loc[:, \"Gate\"].map({1: 0, 2: 1})\nprint(\"The dataset contains \",rit1.isnull().sum().sum(),\" null values\")\nprint('The first 10 elements of the cleaned dataset are shown below')\nrit1.head(10)\n\n","fb28f56e":"from sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(rit1, test_size=0.2, random_state=42)\ntrain_labels=train_set.copy()['Gate']\ntest_labels=test_set.copy()['Gate']\ntest_no_labels=test_set.copy().drop('Gate',axis=1)\nprint('The train set has ',train_set.shape[0],' data points')\nprint('The test set has ',test_set.shape[0],' data points')\n\n\n","8f736ff0":"\n\ntrain_set.hist(bins=25, figsize=(20,15))\n\nplt.show()\n","eee1d126":"ax=sb.pairplot(train_set,hue='Gate',plot_kws={'alpha': 0.3})\nax.fig.set_size_inches(20, 20);\n","d45317eb":"fig,ax =plt.subplots()\nfig.set_size_inches(11.7, 8.27)\nax = sb.scatterplot(x=\"FSCH\", y=\"SSCH\", hue=\"Gate\",  data=train_set,alpha=0.3)\nax.set_title(\"Scattering Parameters\");","f56218ed":"fig,ax =plt.subplots()\nfig.set_size_inches(11.7, 8.27)\nax = sb.scatterplot(x=\"FL1H\",y=1, hue=\"Gate\",  data=train_set,alpha=0.3)\nax.set_title(\"FL1H Parameter\");\n\nax.set_yticklabels([]);","cfe0eead":"\n\n#correlation matrix\ncorrmat = train_set.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsb.heatmap(corrmat, vmax=.8, square=True,cmap='YlGnBu');\n\n","10a8ed69":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\ntrain_no_labels=train_set.drop('Gate',axis=1)\nprincipalComponents = pca.fit_transform(train_no_labels)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['pc1', 'pc2','pc3'])\nfinalDf = pd.concat([principalDf, train_set['Gate']], axis = 1)\n\ns=pca.explained_variance_ratio_.sum()\n","35174223":"print('The first 3 principal components explain',round(s*100,2),'% of the variance of the data')","dfb315f9":"\n\n\n\n\n\n\n\nax=sb.pairplot(finalDf,hue='Gate',plot_kws={'alpha': 0.3},vars=['pc1','pc2','pc3'])\nax.fig.set_size_inches(20, 20)\n\n\n\n","af094d28":"from sklearn.cluster import KMeans\nKmean = KMeans(n_clusters=2)\nKmean.fit(train_no_labels)\nkmlabels=pd.DataFrame(Kmean.labels_)\ntkl=train_set.copy()\nkmlabels = kmlabels.set_index(train_set.index)\ntkl['km']=kmlabels\n# Running k means on separate occasions may yield different values for the labels, in this case 0 or 1\n# The following is a probably bad way of setting the output of the k means labelling to be the one that I want so the graph labels are correct\n\n\ndef g(row):\n    if (row['km'] == 0):\n        val = 1\n    elif (row['km']==1):\n        val = 0\n   \n    return val\n\nif((tkl['km'].iloc[0]!=0) and (tkl['km'].iloc[1]!=0)):\n    tkl['km']=tkl.apply(g,axis=1)\n\ndef f(row):\n    if (row['Gate'] == row['km'] and row['Gate']==0):\n        val = 'Gate 0'\n    elif (row['Gate'] == row['km'] and row['Gate']==1):\n        val = 'Gate 1'\n    elif row['Gate'] != row['km']:\n        val = 'Misclassified'\n    \n    return val\n\ntkl['colorlabel'] = tkl.apply(f, axis=1)\n\ntkl.head()\nprint('The Scatter plot of the K Means clustering is shown below\\n The green points correspond to points that were assigned to the wrong class')","9a83c059":"ax=sb.pairplot(tkl,hue='colorlabel',plot_kws={'alpha': 0.3},vars=['FSCH', 'SSCH', 'FL1H', 'FL2H', 'FL3H', 'FL1A', 'FL1W','Gate'])\nax.fig.set_size_inches(20, 20);\n","4fe2c73f":"ax=sb.pairplot(data=tkl,hue='colorlabel',vars=['FL1H'])\nax.fig.set_size_inches(12, 8);\n","fac7a481":"from sklearn.metrics import adjusted_rand_score\nadjrand=adjusted_rand_score(tkl['Gate'],tkl['km'])\nrand=(tkl['colorlabel'].value_counts()[0]+tkl['colorlabel'].value_counts()[1])\/(tkl['colorlabel'].value_counts().sum())\nprint('The Rand Index is ',round(rand,4))\nprint('The Adjusted Rand Index is ',round(adjrand,4))","3d85e8a4":"\nfrom pandas.plotting import radviz\nfig = plt.figure( )\nfig.set_size_inches(12,10)\n\ntr=tkl.drop(['Gate','colorlabel'],axis=1)\ntr.head()\nrad_viz = pd.plotting.radviz(tr, 'km',color=['blue','orange'],alpha=0.5)\n","1d3338a8":"\nfrom sklearn.cluster import AgglomerativeClustering\ns=AgglomerativeClustering(n_clusters=2,linkage='single')\ns.fit(train_no_labels)\nrs=adjusted_rand_score(tkl['Gate'],s.labels_)\nprint('Adjusted Rand Index for Hierarchical Clustering with single linkage is ',round(rs,4))\n\na=AgglomerativeClustering(n_clusters=2,linkage='average')\na.fit(train_no_labels)\nra=adjusted_rand_score(tkl['Gate'],a.labels_)\nprint('Adjusted Rand Index for Hierarchical Clustering with average linkage is ',round(ra,4))\n\nc=AgglomerativeClustering(n_clusters=2,linkage='complete')\nc.fit(train_no_labels)\nrc=adjusted_rand_score(tkl['Gate'],c.labels_)\nprint('Adjusted Rand Index for Hierarchical Clustering with complete linkage is ',round(rc,4))\n\nw=AgglomerativeClustering(n_clusters=2,linkage='ward')\nw.fit(train_no_labels)\nrw=adjusted_rand_score(tkl['Gate'],w.labels_)\nprint('Adjusted Rand Index for Hierarchical Clustering with ward linkage is ',round(rw,4))","0315d92e":"label=train_set.copy().pop('Gate')\nlut = dict(zip(label.unique(), ['blue','orange']))\nrow_colors = label.map(lut)\n\nsb.set(rc={'figure.figsize':(12,8)})\ng = sb.clustermap(train_no_labels, col_cluster=False,method='ward',row_colors=row_colors)\nplt.title('Hierarchical Clustering with ward linkage');","f052f1e1":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9,10]}\nknn=KNeighborsClassifier()\ngrid_search=GridSearchCV(knn,param_grid,cv=5,scoring='accuracy',return_train_score=True)\ngrid_search.fit(train_no_labels,train_labels);","e5bc1d8b":"c=grid_search.cv_results_\nprint ('10 K Nearest Neighbors classifiers were trained on 5 stratified subsets of the data and evaluted on the part that was not used for training\\n')\nprint('Evaluating K Nearest Neighbor Classifier predictions...\\n')\nprint ('The mean accuracy score for the 10 classifiers on the 5 folds are shown below\\n')\n\nfor mean_score,params in zip(c['mean_test_score'],c['params']):\n    print (round(mean_score,5),params)\nprint ('The optimal model has mean score',round(c['mean_test_score'][4],4))","e83ac54f":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import SGDClassifier\nlog_clf=SGDClassifier(loss='log',random_state=42)\ncv=cross_validate(log_clf,train_no_labels,train_labels,scoring='accuracy',cv=StratifiedKFold(5))\ns=0\n\nprint('Evaluating Logistic Regression Model predictions...\\n')\nfor i,x in enumerate(cv['test_score']):\n    s+=x\n    print('The accuracy of the logistic classifier on fold',i+1,'is',round(x,5))\n\nprint('\\nThe mean of the accuracy of the model on the 5 folds is',round(s\/5,4))\n\n    \n\n","c562168b":"from sklearn.tree import DecisionTreeClassifier\nparam_grid={'max_depth':[2,4,6,8,10]}\ntree=DecisionTreeClassifier()\ngrid_search=GridSearchCV(tree,param_grid,cv=5,scoring='accuracy',return_train_score=True)\ngrid_search.fit(train_no_labels,train_labels);","18002fb3":"c=grid_search.cv_results_\nprint ('5 Decision Tree Classifiers were trained on 5 stratified subsets of the data and evaluted on the part that was not used for training\\n')\nprint('Evaluating Decison Tree Classifier predictions...\\n')\nprint ('The mean accuracy score for the 5 classifiers on the 5 folds are shown below\\n')\n\nfor mean_score,params in zip(c['mean_test_score'],c['params']):\n    print (round(mean_score,5),params)\nprint('\\nThe optimal depth of the decision tree with the settings chosen is',grid_search.best_params_['max_depth'])\n","5fd9a9fb":"import tensorflow as ts\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense\n\n\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=7, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(train_no_labels, train_labels, epochs=100, batch_size=10,verbose=0)\nprint('Training Multi Layer Perceptron...')\nmodel.evaluate(train_no_labels,train_labels)\nprint('Evaluating model on test set')\ny_pred=model.predict_classes(test_no_labels)\nmlp_score=accuracy_score(test_labels,y_pred)\n\nprint('The Accuracy of the Multi Layer Perceptron on the test set is',round(mlp_score,4))","c9fb9860":"from sklearn.ensemble import RandomForestClassifier\nforest_clf=RandomForestClassifier(n_estimators=100,max_depth=4);\nforest_clf.fit(train_no_labels,train_labels);\nprint('Evaluating Random Forest Classifier predictions on the training set...\\n')\nprint('The accuracy of the Random Forest Classifier with 100 trees on the training set is',round(forest_clf.score(train_no_labels,train_labels),4))\n","d9aad1a8":"\n\n\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf=VotingClassifier(estimators=[('for',forest_clf),('log',SGDClassifier(loss='log',random_state=42)),('knn',KNeighborsClassifier(n_neighbors=5))],voting='hard')\nvoting_clf.fit(train_no_labels,train_labels);\nprint('Evaluating Voting Classifier predictions on the training set...\\n')\nprint('The accuracy of the Voting Classifier on the training set is',round(voting_clf.score(train_no_labels,train_labels),4))","3ebd8e9a":"\n\n\n\ny_test = np.asarray(test_labels)\n\n\n\nknn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(train_no_labels,train_labels)\n\n\nprint('Evaluating models on test set...\\n')\n\n\n    \n\nknn.fit(train_no_labels,train_labels)\nkpred_labels=knn.predict(test_no_labels)\nkmis= np.where(y_test != knn.predict(test_no_labels))\n\n\nprint('Accuracy score for KNeighbors Classifier on test set is',round(100*accuracy_score(test_labels,kpred_labels),4),'%')\n\nlog_clf.fit(train_no_labels,train_labels)\nlpred_labels=log_clf.predict(test_no_labels)\nlmis= np.where(y_test != log_clf.predict(test_no_labels))\n\nprint('Accuracy score for Logistic Regression model on test set is',round(100*accuracy_score(test_labels,lpred_labels),4),'%')\n\n\n\nprint('Accuracy score for Multi Layer Perceptron on test set is',round(100*mlp_score,4),'%')\n\n\n\nvoting_clf.fit(train_no_labels,train_labels)\nvpred_labels=voting_clf.predict(test_no_labels)\nvmis= np.where(y_test != voting_clf.predict(test_no_labels))\n\nprint('Accuracy score for Voting Classifier on test set is',round(100*accuracy_score(test_labels,vpred_labels),4),'%')\n\nforest_clf.fit(train_no_labels,train_labels)\nfpred_labels=forest_clf.predict(test_no_labels)\nfmis= np.where(y_test != forest_clf.predict(test_no_labels))\n\n\nprint('Accuracy score for Random Forest Classifier on test set is',round(accuracy_score(test_labels,fpred_labels)*100,4),'%')\n\n\n","abe340e5":"misclassified=set()\nmisclassified.update(vmis[0])\nmisclassified.update(kmis[0])\nmisclassified.update(fmis[0])\nmisclassified   #This set contains indices of misclassified elements in y_test\n                #Must retrieve indices from test_labels\n    \nmisclassified_t_indices=set()\nfor val in misclassified:\n    misclassified_t_indices.add(test_labels.index[val])\n    \n\nprint('Indices of misclassified points by the models are',misclassified_t_indices,'\\n')     #These are the indices where the cell is misclassified by the f,k,v models in test_set\n\n\ndef get_sigma_fl1h(idx):\n    gate=rit1[rit1.index==idx]['Gate'].values[0]\n    classified_gate=1-gate\n    \n    mean=rit1[rit1['Gate']==gate]['FL1H'].mean()\n    classified_mean=rit1[rit1['Gate']==classified_gate]['FL1H'].mean()\n    \n    std=rit1[rit1['Gate']==gate]['FL1H'].std()\n    cls_std=rit1[rit1['Gate']==classified_gate]['FL1H'].std()\n    val=rit1.loc[idx]['FL1H']\n    return (val-mean)\/std,(val-classified_mean)\/cls_std\n\nfor i,val in enumerate(misclassified_t_indices):\n    print ('Misclassified data point',i+1,'is',get_sigma_fl1h(val)[0],'standard deviations away from the mean of the FL1H parameter of it\\'s correct gate\\n')\n    print ('Misclassified data point',i+1,'is',get_sigma_fl1h(val)[1],'standard deviations away from the mean of the FL1H parameter of it\\'s classified gate\\n')","d387ac01":"<a id=\"scater\"><\/a>\n### __Scatter Plots__\n\n\n- __Scatter Matrix__\n\nBelow is a matrix of the scatter plots of all of the variables in our dataset. Some of the scatter plots show good sepration between the two gates which will be useful for analysis. In particular, it can now be seen that the two modes of the FL1H histogram correspond to the two different gates.The FL1H seems to be a very important parameter in the unknown gating protocol as there is a lot of separation between the gates in it's scatter plots. Also a non zero FL1A variable tends to correspend to the data point belonging to Gate 2.\n\n\n\n","7758ad7e":"<a id=\"mlp\"><\/a>\n- __Multi Layer Perceptron__\n\nA multi layer perceptron is a type of Artificial Neural Network that consists of an input layer, an output layer and one or more hidden layers with each layer consisting of artificial neurons. Artificial Neural Networks are biologically inspired learning methods that mimic the human brain. An MLP with 12 neurons in the hidden layer is trained below.\n\n\n","f435ee13":"<a id=\"km\"><\/a>\n-  __K Means Clustering__\n\nK means clustering splits our data into k disjoint partitions according to a specific algorithm. The clustering algorithm seeks to split the data into k clusters so as to minimize the within cluster variance. In this case the clustering is done in 7 dimensional space and so is hard to visualise.The data usually needs to be scaled but in this case the variables are of a similar scale so standardisation is not needed. The training set data was delabelled and then k means clustering was performed on it. The output of this clustering was two labelled groups, one of which corresponded to Gate 0 and the other to Gate 1. This correspndence was identified and then points which were correctly classified by the k-means clustering algorithm were plotted in blue and orange for Gates 0 and 1 respectively. Points that were misclassified were labelled as green. External evaluation of the k means clustering was applied where the clusters were compared to the known class labels. \n\nThe K means algorithm works by initally setting k random means $m^1_1,m^1_2,m^1_3,...,m^1_k$ in the 7D space of variables.\n\n__Assignment Step__<br>\nEach observation $x_i$ is assigned to the mean $m_j$ which is closest to the observation in the space according a certain distance metric. In this case the Euclidean metric is used.This step generates k sets $S_i$ that contain all the elements that are closest to the mean $m_i^t$.\n$$d(x_i,x_j)=\\sum_{n=1}^{7}(x_{i(n)}-x_{j(n)})^2$$\n\n\n__Update Step__<br>\nThe means of the sets are then calculated and set as the means for the next assignment step.\n$$m_i^{t+1}=\\frac{1}{|{S_i^t}|}\\sum_{x_j\\in{S_j^{t+1}}}x_j$$\n\nThe algorithm is continued until converge although it is not guaranteed top converge.\n\n","492d92c0":"<a id=\"res\"><\/a>\n## 7. Results \n\nThe various classifiers are now used on the test set to predict new values. This will give a true measure of the predictive power of the models on data that it has not seen before.","bdcf8fd5":"Only the Cluster plot for the ward linkage method is shown as it is the best method according to the Adjusted Rand Index. The clusters are displayed on the left in a dendrogram. It's clear that there is a few data points mislabelled. Looking at the data points that are in the blue cluster but should be in the orange cluster it is clear why. The FL3H values are outliers within the cluster. The orange cluster tends to have higher FL3H values and this is why they are misclassified. These high readings could be down to an error in the detection equipment or random chance. Recalibration of the measurement equipment or increasing the sample size should resolve these issues if they are the cause.\n\n- This plot again shows that data points with a high FL1H value tend to belong to Gate 1.\n- It also shows that data points with a low FL1A value are more likely to belong to Gate 0","7449639f":"This result is greater than any of the indidual Decision Trees shown above.","de42ef6e":"<a id=\"lr\"><\/a>\n- __Logictic Regression__\n\nA Logistic Regression model is a statistical model used to estimate the probability of a data point belonging to a certain class. The Logistic Regression model attempts to capture the relationship between the input variables and class in the training phase. This knowledge is then used to predict the class of new data points. Logistic Regression is similar to linear regression but the output is limited to the the range $(0,1)$. The output correspnds to the probability of a data point belonging to a particular class.\n\n*Stochastic Gradient Descent is used to train this model due to it's speed of convergence*","b9b5f015":"It appears there may be a relationship here as it appears for a given FSCH value, data points belonging to gate 1 tend to have a lower SSCH value than gate 2.\n\n\n- __FL1H Parameter__\n","1633ca51":"This scatter plot shows that the two modes of the histogram of the FL1H parameter correspond to the different gates. Data points belonging to gate 1 tend to have a higher FL1H parameter value than those of gate 2. This will be useful in a model classifying new data points.","ee962e72":"It can be seen from the correlation matrix that the FL1H parameter is indeed very important in determining what Gate a new data point will belong to. Also note that the SSCH variable has a very low correlation value with the gate variable and most other variables and so will probabbly be the least important variable in further analysis.","c32eb284":"- __Scattering parameters__\n\nIt was mentioned above that the forward scattering parameter is related to the size of the cell and the side scattering parameter is related to the internal complexity of the cell. Seeing as different cell types may differ in size and internal structure, these parameters should help to differentiate between cells.\n","3c2223ee":"This shows that all of the models achieved above 95% accuracy in classifying the cell as the same class that the reseacher who made this dataset did. It does not tell us the actual cell types because it is unknown whether the researcher's gating protocol was effective or if the flow cytometer was functioning correctly.\n<a id=\"misclf\"><\/a>\n- __Misclassified points__\n\nThe points that were misclassified by some of the models were analysed below. The top 3 most accurate models were chosen and the points that were misclassified by them were analysed. The FL1H parameter is very important for classifying the cells as is shown in the correlation matrix. Thus if a cell has a FL1H parameter value that is an outlier for it's group then presumably it may be misclassified. This presumption is verified below.\n\n","69271242":"<a id=\"hc\"><\/a>\n- __Hierarchical Clustering__\n\nHierarchical clustering creates a tree-like structure to create groups of observations which are similar.The aim of hierarchical clustering is to separate the data into clusters that have small variance within clusters and a large distance betwen clusters. Hierarchical clustering works by assigning each data point $x_i$ id assigned to a group on its own $S_i$ . The distance between groups is measured between the closest points is measured  using the Euclidean metric and the distance between groups is measured using either Single,Average,Complete or Ward linkage. The Single Average and Complete methods find the minimum,average and maximum distance respectively between points in each group. The Ward method determines that proximity between two clusters $S_i,S_j$ is the magnitude by which the summed square $\\sum_S$ in their joint cluster $S_{12}$ will be greater than the combined summed square in these two clusters.\n$$ \\sum_S=\\sum_{x_j\\in{S}}(x_j-\\bar{x})^2$$\n\n\nThe closest groups according to the linkage criterion are then joined and combined to form a single new group. This process is repeated until there is only one group remaining.This method is sensitive to the scale of the data but there is no need to scale the data in this case.\n","e06a12eb":"<a id=\"pca\"><\/a>\n### __Principal Component Analysis__\n\n","0bd6cf65":"<a id=\"conc\"><\/a>\n## 8. Conclusion\nFlow Cytometry data from an experiment using the drug Rituximab was analysed to determine which gate a particular data point\/cell belonged to.These gates were assigned by the researcher according to an unknown gating protocol. Both unsupervised and supervised methods were employed to investigate the structure of the data and to see if each gate had some defining characteristics. It was found that the FL1H parameter was important in determing which gate a particular cell belonged to. The supervised models employed all achieved over 90% accuracy on the test set.","29a6cd09":"<a id=\"split\"><\/a>\n### __Split Data__\nThe data is now split into a training set to find parameters for the models that are used and a test set which is used to test the ability of the models to generalise to unseen data. An 80\/20 split is used in this case.\n","bbeeb950":"<a id=\"knn\"><\/a>\n- __K Nearest Neighbors__\n\nThe K Nearest Neighbors model is a simple model where a data point $x_i$ is assigned to a class $C_j$ according to the following algorithm.\n\n1.  The K nearest points are calculated by evaluating the distance betwenn $x_i$ and it's neighbouring points.\n2. The point $x_i$ is assigned to the class $C_j$ which most frequently occurs among the K nearest points\n\n*The Euclidean metric is used again in this case*","ddb413e6":"This shows that misclassified points tend to have FL1H values that are not typical for points of it's correct gate. It also shows that these points tend to be a lesser number of standard deviations away from the mean of their classified class than their actual class.","bd5fcbe8":"- __Correlation Matrix__\n\nThe correlation matrix is a graphical representation of the strength of the relationship between variables","c150056b":"# Rituximab Data Analysis\n\n---\n\n\n\n\n30\/10\/2019\n\n1. [Introduction](#Introduction)\n    - [About Rituximab](#Rit)\n    - [Flow Cytometry](#flow)\n    - [Gating](#gating)\n    - [Problem Definition](#def)\n2. [Data](#Data)\n    - [Load Data](#Load Data)\n    - [Data Description](#description)\n    - [Clean Data](#clean)\n    - [Split Data](#split)\n3. [Exploratory Data Analysis](#eda)\n    - [Variable Histograms](#hist)\n    - [Scatter Plots](#scatter)\n    - [Principal Component Analysis](#pca)\n4. [Unsupervised Learning Models](#unsup)\n    - [K Means Clustering](#km)\n    - [Hierarchical Clustering](#hc)\n5. [Supervised Classification Models](#sup)\n    - [K Nearest Neighbors](#knn)\n    - [Logistic Regression](#lr)\n    - [Decision Tree Classifier](#dt)\n    - [Multi Layer Perceptron](#mlp)\n6. [Ensemble Methods](#em)\n    - [Bagging Methods](#bag)\n    - [Voting Classifier](#vote)\n7. [Results](#res)\n    - [Misclassified points](#misclf)\n8. [Conclusion](#conc)\n    \n    \n    ","e60c9d74":"<a id=\"Introduction\"><\/a>\n## 1. __Introduction__\n\n---\n<a id=\"Rit\"><\/a>\n### __About Rituximab__\nRituximab is a medication used to treat certain types of cancer and autoimmune diseases such as non-Hodghkin's lymphoma, chronic lymphocytic leukemia and rheumatoid arthritis. It is a monoclonal antibody that targets the surface of leukaemia and lymphoma cells.Rituximab works by targeting a protein called CD20. It sticks to the CD20 proteins that it finds which allows the patient's immune system to target these cells and kill them. Rituximab primarily tends to affect the malignant B cells with the highest levels of CD20.\n \n <a id=\"flow\"><\/a>\n ### __Flow Cytometry__\nOriginally developed in the 1960's flow cytometry is laser-based technology to analsye the characteristics of heterogenous cell populations. It used extensively in the life sciences as it as a particularly powerful method that allows researchers to quickly analyze cell populations in a liquid medium to identify, serparate and characterise different cell types. The researcher can simulataneously measure multiple parameters about single cells including cell size and cell volume. Different cell subpopulations can also be counted and sorted using this technique.\n \nThe cells are stained before entering the flow cytometer with flurophores which recognize a target feature on or in the cell. In a flow cytometer, a sample of cells that are suspended in a fluid are passed through a small nozzle. This produces a very thin stream of cells that travel past a laser beam, one cell at a time. The laser light is scattered by the cells and detected using insturments to measure forward scatter and side scatter which measure the amount of light that passes through the sample and the light that is scatter orthonal to the beam direction. The amount of scattering depends on the cell size and granuarity. Flourescence detectors also measure the flourescence emitted from the stained cells.\n\n<a id=\"gating\"><\/a>\n### __Gating__\nThe data generated by flow cytometers can be plotted in multiple dimensions. The regions on these plots can then be separated by creating a series of subset extractions. This process is called gating and is used to identify distinct populations of cells within a dataset. In summary, a gate or a region is a boundary drawn around a subpopulation to isolate events for analysis or sorting. The data points given in the Rituximab dataset are gated according to an unknown gating protocol.\n<a id=\"def\"><\/a>\n### __Problem Definition__\nThe goal of this notebook is to come up with a model to classify a new data point as a particular gate (without knowing the underlying gate protocol) given it's measured values. \n","bd7babb4":"The scatter matrix of the first 3 Principal Components is plotted below. There isn't any clear separation shown in these scatter plots so the Principal Component Analysis will not be used directly in building a classification model.","e5d53429":"<a id=\"Data\"><\/a>\n## 2. __Data__\n---\n<a id=\"Load Data\"><\/a>\n### __Load Data__\nThe dataset is loaded into a Dataframe from a CSV File. The first 15 values of the dataset are printed in a table below.\n","ab136a87":"The Voting Classifier is actually worse in this case than the Random Forest Classifier in this case but not by a significant amount.","21644f72":"<a id=\"em\"><\/a>\n## 6. Ensemble Methods\n\nThe basic idea of Ensemble methods used in classification is that the aggregate of the model's answers are likely to be better than single model's answer. For example if you had 1000 indepedent classifiers which were each correct only 51% of the time and if you choose to classify a new data point as the class which it was frequently classified as then this ensemble classifier would be right about 75% of the time.However this is only true if the classifiers made uncorrelated errors which is not true because they were trained on the same data. Ensemble methods work when the component classifiers are as independent as possible. This can be done by trainiing them on different subsets of data or using completely different algorithms.\n<a id=\"bag\"><\/a>\n- __Bagging Methods__\n\nThe bagging method uses the former way of producing a varied set of classifiers by training them on different subsets of data.These classifiers then vote on which class they think a new data point is. This data point is assigned to the class with the majority of votes.The Random Forest Classifier is an example of an ensemble bagging method. A Random Forest Classifier is a bagging method where many Decision Tree Classifiers(with a slight variation in how they are trained to speed up the algorithm) are trained on different subsets of data.The Trees then vote on the classes and the mode is chosen. The depth of the Decision Trees used below is set as the optimal depth found above.\n\n","42d35726":"The optimum K Nearest Neighbors Classifer on the training set is when $K=5$.","a0bf5eb6":"<a id=\"hist\"><\/a>\n### __Variable Histograms__\nAll of the histograms are positively skewed. Notably FL1H has a clear bimodal distibution which should be explored. Looking at the gate histogram above, approximately 850 data points are classified as gate 1 and approximately 200 are classified as gate 0. This means that if a extremely basic classification model is chosen where every input is assigned to Gate 0 then we would expect roughly $(\\frac{850}{1099}*100)\\% \\approx 73\\% $ accuracy. This is used as a minimum benchmark for any model that is analysed as a model with lesser accuracy than this is very poor.\n","9fce7232":"<a id=\"unsup\"><\/a>\n## __4. Unsupervised Learning Models__\n---\n\nThe goal of the models presented in this section is to identify subsets of the training set which are similar to the identified gates using unlabelled data. For example, the scatter plot of the FL1H parameter with the Gate label shows that data points with label 0 tend to have an FL1H value of less than 400 and ones with label 1 tend to have a value greater than 400. There is a lot of uncertainty labelling a new data point with an FL1H value of around 400 because the distributions merge significantly around 400. Therefore additional features will be need to classify these points.\n\n\n\n\n\n- __Performance Metrics__\nIn this analysis external evaluation is carried out whereby the labels given to the clusters are compared to the actual labels.The clustering models are evaluated using the Adjusted Rand Index. The Rand Index measures the proportion of correct decisions made by the algorithm and the Adjusted Rand Index is the corrected for chance version of the Rand Index. It needs to be corrected because even a random assignmentcan lead to a large Rand Index value.\n\n$$ \\text{Rand Index}=\\frac{TP+TN}{TP+TN+FP+FN}$$\n\nwhere \n- $TP=\\text{True Positives}$ \n- $TN=\\text{True Negatives}$\n- $FP=\\text{False Positives}$\n- $FN=\\text{False Negatives}$\n\n$$\\text{Adjusted Rand}=\\frac{\\text{Rand Index}-\\text{Expected Rand Index}}{\\max{\\text{Rand Index}}-\\text{Expected Rand Index}}$$\n","14b0adb0":"<a id=\"sup\"><\/a>\n## 5. Supervised Classification Methods\n---\n\nSupervised Learning is a subset of machine learning where labelled training data is used to infer a model which is used to classify new data points. The goal is to be able to provide accurate predictions of unseen data points therefore the must generalize from the structure of the training data to produce reasonable outputs.\n\nK-Fold Cross Validation is where the training set is split randomly split K times(into K folds) and at each iteration a subset of it is used for training a learning model and the other set is used for evaluation of the model. It helps to eliminate overfitting of a model where the model poor resultson unseen data but gives good results on the training data.The following models will be evaluated using the Accuracy scoring metric given below. In the models below, stratified K fold validation is used to ensure that each fold is representative of the underlying data.\n\n- __Performance Metric__\nThe following performance metric is used to evaluate the effectiveness of the models.\n\n$$\\text{Accuracy}=\\frac{\\text{Number of Correct Decisons Made}}{\\text{Total Decisions Made}}$$\n\n","ce1151ea":"The results of the k means clustering above show:\n\n- The subset of datapoints where $ 0 \\leq \\text{FL1H} \\leq  300$ are similar to Gate 0\n- The subset of datapoints where $ 600 \\leq \\text{FL1H}$ are similar to Gate 1\n- There are many other subsets which seem to be well separated linearly in the 2D scatter plots such as the FL1H\/FL2H scatter plot.\n\n*If the data were plotted in the full 7 dimensions instead of 2-D scatter plots, it would probably reveal much greater separation between the clusters than is visible in the 2-D plots*\n","83eb1a6f":"<a id=\"vote\"><\/a>\n- __Voting Classifiers__\n\nA voting classifier consists of many individual classifiers.The voting classifiers aggregates the votes of each of these classifiers and predicts the class that gets the most votes. This particular type of voting classifer is a hard voting classifier since it doesnt take into account the class probabilities.In the Voting Classifier below, the component classifiers are a K Nearest Neighbours classifier with $K=5$, a Logistic Regression model and a Random Forest Classifier. ","a8126de1":"<a id=\"dt\"><\/a>\n- __Decision Tree Classifier__\n\nA decison tree classifier is a tree structure where each internal  non-leaf node represents an input variable. There are edges connecting the node to child nodes. When a new data point is inputted into the trained classier and reaches a new node, a decison is made as to which child node to go to. This is repeated at each step until the point reaches a leaf node and is classified. The decisions to be made at each node are determined during the training prcoess.These rules are chosen mathematically so that variables' values are selected to get the best split to differentiate observations based on the dependent variable. This ensures that the classification should have a small error.\n\n","dba90b1c":"<a id=\"description\"><\/a>\n\n## __Data Description__\n\n-  __Scattering Parameters__\nThe first two parameters (FSCH\/SSCH) are measurements of the scattering of the laser beam in the laser beams direction and orthogonal to the laser beam's direction. The amount of scattering in the forward direction (FSCH) is dependent on the cell size.The scattering in the orthogonal direction is also dependent on the size of the cell but is additionally dependent on the cell's internal structure and it's membrane. These parameters can thus be simplified as measures of a cell's size (FSCH) and a cell's internal complexity (SSCH) however in reality this is not taking into account the full complexity of the problem.\n\n-  __Flourescence Parameters__\nFL1H\/FL2H\/FL3H\/FL1A\/FL1W are flourescence parameters. When a fluorescent dye is conjugated to a monoclonal antibody, it can be used to identify a particular cell type based on the individual properties of the cell.In a mixed population of cells, different fluorochromes can be used to distinguish separate subpopulations and this is why multiple flourescence parameters are recorded. As the fluorescing cell passes through the laser beam, it creates a peak or pulse of photon emission over time. These are detected and converted to a voltage pulse, known as an event. The total pulse height and area is measured by the flow cytometer. The measured voltage pulse area will correlate directly to the intensity of fluorescence for that event. Each event is given a channel number depending on its measured intensity; the more intense the fluorescence, the higher the channel number the event is assigned.\n\n\n-  __Time__\nThe time variable is not used in the analysis below but it can be used to detect physiological changes in cell populations when a given sample is analysed at different times. For example, it could detect the change in the proportion of healthy cells to malignant cells in time after a drug treatment has been applied.\n\n-  __Gate__\nThis is the label given to each cell by the researcher according to a specific gating protocol. The gating protocol used is an unknown.\n\n","ed78f0b4":"<a id=\"eda\"><\/a>\n## __3. Exploratory Data Analysis__\n---\nThe data is now analysed with both graphical and descriptive statistics to look for  potential problems , patterns and correlations with the target variable (Gate).\n\n\n","129f8e5c":"<a id=\"clean\"><\/a>\n## __Clean Data__\n\n- In this data two gates have been identified and labelled as 1 and 2. These will now be relabelled as 0 and 1 respectively.Data points with a Gate labelled as -1 are deemed to be noisy data points and are discarded.\n- In the analysis below the Time variable will not be explored and is discarded. \n- The dataset is also checked for null values.\n\n\n\n"}}