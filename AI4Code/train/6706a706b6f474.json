{"cell_type":{"926cf9d2":"code","4fded81a":"code","74486f18":"code","9112e496":"code","f130e484":"code","05b12d61":"code","be4d09be":"code","acf867f7":"code","53c73aa8":"code","d0270754":"code","ea3c0b78":"code","bf6704de":"code","b628fc34":"code","a66de9ca":"code","c786532a":"code","2723914e":"code","d3a0397b":"code","45014913":"code","76708845":"code","8a9564a3":"code","0912247b":"code","28ae134b":"markdown","00f9bad9":"markdown","2d226b0c":"markdown","7e16805f":"markdown","ae2cc7f3":"markdown","55745a2a":"markdown","d4422bc2":"markdown","19217510":"markdown","39a8fee8":"markdown"},"source":{"926cf9d2":"from sklearn.datasets import load_iris\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nplt.style.use(\"bmh\")","4fded81a":"iris_data = load_iris()\niris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\niris_df[\"Class\"] = iris_data.target","74486f18":"iris_df.head()","9112e496":"iris_df.shape","f130e484":"iris_data.target_names","05b12d61":"# check missing data\niris_df.isnull().sum()","be4d09be":"sns.pairplot(iris_df, hue=\"Class\", corner=True)","acf867f7":"iris_df.dtypes","53c73aa8":"iris_df.describe()","d0270754":"# Split data","ea3c0b78":"iris_data_X = iris_data.data\niris_data_X","bf6704de":"iris_data_y = iris_data.target\niris_data_y","b628fc34":"Xtrain, Xtest, ytrain, ytest = train_test_split(iris_data_X, iris_data_y, test_size=0.3, random_state=2)","a66de9ca":"lrparam = {\"fit_intercept\": [True, False], \"normalize\": [True, False], \"copy_X\": [True, False]}\nlrgrid = GridSearchCV(LinearRegression(), lrparam, cv=10)\nlrgrid.fit(Xtrain, ytrain)\nprint(\"Best Linear Regression score:\", lrgrid.best_score_)\nprint(\"Best Linear Regression estimator:\", lrgrid.best_estimator_)","c786532a":"logrparam = {\"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\"], \"C\": np.linspace(0.00002, 1, 100)}\nlogrrand = RandomizedSearchCV(LogisticRegression(max_iter=1000), logrparam, cv=5, n_iter=15, scoring=\"accuracy\")\nlogrrand.fit(Xtrain, ytrain)\nprint(\"Best Logistic Regression score:\", logrrand.best_score_)\nprint(\"Best Logistic Regression estimator:\", logrrand.best_estimator_)","2723914e":"svcparam = {\"C\": [0.1, 1, 10, 100, 1000], \"kernel\": [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]}\nsvcgrid = GridSearchCV(SVC(), svcparam, cv=5, scoring=\"accuracy\")\nsvcgrid.fit(Xtrain, ytrain)\nprint(\"Best SVC score:\", svcgrid.best_score_)\nprint(\"Best SVC estimator:\", svcgrid.best_estimator_)","d3a0397b":"gnbparam = {\"var_smoothing\": np.logspace(0, -9, num=100)}\ngnbgrid = GridSearchCV(GaussianNB(), gnbparam, cv=10, scoring=\"accuracy\")\ngnbgrid.fit(Xtrain, ytrain)\nprint(\"Best GaussianNB score:\", gnbgrid.best_score_)\nprint(\"Best GaussianNB estimator:\", gnbgrid.best_estimator_)","45014913":"dtparam = {\"max_depth\": [3, None], \"max_features\": randint(1, 4), \"criterion\": [\"gini\", \"entropy\"]}\ndtrand = RandomizedSearchCV(DecisionTreeClassifier(), dtparam, cv=5, n_iter=15, scoring=\"accuracy\")\ndtrand.fit(Xtrain, ytrain)\nprint(\"Best DecisionTreeClassifier score:\", dtrand.best_score_)\nprint(\"Best DecisionTreeClassifier estimator:\", dtrand.best_estimator_)","76708845":"rfparam = {\"n_estimators\": [int(x) for x in np.linspace(200, 2000, 10)], \"max_features\": [\"log2\", \"sqrt\", \"auto\"],\n           \"criterion\": [\"entropy\", \"gini\"], \"max_depth\": [int(x) for x in np.linspace(10, 110, 11)],\n           \"min_samples_split\": [2, 3, 5, 10], \"min_samples_leaf\": [1, 2, 4, 5, 8], \"bootstrap\": [True, False]}\nrfrand = RandomizedSearchCV(RandomForestClassifier(), rfparam, cv=5, n_iter=5, scoring=\"accuracy\")\nrfrand.fit(Xtrain, ytrain)\nprint(\"Best RandomForestClassifier score:\", rfrand.best_score_)\nprint(\"Best RandomForestClassifier estimator:\", rfrand.best_estimator_)","8a9564a3":"prediction = svcgrid.best_estimator_.predict(Xtest)","0912247b":"mat = confusion_matrix(ytest, prediction)\nsns.heatmap(mat, square=True, annot=True)\nplt.xlabel(\"Predicted value\")\nplt.ylabel(\"True value\")","28ae134b":"There aren't missing values in this dataset.","00f9bad9":"Since this is a supervised learning problem, I will try Linear Regression, Logistic Regression, SVC, GaussianNB, DecisionTreeClassifier and RandomForestClassifier.","2d226b0c":"# Iris","7e16805f":"Notes:\n* The dataset contains 3 classes of iris and 50 instances of each class.\n* Classes:\n  * 0: Setosa\n  * 1: Versicolor\n  * 2: Virginica","ae2cc7f3":"Since every column is numeric, I won't make changes.","55745a2a":"The best estimator is SVC.","d4422bc2":"Let's see how the variables relate to each other.","19217510":"The objective of this notebook is to predict the class of iris plant.","39a8fee8":"The dataset is from sklearn.datasets."}}