{"cell_type":{"78e5a4f8":"code","c4a262f6":"code","a32faf05":"code","0d20ec62":"code","ab1ab9c2":"code","91cd3c98":"code","ac70a378":"code","17250bc0":"code","9d6e5eb6":"code","49dd2359":"code","004b62ae":"code","ac3b9d07":"code","e6677fa5":"code","98b472b1":"code","c056c874":"code","b7778625":"code","62a7b8e1":"code","3eab407e":"code","952ef327":"code","90f4692a":"code","ee7c225f":"code","a5b1f0c9":"code","8b57a449":"code","f1d9b49c":"code","b28af545":"code","06084a3f":"code","ae686d16":"code","b64f0c03":"code","8d116b4f":"code","dc47bd5d":"markdown","1f9afcb1":"markdown","b808c710":"markdown","25d1cdda":"markdown","49fa08bd":"markdown","c63a5a8c":"markdown","8098fe60":"markdown","41f86c3c":"markdown","32779dc5":"markdown","a071d755":"markdown","a6fd92ba":"markdown","0056f4a0":"markdown","3c1c6535":"markdown","9cd0d13c":"markdown","41ff3db1":"markdown","72951672":"markdown","1b8d5b6e":"markdown","4df7ce2d":"markdown","fd2d2dbb":"markdown","4a60b503":"markdown","afba41c0":"markdown","f4b43ba8":"markdown","3fbfeacb":"markdown","f455f329":"markdown","d53f2faa":"markdown","7251d0cf":"markdown","ad89d17a":"markdown","e0a56e83":"markdown"},"source":{"78e5a4f8":"# Modules for data manipulation and shaping  \nimport numpy as np\nimport pandas as pd\n\n# Modules for plotting \nimport matplotlib.pyplot as plt\nimport seaborn as sns","c4a262f6":"# The following code omits python warnings from outpoint  \nimport warnings\nwarnings.filterwarnings(\"ignore\")","a32faf05":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","0d20ec62":"x = train.loc[:, train.columns != 'SalePrice']\ny = train[['Id', 'SalePrice']].set_index('Id')['SalePrice']","ab1ab9c2":"combined_data = pd.concat([x, test])\ncombined_data.set_index('Id', inplace=True)","91cd3c98":"contains_missing_values = list(combined_data.isna().sum()[combined_data.isna().sum() != 0].index)","ac70a378":"NA_list = ['Exterior2nd','MasVnrType','FireplaceQu','Alley','PoolQC','Fence','MiscFeature','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','GarageCond']\nMode_list = ['MSZoning','Utilities','Exterior1st','Electrical','KitchenQual','Functional','GarageYrBlt','SaleType']\nZero_list = ['LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']\n\n# Each loop runs thru the respective list and imputes the np.nan\u2019s with the relevant approache\nfor a in NA_list:\n    combined_data[a].fillna('NA', inplace = True)\nfor a in Mode_list:\n    combined_data[a].fillna(combined_data[a].mode()[0], inplace = True)\nfor a in Zero_list:\n    combined_data[a].fillna(0, inplace = True)","17250bc0":"# The list comprehension below generates a list of columns whose entries are of the object data type\none_hot_list = [a for a in combined_data.columns if combined_data[a].dtype == 'object']\n\n# The following code implement one-hot encoding\ncombined_data = pd.get_dummies(combined_data, columns = one_hot_list, prefix = one_hot_list, drop_first = True)","9d6e5eb6":"combined_data = (combined_data - combined_data.min())\/(combined_data.max()-combined_data.min())","49dd2359":"from scipy.stats import skew","004b62ae":"sns.distplot(y)\nprint(\"Skew :\",round(skew(y),4))","ac3b9d07":"sns.distplot(np.log1p(y))\nprint(\"Skew :\", round(skew(np.log1p(y)),4)) ","e6677fa5":"# The following code log transformes our data\nx = np.log1p(combined_data.iloc[:1460,:])\ny = np.log1p(y)\ntest = np.log1p(combined_data.iloc[1460:,:])","98b472b1":"# This code initializing a model using all variables as predictors.\n\nimport statsmodels.api as sm\n\nlm = sm.OLS(y,x)\nmodel = lm.fit()\n\n# This line retrieves the predictor name associated with the coefficient having the highest p-value \npredictor = model.pvalues.sort_values(ascending=False).index[0]\n\n# This line retrieves the highest p-value\nmax_pvalue = model.pvalues.sort_values(ascending=False)[0]","c056c874":"while max_pvalue > 0.05:\n    x = x.loc[:, x.columns != predictor]\n    test = test.loc[:, test.columns != predictor]\n    \n    lm = sm.OLS(y,x)\n    model = lm.fit()\n\n    predictor = model.pvalues.sort_values(ascending=False).index[0] \n    max_pvalue = model.pvalues.sort_values(ascending=False)[0]","b7778625":"# Modules that implement machine learning algorithms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n# Modules for model evaluation and parameter tuning \nfrom sklearn.model_selection import  cross_val_score, GridSearchCV","62a7b8e1":"import random\nrandom.seed(123)","3eab407e":"LR_model = LinearRegression()\nRidge_model = Ridge()\nLesso_model = Lasso()\nDTR_model = DecisionTreeRegressor()\nGBR_model = GradientBoostingRegressor()\nXGB_model = XGBRegressor()","952ef327":"# The following code calculates the average R squared value using cross validation\n\nLR_score = round(np.average(cross_val_score(LR_model, x, y, cv = 5, scoring = 'r2')),4)\nRidge_score = round(np.average(cross_val_score(Ridge_model, x, y, cv = 5, scoring = 'r2')),4)\nLesso_score = round(np.average(cross_val_score(Lesso_model, x, y, cv = 5, scoring = 'r2')),4)\nDTR_score = round(np.average(cross_val_score(DTR_model, x, y, cv = 5, scoring = 'r2')),4)\nGBR_score = round(np.average(cross_val_score(GBR_model, x, y, cv = 5, scoring = 'r2')),4)\nXGB_score = round(np.average(cross_val_score(XGB_model, x, y, cv = 5, scoring = 'r2')),4)","90f4692a":"print(' ------ R squared ------ ',)\nprint('Linear Regression    :',LR_score)\nprint('Ridge Regression     :',Ridge_score)\nprint('Lesso Regression     :',Lesso_score)\nprint('Decision Tree        :',DTR_score)\nprint('Gradient Boosting    :',GBR_score)\nprint('X-Gradient Boosting  :',XGB_score)","ee7c225f":"print('--- root mean squared logarithmic erro ---',)\nprint('Ridge Regression     :',round(np.sqrt(-1*np.average(cross_val_score(Ridge_model, x, y, cv = 5, scoring = 'neg_mean_squared_log_error'))),4))\nprint('Gradient Boosting    :',round(np.sqrt(-1*np.average(cross_val_score(GBR_model, x, y, cv = 5, scoring = 'neg_mean_squared_log_error'))),4))\nprint('X-Gradient Boosting  :',round(np.sqrt(-1*np.average(cross_val_score(XGB_model, x, y, cv = 5, scoring = 'neg_mean_squared_log_error'))),4))","a5b1f0c9":"# This code defines a dictionary containing the parameters to be tuned as well as the values to test\nparameters = {'alpha':[1,1.5,2]}\n\n# This code implants the grid search algorithm\nclf = GridSearchCV(Ridge_model, parameters, cv = 5, scoring = 'neg_mean_squared_log_error')\nclf.fit(x,y)\n\nprint(\" ------ Ridge Regression ------ \")\nprint(\"Best Score :\",round(np.sqrt(-1*clf.best_score_),4))\nprint('alpha      :', clf.best_estimator_.alpha)\n\nbest_alpha = clf.best_estimator_.alpha","8b57a449":"parameters = {\n    \"max_depth\":[2,3],\n    \"n_estimators\":[150, 175, 200],\n    \"learning_rate\": [0.1, 0.15]\n    }\n\nclf = GridSearchCV(GBR_model, parameters, cv=5,scoring='neg_mean_squared_log_error')\nclf.fit(x,y)\n\nprint(\" ------ Gradient Boosting ------ \")\nprint(\"Best Score:    \",round(np.sqrt(-1*clf.best_score_),4))\nprint('max_depth:     ', clf.best_estimator_.max_depth)\nprint('n_estimators:  ', clf.best_estimator_.n_estimators)\nprint('learning_rate: ', clf.best_estimator_.learning_rate)\n\nGBR_best_max_depth = clf.best_estimator_.max_depth\nGBR_best_n_estimators = clf.best_estimator_.n_estimators\nGBR_best_learning_rate = clf.best_estimator_.learning_rate","f1d9b49c":"parameters = {\n    \"max_depth\":[2,3],\n    \"n_estimators\":[150, 175, 200],\n    'learning_rate':[0.1, 0.15]\n    }\nclf = GridSearchCV(XGB_model, parameters, cv=5,scoring='neg_mean_squared_log_error')\nclf.fit(x,y)\n\nprint(\" ------ X-Gradient Boosting  ------ \")\nprint(\"Best Score:    \",round(np.sqrt(-1*clf.best_score_),4))\nprint('max_depth:     ', clf.best_estimator_.max_depth)\nprint('n_estimators:  ', clf.best_estimator_.n_estimators)\nprint('learning_rate: ', clf.best_estimator_.learning_rate)\n\nXGB_best_max_depth = clf.best_estimator_.max_depth\nXGB_best_n_estimators = clf.best_estimator_.n_estimators\nXGB_best_learning_rate = clf.best_estimator_.learning_rate","b28af545":"Ridge_model = Ridge(alpha=best_alpha)\nGBR_model = GradientBoostingRegressor(max_depth = GBR_best_max_depth, n_estimators = GBR_best_n_estimators, learning_rate = GBR_best_learning_rate)\nXGB_model = GradientBoostingRegressor(max_depth = XGB_best_max_depth, n_estimators = XGB_best_n_estimators, learning_rate = XGB_best_learning_rate)\n\n# The following lines of code preform cross validation and score the above models  \nRidge_score = round(np.sqrt(-1*np.average(cross_val_score(Ridge_model, x, y, cv = 5, scoring = 'neg_mean_squared_log_error'))),4)\nGBR_score = round(np.sqrt(-1*np.average(cross_val_score(GBR_model, x, y, cv = 5, scoring = 'neg_mean_squared_log_error'))),4)\nXGB_score = round(np.sqrt(-1*np.average(cross_val_score(XGB_model, x, y, cv = 5, scoring = 'neg_mean_squared_log_error'))),4)\n\nprint('--- root mean squared logarithmic error ---',)\nprint('Ridge Regression     :',Ridge_score)\nprint('Gradient Boosting    :',GBR_score)\nprint('X-Gradient Boosting  :',XGB_score)","06084a3f":"Ridge_model.fit(x,y)\nprediction_Ridge = Ridge_model.predict(test)\n\nGBR_model.fit(x,y)\nprediction_GBR = GBR_model.predict(test)\n\nXGB_model.fit(x,y)\nprediction_XGB = XGB_model.predict(test)","ae686d16":"prediction_Ridge_GBR = np.expm1((prediction_Ridge + prediction_GBR)\/2)\nprediction_Ridge_XBG = np.expm1((prediction_Ridge + prediction_XGB)\/2)","b64f0c03":"# This code extracts the property Id and converts it into a dataframe\ntest_id = pd.DataFrame(test.index)\ntest_id.columns = ['Id']\n\n## --- prediction_Ridge_GBR --- ##\nprediction_Ridge_GBR = pd.DataFrame(prediction_Ridge_GBR)\nprediction_Ridge_GBR.columns = ['SalePrice']\nsubmission_Ridge_GBR = test_id.join(prediction_Ridge_GBR)\n\n## --- prediction_Ridge_XBG --- ##\nprediction_Ridge_XBG = pd.DataFrame(prediction_Ridge_XBG)\nprediction_Ridge_XBG.columns = ['SalePrice']\nsubmission_Ridge_XBG = test_id.join(prediction_Ridge_XBG)","8d116b4f":"submission_Ridge_GBR.to_csv('submission_Ridge_GBR.csv', index = False)\nsubmission_Ridge_XBG.to_csv('submission_Ridge_XBG.csv', index = False)","dc47bd5d":"We used backward elimination in order to perform variable selecton on our models. Our goal was to drop all variables having a negative impact on the predictive power of our models. ","1f9afcb1":"Note that the R squared for both the Linear Regression and Lasso model were negative, this suggest that our regression lines are resulting in  greater errors than simply using the mean value of sale price.\n\nReference for our interpretation: http:\/\/www.fairlynerdy.com\/what-is-r-squared\/\n\nGiven the performance of the aforementioned models, we opted to dismiss them as prospective models. \n\n---\nMoving forward, we will use **root mean squared logarithmic error** to measure and compare our models\u2019 peformances.\n\nThe function from RMSLE is: \n$$\n\\sqrt{\\frac{1}{N}\\sum_{i=1}^N [log(1+p_i) - log(1+y_i)]^2}\n$$","b808c710":"Given that skewness is less than the absolute value of 0.5, we conclude that the log transformed distribution is approximately symmetric.","25d1cdda":"Using the hyper parameters defined in the previous lines of code, we redefined the three models. We then performed cross validation to confirm that our errors have indeed decreased.","49fa08bd":"## 1: Data pre-processing","c63a5a8c":"**Exploring skewnes**\n\nIt is important to check for skewness of the target variable when performing regression. The target variable should be log-transformed if it has a skewed distribution. This helps to improve the linearity of the data.\n\nThe references for how we dealt with interpretation of skewness and log-transformation is:\nhttps:\/\/brownmath.com\/stat\/shape.htm\n,\nhttps:\/\/www.dataquest.io\/blog\/kaggle-getting-started\/","8098fe60":"## 2: Model testing","41f86c3c":"## 3: Making predictions","32779dc5":"We implement seeding for the reproducibility of results","a071d755":"**Normalising**\n\nBecause our variables are measured in different scales, we performed normalisation in order to adjust the values onto a common [0,1] scale.\n\nWe used the following formula;\n$$\nx = \\frac{x-x_{min}}{x_{max}-x_{min}}\n$$","a6fd92ba":"### Backward elimination","0056f4a0":"We used **R squared** to measure and compare our models' perfomances.\n\nThe function used is:\n$$\nR^2 = 1 - \\frac{\\sum_{}(y_i - \\hat{y})^2}{\\sum_{}(y_i - \\bar{y})^2}\n$$","3c1c6535":"### Building the models","9cd0d13c":"In order to explore the missing data, we constructed a list of all variables containing np.nan elements.","41ff3db1":"## Results\n\nWhen submitting our CSVs to kaggle, we got 0.12070 and 0.12062 for average between Ridge and Gradient Boosting, and Ridge and X-Gradient Boosting respectively.","72951672":"**One-hot encoding**\n\nGiven that machine learning models in scikit learn can not handle textual data, we used one-hot encoding to generate dummy variables.","1b8d5b6e":"We combined the testing and training data into one large dataset `combined_data`, this was so  we could more efficiently deal with the data cleaning process by applying all transformations onto one dataset.","4df7ce2d":"We opted to aggregate the above models, based on literature, they tend to yield better results by reducing error.\n\nReference: https:\/\/www.dummies.com\/programming\/big-data\/data-science\/10-ways-improve-machine-learning-models\/ ","fd2d2dbb":"Implementation of backwards elimination \n\nThe following while loop looks at the highest coefficient p-value and if \nit is greater than 0.05, it drops the respective column, then re-builds the model generating new p-values. \nThe loop is terminated when the highest p-value is less the 0.05.","4a60b503":"In this notebook, we will go through the steps our team took in solving the Kaggle challenge _\u201cHouse Prices: Advanced Regression Techniques\u201d_. We have divided the notebook into the following three sections, in which the following tasks where performed.\n\n**1: Data pre-processing**\n* Importing basic modules\n* Dealing with missing values\n* Data scaling\n* Data transforming\n* Feature selection\n\n**2: Model testing**\n* Importing machine learning  modules\n* Building and comparing basic models\n* Parameter tuning\n\n**3: Making prediction**\n* Building final model\n* Constructing submission document","afba41c0":"We separated our data into two, `x` being the predictor variables and `y` the target variable.","f4b43ba8":"Given that skewness is greater than 1, we conclude the distribution is highly skewed.","3fbfeacb":"**Imputing missing values** \n\nUsing the list `contains_missing_values` as a reference, we read through _\u201ddata_description.txt\u201d_ , a file detailing a full description of each variable. \n\nFrom an understanding of each variable, we were able to decide on how to best impute the missing values. We decided on the following three possible designations; \n\n- `NA _list`, a list of columns where the np.nan\u2019s will be imputed with \u201dNA\u201d, meening a condition is non-applicable.\n\n- `Mode_list`, a list of columns where the np.nan\u2019s will be imputed with the mode of the column. \n\n- `Zero _list`, a list of columns where the np.nan\u2019s will be imputed with the value 0. ","f455f329":"From the home directory we import the two data sets, `train` and `test`.","d53f2faa":"The following cell produces prediction for sale price based on the `test` dataframe.","7251d0cf":"### Constructing Submission file\nThe last two code cells construct two submission documents, one based on each aggregated prediction. ","ad89d17a":"# House Prices: EDSA team 21","e0a56e83":"### Parameter tuning \nThe following three code cells implement parameter tuning using the GridSearchCV function. "}}