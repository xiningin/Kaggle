{"cell_type":{"8f001c80":"code","8cb39125":"code","5871a66a":"code","4b0aed79":"code","91864f11":"code","07bb3385":"code","4fb09d3c":"code","e6251e6a":"code","fc2aee0d":"code","e280baee":"code","757b977c":"code","386666e1":"code","deec2b3f":"code","7192187c":"code","86af10a3":"code","3487e148":"code","23b7567d":"code","630a6433":"code","04a4c223":"code","b5f9e390":"code","41fc6eb5":"code","d9f1829f":"code","108e03b5":"code","5ba70a24":"markdown","04789ff3":"markdown","2753970f":"markdown","ac0418a0":"markdown","323619c9":"markdown","9d67022b":"markdown","b3d44df9":"markdown","c67cc291":"markdown","6f98b7be":"markdown","e4027912":"markdown","c73695cf":"markdown","465dd11b":"markdown","1667ba10":"markdown","f49a6509":"markdown","2e619566":"markdown","5e1553fd":"markdown","50218617":"markdown","c909c35b":"markdown","6e408bea":"markdown","42f8d1ab":"markdown","b7602926":"markdown","e88fa0f1":"markdown","a3aabd99":"markdown","127c080d":"markdown","9fcfba4e":"markdown"},"source":{"8f001c80":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8cb39125":"df = pd.read_csv(\"\/kaggle\/input\/advertising-dataset\/advertising.csv\")\ndf.head()","5871a66a":"df = df.drop(['Radio','Newspaper'],axis = 1)","4b0aed79":"df.head()","91864f11":"print(f\"SHAPE :-  {df.shape}\")\nprint(f\"ROWS :-  {df.shape[0]}\")\nprint(f\"FEATURES :-  {df.shape[1]}\")","07bb3385":"df.info()","4fb09d3c":"df.describe()","e6251e6a":"X = df['TV'].values\ny = df['Sales'].values","fc2aee0d":"plt.scatter(X,y,color = 'blue',label = 'scatter plot')\nplt.title('Relatioship between TV and sales')\nplt.xlabel('TV')\nplt.ylabel('Sales')\nplt.legend(loc=4)\nplt.show()","e280baee":"# Visualise the relationship between the features and the response using scatterplots\nsns.scatterplot(X,y)\nplt.title('Relatioship between TV and sales')\nplt.xlabel('TV')\nplt.ylabel('Sales')\nplt.show()\n\n# plot a pairplot also for df\n\nsns.pairplot(df)\nplt.show()","757b977c":"print(X.shape)\nprint(y.shape)","386666e1":"# Reshape X and y\n\nX = X.reshape(-1,1)\ny = y.reshape(-1,1)","deec2b3f":"# Print the dimensions of X and y after reshaping\n\nprint(X.shape)\nprint(y.shape)","7192187c":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=30)","86af10a3":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","3487e148":"# import LinearRegression module\nfrom sklearn.linear_model import LinearRegression\n\n# Instantiate the linear regression object MODEL\nMODEL = LinearRegression()\n\n# Fit and train the model using training data sets\nMODEL.fit(X_train , y_train)\n\n# Predict on the test data\ny_predict = MODEL.predict(X_test)","23b7567d":"m = MODEL.coef_\nc = MODEL.intercept_","630a6433":"print(f\"Estimated model slope {m[0][0]}\")\nprint(f\"Estimated model intercept {c[0]}\")","04a4c223":"from sklearn.metrics import mean_squared_error\nMSE = mean_squared_error(y_test , y_predict)\nRMSE = np.sqrt(MSE)\nprint(\"RMSE value: {:.4f}\".format(RMSE))","b5f9e390":"from sklearn.metrics import r2_score\nprint(\"R2 Score value {:.4f}\".format(r2_score(y_test,y_predict)))","41fc6eb5":"plt.scatter(X,y,color = 'blue',label = 'scatter plot')\nplt.plot(X_test,y_predict,color = 'black',linewidth = 3,label = 'Regression Line')\nplt.title('Relatioship between TV and sales')\nplt.xlabel('TV')\nplt.ylabel('Sales')\nplt.legend(loc=4)\nplt.show()","d9f1829f":"plt.scatter(MODEL.predict(X_train), MODEL.predict(X_train) - y_train, color = 'red', label = 'Train data')\nplt.scatter(MODEL.predict(X_test), MODEL.predict(X_test) - y_test, color = 'blue', label = 'Test data')\nplt.hlines(xmin = 0, xmax = 50, y=0,linewidth=3)\nplt.title(\"RESIDUAL ERRORS\")\nplt.legend(loc=4)\nplt.show()","108e03b5":"# Checking for Overfitting or Underfitting the data by calculation score using score function.\nprint(\"Training Score:- {:.4f}\".format(MODEL.score(X_train,y_train)))\nprint(\"Test Score:- {:.4f}\".format(MODEL.score(X_test,y_test)))","5ba70a24":"# Data Visualization","04789ff3":"As you can see above that I have simple linear model with train and test score close to each other, i.e the model is not overfitting or underfitting.","2753970f":"## Visualising Data Using Seaborn\n","ac0418a0":"## Making predictions\n\n\nTo make prediction, on an individual TV value, \n\n\n\t\tlm.predict(Xi)\n        \n\nwhere Xi is the TV data value of the ith observation.\n\n","323619c9":"## Interpretation and Conclusion\n\n\nThe RMSE value has been found to be 2.3847. It means the standard deviation for our prediction is  2.3847. which is quite less. Sometimes we can also expect the RMSE to be less than 2.3847. So, the model is good fit to the data. \n\n\nIn business decisions, the benchmark for the R2 score value is 0.7. It means if R2 score value >= 0.7, then the model is good enough to deploy on unseen data whereas if R2 score value < 0.7, then the model is not good enough to deploy. Our R2 score value has been found to be  0.8042. It means that this model explains  80.42 % of the variance in our dependent variable. So, the R2 score value confirms that the model is good enough to deploy because it provides good fit to the data.\n","9d67022b":"# Importing Libraries and Dataset","b3d44df9":"### Reshaping X and y\nWhen working with only one feature variable, so we need to reshape using Numpy reshape() method.\n\nE.g, If you have an array of shape (3,2) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 6 rows, hence, (6,1)","c67cc291":"Understanding the Data\nLet's start with the following steps:\n\n1. Importing data using the pandas library\n2. Understanding the structure of the data","6f98b7be":"## Visual exploratory data analysis\n\nVisualize the relationship between X and y by plotting a scatterplot between X and y.\n#### Plot scatter plot between X and y","e4027912":"## Independent and Dependent Variables\n\n\n### Independent variable\n\nIndependent variable is also called Input variable and is denoted by X. In practical applications, independent variable is also called Feature variable or Predictor variable. We can denote it as:-\n\nIndependent or Input variable (X) = Feature variable = Predictor variable \n\n\n### Dependent variable\n\nDependent variable is also called Output variable and is denoted by y. \n\nDependent variable is also called Target variable or Response variable. It can be denoted it as follows:-\n\nDependent or Output variable (y) = Target variable = Response variable\n","c73695cf":"## Train test split\n- Split the dataset into two sets namely - train set and test set.\n- The model learn the relationships from the training data and predict on test data.","465dd11b":"## Simple Linear Regression - Model Assumptions\n\nThe Linear Regression Model is based on several assumptions which are listed below:-\n\ni.\tLinear relationship\nii.\tMultivariate normality\niii.\tNo or little multicollinearity\niv.\tNo auto-correlation\nv.\tHomoscedasticity\n\n\n### i.\tLinear relationship\n\n\nThe relationship between response and feature variables should be linear. This linear relationship assumption can be tested by plotting a scatter-plot between response and feature variables.\n\n\n### ii.\tMultivariate normality\n\nThe linear regression model requires all variables to be multivariate normal. A multivariate normal distribution means a vector in multiple normally distributed variables, where any linear combination of the variables is also normally distributed.\n\n\n### iii.\tNo or little multicollinearity\n\nIt is assumed that there is little or no multicollinearity in the data. Multicollinearity occurs when the features (or independent variables) are highly correlated.\n\n\n### iv.\tNo auto-correlation\n\nAlso, it is assumed that there is little or no auto-correlation in the data. Autocorrelation occurs when the residual errors are not independent from each other.\n\n\n### v.\tHomoscedasticity\n\nHomoscedasticity describes a situation in which the error term (that is, the noise in the model) is the same across all values of the independent variables. It means the residuals are same across the regression line. It can be checked by looking at scatter plot.\n","1667ba10":"### Difference in dimensions of X and y after reshaping\n\nEquation of linear regression<br>\n$y = c + m_1x_1 + m_2x_2 + ... + m_nx_n$\n\n-  $y$ is the response\n-  $c$ is the intercept\n-  $m_1$ is the coefficient for the first feature\n-  $m_n$ is the coefficient for the nth feature<br>\n\nIn our case:\n\n$y = c + m_1 \\times TV$\n\nThe $m$ values are called the model **coefficients** or **model parameters**.\n\n","f49a6509":"## Checking for Overfitting and Underfitting\n\n\nWe will see training set score and test set score.\n\nYou can excpect the training set score to be 0.7996, which is averagely good. So, the model learned the relationships quite appropriately from the training data. Thus, the model performs good on the test data as test score will be  0.8149. It is a clear sign of good fit\/ balanced fit. Hence, we can validated our finding that the linear regression model provides good fit to the data. \n\n\n**Underfitting**: Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). \n\n**Overfitting**: Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n","2e619566":"###  Simple Linear Regression (SLR)\n\nSimple Linear Regression (or SLR) is the simplest model in machine learning. It models the linear relationship between the independent and dependent variables. \n\nThis assignment is based on the TV and Sales data .\nThere is one independent or input variable which represents the TV data and is denoted by X. Similarly, there is one dependent or output variable which represents the Sales and is denoted by y. We want to build a linear relationship between these variables. This linear relationship can be modelled by mathematical equation of the form:-\n\t\t\t\t \n                 \n                 Y = \u03b20   + \u03b21*X    -------------   (1)\n                 \n\nIn this equation, X and Y are called independent and dependent variables respectively,\n\n\u03b21 is the coefficient for independent variable and\n\n\u03b20 is the constant term.\n\n\u03b20 and \u03b21 are called parameters of the model.\n \n\n\nFor simplicity, we can compare the above equation with the basic line equation of the form:-\n \n                   y = ax + b       ----------------- (2)\n\nWe can see that \n\nslope of the line is given by, a =  \u03b21,  and\n\nintercept of the line by b =  \u03b20. \n\n\nIn this Simple Linear Regression model, we want to fit a line which estimates the linear relationship between X and Y. So, the question of fitting reduces to estimating the parameters of the model \u03b20 and \u03b21. \n\n \n\n## Ordinary Least Square Method\n\nThe TV and Sales data are given by X and y respectively. We can draw a scatter plot between X and y which shows the relationship between them.\n\n \n\nNow, our task is to find a line which best fits this scatter plot. This line will help us to predict the value of any Target variable for any given Feature variable. This line is called **Regression line**. \n\n\nWe can define an error function for any line. Then, the regression line is the one which minimizes the error function. Such an error function is also called a **Cost function**. \n\n\n\n","5e1553fd":"## Regression metrics for model performance\nFor regression problems, there are two ways to compute the model performance. They are RMSE (Root Mean Square Error) and R-Squared Value. \n### RMSE\nRMSE is the standard deviation of the residuals. So, RMSE gives us the standard deviation of the unexplained variance by the model. It can be calculated by taking square root of Mean Squared Error.RMSE is an absolute measure of fit. It gives us how spread the residuals are, given by the standard deviation of the residuals. The more concentrated the data is around the regression line, the lower the residuals and hence lower the standard deviation of residuals. It results in lower values of RMSE. So, lower values of RMSE indicate better fit of data. \n```ruby\nfrom sklearn.metrics import mean_square_error\nMSE = mean_square_error(y_test , y_predict)\nRMSE = np.sqrt(MSE)\nprint(\"RMSE value: {:.4f}\".format(RMSE))\n```\n### R-Squared\n(R2) Correlation explains the strength of the relationship between an independent and dependent variable,whereas R-square explains to what extent the variance of one variable explains the variance of the second variable. Hence It may also be known as the coefficient of determination. So, if the R2 of a model is 0.50, then approximately half of the observed variation can be explained by the model's inputs.In general, the higher the R2 Score value, the better the model fits the data. Usually, its value ranges from 0 to 1. So, we want its value to be as close to 1. Its value can become negative if our model is wrong.\n```ruby\nfrom sklearn.metrics import r2_score\npritn(\"R2 Score value {:.4f}\".format(r2_score(y_test,y_predict)))\n```\n-   In business decisions, the benchmark for the R2 score value is 0.7. It means if R2 score value >= 0.7, then the model is good enough to deploy on unseen data whereas if R2 score value < 0.7, then the model is not good enough to deploy.\n-   If R2 score value has been found to be  0.7646. It means that this model explains 76.46 % of the variance in the dependent variable. So, the R2 score value confirms that the model is good enough to deploy because it provides good fit to the data.","50218617":"## Plotting residual errors","c909c35b":"###  Linear Regression\n\n\nLinear Regression is a statistical technique which is used to find the linear relationship between dependent and one or more independent variables. This technique is applicable for Supervised learning Regression problems where we try to predict a continuous variable.\n\n\nLinear Regression can be further classified into two types \u2013 Simple and Multiple Linear Regression. It is the simplest form of Linear Regression where we fit a straight line to the data.\n\n","6e408bea":"## Residual analysis\n\nA linear regression model may not represent the data appropriately. The model may be a poor fit to the data. So, we should validate our model by defining and examining residual plots.\n\nThe difference between the observed value of the dependent variable (y) and the predicted value (y\u0302i) is called the residual and is denoted by e or error. The scatter-plot of these residuals is called residual plot.\n\nIf the data points in a residual plot are randomly dispersed around horizontal axis and an approximate zero residual mean, a linear regression model may be appropriate for the data. Otherwise a non-linear model may be more appropriate.\n\nIf we take a look at the generated \u2018Residual errors\u2019 plot, we can clearly see that the train data plot pattern is non-random. Same is the case with the test data plot pattern.\nSo, it suggests a better-fit for a non-linear model. \n\n","42f8d1ab":"The above graph shows some sort of relationship between sales and TV. Don't you think this shows positive linear relation? i.e when As TV's value increases sales increases ans same is vise-versa.","b7602926":"## Checking dimensions of X and y\n\nChecking the dimensions of X and y to make sure they are in right format for Scikit-Learn API. \n","e88fa0f1":"## Model slope and intercept term\n\nThe model slope is given by lm.coef_ and model intercept term is given by lm.intercept_. \n\nfor example. if the estimated model slope and intercept values are 1.60509347 and  -11.16003616.\n\nSo, the equation of the fitted regression line will be:-\n\ny = 1.60509347 * x - 11.16003616  \n\n","a3aabd99":"See the difference in diminsions of X and y before and after reshaping.\n\nIt is essential in this case because getting the feature and target variable right is an important precursor to model building.","127c080d":"# Exploratory Data Analysis","9fcfba4e":"# Mechanics of the model\n\nThe steps to build any model can be divided as follows: \n\n- Split the dataset into two sets \u2013 the training set and the test set. Then, instantiate the regressor lm and fit it on the training set with the fit method. \n\n- In this step, the model learned the relationships between the training data (X_train, y_train). \n\n- Now the model is ready to make predictions on the test data (X_test). Hence, predict on the test data using the predict method. \n"}}