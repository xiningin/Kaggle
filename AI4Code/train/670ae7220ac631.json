{"cell_type":{"15189492":"code","35b231ad":"code","f7ee8c9d":"code","af025618":"code","efb68222":"code","df1c60b9":"code","73c967e6":"code","4afdc82f":"code","f3f00259":"code","e8ad4d6c":"code","d0dd8cb2":"code","a4c3bc43":"code","9b28782a":"code","c158a8e3":"code","e63281e8":"code","c5e8fe81":"code","88ff0927":"code","ed6070e5":"code","29ac6065":"markdown","4e5a643c":"markdown","488102e2":"markdown","9884bf9c":"markdown","1e4aeae3":"markdown","6284c871":"markdown","234d6467":"markdown","1e6b741e":"markdown","131ee648":"markdown","3e7f2c26":"markdown","4e2864b8":"markdown","f317d426":"markdown","e69bd4a3":"markdown","4b672292":"markdown","9e6617fc":"markdown","2bb4726e":"markdown","81fc9b26":"markdown","6e2773c2":"markdown","ee945339":"markdown","0f6b439c":"markdown","c88c6be8":"markdown"},"source":{"15189492":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('..\/input\/iris\/Iris.csv')","35b231ad":"# Heading of columns\ndf.columns","f7ee8c9d":"# This will return first 5 rows of data\ndf.head()","af025618":"# This will return the information about columns such as counts and data type \ndf.info()","efb68222":"# Very important function for getting statastical glimpse of data: such as mean, median, maximum and minimun entries and 25th, 50th and 75th percentile.\ndf.describe()","df1c60b9":"# adding first 100 rows of second and third columns which are SepalLengthCm, SepalWidthCm to X.\nX = df.iloc[0:100, [1, 2]].values\n# adding the classified species of first 100 rows to y.\ny = df.iloc[0:100, 5].values","73c967e6":"# np.where syntax goes like this : Where True, yield x, otherwise yield y.\n# set output lable value to 1 if it is setosa and 0 if versicolor.\ny = np.where(y == 'Iris-setosa', 1, 0)","4afdc82f":"X_std = np.copy(X)\n\n# Standardization of variables = subtracting the mean and dividing by the standard deviation\n\nX_std[:,0] = (X_std[:,0] - X_std[:,0].mean()) \/ X_std[:,0].std()\nX_std[:,1] = (X_std[:,1] - X_std[:,1].mean()) \/ X_std[:,1].std()\n","f3f00259":"# Define Logistic Regression hypothesis or sigmoid function\ndef sigmoid(X, theta):\n    \n    z = np.dot(X, theta[1:]) + theta[0]\n    \n    return 1.0 \/ ( 1.0 + np.exp(-z))","e8ad4d6c":"# Define Logistic Regression Cost Function\ndef lrCostFunction(y, hx):\n  \n    # compute cost for given theta parameters\n    j = -y.dot(np.log(hx)) - ((1 - y).dot(np.log(1-hx)))\n    \n    return j","d0dd8cb2":"# Gradient Descent function to minimize the Logistic Regression Cost Function.\ndef lrGradient(X, y, theta, alpha, num_iter):\n    # empty list to store the value of the cost function over number of iterations\n    cost = []\n    \n    for i in range(num_iter):\n        # call sigmoid function \n        hx = sigmoid(X, theta)\n        # calculate error\n        error = hx - y\n        # calculate gradient\n        # gradient is dot product of transpose of features(here X.T) and error(predictions(hx) - labels(y))\n        grad = X.T.dot(error)\n        # update values in theta\n        theta[0] = theta[0] - alpha * error.sum()\n        theta[1:] = theta[1:] - alpha * grad\n        \n        cost.append(lrCostFunction(y, hx))\n        \n    return cost        \n","a4c3bc43":"# m = Number of training examples\n# n = number of features\nm, n = X.shape\n\n\n# initialize theta(weights) parameters to zeros\ntheta = np.zeros(1+n)\n\n# set learning rate to 0.01 and number of iterations to 500\nalpha = 0.01\nnum_iter = 500","9b28782a":"cost = lrGradient(X_std, y, theta, alpha, num_iter)","c158a8e3":"plt.plot(range(1, len(cost) + 1), cost)\nplt.xlabel('Iterations')\nplt.ylabel('Cost')\nplt.title('Logistic Regression')","e63281e8":"# print theta paramters \nprint ('\\n Logisitc Regression bias(intercept) term :', theta[0])\nprint ('\\n Logisitc Regression estimated coefficients (SepalLenght, SepalWidth) :', theta[1:])","c5e8fe81":"# function to predict the output label using the parameters\ndef lrPredict(X):\n    \n    # set output lable value to 1(setosa) if it is gt 0.5 and 0(versicolor) if not.\n    return np.where(sigmoid(X,theta) >= 0.5, 1, 0)","88ff0927":"y_pred = lrPredict(X_std)\n\n# To check accuracy score\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y, y_pred, normalize=True)*100)","ed6070e5":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_boundry(X, y, classifier, h=0.02):\n    # h = step size in the mesh\n  \n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n                         np.arange(x2_min, x2_max, h))\n    Z = classifier(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    # plot class samples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)\n\nplot_decision_boundry(X_std, y, classifier=lrPredict)\nplt.title('Standardized Logistic Regression - Gradient Descent')\nplt.xlabel('sepal length ')\nplt.ylabel('sepal width ')\nplt.legend(loc='upper left')\nplt.tight_layout()\n","29ac6065":"We can see from the graph that the cost function converges near the value 1.5.","4e5a643c":"Now comes the understanding the equation part.\n\n","488102e2":"Now let's make a prediction function and see the accuracy of our model ","9884bf9c":"Now we'll perform **Gradient Descent** to reduce or minimize the cost value. (min J(\u03b8))\n\nTo minimize our cost function we need to run the gradient descent function on each parameter:\n\n![1_1--MUhjPjOL7oYdVo7R6gQ.png](attachment:1_1--MUhjPjOL7oYdVo7R6gQ.png)\n\n![1_Ecea3jVIRxK4Mkrh_Nie4w.jpg](attachment:1_Ecea3jVIRxK4Mkrh_Nie4w.jpg)\n\nTo know more, here's article that describes the partial differential of cost function : https:\/\/medium.com\/analytics-vidhya\/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d","1e4aeae3":"In order to map predicted values to probabilities, we use the Sigmoid function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.\n\n![1_Gp5E23P5d2PY5D5kOo8ePw.png](attachment:1_Gp5E23P5d2PY5D5kOo8ePw.png)\n\nThe graph of sigmoid function looks like this :\n\n![1_OUOB_YF41M-O4GgZH_F2rw.png](attachment:1_OUOB_YF41M-O4GgZH_F2rw.png)","6284c871":"# Explanation of equations and terms","234d6467":"# Now finally let's see the decision boundary and plotting of elements.\n# \nThe whole plotting function is explained neatly in this stackoverflow thread. I recommend to go through it once.\nhttps:\/\/stackoverflow.com\/questions\/44443993\/matplotlib-colors-listedcolormap-in-python","1e6b741e":"# Data Selection\n\n* Now, we've known that the dataset has 4 variable parameters namely SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm. \n* And it has three classification namely Iris-setosa, Iris-versicolor and Iris-virginica.\n\nBut for the sake of simplicity and ease of undrstanding, we'll pick only two parameters and will classify two species namely Iris-setosa and Iris-versicolor.","131ee648":"# Let's import the libraries and the dataset.","3e7f2c26":"log(p\/(1-p)) = logit(p) = \u03b2\u2080 + \u03b21X + \u03b22X\n\nPlugging the values of \u03b2\u2080, \u03b21 and \u03b22, we get :\n\nlog(p\/(1-p)) = logit(p) = \u20130.94466 - 6.3423xSepalLenght + 4.5580xSepalWidth","4e2864b8":"Now let's train our model and fit parameters:","f317d426":"# Accuracy score","e69bd4a3":"For logistic regression, the Cost function is defined as:\n\n![1_2g14OVjyJqio2zXwJxgj2w.png](attachment:1_2g14OVjyJqio2zXwJxgj2w.png)\n\nThe above two functions can be compressed into a single function like this :\n\n![1__52kKSp8zWgVTNtnE2eYrg.png](attachment:1__52kKSp8zWgVTNtnE2eYrg.png)\n\n","4b672292":"# Now we'll see basic data explorations and get glimpse of dataset.","9e6617fc":"For logistic regression we are going to use this equation i.e.\n\n\n**\u03c3(Z) = \u03c3(\u03b2\u2080 + \u03b21X + \u03b22X)**\n\n**Z = \u03b2\u2080 + \u03b21X + \u03b22X**\n\n\nh\u0398(x) = sigmoid(Z)\n\ni.e. \n\n**h\u0398(x) = 1\/(1 + e^-(\u03b2\u2080 + \u03b21X + \u03b22X)**","2bb4726e":"The hypothesis of logistic regression tends it to limit the cost function between 0 and 1. Therefore linear functions fail to represent it as it can have a value greater than 1 or less than 0 which is not possible as per the hypothesis of logistic regression.\n\n![1_GnceHPIeThNShGSmYzE4eA.png](attachment:1_GnceHPIeThNShGSmYzE4eA.png)","81fc9b26":"Here we have log of odds of probability, so it should be interpreted in this manner :\n\nlogit(p)=log(p\/1\u2212p)=\u03b20+\u03b21x1+\u22ef+\u03b2kxk.\n\n![Screenshot%20%28201%29.png](attachment:Screenshot%20%28201%29.png)","6e2773c2":"> This is my first ever notebook. I've tried to explain it in the way that I wanted it when I was looking for the basics. Rather than using any library, I've kept it in original form, so we can see the mathematics behind it. I'm new to the field of Data Science, so any suggestion is always appreciated ! \n\n> If this notebook to be useful, Please Upvote!!!","ee945339":"# Standardization of variables\n\nNow we'll standarize variables to reduce or to remove multicollinearity caused by higher-order terms.\n\nMore information about standardization of variables can be found here : https:\/\/statisticsbyjim.com\/regression\/standardize-variables-regression\/","0f6b439c":"Now let's plot number of iterations on the x axis and cost function on y axis to see if the cost converges to minimum or not.","c88c6be8":"We can call a Logistic Regression a Linear Regression model but the Logistic Regression uses a more complex cost function, this cost function can be defined as the \u2018Sigmoid function\u2019 or also known as the \u2018logistic function\u2019 instead of a linear function."}}