{"cell_type":{"54eaa414":"code","47ab0f0e":"code","824edb14":"code","62a42715":"code","9b724d56":"code","9de5f2d8":"code","28a8462f":"code","dd74792a":"code","a9cc0b91":"code","4c8d4c93":"code","6c511984":"code","bd2a4fba":"code","ac9fcdee":"code","fa58f659":"code","0d74e7fe":"code","20da35dc":"code","bf37faf9":"code","810f3175":"code","3a2108a5":"code","acda2f0f":"code","c9a83ec0":"code","26c2a9a7":"code","243305bf":"code","fd7a250e":"code","7b56408a":"code","587e4b96":"code","aff9b48b":"code","1ea9a050":"code","87dee21b":"code","754862c6":"code","e4a0c54f":"code","2fa984c5":"code","25aecf10":"code","fc32c4d7":"code","b063467b":"code","4a966720":"code","2783f304":"code","34d1eba7":"code","6de6d8be":"code","c82299ea":"code","796f6f60":"code","a8068988":"markdown","6bacdc43":"markdown","b395a5cb":"markdown","4aa91800":"markdown","9d8e1049":"markdown","d30bb81d":"markdown","629603c2":"markdown","32bd6b16":"markdown","88265038":"markdown","59ee290b":"markdown","bb7763c2":"markdown","5110bcd1":"markdown","3e6e7f0b":"markdown","45d89ae3":"markdown","22f8a3bb":"markdown","982871ab":"markdown","6220bc9b":"markdown","1dc82867":"markdown","f7bd42dc":"markdown","011f7c9f":"markdown","1fe4eaaa":"markdown","68a5f6c7":"markdown","7ff8cbfa":"markdown","d1a45d70":"markdown","83c5725f":"markdown","f3080815":"markdown","0ff1b4af":"markdown","e65b7e42":"markdown","56ed6fdd":"markdown","43ddefc5":"markdown","76cafe89":"markdown","015c3aad":"markdown","3a1b142f":"markdown","ac658820":"markdown","6a079309":"markdown","ce5eef6e":"markdown","54fb5b98":"markdown","342cf7db":"markdown","3014662f":"markdown","29f35adc":"markdown","2fb182f1":"markdown","b9bc3768":"markdown","da1bd95a":"markdown","905da2b5":"markdown","463834af":"markdown","85757cc8":"markdown","1a66029b":"markdown","09de9621":"markdown","58583c76":"markdown","a210e011":"markdown","8cb39d42":"markdown","0b4abc2a":"markdown","d0ce8446":"markdown","dd03a1e9":"markdown","d2eb3880":"markdown","e66e318a":"markdown","d845b990":"markdown","5234eff6":"markdown","8be2dc8e":"markdown","0399b8a6":"markdown","4f638ac1":"markdown","a00babd7":"markdown","9294dfb1":"markdown","211a2451":"markdown"},"source":{"54eaa414":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n# the module includes Statistical functions\nfrom scipy import stats\nfrom scipy.stats import norm\n# the module to preprocess the dataset\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_absolute_error\n# just ignore the warning information and don't show them\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Use it to replace AI\"plt\"","47ab0f0e":"train = pd.read_csv('..\/input\/train.csv')\nprint(train)\npd.set_option('max_colwidth',100)","824edb14":"print(train.columns)","62a42715":"sheet_1 = pd.DataFrame({'Variable': ['GrLivArea', 'LotArea', 'Neighborhood', 'OverallQual', 'YearBuilt', '1stFlrSF', 'TotalBsmtSF', 'TotRmsAbvGrd', 'FullBath', 'CentralAir', 'GarageCars', 'GarageCars'],\n        'Segment': [1, 1, 2, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n        'Data Type': [0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n        'Comments': ['Above grade (ground) living area square', 'Lot size in square feetStreet', 'Physical locations within Ames city limits', 'Rates the overall material and finish of the house', 'Original construction dateYearRemodAdd', 'First Floor square feet', 'Total square feet of basement area Heating', 'Total rooms above grade (does not include bathrooms)', 'Basement full bathrooms', 'Central air conditioning', 'Size of garage in square feet GarageQual', 'Size of garage in car capacityCar']})\nsheet_1","9b724d56":"print(train['SalePrice'].describe())","9de5f2d8":"p = sns.distplot(train['SalePrice'],color = 'purple', fit = norm, axlabel = 'SalePrice',label = 'Saleprice', hist = True)\nfig = plt.figure()\np.set_title('SalePrice distribution')\np.set(xlabel='SalePrices')\np.set(ylabel='Frequency')\n# return an object\nprint(p)\nres = stats.probplot(train['SalePrice'], plot = plt)","28a8462f":"sheet_2 = pd.DataFrame({'concept\\\\range': ['Kurtosis', 'Skewness'],\n                        '< 0': ['Flatter than the peak of Normal Distribution', 'Negative deviation is larger and the tail is in the left'],\n                        '= 0': ['Of the same steepness witn the oeak of Normal Distribution', 'Distribution form is the same as Normal Distritution'],\n                        '> 0': ['More steep than the peak of Normal Distribution', 'Positive deviation is larger and the tail is in the right']})\nsheet_2","dd74792a":"sheet_2.to_excel('sheet_2.xlsx', sheet_name = 'Sheet_2')","a9cc0b91":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","4c8d4c93":"data = pd.concat([train['SalePrice'], train['GrLivArea']],axis = 1)\ndata.plot.scatter(x = 'GrLivArea', y = 'SalePrice', xlim = (0,6000), ylim = (0,800000),title = 'the relationship between SalePrice and GrLivArea')","6c511984":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis = 1)\ndata.plot.scatter(x = 'TotalBsmtSF', y = 'SalePrice', xlim = (0,6000), ylim = (0,800000),title = 'the relationship between SalePrice and TotalBsmtSF')","bd2a4fba":"data = pd.concat([train['SalePrice'],train['LotArea']],axis = 1)\ndata.plot.scatter(x = 'LotArea', y = 'SalePrice', xlim = (0, 25000), ylim = (0,800000),title = 'the relationship between SalePrice and LotArea')","ac9fcdee":"data = pd.concat([train['SalePrice'], train['1stFlrSF']], axis = 1)\ndata.plot.scatter(x = '1stFlrSF', y = 'SalePrice', xlim = (0,6000), ylim = (0,800000),title = 'the relationship between SalePrice and 1stFlrSF')","fa58f659":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis = 1)\nf, ax = plt.subplots(figsize = (35, 25))\nfig = sns.boxplot(x = 'YearBuilt', y = 'SalePrice', data = data,)\nfig.set(ylim=(0,1000000))","0d74e7fe":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis = 1)\nf, ax = plt.subplots(figsize = (15, 10))\nfig = sns.boxplot(x = 'OverallQual', y = 'SalePrice', data = data, hue ='OverallQual')\nfig.set(xlim=(0,10))","20da35dc":"data = pd.concat([train['SalePrice'],train['Neighborhood']], axis = 1)\nf, ax = plt.subplots(figsize = (15, 10))\nfig = sns.boxplot(x = 'Neighborhood', y = 'SalePrice', data = data, hue = 'Neighborhood')\nfig.set(ylim = (0,800000))","bf37faf9":"# get the correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize = (10, 8))\nsns.heatmap(corrmat, vmax = 0.8, linecolor = 'black', square = 'True' )","810f3175":"names = ['CentralAir', 'Neighborhood']\n# classify the type\nfor x in names:\n    label = preprocessing.LabelEncoder()\n    train[x] = label.fit_transform(train[x])\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(corrmat, vmax = 0.8, linecolor = 'black',square = True)","3a2108a5":"k = 10\nf, ax = plt.subplots(figsize = (10,8))\n# get ten most relavant ones\ncols  = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\n# get the correlation matrix\ncm = np.corrcoef(train[cols].values.T)\n# set the font\nsns.set(font_scale = 1.25)\nhm = sns. heatmap(cm, cbar = True , annot = True, square = True, fmt = '.2f', annot_kws = {'size':10}, yticklabels = cols.values,xticklabels = cols.values)\nplt.show()","acda2f0f":"sheet_3 = pd.DataFrame({'Variable': ['GrLivArea', 'OverallQual', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt'],\n        'Segment': [1, 1, 1, 1, 1, 0, 0],\n        'Data Type': [0, 1, 0, 0, 0, 0, 1],\n        'Comments': ['Above grade (ground) living area square', 'Rates the overall material and finish of the house', 'Size of garage in car capacityCar', 'Total square feet of basement area Heating', 'Basement full bathrooms', 'Total rooms above grade (does not include bathrooms)', 'Original construction dateYearRemodAdd']})\nsheet_3","c9a83ec0":"sheet_3.to_excel('sheet_3.xlsx', sheet_name = 'Sheet_3')","26c2a9a7":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nsns.pairplot(train[cols], size = 5)\nplt.show()","243305bf":"sheet_4 = pd.DataFrame({'Mechanism': ['Missing Completely at Random(MCAR)', 'Missing at Random(MAR)', 'Not Missing at Random(NMAR)'],\n                        'Comment': ['The missing data has nothing to do with complete variables and incomplete variables', 'The data deficency just depends on the complete variables', 'The data deficency just depends on the incomplete variables and can not be ignored']})\nsheet_4","fd7a250e":"sheet_4.to_excel('sheet_4.xlsx', sheet_name = 'Sheet_4')","7b56408a":"# count the number of the missing data of every variable and sort them in descending arrangement.\ntotal = train.isnull().sum().sort_values(ascending = False)\n# count the percentage of missing data\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending = False)\n# show in a DataFrame,keys is just used to control the index\nmissing_data = pd.concat([total, percent], axis = 1,keys = ['Total', 'Percent'])\nmissing_data.head(20)","587e4b96":"train = train.drop((missing_data[missing_data['Total'] > 1]).index, 1)\ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)\n# check if have delt with all missing data\ntrain.isnull().sum().max()","aff9b48b":"# to get the mean and variance of normal distribution\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:, np.newaxis])\nlow_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][: 10]\nhigh_range = saleprice_scaled[saleprice_scaled[:, 0].argsort()][-10:]\nprint('low of the distribution:', low_range)\nprint('high of the distribution:', high_range)","1ea9a050":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis = 1)\ndata.plot.scatter(x = 'GrLivArea', y = 'SalePrice', ylim = (0, 800000))","87dee21b":"# find the Id\ntrain.sort_values(by = 'GrLivArea',ascending = False)[:2]","754862c6":"# delete the two points by finding their Id\ntrain = train.drop(train[train['Id'] ==1299].index)\ntrain = train.drop(train[train['Id'] ==524].index)","e4a0c54f":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis = 1)\ndata.plot.scatter(x = 'TotalBsmtSF', y = 'SalePrice', ylim = (0, 800000))","2fa984c5":"train['SalePrice'] = np.log(train['SalePrice'])\nsns.distplot(train['SalePrice'],  fit = norm)\nfig = plt.figure()\nres  = stats.probplot(train['SalePrice'], plot = plt)","25aecf10":"sns.distplot(train['GrLivArea'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train['GrLivArea'], plot = plt)","fc32c4d7":"train['GrLivArea'] = np.log(train['GrLivArea'])\nsns.distplot(train['GrLivArea'], fit = norm)\nfit = plt.figure()\nres = stats.probplot(train['GrLivArea'], plot = plt)","b063467b":"sns.distplot(train['TotalBsmtSF'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train['TotalBsmtSF'], plot = plt)","4a966720":"train['HasBsmt'] = pd.Series(len(train['TotalBsmtSF']), index = train.index)\ntrain['HasBsmt'] = 0\ntrain.loc[train['TotalBsmtSF'] > 0, 'HasBsmt'] = 1\ntrain.loc[train['HasBsmt'] == 1, 'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])\nsns.distplot(train[train['TotalBsmtSF'] > 0]['TotalBsmtSF'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train[train['TotalBsmtSF'] > 0]['TotalBsmtSF'], plot = plt)","2783f304":"plt.scatter(train['GrLivArea'], train['SalePrice'])","34d1eba7":"plt.scatter(train[train['TotalBsmtSF'] > 0]['TotalBsmtSF'], train[train['TotalBsmtSF'] > 0]['SalePrice'])","6de6d8be":"cols = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nx1 = train[cols].values\ny1 = train['SalePrice'].values\n# train the data\nmodel = linear_model.LinearRegression()\nmodel.fit(x1, y1)\nprint('The coefficients:', model.coef_)\nprint('The intercept:', model.intercept_)","c82299ea":"test = pd.read_csv('..\/input\/train.csv')\ncols = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt']\nx2 = test[cols].values\ny2 = model.predict(x2)\nY = pd.DataFrame({'y2':model.predict(x2)}, index = np.arange(2, len(y2)+2))\nY = Y.drop([1461])\nY","796f6f60":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['SalePrice'] = np.log(sample_submission['SalePrice'])\nprint(mean_absolute_error(sample_submission['SalePrice'], Y))","a8068988":"## Deal with the missing data","6bacdc43":"![content.png](attachment:content.png)","b395a5cb":"# Description of problem","4aa91800":"&ensp;&ensp;Pairplot can not only show the univariate dstribution of the data for the variable in that column,but also show a subset of variables or plot different variables on the rows and columns.We have know that the variables in sheet_3 are the most relavant ones with SalePrice,and in order for more intuitive,here,we draw scatterplots for joint relationshipi and histograms for univariable distributions.","9d8e1049":"&ensp;&ensp;Before processing the missing data, it is quite significant to know the mechanism and form of it.We know that cmplete variable means variables without missing data,and incomplete variable means variables with missing data.In order to how it more intuitively,we establish a sheet to explain the three kinds of mechanism.","d30bb81d":"## Conslusion","629603c2":"&ensp;&ensp;In this competition, we first calmly analysized the problem and chose some variables as representations to study.Then we made deep analysis to select the really typical variables by drawing graphicals, correlation matrix, heatmap and so on. Next, in order to improve accuracy, we dealt with missing data, outliers, and standardized the variables by using log transfortation. Finally, we trained data and got their coefficients to test our model.We conclused that our model was acceptative and could used to predict the house prices.","32bd6b16":"&ensp;&ensp;Now we have finished data preprocessing and standardization, and we try to show the scatter plots again to see the effect.","88265038":"&ensp;&ensp;As the result shows, it seems to be a normal distribution and then we try to show it with an image.","59ee290b":"&ensp;&ensp;In the base of understanding the problem, now we should not only think about the possible factors affecting the house prices, but also use the datasets to check if there is a strong relationship between them. Then, we establish a suitable model to find out the to analize this relationship and exeriment it. Finally, we shoulid use what we have experimented to predict the future house preices.","bb7763c2":"&ensp;&ensp;We successfully transform it into seemly a normal distribution and we can also perform the same operation to the other variables.","5110bcd1":"## Strengths and weaknesses","3e6e7f0b":"&ensp;&ensp;In this context, we just chose some typical variables to explore representatively and ignored some unexpected effects of other variables, which may result in deviation and limit the resulit in fact. ","45d89ae3":"&ensp;&ensp;From the above result, we know that the data includes so mach variables. In order to well understand the meanings of them, we should spend some time to learn them in the data_description.text.For example, 'OverallQuall'the same meaning as 'builing', is a variable relates to the physical characteristics of the building. 'TotalBsmtSF'shows the house's space properties and 'Neighborhood' tells us where the house located.And so on...  \n&ensp;&ensp;After the observation of the train.csv file, we found that some of them are numbers, and others are categories. And according to our experience, buyers may consider the different types of varibles when deciding the house prices. So we first assump that the variables may have the same symbols and we try to devide them into many different fields, and make the variables that have the same symbols in the same one.  \n&ensp;&ensp;But after understanding the descriptions of variables in the train.csv file, we found it's not convenient and time-consuming to analize all the varibles. So, we searched multitude of materials and just decided to select some typical variables to experiment. Considering carefully, we thought that the \"building, space, location\"three factors may mainly affect the problem, so we will choose some variables related with these three types to analysize first, such as 'TotRmsAbvGrd', '1stFlrSF', 'FullBath', 'LotArea', 'Neighborhood', 'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'CentralAir', 'GarageCars', 'GarageArea' and 'GrLivArea'. These variables may play an important role in this problem. We hope we can find the clear relationship between the variate and the problem.  \n&emsp;Here,we establish a new excel sheet to explain the segment,datatype and comments of the above selected variables. We use segment to describe the variables'types, and just as the followeing: \"building--0\", \"space--1\", \"location--2\". We use Data Type to distinguish the different types of the data, and \"0\" is used for numberic type, \"1\" is used for catelogic ones. Besides, we use commens to describe the views about the varibles. ","22f8a3bb":"# Solution of problem","982871ab":"&ensp;&ensp;Maybe we can delete some values that more than 3000, but in fact those values still follow the crowd and  we do not need to do so.","6220bc9b":"### Weaknesses","1dc82867":"## Download and input the datasets\u00b6","f7bd42dc":"#### Homoscedasticity test","011f7c9f":"## Data simulation","1fe4eaaa":"&ensp;&ensp;We can see that the above variables we selected are roughly relative to the SaplePrice,but it's still subjective and maybe we lose some more important details.So next we should make further analysis. ","68a5f6c7":"&ensp;&ensp;We found that there were no null numbers in dataset,and we can continue next step!","7ff8cbfa":"&ensp;&ensp;Then we try to find what kind of mechanism it belows.","d1a45d70":"&ensp;&ensp;From the data value in each cell,we are more confident that the above 9 variables are truely relavant to the SalePrice.And the coefficient of correlation between \"GarageArea\" and \"SalePrice\" is almost the same as the coefficient of correlation between \"GarageCars\" and \"SalePrice\",so we can just select one of them to explore.So does the \"TotalBsmtSF\" and \"1stFlrSF\".Therefore,we finally choose the following 8 variables to explore and we establish a sheet to show them.","83c5725f":"&ensp;&ensp;Correlation matrix is a systematic comprehensive evaluation method,it majorly uses matrix to show the relationship between the relevant evaluation indicators,of every variable and the value evaluation quantity of specific indicators in the program.Below we will try to get the correlation matrix between every variables,and show it with the heatmap.","f3080815":"&ensp;&ensp;As the plot shows,there are two point that bigger 'GrLivArea'are obviously strange and they do not follow the crowd,so we assump that the houses may remote and in low price.Then we will delete these two points.","0ff1b4af":"&ensp;&ensp;Then we just choose the 10 most relevant variables to get their correlation matrix and plot the heatmap again.","e65b7e42":"&ensp;&ensp;If missing data is in the train set,it will imply a reduction of the sample size or may result in model deviation.What's wore,the false analysis of variables may result in false classification and prediction.So before trainning the dataset,we should pay high attention to the missing data and reduce the model deviation.","56ed6fdd":"**See the distribution of 'GrLivArea'.**","43ddefc5":"&ensp;&ensp;We used mean_absolute_error to check if our model is right. Because our data values are quite high,the mean_absolute_error,799.7038816881698 is acceptable.And we are delighted that our model is considently right.","76cafe89":"## Further analysis","015c3aad":"&ensp;&ensp;After normal standardization, we can find that values in low_range is similar to each other,ranging near 0.But values in high_range are far away from 0 and values over 7 are out of normal range.So we know outliers existing and we have to do double variables analysis and drop some data values.","3a1b142f":"&ensp;&ensp;We found that the above matrix just select numerical variables,and we sould also check the relationship between the SalePrice and the nonnumerical ones.Below we use sklearn to check it.","ac658820":"&ensp;&ensp;From the above two plots,we can firstly see that the following variables are more relative to the SalePrice.They are 'OverallQual','YearBuilt','ToatlBsmtSF','1stFlrSF','GrLiveArea','FullBath','TotRmsAbvGrd','GarageCars' and 'GarageArea'.And 'CentralAir' and 'Neighborhood'two variables have little effect on SalePrice,so we can ignore them in the following exploration.","6a079309":"$$S\uff1d(X^-M_0)\/\u03b4$$","ce5eef6e":"&ensp;&ensp;There is a column called 'Saleprice', and it is just what we pay high attention to. In this problem, we first see its describe.","54fb5b98":"#### Univariable analysis","342cf7db":"# Analysis of problem","3014662f":"&ensp;&ensp;It's delighted that 'SalePrice'and 'GrLivArea' is in Homoscedasticity,same as the 'SalePrice' and 'TotalBsmtSF'.And by now we have made sure that the preconditioning has all finished and we can begain to train the samples.","29f35adc":"&ensp;&ensp;We use probplot to calculaes a best-fit line for the data and plots the results using a given plot to make further analysis. The Kurtosist is used to describe the steepness of the distribution of all values of a variable,and we use the following computational formulas to measure it.","2fb182f1":"&ensp;&ensp;Here we can see that the Kurtosis is 6.536282,which is larger than zero and means that the datas are so centralized.Also,the Skewness is 1.882876,and its tail is in the right,which means the positive deviation is larger,that's to say,most datas focus on the right.In conclusion, SalePrice is not normal and does not follow the diagonal line.","b9bc3768":"**Standardize 'GrLivArea' as noormal distribution.**","da1bd95a":"&ensp;&ensp;When buyers decide to buy a house, the prices of house may become one of the considerable elements. And according to the report, there are many factors can affect the price of house in the negotiations.In this competition, we are required to use the 79 explanatory varibles describing almost every aspect of residential homes in Ames, lowa to analize,and then predict the final price of each house.","905da2b5":"&ensp;&ensp;In this part, we will  establish a threshold to define an observation as an outlier.Therefore, we will standardize the data,which means to convert data values to mean value of 0 and a standard deviation of 1. ","463834af":"&ensp;&ensp;The Skewness is used to describe the symmetry of a variable's value distribution,and we use the following computational formulas to measure it.","85757cc8":"&ensp;&ensp;According to the coeddicients, we use test data to predict SalePrice.And we will establish a DataFrame to save the data.","1a66029b":"![image.png](attachment:image.png)","09de9621":"&ensp;&ensp;In order to show our thought more intuitively, we drew a mind map for you to read.","58583c76":"&ensp;&ensp;Linear regression is easy to understand and operate.Besides, ploting graphs can directly show the relationships between variables. Linear regression can accurately measure the correlation degree and regression fitting degree between various variables, improving the pediction effect. ","a210e011":"&ensp;&ensp;Then we estabish a sheet to further introduce the some relative concepts of these two statistics.","8cb39d42":"&ensp;&ensp;Below we begin to check if the above variables we selected are really the major ones.","0b4abc2a":"## Deal  with outliers","d0ce8446":"&ensp;&ensp;We can see there are many missing data in the train dataset, and for convenience we firstly consider that when more than 10% of the data is missing, the corresponding variable can be ignored and deleted directly.Then we observed all the variables again and found variables like 'PoolQC', 'MiscFeature', 'Alley' etc.truely have little effect on 'SalePrice', so we can excatly pretend them not exist and delete them directly. After that, only the variables whose missing percent below 6% are left. But next after deep observation of the sample dataset and take all the analysis above into consideration, we found that the information of 'Garage-X'can be replaced by 'GarageCars', and 'MasVnr-X'can be replaced by 'YearBuilt'.As we have take 'GarageCars'and 'YearBuit'into consideration, so we can ignore these variables.There is just one missing data in variable 'Electrical', so we can choose to delete the missing data but not this variable.\nTherefore,finally,we came a conclusion that we should next directly delete the varables whose total of missing data is more one. Just as the following shows.","dd03a1e9":"&ensp;&ensp;Here we will choose the most relavant as typical variables to train the data and get the coefficients","d2eb3880":"&ensp;&ensp;As is known to us, the boxplot can clearly show the maximum,minimum, median,and the quantile.Therefore,for convenience,we use the boxplot to see the symbols of SalePrice of each variable.And then we can obviously see the differences of them,check if there are some abnormal values among them and it's beneficial to the further quest.","e66e318a":"**See the distribution of 'TotalBsmtSF'**","d845b990":"$$\u03b2\uff1dM_4\/\u03c3^4$$","5234eff6":"&ensp;&ensp;Although we have dealt with the missing data, but the outliers is more complex and also should be pay higher attention to. If not, it will also affect our model and the result. Here we just choose the standard deviation of SalePrice to do a quick analysis by plotting a set of scatter plots.","8be2dc8e":"&ensp;&ensp;We can see that there are many values existing,and them do not allow us to  do log transformations.So we should creat a binary variable, and then do a log transformation to all non-zero observations, ignoring those with value zero.","0399b8a6":"### Strengths","4f638ac1":"# Objective evaluation","a00babd7":"&ensp;&ensp;Following we hoose some typical variables to set a exam for explaination.","9294dfb1":"&ensp;&ensp;From all above figures,we can obviously saw that the relationship between 'GrLivArea'\n'TotalBsmtSF' and SalePrice seems like a linear line,which almost act like a border.That's to say,the border make sense that the dots clouds are below the line.It seems that the Fullbath is equal to the GrLivArea,but after careful obversation,it's unexpected if the amount of Fullbath is larger than the GrLivArea.Also,the plot concerning 'SalePrice' and 'Overallqual','SalePrice' and 'YearBuilt'appears to be a exponential function.And so on...","211a2451":"## Univariate analysis"}}