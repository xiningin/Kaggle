{"cell_type":{"a2450d15":"code","6bf6fe2d":"code","e11e19a0":"code","4cc2ca11":"code","43d0d51e":"code","f97c5302":"code","7079d2cb":"code","a230a142":"code","dbd22579":"code","9e42291d":"code","066e8b0a":"code","5e352bd3":"code","854c23e8":"code","1ccac3a5":"code","31baa852":"code","503bab69":"code","11698197":"code","785090f2":"code","e02d3571":"code","c6920286":"code","10eb07cf":"code","d06b9b2e":"code","9dd314e4":"code","eaca441b":"code","7738295d":"code","ce09f461":"code","04be457d":"code","f56a3868":"code","a844304d":"code","ce7671b1":"code","c1e3ea2d":"code","704c21d0":"code","8396afb8":"code","4037bbd1":"code","58f822b9":"code","3d7a0cac":"markdown","4af3502c":"markdown","4ea87fd1":"markdown","e580feba":"markdown","790eb24b":"markdown","65504f3b":"markdown","280dc47b":"markdown","9bdc67cc":"markdown","d5fdec90":"markdown","6bcbb54d":"markdown","8d302a86":"markdown","6f04a82f":"markdown","26c420f2":"markdown","15cbed3d":"markdown","a8f8b9c8":"markdown","c92f5c79":"markdown","46ea7bd6":"markdown","d39de729":"markdown","9014bc70":"markdown","565e6659":"markdown","c2c82cec":"markdown"},"source":{"a2450d15":"#Copied\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Cuz why not\nimport seaborn as sns #Not using it now\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6bf6fe2d":"#Lets just see the November data\ndf = pd.read_csv(\"\/kaggle\/input\/ecommerce-events-history-in-cosmetics-shop\/2019-Nov.csv\")\n\n#Formalities time conversion\ndf['event_time'] = pd.to_datetime(df['event_time'],infer_datetime_format=True)\n\n##Encode? Lets not do that now. categorical\/label encoding of the session IDs (instead of string - save memory\/file size):\n#Actually its new to me to know that it reduces the size so thanks\n#df['user_session'] = df['user_session'].astype('category').cat.codes\n\n#Check how many row and column\nprint(df.shape)\n#Formalities see the topmost data\ndf.head()","e11e19a0":"df.info()","4cc2ca11":"#Oh so this is how you count cat value occurence. That's neat\nev_count = df[\"event_type\"].value_counts()\n#Might as well draw a pie chart\n#func to get percent string to show to  \ndef func(pct, allvals):\n    absolute = int(pct\/100.*np.sum(allvals))\n    return \"{:.1f}%\\n({:d} entry)\".format(pct, absolute)\nax = ev_count.plot.pie(figsize=(6,6),radius=2,autopct=lambda pct: func(pct, ev_count),textprops=dict(color=\"w\"),legend=True)\nax.legend(loc=2)","43d0d51e":"#Might do the same to product_id and others\nprid_s = df[\"product_id\"].value_counts()\n#Of course not, the product_id is long you'd waste space printing it \nprint('no. of products: ',len(prid_s))\n#Use describe instead\nprint(prid_s.describe())\n#So what we count here is how active ea product is, it doesnt matter what the activity though might be only viewing\n#carting purchasing and even removing. But we get the big picture here that how active each product was on Nov 2019\n#So we gonna step up to see which is most popular\nprid_s = prid_s.sort_values(ascending=False) \nprint('Top 10 Active Products')\nprint(prid_s.head(10))\n#Sadly we dont know what the real product which lies on top of the list\n#Lets step up to see the product access count distribution (with histogram of course)\nprid_s.plot.hist(bins=100)\n#That doesnt look good right,try this\n#prid_s.plot.hist(bins=[0,10,20,50,100,200,500,1000,2000,5000,10000,20000],logx=True,logy=True)","f97c5302":"category_dict=df[['category_id','product_id']].drop_duplicates()\nprint('no. of category',len(df['category_id'].drop_duplicates()))\ncat_counts=category_dict['category_id'].value_counts().sort_values(ascending=False)\nprint('Top 10 Category')\nprint(cat_counts.head(10))\nprint('Last 10 Category')\nprint(cat_counts.tail(10))\ncat_counts.plot.bar(logy=True)","7079d2cb":"print(type(cat_counts)) #Check the type of cat_count oh no its a series\ncat_act_c=df['category_id'].value_counts()\nprint(cat_act_c.head(10))\nprint(cat_counts.head(10))\nprint(len(cat_act_c.index))\nprint(len(cat_counts.index))\nprint(cat_act_c.index[:10])\nprint(cat_counts.index[:10])\n#we can see the number of cat is same however it isnt aligned\n#if we join\ndf_act_cat=pd.DataFrame(cat_act_c)\ndf_act_cat.rename(columns={'category_id':'activeness'}, inplace=True)\ndf_cnt_cat=pd.DataFrame(cat_counts)\ndf_cnt_cat.rename(columns={'category_id':'n_product'}, inplace=True)\nprint(df_act_cat.head(10))\nprint(df_cnt_cat.head(10))\njoined = df_act_cat.join(df_cnt_cat)\njoined.head(10)\n# Oh it actually joined well because the index is already the category number\njoined.plot.scatter(x='n_product',y='activeness',figsize=(16,8))","a230a142":"print(df['category_code'].value_counts())\nprint('allRows =',len(df.index))\nprint('nulls =',len(df.index)-sum(df['category_code'].value_counts()))\n##df.drop([\"event_time\"],axis=1).nunique()","dbd22579":"print(df['brand'].value_counts())\nprint('allRows =',len(df.index))\nprint('nulls =',len(df.index)-sum(df['brand'].value_counts()))\nprint(df['brand'].value_counts().head(20))\nprint('brand accessed ', sum(df['brand'].value_counts())\/len(df.index)*100)","9e42291d":"c_p_act = df['price'].value_counts()\nc_p_act.sort_index(inplace=True)\nprint(c_p_act.head(10))\nc_p_act.plot.line()# Huh there are negative price??? And zero prices oh come on wth","066e8b0a":"#Looks like a detective job here\nprint(df.loc[df['price']<0][['product_id','price']].drop_duplicates())","5e352bd3":"#So we're down to five lets see the transaction of each product\nprint(df.loc[df['product_id']==5716855][['event_time','event_type','product_id','price','user_id']])\n#Straight purchase huh\nprint(df.loc[df['product_id']==5716859][['event_time','event_type','product_id','price','user_id']])\nprint(df.loc[df['product_id']==5716857][['event_time','event_type','product_id','price','user_id']])\nprint(df.loc[df['product_id']==5716861][['event_time','event_type','product_id','price','user_id']])\nprint(df.loc[df['product_id']==5670257][['event_time','event_type','product_id','price','user_id']])","854c23e8":"print(len(df[['product_id','price']].drop_duplicates()))\nprint(df[['product_id','price']].drop_duplicates().sort_values(by='product_id').head(20))\nprint(df.drop([\"event_time\"],axis=1).nunique())\n#oh there may be a change of price, so, does the aforementioned thing is pure error?\n#Lets see the most changing product\nprice_list=df[['product_id','price']].drop_duplicates()\nprint(price_list['product_id'].value_counts().head(10))","1ccac3a5":"#lets see how product 5900886 \nprint(df.loc[df['product_id']==5900886][['event_time','event_type','product_id','price','user_id']].iloc[100:150])","31baa852":"def day_far(series):\n    time_span=series.max()-series.min()\n    if time_span.days==0: \n        return 1\n    else:\n        return time_span.days\npvt_pp=pd.pivot_table(df,values=['event_time','event_type'],index=['product_id','price'],aggfunc={'event_time':day_far,'event_type':len})","503bab69":"pvt_pp['act_p_day']=pvt_pp['event_type']\/pvt_pp['event_time']\nprint(pvt_pp.head(10))\npvt_pp.reset_index().plot.scatter(x='price',y='act_p_day',figsize=(16,8))","11698197":"print(pvt_pp.head(10))\npvt_pp.loc[5900886].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))\npvt_pp.loc[5906079].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))\npvt_pp.loc[5816649].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))\npvt_pp.loc[5788139].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))\npvt_pp.loc[5900579].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))\npvt_pp.loc[5901864].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))\npvt_pp.loc[5900883].reset_index().plot.line(x='price',y='act_p_day',figsize=(16,8))","785090f2":"df['user_id'].value_counts()","e02d3571":"#oh 527021202 is the most active user lets see whats done\n#df.loc[(df['user_id']==527021202)&(df['event_time']>pd.to_datetime('2019-11-06'))&(df['event_time']<pd.to_datetime('2019-11-08'))].tail(60)\ndf.loc[(df['user_id']==527021202)&(df['event_type']=='purchase')].tail(60)\n#Oh no the most active user doesnt purchase anything ","c6920286":"cross_usev=pd.crosstab(df['user_id'],df['event_type'])\nprint(cross_usev.sort_values(by='purchase',ascending=False).head(20))","10eb07cf":"#so 557790271 is seemingly the most information rich user, it also has a good proportion of each event type\n#lets see when they did purchase \ndf.loc[(df['user_id']==557790271)&(df['event_type']=='purchase')]\n#big purchase list lets see whats going on 13 Nov\ndata_snippet = df.loc[(df['user_id']==557790271)&(df['event_time']>pd.to_datetime('2019-11-12'))&(df['event_time']<pd.to_datetime('2019-11-14'))]\n#that what we typically want to see a session(multiple sessions actually) started with viewing and ended with purchase \n#im curious of how it went for each product\ndata_snippet.sort_values(by=['product_id','event_time']).head(31)","d06b9b2e":"data_snippet.sort_values(by=['product_id','event_time']).tail(31)","9dd314e4":"fig, ax = plt.subplots()\ncross_usev.plot(kind='scatter',x='view',y='cart',c='purchase',s=8,colormap='PiYG', figsize=(16,8), xlim=(0,4000), ylim=(0,1000), ax=ax)","eaca441b":"#purchasers\npurchaser=cross_usev.loc[cross_usev['purchase']>0]\nprint(purchaser.describe())\npurchaser.plot(kind='scatter',x='view',y='cart',c='purchase',s=8,colormap='PiYG', figsize=(16,8), xlim=(0,4000), ylim=(0,1000))\n","7738295d":"big_shot=cross_usev.loc[cross_usev['purchase']>100]\nbig_shot.plot(kind='scatter',x='view',y='cart',c='purchase',s=20,colormap='PiYG', figsize=(16,8), xlim=(0,4000), ylim=(0,1000))\n","ce09f461":"typical_purchaser=cross_usev.loc[(cross_usev['purchase']<20)&(cross_usev['purchase']>0)]\ntypical_purchaser.plot(kind='scatter',x='view',y='cart',c='purchase',s=20,colormap='PiYG', figsize=(16,8), xlim=(0,4000), ylim=(0,1000))\n","04be457d":"non_purchaser=cross_usev.loc[cross_usev['purchase']==0]\nnon_purchaser.plot(kind='scatter',x='view',y='cart',c='black',s=20, figsize=(16,8), xlim=(0,4000), ylim=(0,1000))","f56a3868":"cross_usev['delta_cart']=cross_usev['cart']-cross_usev['remove_from_cart']\ncross_usev.plot(kind='scatter',x='view',y='delta_cart',c='purchase',s=8,colormap='PiYG', figsize=(16,8), xlim=(0,4000), ylim=(-600,600))\ntypical_purchaser=cross_usev.loc[(cross_usev['purchase']<20)&(cross_usev['purchase']>0)]\ntypical_purchaser.plot(kind='scatter',x='view',y='delta_cart',c='purchase',s=20,colormap='PiYG', figsize=(16,8), xlim=(0,4000), ylim=(-600,600))\nno_purchaser=cross_usev.loc[(cross_usev['purchase']==0)]\nno_purchaser.plot(kind='scatter',x='view',y='delta_cart',c='black',s=20, figsize=(16,8), xlim=(0,4000), ylim=(-600,600))\n","a844304d":"print(df.drop([\"event_time\"],axis=1).nunique())\nprint(df[['product_id','price','brand']].loc[df['product_id']==5809910].drop_duplicates())","ce7671b1":"len(cross_usev.loc[cross_usev['purchase']>0].index)","c1e3ea2d":"# 1. Get Activity of each product\ncross_prev = pd.crosstab(df['product_id'],df['event_type'])\nprint(cross_prev.head(5))\n# 2. Get Price Average\npivot_prodprice = pd.pivot_table(df,index=['product_id'],values=['price'],aggfunc=np.mean)\nprint(pivot_prodprice.head(5))\n# 3. Get Category\nunique_cat_pr = df[['category_id','product_id']].drop_duplicates()\ncat_size = unique_cat_pr['category_id'].value_counts()\nprint(cat_size.head(5))\n# 4. Get number of unique user\nunique_user_event=df[['product_id','user_id','event_type']].drop_duplicates()\ncross_prevuser=pd.crosstab(unique_user_event['product_id'],unique_user_event['event_type'])\nprint(cross_prevuser.head(5))\n","704c21d0":"# 5. Isbranded\ndf_prbr=df[['product_id','brand']].drop_duplicates()\nprint(len(df_prbr.index)) #oh there are some product that brand added later\ndf_prbranded=df.loc[df['brand'].notnull(),['product_id']].drop_duplicates()\nnp_prbrand=df_prbranded.to_numpy()\nprint(len(df_prbranded.index)\/43419)\ndf_prcat = df[['product_id','category_id']].drop_duplicates()\n#trytrylah = np.where((df_prcat['product_id'] in [232423,121212]), True, False)\ndef isBrandPoduct(series):\n    if series['product_id'] in np_prbrand:\n        return True\n    else:\n        return False\ndf_prcat['isBranded'] = df_prcat.apply(isBrandPoduct,axis='columns')    \nprint(df_prcat.head(25))\n# Wow so long for just labeling this, help me shorten this plz\n# now lets jjjjoin","8396afb8":"#Use df_prcat as the base\njoin_1 = df_prcat.join(cross_prev,on='product_id')\n#print(join_1.head(5))\n#print(cross_prev.loc[[5802432,5844397,5837166,5876812,5826182]])\n#for k in [5802432,5844397,5837166,5876812,5826182]:\n#    print(df_prcat.loc[df_prcat['product_id']==k])\njoin_2 = join_1.join(pivot_prodprice,on='product_id')\njoin_2.rename(columns={\"price\": \"avg_price\"},inplace=True)\n#print(join_2.head(5))\njoin_3 = join_2.join(cat_size,on='category_id',rsuffix='_new')\njoin_3.rename(columns={\"category_id_new\": \"competitor\"},inplace=True)\n#print(join_3.head(5))\njoin_4 = join_3.join(cross_prevuser,on='product_id',rsuffix='_user')\n#print(join_4.head(5))\n#print(cross_prev.loc[5802432])\n#print(cross_prevuser.loc[5802432])\njoin_4.to_csv(\"product_score.csv.gz\",index=False,compression=\"gzip\")","4037bbd1":"#Time is a bit confusing\n#lets experiment a little bit\n#remember this\n#def day_far(series):\n#    time_span=series.max()-series.min()\n#    if time_span.days==0: \n#        return 1\n#    else:\n#        return time_span.days\nspan = df['event_time'].max()-df['event_time'].min()\nprint(type(span))\n# so type is pandas Timedelta\nprint(span.value,'nanoseconds')\nprint(span.days,'days')\nprint(span.seconds,'seconds')\nactual_seconds=span.value\/1000000000\nprint('actual_seconds',actual_seconds)\n#how many seconds a day?\nday_sec=60*60*24\n#span second should be same as\nsecond_left = actual_seconds%day_sec\nprint('seconds_left',second_left)\n#on the mark\nsekon=span.seconds%60\nminut=((span.seconds-sekon)\/60)%60\nhourz=(span.seconds-sekon-60*minut)\/3600\nprint('so there should be',hourz,'hours',minut,'minute',sekon,'seconds')","58f822b9":"#Now Getting easy customer scores\n\n# 1. Get Activity of each customer\ncross_usev = pd.crosstab(df['user_id'],df['event_type'])\nprint(cross_usev.head(5))\n# 2. Get n unique product\nunique_us_pr = df[['user_id','product_id']].drop_duplicates()\nuser_reach = unique_us_pr['user_id'].value_counts()\nprint(user_reach.head(5))\n# 3. Get unique product on each event\nunique_us_pr_ev=df[['user_id','product_id','event_type']].drop_duplicates()\ncross_usevprod=pd.crosstab(unique_us_pr_ev['user_id'],unique_user_event['event_type'])\nprint(cross_usevprod.head(5))\n# 4. Get n unique category\nunique_us_cat = df[['user_id','category_id']].drop_duplicates()\nuser_diver = unique_us_cat['user_id'].value_counts()\nprint(user_diver.head(5))\n# 5. Get unique category on each event\nunique_us_cat_ev=df[['user_id','category_id','event_type']].drop_duplicates()\ncross_usevcat=pd.crosstab(unique_us_cat_ev['user_id'],unique_user_event['event_type'])\nprint(cross_usevcat.head(5))","3d7a0cac":"No pattern can be seen in plotting cart to view that correlates with purchase.\n<br>\nMoreover, there is no distinguishable pattern between no-purchaser to your typical purchaser. They just putting into cart and viewing the same amount\n<br>typically more view lead to more purchase alternatively more carting also lead to more purchase. Lets incorporate the remove from cart information","4af3502c":"### Get all the value wanted in separate tables ","4ea87fd1":"### Want to know the product_id\nWhat we can see in general is how many entry per product, in this case we saw the popularity of ea product_id in term of access, so it doesnt linearly mean its more successful. On average (median), we might see a product be accessed 34 times a month, meanwhile the numerical mean is about 100 so the distribution is heavily unbalanced toward the lower end. That means a ton of unclicked things in the net (and here i thought online business is the way to get easy money, well thats not the case here). <br>\nWe can see clear imbalances in the top 10 as the second place is about the third of the first place. Wow, million people didnt realized this. Thats the edge of capitalism for us, where the best just getting better and the worst gets even worse. So to warp it up, i presented the graph on log scale. ","e580feba":"### Whats next\nWe have plotted several graphs for each column data. We got the basic information on each column. What I can remark from the data is as follows:\n* There are 4 type of event with purchase is only 7% of all the entry.\n* 43419 kind of products in 491 category, on average accessed 34 times a month. \n* The most popular product is branded nail polish product.\n* Brand have tendency to encourage activity, 57% data is branded. Is it really the case?\n* Price can change, price affect activity, price can be negative?? \n* 368232 Users only 31524 do purchase, even the most active doesnt purchase.\n* Session may change on access it should have no meaning.\n* Customer event type doesnt correlate to each other. Activity intent may be purely externally driven *\n<br><br>\nSo what to do next?\n<br> The big goal here is to produce something digestible by some machine learning scheme\n<br> From the forked notebook, we can do the same target as:\n Predicting purchase of a product\n<br> However, I might do a different approach on it since the session is unreliable \n<br> I would make two tables first\n* Product scores\n* Customer scores","790eb24b":"As we see, most of the code is null, i dont know whats the importance of this column however the category_id should just be sufficient to hold the information. ","65504f3b":"Although null value is still abundant the brand is a different story altogether. Brand might encourage activity through itself. Look at the number on the top brand, it rivals the activity of the top product. And (after several googling) i found out that 4 of the top 5 is nail polish product (I couldnt find irisk).  <br>\n\nOh yeah does the price affect the popularity of a product?\nFirst lets see the popularity of each price level","280dc47b":"### Want to know sessions\nSession is a bit confusing lets see sessions done by a user\n1. Look for a user that has most activity","9bdc67cc":"I dont know, it doesnt seem like pricing error because no price changed in a period of a month. All the event type is direct purchase (so you can buy without adding to cart huh). Although the price should affect consumption behavior, it can be only seen a low number of activity on this errorneous pricing","d5fdec90":"### Want to know - categories\nThere are 491 category. Wow thats a lot (even this is only in cosmetics). As we can see, the category isnt evenly distributed, I plotted the number of product in each category. The top encloses thousands of product however the last may seem like exclusive product categories <br>\n\nWhat else we might uncover from category?\n<br>Oh lets try to scatter plot the activeness of each category to the number of product","6bcbb54d":"### Join them all","8d302a86":"# Data Preparation and Analysis\n* Well, This is forked from the previous notebook because Im lazy doing and typing the importing formalities\n* I dont know what to do but i found something interesting while working on Excel\n* Might also try to see something in Phyton","6f04a82f":"### About the data types  \nWe see there is 4635837 row of data and 9 columns\n<br> event_time is datetime (of course)\n<br> event_type is object, its string so it must be categoric\n<br> product_id is integer, but i know it should be categoric\n<br> actually most of the data is categoric, the only numeric is price \n<br> and even it doesnt mean a thing, like, i see that (in Excel) its same for the same product meant its the retail price whereas the quantity bought doesnt exist in the data\n","26c420f2":"The figure represents activity in opposition to product number. Hypothetically, it would follow a straight line as more popular a product category tend to attract more business doer. And so, it increases the chance of getting exposure on the category. We observed a bound on activity per product and you may draw a straight line that approximate a linear model of the relation","15cbed3d":"Here we can see a typical view>>cart>>purchase on product 5304 \n<br>however mostly view process is skipped\n<br>we also see typical add remove sequences\n<br>lets see the rest of the data","a8f8b9c8":"#### Product Scores\nOn each product, basically we want to know:\n1. Activity (number of each event type)\n2. Price (Average)\n3. Category (maybe how many competitor)\n4. User (User purchasing, user viewing)\n5. Brand (is branded?)\n\n#### Customer Scores\nAs for customer, we might be wanting:\n1. Active time\n2. Activity (for each event type)\n3. Num Product (for each event type)\n4. Num Product cat (for each event type)\n5. Total Transaction (as only retail price shown it should be unavailable)","c92f5c79":"Now that we have used crosstab on event_type we might as well see the effect of action to purchasing number on each consumer ","46ea7bd6":"Crosstab is a powerful tool\/function, with this only we can do most basic analytics. I found the thing from AnalyticsVidhya you might check it out as well [Link](https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/12-pandas-techniques-python-data-manipulation\/)","d39de729":"Lets get back to customer scoring","9014bc70":"### Now user scores","565e6659":"it seemed like an exponential pattern. However, it look more arbitrary than having any pattern. It should be more meaningful if we see it on each product","c2c82cec":"### Want to know event_type\nThere are only 4 types of event\n* View, yeah just stroll through the product page and buy nothing, most of us do that, especially during work time\n* Cart, add to cart, cuz why not, not gonna buy it though\n* Remove_from_cart, see what i say, some of us just want to put it in and put them out all again\n* Purchase, good job, you are model netizens now just pour in more money\n\nIntuitively when seeing these 4 types of event it leads us to think that a session will consists of:\n* Start >> VIEW >> CART >> REMOVE\/PURCHASE\n\nHowever, thats not the case here\n<br> Anyway we can see that only 7% of the entry is actual purchase\n<br> Removing and Purchasing combined into 27% of the data\n<br> And adding to cart consist of 28.3% data. Huh\n<br> Huh, whats going on? the number doesnt add up."}}