{"cell_type":{"cededd8e":"code","cc1cd219":"code","8ff0cb85":"code","6b8b7674":"code","927cd9fb":"code","fac13a19":"code","3291b05e":"code","45dc03f9":"code","143bab66":"code","85dd1a76":"code","50158c9f":"code","ca763cd2":"code","d22750ac":"code","7b0c1667":"code","0c0c9e2a":"code","e9c00b29":"code","123e055e":"code","13029710":"code","cc440451":"code","292f4718":"code","62a08a16":"code","03e82253":"code","3eb0ad68":"code","1d57d156":"code","5a438c8e":"code","9b5402f2":"code","24e90a5d":"code","d4e11eaf":"code","3e0514b2":"code","ca87e26f":"code","e35ceabf":"code","f5772817":"code","9d1c0bbb":"code","654c069d":"code","ee2d5578":"code","cc46ac8d":"code","10337587":"code","fdd47208":"code","bdd9749d":"code","7ab0c9d2":"code","ffc7d052":"code","925d26c4":"code","d97f6bad":"code","4b4127f0":"code","387cee71":"code","d412df84":"code","648af038":"code","14df4455":"code","5d9ab5af":"code","2d310372":"code","24b15946":"code","380c63df":"code","7a806efa":"code","00308b86":"code","c5e33fd6":"code","b6f5d892":"code","b7e1afe9":"code","2b55f511":"code","aa418b27":"code","83a67ca7":"code","8cc091a5":"code","29b1beaf":"code","22037d13":"code","6c980aad":"code","3e22c4bf":"code","1b0fb6aa":"code","014e09f0":"code","e39c61dc":"code","c5f69ef7":"code","f2d1c2bb":"code","6940381d":"code","bcf1843d":"code","5f0b7293":"code","db2c838e":"code","cff4f37b":"code","f05cf6ba":"code","a158294c":"markdown","a0713631":"markdown","238199ab":"markdown","873356c3":"markdown","1aad9133":"markdown"},"source":{"cededd8e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport geocoder\nimport tensorflow_hub as hub","cc1cd219":"train=pd.read_csv(r\"C:\\Data Science for Telecom\\NLP\\Twitter Prediction\\train.csv\")","8ff0cb85":"train.head()","6b8b7674":"test=pd.read_csv(r\"C:\\Data Science for Telecom\\NLP\\Twitter Prediction\\test.csv\")","927cd9fb":"test.head()","fac13a19":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","3291b05e":"# Class distribution\n\nx=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","45dc03f9":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_prob =train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len_prob,color='red')\nax1.set_title(\"Disaster_tweets\")\ntweet_len_noprob=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len_noprob,color='green')\nax2.set_title(\"Non_disastor_tweets\")","143bab66":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_word_dis = train[train['target']==1]['text'].str.split().map(lambda x:len(x))\nax1.hist(tweet_len_word_dis,color='red')\nax1.set_title(\"Disastor_tweets\")\ntweet_len_word_nodis = train[train['target']==0]['text'].str.split().map(lambda x:len(x))\nax2.hist(tweet_len_word_nodis,color='green')\nax2.set_title(\"No_Disastor_tweets\")","85dd1a76":"fig,(ax1,ax2)= plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word.map(lambda x:np.mean(x)),ax=ax1,color='red')\nax1.set_title(\"disastor_tweet\")\nword_nodis =train[train['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word.map(lambda x:np.mean(x)),ax=ax2,color='green')\nax2.set_title(\"no_disastor_tweet\")","50158c9f":"def create_corpus(target):\n    corpus=[]\n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return(corpus)","ca763cd2":"corpus=create_corpus(0)\nlen(corpus)","d22750ac":"dic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\ndic","7b0c1667":"top=sorted(dic.items(),key=lambda x:x[1],reverse=True)[:10]\ntop","0c0c9e2a":"x,y=zip(*top)\nplt.bar(x,y)","e9c00b29":"corpus1=create_corpus(1)\ndic1=defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        dic1[word]+=1\n","123e055e":"top1 = sorted(dic1.items(),key=lambda x:x[1],reverse=True)[:10]","13029710":"top1","cc440451":"x1,y1=zip(*top1)\nplt.bar(x1,y1)","292f4718":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\ndic=defaultdict(int)\nimport string\nspecial=string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x,y)","62a08a16":"corpus=create_corpus(0)\ndic=defaultdict(int)\nfor i in corpus:\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x,y)","03e82253":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","3eb0ad68":"sns.barplot(x=x,y=y)","1d57d156":"df=train.append(test).reset_index(drop=True)","5a438c8e":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","9b5402f2":"def remove_url(text):\n    url = re.compile(r'https:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\nremove_url(example)","24e90a5d":"df['text']=df['text'].apply(lambda x : remove_url(x))","d4e11eaf":"def remove_html(text):\n    html=re.compile('<.*?>')\n    return html.sub(r'',text)\n","3e0514b2":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"\n\nprint(remove_html(example))","ca87e26f":"df['text']=df['text'].apply(lambda x : remove_html(x))","e35ceabf":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","f5772817":"remove_emoji(\"Omg #another Earthquake \ud83d\ude14\ud83d\ude14\")","9d1c0bbb":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","654c069d":"def remove_punct(text):\n    try:\n        table =str.maketrans(\"\",\"\",string.punctuation)\n        return text.translate(table)\n    except:\n        return(text)","ee2d5578":"print(remove_punct(\"is , an the value\"))","cc46ac8d":"df['text']=df['text'].apply(lambda x: remove_punct(x))","10337587":"from spellchecker import SpellChecker","fdd47208":"spell=SpellChecker()\ndef spell_correction(text):\n    try:\n        spelled =[]\n        uncorrect =spell.unknown(text.split())\n        for word in text.split():\n            if word in uncorrect:\n                spelled.append(spell.correction(word))\n            else:\n                spelled.append(word)\n        return(\" \".join(spelled))\n    except:\n        return(text)","bdd9749d":"example=\"kiss me plese\"\nspell_correction(example)","7ab0c9d2":"def ascii_remover(text):\n    try:\n        all_ascii = ''.join(char for char in text if ord(char) < 128)\n        return(all_ascii)\n    except:\n        return(text)","ffc7d052":" ascii_remover(\"45\u00c3\u00a5\u00c2\u00a1 5'12.53N   14\u00c3\u00a5\u00c2\u00a1 7'24.93E\")\n","925d26c4":"df['text']=df['text'].apply(lambda x:ascii_remover(x))","d97f6bad":"df['location']=df['location'].apply(lambda x:ascii_remover(x))","4b4127f0":"def replace(text):\n    try:\n        \n        text_replace=text.replace(\"T: \", \"\")\n        return(text_replace)\n    except:\n        return(text)","387cee71":"df['location']=df['location'].apply(lambda x:replace(x))","d412df84":"#df['location']=df['location'].apply(lambda x: remove_punct(x))","648af038":"from geopy.geocoders import Nominatim","14df4455":"def reverse_geocoder(text):\n    \n    geolocator = Nominatim()\n    try:\n        location = geolocator.reverse(text)\n        address=location.address\n        if address is None:\n            return(text)\n        else:\n            return(address)\n    except:\n        return(text)\n    ","5d9ab5af":"df1=df[df['id']==9662]\ndf1","2d310372":"df1['location']=df1['location'].apply(lambda x:reverse_geocoder(x))\ndf1","24b15946":"df['location']=df['location'].apply(lambda x:reverse_geocoder(x))","380c63df":"df['location']=df['location'].apply(lambda x:spell_correction(x))","7a806efa":"def string_capitalize(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.capitalize())\n        return(\" \".join(word1))\n        \n    except:\n        return(text)","00308b86":"df['location']=df['location'].apply(lambda x:string_capitalize(x))\ndf['text']=df['text'].apply(lambda x:string_capitalize(x))\ndf['text']=df['text'].apply(lambda x:spell_correction(x))","c5e33fd6":"from geotext import GeoText","b6f5d892":"def geo_cities(text):\n    try:\n        places=GeoText(text)\n        city=places.cities\n        country=places.countries\n\n        if len(city)>0:\n            \n            return(city[0])\n        elif len(country)>0:\n            return(country[0])\n        else :\n            return(text)\n    except:\n        return(text)","b7e1afe9":"df['location']=df['location'].apply(lambda x: geo_cities(x))","2b55f511":"df['location']=df['location'].apply(lambda x: remove_punct(x))","aa418b27":"df['keyword']=df['keyword'].apply(lambda x:spell_correction(x))","83a67ca7":"#nltk.download('averaged_perceptron_tagger')","8cc091a5":"from pattern.text.en import singularize\ndef singular(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(singularize(word))\n        return(\" \".join(word1))\n        \n    except:\n        return(text)\n        ","29b1beaf":"df['text']=df['text'].apply(lambda x:singular(x))","22037d13":"def string_lower(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.lower())\n        return(\" \".join(word1))\n        \n    except:\n        return(text)","6c980aad":"df['text']=df['text'].apply(lambda x:string_lower(x))","3e22c4bf":"\ndef keyword(text):\n    disastor=['flooding','wildfire','bombed','bagging','kill','dead','apocalypse','calamity','catastrophe','collapse','crash','debacle','defeat','emergency','failure','fiasco','flood','harm','hazard','holocaust','mishap','setback','tragedy','woe','adversity','affliction','bale','bane','blight','blow','bust','casualty','cataclysm','collision','depression','exigency','fall','flop','grief','misadventure','mischance','misfortune','reverse','rock','rough','ruin','ruination','slip','stroke','undoing','upset','washout','act of God','bad luck','bad news','fell stroke','hard luck','hot water','ill luck','the worst','crack-up','disaster','fender-bender','fluke','pileup','rear ender','smash','smashup','stack-up','total','wrack-up','bad break','bummer','can of worms','clutch','contretemps','crunch','difficulty','distress','downer','drag','evil eye','hard knocks','hard times','hardship','hurting','ill fortune','jam','jinx','kiss of death','misery','on the skids','pain in the neck','poison','sorrow','suffering','tough luck','trial','trouble','annoyance','bad trip','disappointment','irritation','bum trip','depressing experience','raw deal','rotten hand','unhappy situation','unpleasant experience','unpleasant situation','unpleasent','burden','b\u00eate noir','curse','despair','destruction','downfall','fatal attraction','nuisance','pest','plague','scourge','torment','venom','balk','bolt from the blue','bombshell','chagrin','comedown','disgruntlement','frustration','jolt','letdown','shock','chance','contingency','accident','alluvion','culmination','curtains','denouement','desolation','devastation','end','fatality','finale','havoc','ill','infliction','meltdown','termination','upshot','waterloo','wreck','embarrassment','predicament','agitation','clamor','commotion','ferment','furor','outcry','quaking','rocking','seism','shaking','tottering','trembling','tumult','turbulence','upheaval','upturn','big trouble','change','climacteric','climax','confrontation','corner','crossroad','crux','deadlock','dilemma','dire straits','entanglement','extremity','height','hot potato','hour of decision','imbroglio','impasse','juncture','mess','moment of truth','necessity','pass','perplexity','pickle','pinch','plight','point of no return','pressure','puzzle','quandary','situation','stew','strait','trauma','turning point','urgency','Cancer','cross','evil','hydra','ordeal','pestilence','tribulation','vexation','voodoo','beating','blue ruin','breakdown','defeasance','dissolution','drubbing','licking','overthrow','reversal','rout','shellacking','trouncing','vanquishment','bitter pill','blind alley','blunder','bringdown','discouragement','dud','error','false alarm','faux pas','fizzle','flash in the pan','inefficacy','lemon','miscalculation','mistake','obstacle','old one-two','destitution','hard time','holy mess','indigence','need','poverty','privation','rigor','rotten luck','scrape','straits','throe','ticklish spot','tough break','unholy mess','vicissitude','want','Judgment Day','Moira','annihilation','circumstance','conclusion','condemnation','death','decree','destination','destiny','fixed future','foreordination','fortune','handwriting on wall','judgment','karma','kismet','lap of the gods','lot','opinion','portion','predestination','predetermination','sentence','verdict','way the ball bounces','way the cookie crumbles','acme','acuteness','apex','apogee','border','bound','boundary','brim','brink','butt','consummation','crisis','depth','edge','excess','extreme','extremes','frontier','last','margin','maximum','nadir','outside','pinnacle','pole','remote','rim','terminal','terminus','tip','top','verge','vertex','zenith','abasement','capitulation','degradation','diminution','dive','drop','humiliation','loss','resignation','surrender','tumble','deadliness','destructiveness','dying','inevitability','lethality','lethalness','mortality','necrosis','noxiousness','poisonousness','virulence','abortion','botched situation','dumb thing to do','dumb trick','farce','flap','miscarriage','route','screwup','stunt','bomb','loser','nonstarter','Herculean task','asperity','austerity','case','danger','discomfort','drudgery','fatigue','grievance','injury','labor','oppression','peril','persecution','rainy day','toil','travail','uphill battle','worry','ache','black and blue','boo-boo','bruise','chop','detriment','disadvantage','disservice','down','gash','ill-treatment','mark','mischief','nick','ouch','outrage','pain','pang','prejudice','scratch','sore','soreness','wound','wrong','bereavement','cost','damage','debit','debt','deficiency','depletion','deprivation','disappearance','dispossession','forfeiture','hurt','impairment','losing','mislaying','misplacing','perdition','retardation','sacrifice','shrinkage','squandering','waste','wreckage','lapse','ill-fortune','rear-ender','crime','crying shame','regret','shame','sin','clobbering','confusion','flight','hiding','retreat','romp','shambles','shutout','thrashing','trashing','walkover','waxing','whipping','about-face','alteration','convulsion','disorder','disruption','disturbance','eruption','explosion','flip-flop','new ball-game','new deal','outbreak','outburst','revolution','shakeout','stirring','switch','temblor','tremor','turmoil','turnaround','punishment','retribution','downpour','erosion','gully','agony','anguish','bemoaning','blues','care','dejection','deploring','dole','gloom','grieving','headache','heartache','heartbreak','lamentation','melancholy','rain','rue','sadness','unhappiness','wretchedness','attack','armageddon','aftershock','typhoon','asteroid','tsunami','natural disasters','volcano','tornado','avalanche','earthquake','blizzard','drought','bushfire','dust storm','magma','twister','windstorm','heat wave','cyclone','forest fire','fire','hailstorm','lava','lightning','high-pressure','hail','hurricane','seismic','whirlpool','Richter scale','whirlwind','cloud','thunderstorm','barometer','gale','blackout','gust','force','low-pressure','volt','snowstorm','rainstorm','storm','nimbus','violent storm','sandstorm','Beaufort scale','fatal','cumulonimbus','lost','money','tension','uproot','underground','destroy','arsonist','wind scale','arson','rescue','permafrost','fault','shelter','ablaze'\n]\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            if word in disastor:\n                word1.append(word)\n        return(\",\".join(word1))\n    \n    except:\n        return(\"\")","1b0fb6aa":"y1 = df.loc[df['keyword']=='']\nindex_y1=list(y1.index.values)\nprint(index_y1)\n\nfor i in index_y1:\n    df['keyword'].iat[i]=keyword(df['text'].iat[i])","014e09f0":"df['keyword'].iat[3]","e39c61dc":"df['text']=df['text'].apply(lambda x:string_capitalize(x))","c5f69ef7":"from nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk import Tree\ndef get_continuous_chunks(text, label):\n    try:\n        chunked = ne_chunk(pos_tag(word_tokenize(text)))\n        prev = None\n        continuous_chunk = []\n        current_chunk = []\n\n        for subtree in chunked:\n            if type(subtree) == Tree and subtree.label() == label:\n                current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n            elif current_chunk:\n                named_entity = \" \".join(current_chunk)\n                if named_entity not in continuous_chunk:\n                    continuous_chunk.append(named_entity)\n                    current_chunk = []\n            else:\n                continue\n\n        return continuous_chunk\n    except:\n        return(\"\")","f2d1c2bb":"get_continuous_chunks(\"Haha South Tampa Is Getting Flooded Hah Wait A Second I Live In South Tampa What Am I Gonna Do What Am I Gonna Do Fvck Flooding\",'GPE')","6940381d":"y = df.loc[df['location']==\"\"]\nindex_y=list(y.index.values)\nprint(index_y)\n\nfor i in index_y:\n    df['location'].iat[i]=get_continuous_chunks(df['text'].iat[i],'LOCATION')","bcf1843d":"y = df.loc[df['location']==\"\"]\nindex_y=list(y.index.values)\nprint(index_y)\n\nfor i in index_y:\n    df['location'].iat[i]=get_continuous_chunks(df['text'].iat[i],'GPE')","5f0b7293":"df.head(100)","db2c838e":"def remove_ing(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.replace(\"ing\",\"\"))\n        return(\" \".join(word1))\n    \n    except:\n        return(\"\")","cff4f37b":"df['text']=df['text'].apply(lambda x:remove_ing(x))\ny1 = df.loc[df['keyword']=='']\nindex_y1=list(y1.index.values)\nprint(index_y1)\n\nfor i in index_y1:\n    df['keyword'].iat[i]=keyword(df['text'].iat[i])","f05cf6ba":"df.to_csv(r'C:\\Data Science for Telecom\\NLP\\Twitter Prediction\\output_datafile.csv', index=False)","a158294c":"## Perc distribution of word length per tweet","a0713631":"## Spread of Punctuations","238199ab":"## Word Length Distribution","873356c3":"## Character length distribution","1aad9133":"To Capture those cities\/geographical areas which are made up of 2 words with a space between them e.g. South Tampa , North Wales etc.."}}