{"cell_type":{"a65fe9a9":"code","82653188":"code","15423978":"code","e7cfeb4b":"code","66fb9d3d":"code","495f1e22":"code","27fea994":"code","9528d74b":"code","2cfad966":"code","352606f2":"code","6ec66f7a":"code","bf250ffe":"code","987ab524":"code","e8655acf":"code","3aec38e3":"code","e512c311":"code","e1e122c1":"code","3d9b61f4":"code","acab9bd1":"code","6cd18967":"code","566246cd":"code","33356d1c":"code","a690b343":"code","93720180":"code","df29d03c":"code","041b86fa":"code","254d0755":"code","7bb276d2":"code","a567acba":"markdown","859662eb":"markdown","7cb57b62":"markdown","37006cf5":"markdown","54836d23":"markdown","451b5a85":"markdown","aee2e174":"markdown","5eef8e32":"markdown","c73ea951":"markdown","f5954315":"markdown","b016385a":"markdown","47dca286":"markdown","b05ad46d":"markdown","5fcae764":"markdown","8376e27e":"markdown","d0f792d5":"markdown","216a4768":"markdown","75730e57":"markdown","5413d61b":"markdown","2088bcc2":"markdown","cb2d9d38":"markdown","bbe5c228":"markdown","5bc38ee5":"markdown","b3aafc62":"markdown","5b309b16":"markdown","61b3f3bb":"markdown","5e956e13":"markdown","071280d7":"markdown","adf5210e":"markdown","d4b5e4a7":"markdown","605709d5":"markdown"},"source":{"a65fe9a9":"%matplotlib inline\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import GRU,SimpleRNN,LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport IPython\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom wordcloud import WordCloud","82653188":"valid = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation-processed-seqlen128.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train-processed-seqlen128.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test-processed-seqlen128.csv\")\nsubmit = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv\")\ntrain = train[['id', 'comment_text', 'input_word_ids', 'input_mask','all_segment_id', 'toxic']].iloc[:20000] #limit p\u00e5 tr\u00e4nings set","15423978":"train.info()","e7cfeb4b":"train.tail(12)","66fb9d3d":"valid.tail(12)","495f1e22":"test.tail(12)","27fea994":"train.isnull().any(),test.isnull().any() # test om det finns null v\u00e4rden i v\u00e5r tr\u00e4ningsdata","9528d74b":"train_distribution = train[\"toxic\"].value_counts().values\nvalid_distribution = valid[\"toxic\"].value_counts().values\n\nnon_toxic = [train_distribution[0] \/ sum(train_distribution) * 100, valid_distribution[0] \/ sum(valid_distribution) * 100]\ntoxic = [train_distribution[1] \/ sum(train_distribution) * 100, valid_distribution[1] \/ sum(valid_distribution) * 100]\n\nplt.figure(figsize=(9,6))\nplt.bar([0, 1], non_toxic, alpha=.4, color=\"r\", width=0.35, label=\"non-toxic\")\nplt.bar([0.4, 1.4], toxic, alpha=.4, width=0.35, label=\"toxic\")\nplt.xlabel(\"Dataset\")\nplt.ylabel(\"Percentage\")\nplt.xticks([0.2, 1.2], [\"train\", \"valid\"])\nplt.legend(loc=\"upper right\")\n\nplt.show()","2cfad966":"print(f\"Tr\u00e4ningsdata: \\nnon-toxic rate: {train_distribution[0] \/ sum(train_distribution) * 100: .2f} %\\ntoxic rate: {train_distribution[1] \/ sum(train_distribution) * 100: .2f} %\")\nprint(f\"Valideringsdata: \\nnon-toxic rate: {valid_distribution[0] \/ sum(valid_distribution) * 100: .2f} %\\ntoxic rate: {valid_distribution[1] \/ sum(valid_distribution) * 100: .2f} %\")","352606f2":"# m\u00f6jligg\u00f6r unnders\u00f6kning av l\u00e4ngden p\u00e5 kommentarerna i tr\u00e4ningsdata\ntrain['char_length'] = train['comment_text'].apply(lambda x: len(str(x)))","6ec66f7a":"# visar plottad histogram f\u00f6r l\u00e4ngd p\u00e5 kommentarer (antal tecken)\nsns.set()\ntrain['char_length'].hist()\nplt.show()","bf250ffe":"def nonan(x):\n    if type(x) == str:\n        return x.replace(\"\\n\", \"\")\n    else:\n        return \"\"\n\ntext = ' '.join([nonan(abstract) for abstract in train[\"comment_text\"]])\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text)\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Frekventa ord i kommentarer')","987ab524":"subset = train.query(\"toxic == 0\")\ntext = subset.comment_text.values\nwc = WordCloud(background_color=\"black\",max_words=1500)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(7.5, 7.5))\nplt.axis(\"off\")\nplt.title(\"Frekventa ord i icke-hatfulla kommentarer\", fontsize=16)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17))\nplt.show()\n\nsubset = train.query(\"toxic == 1\")\ntext = subset.comment_text.values\nwc = WordCloud(background_color=\"black\",max_words=1500)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(7.5, 7.5))\nplt.axis(\"off\")\nplt.title(\"Frekventa ord i hatfulla kommentarer\", fontsize=16)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17))\nplt.show()","e8655acf":"train = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\nvalidation = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\n\ntrain.drop(['severe_toxic','obscene','threat','insult','identity_hate'], axis=1, inplace=True) # droppar klassificeringar av hatfulla kommentarer","3aec38e3":"train = train.loc[:15000,:] #best\u00e4mmer delm\u00e4ng\ntrain.shape #h\u00e4mtar form p\u00e5 tr\u00e4nings set","e512c311":"train['comment_text'].apply(lambda x:len(str(x).split())).max() # h\u00e4mtar max antal tecken i kommentar","e1e122c1":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n                                                  stratify=train.toxic.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","3d9b61f4":"def roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","acab9bd1":"# anv\u00e4nder keras tokenizer\ntoken = text.Tokenizer(num_words=None)\nmax_len = 1500\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n#zero paddar sekvenserna\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","6cd18967":"# Uppt\u00e4ck h\u00e5rdvara, returnera l\u00e4mplig distributionsstrategi\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","566246cd":"# En enkel modell (SimpleRNN) med ett dense layer\n\nwith strategy.scope():\n    \n    model = Sequential() #sekventiellt n\u00e4tverk\n    model.add(Embedding(len(word_index) + 1, #konverterar till 300 dimensionell vektor\n                     300,\n                     input_length=max_len))\n    model.add(SimpleRNN(100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel.summary()","33356d1c":"history = model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64)","a690b343":"scores = model.predict(xvalid_pad)\nprint(\"AUC: %.2f%%\" % (roc_auc(scores,yvalid)))","93720180":"# load the GloVe vectors in a dictionary:\n\nembeddings_index = {}\nf = open('\/kaggle\/input\/glove840b300dtxt\/glove.840B.300d.txt','r',encoding='utf-8') #precis som v\u00e5r data f\u00f6r kommentarer s\u00e5 l\u00e4gger vi till GloVe i v\u00e5r Kaggle-databas\nfor line in tqdm(f):\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","df29d03c":"# skapa en embedding matris f\u00f6r orden vi har i v\u00e5r dataupps\u00e4ttning\nembedding_matrix = np.zeros((len(word_index) + 1,300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","041b86fa":"%%time\nwith strategy.scope():\n    \n    model = Sequential() # sekventiellt n\u00e4tverk\n    model.add(Embedding(len(word_index) + 1, # embedding lager som nu anv\u00e4nder GloVe ist\u00e4llet\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\n\n    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences=True)) # tre LSTM lager med 100 units, dropout har lagts till\n    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3,return_sequences=True))\n    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel.summary()","254d0755":"model.fit(xtrain_pad, ytrain, nb_epoch=5, batch_size=64)","7bb276d2":"scores = model.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))","a567acba":"Vi anv\u00e4nder oss utav Word embeddings\n\nWord embeddings \u00e4r en typ av ord representation som till\u00e5ter ord med liknande mening att ha liknande representation\n\nMer specifikt s\u00e5 anv\u00e4nder vi oss utav GloVe, vilket \u00e4r ett av de f\u00f6rtr\u00e4nade embeddings som jag tidigare n\u00e4mnde i v\u00e5r mer enkla modell. F\u00f6r att f\u00f6renkla allt s\u00e5 kan man s\u00e4ga att meningen bakom GloVe som ett pre-trained word embedding \u00e4r att h\u00e4rleda f\u00f6rh\u00e5llandet mellan ord fr\u00e5n global statistik.\n\n\"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\" - Stanford","859662eb":"Ist\u00e4llet f\u00f6r att g\u00e5 p\u00e5 \u00f6gonm\u00e5tt s\u00e5 kan vi ta fram de exakta procentv\u00e4rdena f\u00f6r v\u00e5r data...\n\nVi vet nu att v\u00e5r tr\u00e4ningsdata best\u00e5r av 9,74% hatfulla kommentarer och att i v\u00e5r valideringsdata s\u00e5 \u00e4r detta v\u00e4rde ist\u00e4llet 15,38%.","7cb57b62":"Pr\u00f6var f\u00f6rst med en enkel modell f\u00f6r att se vilket resultat som n\u00e5s\n\nmodel.Sequential() s\u00e4ger \u00e5t keras att vi kommer bygga v\u00e5rt n\u00e4tverk sekventiellt. Sedan s\u00e5 l\u00e4gger vi f\u00f6rst till v\u00e5rt Embedding lager som \u00e4r ett lager av neuroner som tar in en input som \u00e4r en 'one hot' vektor av varje ord och konverterar den till en 300 dimensionell vektor. Den ger oss word embedding som liknar word2vec - word2vec g\u00f6r om text till en numerisk form s\u00e5 att neural networks kan f\u00f6rst\u00e5. I denna modell s\u00e5 \u00e4r v\u00e5rt embedding lager inte pre-trained; pre-trained embedding \u00e4r en embedding som har tr\u00e4nats i en task f\u00f6r att sedan anv\u00e4ndas i en liknande task. Dessa embeddings \u00e4r tr\u00e4nade p\u00e5 stora dataupps\u00e4ttningar, lagrade, och anv\u00e4nds sedan f\u00f6r att l\u00f6sa andra problem. Sedan s\u00e5 har vi ett SimpleRNN lager med 100 units, notera att vi i denna modell inte anv\u00e4nder dropout. Sist s\u00e5 har vi v\u00e5rt Dense layer med en sigmoid aktiveringsfunktion som tar outputen fr\u00e5n det tidigare lagret f\u00f6r att g\u00f6ra en prediction.","37006cf5":"Nedanst\u00e5ende kod f\u00f6rs\u00f6ker att anv\u00e4nda Kaggle TPU och d\u00e4refter returnerna l\u00e4mplig distruberings strategi","54836d23":"Vi unders\u00f6ker vad max antal tecken kan vara i en kommentar vilket kan underl\u00e4tta f\u00f6r oss senare med padding.","451b5a85":"# Diskussion\n\n**Vad f\u00f6r slutsatser kan vi g\u00f6ra?**\n\nDet \u00e4r klart och tydligt att en allt f\u00f6r simpel modell inte kommer prestera bra f\u00f6r v\u00e5rt problem d\u00e5 det \u00e4r en stor datam\u00e4ngd och ett komplext problem. V\u00e5r andra modell som var lite mer anpassad f\u00f6r problemet presterade b\u00e4ttre och n\u00e5dde bra resultet f\u00f6r AUC. Dock s\u00e5 \u00e4r s\u00e5klart inte modellen perfekt p\u00e5 n\u00e5got vis, jag tror att det g\u00e5r att l\u00f6sa problemet allt b\u00e4ttre med en mer advancerad l\u00f6sning\/modell. N\u00e5got som kommer till tanke efter duggorna vi hade \u00e4r BERT och dess f\u00f6rm\u00e5ga att l\u00f6sa s\u00e5dana h\u00e4r problem effektivt. BERT har setts som en ny era inom Natural Language Processing (NLP) och \u00e4r en modell som har slagit nya rekord f\u00f6r hur bra en modell kan hantera spr\u00e5kbaserade problem. BERT-modellen \u00e4r open-sourced och d\u00e4rmed tillg\u00e4nglig f\u00f6r alla att ladda ner versioner av modellen som redan har blivit tr\u00e4nade p\u00e5 stora dataupps\u00e4ttningar. Detta g\u00f6r det m\u00f6jligt f\u00f6r att vem som helst som bygger en machine learning modell f\u00f6r spr\u00e5kbehandling att anv\u00e4nda denna kraftfulla komponent vilket sparar tid, energi, kunskap och resurser som hade g\u00e5tt till att tr\u00e4na en spr\u00e5kbehandlings modell fr\u00e5n grunden. Jag skulle s\u00e4ga att mitt perspektiv fram\u00e5t \u00e4r att implementera BERT f\u00f6r detta problem och se vad man hade f\u00e5tt f\u00f6r resultat och hur smidigt det skulle vara j\u00e4mf\u00f6rt med mitt ursprungliga tillv\u00e4gag\u00e5ngs\u00e4tt. Jag har inte jobbat med NLP till denna grad f\u00f6rr men det verkar v\u00e4ldigt intressant och anv\u00e4ndbart, detta k\u00e4nns som att detta \u00e4r endast ett scenario d\u00e4r det kan vara smart att implementera l\u00f6sningar med NLP. Hatfulla kommentarar tror jag p\u00e5verkar m\u00e4nniskor allt mer \u00e4n vad man tror och ser, speciellt online d\u00e4r m\u00e4nniskor oftast \u00e4r anonyma n\u00e4r dessa kommentarer g\u00f6rs. Det g\u00e5r fr\u00e5n person till person, personligen tror jag inte att jag skulle bli allt f\u00f6r p\u00e5verkad av s\u00e5dana kommentarer, men det \u00e4r naturligt att m\u00e5nga m\u00e4nniskor skulle bli det och d\u00e4rf\u00f6r s\u00e5 \u00e4r det viktigt att s\u00e5dana h\u00e4r problem unders\u00f6ks mer s\u00e5 att vi effektivt kan motverka hatfulla kommentarer online f\u00f6r att m\u00e4nniskor ska f\u00e5 en b\u00e4ttre upplevelse!\n\n\n**Litteratur och kod som anv\u00e4nts:**\n\nBinary crossentropy\n\nhttps:\/\/peltarion.com\/knowledge-center\/documentation\/modeling-view\/build-an-ai-model\/loss-functions\/binary-crossentropy\n\nPretrained word embedding NLP\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/pretrained-word-embeddings-nlp\/\n\nWord2Vec\n\nhttps:\/\/pathmind.com\/wiki\/word2vec\n\nGloVe (dataupps\u00e4ttning och information)\n\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/\n\nAUC\n\nhttps:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc\n\nNLP och Wordcloud's\n\nhttps:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n\nKaggle T\u00e4vlingen som detta projekt \u00e4r baserat p\u00e5, dataupps\u00e4ttningen har h\u00e4mtats h\u00e4rifr\u00e5n\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/overview\n\nExploratory Data Analysis (EDA), bra f\u00f6rklaring och visualisering av data.\n\nhttps:\/\/www.kaggle.com\/jagangupta\/stop-the-s-toxic-comments-eda\n\nEn kernel f\u00f6r liknande \u00e4ldre t\u00e4vling\n\nhttps:\/\/www.kaggle.com\/rhodiumbeng\/classifying-multi-label-comments-0-9741-lb\n","aee2e174":"Vi b\u00f6rjar som vanligt med att h\u00e4mta n\u00f6dv\u00e4ndiga bibliotek","5eef8e32":"# Metod","c73ea951":"N\u00e4r vi tittar p\u00e5 v\u00e5rt valideringsdata s\u00e5 ser vi att kommentarerna nu ist\u00e4llet \u00e4r i spr\u00e5ken turkiska, spanska och italienska. Precis som tidigare s\u00e5 \u00e4r kommentarerna \u00e4ven h\u00e4r antingen hatfulla eller inte. Dessa tre spr\u00e5k \u00e4r inte de \u00e4nda i denna data, enligt t\u00e4vlingens sida s\u00e5 ser uppdelningen ut s\u00e5h\u00e4r f\u00f6r v\u00e5rt valideringsdata:\n\nTurkiska = 38%\n\nSpanska = 31%\n\nAndra spr\u00e5k = 31%","f5954315":"# **Introduktion**\n\nAtt vara anonym p\u00e5 internet g\u00f6r det m\u00f6jligt f\u00f6r m\u00e4nniskor att s\u00e4ga v\u00e4ldigt hatfulla saker som de normalt inte hade sagt i person. Dessa hatfulla kommentarer kan ha en stor effekt p\u00e5 m\u00e4nniskor och d\u00e4rf\u00f6r \u00e4r syftet med detta projekt att f\u00f6rs\u00f6ka filtrera ut hatfulla kommentarer. Detta projekt \u00e4r baserat p\u00e5 en \u00f6ppen t\u00e4vling p\u00e5 Kaggle vid namn \"Jigsaw Multilingual Toxic Comment Classification\" - allts\u00e5 s\u00e5 skall en klassifiering g\u00f6ras av hatfulla kommentarer i flertal spr\u00e5k. Genom att identifiera och filtrera hatfulla kommentarer online s\u00e5 kan vi f\u00e5 en s\u00e4krare online-upplevelse som leder till h\u00f6gre produktivitet och n\u00f6je.\n\nProjektet utf\u00f6rs h\u00e4r p\u00e5 Kaggle f\u00f6r enkelhetens skull d\u00e5 vid f\u00f6rs\u00f6k att anv\u00e4nda collab f\u00f6r att l\u00f6sa problemet s\u00e5 har flertal problem uppst\u00e5tt.\n\nKaggle T\u00e4vling L\u00e4nk:\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/data","b016385a":"**Modellering**","47dca286":"I ovanst\u00e5ende tabeller s\u00e5 har vi sett att det finns b\u00e5de hatfulla och icke-hatfulla kommentarer i v\u00e5r tr\u00e4nings- och validerings dataset, dock s\u00e5 verkar det som att de flesta kommentarer i v\u00e5r data \u00e4r icke-hatfulla. Nedanst\u00e5ende kod \u00e4r till f\u00f6r att belysa hur uppdelningen faktiskt ser ut rent procentm\u00e4ssigt.\n\nVi ser att v\u00e5rt tr\u00e4nings set best\u00e5r av cirka 85-90% icke-hatfulla kommentarer och d\u00e4rmed ca. 10-15% kommentarer som \u00e4r hatfulla. V\u00e5r validerings set \u00e4r n\u00e4rmare 80-82% n\u00e4r det kommer till icke-hatfulla kommentarer och ca. 18% hatfulla kommentarer.","b05ad46d":"H\u00e4r ser vi tabellen f\u00f6r v\u00e5rt tr\u00e4ningsdata och d\u00e4rmed n\u00e5gra exempel p\u00e5 hur kommentarerna kan se ut. Vi ser att alla \u00e4r, som tidigare n\u00e4mnt, p\u00e5 engelska och att vissa \u00e4r hatfulla (toxic) och vissa inte.","5fcae764":"Tittar vi p\u00e5 v\u00e5rt histogram ser vi att de flesta kommentarerna \u00e4r inom 500 tecken samtidigt som vissa g\u00e5r mot 2000+ tecken.","8376e27e":"Tr\u00e4ning av modellen","d0f792d5":"Fr\u00e5n vad jag har sett h\u00e4r p\u00e5 Kaggle s\u00e5 anv\u00e4nds AUC score f\u00f6r sj\u00e4lva valideringen, nedanst\u00e5ende kod skapar en funktion som h\u00e4mtar AUC score som kommer att anv\u00e4ndas. AUC v\u00e4rdet ligger mellan 0 och 1, en modell med 100% fel predictions har ett AUC p\u00e5 0.0; en modell som har 100% r\u00e4tt har en AUC p\u00e5 1.0. AUC \u00e4r d\u00e4rmed ett s\u00e4tt att m\u00e4ta prestation f\u00f6r en modell och \u00e4r ett sammanlagt m\u00e5tt p\u00e5 prestanda \u00f6ver alla m\u00f6jliga klassificeringsgr\u00e4nser.\n\n*\"AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\" - Machine Learning Crash Course Google*","216a4768":"Ber\u00e4kna AUC Score","75730e57":"**H\u00e4mtar tr\u00e4nings, validerings och test dataset (CSV filer) **","5413d61b":"# Resultat\n\nResults \u2013 What answer was found to the research question; what did the study find? Was the tested hypothesis true?\n\n**Resultat f\u00f6rsta, enkla, modell**\n\nEpoch 5\/5\n\n12000\/12000 [==============================] - 189s 16ms\/step - loss: 0.0015 - accuracy: 0.9999\n\nAUC: 0.89\n\nVi kan se att v\u00e5r modell n\u00e5r en noggrannhet p\u00e5 1 vilket sj\u00e4lvklart tyder p\u00e5 att vi \u00f6vertr\u00e4nar extremt mycket. Detta visar att vi b\u00f6r kanske anv\u00e4nda en mer avancerad modell och t.ex. anv\u00e4nda dropout f\u00f6r att n\u00e5 b\u00e4ttre resultat. \u00c4ven fast modellen \u00e4r simpel och \u00f6vertr\u00e4nar s\u00e5 n\u00e5ddes fortfarande ett AUC score p\u00e5 0.82 utan mycket anstr\u00e4ngning.\n\n**Resultat andra, mer komplexa, modell**\n\nEpoch 5\/5\n12000\/12000 [==============================] - 801s 67ms\/step - loss: 0.1069 - accuracy: 0.9589\n\nAUC: 0.97\n\nDenna modell presterar b\u00e4ttre j\u00e4mf\u00f6rt med den tidigare. Tittar vi p\u00e5 v\u00e5r f\u00f6rlust och noggrannhet f\u00f6r b\u00e5da modellerna s\u00e5 ser vi en stor skillnad i resultat.\n\nUrsprunglig modell:\n\nloss: 0.0015 accuracy: 0.9999\n\nNy modell:\n\nloss: 0.1069 accuracy: 0.9589\n\nResultaten fr\u00e5n den f\u00f6rsta modellen \u00e4r icke-trov\u00e4rdigt och visar tecken p\u00e5 \u00f6vertr\u00e4ning, om ens noggrannhet ligger p\u00e5 1 s\u00e5 \u00e4r det n\u00e5got som inte st\u00e4mmer helt enkelt.\n\nI v\u00e5r andra modell s\u00e5 ser resultaten mycket mer trov\u00e4rdiga ut med en f\u00f6rlust p\u00e5 0.1069 och en noggrannhet p\u00e5 0.9589. Samt s\u00e5 n\u00e5dde vi en AUC p\u00e5 0.97 vilket \u00e4r betydligt b\u00e4ttre \u00e4n v\u00e5r f\u00f6rsta modell som till och med var extremt \u00f6vertr\u00e4nad. Detta syftar p\u00e5 att v\u00e5r nya modell har presterat bra f\u00f6r v\u00e5rt problem.\n","2088bcc2":"V\u00e5rt tr\u00e4nings set delar upp hatfulla kommentarer i olika klasser:\n\nsevere toxic\n\nobscene\n\nthreat\n\ninsult\n\nidentity hate\n\nF\u00f6r att g\u00f6ra det l\u00e4ttare att tackla detta problem s\u00e5 kommer jag att ta bort dessa 5 klassificeringar och ist\u00e4llet tackla problemet som ett bin\u00e4rt klassificerings problem - d.v.s. s\u00e5 \u00e4r antingen en kommentar hatfull eller inte. Samt s\u00e5 kommer vi att g\u00f6ra v\u00e5r tr\u00e4ning p\u00e5 en delm\u00e4ng av v\u00e5rt dataset f\u00f6r att g\u00f6ra det enklare att tr\u00e4na modellen.","cb2d9d38":"**Data unders\u00f6kning och visualisering**\n\nDet \u00e4r viktigt att vi unders\u00f6ker v\u00e5r data s\u00e5 att vi enklare kan arbeta med den och n\u00e5 en s\u00e5 bra l\u00f6sning som m\u00f6jligt. Enligt sj\u00e4lva t\u00e4vlingens instruktioner s\u00e5 \u00e4r den prim\u00e4ra datan f\u00f6r t\u00e4vlingen, i varje tillg\u00e4nglig fil, sj\u00e4lva comment_text kolumnen. Denna kolumn inneh\u00e5ller texten av en kommentar som har blivit klassificerad som 'toxic' eller 'non-toxic' (0 eller 1 i toxic kolumnen). V\u00e5rt tr\u00e4ning set's kommentarer \u00e4r enbart p\u00e5 engelska och kommer antingen fr\u00e5n 'Civil' kommentarer eller Wikipedia talk page edits. V\u00e5rt test set's kommentarer \u00e4r dock inte i engelska och best\u00e5r ist\u00e4llet av ett flertal andra spr\u00e5k.","bbe5c228":"Wordcloud \u00e4r en funktion som g\u00f6r det m\u00f6jligt att se vilka ord som \u00e4r mest frekventa i en datam\u00e4ngd av texter, vilket \u00e4r perfekt att anv\u00e4nda i v\u00e5r sits d\u00e4r vi unders\u00f6ker hatfulla ord bland en st\u00f6rre m\u00e4ngd icke-hatfulla ord.\n\nTittar vi v\u00e5r Wordcloud nedan ser vi att de mest frekventa orden (st\u00f6rre) \u00e4r icke-hatfulla med ord som *article, page, wikipedia* och att det \u00e4r sv\u00e5rare att hitta hatfulla ord bland dessa. Detta \u00f6verensst\u00e4mmer med v\u00e5r tidigare framtagna statistik om uppdelningen av hatfulla och icke-hatfulla kommentarer i v\u00e5r tr\u00e4ningsdata.","5bc38ee5":"V\u00e5rt testdata visar h\u00e4r kommentarer med flera olika spr\u00e5k.","b3aafc62":"Tokenization\n\nI en RNN s\u00e5 sker v\u00e5r input av en mening ord f\u00f6r ord, keras tokenizer tar alla unika ord och formar i princip en ordbok med ord och dess antal f\u00f6rekomster som v\u00e4rden. Sedan s\u00e5 sorteras ordboken i fallande ordningsf\u00f6ljd och tilldelar det f\u00f6rsta v\u00e4rdet 1, andra v\u00e4rdet 2 och s\u00e5 vidare. Till exempel, vi s\u00e4ger att av alla kommentarer s\u00e5 anv\u00e4ndes ordet \"article\" mest, den hade d\u00e5 f\u00e5tt index 1 och vektorn som representerar order \"article\" hade varit en s\u00e5 kallad \"one-hot\" vektor med v\u00e4rdet 1 p\u00e5 position 1, resterande v\u00e4rden 0.","5b309b16":"D\u00e5 det \u00e4r sv\u00e5rt att hitta hatfulla kommentarer bland de icke-hatfulla i en Wordcloud som inte separerar bland de tv\u00e5 s\u00e5 g\u00f6r vi nu tv\u00e5 stycken nya Wordcloud's.\n\nV\u00e5ra f\u00f6rsta Wordcloud visar nu endast ord som \u00e4r frekventa i icke-hatfulla kommentarer, dessa ord k\u00e4nner vi igen fr\u00e5n f\u00f6rra Wordclouden.\n\nV\u00e5ran andra Worldcloud visar d\u00e4rmed endast frekventa ord i hatfulla kommentarer.\n\nDet blir tydligt att kommentarer som har klassats som icke-hatfulla inte inneh\u00e5ller n\u00e5gon form av sv\u00e4rord eller liknande medans kommentarer som har klassats som hatfulla inneh\u00e5ller en m\u00e4ngd olika ord som kan klassas som hatfulla. T.ex. 'fuck', 'ni**er', 'dickhead' och 'cunt'.","61b3f3bb":"Det kan vara bra om vi ser om det finns null v\u00e4rden i v\u00e5r testdata. Om det finns null v\u00e4rden s\u00e5 b\u00f6r vi ers\u00e4tta dom innan vi g\u00e5r vidare - om vi l\u00e4mnar null v\u00e4rdena som dom \u00e4r s\u00e5 kan det skapa problem f\u00f6r oss i senare stadier.","5e956e13":"Vi kikar p\u00e5 v\u00e5rt tr\u00e4nings dataset och dess format.","071280d7":"Det verkar som att vi inte beh\u00f6ver ta itu med n\u00e5gra null v\u00e4rden, vilket \u00e4r positivt.","adf5210e":"D\u00e5 kommentarerna \u00e4r texter i olika storlekar s\u00e5 \u00e4r jag nyfiken om hur mycket l\u00e4ngden p\u00e5 kommentarerna faktiskt varierar i v\u00e5r data.","d4b5e4a7":"Mer komplex modell\n\nF\u00f6rst s\u00e5 ber\u00e4knar vi v\u00e5r embedding matris f\u00f6r v\u00e5r ordupps\u00e4ttning fr\u00e5n den pre-trained GloVe vektorn. Sedan, medans vi bygger v\u00e5rt embedding lager s\u00e5 passerar vi embedding matrisen som weights till lagret ist\u00e4llet f\u00f6r att tr\u00e4na den \u00f6ver ordupps\u00e4ttningen; d\u00e4rf\u00f6r s\u00e5 s\u00e4tter vi trainable = False. Resterande delar av modellen \u00e4r likande bortsett fr\u00e5n att vi nu an\u00e4vnder 3 stycken LSTM layers med 100 units ist\u00e4llet f\u00f6r en enstaka SimpleRNN. Jag har valt i b\u00e5da modellerna att anv\u00e4nda mig av binary crossentropy som f\u00f6rlustfunktion, detta \u00e4r f\u00f6r att det vi f\u00f6rs\u00f6ker l\u00f6sa \u00e4r, om vi f\u00f6renklar det, bin\u00e4rt. Med det menar jag att antingen s\u00e5 \u00e4r en kommentar toxic (1) eller non-toxic (0). F\u00f6rlustfunktionen binary crossentropy k\u00e4ndes d\u00e5 l\u00e4mplig d\u00e5 den anv\u00e4nds f\u00f6r ja\/nej klassificeringar.","605709d5":"F\u00f6rbereder v\u00e5r data"}}