{"cell_type":{"418fd445":"code","c44e3d36":"code","cf277535":"code","866e0153":"code","d8c62799":"code","3dc7f2f7":"code","c6bc4484":"code","7350a775":"code","8752bc6c":"code","79ddc9be":"code","e27653f5":"code","6500e0d4":"code","96e571db":"code","34eac2f8":"code","8d2a21d1":"code","ebd89c3b":"code","297be897":"code","bde57fad":"code","c7157e38":"code","beed4ab4":"code","5ba9594c":"code","5bd1d59d":"code","8cedb566":"code","9b96d8e5":"code","8d6558a0":"code","5ea030b7":"code","a8e91ead":"code","cb11ef1f":"code","9a22351d":"code","a5fee155":"code","cec3c998":"code","804892e6":"code","6005a9be":"code","b6ca8151":"code","3a81291f":"code","6b504c10":"code","80271997":"code","cc434284":"code","8e782caf":"code","b9ff6ae0":"code","4905af49":"code","100190cb":"code","0ae5e115":"code","4853ec86":"code","5efe3d3b":"code","8aa90fe4":"code","31b90597":"code","5d1aac13":"code","20b0ccd7":"markdown","f4c39e56":"markdown","0bb29de8":"markdown","c792f4a0":"markdown","0991997f":"markdown","b11f9bc2":"markdown","50e7dab3":"markdown","43b4638d":"markdown","0241aa2f":"markdown","22b2b567":"markdown","55ce5519":"markdown"},"source":{"418fd445":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c44e3d36":"from bs4 import BeautifulSoup\nfrom collections import Counter,defaultdict\nfrom gensim.models import Word2Vec,KeyedVectors\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport re\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,KFold,StratifiedShuffleSplit,cross_val_predict\nfrom lightgbm import LGBMClassifier as lg\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nimport string\nimport math\nimport transformers\nfrom unidecode import unidecode\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n%matplotlib inline","cf277535":"train_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train.json\").reset_index(drop=True)\ntest_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_test.json\").reset_index(drop=True)\ntrain_extra_df= pd.read_json(\"..\/input\/github-bugs-prediction\/embold_train_extra.json\").reset_index(drop=True)","866e0153":"def fx(x):\n    return x['title'] + \" \" + x['body']   \ntrain_df['text']= train_df.apply(lambda x : fx(x),axis=1)\ntest_df['text']= test_df.apply(lambda x : fx(x),axis=1)","d8c62799":"cList = {\n            \"i'm\": \"i am\",\n            \"you're\": \"you are\",\n            \"it's\": \"it is\",\n            \"we're\": \"we are\",\n            \"we'll\": \"we will\",\n            \"That's\":\"that is\",\n            \"haven't\":\"have not\",\n            \"let's\":\"let us\",\n            \"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n            \"aren't\": \"are not \/ am not\",\n            \"can't\": \"cannot\",\n            \"can't've\": \"cannot have\",\n            \"'cause\": \"because\",\n            \"could've\": \"could have\",\n            \"couldn't\": \"could not\",\n            \"couldn't've\": \"could not have\",\n            \"didn't\": \"did not\",\n            \"doesn't\": \"does not\",\n            \"don't\": \"do not\",\n            \"hadn't\": \"had not\",\n            \"hadn't've\": \"had not have\",\n            \"hasn't\": \"has not\",\n            \"haven't\": \"have not\",\n            \"he'd\": \"he had \/ he would\",\n            \"he'd've\": \"he would have\",\n            \"he'll\": \"he shall \/ he will\",\n            \"he'll've\": \"he shall have \/ he will have\",\n            \"he's\": \"he has \/ he is\",\n            \"how'd\": \"how did\",\n            \"how'd'y\": \"how do you\",\n            \"how'll\": \"how will\",\n            \"how's\": \"how has \/ how is \/ how does\",\n            \"I'd\": \"I had \/ I would\",\n            \"I'd've\": \"I would have\",\n            \"I'll\": \"I shall \/ I will\",\n            \"I'll've\": \"I shall have \/ I will have\",\n            \"I'm\": \"I am\",\n            \"I've\": \"I have\",\n            \"isn't\": \"is not\",\n            \"it'd\": \"it had \/ it would\",\n            \"it'd've\": \"it would have\",\n            \"it'll\": \"it shall \/ it will\",\n            \"it'll've\": \"it shall have \/ it will have\",\n            \"it's\": \"it has \/ it is\",\n            \"let's\": \"let us\",\n            \"ma'am\": \"madam\",\n            \"mayn't\": \"may not\",\n            \"might've\": \"might have\",\n            \"mightn't\": \"might not\",\n            \"mightn't've\": \"might not have\",\n            \"must've\": \"must have\",\n            \"mustn't\": \"must not\",\n            \"mustn't've\": \"must not have\",\n            \"needn't\": \"need not\",\n            \"needn't've\": \"need not have\",\n            \"o'clock\": \"of the clock\",\n            \"oughtn't\": \"ought not\",\n            \"oughtn't've\": \"ought not have\",\n            \"shan't\": \"shall not\",\n            \"sha'n't\": \"shall not\",\n            \"shan't've\": \"shall not have\",\n            \"she'd\": \"she had \/ she would\",\n            \"she'd've\": \"she would have\",\n            \"she'll\": \"she shall \/ she will\",\n            \"she'll've\": \"she shall have \/ she will have\",\n            \"she's\": \"she has \/ she is\",\n            \"should've\": \"should have\",\n            \"shouldn't\": \"should not\",\n            \"shouldn't've\": \"should not have\",\n            \"so've\": \"so have\",\n            \"so's\": \"so as \/ so is\",\n            \"that'd\": \"that would \/ that had\",\n            \"that'd've\": \"that would have\",\n            \"that's\": \"that has \/ that is\",\n            \"there'd\": \"there had \/ there would\",\n            \"there'd've\": \"there would have\",\n            \"there's\": \"there has \/ there is\",\n            \"they'd\": \"they had \/ they would\",\n            \"they'd've\": \"they would have\",\n            \"they'll\": \"they shall \/ they will\",\n            \"they'll've\": \"they shall have \/ they will have\",\n            \"they're\": \"they are\",\n            \"they've\": \"they have\",\n            \"to've\": \"to have\",\n            \"wasn't\": \"was not\",\n            \"we'd\": \"we had \/ we would\",\n            \"we'd've\": \"we would have\",\n            \"we'll\": \"we will\",\n            \"we'll've\": \"we will have\",\n            \"we're\": \"we are\",\n            \"we've\": \"we have\",\n            \"weren't\": \"were not\",\n            \"what'll\": \"what shall \/ what will\",\n            \"what'll've\": \"what shall have \/ what will have\",\n            \"what're\": \"what are\",\n            \"what's\": \"what has \/ what is\",\n            \"what've\": \"what have\",\n            \"when's\": \"when has \/ when is\",\n            \"when've\": \"when have\",\n            \"where'd\": \"where did\",\n            \"where's\": \"where has \/ where is\",\n            \"where've\": \"where have\",\n            \"who'll\": \"who shall \/ who will\",\n            \"who'll've\": \"who shall have \/ who will have\",\n            \"who's\": \"who has \/ who is\",\n            \"who've\": \"who have\",\n            \"why's\": \"why has \/ why is\",\n            \"why've\": \"why have\",\n            \"will've\": \"will have\",\n            \"won't\": \"will not\",\n            \"won't've\": \"will not have\",\n            \"would've\": \"would have\",\n            \"wouldn't\": \"would not\",\n            \"wouldn't've\": \"would not have\",\n            \"y'all\": \"you all\",\n            \"y'all'd\": \"you all would\",\n            \"y'all'd've\": \"you all would have\",\n            \"y'all're\": \"you all are\",\n            \"y'all've\": \"you all have\",\n            \"you'd\": \"you had \/ you would\",\n            \"you'd've\": \"you would have\",\n            \"you'll\": \"you shall \/ you will\",\n            \"you'll've\": \"you shall have \/ you will have\",\n            \"you're\": \"you are\",\n            \"you've\": \"you have\"\n           }","3dc7f2f7":"c_re = re.compile('(%s)' % '|'.join(cList.keys()))\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)","c6bc4484":"def remove_emoji(string):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', string) ","7350a775":"def remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data","8752bc6c":"def removeSpecialChars(data):\n    '''\n    Removes special characters which are specifically found in tweets.\n    '''\n    #Converts HTML tags to the characters they represent\n    soup = BeautifulSoup(data, \"html.parser\")\n    data = soup.get_text()\n\n    #Convert www.* or https?:\/\/* to empty strings\n    data = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','',data)\n\n    #Convert @username to empty strings\n    data = re.sub('@[^\\s]+','',data)\n    \n    #remove org.apache. like texts\n    data =  re.sub('(\\w+\\.){2,}','',data)\n\n    #Remove additional white spaces\n    data = re.sub('[\\s]+', ' ', data)\n    \n    data = re.sub('\\.(?!$)', '', data)\n\n    #Replace #word with word\n    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n\n    return data ","79ddc9be":"def remove_nonenglish_charac(string):\n    return re.sub('\\W+','', string )","e27653f5":"extra_punctuations = ['','.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '\u2013','&']\nstopword_list = stopwords.words('english') + list(string.punctuation)+ extra_punctuations + ['u','the','us','say','that','he','me','she','get','rt','it','mt','via','not','and','let','so','say','dont','use','you']","6500e0d4":"def clean_text(data):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer() \n    tokenizer=TweetTokenizer()\n    data = unidecode(data)\n    data = expandContractions(data)\n    tokens = tokenizer.tokenize(data)\n    data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n    data = re.sub('\\b\\w{,2}\\b', '', data)\n    data = re.sub(' +', ' ', data)\n    data = removeSpecialChars(data)\n    data = remove_emoji(data)\n    data= [stemmer.stem(w) for w in data.split()]\n    return ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])","96e571db":"train_df['text'] = train_df['text'].apply(lambda x: clean_text(x))","34eac2f8":"def Split_Data(df,col_name):\n    #Split the dataset into training and testing sets\n    df_y=df['label']\n    return train_test_split(df[col_name],df_y,test_size=0.3,random_state=42)","8d2a21d1":"def tokenize_data_with_padding(train_x,val_x):\n    tokenizer=Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_x))\n    train_x=tokenizer.texts_to_sequences(train_x)\n    val_x=tokenizer.texts_to_sequences(val_x)\n    train_x=pad_sequences(train_x,maxlen=maxlen)\n    val_x=pad_sequences(val_x,maxlen=maxlen)\n    print(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\n    print(\"Target training Values Shape\".format(),train_y.shape)\n    print(\"Padded and Tokenized Validation Sequence\".format(),val_x.shape)\n    print(\"Target validation Values Shape\".format(),val_y.shape)\n    return train_x,val_x,tokenizer","ebd89c3b":"train_x,val_x,train_y,val_y=Split_Data(train_df,'text')","297be897":"maxlen=1000\nmax_features=5000 \nembed_size=300","bde57fad":"train_x, val_x, tokenized_data = tokenize_data_with_padding(train_x,val_x)","c7157e38":"def del_obj(*objs):\n    for obj in objs:\n        del obj\n        gc.collect()","beed4ab4":"def get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')","5ba9594c":"def create_Embedding_matrix(EMBEDDING_FILE,tokenizer):\n    # Create the dictionary of pretrained embedding\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n    \n    # prepared multidimentional dictionary\n    embeds = np.stack(embeddings_index.values())\n    emb_mean,emb_std = embeds.mean(), embeds.std()\n    embed_size = embeds.shape[1]\n    \n    # prepare a gausian distribution\n    word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","5bd1d59d":"!pip install MiniAttention","8cedb566":"import MiniAttention.MiniAttention as ma","9b96d8e5":"def seq2seq_encoder_decoder_glove_bilstm_hybrid_attention(model,maxlen,max_features,embed_size,embedding_matrix,emb_name):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=Bidirectional(model,merge_mode=\"sum\")\n    encoder_outputs,encoder_flstm_h,encoder_flstm_c,encoder_blstm_h,encoder_blstm_c = encoder_lstm_cell(encoder_embed_attention)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_flstm_h + encoder_blstm_h,encoder_flstm_c + encoder_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell = model\n    decoder_outputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell = Dense(16,activation='relu')\n    decoder_d_output = decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2 = Dense(3,activation='softmax')\n    decoder_output = decoder_dense_cell2(decoder_d_output)\n    model = Model([encoder_inp,decoder_inp],decoder_output)         \n    model.summary()\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    plot_model(\n        model,to_file=emb_name,\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96)\n    model.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)","8d6558a0":"golve_emd_file = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\ngolve_emd_matrix = create_Embedding_matrix(golve_emd_file,tokenized_data)","5ea030b7":"seq2seq_encoder_decoder_glove_bilstm_hybrid_attention(LSTM(60,return_state=True),maxlen,max_features,len(golve_emd_matrix[0]),golve_emd_matrix,\"seq2seq_encoder_decoder_model_glovetext.png\")","a8e91ead":"class Bahadanu_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Bahadanu_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        score=self.Wv(tf.nn.tanh(self.Wq(q)+self.Wk(v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n        context_vector=(attention_wts * v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n        return context_vector,attention_wts","cb11ef1f":"class Luong_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Luong_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        score=(q)*(v)\n        attention_wts=tf.nn.softmax(score,axis=1)\n        context_vector=(attention_wts*v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n        return context_vector,attention_wts\n    ","9a22351d":"class Graves_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Graves_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        score=tf.math.cos((q)*(v))\n        attention_wts=tf.nn.softmax(score,axis=1)\n        context_vector=(attention_wts * v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n        return context_vector,attention_wts","a5fee155":"def encoder_decoder_with_attention(sample_attn,maxlen,max_features,embed_size,embedding_matrix,img_name):\n    \n    encoder_inp = Input(shape=(maxlen,))\n    encoder_embed = Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_gru_cell = GRU(60,return_state=True)\n    encoder_outputs,encoder_state_flstm_h =encoder_gru_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h]\n    \n    #Creating GRU decoder model and feeding the output states of gru encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    attention=sample_attn\n    \n    context_vector,attention_weights = attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_gru_cell = GRU(60,return_state=True)\n    decoder_outputs,decoder_state_flstm_h= decoder_gru_cell(decoder_embed,initial_state=attention_weights)\n    decoder_dense_cell= Dense(64,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(3,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    plot_model(\n        model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_bahdanau_attention.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96)\n\n    model.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)","cec3c998":"encoder_decoder_with_attention(Bahadanu_Attention(60),maxlen,max_features,len(golve_emd_matrix[0]),golve_emd_matrix,\"Seq2Seq_bahdanau_attention.png\")","804892e6":"encoder_decoder_with_attention(Luong_Attention(128),maxlen,max_features,len(golve_emd_matrix[0]),golve_emd_matrix,\"Seq2Seq_luong_attention.png\")","6005a9be":"encoder_decoder_with_attention(Graves_Attention(128),maxlen,max_features,len(golve_emd_matrix[0]),golve_emd_matrix,\"Seq2Seq_luong_attention.png\")","b6ca8151":"class Scaled_Dot_Product_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Attention,self).__init__()\n        self.units = units\n        self.Wq = tf.keras.layers.Dense(self.units)\n        self.Wk = tf.keras.layers.Dense(self.units)\n        self.Wv = tf.keras.layers.Dense(60)\n    def call(self,q,v,n):\n        score=(q* v)\/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n        context_vector=(attention_wts*v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n        return context_vector,attention_wts\n        ","3a81291f":"def encoder_decoder_with_scaled_attention(sample_attn,maxlen,max_features,embed_size,embedding_matrix,img_name):\n    \n    encoder_inp = Input(shape=(maxlen,))\n    encoder_embed = Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_gru_cell = GRU(60,return_state=True)\n    encoder_outputs,encoder_state_flstm_h =encoder_gru_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h]\n    \n    #Creating GRU decoder model and feeding the output states of gru encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    attention=sample_attn\n    context_vector,attention_weights = attention(encoder_state_flstm_h,encoder_outputs,64)\n    decoder_gru_cell = GRU(60,return_state=True)\n    decoder_outputs,decoder_state_flstm_h= decoder_gru_cell(decoder_embed,initial_state=attention_weights)\n    decoder_dense_cell= Dense(64,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(3,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    plot_model(\n        model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_bahdanau_attention.png\",\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96)\n\n    model.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)","6b504c10":"encoder_decoder_with_scaled_attention(Scaled_Dot_Product_Attention(128),maxlen,max_features,len(golve_emd_matrix[0]),golve_emd_matrix,\"Seq2Seq_luong_attention.png\")","80271997":"class  Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        score=(self.Wq(q)*self.Wk(k))\/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n        context_vector=(attention_wts*v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n        return context_vector,attention_wts","cc434284":"def chunks(l,n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","8e782caf":"def encoder_decoder_with_scaled_attention(maxlen,max_features,embed_size,embedding_matrix,img_name):\n    \n    encoder_inp = Input(shape=(maxlen,))\n    encoder_embed = Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_gru_cell = GRU(60,return_state=True)\n    encoder_outputs,encoder_state_flstm_h =encoder_gru_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h]\n    \n    #Creating GRU decoder model and feeding the output states of gru encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    attention=Scaled_Dot_Product_Self_Attention(60)\n    context_vector,attention_weights = attention(encoder_state_flstm_h,encoder_outputs,64,64)\n    decoder_gru_cell = GRU(60,return_state=True)\n    decoder_outputs,decoder_state_flstm_h= decoder_gru_cell(decoder_embed,initial_state=[attention_weights])\n    decoder_dense_cell= Dense(64,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(3,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    plot_model(\n        model,to_file=img_name,\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96)\n\n    model.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)","b9ff6ae0":"encoder_decoder_with_scaled_attention(maxlen,max_features,len(golve_emd_matrix[0]),golve_emd_matrix,\"Seq2seq_Scaled_Dot_Product_Self_Attention.png\")","4905af49":"del_obj(train_x,val_x,train_y,val_y,golve_emd_matrix) ","100190cb":"import transformers\nfrom transformers import AutoTokenizer,AutoModelForQuestionAnswering","0ae5e115":"dist_train_df=train_df[:20000]\ntrain_x,val_x,train_y,val_y=Split_Data(dist_train_df,'text')\ntrain_x, val_x, tokenized_data = tokenize_data_with_padding(train_x,val_x)","4853ec86":"def Get_transformer_embedding(string_list,pretrained_model, batch_size=64):\n    \n    # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","5efe3d3b":"distilbert_embeddings_multilingual = Get_transformer_embedding(dist_train_df.text.values,'distilbert-base-multilingual-cased')","8aa90fe4":"ditbert_embed_size=768","31b90597":"def distilbert_encoder_decoder_with_attention(maxlen,embedding_matrix,img_name):\n    \n    encoder_inp = Input(shape=(maxlen,))\n    encoder_embed = Embedding(embedding_matrix.shape[0],ditbert_embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_gru_cell = GRU(60,return_state=True)\n    encoder_outputs,encoder_state_flstm_h =encoder_gru_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h]\n    \n    #Creating GRU decoder model and feeding the output states of gru encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(embedding_matrix.shape[0],ditbert_embed_size,weights=[embedding_matrix])(decoder_inp)\n    attention=Scaled_Dot_Product_Self_Attention(60)\n    context_vector,attention_weights = attention(encoder_state_flstm_h,encoder_outputs,64,64)\n    decoder_gru_cell = GRU(60,return_state=True)\n    decoder_outputs,decoder_state_flstm_h= decoder_gru_cell(decoder_embed,initial_state=[attention_weights])\n    decoder_dense_cell= Dense(64,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(3,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    plot_model(\n        model,to_file=img_name,\n        show_shapes=True,\n        show_layer_names=True,\n        rankdir=\"TB\",\n        expand_nested=False,\n        dpi=96)\n\n    model.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)","5d1aac13":"distilbert_encoder_decoder_with_attention(maxlen,distilbert_embeddings_multilingual,\"distilbert_encoder_decoder_Self_Attention.png\")","20b0ccd7":"## Local Attentions:\n* Local attention only focuses on a small subset of source positions per target words unlike the entire source sequence as in global attention\n* Computationally less expensive than global attention\n* The local attention model first generates an aligned position P\u209c for each target word at time t.\n* The context vector \ud835\udcb8\u209c is derived as a weighted average over the set of source hidden states within selected the window\n* The aligned position can be monotonically or predictively selected <br>\n**Formulas**\n![image.png](attachment:image.png)","f4c39e56":"## Sed2Seq model with pretrained embedding matrix\n- Here I am applying pretrained static embedding (like Glove-embedding) to the seq2seq encoder decode model comprising of LSTM model.","0bb29de8":"## Sequence to Sequence Model\n- Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n- A sequence to sequence model aims to map a fixed-length input with a fixed-length output where the length of the input and output may differ.\n- For example, translating \u201cWhat are you doing today?\u201d from English to Chinese has input of 5 words and output of 7 symbols (\u4eca\u5929\u4f60\u5728\u505a\u4ec0\u9ebc\uff1f). Clearly, we can\u2019t use a regular LSTM network to map each word from the English sentence to the Chinese sentence.\n- In Sequence to Sequence models without attention, we process and predict the sentence sequentially\n![image.png](attachment:image.png)\n### Encoder\n- A stack of several recurrent units (LSTM or GRU cells for better performance) where each accepts a single element of the input sequence, collects information for that element and propagates it forward.\n- In question-answering problem, the input sequence is a collection of all words from the question. Each word is represented as x_i where i is the order of that word.\n### Encoder Vector\n- This is the final hidden state produced from the encoder part of the model. It is calculated using the formula above.\n- This vector aims to encapsulate the information for all input elements in order to help the decoder make accurate predictions.\n- It acts as the initial hidden state of the decoder part of the model.\n### Decoder\n- A stack of several recurrent units where each predicts an output y_t at a time step t.\n- Each recurrent unit accepts a hidden state from the previous unit and produces and output as well as its own hidden state.\n- In the question-answering problem, the output sequence is a collection of all words from the answer. Each word is represented as y_i where i is the order of that word.\n*************************************************************************************************************************************************************************\n- We calculate the outputs using the hidden state at the current time step together with the respective weight W(S). Softmax is used to create a probability vector which will help us determine the final output ","c792f4a0":"## Multiplicative Attention\n![image.png](attachment:image.png)\n- Luong, et al., 2015 proposed the \u201cglobal\u201d and \u201clocal\u201d attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n-The commonality between Global and Local attention\n\n* At each time step t, in the decoding phase, both approaches, global and local attention, first take the hidden state h\u209c at the top layer of a stacking LSTM as an input.\n* The goal of both approaches is to derive a context vector \ud835\udcb8\u209c to capture relevant source-side information to help predict the current target word y\u209c\n* Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions.\n* Global and local attention models differ in how the context vector \ud835\udcb8\u209c is derived\n* Before we discuss the global and local attention, let\u2019s understand the conventions used by Luong\u2019s attention mechanism for any given time t\n    * \ud835\udcb8\u209c : context vector\n    * a\u209c : alignment vector\n    * h\u209c : current target hidden state\n    * h\u209b : current source hidden state\n    * y\u209c: predicted current target word\n    * h\u02dc\u209c : Attentional vectors\n* The global attentional model considers all the hidden states of the encoder when calculating the context vector \ud835\udcb8\u209c.\n* A variable-length alignment vector a\u209c equal to the size of the number of time steps in the source sequence is derived by comparing the current target hidden state h\u209c with each of the source hidden state h\u209b\n* The alignment score is referred to as a content-based function for which we consider three different alternatives","0991997f":"## Graves Cosine Attention\nHere we apply ,cosine transformation on the Dot product Attention.\n","b11f9bc2":"## Hybrid Encoder Decoder With Attention\n- This section will comprise of Hybrid Encoder Decoder Architectures with variants of Attention Mechanisms. For an introduction attention refers to allowing certain neural weights to be focussed during training and this in turn assists in model performance.\n- In Sequence to Sequence models without attention, we process and predict the sentence sequentially. However, it is possible and highly probable that the prediction of a word from one language to another in NMT may depend on words before or after that specific word in the sentence.\n- It uses stacked recurrent neural networks on word level followed by attention model to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Then the same procedure applied to the derived sentence vectors which then generate a vector who conceives the meaning of the given document and that vector can be passed further for text classification. It uses stacked recurrent neural networks on word level followed by attention model to extract such words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector. Then the same procedure applied to the derived sentence vectors which then generate a vector who conceives the meaning of the given document and that vector can be passed further for text classification.\n![image.png](attachment:image.png)","50e7dab3":"- That return sequences return the hidden state output for each input time step.\n- That return state returns the hidden state output and cell state for the last input time step.\n- That return sequences and return state can be used at the same time.","43b4638d":"## Bahdanau Attention\n- Bahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n\n- let\u2019s understand the Attention mechanism suggested by Bahdanau\n\n- All hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\n- The attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\n- The model predicts a target word based on the context vectors associated with the source position and the previously generated target words.\n#### Alignment Score\n- The alignment score maps how well the inputs around position \u201cj\u201d and the output at position \u201ci\u201d match. The score is based on the previous decoder\u2019s hidden state, s\u208d\u1d62\u208b\u2081\u208e just before predicting the target word and the hidden state, h\u2c7c of the input sentence.\n- The decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector. The alignment vector that has the same length with the source sequence and is computed at every time step of the decode.\n##### Attention Weights\n- We apply a softmax activation function to the alignment scores to obtain the attention weights.\n- Equations of bahadanau attention\n![image.png](attachment:image.png)","0241aa2f":"![image.png](attachment:image.png)\n","22b2b567":"## Basic seq2seq model without pretrained embedding","55ce5519":"![image.png](attachment:image.png)"}}