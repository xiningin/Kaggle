{"cell_type":{"bc076e38":"code","80303a96":"code","79bd89f1":"code","fe823edb":"code","7a31ef49":"code","3c8488dd":"code","e813fb47":"code","c44edb20":"code","ede9ec33":"code","a86cd311":"code","fe33ccc3":"code","bd2f1cfe":"code","9cf1a20f":"code","7393c976":"code","fadad6ed":"code","208ed6dd":"code","61ea71ce":"code","feef2018":"code","651029e9":"code","ebdb2d83":"code","e0ff4947":"code","bf6b16ab":"code","1888f153":"code","cf79691f":"code","92918ce1":"code","10948307":"code","a43471e2":"code","62d4f4f8":"code","a0f0f589":"code","63f41ff4":"code","3b88e0d8":"code","991fd739":"code","15ae23b7":"code","745886a6":"code","1286b26c":"code","82d366af":"code","f28d9d7c":"code","4e92d3af":"code","43067710":"code","bd56ff4a":"code","44364d1e":"code","6905f4a4":"code","01de5330":"code","10365a98":"code","a1e671c0":"code","0fdd6fe4":"code","6527e23b":"code","0f055ebd":"code","88432f6c":"code","de0a1508":"code","288ed935":"code","51ab8fb9":"code","7e7ffce8":"code","3279e3da":"code","3d0d7441":"code","5bcbd3c2":"code","d6bc61b2":"code","2337c7d7":"markdown","7d6df46a":"markdown","eeb7638f":"markdown","fccabc38":"markdown","806380cc":"markdown","a294246c":"markdown","c56bd50d":"markdown","98689861":"markdown","de6251d7":"markdown","2d270598":"markdown","beb1f61c":"markdown","c4c24920":"markdown","057361c0":"markdown","7ed6daad":"markdown","a9a85d2d":"markdown"},"source":{"bc076e38":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport tensorflow as tf\nimport sklearn\n\nimport codecs\nfrom tqdm import tqdm","80303a96":"raw = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None)","79bd89f1":"raw.head()","fe823edb":"def stratified_sample_df(df, col, n_samples):\n    n = min(n_samples, df[col].value_counts().min())\n    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n    df_.index = df_.index.droplevel(0)\n    return df_","7a31ef49":"df = stratified_sample_df(raw, 0, 100000).reset_index(drop=True)\ndf","3c8488dd":"df.rename(columns = {0:'target', 5:'text'}, inplace = True)","e813fb47":"df = df.drop([1, 2, 3, 4], axis=1)","c44edb20":"df.head()","ede9ec33":"df_preprocess = df.copy()","a86cd311":"tweets = df_preprocess.text.copy()","fe33ccc3":"tweets","bd2f1cfe":"def removeWordWithChar(text, char_list):\n    #Remove words in a text that contains a char from the list.\n    text = text.split()\n    res = [ele for ele in text if all(ch not in ele for ch in char_list)]\n    res = ' '.join(res)\n    return res\n\nchar_list = ['@', '#', 'http', 'www', '\/']\n\nremoveWordWithChar(tweets[1], char_list)","9cf1a20f":"tweets_cleaned = []\nfor t in tweets:\n    tweets_cleaned.append(removeWordWithChar(t, char_list))","7393c976":"tweets_cleaned[0]","fadad6ed":"len(tweets_cleaned)","208ed6dd":"def tokenize(texts):\n    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n\n    texts_tokens = []\n    for i, val in enumerate(texts):\n        text_tokens = tokenizer.tokenize(val.lower())\n\n        for i in range(len(text_tokens) - 1, -1, -1):\n            if len(text_tokens[i]) < 4:\n                del(text_tokens[i])\n\n        texts_tokens.append(text_tokens)\n        \n    return texts_tokens","61ea71ce":"tweets_tokens = tokenize(tweets_cleaned)\n\ntweets_tokens[:1]","feef2018":"def removeSW(texts_tokens):\n    stopWords = set(stopwords.words('english'))\n    texts_filtered = []\n\n    for i, val in enumerate(texts_tokens):\n        text_filtered = []\n        for w in val:\n            if w not in stopWords:\n                text_filtered.append(w)\n        texts_filtered.append(text_filtered)\n        \n    return texts_filtered","651029e9":"tweets_filtered = removeSW(tweets_tokens)\n\ntweets_filtered[:1]","ebdb2d83":"def lemma(texts_filtered):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    texts_lem = []\n\n    for i, val in enumerate(texts_filtered):\n        text_lem = []\n        for word in val:\n            text_lem.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n        texts_lem.append(text_lem)\n    \n    return texts_lem","e0ff4947":"nltk.download('wordnet')\n\ntweets_lem = lemma(tweets_filtered)\n\ntweets_lem[:1]","bf6b16ab":"tweets_ready = []\nfor tweet in tweets_lem:\n    string = ' '\n    string = string.join(tweet)\n    tweets_ready.append(string)","1888f153":"len(tweets_ready)","cf79691f":"df_preprocess['tweet'] = tweets_ready\ndf_preprocess['target'] = df.target.replace(4, 1)","92918ce1":"df_preprocess","10948307":"GLOVE_EMB = '..\/input\/glove42b300dtxt\/glove.42B.300d.txt'\nEMBEDDING_DIM = 300","a43471e2":"%%time\nembeddings_index = {}\nf = codecs.open(GLOVE_EMB, encoding='utf-8')\nfor line in tqdm(f):\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","62d4f4f8":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_preprocess.tweet)\n\nword_index = tokenizer.word_index\nvocabulary_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size :\", vocabulary_size)","a0f0f589":"embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","63f41ff4":"embedding_matrix.shape","3b88e0d8":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, SpatialDropout1D, GlobalMaxPooling1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","991fd739":"BATCH_SIZE = 1024\nEPOCHS = 30","15ae23b7":"X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df_preprocess['tweet'], df_preprocess['target'], test_size=0.2, random_state=123)\nprint(\"Train Data size:\", len(X_train))\nprint(\"Test Data size\", len(X_test))","745886a6":"MAX_SEQUENCE_LENGTH = 30","1286b26c":"from keras.preprocessing.sequence import pad_sequences\n\nX_train = pad_sequences(tokenizer.texts_to_sequences(X_train),\n                        maxlen = MAX_SEQUENCE_LENGTH)\nX_test = pad_sequences(tokenizer.texts_to_sequences(X_test),\n                       maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\",X_train.shape)\nprint(\"Testing X Shape:\",X_test.shape)","82d366af":"embedding_matrix.shape","f28d9d7c":"embedding_layer = tf.keras.layers.Embedding(vocabulary_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","4e92d3af":"es=EarlyStopping(monitor='val_loss',\n                 mode='min',\n                 verbose=1,\n                 patience=5)\n\nreduce_lr = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","43067710":"inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n\nx = embedding_layer(inputs)\nx = Dropout(0.5)(x)\n\n# Conv1D x2 + global max pooling\nx = Conv1D(128, 7, activation=\"relu\")(x)\nx = Conv1D(128, 7, activation=\"relu\")(x)\nx = GlobalMaxPooling1D()(x)\n\n# Vanilla hidden layer:\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\n\noutputs = Dense(1, activation=\"sigmoid\")(x)\n\nmodel_A = tf.keras.Model(inputs, outputs)","bd56ff4a":"model_A.summary()","44364d1e":"model_A.compile(optimizer=Adam(), loss='binary_crossentropy',\n              metrics=['accuracy'])","6905f4a4":"%%time\nhistory_A = model_A.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_test, y_test), callbacks=[es, reduce_lr])","01de5330":"plt.plot(history_A.history['accuracy'])\nplt.plot(history_A.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.title('CNN with word embedding glove')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')","10365a98":"inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n\nx = embedding_layer(inputs)\nx = SpatialDropout1D(0.2)(x)\n\n# Conv1D + LSTM (bidirectional)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n\n# Vanilla hidden layer:\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\n\noutputs = Dense(1, activation=\"sigmoid\")(x)\n\nmodel_B = tf.keras.Model(inputs, outputs)","a1e671c0":"model_B.summary()","0fdd6fe4":"model_B.compile(optimizer=Adam(), loss='binary_crossentropy',\n              metrics=['accuracy'])","6527e23b":"%%time\nhistory_B = model_B.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])","0f055ebd":"plt.plot(history_B.history['accuracy'])\nplt.plot(history_B.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.title('RNN with word embedding glove')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')","88432f6c":"import codecs\nfrom tqdm import tqdm\nembeddings_index = {}\nf = codecs.open('..\/input\/fasttext\/wiki.simple.vec', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('found %s word vectors' % len(embeddings_index))","de0a1508":"embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","288ed935":"embedding_matrix.shape","51ab8fb9":"embedding_layer = tf.keras.layers.Embedding(vocabulary_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","7e7ffce8":"inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n\nx = embedding_layer(inputs)\nx = SpatialDropout1D(0.2)(x)\n\n# Conv1D + LSTM (bidirectional)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n\n# Vanilla hidden layer:\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\n\noutputs = Dense(1, activation=\"sigmoid\")(x)\n\nmodel_B = tf.keras.Model(inputs, outputs)","3279e3da":"model_B.compile(optimizer=Adam(), loss='binary_crossentropy',\n              metrics=['accuracy'])","3d0d7441":"history_B = model_B.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(X_test, y_test), callbacks=[es, reduce_lr])","5bcbd3c2":"plt.plot(history_B.history['accuracy'])\nplt.plot(history_B.history['val_accuracy'])\nplt.ylabel('Accuracy')\nplt.title('RNN with word embedding fasttext')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')","d6bc61b2":"1+1","2337c7d7":"### Stopwords","7d6df46a":"## Prepare training","eeb7638f":"### Callbacks","fccabc38":"### Create train \/ test sets","806380cc":"## Word embedding : Glove","a294246c":"## Model A : CNN","c56bd50d":"### Lemma","98689861":"## Cleaning dataset","de6251d7":"### Embedding layer","2d270598":"## Model B : RNN","beb1f61c":"### Tokenize","c4c24920":"## Pre-proccessing text","057361c0":"### Remove hashtags, @ and url","7ed6daad":" ## Another Word Embedding : FastText","a9a85d2d":"## Sampling raw dataset"}}