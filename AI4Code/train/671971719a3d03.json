{"cell_type":{"e43065d9":"code","08610258":"code","9f0cd757":"code","1cc77f58":"code","71cab606":"code","3068180d":"code","4045eafa":"code","226072bd":"code","fd88b2fc":"code","35a69f57":"code","4078677d":"markdown","3dc892e1":"markdown","29528833":"markdown","da5bc84d":"markdown","d3485637":"markdown","114f5d6c":"markdown","f6c3da3a":"markdown","ce5de601":"markdown","60570820":"markdown"},"source":{"e43065d9":"from keras.models import Sequential\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.layers.merge import Concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nimport cv2\nimport os\nimport numpy as np\nfrom numpy import genfromtxt\nimport pandas as pd\nimport tensorflow as tf\nfrom utils import LRN2D\nimport utils\n\n%load_ext autoreload\n%autoreload 2\n\nnp.set_printoptions(threshold=np.nan)","08610258":"myInput = Input(shape=(96, 96, 3))\n\nx = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\nx = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\nx = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\nx = Activation('relu')(x)\nx = ZeroPadding2D(padding=(1, 1))(x)\nx = MaxPooling2D(pool_size=3, strides=2)(x)\nx = Lambda(LRN2D, name='lrn_1')(x)\nx = Conv2D(64, (1, 1), name='conv2')(x)\nx = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\nx = Activation('relu')(x)\nx = ZeroPadding2D(padding=(1, 1))(x)\nx = Conv2D(192, (3, 3), name='conv3')(x)\nx = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\nx = Activation('relu')(x)\nx = Lambda(LRN2D, name='lrn_2')(x)\nx = ZeroPadding2D(padding=(1, 1))(x)\nx = MaxPooling2D(pool_size=3, strides=2)(x)\n\n# Inception3a\ninception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\ninception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\ninception_3a_3x3 = Activation('relu')(inception_3a_3x3)\ninception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\ninception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\ninception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\ninception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n\ninception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\ninception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\ninception_3a_5x5 = Activation('relu')(inception_3a_5x5)\ninception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\ninception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\ninception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\ninception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n\ninception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\ninception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\ninception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\ninception_3a_pool = Activation('relu')(inception_3a_pool)\ninception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n\ninception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\ninception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\ninception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n\ninception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n\n# Inception3b\ninception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\ninception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\ninception_3b_3x3 = Activation('relu')(inception_3b_3x3)\ninception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\ninception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\ninception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\ninception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n\ninception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\ninception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\ninception_3b_5x5 = Activation('relu')(inception_3b_5x5)\ninception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\ninception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\ninception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\ninception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n\ninception_3b_pool = Lambda(lambda x: x**2, name='power2_3b')(inception_3a)\ninception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\ninception_3b_pool = Lambda(lambda x: x*9, name='mult9_3b')(inception_3b_pool)\ninception_3b_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_3b')(inception_3b_pool)\ninception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\ninception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\ninception_3b_pool = Activation('relu')(inception_3b_pool)\ninception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n\ninception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\ninception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\ninception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n\ninception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n\n# Inception3c\ninception_3c_3x3 = utils.conv2d_bn(inception_3b,\n                                   layer='inception_3c_3x3',\n                                   cv1_out=128,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=256,\n                                   cv2_filter=(3, 3),\n                                   cv2_strides=(2, 2),\n                                   padding=(1, 1))\n\ninception_3c_5x5 = utils.conv2d_bn(inception_3b,\n                                   layer='inception_3c_5x5',\n                                   cv1_out=32,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=64,\n                                   cv2_filter=(5, 5),\n                                   cv2_strides=(2, 2),\n                                   padding=(2, 2))\n\ninception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\ninception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n\ninception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n\n#inception 4a\ninception_4a_3x3 = utils.conv2d_bn(inception_3c,\n                                   layer='inception_4a_3x3',\n                                   cv1_out=96,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=192,\n                                   cv2_filter=(3, 3),\n                                   cv2_strides=(1, 1),\n                                   padding=(1, 1))\ninception_4a_5x5 = utils.conv2d_bn(inception_3c,\n                                   layer='inception_4a_5x5',\n                                   cv1_out=32,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=64,\n                                   cv2_filter=(5, 5),\n                                   cv2_strides=(1, 1),\n                                   padding=(2, 2))\n\ninception_4a_pool = Lambda(lambda x: x**2, name='power2_4a')(inception_3c)\ninception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\ninception_4a_pool = Lambda(lambda x: x*9, name='mult9_4a')(inception_4a_pool)\ninception_4a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_4a')(inception_4a_pool)\ninception_4a_pool = utils.conv2d_bn(inception_4a_pool,\n                                   layer='inception_4a_pool',\n                                   cv1_out=128,\n                                   cv1_filter=(1, 1),\n                                   padding=(2, 2))\ninception_4a_1x1 = utils.conv2d_bn(inception_3c,\n                                   layer='inception_4a_1x1',\n                                   cv1_out=256,\n                                   cv1_filter=(1, 1))\ninception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n\n#inception4e\ninception_4e_3x3 = utils.conv2d_bn(inception_4a,\n                                   layer='inception_4e_3x3',\n                                   cv1_out=160,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=256,\n                                   cv2_filter=(3, 3),\n                                   cv2_strides=(2, 2),\n                                   padding=(1, 1))\ninception_4e_5x5 = utils.conv2d_bn(inception_4a,\n                                   layer='inception_4e_5x5',\n                                   cv1_out=64,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=128,\n                                   cv2_filter=(5, 5),\n                                   cv2_strides=(2, 2),\n                                   padding=(2, 2))\ninception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\ninception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n\ninception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n\n#inception5a\ninception_5a_3x3 = utils.conv2d_bn(inception_4e,\n                                   layer='inception_5a_3x3',\n                                   cv1_out=96,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=384,\n                                   cv2_filter=(3, 3),\n                                   cv2_strides=(1, 1),\n                                   padding=(1, 1))\n\ninception_5a_pool = Lambda(lambda x: x**2, name='power2_5a')(inception_4e)\ninception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\ninception_5a_pool = Lambda(lambda x: x*9, name='mult9_5a')(inception_5a_pool)\ninception_5a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_5a')(inception_5a_pool)\ninception_5a_pool = utils.conv2d_bn(inception_5a_pool,\n                                   layer='inception_5a_pool',\n                                   cv1_out=96,\n                                   cv1_filter=(1, 1),\n                                   padding=(1, 1))\ninception_5a_1x1 = utils.conv2d_bn(inception_4e,\n                                   layer='inception_5a_1x1',\n                                   cv1_out=256,\n                                   cv1_filter=(1, 1))\n\ninception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n\n#inception_5b\ninception_5b_3x3 = utils.conv2d_bn(inception_5a,\n                                   layer='inception_5b_3x3',\n                                   cv1_out=96,\n                                   cv1_filter=(1, 1),\n                                   cv2_out=384,\n                                   cv2_filter=(3, 3),\n                                   cv2_strides=(1, 1),\n                                   padding=(1, 1))\ninception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\ninception_5b_pool = utils.conv2d_bn(inception_5b_pool,\n                                   layer='inception_5b_pool',\n                                   cv1_out=96,\n                                   cv1_filter=(1, 1))\ninception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n\ninception_5b_1x1 = utils.conv2d_bn(inception_5a,\n                                   layer='inception_5b_1x1',\n                                   cv1_out=256,\n                                   cv1_filter=(1, 1))\ninception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n\nav_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\nreshape_layer = Flatten()(av_pool)\ndense_layer = Dense(128, name='dense_layer')(reshape_layer)\nnorm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n\n\n# Final Model\nmodel = Model(inputs=[myInput], outputs=norm_layer)","9f0cd757":"# Load weights from csv files (which was exported from Openface torch model)\nweights = utils.weights\nweights_dict = utils.load_weights()\n\n# Set layer weights of the model\nfor name in weights:\n  if model.get_layer(name) != None:\n    model.get_layer(name).set_weights(weights_dict[name])\n  elif model.get_layer(name) != None:\n    model.get_layer(name).set_weights(weights_dict[name])","1cc77f58":"def image_to_embedding(image, model):\n    #image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n    image = cv2.resize(image, (96, 96)) \n    img = image[...,::-1]\n    img = np.around(np.transpose(img, (0,1,2))\/255.0, decimals=12)\n    x_train = np.array([img])\n    embedding = model.predict_on_batch(x_train)\n    return embedding\n","71cab606":"def recognize_face(face_image, input_embeddings, model):\n\n    embedding = image_to_embedding(face_image, model)\n    \n    minimum_distance = 200\n    name = None\n    \n    # Loop over  names and encodings.\n    for (input_name, input_embedding) in input_embeddings.items():\n        \n       \n        euclidean_distance = np.linalg.norm(embedding-input_embedding)\n        \n\n        print('Euclidean distance from %s is %s' %(input_name.split('_')[0], euclidean_distance))\n\n        \n        if euclidean_distance < minimum_distance:\n            minimum_distance = euclidean_distance\n            name = input_name\n    \n    if minimum_distance < 0.8:\n        return str(name)\n    else:\n        return None","3068180d":"import glob\n\ndef create_input_image_embeddings():\n    input_embeddings = {}\n\n    for file in glob.glob(\"images\/*\"):\n        person_name = os.path.splitext(os.path.basename(file))[0]\n        image_file = cv2.imread(file, 1)\n        input_embeddings[person_name] = image_to_embedding(image_file, model)\n\n    return input_embeddings\n\ndef recognize_faces_in_cam(input_embeddings):\n    \n\n    cv2.namedWindow(\"Face Recognizer\")\n    vc = cv2.VideoCapture(0)\n   \n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n    \n    \n    while vc.isOpened():\n        _, frame = vc.read()\n        img = frame\n        height, width, channels = frame.shape\n\n        \n        \n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n\n        # Loop through all the faces detected \n        identities = []\n        for (x, y, w, h) in faces:\n            x1 = x\n            y1 = y\n            x2 = x+w\n            y2 = y+h\n\n           \n            \n            face_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]    \n            identity = recognize_face(face_image, input_embeddings, model)\n            \n            \n\n            if identity is not None:\n                img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,255,255),2)\n                cv2.putText(img, str(identity.split('_')[0]), (x1+5,y1-5), font, 1, (255,255,255), 2)\n        \n        key = cv2.waitKey(100)\n        cv2.imshow(\"Face Recognizer\", img)\n\n        if key == 27: # exit on ESC\n            break\n    vc.release()\n    cv2.destroyAllWindows()","4045eafa":"cam = cv2.VideoCapture(0)\n\nface_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\ncount = 0\nwhile(True):\n    ret, img = cam.read()\n    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_detector.detectMultiScale(img, 1.3, 5)\n    for (x,y,w,h) in faces:\n        x1 = x\n        y1 = y\n        x2 = x+w\n        y2 = y+h\n        cv2.rectangle(img, (x1,y1), (x2,y2), (255,255,255), 2)     \n        count += 1\n        # Save the captured image into the datasets folder\n        cv2.imwrite(\"images\/Anurag_\" + str(count) + \".jpg\", img[y1:y2,x1:x2])\n        cv2.imshow('image', img)\n    k = cv2.waitKey(1000) & 0xff # Press 'ESC' for exiting video\n    if k == 27:\n        break\n    elif count >= 10: # Take 30 face sample and stop video\n         break\ncam.release()\ncv2.destroyAllWindows()","226072bd":"input_embeddings = create_input_image_embeddings()\nrecognize_faces_in_cam(input_embeddings)\n","fd88b2fc":"for file in glob.glob(\"images\/Train\/*\"):\n    person_name = os.path.splitext(os.path.basename(file))[0]\n    print(person_name)\n    image_file = cv2.imread(file, 1)","35a69f57":"os.path.splitext('Anurag')[0]","4078677d":"## Capturing the face image\nFollowing code captures 10 face images of the person. They all are stored in **\"images\"** folder with the name User_1 to User_10. Select a good captured image from the set of 10 images. Rename it with the name of person and delete rest of them. This image will be used for recognizing the the identity of the person using one shot learning.","3dc892e1":"# Import the prerequisite libraries\n\nWe will be importing utils.py from https:\/\/github.com\/iwantooxxoox\/Keras-OpenFace\/blob\/master\/utils.py (available with code) which contains utility functions to create the neural network and load the weights assoiated with it.","29528833":"# Loading the model with pretrained weights\n\nFaceNet is trained by minimizing the triplet loss. I will be  loading a previously trained model. weights are avaiable at https:\/\/github.com\/iwantooxxoox\/Keras-OpenFace in the \"weights\" folder which is also provided in this source.\n\nThis can take a couple of minutes to execute and depends on the speed of your machine.\n","da5bc84d":"# Contructing the neural network model\nThe model here constructed is based on FaceNet's Inception model.\n\nThe implementation of model is available at: https:\/\/github.com\/iwantooxxoox\/Keras-OpenFace","d3485637":"## About <font color=blue>create_input_image_embeddings<\/font> function\nThis function generates 128 dimensional image ebeddings of all the images stored in the \"images\" directory by feed forwarding the images to a trained neural network. It creates a dictionary with key as the name of the face and value as embedding\n\n\n## About <font color=blue>recognize_faces_in_cam<\/font> function\nThis function capture image from the webcam, detect a face in it and crop the image to have a face only, which is then passed to recognize_face function. ","114f5d6c":"## About <font color=blue>image_to_embedding<\/font> function        \nWhen the model is loaded with pre trained weights, then we can create the **128 dimensional embedding vectors** for all the face images stored in the \"images\" folder. **\"image_to_embedding\"** function pass an image to the Inception network to generate the embedding vector.","f6c3da3a":"# Lets run the face recognizer program :-)","ce5de601":"## About <font color=blue>recognize_face<\/font> function\nThis function calculate similarity between the captured image and the images that are already been stored. It passes the image to the trained neural network to generate its embedding vector. Which is then compared with all the embedding vectors of the images stored by calculating L2 Euclidean distance. \n\nIf the minimum L2 distance between two embeddings is less than a threshpld (here I have taken the threashhold as .68 (which can be adjusted) then we have a match.","60570820":"# Basic face recognizer using pre trained model\n\nBelow the is the implementation of a very basic face recognizer which can identify the face of the person showing on a web cam.\nThe implementation is inspired by two path breaking papers on facial recognition using deep convoluted neural network, namely FaceNet and DeepFace.\n\nI have used pre trained model Keras-OpenFace which is an open source Keras implementation of the OpenFace (Originally Torch implemented) \n\nThe pretrained model that I have used is by Victor Sy Wang's implementation and was loaded using his code: https:\/\/github.com\/iwantooxxoox\/Keras-OpenFace."}}