{"cell_type":{"f0b59283":"code","92f386fd":"code","88b717cc":"code","105702d8":"code","0e734c3f":"code","e3109831":"code","53af39b8":"code","4254c2b4":"code","7ea63fde":"code","c8a5c432":"code","6f5269f7":"code","e7253dcb":"code","0fa5e377":"code","faab7135":"code","3e374a91":"code","c174ab43":"code","9dde412f":"code","89655664":"code","05e3c0df":"code","03fe8091":"code","46783c99":"code","97cd4251":"code","e9d979f4":"code","0b5565b3":"code","9041b44b":"code","f93bb4dd":"code","ec0db8b6":"code","42678ea8":"code","6556d661":"code","b4fb94df":"code","70b7174e":"code","efcc5b76":"code","427023fe":"code","9e465bc6":"code","9568e57e":"code","a2c95927":"code","d9fc4f98":"code","c01f54eb":"code","027d6096":"code","36d547e8":"code","c59744b8":"code","c275e71c":"code","5deaa866":"code","30c1c0b5":"code","6ca33ea7":"code","8d471e38":"code","5f1be2c8":"code","2bce3834":"code","3dcd2197":"code","3b407465":"code","728a4f5c":"code","54a80a5c":"code","69495f56":"code","72faa547":"code","e6695413":"code","d9d95253":"code","7493fe71":"code","e51e7ba0":"code","08b37410":"code","ce046d5b":"code","206eea09":"code","921a2053":"code","4781a91a":"code","1a6bda6a":"code","ef38de99":"code","ff3bcb74":"code","9d5abbe6":"code","56a22293":"code","10a9ed9b":"code","fe7dddf3":"code","deed5d0e":"code","80913354":"code","c144bde9":"code","c5121082":"code","a6068f08":"code","c7fe9bf8":"code","2d3405dd":"code","60a89c32":"code","940b8a76":"code","d3c0eadd":"code","924832a5":"code","774a0dcd":"code","3d4f5c41":"code","01f5f9a3":"code","16f710cb":"code","eb24dbcf":"code","4d4ec214":"code","e048bd9d":"code","86f34ff2":"code","39440020":"code","68b704c1":"code","1de68f07":"code","43e2a270":"code","b866745a":"code","7c35a08f":"code","d281f597":"code","f71e0732":"code","7777f1e1":"code","787bfb28":"code","283a4004":"code","16e8c546":"code","9f5303cc":"code","2a5a37ba":"code","a9365e0b":"code","26539bac":"code","abf1db1f":"code","6f524851":"code","9292c1e8":"code","cbc343b1":"code","0ba1a3e3":"code","eb05de96":"code","6abe16ab":"code","88b0c470":"code","42036349":"code","d1e47a4e":"code","e676befb":"code","fa412d69":"code","f359eafa":"code","7c91c1b5":"code","a222f8c1":"code","9bbb4888":"code","276f4cf3":"code","a91fa25e":"code","be6c1926":"code","827e9710":"code","fa88cfaf":"code","eaa7546a":"code","735dd7b4":"code","a7aba75e":"code","36552e36":"code","9da3e6a2":"code","2edde443":"code","743ff1e5":"code","189435c8":"code","86733e96":"code","0ce718d0":"code","e8ca20a0":"code","3260a2d9":"code","c120788f":"code","d83c4f98":"code","1e6f4000":"code","cabef830":"code","a6982d62":"code","a1d780c2":"code","e6c9dcd4":"code","f839350d":"code","58dd54de":"code","28fb7015":"code","648463f6":"code","6db594df":"code","2bd316a0":"code","4c340979":"code","b2c52115":"code","eacdf48b":"code","1735542c":"code","82bfc611":"code","347d3162":"code","8226172c":"code","b7142fa7":"code","9c5dcdb9":"markdown","de81e653":"markdown","a311d482":"markdown","6d20eee8":"markdown","10d90379":"markdown","921f981d":"markdown","738001e7":"markdown","96aa4151":"markdown","13895ae0":"markdown","615bdf4d":"markdown","38efcc1a":"markdown","1ea16b15":"markdown","0a804576":"markdown","9d0085dc":"markdown","35b30df4":"markdown","da3d5e59":"markdown","4adad3d5":"markdown","85d85331":"markdown","a9a30e85":"markdown","e75f8896":"markdown","50621421":"markdown","52f4452e":"markdown","a278a11c":"markdown","ede89d86":"markdown","640297f6":"markdown","937322f7":"markdown","04c5f3da":"markdown","a090b4ca":"markdown","8a348bd9":"markdown","09cbd9c3":"markdown","028d1f82":"markdown","e5f369dc":"markdown","962ee4b9":"markdown","5c14bb43":"markdown","5ca2aa49":"markdown","a253d4c2":"markdown","36db500d":"markdown","c47dcb8d":"markdown","3542add8":"markdown","8c249e5b":"markdown","7bf702f0":"markdown","efeed884":"markdown","6e3bbdf4":"markdown","ca2e724d":"markdown","2a7db7fb":"markdown","e3e08bc6":"markdown","19a61bdf":"markdown","ecf9d782":"markdown","34015539":"markdown","262df135":"markdown","8dd1e598":"markdown","8bc84722":"markdown","1c132f72":"markdown","fa43a2ec":"markdown","02e45bce":"markdown","3e8c0c4e":"markdown","cbacfd7f":"markdown","2b6cfd1a":"markdown","cacf2285":"markdown","fce4bdbb":"markdown","85fa2b1f":"markdown","f63173a5":"markdown","b95a6bc6":"markdown","a9eb43ad":"markdown","30b65ffd":"markdown","5a34401e":"markdown","14efd630":"markdown","4bfd23d8":"markdown","4cd9e4df":"markdown","27437412":"markdown","e0fd3c32":"markdown","ae67f3da":"markdown","163054ae":"markdown","8ebfafe3":"markdown","8d10b6c6":"markdown","6469fc6a":"markdown","76624100":"markdown","186e37a2":"markdown","73d1cf5f":"markdown","bf5516f4":"markdown","1471f12f":"markdown","35fc7689":"markdown","94b0baf6":"markdown","89e1e963":"markdown","d8468d22":"markdown","1d76dc84":"markdown","06a4d239":"markdown","25a16df3":"markdown","b4324254":"markdown","55cda054":"markdown","f9d61af5":"markdown","79b4efe1":"markdown","4272972d":"markdown","7006b695":"markdown","6a9083d3":"markdown","b4ff8a9a":"markdown","f0080067":"markdown","dd21cf82":"markdown","21d5a44c":"markdown","ab7440c7":"markdown","a529f56c":"markdown","deef9a35":"markdown","a75ec42a":"markdown","fdc36609":"markdown","c74988a8":"markdown","5146b797":"markdown","a2b4eeb5":"markdown","063daea6":"markdown","341493ee":"markdown","d916aef6":"markdown","ceae3342":"markdown","043b2c50":"markdown","f7c42f20":"markdown","5957ea95":"markdown","715a5a70":"markdown","36fdc43f":"markdown","c38dd2ef":"markdown","09f168a3":"markdown","22a0b8d4":"markdown","9c8a42f9":"markdown","513cb55a":"markdown","972b4ae1":"markdown","832200f1":"markdown","61b116c5":"markdown","40988345":"markdown","33b1040c":"markdown","b02e06a6":"markdown","14e007f9":"markdown","3d82af0c":"markdown","8e32de74":"markdown","3b223cfd":"markdown","5e7da99a":"markdown"},"source":{"f0b59283":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","92f386fd":"# Importing Pandas and NumPy\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 50000)","88b717cc":"import matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(color_codes=True)","105702d8":"# Importing the dataset\n# Please note that the csv file needs to be in the same directory as the python file\nleads = pd.read_csv(\"..\/input\/Leads.csv\")\nleads.head()","0e734c3f":"# Let's check the dimensions of the dataframe\nleads.shape","e3109831":"# let's look at the statistical aspects of the dataframe\nleads.describe()","53af39b8":"# Let's see the type of each column\nleads.info()","4254c2b4":"#Checking for any duplicates in the data frame\nleads.loc[leads.duplicated()]","7ea63fde":"leads.head()","c8a5c432":"leads.drop(['Magazine', 'Receive More Updates About Our Courses', 'Update me on Supply Chain Content'\n           ,'Get updates on DM Content', 'I agree to pay the amount through cheque'],axis=1,inplace=True)","6f5269f7":"leads.drop(['Country', 'City'],axis=1,inplace=True)","e7253dcb":"leads.drop(['Lead Number','Prospect ID'],axis=1,inplace=True)","0fa5e377":"# finding the count of Select string in all columns and storing the same in a new df\ndf = leads.eq('Select').sum().to_frame().T\n# creating a new column in the dataframe and storing the count of Select of each column\ndf.loc['Count_of_Select'] = leads.eq('Select').sum()\n# storing the total values of each column in a new column\ndf.loc['Total'] = leads.count()\n# renaming the index name\ndf = df.drop(index={0})\n# finding the percentage of the Select values\ndf.loc['Percent'] = df.loc['Count_of_Select']\/df.loc['Total'] * 100\n# transposing the dataframe for better viewing\ndf = df.T\n# soring the values based on the Percentage\ndf = df.sort_values(by=\"Percent\",ascending = False)\n# removing the unnecessary columns\ndf = df.iloc[:,[0,2]]\n# fining all columns where the Percentage of Select is more than 0\ndf = df.loc[df['Percent'] > 0, :]\n# Printing the df\ndf","faab7135":"# replacing Select with NaN for Specialization column\nleads.Specialization = leads.Specialization.str.strip().replace('Select', np.nan)\n# replacing Select with NaN for How did you hear about X Education column\nleads['How did you hear about X Education'] = leads['How did you hear about X Education'].str.strip().replace('Select', np.nan)\n# replacing Select with NaN for Lead Profile column\nleads['Lead Profile'] = leads['Lead Profile'].str.strip().replace('Select', np.nan)","3e374a91":"leads.head()","c174ab43":"# Checking the percentage of missing values\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","9dde412f":"# removing any column which has more than 50% null values\nleads = leads.loc[:,leads.isnull().sum()\/leads.shape[0]*100<50]\nprint(leads.shape)","89655664":"# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","05e3c0df":"varlist =  ['Asymmetrique Activity Index', 'Asymmetrique Profile Index']\n\ndef new_map(x):\n    return x.map({'01.High': 1, \"02.Medium\": 2, \"03.Low\": 3})\n\nleads[varlist] = leads[varlist].apply(new_map)","03fe8091":"# Creating a new df with these 4 columns\ncorr = leads.loc[:,['Asymmetrique Profile Score', 'Asymmetrique Activity Score', 'Asymmetrique Profile Index','Asymmetrique Activity Index']].corr()\n# Plotting a heat map for these 4 columns\nsns.heatmap(corr, annot = True,cmap=\"Blues\")","46783c99":"# dropping the aforementioned columns\nleads.drop(['Asymmetrique Activity Index', 'Asymmetrique Profile Index'],axis=1,inplace=True)\n# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","97cd4251":"# checking the count of different values within the column\nleads['Asymmetrique Profile Score'].value_counts()","e9d979f4":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['Asymmetrique Profile Score'].unique())\n# Plotting a histogram for visualizing the data\nleads['Asymmetrique Profile Score'].plot.hist(bins = num_unique_values)","0b5565b3":"# checking the mean of the column\nprint(\"Mean is \",leads['Asymmetrique Profile Score'].mean())\n# checking the mode of the column\nprint(\"Mode is \",leads['Asymmetrique Profile Score'].mode())\n# checking the median of the column\nprint(\"Median is \",leads['Asymmetrique Profile Score'].median())","9041b44b":"# imputing the value of median to the null values\nleads.loc[pd.isnull(leads['Asymmetrique Profile Score']),['Asymmetrique Profile Score']]=16","f93bb4dd":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['Asymmetrique Profile Score'].unique())\n# Plotting a histogram for visualizing the data\nleads['Asymmetrique Profile Score'].plot.hist(bins = num_unique_values)","ec0db8b6":"# checking the count of different values within the column\nleads['Asymmetrique Activity Score'].value_counts()","42678ea8":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['Asymmetrique Activity Score'].unique())\n# Plotting a histogram for visualizing the data\nleads['Asymmetrique Activity Score'].plot.hist(bins = num_unique_values)","6556d661":"# checking the mean of the column\nprint(\"Mean is\",leads['Asymmetrique Activity Score'].mean())\n# checking the mode of the column\nprint(\"Mode is\",leads['Asymmetrique Activity Score'].mode())\n# checking the median of the column\nprint(\"Median is\",leads['Asymmetrique Activity Score'].median())","b4fb94df":"# imputing the value of median to the null values\nleads.loc[pd.isnull(leads['Asymmetrique Activity Score']),['Asymmetrique Activity Score']]=14","70b7174e":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['Asymmetrique Activity Score'].unique())\n# Plotting a histogram for visualizing the data\nleads['Asymmetrique Activity Score'].plot.hist(bins = num_unique_values)","efcc5b76":"# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","427023fe":"# checking the count of different values within the column\nleads['Specialization'].value_counts()","9e465bc6":"# plotting a count plot for the column\nsns.countplot(x= 'Specialization', data = leads)\nplt.xticks(rotation=90)","9568e57e":"# Creating a new value Others and replacing it with null values\nleads['Specialization'] = leads['Specialization'].replace(np.nan, 'Others')","a2c95927":"# plotting a count plot for the column\nsns.countplot(x= 'Specialization', data = leads)\nplt.xticks(rotation=90)","d9fc4f98":"# checking the count of different values within the column\nleads['What matters most to you in choosing a course'].value_counts()","c01f54eb":"# plotting a count plot for the column\nsns.countplot(x= 'What matters most to you in choosing a course', data = leads)\nplt.xticks(rotation=90)","027d6096":"# dropping the aforementioned column\nleads.drop(['What matters most to you in choosing a course'],axis=1,inplace=True)\n# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","36d547e8":"# checking the count of different values within the column\nleads['What is your current occupation'].value_counts()","c59744b8":"# plotting a count plot for the column\nsns.countplot(x= 'What is your current occupation', data = leads)\nplt.xticks(rotation=90)","c275e71c":"# imputing the value Unemployed to the null values\nleads.loc[pd.isnull(leads['What is your current occupation']),['What is your current occupation']]='Unemployed'","5deaa866":"# plotting a count plot for the column\nsns.countplot(x= 'What is your current occupation', data = leads)\nplt.xticks(rotation=90)","30c1c0b5":"# checking the count of different values within the column\nleads['TotalVisits'].value_counts().head(10)","6ca33ea7":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['TotalVisits'].unique())\n# Plotting a histogram for visualizing the data\nleads['TotalVisits'].plot.hist(bins = num_unique_values)","8d471e38":"# imputing the value of mode to the null values\nleads.loc[pd.isnull(leads['TotalVisits']),['TotalVisits']]=0.0","5f1be2c8":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['TotalVisits'].unique())\n# Plotting a histogram for visualizing the data\nleads['TotalVisits'].plot.hist(bins = num_unique_values)","2bce3834":"# Checking the percentage of missing values\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","3dcd2197":"# checking the count of different values within the column\nleads['Page Views Per Visit'].value_counts()","3b407465":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['Page Views Per Visit'].unique())\n# Plotting a histogram for visualizing the data\nleads['Page Views Per Visit'].plot.hist(bins = num_unique_values)","728a4f5c":"# imputing the value of mode to the null values\nleads.loc[pd.isnull(leads['Page Views Per Visit']),['Page Views Per Visit']]=0.0","54a80a5c":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(leads['Page Views Per Visit'].unique())\n# Plotting a histogram for visualizing the data\nleads['Page Views Per Visit'].plot.hist(bins = num_unique_values)","69495f56":"# Checking the percentage of missing values\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","72faa547":"# checking the count of different values within the column\nleads['Tags'].value_counts()","e6695413":"# plotting a count plot for the column\nsns.countplot(x= 'Tags', data = leads)\nplt.xticks(rotation=90)","d9d95253":"# imputing the value of mode to the null values\nleads.loc[pd.isnull(leads['Tags']),['Tags']]='Will revert after reading the email'","7493fe71":"# plotting a count plot for the column\nsns.countplot(x= 'Tags', data = leads)\nplt.xticks(rotation=90)","e51e7ba0":"# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","08b37410":"# checking the count of different values within the column\nleads['Last Activity'].value_counts()","ce046d5b":"# plotting a count plot for the column\nsns.countplot(x= 'Last Activity', data = leads)\nplt.xticks(rotation=90)","206eea09":"# imputing the value of mode to the null values\nleads.loc[pd.isnull(leads['Last Activity']),['Last Activity']]='Email Opened'","921a2053":"# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","4781a91a":"# checking the count of different values within the column\nleads['Lead Source'].value_counts()","1a6bda6a":"# plotting a count plot for the column\nsns.countplot(x= 'Lead Source', data = leads)\nplt.xticks(rotation=90)","ef38de99":"# correcting the values\nleads=leads.replace({'Lead Source': {'google': 'Google'}})","ff3bcb74":"# plotting a count plot for the column\nsns.countplot(x= 'Lead Source', data = leads)\nplt.xticks(rotation=90)","9d5abbe6":"# imputing the value of Google in place of null values.\nleads.loc[pd.isnull(leads['Lead Source']),['Lead Source']]='Google'","56a22293":"# Checking the percentage of missing values now\nmissingValPercent = leads.isnull().sum()\/leads.shape[0]*100\nprint(missingValPercent.sort_values(ascending = False))","10a9ed9b":"# Initializing the figure\nfig = plt.figure(figsize = (12,8))\n# prining the boxplot\nsns.boxplot(data=leads)\n# setting the title of the figure\nplt.title(\"PC Distribution\", fontsize = 12)\n# setting the y-label\nplt.ylabel(\"Range\")\n# setting the x-label\nplt.xlabel(\"Columns\")\nplt.xticks(rotation=90)\n\n# printing the plot\nplt.show()","fe7dddf3":"leads.describe(percentiles=[.05,.25, .5, .75, .90, .95, .99])","deed5d0e":"# before we move forward, lets create a copy of the existing df\nleads1=leads.copy()","80913354":"# setting the lower whisker\nQ1 = leads['TotalVisits'].quantile(0.05)\n    # setting the upper whisker\nQ3 = leads['TotalVisits'].quantile(0.95)\n    # setting the IQR by dividing the upper with lower quantile\nIQR = Q3 - Q1\n    # performing the outlier analysis\nleads = leads[(leads['TotalVisits'] >= Q1) & (leads['TotalVisits'] <= Q3)]","c144bde9":"# setting the lower whisker\nQ1 = leads['Page Views Per Visit'].quantile(0.05)\n    # setting the upper whisker\nQ3 = leads['Page Views Per Visit'].quantile(0.95)\n    # setting the IQR by dividing the upper with lower quantile\nIQR = Q3 - Q1\n    # performing the outlier analysis\nleads = leads[(leads['Page Views Per Visit'] >= Q1) & (leads['Page Views Per Visit'] <= Q3)]","c5121082":"# Checking the shape of the df now\nleads.shape","a6068f08":"# checking the different percentiles now\nleads.describe(percentiles=[.05,.25, .5, .75, .90, .95, .99])","c7fe9bf8":"# Initializing the figure\nfig = plt.figure(figsize = (12,8))\n# prining the boxplot\nsns.boxplot(data=leads)\n# setting the title of the figure\nplt.title(\"PC Distribution\", fontsize = 12)\n# setting the y-label\nplt.ylabel(\"Range\")\n# setting the x-label\nplt.xlabel(\"Columns\")\nplt.xticks(rotation=90)\n\n# printing the plot\nplt.show()","2d3405dd":"# Let's look at the scarifice\nprint(\"Shape before outlier treatment: \",leads1.shape)\nprint(\"Shape after outlier treatment: \",leads.shape)\n\nprint(\"Percentage data removal is around {}%\".format(round(100*(leads1.shape[0]-leads.shape[0])\/leads1.shape[0]),2))","60a89c32":"# Finding all the rows where the loan was approved and moving them to a new df\ndf_convert = leads[leads['Converted'] == 1]\n# Finding all the rows where the loan was cancelled and moving them to a new df\ndf_not_convert = leads[leads['Converted'] == 0]","940b8a76":"# Finding the categorical columns and printing the same.\ncategorical = leads.select_dtypes(exclude=['int64','float64'])\ncategCols = categorical.columns\ncategCols","d3c0eadd":"for i,col in enumerate(categCols):\n    plt.figure(1)\n    plt.figure(figsize=(15,5))\n    for j,df in enumerate([df_convert,df_not_convert]):\n        dfTemp = df[col].value_counts()\n        dfTemp = dfTemp.to_frame()\n        dfTemp.index.name=col\n        dfTemp.rename(columns={col:'Count'},inplace=True)\n        plt.subplots_adjust(wspace = 1)\n        plt.subplot(1,2,j+1)\n        plt.xticks(rotation=90)\n        sns.barplot(x= dfTemp.index, y = dfTemp.Count)\n        if j==0:\n            plt.title(col + \" for Customers who converted\")\n        else:\n            plt.title(col + \" for Customers who did not convert\")","924832a5":"# Finding the numrical columns and printing the same.\nnumCols = leads.select_dtypes(include=['int64','float64'])\n# Sorting the columns\nnumCols = numCols[sorted(numCols.columns)]\n# printing the columns\nprint(numCols.columns)","774a0dcd":"# Explicitly controlling the SettingWithCopy settings\npd.set_option('mode.chained_assignment', None)\n# deleting the Converted column since we do not want to print the same with itself\nnumCols = numCols.drop(['Converted'],axis=1)\n\n# running a for loop to print the plots \nfor i,col in enumerate(numCols):\n    plt.figure(1)\n    plt.figure(figsize=(15,5))\n    for j,df in enumerate([df_convert,df_not_convert]):\n        plt.subplots_adjust(wspace = 1)\n        plt.subplot(1,2,j+1)\n        plt.xticks(rotation=90)\n        df[col] = df[col].fillna(0).astype(int)\n        sns.distplot(df[col])\n        if j==0:\n            plt.title(col + \" for Customers who converted\")\n        else:\n            plt.title(col + \" for Customers who did not convert\")\n","3d4f5c41":"sns.countplot(x = \"Lead Origin\", hue = \"Converted\", data = leads)\nplt.xticks(rotation = 90)","01f5f9a3":"# List of variables to map\n\nvarlist =  ['Do Not Email', 'Do Not Call', 'Search', 'Newspaper Article', 'X Education Forums', 'Newspaper'\n           , 'Digital Advertisement', 'Through Recommendations'\n           , 'A free copy of Mastering The Interview']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function\nleads[varlist] = leads[varlist].apply(binary_map)","16f710cb":"leads.head()","eb24dbcf":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy_var = pd.get_dummies(leads[['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation',\n                              'Tags','Last Notable Activity']], drop_first=True)\ndummy_var.head()","4d4ec214":"# Adding the results to the master dataframe\nleads = pd.concat([leads, dummy_var], axis=1)\nleads.head()","e048bd9d":"# dropping the original columns since they are not required\nleads = leads.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','Tags','Last Notable Activity'], axis = 1)","86f34ff2":"# checking the df\nleads.head()","39440020":"leads.shape","68b704c1":"# Normalising continuous features\ndf = leads[['TotalVisits','Total Time Spent on Website','Page Views Per Visit','Asymmetrique Activity Score',\n             'Asymmetrique Profile Score']]\nnormalized_df=(df-df.mean())\/df.std()\nleads = leads.drop(['TotalVisits','Total Time Spent on Website','Page Views Per Visit','Asymmetrique Activity Score',\n             'Asymmetrique Profile Score'], 1)\nleads = pd.concat([leads,normalized_df],axis=1)\nleads.head()","1de68f07":"converted = (sum(leads['Converted'])\/len(leads['Converted'].index))*100\nconverted","43e2a270":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = leads.drop(['Converted'],axis=1)\n\n# Putting response variable to y\ny = leads['Converted']\n\ny.head()","b866745a":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)","7c35a08f":"import statsmodels.api as sm","d281f597":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","f71e0732":"# Importing RFE and LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\n# Running RFE with the output number of the variable equal to 15\nlogreg = LogisticRegression()\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)\nrfe.support_","7777f1e1":"# RFE function will now determine the ranking of all the variables and rank them for our use.\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","787bfb28":"# Here we are taking the top 15 columns which are recommended by the RFE function\ncol = X_train.columns[rfe.support_]\ncol","283a4004":"# Printing the columns which are being discarded from further analysis\nX_train.columns[~rfe.support_]","16e8c546":"X_train1 = X_train[col]\n# Adding a constant variable since the statsmodels library does not come with a constant variable built-in\nX_train_sm1 = sm.add_constant(X_train1)\n# Running the linear model\nlogm2 = sm.GLM(y_train,X_train_sm1, family = sm.families.Binomial())\nres = logm2.fit()\n#Let's see the summary of our linear model\nres.summary()","9f5303cc":"# dropping the column Tags_invalid number\ncol1 = col.drop('Tags_Lateral student',1)","2a5a37ba":"X_train1 = X_train[col1]\n# Adding a constant variable since the statsmodels library does not come with a constant variable built-in\nX_train_sm1 = sm.add_constant(X_train1)\n# Running the linear model\nlogm2 = sm.GLM(y_train,X_train_sm1, family = sm.families.Binomial())\nres = logm2.fit()\n#Let's see the summary of our linear model\nres.summary()","a9365e0b":"# Dropping the column `Last Notable Activity_Had a Phone Conversation`\ncol2 = col1.drop('Last Notable Activity_Had a Phone Conversation',1)","26539bac":"X_train1 = X_train[col2]\n# Adding a constant variable since the statsmodels library does not come with a constant variable built-in\nX_train_sm1 = sm.add_constant(X_train1)\n# Running the linear model\nlogm2 = sm.GLM(y_train,X_train_sm1, family = sm.families.Binomial())\nres = logm2.fit()\n#Let's see the summary of our linear model\nres.summary()","abf1db1f":"# Dropping the column Tags_Lateral student\ncol3 = col2.drop('Lead Source_Welingak Website',1)","6f524851":"X_train1 = X_train[col3]\n# Adding a constant variable since the statsmodels library does not come with a constant variable built-in\nX_train_sm1 = sm.add_constant(X_train1)\n# Running the linear model\nlogm2 = sm.GLM(y_train,X_train_sm1, family = sm.families.Binomial())\nres = logm2.fit()\n#Let's see the summary of our linear model\nres.summary()","9292c1e8":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","cbc343b1":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train1.columns\nvif['VIF'] = [variance_inflation_factor(X_train1.values, i) for i in range(X_train1.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0ba1a3e3":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm1)\ny_train_pred[:10]","eb05de96":"# lets reshape the values.\ny_train_pred = y_train_pred.values.reshape(-1)\n# checking some values\ny_train_pred[:10]","6abe16ab":"# creating a new df with the predicted values.\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\n# adding the custID to the dataframe\ny_train_pred_final['CustID'] = y_train.index\n# checking the new df now\ny_train_pred_final.head()","88b0c470":"# creating the new column\ny_train_pred_final['predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","42036349":"# importing the necessary libraries\nfrom sklearn import metrics\n# Creating the Confusion matrix to calculate the Metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","d1e47a4e":"# Let's check the overall accuracy.\nprint(\"The overall Accuracy score for train set is\",metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","e676befb":"# Pulling out all the necessary values from the confusion matrix\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","fa412d69":"# Let's see the sensitivity of our logistic regression model\nprint(\"The sensitivity for train set is\",round(TP \/ float(TP+FN),4))","f359eafa":"# Let us calculate specificity\nprint(\"The specificity for train set is\",round(TN \/ float(TN+FP),4))","7c91c1b5":"# Calculate false postive rate - predicting converted when customer does not have converted\nprint(\"The false postive rate for train set is\",FP\/ float(TN+FP))\n# Positive predictive value \nprint (\"The Positive predictive value for train set is\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"The Negative predictive value for train set is\",TN \/ float(TN+ FN))","a222f8c1":"from sklearn.metrics import precision_score, recall_score\nprint (\"The Precision Score for train set is\",precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","9bbb4888":"print (\"The Recall Score for train set is\",recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","276f4cf3":"# importing the libraries\nfrom sklearn.metrics import precision_recall_curve","a91fa25e":"# checking the Original and Predicted values\ny_train_pred_final.Converted, y_train_pred_final.predicted","be6c1926":"# settin the precision and recall values along with the threshold\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.predicted)","827e9710":"# printing a graph to check the intersection point.\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","fa88cfaf":"# initializing a function to plot the ROC curve\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","eaa7546a":"# setting the FPR and TPR and the threshold\nfpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","735dd7b4":"# plotting the ROC curve with the Converted and Converted_Prob\ndraw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","a7aba75e":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","36552e36":"# Now let's calculate accuracy, sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","9da3e6a2":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","2edde443":"# creating a new column and multiplying the probabilities with 100 \ny_train_pred_final['Lead_Score'] = y_train_pred_final['Converted_Prob'].apply(lambda x:x*100)","743ff1e5":"# creating a new column based on the above mentioned column using the ROC cutoff\ny_train_pred_final['final_predicted'] = y_train_pred_final.Lead_Score.map( lambda x: 1 if x > 42 else 0)\n\ny_train_pred_final.head()","189435c8":"# Creating the Confusion matrix to calculate the Metrics\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nprint(confusion)","86733e96":"# Let's check the overall accuracy.\nprint(\"The overall Accuracy score for train set is\",metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","0ce718d0":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","e8ca20a0":"# Let's see the sensitivity of our logistic regression model\nprint(\"The sensitivity for train set is\",round(TP \/ float(TP+FN),4))","3260a2d9":"# Let us calculate specificity\nprint(\"The specificity for train set is\",round(TN \/ float(TN+FP),4))","c120788f":"# Calculate false postive rate - predicting converted when customer does not have converted\nprint(\"The false postive rate for train set is\",FP\/ float(TN+FP))\n# Positive predictive value \nprint (\"The Positive predictive value for train set is\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"The Negative predictive value for train set is\",TN \/ float(TN+ FN))","d83c4f98":"from sklearn.metrics import precision_score, recall_score\nprint (\"The Precision Score for train set is\",precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","1e6f4000":"print (\"The Recall Score for train set is\",recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","cabef830":"\"{:2.2f}\".format(metrics.roc_auc_score(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob))","a6982d62":"# using the columns in train dataset for our test set analysis\nX_test = X_test[X_train1.columns]\nX_test.head()","a1d780c2":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","e6c9dcd4":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","f839350d":"# Let's see the head\ny_pred_1.head()","58dd54de":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\n# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final.head()","28fb7015":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Prob'})\n# Let's see the head of y_pred_final\ny_pred_final.head()","648463f6":"# creating a new column and multiplying the probabilities with 100 \ny_pred_final['Lead_Score'] = y_pred_final['Converted_Prob'].apply(lambda x:x*100)","6db594df":"# creating a new column based on the above mentioned column using the ROC cutoff\ny_pred_final['final_predicted'] = y_pred_final.Lead_Score.map(lambda x: 1 if x > 42 else 0)\ny_pred_final.head()","2bd316a0":"# Let's check the overall accuracy.\nprint(\"The overall Accuracy score for test set is\",metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted))","4c340979":"confusion = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion","b2c52115":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","eacdf48b":"# Let's see the sensitivity of our logistic regression model\nprint(\"The sensitivity for test set is\",round(TP \/ float(TP+FN),4))","1735542c":"# Let us calculate specificity\nprint(\"The specificity for test set is\",round(TN \/ float(TN+FP),4))","82bfc611":"# Calculate false postive rate - predicting converted when customer does not have converted\nprint(\"The false postive rate for test set is\",FP\/ float(TN+FP))\n# Positive predictive value \nprint (\"The Positive predictive value for test set is\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"The Negative predictive value for test set is\",TN \/ float(TN+ FN))","347d3162":"from sklearn.metrics import precision_score, recall_score\nprint (\"The Precision Score for test set is\",precision_score(y_pred_final.Converted, y_pred_final.final_predicted))","8226172c":"print (\"The recall Score for test set is\",recall_score(y_pred_final.Converted, y_pred_final.final_predicted))","b7142fa7":"\"{:2.2f}\".format(metrics.roc_auc_score(y_pred_final.Converted, y_pred_final.Converted_Prob))","9c5dcdb9":"#### 2.2.2 Asymmetrique Activity Score column check","de81e653":"#### 11.2 Calculate sensitivity","a311d482":"Now, upon further checking the data, we can see that there are columns having values like ***Select***. We will assume that these values are those where the ```data could not be captured``` and will treat them as <font color='red'>***missing values***<\/font> as well.\n\n- We will have to check what is the percentage of the values of `Select` in each column","6d20eee8":"#### 2.2.6 TotalVisits column check","10d90379":"We have chosen to take mode here as well.","921f981d":"## Problem Statement:\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n\nThe company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n\nNow, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone.\n\nThere are a lot of leads generated in the initial stage (top) but only a few of them come out as paying customers from the bottom. In the middle stage, you need to nurture the potential leads well (i.e. educating the leads about the product, constantly communicating etc. ) in order to get a higher lead conversion.","738001e7":"## Step 4:  EDA","96aa4151":"##### Creating a dataframe with the actual converted flag and the predicted probabilities","13895ae0":"### 2.1 Lets remove some un-important columns","615bdf4d":"### 4.1.1 Visualising Categorical Variables","38efcc1a":"#### 11.4 Calculate FPR, PP and NP","1ea16b15":"#### 9.2 Calculate sensitivity","0a804576":"Lets check the summary again","9d0085dc":"#### 2.2.3 Specialization column check","35b30df4":"The above 3 columns have Select as a value hence we will replace this with NaN values.","da3d5e59":"#### 6.5 Precision and Recall","4adad3d5":"Lets check the columns which have been selected by RFE","85d85331":"#### 9.6 Calculate AUC Score","a9a30e85":"Lets check the distribution again","e75f8896":"#### Inference:\nSince majority of the values are Unemployed and an Other category already exists, let's just update the null values to Unemployed.","50621421":"Lets visualize these numerical columns w.r.t the lead conversion.","52f4452e":"### 5.1 Splitting Data into Training and Test Sets","a278a11c":"### 5.4 Assessing the model with StatsModels","ede89d86":"#### Inference\nFrom the curve above, `0.42` is the optimum point to consider as a cutoff probability.","640297f6":"As we can see from the box plot, there are some outliers in the TotalVisits and Page Views Per Visit columns. Let's remove the outliers from these columns.","937322f7":"### 2.2 Checking for Missing Values and Treating Them","04c5f3da":"## Recommendations\n- For the 2 months during which X Education hires some interns, we can __reduce__ the Lead Score cutoff from 0.42 to some lower value like 0.35 or 0.3. This will increase the number of users which the model will predict as potential conversion users even when their conversion probability is low. \n\n\n- Also, when the target is reached for a quarter and if the company wants to minimize the rate of useless phone calls, can __increase__ the Lead Score cutoff from 0.42 to some higher value like 0.50 or 0.60. So that only high conversion probability people can be called over the calls.","a090b4ca":"### 4.1 Univariate Analysis","8a348bd9":"Lets plot a box plot to check the outliers","09cbd9c3":"Lets check the Distribution now","028d1f82":"#### 2.2.4 What matters most to you in choosing a course column check","e5f369dc":"##### Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0","962ee4b9":"## 2.3 Outlier Analysis","5c14bb43":"### 5.3 Feature Selection Using RFE","5ca2aa49":"### Inference:\nWe are keeping `50%` as the threshold and so all those columns which have missing values more than or equal to 50% will be dropped.","a253d4c2":"#### Model 2:","36db500d":"Before we check these columns, lets fix the data for the column 'Asymmetrique Profile Index' and 'Asymmetrique Activity Index'","c47dcb8d":"### 4.2 Bivariate Analysis","3542add8":"#### 6.1 Calculate Accuracy","8c249e5b":"#### Model 1:","7bf702f0":"## Step 1: Reading and Understanding the Data","efeed884":"Lets check the VIF as well.","6e3bbdf4":"## Business Goal\n1. Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\n2. There are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.","ca2e724d":"#### Inference:\nAs we can see that the column `Last Notable Activity_Had a Phone Conversation` has a p-value of >0.05 hence we will drop it.","2a7db7fb":"#### 11.1 Calculate Accuracy","e3e08bc6":"Now the distribution looks good and we dont see any outliers in the numerical columns any longer.","19a61bdf":"#### 5.4.1 Mixed Approach:","ecf9d782":"Lets Inspect the Dataframe","34015539":"We have almost 38% converted rate.","262df135":"Lets check the summary again","8dd1e598":"### Checking the Converted Rate","8bc84722":"#### 2.2.10 Lead Source column check","1c132f72":"### 5.2 Running Our First Training Model","fa43a2ec":"There are 2 types of outliers and we will treat the outliers since they can skew our dataset.\n\n- Statistical\n- Domain specific","02e45bce":"Lets visualize these categorical columns w.r.t the lead conversion.","3e8c0c4e":"#### 2.2.5 What is your current occupation column check","cbacfd7f":"Lets check the correlation between these columns using a heat map","2b6cfd1a":"#### Inference:\nAs we can see that the column `Tags_Lateral student` has a p-value of >0.05 hence we will drop it.","cacf2285":"#### 11.7 Calculate AUC Score","fce4bdbb":"We have chosen to take mode here as well.","85fa2b1f":"## Result\n#### We have achieved the Lead Conversion Rate (Precision) of our model of about `92%` and AUC score of `0.93` which is good.","f63173a5":"#### 9.5 Precision and Recall","b95a6bc6":"Lets run the logistic regression with all the variables and check the summary of the same.","a9eb43ad":"## Step 2: Data Cleaning","30b65ffd":"#### Inference:\nAs we can see now that all the p-values are below <0.05.","5a34401e":"We have chosen to take mode here as well.","14efd630":"## Step 5: Model Building\nLet's start by splitting our data into a training set and a test set.","4bfd23d8":"#### 2.2.7 Page Views Per Visit column check","4cd9e4df":"There are few columns which have values same across all the data points. These seem to be of no importance then. Therefore dropping them.","27437412":"### Feature Standardisation","e0fd3c32":"#### Model 3:","ae67f3da":"#### Inference:\nAs we can see that the column `Lead Source_Welingak Website` has a p-value of >0.05 hence we will drop it.","163054ae":"### Inference:\n1. Asymmetrique Activity Score is centered around 14 for both categories but there is a spike at score 15 for converted users w.r.t the un-converted users.\n2. Asymmetrique Profile Score has a longer tail for converted users as opposed to on-converted users.\n3. Lead number seems to be same for both categories.\n4. Page Views per visit seems to be similar for both categoires.\n5. Total time spent on website seems to be more for converted users as opposted to un-converted users.\n6. TotalVisits seems to be similar for both categories.","8ebfafe3":"After passing the selected columns by RFE we will manually evaluate each models p-value and VIF value. Unless we find the acceptable range for `p-values (<0.05)` and `VIF (<5)` we will keep dropping the variables one at a time based on below criteria:\n\n- `High p-value High VIF` : Drop the variable\n- `High p-value Low VIF` or `Low p-value High VIF` : Drop the variable with high p-value first\n- `Low p-value Low VIF` : accept the variable","8d10b6c6":"#### Inference:\nAll variables have a good value of VIF(<5). So we need not drop any more variables and we can proceed with making predictions using this model only","6469fc6a":"Lets remove the outliers from the `TotalVisits` column 1st.","76624100":"Lets check the distribution now","186e37a2":"There are no duplicate values in the dataset.","73d1cf5f":"#### 6.3 Calculate specificity","bf5516f4":"### Inference:\nAs all 3 metrics are comparable. But since we want missing value to be imputed with an integer, taking median i.e. 16.","1471f12f":"Since most of the values are 0.0, replacing the nulls will the mode.","35fc7689":"#### 9.4 Calculate FPR, PP and NP","94b0baf6":"Now we dont have any columns having null values.","89e1e963":"As we can see from the graph and table above, there are some outliers in the dataset. Lets treat these outliers. We will keep the lower quantile at 0.05 and higher quantile at 0.95.","d8468d22":"#### Inference:\nThe distribution has not changed post imputation","1d76dc84":"### 4.1.2 Visualising Numerical Variables","06a4d239":"### Inference:\nAs all 3 metrics are nearly 14. Replacing the null values with 14 for 'Asymmetrique Activity Score'.","25a16df3":"### Inference:\n\nAs seen from the graph, 'Asymmetrique Profile Score' is highly negatively correlated with 'Asymmetrique Profile Index' and 'Asymmetrique Activity Score' is highly negatively correlated with 'Asymmetrique Activity Index'. \n\nHence, we take decision to `drop` the two columns - 'Asymmetrique Activity Index' and 'Asymmetrique Profile Index'.","b4324254":"### Inference:\nSince the distribution has not changed much before and after the null value imputation, we should be good here.","55cda054":"### Inference:\nAs we can see that there are too many variables to analyse here hence lets use RFE to select only the important variables which would be used to build our final model.","f9d61af5":"#### Inference:\nAs we can see now that Others Category has the highest number of values post imputation.","79b4efe1":"#### 6.4 Calculate FPR, PP and NP","4272972d":"Now that we have removed all the columns with more than 50% null values, we are still left with columns which have a high percentage of null values. \n\nLets check these individually and see how to treat such columns.","7006b695":"#### 2.2.9 Last Activity column check","6a9083d3":"## Final Conclusion:\n- The top 3 variables which contribute most towards the probability of a lead getting converted are: \n    - Tags_Lost to EINS\n    - Tags_Closed by Horizzon\n    - Tags_Will revert after reading the email\t\n","b4ff8a9a":"#### 2.2.8 Tags column check","f0080067":"Now we have decided to `impute` the values of either the `mean\/ median or mode` with the null values. Hence lets check what these 3 metrics provide us.","dd21cf82":"### Inference:\n\nNow one of the reasons as to why there are so many null values for Specialization could be that lead has not entered any specialization if his\/her option is not availabe on the list.\n\nHence we decide to make a category \"Others\" for missing values. ","21d5a44c":"Visualizing the above using a countplot.","ab7440c7":"#### Converting some binary variables (Yes\/No) to 0\/1","a529f56c":"#### 11.5 Calculate Precision Score","deef9a35":"### Step 10: Making predictions on the test set","a75ec42a":"#### 6.5.1 Precision and recall tradeoff","fdc36609":"### 4.2 Dummy Variables","c74988a8":"#### Inference:\n\nAs we can see, mostly the values are 'Better Career Prospects'. So, this column is mostly acting as a constant and has no variation. So, we can ignore this column for analysis and will drop the same. ","5146b797":"### Inference:\n1. The lead origin for customers who either converted or did not is the same i.e Landing Page Submission but Lead Add Form is considerably more for converted users vs non-converted.\n2. Google and Direct traffic generates maximum number of leads.\n3. The do not email value of YES for customers who did not convert is more as compared to the ones who did convert.\n4. There are some YES values for Do not call for customers who converted vs the ones who did not.\n5. The last Activity for the customers who converted is most for SMS sent vs Email Opened for non converts.\n6. The Specialization is more for Others for both conversions and non-conversions\n7. Working professionals are more customers who converted.\n8. Better Career prospects is most in numbers for both converted and non-converted customers.\n9. The newspaper Article, X Education forums, Newspaper columns, Digital Advertisement, Through Recommendations, A free copy of Mastering the interview are not related to lead conversion.\n10.  `Will revert after reading the email` is the most chosen Tag for converted customers whereas for non-converts its `Ringing`\n11. Last Notable Activity for the customers who converted is most for SMS sent vs Modified for non converts.","a2b4eeb5":"Looking at the data, intuitively it seems 'Asymmetrique Profile Score', 'Asymmetrique Activity Score', 'Asymmetrique Profile Index' and 'Asymmetrique Activity Index' have some correlation. Let us explore that.","063daea6":"## Metrics Comparision:\n- The accuracy for our train set is `~0.8651` and for our test set is `~0.85`\n- The sensitivity for our train set is `~0.707` and for our test set is `~0.669`\n- The specificity for our train set is `~0.963` and for our test set is `~0.965`\n- The false postive rate for our train set is `~0.036` and for our test set is `~0.034`\n- The Positive predictive value for our train set is `~0.92` and for our test set is `~0.925`\n- The Negative predictive value for our train set is `~0.8432` and for our test set is `~0.82`\n- The Precision for our train set is `~0.923` and for our test set is `~0.925`\n- The recall for our train set is `~0.702` and for our test set is `~0.669`\n- The AUC Score for our train set is `~0.93` and for our test set is `~0.92`\n","341493ee":"### Step 8: Finding Optimal Cutoff Point\nOptimal cutoff probability is that prob where we get balanced sensitivity and specificity","d916aef6":"As we can see that there is a duplicate in this column. Lets correct that.","ceae3342":"## Step 6: Checking the metrics for train set","043b2c50":"#### 9.1 Calculate Accuracy","f7c42f20":"Checking the graph now.","5957ea95":"#### 11.6 Calculate recall Score","715a5a70":"## Step 11: Checking Metrics for test set","36fdc43f":"Checking the issing values again","c38dd2ef":"Also, as X Education is an online education portal; we drop Country and City columns as well.","09f168a3":"### Inference:\nSince the distribution has not changed much before and after the null value imputation, we should be good here.","22a0b8d4":"#### 11.3 Calculate specificity","9c8a42f9":"### Step 7: Plotting the ROC Curve\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","513cb55a":"Lets check the columns which have been removed by RFE","972b4ae1":"Lead Number and Prospect ID are also unique identifiers for each Lead and does not add any value for the analysis. Hence dropping the same.","832200f1":"#### 6.2 Calculate sensitivity","61b116c5":"#### 2.2.1 Asymmetrique Profile Score column check","40988345":"The distribution of 'Asymmetrique Activity Score' seems to be a Normal Distribution. So mean, median and mode- all 3 should be almost same.","33b1040c":"Lets check the null values again","b02e06a6":"Before we move forward, we will divide the dataset into 2 parts and figure out if there are any columns which impact the lead conversion.\n\nWe will divide the dataset into 2 parts:\n   1. Lead Converted\n   2. Lead Not converted","14e007f9":"#### 9.3 Calculate specificity","3d82af0c":"## Step 9: Model Evaluation","8e32de74":"Now Lets remove the outliers from the `Page Views Per Visit` column.","3b223cfd":"As the % removal of data post outlier treatment is around 7%, we should be good here.","5e7da99a":"Now that we have our data corrected, we can go ahead with the missing value check"}}