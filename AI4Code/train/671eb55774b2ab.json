{"cell_type":{"2852d9ca":"code","fc1ef0d5":"code","9840298c":"code","6ab4e9f5":"code","3237b4e4":"code","2ce9612d":"code","eb5f16c0":"code","47c80691":"code","a56c91cb":"code","782b4f6d":"code","99644a29":"code","7b0ada75":"code","3e988d96":"code","ca4f527e":"code","f9751474":"code","d44894b1":"code","10333f79":"code","6df0ab81":"code","4483276f":"code","da3c6e3a":"code","a7beea35":"code","9359a472":"code","90febde2":"code","ba846c1a":"code","d296eede":"code","61e63796":"code","14630411":"code","d1bcc698":"markdown","29cb987e":"markdown","d840de28":"markdown","3c3e4ea4":"markdown","f009ce5d":"markdown","a300df52":"markdown","639ae83f":"markdown","c661a591":"markdown","d7a9f4f4":"markdown","ef7752c1":"markdown","317cd635":"markdown","8fb4ee12":"markdown","0815e761":"markdown","e40758b3":"markdown","3e923108":"markdown","e02d3d08":"markdown","d60dc8ed":"markdown","0909f20c":"markdown","ca7ca7af":"markdown","3118600f":"markdown","0548b1d5":"markdown","4565566a":"markdown","56112fc4":"markdown","6c3d4be0":"markdown","3c3545a7":"markdown","1e097c83":"markdown","5b7315a2":"markdown","94ddb010":"markdown","4d49c9b1":"markdown","612221a9":"markdown","a36d32bc":"markdown","1c049ab3":"markdown","7d6bd660":"markdown","e886636f":"markdown","b1302ff8":"markdown","5f35f694":"markdown","e7d75662":"markdown","65c72ca6":"markdown","1a599d82":"markdown","05078a9d":"markdown","119b945f":"markdown","482a7f7f":"markdown","5ad2919c":"markdown","a167ad36":"markdown","61a60714":"markdown","9aeb1d22":"markdown"},"source":{"2852d9ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\"\"\"\nData Manipulating\n\"\"\"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\"\"\"\nVisualization\n\"\"\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc1ef0d5":"data = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","9840298c":"data.head()","6ab4e9f5":"data.info()","3237b4e4":"fig,ax = plt.subplots(figsize=(8,6))\nsns.countplot(data.diagnosis)\nplt.show()","2ce9612d":"fig,ax = plt.subplots(figsize=(8,6))\nsns.distplot(data[\"radius_mean\"],color=\"#FC2D2D\")\nplt.show()","eb5f16c0":"fig,ax = plt.subplots(figsize=(8,6))\nsns.distplot(data[\"texture_mean\"],color=\"#F739F7\")\nplt.show()","47c80691":"fig,ax = plt.subplots(figsize=(8,6))\nsns.distplot(data[\"smoothness_mean\"],color=\"#F5BA5D\")\nplt.show()","a56c91cb":"def outlier_index_detector(df,features):\n    indexes = []\n    result = []\n    for ftr in features:\n        \n        Q1 = df.describe()[ftr][\"25%\"] # Lower quartile\n        Q3 = df.describe()[ftr][\"75%\"] # Upper quartile\n        IQR = Q3 - Q1 # IQR\n        STEP = IQR*1.5 # Outlier Step\n        \n        ind = data[(data[ftr]<Q1-STEP) | (data[ftr]>Q3+STEP)].index.values\n        for i in ind:\n            indexes.append(i)\n    \n    for index in indexes:\n        \n        indexes.remove(index) \n        if index in indexes: # More than 2\n            indexes.remove(index)\n            \n            if index in indexes: # More than 3\n                indexes.remove(index)\n                \n                if index in indexes: # More than 4\n                    indexes.remove(index)\n                    \n                    if index in indexes: # Append Final Result\n                        result.append(index)\n            \n    \n    return result\n    ","782b4f6d":"feature_names = (list(data))\nfeature_names.remove(\"id\")\nfeature_names.remove(\"diagnosis\")\nfeature_names.remove(\"Unnamed: 32\")\nprint(feature_names)","99644a29":"outliers = outlier_index_detector(data,feature_names)\nprint(outliers)","7b0ada75":"outliers = list(np.unique(outliers))\nprint(\"There are {} outlier rows \\n\".format(len(outliers)))\nprint(outliers)","3e988d96":"print(\"Len of the dataset before dropping outliers\",len(data))\ndata.drop(outliers,inplace=True)\nprint(\"Len of the dataset after dropping outliers\",len(data))","ca4f527e":"fig,ax = plt.subplots(figsize=(20,20))\nsns.heatmap(data.corr(),annot=True,linewidths=1.5,fmt=\"0.1f\")\nplt.show()","f9751474":"fig,ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(x=\"radius_mean\",y=\"texture_mean\",data=data,color=\"#670F91\")\nplt.show()","d44894b1":"fig,ax = plt.subplots(figsize=(10,8))\nsns.scatterplot(x=\"radius_mean\",y=\"smoothness_mean\",data=data,color=\"#BD6F4B\")\nplt.show()","10333f79":"data.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)","6df0ab81":"print(\"First 5 entries\",data.diagnosis[:5])\ndata.diagnosis = [0 if each == \"M\" else 1 for each in data.diagnosis]\nprint(data.diagnosis[:5])","4483276f":"data.tail()","da3c6e3a":"data = (data-np.min(data)) \/ (np.max(data)-np.min(data))\ndata.head()","a7beea35":"from sklearn.model_selection import train_test_split\nx = data.drop(\"diagnosis\",axis=1) \ny = data.diagnosis\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)","9359a472":"print(\"Len of the x_train\",len(x_train))\nprint(\"Len of the x_test \",len(x_test))\nprint(\"Len of the y_train\",len(y_train))\nprint(\"Len of the y_test \",len(y_test))","90febde2":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense","ba846c1a":"def build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=12,kernel_initializer=\"uniform\",activation=\"tanh\",input_dim=30))\n    classifier.add(Dense(units=6,kernel_initializer=\"uniform\",activation=\"tanh\"))\n    classifier.add(Dense(units=1,kernel_initializer=\"uniform\",activation=\"sigmoid\")) # Output Layer\n    classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n    return classifier\n                   ","d296eede":"classifier = KerasClassifier(build_fn=build_classifier,epochs=100)\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator=classifier,X=x_train,y=y_train,cv=3)\n\nprint(\"Mean of CV scores\",accuracies.mean())\nprint(\"Variance of CV scores\",accuracies.std())\n","61e63796":"classifier.fit(x_train,y_train)","14630411":"print(\"Our train score is\",classifier.score(x_train,y_train))\nprint(\"Our test score is \",classifier.score(x_test,y_test))\n","d1bcc698":"* Axis 1 is columns (features) and axis 0 is rows (entries)","29cb987e":"## Cross Validation\n\nIn this section I am going to do cross validation and check the score. I am going to use sklearn library's cross_validation_score. But before this I am going to create my keras classifier","d840de28":"* There are two unrelevant features in the dataset: id, Unnamed: 32. We have to drop them\n* There are 31 features in the dataset except the unrelevants.\n","3c3e4ea4":"## Creating Model Function\nIn this section I am going to define model function. You know, I am going to use keras library and I have to define my model.","f009ce5d":"## Fitting Model\nIn this section I am going to fit my model using my x_train and y_train arrays.","a300df52":"# Simple Data Analyses\n\nIn this section I am going to do some Simple EDA. But I can not examine all the features because there are 31 features. So I am going to examine some features.","639ae83f":"## Correlation Between Features\n\nIn this sub-section I am going to use features that I've used in simple data analyses. Let's begin.\n","c661a591":"* What a confusing heatmap. \n* There are too many strong positive correlations between the features\n* Unlike the positive correlations, there is not any strong negative correlation between the features.","d7a9f4f4":"And now I am going to import dataset. Our dataset's format is csv, so I will use pandas' read_csv method for importing.","ef7752c1":"## Diagnosis Countplot","317cd635":"* There is no correlation between these features. ","8fb4ee12":"* Good news! Our dataset does not have any nan values, so we will not fill any nan values.","0815e761":"# Detailed Data Analyses\n\nIn this section I am going to examine correlations between features. But I can not examine all correlations between features because you know there are 31 features in the dataset.","e40758b3":"### What Did I Do In This Function\n1. I've started a for loop that each iteration is a different feature.\n1. I've compute outlier step using outlier formula\n1. I've find the indexes for each feature\n    * For Instance:\n        * Feature 1 Outlier Index : 32,12,42\n        * Feature 2 Outlier Index : 32,16,89\n    \n    So we can say there are two outliers in the first row\n1. I've append indexes into my list.\n1. I've created a filter.\n1. I've filtered rows that have outliers more than four.\n1. I've append them into my result list.","3e923108":"# Modeling\n\nFinally we came! In this section I am going to create our deep learning model. I am going to follow these steps.\n* Creating Model Function\n* Cross Validation\n* Fitting Model\n* Prediction and Result","e02d3d08":"# Importing Libraries and Dataset\nIn this section I am going to import libraries and dataset that I will use. However I am not going to import deep learning libraries and scikit-learn in this section, I am going to import them when I use them. ","d60dc8ed":"### Radius Mean - Smoothness Mean Scatter Plot","0909f20c":"## Dropping Unrelevant Features\nI know, this is easy, however I've said this in the beginning of the kernel. I am going to explain everything as much as I can step by step. ","ca7ca7af":"### What Did I Do In This Function\n1. I've created sequential object (you can think it is empty model)\n1. I've add my layers\n    * Units = How many nodes in the layer\n    * kernel_initializer= How do algorithm initalize weights and bias.\n    * Activation = activation function like tanh,relu and sigmoid\n1. I've compile my model.\n\n### Why did I use Sigmoid Function In Output Layer?\nBecause in this kernel we will do binary classification (0 and 1) and we use sigmoid activation function for this.\n\nI know there are questions in your mind, I would want to explain everything with more detail, but I am a beginner in deep learning and I am not very good at English, but if you want to learn details, you can check my teacher's kernel:\n\n*https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners*","3118600f":"I think it is enough for having a bit more idea about the dataset. Let's move on to the next section.","0548b1d5":"We completed this section. Let's move on!","4565566a":"## Converting Label Feature Into Int64\nIn this section I am going to convert label (diagnosis) into int64. In order to do this I am going to use list comprehension from vanilla python.\n\n     We love python <3","56112fc4":"## Smoothness Mean Histogram","6c3d4be0":"## Radius Mean Histogram","3c3545a7":"* Our arrays are ready. Let's check the lenghts of our arrays.\n","1e097c83":"## Train Test Split\n\nIn this section I am going to split the dataset into train and test. In order to do this I am going to use sklearn library's train_test_split function. ","5b7315a2":"## Texture Mean Histogram","94ddb010":"## Scaling\nWe are approaching to the most exciting section. In this section I am going to normalize dataset. In order to do this I am going to use this formula\n\n     (value - min(data)) \/( max(data) - min(data))","4d49c9b1":"* As we can see there is a little correlation between them.\n","612221a9":"* Texture mean plot is so similar with radius mean","a36d32bc":"## Prediction and Result\nIn this section I am going to predict my test values and compute accuracy.","1c049ab3":"# Preprocessing\n\nIn this section I am going to prepare the dataset for modeling. I am going to follow these steps:\n* Dropping Unrelevant Features\n* Converting Label Feature Into Int64\n* Scaling (Normalizing)\n* Train Test Split\n\nLet's start with dropping unrelevant features.\n","7d6bd660":"* Yea, our first five diagnosis is 0 but do not worry there are 1 values in the dataset. ","e886636f":"## Correlation Heatmap\n\nFirst I am going to use correlation heatmap for diagnosing all the relations between the features.","b1302ff8":"# Outlier Detection\n\nIn this section I am going to drop outlier values. One day I've heard something about outlier values from a data scientist, he said outliers are silent killers, so you have to drop them from dataset. Yes, he is definitely right. \n\nBut I am not going to drop the rows that only have one outlier value. I am going to drop the rows that have outlier values more than five. I have to do this because I do not want to drop so many rows.\n\nLet's go!","5f35f694":"# Dataset Overview\n\nIn this section I am going to examine the dataset.","e7d75662":"* As we can see most of the dataset's radius_mean is between 10 and 20\n","65c72ca6":"* Our cross validation score mean is %97.6\n* Our cross validation variance is 0.01","1a599d82":"# Introduction\n\nHello people, welcome to my kernel! In this kernel I am going to apply deep learning to breast cancer dataset. I am going to tell everything step by step. Before the start. Let's take a look at our schedule\n\n# Schedule\n1. Importing Libraries and Dataset\n1. Dataset Overview\n1. Simple Data Analyses\n    * Diagnosis Countplot\n    * Radius Mean Histogram\n    * Texture Mean Histogram\n    * Smoothness Mean Histogram\n1. Outlier Detection\n1. Detailed Data Analyses\n    * Correlation Heatmap\n    * Correlation Between Features\n        * Radius Mean - Texture Mean Scatter Plot\n        * Radius Mean - Smoothnes Mean Scatter Plot\n1. Preprocessing\n    * Dropping Unrelevant Features\n    * Converting Label Feature Into Int64\n    * Scaling (Normalizing)\n    * Train Test Split\n1. Modeling\n    * Creating Model Function\n    * Cross Validation\n    * Fitting Model\n    * Prediction and Result\n1. Conclusion\n    ","05078a9d":"As you can see there are same values in list. So I am going to drop them.\n","119b945f":"And we are ready to drop them\n","482a7f7f":"# Conclusion\n\nThanks for your attention, if there are questions in your mind, you can ask them in comment section I will definitely answer them as much as I can. \n\nIf you see any mistakes or problems in my kernel, please contact with me.","5ad2919c":"* As we can see there are two labels in our dataset. M and B\n* Most of the dataset are B labeled.\n","a167ad36":"### Radius Mean - Texture Mean Scatter Plot\nWe've said that these two feature's histograms are similar. But It does not means they have strong correlation. Let's take a look at our scatter plot.","61a60714":"* And now I am going to drop them for improving my future model.","9aeb1d22":"* As we can see our smoothness_mean values are between 1 and 0.\n* So it means that if we do not normalize our dataset, there will be problems.\n* Most of the values is between 0.06 and 0.14"}}