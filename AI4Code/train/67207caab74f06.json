{"cell_type":{"625c0afb":"code","5466a497":"code","31d5b834":"code","a289b5bf":"code","4d9cbfaa":"code","446a1e4b":"code","6791bedb":"code","b64d9702":"code","5a4f1835":"code","3b836472":"code","bc435eaa":"code","5b8cf8e5":"code","cb7c4487":"code","2b8a238b":"code","7a933a7a":"code","67f1964e":"code","67bdec12":"code","8f0d064f":"code","42710813":"code","390bc759":"code","65603e7e":"code","b12edd12":"code","cb79b38f":"code","db3b4c80":"code","c2ea6176":"code","580207b4":"code","d94a27e6":"code","2588cd08":"code","2b0abaaf":"code","603ec936":"code","75f78865":"code","3cb47131":"code","7b08d58f":"code","96185a22":"code","4994db83":"code","1afa00c3":"code","b82b1613":"markdown"},"source":{"625c0afb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5466a497":"import os, sys\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import (Input,LSTM,GRU,Dense,\n                          Embedding,Bidirectional,RepeatVector,\n                          Concatenate,Activation,Dot,Lambda)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras.backend as K","31d5b834":"# function to do softmax over time\ndef softmax_over_time(x):\n  assert(K.ndim(x) > 2)\n  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n  s = K.sum(e, axis=1, keepdims=True)\n  return e\/s","a289b5bf":"# config variables\nBATCH_SIZE = 64\nEPOCHS = 100\nLATENT_DIM = 256\nLATENT_DIM_DECODER = 256\nNUM_SAMPLES = 10000\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 20000\nEMBEDDING_DIM = 100","4d9cbfaa":"input_texts = [] # sentence in original language\ntarget_texts = [] # sentence in target language\ntarget_texts_inputs = [] # sentence in target language offset by 1","446a1e4b":"# load in the data\nt = 0\nfor line in open('\/kaggle\/input\/twitter-auto\/twitter_tab_format.txt'):\n  t+=1\n  if t>NUM_SAMPLES:\n    break\n  # input and target are seperated by '\\t'\n  if '\\t' not in line:\n    continue\n  input_text, translation = line.split('\\t')\n  target_text = translation + ' <eos>'\n  target_text_input = '<sos> ' + translation\n  input_texts.append(input_text)\n  target_texts.append(target_text)\n  target_texts_inputs.append(target_text_input)\nprint('num samples:',len(input_texts))","6791bedb":"tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer_inputs.fit_on_texts(input_texts)\ninput_sequences = tokenizer_inputs.texts_to_sequences(input_texts)","b64d9702":"# word to index mapping for input language\nword2idx_inputs = tokenizer_inputs.word_index\nprint('Found %s unique input tokens.'%len(word2idx_inputs))","5a4f1835":"# max length input seq\nmax_len_input = max(len(s) for s in input_sequences)","3b836472":"# tokenize the outputs\ntokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\ntokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)\ntarget_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\ntarget_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)","bc435eaa":"# word to index mapping for output language\nword2idx_outputs = tokenizer_outputs.word_index\nprint('Found %s unique output tokens.'%len(word2idx_outputs))","5b8cf8e5":"num_words_output = len(word2idx_outputs)+1\n# max length output seq\nmax_len_target = max(len(s) for s in target_sequences)","cb7c4487":"# padding the sequences\nencoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\nprint('Encoder data shape:',encoder_inputs.shape)\n\ndecoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\nprint('Decoder data shape:',decoder_inputs.shape)\n\ndecoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')","2b8a238b":"# loading pre-trained word vectors\nword2vec = {}\nwith open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.%sd.txt'%EMBEDDING_DIM) as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:],dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.'%len(word2vec))","7a933a7a":"# prepare embedding matrix\nnum_words = min(MAX_NUM_WORDS, len(word2idx_inputs)+1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx_inputs.items():\n  if i < MAX_NUM_WORDS:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[i] = embedding_vector","67f1964e":"# creating embedding layer\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=max_len_input,\n                            trainable=False)","67bdec12":"decoder_targets_one_hot = np.zeros((len(input_texts),\n                                   max_len_target,\n                                   num_words_output),dtype='float32')\nfor i,d in enumerate(decoder_targets):\n  for t,word in enumerate(d):\n    decoder_targets_one_hot[i, t, word] = 1","8f0d064f":"# build the model\n# setup the encoder\nencoder_inputs_placeholder = Input(shape=(max_len_input,))\nx = embedding_layer(encoder_inputs_placeholder)\nencoder = Bidirectional(LSTM(LATENT_DIM, return_sequences=True, dropout=0.5))\nencoder_outputs = encoder(x)","42710813":"# setup the decoder\ndecoder_inputs_placeholder = Input(shape=(max_len_target,))\ndecoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\ndecoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)","390bc759":"# Attention layer\nattn_repeat_layer = RepeatVector(max_len_input)\nattn_concat_layer = Concatenate(axis=-1)\nattn_dense1 = Dense(10, activation='tanh')\nattn_dense2 = Dense(1, activation=softmax_over_time)\nattn_dot = Dot(axes=1)","65603e7e":"def one_step_attention(h, st_1):\n  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n \n  # copy s(t-1) Tx times\n  # now shape = (Tx, LATENT_DIM_DECODER)\n  st_1 = attn_repeat_layer(st_1)\n\n  # Concatenate all h(t)'s with s(t-1)\n  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n  x = attn_concat_layer([h, st_1])\n\n  # Neural net first layer\n  x = attn_dense1(x)\n\n  # Neural net second layer with special softmax over time\n  alphas = attn_dense2(x)\n\n  # \"Dot\" the alphas and the h's\n  # Remember a.dot(b) = sum over a[t] * b[t]\n  context = attn_dot([alphas, h])\n\n  return context","b12edd12":"# decoder after attention\ndecoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\ndecoder_dense = Dense(num_words_output, activation='softmax')","cb79b38f":"initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\ninitial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\ncontext_last_word_concat_layer = Concatenate(axis=2)","db3b4c80":"# s, c will be re-assigned in each iteration of the loop\ns = initial_s\nc = initial_c\n\n# collect outputs in a list at first\noutputs = []\nfor t in range(max_len_target): # Ty times\n  # get the context using attention\n  context = one_step_attention(encoder_outputs, s)\n\n  # we need a different layer for each time step\n  selector = Lambda(lambda x: x[:, t:t+1])\n  xt = selector(decoder_inputs_x)\n  \n  # combine \n  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n\n  # pass the combined [context, last word] into the LSTM\n  # along with [s, c]\n  # get the new [s, c] and output\n  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n\n  # final dense layer to get next word prediction\n  decoder_outputs = decoder_dense(o)\n  outputs.append(decoder_outputs)","c2ea6176":"# 'outputs' is now a list of length Ty\n# each element is of shape [batch_size, output_vocab_size]\n# therefore if we simply stack all the outputs into 1 tensor\n# it would be of shape TxNxD\n# we would like it to be of shape NxTxD\n\n# so we stack and transpose\ndef stack_and_transpose(x):\n  x = K.stack(x)\n  x = K.permute_dimensions(x, pattern=(1, 0, 2))\n  return x","580207b4":"# making it a layer\nstacker = Lambda(stack_and_transpose)\noutputs = stacker(outputs)","d94a27e6":"# create the model\nmodel = Model(inputs=[encoder_inputs_placeholder,\n                      decoder_inputs_placeholder,\n                      initial_s,\n                      initial_c],\n              outputs=outputs)","2588cd08":"# compile the model\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',\n              metrics=['accuracy'])","2b0abaaf":"z = np.zeros((len(encoder_inputs), LATENT_DIM_DECODER)) # initial [s, c]\nr = model.fit(\n  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n  batch_size=BATCH_SIZE,\n  epochs=15,\n  validation_split=0.2\n)","603ec936":"## Make predictions\nencoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n\n# T=1 decoder model\nencoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM*2))\ndecoder_inputs_single = Input(shape=(1,))\ndecoder_inputs_single_x = decoder_embedding(decoder_inputs_single)","75f78865":"context = one_step_attention(encoder_outputs_as_input, initial_s)\ndecoder_lstm_input = context_last_word_concat_layer([context,decoder_inputs_single_x])","3cb47131":"o,s,c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s,initial_c])\ndecoder_outputs = decoder_dense(o)","7b08d58f":"decoder_model = Model(inputs=[decoder_inputs_single,\n                              encoder_outputs_as_input,\n                              initial_s,\n                              initial_c],\n                      outputs=[decoder_outputs, s, c])","96185a22":"# map indices back to real words\nidx2word_eng = {v:k for k,v in word2idx_inputs.items()}\nidx2word_trans = {v:k for k,v in word2idx_outputs.items()}","4994db83":"def decode_sequence(input_seq):\n  enc_out = encoder_model.predict(input_seq)\n  \n  target_seq = np.zeros((1,1))\n  target_seq[0,0] = word2idx_outputs['<sos>']\n  \n  eos = word2idx_outputs['<eos>']\n  \n  s = np.zeros((1, LATENT_DIM_DECODER))\n  c = np.zeros((1, LATENT_DIM_DECODER))\n  \n  # creating the translation\n  output_sentence = []\n  for _ in range(max_len_target):\n    o,s,c = decoder_model.predict([target_seq, enc_out, s, c])\n    \n    idx = np.argmax(o.flatten())\n    \n    if eos==idx:\n      break\n      \n    word = ''\n    \n    if idx>0:\n      word = idx2word_trans[idx]\n      output_sentence.append(word)\n    \n    target_seq[0,0] = idx\n  \n  return ' '.join(output_sentence)","1afa00c3":"# i = np.random.choice(len(input_texts))\n# input_seq = encoder_inputs[i:i+1]\n# translation = decode_sequence(input_seq)\n# print('Input Sentence:',input_texts[i])\n# print('Predicted Translation:',translation)","b82b1613":"### Twitter ChatBot using Attention"}}