{"cell_type":{"32b690c9":"code","160401f1":"code","2aeadaed":"code","5d161b9d":"code","96fa7a25":"code","d4fe25de":"code","99baf52c":"code","90ede4dd":"code","941fdc2a":"code","275e72ce":"code","8d6b91a6":"code","08958244":"code","9794c837":"code","5290226a":"code","db6d23fa":"code","ed484a58":"code","65e0ccdc":"code","315c5351":"code","6f14b865":"code","88e074aa":"code","6bf500e2":"code","bfe6d92a":"code","8356c27d":"code","b3056c36":"code","29e4241e":"code","771d0d4e":"code","c50c5347":"code","208d175e":"code","a3a1e2f2":"code","dc42c40e":"code","f3d38857":"code","9c3f9428":"code","b1bf2993":"code","79e19a6b":"code","3b55cd08":"code","3fcd7543":"code","540704e2":"code","322ccb9a":"code","de47409b":"code","10354a68":"code","665cd001":"code","8a25843b":"markdown","b594218e":"markdown","9e1a225f":"markdown","71b80b7f":"markdown","8e60b3aa":"markdown","570e00d3":"markdown","9ee06642":"markdown","a7e2752c":"markdown","8e9ad11c":"markdown","7c7b179a":"markdown","882ffc16":"markdown","fd08d124":"markdown","2903fe9d":"markdown","7330ee9d":"markdown","bbd931aa":"markdown","5e81cd4e":"markdown","defc9be0":"markdown","54cd1351":"markdown","2af0acb0":"markdown","9ee873a8":"markdown","48d80885":"markdown","9fc529b4":"markdown","d70a39b3":"markdown","e6e1ae48":"markdown","0a74e5cd":"markdown"},"source":{"32b690c9":"#Import library that needed.\nimport numpy as np # linear algebra.\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv).\n#import visualization library.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.metrics import accuracy_score #Import accuracy model library.","160401f1":"# Open datasets. This datasets is separated because train data will be used to training machine learning models, and test data will be used to test the best machine learning model.\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","2aeadaed":"#See the first three of train dataset.\ntrain.head(3)","5d161b9d":"#See the first three of test dataset.\ntest.head(3)","96fa7a25":"#Sum null values in every column train data.\npd.isnull(train).sum()","d4fe25de":"#Drop column that not really have useful information to know how much survived. It's Cabin and Ticket feature.\ntrain = train.drop(['Cabin', 'Ticket'], axis = 1)","99baf52c":"#drop column that have a little null values.\ntrain = train.dropna(subset = ['Embarked'])","90ede4dd":"#Using median of the Age for filling null values.\ntrain['Age'] = train['Age'].fillna(train['Age'].median())","941fdc2a":"#Check duplicated values. This dataset hasn't duplicate data so there is no need to worry about duplicated data :)!\nduplicate = train[train.duplicated()]\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(duplicate)","275e72ce":"#We use train datafile because Survived column in test datafile not yet filled. Figure train datafile also bring picture how related columns to the Survived people.\nax = train['Survived'].value_counts().plot(kind='pie',\n                                    title=\"Number for Survived People\", autopct=\"%.1f%%\")\nax.set_ylabel(\" \")\nplt.show()","8d6b91a6":"#See correlation between Sex and Survived column.\nsns.countplot(x ='Survived', data = train, hue='Sex')\nplt.show()","08958244":"#See correlation between Pclass and Survived column.\nsns.countplot(x ='Survived', data = train, hue='Pclass')\nplt.show()","9794c837":"#See correlation between Embarked and Survived column.\nsns.countplot(x ='Survived', data = train, hue='Embarked')\nplt.show()","5290226a":"# Explore Age distibution within Survived column. \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Grey\", shade=True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax=g, color=\"Green\", shade=True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","db6d23fa":"# Explore SibSp distibution within Survived column. \ng = sns.kdeplot(train[\"SibSp\"][(train[\"Survived\"] == 0)], color=\"Grey\", shade=True)\ng = sns.kdeplot(train[\"SibSp\"][(train[\"Survived\"] == 1)], ax=g, color=\"Green\", shade=True)\ng.set_xlabel(\"Sibling\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","ed484a58":"# Explore Parch distibution within Survived column. \nsns.countplot(y=\"Parch\", hue=\"Survived\", data=train)\nplt.show()","65e0ccdc":"#Count women survived in Titanic disaster.\nwomen = train.loc[train.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","315c5351":"#Count men survived in Titanic disaster.\nmen = train.loc[train.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","6f14b865":"#Define Survived column as target.\ny = train['Survived']","88e074aa":"#Find correlation between feature.\ncor = train.corr()\ncor = pd.DataFrame(cor)\nsns.heatmap(cor, vmin=-0.5, cmap=\"YlGnBu\")","6bf500e2":"#initialize label endoce on Sex column.\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain[\"Sex\"] = le.fit_transform(train[\"Sex\"].values)\ntrain.head()","bfe6d92a":"#Initialize features columns to submit on models.\nfeature = ['Fare', 'Age', 'Parch', 'SibSp', 'Sex']\nX = train[feature].values","8356c27d":"# Change fare until age column to scaller.\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX[:, :2] = sc.fit_transform(X[:, :2])\nX","b3056c36":"#Split train and test in X and y values. This split will be used to put in models (train) and evaluated it (test).\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)","29e4241e":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X_train, y_train)\ny_pred_rc = model.predict(X_test)\naccuracy_score(y_test, y_pred_rc)","771d0d4e":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred_dt = classifier.predict(X_test)\naccuracy_score(y_test, y_pred_dt)","c50c5347":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred_gb = classifier.predict(X_test)\naccuracy_score(y_test, y_pred_gb)","208d175e":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred_sv = classifier.predict(X_test)\naccuracy_score(y_test, y_pred_sv)","a3a1e2f2":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 100, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\ny_pred_nc = classifier.predict(X_test)\naccuracy_score(y_test, y_pred_nc)","dc42c40e":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred_lr = classifier.predict(X_test)\naccuracy_score(y_test, y_pred_lr)","f3d38857":"#Sum null values in every column train data.\npd.isnull(test).sum()","9c3f9428":"#Drop column that not really have useful information to know how much survived. It's Cabin and Ticket feature.\ntest = test.drop(['Cabin', 'Ticket'], axis = 1)","b1bf2993":"#Using median of the Age for filling null values.\ntest['Age'] = test['Age'].fillna(test['Age'].median())","79e19a6b":"#Check duplicated values. This dataset hasn't duplicate data so there is no need to worry about duplicated data :)!\nduplicate = test[test.duplicated()]\nprint(\"Duplicate Rows except first occurrence based on all columns are :\")\nprint(duplicate)","3b55cd08":"#Drop null values if there is still any null values.\ntest.dropna(inplace=True)","3fcd7543":"#initialize label endoce as first step.\ntest[\"Sex\"] = le.fit_transform(test[\"Sex\"].values)","540704e2":"#Initialize features columns to submit on models.\nfeature = ['Fare', 'Age', 'Parch', 'SibSp', 'Sex']\nX_tes = test[feature].values","322ccb9a":"# Change fare until age column to scaller\nX_tes[:, :2] = sc.fit_transform(X_tes[:, :2])\nX_tes","de47409b":"#Predict test dataset using Random Forest Classifier model.\ntest_pred = model.predict(X_tes)","10354a68":"#Make submission dataset.\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': test_pred})\noutput.to_csv('my_submission.csv', index=False)","665cd001":"#See the first three of submission dataset.\noutput.head(3)","8a25843b":"![Why_Sank](https:\/\/sayingimages.com\/wp-content\/uploads\/true-story-titanic-meme.jpg)","b594218e":"From this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Therefore,gender seems to be such a strong indicator of survival.","9e1a225f":"**1. Random Forest Classifier**\n\nThis model is constructed of several \"trees\" (We'll construct 100!) that will consider each passenger's data on the first branch and vote on whether the individual survived.  Then, the random forest model makes the outcome with the most votes.\n\n![](https:\/\/www.researchgate.net\/publication\/337361248\/figure\/fig1\/AS:826947454124035@1574171045866\/Cartoon-representation-of-a-random-forest-classifier.png)","71b80b7f":"The code above shows only the first 3 rows of each table, but all of the data is there (all 891 rows of **train.csv** and all 418 rows of **test.csv**).","8e60b3aa":"The test dataset must be cleaned to make it ready to input in model production. After that, I used save code for the new predictions in a CSV file **my_submission.csv**.","570e00d3":"This notebook contain memes,decriptive, and predictive analytics from Titanic datasets. As all already know, Titanic is a famous disaster of shipwrecks history. It was the disaster of sinking a RMS Titanic in 1912 that already filmed in 1997 and 2014 (3D). That films tell about romantic story of Jack and Rose (played by Leonardo DiCaprio and Kate Winslet) from different social status. But unfortunately, their story ends because Jack prefers to save Rose to put on the only one small board founded.","9ee06642":"![titanic](https:\/\/i.imgflip.com\/35ceha.jpg)","a7e2752c":"**2. Decision Tree Classifier**\n\nThis model is also constructed of tree. But only a tree. Each passenger's data will have survived outcome if it go though right branchs.\n\n![](https:\/\/miro.medium.com\/max\/1200\/1*xGsYc6aXehD7lyoLEn-mMA.png)","8e9ad11c":"At the end, only using gender-based to make survival predictions is the worst. Since we have multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions. To make conclusion prediction from several columns at once, we'll use machine learning to automate this for us. Or, if not using it, we would take a long time to consider all possible patterns.\n","7c7b179a":"![Trans](https:\/\/preview.redd.it\/4amlbp9qlxi31.jpg?auto=webp&s=1a6d90f52b25d6930945a71dafec531c9eb368f7)","882ffc16":"**5. K-Nearest Neighbor (KNN)**\n\nThis model already separated class that have some feature requirement. Therefore, every passanger would be judged based on each of the features that owned.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final1_ibdm8a.png)","fd08d124":"# ***Visualization Dataset***\nSince datafile is already complete, I want to picture features to the Survived column. Is this disaster killed more men than women? We'll check if this pattern holds true in the data (in train.csv).","2903fe9d":"**4. Support Vector Machine (SVM)**\n\nThis model contain a vector that seperate every class. Every class have some feature requirements that every passanger must have to join the class.\n\n![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1526288453\/index3_souoaz.png)","7330ee9d":"![memes](https:\/\/memegenerator.net\/img\/instances\/81953085.jpg)","bbd931aa":"**3. Naive Bayes**\n\nThis is make every passanger may or may not turn out to be survived. This model evaluate based on presence (or absence) of a particular feature of a class.\n\n![](https:\/\/images.deepai.org\/glossary-terms\/0f7ab6cd59a845a1a3e4225ebe718171\/naive_bayes.png)\n","5e81cd4e":"**6. Logistic Regression**\n\nThis model will be make every passanger may or may not in the certain class. They would be judged based on feature that they have.\n\n![](https:\/\/www.equiskill.com\/wp-content\/uploads\/2018\/07\/WhatsApp-Image-2020-02-11-at-8.30.11-PM.jpeg)","defc9be0":"Codes cell below look for patterns in five different columns (**\"Fare\"**, **\"Age\"**, **\"Sex\"**, **\"SibSp\"**, and **\"Parch\"**) of the data. That feature choosed because have high correlation with **\"Survived\"** column. They construct in classification machine learning models based on patterns in the **train.csv** file, before generating survived predictions from the feature passengers in **test.csv**. ","54cd1351":"# **Submission**","2af0acb0":"Based on accuracy of models to predicted, we concluded Random Forest Classifier is the best model. It have the highest rating on the accuracy, 0,847. I used this model to predict in test dataset.","9ee873a8":"# **Cleaning train dataset**\n\nSince train datasets will be used to help us predict from all the machine learning models,this dataset must be cleaned and droped unnecessary columns.","48d80885":"# **Machine Learning Models**","9fc529b4":"![Rose and Jack](https:\/\/memezila.com\/wp-content\/I-want-my-tears-back-meme-2464.png)","d70a39b3":"![RoseJack](https:\/\/memezila.com\/wp-content\/How-I-enter-other-peoples-relationships-meme-1076.png)","e6e1ae48":"# Data Preparation","0a74e5cd":"As before, make sure datasets can be understanded. This datasets are telling about passenger data that including gender, embarked, and so on.\n\nThis is the explaining of every column in datasets.\n\n* PassangerId, Name, Age, Sex: self-explanatory.\n* Survived: prediction target. If 1 = survived; 0 = not survived\n* Pclass: A proxy for socio-economic status (SES) : 1st = Upper; 2nd = Middle; 3rd = Lower.\n* Sibsp: The dataset defines family relations in this way. There is sibling = brother, sister, stepbrother, stepsister; spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n* Parch: The dataset defines family relations in this way parent = mother, father; child = daughter, son, stepdaughter, stepson; some children travelled only with a nanny, therefore parch=0 for them.\n* Fare: tariff for passanger.\n* Embarked: port of harbour in this way C = Cherbourg; Q = Queenstown; S = Southampton"}}