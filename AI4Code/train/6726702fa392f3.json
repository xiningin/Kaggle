{"cell_type":{"5375f238":"code","eeb7265f":"code","003fc4ac":"code","71923927":"code","41f532cc":"code","a2355739":"code","b2f4bc3b":"code","38d19db9":"code","798946e7":"code","cb7656c8":"code","7453fa46":"code","9ebd1970":"code","70144392":"code","885b6b27":"code","a7d46b54":"code","b7ea3bc7":"code","37cc8903":"code","f34c0fe4":"code","00222734":"code","a7d9c992":"code","394240f7":"code","7400b5d3":"code","66accce8":"code","05443a05":"code","7bb8a1e5":"code","949c4e44":"code","0f1130b4":"code","4691f193":"code","a46d37f7":"code","b90d4c14":"code","491a411b":"code","64e45aeb":"code","015da56e":"code","67bb075d":"markdown","2c8915d4":"markdown","71268304":"markdown","3f8b580e":"markdown","ee1d748f":"markdown","c0b09e3d":"markdown"},"source":{"5375f238":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\n","eeb7265f":"train = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nsub = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\n                    ","003fc4ac":"train.head()","71923927":"from tqdm import tqdm\nimport os\ntrain_name, train_text = [],[]\n\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    train_name.append(f.replace(\".txt\",\"\"))\n    train_text.append(open('..\/input\/feedback-prize-2021\/train\/'+f,'r').read())\n    ","41f532cc":"train_text = pd.DataFrame({'id':train_name,'text':train_text})","a2355739":"train = pd.merge(train,train_text,how='left', on='id')","b2f4bc3b":"train_text.head()","38d19db9":"train.head()","798946e7":"test_names, test_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/test'))):\n    test_names.append(f.replace('.txt', ''))\n    test_texts.append(open('..\/input\/feedback-prize-2021\/test\/' + f, 'r').read())\ntest_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\ntest_texts","cb7656c8":"test = pd.merge(sub, test_texts, how = 'left', on = 'id')\ntest","7453fa46":"newdf = train[['discourse_text','discourse_type']]\nnewdf.head()","9ebd1970":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nnewdf['discourse_type'] = le.fit_transform(newdf['discourse_type'])","70144392":"print('len of train df {}'.format(len(newdf)))\nnewdf.head()","885b6b27":"split = int(0.8*len(newdf))\ntraindf = newdf[:split]\ntestdf = newdf[split:]\nprint('train df len {}'.format(len(traindf)))\nprint('train df len {}'.format(len(testdf)))","a7d46b54":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","b7ea3bc7":"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(traindf['discourse_text'].values.tolist() + testdf['discourse_text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(traindf['discourse_text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(testdf['discourse_text'].values.tolist())","37cc8903":"from sklearn.naive_bayes import MultinomialNB","f34c0fe4":"nb = MultinomialNB()","00222734":"nb = MultinomialNB()","a7d9c992":"nb.fit(train_tfidf,train_y)","394240f7":"ypred = nb.predict(test_tfidf)","7400b5d3":"from sklearn.metrics import accuracy_score\naccuracy_score(test_y, ypred)","66accce8":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    text = text.replace('x', '')\n    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text","05443a05":"traindf['discourse_text'] = traindf['discourse_text'] .apply(clean_text)","7bb8a1e5":"traindf.head()","949c4e44":"from nltk.tokenize import sent_tokenize,word_tokenize\n# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100 \ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(traindf['discourse_text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","0f1130b4":"X = tokenizer.texts_to_sequences(traindf['discourse_text'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","4691f193":"Y = pd.get_dummies(traindf['discourse_type']).values","a46d37f7":"Y.shape","b90d4c14":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","491a411b":"from keras import Sequential\nfrom keras.layers import SpatialDropout1D","64e45aeb":"import seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline","015da56e":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(7, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nepochs = 20\nbatch_size = 64\n\nhistory = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n","67bb075d":"# Part 2","2c8915d4":"# Whats in this kernal\n*  Basic EDA\n* Data Cleaning\n* Baseline Model","71268304":"# Vectorizing and Training","3f8b580e":"# Importing required libraries # ","ee1d748f":"# MultinomialNB","c0b09e3d":"# Label Encoder"}}