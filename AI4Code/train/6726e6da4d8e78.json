{"cell_type":{"6a94708d":"code","868e94c1":"code","d60d2a6e":"code","73533064":"code","a32faf99":"code","e3f6f9b2":"code","797875ae":"code","dc424479":"code","e01dcedf":"code","40be68e4":"code","7e11c0a0":"code","9092db58":"code","4c8738f2":"code","20bb10e0":"code","1b217aed":"code","666f4407":"code","4ef29841":"code","ecbd1cc7":"code","0f70222a":"markdown","fa157405":"markdown","9cb520b1":"markdown","b7e27c2d":"markdown","8e0832a1":"markdown","c348037d":"markdown","fd80b2c1":"markdown","b595ecd9":"markdown","6b7b271a":"markdown","89d83b14":"markdown","6c107c4e":"markdown","09e77019":"markdown"},"source":{"6a94708d":"import tensorflow as tf\nimport tensorflow.keras.layers as tfl\nimport os\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","868e94c1":"path='..\/input\/human-faces'\ndef images_upload(path):\n    images=[]\n    for root,subfolders,files in os.walk(path):\n        for file in tqdm(files):\n            filename=root+os.sep+file\n            if filename.endswith('jpg') or filename.endswith('png'):\n                images.append(filename)\n    return images\nimages=images_upload(path)","d60d2a6e":"def convert_image_labels(images):\n    labels=[]\n    for i in tqdm(images):\n        i = cv2.imread(i)\n        i=cv2.cvtColor(i, cv2.COLOR_BGR2RGB)\n        res_i=cv2.resize(i,(128,128))\n        del i\n        labels.append(res_i)\n    return labels\n\n\ndef convert_image_inputs(images):\n    inputs=[]\n    for z in tqdm(images):\n        z = cv2.imread(z)\n        z=cv2.cvtColor(z,  cv2.COLOR_BGR2GRAY)\n        res_z=cv2.resize(z,(128,128))\n        del z\n        inputs.append(res_z)\n    return inputs\n\nlabels=convert_image_labels(images)\ninputs=convert_image_inputs(images)","73533064":"def show_labels(labels):\n    plt.figure(figsize=(10,10))\n    for i in range(9):\n        idx=np.random.randint(0,len(labels))\n        plt.subplot(3,3,i+1)\n        img=labels[idx]\n        plt.imshow(img)\n\nshow_labels(labels)","a32faf99":"def show_input(inputs):\n    plt.figure(figsize=(10,10))\n    for i in range(9):\n        idx=np.random.randint(0,len(inputs))\n        plt.subplot(3,3,i+1)\n        img=inputs[idx]\n        plt.imshow(img,cmap='gray')\nshow_input(inputs)\n","e3f6f9b2":"def images_compare(inputs,labels):\n    idx_new=np.random.randint(0,len(labels))\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1,2,1)\n    ax1.imshow(labels[idx_new])\n    ax2 = fig.add_subplot(1,2,2)\n    ax2.imshow(inputs[idx_new],cmap='gray')\n    plt.show()\nimages_compare(inputs,labels)","797875ae":"def build_model(inputsize=(128,128,1)):\n    input = tf.keras.Input(shape=(inputsize))\n\n    conv = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv1')(\n        input)\n    x=tfl.BatchNormalization()(conv)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv2')(\n        x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x1 = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv3')(\n        x)\n    x=tfl.BatchNormalization()(x1)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='MaxPool1')(x)\n\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv4')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv5')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x2 = tfl.Conv2D(128, (3, 3), padding=\"same\", strides=(1, 1),kernel_initializer='he_normal',\n                    name='Conv6')(x)\n    x=tfl.BatchNormalization()(x2)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), name='MaxPool2')(x)\n\n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv7')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv8')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x3 = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv9')(x)\n    x=tfl.BatchNormalization()(x3)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='MaxPool3')(x)\n\n    \n    x = tfl.Conv2D(512, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv10')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(512, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv11')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x4 = tfl.Conv2D(512, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv12')(x)\n    x=tfl.BatchNormalization()(x4)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.MaxPool2D(pool_size=(2, 2), strides=(2, 2), name='MaxPool4')(x)\n\n    x = tfl.Conv2D(1024, (3, 3), padding=\"same\",name='Conv13')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(1024, (3, 3), padding=\"same\",name='Conv14')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(1024, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv15')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2DTranspose(512, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x4], axis=3)\n    \n    x = tfl.Conv2D(512, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv16')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(512, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv17')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(512, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv18')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2DTranspose(256, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x3], axis=3)\n\n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv19')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv20')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(256, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv21')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2DTranspose(128, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x2], axis=3)\n\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv22')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv23')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(128, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv24')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\")(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n\n    x = tfl.concatenate([x, x1], axis=3)\n\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv25')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",kernel_initializer='he_normal', name='Conv26')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    x = tfl.Conv2D(64, (3, 3), padding=\"same\",name='Conv27')(x)\n    x=tfl.BatchNormalization()(x)\n    x=tfl.LeakyReLU()(x)\n    outputs = tfl.Conv2D(3, (1, 1), padding=\"same\", activation='sigmoid', name='Outputs')(x)\n    final_model = tf.keras.Model(inputs=input, outputs=outputs)\n    final_model.summary()\n    return final_model","dc424479":"def split_data(inputs,labels,test_size=0.2):\n    labels=np.array(labels)\n    inputs=np.array(inputs)\n    x_train, x_test, y_train, y_test = train_test_split(inputs, labels, test_size=test_size)\n    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.05)\n    return x_train, x_valid,x_test ,y_train, y_valid,y_test\n","e01dcedf":"def callbacks(patience=5):\n    checkpoint = tf.keras.callbacks.ModelCheckpoint('seg_model.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n    early=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=patience,min_delta=0.001)\n    callbacks_list=[checkpoint, early]\n    return callbacks_list","40be68e4":"mymodel=build_model()\n\nimg_file = '.\/model_arch.png'\ntf.keras.utils.plot_model(mymodel, to_file=img_file, show_shapes=True, show_layer_names=True)\n","7e11c0a0":"x_train, x_valid,x_test ,y_train, y_valid,y_test=split_data(inputs,labels,test_size=0.05)\nx_train, x_valid, x_test=x_train\/255.0,x_valid\/255.0,x_test\/255.0\ny_train, y_valid, y_test=y_train\/255.0,y_valid\/255.0,y_test\/255.0\n","9092db58":"mymodel.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.0, nesterov=True),loss=tf.keras.losses.mean_squared_error,metrics=['acc'])\nhist=mymodel.fit(x_train,y_train,batch_size=32,epochs=200,validation_data=(x_valid,y_valid),callbacks=callbacks())","4c8738f2":"plt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title(\"model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend([\"loss\",\"Validation Loss\"])\nplt.show()","20bb10e0":"plt.plot(hist.history[\"acc\"])\nplt.plot(hist.history['val_acc'])\nplt.title(\"model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\"])\nplt.show()","1b217aed":"pred=mymodel.predict(x_test)","666f4407":"\nplt.imshow(y_test[4])","4ef29841":"plt.imshow(pred[4])","ecbd1cc7":"plt.imshow(x_test[4],cmap='gray')","0f70222a":"<span style=\"font-family: Times New Roman, Times, serif;;\"> <span style=\"color:black;\"><span style=\"font-size:42px;\">  **Import Libraries**","fa157405":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Setting the Callbacks<\/b>\n\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">Modelcheck point was used to save the best parameters while the Earlystopping callback was used to prevent from the model to be trained if there is no additional change.<\/p>","9cb520b1":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Training The Data<\/b>\n    <p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\"> I will use SGD as the optimizr and MSE as the loss function.\n<\/p>","b7e27c2d":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Exploring the data<\/b>\n\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">In this section, I will show random RGB and grayscale images to test the labels and inputs.<\/p>","8e0832a1":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Predications<\/b>\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">Next I will compare between the original image with the model predication.<\/p>","c348037d":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b> Introduction<\/b><\/div>\n\n<p style=\"padding: 10px;color:black;font-size:140%;font-family: Times New Roman, Times, serif;\"> The goal of this project is to <b>colorize<\/b> black-and-white images, which was a difficult task a few years ago but has recently become easier with the rise of deep learning. \nColorization can be done quite well using Photoshop, but it requires little knowledge. Furthermore, depending on the number of images you have, it can take hours or even days. To achieve the desired look, a wide variety of color combinations are required. The first step in colorizing images is to convert them from RGB to grayscale, which will then be used as input(grayscale) and output(RGB). \nUsing <b>Unet<\/b> architecture , the model learns how to colorize images. I'll be happy for any feedback or comment and feel free to send me any questions about the model! thanks in advance&#128512;<\/p>\n\n\n\n","fd80b2c1":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Accuracy\/Loss Plot<\/b>\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">To learn how well the model is learning to colorize images, loss and accuracy plots will be used.<\/p>","b595ecd9":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Loading Data & Data Preprocessing<\/b>\n\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">In this section, I'll use the OS library to upload various face images. This section will also include converting RGB images to Grayscale with the OpenCV library. The RGB images will serve as labels, and the Grayscale images will serve as input.<\/p>","6b7b271a":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Conclusions<\/b>\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">The model performed poorly when asked to colorize a grayscale image. Many papers use a LAB image instead of an RGB image because two numbers (a,b) are easier to predict than three. I tried it as well, but the results are nearly identical. Even if more images are used, it will be extremely difficult for any model to predict the color of the eyes, hair, and skin tone. Using GAN as a solution to this problem is an ideal option. With GAN, the model can predict whether an image is \"real\" in terms of colorization.\nUsing L1 as the loss function without GAN is problematic because when attempting to guess what color is correct, the model frequently uses colors such as \"gray\" or \"brown\" because when it is unsure which color is best, it takes the average and uses these colors to reduce the L1 loss as much as possible.\n.<\/p>","89d83b14":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Constructing The Model<\/b>\n\n\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">As mentioned in the beginning this model is an Unet model. The backbone is based on VGG with skip connections. <\/p>","6c107c4e":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Data Insights<\/b>\n\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">Although most of the images are in RGB format there are few that are in Gray scale format which can influence on the result so that should be taken into account.<\/p>","09e77019":" <div style=\"color:black;\n           display:fill;\n           border-radius:5px;\n           font-size:300%;\n           font-family: Times New Roman, Times, serif;\n    letter-spacing:0.5px\"><b>Split the data<\/b>\n\n<p style=\"padding: 10px;color:black;font-size:45%;font-family: Times New Roman, Times, serif;\">As mentioned in the beginning this model is an Unet model. The backbone is based on VGG with skip connections. \nI'm going to split the data into train, test, and validation using the train test split model from sklearn. Since I have 7000 images, I will use the majority of them for the train because training a colorizing model is not an easy task.\n<\/p>"}}