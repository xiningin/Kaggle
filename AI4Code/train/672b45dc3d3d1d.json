{"cell_type":{"c5f3df52":"code","7e9f3ba7":"code","c3c8cd1a":"code","f483468e":"code","47c79cf4":"code","d9a97adb":"code","6a913c7e":"code","d49ff631":"code","58e32b36":"code","e077cc90":"code","428b4c19":"code","73cb6a50":"markdown","5f2ee03a":"markdown","1b6e4021":"markdown","877e944b":"markdown","ff46b814":"markdown","38d5adfe":"markdown"},"source":{"c5f3df52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport nltk\nnltk.download('punkt')\nimport os\nimport re\nimport io\nimport string\nimport pandas as pd\nfrom nltk import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import text\nfrom keras.models import Sequential\nfrom keras.layers import  Activation\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate\n\n\n","7e9f3ba7":"training_data = pd.read_csv('\/kaggle\/input\/natural-language-processing-with-disaster-tweets\/kaggle nlp\/train.csv')\ntesting_data = pd.read_csv('\/kaggle\/input\/natural-language-processing-with-disaster-tweets\/kaggle nlp\/test.csv')\n\ntraining_data.head(5)","c3c8cd1a":"train_X = training_data['text']\ntrain_Y = training_data['target'] \n\ntest_X = testing_data['text']\n","f483468e":"def preprocess(raw_text):\n    #regex found in: https:\/\/stackoverflow.com\/questions\/28077049\/regex-matching-emoticons\n    smiley_regex = '(:\\w+:|<[\/\\]?3|[()\\\\D|*$][-^]?[:;=]|[:;=B8][-^]?[3DPp@$*\\)(\/|])(?=\\s|[!.?]|$)'\n    #replacement character for emoticons\n    replace_char_regex = '\u00a7'\n    #takes raw text and replaces <br\/> with a space\n    raw_text = raw_text.replace('<br \/>',' ')\n    #find all emoticons, defined in smiley_regex and save them in variable emoticons\n    emoticons = re.findall(smiley_regex, raw_text)\n    #replace all found emoticons in the text with the replacement char, defined in replace_char_regex\n    text_replaces = re.sub(smiley_regex, replace_char_regex , raw_text)\n    \n    #make all text symbols lower case\n    lower_text = text_replaces.lower()\n    #removes all punctuations from the text    \n    entry_no_punct = lower_text.translate(str.maketrans('', '', string.punctuation))\n    \n    #replace the replace_char_regex with the original emoticons again on the right possitions in text\n    for emoticon in emoticons:\n        entry_no_punct = entry_no_punct.replace(replace_char_regex, emoticon, 1)\n\n    #replace all digits with '#'\n    entry_hash_num = ''.join([s if not s.isdigit() else '#' for s in entry_no_punct])\n\n    return entry_hash_num\n\n\n\ntraining_data['text'] = training_data['text'].apply(lambda x: preprocess(x))\ntesting_data['text'] = testing_data['text'].apply(lambda x: preprocess(x))\n\ntrain_X = training_data['text']\ntrain_Y = training_data['target'] \n\ntest_X = testing_data['text']","47c79cf4":"tokenizer = text.Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(train_X)\n\n# converting each dataset into a vector of numbers \ntrain_set = tokenizer.texts_to_sequences(train_X)\ntest_set = tokenizer.texts_to_sequences(test_X)\n\n# converting each labels subset into a numpy array\ntrain_labels = train_Y.to_numpy()\n\n\n# making our data consistent and un changble by making the vectors have a fixed max length\ntrain_data = keras.preprocessing.sequence.pad_sequences(train_set, maxlen=350)\ntest_data = keras.preprocessing.sequence.pad_sequences(test_set, maxlen=350)","d9a97adb":"def define_vanilla_model(length, vocab_size):\n    input=Input(shape=(length,))\n    embedding=Embedding(vocab_size, 150)(input)\n    conv = Conv1D(filters=32, kernel_size=3, activation='relu')(embedding)\n    pool = MaxPooling1D(pool_size=3)(conv)\n    flatten = Flatten()(pool)\n    dense1 = Dense(10, activation='relu')(flatten)\n    \n    #second dense layer to get a accuracy representation between 0 and 1\n    out = Dense(1, activation='sigmoid')(dense1)\n\n    model = Model(input, outputs=out)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    plot_model(model, show_shapes=True, to_file='define_model.png')\t\n    return model    ","6a913c7e":"vanilla_model = define_vanilla_model(350,10000)\nvanilla_model.fit(train_data, train_labels, epochs=10, batch_size=16)\n","d49ff631":"test_Y = vanilla_model.predict(test_data)","58e32b36":"def predictions(tweets):\n    predict = []\n    temp_tweets = np.array(tweets)\n    for i in range(len(tweets)):\n        if temp_tweets[i] >= 0.7:\n            predict.append(1)\n        else:\n            predict.append(0)\n    return predict","e077cc90":"import seaborn as sns\npredict = predictions(test_Y)\noutput = pd.DataFrame({'id': testing_data.id, 'target': predict})\noutput.to_csv('output.csv', index=False)","428b4c19":"plotting = pd.DataFrame(predict,columns=[\"target\"])\nsns.countplot(x=\"target\",data=plotting)","73cb6a50":"## Loading the training and testing datasets ","5f2ee03a":"## Preprocessing the data ","1b6e4021":"# Seperating the data \n","877e944b":"## Fitting the model ","ff46b814":"## Predictions \n","38d5adfe":"## Creating the model\nfor this competition i used CNN neural networks, which are usaally used in image classification, but as it's gonna turn out, it can be used for sentiment analysis and machine learning text classification tasks.\n"}}