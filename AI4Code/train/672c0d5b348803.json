{"cell_type":{"074c346d":"code","46826eb0":"code","170b0db2":"code","b5c5e0b9":"code","be43cabe":"code","7f98152e":"code","e3201203":"code","56179765":"code","ec5ac5bd":"code","44289da2":"code","297350a4":"code","4c39bd7a":"code","47026dcc":"code","d1396e04":"code","750e223c":"code","563a1132":"code","1b7b9c4e":"markdown","ea206910":"markdown","3f5593d3":"markdown","cdcb9cc2":"markdown","b3467191":"markdown","6934add9":"markdown","3765d7fa":"markdown","a33db278":"markdown"},"source":{"074c346d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport time\nimport tensorflow as tf\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.layers import GRU, Dense, Activation, Dropout, CuDNNGRU, concatenate, Input\nfrom keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","46826eb0":"train = pd.read_csv('..\/input\/training_set.csv')\ntrain.head()","170b0db2":"ss = StandardScaler()\ntrain[['mjd', 'flux', 'flux_err']] = ss.fit_transform(train[['mjd', 'flux', 'flux_err']])\ntrain.head()","b5c5e0b9":"def get_timeseries_by_column_name(train, passband, column_name):\n    train_passband = train[train.passband == passband]\n    ## This is where magic happens\n    train_column_timeseries = train_passband.groupby(['object_id', 'passband'])[column_name].apply(lambda df: df.reset_index(drop=True)).unstack()\n    train_column_timeseries.fillna(0, inplace=True)\n    train_column_timeseries  = train_column_timeseries.reset_index()\n    train_column_timeseries['feature_id'] = column_name\n    return train_column_timeseries\n\ndef get_timeseries(train, passband):\n    df = pd.concat([get_timeseries_by_column_name(train, passband, column_name) for column_name in ['mjd', 'flux', 'flux_err', 'detected']])\n    df = df.sort_values(['object_id', 'passband'])\n    return df","be43cabe":"# One example for flux where passband=0\nget_timeseries_by_column_name(train, passband=0, column_name='flux').head()","7f98152e":"# for each passband we get input rows for mjd, flux, flux_err, detected\ntrain_list = []\nfor passband in range(0, 6):\n    train_passband =  get_timeseries(train, passband=passband)\n    train_list.append(train_passband)\n    print(\"train_list[%d]\" % passband, train_list[passband].shape)","e3201203":"num_features = len(['mjd', 'flux', 'flux_err', 'detected'])\ndrop_features= [ \"feature_id\", \"object_id\", \"passband\"]\nX_train_list = []\nfor passband in range(0, 6):\n    num_columns = len(train_list[passband].columns) - len(drop_features)\n    print(\"num_columns[%d]:\" % passband, num_columns)\n    X_train_list.append(train_list[passband].drop(drop_features, axis=1).values.reshape(-1, num_features, num_columns).transpose(0, 2, 1))\n    print(\"X_train_list[%d].shape:\" % passband, X_train_list[passband].shape)","56179765":"meta_train = pd.read_csv('..\/input\/training_set_metadata.csv')\nmeta_train.head()","ec5ac5bd":"# List all classes\nclasses = sorted(meta_train.target.unique())\nclasses","44289da2":"class_map = dict()\nfor i,val in enumerate(classes):\n    class_map[val] = i\nclass_map","297350a4":"# We only need target for earch object_id, so using feature_id=='flux'\nmerged_train = train_list[0][train_list[0].feature_id == 'flux'].merge(meta_train, on='object_id', how='left')\nmerged_train = merged_train.drop(['ra',\t'decl',\t'gal_l',\t'gal_b',\t'ddf',\t'hostgal_specz',\t'hostgal_photoz',\t'hostgal_photoz_err',\t'distmod',\t'mwebv'], axis=1)\nmerged_train.head()","4c39bd7a":"targets = merged_train.target\ntarget_map = np.zeros((targets.shape[0],))\ntarget_map = np.array([class_map[val] for val in targets])\nY = to_categorical(target_map)\nY.shape","47026dcc":"def multi_weighted_logloss(y_ohe, y_p):\n    \"\"\"\n    @author olivier https:\/\/www.kaggle.com\/ogrellier\n    multi logloss for PLAsTiCC challenge\n    \"\"\"\n    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n    # Transform to log\n    y_p_log = np.log(y_p)\n    # Get the log for ones, .values is used to drop the index of DataFrames\n    # Exclude class 99 for now, since there is no class99 in the training set \n    # we gave a special process for that class\n    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n    # Get the number of positives for each class\n    nb_pos = y_ohe.sum(axis=0).astype(float)\n    # Weight average and divide by the number of positives\n    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n    y_w = y_log_ones * class_arr \/ nb_pos    \n    loss = - np.sum(y_w) \/ np.sum(class_arr)\n    return loss\n\n# https:\/\/www.kaggle.com\/c\/PLAsTiCC-2018\/discussion\/69795\ndef mywloss(y_true,y_pred):  \n    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)\/wtable))\n    return loss\n\ndef plot_loss_acc(history):\n    plt.plot(history.history['loss'][1:])\n    plt.plot(history.history['val_loss'][1:])\n    plt.title('model loss')\n    plt.ylabel('val_loss')\n    plt.xlabel('epoch')\n    plt.legend(['train','Validation'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['acc'][1:])\n    plt.plot(history.history['val_acc'][1:])\n    plt.title('model Accuracy')\n    plt.ylabel('val_acc')\n    plt.xlabel('epoch')\n    plt.legend(['train','Validation'], loc='upper left')\n    plt.show()\n","d1396e04":"n_classes = len(classes)\n\ndef weight_variable(shape, name=None):\n    return np.random.normal(scale=.01, size=shape)\n\ndef build_model():\n    def basic_layer(input_):\n        output = GRU(64,\n                     kernel_initializer=weight_variable,\n                     dropout=0.5,\n                     recurrent_dropout=0.5,\n                     return_sequences=True)(input_)\n        output = Dropout(0.5)(output)\n        output = GRU(32, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)(output)\n        output = GRU(16, dropout=0.5, recurrent_dropout=0.5)(output)\n        output = Dense(32)(output)\n        return output\n    \n    # Keras functional API supports multiple inputs!\n    input0 = Input(shape=(None, num_features), dtype='float32', name='passband0')\n    input1 = Input(shape=(None, num_features), dtype='float32', name='passband1')\n    input2 = Input(shape=(None, num_features), dtype='float32', name='passband2')\n    input3 = Input(shape=(None, num_features), dtype='float32', name='passband3')\n    input4 = Input(shape=(None, num_features), dtype='float32', name='passband4')\n    input5 = Input(shape=(None, num_features), dtype='float32', name='passband5')\n    \n    merged_output = concatenate([basic_layer(input0),basic_layer(input1),basic_layer(input2),basic_layer(input3),basic_layer(input4),basic_layer(input5)])\n    merged_output = Dense(64)(merged_output)\n    final_output = Dense(len(classes), activation='softmax')(merged_output)\n    return Model(inputs=[input0, input1, input2, input3, input4, input5], outputs=[final_output])","750e223c":"y_count = Counter(target_map)\nwtable = np.zeros((len(classes),))\nfor i in range(len(classes)):\n    wtable[i] = y_count[i] \/ target_map.shape[0]","563a1132":"batch_size = 512\ny_map = target_map\ny_categorical = Y\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nstart = time.time()\nclfs = []\noof_preds = np.zeros((len(X_train_list[0]), len(classes)))\nepochs = 5000\n\nfor fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n    checkPoint = ModelCheckpoint('.\/keras.model',monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n    X0 = X_train_list[0]\n    X1 = X_train_list[1]\n    X2 = X_train_list[2]\n    X3 = X_train_list[3]\n    X4 = X_train_list[4]\n    X5 = X_train_list[5]\n    x_train0, x_train1, x_train2, x_train3, x_train4, x_train5, y_train = X0[trn_], X1[trn_], X2[trn_], X3[trn_], X4[trn_], X5[trn_], y_categorical[trn_]\n    x_valid0, x_valid1, x_valid2, x_valid3, x_valid4, x_valid5, y_valid = X0[val_], X1[val_], X2[val_], X3[val_], X4[val_], X5[val_], y_categorical[val_]\n    \n    model = build_model()    \n    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)#\n    early_stopping = EarlyStopping(monitor='loss', patience=30, verbose=1)\n    model.compile(loss=mywloss, optimizer=optimizer, metrics=['accuracy'])\n    history = model.fit([x_train0, x_train1, x_train2, x_train3, x_train4, x_train5], y_train,\n                    validation_data=[[x_valid0, x_valid1, x_valid2, x_valid3, x_valid4, x_valid5], y_valid], \n                    epochs=epochs,\n                        batch_size=batch_size,\n                    shuffle=True,verbose=1,callbacks=[checkPoint, early_stopping])      \n    plot_loss_acc(history)\n    \n    print('Loading Best Model')\n    model.load_weights('.\/keras.model')\n    # # Get predicted probabilities for each class\n    oof_preds[val_, :] = model.predict([x_valid0, x_valid1, x_valid2, x_valid3, x_valid4, x_valid5],batch_size=batch_size)\n    print(multi_weighted_logloss(y_valid, model.predict([x_valid0, x_valid1, x_valid2, x_valid3, x_valid4, x_valid5],batch_size=batch_size)))\n    clfs.append(model)\n    \nprint('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))\n\nelapsed_time = time.time() - start\nprint(\"elapsed_time:\", elapsed_time)","1b7b9c4e":"## Ideas for improvement\n- Add one more input for metadata.\n- Derive some features as timeseries and feed into the RNN?","ea206910":"## Transform for RNN input\n Input to RNN is (batch_size, timesteps, input_dim).\n - batch_size is batch_size :)\n - timesteps is 75 for passband0,\n - input_dim is number of feature the timeseries has, here we have 4 ['mjd', 'flux', 'flux_err', 'detected'].","3f5593d3":"## Standardize\nStandardize input before we process data. NN likes properly scaled data.","cdcb9cc2":"As you can see the shapes above, each passband __may have__ different length of time series. eg) passband0 has len = 75, but passband1 = 61.","b3467191":"## Keras + RNN(GRU) to handle passbands as timeseries\nMost of the kernels I saw here were not treating passbands as timeseries, but calculating meta feature such as mean\/min\/max of flux.\nHere I'm making simple kernel which uses all the timeseries data as input of RNN(GRU).\nI hope this helps your exploration. I'd appreciate your insights and feedback.","6934add9":"## Magical transformation\nAs you see above, train data can be grouped by object_id, passband, list of [flux] order by mjd.\n\nMeaning 1 train input row would be<br>\n``\nobject_id = 615, passband=1, flux0 = 0.228287, flux1 = xxx, ... flux72 = xxx\n``\n\nSimilary we can have similar row for flux_err.<br>\n``\nobject_id = 615, passband=1, flux_err0 = 0.0.005226, flux_err = xxx, ... flux_err = xxx\n``\n\nThe following methods do the transformation!","3765d7fa":"## Merge target value from metadata","a33db278":"## RNN(GRU) begins here :)\nNote that\n- I'm using Keras functional API which supports multples inputs.\n- I intentionally __chose small values for GRU parameters and epochs for demo purpose__, as GRU is slow to train."}}