{"cell_type":{"ddc82035":"code","8d643be3":"code","9286d747":"code","5b461c27":"code","d7b45f3f":"code","56218420":"code","d5a55656":"code","066ce339":"code","586bf8d7":"code","8f617d85":"code","757e06f7":"code","f98760fe":"code","4af60f01":"code","1c3170ef":"code","4e498c37":"code","285cdf11":"code","b8f3b326":"code","2e9069ca":"code","e308c974":"code","0887fa92":"code","e5d9cf86":"code","9b205014":"code","cdf92c15":"code","b11a3cde":"code","344cbff8":"code","b2bf1bb1":"code","13743e5a":"code","22f8c56b":"code","c175762b":"code","4c1ceff1":"code","398d2d91":"code","00274f98":"code","1b1433df":"code","6abc2503":"code","60f25d29":"code","ebfe882e":"code","ba5b3aa3":"code","27f32f05":"code","91cff680":"code","7bf0365e":"code","d0cab0cc":"code","cd6a44ec":"code","34e0452b":"code","87be062c":"code","93f48619":"code","5976a84e":"code","44b19cc6":"code","caf7c040":"code","62a33842":"code","ff927bd2":"code","c98f9aa5":"code","b0dfce03":"code","44380dd9":"code","0647b5bd":"code","44a1fba0":"code","5c14c1bc":"code","6410e860":"code","b7db66b0":"code","add83f4c":"code","cb485672":"code","c9363158":"code","18eea322":"code","f946f746":"code","8c11bcca":"code","c0916b7a":"code","c5256410":"code","995c41ce":"code","d69d614f":"code","e70e9301":"code","98047002":"code","671303f2":"code","5066478b":"code","3cd38389":"code","85d496d0":"code","f22796ec":"code","ad2c7282":"code","6d807e98":"code","efabd522":"code","a2235bad":"code","2841fbb1":"code","717424f0":"code","ecbfd3f0":"code","17da57a2":"code","25b38feb":"code","ba037733":"code","ca9acd40":"code","9ba27225":"code","af9656e5":"code","c1dc2551":"code","14c666fd":"code","0543aa7f":"code","740554fc":"code","4dd4a4aa":"code","b37d50e6":"code","426e425a":"code","86aed8a8":"code","3d6e8fa5":"code","f073942a":"code","15a8f52a":"code","7773b9cd":"code","cb35cf9e":"code","fe2d4d2b":"code","9ab01306":"code","215f8874":"code","c642ea51":"code","3b67bf65":"code","9ce73094":"code","e5eb433e":"code","00acb605":"code","d9b805eb":"code","f8898a26":"code","e0135a5f":"code","e483fee1":"code","6bb360f2":"code","68c89b20":"code","cc3f4a7d":"code","21b835c8":"code","3dcecf0b":"code","a77f946f":"code","d2a0a56b":"code","ceaca0a6":"code","cb4acc7c":"code","732afcb7":"code","25941cb0":"code","7686507c":"markdown","d917817c":"markdown","f3c7ee34":"markdown","dbe2c32c":"markdown","22f6ca7c":"markdown","7a8682fb":"markdown","811017ec":"markdown","8ea9ebd6":"markdown","fd4d9372":"markdown","efb545c2":"markdown","788ca7b6":"markdown","242387ae":"markdown","39297178":"markdown","b3391ae2":"markdown","c4d65994":"markdown","ef38cc40":"markdown","07cf747a":"markdown","83934bb2":"markdown","1289b91b":"markdown","bdbbf3f4":"markdown","f651fa07":"markdown","73155260":"markdown","93f1696d":"markdown","8c49aae7":"markdown","2740dc75":"markdown","e17cb995":"markdown","ca189293":"markdown","ad0578de":"markdown","e12f1f9f":"markdown","057430e8":"markdown"},"source":{"ddc82035":"#Python libraries\n#Classic,data manipulation \nimport numpy as np\nimport pandas as pd\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_rows\",None)\nfrom sklearn import preprocessing \nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score,accuracy_score\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import StackingClassifier\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler \nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\n# Dataprep\n# Modeling \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom imblearn.combine import SMOTEENN #resampling\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n#Import pipeline to allow you to do multiple steps at once\nfrom imblearn.pipeline import Pipeline, make_pipeline\n","8d643be3":"df= pd.read_csv('..\/input\/diabetes-health-indicators-dataset\/diabetes_012_health_indicators_BRFSS2015.csv')\n","9286d747":"df.info()","5b461c27":"df.shape","d7b45f3f":"df.sample(100)","56218420":"df.columns","d5a55656":"# to rename the some columns important of dataset\ndf.rename(columns={'Diabetes_012': 'Diabetes_Type'}, inplace=True)\ndf.head()","066ce339":"df.isna().sum() # No missing value","586bf8d7":"df.BMI.unique()","8f617d85":"df.GenHlth.unique()","757e06f7":"df.MentHlth.unique()","f98760fe":"df.PhysHlth.unique()","4af60f01":"df.Age.unique()","1c3170ef":"df.Education.unique()","4e498c37":"df.Income.unique()","285cdf11":"df.Diabetes_Type.unique()","b8f3b326":"Diabetes=df['Diabetes_Type']\nDiabetes.value_counts()","2e9069ca":"df['Diabetes_Type'].replace({2.0: 1.0},inplace = True)","e308c974":"Diabetes=df['Diabetes_Type']\nDiabetes.value_counts()","0887fa92":"corr = df.corr()\nfig, ax = plt.subplots(figsize=(25,15)) \nsns.heatmap(corr,annot=True, cmap = \"Blues\", linewidth = 0.30)\nplt.title(\"Correlation matrix of features\")\nplt.show()","e5d9cf86":"df['Diabetes_Type'] = df['Diabetes_Type'].astype('int')","9b205014":"df['Diabetes']=df['Diabetes_Type']","cdf92c15":"df['Diabetes'] = df['Diabetes_Type'].map({0:'No Diabetes', 1:'Diabetes'})","b11a3cde":"Diabetes=df['Diabetes_Type']\nDiabetes.value_counts()","344cbff8":"diabetes_bp = df.groupby(['Diabetes_Type', 'HighBP']).size().reset_index(name = 'Count')\nprint(diabetes_bp)","b2bf1bb1":"df.columns\n","13743e5a":"df['GH']=df['GenHlth']","22f8c56b":"df['GH'] = df['GH'].map({1:5, 2:4 ,3:3 ,4:2 , 5:1})","c175762b":"df.head()","4c1ceff1":"df.isna().sum()","398d2d91":"smaller_df=df.loc[:,['Diabetes_Type', 'HighBP', 'HighChol', 'BMI', 'HeartDiseaseorAttack','PhysActivity', 'GenHlth','MentHlth','DiffWalk', 'Age']]\nsmaller_df1=df.loc[:,['Diabetes_Type', 'HighBP', 'HighChol', 'BMI', 'HeartDiseaseorAttack','PhysActivity', 'GenHlth','MentHlth','DiffWalk', 'Age']]                  \n                \n","00274f98":"df_train, df_test = train_test_split(smaller_df, test_size=0.20, random_state=0)\ndf_train, df_val = train_test_split(df_train, test_size=0.20, random_state=0)\n\nx,y = df_train.drop(['Diabetes_Type'],axis=1),df_train['Diabetes_Type']\n","1b1433df":"# The figure above display the correlation between the features and the target, for this i choose only these features\nx_train1,y_train = df_train.drop(['Diabetes_Type'],axis=1),df_train['Diabetes_Type']\nx_val1,y_val = df_val.drop(['Diabetes_Type'],axis=1),df_val['Diabetes_Type']\nx_test1,y_test= df_test.drop(['Diabetes_Type'],axis=1),df_test['Diabetes_Type']","6abc2503":"print(x_train1.shape)\nprint(x_val1.shape)\nprint(x_test1.shape)","60f25d29":"#For smaller df1\nscaler = MinMaxScaler()\nscaler.fit(x_train1)\n\nx_train = scaler.transform(x_train1)\nx_test= scaler.transform(x_test1)\nx_val=scaler.transform(x_val1)","ebfe882e":"# create a dict to store the scores of each model\nmodels_evalutions = {'Model':[],\n                     'Accuracy':[],\n                     'Precision':[],\n                     'Recall':[], \n                     'F1 score':[]}","ba5b3aa3":"#model_names = [\"knn_final\", \"lr_final\",\"Dt_final\",\"rf_final\"]\n#model_vars = [eval(n) for n in model_names]\n#model_list = list(zip(model_names, model_vars))","27f32f05":"x0_train=x_train.copy()\nx0_val=x_val.copy()","91cff680":"# Using KNN (smaller df1) train on training set, and Test on testing set \nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(x0_train, y_train)\nprint(\"The score for kNN without cross val and without using smote is\")\nprint(\"Training set: {:6.2f}%\".format(100*knn.score(x0_train, y_train)))\nprint(\"Validation set: {:6.2f}%\".format(100*knn.score(x0_val, y_val)))\nprint(\"Test set: {:6.2f}%\".format(100*knn.score(x_test, y_test)))","7bf0365e":"#test the baseline model for smaller df\n#prediction\nval_pred=knn.predict(x0_val)\n#Accuracy\nconfusion_hard = confusion_matrix( y_val, val_pred)\naccuracy = accuracy_score(y_val , val_pred)\nprecision = precision_score(y_val , val_pred)\nrecall = recall_score(y_val , val_pred)\nf1 = f1_score(y_val,val_pred) \nprint('\\nKNN Accuracy for validation set=: {0:.4f}, \\nprecision: {1:.4f}, \\nrecall: {2:.4f},\\\n\\nF1: {3:.4f}'.format(accuracy, precision, recall, f1))","d0cab0cc":"cm = confusion_matrix(y_val, val_pred)\nclass_label = [\"No_Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\",  cmap = \"Blues\" )\nplt.title('Confusion matrix for knn befoes balancing two classes and using cross validation', fontsize = 20); # title with fontsize 20","cd6a44ec":"kf = KFold(n_splits=5, random_state=42, shuffle=True)","34e0452b":"knn = KNeighborsClassifier(n_neighbors=5)\naccuracy_score1=[]\nf1_score1 = []\npercision_score1 = []\nrecall_score1 = []\n\n# enumerate the splits and summarize the distributions\nfor train_ix, test_ix in kf.split(x, y):\n    # select rows\n    train_x, test_X = x.iloc[train_ix], x.iloc[test_ix]\n    train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]\n    #print(train_X.shape, train_y.shape)\n    #print(test_X.shape, test_y.shape)\n    oversample = SMOTE(random_state = 0)\n    train_x, train_y = oversample.fit_resample(train_x, train_y)\n    #scores= cross_val_score(knn, test_X, test_y, cv=5, scoring='accuracy') \n    knn.fit(train_x, train_y)\n    y_pred =knn.predict(test_X)\n    #score = f1_score(test_y,y_pred)\n    accuracy_score1.append(metrics.accuracy_score(test_y, y_pred))\n    percision_score1.append(metrics.precision_score(test_y, y_pred))\n    recall_score1.append(metrics.recall_score(test_y, y_pred))\n    f1_score1.append(metrics.f1_score(test_y, y_pred))\n\n\n    \nprint(\"kNN accuracy score: \\t\")\nprint(sum(accuracy_score1) \/ len(accuracy_score1))\nprint(\"----------------\")    \nprint(\"kNN score: \\t\")\nprint(sum(f1_score1) \/ len(f1_score1))\nprint(\"----------------\")\nconf_mat3 = confusion_matrix(test_y, y_pred)\nprint(\"kNN confusion matrix: \\n\",conf_mat3)\nprint(\"----------------\")\nprint(\"KNN precision score\")\nprint(sum(percision_score1) \/ len(percision_score1))\nprint(\"----------------\")\nprint(\"KNN recall_score\")\nprint(sum(recall_score1) \/ len(recall_score1))\nprint(\"----------------\")\n#print score for all evaluations\n","87be062c":"models_evalutions['Model'].append(\"KNN after balance our target's labels\")\nmodels_evalutions['Accuracy'].append(accuracy_score(test_y, y_pred))\nmodels_evalutions['Precision'].append(precision_score(test_y, y_pred))\nmodels_evalutions['Recall'].append(recall_score(test_y, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(test_y, y_pred))","93f48619":"knn_final = knn.n_neighbors","5976a84e":"cm =confusion_matrix(test_y, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for knn with cross validation and Smote', fontsize = 20); # title with fontsize 20","44b19cc6":"x2_train=x_train.copy()\nx2_val=x_val.copy()","caf7c040":"#Before balance classes\nlr=LogisticRegression()\nprams ={\"penalty\": [ 'l1', 'l2'],\n       \"C\": [0.5 , 0.7,0.8 , 1 , 2.0 , 3.0]}\n\nlr_cv= GridSearchCV(lr , param_grid=prams, n_jobs=-1 ,cv=10)\nlr_cv.fit(x2_train , y_train )\n\nprint(\"Best params: \", lr_cv.best_params_)\nprint(\"Best estimator: \" ,lr_cv.best_estimator_)\nprint(\"Best score: \", lr_cv.best_score_)\n\nprint(\"Training Score before balance the labels:\",lr_cv.score(x2_train, y_train))\nprint(\"Validation Score before balance the labels:\",lr_cv.score(x2_val, y_val))\n\ny_pred = lr_cv.predict(x2_val)\nprint(\"\\nLogistic Regression Accuracy=\",accuracy_score(y_val, y_pred))\nprint(\"Logistic Regression F1 score=\",f1_score(y_val, y_pred))","62a33842":"lr_cv.best_score_","ff927bd2":"lr_final = lr_cv.best_estimator_\nlr_final","c98f9aa5":"lr_final.score(x2_train , y_train)","b0dfce03":"lr_final.score(x_val , y_val)","44380dd9":"#experiment2-1 with random over sampling\nlg1 = LogisticRegression(C=1,penalty=\"l2\")\n\n# randomly oversample positive samples\nROS = RandomOverSampler(random_state=42)\n\nX_tr_rs, y_tr_rs = ROS.fit_resample(x2_train, y_train)\n\nlg1.fit(X_tr_rs, y_tr_rs)\nprint(\"Training Score after balance the labels (RandomOverSampler):\",lg1.score(X_tr_rs, y_tr_rs))\nprint(\"Validation Score after balance the labels (RandomOverSampler)\",lg1.score(x_val, y_val))\n#model_eval(model3,X_test_std,y_test)","0647b5bd":"#experiment2-2 whith random under sampling\nlg2 = LogisticRegression(C=1,penalty=\"l2\")\n\nRUS = RandomUnderSampler(random_state=42)\n\nX_tr_us, y_tr_us = RUS.fit_resample(x2_train, y_train)\n\nlg2.fit(X_tr_us, y_tr_us)\nprint(\"Training Score after balance the labels (RandomUnderSampler)\",lg2.score(X_tr_us, y_tr_us))\nprint(\"Validation Score after balance the labels (RandomUnderSampler):\",lg2.score(x2_val, y_val))\n#model_eval(lg2,X_test_std,y_test)","44a1fba0":"#experiment2-3 whith balanced weighted classes sampling\nlg3 = LogisticRegression(C=1,penalty=\"l2\",class_weight='balanced')\n\nlg3.fit(x2_train, y_train)\n#y_pred=lg3.predict(x2_val)\nprint(\"Training Score after Balanced class weights Logistic Regression\",lg3.score(x2_train, y_train))\nprint(\"Validation Score after Balanced class weights Logistic Regression:\",lg3.score(x2_val, y_val))\n","5c14c1bc":"#experiment2-4 whith Smote\nlg = LogisticRegression(C=1,penalty=\"l2\")\n\nSMT = SMOTE(random_state=42)\n\nX_tr_smt, y_tr_smt = SMT.fit_resample(x2_train, y_train)\n\nlg.fit(X_tr_smt, y_tr_smt)\ny_pred=lg.predict(x2_val)\n\nprint(\"Training Score after balance the labels (Smote):\",lg.score(X_tr_smt, y_tr_smt))\nprint(\"Validation Score after balance the labels (Smote):\",lg.score(x2_val, y_val))\n#model_eval(model3,X_test_std,y_test)","6410e860":"# classification report for logisitic\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","b7db66b0":"models_evalutions['Model'].append(\"LogisticRegression with Smote\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","add83f4c":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for LogisticRegression ', fontsize = 20); # title with fontsize 20","cb485672":"x3_train=x_train.copy()\nx3_val=x_val.copy()","c9363158":"# finding the best parameters for the decision tree\nparam_grid = {'criterion' :['gini', 'entropy'],'max_depth': [4, 6, 10, 12]}\n\ntree_clas = DecisionTreeClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=tree_clas, param_grid=param_grid, cv=5, verbose=True, scoring = 'f1')\ngrid_search.fit(x3_train, y_train)\n\nprint(grid_search.best_estimator_)","18eea322":"Dt_final = grid_search.best_estimator_\nDt_final","f946f746":"# trying with entropy, since it didn't show in the previose step\ntree = DecisionTreeClassifier(criterion='entropy',\n                                     max_depth=10,\n                                     max_features='auto',\n                                     random_state=42)\n\ntree.fit(x3_train,y_train)\n\nprint(\"Training Score In Decision Tree Classification:\",tree.score(x3_train, y_train))\nprint(\"Validation Score In Decision Tree: Classification\",tree.score(x2_val, y_val))\ny_pred = tree.predict(x3_val)\n\nprint(\"DT Accuracy=\",accuracy_score(y_val, y_pred))\nprint(\"DT F1 score=\",f1_score(y_val, y_pred))","8c11bcca":"# classification report for Decision Tree \nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","c0916b7a":"models_evalutions['Model'].append(\"Decision Tree Classification\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","c5256410":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Decision Tree classification', fontsize = 20); # title with fontsize 20","995c41ce":"x5_train=x_train.copy()\nx5_val=x_val.copy()","d69d614f":"params = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [4, 6, 10, 12],\n    'random_state': [13]\n}","e70e9301":"#kf = KFold(n_splits=5, random_state=42, shuffle=False)\nexample_params = {\n        'n_estimators': 50,\n        'max_depth':4 ,\n        'random_state': 13\n    }\n    ","98047002":"imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(n_estimators=5, random_state=13))\ncross_val_score(imba_pipeline, x5_train, y_train, scoring='f1', cv=kf)","671303f2":"new_params = {'randomforestclassifier__' + key: params[key] for key in params}\ngrid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='f1',return_train_score=True)\ngrid_imba.fit(x5_train, y_train)","5066478b":"print(\"Training Score In Random Forest Classification:\",grid_imba.score(x5_train, y_train))\nprint(\"Validation Score In Random Forest Classification:\",grid_imba.score(x5_val, y_val))","3cd38389":"rf_best=grid_imba.best_params_\nrf_best","85d496d0":"# Random Forest with best hyperparameter\nrf_best = RandomForestClassifier(n_estimators=100,\n                                 max_depth=12,\n                                 random_state=13)\nrf_best.fit(x5_train, y_train)\ny_pred = rf_best.predict(x5_val)\n","f22796ec":"print(\"Training Score In Random Forest Classification with best parameters:\",rf_best.score(x5_train, y_train))\nprint(\"Validation Score In Random Forest Classification:with best parameters\",rf_best.score(x5_val, y_val))","ad2c7282":"# classification report for Random forest \nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","6d807e98":"models_evalutions['Model'].append(\"RandomForestClassifier_best parameters\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","efabd522":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues');\nplt.title('Confusion matrix for Random Forest classification with best parameters', fontsize = 20); # title with fontsize 20","a2235bad":"x6_train=x_train.copy()\nx6_val=x_val.copy()","2841fbb1":"model_names = [\"rf_best\",\"Dt_final\"]\n\nmodel_vars = [eval(n) for n in model_names]\nmodel_list = list(zip(model_names, model_vars))","717424f0":"model_names","ecbfd3f0":"model_list","17da57a2":"for model_name in model_names:\n    curr_model = eval(model_name)\n    print(f'{model_name} score: {curr_model.score(x_val, y_val)}')","25b38feb":"# create voting classifier\nvoting_classifer = VotingClassifier(estimators=model_list,voting='hard', n_jobs=-1)\nvoting_classifer.fit(x6_train, y_train)","ba037733":"# get accuracy (model to beat: RF with 0.8136 accuracy)\ny_pred = voting_classifer.predict(x_val)","ca9acd40":"print(\"Training Score In Hard Voting and select the best model scores(DT& RF))\",voting_classifer.score(x6_train, y_train))\nprint(\"Training Score In HRD Voting and select the best model scores(DT& RF))\",voting_classifer.score(x6_val, y_val))","9ba27225":"# classification report for Voting \nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","af9656e5":"models_evalutions['Model'].append(\"VotingClassifier-Hard\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","c1dc2551":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Voting', fontsize = 20); # title with fontsize 20","14c666fd":"x7_train=x_train.copy()\nx7_val=x_val.copy()","0543aa7f":"# create voting classifier\nvoting_classifer = VotingClassifier(estimators=model_list,voting='soft', n_jobs=-1)\nvoting_classifer.fit(x7_train, y_train)","740554fc":"print(\"Training Score In Average Voting and select the best model scores(DT& RF))\",voting_classifer.score(x7_train, y_train))\nprint(\"Training Score In Avaerage Voting and select the best model scores(DT& RF))\",voting_classifer.score(x7_val, y_val))","4dd4a4aa":"# Get accuracy (model to beat: RF with 0.8136 accuracy)\ny_pred = voting_classifer.predict(x7_val)","b37d50e6":"# classification report for voting avarage\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","426e425a":"models_evalutions['Model'].append(\"VotingClassifier-Average Voting\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","86aed8a8":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Avarege Voting', fontsize = 20); # title with fontsize 20","3d6e8fa5":"x8_train=x_train.copy()\nx8_val=x_val.copy()","f073942a":"# create voting classifier\nweights = [1.5,3.8]\nvoting_model = VotingClassifier(estimators=model_list, voting='soft', weights = weights, n_jobs=-1)\nvoting_model.fit(x8_train, y_train)","15a8f52a":"# Get accuracy (model to beat: RF with 0.8136 accuracy)\ny_pred = voting_model.predict(x8_val)","7773b9cd":"print(\"Training Score In Weighted Voting and select the best model scores(DT& RF))\",voting_model.score(x7_train, y_train))\nprint(\"Training Score In Weighted Voting and select the best model scores(DT& RF))\",voting_model.score(x7_val, y_val))","cb35cf9e":"# classification report for Naive Bayes\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","fe2d4d2b":"models_evalutions['Model'].append(\"VotingClassifier-Weighted Voting\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","9ab01306":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for Wieghted Voting', fontsize = 20); # title with fontsize 20","215f8874":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier\ntree_clas = DecisionTreeClassifier(random_state=42)","c642ea51":"bag_model = BaggingClassifier(\n    base_estimator = DecisionTreeClassifier(),\n    n_estimators = 100,\n    max_samples = 0.8,\n    oob_score=True,\n    random_state = 0\n)","3b67bf65":"bag_model.fit(x_train, y_train)","9ce73094":"bag_model.oob_score_","e5eb433e":"bag_model.score(x_val,y_val)","00acb605":"y_pred = tree.predict(x_val)\n\nprint(\"DT Accuracy=\",accuracy_score(y_val, y_pred))\nprint(\"DT F1 score=\",f1_score(y_val, y_pred))","d9b805eb":"# classification report Baggin\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","f8898a26":"models_evalutions['Model'].append(\" BaggingClassifier\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","e0135a5f":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix for BaggingClassifier', fontsize = 20); # title with fontsize 20","e483fee1":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier","6bb360f2":"nb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Naive Bayes val score: \",nb.score(x_val,y_val))\nprint(\"Naive Bayes Train score: \",nb.score(x_train,y_train))\ny_pred_nb = nb.predict(x_val)","68c89b20":"y_pred=nb.predict(x_val)","cc3f4a7d":"# classification report for Naive Bayes\nprint(classification_report(y_val, y_pred, digits=3, zero_division = 1))\nacc_nb = accuracy_score(y_val, y_pred)\nrecall_nb = recall_score(y_val, y_pred, average=\"binary\")\nprint(\"ACCURACY:\",accuracy_score(y_val, y_pred))\nprint(\"RECALL:\",recall_score(y_val, y_pred, average=\"binary\"))","21b835c8":"models_evalutions['Model'].append(\"Naive-B\")\nmodels_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))\nmodels_evalutions['Precision'].append(precision_score(y_val, y_pred))\nmodels_evalutions['Recall'].append(recall_score(y_val, y_pred))\nmodels_evalutions['F1 score'].append(f1_score(y_val, y_pred))","3dcecf0b":"# plotting confusion mtrix\ncm = confusion_matrix(y_val, y_pred)\nclass_label = [\"No-Diabetes\", \"Diabetes\"]\ndf_cm = pd.DataFrame(cm, index = class_label, columns = class_label)\nsns.heatmap(df_cm, annot = True, fmt = \"d\", cmap='Blues')\nplt.title('Confusion matrix forGaussianNB', fontsize = 20); # title with fontsize 20","a77f946f":"result= pd.DataFrame.from_dict(models_evalutions)\nresult","d2a0a56b":"cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nresult.plot.bar(x='Model',y=['Accuracy','Precision','Precision','Recall','F1 score'], cmap='Blues_r', figsize=(10,10))\nplt.title(\"Comparison of Scores of the Models\")\nplt.ylabel(\"Models\")\nplt.xlabel(\"ML Algorithms's scores in Diabetes Dataset\")\nplt.show()","ceaca0a6":"import warnings\nimport pickle\nfrom sklearn.pipeline import make_pipeline\nwarnings.filterwarnings(\"ignore\")","cb4acc7c":"lg = LogisticRegression()\nlg.fit(x_train, y_train)","732afcb7":"data = {\"model\":lg, \"scaler\": scaler}\nwith open('saved_steps.pkl', 'wb') as file:\n    pickle.dump(data, file)","25941cb0":"x_train","7686507c":"# Load Dataset ","d917817c":"Becouse of our target's labels imbalance we will use Smote to balance them with cross validation.","f3c7ee34":"Education level (EDUCA see codebook) scale 1-6:\n> 1 = Never attended school or only kindergarten \n\n> 2 = Grades 1 through 8 (Elementary)\n\n> 3 = Grades 9 throug 11 (Some high school)\n\n> 4 = Grade 12 or GED (High school graduate)\n\n> 5 = College 1 year to 3 years (Some college or technical school) \n\n> 6 = College 4 years or more (College graduate)","dbe2c32c":"## Experiment 6-1: Ensembling with Voting","22f6ca7c":"## Scalling to give us fair distrubtion btw features","7a8682fb":"## Expreiment 2: Logistic Regression Model with smote","811017ec":"## Baseline Model","8ea9ebd6":"13-level age categorys:(_AGEG5YR see codebook)\n> 1 = 18-24 \n\n> 9 = 60-64 \n\n> 13 = 80 or older \n","fd4d9372":"The Class Imbalance classification divided into three:\n\n> 1. Before model training: Resampling strategies (oversampling, undersampling)\n\n> 2. During model training: Training with adjusted class weights\n\n> 3. After model training: Adjusting the decision threshold (F1 optimization strategy)","efb545c2":"# Build models","788ca7b6":"Now thinking about your physical health, for how many days during the past 30 days \nwas your physical health not good?\n\n> scale 1-30 days ","242387ae":"## Experiment 1-1: K-nearest Neighbors Classification without using cross vaidation","39297178":"# preprocessing","b3391ae2":"## Experiment 1: Split data to training,validation and test set","c4d65994":"# Explore Dataset","ef38cc40":"## Experiment 3: Decision Tree Classification","07cf747a":"## Experiment 1-2: K-nearest Neighbors Classification with cross validation & Smote","83934bb2":"## Expreiment 5: Random Forest Classification ","1289b91b":"Would you say that in general your health is:scale 1-5:\n> 1 = excellent \n\n> 2 = very good \n\n> 3 = good \n\n> 4 = fair \n\n> 5 = poor\n","bdbbf3f4":"# Deplyment ","f651fa07":"Income scale 1-8:\n> 1 = less than $10,000 \n\n> 5 = less than $35,000 \n\n> 8 = $75,000 or more\n\n","73155260":"## Experiment 6-2: Ensembling with Average Voting","93f1696d":"# Cleaning Dataset","8c49aae7":"# Import Libraries","2740dc75":"## Experiment 6-3: Ensembling with Weighted Voting","e17cb995":"Mental health scale:\n\n> scale 1-30 days .","ca189293":"# Expreiment 7: Bagging","ad0578de":"# Expreiment 8 : Naive Bayes","e12f1f9f":"# Visualze the correlation ","057430e8":"the best result was for Smote "}}