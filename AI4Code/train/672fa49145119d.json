{"cell_type":{"730f27ab":"code","71424a38":"code","63ffc45c":"code","b1caad83":"code","830cd6b9":"code","883de0b0":"code","ffbf20f1":"code","6fd6a4ae":"code","74686726":"code","7398a2ea":"code","76d9f982":"code","20a985a3":"code","1aae1aa1":"code","c93d1600":"code","5dd9ffc9":"code","740047dc":"code","37c18bc1":"code","aa81c6c5":"code","573027cf":"code","f33124f6":"code","1fc660af":"code","e68d5429":"code","22b42546":"code","1c6c34ee":"code","fad8fefc":"code","6adb38d9":"markdown","e7b089e2":"markdown","5d713623":"markdown","fe0482ae":"markdown","915fc730":"markdown","792114e7":"markdown","28c653e8":"markdown","6b192a7f":"markdown","834b5094":"markdown","d0fea787":"markdown","3848dc94":"markdown","a75ae95d":"markdown","d52e0019":"markdown","669a71a2":"markdown","164e21a5":"markdown","91aaaf5b":"markdown","e7a55ed0":"markdown","637dac3f":"markdown","304ec6cf":"markdown","96fc4488":"markdown","d3d36703":"markdown","bd365911":"markdown","345f7123":"markdown","fb68725b":"markdown","e54e4a4c":"markdown","2d78153a":"markdown","6dcfc8cf":"markdown","822be85f":"markdown","b1ab9463":"markdown","91fd144c":"markdown","ffa54a1f":"markdown","b74fa549":"markdown","cc8ce5c3":"markdown","16a2378e":"markdown","3924d322":"markdown","f6101d4b":"markdown"},"source":{"730f27ab":"import os\nimport warnings\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n# Mute warnings\nwarnings.filterwarnings('ignore')","71424a38":"def load_data():\n    # Carregando os dados\n    data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    # Concatenando para tratamento dos dados\n    df = pd.concat([df_train, df_test])\n    # Pre-processamento\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Retornar as divis\u00f5es\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","63ffc45c":"data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\ndf.Exterior2nd.unique()","b1caad83":"def clean(df):\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    # Alguns valores de GarageYrBlt est\u00e3o corrompidos, ent\u00e3o vou substituir pelo\n    # ano de constru\u00e7\u00e3o da casa.\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Renomeando algumas colunas\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"Threeseasonporch\",\n    }, inplace=True,\n    )\n    return df","830cd6b9":"# As colunas j\u00e1 est\u00e3o com encode correto (float para cont\u00ednua e int para discreta)\n# Mas MSSubClass \u00e9 int, deveria ser nominative (categ\u00f3rica)\n\n# Vari\u00e1veis nominativas categ\u00f3ricas:\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# Em ordem:\n\n# Pandas chama as categorias de levels, qualitativamente:\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Adicionando None para n\u00e3o existente\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Categorias nominais\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Adiciona None para n\u00e3o existente\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Categorias ordinais\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","883de0b0":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","ffbf20f1":"df_train, df_test = load_data()","6fd6a4ae":"# Visualiza\u00e7\u00e3o dos valores.\n#display(df_train)\n#display(df_test)\n\n# Informa\u00e7\u00e3o dos tipos e valores ausentes.\n#display(df_train.info())\n#display(df_test.info())","74686726":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding para categ\u00f3ricas\n\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","7398a2ea":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","76d9f982":"\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # Todas as features discretas devem ser do tipo dint\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","20a985a3":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)\nmi_scores","1aae1aa1":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]","c93d1600":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores)\n\nscore_dataset(X, y)","5dd9ffc9":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X","740047dc":"\ndef mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    # Posteriormente descobri que essa feature n\u00e3o serve hahaha.\n    # X[\"TotalOutsideSF\"] = \\\n    #     df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n    #     df.Threeseasonporch + df.ScreenPorch\n    return X\n\n\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X","37c18bc1":"cluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]\n\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features e dando join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","aa81c6c5":"def apply_pca(X, standardize=True):\n    # Padronizando\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Criando os principais componentes\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convertendo para Dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Criando loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # Transpondo a matriz dos loadings\n        columns=component_names,  # Colunas s\u00e3o os principais componentes\n        index=X.columns,  # Linhas s\u00e3o as features originais\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Cria figura\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Vari\u00e2ncia explicada\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Vari\u00e2ncia acumulada\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up da figura\n    fig.set(figwidth=8, dpi=100)\n    return axs","573027cf":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","f33124f6":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","1fc660af":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new","e68d5429":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Aqui vamos dar fit no encoder usando um split e transformando a feture\n    # em outro. Iterando por todos os splits de todos os folds at\u00e9 uma\n    # completa transforma\u00e7\u00e3o, tamb\u00e9m treinando o encoder a cada fold.\n    \n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # Tiramos a media dos encoders para treinar o modelo.\n    \n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","22b42546":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combina os splits se o test data for dado.\n    # Se estamos criando features para o test set, vamos utilziar todo\n    # dado poss\u00edvel. Depois de criarmos as features, vamos re-criar os splits.\n    \n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    # X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n\n    # Clustering\n    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # PCA\n    X = X.join(pca_inspired(X))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n\n    # refazendo splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","1c6c34ee":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=6,           # M\u00e1xima profundidade de cada \u00e1rvore - recomendo de 2 a 10\n    learning_rate=0.01,    # Efeito de cara \u00e1rvore - recomendo de 0.0001 a 0.1\n    n_estimators=1000,     # N\u00famero de \u00e1rvores (boosting rounds) - recomendo de 1000 a 8000\n    min_child_weight=1,    # M\u00ednimo de casas nas folhas - recomendo de 1 a 10\n    colsample_bytree=0.7,  # Fra\u00e7\u00e3o de features (colunas) por \u00e1rvore - recomendo de 0.2 a 1.0\n    subsample=0.7,         # Fra\u00e7\u00e3o de inst\u00e2ncias (linhas) por \u00e1rvore - recomendo de 0.2 a 1.0\n    reg_alpha=0.5,         # L1 regularization (LASSO) - recomendo de 0.0 a 10.0\n    reg_lambda=1.0,        # L2 regularization (RIDGE) - recomendo de 0.0 a 10.0\n    num_parallel_tree=1,   # Colocar > 1 para random forests com boost\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","fad8fefc":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","6adb38d9":"Grupos de features com alta correla\u00e7\u00e3o apresentam loadings interessantes.","e7b089e2":"## Estabelecendo a baseline ##\n\nA baseline serve para julgar o score do nosso feature engineering.\n\nVamos computar o cross-valitation score para nosso feature set, utilizando XGBoost como modelo de predi\u00e7\u00e3o.","5d713623":"<a id=\"3\"><\/a> \n# Passo 3 - Criando Features #\n\nAgora vamos come\u00e7ar a trabalhar as features no nosso dataset:\n\nModulando nosso c\u00f3digo, podemos fazer uma fun\u00e7\u00e3o que receber\u00e1 um dataframe preparado e atrav\u00e9s de uma pipeline far\u00e1 as transforma\u00e7\u00f5es que quero.","fe0482ae":"Verificando se h\u00e1 ganho:","915fc730":"A utiliza\u00e7\u00e3o de label encoding \u00e9 perfeita para o meu caso, pois tenho afinidade com o XGBoost. Tamb\u00e9m \u00e9 muito recomendado para tratamento de vari\u00e1veis categ\u00f3ricas.","792114e7":"## Principal Component Analysis ##\n\nO PCA serve para realizar a redu\u00e7\u00e3o dimensional do problema atrav\u00e9s da an\u00e1lise dos *loadings* de cada componente, em termos de varia\u00e7\u00e3o.","28c653e8":"Voc\u00ea pode fazer o ajuste a m\u00e3o, ou ser um pouco inteligente (ou pregui\u00e7oso) e usar o auto-tune do [Optuna](https:\/\/optuna.readthedocs.io\/en\/stable\/index.html) com XGBoost:\n```\nimport optuna\n\ndef objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb)\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=20)\nxgb_params = study.best_params\n```\n\nS\u00f3 copiar e utilizar, pode gerar um desconforto por demorar. H\u00e1 tamb\u00e9m as ferramentas de visualiza\u00e7\u00e3o: [Optuna's visualizations](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/10_key_features\/005_visualization.html).","6b192a7f":"Conte\u00fado:\n\n1. [Preliminares](#1)\n1. [Feature Utility Scores](#2)\n1. [Criando Features](#3)\n1. [Hyperparameter Tuning](#4)\n1. [Treinamento do modelo e predi\u00e7\u00f5es finais](#5)\n1. [Conclus\u00e3o](#6)\n\nOBS: Estarei citando um dataset que foi abordado no curso de Feature Engineering: House Prices.","834b5094":"Para uma an\u00e1lise mais profunda, podemos analisar a matriz de correla\u00e7\u00e3o do dataset:","d0fea787":"O baseline serve para ajudar na visualiza\u00e7\u00e3o do score.","3848dc94":"<a id=\"2\"><\/a> \n# Passo 2 - Feature Utility Scores #\n\nAqui vamos avaliar o score da utiliza\u00e7\u00e3o das features do dataset, usando: `make_mi_scores` e `plot_mi_scores`:","a75ae95d":"Pode ser melhor explicado com um exemplo:","d52e0019":"Use assim:\n\n```\nencoder = CrossFoldEncoder(MEstimateEncoder, m=1)\nX_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n```\n\nVoc\u00ea pode transformar qualquer encoder da biblioteca: [`category_encoders`](http:\/\/contrib.scikit-learn.org\/category_encoders\/) em um cross-fold encoder. O [`CatBoostEncoder`](http:\/\/contrib.scikit-learn.org\/category_encoders\/catboost.html) vale o teste. \u00c9 similar ao `MEstimateEncoder` mas usa alguns truques para previnir o overfitting. Apenas mudando o par\u00e2metro de smoothing de `m` para `a`.","669a71a2":"<a id=\"1\"><\/a> \n# Passo 1 - Preliminares #\n## Defini\u00e7\u00f5es iniciais ##","164e21a5":"\n\n```\ndef create_features(df):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    X = X.join(create_features_1(X))\n    X = X.join(create_features_2(X))\n    X = X.join(create_features_3(X))\n    # ...\n    return X\n```\n\nLeAgora vamos definir as transforma\u00e7\u00f5es de ``label encoding`` para as vari\u00e1veis categ\u00f3ricas","91aaaf5b":"## Carregamento dos dados ##","e7a55ed0":"# Esse Notebook \u00e9 um tutorial intermedi\u00e1rio para implementa\u00e7\u00e3o de Machine Learning","637dac3f":"Aqui podemos ver as features informativas e n\u00e3o informativas. O foco dever\u00e1 ser direcionado a manipula\u00e7\u00e3o dessas features relevates. Features com 0 de escore ser\u00e3o droppadas.","304ec6cf":"Algumas ideias a se explorar:\n- Intera\u00e7\u00f5es das features: qualidade `Qual` e condi\u00e7\u00e3o `Cond` .`As features `OverallQual` e `OverallCond` s\u00e3o features de alto score, podendo transform\u00e1-las em uma mista advinda do produto.\n- Normaliza\u00e7\u00e3o dos dados num\u00e9ricos.\n- Intera\u00e7\u00f5es de num\u00e9ricas com categ\u00f3ricas, como em: `BsmtQual` e `TotalBsmtSF`.\n- Outras opera\u00e7\u00f5es probabil\u00edsticas para valores, como: m\u00e9dia, mediana, moda, etc.","96fc4488":"### Manipulando dados em falta ###\n\nBasicamente substituir por 0 (num\u00e9rico) ou None (ausente).","d3d36703":"<a id=\"4\"><\/a> \n# Passo 4 - Hyperparameter Tuning #\n\nAgora vamos fazer alguns ajustes de hyperpar\u00e2metros com o XGBoost, antes de come\u00e7ar as predi\u00e7\u00f5es.","bd365911":"## k-Means Clustering ##","345f7123":"Posteriormente, vou adicionar `drop_uninformative` como fun\u00e7\u00e3o para o nosso pipeline de cria\u00e7\u00e3o de features.","fb68725b":"Revisando os scores das features:","e54e4a4c":"### Encode nos dados num\u00e9ricos e categ\u00f3ricos ###","2d78153a":"## Criando features com Pandas ##","6dcfc8cf":"## Target Encoding ##\n\nAqui vamos utilizar as t\u00e9cnicas de folds e train-set:\n\n1. Separando em folds, tendo duas divis\u00f5es (teste-treino)\n2. Treino o encoder em treino e aplico a trasforma\u00e7\u00e3o no teste.\n3. Repito para todos os splits.","822be85f":"## Pr\u00e9-processamento dos dados ##\n\n- **Carregar** os dados pelo .csv;\n- **Limpar** os dados, tirando inconsist\u00eancias e erros;\n- **Encode** nos dados num\u00e9ricos e categ\u00f3ricos;\n- **Impute** valores em branco.","b1ab9463":"## Criando o seu Feature Set ##\n\nAgora vamos combinar tudo, colocando as transforma\u00e7\u00f5es em fun\u00e7\u00f5es separadas para melhor teste de combina\u00e7\u00e3o, as que n\u00e3o est\u00e3o comentadas s\u00e3o as que acredito serem melhores, mas estou aberto a sugest\u00f5es. Fique a vontade para modificar e fazer sua pr\u00f3pria pipeline!","91fd144c":"<a id=\"6\"><\/a> \n# Conclus\u00e3o\n* Quaisquer d\u00favidas, responderei vide coment\u00e1rio ou email\n* Recomenda\u00e7\u00f5es e gratifica\u00e7\u00f5es s\u00e3o sempre bem-vindas!\n* Um abra\u00e7o!\n\n***Obrigado pelo seu tempo,***\n***Lucas Silva***","ffa54a1f":"Descomentar para visualizar algumas caracter\u00edsticas:","b74fa549":"<a id=\"5\"><\/a>\n# Passo 5 - Treinamento do modelo e predi\u00e7\u00f5es finais #\nAgora com tudo perfeitamente feito e operante (assim espero), podemos partir finalmente para as predi\u00e7\u00f5es! Essa pr\u00f3xima c\u00e9lula far\u00e1:\n- Cria\u00e7\u00e3o da feature set com os dados originais;\n- Treinamento do XGBoost com o Dataset de treinamento;\n- Uso do modelo de treinamento, para fazer as predi\u00e7\u00f5es no set de teste.\n- Exportar para csv e fim :)","cc8ce5c3":"### Limpando os dados ###","16a2378e":"### Aplica\u00e7\u00e3o do PCA - Indicando Outliers ###\n\nA remo\u00e7\u00e3o de outliers, ou seja, valores absurdos ou fora do padr\u00e3o, quase sempre acarretar\u00e1 uma melhoria significante no nosso modelo.\n\nNesse dataset, casas em Edwards com venda parcial foram respons\u00e1veis por alguns casos de outliers:","3924d322":"Comparando com as descri\u00e7\u00f5es dos dados, podemos reformular algumas inconsist\u00eancias.","f6101d4b":"E aqui temos alguns exemplos de transforma\u00e7\u00f5es de features, desenvolvidas atrav\u00e9s da an\u00e1lise do comportamento do Dataset:"}}