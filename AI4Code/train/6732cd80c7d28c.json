{"cell_type":{"5f1cc6a6":"code","64571e8a":"code","62257132":"code","4059f502":"code","f7de2544":"code","64769817":"code","b6d1daa9":"code","a76cb49c":"code","083c2cce":"code","da6ad425":"code","2930f64d":"code","fc27c25b":"code","f77b8082":"code","3cc6dbb9":"code","00df97cf":"code","744501a7":"code","187d2b1f":"code","de5a932b":"code","afc6e85f":"code","7791d6bb":"code","c66b3d90":"code","c5ffb3d2":"code","77b9096f":"code","3e34a180":"code","6ba7b7ac":"code","baa743c6":"code","b3fbb401":"code","3f8f3198":"code","20e3b28a":"code","d6bfc6e8":"code","3269791c":"code","1547d402":"code","4d010386":"code","fa66b020":"code","dfe4446c":"code","259af5c1":"code","b0d69a8f":"code","4ecbbd45":"code","0b9b7f2c":"code","5f8d5f50":"code","1b15b056":"code","6369ad8b":"code","19582d34":"code","3a8ecbe6":"code","406a0502":"code","d230cdcc":"code","e57d4e8b":"code","c752a6aa":"code","5d027063":"code","389d42b7":"code","ef84db78":"code","b84be0c8":"code","187ab4b0":"markdown","d8817648":"markdown","189e0a89":"markdown","007c40cd":"markdown","6c557b0e":"markdown","f4a3713c":"markdown","56b09dd2":"markdown","72d5a4c5":"markdown","5ab5d4bf":"markdown"},"source":{"5f1cc6a6":"# Importing the libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","64571e8a":"# Importing the dataset\n\ndataset = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')","62257132":"# Viewing the first five rows of dataset\n\ndataset.head()","4059f502":"# We can see all the columns have categorical value.\n# We have 22 features (independent variables) and a dependent variable (class).\n\n# We will continue with data preprocessing but lets get some insights of the dataset first.","f7de2544":"# Visualising the number of mushrooms that fall in each class - p = poisonous, e=edible\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize']=8,8 \ns = sns.countplot(x = \"class\", data = dataset)\nfor p in s.patches:\n    s.annotate(format(p.get_height(), '.1f'), \n               (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 9), \n                textcoords = 'offset points')\nplt.show()","64769817":"# In the given dataset we have 3916 poisonous mushrooms and 4208 edible mushrooms","b6d1daa9":"features = dataset.columns\nprint(features)","a76cb49c":"f, axes = plt.subplots(22,1, figsize=(15,150), sharey = True) \nk = 1\nfor i in range(0,22):\n    s = sns.countplot(x = features[k], data = dataset, ax=axes[i], palette = 'GnBu')\n    axes[i].set_xlabel(features[k], fontsize=20)\n    axes[i].set_ylabel(\"Count\", fontsize=20)\n    axes[i].tick_params(labelsize=15)\n    k = k+1\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.1f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 9), \n        fontsize = 15,\n        textcoords = 'offset points')","083c2cce":"# From above graph we can see how many mushrooms belong to each category in each feature","da6ad425":"f, axes = plt.subplots(22,1, figsize=(15,150), sharey = True) \nk = 1\nfor i in range(0,22):\n    s = sns.countplot(x = features[k], data = dataset, hue = 'class', ax=axes[i], palette = 'CMRmap')\n    axes[i].set_xlabel(features[k], fontsize=20)\n    axes[i].set_ylabel(\"Count\", fontsize=20)\n    axes[i].tick_params(labelsize=15)\n    axes[i].legend(loc=2, prop={'size': 20})\n    k = k+1\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.1f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 9), \n        fontsize = 15,\n        textcoords = 'offset points')","2930f64d":"# From above graph we can see how many mushrooms belong to each category in each feature and among those how many are edible and how many are \n# poisonous mushrooms.","fc27c25b":"df1 = dataset[dataset['class'] == 'p']\ndf2 = dataset[dataset['class'] == 'e']\nprint(df1)\nprint(df2)","f77b8082":"# Creating independent and dependent variables\nx = dataset.iloc[:,1:].values\ny = dataset.iloc[:,0].values","3cc6dbb9":"print(x)","00df97cf":"print(len(x[0]))","744501a7":"print(y)","187d2b1f":"# Finding missing values\n\ndataset.isna().sum()","de5a932b":"dataset.info()","afc6e85f":"# Categories in each feature x\ncolumn_list = dataset.columns.values.tolist()\n#print(column_list)\nfor column_name in column_list:\n    print(column_name)\n    print(dataset[column_name].unique())","7791d6bb":"# Label encoding y - dependent variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","c66b3d90":"print(y)","c5ffb3d2":"# One hot encoding independent variable x\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()\nx = onehotencoder.fit_transform(x).toarray()","77b9096f":"print(x[0])","3e34a180":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","6ba7b7ac":"# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 3)\nx_train = pca.fit_transform(x_train)\nx_test = pca.transform(x_test)","baa743c6":"# Training the Logistic Regression Model on the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(x_train, y_train)","b3fbb401":"# Predicting the test set\ny_pred = classifier.predict(x_test)","3f8f3198":"# Making the confusion matrix and calculating accuracy score\nacscore = []\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","20e3b28a":"# Training the Naive Bayes Classification model on the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","d6bfc6e8":"# Predicting the test set\ny_pred = classifier.predict(x_test)","3269791c":"# Making the confusion matrix and calculating the accuarcy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","1547d402":"# Training the RBF Kernel SVC on the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state=0)\nclassifier.fit(x_train, y_train)","4d010386":"# predicting test set\ny_pred = classifier.predict(x_test)","fa66b020":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","dfe4446c":"# Calculating the optimum number of neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor neighbors in range(3,10,1):\n    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric='minkowski')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\nplt.plot(list(range(3,10,1)), list1)\nplt.show()","259af5c1":"# Training the K Nearest Neighbor Classification on the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nclassifier.fit(x_train, y_train)","b0d69a8f":"# Predicting the test set\ny_pred = classifier.predict(x_test)","4ecbbd45":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","0b9b7f2c":"# Training the Decision Tree Classification on the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state=0)\nclassifier.fit(x_train, y_train)","5f8d5f50":"# Predicting the test set \ny_pred = classifier.predict(x_test)","1b15b056":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","6369ad8b":"# Training the XGBoost Classification on the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(x_train,y_train)","19582d34":"# Predicting the test set\ny_pred = classifier.predict(x_test)","3a8ecbe6":"# Making the confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","406a0502":"# Finding the optimum number of n_estimators\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nlist1 = []\nfor estimators in range(10,150):\n    classifier = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n    classifier.fit(x_train, y_train)\n    y_pred = classifier.predict(x_test)\n    list1.append(accuracy_score(y_test,y_pred))\n#print(mylist)\nplt.plot(list(range(10,150)), list1)\nplt.show()","d230cdcc":"# Training the Random Forest Classification on the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(criterion = 'entropy', random_state = 0, n_estimators = 100)\nclassifier.fit(x_train, y_train)","e57d4e8b":"# Predicting the test set\ny_pred = classifier.predict(x_test)","c752a6aa":"# Making the confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nacscore.append(ac)\nprint(cm)\nprint(ac)","5d027063":"# Printing accuracy score of all the classification models we have applied \nprint(acscore)","389d42b7":"models = ['LogisticRegression','NaiveBayes','KernelSVM','KNearestNeighbors','DecisionTree','XGBoost','RandomForest']","ef84db78":"# Visualising the accuracy score of each classification model\nplt.rcParams['figure.figsize']=15,8 \nplt.style.use('dark_background')\nax = sns.barplot(x=models, y=acscore, palette = \"rocket\", saturation =1.5)\nplt.xlabel(\"Classifier Models\", fontsize = 20 )\nplt.ylabel(\"% of Accuracy\", fontsize = 20)\nplt.title(\"Accuracy of different Classifier Models\", fontsize = 20)\nplt.xticks(fontsize = 13, horizontalalignment = 'center', rotation = 0)\nplt.yticks(fontsize = 13)\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate(f'{height:.2%}', (x + width\/2, y + height*1.02), ha='center', fontsize = 'x-large')\nplt.show()","b84be0c8":"# So among all classification model Random Forest Classification has highest accuracy score = 99.75%.","187ab4b0":"----","d8817648":"---","189e0a89":"---","007c40cd":"----","6c557b0e":"---","f4a3713c":"---","56b09dd2":"----","72d5a4c5":"---","5ab5d4bf":"---"}}