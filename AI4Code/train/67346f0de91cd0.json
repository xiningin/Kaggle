{"cell_type":{"49c6b090":"code","b4648033":"code","70ccf0ae":"code","f7952fce":"code","41b60f2a":"code","338114cd":"code","3a9ab720":"code","efe02632":"code","6abd6c99":"code","aa03a7d7":"code","389ec4e5":"code","f9d653c3":"code","915d78a5":"code","675c0ddd":"code","3f786fe7":"code","d4df2a3b":"code","3b6d10f0":"code","29f01254":"markdown","bf969b03":"markdown","f4c6cf5f":"markdown","3eaec152":"markdown","06a2614d":"markdown","fe9246a2":"markdown","9140b252":"markdown","0c2c2bc9":"markdown","9ef62107":"markdown","14ae54db":"markdown","d845ef3b":"markdown","8995f1c2":"markdown"},"source":{"49c6b090":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\n\n\n\n\n#data convert to object\ndata=pd.read_csv('..\/input\/asdfvef\/bank-full.csv',sep=';')\n\nprint(data.head())\nprint(\"*\"*100)\n\n\nprint(data.columns)\nprint(\"*\"*100)\n\n#the data not contains null\nprint(\"*\"*100)\nprint(data.info())\n\n","b4648033":"#firstly we have to make the data numerical\n#marital\nevlilik=data[\"marital\"].value_counts(dropna=False)\nprint(evlilik)\ndata[\"marital\"]=[1 if i == 'single' else 0 if  i=='married' else 2 for i in data[\"marital\"]]\n\n'''\nmarried = 0\nsingle = 1\nelse = 2\n'''\n#get dummiesleri g\u00f6rselle\u015ftirmeden sonra predict algoritmas\u0131ndan \u00f6nce yazal\u0131m\n#data=pd.get_dummies(data,columns=['marital'])\n\n#job\njobsay\u0131s\u0131=data[\"job\"].value_counts(dropna=False)\nprint(jobsay\u0131s\u0131)\ndata['job'].value_counts()\ndata['job']=data['job'].map({'blue-collar':1,'management':2,'technician':3,'admin.':4,'services':5,'retired':6,'self-employed':7,'entrepreneur':8,'unemployed':9,'housemaid':10,'student':11,'unknown':np.nan})\n\n'''\n'blue-collar':1\n'management':2\n'technician':3\n'admin.':4\n'services':5\n'retired':6\n'self-employed':7\n'entrepreneur':8\n'unemployed':9\n'housemaid':10\n'student':11\n'unknown':12\n'''\n\n#days\nprint(\"*********************groupby1***********************\")\ngroupby1=data.groupby(['day','y']).size().reset_index().groupby('day')[[0]].max()\nprint(groupby1)\n\n#camping\nnumbercamping=data[\"campaign\"].value_counts(dropna=False)\n\n#e\u011fitim\ne\u011fitimsay\u0131s\u0131=data[\"education\"].value_counts(dropna= False)\nprint(e\u011fitimsay\u0131s\u0131)\ndata['education']=data['education'].map({'unknown':np.nan,'primary':1,'secondary':2,'tertiary':3})\n#unknown kolonunu ortalamay\u0131 bozmadan doldurmaya \u00e7al\u0131\u015fal\u0131m\n\n#defaut : temerr\u00fcte kalm\u0131\u015f borcu var m\u0131\ndefaultsay\u0131s\u0131=data['default'].value_counts(dropna=False)\nprint(defaultsay\u0131s\u0131)\n\ndef defaultbelirleme(value):\n    if value=='no':\n        return 0\n    else:\n        return 1   \ndata['default']=data['default'].apply(defaultbelirleme)\n\n\n#housing bor\u00e7la ev var m\u0131?\nhousing=data['housing'].value_counts(dropna=False)\nprint(housing)\n#data['housing'] = data['housing'].map({})\ndef defaultbelirleme(value):\n    if value=='no':\n        return 0\n    else:\n        return 1\ndata['housing']=data['housing'].apply(defaultbelirleme)\n\nprint(\"*********************groupby2***********************\")\ngroupby2=data.groupby(['housing','age']).size().reset_index().groupby('age')[[0]].max()\nprint(groupby2)\nprint(groupby2.sum())\n'''0= no \n   1= yes\n'''\nplt.style.use(\"seaborn-whitegrid\")\ng=sns.FacetGrid(data,col='housing', height=4, aspect=2)\ng.map(sns.distplot, 'age',bins=25)\nplt.show()\n#60 tan sonra ev borucu olan yok.ev borcu olanlar 25 -50 aras\u0131 \u00e7o\u011funluk\n\n#loan \ndata['loan']=data['loan'].replace({'no':0, 'yes':1})  #replace and map same methods\n#bor\u00e7 varm\u0131 ?\n\n#mounth\naylar=data['month'].value_counts()\nprint(aylar)\ndata['month']=data['month'].map({'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12})\n\n#contact \ncontact1=data['contact'].value_counts()\ndata['contact']=data['contact'].map({'cellular':0,'telephone':1,'unknown':np.nan})\n\n#balance\n#\u00f6ncelikle datam\u0131z\u0131 normalize edelim \u00fcst\u00fcne outlier hesaplayal\u0131m ve gerekirse \u00e7\u0131karal\u0131m\nprint(data['balance'].min())\nprint(data['balance'].max())\n#form\u00fcl\u00fcm\u00fcz =Xsc=(X\u2212Xmin)\/(Xmax\u2212Xmin)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaling=MinMaxScaler()\ndata['balance']=scaling.fit_transform(data[['balance']])\n#duration\n#his attribute highly affects the output target (e.g., if duration=0 then y='no')\n\nprint(\"******************************\")\nprint(data.info())\n\n#preoutcome :\u00f6nceki pazarlama durumu\npout=data['poutcome'].unique()\nprint(pout)\ndata['poutcome']=data['poutcome'].map({'failure':0,'success':1,'other':2,'unknown':np.nan})\n\n#durationu normalize edelim\n# form\u00fcl\u00fcm\u00fcz:Xsc=(X\u2212Xmin)\/(Xmax\u2212Xmin)\ndata['duration']=data['duration'].apply(lambda v :((v-data['duration'].min()) \/(data['duration'].max()-data['duration'].min())))\nprint(\"*********************groupby3***********************\")\ngroupby3=data.groupby(['month','duration']).size().reset_index().groupby('month')[[0]].mean()\nprint(groupby3)\n#5-6-7-8 month have most duration\n#y encoder\ndata['y']=data['y'].replace({'no':0, 'yes':1})\nprint(\"y say\u0131lar\u0131\")\nnumbery=data['y'].value_counts()\nprint(numbery)\n\n\n#de\u011fi\u015ftirdi\u011fimiz datay\u0131 farkl\u0131 bir csv dosyas\u0131na ald\u0131k.\ndata.to_csv(\"bank.preprocessed.csv\",index=False)\nnew_df=pd.read_csv(\"bank.preprocessed.csv\")\nprint(new_df.head())\n\ndescribe=new_df.describe()\nprint(describe)\n","70ccf0ae":"plt.style.use('ggplot')\nplt.subplots(figsize=(15,12))\nsns.heatmap(new_df.corr(),annot=True,fmt='.2f',color='red',cmap='magma')\nplt.show()\ncorr=new_df.corr()['y'].sort_values(ascending=False)\nprint(corr)\nplt.show()\n\n\n#\u00e7\u0131kar\u0131mlar\n#1-)duration s\u00fcresi y\u00fczde 40 corr gerekirse yeni featurelar \u00fcretilebilir\n#2-)housing y\u00fczde -14\n#3-)poutcome y\u00fczde +14\n#4-)pdays y\u00fczde 10 pday m\u00fc\u015fteri ile ileti\u015fime ge\u00e7ilen g\u00fcnden ge\u00e7en zaman 999=daha \u00f6nce ula\u015f\u0131lamad\u0131 anlam\u0131na gelir\n#5-)hpusing ile month aras\u0131nda y\u00fczde 17","f7952fce":"#outlier i\u00e7in;\nplt.style.use(\"seaborn-whitegrid\")\nnew_df.plot.box(figsize=(12,10))\nplt.xticks(list(range(len(new_df.columns))), new_df.columns,rotation='vertical')\nplt.show()","41b60f2a":"#age\nplt.subplots(figsize=(12,10))\nplt.hist(new_df['age'], bins=50,color='red')\nplt.xlabel(\"Age\", color = \"red\", size = 16)\nplt.ylabel(\"Amount\", color = \"red\", size = 16)\nplt.show()","338114cd":"def bar_graph(variable):\n    var=new_df[variable]\n    varValue=var.value_counts()\n    plt.style.use(\"ggplot\")\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.values)\n    plt.ylabel('frequence')\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable, varValue))\n\ncategory1=[\"marital\",\"education\",\"default\"]\nfor each in category1:\n    bar_graph(each)","3a9ab720":"#age and y\nplt.style.use(\"seaborn-whitegrid\")\n\ng=sns.FacetGrid(new_df,col='y', size=12)\ng.map(sns.distplot, 'age',bins=25)\nplt.show()","efe02632":"#housing age an y\ng=sns.FacetGrid(new_df,col='y',row='housing')\ng.map(plt.hist,'age',bins=25)\ng.add_legend()\nplt.show()","6abd6c99":"\n\n#duration\ng=sns.FacetGrid(new_df,col='y',size=4, aspect=2)\ng.map(sns.distplot, 'duration',bins=50)\nplt.show()\n#0.1 ve a\u015fa\u011f\u0131s\u0131 \u00e7ok y\u00fcksek ihtimal 0\n#0.2 den yukar\u0131s\u0131 1 e \u00e7ok yak\u0131n\nsns.jointplot(new_df.loc[:,'duration'], new_df.loc[:,'housing'], kind=\"regg\", color=\"#ce1414\")\nplt.show()\n\n","aa03a7d7":"sns.countplot(x='housing', data=new_df)\nplt.show()\nprint(\"*********************housing and y = 1 *************************\")\nnumberhousi=((new_df['housing'] == 1) & (new_df['y'] == 1)).value_counts()\nprint(numberhousi)\n#ikisininde bir oldu\u011fu sadece 1935 m\u00fc\u015fteri var bu bilgi ile kolon in\u015fa etmeliyiz.","389ec4e5":"#poutcome\nsns.countplot(x='poutcome', data=new_df)\nplt.show()\n\n\nnumberpout=((new_df['poutcome'] == 1) & (new_df['y'] == 1)).value_counts()\nprint(numberpout)","f9d653c3":"#pdays\nnew_df['pdays']=new_df['pdays'].replace(-1,999)\n#999 means client was not previously contacted)\nplt.subplots(figsize=(12,10))\nplt.hist(new_df['pdays'], bins=50,color='blue')\nplt.xlabel(\"How many days ago was the connect passed?\")\nplt.show()","915d78a5":"#for detect outl\u015fers\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        #1st quartile\n        Q1=np.percentile(df[c],25)\n        #3st quartile\n        Q3=np.percentile(df[c],75)\n        #IQR \n        IQR=Q3-Q1\n        #outlier step\n        outlier_step=IQR*1.5\n        #detect outlier\n        outlier_list_col=df[(df[c] < Q1-outlier_step) | (df[c] > Q3 + outlier_step)].index\n        #store indeces\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i , v in outlier_indices.items() if v > 2)\n        \n    return multiple_outliers","675c0ddd":"\n\n'''               *********FEATURE ENG\u0130NEER\u0130NG**************           '''\n#most correlation with y.\n#we create new two column\nprint(\"***********number of y************\")\nprint(new_df['y'].value_counts())\n\nnew_df['low_duration']=[0 if i<0.08 else 1 for i in new_df['duration']]\nprint(\"***********number of low y************\")\nprint(new_df['low_duration'].value_counts())\n\n\nnew_df['high_duration']=[1 if i>0.22 else 0 for i in new_df['duration']]\nprint(\"***********number of high y************\")\nprint(new_df['high_duration'].value_counts())\n\n#do\u011fru kulland\u0131\u011f\u0131mdan emin de\u011filim buraya previousu da ekle\n#pdays and previous\nnew_df['accessible customerspart1']=[1 if i<200 else 0 for i in new_df['pdays']]\nnew_df['accessible customerspart2']=[1 if i>=2 else 0 for i in new_df['previous']]\nnew_df['accessible customers']=new_df['accessible customerspart1']+new_df['accessible customerspart2']\n#we use only accessible customers columns because it contains other parts so:\nnew_df.drop(columns=['accessible customerspart1','accessible customerspart2'],axis=1,inplace=True)\n\n#day  7 20 aras\u0131n\u0131 bir yerde \u00f6teki de\u011ferleri bir yerde ayr\u0131\u015ft\u0131ral\u0131m\nnew_df['d-day']=[1 if (i>11 & i<22) else 0 for i in new_df['day']]\n#housing \n#60 tan sonra ev borucu olan yok.ev borcu olanlar 25 -50 aras\u0131 \u00e7o\u011funluk\nnew_df['d-age']=[0 if (i<22 | i>60) else 1 if (i>=25 & i<=50) else 2 for i in new_df['age']]\nnew_df['d-housing1']=new_df['d-age']+new_df['housing']\nnew_df['d-housing']=[1 if i==2 else 0 for i in new_df['d-housing1']]\n\n\n#new features corr with y\nonly_NewFeatures=new_df.drop(columns=['age','job','marital','education','default','day','contact','d-housing1'],axis=1)\nplt.subplots(figsize=(15,12))\nsns.heatmap(only_NewFeatures.corr(),annot=True,fmt='.2f',color='red',cmap='coolwarm')\nplt.show()\n\nnew_df.drop(columns=['day','d-age'],axis=1,inplace=True)\n#GET DUMM\u0130ES\nnew_df=pd.get_dummies(new_df,columns=['job','marital','education','housing','month','default','contact','loan','poutcome','accessible customers'])","3f786fe7":"\n'''                         **********BU\u0130LD MODELS**********             '''\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\n\nX=new_df.drop(columns=['y'],axis=1)\ny=new_df.iloc[:,6]\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=35)\nprint(\"x_train: \" , len(X_train))\nprint(\"X_test: \" , len(X_test))\nprint(\"y_train: \" , len(X_train))\nprint(\"y_test: \" , len(y_test))\n\n\n\nprint(\"**********************************************************\")\nprint(\"LogisticRegression\")\nlogreg=LogisticRegression(C=0.01)\nlogreg.fit(X_train,y_train)\nacc_log_train=logreg.score(X_train,y_train)*100\nacc_log_test=logreg.score(X_test,y_test)*100\nprint('Training accuracy : % {}'.format(acc_log_train))\nprint('Test accuracy : % {}'.format(acc_log_test))\nprint('Logistic Regression confusion matrix')\ny_pred1=logreg.predict(X_test)\ncm=confusion_matrix(y_test,y_pred1)\nprint(cm)\nskor1=metrics.accuracy_score(y_pred1,y_test)\nprint(\"skor1:\",skor1)\n\n\nprint(\"**********************************************************\")\nprint(\"RandomForestClassifier\")\nrdc=RandomForestClassifier(max_features=15,n_estimators=100,criterion='gini',random_state=42)\nrdc.fit(X_train,y_train)\nacc_rdc_train=rdc.score(X_train,y_train)*100\nacc_rdc_test=rdc.score(X_test,y_test)*100\nprint('Training accuracy : % {}'.format(acc_rdc_train))\nprint('Test accuracy : % {}'.format(acc_rdc_test))\nprint('RandomForestClassifier confusion matrix')\ny_pred2=rdc.predict(X_test)\ncm=confusion_matrix(y_test,y_pred2)\nprint(cm)\nskor2=metrics.accuracy_score(y_pred2,y_test)\nprint(\"skor2:\",skor2)\n\nprint(\"**********************************************************\")\nprint(\"DecisionTreeClassifier\")\ndc=DecisionTreeClassifier(max_depth =20,min_samples_split=400,criterion=\"gini\")\ndc.fit(X_train,y_train)\nacc_dc_train=dc.score(X_train,y_train)*100\nacc_dc_test=dc.score(X_test,y_test)*100\nprint('Training accuracy : % {}'.format(acc_dc_train))\nprint('Test accuracy : % {}'.format(acc_dc_test))\nprint('DecisionTreeClassifier confusion matrix')\ny_pred3=dc.predict(X_test)\ncm=confusion_matrix(y_test,y_pred3)\nprint(cm)\nskor3=metrics.accuracy_score(y_pred3,y_test)\nprint(\"skor3:\",skor3)\n\nprint(\"**********************************************************\")\nprint(\"KNeighborsClassifier\")\nknn=KNeighborsClassifier(metric='manhattan', n_neighbors=19, weights='uniform')\nknn.fit(X_train,y_train)\nacc_knn_train=knn.score(X_train,y_train)*100\nacc_knn_test=knn.score(X_test,y_test)*100\nprint('Training accuracy : % {}'.format(acc_knn_train))\nprint('Test accuracy : % {}'.format(acc_knn_test))\nprint('KNeighborsClassifier confusion matrix')\ny_pred4=knn.predict(X_test)\ncm=confusion_matrix(y_test,y_pred4)\nprint(cm)\nskor4=metrics.accuracy_score(y_pred4,y_test)\nprint(\"skor4:\",skor4)\n\n\nprint(\"**********************************************************\")\nprint(\"SVC\")\nsvc=SVC()\nsvc.fit(X_train,y_train)\nacc_svc_train=svc.score(X_train,y_train)*100\nacc_svc_test=svc.score(X_test,y_test)*100\nprint('Training accuracy : % {}'.format(acc_svc_train))\nprint('Test accuracy : % {}'.format(acc_svc_test))\nprint('SVC confusion matrix')\ny_pred5=svc.predict(X_test)\ncm=confusion_matrix(y_test,y_pred5)\nprint(cm)\nskor5=metrics.accuracy_score(y_pred5,y_test)\nprint(\"skor5:\",skor5)\n\n\nprint(\"**********************************************************\")\nprint(\"GaussianNB\")\ngaussian=GaussianNB()\ngaussian.fit(X_train,y_train)\nacc_gaussian_train=gaussian.score(X_train,y_train)*100\nacc_gaussian_test=gaussian.score(X_test,y_test)*100\nprint('Training accuracy : % {}'.format(acc_gaussian_train))\nprint('Test accuracy : % {}'.format(acc_gaussian_test))\nprint('SVC confusion matrix')\ny_pred6=gaussian.predict(X_test)\ncm=confusion_matrix(y_test,y_pred6)\nprint(cm)\nskor6=metrics.accuracy_score(y_pred6,y_test)\nprint(\"skor6:\",skor6)\n\nprint(\"**********************************************************\")","d4df2a3b":"\n'''\nprint(\"************HYPERPARAMETER TUR\u0130NG-GR\u0130D SEARCH-CROSS VAL\u0130DAT\u0130ON******************\")\nprint(\"SVC\")\nlogreg_param_grid={\"C\":np.logspace(-3,3,7),\n                   \"penalty\":[\"l1\",\"l2\"]}\ngs=GridSearchCV(estimator=logreg,\n                param_grid=logreg_param_grid,\n                scoring='accuracy',\n                cv =10,\n                n_jobs=-1)\ngrid_search=gs.fit(X_train,y_train)\neniyisonuc=grid_search.best_score_\neniyiparametreler=grid_search.best_params_\nprint(eniyisonuc)\nprint(eniyiparametreler)\n\nprint(\"RANDOMFORESTCLASS\u0130F\u0130ER\")\nrfc_param_grid={\"max_features\": [1,3,10],\n                \"min_samples_split\":[1,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\ngs=GridSearchCV(estimator=rdc,\n                param_grid=rfc_param_grid,\n                scoring='accuracy',\n                cv =10,\n                n_jobs=-1)\ngrid_search=gs.fit(X_train,y_train)\neniyisonuc=grid_search.best_score_\neniyiparametreler=grid_search.best_params_\nprint(eniyisonuc)\nprint(eniyiparametreler)\n\nprint(\"DEC\u0130S\u0130ONTREE\")\ndt_param_grid ={\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\ngs=GridSearchCV(estimator=dc,\n                param_grid=dt_param_grid,\n                scoring='accuracy',\n                cv =10,\n                n_jobs=-1)\ngrid_search=gs.fit(X_train,y_train)\neniyisonuc=grid_search.best_score_\neniyiparametreler=grid_search.best_params_\nprint(eniyisonuc)\nprint(eniyiparametreler)\n\nprint(\"KNeighborsClassifier\")\nknn_param_grid={\"n_neighbors\": np.linspace(1,19,10, dtype =int).tolist(),\n                \"weights\":[\"uniform\",\"distance\"],\n                \"metric\":[\"euclidean\",\"manhattan\"]}\ngs=GridSearchCV(estimator=knn,\n                param_grid=knn_param_grid,\n                scoring='accuracy',\n                cv =10,\n                n_jobs=-1)\ngrid_search=gs.fit(X_train,y_train)\neniyisonuc=grid_search.best_score_\neniyiparametreler=grid_search.best_params_\nprint(eniyisonuc)\nprint(eniyiparametreler)\n\nprint(\"SUPPORtVECTORMACH\u0130NE\")\nsvc_param_grid={\"kernel\" : [\"rbf\"],\n                \"gamma\":[0.001,0.01,0.1,1],\n                \"C\":[1,10,50,100,200,300,100]}\ngs=GridSearchCV(estimator=svc,\n                param_grid=svc_param_grid,\n                scoring='accuracy',\n                cv =10,\n                n_jobs=-1)\ngrid_search=gs.fit(X_train,y_train)\neniyisonuc=grid_search.best_score_\neniyiparametreler=grid_search.best_params_\nprint(eniyisonuc)\nprint(eniyiparametreler)\n'''","3b6d10f0":"\n\n'''ENSEMBLE'''\nvotingC = VotingClassifier(estimators=[(\"rdc\",rdc),\n                                       (\"dc\",dc),\n                                       (\"knn\",knn)],\n                                        voting= 'hard', n_jobs = -1)\n'''when voting=soft ==  0.9027691763248695'''\nvotingC=votingC.fit(X_train,y_train)\nensemble=accuracy_score(votingC.predict(X_test),y_test)\nprint(\"Ensemble uygulanm\u0131\u015f y\u00fczdelik :\",accuracy_score(votingC.predict(X_test),y_test))\n\n\nimport pandas as pd \n  \n# intialise data of lists. \nlastdata = {'Name':['RFC', 'DTC', 'KNN', 'Ensemble'], 'skorlar':[skor2, skor3,skor4,ensemble]} \n  \n# Create DataFrame \ndflast = pd.DataFrame(lastdata) \n\n\ng = sns.barplot(x='Name',y='skorlar',data=dflast)\ng.set_xlabel(\"Models\")\ng.set_xlabel(\"Models numbers\")\ng.set_title(\"models graph\")\nplt.show()","29f01254":"We set 6 different model and use confusion matrix.The accuracy is not enough for conclusion we have to listen confusion matrix. It says many things about target.","bf969b03":"we can see that columns, types and data null informations.","f4c6cf5f":"there are many outlier data but \u0131 did not drop that because our data already unbalanced.So every row is considerable for us!","3eaec152":"# ***************** DATA V\u0130SUAL\u0130ZAT\u0130ON *************\n\nVisualization so important for understand.We try to find correlation between the columns.","06a2614d":"Okey guys we go to deep step by step in data.and make dummies type.","fe9246a2":"guys models parameters come to gridsearch but the process take long time so \u0131 make comment the grid search codes.","9140b252":"may be we improve the skor with VotingClassifier and if you select voting=hard you will be 0.005 point higher than selecting soft. few but much for us.","0c2c2bc9":"The first job is always examine the data in your mind.","9ef62107":"ABOUT DATA\n\n1 - age (numeric)\n2 - job : type of job\n3 - marital : marital status\n4 - education \n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\nrelated with the last contact of the current campaign:\n8 - contact: contact communication type (categorical: 'cellular','telephone')\n9 - month: last contact month of year\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nother attributes:\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\nsocial and economic context attributes\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n\nOutput variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')","14ae54db":"Hello everyone this is my first kernel in kaggle .Please Upvote if you like my Kernel! Good works..","d845ef3b":"also we go to deep for new correlation...","8995f1c2":"yeah we gain ground! but feature engineering is the fateful point especially for unbalanced data set."}}