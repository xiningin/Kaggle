{"cell_type":{"9f710455":"code","00d9d57c":"code","0b3e2434":"code","cd7428ec":"code","3af0a1d8":"code","5c57761d":"code","32d95bf7":"code","a3fe032c":"code","178962de":"code","a378e3d2":"code","449c50fb":"code","14a7f40a":"code","9ce4242c":"code","9d594b90":"code","b659d7d9":"code","224e6b13":"code","3893eaef":"code","cb69c439":"code","7585eb9c":"code","09da8fbf":"code","4659a541":"code","8e671b0d":"code","93764497":"code","5dcc0cf3":"code","7ebbb95c":"code","575c84c5":"code","12dccdb3":"code","4423d4b7":"code","22e9b5fe":"code","23fa59a0":"code","9300456b":"code","aed4007e":"code","0265e2dd":"code","258d88fe":"code","27743705":"code","dc1e390f":"code","4ac14ab1":"code","8a49e95b":"code","0a71ec98":"code","9b408f60":"markdown","2fe322fc":"markdown","d208a316":"markdown","66172c4e":"markdown","2d872ba7":"markdown","84acae2e":"markdown","32782acb":"markdown","00ee2c2d":"markdown","d6bf50fe":"markdown","f1f30ae9":"markdown","1cee8b1d":"markdown","cff8f3e2":"markdown","2c421ccd":"markdown","c4fae843":"markdown","dba83336":"markdown","0505ef46":"markdown","746c753e":"markdown","523d2c3f":"markdown","8b21a0df":"markdown","fa482dbb":"markdown","9f539a16":"markdown","63857246":"markdown","aba50045":"markdown","0ecf1cca":"markdown","be208053":"markdown","85a768f4":"markdown","4a1fb72b":"markdown","7020e165":"markdown","de8bb8a6":"markdown","16755612":"markdown","9061f1d0":"markdown","5aa893ab":"markdown","3896206a":"markdown","00e5a684":"markdown","d6eacd27":"markdown","a47dff29":"markdown","92b8f127":"markdown","87f24d6c":"markdown","310b69a1":"markdown","b40f236d":"markdown","ecd739c3":"markdown"},"source":{"9f710455":"# Libraries\nimport pandas as pd\nimport numpy as np\nfrom requests import get\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport os\nimport re","00d9d57c":"# The URL I want to scrap data on\nurl = 'https:\/\/www.phrases.org.uk\/meanings\/phrases-and-sayings-list.html'\n\n# Prepare GET request\nresponse = get(url)\n\n# Retrieve the webpage and store it as an bs4.BeautifulSoup object\nhtml_soup = BeautifulSoup(response.text, 'html.parser')","0b3e2434":"quotes = html_soup.find_all('p', class_ = 'phrase-list')\nsize = len(quotes)\nquotes[:5]","cd7428ec":"cleaned_quotes = [quotes[i].text for i in range(size)]\ncleaned_quotes[:5]","3af0a1d8":"href_quotes = [quotes[i].a['href'] for i in range(size)]","5c57761d":"# The base link\nBASE_LINK = 'https:\/\/www.phrases.org.uk\/meanings\/'\n\ndef get_explanations(url):\n    \n    # This chunk of code is the same we used in the begining of this notebook\n    url = url\n    response = get(BASE_LINK + url)\n    html_soup = BeautifulSoup(response.text, 'html.parser')\n    \n    quote_explanation = html_soup.find_all('p', class_ = 'meanings-body')\n    if len(quote_explanation) >= 1:\n        quote_explanation = str(quote_explanation[0].text)\n    else:\n        quote_explanation = \"NO INFORMATION\"\n        \n    return quote_explanation","32d95bf7":"%%time\n# This might take a while, you can grab a coffee or just reduce dimensionality.\n# Here I chose only the five first quotes so it runs faster\nnumber_of_quotes = 5\nassert number_of_quotes < len(quotes)\n\nexplanations = [get_explanations(i) for i in tqdm(href_quotes[:number_of_quotes])]","a3fe032c":"# Constructing the final dataframe\nquotes_dataframe = pd.DataFrame()\nquotes_dataframe['text'] = quotes[:number_of_quotes]\nquotes_dataframe['text'] = quotes_dataframe['text'].apply(lambda x:x.text)\nquotes_dataframe['explanation'] = explanations\nquotes_dataframe['origin'] = 'English'\n\nquotes_dataframe.head()","178962de":"# Save all the data in .csv file\nquotes_dataframe.to_csv('English_phrases_and_sayings.csv')","a378e3d2":"# The URL I want to scrap data on\nurl = 'https:\/\/www.chinahighlights.com\/travelguide\/learning-chinese\/chinese-sayings.htm'\n\n# Prepare GET request\nresponse = get(url)\n\n# Retrieve the webpage and store it as an bs4.BeautifulSoup object\nhtml_soup = BeautifulSoup(response.text, 'html.parser')","449c50fb":"proverbs_container = html_soup.find_all('div', class_ = 'col-md-19 col-sm-19 col-xs-24 pull-right')","14a7f40a":"def starts_with_digit(text):\n    \n    \"\"\"\n    Return boolean, does 'text' starts with a number\n    text: string\n    \"\"\"\n    \n    output = False\n    # This pattern finds if a string is starting is a number\n    pattern = re.compile(r'\\d*')\n    # If this pattern match with something, return True\n    if pattern.search(text).group() != '':\n        output = True\n        \n    return output","9ce4242c":"# All the p tags in proverbs_container\nto_browse = proverbs_container[0].find_all('p')","9d594b90":"# List of sentence starting with a number\nmask = [starts_with_digit(quote.text) for quote in to_browse]\n\n# Filter to get all the proverbs\nlist_of_proverbs =[to_browse[i].text for i in range(len(mask)) if mask[i] == True]","b659d7d9":"pattern_chinese = re.compile(r'((?<=\\d\\.)(.*?)(?=\\())')\npattern_chinese.search(\"6. \u4e09\u4e2a\u548c\u5c1a\u6ca1\u6c34\u559d\u3002 (S\u0101n g\u00e8 h\u00e9sh\u00e0ng m\u00e9i shu\u01d0 h\u0113. 'three monks have no water to drink') \u2014 Too many cooks spoil the broth.\").group()","224e6b13":"pattern_pin_yin = re.compile(r'((?<=\\()(.*?)(?=\\)))')\npattern_pin_yin.search(\"6. \u4e09\u4e2a\u548c\u5c1a\u6ca1\u6c34\u559d\u3002 (S\u0101n g\u00e8 h\u00e9sh\u00e0ng m\u00e9i shu\u01d0 h\u0113. 'three monks have no water to drink') \u2014 Too many cooks spoil the broth.\").group()","3893eaef":"pattern_translation = re.compile(r'(?<=\\\u2014)(.*?)$')\npattern_translation.search(\"6. \u4e09\u4e2a\u548c\u5c1a\u6ca1\u6c34\u559d\u3002 (S\u0101n g\u00e8 h\u00e9sh\u00e0ng m\u00e9i shu\u01d0 h\u0113. 'three monks have no water to drink') \u2014 Too many cooks spoil the broth.\").group()","cb69c439":"chinese_proverbs = pd.DataFrame()\nchinese_proverbs['all_text'] = list_of_proverbs\nchinese_proverbs['in_chinese'] = chinese_proverbs['all_text'].apply(lambda x:pattern_chinese.search(x).group())\nchinese_proverbs['pin_yin'] = chinese_proverbs['all_text'].apply(lambda x:pattern_pin_yin.search(x).group())\nchinese_proverbs['text'] = chinese_proverbs['all_text'].apply(lambda x:pattern_translation.search(x).group())\nchinese_proverbs['category'] = \"-1\"\nchinese_proverbs['origin'] = \"Chinese\"\n\nchinese_proverbs = chinese_proverbs.drop(['all_text'], axis=1)","7585eb9c":"proverbs_container[0].find_all('h2')","09da8fbf":"# I define a list with the name of categories\ncategories = ['Wisdom', 'Friendship', 'Love', 'Family', 'Encouragement', 'Education', 'Literature', 'Dragons']\n\n# I define a list of proverbs per category (same order)\nnumber_of_quotes_per_category = [26, 10, 10, 10, 21, 10 ,30 ,10]\n\n# Put both list in a dict\ndict_categories = dict(zip(categories, number_of_quotes_per_category))","4659a541":"# I ensure that we have same number of proverbs\nlen(list_of_proverbs) == sum(number_of_quotes_per_category)","8e671b0d":"# I define a cumsum list to get range index of proverbs that are contained in a category\ncumsum = np.cumsum(number_of_quotes_per_category)","93764497":"# I complete the 'category' column\nchinese_proverbs.loc[:cumsum[0], 'category'] = categories[0]\nfor index in range(6):\n    chinese_proverbs.loc[cumsum[index]:cumsum[index+1], 'category'] = categories[index+1]\nchinese_proverbs.loc[cumsum[6]:, 'category'] = categories[7]","5dcc0cf3":"chinese_proverbs.head()","7ebbb95c":"# Save all the data in .csv file\nchinese_proverbs.to_csv('Chinese_proverbs.csv')","575c84c5":"# The URL I want to scrap data on\nurl = \"https:\/\/frenchtogether.com\/french-idioms\/\"\n\n# Prepare GET request\nresponse = get(url)\n\n# Retrieve the webpage and store it as an bs4.BeautifulSoup object\nhtml_soup = BeautifulSoup(response.text, 'html.parser')","12dccdb3":"quotes = html_soup.find_all('h3')\n\n# Get the list of all french quotes\nfrench_quotes = [quote.text for quote in quotes]\n\n# I got these elements that I need to clean\nprint(french_quotes[-2:])\n\n# Excluding the two last elements which are not quotes\nfrench_quotes = french_quotes[:-2]","4423d4b7":"# For each quote, get all p tags that have all the necessary information\nall_texts = html_soup.find_all('p')","22e9b5fe":"def has_strong_tag(quote, chunk):\n    assert chunk in ['Literally', 'Meaning', 'English counterpart']\n    if chunk in quote.contents[0] :\n        return True\n    else:\n        return False","23fa59a0":"# Get indexes of elements that contain specified keywords \nliterally_text_indexes = np.where([has_strong_tag(all_texts[i], 'Literally') for i in range(len(all_texts))])[0]\nmeaning_text_indexes = np.where([has_strong_tag(all_texts[i], 'Meaning') for i in range(len(all_texts))])[0]\neng_cnt_text_indexes = np.where([has_strong_tag(all_texts[i], 'English counterpart') for i in range(len(all_texts))])[0]","9300456b":"all_texts_arr = np.array(all_texts)\n\n# Filter to keep all the Literally texts\nliterally_filtered = list(all_texts_arr[literally_text_indexes])\nliterally_real = [i.contents[1] for i in literally_filtered if 'strong' in str(i.contents[0])]\n\n# Filter to keep all the Meaning texts\nmeanings_filtered = list(all_texts_arr[meaning_text_indexes])\nmeanings_real = [i.contents[1] for i in meanings_filtered if 'strong' in str(i.contents[0])]\n\n# Filter to keep all the English Counterpart texts\neng_cnt_filtered = list(all_texts_arr[eng_cnt_text_indexes])\neng_cnt_real = [i.contents[1] for i in eng_cnt_filtered if 'strong' in str(i.contents[0])]","aed4007e":"# Cleaning\nto_find = [not 'strong' in str(i.contents[0]) for i in literally_filtered]\nprint(np.where(to_find)[0][0])\nprint(literally_filtered[67])\nto_find = [not ':' in str(i.contents[1]) for i in meanings_filtered if 'strong' in str(i.contents[0])]\nprint(np.where(to_find)[0][0])\nprint(meanings_filtered[1])","0265e2dd":"all_texts_arr[literally_text_indexes[67]]\nall_texts_arr[meaning_text_indexes[1]]","258d88fe":"literally_text_indexes = [i for i in literally_text_indexes if i != literally_text_indexes[67]]\nmeaning_text_indexes = [i for i in meaning_text_indexes if i != meaning_text_indexes[1]]","27743705":"# Construct DataFrame\ncolumn_data = ['in_french', 'literally', 'meaning', 'text']\nfrench_expressions = pd.DataFrame(index = range(0, 90) ,columns = column_data)\n\nfrench_expressions['in_french'] = french_quotes\nfrench_expressions['literally'] = literally_real    \nfrench_expressions['meaning'] = meanings_real\nfrench_expressions['lit_index'] = literally_text_indexes\nfrench_expressions['mea_index'] = meaning_text_indexes\nfrench_expressions['origin'] = \"French\"\n\nfor index, pivot in enumerate(eng_cnt_text_indexes):\n    for i in range(len(french_expressions)-1):\n        if ((french_expressions.loc[i,'mea_index']<pivot) and (french_expressions.loc[i+1,'mea_index']>pivot)):\n            french_expressions.loc[i, 'text'] = eng_cnt_real[index]\nfrench_expressions.loc[89, 'text'] = eng_cnt_real[-1]","dc1e390f":"# Cleaning dataframe\nfrench_expressions.literally = french_expressions.literally.apply(lambda x:str(x).replace(':','').replace(';', ''))\nfrench_expressions.meaning = french_expressions.meaning.apply(lambda x:str(x).replace(':','').replace(';', ''))\nfrench_expressions.text = french_expressions.text.apply(lambda x:str(x).replace(':','').replace(';', ''))\nfrench_expressions = french_expressions.drop(['lit_index', 'mea_index'], axis = 1)","4ac14ab1":"french_expressions.head()","8a49e95b":"# Save all the data in .csv file\nfrench_expressions.to_csv('French_expressions.csv', index=False)","0a71ec98":"Eng_quotes = pd.read_csv(\"..\/input\/phrases-and-sayings\/English_phrases_and_sayings.csv\")\nChi_quotes = pd.read_csv(\"..\/input\/phrases-and-sayings\/Chinese_proverbs.csv\")\nFre_quotes = pd.read_csv(\"..\/input\/phrases-and-sayings\/French_expressions.csv\")\n\nnew_df = pd.concat([Eng_quotes, Chi_quotes, Fre_quotes], join=\"inner\").reset_index(drop=True)\nnew_df.to_csv('Concatenated_quotes.csv')","9b408f60":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---\n\n# <font color = '#957DAD'>BONUS<\/font>","2fe322fc":"<div align='justify'><font size=\"3\">Note that at any time, each of the website I was working on to scrap quotes might be updated and my code is likely not to work anymore. I will save an image of this notebook so it displays output. The most important in this notebook is to show the process of analyzing a webpage and scraping its data. A webpage is never 100% changing. In most cases, only few adjustements are necessary.<\/font><\/div>","d208a316":"<div align='justify'><font size=3>Now I would like to split each row into several part so I can create a dataframe with columns with the following information:<\/font>\n<br>\n<br>- <font size=3 color ='#D291BC'>The Chinese version of the proverb<\/font> ('<b>in_chinese<\/b>')\n<br>- <font size=3 color ='#D291BC'>The PinYin version with its wordwise translation in English<\/font> ('<b>pin_yin<\/b>')\n<br>- <font size=3 color ='#D291BC'>The English version of the proverb<\/font> ('<b>text<\/b>')<\/div>","66172c4e":"<div align='justify'><font size=3>Now that we have retrieved all the proverbs, we can also associate them with a category. The h2 tags contain the categories into which the different proverbs are divided. We even have the number of proverbs per category, which will allow us to have the exact number of proverbs.<\/font><\/div>","2d872ba7":"<div align='justify'><font size=3>I retrieved the list of all href links associated with each quote. Now for each of these link, a generic scraping will be performed to retrieve the explanation. It is not always that easy because sometimes, the link can be a totally different website with a different organisation of tags and not the same class names. So I will try my best to get as many explanations as I can.<\/font><\/div>","84acae2e":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---\n\n# <div id=\"chap2\"><font color = '#957DAD'>PART II: Chinese Proverbs<\/font><\/div>","32782acb":"# <font color = '#957DAD'>What is web scraping ?<\/font>\n\n<div align='justify'><font size=3>Web scraping, also known as web data extraction, is the process of retrieving or \u201cscraping\u201d data from a website. Unlike the mundane, mind-numbing process of manually extracting data, web scraping uses intelligent automation to retrieve hundreds, millions, or even billions of data points from the internet\u2019s seemingly endless frontier.<\/font><\/div>","00ee2c2d":"(?<=\\() indicates that my pattern start after '('.\n\n(?=\\)) indicates that my pattern will end before ')'.\n\n(.*?) indicates that my pattern catches everything.\n\nOn a whole, I want to get all characters between '(' and ')' excluded.","d6bf50fe":"<div align='justify'><font size=3>Once each of the pattern works on an example, I can try to retrieve all of the matching strings for each observation and drop the column 'all_text'. Now there is still one missing information...<\/font><\/div>","f1f30ae9":"> <div align='justify'><font size=3>I noticed that most of the time, the pages containing explanations have the same structure. I need to find p tag whose class is meanings-body and then if the structure is regular, the explanation can be scraped directly with the .text attribute. When the page is different, I will have a length of quote_explanation equal to zero. So I don't get any error message, I simply put 'NO INFORMATION' string instead of the true explanation.<\/font><\/div>","1cee8b1d":"(?<=\\d\\.) indicates that my pattern start after the number at the beginning of the sentence.\n\n(?=\\() indicates that my pattern will end before '('.\n\n(.*?) indicates that my pattern catches everything.\n\nOn a whole, I want to get all characters between the number and '(' excluded.","cff8f3e2":"<div align='justify'><font size=3>Here we get all the tags in our proverbs_container that are of type p. After retrieving all of it, I define a mask which will return for each p tag if the sentence contained in this tags starts with a number. I will use the previous function for that task. Once I get this mask, I will just have to filter all my p tags defined in 'to_browse' variable.<\/font><\/div>","2c421ccd":"# <font color = '#957DAD'>References<\/font>\n\n* https:\/\/www.scrapinghub.com\/what-is-web-scraping\/ \n* https:\/\/www.phrases.org.uk\/meanings\/phrases-and-sayings-list.html\n* https:\/\/regex101.com\/\n* https:\/\/www.chinahighlights.com\/travelguide\/learning-chinese\/chinese-sayings.htm","c4fae843":"<div align='justify'><font size=\"3\">If by chance you would like to get both English and Chinese quotes in one dataframe, I do it for you.<\/font><\/div>","dba83336":"<div align='center'><font size=\"5\" color='#353B47'>Scraping famous phrases and sayings<\/font><\/div>\n<div align='center'><font size=\"4\" color=\"#353B47\">For an ironclad repartee<\/font><\/div>\n<br>\n<hr>","0505ef46":"<div align='justify'><font size=\"3\">Here, I retrieve the index of the element that should be removed and I delete it. The two lists will be same length than the number of french quotes I retrieved earlier.<\/font><\/div>","746c753e":"> # <div align=\"center\"><font color = '#957DAD'>What are Regular Expressions (RE) ?<\/font><\/div>\n> <img src=\"https:\/\/www.oreilly.com\/content\/wp-content\/uploads\/sites\/2\/2019\/06\/email-regex_crop-ae942dc427c8cebd3a83c52d17389123.jpg\" width=400>\n> <br>\n> <div align='justify'><font size=3>A regular expression is a sequence of characters that define a search pattern. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation. So basically you will have to learn the Regex language to match whatever you whant on a string. I strongly recommend to use <a href=\"https:\/\/regex101.com\/\">this website<\/a>, which is a live testing regex website.<\/font><\/div>","523d2c3f":"# <font color = '#957DAD'>What is HTTP?<\/font>\n\n<div align='justify'><font size=3>HTTP is a set of protocols designed to enable communication between clients and servers. It works as a request-response protocol between a client and server. A web browser may be the client, and an application on a computer that hosts a web site may be the server. To request a response from the server, there are mainly two methods:<\/font><\/div>\n<br>\n\n- <font color ='#D291BC'>GET<\/font> : to request data from the server.\n- <font color ='#D291BC'>POST<\/font> : to submit data to be processed to the server.","8b21a0df":"<div align='justify'><font size=3>Now let's scrap a different website to get Chinese proverbs. They have the peculiarity of having an element of morality, not without a sense of humour as well.<\/font><\/div>","fa482dbb":"<hr>\n<br>\n<div align='justify'><font color=\"#353B47\" size=\"4\">Thank you for taking the time to read this notebook. I hope that I was able to answer your questions or your curiosity and that it was quite understandable. <u>any constructive comments are welcome<\/u>. They help me progress and motivate me to share better quality content. I am above all a passionate person who tries to advance my knowledge but also that of others. If you liked it, feel free to <u>upvote and share my work.<\/u> <\/font><\/div>\n<br>\n<div align='center'><font color=\"#353B47\" size=\"3\">Thank you and may passion guide you.<\/font><\/div>","9f539a16":"<div align='justify'><font size=3>Now we have all the quotes cleaned. The next step consists in retrieving explanations of each quote of that list. To do so, I need to create a list of links so I can scrap the explanation associated with the quote. Let's retrieve our first list of quotes. I can access a tag and href property directly with the following line of code:<\/font><\/div>\n<br>\n\n> <font size = 3>text.a['href']<\/font>","63857246":"<div align=\"center\"><img src=https:\/\/www.apc.edu.ph\/wp-content\/uploads\/2019\/05\/Its-Raining-Cats-and-Dogs_Beatrice-Baylosis.png width=\"60%\"><\/div>","aba50045":"<div align='justify'><font size=3>We obtain a list containing all the p tags of the targeted class. However, we can see that this class covers an a tag in which a link is specified by the href property. This link will be useful to retrieve the explanation of the saying. Indeed, each saying is a link to a page explaining it and its origin.<\/font><\/div>","0ecf1cca":"<div align='justify'><font size=\"3\">Now I have a list of all p tags, I want to retrieve these strong tags with the keywords I mentionned. I define a function which returns if a text as a strong tag in it and if it contains the specified keyword.<\/font><\/div>","be208053":"<div align='justify'><font size=\"3\">The dataframe is almost ready to be used. I perform some cleaning on it, save it and add it to the database.<\/font><\/div>","85a768f4":"<div align='justify'><font size=\"3\">So now, you can wonder why I insist to get the indexes of the Literally and Meaning texts. For a simple reason: in the english counter part texts, I got only 87 elements, so I am not able to associate the french expression with the right english counterpart as there can be an offset introduced at any time. However, I know that a french quote with h3 tag is followed by a Literally text in a p tag which is followed by a Meaning text with a p tag and a english counterpart with also a p tag. So according to the index it has on all p tags, I can set conditions based on english counterpart indexes to find the right position across all french quotes.<\/font><\/div>","4a1fb72b":"<div align='justify'><font size=\"3\">Once I managed to retrieve the html page, I retrieve the tag that allows me to have the list of expressions. The list is easy to retrieve because it is in a specific h3 tag. Only the last two items in the list of retrieved expressions are irrelevant.<\/font><\/div>","7020e165":"---\n\n## <div id=\"summary\">Table of contents<\/div>\n\n**<font size=\"2\"><a href=\"#chap1\">1. English phrases and sayings<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap2\">2. Chinese proverbs<\/a><\/font>**\n**<br><font size=\"2\"><a href=\"#chap3\">3. French expressions<\/a><\/font>**","de8bb8a6":"<div align='justify'><font size=3>Now we are sure to have all the proverbs contained in this variable. To retrieve these proverbs, we need to retrieve only the lines starting with a number. We'll need to do a pattern search in a string of characters, this is called regular expression.<\/font><\/div>","16755612":"<div align='justify'><font size=\"3\">After each h3 tag, there is a succession of p tags. These tags contain the information I want to retrieve: the texts coming along with the strong tag Literally, Meaning and English counterpart.<\/font><\/div>","9061f1d0":"<div align='justify'><font size=\"3\">Then, in a first step I will apply this function to all elements of all_texts list three times, one time per keyword. I will get a list of booleans. With np.where, I can get the indexes of all True values.<\/font><\/div>","5aa893ab":"<div align='justify'><font size=\"3\">Now let's retrieve some French expresions on another website.<\/font><\/div>","3896206a":"<div align='justify'><font size=\"3\">Once I retrieve my indexes for each keyword, I can get the text associated with the specified keyword. Unfortunately, I get 91 elements in the literally_text_indexes and meaning_text_indexes lists instead of 90 and 87 for the eng_cnt_text_indexes. I need to filter the first two ones to be confident on their size.<\/font><\/div>","00e5a684":"<div align='justify'><font size=3 color ='#D291BC'><i>It is raining ropes, take some seed, I am spliting my pear...<\/i> All of these are wrongly translated. I wanted to create this playful notebook to get started with scraping and get as many sayings as possible. First I focuse my research on England country, then I will add more and more countries<\/font><\/div>","d6eacd27":"# <font color = '#957DAD'>Time to scrap !<\/font>\n\n---\n\n# <div id=\"chap1\"><font color = '#957DAD'>PART I: Scraping English sayings<\/font><\/div>","a47dff29":"<div align='center'><font size=\"5\" color='#353B47'>Never give up<\/font><\/div>\n<br>\n\n<div align=\"center\"><img src=\"https:\/\/i.dawn.com\/primary\/2020\/06\/5ef3b46a5fa89.jpg\"><\/div>","92b8f127":"(?<=\\\u2014) indicates that my pattern start after '\u2014'.\n\n$ indicates that my pattern will catch all the remaining characters.\n\n(.*?) indicates that my pattern catches everything.\n\nOn a whole, I want to get all characters after '\u2014'.","87f24d6c":"<div align='justify'><font size=3>Now all the source code is stored into html_soup and we can work with it and select what we need. You will see that bs4 is a powerful library that can save you a lot of time.<\/font><\/div>","310b69a1":"**<font size=\"2\"><a href=\"#summary\">Back to summary<\/a><\/font>**\n\n---\n\n# <div id=\"chap3\"><font color = '#957DAD'>PART III: FRENCH EXPRESSIONS<\/font><\/div>","b40f236d":"<div align='justify'><font size=3>This website is not built in the same way, so you will have to adapt the way you retrieve the data. Unfortunately, there are no miracle methods to automatically adapt to any website, you have to do it on a case by case basis. Here, all the quotes are contained in a div tag.<\/font><\/div>","ecd739c3":"<div align='justify'><font size=3>There is not much things in common with the English phrases and sayings dataset except the 'text' and 'origin' column. It does not really matter, but if you want to join these two datasets, You can do it easily if you keep text and origin columns. My goal here was more focused on scraping different websites and to show that it can be totally different perspectives to scrap data on static web pages.<\/font><\/div>"}}