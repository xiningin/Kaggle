{"cell_type":{"fac4b386":"code","94bf2e76":"code","3f786acb":"code","259fa25d":"code","b7c5667d":"code","e5bd9483":"code","f9b2fc45":"code","e959dba0":"code","23ff9525":"code","351bfcd7":"code","1181c9e7":"code","ba37b0b0":"code","7e3227f3":"code","923a97fb":"code","d3746fec":"code","89aefe6e":"code","fc68a08f":"code","15cf836a":"code","2259aeb1":"code","04f277df":"code","c28bc5cb":"code","e1501f89":"code","e2b7621e":"code","aaba6940":"markdown","9eef784d":"markdown","53decd0f":"markdown","b7f20732":"markdown","81fba6c4":"markdown","c578d8e0":"markdown","dad475b3":"markdown","48deb071":"markdown"},"source":{"fac4b386":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","94bf2e76":"print(os.listdir(\"..\/input\"))","3f786acb":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","259fa25d":"train_df.shape","b7c5667d":"train_df.info()","e5bd9483":"train_df.isnull().values.any()","f9b2fc45":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","e959dba0":"test_df.shape","23ff9525":"test_df.info()","351bfcd7":"test_df.isnull().values.any()","1181c9e7":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","ba37b0b0":"%%time\ntrain_df = load_df()\ntest_df = load_df(\"..\/input\/test.csv\")","7e3227f3":"train_df.head()","923a97fb":"train_df.info()","d3746fec":"train_df.isnull().values.any()","89aefe6e":"train_df_describe = train_df.describe()\ntrain_df_describe","fc68a08f":"test_df.head()","15cf836a":"test_df.shape","2259aeb1":"test_df.info()","04f277df":"test_df_describe = test_df.describe()\ntest_df_describe","c28bc5cb":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission.head()","e1501f89":"sample_submission['PredictedLogRevenue'] = np.log(1.26)\nsample_submission.to_csv('simple_mean.csv', index=False)","e2b7621e":"sample_submission.head()","aaba6940":"We see that the input folder only contains three files ```train.csv```, ```test.csv```, and ```sample_submission.csv```. It seems that for this competition we don't have to do any complicated combination and mergers of files.\n\nNow let's import and take a glimpse at these files.","9eef784d":"Now let us look at the input folder. Here we find all the relevant files for this competition.","53decd0f":"Let's now look at the structure of the flattened files:","b7f20732":"A few things immediately stand out:\n\n1. There is almost a million datapoints, with 12 raw \"features\"\n2. The most interesting features seem to be in JSON format. We will need to do something about them to get them into a format that is suitable for modeling.\n3. There don't seem to be any missing values in the dataset, at least as far as the core raw features are concerned. Once we process JSON, it is possible that we'll find some missing values. Stay tuned.\n\nNow let's take a look at the test dataset.","81fba6c4":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","c578d8e0":"A few things about the test set that we notice:\n\n1. There is a close match in terms of the size with the train set (800,000 vs 900,000)\n2. It also contains the JSON values features.\n3. There don't seem to be any missing values in the dataset either.\n\nNow let's reload the data, dealing with the JSON fields. For this we'll use [Julian's kernel function](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook):\n","dad475b3":"The simplest \"model\" that we could build is to predict the mean value fro all test entries:","48deb071":"And now we'll look at the structure of the flattened test files:"}}