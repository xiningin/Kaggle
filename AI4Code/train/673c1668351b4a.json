{"cell_type":{"9056a8c5":"code","db9f8ee6":"code","eb87301f":"code","1eac41e7":"code","e97b8db0":"code","bb81a399":"code","d9845433":"code","9f077b9b":"code","a565413b":"code","89e6323c":"code","e7472c23":"code","99d2ce8f":"code","d21e3413":"code","d360ffcf":"code","888b1e4a":"code","7ed6ac6c":"code","7588e1a9":"code","a37a9912":"code","ae224322":"code","2abeda2f":"code","4289315e":"code","918d94fc":"code","5a856fb2":"code","e393ce3a":"code","a19bd7ca":"code","17c4d225":"code","eb5021a1":"code","61eb9cf2":"code","ead92a35":"code","8c571c1c":"code","94d499de":"code","cc63cf94":"markdown","21b12c1e":"markdown","59584bea":"markdown","75527018":"markdown","ae0aeeb1":"markdown","b9b00009":"markdown","cddea7ee":"markdown","49b588bc":"markdown","03d94d56":"markdown","b68f8795":"markdown","00bfd442":"markdown","5d663505":"markdown","71281e8d":"markdown","5f255dc9":"markdown","fc3923e7":"markdown","6085eda4":"markdown","b3f72ec1":"markdown","c707c06a":"markdown","e313042e":"markdown","be9aa10e":"markdown","e1a03e99":"markdown"},"source":{"9056a8c5":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","db9f8ee6":"data = pd.read_csv('..\/input\/train.csv')","eb87301f":"data.head()","1eac41e7":"data.info()","e97b8db0":"# Check if this data contains missing values\ndata.isnull().sum().max()","bb81a399":"data['price_range'].value_counts()","d9845433":"data.describe().T","9f077b9b":"corr = data.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr, square=True, annot=True, annot_kws={'size':8})","a565413b":"sns.jointplot(data['ram'], data['price_range'],kind='kde')","89e6323c":"sns.boxplot(data['price_range'], data['ram'])","e7472c23":"X = data.drop(columns='price_range')\ny = data['price_range']","99d2ce8f":"X.var()","d21e3413":"sns.distplot(X['m_dep'])","d360ffcf":"# Remove non-ordinal\nX = X.drop(columns=['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi'])\n# Remove colinearity\nX = X.drop(columns=['fc', 'px_width', 'sc_w'])\n# Remove low variance\nX = X.drop(columns=['m_dep', 'clock_speed'])\n\n\n# X = X[['ram']]\nX.info()","888b1e4a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","7ed6ac6c":"from sklearn.preprocessing import StandardScaler\n\n# Normalize Training Data \nscaler = StandardScaler().fit(X_train)\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)\n\n#Converting numpy array to dataframe\nX_train_std_df = pd.DataFrame(X_train_std, index=X_train.index, columns=X_train.columns)\nX_test_std_df = pd.DataFrame(X_test_std, index=X_test.index, columns=X_test.columns) ","7588e1a9":"X_train_std_df.head()","a37a9912":"train_std_data = pd.concat([X_train_std_df, y_train], axis=1)\ntrain_std_data.var().sort_values()","ae224322":"plt.figure(figsize=(15,10))\nsns.heatmap(train_std_data.corr(method='spearman'), annot=True)","2abeda2f":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()","4289315e":"from sklearn.model_selection import cross_val_score","918d94fc":"def plot_validation(param_grid, clf, X_train, y_train):\n    val_error_rate = []\n\n    for key in param_grid.keys():\n        param_range = param_grid[key]\n        for param in param_range:\n            # https:\/\/stackoverflow.com\/questions\/337688\/dynamic-keyword-arguments-in-python\n            val_error = 1 - cross_val_score(clf.set_params(**{key: param}), X_train, y_train, cv=5).mean()\n            val_error_rate.append(val_error)\n\n        plt.figure(figsize=(15,7))\n        plt.plot(param_range, val_error_rate, color='orange', linestyle='dashed', marker='o',\n                 markerfacecolor='black', markersize=5, label='Validation Error')\n\n        plt.xticks(np.arange(param_range.start, param_range.stop, param_range.step), rotation=60)\n        plt.grid()\n        plt.legend()\n        plt.title('Validation Error vs. {}'.format(key))\n        plt.xlabel(key)\n        plt.ylabel('Validation Error')\n        plt.show()\n    \n\nneighbors_range = range(1,200,5)\nparam_grid = {'n_neighbors': neighbors_range}\nplot_validation(param_grid, knn, X_train_std_df, y_train)","5a856fb2":"best_k = 136\n\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train_std_df, y_train)\n1-knn.score(X_test_std_df, y_test)","e393ce3a":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='linear')","a19bd7ca":"c_range =  range(1,200,20)\nparam_grid = {'C': c_range}\nplot_validation(param_grid, svm, X_train_std_df, y_train)","17c4d225":"best_c = 21\nsvm = SVC(kernel='linear',C=best_c)\nsvm.fit(X_train_std_df, y_train)\nsvm.score(X_test_std_df, y_test)","eb5021a1":"# Using GridSearchCV to tune hyperparameters\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': c_range,\n              'gamma': [.1, .5, .10, .25, .50, 1]}\ngs = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\ngs.fit(X_train_std_df,y_train)","61eb9cf2":"print(\"The best hyperparameters {}.\".format(gs.best_params_))\nprint(\"The Mean CV score of the best_estimator is {:.2f}.\".format(gs.best_score_))","ead92a35":"svm = SVC(kernel='rbf',C=1, gamma=0.1)\nsvm.fit(X_train_std_df, y_train)\nsvm.score(X_test_std_df, y_test)","8c571c1c":"from sklearn.metrics import classification_report,confusion_matrix\n\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train_std_df, y_train)\npred = knn.predict(X_test_std_df)\n\nprint(knn.score(X_test_std_df,y_test))\nprint(classification_report(y_test,pred))\n\nmatrix=confusion_matrix(y_test,pred)\nplt.figure(figsize = (10,7))\nsns.heatmap(matrix,annot=True)","94d499de":"svm = SVC(kernel='linear',C=best_c)\nsvm.fit(X_train_std_df, y_train)\npred = svm.predict(X_test_std_df)\n\nprint(svm.score(X_test_std_df,y_test))\nprint(classification_report(y_test,pred))\n\nmatrix=confusion_matrix(y_test,pred)\nplt.figure(figsize = (10,7))\nsns.heatmap(matrix,annot=True,fmt=\".2f\")","cc63cf94":"### KNN","21b12c1e":"## Data Preprocessing","59584bea":"### GridSearchCV","75527018":"### Train\/Test Split","ae0aeeb1":"## Future works\nIt is noticable that using only relevant features can significantly improve the performance of KNN and SVM. In other words, use all the features can worse the performance.\n\nIt's worth trying starting with `ram`, `battery_power`,`px_height`, and perform feature selection process. \n\n[Feature Selection techniques in Machine Learning - Towards Data Science](https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e)","b9b00009":"**Recall**: The **C** parameter tells the SVM optimization how much you want to avoid misclassifying each training example. \n- For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. \n- Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.\n\nThus for a very large value of C probably leads to overfitting of the model and for a very small value of C probably leads to underfitting. Thus the value of C must be chosen in such a way that it generalises the unseen data well.\n","cddea7ee":"## Non-linear SVM\n\nTechnically, the **gamma** parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points.  Intuitive explaination can be found [here](https:\/\/chrisalbon.com\/machine_learning\/support_vector_machines\/svc_parameters_using_rbf_kernel\/).\n\nWith one hyperparameter, we can plot validation curve as above, but with more than one hyperparameter, we cannot. Therefore, we use GridSearchCV as a more proper and convinient way.","49b588bc":"### Tuning using K-fold Cross Validation\n\nWe can tune using only Train\/Validation set, but there are still problems:\n- We use less training data\n- The model can potentially overfit to validation data, because:\n    - It is optimized based on validation data.\n    - Validation data can be only a small subset -> Cannot represent the population distribution.\n    \n**Solution**: A smart way is using `K-Fold CV`","03d94d56":"# Mobile Price Classification\n\n\n### Meet Bob\nBob has started his own mobile company. He wants to give tough fight to big companies like Apple,Samsung etc.\n\nHe does not know how to estimate price of mobiles his company creates. In this competitive mobile phone market you cannot simply assume things. To solve this problem he collects sales data of mobile phones of various companies.\n\nBob wants to find out some relation between features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price. But he is not so good at Machine Learning. So he needs your help to solve this problem.\n\n> **Notice**: In this problem you do not have to predict actual price but a price range indicating how high the price is.\n\n---\n\n## Target\nIn this Project,On the basis of the mobile Specification like Battery power, 3G enabled , wifi ,Bluetooth, Ram etc we are predicting Price range of the mobile.\n\n## About the Dataset\n- **battery_power**: Total energy a battery can store in one time measured in mAh\n- **blue**: Has bluetooth or not\n- **clock_speed**: Speed at which microprocessor executes instructions\n- **dual_sim**: Has dual sim support or not\n- **fc**: Front Camera mega pixels\n- **four_g**: Has 4G or not\n- **int_memory**: Internal Memory in Gigabytes\n- **m_dep**: Mobile Depth in cm\n- **mobile_wt**: Weight of mobile phone\n- **n_cores**: Number of cores of processor\n- **pc**: Primary Camera mega pixels\n- **px_height**: Pixel Resolution Height\n- **px_width**: Pixel Resolution Width\n- **ram**: Random Access Memory in Megabytes\n- **sc_h**: Screen Height of mobile in cm\n- **sc_w**: Screen Width of mobile in cm\n- **talk_time**: Longest time that a single battery charge will last when you are\n- **three_g**: Has 3G or not\n- **touch_screen**: Has touch screen or not\n- **wifi**: Has wifi or not","b68f8795":"#### Low positive correlation: \n- `pc` and `fc`\n- `three_g` and `four_g`\n- `px_width` and `px_height`\n- `sc_w` and `sc_height`\n\n#### High positive correlation:\n- `ram` and `price_range`.\n","00bfd442":"### Scaling the Features\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n\nIf left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n\nTo supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling.\n\nSince both of KNN and SVM utitlize distance calculation behind the scene, we need to scale our features so that the large-valued features do not dominate the other features. More can be referenced [here](https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e) and [here](http:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html#about-standardization).\n\n**Notice**: It is worth trying not scaling the features. So that we can see that for SVM,\n- The training time is very long (computationally expensive)\n- The model overfits\n\n### Multiple ways to scale\n- **Standard Scaler (Standardization)**: Scale the feature by shifting the mean back to 0 and variance to 1. By this way, we only shift the mean value to 0 and keep the distribution the same. Furthermore, this way can presereve the outliers in case they can contribute additional information to the problem.\n- **Min-max Scaler (Normalization)**: This one scales the range of values to between 0 and 1 and also eliminates the outliers.\n- Read more:\n    - Python machine Learning - SebastianRaschka\n    - [Medium - Why, How and When to scale your features ?](https:\/\/medium.com\/greyatom\/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n    - [Quora](https:\/\/www.quora.com\/When-should-you-perform-feature-scaling-and-mean-normalization-on-the-given-data-What-are-the-advantages-of-these-techniques)\n    - [Kaggle - Very good notebook](https:\/\/www.kaggle.com\/rtatman\/data-cleaning-challenge-scale-and-normalize-data?scriptVersionId=2945378)\n    - [statsStackExchange - Scaling ruins the result](https:\/\/stats.stackexchange.com\/questions\/172795\/scaling-for-svm-destroys-my-results)\n    - [GeeksforGeeks - How and when to apply Scaling](https:\/\/www.geeksforgeeks.org\/python-how-and-where-to-apply-feature-scaling\/)\n\n**Question here is:** \n- Which one keeps the original distribution of the data ?\n    - Ans: Both\n    \n### More about Standardization and Normalization\n- [StatisticsHowTo - Standardized Data](https:\/\/www.statisticshowto.datasciencecentral.com\/standardized-values-examples\/)\n- [StatisticsHowTo - Normalized Data](https:\/\/www.statisticshowto.datasciencecentral.com\/normalized\/)\n\n### Multiple ways to process the scaling\n- Fit the scaler on the whole dataset, scale the whole dataset and then train\/test split\n- Train\/test split and Fit the scaler (to obtain `mean` and `std`) to training set and then scale both training + testing set\n- Train\/test split and Fit 2 different scalers on the training and testing set and scale them.\n- **The correct way is:** the 2nd answer.","5d663505":"## Exploring Data","71281e8d":"## Inspecting Results","5f255dc9":"## SVM","fc3923e7":"**To sum up**: The dataset is already well-balanced and does not contain any missing values.","6085eda4":"### SVM","b3f72ec1":"### References\n- https:\/\/www.kaggle.com\/azzion\/svm-for-beginners-tutorial\n- https:\/\/www.kaggle.com\/vikramb\/mobile-price-prediction\/notebook\n- https:\/\/www.kaggle.com\/nirajvermafcb\/support-vector-machine-detail-analysis","c707c06a":"## Preparing Data for Classification","e313042e":"## KNN","be9aa10e":"### Overfitting\n- Seems like our model overfit slightly.\n- The most likely cause is the feature **ram**. Which has a high correlation with target variable and a high scale comparing to other features.\n\n\n\n### Additional refs for Overfitting\n- DataQuest\n- https:\/\/stackoverflow.com\/questions\/37776333\/why-too-many-features-cause-over-fitting\n- https:\/\/elitedatascience.com\/overfitting-in-machine-learning\n- https:\/\/elitedatascience.com\/python-machine-learning-tutorial-scikit-learn\n- https:\/\/stats.stackexchange.com\/questions\/202318\/how-much-is-too-much-overfitting\n- http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.556.7571&rep=rep1&type=pdf\n- https:\/\/machinelearningcoban.com\/2017\/03\/04\/overfitting\/\n- https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html","e1a03e99":"### Feature Selection\n- Eliminate non-numerical\n- Eliminate non-ordinal\n- Eliminate features that have missing values\n- Eliminate colinearity (pc \/ fc, px_width \/ px_height, sc_h \/ sc_w)\n- Eliminate low variance features (since they do not carry much information)"}}