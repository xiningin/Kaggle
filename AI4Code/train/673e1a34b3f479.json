{"cell_type":{"4d1f41fa":"code","263ba442":"code","b4ae0648":"code","2cdb9e8d":"code","506a5ecd":"code","282a6613":"code","b8a6f474":"code","258423ca":"code","a82a9946":"code","f68c548d":"code","c4006a29":"code","11075d51":"code","d3cc22c5":"code","9522b74c":"code","655edaa5":"code","493577ad":"code","bd79d316":"code","7a8086e2":"code","4094c999":"code","935f24d0":"code","ad51d641":"code","509dfd42":"code","6a949fb5":"code","9c4f40f3":"code","71e6059d":"code","3fe84a32":"code","476237d6":"code","ff69c852":"code","29f71408":"code","64a6d11a":"code","2afab54f":"code","9dcb5e1c":"markdown","bf69e1ba":"markdown","41238aa6":"markdown","67a03260":"markdown","45385a6b":"markdown","a2d6822f":"markdown","cd87996c":"markdown","9980d2c3":"markdown","806b74fe":"markdown","7cb62f72":"markdown","4072e067":"markdown","09589f85":"markdown","b9be43f2":"markdown","79477758":"markdown","609c385e":"markdown","34769822":"markdown","c10ccc67":"markdown","eb2d3160":"markdown","baf5a9a2":"markdown","e0da511b":"markdown","1ac7a621":"markdown","6327899d":"markdown","f71f2b30":"markdown","2ad49d27":"markdown","be4cdf5d":"markdown"},"source":{"4d1f41fa":"# importing the libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder","263ba442":"# defining the dataset\ndatapath = '..\/input\/mushroom\/mushroom.csv'\ndf = pd.read_csv(datapath)","b4ae0648":"print(df.describe())","2cdb9e8d":"print(df.head())","506a5ecd":"print(df.tail())","282a6613":"print(df.shape)","b8a6f474":"print(df.columns)","258423ca":"print(df.nunique())","a82a9946":"print(df.info())","f68c548d":"sb.countplot(x='Class', data=df)","c4006a29":"# pre-processing the data\nle = LabelEncoder()\nfor feature in df.columns :\n    df[feature] = le.fit_transform(df[feature])","11075d51":"print(df.head())","d3cc22c5":"plt.figure(figsize=(20, 15))\ncorr = df.corr()\nsb.heatmap(corr, annot = True)\nplt.show()","9522b74c":"df = df.drop([\"VeilType\"],axis=1)","655edaa5":"print('VeliType' in df.columns)","493577ad":"df_div = pd.melt(df, \"Class\", var_name=\"Characteristics\")\nfig, ax = plt.subplots(figsize=(20,15))\np = sb.violinplot(ax = ax, x=\"Characteristics\", y=\"value\", hue=\"Class\", split = True, data=df_div, inner = 'quartile', palette = 'Set1')\ndf_no_class = df.drop([\"Class\"],axis = 1)\np.set_xticklabels(rotation = 90, labels = list(df_no_class.columns));","bd79d316":"# Split into features and classes\nx = df.loc[:, df.columns != \"Class\"]\ny = df[\"Class\"]","7a8086e2":"# splitting data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)","4094c999":"# algorithm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier","935f24d0":"# model object\nclfBayes = GaussianNB()\nclfKNN = KNeighborsClassifier()\nclfForest = RandomForestClassifier()\nclfSVM = SVC()\nclfLR = LogisticRegression()\nclfTree = DecisionTreeClassifier()\nclfNeural = MLPClassifier()","ad51d641":"# fitting model in decision tree\nclfTree = clfTree.fit(x_train,y_train)","509dfd42":"from sklearn import tree\nimport graphviz\ndot_data = tree.export_graphviz(clfTree, out_file=None,\n                         feature_names=x.columns,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraphviz.Source(dot_data)","6a949fb5":"features_list = x.columns.values\nfeature_importance = clfTree.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\n\nplt.figure(figsize=(20,7))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\nplt.xlabel('Importance')\nplt.title('Feature importances')\nplt.draw()\nplt.show()","9c4f40f3":"drop_list = ['CapColor', 'Odor', 'GillAttachment', 'StalkRoot', 'GillColor', 'StalkSurfaceAboveRing', 'StalkColorAboveRing', 'StalkColorBelowRing', 'RingType' ,'CapShape']\nfor drop_feature in drop_list:\n  df = df.drop([drop_feature],axis=1)","71e6059d":"print(df.columns)\nprint(df.columns.size)","3fe84a32":"# accuracy metrix\nmodels = []\n\n# fitting model in bayes\nclfBayes.fit(x_train,y_train)\npred = clfBayes.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n\n# fitting model in knn\nclfKNN.fit(x_train,y_train)\npred = clfKNN.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n\n# fitting model in forest\nclfForest.fit(x_train,y_train)\npred = clfForest.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n\n# fitting model in bayes\nclfSVM.fit(x_train,y_train)\npred = clfSVM.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n\n# fitting model in Logistic regression\nclfLR.fit(x_train,y_train)\npred = clfLR.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n\n\n# fitting model in decision tree\nclfTree.fit(x_train,y_train)\npred = clfTree.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n\n# fitting model in neural network\nclfNeural.fit(x_train,y_train)\npred = clfNeural.predict(x_test)\nmodels.append(accuracy_score(y_test,pred))\n","476237d6":"# printing accuracy\nprint('Bayes accuracy = ', models[0])\nprint('KNN accuracy = ', models[1])\nprint('Random Forest accuracy = ', models[2])\nprint('SVM accuracy = ', models[3])\nprint('Logistic Regression accuracy = ', models[4])\nprint('Decision Tree accuracy = ', models[5])\nprint('Neural Network accuracy = ', models[6])\n","ff69c852":"names = ['Bayes', 'KNN', 'RF', 'SVM', 'LR', 'DT', 'NN']\nplt.figure(figsize=(20,5))\nplt.bar(names, models, color=['black', 'red', 'green', 'blue', 'cyan', 'yellow', 'purple'])\nplt.show()","29f71408":"y_pred = clfNeural.predict(x_test)\nconf = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6, 6))\nsb.heatmap(conf , annot = True,  linewidths=.5, cbar =None)\nplt.title('Neural Network Classifier confusion matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","64a6d11a":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n\nplt.figure(figsize=(8,8))\nplt.title('Reciever Operating Characteristics')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.plot(fpr,tpr, color='purple')\nplt.show()","2afab54f":"plt.figure(figsize=(8,8))\nplt.title('Area Under Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nauc = metrics.roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"auc = \"+str(auc), color='darkorange')\nplt.legend()\nplt.show()","9dcb5e1c":"Now let's see accuracy visually.","bf69e1ba":"Perfect. Let's check the importance of the features.","41238aa6":"Unnecessary features has removed. now let's train our models for all 7 models. for evaluating accuracy, we need an accuracy matrix.\n","67a03260":"perfect. we can see that feature Velitype has no relation at all. we can simply drop the feature. for that, we use\n\n```\ndf = df.drop([\"VeilType\"],axis=1)\n```\n","45385a6b":"\n\n```\nAUC Curve\n```\n\n","a2d6822f":"Cool. it has dropped. now let's have a quick look at the characteristics of the data. for that, we check\n\n```\nsb.violinplot(ax = ax, x=\"Characteristics\", y=\"value\", hue=\"Class\", split = True, data=df_div, inner = 'quartile', palette = 'Set1')\ndf_no_class = df.drop([\"Class\"],axis = 1)\n```\n\n","cd87996c":"\n\n```\n\"\"\"\n    mushroom classification problem\n    author: Nur Mohammad Bijoy\n    contact: nurmdbijoy@gmail.com\n    dataset: https:\/\/github.com\/nurbijoy\/research\/blob\/master\/dataset\/mushroom.csv\n    credits: udacity.com, towardsdatascience.com, kaggle.com, google.com\n\"\"\"\n```\n\n\n\nfirst of all, let's import important libraries","9980d2c3":"```\nROC Curve\n```\n","806b74fe":"we'll train our data into 7 models and check which one classifies perfectly.\nfor that we'll use 7 models names:\n\n```\nNaive Bayes\nK Nearest Neighbour\nRandom Forest\nSupport Vector Machine\nLogistic Regression\nDecision Tree\nNeural Network\n```\nNow let's create object of the models.","7cb62f72":"So cool. Now let's work with the models. For that, at first we need to split the data into features and class.\nfor that we use, \n```\nx = df.loc[:, df.columns != \"Class\"]\ny = df[\"Class\"]\n```\n\n","4072e067":"ok. we have checked the dataset. the dataset contains 8123 entries each having 23 columns. we can see that, none of the entries contains null values. so far it's okay. now let's visualize the data.","09589f85":"Almost done. A few things need to be performed.\nWe'll measure model performance. For that we'll check confusion matrix and AUC, ROC curve.\nlet's start with the confusion matrix.","b9be43f2":"trained the model. now let's visualize how it works. for that, we need to import grapviz library. let's do it.","79477758":"Now let's check whether it has dropped or not.\nfor that we simply check,\n\n```\n'VeliType' in df | returns True if exist otherwise False\n```\n","609c385e":"perfect. Now let's check features list.","34769822":"so far, it's okay. but the problem is that, the data is in the form of string. our model can't work with the string data. we need numeric data.for that, we need to encode the data.\n\n```\nLabelEncoder() | performs label encoding.\n```\nThat means, it replaces the string data to numeric data.\n","c10ccc67":"now let's define the the datapath.","eb2d3160":"ok, datapath is set. now let's get familiar with the dataset. for this, we'll check dataframe information such as \n\n```\ndf.describe() | shows necessary information about the dataset\ndf.head() | head of the dataset\ndf.tail() | tail of the dataset\ndf.shape | dataset shape\ndf.columns | dataset columns\ndf.nunique() | unique feature types count\ndf.info() | doing some data analysis\n```\n\n","baf5a9a2":"splitted successfully. now let's import models.","e0da511b":"now it's time to print accuracy.\n","1ac7a621":"the data successfully splitted into features and class. now let's split the data into train and test.\nfor that, we use\n\n```\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n```\nhere, test_size=0.3 means we split whole data into 70:30 form so that 70% data will be used for training and 30% data will be used for testing.\n","6327899d":"For the Decision Tree classifier, CapColor, Odor, GillAttachment, StalkRoot, GillColor, StalkSurfaceAboveRing, StalkColorAboveRing, StalkColorBelowRing, RingType and CapShape is not import. We can drop the features.","f71f2b30":"the data was encoded successfully. we can see that the data contains only numeric values.\nnow let's check correlation matrix to find the correlation among the features. for that, we use\n\n\n```\ndf.corr() | finds the correlation matrix\nsb.heatmap(df.corr(), annot = True) | visualize the correlation matrix\n```\n","2ad49d27":"let's visualize the Decision Tree classifier for simplicity how it works.\nfirst let's create the model object.","be4cdf5d":"okay. lebel encoding is complete. let's check whether it worked or not.\nfor that, we check dataframe head."}}