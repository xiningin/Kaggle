{"cell_type":{"b43d65a8":"code","dda2e5b0":"code","4566b61e":"code","86a87632":"code","63962a62":"code","b577d54f":"code","20d2dccd":"code","f616ef0d":"code","05bd1872":"code","d6d72961":"code","9fbe5ac6":"code","dc69ebbb":"markdown","4b63a422":"markdown","b32badc6":"markdown","3d0c6b71":"markdown","780f1442":"markdown","20f9370d":"markdown","e87556b8":"markdown"},"source":{"b43d65a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dda2e5b0":"\nimport os\nimport re\n#import cmudict\n\n\nimport torch\nfrom transformers import BertTokenizer, BertModel, BertConfig,BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pandas as pd\n","4566b61e":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","86a87632":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_path = '..\/input\/bert-base-uncased'\ntokenizer= BertTokenizer.from_pretrained(model_path)\nmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\nmodel.to(device)\noptim = AdamW(model.parameters(), lr=5e-5)\n","63962a62":"class loader(torch.utils.data.Dataset):\n    def __init__(self, train_tokens, labels):\n        self.train_tokens = train_tokens\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        \n        #train_tokens= tokenizer.batch_encode_plus(self.text_list,max_length=512,padding='longest',truncation=True)\n        \n\n        item = {key: torch.tensor(val[idx]) for key, val in self.train_tokens.items()}\n        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.float)\n        return item\n\n    def __len__(self):\n        return len(self.labels)","b577d54f":"train_tokens= tokenizer.batch_encode_plus(train['excerpt'].tolist(),max_length=512,padding='longest',truncation=True)\ntrain_dataset = loader(train_tokens, train.target.values)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)","20d2dccd":"for epoch in range(3):\n    for batch in train_loader:\n\n        print(\".\", end=\"\", flush=True)        \n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs[0]\n        loss.backward()\n        optim.step()","f616ef0d":"\ntest_tokens= tokenizer.batch_encode_plus(test['excerpt'].tolist(),max_length=512,padding='longest',truncation=True)\ntest['target']=0\ntest_dataset = loader(test_tokens, test.target.values)\n\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)","05bd1872":"total_preds = []\n\n\n# iterate over batches\nfor step,batch in enumerate(test_loader):\n    print(step)\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n    with torch.no_grad():\n    \n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n   \n        preds = outputs[1].detach().cpu().numpy()\n    \n        total_preds.append(preds)\n","d6d72961":"test['target']= np.concatenate(total_preds, axis=0)","9fbe5ac6":"submission= test[['id', 'target']]\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","dc69ebbb":"#Load Data","4b63a422":"#create a loader","b32badc6":"#data loader for test dataset","3d0c6b71":"#predict on test dataset","780f1442":"#load bert base uncased for sequence classifictaion. Use one label for regression","20f9370d":"#submit!!","e87556b8":"#we will jusrt train it for 3 epochs . I have not put validation here. Can use to get even better results."}}