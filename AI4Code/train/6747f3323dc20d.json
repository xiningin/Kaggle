{"cell_type":{"69a1095d":"code","1e30744f":"code","2037098b":"code","0dd113cb":"code","2a393d03":"code","8c5aaaa9":"code","ce3bba54":"code","b2d813c0":"code","5c238e85":"code","c733eb3f":"code","bebf8fe3":"code","86bcd3bd":"code","2896b0c1":"code","d3ebc955":"code","b4765012":"code","dce521d8":"code","2e206d06":"code","defce5b4":"code","a30bc349":"code","85caa74f":"code","6d528ddc":"code","2739c589":"code","344b226b":"code","d9607166":"code","15b6aa21":"code","bc427c2b":"code","c367bec1":"code","11311db8":"code","2f5f0e4a":"code","56bdbbbf":"code","d4e2b6b6":"code","acbf19e4":"code","0f8eec72":"code","c6a4cf45":"code","4d5f05f5":"code","53f9fad8":"code","b16b0b38":"code","ff38b2ad":"code","79f19b7c":"code","0de2a478":"code","9d6aaf31":"code","2f175901":"code","116f8f11":"code","3c763524":"code","44b977b4":"code","f2f5f4b4":"code","7e4614c9":"code","07c10fc4":"code","67b42100":"code","60cc7a19":"code","d5131501":"code","6f85a485":"code","47504ee9":"code","c629ab5e":"code","5208adf7":"code","71cd334c":"code","af9e1404":"code","0aba583b":"code","346e1498":"code","9a815710":"code","75a1a4d9":"code","eae0c014":"code","ad7d726c":"code","9efab684":"code","ecee7933":"code","0c470d5e":"code","a0225a97":"code","50cfa3d7":"code","768ea589":"code","93433d61":"code","c48bbdb2":"code","212f2dfb":"code","2b993d80":"code","7689bd69":"code","21d79fc0":"code","f751624f":"code","5744f895":"code","c2eb5db6":"code","4f3de96c":"code","fdb8dd45":"code","d1671705":"code","46ba6858":"code","1815c698":"code","8f377c2b":"code","cb951ccb":"code","76912aa4":"code","1b43883c":"code","18e92e00":"code","ea9128cf":"code","9f3b5c55":"code","5b0ea203":"code","e545a6bf":"code","578b8522":"code","969620c8":"code","fdbb50c2":"code","c3becd21":"code","60e40b57":"code","37e9c2a2":"code","02fec1ee":"code","3242e1cf":"code","46e0c4d5":"code","7d3108bd":"code","87d2cd18":"code","2238c789":"code","e0ae891e":"code","ed34ebfd":"code","7c7b3ba6":"code","dd1c79f0":"code","50298a5c":"code","33c8d4da":"code","021e1fb6":"code","6344ee52":"code","a5a120bf":"code","98615a3b":"code","04148520":"code","500cbcac":"code","a606c4b6":"code","14cecbca":"code","f195de63":"code","b652a454":"code","3f378bab":"code","c6607261":"code","ebb0b349":"code","97b78623":"code","424356a5":"code","f0aba721":"code","2f5ac6d4":"code","f2efb3f1":"code","6aaf71f0":"code","edfc0694":"code","b9c8cf66":"code","4fc6c18c":"code","652069a0":"code","222c3e14":"code","8d235c8f":"code","c3681c06":"code","95748492":"code","3e1c9aa6":"code","1f3d4ee1":"code","06ed9729":"code","8bb80046":"code","c3cbb1f8":"code","58b3cdea":"code","5f20c881":"code","8a8cdb8c":"code","3a0c627c":"code","443114c0":"code","179f5e23":"code","e537b8c6":"markdown","772d2244":"markdown","dde722e0":"markdown","d83f2813":"markdown","706861e1":"markdown","449370b4":"markdown","b992385d":"markdown","cf8da644":"markdown","835d4d23":"markdown","e62175e9":"markdown","172ceab2":"markdown","6c004f3d":"markdown","b17ed9ef":"markdown","79e761ee":"markdown","53372850":"markdown","e9712b1a":"markdown","32f24493":"markdown","9c83c8f9":"markdown","9b3ff2d0":"markdown","7cd7e865":"markdown","1dfe7f4e":"markdown","0ce399b3":"markdown","6d85ea92":"markdown","e0dd43a2":"markdown","a5d863df":"markdown","2af8736a":"markdown","c944dc02":"markdown","184ec0e1":"markdown","9c2398d6":"markdown","58442d02":"markdown","21c4d2f1":"markdown","ee80f007":"markdown","ee84d573":"markdown","40aca2d6":"markdown","989832f4":"markdown","2591aaf6":"markdown","7eed1642":"markdown","83e11dad":"markdown","7d8f8e24":"markdown","b407283b":"markdown"},"source":{"69a1095d":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\nfrom scipy.stats import pearsonr","1e30744f":"df = pd.read_csv('..\/input\/airlines-customer-satisfaction\/Invistico_Airline.csv')\ndf.tail()","2037098b":"df_fe = df.copy()","0dd113cb":"df_fe.info()","2a393d03":"df_fe.columns","8c5aaaa9":"df_fe['Seat comfort'].value_counts()","ce3bba54":"def replaceSeatComfort(df_fe):\n    return 0 if(df_fe['Seat comfort'] <= 3) else 1\n\ndf_fe['Seat comfort'] = df_fe.apply(lambda df_fe: replaceSeatComfort(df_fe), axis = 1)\n\ndf_fe.head()","b2d813c0":"df_fe['Seat comfort'].value_counts()","5c238e85":"df_fe['Departure\/Arrival time convenient'].value_counts()","c733eb3f":"def replaceDATC(df_fe):\n    return 0 if(df_fe['Departure\/Arrival time convenient'] <= 3) else 1\n\ndf_fe['Departure\/Arrival time convenient'] = df_fe.apply(lambda df_fe: replaceDATC(df_fe), axis = 1)\n\ndf_fe.head()","bebf8fe3":"df_fe['Departure\/Arrival time convenient'].value_counts()","86bcd3bd":"df_fe['Food and drink'].value_counts()","2896b0c1":"def replaceDATC(df_fe):\n    return 0 if(df_fe['Food and drink'] <= 3) else 1\n\ndf_fe['Food and drink'] = df_fe.apply(lambda df_fe: replaceDATC(df_fe), axis = 1)\n\ndf_fe.head()","d3ebc955":"df_fe['Food and drink'].value_counts()","b4765012":"df_fe['Gate location'].value_counts()","dce521d8":"def replaceGL(df_fe):\n    return 0 if(df_fe['Gate location'] <= 3) else 1\n\ndf_fe['Gate location'] = df_fe.apply(lambda df_fe: replaceGL(df_fe), axis = 1)\n\ndf_fe.head()","2e206d06":"df_fe['Gate location'].value_counts()","defce5b4":"df_fe['Inflight wifi service'].value_counts()","a30bc349":"def replaceIWS(df_fe):\n    return 0 if(df_fe['Inflight wifi service']<= 3) else 1\n\ndf_fe['Inflight wifi service'] = df_fe.apply(lambda df_fe: replaceIWS(df_fe), axis = 1)\n\ndf_fe.head()","85caa74f":"df_fe['Inflight wifi service'].value_counts()","6d528ddc":"df_fe['Inflight entertainment'].value_counts()","2739c589":"def replaceIE(df_fe):\n    return 0 if(df_fe['Inflight entertainment'] <= 3) else 1\n\ndf_fe['Inflight entertainment'] = df_fe.apply(lambda df_fe: replaceIE(df_fe), axis = 1)\n\ndf_fe.head()","344b226b":"df_fe['Inflight entertainment'].value_counts()","d9607166":"df_fe['Online support'].value_counts()","15b6aa21":"def replaceOS(df_fe):\n    return 0 if(df_fe['Online support'] <= 3) else 1\n\n\ndf_fe['Online support'] = df_fe.apply(lambda df_fe: replaceOS(df_fe), axis = 1)\n\ndf_fe.head()","bc427c2b":"df_fe['Online support'].value_counts()","c367bec1":"df_fe['Ease of Online booking'].value_counts()","11311db8":"def replaceEOOB(df_fe):\n    return 0 if(df_fe['Ease of Online booking'] <= 3) else 1\n\ndf_fe['Ease of Online booking'] = df_fe.apply(lambda df_fe: replaceEOOB(df_fe), axis = 1)\n\ndf_fe.head()","2f5f0e4a":"df_fe['Ease of Online booking'].value_counts()","56bdbbbf":"df_fe['On-board service'].value_counts()","d4e2b6b6":"def replaceOBS(df_fe):\n    return 0 if(df_fe['On-board service'] <= 3) else 1\n\ndf_fe['On-board service'] = df_fe.apply(lambda df_fe: replaceOBS(df_fe), axis = 1)\n\ndf_fe.head()","acbf19e4":"df_fe['On-board service'].value_counts()","0f8eec72":"df_fe['Leg room service'].value_counts()","c6a4cf45":"def replaceLRS(df_fe):\n    return 0 if(df_fe['Leg room service'] <= 3) else 1\n\ndf_fe['Leg room service'] = df_fe.apply(lambda df_fe: replaceLRS(df_fe), axis = 1)\n\ndf_fe.head()","4d5f05f5":"df_fe['Leg room service'].value_counts()","53f9fad8":"df_fe['Baggage handling'].value_counts()","b16b0b38":"def replaceBH(df_fe):\n    return 0 if(df_fe['Baggage handling'] <= 3) else 1\n\ndf_fe['Baggage handling'] = df_fe.apply(lambda df_fe: replaceBH(df_fe), axis = 1)\n\ndf_fe.head()","ff38b2ad":"df_fe['Baggage handling'].value_counts()","79f19b7c":"df_fe['Checkin service'].value_counts()","0de2a478":"def replaceCS(df_fe):\n    return 0 if(df_fe['Checkin service'] <= 3) else 1\n\ndf_fe['Checkin service'] = df_fe.apply(lambda df_fe: replaceCS(df_fe), axis = 1)\n\ndf_fe.head()","9d6aaf31":"df_fe['Checkin service'].value_counts()","2f175901":"df_fe['Cleanliness'].value_counts()","116f8f11":"def replaceCleanliness(df_fe):\n    return 0 if(df_fe['Cleanliness'] <= 3) else 1\n\ndf_fe['Cleanliness'] = df_fe.apply(lambda df_fe: replaceCleanliness(df_fe), axis = 1)\n\ndf_fe.head()","3c763524":"df_fe['Cleanliness'].value_counts()","44b977b4":"df_fe['Online boarding'].value_counts()","f2f5f4b4":"def replaceOB(df_fe):\n    return 0 if(df_fe['Online boarding'] <= 3) else 1\n\ndf_fe['Online boarding'] = df_fe.apply(lambda df_fe: replaceOB(df_fe), axis = 1)\n\ndf_fe.head()","7e4614c9":"df_fe['Online boarding'].value_counts()","07c10fc4":"cats = ['Seat comfort',\n       'Departure\/Arrival time convenient', 'Food and drink', 'Gate location',\n       'Inflight wifi service', 'Inflight entertainment', 'Online support',\n       'Ease of Online booking', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Cleanliness', 'Online boarding']","67b42100":"for col in cats:\n    print(f'''Value count kolom {col}:''')\n    print(df_fe[col].value_counts())\n    print()","60cc7a19":"df_fe['Point of Satisfaction'] = df_fe['Seat comfort'] + df_fe['Departure\/Arrival time convenient'] + df_fe['Food and drink'] + df_fe['Gate location'] + df_fe['Inflight wifi service'] + df_fe['Inflight entertainment'] + df_fe['Online support'] + df_fe['Ease of Online booking'] + df_fe['On-board service'] + df_fe['Leg room service'] + df_fe['Baggage handling'] + df_fe['Checkin service'] + df_fe['Cleanliness'] + df_fe['Online boarding']","d5131501":"df_fe.tail()","6f85a485":"def perbandingan1(df_fe):\n    satisfied = df_fe['satisfaction'] == 'satisfied' \n    point = df_fe['Point of Satisfaction']\n    return pd.concat([satisfied, point], axis = 1, keys = ['satisfied', 'point'])\n\nperbandingan1(df_fe)","47504ee9":"df_fe_s = df_fe[df_fe['satisfaction']=='satisfied']\ndf_fe_d = df_fe[df_fe['satisfaction']=='dissatisfied']\n\n# Show distribution graph each satisfaction by point (customer)\n## show satisfied\nsns.set_style('whitegrid')\nsns.set_context('paper', font_scale=1.3)\nfig,ax = plt.subplots(1,1,figsize=(18,9))\nsns.countplot(data=df_fe_s,x='Point of Satisfaction')\nax.set_title('Satisfied Count by Point')\nplt.tight_layout()\nplt.show()\n\n## show dissatisfied\nsns.set_style('whitegrid')\nsns.set_context('paper', font_scale=1.3)\nfig,ax = plt.subplots(1,1,figsize=(18,9))\nsns.countplot(data=df_fe_d,x='Point of Satisfaction')\nax.set_title('Dissatisfied Count by Point')\nplt.tight_layout()\nplt.show()","c629ab5e":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\nfrom scipy.stats import pearsonr\n\nimport pandas as pd\npd.set_option('max_columns', None)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n","5208adf7":"sns.set(rc={'figure.figsize':(20.7,8.27)})\nsns.set_style('whitegrid')\nsns.color_palette('dark')\nplt.style.use('fivethirtyeight')","71cd334c":"from matplotlib import rcParams\nrcParams['figure.figsize'] = 12, 4\nrcParams['lines.linewidth'] = 3\nrcParams['xtick.labelsize'] = 'x-large'\nrcParams['ytick.labelsize'] = 'x-large'","af9e1404":"df_1 = pd.read_csv('..\/input\/airlines-customer-satisfaction\/Invistico_Airline.csv')\ndata = df_1.copy()\ndata.head()","0aba583b":"data.info()","346e1498":"data.columns","9a815710":"numeric=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndf_num=data.select_dtypes(include=numeric)\ndf_num.head()","75a1a4d9":"df_cat=data.select_dtypes(include='object')\ndf_cat.head()","eae0c014":"describeNum = data.describe(include =['float64', 'int64', 'float', 'int'])\ndescribeNum.T.style.background_gradient(cmap='viridis',low=0.2,high=0.1)","ad7d726c":"describeNumCat = data.describe(include=['O'])\ndescribeNumCat.T.style.background_gradient(cmap='viridis',low=0.2,high=0.1)","9efab684":"cats = ['satisfaction',\t'Gender',\t'Customer Type',\t'Type of Travel',\t'Class'] \nfor col in cats:\n    print(f'''Value count kolom {col}:''')\n    print(data[col].value_counts())\n    print()","ecee7933":"ins1 = data.copy()","0c470d5e":"ins1['GenderXClassXCustomerType'] = ins1['Gender'] +'-'+ins1['Class'] +'-'+ins1['Customer Type']\ninsight1 = ins1.groupby(['GenderXClassXCustomerType','satisfaction']).agg({'Gender':'count'}).reset_index()\ninsight1.columns = ['GenderXClassXCustomerType','satisfaction','JumlahCustomer']\ninsight1","a0225a97":"gcc = insight1['GenderXClassXCustomerType']\ns = insight1['satisfaction']\njcc = insight1['JumlahCustomer']","50cfa3d7":"plt.figure(figsize=(15,15))\nplt.title('Pengaruh Gender-Class-CustomerType Terhadap Satisfaction Customer', fontsize = 16);\nsns.barplot(x = jcc, y = gcc, hue = s);\nplt.ylabel('Gender-Class-CustomerType', fontsize = 12)\nplt.xlabel('Jumlah Customer', fontsize = 12);\n","768ea589":"ins2 = data.copy()","93433d61":"fig,axs = plt.subplots(2,2,figsize=(14, 14))\ncols=['Gender', 'Customer Type', 'Type of Travel', 'Class']\nc=0\nfor i in range(2):\n  for j in range(2):\n    sns.countplot(data=ins2,x=cols[c],hue='satisfaction',ax=axs[i][j])\n    axs[i][j].set_title('Customer Satisafaction as per {}'.format(cols[c]))\n    c+=1","c48bbdb2":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Binary encoding\n    df['Gender'] = df['Gender'].replace({\n        'Female': 0,\n        'Male': 1\n    })\n    df['Customer Type'] = df['Customer Type'].replace({\n        'disloyal Customer': 0,\n        'Loyal Customer': 1\n    })\n    df['Type of Travel'] = df['Type of Travel'].replace({\n        'Personal Travel': 0,\n        'Business travel': 1\n    })\n    df['Class'] = df['Class'].replace({\n        'Eco': 0,\n        'Eco Plus': 1,\n        'Business': 2\n    })\n    \n    return df","212f2dfb":"X = preprocess_inputs(data)","2b993d80":"X.head()","7689bd69":"data = X.copy()\ndf = X.drop('satisfaction', axis=1).copy()\nnonbinary_columns = [column for column in df.columns if len(df[column].unique()) > 2]\n\n","21d79fc0":"plt.figure(figsize=(20, 20))\n\nfor i, column in enumerate(nonbinary_columns):\n    plt.subplot(4, 6, i + 1)\n    sns.boxplot(data=df[column], palette='Blues')\n    #sns.boxplot(y=df[column],color='green')\n    plt.title(column)\n\nplt.suptitle('Boxplots With Outliers', size=30)\nplt.show()","f751624f":"data.corr()","5744f895":"fig, ax = plt.subplots(figsize = (25,12))\nsns.heatmap(abs(data.corr()), annot = True);","c2eb5db6":"null=pd.DataFrame(data.isnull().sum(),columns=['Null Values'])\nnull['%']=(data.isna().sum()\/len(data)*100)\nnull = null[null['%'] > 0]\nnull.style.background_gradient(cmap='viridis',low =0.2,high=0.1)","4f3de96c":"data.dropna(subset=['Arrival Delay in Minutes'], inplace=True)\ndata.isnull().sum()","fdb8dd45":"data.head()","d1671705":"from scipy import stats","46ba6858":"print(f'Row counts before removing outlier: {len(data)}')\n\nfiltered_entries = np.array([True] * len(data))\nfor col in ['Checkin service','On-board service']:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low_limit = Q1 - (IQR * 1.5)\n    high_limit = Q3 + (IQR * 1.5)\n\n    filtered_entries = ((data[col] >= low_limit) & (data[col] <= high_limit)) & filtered_entries\n    \ndata = data[filtered_entries]\n\nprint(f'Row counts after removing outlier: {len(data)}')","1815c698":"print(f'Row counts before using Tarmac Delay Rules : {len(data)}')\n\ndata = data.drop(data[(data['Departure Delay in Minutes'] > 180)].index)\n\nprint(f'Row counts after using Tarmac Delay Rules : {len(data)}')","8f377c2b":"print(f'Row counts before using Tarmac Delay Rules : {len(data)}')\n\ndata = data.drop(data[(data['Arrival Delay in Minutes'] > 180)].index)\n\nprint(f'Row counts after using Tarmac Delay Rules : {len(data)}')","cb951ccb":"plt.figure(figsize=(20, 20))\n\nfor i, column in enumerate(nonbinary_columns):\n    plt.subplot(4, 6, i + 1)\n    sns.boxplot(data=data[column], palette='Blues')\n    #sns.boxplot(y=df[column],color='green')\n    plt.title(column)\n\nplt.suptitle('Boxplots Without Outliers', size=30)\nplt.show()","76912aa4":"data.duplicated().sum()","1b43883c":"data.describe()","18e92e00":"data.head()","ea9128cf":"def coba_preprocess(df):\n    df = df.copy()\n\n    df['satisfaction'] = df['satisfaction'].replace({'dissatisfied' : 1,'satisfied' : 0})\n\n    return df","9f3b5c55":"dffill = coba_preprocess(data)\ndffill.head()","5b0ea203":"dffill.shape","e545a6bf":"dffill","578b8522":"dffill.satisfaction.value_counts()","969620c8":"dffill_clean = dffill.drop(['Gender', 'Age', 'Type of Travel', 'Flight Distance', 'Departure Delay in Minutes', \n                            'Arrival Delay in Minutes'], axis = 1)","fdbb50c2":"dffill_clean","c3becd21":"#Make Necessary Import\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom imblearn import over_sampling\n\nsns.set(rc={'figure.figsize':(16,8)})\nsns.set_style('whitegrid')\nsns.color_palette('dark')\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom xgboost import XGBClassifier\n\n\n\nprint('numpy version : ',np.__version__)\nprint('pandas version : ',pd.__version__)\nprint('seaborn version : ',sns.__version__)","60e40b57":"df = dffill_clean","37e9c2a2":"df","02fec1ee":"df.head()","3242e1cf":"data = df.copy()\ndata","46e0c4d5":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\n\ndef eval_classification(model, pred, xtrain, ytrain, xtest, ytest):\n    print('Accuracy (Test Set): %.2f' % accuracy_score(y_test, pred))\n    print('Precision (Test Set): %.2f' % precision_score(y_test, pred))\n    print('Recall (Test Set): %.2f' % recall_score(y_test, pred))\n    print('F1-Score (Test Set): %.2f' % f1_score(y_test, pred))\n\n    fpr, tpr, thresholds = roc_curve(y_test, pred, pos_label=1) # pos_label: label yang kita anggap positive\n    print('AUC: %.2f' % auc(fpr, tpr))\n    \ndef show_feature_importance(model):\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\n    ax.invert_yaxis()\n\n    plt.xlabel('score')\n    plt.ylabel('feature')\n    plt.title('feature importance score')\n\ndef show_best_hyperparameter(model, hyperparameters):\n    for key, value in hyperparameters.items() :\n        print('Best '+key+':', model.get_params()[key])","7d3108bd":"data.columns","87d2cd18":"from sklearn.model_selection import train_test_split","2238c789":"features = ['Class', 'Customer Type', 'Seat comfort', 'Departure\/Arrival time convenient', 'Food and drink', 'Gate location', 'Inflight wifi service', \n            'Inflight entertainment', 'Online support', 'Ease of Online booking', 'On-board service', 'Leg room service',\n            'Baggage handling', 'Checkin service', 'Cleanliness', 'Online boarding']\n\nprint(len(features))","e0ae891e":"features","ed34ebfd":"X = data[features]\ny = data['satisfaction']","7c7b3ba6":"X.head()","dd1c79f0":"y.head()","50298a5c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","33c8d4da":"X_train.shape","021e1fb6":"X_test.shape","6344ee52":"from sklearn.tree import DecisionTreeClassifier","a5a120bf":"clf = DecisionTreeClassifier(random_state=42)","98615a3b":"clf.fit(X_train, y_train)","04148520":"y_pred = clf.predict(X_test)\nprobs = clf.predict_proba(X_test)[:,1]","500cbcac":"y_pred","a606c4b6":"probs","14cecbca":"from sklearn import metrics","f195de63":"fpr, tpr, thresholds = metrics.roc_curve(y_test, probs, pos_label=1)\nauc = metrics.auc(fpr, tpr)\nprint('AUC: '+str(round(auc*100,2))+'%')","b652a454":"accuracy = metrics.accuracy_score(y_test, y_pred)\nprint('accuracy: '+str(round(accuracy*100,2))+'%')","3f378bab":"precision = metrics.precision_score(y_test, y_pred)\nprint('precision: '+str(round(precision*100,2))+'%')","c6607261":"recall = metrics.recall_score(y_test, y_pred)\nprint('recall: '+str(round(recall*100,2))+'%')","ebb0b349":"f1_score = metrics.f1_score(y_test, y_pred)\nprint('f1_score: '+str(round(f1_score*100,2))+'%')","97b78623":"performance_log = pd.DataFrame(columns=['experiment','AUC','Accuracy'])\n\nperf = {\n    'experiment': 'initial model',\n    'AUC': str(round(auc*100,2))+'%',\n    'Accuracy': str(round(accuracy*100,2))+'%',\n    'Precision': str(round(precision*100,2))+'%',\n    'Recall': str(round(recall*100,2))+'%',\n    'F1_Score': str(round(f1_score*100,2))+'%'\n\n\n\n}\nperformance_log = performance_log.append(perf, ignore_index=True)","424356a5":"performance_log","f0aba721":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier","2f5ac6d4":"models = {\n    'Logistic'             : LogisticRegression(),\n    'SVM'                  : SVC(probability=True),\n    'DecisionTree'         : DecisionTreeClassifier(),\n    'RandomForest'         : RandomForestClassifier(),\n    'GradientBoosting'     : GradientBoostingClassifier(),\n    'AdaBoostClassifier'   : AdaBoostClassifier(),\n    'Xgboost'              : XGBClassifier(eval_metric='logloss')\n}\n\nfor model_name, clf in models.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)[:,1]\n\n    print('\\n')\n    print('Evaluate model: {}'.format(model_name))\n\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, probs, pos_label=1)\n    auc = metrics.auc(fpr, tpr)\n    print('AUC: '+str(round(auc*100,2))+'%')\n\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    print('accuracy: '+str(round(accuracy*100,2))+'%')\n    \n    precision = metrics.precision_score(y_test, y_pred)\n    print('precision: '+str(round(precision*100,2))+'%')\n        \n    recall = metrics.recall_score(y_test, y_pred)\n    print('recall: '+str(round(recall*100,2))+'%')\n\n    f1_score = metrics.f1_score(y_test, y_pred)\n    print('f1_score: '+str(round(f1_score*100,2))+'%')\n    \n    \n    print('\\n')","f2efb3f1":"clf = XGBClassifier(random_state=42, eval_metric='logloss')\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprobs = clf.predict_proba(X_test)[:,1]\n\nprint('\\n')\nprint('Model with Random State : 42')\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, probs, pos_label=1)\nauc = metrics.auc(fpr, tpr)\nprint('AUC: '+str(round(auc*100,2))+'%')\n\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint('accuracy: '+str(round(accuracy*100,2))+'%')\n    \nprecision = metrics.precision_score(y_test, y_pred)\nprint('precision: '+str(round(precision*100,2))+'%')\n        \nrecall = metrics.recall_score(y_test, y_pred)\nprint('recall: '+str(round(recall*100,2))+'%')\n\nf1_score = metrics.f1_score(y_test, y_pred)\nprint('f1_score: '+str(round(f1_score*100,2))+'%')\n        \nprint('\\n')","6aaf71f0":"perf = {\n    'experiment': 'Find The Best Model (Xgboost)',\n    'AUC': str(round(auc*100,2))+'%',\n    'Accuracy': str(round(accuracy*100,2))+'%',\n    'Precision': str(round(precision*100,2))+'%',\n    'Recall': str(round(recall*100,2))+'%',\n    'F1_Score': str(round(f1_score*100,2))+'%'\n}\nperformance_log = performance_log.append(perf, ignore_index=True)","edfc0694":"performance_log","b9c8cf66":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport numpy as np","4fc6c18c":"#List of Hyper-parameters will be tested\nhyperparameters = {\n                    'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],\n                    'min_child_weight' : [int(x) for x in np.linspace(1, 20, num = 11)],\n                    'gamma' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'tree_method' : ['auto', 'exact', 'approx', 'hist'],\n\n                    'colsample_bytree' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'eta' : [float(x) for x in np.linspace(0, 1, num = 100)],\n\n                    'lambda' : [float(x) for x in np.linspace(0, 1, num = 11)],\n                    'alpha' : [float(x) for x in np.linspace(0, 1, num = 11)]\n                    }","652069a0":"hyperparameters","222c3e14":"# Init\nclf = XGBClassifier(random_state=42, eval_metric='logloss')\nclf_tuned = RandomizedSearchCV(clf, hyperparameters, cv=5, random_state=42, scoring='recall')\nclf_tuned.fit(X_train,y_train)","8d235c8f":"# Predict & Evaluation\ny_pred = clf_tuned.predict(X_test)\nprobs = clf_tuned.predict_proba(X_test)[:,1]","c3681c06":"y_pred","95748492":"probs","3e1c9aa6":"fpr, tpr, thresholds = metrics.roc_curve(y_test, probs, pos_label=1)\nauc = metrics.auc(fpr, tpr)\nprint('AUC: '+str(round(auc*100,2))+'%')\n\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint('accuracy: '+str(round(accuracy*100,2))+'%')\n    \nprecision = metrics.precision_score(y_test, y_pred)\nprint('precision: '+str(round(precision*100,2))+'%')\n        \nrecall = metrics.recall_score(y_test, y_pred)\nprint('recall: '+str(round(recall*100,2))+'%')\n\nf1_score = metrics.f1_score(y_test, y_pred)\nprint('f1_score: '+str(round(f1_score*100,2))+'%')\n        \nprint('\\n')","1f3d4ee1":"perf = {\n    'experiment': 'Find The Best Model (Xgboost) - Hyperparameter Tunning',\n    'AUC': str(round(auc*100,2))+'%',\n    'Accuracy': str(round(accuracy*100,2))+'%',\n    'Precision': str(round(precision*100,2))+'%',\n    'Recall': str(round(recall*100,2))+'%',\n    'F1_Score': str(round(f1_score*100,2))+'%'\n}\nperformance_log = performance_log.append(perf, ignore_index=True)","06ed9729":"#Check the best Hyperparameters\nshow_best_hyperparameter(clf_tuned.best_estimator_, hyperparameters)","8bb80046":"performance_log","c3cbb1f8":"#AUC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","58b3cdea":"clf_tuned.best_estimator_.feature_importances_","5f20c881":"feat_importances = pd.Series(clf_tuned.best_estimator_.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\nax.invert_yaxis()\n\nplt.xlabel('Score')\nplt.ylabel('Feature')\nplt.title('Feature Importance Score')","8a8cdb8c":"performance_log","3a0c627c":"import pickle\npickle.dump(clf_tuned, open('pima.pickle.dat', 'wb'))","443114c0":"loaded_model = pickle.load(open('pima.pickle.dat', 'rb'))\nresult = loaded_model.score(X_test,y_test)\nprint('Model Score : ', str(round(result,2)*100), '%')","179f5e23":"print('train accuracy : ', str(round(loaded_model.score(X_train, y_train),2)*100), '%')\nprint('test accuracy : ', str(round(loaded_model.score(X_test, y_test),2)*100), '%')","e537b8c6":"##### Looking for Duplicate Data","772d2244":"##### Feature Importance","dde722e0":"#### Categorical Value Counting","d83f2813":"##### Standarization","706861e1":"#### Hyperparamter Tuning on XGBoost","449370b4":"#### Evaluate The Model","b992385d":"# Third Step, Using Machine Learning to Clean Data","cf8da644":"#### Looking for Numerical Data","835d4d23":"We use score 0-5 as our starter to analyze the pattern.\nWe separate 0-2 as dissatisfied, 3 as neutral, and 4-5 as satisfied\nWe choose score 0-2 as 0 and score 4-5 as 1\n\n0-14 represents the total amount of service that are choose as satisfied\/dissatisfied by the customers (14 means that the customers choose 1\/satisfied for every services)\n\nWe separate 0-6 as dissatisfied and 8-14 as satisfied.\n\nFrom this visualization, we assumed that this pattern cannot represent the satisfaction within the Airlines data since there's also a lot of customers that choose satisfied (8-14) in Dissatisfaction Pattern. There's also 14 score in Dissatisfaction Pattern, it means that there're customers that choose satisfied for all services but give Dissatisfied as their end results.\n\nIn order to analyze deeper into the data we decided to user Machine Learning.","e62175e9":"#### Train the Model","172ceab2":"#### Looking for Categorical Data","6c004f3d":"##### Looking for Missing Values","b17ed9ef":"#### Correlation Heatmap","79e761ee":"# Conclusions","53372850":"Countplot conclusions : \n1. Female Customers have higher satisfaction than Male Customers.\n2. Loyal Customers have higher satisfaction than Disloyal Customers.\n3. Business Travel has higher customer satisfaction than Personal Travel.\n4. Business Class has the highest satisfaction between the 3 airlines classes.","e9712b1a":"#### Data Cleansing and Pre-processing","32f24493":"##### Outlier Removal","9c83c8f9":"#### Graphical Approach","9b3ff2d0":"#### Model Evaluation","7cd7e865":"After using Machine Learning to analyze customer satisfaction, we find that XGBoost is the best machine learning model to predict our customer satisfaction data.\n\nThis model isn't Overfitting or Underfitting since the the accuracy differences between train and test data is just 2%.\n\nThere are 2 services that are highly affects customer satisfaction in this Airlines data : \n1. Inflight Entertainment\n2. Seat and Comfort\n\nInvistico Airlines can choose to upgrade\/investing more money and effort in those 2 services to improve their customer satisfactions.","1dfe7f4e":"#### EDA","0ce399b3":"##### Log the Performance","6d85ea92":"#### Numerical Approach","e0dd43a2":"Barplot Conclusions : \n1. In Business Class, both Male and Female Loyal Customers have high Satisfaction\n2. In Eco Class, Female Loyal Customers have higher Satisfaction compare to Male Loyal Customers.","a5d863df":"#### Log the Performance","2af8736a":"#### Performance Summary","c944dc02":"#### Import Data","184ec0e1":"After using XGBoost Machine Learning and looking for Feature Importance, we find that there are 2 features that are highly affect customer Dissatisfaction. They are Infligt Entertainment and Seat Comfort.","9c2398d6":"##### Remove data by Tarmac Delay Rules","58442d02":"#### Data Description","21c4d2f1":"With only 2 percents, we assume that our model isn't overfitting\/underfitting","ee80f007":"#### Feature Engineering","ee84d573":"After we finding out that XGBoost has the highest recall and AUC score, we try to use random_state=42 and run the XGBoost once more to see whether the XGBoost performance will stay the same or not after applying random state","40aca2d6":"# Second Step, Make a Clean Data out of the Raw Data","989832f4":"# First, we try to see if there is satisfaction patterns within satisfaction column","2591aaf6":"#### Modelling Experiment","7eed1642":"#### Feature Encoding","83e11dad":"##### We will try to use 7 Machine Learning Models : \n1. LogisticRegression\n2. SVM\n3. DecisionTree\n4. RandomForest\n5. GradientBoosting\n6. AdaBoostClassifier\n7. Xgboost","7d8f8e24":"ARRAI TEAM members : \n1. Archi Koerniawan\n2. Ahmad Firdaus\n3. Richard Sutantyo","b407283b":"#### Split to Train & Test Dataset"}}