{"cell_type":{"20fff343":"code","16b1eed5":"code","b2597397":"code","8a24ffae":"code","38b0d874":"code","5ed39508":"code","7183dc10":"code","c6497a83":"code","78be7482":"code","9a4bb038":"code","eb91cf18":"code","356fdb06":"code","c7c5a5c6":"code","5d726953":"code","4e8288c1":"code","e3ae65c9":"code","355c9b95":"code","9283254f":"code","74bf25a5":"code","85782470":"code","0ffdc795":"code","f5d37c25":"code","ff37e9ba":"code","768889d2":"code","fc6a81f7":"code","8265fc7b":"code","c77b62df":"code","4164c2c4":"code","0e9d800b":"code","1dc028e9":"code","e2e5db2c":"code","397b3898":"code","66cadb55":"code","6fee72be":"code","265de9a5":"code","a91a2274":"code","ddc71f6e":"code","ea80b53b":"code","60076ea2":"code","c4e28792":"code","8072d6dd":"code","05ca2d54":"code","9973b265":"code","2ec9c00e":"code","70afa7c9":"code","82fed39e":"code","c85ee17b":"code","8e5d1455":"code","e4ef1651":"code","997b34eb":"code","a15f4e03":"code","09282299":"code","0803be4a":"code","dd784d46":"code","9c70dacf":"code","de993b30":"code","59264eba":"code","79671297":"code","300cdde1":"code","9373795c":"code","41f53d3a":"code","01e7b2c6":"code","5f996d8b":"code","6b83a3f2":"code","d12c9b17":"code","3b888cc6":"code","66c959af":"code","290b4a2a":"code","e2b80d70":"code","8f4d9f13":"markdown","b213cda4":"markdown","c1437b54":"markdown","c0f10c96":"markdown","833559dd":"markdown","3435ba0b":"markdown","b489f54a":"markdown","1f7a73c2":"markdown","2fae61ad":"markdown","52872bcd":"markdown","4bdf84a5":"markdown","f99c2e39":"markdown","bce8311a":"markdown","b9d3a33c":"markdown","576939c4":"markdown","98ee3ec9":"markdown","318d7087":"markdown","e50a88d5":"markdown","2a2f09f7":"markdown","fa9df62f":"markdown","cb21652c":"markdown","16cac997":"markdown","c08431d5":"markdown","a875c256":"markdown","06373a5e":"markdown","59d3d781":"markdown","995a0f7a":"markdown","90f7316f":"markdown","0be31e11":"markdown","7f4c6239":"markdown","9a8a0fc1":"markdown","a3f67c99":"markdown","84959c27":"markdown","375ac57e":"markdown"},"source":{"20fff343":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","16b1eed5":"df  = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","b2597397":"df.columns","8a24ffae":"df.head()","38b0d874":"df.describe()","5ed39508":"df.shape","7183dc10":"df.isnull().sum()","c6497a83":"bmi_mean = df['bmi'].mean()\ndf['bmi'].fillna(value=bmi_mean, inplace=True)\nbmi_mean","78be7482":"df.isnull().sum().sum()","9a4bb038":"# We don't need the id I will drop it\ndf.drop('id', axis=1, inplace=True)\ndf.head()","eb91cf18":"plt.figure(figsize=(12,5))\nsns.distplot(df['age'], bins=15);","356fdb06":"plt.figure(figsize=(12,10))\n\nsns.distplot(df[df['stroke'] == 0][\"age\"], color='green')\nsns.distplot(df[df['stroke'] == 1][\"age\"], color='red')\n\nplt.title('No Stroke vs Stroke by Age', fontsize=15)\nplt.xlim([18,100])\nplt.show()","c7c5a5c6":"sns.countplot(x='gender', data=df, hue='stroke');","5d726953":"df['gender'].value_counts()","4e8288c1":"df.drop(df.loc[df['gender']=='Other'].index, inplace=True)","e3ae65c9":"sns.countplot(x='gender', data=df, hue='stroke');","355c9b95":"sns.countplot(x='stroke', data=df)\ndf.stroke.value_counts()","9283254f":"plt.figure(figsize=(12,5))\nsns.lineplot(data=df, x=\"age\", y=\"bmi\", hue='gender', ci=None);","74bf25a5":"plt.figure(figsize=(12,5))\nsns.lineplot(data=df, x=\"age\", y=\"avg_glucose_level\", hue='stroke', ci=None);","85782470":"plt.figure(figsize=(12,10))\n\nsns.distplot(df[df['stroke'] == 0][\"bmi\"], color='green')\nsns.distplot(df[df['stroke'] == 1][\"bmi\"], color='red') \n\nplt.title('No Stroke vs Stroke by BMI', fontsize=15)\nplt.xlim([10,100])\nplt.show()","0ffdc795":"fig = plt.figure(figsize=(7,7))\nsns.distplot(df.avg_glucose_level, color=\"green\", label=\"avg_glucose_level\", kde= True)\nplt.legend();","f5d37c25":"plt.figure(figsize=(12,5))\nsns.scatterplot(x='avg_glucose_level', y='bmi', hue='stroke', data=df);","ff37e9ba":"sns.countplot(x='smoking_status', data=df);","768889d2":"plt.figure(figsize=(12,5))\nsns.boxplot(y='age', x='smoking_status',hue='stroke' ,data=df)","fc6a81f7":"sns.countplot(x='Residence_type', hue='stroke', data=df)","8265fc7b":"plt.figure(figsize=(12,5))\nsns.boxplot(y='avg_glucose_level', x='heart_disease',hue='stroke' ,data=df)","c77b62df":"plt.figure(figsize=(12,5))\nsns.boxplot(y='age', x='heart_disease',hue='stroke' ,data=df)","4164c2c4":"sns.countplot(x='work_type', hue='ever_married', data=df);","0e9d800b":"plt.figure(figsize=(12,5))\nsns.histplot(x='bmi', hue='ever_married', data=df, bins=50)","1dc028e9":"sns.pairplot(df, size = 2.5)","e2e5db2c":"correlation = df.corr()\nfig, axes = plt.subplots(figsize=(7, 7))\nsns.heatmap(correlation, vmax=.8, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10});","397b3898":"X = df.iloc[:,0:-1].values\ny = df.iloc[:, -1].values\n# This will split the daa into target and values column with arrays shape","66cadb55":"X","6fee72be":"y","265de9a5":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder","a91a2274":"# with np.printoptions(threshold=np.inf):\n#     print(X)\n# # I used this to know the index of columns I want to convert as it's a numpy array and that normal one doesn't display the full data","ddc71f6e":"l_e = LabelEncoder()\nX[:, 0] = l_e.fit_transform(X[:, 0]) # gender column\nX[:, 4] = l_e.fit_transform(X[:, 4]) # ever_married column\nX[:, 6] = l_e.fit_transform(X[:, 6]) # Residence_type column","ea80b53b":"c_t = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [5,9])], remainder= 'passthrough')\nX = np.array(c_t.fit_transform(X))\n# I will use it one 'work_type', 'smoking_status'","60076ea2":"X","c4e28792":"X.shape, y.shape","8072d6dd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","05ca2d54":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","9973b265":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","2ec9c00e":"print (sum(y_train == 1))\nprint (sum(y_train == 0))","70afa7c9":"from imblearn.over_sampling import SMOTE","82fed39e":"smote = SMOTE(random_state=42)\nX_train, y_train = smote.fit_resample(X_train, y_train.ravel())","c85ee17b":"print (X_train.shape)\nprint (y_train.shape)\nprint (sum(y_train == 1))\nprint (sum(y_train == 0))","8e5d1455":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","e4ef1651":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","997b34eb":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscore = cross_val_score(model, X_train, y_train, cv = 6)\nprecision = precision_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint ('train score of LogisticRegression is', score.mean(),'%')\nprint ('--')\nprint ('Precision score is ', precision)\nprint ('--')\nprint ('ROC Score is', roc)\nprint ('--')\nprint ('Recall Score is ', recall)","a15f4e03":"y_pred_prob = model.predict_proba(X_test)[:,1]\n\n# instantiating the roc_cruve\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\n# plotting the curve\nplt.figure(figsize = (8, 8))\nplt.plot([0,1],[0,1],\"k--\",'r+')\nfigsize=(16,12)\nplt.plot(fpr,tpr,color = '#b01717', label = 'AUC = %0.3f' % roc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\" Logistic Regression ROC Curve\")\nplt.legend()\nplt.show()","09282299":"plt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Oranges', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","0803be4a":"model = SVC(probability=True)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscore = cross_val_score(model, X_train, y_train, cv = 6)\nprecision = precision_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint ('train score of SVC is', score.mean(),'%')\nprint ('--')\nprint ('Precision score is ', precision)\nprint ('--')\nprint ('ROC Score is', roc)\nprint ('--')\nprint ('Recall Score is ', recall)","dd784d46":"y_pred_prob = model.predict_proba(X_test)[:,1]\n# instantiating the roc_cruve\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\n# plotting the curve\nplt.figure(figsize = (8, 8))\nplt.plot([0,1],[0,1],\"k--\",'r+')\nfigsize=(16,12)\nplt.plot(fpr,tpr,color = '#b01717', label = 'AUC = %0.3f' % roc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\" SVC ROC Curve\")\nplt.legend()\nplt.show()","9c70dacf":"plt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Oranges', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","de993b30":"model = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscore = cross_val_score(model, X_train, y_train, cv = 6)\nprecision = precision_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint ('train score of SVC is', score.mean(),'%')\nprint ('--')\nprint ('Precision score is ', precision)\nprint ('--')\nprint ('ROC Score is', roc)\nprint ('--')\nprint ('Recall Score is ', recall)","59264eba":"y_pred_prob = model.predict_proba(X_test)[:,1]\n# instantiating the roc_cruve\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\n# plotting the curve\nplt.figure(figsize = (8, 8))\nplt.plot([0,1],[0,1],\"k--\",'r+')\nfigsize=(16,12)\nplt.plot(fpr,tpr,color = '#b01717', label = 'AUC = %0.3f' % roc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\" KNeighbors ROC Curve\")\nplt.legend()\nplt.show()","79671297":"plt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Oranges', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","300cdde1":"model =  RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscore = cross_val_score(model, X_train, y_train, cv = 6)\nprecision = precision_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint ('train score of SVC is', score.mean(),'%')\nprint ('--')\nprint ('Precision score is ', precision)\nprint ('--')\nprint ('ROC Score is', roc)\nprint ('--')\nprint ('Recall Score is ', recall)","9373795c":"y_pred_prob = model.predict_proba(X_test)[:,1]\n# instantiating the roc_cruve\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\n# plotting the curve\nplt.figure(figsize = (8, 8))\nplt.plot([0,1],[0,1],\"k--\",'r+')\nfigsize=(16,12)\nplt.plot(fpr,tpr,color = '#b01717', label = 'AUC = %0.3f' % roc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\" Random Forest ROC Curve\")\nplt.legend()\nplt.show()","41f53d3a":"plt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Oranges', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","01e7b2c6":"from sklearn.model_selection import GridSearchCV","5f996d8b":"log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.025,0.05]}\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params, scoring = 'accuracy',cv = 6)\ngrid_log_reg.fit(X_train, y_train)\nbest_score = grid_log_reg.best_score_\nbest_params = grid_log_reg.best_params_\nprint ('Best Score is',best_score * 100)\nprint ('Best Parameters is', best_params)","6b83a3f2":"svc_params = {'C':[0.5,0.75,1, 1.5],'kernel':['linear', 'rbf']}\nsvc_clf = GridSearchCV(SVC(), svc_params, scoring = 'accuracy',cv = 6)\nsvc_clf.fit(X_train, y_train)\nbest_score = svc_clf.best_score_\nbest_params = svc_clf.best_params_\nprint ('Best Score is',best_score * 100)\nprint ('Best Parameters is', best_params)","d12c9b17":"kn_params = {'n_neighbors':[5,7,8,10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}\nkn = GridSearchCV(KNeighborsClassifier(), kn_params, scoring = 'accuracy',cv = 6)\nkn.fit(X_train, y_train)\nbest_score = kn.best_score_\nbest_params = kn.best_params_\nprint ('Best Score is',best_score * 100)\nprint ('Best Parameters is', best_params)","3b888cc6":"rf_params = {'n_estimators':[100,150,200],'criterion':['gini','entropy'],}\nrf = GridSearchCV(RandomForestClassifier(), rf_params, scoring = 'accuracy',cv = 6)\nrf.fit(X_train, y_train)\nbest_score = rf.best_score_\nbest_params = rf.best_params_\nprint ('Best Score is',best_score * 100)\nprint ('Best Parameters is', best_params)","66c959af":"model =  RandomForestClassifier(n_estimators=200, criterion='entropy' )\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscore = cross_val_score(model, X_train, y_train, cv = 10)\nprecision = precision_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint ('train score of SVC is', score.mean(),'%')\nprint ('--')\nprint ('Precision score is ', precision)\nprint ('--')\nprint ('ROC Score is', roc)\nprint ('--')\nprint ('Recall Score is ', recall)","290b4a2a":"y_pred_prob = model.predict_proba(X_test)[:,1]\n# instantiating the roc_cruve\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\n# plotting the curve\nplt.figure(figsize = (8, 8))\nplt.plot([0,1],[0,1],\"k--\",'r+')\nfigsize=(16,12)\nplt.plot(fpr,tpr,color = '#b01717', label = 'AUC = %0.3f' % roc)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\" SVC ROC Curve\")\nplt.legend()\nplt.show()","e2b80d70":"plt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Oranges', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","8f4d9f13":"I will use **GridSearchCV** to find the best hyperparameters\n<br>\nSo what is it ?\n<br>\ncv: number of cross-validation you have to try for each selected set of hyperparameters. verbose: you can set it to 1 to get the detailed print out while you fit the data to GridSearchCV\n<br>\nin the end, you can select the best parameters from the listed hyperparameters.","b213cda4":"### 3 - KNeighbors","c1437b54":"![6b90c98fda17445646a21305bc75bfc7.jpg](attachment:6b90c98fda17445646a21305bc75bfc7.jpg)","c0f10c96":"I hope you learned anything from this notebook, Thank You","833559dd":"### 4 - Random Forest  ","3435ba0b":"### Upsampling the Data","b489f54a":"Now let's do some EDA to understand our data more","1f7a73c2":"We have 201 null values on bmi column, well I will replce these NAN values with the mean as we don't have much data and BMI don't change that much","2fae61ad":"I will use one hot encoder for teatures that aren't binary like zero and ones and I will use label encoding for categorical features that are binary features","52872bcd":"### Logistic Regression","4bdf84a5":"### Support Vector Machine","f99c2e39":"What is **One hot Encoding** ?\n<br>\nOne hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector.","bce8311a":"### Scaling the Data","b9d3a33c":"Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information, the output will range from 0 to 1","576939c4":"Here is our main problem, if we trained our model on the current it will always assume that there is no strokes due that no strokes is much mroe than no strokes we will use upsampling technique","98ee3ec9":"### 2 - Support Vector Machine","318d7087":"Now Let's apply the highest accuracy model with best hyperparameters","e50a88d5":"# Explatory Data Analysis","2a2f09f7":"### KNeighbors","fa9df62f":"Well am not surprised that married people have higher bmi  (:","cb21652c":"# Hyperparameters Tuning","16cac997":"# Data Preprocessing","c08431d5":"We cannott tell much here as we didn't upsample the data yet","a875c256":"What is **Upsampling** ?\n<br>\nUpsampling is a procedure where synthetically generated data points (corresponding to minority class) are injected into the dataset. After this process, the counts of both labels are almost the same. This equalization procedure prevents the model from inclining towards the majority class, We use this to prevent overfiting in machine learning as poeple had no strokes much more than people hadn't strokes","06373a5e":"is seems that there is only 1 value of other in gender column I will drop it","59d3d781":"We can assume here that strokes usually happens on higher glucose levels","995a0f7a":"### 1- LogisticRegression","90f7316f":"ok generally females are more than males we don't have much strokes in our data here that's a problem for the machine learning part as we don't need our model to overfit on non strokes","0be31e11":"it's very obious that people get strokes in elder ages","7f4c6239":"### Label Encoding","9a8a0fc1":"the age column is a little left skewed with a peak around 60s","a3f67c99":"**What is Stroke ?**\n<br>\nIt happens when the brain's blood vessels become narrowed or blocked, causing severely reduced blood flow (ischemia). Blocked or narrowed blood vessels are caused by fatty deposits that build up in blood vessels or by blood clots or other debris that travel through your bloodstream and lodge in the blood vessels in your brain.","84959c27":"### Random Forest","375ac57e":"Our goal here is to predict whether person will get stroke or no based on some features that we have\n<br>\n- 1) id: unique identifier\n<br>\n- 2) gender: \"Male\", \"Female\" or \"Other\"\n<br>\n- 3) age: age of the patient\n<br>\n- 4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n<br>\n- 5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n<br>\n- 6) ever_married: \"No\" or \"Yes\"\n<br>\n- 7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n<br>\n- 8) Residence_type: \"Rural\" or \"Urban\"\n<br>\n- 9) avg_glucose_level: average glucose level in blood\n<br>\n- 10) bmi: body mass index\n<br>\n- 11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n<br>\n- 12) stroke: 1 if the patient had a stroke or 0 if not\n<br>\n<br>\nNote: \"Unknown\" in smoking_status means that the information is unavailable for this patient."}}