{"cell_type":{"8f7ac3f7":"code","4e8f78d7":"code","ee048d4b":"code","be7d78ca":"code","c3dcf297":"code","28efaa00":"code","f58fbdb6":"code","42b21e1f":"code","1b2ec34d":"code","29a3b97c":"code","e06213f3":"code","f02c265a":"code","90d78fdc":"code","052f2e47":"code","b8cdd93f":"code","e61dcf32":"code","399d0b6e":"code","9f7b7094":"code","1bce0596":"code","0f5641fd":"code","6f32a527":"code","973ed57e":"code","4e9c7a65":"code","bc18dace":"code","a77ff2a6":"code","5e07f6a7":"code","d5bd607e":"code","9e70734c":"code","0485913c":"code","a4d5daa1":"code","6a7a58f3":"markdown","1eeb40da":"markdown","3d70e587":"markdown","41c2d519":"markdown","e9703e99":"markdown","4fa8b465":"markdown","b210502a":"markdown"},"source":{"8f7ac3f7":"%pip install efficientnet_pytorch","4e8f78d7":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os, sys, re, cv2, random, socket, time, shutil, json\nfrom datetime import datetime, timedelta\nfrom PIL import Image, ImageDraw\nfrom pathlib import Path\nfrom termcolor import colored\n\n# Torch\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom torchvision import transforms, models\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom efficientnet_pytorch import EfficientNet\n\nfrom sklearn.metrics import f1_score\n\nimport optuna","ee048d4b":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","be7d78ca":"data_path = Path(\"..\/input\/classification-with-limited-data\/food\/\")","c3dcf297":"# Bounding box cords per image\ndf_train_labels = pd.read_csv(data_path\/\"train\"\/\"train.csv\")\ndf_train_labels.set_index(\"Id\", inplace=True)","28efaa00":"df_train_labels.head()","f58fbdb6":"# Train images\ntrain_image_paths = [p for p in (data_path\/'train'\/'images').iterdir()]\ntest_image_paths = [p for p in (data_path\/'test'\/'images').iterdir()]\n# Split into balanced train and validation set\nval_images = []\nfor category in range(107):\n    for row in df_train_labels[df_train_labels[\"Expected\"] == category].sample(2).iterrows():\n        val_images.append((data_path\/'train'\/'images'\/row[0]))\n\ntrain_images = [p for p in train_image_paths if p not in val_images]","42b21e1f":"len(train_image_paths), len(test_image_paths), len(train_images), len(val_images)","1b2ec34d":"# verify balance\ndf_train_labels.loc[[p.name for p in val_images]][\"Expected\"].value_counts().unique()","29a3b97c":"train_image_paths[:5]","e06213f3":"print(df_train_labels.iloc[100][\"Expected\"])\nImage.open(data_path\/\"train\"\/\"images\"\/df_train_labels.iloc[100].name)","f02c265a":"class MensaClassificationDataset(Dataset):\n    \n    def __init__(self, im_paths: list, labels_path=None, transform=None):\n        super().__init__()\n        if labels_path:\n            self.df_labels = pd.read_csv(labels_path)\n            self.df_labels.set_index('Id', inplace=True)\n        else:\n            self.df_labels = labels_path\n        self.im_paths = im_paths\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.im_paths)\n    \n    def __getitem__(self, idx):\n        if idx >= self.__len__():\n            raise IndexError(\"Index out of range.\")\n        \n        im_id = self.im_paths[idx].name\n        \n        sample = {}\n        sample['file_name'] = str(self.im_paths[idx])\n        sample['image_id'] = im_id\n        \n        image = Image.open(sample['file_name'])\n        sample['width'] = image.width\n        sample['height'] = image.height\n        \n        if self.transform:\n            image = self.transform(image)\n        sample['image'] = image\n        \n        if self.df_labels is not None:\n            sample['label'] = self.df_labels.loc[im_id]['Expected']\n        \n        return sample","90d78fdc":"def get_train_val_dataloader(config, train_im_paths=None, val_im_paths=None, shuffle_split=True):\n    \n    if (train_im_paths is not None) and (val_im_paths is not None):\n        train_images, val_images = train_im_paths, val_im_paths\n    \n    # shuffle balanced train\/validation split\n    if shuffle_split:\n        val_images = []\n        for category in range(107):\n            for row in df_train_labels[df_train_labels[\"Expected\"] == category].sample(2).iterrows():\n                val_images.append((data_path\/'train'\/'images'\/row[0]))\n        train_images = [p for p in train_image_paths if p not in val_images]\n    \n    # Random augmentations applied on data sampling\n    augmentations = [\n            getattr(transforms, aug[0])(**aug[1]) for aug in config[\"TRAIN_TRANSFORMS\"].items()\n    ]\n    # ImageNet normalization\n    normalization = [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) if not config[\"ADVPROP\"] \\\n        else transforms.Lambda(lambda img: img * 2.0 - 1.0)\n    ]\n\n    data_transform = transforms.Compose(augmentations + normalization)\n    \n    # Create datasets\n    train_dataset = MensaClassificationDataset(train_images, \n                                               data_path\/\"train\"\/\"train.csv\", \n                                               transform=data_transform)\n    val_dataset = MensaClassificationDataset(val_images, \n                                             data_path\/\"train\"\/\"train.csv\",\n                                             transform=transforms.Compose(normalization))\n    \n    # Create dataloader\n    train_loader = DataLoader(train_dataset, \n                              batch_size=config[\"TRAIN_BATCH_SIZE\"], \n                              shuffle=True,\n                              num_workers=2)\n    val_loader = DataLoader(val_dataset,\n                            batch_size=config[\"VAL_BATCH_SIZE\"],\n                            num_workers=2\n                           )\n    return train_loader, val_loader","052f2e47":"from contextlib import contextmanager\n# Helper\n@contextmanager\ndef evaluation_context(model):\n    training_mode = model.training\n    model.eval()\n    yield\n    model.train(training_mode)\n    \n# Timer\nclass Timer:\n    def __init__(self, epochs=None):\n        self.last_time = time.time()\n        self.epochs = epochs\n        self.cur_epoch = 0\n        self.avg_duration = 0\n        self.total = 0\n        \n    def update(self):\n        \"\"\"Computes running average and total time. Returns estimated time remaining based on the number of epochs.\"\"\"\n        # Get duration of epoch\n        now = time.time()\n        elapsed =  now - self.last_time\n        self.last_time = now\n        \n        # Update average\n        self.avg_duration += (1 \/ (self.cur_epoch + 1)) * (elapsed - self.avg_duration)\n        self.cur_epoch += 1\n        \n        # Update total\n        self.total += elapsed\n        \n        return timedelta(seconds=(self.avg_duration * (self.epochs - self.cur_epoch)))\n        ","b8cdd93f":"def train(model, train_loader, loss_fn, optimizer, epoch, writer=None):\n    \"\"\"Train model for one epoch with given optimizer.\"\"\"\n    \n    loss, losses, accuracies, predictions = 0, [], [], []\n    \n    for i, sample in enumerate(train_loader):\n        def closure():\n            optimizer.zero_grad()\n\n            im = sample['image'].to(device)\n            output = model(im)\n            probs = F.softmax(output, dim=1)\n\n            target = sample['label'].to(device)\n            loss = loss_fn(probs, target)\n            losses.append(loss.item())\n            loss.backward()\n            \n            prediction = probs.argmax(axis=1)\n            num_correct = (prediction == target).sum().item()\n            accuracies.append(num_correct \/ len(target))\n            predictions.extend(zip(prediction.tolist(), target.tolist()))\n        \n            return loss\n        \n        optimizer.step(closure)\n        \n        if (i % 4 == 0) and (i != 0):\n            if writer:\n                idx = epoch * int(len(train_loader.dataset) \/ train_loader.batch_size + 1) + i\n                writer.add_scalar(\"loss\/train\", np.mean(losses), idx)\n                writer.add_scalar(\"accuracy\/train\", np.mean(accuracies), idx)\n                        \n    return losses, accuracies, predictions","e61dcf32":"def validate(model, val_loader, loss_fn, epoch, writer=None):\n    \"\"\"Validate model. Optionally write statistics for Tensorboard.\"\"\"\n    \n    with evaluation_context(model):\n        losses, accuracies, predictions = [], [], []\n        \n        for i, sample in enumerate(val_loader):\n            im = sample['image'].to(device)\n            output = model(im)\n            probs = F.softmax(output, dim=1)\n            \n            # loss\n            target = sample['label'].to(device)\n            loss = loss_fn(probs, target)\n            \n            # accuracy\n            prediction = probs.argmax(axis=1)\n            num_correct = (prediction == target).sum().item()\n            accuracies.append(num_correct \/ len(target))\n            \n            if writer:\n                idx = epoch * int(len(val_loader.dataset) \/ val_loader.batch_size + 1) + i\n                writer.add_scalar(\"loss\/validation\", loss, idx)\n                writer.add_scalar(\"accuracy\/validation\", accuracies[-1], idx)\n                \n            losses.append(loss.item())\n            predictions.extend(zip(prediction.tolist(), target.tolist()))\n            \n    return losses, accuracies, predictions","399d0b6e":"def build(config):\n    # Model pretrained on ImageNet\n    if config[\"BACKBONE\"][:6] == \"resnet\":\n        model = getattr(models, config[\"BACKBONE\"])(pretrained=True, advprop=config[\"ADVPROP\"])\n        model.fc = getattr(nn, config[\"CLASSIFIER\"])(2048, config[\"NUM_CLASSES\"])\n\n    elif config[\"BACKBONE\"][:12] == \"efficientnet\":\n        model = EfficientNet.from_pretrained(config[\"BACKBONE\"], num_classes=config[\"NUM_CLASSES\"], dropout_rate=config[\"DROPOUT_RATE\"])\n    else:\n        raise NotImplementedError\n\n    model.to(device)\n\n    # Only train last layer\n    # -> Maintain features learned from ImageNet, Force usage of these features\n    if config[\"FREEZE_FEATURE_EXTRACTOR\"]:\n        for param in model.named_parameters():\n            if param[0] not in [\"fc.weight\", \"fc.bias\", \"_bn1.weight\", \"_bn1.bias\", \"_fc.weight\", \"_fc.bias\"]:\n                param[1].requires_grad = False\n\n    # Initialize loss and optimizer\n    loss_fn = getattr(nn, config[\"LOSS_FN\"])()\n    optimizer = getattr(torch.optim, config[\"OPTIMIZER\"])\n    optimizer = optimizer(\n        model.parameters(),\n        lr=config[\"INIT_LEARNING_RATE\"]\n    )\n    \n    if config[\"OPTIMIZER\"] in [\"Adam\", \"RMSprop\"]:\n        optimizer.weight_decay = config[\"WEIGHT_DECAY\"]\n    \n    if config[\"LR_SCHEDULER\"]:\n        lr_scheduler = getattr(torch.optim.lr_scheduler, config[\"LR_SCHEDULER\"])(optimizer,\n                                                                            mode='min',\n                                                                            verbose=True)\n    else:\n        lr_scheduler = None\n        \n    return model, optimizer, loss_fn, lr_scheduler","9f7b7094":"def run(config, model, train_loader, val_loader, optimizer, loss_fn, lr_scheduler, log_path=None, store_checkpoints=False, writer=None, resume_from=None, trial=None):\n    \n    print(\"Initialize training run with configuration:\", \"\\n\", config)\n    \n    timer = Timer(epochs=config[\"EPOCHS\"])\n    \n    if store_checkpoints:\n        assert log_path is not None \n        os.makedirs(log_path\/\"checkpoints\", exist_ok=True)\n        with open(log_path\/\"train_config.json\", 'w+') as f:\n            json.dump(config, f, indent=4)\n\n    print(\"Start Training run.\")\n            \n    # Train loop\n    val_stopping_losses = []\n    for epoch in range(config[\"EPOCHS\"]):\n        losses, accuracies, predictions = train(model, train_loader, loss_fn, optimizer, epoch, writer=writer)\n        val_losses, val_accuracies, val_predictions = validate(model, val_loader, loss_fn, epoch, writer=writer)\n\n        val_loss = np.mean(val_losses)\n        \n        if lr_scheduler:\n            lr_scheduler.step(val_loss)\n\n        #F1 Score\n        predictions, targets = list(zip(*(predictions + val_predictions)))\n        f1 = f1_score(targets, predictions, average=\"macro\")\n\n        if writer:\n            writer.add_scalar(\"f1-score\", f1, epoch)\n\n        if store_checkpoints and ((epoch + 1) % 5 == 0):\n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict()\n            }, log_path\/\"checkpoints\"\/f\"{config['BACKBONE']}_pretrained_epoch_{epoch + 1}.pt\")\n\n        print(colored(f\"[{datetime.now()}]\", \"green\"), \n              f\"Epoch {epoch+1} finished.\", \n              \"Estimated time remaining:\", colored(timer.update(), \"green\") + \".\", \"\\n\",\n              f\"Average train loss: {np.round(np.mean(losses), 4)}.\", \n              f\"Average validation loss: {np.round(val_loss, 4)}.\",\n              f\"Average F1-Score: {np.round(f1, 4)}\", \"\\n\",\n              f\"Average train accuracy: {np.round(np.mean(accuracies), 4)}.\",\n              f\"Average validation accuracy: {np.round(np.mean(val_accuracies), 4)}.\")\n        \n        # Early stopping\n        if len(val_stopping_losses) == 20:\n            if (val_stopping_losses.pop(0) - val_loss) <= config[\"EARLY_STOPPING_EPSILON\"]:\n                print(\"Early stop. No significant improvement in validation loss after 20 epochs.\")\n                break\n        val_stopping_losses.append(val_loss)\n        \n        if trial:\n            trial.report(np.mean(val_accuracies), epoch)\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n    print(\"Total execution time:\", colored(timedelta(seconds=timer.total), \"green\") + \".\")\n    \n    if writer:\n        writer.close()\n        \n    return np.mean(val_accuracies), epoch, val_stopping_losses","1bce0596":"# Train configuration\nconfig = {\n    \"BACKBONE\": \"efficientnet-b0\",\n    \"DROPOUT_RATE\": 0.1,\n    \"ADVPROP\": True,\n    \"FREEZE_FEATURE_EXTRACTOR\": True,\n    \"CLASSIFIER\": \"Linear\",\n    \"EPOCHS\": 10,\n    \"TRAIN_TRANSFORMS\": {\n        \"ColorJitter\": {\"brightness\":.2, \"contrast\":.2, \"saturation\":.2, \"hue\":.2},\n        \"RandomHorizontalFlip\": {},\n        \"RandomVerticalFlip\": {},\n        \"RandomAffine\": {\"degrees\": 45}\n    },\n    \"TRAIN_BATCH_SIZE\": 64,\n    \"VAL_BATCH_SIZE\": 32,\n    \"NUM_CLASSES\": df_train_labels[\"Expected\"].unique().size,\n    \"LOSS_FN\": \"CrossEntropyLoss\",\n    \"OPTIMIZER\": \"Adam\",\n    \"INIT_LEARNING_RATE\": 0.001,\n    \"WEIGHT_DECAY\": 0.0000001,\n    \"LR_SCHEDULER\": \"ReduceLROnPlateau\",\n    \"EARLY_STOPPING_EPSILON\": 0.0001\n}","0f5641fd":"# Initialize\ntrain_loader, val_loader = get_train_val_dataloader(config)\nmodel, optimizer, loss_fn, lr_scheduler = build(config)","6f32a527":"# Prepare logging\npath = Path(\"runs\", time.strftime(\"%d-%m-%y_%H:%M\", time.gmtime()) + \"_\" + socket.gethostname())\nconfig[\"PATH\"] = str(path)\n\nwriter = SummaryWriter(log_dir = config[\"PATH\"])\n\n# Train\nrun(config, model, train_loader, val_loader, optimizer, loss_fn, lr_scheduler, log_path=path, store_checkpoints=True, writer=writer)","973ed57e":"def objective(trial):\n    \n    # define search space\n    # model\n    config[\"FREEZE_FEATURE_EXTRACTOR\"] = False\n    config[\"BACKBONE\"] = trial.suggest_categorical(\"backbone\", [\"efficientnet-b0\", \"efficientnet-b1\", \"efficientnet-b2\"])\n    config[\"ADVPROP\"] = trial.suggest_categorical(\"advprop\", [True, False])\n    config[\"DROPOUT_RATE\"] = trial.suggest_float(\"dropout_rate\", 0, 0.1)\n    \n    # optimizer\n    config[\"OPTIMIZER\"] = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n    config[\"INIT_LEARNING_RATE\"] = trial.suggest_float(\"lr\", 5e-5, 1e-2, log=True)\n    if config[\"OPTIMIZER\"] in [\"Adam\", \"RMSprop\"]:\n        config[\"WEIGHT_DECAY\"] = trial.suggest_categorical('weight_decay_to_lr_ratio', ([0] + np.geomspace(1e-6, 1e-3, 9).tolist())) * config[\"INIT_LEARNING_RATE\"]\n        \n    # learning rate scheduler\n    config[\"LR_SCHEDULER\"] = trial.suggest_categorical(\"lr_scheduler\", [\"ReduceLROnPlateau\", \"\"])\n    \n    # augmentation\n    for key in config[\"TRAIN_TRANSFORMS\"][\"ColorJitter\"]:\n        config[\"TRAIN_TRANSFORMS\"][\"ColorJitter\"][key] = trial.suggest_float(key, 0, .5)\n    config[\"TRAIN_TRANSFORMS\"][\"RandomAffine\"][\"degrees\"] = trial.suggest_float(\"degrees\", 0, 180)\n    if config[\"BACKBONE\"] == \"efficientnet-b0\":\n        config[\"TRAIN_BATCH_SIZE\"] = trial.suggest_categorical(\"batchsize\", [64, 128])\n    else:\n        config[\"TRAIN_BATCH_SIZE\"] = 64\n    \n    # initialize\n    config[\"EPOCHS\"] = 30\n    train_loader, val_loader = get_train_val_dataloader(config)\n    model, optimizer, loss_fn, lr_scheduler = build(config)\n    \n    # evaluate metric to optimize\n    accuracy, _, _ = run(config, model, train_loader, val_loader, optimizer, loss_fn, lr_scheduler, trial=trial)\n    \n    return accuracy","4e9c7a65":"# Resume study from previous execution\n!cp ..\/input\/optunastudy\/study_final.db .\/","bc18dace":"# create optuna study\nstudy = optuna.create_study(\n    storage='sqlite:\/\/\/study_final.db',\n    study_name='study_final',\n    sampler=optuna.samplers.TPESampler(),\n    pruner=optuna.pruners.HyperbandPruner(),\n    direction=\"maximize\",\n    load_if_exists=True\n)","a77ff2a6":"study.optimize(objective, n_trials=10)\n\npruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\ncomplete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n\nprint(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))","5e07f6a7":"# Select trial that performs good consistently over the final epochs\nbest_trial = None\nbest_value = 0\n\nfor trial in study.trials:\n    if trial.state == optuna.trial.TrialState.COMPLETE:\n        final_accuracies = list(trial.intermediate_values.values())[-5:]\n        if (np.mean(final_accuracies) - np.std(final_accuracies)) > best_value:\n            best_value = np.mean(final_accuracies) - np.std(final_accuracies)\n            best_trial = trial\n            \nprint(\"Best trial:\")\n\nprint(\"  Value: \", best_value)\n\nprint(\"  Params: \")\nfor key, value in best_trial.params.items():\n    print(\"    {}: {}\".format(key, value))","d5bd607e":"# Optimal train configuration\nconfig = {\n    \"BACKBONE\": best_trial.params[\"backbone\"],\n    \"DROPOUT_RATE\": best_trial.params[\"dropout_rate\"],\n    \"ADVPROP\": best_trial.params[\"advprop\"],\n    \"FREEZE_FEATURE_EXTRACTOR\": False,\n    \"CLASSIFIER\": \"Linear\",\n    \"EPOCHS\": 30,\n    \"TRAIN_TRANSFORMS\": {\n        \"ColorJitter\": {\"brightness\": best_trial.params[\"brightness\"],\n                        \"contrast\": best_trial.params[\"contrast\"], \n                        \"saturation\": best_trial.params[\"saturation\"],\n                        \"hue\": best_trial.params[\"hue\"]},\n        \"RandomHorizontalFlip\": {},\n        \"RandomVerticalFlip\": {},\n        \"RandomAffine\": {\"degrees\": best_trial.params[\"degrees\"]}\n    },\n    \"TRAIN_BATCH_SIZE\": 64 if \"batchsize\" not in best_trial.params.keys() else best_trial.params[\"batchsize\"],\n    \"VAL_BATCH_SIZE\": 32,\n    \"NUM_CLASSES\": df_train_labels[\"Expected\"].unique().size,\n    \"LOSS_FN\": \"CrossEntropyLoss\",\n    \"OPTIMIZER\": best_trial.params[\"optimizer\"],\n    \"INIT_LEARNING_RATE\": best_trial.params[\"lr\"],\n    \"WEIGHT_DECAY\": (best_trial.params[\"weight_decay_to_lr_ratio\"] * best_trial.params[\"lr\"]),\n    \"LR_SCHEDULER\": best_trial.params[\"lr_scheduler\"],\n    \"EARLY_STOPPING_EPSILON\": 0.0001\n}","9e70734c":"# select best model from several initializatios\nevaluation = []\nconfig[\"EPOCHS\"] = 100\nfor i in range(12):\n    path = Path(\"runs\", time.strftime(\"%d-%m-%y_%H:%M\", time.gmtime()) + \"_\" + socket.gethostname())\n    config[\"PATH\"] = str(path)\n    writer = SummaryWriter(log_dir = config[\"PATH\"])\n    \n    train_loader, val_loader = get_train_val_dataloader(config)\n    \n    model, optimizer, loss_fn, lr_scheduler = build(config)\n    evaluation.append((path.name, *(run(config, model, train_loader, val_loader, optimizer, loss_fn, lr_scheduler, log_path=path, store_checkpoints=True, writer=writer))))\n    \n# get model with lowest final validation losses\nidx = np.argmin(np.mean(list(zip(*evaluation))[3][0][10:15]))\nrun_id = evaluation[idx][0]\nepoch = evaluation[idx][2]\npath = Path(\"runs\", run_id)\n\nprint(f\"Select run {idx + 1} with id {run_id}.\")","0485913c":"def get_predictions(model, dataloader):\n    predictions = []\n    \n    for i, sample in enumerate(dataloader):\n        \n        with evaluation_context(model):\n            im = sample['image'].to(device)\n            output = model(im)\n            probs = F.softmax(output, dim=1)\n            prediction = probs.argmax(axis=1)\n            \n            if 'label' in sample.keys():\n                predictions.extend(zip(prediction.tolist(), \n                                       sample['label'].tolist(),\n                                       sample['image_id']))\n            else:\n                predictions.extend(zip(prediction.tolist(), \n                                       sample['image_id']))\n    return predictions","a4d5daa1":"# ImageNet normalization\nnormalization = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) if not config[\"ADVPROP\"] \\\n    else transforms.Lambda(lambda img: img * 2.0 - 1.0)\n])\n\n# get test dataset predictions\ntest_dataset = MensaClassificationDataset(\n    test_image_paths,\n    transform=normalization)\ntest_dataloader = DataLoader(test_dataset)\n\nepoch += 1\nfor chkpt in range(4):\n    chkpt_epoch = epoch - (epoch % 5) - (chkpt * 5)\n    if chkpt_epoch >= 0:\n        # get predictions of last three checkpoints\n        state = torch.load(path\/\"checkpoints\"\/f\"{config['BACKBONE']}_pretrained_epoch_{chkpt_epoch}.pt\")\n        model.load_state_dict(state[\"model_state_dict\"])\n\n        test_preds = get_predictions(model, test_dataloader)\n        test_preds, test_ids = list(zip(*test_preds))\n\n        df_submission = pd.DataFrame({\n            \"Id\": test_ids,\n            \"Expected\": test_preds\n        }).set_index(\"Id\")\n\n        df_submission.to_csv(f\"submission_final_{chkpt_epoch}.csv\")","6a7a58f3":"## 2. Custom Pytorch Mensa Dataset","1eeb40da":"## 3. Setup training logic","3d70e587":"## 1. A Look at the Data","41c2d519":"## 4. Establish baseline, train pretrained EfficientNet (b0), freeze feature extractor","e9703e99":"## 5. Get predictions and create submission","4fa8b465":"# WS2020\/21 Few-shot object classification","b210502a":"> # 4. Fine tune the whole model with hyperparameter optimzation\nFind optimal hyperparameter configuration for data augmentation, model and learning process."}}