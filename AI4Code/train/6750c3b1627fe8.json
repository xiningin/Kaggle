{"cell_type":{"57b6dd8c":"code","fbc87cf5":"code","2cc6c459":"code","813aeb4f":"code","437cc375":"code","4e347e6d":"code","e8e32362":"code","4582a18f":"code","fdcaa2eb":"code","94675c58":"code","36348c38":"code","cf75baff":"code","1b7226bc":"code","058b3408":"code","61c3b433":"code","295cca00":"code","50635ba6":"code","25262d3b":"code","d6e30f11":"code","34b79c6c":"code","f2fbbc78":"code","aa515940":"code","7e29ee76":"code","91ab06b9":"code","59301652":"code","cc7a531f":"code","3b48b955":"markdown","9bc70f92":"markdown","5328454a":"markdown","8ec8d77e":"markdown","9d323a12":"markdown","7c31617b":"markdown","2c6bdf04":"markdown","92040244":"markdown","c3ad5aad":"markdown","7fc149b5":"markdown","c665535a":"markdown","9435e534":"markdown","39023b06":"markdown","2de00923":"markdown","c34a07ee":"markdown","3d9111a9":"markdown","4dde2336":"markdown","7c3e1f37":"markdown","8c80f231":"markdown","888a6e5c":"markdown","76e45bcf":"markdown","5b031068":"markdown","0d71f55e":"markdown","6215c42d":"markdown","05056da0":"markdown","c93f9e52":"markdown"},"source":{"57b6dd8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as visual\nimport matplotlib.pyplot as plt    # for plotting the data.\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport math\n\nfrom sklearn.preprocessing import StandardScaler\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbc87cf5":"whole_data = pd.read_csv(\"\/kaggle\/input\/breast-cancer\/data5.csv\")","2cc6c459":"whole_data","813aeb4f":"whole_data.drop([\"id\"], axis = 1, inplace =True)","437cc375":"# whole_data = whole_data.replace({\"M\": 1, \"B\": 0})\nwhole_data.diagnosis = [1.00 if i ==\"M\" else 0.00 for i in whole_data.diagnosis]","4e347e6d":"whole_data_float = np.array(whole_data, dtype = float)","e8e32362":"y = whole_data.diagnosis.values.reshape(-1,1)\nx = whole_data.drop([\"diagnosis\"], axis = 1)","4582a18f":"x_inputs = np.vstack((whole_data.radius_mean, whole_data.texture_mean,\n                      whole_data.perimeter_mean, whole_data.area_mean, \n                      whole_data.smoothness_mean, whole_data.compactness_mean,\n                      whole_data.concavity_mean, whole_data.concave_points_mean,\n                      whole_data.symmetry_mean, whole_data.fractal_dimension_mean,\n                      whole_data.radius_se, whole_data.texture_se, whole_data.perimeter_se,\n                      whole_data.area_se, whole_data.smoothness_se, whole_data.compactness_se, \n                      whole_data.concavity_se, whole_data.concave_points_se, \n                      whole_data.symmetry_se, whole_data.fractal_dimension_se,\n                      whole_data.radius_worst, whole_data.texture_worst, \n                      whole_data.perimeter_worst, whole_data.area_worst,\n                      whole_data.smoothness_worst, whole_data.compactness_worst,\n                      whole_data.concavity_worst, whole_data.concave_points_worst,\n                      whole_data.symmetry_worst, whole_data.fractal_dimension_worst))","fdcaa2eb":"x_inputs = np.transpose(x_inputs)","94675c58":"y_outputs = y","36348c38":"scaler1 = MinMaxScaler(feature_range = (0,1))\nnormalized_x_inputs = scaler1.fit_transform(x_inputs)\n\nscaler2 = MinMaxScaler(feature_range = (0,1))\nnormalized_y_outputs = scaler2.fit_transform(y_outputs)","cf75baff":"xtrain, xtest, ytrain, ytest = train_test_split(normalized_x_inputs, normalized_y_outputs, test_size = 0.3, random_state = 42)","1b7226bc":"model = Sequential()","058b3408":"model.add(Dense(70, input_dim = 30, activation = \"relu\"))","61c3b433":"model.add(Dense(1, activation = \"linear\"))","295cca00":"model.summary()","50635ba6":"model.compile(loss = \"mean_squared_error\", optimizer = \"adam\", metrics = [\"accuracy\"])","25262d3b":"model.fit(xtrain, ytrain, epochs = 100, batch_size = 30)","d6e30f11":"test_loss, test_accuracy = model.evaluate(xtest, ytest)\nprint(\"Test accuracy: %.2f \" %(test_accuracy * 100))","34b79c6c":"predictions = model.predict(normalized_x_inputs)","f2fbbc78":"correct_values = scaler2.inverse_transform(predictions)","aa515940":"print(\"Mean squared error : \", mean_squared_error(y_outputs[:,0], correct_values[:,0]))","7e29ee76":"plt.figure(1)\n#ax1 = plt.subplot(121)\nplt.title(\"Cancer Data Error Graph\")\nplt.plot(y_outputs[:100,0], color = \"blue\", label = \"Real Output\")   \nplt.plot(correct_values[:100,0], color = \"red\", linestyle= \"--\", label = \"Predicted Output\")\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"X (m)\")\nplt.legend()\nplt.show()","91ab06b9":"print(\"Mean squared error : \", mean_squared_error(y_outputs[:,0], correct_values[:,0]))","59301652":"plt.hist(y_outputs)\nplt.title(\"Number of People Having Malignant or \\n Bening Tumor as a Result of Test \")\nplt.xlabel(\"Bening \/ Malignant\")\nplt.ylabel(\"Number of People\")\nplt.show()","cc7a531f":"features_mean=list(x)\ndfM=whole_data[whole_data['diagnosis'] == 1]\ndfB=whole_data[whole_data['diagnosis'] == 0]\nplt.rcParams.update({'font.size': 8})\nfig, axes = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\naxes = axes.ravel()\n\nfor idx,ax in enumerate(axes):\n    ax.figure\n    binwidth= (max(whole_data[features_mean[idx]]) - min(whole_data[features_mean[idx]]))\/50\n    ax.hist([dfM[features_mean[idx]],dfB[features_mean[idx]]], bins=np.arange(min(whole_data[features_mean[idx]]),\n            max(whole_data[features_mean[idx]]) + binwidth, binwidth) , alpha=0.5, stacked=True, density = True,\n            label=['M','B'],color=['r','g'])\n    ax.legend(loc='upper right')\n    ax.set_title(features_mean[idx] + \"(mm)\")\nplt.tight_layout()\nplt.show()","3b48b955":"converted data type from DataFrame to float64","9bc70f92":"Sequential model mean, layers are connected each other one by one.","5328454a":"Then I loaded the data with using pandas and named by \u201cwhole_data\u201d.\nId columns was dropped. I do not need to detect tumor behaviour.","8ec8d77e":"During the training part, while loss function values were decreasing, our accuracies are increasing.","9d323a12":"Dataset information:\n* Dataset Characteristics: Multivariate\n* Attribute Characteristics: Real\n* Attribute Characteristics: Classification\n* Number of Instances: 569\n* Number of Attributes: 32\n* Missing Values: No","7c31617b":"## Column names and meanings:\n* id: ID number\n* diagnosis: The diagnosis of breast tissues (M = malignant, B = benign)\n* radius_mean: mean of distances from center to points on the perimeter\n* texture_mean: standard deviation of gray-scale values\n* perimeter_mean: mean size of the core tumor\n* area_mean: area of the tumor\n* smoothness_mean: mean of local variation in radius lengths\n* compactness_mean: mean of perimeter^2 \/ area - 1.0\n* concavity_mean: mean of severity of concave portions of the contour\n* concave_points_mean: mean for number of concave portions of the contour\n* symmetry_mean\n* fractal_dimension_mean: mean for \"coastline approximation\" - 1\n* radius_se: standard error for the mean of distances from center to points on the perimeter\n* texture_se: standard error for standard deviation of gray-scale values\n* perimeter_se\n* area_se\n* smoothness_se: standard error for local variation in radius lengths\n* compactness_se: standard error for perimeter^2 \/ area - 1.0\n* concavity_se: standard error for severity of concave portions of the contour\n* concave_points_se: standard error for number of concave portions of the contour\n* symmetry_se\n* fractal_dimension_se: standard error for \"coastline approximation\" - 1\n* radius_worst: \"worst\" or largest mean value for mean of distances from center to points on the perimeter\n* texture_worst: \"worst\" or largest mean value for standard deviation of gray-scale values\n* perimeter_worst\n* area_worst\n* smoothness_worst: \"worst\" or largest mean value for local variation in radius lengths\n* compactness_worst: \"worst\" or largest mean value for perimeter^2 \/ area - 1.0\n* concavity_worst: \"worst\" or largest mean value for severity of concave portions of the contour\n* concave_points_worst: \"worst\" or largest mean value for number of concave portions of the contour\n* symmetry_worst\n* fractal_dimension_worst: \"worst\" or largest mean value for \"coastline approximation\" - 1","2c6bdf04":"* values -> for converting series to int.\n* \"drop\" for dropping \"d\"iagnosis\" column.","92040244":"70% of data, 30 inputs, activation fnc: relu.","c3ad5aad":"We took transpose to convert it 569 * 30.","7fc149b5":"M was converted 1, B was converted 0.","c665535a":"* We use pandas for reading the dataset\n* Numpy is used for numerical data handling\n* Matplotlib is used for plotting the data\n* train_test_split, separates the data two parts as train and test. In this way, first part of the data we can teach our machine the real values (training part), then we can generate predicted values with using second part of the data (testing part).\n* Sequential is used in Keras to create models layer by layer\n* Dense layer mean that input neuron is connected to output neuron like a simple neurol network, fully connected layer.\n* MinMaxScaler is used to give range\n* mean_squared_error is used to calculate the real data and predicted data\u2019s MSE.","9435e534":"We came to the most important part. Creating neural network part.","39023b06":"* y -> diagnosis\n* x -> whole_data - diagnosis","2de00923":"* Using MinMaxScaler function, we normalized the data. In other words, we shrank the range between 0 and 1.\n* Then we normalized inputs, and outputs.","c34a07ee":"## normalization part:","3d9111a9":"* For the compiling part, I calculated errors as a mean squared error, and I used Adam optimizer.\n* In order to see the accuracies, we chose metrics as an accuracy. While calculating the error,\ngenerally mean squared error is used instead of normal error.\n* This is because, our errors can be positive and also hey can be negative sometimes.\n* For example, after testing part, our error can be 0.5 and the other can be -0.5.\n* When I add them (0.5 + (-0.5)), it seems we do not have any error.\n* But it is not the correct. In order to get reed of the negative results, we use mean squared error.\n* That\u2019s why I preferred loss function as MSE.","4dde2336":"axis = 1, for dropping whole column.","7c3e1f37":"We implemented 100 times forward propagation and 100 times backward propagation in groups of 30s.","8c80f231":"After I found predicted values, mean squared errors and accuracies, we plotted the results for first 100 data.","888a6e5c":"After training the data, we gave the machine remaining part of the data (30% of the data) in order to see the predictions.","76e45bcf":"2 different types outputs, activation fnc: Linear","5b031068":"## Aim of the project: \n* I used artificial neural networks in order to train cancer data and then we tested it to predict wheather the person has melignant tumor or bening tumor.\n* **In this project, we use the Neural Network technique to determine whether a person cancer or not.\nFirst we train 70% of the given data then test if the machine learn it well or not.","0d71f55e":"## Code part:","6215c42d":"* After normalization part, data was seperated using train test split method. We separated %70 of data as training part and %30 of data as testing part.\n* In training part, the data is given both inputs and outputs in order to fit the parameters of the model.\n* In test part, only inputs are given, and results are expected to be estimated.","05056da0":"We wrote this code to see the accuracy results after the training part:","c93f9e52":"In order to convert normalized form to the real data form I used inverse transform method"}}