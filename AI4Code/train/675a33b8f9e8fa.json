{"cell_type":{"b7370d7e":"code","01b40c39":"code","1c22d2f6":"code","a3fb6101":"code","99dcf64c":"code","bcb1d6b0":"code","895b9c4a":"code","2fc1cc14":"code","63d7dedd":"code","929baa29":"code","0b713067":"code","cfb78027":"code","659f2cb7":"code","f88d4d4b":"code","ded6e447":"code","9503f3b3":"code","1fd58397":"code","3a432aee":"code","2ec363bf":"code","c381c73d":"code","f71b0fbb":"code","ae1b6987":"code","035ae3eb":"code","4e60153e":"code","7633d6d4":"code","cd9d0c5b":"code","b1c1197d":"code","b6c69772":"code","ec30adbc":"code","32cb461c":"code","84a29472":"code","a81fb899":"code","1908d4cd":"code","5f99a917":"code","3f96d781":"code","d9ef888f":"code","73cd86b6":"code","c2299bfc":"code","419fe8b0":"code","a96906ab":"code","0b22d6d3":"code","4048c463":"code","0dfa0af6":"markdown","ad93c744":"markdown","55a38791":"markdown","c72ad63c":"markdown","e9b50d1a":"markdown","e39c97cd":"markdown","73c6816f":"markdown","8a4131d1":"markdown","ed9c6402":"markdown"},"source":{"b7370d7e":"import pandas as pd\n\nfile_path = r\"..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\"\nairbnb = pd.read_csv(file_path)\nairbnb.head()","01b40c39":"airbnb.columns","1c22d2f6":"airbnb.describe()\n## calculated_host_listings_count =  count occurance of hostid ","a3fb6101":"%matplotlib inline\nimport matplotlib.pyplot as plt\nairbnb.hist(bins=50, figsize=(20,15))\nplt.show()","99dcf64c":"from pandas.plotting import scatter_matrix\n\nattributes = [\"neighbourhood_group\", \"room_type\",'number_of_reviews','calculated_host_listings_count', 'availability_365',\"price\"]\nscatter_matrix(airbnb[attributes], figsize=(16, 9))","bcb1d6b0":"import matplotlib.image as mpimg\nimport numpy as np\nfrom PIL import Image\n\nimg = Image.open('..\/input\/new-york-city-airbnb-open-data\/New_York_City_.png')  \n\nax = airbnb.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(16,9),\n                       s=airbnb['calculated_host_listings_count'], label=\"host_listings_count\",\n                       color=airbnb[\"price\"], cmap=plt.get_cmap(\"jet\"),\n                       colorbar=True, alpha=0.4,\n                      )\nplt.imshow(img,  alpha=0.5,extent=[-74.244420,-73.712990,40.499790,40.913060],\n           cmap=plt.get_cmap(\"jet\"))\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\n\nplt.legend(fontsize=16)\nplt.show()","895b9c4a":"## boardline for neighbourhood_group \nimport geopandas as gpd\ndf =  gpd.read_file(gpd.datasets.get_path('nybb'))\nax = df.plot(figsize = (10,10),alpha = 0.5,edgecolor = 'k')\n##plt.grid(b=True, which='major', color='#666666', linestyle='-')\n##df\naverage = airbnb[['neighbourhood_group','price']].groupby('neighbourhood_group').mean().reset_index()\naverage.rename(columns={'price':\"Ave\"},inplace =True)\naverage.rename(columns={'neighbourhood_group':\"BoroName\"},inplace =True)\ndf = df.merge(average,on = 'BoroName' )\ndf","2fc1cc14":"### The average price of airbnb in each neighbourhood_group: Manhattan has the highest value\nimport matplotlib.pyplot as plot\nfig,ax = plt.subplots(1,1)\nfig.set_size_inches(16,9)\nplt.grid(b=True, which='major', color='#666666', linestyle='-')\ndf.plot(column = 'Ave',cmap = 'Blues', ax =ax,legend = True)","63d7dedd":"##\nairbnb['price'].hist(figsize =(16,9),bins = 20)\n","929baa29":"## since most of airbnb is lower than 2000, I only select price from 0-100. \nimport numpy as np\na = np.arange(airbnb['price'].min(),airbnb['price'].max()+2000,1000).tolist()\nb = np.arange(0,11,1)\nresultA = pd.DataFrame({'label':b})\nairbnb['priceType'] = pd.cut(airbnb['price'],bins = a,labels =b )\nresultA['range'] = a[:-1]\nresultA['num'] = airbnb['priceType'].value_counts()\nresultA['percentage'] = pd.DataFrame(airbnb['priceType'].value_counts()\/airbnb['priceType'].count())\n\nresultA","0b713067":"import seaborn as sns\nf,ax =plt.subplots(figsize =(16,9))\nsns.barplot(x = resultA['range'],y = resultA['percentage'])","cfb78027":"## since most of airbnb is lower than 2000, I only select price from 0-600. \nairSub = airbnb.loc[airbnb['price'] <=1000]\nairSub['price'].hist(figsize =(16,9),bins = 20)","659f2cb7":"## correlationship for each features\ncorr = airSub.corr()\nplt.figure(figsize= (16,9))\nplt.title('Correlation of features in Airbnb data set',fontsize =18)\nsns.heatmap(corr,annot = True, cmap ='RdYlGn',linewidths = 0.2, annot_kws ={'size':10})\nplt.show()","f88d4d4b":"## 1.drop some unrelated features: name,host_name,id,last_review,host_id \nairbnbFull = airSub.drop(['name','host_name','id','last_review','host_id','priceType'],axis =1)\nairbnbFull.head()","ded6e447":"## reviews_per_month will be filled by using simpleImputer(strategy=\"median\")\nairbnbFull.isnull().sum()\n","9503f3b3":"sample_incomplete_rows = airbnbFull[airbnbFull.isnull().any(axis=1)]\nsample_incomplete_rows.head()","1fd58397":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.impute import SimpleImputer","3a432aee":"airbnbFull.dtypes","2ec363bf":"##In the following section, train_test_split,gridsearchCV and RandomizedSearchCV will be used to build model separately","c381c73d":"X = airbnbFull.drop(['price'],axis =1)\ny = airbnbFull[['price']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    random_state=0)\n## classify the attributes based on their type:text & num\na = dict(X.dtypes)\ntext_attribs = []\nnum_attribs = []\nfor attribs,ctype in a.items():\n    if ctype == 'object':\n        text_attribs.append(attribs)\n    else:\n        num_attribs.append(attribs)","f71b0fbb":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","ae1b6987":"## test for LinearRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nNum_pipeline = Pipeline([('std_scaler', StandardScaler()),('imputer', SimpleImputer(strategy=\"median\"))])\n\nText_pipeline = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(transformers=[(\"num_pipeline\", Num_pipeline,num_attribs),(\"Text_pipeline\",Text_pipeline,text_attribs)])\n\nfull_pipeline = Pipeline(steps = [('preprocessor',preprocessor),('linear',LinearRegression())])\nfull_pipeline.fit(X_train, y_train.values.ravel())\nprint(\"model score: %.3f\" % full_pipeline.score(X_test, y_test))","035ae3eb":"## measure the result of four algorithm based on Mean Squared Error,Mean Absoluate Error.\n##(Note: it will take at least 10 mins to get result. Please let me know if there are anything I can improve. Many thanks!)\nalgorithm = [('linear',LinearRegression()),('DT',DecisionTreeRegressor(random_state=42)),\n            ('RF',RandomForestRegressor(n_estimators=100, random_state=42)),('SVR',SVR(kernel=\"linear\"))]\n\nresult = pd.DataFrame(columns=['Algorithm','Model Score','Mean Squared Error','Mean Absoluate Error'])\n    \nfor index, name in enumerate(algorithm):\n    if index <=1:\n        full_pipeline = Pipeline(steps = [('preprocessor',preprocessor),name])\n        full_pipeline.fit(X_train, y_train)\n        X_predictionL = full_pipeline.predict(X_test)\n        mse = mean_squared_error(y_test, X_predictionL)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(y_test, X_predictionL)\n        modelScore = full_pipeline.score(X_test, y_test)\n        result=result.append([{'Algorithm':name[0],'Model Score':modelScore,'Mean Squared Error':rmse,'Mean Absoluate Error':mae}]) \n    else:\n        full_pipeline = Pipeline(steps = [('preprocessor',preprocessor),name])\n        full_pipeline.fit(X_train, y_train.values.ravel())\n        X_predictionL = full_pipeline.predict(X_test)\n        mse = mean_squared_error(y_test, X_predictionL)\n        rmse = np.sqrt(mse)\n        mae = mean_absolute_error(y_test, X_predictionL)\n        modelScore = full_pipeline.score(X_test, y_test)\n        result=result.append([{'Algorithm':name[0],'Model Score':modelScore,'Mean Squared Error':rmse,'Mean Absoluate Error':mae}])\nresult","4e60153e":"from sklearn.model_selection import cross_val_score","7633d6d4":"###algorithm = [('linear',LinearRegression()),('DT',DecisionTreeRegressor(random_state=42)),('RF',RandomForestRegressor(random_state=42)),('SVR',SVR(kernel=\"linear\"))]\nresultK = pd.DataFrame(columns=['Algorithm','Mean of Mean Squared Error','STD of Mean Squared Error'])\n## linear \nfull_pipeline = Pipeline(steps = [('preprocessor',preprocessor),('linear',LinearRegression())])\n\nscores = cross_val_score(full_pipeline, X,y,scoring=\"neg_mean_squared_error\", cv=10,n_jobs = -1)\nlin_rmse_scores = np.sqrt(-scores)\nresultK= resultK.append([{'Algorithm':'linear','Mean of Mean Squared Error':lin_rmse_scores.mean(),'STD of Mean Squared Error':lin_rmse_scores.std()}])","cd9d0c5b":"## DT\nfull_pipeline = Pipeline(steps = [('preprocessor',preprocessor),('DT',DecisionTreeRegressor(random_state=42))])\nscores = cross_val_score(full_pipeline,X,y,scoring=\"neg_mean_squared_error\", cv=10,n_jobs = -1)\nDT_rmse_scores = np.sqrt(-scores)\n\nresultK = resultK.append([{'Algorithm':'DT','Mean of Mean Squared Error':DT_rmse_scores.mean(),'STD of Mean Squared Error':DT_rmse_scores.std()}])\nresultK","b1c1197d":"## Four algorithm \n## However,it will take very long time to get result,so I didn't run it. Need further imporve. \nalgorithm = [('linear',LinearRegression()),('DT',DecisionTreeRegressor(random_state=42)),\n            ('RF',RandomForestRegressor(n_estimators=100, random_state=42)),('SVR',SVR(kernel=\"linear\"))]\n\nresultK = pd.DataFrame(columns=['Algorithm','Mean of Mean Squared Error','STD of Mean Squared Error'])\n    \nfor index, name in enumerate(algorithm):\n    if index <=1:\n        full_pipeline = Pipeline(steps = [('preprocessor',preprocessor),name])\n        scores = cross_val_score(full_pipeline, X,y,scoring=\"neg_mean_squared_error\", cv=10,n_jobs = -1)\n        rmse_scores = np.sqrt(-scores)\n        resultK = resultK.append([{'Algorithm':name[0],'Mean of Mean Squared Error':rmse_scores.mean(),'STD of Mean Squared Error':rmse_scores.std()}])\n    else:\n        full_pipeline = Pipeline(steps = [('preprocessor',preprocessor),name])\n        scores = cross_val_score(full_pipeline, X,y.values.ravel(),scoring=\"neg_mean_squared_error\", cv=10,n_jobs = -1)\n        rmse_scores = np.sqrt(-scores)\n        resultK = resultK.append([{'Algorithm':name[0],'Mean of Mean Squared Error':rmse_scores.mean(),'STD of Mean Squared Error':rmse_scores.std()}])\nresultK","b6c69772":"### find the name of parameter in the RF for gridSearchCV. Name just changed if you include algorithm in pipeline\nsorted(full_pipeline .get_params().keys())","ec30adbc":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'n_estimators': [2,4,8], 'max_features':[2,4,8]},{'bootstrap': [False], 'n_estimators': [3, 6], 'max_features': [3,6,8]}]\n\n\ngrid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5,scoring='neg_mean_squared_error',n_jobs = -1,return_train_score=True)\nX_prepared = preprocessor.fit_transform(X_train)\ngrid_search.fit(X_prepared,y_train.values.ravel())","32cb461c":"grid_search.cv_results_","84a29472":"grid_search.best_params_","a81fb899":"grid_search.best_estimator_","1908d4cd":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params) ##rmse","5f99a917":"feature_importances = grid_search.best_estimator_.feature_importances_\nattributes = list(airbnbFull)\nsorted(zip(feature_importances, attributes), reverse=True)","3f96d781":"final_model = grid_search.best_estimator_\nX_test_prepared = preprocessor.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmseGrid = np.sqrt(final_mse)\nfinal_rmseGrid","d9ef888f":"## Same method as above GridSearch,but the algorithm is included in pipeline. However, the best_params_ is different. \nfrom sklearn.model_selection import GridSearchCV\n\n\nparam_grid = [{'RF__n_estimators': [2,4,8], 'RF__max_features':[2,4,8]},{'RF__bootstrap': [False], 'RF__n_estimators': [3, 6], 'RF__max_features': [3,6,8]}]\nfull_pipeline = Pipeline(steps = [('preprocessor',preprocessor),('RF',RandomForestRegressor(random_state=42))])\n\ngrid_search2 = GridSearchCV(full_pipeline, param_grid, cv=5,scoring='neg_mean_squared_error',n_jobs = -1,return_train_score=True)\ngrid_search2.fit(X_train,y_train.values.ravel())","73cd86b6":"grid_search2.best_params_","c2299bfc":"## Since final_model2 has already included preprocessor, the transform of X_test is not required. \nfinal_model2 = grid_search2.best_estimator_\n\n\nfinal_predictions2 = final_model2.predict(X_test)\n\nfinal_mse2 = mean_squared_error(y_test, final_predictions2)\nfinal_rmseGrid2 = np.sqrt(final_mse2)\nfinal_rmseGrid2","419fe8b0":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=50),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error',n_jobs = -1, random_state=42)\nX_prepared = preprocessor.fit_transform(X_train)\nrnd_search.fit(X_prepared,y_train.values.ravel())","a96906ab":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","0b22d6d3":"rnd_search.best_estimator_","4048c463":"final_model = rnd_search.best_estimator_\nX_test_prepared = preprocessor.transform(X_test)\n\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmseRand = np.sqrt(final_mse)\nfinal_rmseRand","0dfa0af6":"# 1. Split data by train_test_split(0.8\/0.2)","ad93c744":"# Split data by k-fold","55a38791":"# missing value","c72ad63c":"# GridSearchCV","e9b50d1a":"# RandomizedSearchCV","e39c97cd":"# Data Exploration","73c6816f":"# Sklearn Pipeline","8a4131d1":"This notebook mainly focuses on data visualisation(especially for geographical information of Airbnb in New York) and apply Sklearn to build prediction model(linear regressor, Regression Tree, Random Forest Regressor and SVM) based on different splitting method(1. Train_test_split, 2.K_fold,3. GridSearchCV, 4. RandomizedSearchCV).\nRandom Forest Regression has the best result among the four algorithms in the Train_test_split method. And, it has further improved in the following three splitting methods(K_fold, GridSearchCV, RandomizedSearchCV).\n\n**Problem:\n1.Random Forest Regression will take a very long time to get the result in K_fold splitting method. This area will be further improved in the future. \n2.Please let me know if anything I can further improve. Many thanks!\n    \n**Plan:\nData preprocessing will be further improved \nMore data visualisation will be added in the future. \nMore algorithms will be applied in this model\n","ed9c6402":"# Airbnb New York- learn from chapter 2"}}