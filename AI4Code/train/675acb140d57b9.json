{"cell_type":{"d020e8b3":"code","ae0d0754":"code","5acf897b":"code","31cc3d1d":"code","4f489602":"code","a9f025d6":"code","a3678a53":"code","3cd6ff50":"code","1db0bd32":"code","699bb6d4":"code","901d3a09":"code","196ccbee":"code","ab8a3235":"code","8a2f866b":"code","66d127bf":"code","8154e8f3":"code","1871b024":"code","510133f8":"code","c315bbba":"code","5467abe5":"code","e114d8f8":"code","c6a966bf":"code","7605267f":"code","bd30be68":"code","09e9a90d":"code","4ed6a427":"code","4df318b3":"code","71fbaf87":"code","4bbce201":"code","3606f3ab":"code","d3511352":"code","2026bd69":"code","fec2f8ec":"code","d865010f":"code","75abfb24":"code","9f01c3ac":"code","6ece621a":"code","0273a110":"markdown","2164f570":"markdown","fae3b1c8":"markdown","89efd77e":"markdown","c0e1e8ba":"markdown","70b27618":"markdown","bec9c4e5":"markdown","8f817af5":"markdown","0178ab30":"markdown","36d8d242":"markdown","0424d02f":"markdown","23568f38":"markdown","a3feeb34":"markdown","2ffad117":"markdown","710c2e61":"markdown","233652d4":"markdown","47441d10":"markdown","0ad51e2e":"markdown","8b7c89ec":"markdown","79d9b01a":"markdown","488dcc03":"markdown","f3cd458b":"markdown","6c0be8d8":"markdown","151ec206":"markdown","14a7dacf":"markdown","e1160e93":"markdown","7d2da325":"markdown","1fd07354":"markdown","22df8606":"markdown","ee416452":"markdown","5f9bef26":"markdown","50bdec50":"markdown","246f2b47":"markdown","3b28bbaa":"markdown","9cb6ecfc":"markdown","f9a4434c":"markdown","a9961511":"markdown","04b7427b":"markdown"},"source":{"d020e8b3":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Modeling and Prediction\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import confusion_matrix","ae0d0754":"# Download training data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","5acf897b":"# Display the first 5 rows of the training dataframe.\n","31cc3d1d":"# Display basic information about training data\ntrain.info()","4f489602":"# Download test data\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.tail(7)  # Display the 7 last rows of the training dataframe","a9f025d6":"# Display basic information about the test data\n","a3678a53":"# Download submission sample file\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","3cd6ff50":"# Display the 10 first rows of the submission dataframe\n","1db0bd32":"def highlight(value):\n    # The painting of the cell in different colors depending on the value: \n    # >= 0.5 - palegreen, < 0.5 - pink\n    \n    if value >= 0.5:\n        style = 'background-color: palegreen'\n    else:\n        style = 'background-color: pink'\n    return style","699bb6d4":"# Pivot table\npd.pivot_table(train, values='Survived', index=['Sex']).style.applymap(highlight)","901d3a09":"# Calculation the new feature \"Family_size\" as the size of each passenger's family by \n# the number of his \/ her siblings (\"SibSp\") and his \/ her parents and descendants (\"Parch\")\ntrain['Family_size'] = (train['SibSp'] + train['Parch']) \/\/ 2  # it is wrong!","196ccbee":"# Pivot table for input features 'Sex' and 'Family_size' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Sex', 'Family_size']).style.applymap(highlight)","ab8a3235":"# Analysis missing data for feature \"Age\"\nmissing_age_values = train['Age'].isnull().sum() ","8a2f866b":"# Calculate what percentage of the value of missing_age_values is relative \n# to the total number of values (dataframe length) and round it to two decimal places. \n# Save it into missing_age_values_per_cent.\nmissing_age_values_per_cent = missing_age_values\/1000  # it is wrong!","66d127bf":"# Output missing_age_values_per_cent\nprint(f'Feature \"Age\" has {missing_age_values_per_cent}, %')","8154e8f3":"# Calculation the average (mean) value of all data of Age. Save it into mean_age.\nmean_age = 30   # it is wrong!","1871b024":"# Round mean_age to two decimal places and print it.\n","510133f8":"# Statistics for training data\ntrain.describe()","c315bbba":"# Number of unique values of age\nnumber_age_unique_values = len(train['Age'].unique())\nnumber_age_unique_values","5467abe5":"# Divide all age values train['Age'] by 7 and save in new feature train['Age_7'].\ntrain['Age_7'] = train['Age']     # it is wrong!","e114d8f8":"train['Age_7'] = train['Age_7'].fillna(mean_age \/\/ 7).astype('int')","c6a966bf":"# Print the number of unique values of the feature \"Age\"\n","7605267f":"# Pivot table for input features 'Sex', 'Family_size' and 'Age' and output feature 'Survived'\n","bd30be68":"# Decision trees work with numbers, not words, so we must encode feature \"Sex\" by numbers\ntrain['Sex'].replace({'male': 0, 'female': 1}).head()","09e9a90d":"# Copy commands (see above) to this function to calculate new features or process existing ones\n\ndef df_transform(df):\n    # FE for df\n    \n    # Number of family members - feature \"Family_size\"\n    df['Family_size'] = (df['SibSp'] + df['Parch']) \/\/ 2  # it is wrong!\n    \n    # Age multiple of 7\n    df['Age_7'] = df['Age']    # it is wrong!\n    \n    # Average age of all dataset multiple of 7\n    mean_age = 30 \/\/ 7   # it is wrong!\n    \n    # Replace missing age values to average age and rounding them to integers\n    df['Age_7'] = df['Age_7'].fillna(mean_age).astype('int')\n    \n    # Encoding feature \"Sex\" by numbers\n    df['Sex'] = df['Sex'].replace({'male': 0, 'female': 1})\n    \n    # Select the main features\n    df = df[['Family_size','Age_7','Sex']]\n    \n    return df","4ed6a427":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('Survived')","4df318b3":"# FE to training dataset\ntrain = df_transform(train)\n\n# Statistics of training dataset\ntrain.info()","71fbaf87":"train","4bbce201":"# FE to test dataset\ntest = df_transform(test)\ntest.info()","3606f3ab":"test","d3511352":"# Select model as Decision Tree Classifier \n# \"Classifier\" because target has limited (integer) number of classes, in this case 2 classes = [0, 1] or [\"No Survived\", \"Survived\"]\n# For a small amount of data, it is better to choose a smaller parameter max_depth - from 3 to 5, let give 4\n# at a more complex Level we will teach the program to calculate it automatically\nmodel = DecisionTreeClassifier(max_depth=4, random_state=42)\n\n# Training model\nmodel.fit(train, target)","2026bd69":"# Visualization - build a plot with Decision Tree\nplt.figure(figsize=(20,12))\nplot_tree(model, filled=True, rounded=True, class_names=[\"No Survived\", \"Survived\"], feature_names=train.columns) ","fec2f8ec":"# Prediction for training data\ny_train = model.predict(train).astype(int)","d865010f":"confusion_matrix(target, y_train)","75abfb24":"# Prediction of target for test data\ny_pred = model.predict(test).astype(int)","9f01c3ac":"# Saving the result into submission file\nsubmission[\"Survived\"] = y_pred\nsubmission.to_csv('submission.csv', index=False) # Competition rules require that no index number be saved\n\n# Building the Histogram of predicted target values for test data\nsubmission['Survived'].hist()","6ece621a":"# Calculation of the mean value of forecasting data\nsubmission['Survived'].mean()","0273a110":"### Decision Tree Visualization","2164f570":"### Confusion matrix","fae3b1c8":"### It is recommended to start working with this notebook from study:\n* the [description of the data](https:\/\/www.kaggle.com\/c\/titanic\/data)\n* the [task](https:\/\/www.kaggle.com\/c\/titanic) of the competition\n* the [my lecture](https:\/\/www.youtube.com\/watch?v=WERtPBptOWw&list=PL4DHq-xU-ebUiB6T6vjd0SoDha4GOm8zV&index=2&t=951s) about this notebook in YouTube (in Ukrainain).","89efd77e":"**TASK:** Calculate what percentage of the value of missing_age_values is relative to the total number of values (dataframe length) and round it to two decimal places. Save it into missing_age_values_per_cent.","c0e1e8ba":"**TASK:** Display the 10 first rows of the submission dataframe","70b27618":"**ADDITIONAL TASK:** \n1. Try changing the value of random_state (0, 1, ... 42 - choose some values) in the DecisionTreeClassifier() in section 4 and check for changes in the forecast results (in the confusion matrix, in the histogram above, and in the mean value of forecasting data).\n2. Submit each version of the forecast (with a different list of features, with a different value of max_depth - see additional tasks above) to the competition and see if you can rise in the rankings.","bec9c4e5":"## 5. Prediction & Submission<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","8f817af5":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","0178ab30":"**ADDITIONAL TASKS:**\n1. Experiment with resizing (in the \"figsize = ()\") the drawing with the decision tree.\n2. Try changing the value of max_depth above so that the number of correct predictions (prediction values 0 when 0 and 1 when 1 ib the confusion matrix) is greater.","36d8d242":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Titanic - Top score : one line of the prediction](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-score-one-line-of-the-prediction)\n* [Three lines of code for Titanic Top 25%](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1-titanic-decision-tree)\n* [Three lines of code for Titanic Top 20%](https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20)","0424d02f":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","23568f38":"**TASK:** Display basic information about the test data","a3feeb34":"**TASK:** Correct the formula that determines the size of each passenger's family (the new feature: train['Family_size']) by the number  of his \/ her siblings (\"SibSp\") and his \/ her parents and descendants (\"Parch\")","2ffad117":"**ADDITIONAL TASKS:**\n1. Replace the feature \"Family_size\" to cabin class \"Pclass\" and \"Age_7\" to \"Embarked\".\n2. Replace the value of feature \"Embarked\" with the numbers 0, 1 and 2 (for example: 0 => C = Cherbourg, 1 => Q = Queenstown, 2 => S = Southampton). ","710c2e61":"<a class=\"anchor\" id=\"0\"><\/a>\n# [AI-ML-DS : Training for beginners](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-for-beginners-in-kaggle). Level 1 (very simple). 2020\n## Kaggle GM, Prof. [@vbmokin](https:\/\/www.kaggle.com\/vbmokin)\n### [Vinnytsia National Technical University](https:\/\/vntu.edu.ua\/), Ukraine\n#### [Chair of the System Analysis and Information Technologies](http:\/\/mmss.vntu.edu.ua\/index.php\/ua\/)","233652d4":"**TASK:** Build a pivot table for input features 'Sex', 'Family_size' and 'Age' and output feature 'Survived'.","47441d10":"**TASK:** Divide all age values train['Age'] by 7 and save in new feature train['Age_7'].","0ad51e2e":"Let's make the following assumptions:\n- **Women** have a better chance of surviving ==> feature \"**Sex**\" is important\n- **Single people** have a worse chance of surviving ==> the **number of family members** is important\n- **Children** have a better chance of survival ==> **Age** is important","8b7c89ec":"**It is important to make sure** that all features in the training and test datasets:\n* do not have missing values (number of non-null values = number of entries of index) \n* all features have a numeric data type (int8, int16, int32, int64 or float16, float32, float64).","79d9b01a":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","488dcc03":"**TASK:** Copy your commands (see above) to this function to calculate new features or process existing ones","f3cd458b":"**TASK:** Calculation the average (mean) value of all data of Age. Save it into mean_age.","6c0be8d8":"The result of FE is usually combined into one function so that it is convenient to apply to different datasets","151ec206":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","14a7dacf":"## Competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic)","e1160e93":"Next, you should commited this notebook, then in the \"Output\" section find the file \"submission.csv\" and press button \"Submit\" for the submission it to the competition.","7d2da325":"### Feature \"Family_size\"","1fd07354":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE](#3)\n1. [Modeling](#4)\n1. [Prediction & Submission](#5)","22df8606":"We reduce the number of unique age values by 7 times (7 because children under 14 (14 \/\/ 7 = 2) were considered a child at that time).","ee416452":"Bad solution, but the easiest way is to replace the missing data with the average.","5f9bef26":"## The general task (10 steps):\nI. Solve all tasks after the word \"**TASK:**\", entering the code in the cell under each such task.\n\nII. Click on the top: \"Run All\".\n\nIII. Compare your results with the answers in the previous version of my notebook. If something is different - fix it. If you find an error or inaccuracy in comments or tasks in my notebook, please write about it in the comment to that notebook.\n\nIV. Perform \"**ADDITIONAL TASK:**\" (at least a few) so that the performance results (plots, feature names and\/or forecast) in your version of the notebook begin to differ from my original notebook.\n\nV. Prepare the notebook for publication:\n\n* rename by changing the name at the top of the window (to the left of the words \"Draft saved\"), for example to the following: \n**Titanic: very simple Decision Tree with tuning**\nor \n**AI-ML-DS : Titanic - Decision Tree - SAIT VNTU PTL**\nwhere at the end you write the name of your team in this competition;\n* at the beginning of your version of the notebook add 3 new cells of type \"Markdown\" with the following text:\n\n-> *in the first cell* - a reference to the original source (Kaggle himself writes it on the right, but it is customary to duplicate it at the beginning to explain that this notebook is a development of another):\n\n**Thanks to [AI-ML-DS Training. L1A: Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1a-titanic-decision-tree)**\n\n-> *in the second cell* - a shortlist of what you updated, changed, deleted, etc., for example (**be sure to edit it!**):\n\n### My upgrade:\n* add new plots;\n* replaced feature \"Age\" to \"Pclass\";\n* add a new feature \"Fare\";\n* model tuning (changed parameter \"max_depth\");\n* improved accuracy - LB score increase to 0.78883\n\n-> *in the third cell* (you can combine with the second, ie immediately write what is added and why) - give an explanation of the reasons for such actions: ideas, hypotheses, experimental plan, for example:\n\nI'm sure that class of cabin \"Pclass\" and fare of ticket \"Fare\" is a more important feature than \"Age\".\n\nIf your team consists of some members please write: \"We sure...\".\n\nVI. \"Save & Run All (Commit)\" of your notebook - press the button \"Save Version\" in the top right corner of the window and press \"Save\". When done, scroll through and make sure there are no ERROR messages in any cell. Sometimes it can be executed successfully, but half of the cells are with errors, then the result of execution will be incorrect.\n\nVII. If it's a notebook is on a certain dataset (not for competition), go to the next step right away. Or if this notebook for the Prize Kaggle Competition then after the committing of running go to the section \"Output\" and press \"Submit\". Make sure the system successfully accepted your submission and wrote your LB score. Calculation LB score can take some time (from a few seconds to 9 hours). If it outputs an error and LB score does not output, then return to the editor and correct the error.\n\nVIII. Publish the successfully committed notebook: press the button \"Sharing\" in the top right corner of the window, in the form at the top press \"Private\" and select \"Public\" in the list, then press \"Save\". \n\nIX. Choose tags (optional) - press \"Edit tags\" and choose for example \"exploratory data analysis\", \"feature engineering\", \"classification\", \"beginner\".\n\nX. If you are a student of my course, then please add a comment to the notebook, where you tag me, which will then allow me to quickly find it, for example, the following text:\n\n-> bad option:\n\n@vbmokin, please see how we solved this problem\n\n-> best option:\n\nThe notebook was prepared by team \"SAIT VNTU PTL\" from the course \"AI-ML-DS Training\" (tutor - @vbmokin) for the competition \"Titanic: Machine Learning from Disaster\".\n\n\n\n**Good luck!**","50bdec50":"**TASK:** Round mean_age to two decimal places and print it.","246f2b47":"# The concept of training:\n* the **last version (commit)** of the notebook has:\n        * the basic tasks (after \"TASK:\")\n        * the additional tasks for self-execution (after \"ADDITIONAL TASK:\")\n* the **previuos version (commit)** of the notebook has **answers** for the basic tasks","3b28bbaa":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","9cb6ecfc":"### Feature \"Age\"","f9a4434c":"**TASK:** Display the first 5 rows of the training dataframe.","a9961511":"**TASK:** Determine how many unique values the feature \"Age\" contains and print it.","04b7427b":"### Feature \"Sex\""}}