{"cell_type":{"cbcce53d":"code","b3fa88dd":"code","9d4f6648":"code","74089d16":"code","0f07c3f5":"code","d0692d0d":"code","f589746e":"code","a253eec0":"code","d90ba381":"code","8344137f":"code","ca9bc341":"code","6e95dc04":"code","117a5818":"code","b0b507a7":"code","756713ab":"code","63772a0a":"code","43d4dd24":"code","9124ddba":"code","e196e180":"code","a3897b1c":"code","eb6f80fd":"code","cb6d0fe7":"code","60fdbbe6":"code","85368541":"code","4abd26d9":"code","101cb183":"code","14763ea3":"code","b5204f08":"code","5c30d361":"code","6bf724ee":"code","96ca6a5e":"code","fff7dcc7":"code","63fff12e":"code","a676792a":"code","220d9d0a":"code","7a4cdd01":"code","ffb392cb":"code","15ea81c3":"code","5d0ccfe5":"code","f087fa4d":"code","85c761ab":"code","6d8fef40":"code","4d21a318":"code","5b69eab4":"code","ef2d5cf0":"code","3c461e11":"code","54d8a3ae":"code","c6ae473d":"code","aef053fa":"code","476ad3e1":"code","23c0fb19":"markdown","78e9bf8e":"markdown","24b39cca":"markdown","d6ae8297":"markdown","52176353":"markdown","9f6a00b9":"markdown","374870b2":"markdown","539b27a3":"markdown","2cd814f5":"markdown","20681d24":"markdown","c3814c74":"markdown","6bd73e8f":"markdown","e300595d":"markdown","fcfd89d6":"markdown","9349721a":"markdown","19c6cab2":"markdown","73d97bc9":"markdown","4ae004fc":"markdown","1bfa9b8e":"markdown","3f594f4e":"markdown"},"source":{"cbcce53d":"import tensorflow as tf\nimport matplotlib.image as img\n%matplotlib inline\nimport numpy as np\nfrom collections import defaultdict\nimport collections\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nimport cv2","b3fa88dd":"# Check if GPU is enabled\nprint(tf.__version__)\nprint(tf.test.gpu_device_name())","9d4f6648":"%cd \/kaggle\/input\/food-101\/","74089d16":"# Helper function to download data and extract\ndef get_data_extract():\n  if \"food-101\" in os.listdir():\n    print(\"Dataset already exists\")\n  else:\n    print(\"Downloading the data...\")\n    !wget http:\/\/data.vision.ee.ethz.ch\/cvl\/food-101.tar.gz\n    print(\"Dataset downloaded!\")\n    print(\"Extracting data..\")\n    !tar xzvf food-101.tar.gz\n    print(\"Extraction done!\")","0f07c3f5":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\n#get_data_extract()","d0692d0d":"# Check the extracted dataset folder\n!ls food-101\/","f589746e":"os.listdir('food-101\/images')","a253eec0":"os.listdir('food-101\/meta')","d90ba381":"!head food-101\/meta\/train.txt","8344137f":"!head food-101\/meta\/classes.txt","ca9bc341":"# Visualize the data, showing one image per class from 101 classes\nrows = 17\ncols = 6\nfig, ax = plt.subplots(rows, cols, figsize=(25,25))\nfig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24) # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\ndata_dir = \"food-101\/images\/\"\nfoods_sorted = sorted(os.listdir(data_dir))\nfood_id = 0\nfor i in range(rows):\n  for j in range(cols):\n    try:\n      food_selected = foods_sorted[food_id] \n      food_id += 1\n    except:\n      break\n    if food_selected == '.DS_Store':\n        continue\n    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n    ax[i][j].imshow(img)\n    ax[i][j].set_title(food_selected, pad = 10)\n    \nplt.setp(ax, xticks=[],yticks=[])\nplt.tight_layout()\n# https:\/\/matplotlib.org\/users\/tight_layout_guide.html\n","6e95dc04":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src,dest):\n  classes_images = defaultdict(list)\n  with open(filepath, 'r') as txt:\n      paths = [read.strip() for read in txt.readlines()]\n      for p in paths:\n        food = p.split('\/')\n        classes_images[food[0]].append(food[1] + '.jpg')\n\n  for food in classes_images.keys():\n    print(\"\\nCopying images into \",food)\n    if not os.path.exists(os.path.join(dest,food)):\n      os.makedirs(os.path.join(dest,food))\n    for i in classes_images[food]:\n      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n  print(\"Copying Done!\")","117a5818":"# Prepare train dataset by copying images from food-101\/images to food-101\/train using the file train.txt\n%cd \/\nprint(\"Creating train data...\")\nprepare_data('\/kaggle\/input\/food-101\/food-101\/meta\/train.txt', '\/kaggle\/input\/food-101\/food-101\/images', 'train')","b0b507a7":"# Prepare test data by copying images from food-101\/images to food-101\/test using the file test.txt\nprint(\"Creating test data...\")\nprepare_data('\/kaggle\/input\/food-101\/food-101\/meta\/test.txt', '\/kaggle\/input\/food-101\/food-101\/images', 'test')","756713ab":"# Check how many files are in the train folder\nprint(\"Total number of samples in train folder\")\n!find train -type d -or -type f -printf '.' | wc -c","63772a0a":"# Check how many files are in the test folder\nprint(\"Total number of samples in test folder\")\n!find test -type d -or -type f -printf '.' | wc -c","43d4dd24":"# List of all 101 types of foods(sorted alphabetically)\ndel foods_sorted[0] # remove .DS_Store from the list","9124ddba":"foods_sorted","e196e180":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n  if os.path.exists(dest):\n    rmtree(dest) # removing dataset_mini(if it already exists) folders so that we will have only the classes that we want\n  os.makedirs(dest)\n  for food_item in food_list :\n    print(\"Copying images into\",food_item)\n    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n      ","a3897b1c":"# picking 51 food items and generating separate data folders for the same\nfood_list = ['macarons',\n 'french_toast',\n 'beef_carpaccio',\n 'hot_and_sour_soup',\n 'seaweed_salad',\n 'pulled_pork_sandwich',\n 'lobster_roll_sandwich',\n 'carrot_cake',\n 'red_velvet_cake',\n 'grilled_cheese_sandwich',\n 'spring_rolls',\n 'omelette',\n 'fried_calamari',\n 'caprese_salad',\n 'ramen',\n 'grilled_salmon',\n 'hamburger',\n 'miso_soup',\n 'bread_pudding',\n 'crab_cakes',\n 'cheesecake',\n 'cup_cakes',\n 'waffles',\n 'fish_and_chips',\n 'macaroni_and_cheese',\n 'chocolate_mousse',\n 'chicken_curry',\n 'caesar_salad',\n 'nachos',\n 'frozen_yogurt',\n 'ice_cream',\n 'club_sandwich',\n 'strawberry_shortcake',\n 'steak',\n 'garlic_bread',\n 'chicken_wings',\n 'greek_salad',\n 'chocolate_cake',\n 'samosa',\n 'sushi',\n 'beef_tartare',\n 'apple_pie',\n 'pizza',\n 'french_onion_soup',\n 'hot_dog',\n 'chicken_quesadilla',\n 'pancakes',\n 'fried_rice',\n 'cheese_plate',\n 'onion_rings',\n 'french_fries']\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'","eb6f80fd":"len(food_list)","cb6d0fe7":"print(\"Creating train data folder with new classes\")\ndataset_mini(food_list, src_train, dest_train)","60fdbbe6":"print(\"Total number of samples in train folder\")\n\n!find train_mini -type d -or -type f -printf '.' | wc -c","85368541":"print(\"Creating test data folder with new classes\")\ndataset_mini(food_list, src_test, dest_test)","4abd26d9":"print(\"Total number of samples in test folder\")\n!find test_mini -type d -or -type f -printf '.' | wc -c","101cb183":"K.clear_session()\nn_classes = 51\nimg_width, img_height = 299, 299\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\nnb_train_samples = 2250 #75750\nnb_validation_samples = 750 #25250\nbatch_size = 16\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n\ninception = InceptionResNetV2(weights='imagenet', include_top=False)\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(256,activation='relu')(x)\nx = Dense(128,activation='relu')(x)\nx = Dropout(0.2)(x)\n\npredictions = Dense(51,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\nmodel = Model(inputs=inception.input, outputs=predictions)\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpointer = ModelCheckpoint(filepath='best_model_51class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_51class.log')\n\nhistory = model.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\nmodel.save('model_trained_51class.hdf5')\n","14763ea3":"class_map_51 = train_generator.class_indices\nclass_map_51","b5204f08":"def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n","5c30d361":"plot_accuracy(history,'FOOD101-InceptionResNetV2')\nplot_loss(history,'FOOD101-InceptionResNetV2')","6bf724ee":"model_best = load_model('best_model_51class.hdf5',compile = False)","96ca6a5e":"!ls","fff7dcc7":"! cp best_model_51class.hdf5 \/kaggle\/working\/","63fff12e":"model2 = tf.keras.models.load_model('best_model_51class.hdf5')\n","a676792a":"checkpointer2 = ModelCheckpoint(filepath='2best_model_51class.hdf5', verbose=1, save_best_only=True)\ncsv_logger2 = CSVLogger('2history_51class.log')\n\nhistory2 = model2.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger2, checkpointer2])\n\nmodel2.save('2model_trained_51class.hdf5')","220d9d0a":"plot_accuracy(history2,'2FOOD101-InceptionResNetV2')\nplot_loss(history2,'2FOOD101-InceptionResNetV2')","7a4cdd01":"!ls","ffb392cb":"! cp 2best_model_51class.hdf5 \/kaggle\/working\/","15ea81c3":"checkpointer3 = ModelCheckpoint(filepath='3best_model_51class.hdf5', verbose=1, save_best_only=True)\ncsv_logger3 = CSVLogger('3history_51class.log')\n\nhistory3 = model2.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=50,\n                    verbose=1,\n                    callbacks=[csv_logger3, checkpointer3])\n\nmodel2.save('3model_trained_51class.hdf5')","5d0ccfe5":"plot_accuracy(history3,'3FOOD101-InceptionResNetV2')\nplot_loss(history3,'3FOOD101-InceptionResNetV2')","f087fa4d":"!ls","85c761ab":"! cp 3best_model_51class.hdf5 \/kaggle\/working\/","6d8fef40":"checkpointer4 = ModelCheckpoint(filepath='4best_model_51class.hdf5', verbose=1, save_best_only=True)\ncsv_logger4 = CSVLogger('4history_51class.log')\n\n\nhistory4 = model2.fit_generator(train_generator,\n                    steps_per_epoch = nb_train_samples \/\/ batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples \/\/ batch_size,\n                    epochs=50,\n                    verbose=1,\n                    callbacks=[csv_logger4, checkpointer4])\n\nmodel3.save('4model_trained_51class.hdf5')","4d21a318":"plot_accuracy(history4,'4FOOD101-InceptionResNetV2')\nplot_loss(history4,'4FOOD101-InceptionResNetV2')","5b69eab4":"! cp 4best_model_51class.hdf5 \/kaggle\/working\/","ef2d5cf0":"!ls","3c461e11":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()\nmodel_best = load_model('4best_model_51class.hdf5',compile = False)","54d8a3ae":"def predict_class(model, images, show = True):\n  for img in images:\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img \/= 255.                                      \n\n    pred = model.predict(img)\n    index = np.argmax(pred)\n    food_list.sort()\n    pred_value = food_list[index]\n    if show:\n        plt.imshow(img[0])                           \n        plt.axis('off')\n        plt.title(pred_value)\n        plt.show()","c6ae473d":"# Downloading images from internet using the URLs\n!wget -O samosa.jpg http:\/\/veggiefoodrecipes.com\/wp-content\/uploads\/2016\/05\/lentil-samosa-recipe-01.jpg\n!wget -O applepie.jpg https:\/\/acleanbake.com\/wp-content\/uploads\/2017\/10\/Paleo-Apple-Pie-with-Crumb-Topping-gluten-free-grain-free-dairy-free-15.jpg\n!wget -O pizza.jpg https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a3\/Eq_it-na_pizza-margherita_sep2005_sml.jpg\/800px-Eq_it-na_pizza-margherita_sep2005_sml.jpg\n# If you have an image in your local computer and want to try it, uncomment the below code to upload the image files\n\n# from google.colab import files\n# image = files.upload()","aef053fa":"# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('applepie.jpg')\nimages.append('pizza.jpg')\npredict_class(model_best, images, True)","476ad3e1":"model_best.summary()","23c0fb19":"### **Predicting classes for new images from internet using the best trained model**","78e9bf8e":"### **Fine tune Inception Pretrained model using Food 101 dataset**","24b39cca":"# **Multiclass Classification using Keras and TensorFlow on Food-101 Dataset**\n![alt text](https:\/\/www.vision.ee.ethz.ch\/datasets_extra\/food-101\/static\/img\/food-101.jpg)","d6ae8297":"### **Split the image data into train and test using train.txt and test.txt**","52176353":"### **Visualize random image from each of the 101 classes**","9f6a00b9":"* **Setting compile=False and clearing the session leads to faster loading of the saved model**\n* **Withouth the above addiitons, model loading was taking more than a minute!**","374870b2":"### **Overview** \n* **Download and extract Food 101 dataset**\n* **Understand dataset structure and files** \n* **Visualize random image from each of the 101 classes**\n* **Split the image data into train and test using train.txt and test.txt**\n* **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**\n* **Fine tune Inception Pretrained model using Food 101 dataset**\n* **Visualize accuracy and loss plots**\n* **Predicting classes for new images from internet**\n* **Scale up and fine tune Inceptionv3 model with 11 classes of data**\n* **Model Explainability**\n* **Summary of the things I tried**\n* **Further improvements**\n* **Feedback**","539b27a3":"* **Commented the below cell as the Food-101 dataset is available from Kaggle Datasets and need not be downloaded..**","2cd814f5":"**The dataset being used is [Food 101](https:\/\/www.vision.ee.ethz.ch\/datasets_extra\/food-101\/)**\n* **This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)**\n* **Each type of food has 750 training samples and 250 test samples**\n* **Note found on the webpage of the dataset :  **  \n***On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels.***  \n* **The entire dataset is 5GB in size**","20681d24":"### **Understand dataset structure and files**","c3814c74":"* We now have train and test data ready  \n* But to experiment and try different architectures, working on the whole data with 101 classes takes a lot of time and computation  \n* To proceed with further experiments, I am creating train_min and test_mini, limiting the dataset to 3 classes  \n* Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification, choosing 3 classes is a good start instead of 2","6bd73e8f":"### **Visualize the accuracy and loss plots**","e300595d":"* Keras and other Deep Learning libraries provide pretrained models  \n* These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n* Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n* This helps in faster convergance and saves time and computation when compared to models trained from scratch","fcfd89d6":"**meta** folder contains the text files - train.txt and test.txt  \n**train.txt** contains the list of images that belong to training set  \n**test.txt** contains the list of images that belong to test set  \n**classes.txt** contains the list of all classes of food","9349721a":"* **Summary of the model gives us the list of all the layers in the network along with other useful details**","19c6cab2":"**images** folder contains 101 folders with 1000 images  each  \nEach folder contains images of a specific food class","73d97bc9":"* We currently have a subset of dataset with 51 classes \n* Use the below code to finetune InceptionResNetV2 pretrained model","4ae004fc":"### **Create a subset of data with few classes(3) - train_mini and test_mini for experimenting**","1bfa9b8e":"* **The plots show that the accuracy of the model increased with epochs and the loss has decreased**\n* **Validation accuracy has been on the higher side than training accuracy for many epochs**\n* **This could be for several reasons:**\n  * We used a pretrained model trained on ImageNet which contains data from a variety of classes\n  * Using dropout can lead to a higher validation accuracy\n\n \n","3f594f4e":"### **Download and extract Food 101 Dataset**"}}