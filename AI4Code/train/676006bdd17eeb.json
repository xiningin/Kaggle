{"cell_type":{"ebdeef01":"code","d3143a68":"code","10921411":"code","e0baf862":"code","0227c640":"code","4f099851":"code","1406ee07":"code","aec125c0":"code","3e298021":"code","eed80f5e":"code","cc720a24":"code","7f3609c7":"code","8ecdf06b":"code","1d517ad1":"code","fe9e4b66":"code","086d322e":"code","1934bd46":"code","d4ad2be7":"code","ef50042c":"code","37df8db7":"code","153505bf":"code","2a45fd3a":"code","52718afb":"code","d163b1aa":"code","f6c6adcd":"code","b678d39e":"code","d3f56b5d":"code","e75e90cf":"code","a4e5d59f":"code","13e34c35":"code","fe03dbf9":"code","812cc2f1":"code","91b33172":"code","e2051b54":"code","b2f0de74":"code","7df39a4a":"code","ddfc41fc":"code","a3adf499":"code","d3f191bd":"code","94565103":"code","7770d9a2":"code","3fe33fca":"code","e842a1f3":"code","c950d411":"code","03c0b05f":"code","9ec51540":"code","7f5e43f7":"code","e99b8932":"code","0d21031d":"code","80962079":"code","49092bb0":"code","7c185fac":"code","c24ec7df":"code","a4220d82":"code","df6ec975":"code","f61c0f2e":"code","a479954c":"code","be63cbf9":"code","6706d281":"markdown","6e29e81d":"markdown","24fe3f0b":"markdown","e22811fe":"markdown","726a5398":"markdown","189a23ee":"markdown","2dc3be59":"markdown","487fda72":"markdown","f20742f9":"markdown","0f70616f":"markdown","f8f0d1e7":"markdown","7ae82a65":"markdown","3eb854d6":"markdown","6a8d1445":"markdown","0945df9b":"markdown","815ddb34":"markdown","edfadf50":"markdown","9f7c5b59":"markdown","ce8b6342":"markdown","9373e206":"markdown","c8be4480":"markdown","d88e91a8":"markdown"},"source":{"ebdeef01":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d3143a68":"directory = \"\/kaggle\/input\/airline-passenger-satisfaction\/\"\nfeature_tables = ['train.csv', 'test.csv']\n\ndf_train = directory + feature_tables[0]\ndf_test = directory + feature_tables[1]\n\n# Create dataframes\nprint(f'Reading csv from {df_train}...')\ntrain = pd.read_csv(df_train)\nprint('...Complete')\n\nprint(f'Reading csv from {df_train}...')\ntest = pd.read_csv(df_test)\nprint('...Complete')\n\ndf = pd.concat([train, test])","10921411":"train.head()","e0baf862":"print(train.info())","0227c640":"print(train.dtypes)","4f099851":"print(train.describe(include = 'all'))","1406ee07":"print(train.shape)","aec125c0":"print(train.isnull().sum())","3e298021":"def transform_gender(x):\n    if x == 'Female':\n        return 1\n    elif x == 'Male':\n        return 0\n    else:\n        return -1\n    \ndef transform_customer_type(x):\n    if x == 'Loyal Customer':\n        return 1\n    elif x == 'disloyal Customer':\n        return 0\n    else:\n        return -1\n    \ndef transform_travel_type(x):\n    if x == 'Business travel':\n        return 1\n    elif x == 'Personal Travel':\n        return 0\n    else:\n        return -1\n    \ndef transform_class(x):\n    if x == 'Business':\n        return 2\n    elif x == 'Eco Plus':\n        return 1\n    elif x == 'Eco':\n        return 0    \n    else:\n        return -1\n    \ndef transform_satisfaction(x):\n    if x == 'satisfied':\n        return 1\n    elif x == 'neutral or dissatisfied':\n        return 0\n    else:\n        return -1\n    \ndef process_data(df):\n    df = df.drop(['Unnamed: 0', 'id'], axis = 1)\n    df['Gender'] = df['Gender'].apply(transform_gender)\n    df['Customer Type'] = df['Customer Type'].apply(transform_customer_type)\n    df['Type of Travel'] = df['Type of Travel'].apply(transform_travel_type)\n    df['Class'] = df['Class'].apply(transform_class)\n    df['satisfaction'] = df['satisfaction'].apply(transform_satisfaction)\n    df['Arrival Delay in Minutes'].fillna(df['Arrival Delay in Minutes'].median(), inplace = True)\n    \n    return df\n\ntrain = process_data(train)\ntest = process_data(test)","eed80f5e":"train.head()","cc720a24":"print(train.isnull().sum())","7f3609c7":"train.describe()","8ecdf06b":"labelencoder = LabelEncoder()\ntrain['satisfaction'] = labelencoder.fit_transform(train['satisfaction'])\ntest['satisfaction'] = labelencoder.fit_transform(test['satisfaction'])","1d517ad1":"print('How many people satisfacted by Airline:')\ntrain.groupby('Gender')[['satisfaction']].sum()","fe9e4b66":"print('How many people participated in the study:')\ntrain.groupby('Gender')[['satisfaction']].count()","086d322e":"print('Percentage of satisfacted people :')\ntrain.groupby('Gender')[['satisfaction']].sum()\/ train.groupby('Gender')[['satisfaction']].count()","1934bd46":"pd.crosstab(train['satisfaction'], train['Age'], dropna=True, normalize='columns')\n","d4ad2be7":"k = pd.crosstab(train['Age'],train['satisfaction'], dropna=True, normalize='columns')\nk.plot.bar(stacked=False, figsize=(25, 5))\nprint('Age-satisfaction')\nplt.show()","ef50042c":"print ('Satisfaction of people depending on the class:')\ntrain.groupby('Class')[['satisfaction']].sum()","37df8db7":"print ('How many people participated in the study:')\ntrain.groupby('Class')[['satisfaction']].count()","153505bf":"print ('Percentage of satisfacted people depending from the class:')\ntrain.groupby('Class')[['satisfaction']].sum()\/ train.groupby('Class')[['satisfaction']].count()","2a45fd3a":"corr = train.corr(method='spearman')\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nf, ax = plt.subplots(figsize=(25, 25))\n\n\nsns.heatmap(corr, annot = True, mask=mask, cmap=\"YlGnBu\", center=0,\n            square=True, linewidths=.5)","52718afb":"for a in corr.columns:\n    for b in corr.index:\n        if (a != b) and (abs(corr[a][b]) >= 0.7):\n            print(a,b,'-->',corr[a][b])","d163b1aa":"features = ['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',\n       'Flight Distance', 'Inflight wifi service',\n       'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\ntarget = 'satisfaction'\nX =  train [features]\ny = train [target]\n\nX_train = train[features]\ny_train = train[target].to_numpy()\nX_test = test[features]\ny_test = test[target].to_numpy()","f6c6adcd":"# data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n# fit scaler on training data\nnorm = MinMaxScaler().fit(X_train)\n# transform training data\nX_train_norm = norm.transform(X_train)\n# transform testing dataabs\nX_test_norm = norm.transform(X_test)","b678d39e":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=0)\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_test)\nprint(y_pred)","d3f56b5d":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","e75e90cf":"# Create a matrix 2x2 with a code:\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\nsns.heatmap(pd.DataFrame(confusion_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","a4e5d59f":"# evaluate the model using model evaluation metrics \n# such as accuracy, precision, and recall\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","13e34c35":"# ROC Curve:\n# Receiver Operating Characteristic(ROC) curve is a plot of the true \n# positive rate against the false positive rate. It shows the tradeoff \n# between sensitivity and specificity.\ny_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","fe03dbf9":"# ROC Curve: 0.849\n# AUC score for the case is 0.849. AUC score 1 represents perfect classifier, \n# and 0.5 represents a worthless classifier.","812cc2f1":"y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","91b33172":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","e2051b54":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","b2f0de74":"# intercept of the model\nprint('Intercept:', logreg.intercept_)","7df39a4a":"import lightgbm as lgb\nclf = lgb.LGBMClassifier()\nclf.fit(X_train , y_train)","ddfc41fc":"from sklearn.metrics import accuracy_score\naccuracy_score(clf.predict(X_train) , y_train)","a3adf499":"accuracy_score(clf.predict(X_test) , y_test)","d3f191bd":"import shap","94565103":"explainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X_train)\nshap.summary_plot(shap_values, X_train)","7770d9a2":"from sklearn import preprocessing\nr_scaler = preprocessing.MinMaxScaler()\nr_scaler.fit(train)\n#modified_data = pd.DataFrame(r_scaler.transform(train), index=train['id'], columns=train.columns)\nmodified_data = pd.DataFrame(r_scaler.transform(train), columns=train.columns)\nmodified_data.head()","3fe33fca":"from sklearn.feature_selection import SelectKBest, chi2\nX = modified_data.loc[:,modified_data.columns!='satisfaction']\ny = modified_data[['satisfaction']]\nselector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)])","e842a1f3":"features = ['Type of Travel', 'Class', 'Flight Distance', 'Inflight wifi service',\n       'Online boarding', 'Seat comfort', 'Inflight entertainment',\n       'On-board service', 'Leg room service', 'Cleanliness']\ntarget = 'satisfaction'\nX =  train [features]\ny = train [target]\n\n\n# Split into test and train\nX_train = train[features]\ny_train = train[target].to_numpy()\nX_test = test[features]\ny_test = test[target].to_numpy()\n\n# Normalize Features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","c950d411":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\n\n# fit the model with data\nlogreg.fit(X_train,y_train)\ny_pred=logreg.predict(X_test)\nprint(y_pred)\n","03c0b05f":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","9ec51540":"import time\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report, plot_confusion_matrix, plot_roc_curve\nfrom matplotlib import pyplot as plt \ndef run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    t0=time.time()\n    if verbose == False:\n        model.fit(X_train,y_train.ravel(), verbose=0)\n    else:\n        model.fit(X_train,y_train.ravel())\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    roc_auc = roc_auc_score(y_test, y_pred) \n    time_taken = time.time()-t0\n    print(\"Accuracy = {}\".format(accuracy))\n    print(\"ROC Area under Curve = {}\".format(roc_auc))\n    print(\"Time taken = {}\".format(time_taken))\n    print(classification_report(y_test,y_pred,digits=5))\n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.pink, normalize = 'all')\n    plot_roc_curve(model, X_test, y_test)                     \n    \n    return model, accuracy, roc_auc, time_taken","7f5e43f7":"from sklearn.linear_model import LogisticRegression\n\nparams_lr = {'penalty': 'elasticnet', 'l1_ratio':0.5, 'solver': 'saga'}\n\nmodel_lr = LogisticRegression(**params_lr)\nmodel_lr, accuracy_lr, roc_auc_lr, tt_lr = run_model(model_lr, X_train, y_train, X_test, y_test)","e99b8932":"from sklearn.linear_model import Lasso\nfrom sklearn import linear_model\nfeatures = ['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',\n       'Flight Distance', 'Inflight wifi service',\n       'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\ntarget = 'satisfaction'\nX =  train [features]\ny = train [target]\n\n\n\n# Split into test and train\nX_train = train[features]\ny_train = train[target]\nX_test = test[features]\ny_test = test[target]\nX_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=42)\nlasso = Lasso()\nlasso.fit(X_train,y_train)\ntrain_score=lasso.score(X_train,y_train)\ntest_score=lasso.score(X_test,y_test)\ncoeff_used = np.sum(lasso.coef_!=0)\n\nprint (\"Training score:\", train_score) \nprint (\"Test score:\", test_score)\nprint (\"Number of features used:\", coeff_used)","0d21031d":"lasso001 = Lasso(alpha=0.01, max_iter=10e5)\nlasso001.fit(X_train,y_train)\ntrain_score001=lasso001.score(X_train,y_train)\ntest_score001=lasso001.score(X_test,y_test)\ncoeff_used001 = np.sum(lasso001.coef_!=0)\n\nprint (\"Training score for alpha = 0.01:\", train_score001)\nprint (\"Test score for alpha = 0.01:\", test_score001)\nprint (\"Number of features used: for alpha = 0.01:\", coeff_used001)","80962079":"lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\nlasso00001.fit(X_train,y_train)\ntrain_score00001=lasso00001.score(X_train,y_train)\ntest_score00001=lasso00001.score(X_test,y_test)\ncoeff_used00001 = np.sum(lasso00001.coef_!=0)\n\nprint (\"Training score for alpha = 0.0001:\", train_score00001)\nprint (\"Test score for alpha = 0.0001:\", test_score00001)\nprint (\"Number of features used: for alpha = 0.0001:\", coeff_used00001)","49092bb0":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\nlogreg_train_score=logreg.score(X_train,y_train)\nlogreg_test_score=logreg.score(X_test,y_test)\nprint (\"Logistic Regression training score:\", logreg_train_score)\nprint (\"Logistic Regression  test score:\", logreg_test_score)\n","7c185fac":"plt.subplot(1,2,1)\nplt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=6,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7) # alpha here is for transparency\nplt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=7,color='blue',label=r'Lasso; $\\alpha = 0.01$')\n\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.subplot(1,2,2)\nplt.plot(lasso.coef_,alpha=0.7,linestyle='none',marker='*',markersize=6,color='red',label=r'Lasso; $\\alpha = 1$',zorder=7)\nplt.plot(lasso001.coef_,alpha=0.5,linestyle='none',marker='d',markersize=7,color='blue',label=r'Lasso; $\\alpha = 0.01$')\nplt.plot(lasso00001.coef_,alpha=0.8,linestyle='none',marker='v',markersize=7,color='black',label=r'Lasso; $\\alpha = 0.00001$')\nplt.plot(logreg.coef_,alpha=0.7,linestyle='none',marker='o',markersize=6,color='green',label='Linear Regression',zorder=2)\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.tight_layout()\nplt.show()","c24ec7df":"from sklearn.linear_model import Lasso\nfrom sklearn import linear_model\nfeatures = ['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',\n       'Flight Distance', 'Inflight wifi service',\n       'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\ntarget = 'satisfaction'\nX =  train [features]\ny = train [target]\n\n# Split into test and train\nX_train = train[features]\ny_train = train[target]\nX_test = test[features]\ny_test = test[target]\n\n\nX_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2, random_state=42)\nlassomodel = Lasso(alpha=0.01)\nlassomodel.fit(X_train,y_train)\n\nprint('Intercept:', lassomodel.intercept_)\nprint(\"R-squared:\", lassomodel.score(X_train,y_train))\n\n#predictions = lassomodel.predict(X)\n#print(predictions)\n\ndataframe = pd.DataFrame({\"Feature\":X.columns.tolist(),\"Coefficients\":lassomodel.coef_})\ndataframe.sort_values(\"Coefficients\", ascending = False)","a4220d82":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","df6ec975":"from sklearn.tree import DecisionTreeRegressor\nDecTree=DecisionTreeRegressor(min_samples_leaf=.0001)\nDecTree.fit(X_train,y_train)\n\nprint('R-squared score - training: {:.3f}'.format(DecTree.score(X_train,y_train)))\nprint('R-squared score - test: {:.3f}'.format(DecTree.score(X_test, y_test)))","f61c0f2e":"decision_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 10, random_state=15)\ndecision_tree.fit(X_train, y_train)","a479954c":"from IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw\nimport graphviz  \nfrom sklearn.tree import export_graphviz\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = export_graphviz(decision_tree, out_file=f, max_depth = 5,\n                         impurity = True, feature_names = X_train.columns,\n                         rounded = True, filled= True )\n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree.png'])\n# Annotating chart with PIL\nimg = Image.open(\"tree.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")","be63cbf9":"decision_tree.score(X_test, y_test)","6706d281":"*Correlation*","6e29e81d":"Choosing 10 top features","24fe3f0b":"1. Import libraries","e22811fe":"These are top 10 features impacting on passenger satisfaction. ","726a5398":"Get summary of data","189a23ee":"Does satisfaction depend on gender?\n\nGender\n1 - Female\n0 - Male","2dc3be59":"Clean and Transform Dataset\n\n","487fda72":"Data type of each column","f20742f9":"Notes about the Dataset\n\nThis dataset has already been split into train and test csv files. 80% of the total dataset is in train.csv and 20% is in test.csv","0f70616f":"2. Import data","f8f0d1e7":"So, people`s satisfaction does not depend on gender.","7ae82a65":"Get number of NaN values","3eb854d6":"**Age**","6a8d1445":"*Desicion Tree*","0945df9b":"3. Information about dataset\n\nDepict the dataset","815ddb34":"*Logistic Regression*","edfadf50":"Does satisfaction depend on the Class?\n\nClass\n2 - Bisiness\n1 - Eco\n0 - EcoPlus","9f7c5b59":"Lasso Regression","ce8b6342":"Building Model","9373e206":"The result is telling us that we have 11288+9215 correct predictions and 2188+3285 incorrect predictions.","c8be4480":"So, satisfaction depends on class: Business class passengers have the highest percantage of satisfaction.","d88e91a8":"Get description of data - statistical summary\n\nShape of the dataframe"}}