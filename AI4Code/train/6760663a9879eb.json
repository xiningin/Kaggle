{"cell_type":{"156ad59e":"code","b2c312e2":"code","f310bd5c":"code","870ea660":"code","49fe89e4":"code","0dbf2d34":"code","3eb163e6":"code","a1efbc6c":"code","96fc864f":"code","3de14784":"code","19467415":"code","d14d51ae":"code","fd0460d2":"code","22160113":"code","94efe4ea":"code","3f4e667a":"code","f995a61d":"code","c173a0e1":"code","2ba737b2":"code","fee6da6a":"code","304ca9e9":"code","8bf78722":"code","148f4455":"code","ebd92d60":"code","257dec5b":"code","cdd42d2a":"code","baa0c32e":"code","6bedcccd":"code","644c5679":"code","1ad788e2":"code","cdf5ef31":"code","d76277f8":"code","65a38022":"code","6da2042a":"code","58fb8f65":"code","b7e5bed9":"markdown","40d839c7":"markdown","fc381d45":"markdown","e4763a81":"markdown","5d9c6068":"markdown","b317af15":"markdown","2b536053":"markdown","5523c1dc":"markdown","01d42add":"markdown","af080027":"markdown","a01d5047":"markdown","57261d21":"markdown","a1e3e73d":"markdown","b1c8340c":"markdown","79223a39":"markdown","2c161461":"markdown","b5b4d48f":"markdown","0051cc8c":"markdown","39efbcfa":"markdown","7075a247":"markdown","0cb1a843":"markdown","d5583274":"markdown","3f505a3e":"markdown","cd642cb5":"markdown","57fd5e69":"markdown","d7eb7f18":"markdown"},"source":{"156ad59e":"!pip install etna==1.5.0 --ignore-installed -q 2> \/dev\/null","b2c312e2":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f310bd5c":"TRAIN_PATH = \"..\/input\/tabular-playground-series-jan-2022\/train.csv\"\nTEST_PATH = \"..\/input\/tabular-playground-series-jan-2022\/test.csv\"\nGDP_PATH = \"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\"\nHORIZON = 365","870ea660":"from etna.datasets import TSDataset","49fe89e4":"def load_data():\n    def load(path):\n        data = pd.read_csv(path)\n        data = data.drop(columns=[\"row_id\"])\n        data = data.rename(columns={\"date\":\"timestamp\", \"num_sold\":\"target\"})\n        data[\"segment\"] = data[\"country\"] + \"_\" + data[\"store\"] + \"_\" + data[\"product\"]\n        data[\"timestamp\"] = pd.to_datetime(data[\"timestamp\"])\n        return data\n    train = load(TRAIN_PATH)\n    test = load(TEST_PATH)\n    return train, test","0dbf2d34":"train, test = load_data()\ntrain.head()","3eb163e6":"def transform_target_gdp(df, inverse=False):\n    # see https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model for an explanation\n    gdp_exponent = 1.2121103201489674 \n    gdp_df = pd.read_csv(GDP_PATH, index_col='year')\n    def get_gdp(row):\n        \"\"\"Return the GDP based on row.country and row.timestamp.year\"\"\"\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.timestamp.year, country] ** gdp_exponent\n    if inverse:\n        df[\"target\"] *= df.apply(get_gdp, axis=1)\n    else:\n        df[\"target\"] \/= df.apply(get_gdp, axis=1)\n    return df\n\ndef prepare_target():\n    df = train.copy()\n    df = transform_target_gdp(df)\n    df = df[[\"timestamp\",\"segment\",\"target\"]]\n    df = TSDataset.to_dataset(df)\n    return df","a1efbc6c":"df = prepare_target()\ndf.head()","96fc864f":"def engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    df[\"dayofyear\"] = df.timestamp.dt.dayofyear\n    leap_years_fix_timestamps = (df.timestamp.dt.year != 2016) & (df.timestamp.dt.month >=3) \n    df.loc[leap_years_fix_timestamps, 'dayofyear'] += 1 \n    \n    # Easter\n    import dateutil.easter as easter\n\n    easter_timestamp = df.timestamp.apply(\n        lambda timestamp: pd.Timestamp(easter.easter(timestamp.year))\n    )\n    df['days_from_easter'] = (df.timestamp - easter_timestamp).dt.days.clip(-3, 59)\n    df.loc[df['days_from_easter'].isin(range(12, 39)), 'days_from_easter'] = 12 \n    \n    # Last Wednesday of June\n    wed_june_timestamp = df.timestamp.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    df['days_from_wed_jun'] = (df.timestamp - wed_june_timestamp).dt.days.clip(-5, 5)\n    \n    # First Sunday of November (second Sunday is Father's Day)\n    sun_nov_timestamp = df.timestamp.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    df['days_from_sun_nov'] = (df.timestamp - sun_nov_timestamp).dt.days.clip(-1, 9)\n    \n    return df\n\ndef prepare_exog():\n    df_exog = pd.concat([train, test]).drop(columns=[\"target\"])\n    df_exog = engineer(df_exog)\n    \n    categorical_features = [\"country\", \"product\", \"store\"]\n    df_exog[categorical_features] = df_exog[categorical_features].astype(\"category\")\n    df_exog = df_exog.add_prefix(\"regressor_\")\n    df_exog = df_exog.rename(columns={\"regressor_segment\":\"segment\",\n                                      \"regressor_timestamp\":\"timestamp\"})\n    \n    df_exog = TSDataset.to_dataset(df_exog)\n    return df_exog","3de14784":"df_exog = prepare_exog()\ndf_exog.head()","19467415":"def create_dataset():\n    train, test = load_data()\n    df = prepare_target()\n    df_exog = prepare_exog()\n    ts = TSDataset(df=df, freq=\"D\", df_exog=df_exog)\n    return ts","d14d51ae":"ts = create_dataset()\nts.head()","fd0460d2":"ts.plot()","22160113":"from etna.models import CatBoostModelMultiSegment\n\nmodel = CatBoostModelMultiSegment()","94efe4ea":"from etna.transforms import LogTransform, StandardScalerTransform # math\nfrom etna.transforms import DateFlagsTransform, HolidayTransform # datetime\nfrom etna.transforms import LagTransform # lags\n\ntransforms = [LogTransform(in_column=\"target\"),\n              StandardScalerTransform(in_column=\"target\", mode=\"per-segment\"),\n              DateFlagsTransform(day_number_in_week=False, day_number_in_month=False,\n                                 is_weekend=True, special_days_in_week=[4],\n                                 out_column=\"regressor_date_flag\"),\n              HolidayTransform(iso_code=\"SWE\", out_column=\"regressor_SWE_holidays\"),\n              HolidayTransform(iso_code=\"NOR\", out_column=\"regressor_NOR_holidays\"),\n              HolidayTransform(iso_code=\"FIN\", out_column=\"regressor_FIN_holidays\"),\n              LagTransform(in_column=\"regressor_SWE_holidays\", lags=list(range(5,6)), out_column=\"regressor_SWE_holidays_lag\"),\n              LagTransform(in_column=\"regressor_NOR_holidays\", lags=list(range(2,6)), out_column=\"regressor_NOR_holidays_lag\"),\n              LagTransform(in_column=\"regressor_FIN_holidays\", lags=list(range(2,6)), out_column=\"regressor_FIN_holidays_lag\"),]","3f4e667a":"from etna.pipeline import Pipeline\n\npipeline = Pipeline(model=model, transforms=transforms, horizon=HORIZON)","f995a61d":"from etna.metrics import SMAPE\n\nmetrics, forecasts, _ = pipeline.backtest(ts, metrics=[SMAPE()], aggregate_metrics=True, n_folds=3)","c173a0e1":"from etna.analysis import plot_backtest","2ba737b2":"print(\"SMAPE(mean): \",metrics.mean()[\"SMAPE\"])","fee6da6a":"plot_backtest(forecasts, ts, history_len=HORIZON)","304ca9e9":"def plot_feature_importance(importance, names, top_k=None):\n    if top_k is None:\n        top_k = len(names)\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n    \n    fi_df = pd.DataFrame({'feature_names' : feature_names,\n                          'feature_importance' : feature_importance})\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    plt.figure(figsize=(10,8))\n    sns.barplot(x=fi_df['feature_importance'][:top_k], y=fi_df['feature_names'][:top_k])\n    plt.title('FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","8bf78722":"ts = create_dataset()\npipeline.fit(ts)\nmodel = pipeline.model._base_model.model\nplot_feature_importance(model.get_feature_importance(), model.feature_names_)","148f4455":"from etna.ensembles import VotingEnsemble","ebd92d60":"params = [\n{\n 'depth': 5,\n 'iterations': 1147,\n 'l2_leaf_reg': 0.779126455221549,\n 'random_strength': 8.637529894850367,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.8127462916337822,\n},\n{\n 'depth': 5,\n 'iterations': 1147,\n 'l2_leaf_reg': 0.779126455221549,\n 'random_strength': 8.637529894850367,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.8127462916337822,\n},\n{\n 'depth': 6,\n 'iterations': 1174,\n 'l2_leaf_reg': 0.0443925684503373,\n 'random_strength': 8.864563894739499,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.7263194054569034\n},\n{\n 'depth': 6,\n 'iterations': 1174,\n 'l2_leaf_reg': 0.0443925684503373,\n 'random_strength': 8.864563894739499,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.7263194054569034\n},\n{\n 'depth': 5,\n 'iterations': 1170,\n 'l2_leaf_reg': 0.15106766436681104,\n 'random_strength': 8.857637446656524,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.53404545695121\n},\n{\n 'depth': 5,\n 'iterations': 1193,\n 'l2_leaf_reg': 0.06992597880991089,\n 'random_strength': 8.889861067995088,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.6960270472983301\n},\n{\n 'depth': 5,\n 'iterations': 1170,\n 'l2_leaf_reg': 0.15106766436681104,\n 'random_strength': 8.857637446656524,\n 'one_hot_max_size': 5,\n 'bagging_temperature': 0.53404545695121\n}]","257dec5b":"seeds = [None, 13, 121, 11041999, 3141, 235813, 1501]","cdd42d2a":"pipelines = [Pipeline(model=CatBoostModelMultiSegment(random_seed=seeds[i], **params[i]),\n                      transforms=transforms,\n                      horizon=HORIZON) \n             for i in range(len(seeds))]\nensemble = VotingEnsemble(pipelines=pipelines, n_jobs=5)","baa0c32e":"metrics, forecasts, _ = ensemble.backtest(ts, metrics=[SMAPE()], aggregate_metrics=True, n_folds=3)","6bedcccd":"print(\"SMAPE(mean): \",metrics.mean()[\"SMAPE\"])","644c5679":"from etna.analysis import plot_forecast","1ad788e2":"ts = create_dataset()\nensemble.fit(ts)\nfuture = ensemble.forecast()","cdf5ef31":"plot_forecast(forecast_ts=future, train_ts=ts)","d76277f8":"def prepare_submission():\n    test = pd.read_csv(TEST_PATH)\n    test[\"date\"] = pd.to_datetime(test[\"date\"])\n    test = test.rename(columns={\"date\":\"timestamp\"})\n    \n    df = TSDataset.to_flatten(future.df)\n    df[\"country\"] = df[\"segment\"].str.split(\"_\").apply(lambda x: x[0])\n    df[\"store\"] = df[\"segment\"].str.split(\"_\").apply(lambda x: x[1])\n    df[\"product\"] = df[\"segment\"].str.split(\"_\").apply(lambda x: x[2])\n    \n    df = transform_target_gdp(df, inverse=True)\n    df = pd.merge(df, test, on=[\"timestamp\",\"country\",\"store\",\"product\"])\n    df = df.rename(columns = {\"target\":\"num_sold\"})\n    df = df.sort_values(by=[\"row_id\"])\n    df = df[[\"row_id\",\"num_sold\"]]\n    return df","65a38022":"submission = prepare_submission()","6da2042a":"submission.head()","58fb8f65":"submission.to_csv(\"submission.csv\", index=False)","b7e5bed9":"Random seeds from my head:","40d839c7":"## Transforms\n\nTransforms define the preprocessing and feature engineering steps. They are applied one by one. Don't forget about the prefix for the regressors!","fc381d45":"# Forecasting\n\nNow, let's build the final solution. We will use the ensemble of Catboost models with different random seeds, to make the forecast more robust.","e4763a81":"Let's plot the backtest results and look at the metric","5d9c6068":"## Forecasts","b317af15":"# Solution\n\n","2b536053":"## TSDataset\n\nFinally, we can create TSDataset!","5523c1dc":"## Competition data\n\nFirst of all, we need to:\n1. Load dataset\n2. Rename columns to fit the ETNA format:\n    * `timestamp` - column with time variable \n    * `segment`- column with indicator of individual time series within dataset\n    * `target` - column with target variable","01d42add":"# Dataset\n\nWorking with the ETNA library requires the usage of **TSDataset** - the special structure that holds many time series. Before creating the dataset, we need to prepare the raw data.","af080027":"And the ensemble is ready!","a01d5047":"## Pipeline\nThis is the main framework to evaluate the model and get forecasts.","57261d21":"## Target\n\nLet's use [dataset](https:\/\/www.kaggle.com\/carlmcbrideellis\/gdp-20152019-finland-norway-and-sweden) with yearly **Gross Domestic Product(GDP)** values to transform the target.","a1e3e73d":"## Ensemble","b1c8340c":"# Tabular Playground Series - Jan 2022 with ETNA \ud83c\udf0b","79223a39":"## Feature engineering\nNow, we need to prepare the exogenous data and do some simple feature engineering ","2c161461":"### Feature importance\n\nIt is also easy to get the feature importance values from the underlying model. Let's take a look","b5b4d48f":"Hyperparameters from optuna:","0051cc8c":"# Submission","39efbcfa":"Let's take a look at the forecast to check that everything is fine","7075a247":"Let's take a look at the time series in the dataset","0cb1a843":"\nIn this notebook we will make predictions for [Tabular Playground Series - Jan 2022](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022) with [etna time series library](https:\/\/github.com\/tinkoff-ai\/etna\/).","d5583274":"<a href=\"https:\/\/github.com\/tinkoff-ai\/etna\">\n    <img src=\"https:\/\/img.shields.io\/badge\/GitHub-100000?style=for-the-badge&logo=github&logoColor=white\"  align='left'>\n<\/a>","3f505a3e":"Slightly better :)","cd642cb5":"## Model\nWe will use one Catboost model for all the segments in the dataset. The separate model for each segment(CatBoostModelPerSegment) showed worse results.","57fd5e69":"It is important to remember to simple ideas:\n1. All the columns in the exogenous data known for the future are **regressors**.(requires prefix \"regressor_\" to indicate it)\n2. All categorical features should have type \"category\" to be handled correctly","d7eb7f18":"### Evaluation(backtest)\n\nTo evaluate the resulting pipeline, we are going to run a **backtest** - special cross validation for time series that takes into account ordering by timestamp. The base idea is simple: don't validate on the past."}}