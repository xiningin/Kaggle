{"cell_type":{"75b8a433":"code","1a3d8755":"code","dc61a064":"code","aecd5f6e":"code","41cba4b3":"code","14c92b8a":"code","9c38e04a":"markdown","3de1e0dc":"markdown","48a3c9a0":"markdown","c83078b1":"markdown","6dd3dc39":"markdown","7eb2c2ff":"markdown","5d65ea4c":"markdown","ec38993e":"markdown","3f4c6c2c":"markdown","3f549148":"markdown"},"source":{"75b8a433":"import numpy as np, pandas as pd, os\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","1a3d8755":"from scipy.stats import multivariate_normal\n\ndef get_mv(x,y):\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    cov1 = np.cov(x2.T)\n    m1 = np.mean(x2, axis = 0)\n    \n    zeros = (y==0).astype(bool)\n    x2b = x[zeros]\n    cov2 = np.cov(x2b.T)\n    m2 = np.mean(x2b, axis = 0)\n    \n    mv1 = multivariate_normal(mean=m1, cov=cov1)\n    mv2 = multivariate_normal(mean=m2, cov=cov2)\n    \n    return mv1, mv2\n\n","dc61a064":"def calc_prob(x,mv1,mv2):\n    y_pred2 = np.zeros((len(x),))\n    for i in range(len(x)):\n        a = mv1.pdf(x[i])\n        b = mv2.pdf(x[i])\n        y_pred2[i] = a\/(a+b)\n    return y_pred2","aecd5f6e":"# INITIALIZE VARIABLES\ncols = [c for c in train.columns if c not in ['id', 'target']]\ncols.remove('wheezy-copper-turtle-magic')\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor i in tqdm(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n    \n    # STRATIFIED K-FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        # MODEL AND PREDICT WITH QDA\n        #clf = QuadraticDiscriminantAnalysis(reg_param=0.5)\n        \n        \n        mv1, mv2 = get_mv(train3[train_index,:],train2.loc[train_index]['target'].values)\n        oof[idx1[test_index]] = calc_prob(train3[test_index,:],mv1,mv2)\n        preds[idx2] += calc_prob(test3,mv1,mv2)\/ skf.n_splits\n       \n    #if i%64==0: print(i)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('QDA scores CV =',round(auc,5))","41cba4b3":"# INITIALIZE VARIABLES\ntest['target'] = preds\noof = np.zeros(len(train))\npreds = np.zeros(len(test))\n\n# BUILD 512 SEPARATE MODELS\nfor k in tqdm(range(512)):\n    # ONLY TRAIN WITH DATA WHERE WHEEZY EQUALS I\n    train2 = train[train['wheezy-copper-turtle-magic']==k] \n    train2p = train2.copy(); idx1 = train2.index \n    test2 = test[test['wheezy-copper-turtle-magic']==k]\n    \n    # ADD PSEUDO LABELED DATA\n    test2p = test2[ (test2['target']<=0.01) | (test2['target']>=0.99) ].copy()\n    test2p.loc[ test2p['target']>=0.5, 'target' ] = 1\n    test2p.loc[ test2p['target']<0.5, 'target' ] = 0 \n    train2p = pd.concat([train2p,test2p],axis=0)\n    train2p.reset_index(drop=True,inplace=True)\n    \n    # FEATURE SELECTION (USE APPROX 40 OF 255 FEATURES)\n    sel = VarianceThreshold(threshold=1.5).fit(train2p[cols])     \n    train3p = sel.transform(train2p[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    # STRATIFIED K FOLD\n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3p, train2p['target']):\n        test_index3 = test_index[ test_index<len(train3) ] # ignore pseudo in oof\n        \n        mv1, mv2 = get_mv(train3p[train_index,:],train2p.loc[train_index]['target'].values)\n        oof[idx1[test_index3]] = calc_prob(train3[test_index3,:],mv1,mv2)\n        preds[test2.index] += calc_prob(test3,mv1,mv2) \/ skf.n_splits\n\n       \n    #if k%64==0: print(k)\n        \n# PRINT CV AUC\nauc = roc_auc_score(train['target'],oof)\nprint('Pseudo Labeled QDA scores CV =',round(auc,5))","14c92b8a":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['target'] = preds\nsub.to_csv('submission.csv',index=False)\n\nimport matplotlib.pyplot as plt\nplt.hist(preds,bins=100)\nplt.title('Final Test.csv predictions')\nplt.show()","9c38e04a":"# Step 3 & 4 - Add pseudo label data and recalculate ","3de1e0dc":"# Conclusion\nIn this kernel, we learned what pseudo labeling is, why it works, and how to deploy it. Using it on the data from Instant Gratification competition we observed it increase CV by an impressive 0.005! Pseudo labeling QDA achieved CV 0.970 and LB 0.969. Without pseudo labeling, QDA achieved CV 0.965 and LB 0.965.\n\nWhen you run your kernel locally, it will only pseudo label the public test data (because that is all that `test.csv` contains). When you submit this solution to Kaggle, your submission will load the full `test.csv` and pseudo label both the public and private test data set. Thus you will approximately double your amount of training data for your submissions!","48a3c9a0":"The second function calculates the probability that a datapoint belongs to a multivariate normal distribution given two alernatives. first we calculate the probability for a specific datapoint to come from each of the two multivariate distributions using `multivariate_normal.pdf`. Then we use bayes formula to get the overall probabiity. See e.g. https:\/\/math.stackexchange.com\/questions\/825455\/probability-that-a-sample-comes-from-one-of-two-distributions","c83078b1":"# Submit Predictions","6dd3dc39":"# The 2 main functions: Emperical Covariance and calculate probability","7eb2c2ff":"Same as in the original kernel we use pseudlabelling...","5d65ea4c":"# Intro\nLuckily Chris already put some illustrative pictures in his kernel, I can steal.\nHere is a pictorial explanation using sythetic 2D data. \n  \n## Step 1 - Build first model\nGiven 50 training observations (25 target=1 yellow points, 25 target=0 blue points) we can estimate the multivariate (approx 40 dimensions) normal distributions of each of the two target types (0 & 1) by calculating empiral covariance and mean (see np.cov and np.mean) and then calculate that a given datapoint belongs to distribution A or B using scipy.stats.multivariate.\n\n\n![image](http:\/\/playagricola.com\/Kaggle\/p16419.png)\n\nhttps:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.multivariate_normal.html#scipy.stats.multivariate_normal\nhttps:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.cov.html\n","ec38993e":"# Apply Statistics to Instant Gratification Comp\n## Load Data","3f4c6c2c":"The next functions calculates empirical covariance and mean using numpy per label type and returns the two multivariate normal distibutions as instance.  ","3f549148":"# Who needs models, just do statstics\nFirst of all, 95% of the kernel is stolen from Chris, I just exchanged QDA with heuristic statistics. The main point is, that if we assume our variables to be multivariate normal distributed (which we know from the make_classification function) we can just calcuate the probability that a data point belongs to either of the two ellipses by calculating probablities."}}