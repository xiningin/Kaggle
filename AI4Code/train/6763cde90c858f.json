{"cell_type":{"e613ddcc":"code","c6fc5d53":"code","6ec8c949":"code","18fcfa33":"code","001c7706":"code","ec66787e":"code","330aefa2":"code","4b126a17":"code","b25ef178":"code","ad85bf73":"code","04fd7554":"code","933e72cd":"code","a56c07b3":"code","91c66148":"code","7a13a4bd":"code","43d854ed":"code","4bdc669c":"code","b47946e0":"code","4c265a36":"code","3d7c0971":"code","e97e2bc4":"code","da9cd6e5":"code","bba564a0":"code","68658f21":"code","8afd18cc":"code","d7dd888c":"code","0e152cd9":"code","ca380220":"code","ea8d59f8":"code","78bd8f1b":"code","75aa648b":"code","2272abb7":"code","1813d8fe":"code","3ba76626":"code","9b5bf132":"code","abb64b36":"code","dde609ae":"code","3e57d291":"code","0d850575":"code","5a7bbd91":"code","93d5e669":"code","9c3ef7a0":"code","4629fc6e":"code","84fde96e":"code","32c3e126":"code","23d0ae66":"code","782f55d5":"code","85a374e6":"code","93aba756":"code","dbff9991":"code","e8f2ab03":"code","a2f2054a":"code","bb3f8d7a":"code","5f14b8bd":"code","17bf6e7a":"code","7ab83d15":"code","346fd747":"code","cf920db7":"code","ede76d20":"code","f9c60652":"code","98dd323d":"code","a8e2f29d":"code","c85ca729":"code","0920d42c":"code","4de7c58d":"code","a4cc6bd1":"code","fcc11108":"code","811847bf":"code","631557f7":"code","6099717d":"code","e8aa415e":"code","9c8a47fd":"code","2eb735b9":"code","7f3cf4a6":"code","7efa57bf":"code","4bd72dd7":"code","4c688f88":"code","ee40348f":"code","43c6b54f":"code","52c273be":"code","49d98406":"code","44fabee1":"code","fbb01ac1":"code","7e6d24fb":"code","d01ddc52":"code","d34c06dc":"code","a15e7781":"code","3618d70c":"markdown","6523f8e1":"markdown","9069a80a":"markdown","89276119":"markdown","8098b3d8":"markdown","7417c649":"markdown","9868eaa1":"markdown","64172015":"markdown","5d658593":"markdown","d9572d4b":"markdown"},"source":{"e613ddcc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6fc5d53":"import pandas as pd\nimport numpy as np\n\n# for data visualizations\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nimport pickle\nimport joblib","6ec8c949":"data = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict.csv', index_col = 'Serial No.')\ndata2 = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv', index_col = 'Serial No.')\n\nprint('Shape of data1 : ', data.shape)\nprint('Shape of data2 : ', data2.shape)","18fcfa33":"data = pd.concat([data, data2], axis = 0)","001c7706":"data.head()","ec66787e":"data.shape","330aefa2":"data.columns","4b126a17":"from sklearn.model_selection import train_test_split","b25ef178":"X_train, X_test, y_train, y_test = train_test_split(data.drop(['Chance of Admit '], axis = 1), data['Chance of Admit '], test_size = 0.2, random_state = 21)","ad85bf73":"X_train.head()","04fd7554":"X_test.head()","933e72cd":"y_train.head()","a56c07b3":"y_test.head()","91c66148":"from sklearn.preprocessing import StandardScaler","7a13a4bd":"sc = StandardScaler()","43d854ed":"X_train = sc.fit_transform(X_train)","4bdc669c":"X_test = sc.transform(X_test)","b47946e0":"X_train.shape","4c265a36":"X_test.shape","3d7c0971":"X_train","e97e2bc4":"X_test","da9cd6e5":"joblib.dump(sc, 'scaler.save')","bba564a0":"scaler_loaded = joblib.load('scaler.save')","68658f21":"X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size= 0.2, random_state = 21)","8afd18cc":"print(X_train[:5])\nprint('-' * 10)\nprint(X_train.shape)","d7dd888c":"print(X_val[:5])\nprint('-' * 10)\nprint(X_val.shape)","0e152cd9":"print(y_train[:5])\nprint('-' * 10)\nprint(y_train.shape)","ca380220":"print(y_val[:5])\nprint('-' * 10)\nprint(y_val.shape)","ea8d59f8":"print(X_test[:5])\nprint('-' * 10)\nprint(X_test.shape)","78bd8f1b":"print(y_test[:5])\nprint('-' * 10)\nprint(y_test.shape)","75aa648b":"# work on data distribution for later stages of model training","2272abb7":"from sklearn.linear_model import LinearRegression\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\nlinreg_pred = linreg.predict(X_test)\n\nmse = mean_squared_error(y_test, linreg_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, linreg_pred)\n\nprint(\"Root Mean Squared Error : \",rmse)\nprint(\"R-Squared Error:\", r2)","1813d8fe":"from sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nsvr = SVR(kernel = 'linear')\nsvr.fit(X_train, y_train)\n\nsvr_pred = svr.predict(X_test)\n\nmse = mean_squared_error(y_test, svr_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, svr_pred)\n\nprint(\"Root Mean Squared Error : \",rmse)\nprint(\"R-Squared Error:\", r2)","3ba76626":"from sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nrfr = RandomForestRegressor(100)\nrfr.fit(X_train, y_train)\n\nrfr_pred = rfr.predict(X_test)\n\nmse = mean_squared_error(y_test, rfr_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, rfr_pred)\n\nprint(\"Root Mean Squared Error : \",rmse)\nprint(\"R-Squared Error:\", r2)","9b5bf132":"from xgboost.sklearn import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nxgb = XGBRegressor()\nxgb.fit(X_train, y_train)\n\nxgb_pred = xgb.predict(X_test)\n\nmse = mean_squared_error(y_test, xgb_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, xgb_pred)\n\nprint(\"Root Mean Squared Error : \",rmse)\nprint(\"R-Squared Error:\", r2)","abb64b36":"from sklearn.ensemble import ExtraTreesRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\netr = ExtraTreesRegressor()\netr.fit(X_train, y_train)\n\netr_pred = etr.predict(X_test)\n\nmse = mean_squared_error(y_test, etr_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, etr_pred)\n\nprint(\"Root Mean Squared Error : \",rmse)\nprint(\"R-Squared Error:\", r2)","dde609ae":"from sklearn.preprocessing import PolynomialFeatures","3e57d291":"from sklearn.pipeline import make_pipeline","0d850575":"\"\"\"polyreg = make_pipeline(PolynomialFeatures(2), linear_model.LinearRegression())\npolyreg.fit(X_train,y_train)\npolyreg.score(X_test, y_test)\"\"\"","5a7bbd91":"from sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import tree\n","93d5e669":"# ML Models\n\nclassifiers_single_model = [\n    linear_model.Ridge(alpha = 0.5),\n    linear_model.Lasso(alpha = 0.1),\n    #linear_model.LassoLars(alpha = 0.1),\n    linear_model.BayesianRidge(),\n    linear_model.SGDRegressor(max_iter = 1000, tol = 1e-3),\n    svm.SVR(),\n    neighbors.KNeighborsRegressor(),\n    tree.DecisionTreeRegressor(),\n    linear_model.LinearRegression(),\n    linear_model.Lasso(alpha = 0.12),\n    linear_model.Ridge(alpha = 0.55),\n    linear_model.Ridge(alpha = 0.6),\n    linear_model.Ridge(alpha = 0.65),\n    linear_model.Ridge(alpha = 0.7),\n    make_pipeline(PolynomialFeatures(2), linear_model.LinearRegression())  \n]","9c3ef7a0":"len(classifiers_single_model)","4629fc6e":"X_test.shape","84fde96e":"features_level_1_train = np.zeros((144,14))\nfeatures_level_1_test = np.zeros((180,14))\nfor i, clf in enumerate(classifiers_single_model):\n    print(clf.fit(X_train, y_train))\n    print(clf.score(X_val, y_val))\n    preds = clf.predict(X_val)\n    #print(preds.shape)\n    features_level_1_train[:, i] = preds\n    # preds for test data\n    preds_test = clf.predict(X_test)\n    #features_level_1_test = clf.predict(X_test)\n    features_level_1_test[:, i] = preds_test\n    pickle.dump(clf, open(f'model{i+1}' + '.pkl', 'wb'))\n    ","32c3e126":"features_level_1_train","23d0ae66":"len(features_level_1_train)","782f55d5":"y_val","85a374e6":"len(y_val)","93aba756":"stage_1_ensemble_5_models = [\n    make_pipeline(PolynomialFeatures(2), linear_model.LinearRegression()),\n    neighbors.KNeighborsRegressor(),\n        tree.DecisionTreeRegressor(),\n            linear_model.SGDRegressor(max_iter = 1000, tol = 1e-3),\n    svm.SVR()\n]","dbff9991":"val_size = len(features_level_1_train) * 0.4","e8f2ab03":"val_size = np.ceil(val_size)\nval_size","a2f2054a":"features_level_1_train, features_level_1_val, y_level_1_train, y_level_1_val = train_test_split(features_level_1_train, y_val, test_size = 0.4, random_state = 21)","bb3f8d7a":"features_level_1_train.shape","5f14b8bd":"features_level_1_val.shape","17bf6e7a":"y_level_1_train.shape","7ab83d15":"y_level_1_val.shape","346fd747":"features_level_2_train = np.zeros((58,5))\nfeatures_level_2_test = np.zeros((180, 5))\nfor i, clf in enumerate(stage_1_ensemble_5_models):\n    print(clf.fit(features_level_1_train, y_level_1_train))\n    #print(clf.score(X_test, y_test))\n    print(clf.score(features_level_1_val, y_level_1_val))\n    preds = clf.predict(features_level_1_val)\n    print(preds.shape)\n    features_level_2_train[:, i] = preds\n    \n    preds_test = clf.predict(features_level_1_test)\n    features_level_2_test[:, i] = preds_test\n    \n    pickle.dump(clf, open(f'ensemble_model{i+1}' + '.pkl', 'wb'))\n    \n    ","cf920db7":"features_level_2_train","ede76d20":"features_level_2_train.shape","f9c60652":"y_level_1_val.shape","98dd323d":"features_level_2_test","a8e2f29d":"features_level_2_test.shape","c85ca729":"# CatBoostRegressor, ExtraTreesRegressor, XGBRegressor, RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\n\nstage_2_models = [\n    CatBoostRegressor(iterations = 500, depth = 4),\n    ExtraTreesRegressor(),\n    RandomForestRegressor()    \n]\n\n","0920d42c":"features_level_22_train, features_level_22_val, y_level_2_train, y_level_2_val = train_test_split(features_level_2_train, y_level_1_val, test_size = 0.4, random_state = 21)","4de7c58d":"features_level_22_train","a4cc6bd1":"features_level_22_train.shape","fcc11108":"y_level_2_train","811847bf":"y_level_2_train.shape","631557f7":"features_level_22_val","6099717d":"features_level_22_val.shape","e8aa415e":"y_level_2_val","9c8a47fd":"y_level_2_val.shape","2eb735b9":"features_level_3_train = np.zeros(shape = (24, 3))\nfeatures_level_3_test = np.zeros(shape = (180,3))\n\nfor i, clf in enumerate(stage_2_models):\n    print(clf.fit(features_level_22_train, y_level_2_train))\n    #print(clf.score(X_test, y_test))\n    print(clf.score(features_level_22_val, y_level_2_val))\n    preds = clf.predict(features_level_22_val)\n    print(preds.shape)\n    features_level_3_train[:, i] = preds\n    \n    preds_test = clf.predict(features_level_2_test)\n    print(preds_test.shape)\n    features_level_3_test[:, i] = preds_test\n    pickle.dump(clf, open(f'stage_2_model{i+1}'+'.pkl','wb'))","7f3cf4a6":"features_level_3_train","7efa57bf":"features_level_3_train.shape","4bd72dd7":"y_level_2_val.shape","4c688f88":"features_level_3_test.shape","ee40348f":"y_test.shape","43c6b54f":"meta_model = linear_model.LinearRegression(n_jobs= -1)\nmeta_model.fit(features_level_3_train, y_level_2_val)","52c273be":"meta_model.score(features_level_3_test, y_test)","49d98406":"import pickle","44fabee1":"pickle.dump(meta_model, open('meta_model.pkl', 'wb'))","fbb01ac1":"dir(meta_model)","7e6d24fb":"# saving the ml model in pickle format","d01ddc52":"files = [file for file in os.listdir('\/kaggle\/working') if file.endswith('.pkl')]","d34c06dc":"files","a15e7781":"len(files)","3618d70c":"train & val data split","6523f8e1":"## Model Stacking","9069a80a":"Loading & Preparing the Dataset","89276119":"Split into train & val","8098b3d8":"Stacking of Models","7417c649":"Feature Scaling\/Standardization","9868eaa1":"## This notebook is about the implementation of the Model Stacking Method which is widely used in Data Analysis.\n\n# Hope you like it. And please appreciate my work by giving an UPVOTE! :) THANKS!","64172015":"## Model Deployed on Heroku @ https:\/\/graduate-admission-chances.herokuapp.com\/","5d658593":"Necessary Library Imports","d9572d4b":"Model Training"}}