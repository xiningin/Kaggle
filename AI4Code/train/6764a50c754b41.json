{"cell_type":{"340f0419":"code","dbdabdd0":"code","94dd4b00":"code","ae864b78":"code","08ba5e8f":"code","150c6980":"code","cb1a4ef8":"code","9a11da32":"code","73a07108":"code","f987fce6":"code","93369753":"code","d0f25881":"code","e9d8dedf":"code","9c8461f7":"code","2ddafe8f":"code","765e303a":"code","074f172b":"code","584ec876":"code","27c078b6":"code","a3b48319":"markdown","2f84e863":"markdown","321c30c6":"markdown","b5844ba7":"markdown","3b17334b":"markdown","dc5a58b7":"markdown","0560aa0e":"markdown","7bf262a7":"markdown","0ec8a0c8":"markdown","f0632c40":"markdown","021734b3":"markdown","d773f9dc":"markdown","08d1b009":"markdown","0da7c93f":"markdown","ca245851":"markdown","4aa36118":"markdown","321ce928":"markdown","a2996bea":"markdown","49a6f78c":"markdown","1fb81664":"markdown","b6194bb6":"markdown","de77ea3f":"markdown","d654f69c":"markdown","1c567fee":"markdown","3bd284a6":"markdown","badd1fab":"markdown","0f8dbe62":"markdown","e1bff2a0":"markdown","2397a0a3":"markdown"},"source":{"340f0419":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dbdabdd0":"#read data from csv file\ndf = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\ndf.head()","94dd4b00":"df.info()","ae864b78":"#classes\npd.unique(df[\"Species\"])","08ba5e8f":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(df[\"Species\"])\n\n# show convertion result\nprint(dict(zip(label_encoder.classes_,label_encoder.transform(label_encoder.classes_))))\n\ndf[\"Species\"] = label_encoder.fit_transform(df[\"Species\"])","150c6980":"# first 5 row \ndf.head()","cb1a4ef8":"from sklearn.model_selection import train_test_split\n\n#drop ID column\ndf.drop([\"Id\"],axis=1,inplace = True)\n\nX = df.drop([\"Species\"],axis=1,inplace = False)\nY = df[\"Species\"].values.reshape(-1,1)\n\n#split data \nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state = 0, test_size = 0.2)\n\nprint(\"X_train shape:\",X_train.shape)\nprint(\"X_test shape:\",X_test.shape)\nprint(\"Y_train shape:\",Y_train.shape)\nprint(\"Y_test shape:\",Y_test.shape)","9a11da32":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\n\n#def svm_model(X_train,X_test,Y_train,Y_test,C = 1.0,kernel = 'rbf',degree = 3,gamma = 'scale'):\nsvc = SVC()\naccuracies = cross_val_score(estimator=svc, X = X_train, y = Y_train, cv = 3)\ntrain_score = np.mean(accuracies)\n    \nsvc.fit(X_train,Y_train)\ntest_score = svc.score(X_test,Y_test)\n\nprint(\"Train Score of Default Parameters:\",train_score)\nprint(\"Test Score of Default Parameters:\",test_score)","73a07108":"\ndef visualize_svm_C(C,title):\n    X_petal = df[df.Species != 2].iloc[:,2:4]\n    X_petal = X_petal[X_petal != 2]\n    y = df.Species[df.Species != 2]\n\n\n    model = SVC(kernel='linear', C=C)\n    model.fit(X_petal, y)\n\n    ax = plt.gca()\n\n    plt.scatter(X_petal.iloc[:,0], X_petal.iloc[:,1], c=y, s=50, cmap='autumn')\n    plt.scatter(model.support_vectors_[:,0],model.support_vectors_[:,1])\n    \n\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    xx = np.linspace(xlim[0], xlim[1], 30)\n    yy = np.linspace(ylim[0], ylim[1], 30)\n    YY, XX = np.meshgrid(yy, xx)\n    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n    Z = model.decision_function(xy).reshape(XX.shape)\n\n    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n\n    ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,linewidth=1, facecolors='none', edgecolors='k')\n\n    plt.xlabel(\"Petal Length\",fontsize = 18)\n    plt.ylabel(\"Petal Width\",fontsize = 18)\n    plt.title(title + \" Score:\" + str(model.score(X_petal,y)),fontsize = 18)\n\n","f987fce6":"plt.figure(figsize=(25,15))\nfor i,c in enumerate([0.01,0.1,1,10]):  \n    plt.subplot(2,2,i+1)\n    visualize_svm_C(C = c, title = \"C = \" + str(c))\nplt.show()","93369753":"def meshgrid(x, y, h=.01):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy","d0f25881":"def contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out","e9d8dedf":"X = df.iloc[:, :2] # we only take the first two features.\ny = df.Species\n\ndef visualize_kernels(X,y,ax, title, C  = 1.0 ,kernel='linear',degree = 3,gamma='scale'):\n    # The classification SVC model\n    model = SVC(C = C, kernel=kernel,gamma = gamma,degree = degree)\n    clf = model.fit(X, y)\n    \n    # title for the plots\n    # Set-up grid for plotting.\n    X0, X1 = X.iloc[:, 0], X.iloc[:, 1]\n    xx, yy = meshgrid(X0, X1)\n    contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n    ax.set_ylabel(\"Sepal Length\",fontsize = 18)\n    ax.set_xlabel(\"Sepal Width\", fontsize = 18)\n    ax.set_title(title + \"    Score:\" + str(model.score(X,y)),fontsize = 18)\n","9c8461f7":"fig, ax = plt.subplots(1,4,figsize=(30,5))\nfor i,k in enumerate(['linear','poly','rbf','sigmoid']):\n    visualize_kernels(X,y,ax[i],title = (\"Kernel = \" + k),kernel = k)\nplt.show()","2ddafe8f":"#normalize input and visualize again\nX_sepal = X\nX_norm = (X_sepal - np.min(X_sepal)) \/ (np.max(X_sepal) - np.min(X_sepal))\nfig, ax = plt.subplots(figsize=(10,5))\nvisualize_kernels(X_norm,y,ax,title = (\"Kernel = sigmoid\"),kernel = \"sigmoid\")\nplt.show()","765e303a":"fig, ax = plt.subplots(1,4,figsize=(30,5))\nfor i,d in enumerate([1,3,5,7]):\n    visualize_kernels(X,y,ax[i],title = (\"Degree = \" + str(d)),kernel = 'poly',degree = d)\nplt.show()","074f172b":"fig, ax = plt.subplots(1,4,figsize=(30,5))\nfor i,g in enumerate([0.01,1,10,500]):\n    visualize_kernels(X,y,ax[i],kernel = 'rbf', gamma = g,title = (\"Gamma:\" + str(g)))\nplt.show()","584ec876":"from sklearn.model_selection import GridSearchCV\n\ndef calculate_best_params(grid):\n    svm  = SVC ();\n    svm_cv = GridSearchCV(svm, grid, cv = 3)\n\n    svm_cv.fit(X_train,Y_train)\n    print(\"Best Parameters:\",svm_cv.best_params_)\n    print(\"Train Score:\",svm_cv.best_score_)\n    print(\"Test Score:\",svm_cv.score(X_test,Y_test))","27c078b6":"grid = {\n    'C':[0.01,0.1,1,10],\n    'kernel' : [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n    'degree' : [1,3,5,7],\n    'gamma' : [0.01,1,10,500]\n}\n\ncalculate_best_params(grid)","a3b48319":"After the normalization, result is better but still not good enough. But there are another parameters which effect the prediction result. So, some other parameter optimization can improve the result. The point what I want to draw attention, is sigmoid function expect inputs between 0 and 1.","2f84e863":"Note that if degree is selected 1, It equals to linear kernel.","321c30c6":"<a id='1'><\/a>\n# Load Data and PreCheck","b5844ba7":"Basically kernel is a mathematical function. These function takes data and transform it into a new form. We mentioned about non-linear data under the What is the Support Vector Machine title and we said that we can classifiy datas linearly if we transform data into high dimensions. To do this, we can use kernel parameter. There are several options as kernel function. Options are listed below;\n* linear\n* poly\n* rbf\n* sigmoid\n\nIf we have non-linear data, we can try different functions like poly or rbf. These functions compute third dimension and convert non-linear problem to linear problem. It is called Kernel Trick. Let's visualize the effects of kernel functions to understand what's going on. ","3b17334b":"<a id='9'><\/a>\n## degree","dc5a58b7":"<a id='10'><\/a>\n## gamma","0560aa0e":"<a id='6'><\/a>\n# Parameters Description and Observation of Effects\nIn this part, we examine parameters of SVM separately and we will try to understand effect of different parameter values on result score. Also we will use visualization techniques to understand better.","7bf262a7":"If you look at the graphs above, you can understand effect of gamma easily. When gamma paramater is increased, classification becomes more specific for each data. \n\nLook at gamma = 0.01 graph. There are three area for three class and they are quite general areas. SVM doesn't struggle to fit for every data point. Unlike this, if you look at gamma = 500 graph, SVM struggles to fit every point. So the training score is the best. But do you think that this can cause a problem in the future? **Overfitting**.\n\nGamma is an important parameter for non-linear kernels like **rbf or poly**. If gamma is increased, SVM spends mor effort to fit data and your model can improve but don't forget that if gamma is chosen too high, it can cause overfitting. ","0ec8a0c8":"We can see that our model works well with default values. Now we will try to change each parameters separately and we will see their effects on result. But before start to look parameters, let's understand what is SVM.\n\n### What is Support Vector Machine (SVM) ?\nBasically, Support Vector Machines tries to find data points from each class and defines boundaries between them. The data points which define boundaries are called support vectors and boundaries are called as hyperlanes. Let's visualize it to understand better.\n\n![SVM.png](attachment:SVM.png)\n\nThis is the basic presentation of SVM. The SVM algorithm tries to find the closest data pair which in different class and define a boundary center of them. In picture above, you can see support vectors and hyperlane. Of course this dataset can separate with a linear line. But for dataset which uses in real life , one line is not enough to separate all classes. Imagine that our dataset like below;\n\n![nonlinear-svm.PNG](attachment:nonlinear-svm.PNG)\n\nSo if you have a dataset like shown above, you can understand that single line is not enough to separate classes from each other. A common way to overcome this problem is to work with high dimensions. Now we start to examine wtih first parameter and don't worry about non-linear dataset because we will use sklearn SVM library and we will understand how to overcome non-linear dataset when we look at kernel parameter.","f0632c40":"<a id='4'><\/a>\n# Train Test Split\n\nNow that categorical variables were eliminated, dataset can be splitted. To make this, train_test_split class can be used from sklearn model selection library.","021734b3":"<a id='7'><\/a>\n## Regularization Parameter (C)\n","d773f9dc":"We can say that the goal of the SVM algorithn is low misclassification and large margin. But this two goals are contradictory. Actually, C parameter can be used to provides the balance between these goals. \n* If we choose large value for C, we get small margin but lower misclassification\n* If we choose small value for C, we get large margin but high misclassification compared to large C value.\n\nThere isn't any exact method to choosing correct value of C parameter. But we can test some values and choose the best one. To understand effect of C parameter, some graphs are drew for different C values. Petal length and petal width features of setosa and versicolor are used as dataset. So we can use linear line to separate two classes. \n\nIn the graphs;\n\n* red circles => setosa\n* yellow circles => versicolor\n* blue circles => support vectors","08d1b009":"# Introduction\nIn this kernel,the most common parameters of SVM Algorithm are described and effects of these paremeters on result are observed. First prediction is predicted by default value. After that, parameters of SVM are discussed separately and effects of pararameters will be visualized. <br\/>\n\nFinally, GridSearch algorithm is used to find best values of each parameters. So results can be compared in the conclusion part. To purpose of this kernel, understanding parameters of SVM Classifier algorithm and gain experience about it.\n\n<font color='blue'>\nContent\n    \n1. [Load Data and PreCheck](#1)\n1. [Preparing Data Set](#2)\n    * [Categorical Varibles](#3)\n    * [Train Test Split](#4)\n1. [First Prediction With Default Parameters](#5)\n1. [Parameters Description and Observation of Effects](#6)\n    * [Regularization Parameter (C)](#7)\n    * [kernel](#8)\n    * [degree](#9)\n    * [gamma](#10)\n1. [GridSearch](#11)\n1. [Conclusion](#12)","0da7c93f":"* There are 150 rows and 6 columns.\n* Id is useless feature for classification so we can drop it.\n* Species column categorical and has 3 different category. This column is our target column. It would be better if we convert from object to int64 to avoid any error during implementation of SVM.","ca245851":"* The lowest C value is 0.01 and if you check the graph of C = 0.01, you can see that there are lots of misclassification but it has largest margin.\n* The highest C value is 10 and if you check the graph of C = 10, you can see that there aren't any misclassification but it has lowest margin.\n* Of course this dataset is separeted by SVM easily so all scores are 1.0 and it means that there is no misclassification. But I believe that these graphs show effects of C parameter on SVM.","4aa36118":"* Linear kernel is the simplest one and if your dataset separate with linear line it is useful function. But note that linear kernels don't define any transformation to high dimensions.\n* Polynomial kernel is for calculating non-linear hyperlane. It looks not only given features but also combinations of these features. Degree parameter which will be examined in next part, is about polynomial kernel and it provides more flexible decision boundaries. \n* RBF is the best option for complex dataset. It has infinite number of dimension. More dimension means that more chance to define better hyperlanes.\n* All results are acceptable except sigmoid function. Its accuracy score is about 0.25. Reason of this is structure of sigmoid function. If input values are not between 0 and 1, it can cause of some problem. So let's normalize our inputs and try again. ","321ce928":"In this kernel, we will examine some parameters in depth and I will try to briefly talk about what the others are. The parameters which we will focus on, are C, gamma, kernel, degree and coef0.\n\nThe default parameter of SVM algorithm are shown below;\n* C = 1.0\n* kernel = 'rbf'\n* degree = 3\n* gamma = 'scale'","a2996bea":"Gamma parameter uses with non-linear kernels. The higher gamma try to provide more fitting. So if the gamma parameter select too high, it can cause overfitting. Let's look at effects of different gammas on graphs.","49a6f78c":"Degree of the polynomial kernel. It is just used by poynomial kernel and ignored by other kernels. High polynomial degree yield more flexible decision boundary. Let's visualize that. ","1fb81664":"Four common parameters are examined separately and to understand better what are these parameters and why are they used in SVM, all effects try to visualize. As a result, we can say the following about these parameters and SVM;\n\n* C parameter adjust the margin and misclassification. Large C value means less misclassification and low margin on the other hand high C value more misclassification but large margin.\n* Kernel parameters are functions which calculate hyperlanes. If data can separable with linear line , linear option can be used. It is simplest one to calculate hyperlane. But if data can't separable with linear line then, rbf or poly can be used for transforming high dimensions. (It also calls Kernel Trick). Generally, rbf is more prefer than poly.\n* Gamma is about non-linear kernels and it try to provide more fitting but don't forget gamma parameter which is selected too high, can cause overfitting.\n* Degree parameter is degree of poynomial kernel. It is ignored by other all kernels. High degree means more flexible hyperlane.\n\nI tried to explain what I learnt about SVM parameters during my resarches . If you think there is any problem about this kernel, please feel free to share with me.\nIf you like this kernel, this is my another kernel about [parameters of KNN](https:\/\/www.kaggle.com\/gorkemgunay\/understanding-parameters-of-knn).","b6194bb6":"GridSearch algorithm gave us 'poly' as best kernel parameter but also best degree parameter  is 1. Remember that if degree is 1 for poly kernel, it is same as linear kernel. And when parameters were observed, just two features were used to visualize easily but in in GridSearch all features are included in calculation.\n\nBeginning of this kernel, SVM algorithm was implemented with default values of parameters and the accuracy score is about 0.94 but after finding best parameters with GridSearch algorithm, accuracy score was improved about 0.97.","de77ea3f":"<a id='3'><\/a>\n## Categorical Variables\n\nTo convert from object to int64, using LabelEncoder class is an easy way. LabelEncoder is a part of scikit-learn preprocessing library. Label encoder is applied over the Species column.","d654f69c":"<a id='12'><\/a>\n# Conclusion","1c567fee":"<a id='8'><\/a>\n## kernel","3bd284a6":"As shown above, categorical variables were replaced with int64 variables. LabelEncoder class randomly assigns unique value for each categorical variable. This is a common way to eliminate them.","badd1fab":"<a id='11'><\/a>\n# GridSearch","0f8dbe62":"Train and test datasets were prepared. 120 data are separated for train and 30 data are separated for test. Now we can start to review of SVM algorithm.  First the result of default parameters will be shown and then effects of parameters on result will be examined. ","e1bff2a0":"<a id='5'><\/a>\n# First Prediction with Default Parameters","2397a0a3":"<a id='2'><\/a>\n# Preparing Data Set\n\nThis part includes two subtitles as categorical variabes and train test split. First of all categorical variable is converted to int64 with LabelEncoder class. Then, dataset is splited as train (80%) and test (20%)."}}