{"cell_type":{"d6111a46":"code","e343cc60":"code","557cf1b2":"code","000d65f6":"code","2bba21cb":"code","f9c7af47":"code","703c9a87":"code","a5a34f4a":"code","4fadc26f":"code","ae440da5":"code","2a8b7889":"code","97228210":"code","a4d503e9":"code","049d3923":"code","bdf42f2f":"code","f0233b7d":"code","a622afd2":"code","01abdcaf":"code","2ce56ece":"code","909b2d2c":"code","8d31195b":"code","561178bc":"code","1b742b8c":"code","8c0d5262":"code","67165ce5":"code","08879b36":"code","70aa987e":"code","b709cbaa":"code","4c2cdf2b":"code","b8dacb99":"code","c23c62b0":"code","ae5d3648":"code","d32f499e":"code","3830c54f":"code","97d36299":"code","61e91b51":"code","17e93d4b":"code","fbd2d210":"code","cbb32244":"code","31b24c13":"code","bdb99f1c":"code","88362928":"code","7448b619":"markdown","2b3b1102":"markdown","f9980665":"markdown","6e6e55ca":"markdown","946e540f":"markdown","7ca22fc9":"markdown","2ccef037":"markdown","8d913920":"markdown","8ef6569e":"markdown","486a7a4d":"markdown","7a98876f":"markdown","151c94d4":"markdown","76f084e8":"markdown","6a57842d":"markdown","c50fe63f":"markdown","f3f8d209":"markdown","68bda322":"markdown","1c9da5c9":"markdown","3e378929":"markdown","ceb69efe":"markdown","0e2d3971":"markdown","5dec3c06":"markdown","6982ad13":"markdown","0f859fec":"markdown","ca5b8b36":"markdown","03a1ae81":"markdown","8af8bd1a":"markdown","59523014":"markdown","15554ee2":"markdown","ddf1a68b":"markdown","4ee8bce5":"markdown","0e3fc5dc":"markdown"},"source":{"d6111a46":"!pip install ktrain","e343cc60":"import numpy as np\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport spacy # For tokenization, lemmatization, removing stop words & punctuations.\nfrom keras.layers import LSTM, Dense, Dropout, Activation # Layers of keras used in LSTM Model.\nfrom keras.models import Sequential # Sequential Neural Network\nfrom keras.callbacks import EarlyStopping # Early Stopping Callback in the NN\nimport tensorflow as tf\nimport ktrain # For Bert Model Implementation.\nfrom ktrain import text # Preprocessing text for the Bert Model.\nsns.set()\nnlp = spacy.load(\"en_core_web_sm\",disable=[\"tagger\", \"parser\", \"ner\"])","557cf1b2":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv') # Training Data\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv') # Testing Data\ntrain_len = len(train)\ntest_len = len(test)\nprint('Training Dataset:',train_len)\nprint('Testing Dataset:',test_len)\ntrain.head()","000d65f6":"plt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nax = sns.countplot(x='target',data=train,label=['Not a Disaster','Disaster'])\nax.set_xticklabels(['Disaster','Not a Disaster'])\nplt.suptitle(\"Distribution of Target Values\")\nterms = np.array(['Disaster', 'Not a Disaster'])\nweigtage = np.array([len(train[train['target'] == 1]),len(train[train['target'] == 0])])\nplt.subplot(1, 2, 2)\nplt.pie(weigtage,labels=terms, autopct=\"%1.1f%%\")\nplt.title('target')\nplt.show()","2bba21cb":"plt.figure(figsize=(10,5))\nword_len = train['text'].map(lambda x: len(x))\nplt.hist(word_len)\nplt.xlabel('Length of tweets')\nplt.ylabel('Number of tweets')\nplt.title('Length of tweets v\/s Number of tweets')\nplt.show()","f9c7af47":"plt.figure(figsize=(10,5))\nword_len = train['text'].str.split().map(lambda x: len(x))\nplt.hist(word_len)\nplt.xlabel('Number of Words')\nplt.ylabel('Number of tweets')\nplt.title('Number of Words v\/s Number of Tweets')\nplt.show()","703c9a87":"def missing_val_analysis(data):\n    missing_values = data.isnull().sum()\n    missing_values = missing_values[missing_values > 0].sort_values(ascending = False)\n    missing_values_data = pd.DataFrame(missing_values)\n    missing_values_data.reset_index(level=0, inplace=True)\n    missing_values_data.columns = ['Feature','Number of Missing Values']\n    missing_values_data['Percentage of Missing Values'] = (100.0*missing_values_data['Number of Missing Values'])\/len(data)\n    return missing_values_data","a5a34f4a":"missing_val_analysis(train) # Missing value analysis in the training data.","4fadc26f":"train['keyword'].fillna('',inplace=True)\ntrain['text'] = train['text'] + ' ' + train['keyword']\ntrain['text'] = train['text'].apply(lambda x: x.strip())\ntrain.drop(['keyword'],axis=1,inplace=True)\ntrain.drop(['location'],axis=1,inplace=True)\ntrain.head()","ae440da5":"missing_val_analysis(test)","2a8b7889":"test['keyword'].fillna('',inplace=True)\ntest['text'] = test['text'] + ' ' + test['keyword']\ntest['text'] = test['text'].apply(lambda x: x.strip())\ntest.drop(['keyword'],axis=1,inplace=True)\ntest.drop(['location'],axis=1,inplace=True)\ntest.head()","97228210":"duplicate_records = train[train.duplicated(['text','target'],keep=False)] # Duplicate records with same targets.\nprint('Records having same text and targets:',len(duplicate_records))\nduplicate_records.head()","a4d503e9":"train.drop_duplicates(['text','target'],inplace=True) # Dropping the duplicate records having same targets.","049d3923":"contradicting_records = train[train.duplicated(['text'],keep=False)] # Duplicate records with outliers.\nprint('Records having same text but different targets:',len(contradicting_records))\ncontradicting_records.head()","bdf42f2f":"records_to_drop = [610,2832,3243,3985,4244,4232,4292,4305,4306,4312,4320,4381,4618,5620,6091,6616] # Outliers.\n\ntrain.drop(records_to_drop,inplace=True) # Dropping the outliers.\ntrain = train.reset_index(drop=True) # Resetting the indexes.\ntrain.head()","f0233b7d":"def remove_url(text):\n    return re.sub(r'https?:\/\/\\S+|www\\.\\S+','',text)\ndef remove_char(text):\n    return re.sub(r'[^A-Za-z0-9 ]+', '', text)\n\ndef remove_preprocess(text):\n    return remove_char(remove_url(text))","a622afd2":"train['text'] = train['text'].apply(lambda x: remove_preprocess(x))\ntest['text'] = test['text'].apply(lambda x: remove_preprocess(x))\ntrain.head()","01abdcaf":"data = pd.concat([train,test],axis=0,sort=False)\ndata.drop(['target'],axis=1,inplace=True)\ndata.head()","2ce56ece":"def filtered_token(token):\n    if token.is_stop or token.is_space or token.like_num or token.like_url or token.like_email or token.is_digit or token.is_punct or len(token.lemma_) <= 2:\n        return False\n    return True\n        \n\ndef tokenize(text):\n    tokens = []\n    doc = nlp(text)\n    for token in doc:\n        if filtered_token(token):\n            tokens.append(token.lemma_.lower())\n    return tokens","909b2d2c":"data['tokens'] = data['text'].apply(lambda x: tokenize(x))\ndata.head()","8d31195b":"plt.figure(figsize=(10,5))\nword_len = data['tokens'].map(lambda x: len(x))\nplt.hist(word_len)\nplt.xlabel('Number of tokens')\nplt.ylabel('Number of tweets')\nplt.title('Number of tokens v\/s Number of tweets')\nplt.show()","561178bc":"glove_vec_file = open('..\/input\/glove6b\/glove.6B.300d.txt')\nembeddings = {}\nfor line in glove_vec_file:\n    values = line.split()\n    word = values[0]\n    embedding = np.array(values[1:])\n    embeddings[word] = embedding\nglove_vec_file.close()","1b742b8c":"def embeddings_out(data,maxlen=20):\n    output = np.zeros((data.shape[0],20,300))\n    for ix in range(len(data)):\n        curr_len = min(maxlen,len(data.iloc[ix]['tokens']))\n        for jx in range(curr_len):\n            word = str(data.iloc[ix]['tokens'][jx])\n            if word in embeddings:\n                output[ix][jx] = embeddings[data.iloc[ix]['tokens'][jx]]\n    return output","8c0d5262":"X = embeddings_out(data) # Final training dataset to feed into LSTM model.\nX.shape","67165ce5":"X_train = X[:len(train)]\nX_test = X[len(train):]\ny_train = train['target'].values\nlen(X_train),len(X_test),len(y_train)","08879b36":"def create_model():\n    model = Sequential()\n    model.add(LSTM(64,input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(64,input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(64,input_shape=(X.shape[1],X.shape[2])))\n    model.add(Dropout(0.1))\n    model.add(Dense(64,activation='relu'))\n    model.add(Dense(64,activation='relu'))\n    model.add(Activation('softmax'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n    return model","70aa987e":"model = create_model()\nmodel.summary()","b709cbaa":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nhist = model.fit(X_train,y_train,epochs=100,batch_size=64,shuffle=True,validation_split=0.1,callbacks=[early_stop])","4c2cdf2b":"losses = pd.DataFrame(model.history.history)\nlosses.plot()","b8dacb99":"model = create_model()","c23c62b0":"history = model.fit(x=X_train,y=y_train,\n          batch_size=64,epochs=20,shuffle=True)","ae5d3648":"losses = pd.DataFrame(model.history.history)\nlosses.plot()","d32f499e":"pred = (model.predict(X_test) > 0.5).astype(\"int32\")\npred","3830c54f":"result = pd.DataFrame()\nresult['id'] = test['id']\nresult['target'] = pred\nresult.head()","97d36299":"result.to_csv('submission_LSTM.csv',index=False)","61e91b51":"train_data = train.head(7000).copy()\nval_data = train.tail(525).copy()","17e93d4b":"(X_train, y_train), (X_val, y_val), preproc = text.texts_from_df(train_df=train_data,\ntext_column = 'text',label_columns = 'target',val_df = val_data,maxlen = 256,preprocess_mode = 'bert')","fbd2d210":"model = text.text_classifier(name = 'bert',\n                             train_data = (X_train, y_train),\n                             preproc = preproc)","cbb32244":"learner = ktrain.get_learner(model=model, train_data=(X_train, y_train),\n                   val_data = (X_val, y_val),\n                   batch_size = 16)","31b24c13":"learner.fit_onecycle(lr = 2e-5, epochs = 2)\n\npredictor = ktrain.get_predictor(learner.model, preproc)","bdb99f1c":"result = pd.DataFrame()\nresult['id'] = test['id']\nresult['target'] = predictor.predict(test['text'].values)\nresult['target'] = result['target'].map(lambda x:1 if x=='target' else 0)\nresult.head()","88362928":"result.to_csv('submission_bert.csv',index=False)","7448b619":"<a id='6.3'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Glove Vector Embeddings<\/p>\n\n### Here would be using the glove vectors 300 dimensions file.","2b3b1102":"<a id='3.1'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">    Distribution of Target Values<\/p>\n","f9980665":"<a id='6.2'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Tokenization<\/p>\n\n### Here we would combine the training and testing data since tokenization needs to be performed on both the dataset.Also we would further analyze the combined data to draw out more visualizations.","6e6e55ca":"### Thanks alot for reading this notebook.Feedbacks and upvotes are most welcome !!","946e540f":"<a id='1'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-align:center; border-radius: 15px 50px; padding: 2px\">Loading the Libraries<\/p>","7ca22fc9":"<a id='8.3'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Model Training<\/p>","2ccef037":"<a id='7.1'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Model Creation<\/p>\n\n### Here we have used the Stacked LSTMs to get better accuracy as compared to a single LSTM model.","8d913920":"<a id='8'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">Bert Model<\/p>","8ef6569e":"<a id='3.2'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:;  border-radius: 10px 20px; padding: 2px\">Length of tweets v\/s Number of tweets<\/p>","486a7a4d":"<a id='8.4'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Prediction & Evaluation<\/p>","7a98876f":"### This gives us an accuracy of 0.80079.This can be further improved through hyperparameter tuning of the model but for now we would shift to Bert Model to further improve the accuracy of the model.","151c94d4":"<a id='8.1'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Text Preprocessing for Bert<\/p>","76f084e8":"## Training the model with full training data and optimum number of epochs!!","6a57842d":"<a id='6'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">Data Prepocessing<\/p>","c50fe63f":"<a id='7.3'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Prediction & Evaluation<\/p>","f3f8d209":"### Splitting the dataset into training and testing parts.","68bda322":"<a id='7'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">LSTM Model<\/p>","1c9da5c9":"<p style=\"background-color:skyblue;font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:350%; text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px;\">Natural Language Processing with Disaster Tweets<\/p>\n<img src='https:\/\/images.unsplash.com\/photo-1475776408506-9a5371e7a068?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=954&q=80' style='width:500px; border-radius: 100px 0px;'\/>\n<br\/>\n\n## This notebook includes EDA,cleaning followed by LSTM model implemented using Keras and at last a very simplified and concise implementation of the BERT Model using ktrain","3e378929":"<a id='7.2'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Model Training<\/p>","ceb69efe":"### After carefully examining the training dataset, it is seen that there are multiple records having same text and for some of these texts there are contradicting predicitons.Thus we would analyze this and remove duplicate records along with outliers.","0e2d3971":"<a id='3.3'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Number of Words v\/s Number of Tweets<\/p>","5dec3c06":"<a id='6.1'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Removing URL & Special Characters<\/p>","6982ad13":"<a id='5'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">Outliers Analysis<\/p>","0f859fec":"### Since the number of rows having missing keywords is very less, so we would just fill all the missing values with empty string, add this to the end of text and drop the column.\n### Since location is not likely to help the model with its prediction, therefore we would drop this column for now.","ca5b8b36":"### Since the number of records were very less, therefore by manual inspection ,a list is created containing the indices of all the outliers which needs to be removed.","03a1ae81":"# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-align:center; border-radius: 15px 50px; padding: 2px\">Table of Content<\/p>\n\n* [1. Importing Libraries](#1)\n* [2. Loading the Dataset](#2)\n* [3. Data Visualization](#3)\n    * [3.1 Distribution of Target Values](#3.1)\n    * [3.2 Length of tweets v\/s Number of tweets](#3.2)\n    * [3.3 Number of Words v\/s Number of Tweets](#3.3)\n* [4. Missing Values Analysis](#4)\n* [5. Outliers Analysis](#5)\n* [6. Data Prepocessing](#6)\n    * [6.1 Removing URL & Special Characters](#6.1)\n    * [6.2 Tokenization](#6.2)\n    * [6.3 Glove Vector Embeddings](#6.3)\n* [7. LSTM Model](#7)\n    * [7.1 Model Creation](#7.1)\n    * [7.2 Model Training](#7.2)\n    * [7.3 Prediction & Evaluation](#7.3)\n* [8. Bert Model](#8)\n    * [8.1 Text Preprocessing for Bert](#8.1)\n    * [8.2 Model Creation](#8.2)\n    * [8.3 Model Training](#8.3)\n    * [8.4 Prediction & Evaluation](#8.4)","8af8bd1a":"### We would take the same action here as we took for the training dataset.","59523014":"<a id='8.2'><\/a>\n## <p style=\"opacity:0.8; font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:150%; background: linear-gradient(to right, skyblue, blue); text-shadow: 2px 2px 2px #CCCCCC; text-align:; border-radius: 10px 20px; padding: 2px\">Model Creation<\/p>","15554ee2":"<a id='4'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">Missing Values Analysis<\/p>","ddf1a68b":"<a id='3'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">Data Visualization<\/p>","4ee8bce5":"## We would be using early stopping callback and would use 1\/10th of the training data as validation to estimate the optimum number of epochs that would prevent overfitting","0e3fc5dc":"<a id='2'><\/a>\n# <p style=\"font-family:'Myriad Pro', 'Myriad', helvetica, arial, sans-serif;font-size:200%; background: linear-gradient(to right, black, #eee, black); text-shadow: 2px 2px 2px #CCCCCC; text-align:center; border-radius: 15px 50px; padding: 2px\">Loading the Dataset<\/p>"}}