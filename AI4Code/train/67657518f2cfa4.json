{"cell_type":{"5811d49e":"code","52b272f0":"code","ed0b0e8a":"code","543ae1b8":"code","ac943e41":"code","7dc170ff":"code","013db32f":"code","f778adbe":"code","065d8e1c":"code","eed7b75c":"code","a008ef00":"code","59167221":"code","efd6b82d":"code","3f3b4de1":"code","18c3bdf0":"code","c13cfebb":"code","d7fdf9c5":"code","1f6d193c":"code","b977e424":"code","1130ab2a":"code","f7b3df1e":"code","2364d8d8":"code","64170ac3":"code","d3229125":"code","9e6421cc":"code","89400b0d":"code","411df100":"code","6fb24de5":"code","8c6fd311":"code","fc4b22da":"code","20b6087d":"code","3ca14018":"code","439cc8bc":"code","430b67c5":"code","23294615":"code","9e4596d8":"code","4a75d4ff":"code","6737a9fe":"code","9fe14e9a":"code","8d7b918b":"code","888805b1":"code","785e8fde":"code","fb50bfd5":"code","60810468":"code","1886bb3e":"code","530a3919":"code","d19eaf2e":"code","d97e8d7d":"code","2253b1a5":"code","fb3c8c9d":"markdown","66e1a661":"markdown","4cf84df5":"markdown","a40d6c76":"markdown","ed3a816a":"markdown","fdc4abe7":"markdown","ab9a593b":"markdown","f9f02ad7":"markdown","05590d8a":"markdown","6fc8c39a":"markdown","8df4cfe2":"markdown","ce3b7698":"markdown","4cc84e2f":"markdown","a0d52d70":"markdown","301a0a14":"markdown","bf498ab8":"markdown","2a1e57b1":"markdown","15fc7065":"markdown","999d690d":"markdown","5647ba01":"markdown","18b420d8":"markdown","a034988f":"markdown","592b333f":"markdown","48f89c65":"markdown","b6127b5d":"markdown","f9ce5298":"markdown","18a6f8b2":"markdown","ad4211b6":"markdown","259e19ff":"markdown","ccae4eaa":"markdown","187d789f":"markdown","45ebb86d":"markdown","d347103d":"markdown","c11005ab":"markdown","9e308aa2":"markdown","7c48a6f7":"markdown","68aeb92f":"markdown","15569279":"markdown","3573b254":"markdown","c642a751":"markdown","14d896af":"markdown","fe622679":"markdown","03ce2300":"markdown"},"source":{"5811d49e":"# !pip install spacy","52b272f0":"import spacy","ed0b0e8a":"# !python -m spacy download en_core_web_sm","543ae1b8":"# !python -m spacy validate","ac943e41":"# Load the installed model \"en_core_web_sm\"\nnlp = spacy.load(\"en_core_web_sm\")","7dc170ff":"doc = nlp(\"This is a example text\")","013db32f":"# Import the English language class\nfrom spacy.lang.en import English\n\n# Create the nlp object\nnlp = English()\n\n# Process a text\ndoc = nlp(\"Progress to Contributor to make your voice count!\")\n\n# Print the document text\nprint(doc.text)","f778adbe":"# Import the German language class\nfrom spacy.lang.de import German\n\n# Create the nlp object\nnlp = German()\n\n# Process a text (this is German for: \"Kind regards!\")\ndoc = nlp(\"Liebe Gr\u00fc\u00dfe!\")\n\n# Print the document text\nprint(doc.text)","065d8e1c":"# Import the Spanish language class\nfrom spacy.lang.es import Spanish\n\n# Create the nlp object\nnlp = Spanish()\n\n# Process a text (this is Spanish for: \"How are you?\")\ndoc = nlp(\"\u00bfC\u00f3mo est\u00e1s?\")\n\n# Print the document text\nprint(doc.text)","eed7b75c":"# Import the English language class and create the nlp object\nfrom spacy.lang.en import English\nnlp = English()\n\n# Process the text\ndoc = nlp(\"I like tree kangaroos and narwhals.\")\n\n# Select the first token\nfirst_token = doc[0]\n# Print the first token's text\nprint(first_token.text)","a008ef00":"for i in doc:\n    print(i.text)","59167221":"span = doc[2:4]\nprint(span.text)","efd6b82d":"# Import the English language class and create the nlp object\nfrom spacy.lang.en import English\nnlp = English()\n\n# Process the text\ndoc = nlp(\"I like tree kangaroos and narwhals.\")\n\n# A slice of the Doc for \"tree kangaroos\"\ntree_kangaroos = doc[2:4]\nprint(tree_kangaroos.text)\n\n# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\ntree_kangaroos_and_narwhals = doc[2:6]\nprint(tree_kangaroos_and_narwhals.text)","3f3b4de1":"# Import the Span object\nfrom spacy.tokens import Span\n# Create a Doc object\ndoc = nlp(\"I live in Guwahati Assam\")\n# Span for \"Guwahati\" with label GPE (geopolitical)\nspan = Span(doc, 3, 5, label=\"GPE\")\nspan.text","18c3bdf0":"# Process the text\ndoc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n\n# Iterate over the tokens in the doc\nfor token in doc:\n    # Check if the token resembles a number\n    if token.like_num:\n        # Get the next token in the document\n        next_token = doc[token.i + 1]\n        # Check if the next token's text equals '%'\n        if next_token.text == '%':\n            print('Percentage found:', token.text)","c13cfebb":"nlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(\"This is an another example text.\")\n# Coarse-grained part-of-speech tags\n[token.pos_ for token in doc]","d7fdf9c5":"# Fine-grained part-of-speech tags\n[token.tag_ for token in doc]","1f6d193c":"doc = nlp(\"Microsoft News delivers news from the most popular and trusted publishers.\")\nfor i in doc:\n    print(\"{0} \u2013 {1} \u2013 {2}\".format(i.text, i.lemma_, i.pos_))","b977e424":"doc = nlp(\"This is a simple text example.\")\n# Dependency labels\n[token.dep_ for token in doc]","1130ab2a":"# Syntactic head token (governor)\n[token.head.text for token in doc]","f7b3df1e":"doc = nlp(\"Steve Jobs founded Apple\")\n# Text and label of named entity span\n[(ent.text, ent.label_) for ent in doc.ents]","2364d8d8":"doc = nlp(\"It\u2019s official: Apple is the first U.S. public company to reach a $1 trillion market value\")\nfor i in doc.ents:\n    print(i.text + ' ==== ' + i.label_)","64170ac3":"text = \"It\u2019s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n\n# Process the text\ndoc = nlp(text)\n\nfor token in doc:\n    # Get the token text, part-of-speech tag and dependency label\n    token_text = token.text\n    token_pos = token.pos_\n    token_dep = token.dep_\n    # This is for formatting only\n    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))","d3229125":"text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n\n# Process the text\ndoc = nlp(text)\n\n# Iterate over the entities\nfor ent in doc.ents:\n    # print the entity text and label\n    print(ent.text, ent.label_)","9e6421cc":"text = \"New iPhone X release date leaked as Apple reveals pre-orders by mistake\"\n\n# Process the text\ndoc = nlp(text)\n\n# Iterate over the entities\nfor ent in doc.ents:\n    # print the entity text and label\n    print(ent.text, ent.label_)\n\n# Get the span for \"iPhone X\"\niphone_x = doc[1:3]\n\n# Print the span text\nprint('Missing entity:', iphone_x.text)","89400b0d":"doc = nlp(\"This a sentence. This is another one.\")\n# doc.sents is a generator that yields sentence spans\n[sent.text for sent in doc.sents]","411df100":"doc = nlp(\"I have a brown car\")\n# doc.noun_chunks is a generator that yields spans\n[chunk.text for chunk in doc.noun_chunks]","6fb24de5":"spacy.explain(\"NN\")","8c6fd311":"spacy.explain(\"GPE\")","fc4b22da":"from spacy import displacy","20b6087d":"doc = nlp(\"I live in Guwahati, Assam\")\ndisplacy.render(doc, style=\"dep\")","3ca14018":"doc = nlp(\"Bill Gates founded Microsoft\")\ndisplacy.render(doc, style=\"ent\")","439cc8bc":"doc1 = nlp(\"I like cats\")\ndoc2 = nlp(\"I like dogs\")\n# Compare 2 documents\ndoc1.similarity(doc2)","430b67c5":"# Compare 2 tokens\ndoc1[2].similarity(doc2[2])","23294615":"# Compare tokens and spans\ndoc1[0].similarity(doc2[1:3])","9e4596d8":"# Vector as a numpy array\ndoc = nlp(\"I like cats\")\n# The L2 norm of the token's vector\ndoc[2].vector","4a75d4ff":"doc[2].vector_norm","6737a9fe":"nlp = spacy.load(\"en_core_web_sm\")\nnlp.pipe_names","9fe14e9a":"nlp.pipeline","8d7b918b":"# Function that modifies the doc and returns it\ndef custom_component(doc):\n    print(\"Do something to the doc here!\")\n    return doc\n\n# Add the component first in the pipeline\nnlp.add_pipe(custom_component, first=True)","888805b1":"from spacy.tokens import Doc, Token, Span\ndoc = nlp(\"The sky over Guwahati is blue\")","785e8fde":"# Register custom attribute on Token class\nToken.set_extension(\"is_color\", default=False)\n# Overwrite extension attribute with default value\ndoc[5]._.is_color = True","fb50bfd5":"# Register custom attribute on Doc class\nget_reversed = lambda doc: doc.text[::-1]\nDoc.set_extension(\"reversed\", getter=get_reversed)\n# Compute value of extension attribute with getter\ndoc._.reversed","60810468":"# Register custom attribute on Span class\nhas_label = lambda span, label: span.label_ == label\nSpan.set_extension(\"has_label\", method=has_label)\n# Compute value of extension attribute with method\ndoc[3:5]._.has_label(\"GPE\")","1886bb3e":"# Matcher is initialized with the shared vocab\nfrom spacy.matcher import Matcher\n# Each dict represents one token and its attributes\nmatcher = Matcher(nlp.vocab)\n# Add with ID, optional callback and pattern(s)\npattern = [{\"LOWER\": \"new\"}, {\"LOWER\": \"york\"}]\nmatcher.add('CITIES', None, pattern)\n# Match by calling the matcher on a Doc object\ndoc = nlp(\"I live in New York\")\nmatches = matcher(doc)\n# Matches are (match_id, start, end) tuples\nfor match_id, start, end in matches:\n     # Get the matched span by slicing the Doc\n     span = doc[start:end]\n     print(span.text)","530a3919":"doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n\n# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\npattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n\n# Add the pattern to the matcher and apply the matcher to the doc\nmatcher.add('IOS_VERSION_PATTERN', None, pattern)\nmatches = matcher(doc)\nprint('Total matches found:', len(matches))\n\n# Iterate over the matches and print the span text\nfor match_id, start, end in matches:\n    print('Match found:', doc[start:end].text)","d19eaf2e":"doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n\n# Write a pattern that matches a form of \"download\" plus proper noun\npattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n\n# Add the pattern to the matcher and apply the matcher to the doc\nmatcher.add('DOWNLOAD_THINGS_PATTERN', None, pattern)\nmatches = matcher(doc)\nprint('Total matches found:', len(matches))\n\n# Iterate over the matches and print the span text\nfor match_id, start, end in matches:\n    print('Match found:', doc[start:end].text)","d97e8d7d":"doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n\n# Write a pattern for adjective plus one or two nouns\npattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n\n# Add the pattern to the matcher and apply the matcher to the doc\nmatcher.add('ADJ_NOUN_PATTERN', None, pattern)\nmatches = matcher(doc)\nprint('Total matches found:', len(matches))\n\n# Iterate over the matches and print the span text\nfor match_id, start, end in matches:\n    print('Match found:', doc[start:end].text)","2253b1a5":"# \"love cats\", \"loving cats\", \"loved cats\"\npattern1 = [{\"LEMMA\": \"love\"}, {\"LOWER\": \"cats\"}]\n# \"10 people\", \"twenty people\"\npattern2 = [{\"LIKE_NUM\": True}, {\"TEXT\": \"people\"}]\n# \"book\", \"a cat\", \"the sea\" (noun + optional article)\npattern3 = [{\"POS\": \"DET\", \"OP\": \"?\"}, {\"POS\": \"NOUN\"}]","fb3c8c9d":"# Rule-based matching\n## Using the Matcher","66e1a661":"|OP |\tDescription|\n|---|---|\n|! \t|Negate pattern and match exactly 0 times.|\n|? \t|Make pattern optional and match 0 or 1 times.|\n|+ \t|Require pattern to match 1 or more times.|\n|* \t|Allow pattern to match 0 or more times.|","4cf84df5":"## If You like this kernel Don't forget to upvote","a40d6c76":"# Property extensions (with getter & setter)","ed3a816a":"### The Spanish language class","fdc4abe7":"### Loading statistical models","ab9a593b":"## Custom components","f9f02ad7":"# Visualizing\n\n* For Jupyter notebook, use `displacy.render`. \n* Otherwise, use `displacy.serve` to start a web server and show the visualization in your browser.","05590d8a":"# Pipeline components\n\nFunctions that take a Doc object, modify it and return it.","6fc8c39a":"## Operators and quantifiers\n\nCan be added to a token dict as the \"OP\" key.","8df4cfe2":"# Overview of `SpaCy`\nspaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It's designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. SpaCy includes fewer algorithms, but they provide more precise results.","ce3b7698":"## Token patterns","4cc84e2f":"## Creating a span manually","a0d52d70":"## Introduction to Information Extraction\n`Information Extraction (IE)` is a crucial cog in the field of `Natural Language Processing (NLP)` and linguistics. It\u2019s widely used for tasks such as `Question Answering Systems`, `Machine Translation`, `Entity Extraction`, `Event Extraction`, `Named Entity Linking`, `Coreference Resolution`, `Relation Extraction`, etc.\n\nWe can broadly divide Information Extraction into two branches:\n\n- `Traditional Information Extraction`, the relations to be extracted are pre-defined. \n- `Open Information Extraction`, the relations are not pre-defined. The system is free to extract any relations it comes across while going through the text data.","301a0a14":"# Attribute extensions (with default value)","bf498ab8":"## Getting Started\n\nLet's overview the basic NLP procedures and how they are implemented. It is essential to remember that `SpaCy` turns processed text into `objects`.","2a1e57b1":"## Base noun phrases (needs the tagger and parser)","15fc7065":"## Documents, tokens and spans\n### Processing text\n\nProcessing text with the `nlp` object returns a `Doc` object that holds all information about the tokens, their linguistic features and their relationships.","999d690d":"### The German language class","5647ba01":"# Method extensions (callable method)","18b420d8":"Components can be added `first`, `last (default)`, or `before` or `after` an existing component.","a034988f":"## Linguistic features\n\nAttributes return label IDs. For string labels, use the attributes with an underscore. For example, `token.pos_`.\n\n### POS-tagging and Lemmatization\n\nYou can establish the `lemma` for each `token` as well as its `part of speech`. Use the `token.lemma_` method for lemmas and the `token.pos_` method for parts of speech.","592b333f":"### Check that your installed models are up to date","48f89c65":" ### Tokenization\n\n`SpaCy` automatically divides your document into tokens when you use a `for-loop`.","b6127b5d":"## Label explanations","f9ce5298":"## Spans\n### Accessing spans\n\nSpan indices are exclusive. So `doc[2:5]` is a span starting at token 2, up to \u2013 but not including! \u2013 token 5.","18a6f8b2":"# Accessing word vectors","ad4211b6":"## Different Approaches to Information Extraction\nThere are multiple approaches to perform information extraction automatically. Let\u2019s understand them:\n\n- **Rule-based Approach:** We define a set of rules for the syntax and other grammatical properties of a natural language and then use these rules to extract information from text\n\n- **Supervised:** Let\u2019s say we have a sentence S. It has two entities E1 and E2. Now, the supervised machine learning model has to detect whether there is any relation (R) between E1 and E2. So, in a supervised approach, the task of relation extraction turns into the task of relation detection. The only drawback of this approach is that it needs a lot of labeled data to train a model\n\n- **Semi-supervised:** When we don\u2019t have enough labeled data, we can use a set of seed examples (triples) to formulate high-precision patterns that can be used to extract more relations from the text.\n","259e19ff":"### NLTK vs SpaCy\n|Criteria| \tNLTK |\tSpaCy|\n|---------|-------|-------------|\n|Multi-Language Support |\tYes |\tYes|\n|Types of Inputs and Outputs |\tStrings |\tObjects|\n|Word Vector Support| \tNo \t|Yes|\n|Performance |\tSlow |\tFast|\n|Target Audience |\tResearchers |\tDevelopers|","ccae4eaa":"## Statistical models\nSpaCy contains lots of models in different languages: English, Spanish, German, Chinese, French, etc. The whole list is available [on the official SpaCy site](https:\/\/spacy.io\/models).\n\nwe will work with the `en_core_web_sm` and `en_core_web_lg` model that was trained on blogs, news, and comments. It is used for POS-tagging, named entity recognition, and other tasks. You can install the model from the command line, as shown below:","187d789f":"Note: This Kernel is based on datacamp [spaCy Cheat Sheet: Advanced NLP in Python](https:\/\/www.datacamp.com\/community\/blog\/spacy-cheatsheet) article","45ebb86d":"## Syntactic dependencies (predicted by statistical model)","d347103d":"## Installation\n\nYou need to install SpaCy first; use pip for this:","c11005ab":"## Visualize dependencies","9e308aa2":"# Word vectors and similarity\n\n- To use word vectors, you need to install the larger models ending in md or lg , for example `en_core_web_md`.\n## Comparing similarity","7c48a6f7":"## Glossary\n|Name| \tDescription|\n|---|---|\n|Tokenization |\tSegmenting text into words, punctuation etc.|\n|Lemmatization |\tAssigning the base forms of words, for example: \"was\" \u2192 \"be\" or \"rats\" \u2192 \"rat\".|\n|Sentence Boundary Detection |\tFinding and segmenting individual sentences.|\n|Part-of-speech (POS) Tagging | \tAssigning word types to tokens like verb or noun.|\n|Dependency Parsing |\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n|Named Entity Recognition (NER) |\tLabeling named \"real-world\" objects, like persons, companies or locations.|\n|Text Classification |\tAssigning categories or labels to a whole document, or parts of a document.|\n|Statistical model |\tProcess for making predictions based on examples.|\n|Training |\tUpdating a statistical model with new examples.|","68aeb92f":"## Pipeline information","15569279":"# Extension attributes\n\nCustom attributes that are registered on the global Doc, Token and Span classes and become available as `._.`","3573b254":"You also need to import it afterward. ","c642a751":"## Visualize named entities","14d896af":"### Named Entity Recognition\n\nA named entity is an object of the real world, for example, a person, an organization, etc. `SpaCy` can recognize different named entities in a document by forecasting it with the help of a built-in model. The standard way of named entity recognition (NER) is `doc.ents`.","fe622679":"## Sentences (usually needs the dependency parser)","03ce2300":"### The English language class"}}