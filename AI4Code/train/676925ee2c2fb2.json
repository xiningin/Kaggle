{"cell_type":{"4d05cd1a":"code","15733044":"code","8aea808c":"code","54bf091a":"code","203fb006":"code","23663083":"code","32eb9aae":"code","41911464":"code","2de89943":"code","ff8c93f6":"code","90207b44":"markdown","556d48fc":"markdown","3ea7167e":"markdown","40e84a8e":"markdown"},"source":{"4d05cd1a":"!pip install stable-baselines3","15733044":"import matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common import logger, results_plotter\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n\nfrom shutil import copyfile\nimport os\n\nfrom stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecTransposeImage\nfrom stable_baselines3.common.monitor import Monitor, load_results\n\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import evaluate, make","8aea808c":"EED = 17\nNUM_TIMESTEPS = int(1e7)\nEVAL_FREQ = int(1e4)\nEVAL_EPISODES = int(1e2)\nBEST_THRESHOLD = 0.01 # must achieve a mean score above this to replace prev best self\n\nREWARD_LOST = -1\nREWARD_WON = 1\n\nN_CPU = os.cpu_count()\n\nLOGDIR = os.path.join(\".\",\"logs\",\"custom_ppo_1\")\nMONITOR_LOGS_DIR = os.path.join(LOGDIR,\"monitor_logs\")\nTB_LOGS_DIR = os.path.join(LOGDIR,\"tensorboard_logs\")\nMODEL_DIR = os.path.join(LOGDIR,\"model\")\nCHECKPOINTS_DIR = os.path.join(LOGDIR,\"checkpoints\")","54bf091a":"if not os.path.exists(LOGDIR): \n    os.makedirs(LOGDIR)\nif not os.path.exists(TB_LOGS_DIR):\n    os.makedirs(TB_LOGS_DIR)\nif not os.path.exists(MONITOR_LOGS_DIR):\n    os.makedirs(MONITOR_LOGS_DIR)\nif not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\nif not os.path.exists(CHECKPOINTS_DIR):\n    os.makedirs(CHECKPOINTS_DIR)","203fb006":"class HungryGeeseEnv(gym.Env):\n    \n    def __init__(self, opponents=['random','greedy','greedy-goose.py'], debug=False, warmup_episode_count = 100, warmup_timesteps=5000):\n        super(HungryGeeseEnv, self).__init__()\n        self.opponents = opponents\n        self.opponents_old_lengths = [1 for _ in range(0,len(opponents))]\n        self.opponents_new_lengths = [1 for _ in range(0,len(opponents))]\n        self.env = make(\"hungry_geese\") #, debug=self.debug)\n        self.config = self.env.configuration\n        self.trainer = self.env.train([None, *opponents])\n        \n        self.action_space = spaces.Discrete(4)        \n        self.observation_space = spaces.Box(low=0, high=255\n                                            , shape=(self.config.rows, self.config.columns, 3)\n                                            , dtype=np.uint8) \n        self.reward_range = (-1, 1000)  #TODO why this range?\n        self.last_vert_actions_count = 0\n        self.last_horz_actions_count = 0\n        self.last_action = -1\n        \n        self.episode_count = 0\n        self.timesteps = 0\n        self.warmup_episode_count = warmup_episode_count\n        self.warmup_timesteps = warmup_timesteps\n        \n    def update_opponent_lengths(self, obs):\n        self.opponents_old_lengths = self.opponents_new_lengths\n        self.opponents_new_lengths = [len(geese) for geese in obs[0]['geese'][1:]] \n\n    def update_last_actions(self, action):\n        if self.last_action == action:\n            if action == 0 or action == 3:\n                self.last_vert_actions_count += 1 #counts last consecutive vertical actions\n            else:\n                self.last_horz_actions_count += 1 #counts last consecutive horizontal actions\n        else:\n            if action == 0 or action == 3:\n                self.last_vert_actions_count = 1\n                self.last_horz_actions_count = 0\n            else:\n                self.last_horz_actions_count = 1\n                self.last_vert_actions_count = 0   \n\n    def shape_reward(self, reward):   \n        geese_length_diff = np.array(self.opponents_new_lengths) - np.array(self.opponents_old_lengths)\n        total_geese_length_increase = (geese_length_diff > 0).sum()\n        reward -= total_geese_length_increase * 10\n\n        # prevent agent from taking straight trajectory\n        if self.last_horz_actions_count > 11:\n            reward -= 10\n        if self.last_vert_actions_count > 7:\n            reward -= 10\n\n        return reward\n    \n    def step(self, action):\n        self.update_last_actions(action)\n        my_action = self.transform_action(action)\n        \n        self.timesteps += 1        \n        \n        #opponent_actions = self.transform_actions(action[1:])  #TODO \n        #self.obs = self.env.step([my_action, *opponent_actions])  #TODO      \n        self.obs = self.trainer.step(my_action)  #TODO    \n        self.update_opponent_lengths(self.obs)       \n        x_obs = self.transform_step_observation(self.obs, self.config)\n        # x_reward = self.obs[0].reward\n        # done = (self.obs[0][\"status\"] != \"ACTIVE\")\n        # info = self.obs[0][\"info\"]\n        x_reward = self.obs[1]\n        x_reward = self.shape_reward(x_reward)\n        done = self.obs[2]\n        info = self.obs[3]\n\n        return x_obs, x_reward, done, info\n        \n    def reset(self):\n        self.episode_count += 1\n        self.obs = self.trainer.reset()\n        x_obs = self.transform_observation(self.obs, self.config)\n        return x_obs\n    \n    def load_new_opponents_from_best_model(self):\n        if self.episode_count < self.warmup_episode_count or self.timesteps < self.warmup_timesteps:\n            return True\n        \n        print(\"Loading new opponents from current best model for self play!!!\")\n        loaded_model = PPO.load(os.path.join(MODEL_DIR, \"best_model\")) \n\n        def agent_ppo(obs, config):\n            obs = self.transform_observation(obs, self.config)\n            return self.transform_action(loaded_model.predict(obs, deterministic=True)[0])\n        \n        self.opponents = [agent_ppo]*len(self.opponents)\n        self.trainer = self.env.train([None, *self.opponents])\n        self.reset()\n        \n    def transform_actions(self, actions):\n        _actions = []\n        for action in actions:\n            _actions.append(self.transform_action(action))\n        return _actions\n        \n    def transform_action(self, action):\n        if action == 0:\n            return \"NORTH\"\n        if action == 1:\n            return \"EAST\"\n        if action == 2:\n            return \"WEST\"\n        if action == 3:\n            return \"SOUTH\"\n        \n    def transform_step_observation(self, obs, config):\n        my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n\n        for goose_cell in obs[0].geese[0]:\n            my_board[goose_cell] = 255\n        my_board = my_board.reshape((config.rows, config.columns, 1))\n\n        for goose in obs[0].geese[1:]:\n            for goose_cell in goose:\n                their_board[goose_cell] = 255\n        their_board = their_board.reshape((config.rows, config.columns, 1))\n        \n        for food_cell in obs[0].food:\n            food_board[food_cell] = 255\n        food_board = food_board.reshape((config.rows, config.columns, 1))\n        board = np.concatenate([my_board, their_board, food_board], axis = -1)\n        return board\n\n    def transform_observation(self, obs, config):\n        my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n        food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n\n        for goose_cell in obs.geese[0]:\n            my_board[goose_cell] = 255\n        my_board = my_board.reshape((config.rows, config.columns, 1))\n\n        for goose in obs.geese[1:]:\n            for goose_cell in goose:\n                their_board[goose_cell] = 255\n        their_board = their_board.reshape((config.rows, config.columns, 1))\n        \n        for food_cell in obs.food:\n            food_board[food_cell] = 255\n        food_board = food_board.reshape((config.rows, config.columns, 1))\n        board = np.concatenate([my_board, their_board, food_board], axis = -1)\n        return board","23663083":"# Unused. Useful for vectorised environments \n\ndef make_monitored_gym(rank =0): #TODO pass config\n    def _init():\n        env = HungryGeeseEnv() #TODO pass config\n        #LOGDIR = \"ppo_selfplay\"\n        log_file = os.path.join(LOGDIR, str(rank))\n        env = Monitor(env, log_file, allow_early_resets=True) #TODO  allow_early_resets\n        return env\n    return _init\n\ndef make_gym(rank =0): #TODO pass config\n    def _init():\n        env = HungryGeeseEnv() #TODO pass config\n        #LOGDIR = \"ppo_selfplay\"\n        #log_file = os.path.join(LOGDIR, str(rank))\n        #env = Monitor(env, log_file, allow_early_resets=True) #TODO  allow_early_resets\n        return env\n    return _init","32eb9aae":"env = Monitor(HungryGeeseEnv())\n\nclass LoadNewOpponentsFromBestModelCallback(BaseCallback):\n    def __init__(self, env, verbose: int = 0):\n        super(LoadNewOpponentsFromBestModelCallback, self).__init__(verbose=verbose)\n        self.env = env\n\n    #def __init_callback(self)\n\n    def _on_step(self):\n        env.load_new_opponents_from_best_model()\n        return True\n    \nload_new_opponents_from_best_model_callback = LoadNewOpponentsFromBestModelCallback(env)\n\ncheckpoint_callback = CheckpointCallback(save_freq=1000, save_path=CHECKPOINTS_DIR,\n                                         name_prefix=\"rl_model\")\n\neval_env = VecTransposeImage(DummyVecEnv([lambda:Monitor(HungryGeeseEnv())]))\neval_callback = EvalCallback(eval_env, best_model_save_path=MODEL_DIR,\n                             log_path=LOGDIR, eval_freq=20,\n                             deterministic=True, render=False\n                            , callback_on_new_best=load_new_opponents_from_best_model_callback)\n\nmodel = PPO(policy = 'MlpPolicy'\n                , env = env\n                , verbose = 1\n                , n_steps = 2048*16\n                , batch_size = 128\n                , n_epochs = 50\n                #, tb_log_name = \"ppo_vs_ppo_bfs\" #TODO check if works\n                , tensorboard_log = TB_LOGS_DIR\n                , learning_rate = .01)\n\nmodel.learn(total_timesteps=100000, callback=[checkpoint_callback, eval_callback])","41911464":"def transform_observation(obs, config):\n    my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n\n    for goose_cell in obs.geese[0]:\n        my_board[goose_cell] = 255\n    my_board = my_board.reshape((config.rows, config.columns, 1))\n\n    for goose in obs.geese[1:]:\n        for goose_cell in goose:\n            their_board[goose_cell] = 255\n    their_board = their_board.reshape((config.rows, config.columns, 1))\n\n    for food_cell in obs.food:\n        food_board[food_cell] = 255\n    food_board = food_board.reshape((config.rows, config.columns, 1))\n    board = np.concatenate([my_board, their_board, food_board], axis = -1)\n    return board\n    \ndef transform_actions(actions):\n    if actions == 0:\n        return \"NORTH\"\n    if actions == 1:\n        return \"EAST\"\n    if actions == 2:\n        return \"WEST\"\n    if actions == 3:\n        return \"SOUTH\"","2de89943":"MODEL = os.path.join(LOGDIR,\"model\",\"best_model\")\nSAVE_MODEL = os.path.join(LOGDIR,\"model\",\"last_model\")\nSTATE_DICT = os.path.join(LOGDIR,\"state_dict\")\nloaded_model = PPO.load(MODEL)\nloaded_model.save(SAVE_MODEL)\nprint(loaded_model.policy)\nprint(loaded_model.policy.to('cpu').state_dict())\n\nimport torch\ntorch.save(loaded_model.policy.to('cpu').state_dict(), STATE_DICT)\n","ff8c93f6":"MODEL = os.path.join(LOGDIR,\"model\",\"best_model\")\nprint(MODEL)\nloaded_model = PPO.load(MODEL)\n#print(loaded_model.policy)\nenv = make('hungry_geese', debug=True)\n\ndef agent_ppo(obs, config):\n    obs = transform_observation(obs, env.configuration)\n    #return directions[loaded_model.predict(obs)[0]]\n    return transform_actions(loaded_model.predict(obs, deterministic=True)[0])\n    \n# env.run([agent_ppo,'random','greedy'])\n# env.render(mode=\"ipython\")\n\nfrom kaggle_environments import evaluate\n\nevaluate(\n    \"hungry_geese\",\n    ['random',agent_ppo,'greedy'],\n    num_episodes=10\n)","90207b44":"# Custom Environment ","556d48fc":"\n\n# Imports ","3ea7167e":"[Original NB](https:\/\/www.kaggle.com\/maheshabnave999\/hungry-geese-self-play-agent-using-stable-baseli#Custom-Environment)","40e84a8e":"This kernel attempts to train PPO agent with self play using stable-baselines3 library.\n`EvalCallback` is specified to evaluate model every 20 timesteps. And if better model is formed, it is loaded as opponent using child callback `LoadNewOpponentsFromBestModelCallback`. This callback calls `HungryGeeseEnv.load_new_opponents_from_best_model` to actually load new model as opponents.\nCode works as is. However, there is scope for performance improvement. **Please help me out to improve performance and indicating what I might be missing here.**\n\n(Action and observation transformation functions are referenced from [this](https:\/\/www.kaggle.com\/ryches\/stable-baselines3-starter-wip) kernel.)\n\n**TODO**\n\n- Try increasing Policy NN layers\n\n\n"}}