{"cell_type":{"efce5223":"code","a3d29e8d":"code","713b26c8":"code","be400aed":"code","528a788b":"code","ad390b63":"code","a15e3800":"code","005cc710":"code","8b1845d7":"code","6683220e":"code","0b201f38":"code","f9e9d9e1":"code","ffcc0c73":"code","76ea9b7e":"code","a89ad981":"code","e1ecfdbf":"code","ae8eba80":"code","fac5b0f9":"code","aa8e4dfd":"code","c8ba7147":"code","9ca26e49":"code","85a97513":"code","e71cf9dd":"code","0c03bfd0":"code","eb2d8f77":"code","61b483a8":"code","81d5d602":"code","a794c206":"code","0dd16fd5":"code","102600af":"code","2df0e7f0":"code","c25e218f":"code","5116944d":"markdown","00858d7a":"markdown","463af190":"markdown","4a4870ea":"markdown","3dc24bf3":"markdown","64a40ed1":"markdown","443fc615":"markdown"},"source":{"efce5223":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3d29e8d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score","713b26c8":"df = pd.read_csv('\/kaggle\/input\/mines-vs-rocks\/sonar.all-data.csv', header=None)","be400aed":"df","528a788b":"df.describe().T","ad390b63":"df.info()","a15e3800":"fig, axs = plt.subplots(figsize=(10, 8))\nsns.countplot(df[60], ax=axs)\nplt.show()","005cc710":"df.hist(figsize=(15, 10))","8b1845d7":"plt.figure(figsize=(15, 10))\nsns.heatmap(df.corr())","6683220e":"X = df.drop(columns=60).values\ny = df[60]\ny = y.map({'R' : 0, 'M' : 1}).values","0b201f38":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","f9e9d9e1":"classifiers = [('KNN', KNeighborsClassifier()), \n               ('SVC', SVC()), \n               ('GPC', GaussianProcessClassifier()), \n               ('DTC', DecisionTreeClassifier()), \n               ('RFC', RandomForestClassifier()), \n               ('MLPC', MLPClassifier()), \n               ('ABC', AdaBoostClassifier()), \n               ('GNB', GaussianNB()), \n               ('QDA', QuadraticDiscriminantAnalysis()), \n               ('LDA', LinearDiscriminantAnalysis()), \n               ('LR', LogisticRegression())]","ffcc0c73":"results = []\nnames = []\nscoring = 'accuracy'\n\nfor name, classifier in classifiers:\n    kfold = KFold(n_splits=10, shuffle=True)\n    cv_score = cross_val_score(estimator=classifier, X=X_train, y=y_train, scoring=scoring)\n    results.append(cv_score)\n    names.append(name)\n    print('Classifier: {}, Mean Accuracy: {}, StDev: {}'.format(name, cv_score.mean(), cv_score.std()))","76ea9b7e":"fig, axs = plt.subplots(figsize=(15, 10))\naxs.boxplot(results)\naxs.set_xticklabels(names, fontdict={'size' : 18})\nplt.show()","a89ad981":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","e1ecfdbf":"results_scaled_data = []\nnames = []\n\nfor name, model in classifiers:\n    kfold = KFold(n_splits=10, shuffle=True)\n    cv_score = cross_val_score(estimator=model, X=X_train_scaled, y=y_train, scoring='accuracy')\n    results_scaled_data.append(cv_score)\n    names.append(name)\n    print('Model: {}, Mean Score: {}, Score StDev: {}'.format(name, cv_score.mean(), cv_score.std()))","ae8eba80":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 12), sharex=True, sharey=True)\naxs[0].boxplot(results_scaled_data)\naxs[0].set_title('Standardised Data', fontdict={'size' : 18})\naxs[0].set_xticklabels(names)\naxs[0].grid(linestyle='dashed', linewidth=0.2, color='black')\n\naxs[1].boxplot(results)\naxs[1].set_title('Non-Standardised Data', fontdict={'size' : 18})\naxs[1].grid(linestyle='dashed', linewidth=0.2, color='black')","fac5b0f9":"svc = SVC()\n\nparam_grid = {'C' : [1, 10, 100, 1000], 'kernel' : ['linear', 'rbf', 'sigmoid'], \n              'gamma' : ['scale', 'auto', 1.0, 0.1, 0.01, 0.001, 0.0001]}\n\nkfold = KFold(n_splits=10, shuffle=True)\ngsearch = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=kfold)\ngsearch.fit(X_train_scaled, y_train)\ngsearch.best_params_","aa8e4dfd":"svc = SVC(**gsearch.best_params_)\nsvc.fit(X_train_scaled, y_train)\nsvc_preds = svc.predict(X_test_scaled)","c8ba7147":"svc_cm = confusion_matrix(y_test, svc_preds)\n\nsns.heatmap(svc_cm, annot=True)","9ca26e49":"svc_clf_report = classification_report(y_test, svc_preds)\nprint(svc_clf_report)","85a97513":"mlpc = MLPClassifier()\n\nmlpc_grid = {'hidden_layer_sizes' : [(100,), (100, 100), (150, 150), (200, 200)], \n            'alpha' : [0.0001, 0.001, 0.01, 0.1], \n            'learning_rate' : ['constant', 'invscaling', 'adaptive']}\n\nmlpc_kfold = KFold(n_splits=10, shuffle=True)\nmlpc_search = GridSearchCV(estimator=mlpc, param_grid=mlpc_grid, scoring='accuracy', cv=mlpc_kfold)\nmlpc_search.fit(X_train_scaled, y_train)\nmlpc_search.best_params_","e71cf9dd":"mlp_clf = MLPClassifier(**mlpc_search.best_params_)\nmlp_clf.fit(X_train_scaled, y_train)","0c03bfd0":"mlpc_preds = mlp_clf.predict(X_test_scaled)","eb2d8f77":"mlpc_cm = confusion_matrix(y_test, mlpc_preds)\nsns.heatmap(mlpc_cm, annot=True)","61b483a8":"mlpc_clf_report = classification_report(y_test, mlpc_preds)\nprint(mlpc_clf_report)","81d5d602":"gpc = GaussianProcessClassifier()\n\ngpc_param_grid = {'optimizer' : ['fmin_l_bfgs_b', None], 'n_restarts_optimizer' : [0, 1, 3, 5, 9], \n                 'max_iter_predict' : [50, 100, 200, 500, 1000]}\n\ngpc_kfold = KFold(n_splits=10, shuffle=True)\ngpc_grid_search = GridSearchCV(estimator=gpc, param_grid=gpc_param_grid, scoring='accuracy', cv=gpc_kfold)\ngpc_grid_search.fit(X_train_scaled, y_train)\nprint(gpc_grid_search.best_params_)","a794c206":"gp_clf = GaussianProcessClassifier(**gpc_grid_search.best_params_)\ngp_clf.fit(X_train_scaled, y_train)","0dd16fd5":"gpc_preds = gp_clf.predict(X_test_scaled)","102600af":"gpc_cm = confusion_matrix(y_test, gpc_preds)\nsns.heatmap(gpc_cm, annot=True)","2df0e7f0":"gpc_cr = classification_report(y_test, gpc_preds)\nprint(gpc_cr)","c25e218f":"print('SVC recall score: {}'.format(recall_score(y_test, svc_preds)))\nprint('MLPClassifier recall score: {}'.format(recall_score(y_test, mlpc_preds)))\nprint('GaussianProcessClassifier recall score: {}'.format(recall_score(y_test, gpc_preds)))","5116944d":"### This is a binary classification problem\n### 'R' is for the class Rock and 'M' is for class Mine","00858d7a":"# We do not want our soldiers blown up by live mines. So we choose the model with the highest (positive class) recall score","463af190":"# Connectionist Bench (Sonar, Mines vs. Rocks) Data Set\n\n## Source:\n\n### The data set was contributed to the benchmark collection by Terry Sejnowski, now at the Salk Institute and the University of California at San Deigo. The data set was developed in collaboration with R. Paul Gorman of Allied-Signal Aerospace Technology Center.\n\n\n## Data Set Information:\n\n### The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file \"sonar.rocks\" contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.\n\n### Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.\n\n### The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.\n\n## Source: University of California ML Repository\n### https:\/\/archive.ics.uci.edu\/ml\/datasets\/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)","4a4870ea":"### It looks like standardised data is producing better results, as the accuracy score distributions are a bit higher than the non-standardised data. \n## Next, we pick three best performing models, GPC (Gaussian Process Classifier), MPLC (MPL Classifier) and SVC (Support Vector Classifier), and attempt to improve results further by parameter tuning.","3dc24bf3":"# Tuning SVC","64a40ed1":"# Tuning and applying MLP Classifier ","443fc615":"# Gaussian Process Classifier "}}