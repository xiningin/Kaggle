{"cell_type":{"d030e682":"code","a6c086fb":"code","31ab7777":"code","c7282b27":"code","da49f6dc":"code","67a1025f":"code","127f32f5":"code","a0cc60de":"code","aa440556":"code","e50e37ee":"code","93b8671e":"code","bc319adc":"code","d40c8749":"code","56d9f3bd":"code","63656160":"code","e03abfbd":"code","41b32c80":"code","5247f4c8":"code","52c374c0":"code","3f70ff5e":"code","31c8fb53":"code","a9f7ae7a":"markdown","7fe39c15":"markdown","64acc169":"markdown","6c3f1cc6":"markdown","6051cfbe":"markdown"},"source":{"d030e682":"import json\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split,KFold, GroupKFold,StratifiedKFold\n\nimport tensorflow.keras as keras\n\nimport gc\nimport matplotlib.pyplot as plt\nimport os\n\nfrom tqdm.notebook import tqdm","a6c086fb":"\n\nLOSS_WGTS = [0.3, 0.3, 0.3, 0.05, 0.05] #column weights, need to sum up to 1\n\n\nDIST_NEW = True\nDIST_NEW2 = True\n\nBBP = True\nBBP1 = True\nBBP2 = True\nBBP3 = True\nBBP4 = True\n\nBBP_TOTAL = BBP+BBP1+BBP2+BBP3+BBP4*4\n\n# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']","31ab7777":"sub = pd.read_csv(\"..\/input\/stanford-covid-vaccine\/post_deadline_files\/new_sequences_submission.csv\")","c7282b27":"df = pd.read_csv(\"..\/input\/stanford-covid-vaccine\/post_deadline_files\/new_sequences.csv\")\ndf.head()","da49f6dc":"df['seq_length'] = df['sequence'].apply(lambda x: len(x))\ndf.sort_values(by='seq_length')","67a1025f":"def pandas_list_to_array(df):\n    \"\"\"\n    Input: dataframe of shape (x, y), containing list of length l\n    Return: np.array of shape (x, l, y)\n    \"\"\"\n    \n    return np.transpose(\n        np.array(df.values.tolist()),\n        (0, 2, 1)\n    )\n\n\ndef calc_neighbor(d, dim, n):\n    lst_x,lst_y = np.where(d==n)\n    for c, x in enumerate(lst_x):\n        y = lst_y[c]    \n        if x+1<dim:\n            d[x+1,y] = min(d[x+1,y], n+1)\n        if y+1<dim:\n            d[x,y+1] = min(d[x,y+1], n+1)\n        if x-1>=0:\n            d[x-1,y] = min(d[x-1,y], n+1)\n        if y-1>=0:\n            d[x,y-1] = min(d[x,y-1], n+1)\n    return d\n            \n\ndef get_distance_matrix_2d(Ss):\n    Ds = []\n    n = Ss.shape[0]\n    dim = Ss.shape[1]\n    for i in range(n):\n        s = Ss[i,:,:,0]\n        d = 10+np.zeros_like(s)\n        d[s==1] = 1\n        for i in range(dim):\n            d[i,i] = 0\n        for x in range(0, 9):\n            d = calc_neighbor(d, dim, x)\n        Ds.append(d)\n    Ds =  np.array(Ds) + 1\n    Ds = 1\/Ds\n    Ds = Ds[:, :,:, None]\n    \n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    return Ds[:,:,:,:,0]","127f32f5":"# loss functions\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=(1))\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\n\nfrom tensorflow.keras import losses\n\nclass MSE(losses.MeanSquaredError):\n    def __init__(self, *args, **kwargs):\n        losses.MeanSquaredError.__init__(self, *args, **kwargs)\n\n    def __call__(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n        \n        temp = losses.MeanSquaredError.__call__(self, y_true[:, :, 0], y_pred[:, :, 0], sample_weight=None)\n        temp = tf.sqrt(temp+1e-12)\n        temp = tf.tensordot(temp,sample_weight,1)\/tf.reduce_sum(sample_weight)\n        s = temp*LOSS_WGTS[0]\n#         s = tf.sqrt(temp)*LOSS_WGTS[0]\n        for i in range(1,5):\n            temp = losses.MeanSquaredError.__call__(self, y_true[:, :, i], y_pred[:, :, i], sample_weight=None)\n            temp = tf.sqrt(temp+1e-12)\n            temp = tf.tensordot(temp,sample_weight,1)\/tf.reduce_sum(sample_weight)\n#             s += tf.sqrt(temp)*LOSS_WGTS[i]\n            s += (temp)*LOSS_WGTS[i]\n            \n        return s\n    \n    \nfrom sklearn.metrics import mean_squared_error\n\ndef mean_squared_error1(y_true, y_pred, sample_weight):\n    return np.sum((np.sqrt(np.mean((y_true-y_pred)**2, axis=1)))*sample_weight)\/np.sum(sample_weight)\n\ndef MCRMSE_NAN_sample_wgt(y_true, y_pred, sample_weight=None, loss_cap=None):\n    if loss_cap is not None:\n        y_true_adj = np.minimum(np.maximum(y_true, y_pred-loss_cap), y_pred+loss_cap)\n        return MCRMSE_NAN_sample_wgt(y_true_adj, y_pred, sample_weight=sample_weight, loss_cap=None)\n    \n    y_wgt = tf.ones_like(y_true)\n    y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n\n    s = (mean_squared_error1(y_true[:, :, 0], y_pred[:, :, 0], sample_weight=sample_weight)\/(tf.reduce_mean(y_wgt[:,:, 0])))*LOSS_WGTS[0]\n    for i in range(1,5):\n        s += (mean_squared_error1(y_true[:, :, i], y_pred[:, :, i], sample_weight=sample_weight)\/(tf.reduce_mean(y_wgt[:,:, i])))*LOSS_WGTS[i]\n    return s\n\ndef MCRMSE_NAN_sample_wgt_single(y_true, y_pred, sample_weight=None, loss_cap=None):\n    if loss_cap is not None:\n        y_true_adj = np.minimum(np.maximum(y_true, y_pred-loss_cap), y_pred+loss_cap)\n        return MCRMSE_NAN_sample_wgt_single(y_true_adj, y_pred, sample_weight=sample_weight, loss_cap=None)\n        \n    y_wgt = tf.ones_like(y_true)\n    y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true)\n\n    s = (mean_squared_error1(y_true[:, :], y_pred[:, :], sample_weight=sample_weight)\/(tf.reduce_mean(y_wgt[:,:])))\n    return s\n\n\ndef MCRMSE_NAN(y_true, y_pred, wgt=LOSS_WGTS, loss_cap=None):\n    return MCRMSE_NAN_sample_wgt(y_true, y_pred, sample_weight=tf.ones_like(y_true[:,0,0]), loss_cap=loss_cap)","a0cc60de":"# reverse inputs\ndef reverse_input(train_input):\n    reverse = train_input[:, ::-1, :]\n    return reverse\n\ndef reverse_BBP_3D(mat):\n    return mat[:, ::-1, ::-1,:]","aa440556":"# from https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(\n        L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal')\n    )\n\n\ndef lstm_layer(hidden_dim, dropout):\n    return L.Bidirectional(\n              L.LSTM(hidden_dim,dropout=dropout, return_sequences=True,kernel_initializer = 'orthogonal'))\n\n","e50e37ee":"# from https:\/\/www.kaggle.com\/ragnar123\/wavenet-gru-baseline\n\ndef wave_block(x, filters, kernel_size, n):\n    dilation_rates = [2 ** i for i in range(n)]\n    x = tf.keras.layers.Conv1D(filters = filters, \n                               kernel_size = 1,\n                               padding = 'same')(x)\n    res_x = x\n    for dilation_rate in dilation_rates:\n        tanh_out = tf.keras.layers.Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same', \n                          activation = 'tanh', \n                          dilation_rate = dilation_rate)(x)\n        sigm_out = tf.keras.layers.Conv1D(filters = filters,\n                          kernel_size = kernel_size,\n                          padding = 'same',\n                          activation = 'sigmoid', \n                          dilation_rate = dilation_rate)(x)\n        x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n        x = tf.keras.layers.Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = tf.keras.layers.Add()([res_x, x])\n    return res_x","93b8671e":"# main model edited from https:\/\/www.kaggle.com\/mrkmakr\/covid-ae-pretrain-gnn-attn-cnn\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) \/ np.sqrt(n_factor))([x_Q, x_KT])\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor \/\/ n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.Dropout(rate)(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config, dim=None):\n    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n    \n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128*2, kernel = 3, rate = 0.1)\n    x2 = forward(x1, 64*2, kernel = 6, rate = 0.1)\n    x3 = forward(x2, 32*2, kernel = 15, rate = 0.1)\n    x4 = forward(x3, 16*2, kernel = 30, rate = 0.1)\n    x = L.Concatenate()([x1, x2, x3, x4])\n    \n    for unit in [64*2, 32*2]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\ndef get_ae_model(base, config, dim=None):\n    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64*2, rate = 0.2)\n    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n    \n\n    node_1 = tf.where((node>1-1e-8), node, tf.zeros_like(node))\n    node_0 = tf.where((node<1e-8), node, tf.ones_like(node))\n    node_float = tf.where((node<=1-1e-8)&(node>=1e-8), node, p) \n    \n    loss = - tf.reduce_mean(20 * node_1 * tf.math.log(p + 1e-4) + (1 - node_0) * tf.math.log(1 - p + 1e-4) - 5*(node_float-p)**2)\n    \n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\n\ndef get_model(base, config, dim=None):\n    node = tf.keras.Input(shape = (dim, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (dim, dim, As.shape[3]), name = \"adj\")\n    \n    x = base([node, adj])\n    if not Diversity_type in ['forward']:\n        x = forward(x, 128*2, rate = 0.2)\n    \n    if Diversity_type == 'gru':\n        x = gru_layer(128*2, dropout=0.2)(x)\n    elif Diversity_type == 'lstm':\n        x = lstm_layer(128*2, dropout=0.2)(x)\n    elif Diversity_type == 'forward':\n        x = forward(x, 128*4, kernel=5, rate = 0.1)\n        x = forward(x, 128*4, kernel=3, rate = 0.1)\n        x = forward(x, 128*4, kernel=1, rate = 0.1)\n    elif Diversity_type == 'wave':\n        dropout = 0.1\n        x = wave_block(x, 16*2, 3, 12)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 32*2, 3, 8)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 64*2, 3, 4)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n\n        x = wave_block(x, 128*2, 3, 1)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n    \n    \n    x = x[:, 1:-1,:]\n    x = L.Dense(5)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = MSE(reduction=tf.keras.losses.Reduction.NONE))#mcrmse_loss)\n    return model\n\ndef get_optimizer():\n#     sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n    adam = tf.optimizers.Adam()\n#     radam = tfa.optimizers.RectifiedAdam()\n#     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n#     swa = tfa.optimizers.SWA(adam)\n    return adam","bc319adc":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train):\n    \n    len_app = 28\n    seq_app = 'AGCUAGCUAGCUAGCUAGCUAGCUAGCU'\n    loop_app = 'SSSSMMMMIIIIBBBBHHHHEEEEXXXX'\n    stru_app = '.'*len_app\n    \n    train = train.copy()\n    train['sequence'] = train['sequence'].apply(lambda x: x+seq_app)\n    train['bpRNA_string'] = train['bpRNA_string'].apply(lambda x: x+loop_app)\n    train['structure'] = train['structure'].apply(lambda x: x+stru_app)\n    \n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\", \"s\", \"e\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], ['s']+list(x)+['e']))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"bpRNA_string\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    X_loop = np.concatenate([np.zeros((X_loop.shape[0], 1, X_loop.shape[2])), X_loop, np.zeros((X_loop.shape[0], 1, X_loop.shape[2]))], axis=1)\n    \n\n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    X_structure = np.concatenate([np.zeros((X_structure.shape[0], 1, X_structure.shape[2])), X_structure, np.zeros((X_structure.shape[0], 1, X_structure.shape[2]))], axis=1)\n    \n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    #print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    X_node = np.concatenate([X_node[:, :(-len_app-1), :], X_node[:, -1, :][:, None,:]], axis=1)\n    #print(X_node.shape)\n    return X_node\n\n\n\n","d40c8749":"# copy and edited from https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\n\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSXse')}\ndata_dir = '\/kaggle\/input\/stanford-covid-vaccine\/'\n\ndef pandas_list_to_array(df):\n    \"\"\"\n    Input: dataframe of shape (x, y), containing list of length l\n    Return: np.array of shape (x, l, y)\n    \"\"\"\n    \n    return np.transpose(\n        np.array(df.values.tolist()),\n        (0, 2, 1)\n    )\n\ndef get_pair_idx(arr, sft=0):\n    n = len(arr)\n    out = np.zeros((n))\n    l = []\n    for c, i in enumerate(arr):\n        if i == '.':\n            out[c] = c\n        elif i == '(':\n            l.append(c)\n        else:\n            temp = l.pop()\n            if sft == 0:\n                out[c] = temp\n                out[temp] = c\n            elif sft >= 1:\n                out[c] = min(temp+sft, n-1)\n                out[temp] = max(c-sft, 0)\n            elif sft <= -1:\n                out[c] = max(temp-sft, 0)\n                out[temp] = min(c+sft, n-1)\n    return out\n\ndef calc_dist_to_pair(struct):\n    n = len(struct)\n    out = np.zeros((n))+10000\n    curr_dist = 10000\n    for c,i in enumerate(struct):\n        curr_dist += 1\n        if i in ['(', ')']:\n            out[c] = 1\n            curr_dist = 0\n        else:\n            out[c] = min(out[c], curr_dist)\n    curr_dist = 10000\n    for c,i in enumerate(struct[::-1]):\n        curr_dist += 1\n        if i in ['(', ')']:\n            out[n-1-c] = 0\n            curr_dist = 0\n        else:\n            out[n-1-c] = min(out[n-1-c], curr_dist)\n    return out\n\n\ndef calc_dist_to_single(struct):\n    n = len(struct)\n    out = np.zeros((n))+10000\n    curr_dist = 10000\n    for c,i in enumerate(struct):\n        curr_dist += 1\n        if i == '.':\n            out[c] = 1\n            curr_dist = 0\n        else:\n            out[c] = min(out[c], curr_dist)\n    curr_dist = 10000\n    for c,i in enumerate(struct[::-1]):\n        curr_dist += 1\n        if i == '.':\n            out[n-1-c] = 0\n            curr_dist = 0\n        else:\n            out[n-1-c] = min(out[n-1-c], curr_dist)\n    return out\n\n\ndef preprocess_inputs1(df, token2int, cols=['sequence', 'structure', 'bpRNA_string']):\n    return pandas_list_to_array(\n        df[cols].applymap(lambda seq: [token2int[x] for x in 's'+seq+'e'])\n    )\ndef preprocess_inputs(df, token2int):\n    dict_row_idx = {}\n\n    train_inputs = preprocess_inputs1(df, token2int)\n    new = np.zeros((train_inputs.shape[0], train_inputs.shape[1], len(token2int)))\n    for layer in range(3):\n        for i in range(len(token2int)):\n            new[train_inputs[:, :, layer]==i, i]=1\n\n    if BBP_TOTAL>=1:\n        bbp =[]\n        bbp1 =[]\n        bbp2 = []\n        bbp3 = []\n        bbp4_0 = []\n        bbp4_1 = []\n        bbp4_2 = []\n        bbp4_3 = []\n\n        ids = df.id.values\n        for c, i in enumerate(ids):\n            if 'aug' in str(i):\n                probability = np.load(aug_data_path+f\"bbps_aug\/{i}.npy\")\n            else:\n                probability = np.load(data_dir+'post_deadline_files\/new_sequences_bpps\/%s.npy'%i)\n            if BBP:\n                bbp.append(probability.max(-1).tolist())\n            if BBP1:\n                bbp1.append((1-probability.sum(axis=1)).tolist())\n            if BBP2:\n                srt = np.sort(probability)\n                bbp2.append((srt[:,-1] - srt[:, -2]).tolist())\n            if BBP3:\n                m_lst = probability.max(axis=0)\n                argmax_lst = m_lst[np.argmax(probability, axis=0)]\n                bbp3.append((argmax_lst-m_lst).tolist())\n            if BBP4:\n                pair_idx = get_pair_idx(df.structure.values[c]).astype(int)\n                pij = probability[np.arange(len(pair_idx)),pair_idx]\n                bbp4_0.append(pij.tolist())\n                m_lst = probability.max(axis=0)\n                bbp4_1.append((m_lst-pij).tolist())\n                bbp4_2.append((m_lst[pair_idx]-pij).tolist())\n                s_lst = probability.sum(axis=0)\n                bbp4_3.append((s_lst[pair_idx]-pij).tolist())\n\n        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n        if BBP:\n            temp[:, 1:-1] = np.array(bbp)\n            dict_row_idx['BBP'] = new.shape[2]\n            new = np.concatenate([new, temp[:, :,None]], axis=2)\n        if BBP1:\n            temp[:, 1:-1] = np.array(bbp1)\n            dict_row_idx['BBP1'] = new.shape[2]\n            new = np.concatenate([new, temp[:, :,None]], axis=2)\n        if BBP2:\n            temp[:, 1:-1] = np.array(bbp2)\n            dict_row_idx['BBP2'] = new.shape[2]\n            new = np.concatenate([new, temp[:, :,None]], axis=2)\n        if BBP3:\n            temp[:, 1:-1] = np.array(bbp3)\n            dict_row_idx['BBP3'] = new.shape[2]\n            new = np.concatenate([new, temp[:, :,None]], axis=2)\n        if BBP4: \n            for cnt, b in enumerate([bbp4_0, bbp4_1, bbp4_2, bbp4_3]):\n                dict_row_idx['BBP4_%s'%cnt] = new.shape[2]\n                temp[:, 1:-1] = np.array(b)\n                new = np.concatenate([new, temp[:, :,None]], axis=2)\n            dict_row_idx['BBP4_ed'] = new.shape[2]\n\n            \n    if DIST_NEW:\n        lst_dist = []\n        lst_dist_sqrt = []\n        ids = df.id.values\n        for c, i in enumerate(ids):\n            temp_dist = calc_dist_to_pair(df['structure'].values[c])+1\n            lst_dist.append((1\/temp_dist).tolist())\n            lst_dist_sqrt.append((np.sqrt(1\/temp_dist)).tolist())\n        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n        temp[:, 1:-1] = np.array(lst_dist)\n        new = np.concatenate([new, temp[:, :,None]], axis=2)\n        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n        temp[:, 1:-1] = np.array(lst_dist_sqrt)\n        new = np.concatenate([new, temp[:, :,None]], axis=2)\n        \n    if DIST_NEW2:\n        lst_dist = []\n        lst_dist_sqrt = []\n        ids = df.id.values\n        for c, i in enumerate(ids):\n            temp_dist = calc_dist_to_single(df['structure'].values[c])+1\n            lst_dist.append((1\/temp_dist).tolist())\n            lst_dist_sqrt.append((np.sqrt(1\/temp_dist)).tolist())\n        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n        temp[:, 1:-1] = np.array(lst_dist)\n        new = np.concatenate([new, temp[:, :,None]], axis=2)\n        temp = np.zeros((train_inputs.shape[0], train_inputs.shape[1]))\n        temp[:, 1:-1] = np.array(lst_dist_sqrt)\n        new = np.concatenate([new, temp[:, :,None]], axis=2)\n    \n\n    return new[:,:,len(token2int):]","56d9f3bd":"def get_structure_adj(train):\n    Ss = []\n    for i in (range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    new = np.zeros((Ss.shape[0], Ss.shape[1]+2, Ss.shape[2]+2, Ss.shape[3]))\n    new[:, 1:-1, 1:-1, :] = Ss\n    return new","63656160":"def get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1\/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    return Ds\n\n\ndef padding_2D(Ss):\n    new = np.zeros((Ss.shape[0], Ss.shape[1]+2, Ss.shape[2]+2))\n    new[:, 1:-1, 1:-1] = Ss\n    return new","e03abfbd":"def get_inputs(df_temp):\n    \n\n    X_node = get_input(df_temp).astype(np.float32)\n    X_node_new = preprocess_inputs(df_temp, token2int).astype(np.float32)\n    X_node = np.concatenate([X_node, X_node_new], axis=2)\n    del X_node_new\n\n\n    As = []\n    for id in (df_temp[\"id\"]):\n        a = np.load(data_dir+f\"post_deadline_files\/new_sequences_bpps\/{id}.npy\")\n        As.append(a)\n    As = np.array(As)\n    As = padding_2D(As)\n    Ss = get_structure_adj(df_temp).astype(np.float32)\n    Ds = get_distance_matrix(As)\n    DDs = get_distance_matrix_2d(Ss)\n    As = np.concatenate([As[:,:,:,None],Ss, Ds, DDs], axis = 3).astype(np.float32)\n    del Ss, Ds, DDs\n    return X_node, As\n\ndict_X = {}\ndict_A = {}\nfor i in tqdm(df.id):\n    df_temp = df.loc[[i]]\n    dict_X[i], dict_A[i] = get_inputs(df_temp)\n    \nX_node, As = dict_X[0], dict_A[0]","41b32c80":"%%time\nconfig = {}\nDiversity_type = 'lstm'\nwgts_dir = '..\/input\/ov-v40032-wgts\/'\nbase = get_base(config)\nmodel = get_model(base, config)\nfor m in range(5):\n    model.load_weights(wgts_dir+'model_%s.h5'%m)\n    preds_ls = []\n    for uid in tqdm(df.id):\n        X_node, As = dict_X[uid], dict_A[uid]\n        out1 = model.predict([X_node, As])\n        out2 = model.predict([reverse_input(X_node), reverse_BBP_3D(As)])[:, ::-1, :]\n        out = (out1+out2)\/2\n        \n        single_pred = out[0]\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\n        del out1, out2, out, single_pred, single_df\n        \n    preds_df = pd.concat(preds_ls).set_index('id_seqpos')\n    preds_df.to_csv(\"sub_%s_%s.csv\"%(Diversity_type, m))\n    \n    del preds_df, preds_ls\n    gc.collect()\n\n\ndel base, model\ngc.collect()\n\nK.clear_session()","5247f4c8":"%%time\nconfig = {}\nDiversity_type = 'gru'\nwgts_dir = '..\/input\/ov-v40131-wgts\/'\nbase = get_base(config)\nmodel = get_model(base, config)\nfor m in range(5):\n    model.load_weights(wgts_dir+'model_%s.h5'%m)\n    preds_ls = []\n    for uid in tqdm(df.id):\n        X_node, As = dict_X[uid], dict_A[uid]\n        out1 = model.predict([X_node, As])\n        out2 = model.predict([reverse_input(X_node), reverse_BBP_3D(As)])[:, ::-1, :]\n        out = (out1+out2)\/2\n        \n        single_pred = out[0]\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\n        del out1, out2, out, single_pred, single_df\n        \n    preds_df = pd.concat(preds_ls).set_index('id_seqpos')\n    preds_df.to_csv(\"sub_%s_%s.csv\"%(Diversity_type, m))\n    \n    del preds_df, preds_ls\n    gc.collect()\n\ndel base, model\ngc.collect()\n\nK.clear_session()","52c374c0":"%%time\nconfig = {}\nDiversity_type = 'forward'\nwgts_dir = '..\/input\/ov-v40237-wgts\/'\nbase = get_base(config)\nmodel = get_model(base, config)\nfor m in range(5):\n    model.load_weights(wgts_dir+'model_%s.h5'%m)\n    preds_ls = []\n    for uid in tqdm(df.id):\n        X_node, As = dict_X[uid], dict_A[uid]\n        out1 = model.predict([X_node, As])\n        out2 = model.predict([reverse_input(X_node), reverse_BBP_3D(As)])[:, ::-1, :]\n        out = (out1+out2)\/2\n        \n        single_pred = out[0]\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\n        del out1, out2, out, single_pred, single_df\n        \n    preds_df = pd.concat(preds_ls).set_index('id_seqpos')\n    preds_df.to_csv(\"sub_%s_%s.csv\"%(Diversity_type, m))\n    \n    del preds_df, preds_ls\n    gc.collect()\n\ndel base, model\ngc.collect()\n\nK.clear_session()","3f70ff5e":"%%time\nconfig = {}\nDiversity_type = 'wave'\nwgts_dir = '..\/input\/ov-v40334-wgts\/'\nbase = get_base(config)\nmodel = get_model(base, config)\nfor m in range(5):\n    model.load_weights(wgts_dir+'model_%s.h5'%m)\n    preds_ls = []\n    for uid in tqdm(df.id):\n        X_node, As = dict_X[uid], dict_A[uid]\n        out1 = model.predict([X_node, As])\n        out2 = model.predict([reverse_input(X_node), reverse_BBP_3D(As)])[:, ::-1, :]\n        out = (out1+out2)\/2\n        \n        single_pred = out[0]\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n        preds_ls.append(single_df)\n        del out1, out2, out, single_pred, single_df\n        \n    preds_df = pd.concat(preds_ls).set_index('id_seqpos')\n    preds_df.to_csv(\"sub_%s_%s.csv\"%(Diversity_type, m))\n    \n    del preds_df, preds_ls\n    gc.collect()\n    \ndel base, model\ngc.collect()","31c8fb53":"lst_pred = os.listdir()\nlst_pred = sorted([x for x in lst_pred if x.startswith('sub_')])\nprint(lst_pred)\npreds_df_agg = pd.read_csv(lst_pred[0], index_col=0)\nfor n in lst_pred[1:]:\n    pred_temp = pd.read_csv(n, index_col=0)\n    pred_temp[pred_temp<-0.5] = -0.5\n    pred_temp[pred_temp>6] = 6\n    preds_df_agg += pred_temp\npreds_df_agg = preds_df_agg\/len(lst_pred)\npreds_df_agg = preds_df_agg.reset_index()\n\n\nsubmission = sub[['id_seqpos']].merge(preds_df_agg, on=['id_seqpos'])\nsubmission = submission[sub.columns]\nsubmission.to_csv('submission.csv', index=False)","a9f7ae7a":"## define model","7fe39c15":"## define helper functions","64acc169":"### V6: bug fix\n\n### This kernel reads 4 of my best single models, and do inference on the 233 new sequences. \n\n### If Arnie secondary structure were added at inference time, a equal weight blend of them reaches private 0.34083. However here we don't have Arnie secondary structure. So approximate performance is around 0.3415.\n\n### However, please note that due to significant change in sequence length, the performance of my models on these new sequence are expected to be worse than 0.3415. I still recommend doing PL using longer sequences to finetune the model - the PL sequences can be shorter than the inference sequences due to computational limitations, but at least allowing the model to see more sequence lengths can help decrease dependency on length. ","6c3f1cc6":"## get features","6051cfbe":"# inference"}}