{"cell_type":{"0abb6079":"code","49112227":"code","882c43ec":"code","43b2f30a":"code","cb06356f":"code","e84839e5":"code","9a25bd7e":"code","d3c5430c":"code","033e4b81":"code","8915cbb7":"code","1d1cfafd":"code","5ac07d8e":"code","4d8408fe":"code","5df91107":"code","ba6d5ae9":"code","07a6d5e6":"code","966bbc11":"code","6a35b153":"code","80f26ce3":"code","1db26036":"code","3a53e762":"code","63ab307f":"code","e6a4ac79":"code","2183e650":"code","92495b51":"code","863cf276":"code","4177276b":"code","6d339bac":"code","2ebc9d59":"code","1dd7dae1":"code","59abb357":"code","0b7a2038":"code","6ec05c91":"code","610f9854":"code","ee11b137":"code","eb47204d":"code","556fc400":"code","cd679254":"code","0f59b585":"code","82f2dd8f":"markdown","a287255d":"markdown","28586ef4":"markdown","ec0fe93d":"markdown","26ec91fd":"markdown","c41d86fb":"markdown","bcc30a93":"markdown","0e938a42":"markdown","ee198c4b":"markdown","d78f2db8":"markdown","728dafc5":"markdown","d67612b2":"markdown","801af3b3":"markdown","5f56cb41":"markdown","37a1d88b":"markdown","8ac62f54":"markdown","532a0388":"markdown","78955fa8":"markdown","ac99ad3d":"markdown","2ee92893":"markdown","aa491467":"markdown","762ea620":"markdown","a0a57e2b":"markdown","811992ef":"markdown","c1584861":"markdown","44ebb491":"markdown","120a75bd":"markdown","aac06ab8":"markdown","95ec4251":"markdown"},"source":{"0abb6079":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, LabelBinarizer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor \nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score, cross_val_predict,KFold, GridSearchCV, train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom datetime import datetime","49112227":"warnings.filterwarnings('ignore')\ndata_f = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ndata = data_f.copy()\ndata = data.iloc[:,1:]","882c43ec":"data_cat = data.iloc[:,:10]\ndata_cont = data.iloc[:,10:]","43b2f30a":"data.info()","cb06356f":"data_cat.describe()","e84839e5":"data_cont.describe()","9a25bd7e":"print(\"Null values in continuos variables:{}\\nNull values in categorical variables:{}\".format(data_cont.isna().sum().sum(), data_cat.isna().sum().sum()))","d3c5430c":"sns.pairplot(data_cont)","033e4b81":"sns.set(font_scale = 2)\nfig,ax = plt.subplots(5,2, figsize=(20,30), sharex=True)\naxes = ax.flatten()\nobject_bol = data_cat.dtypes == 'object'\nfor ax, catplot in zip(axes, data_cat.dtypes[object_bol].index):\n    sns.countplot(y=catplot, data=data_cat, ax=ax, order=data_cat[catplot].value_counts().index)\n    ax.xaxis.label.set_size(30)\n    ax.yaxis.label.set_size(30)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.tick_params(axis=\"x\", labelsize=15)\nplt.tight_layout()  \nplt.show()","8915cbb7":"\nfig,ax = plt.subplots(5,2, figsize=(12,12), sharex=False)\naxes = ax.flatten()\nobject_bol = data_cont.dtypes == 'float'\n# print(data_cont.dtypes[object_bol].index)\nfor ax, catplot in zip(axes, data_cont.dtypes[object_bol].index):\n    sns.kdeplot(x=data_cont[catplot],ax=ax,shade = True)\n    ax.xaxis.label.set_size(20)\n    ax.yaxis.label.set_size(20)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.tick_params(axis=\"y\", labelsize=15)\nplt.tight_layout()  \nplt.show()   ","1d1cfafd":"sns.set(font_scale = 1)\nfig, ax = plt.subplots(figsize = (15,15))\nsns.heatmap(data_cont.corr(),ax = ax,annot=True)","5ac07d8e":"warnings.filterwarnings('ignore')\nfig,ax = plt.subplots(5,2, figsize=(20,20), sharex=False)\naxes = ax.flatten()\nobject_bol = data_cont.dtypes == 'float'\nfor ax, catplot in zip(axes, data_cont.dtypes[object_bol].index):\n    res = sns.ecdfplot(data=data_cont,x=catplot,ax=ax)\n    ax.xaxis.label.set_size(20)\n    ax.yaxis.label.set_size(20)\n    ax.tick_params(axis=\"x\", labelsize=15)\n    ax.tick_params(axis=\"y\", labelsize=15)\nplt.tight_layout()  \nplt.show()   ","4d8408fe":"warnings.filterwarnings('ignore')\nfig,ax = plt.subplots(10,1, figsize=(25,30), sharex=False)\naxes = ax.flatten()\nobject_bol = data_cont.dtypes == 'float'\nfor ax, catplot in zip(axes, data_cont.dtypes[object_bol].index):\n    res = sns.boxplot(x=catplot,ax=ax,data=data_cont)\n    ax.xaxis.label.set_size(30)\n    ax.yaxis.label.set_size(30)\n    ax.tick_params(axis=\"x\", labelsize=20) \nplt.tight_layout()  \nplt.show()   ","5df91107":"training_data = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\") ","ba6d5ae9":"#id column is unnecessary\ntraining_data.drop(['id'],axis=1,inplace=True)\ntest_data.drop(['id'],axis=1,inplace=True)","07a6d5e6":"categorical = list(filter(lambda x: 'cat' in x, training_data.columns))\ncontinuous  = list(filter(lambda x: 'cat'not in x, training_data.columns))","966bbc11":"X_train, X_test, Y_train, Y_test = train_test_split(training_data.iloc[:,:-1], training_data.iloc[:,-1], test_size = 0.2, random_state = 42)","6a35b153":"print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","80f26ce3":"cat_pipeline = ColumnTransformer([('encoder', OneHotEncoder(), [idx for idx,_ in enumerate(categorical)])], remainder='passthrough')\ncont_pipeline = ColumnTransformer([('scaler', StandardScaler(), [idx+10 for idx,_ in enumerate(continuous[:-1])])], remainder='drop')","1db26036":"full_pipeline = FeatureUnion(transformer_list = [(\"Categorical_Pipeline\",cat_pipeline),\n                                                 (\"Quantitative_Pipeline\",cont_pipeline)])\n\nx_train = cat_pipeline.fit_transform(X_train)\nx_test = cat_pipeline.transform(X_test)","3a53e762":"# Function to get crossvalidation metrics \ndef get_score(model, x=x_train, y=Y_train, cv = 5, verbose = False, ack=False):\n    scores = cross_val_score(model, x, y, scoring = 'neg_mean_squared_error',cv=cv,n_jobs=-1)\n    mean = np.mean(-scores)\n    std = np.std(-scores)\n    if verbose:\n        print(\"Scores: {}\\nMean: {:.4f}\\nStd: {:.4f}\".format(scores,mean,std))\n    if ack:\n        return scores,mean,std","63ab307f":"# Look performance of default params \nlasso_model = Lasso()\nget_score(lasso_model,cv=10,verbose=True)","e6a4ac79":"# Funtion to find optimal alpha\ndef find_opt_params_lasso(raange, x=x_train, y=Y_train, div_fac=1):\n    alphas = []\n    errors = []\n    for alpha in raange:\n        lr_model = Lasso(alpha= (alpha\/div_fac))\n        lr_model.fit(x,y)\n        alphas.append(alpha\/div_fac)\n        errors.append(get_score(lr_model,ack=True)[1])\n    return alphas, errors","2183e650":"# Plot errors vs alphas \nalphas, errors = find_opt_params_lasso(range(1,100),div_fac = 1000)\nplt.plot(alphas,errors)","92495b51":"#Lasso regression\nlasso_model = Lasso(alpha=0.001)\nlasso_model.fit(x_train, Y_train)\n# get_score(lasso_model,cv=10,verbose=True)","863cf276":"#Evaluation on test set\npredictions = lasso_model.predict(x_test)\nmean_squared_error(Y_test,predictions)","4177276b":"#raw model\nrf_model = RandomForestRegressor(n_jobs=-1)\nget_score(rf_model,y=Y_train.ravel(),verbose=True)","6d339bac":"parameters = {'n_estimators':range(50,300,50),'max_features':('auto','sqrt','log2')}\nsearch_rf_params = RandomizedSearchCV(rf_model, parameters, n_jobs=-1, scoring='neg_mean_squared_error',cv=3)","2ebc9d59":"search_rf_params.fit(x_train,Y_train.ravel())","1dd7dae1":"search_rf_params.best_estimator_","59abb357":"rf_model = RandomForestRegressor(n_estimators= 250, max_features= 'sqrt',n_jobs=-1)\nrf_model.fit(x_train, Y_train.ravel()) ","0b7a2038":"# default predictions\npredictions = rf_model.predict(x_test)\nmean_squared_error(Y_test,predictions)","6ec05c91":"predictions_test = rf_model.predict(cat_pipeline.transform(test_data))","610f9854":"# https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    \n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                    lambda_l1, lambda_l2, min_child_weight):\n\n            params = {'boosting_type': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'verbose': -1,\n                      'early_stopping_round':100}\n\n            params['max_depth'] = int(round(max_depth))\n            params[\"num_leaves\"] = int(round(num_leaves))\n            params[\"n_estimators\"] = int(round(n_estimators))\n            params['learning_rate'] = learning_rate\n            params['subsample'] = subsample\n            params['colsample_bytree'] = colsample_bytree\n            params['lambda_l1'] = max(lambda_l1, 0)\n            params['lambda_l2'] = max(lambda_l2, 0)\n            params['min_child_weight'] = min_child_weight\n\n            score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=False, verbose_eval =False, metrics=['rmse'])\n\n            return -np.min(score['rmse-mean']) # min or max can change best_param\n\n    \n    # use bayesian optimization to search for the best hyper-parameter combination\n    # https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/bayes_opt\/bayesian_optimization.pyta\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                      {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 1000),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = -1\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points=5, n_iter=25) # 20 combinations \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'rmse'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'regression'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set","ee11b137":"## https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\nwarnings.filterwarnings('ignore')\nclass MultiColumnLabelEncoder(BaseEstimator):\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n    \nmult_enc_pipeline = ColumnTransformer([('encoder',MultiColumnLabelEncoder(), [idx for idx,_ in enumerate(categorical)])], remainder='passthrough')\n# trans_x = MultiColumnLabelEncoder(columns = categorical).fit_transform(training_data)\nxtrain = pd.DataFrame(mult_enc_pipeline.fit_transform(training_data.iloc[:,:-1]),columns=training_data.columns[:-1])\nytrain = pd.DataFrame(training_data.iloc[:,-1],columns=['target'])\nbest_params = search_best_param(xtrain,ytrain,categorical)","eb47204d":"def K_Fold_LightGBM(X_train, y_train , cat_features, params_set, num_folds = 5):\n    num = 0\n    models = []\n    folds = KFold(n_splits=num_folds, shuffle=True, random_state=0)\n    type(X_train)\n        # 5 times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        print(f\"     model{num}\")\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        train_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\n        valid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)\n        \n        CV_LGBM = lgb.train(params_set,\n                 train_data,\n                 num_boost_round = 2500,\n                 valid_sets = valid_data,\n                 early_stopping_rounds = 100,\n                 verbose_eval = 50\n                 )\n        # increase early_stopping_rounds can lead to overfitting \n        models.append(CV_LGBM)\n        \n        print(\"Train set RMSE:\", mean_squared_error(train_y,models[num].predict(train_X),squared = False))\n        print(\" Test set RMSE:\", mean_squared_error(valid_y,models[num].predict(valid_X),squared = False))\n        print(\"\\n\")\n        num = num + 1\n        \n    return models\n\nlgbm_models = K_Fold_LightGBM(xtrain,ytrain,categorical,best_params,5)","556fc400":"predictLGBM = lgbm_models[3].predict(mult_enc_pipeline.transform(test_data))","cd679254":"def make_submission_csv(predictions_test):\n    submission_csv = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\n    submission_csv.drop('target',axis=1)\n    submission_csv['target']=predictions_test \n    submission_csv.to_csv('Result_{}.csv'.format(datetime.now().strftime(\"%d_%m_%Y_%H_%M\")),index=False)","0f59b585":"make_submission_csv(predictions_test)","82f2dd8f":"### Looking for trends in continuous features.","a287255d":"# Descriptive Statistics","28586ef4":"### Train Test Split","ec0fe93d":"cont 0, cont 2, cont 6, cont 8 have considerable outliers","26ec91fd":"# Data Preprocessing ","c41d86fb":"### Light GBM Regressor\n","bcc30a93":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Problem Statement](#Problem-Statement)\n    - [Introduction](#Introduction)\n    - [Goal](#Goal)\n    - [Evaluation Metrics](#Evaluation-Metrics)\n* [Importing Libraries](#Importing-Libs)\n* [Descriptive Statistics](#Descriptive-Statistics)\n* [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    - [Categorical Features](#Categorical-Features)\n    - [Continuous Features](#Continuous-Features)\n    - [Target](#Target)\n* [Data Preprocessing](#Data-Preprocessing)\n    - [Train Test Split](#Train-Test-Split)\n    - [Transforms and Pipelines](#Transforms-and-Pipelines)\n* [Modelling](#Modelling)\n    - [Lasso Regression](#Lasso-Regression)\n    - [Random-forest Regression](#Random-Forest)\n    - [LightGBM Regression](#Light-GBM-Regressor)\n* [Submission](#Submission)","0e938a42":"### Correlation","ee198c4b":"### Evaluation Metrics\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n \nwhere is the predicted value, is the original value, and is the number of rows in the test data.","d78f2db8":"There is no variable that is highly correlated to the target variable.<br>\nHowever, Cont 8, Cont 9 and Cont 12 are highly positively correlated to Cont5.","728dafc5":"# Submission","d67612b2":"Error is lowest when alpha is 0.0001 (without std sc) and 0.001 (otherwise) ","801af3b3":"### No. of categories in each categorical variable.\n","5f56cb41":"Light GBM generalizes better than random forest on the test set.","37a1d88b":"# Problem Statement","8ac62f54":"### Transforms and Pipelines","532a0388":"### Introduction\nThe dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.","78955fa8":"### **Lasso Regression**","ac99ad3d":"### Boxplots","2ee92893":"Cat 9 and Cat 8 accommodate a larger variety compared to the rest of the features.","aa491467":"Let us train these models:\n1. Lasso Regression\n2. Random Forest\n3. Light Gradient Boosted Machines\n","762ea620":"# Modelling","a0a57e2b":"### **Random Forest**","811992ef":"# Exploratory Data Analysis","c1584861":"### Cumulative distribution function","44ebb491":"More parameters can be included in the search space to see improvement in performance.","120a75bd":"# Importing Libs","aac06ab8":"### Goal\nFor this competition, we will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous. Hence, this is a regression task.","95ec4251":"### Distributions of the continuous variables."}}