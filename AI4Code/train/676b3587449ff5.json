{"cell_type":{"13ad10da":"code","0ec53285":"code","5bddf640":"code","f6d77d0b":"code","51b52bfa":"code","e277a59b":"code","2bb83060":"code","5e1e0ad6":"code","f6c0c112":"code","d9cb27f9":"code","57a27e9a":"code","41143a37":"markdown"},"source":{"13ad10da":"import pandas as pd","0ec53285":"%ls -lh ..\/input\/data-science-bowl-2019\/","5bddf640":"test = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\ntest.head()","f6d77d0b":"last_event = test.sort_values(['installation_id', 'timestamp']).groupby('installation_id').last().reset_index()\nlast_event.head()","51b52bfa":"ends_with_assessment = last_event['title'].str.contains('Assessment')\nlast_event[~ends_with_assessment]","e277a59b":"sbm_sample = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')\nsbm_sample = pd.merge(sbm_sample, last_event[['installation_id', 'title']], on='installation_id')\nsbm_sample","2bb83060":"labels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\nlabels.head()","5e1e0ad6":"def predict(accuracy):\n    if accuracy > 0.5:\n        return 3\n    \n    if accuracy > 0.4:\n        return 2\n    \n    if accuracy > 0.13:\n        return 1\n    \n    return 0","f6c0c112":"agg = labels.groupby('title').sum()[['num_correct', 'num_incorrect']].reset_index()\nagg['accuracy'] = agg['num_correct'] \/ (agg['num_incorrect'] + agg['num_correct'])\nagg['accuracy_group'] = agg['accuracy'].map(predict)\nagg","d9cb27f9":"sbm = pd.merge(sbm_sample.drop('accuracy_group', axis=1), agg, on='title')\nsbm","57a27e9a":"sbm[['installation_id', 'accuracy_group']].to_csv('submission.csv', index=False)","41143a37":"## What this notebook does:\n- Map `installation_id` in `sample_submission.csv` to the correspoding assessment.\n- Compute the average accuracy of each assessment using `train_labels.csv`.\n- Estimate `accuracy_group` from the average accuracy."}}