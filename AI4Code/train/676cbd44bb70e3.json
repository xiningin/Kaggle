{"cell_type":{"d9f5e011":"code","38f86af1":"code","092a9bbc":"code","75dfed85":"code","78d2ac02":"code","0ab523c7":"code","c159231c":"code","99d9a557":"code","549ffc24":"code","8f961ef6":"code","79e56da4":"code","babc319a":"code","15065701":"code","7077ab4b":"code","e8bc346b":"code","435e25f1":"code","c13d6c5b":"code","3ab74f69":"code","83f9cdba":"code","0177a3c4":"code","3d37dd81":"code","584921ce":"code","c2555cfd":"code","5155d6cd":"code","8b96d346":"code","a7fb19be":"code","a0c7b9e7":"code","b77856fb":"code","ed9b6e9a":"code","61458d69":"code","4b6bd69f":"code","55a359df":"code","309344b4":"code","b28d4624":"code","e27cfd44":"code","ed5fd785":"code","40698239":"code","843d5339":"code","d31ca8da":"code","26c8953a":"code","44e211fc":"code","aa23dc0d":"code","6ff34127":"code","b7369b70":"code","7dd6805d":"code","7598dc36":"code","171f633e":"code","ff5cb941":"code","666cba20":"code","b7d97c39":"code","ff66cab3":"code","307a9a98":"markdown","94b13607":"markdown","870d142a":"markdown"},"source":{"d9f5e011":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38f86af1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","092a9bbc":"df = pd.read_csv('\/kaggle\/input\/glass\/glass.csv')","75dfed85":"df.head()","78d2ac02":"df.info()","0ab523c7":"df.describe()","c159231c":"sns.countplot(df['Type'])","99d9a557":"sns.distplot(df['RI'])","549ffc24":"features = df.columns[:-1].tolist()","8f961ef6":"for feature in features:\n    sns.distplot(df[feature], kde=False)\n    plt.show()","79e56da4":"for feature in features:\n    sns.boxplot(df[feature])\n    plt.show()","babc319a":"for feature in features:\n    sns.violinplot(x = 'Type', y = feature, data=df)\n    plt.legend()\n    plt.show()","15065701":"corr = df.corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n            alpha = 0.7,   cmap= 'coolwarm')","7077ab4b":"from collections import Counter\ndef outlier_hunt(df):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than 2 outliers. \n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in df.columns.tolist():\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        \n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        \n        # Interquartile rrange (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2 )\n    \n    return multiple_outliers   \n\nprint('The dataset contains %d observations with more than 2 outliers' %(len(outlier_hunt(df[features]))))   ","e8bc346b":"outlier_indices = outlier_hunt(df[features])\ndf = df.drop(outlier_indices).reset_index(drop=True)\nprint(df.shape)","435e25f1":"for feature in features:\n    sns.boxplot(x = 'Type', y = feature, data=df)\n    plt.show()","c13d6c5b":"X = df.drop('Type', axis=1).values\n","3ab74f69":"y = df['Type'].values.reshape(-1,1)","83f9cdba":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42, stratify = y)","0177a3c4":"# Fitting Logistic Regression to the Training set\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier_lr = LogisticRegression()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', LogisticRegression())\n]\n\nlr_pipe = Pipeline(steps)","3d37dd81":"parameters = { 'model__C' : [1,10,100,1000,10000],\n               'model__fit_intercept' : [True],\n               'model__multi_class' : ['auto'],\n               'model__tol' : [0.0001],\n               'model__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n               'model__n_jobs' : [-1],\n               'model__max_iter' : [5000],\n               'model__random_state': [42] \n}\nclassifier_lr = GridSearchCV(lr_pipe, parameters, iid=False, cv = 3)\nclassifier_lr = classifier_lr.fit(X_train, y_train.ravel())","584921ce":"from sklearn.metrics import accuracy_score\n\ny_pred_lr_train = classifier_lr.predict(X_train)\naccuracy_lr_train = accuracy_score(y_train, y_pred_lr_train)\nprint(\"Training set: \", accuracy_lr_train)\n\ny_pred_lr_test = classifier_lr.predict(X_test)\naccuracy_lr_test = accuracy_score(y_test, y_pred_lr_test)\nprint(\"Test set: \", accuracy_lr_test)","c2555cfd":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_lr_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","5155d6cd":"from sklearn.metrics import classification_report\nprint( classification_report(y_test, y_pred_lr_test))","8b96d346":"# Fitting classifier to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', KNeighborsClassifier())\n]\nknn_pipe = Pipeline(steps)","a7fb19be":"parameters = { 'model__algorithm' : ['brute', 'kdtree'],\n               'model__metric' : ['minkowski'],\n               'model__p' : [1],\n               'model__n_neighbors' : [3,5,11,19],\n               'model__weights' : ['uniform', 'distance'],\n               'model__n_jobs' : [-1]\n}\nclassifier_knn = GridSearchCV(knn_pipe, parameters, iid=False, cv = 3)\nclassifier_knn = classifier_knn.fit(X_train, y_train.ravel())","a0c7b9e7":"y_pred_knn_train = classifier_knn.predict(X_train)\naccuracy_knn_train = accuracy_score(y_train, y_pred_knn_train)\nprint(\"Training set: \", accuracy_knn_train)\n\ny_pred_knn_test = classifier_knn.predict(X_test)\naccuracy_knn_test = accuracy_score(y_test, y_pred_knn_test)\nprint(\"Test set: \", accuracy_knn_test)","b77856fb":"sns.heatmap(confusion_matrix(y_test, y_pred_knn_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","ed9b6e9a":"from sklearn.metrics import classification_report\nprint( classification_report(y_test, y_pred_lr_test))","61458d69":"# Fitting classifier to the Training set\nfrom sklearn.svm import SVC\nclassifier_svm_kernel = SVC()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', SVC())\n]\nsvm_kernel_pipe = Pipeline(steps)","4b6bd69f":"parameters = { 'model__kernel' : ['rbf', 'poly', 'sigmoid'],\n               'model__C' : [1,10, 50, 100,1000,10000],\n               'model__gamma' : [0.001, 0.01, 0.1, 1, 'scale'],\n               'model__random_state' : [42],\n               'model__degree' : [1,2,3]\n}\nclassifier_svm_kernel = GridSearchCV(svm_kernel_pipe, parameters, iid=False, cv = 3)\nclassifier_svm_kernel = classifier_svm_kernel.fit(X_train, y_train.ravel())","55a359df":"classifier_svm_kernel.best_params_","309344b4":"y_pred_svm_kernel_train = classifier_svm_kernel.predict(X_train)\naccuracy_svm_kernel_train = accuracy_score(y_train, y_pred_svm_kernel_train)\nprint(\"Training set: \", accuracy_svm_kernel_train)\n\ny_pred_svm_kernel_test = classifier_svm_kernel.predict(X_test)\naccuracy_svm_kernel_test = accuracy_score(y_test, y_pred_svm_kernel_test)\nprint(\"Test set: \", accuracy_svm_kernel_test)","b28d4624":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_svm_kernel_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","e27cfd44":"from sklearn.metrics import classification_report\nprint( classification_report(y_test, y_pred_svm_kernel_test))","ed5fd785":"# Fitting classifier to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_dt = DecisionTreeClassifier()\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', DecisionTreeClassifier())\n]\ndt_pipe = Pipeline(steps)","40698239":"# Applying Grid Search to find the best model and the best parameters\nparameters = [ { \"model__max_depth\": np.arange(1,21),\n                 \"model__min_samples_leaf\": [1, 5, 10, 20, 50, 100],\n                 \"model__min_samples_split\": np.arange(2, 11),\n                 \"model__criterion\": [\"gini\"],\n                 \"model__random_state\" : [42]}\n            ]\nclassifier_dt = GridSearchCV(estimator = dt_pipe,\n                           param_grid  = parameters,\n                           cv = 3,\n                           iid = False,\n                           n_jobs = -1)\nclassifier_dt = classifier_dt.fit(X_train, y_train.ravel())","843d5339":"y_pred_dt_train = classifier_dt.predict(X_train)\naccuracy_dt_train = accuracy_score(y_train, y_pred_dt_train)\nprint(\"Training set: \", accuracy_dt_train)\n\ny_pred_dt_test = classifier_dt.predict(X_test)\naccuracy_dt_test = accuracy_score(y_test, y_pred_dt_test)\nprint(\"Test set: \", accuracy_dt_test)","d31ca8da":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_dt_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","26c8953a":"from sklearn.metrics import classification_report\nprint( classification_report(y_test, y_pred_dt_test))","44e211fc":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nclassifier_rf = RandomForestClassifier()\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', RandomForestClassifier())\n]\nrf_pipe = Pipeline(steps)","aa23dc0d":"parameters =  { \"model__n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n                \"model__max_features\": [\"auto\", \"sqrt\"],\n                \"model__max_depth\": np.linspace(10, 110, num = 11),\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__min_samples_leaf\": [1, 2, 4],\n                \"model__bootstrap\": [True, False],\n                \"model__criterion\": [\"gini\"],\n                \"model__random_state\" : [42] }\n            \nclassifier_rf = RandomizedSearchCV(estimator = rf_pipe,\n                                  param_distributions = parameters,\n                                  n_iter = 100,\n                                  cv = 3,\n                                  random_state=42,\n                                  verbose = 4,\n                                  n_jobs = -1)\nclassifier_rf = classifier_rf.fit(X_train, y_train.ravel())","6ff34127":"y_pred_rf_train = classifier_rf.predict(X_train)\naccuracy_rf_train = accuracy_score(y_train, y_pred_rf_train)\nprint(\"Training set: \", accuracy_rf_train)\n\ny_pred_rf_test = classifier_rf.predict(X_test)\naccuracy_rf_test = accuracy_score(y_test, y_pred_rf_test)\nprint(\"Test set: \", accuracy_rf_test)","b7369b70":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_rf_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","7dd6805d":"import xgboost as xgb\nfrom scipy.stats import uniform, randint\n\nxgb_model = xgb.XGBClassifier()\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train, y_train.ravel())","7598dc36":"y_pred_xgb_train = search.predict(X_train)\naccuracy_xgb_train = accuracy_score(y_train, y_pred_xgb_train)\nprint(\"Training set: \", accuracy_xgb_train)\n\ny_pred_xgb_test = search.predict(X_test)\naccuracy_xgb_test = accuracy_score(y_test, y_pred_xgb_test)\nprint(\"Test set: \", accuracy_xgb_test)","171f633e":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_xgb_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","ff5cb941":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_xgb_test))","666cba20":"models = [('Logistic Regression', accuracy_lr_train, accuracy_lr_test),\n          ('KNN', accuracy_knn_train, accuracy_knn_test),\n          ('SVM (Kernel)', accuracy_svm_kernel_train, accuracy_svm_kernel_test),\n          ('Decision Tree Classification', accuracy_dt_train, accuracy_dt_test),\n          ('Random Forest Classification', accuracy_rf_train, accuracy_rf_test),\n          ('XG Boost Classification', accuracy_xgb_train, accuracy_xgb_test),\n         ]","b7d97c39":"predict = pd.DataFrame(data = models, columns=['Model', 'Training Accuracy', 'Test Accuracy'])\npredict","ff66cab3":"f, axes = plt.subplots(2,1, figsize=(14,10))\n\npredict.sort_values(by=['Training Accuracy'], ascending=False, inplace=True)\n\nsns.barplot(x='Training Accuracy', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('Training Accuracy', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\naxes[0].set_xticks(np.arange(0, 1.1, 0.1))\n\npredict.sort_values(by=['Test Accuracy'], ascending=False, inplace=True)\n\nsns.barplot(x='Test Accuracy', y='Model', data = predict, palette='Greens_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('Test Accuracy', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\naxes[1].set_xticks(np.arange(0, 1.1, 0.1))\n\nplt.show()","307a9a98":"**Do not forget to upvote and comment if you have any queries**","94b13607":"**Credits - https:\/\/www.kaggle.com\/tolgahancepel\/glass-classification-analysis-with-eda**","870d142a":"Attribute Information:\n\n1. Id number: 1 to 214 (removed from CSV file)\n2. RI: refractive index\n3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n4. Mg: Magnesium\n5. Al: Aluminum\n6. Si: Silicon\n7. K: Potassium\n8. Ca: Calcium\n9. Ba: Barium\n10. Fe: Iron\n11. Type of glass: (class attribute)\n\n-- 1 buildingwindowsfloatprocessed -- 2 buildingwindowsnonfloatprocessed -- 3 vehiclewindowsfloatprocessed\n-- 4 vehiclewindowsnonfloatprocessed (none in this database)\n-- 5 containers\n-- 6 tableware\n-- 7 headlamps"}}