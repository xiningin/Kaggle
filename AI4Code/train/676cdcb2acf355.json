{"cell_type":{"5762e789":"code","3a81c84c":"code","9d153f88":"code","a27b0653":"code","71c31035":"code","eed516d2":"code","8f78d237":"markdown","8b89ce7a":"markdown","5fb02e12":"markdown","23cdc09f":"markdown","b6c0502b":"markdown","953d7761":"markdown","99e993b3":"markdown","507e87d6":"markdown","7f6728a9":"markdown","38a4fe5b":"markdown","d3737d7c":"markdown","f67d6fa8":"markdown","95e37ed6":"markdown","9acfd6e2":"markdown","7d33ec73":"markdown","b032fc12":"markdown"},"source":{"5762e789":"from pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom IPython.display import Image","3a81c84c":"Image(\"..\/input\/notebookimages\/hy.PNG\")","9d153f88":"from sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=500, n_features=4, n_informative=2,random_state=0, shuffle=False)\n\nf,ax=plt.subplots(2,2,figsize=(14,14))\n\nsns.scatterplot(x=X[:,0], y=y, ax=ax[0,0])\nax[0,0].set_xlabel('Feature 1 Values')\nax[0,0].set_ylabel('Y Values')\nax[0,0].set_title('Sactter Plot : Feature 1 vs Y')\n\nsns.scatterplot(x=X[:,1], y=y,ax=ax[0,1])\nax[0,1].set_xlabel('Feature 2 Values')\nax[0,1].set_ylabel('Y Values')\nax[0,1].set_title('Sactter Plot : Feature 2 vs Y')\n\nsns.scatterplot(x=X[:,2], y=y,ax=ax[1,0])\nax[1,0].set_xlabel('Feature 3 Values')\nax[1,0].set_ylabel('Y Values')\nax[1,0].set_title('Sactter Plot : Feature 3 vs Y')\n\nsns.scatterplot(x=X[:,3], y=y,ax=ax[1,1])\nax[1,1].set_xlabel('Feature 4 Values')\nax[1,1].set_ylabel('Y Values')\nax[1,1].set_title('Sactter Plot : Feature 4 vs Y')\n\nplt.show()","a27b0653":"from warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer, mean_squared_error\n\nrfr = RandomForestRegressor(verbose=0)\nprint('Parameters currently in use:\\n')\npprint(rfr.get_params())\nprint('CV score with default parameters : ',-cross_val_score(rfr, X, y, cv=4, scoring = make_scorer(mean_squared_error, greater_is_better=False)).mean())","71c31035":"from sklearn.model_selection import GridSearchCV\n\n# define the search space.\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [50, 75, 100],\n    'max_features': ['auto'],\n    'min_samples_leaf': [1],\n    'min_samples_split': [2],\n    'n_estimators': [100,200,500,1000]}\n\n# make scorer\nMSE = make_scorer(mean_squared_error, greater_is_better=False)\n\n# Configure the GridSearch model\nmodel = GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=-1, cv=4, scoring=MSE, verbose=2)\n# n_jobs=-1 : Means configured to use parallelism. use n_jobs=1 if use wish not to.\n\n# Training\nmodel.fit(X, y)\n\nprint('Random forest regression...')\nprint('Best Params:', model.best_params_)\nprint('Best CV Score:', -model.best_score_)","eed516d2":"import xgboost as xgb\n\nxgbr = xgb.XGBRegressor(seed=0)\n# A parameter grid for XGBoost\nparam_grid = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n\nmodel = GridSearchCV(estimator=xgbr, param_grid=param_grid, n_jobs=-1, cv=4, scoring=MSE)\nmodel.fit(X, y)\nscore = -model.best_score_\n\nprint('eXtreme Gradient Boosting regression...')\nprint(xgbr)\nprint('Best Params:\\n', model.best_params_)\nprint('Best CV Score:', score)","8f78d237":"## Learn More\n* [What are hyperparameters in machine learning?](https:\/\/www.quora.com\/What-are-hyperparameters-in-machine-learning)\n* [What is the Difference Between a Parameter and a Hyperparameter?](https:\/\/machinelearningmastery.com\/difference-between-a-parameter-and-a-hyperparameter\/)\n* [Hyperparameter optimization](https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization)","8b89ce7a":"## Please upvote if you found it useful and joyful!","5fb02e12":"Now Data is ready for training... First we need a regressor ... lets choose RandomForestRegressor... and see default values of hyperparamters","23cdc09f":"#### Libray Imports","b6c0502b":"## Data for the exercise\n\nFor this exercise let's generate a random regression problem using sklearn.datasets.make_regression.\n\n[Want to know more on make_regression?. have a look..](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_regression.html)","953d7761":"## Hyperparameter tuning using grid search\n\n> The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n([Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization#Grid_search))","99e993b3":"### Solving the problem using XGBRegressor","507e87d6":"Thats default parameters. Our target is to find the best set of hyperparameters from a subset taken from hyperparameter space.\nFor this we need to,\n* Define the parameter grid : -> Subset from the hyperparameter space to search.\n* Make a scorer : -> This scorer will be use to choose the best performing model.\n\nThen we create the GridSearch model using sklearn.model_selection.GridSearchCV and proceed to trainig. Let's see how its done.","7f6728a9":"### Let's see how it goes!. Time to calm the Baby...\n\n![](https:\/\/media.giphy.com\/media\/AGGz7y0rCYxdS\/giphy.gif)\n\n### Oh How Cute! Bravo!!!. Given the best solution!","38a4fe5b":"# Problem Description\n\n### There is an <font color=\"NAVY\">ANGRY BABY<\/font>. What <font color=\"NAVY\">FLAVOR<\/font> of <font color=\"NAVY\">ICE CREAM<\/font> would you think is the best to make the baby pleased?\n\n|||\n|:-:|:-:|\n|![](https:\/\/media.giphy.com\/media\/26gsnlYjswkyY3ENq\/giphy.gif)|![](https:\/\/media.giphy.com\/media\/HuAvEGGh9o8rC\/giphy.gif)|\n\n### Analogy:\n* Angry Baby   -> Problem\n* Ice Cream    -> Choosen ML Algorithm to solve the problem\n* Flavors      -> Configurations\/Properties of the ML Algorithm\n\n\n\n### How are you going to solve the problem?\n1. Choose the most popular flavor :-> <font color=\"GRAY\">Using your Machine Learning algorithm with default hyperparameters, You would most likely end up with a suboptimal model.<\/font>\n2. Choose the flavor baby likes the most :-> <font color=\"GRAY\">Tuning your models Hyperparameters to get the most skillful model.<\/font>\n\n\nKeep Reading to find out how to choose the flavor baby likes the most","d3737d7c":"These parameters express configurations of the model such as its structure or learning rates. They are called hyperparameters.\nThese values cannot be estimated from data. So hyperparameters are usually set before training. Think of it like there exist different flavors of the same machine learning algorithm.\n\n![](https:\/\/media.giphy.com\/media\/QmKtZGQn4cNi6EC15Y\/giphy.gif)\n\n## Some examples of hyperparameters :\n* Number of leaves or depth of a tree\n* Learning rate\n* Number of hidden layers in a deep neural network\n* Number of clusters in a k-means clustering\n\n## Why are they important?.\n\nIn addition to choosing the best suited Machine Learning model for a particular problem, selecting the best flavour of the selected model also decides the performance.\n\n## How to tune hyperparamters?\nThere are several ways of choosing a set of optimal hyperparameters for a learning algorithm.\n* Grid search\n* Manual search \n* Random search\n* Bayesian Optimization and More..\n\nI will stick to grid search in this discussion, links to study others are provided in the bottom of the notebook. ","f67d6fa8":"# Table Of Contents\n\n1. [What are hyperparameters?]()\n1. [Some examples of hyperparameters]()\n1. [Why they are important?]()\n1. [How to tune hyperparamters?]()\n1. [generate a regression problem for the exercise]()\n1. [Hyperparameter Tuning Using Grid Search]()","95e37ed6":"# COMPREHENSIVE GUIDE TO HYPERPARAMETER TUNING\n[Vikum Wijesinghe](https:\/\/www.linkedin.com\/in\/vikumwijesinghe\/) - September 2019\n\nOther Kernels: https:\/\/www.kaggle.com\/vikumsw\/kernels\n\n---","9acfd6e2":"## What are hyperparameters?\n\nLet's create a Random Forest Regressor model for demonstration,","7d33ec73":"Looks like XGBRegressor is way better than the random Forest Regressor.. And that's a another kind of pleaser.. Let's say Choclate.  Although we found out that choclate is betterto calm the baby than ice cream, we only got ice cream. so lets proceed with the best ice cream flavor...","b032fc12":"Explanation : \nWe have scored random forest regressor for following hyperparameter settings.\n\n| bootstrap | max_depth  | max_features  | min_samples_leaf  |  min_samples_split | n_estimators  |\n|:-:|:-:|:-:|:-:|:-:|---|\n| True  | 50 |  'auto' |  1 | 2  |  100 |\n| True  | 50 |  'auto' |  1 | 2  |  100 |\n| True  | 50 |  'auto' |  1 | 2  |  100 |\n| True  | 50 |  'auto' |  1 | 2  | 200 |\n| True  | 75 |  'auto' |  1 | 2  |  200 |\n| True  | 75 |  'auto' |  1 | 2  |  200 |\n| True  | 75 |  'auto' |  1 | 2  |  5000 |\n| True  | 75 |  'auto' |  1 | 2  |  5000 |\n| True  | 100 |  'auto' |  1 | 2  |  5000 |\n| True  | 100 |  'auto' |  1 | 2  |  1000 |\n| True  | 100 |  'auto' |  1 | 2  |  1000 |\n| True  | 100 |  'auto' |  1 | 2  |  1000 |\n\n### Next??\nNow since we have found the best random forest regressor inside our hyperparameter search space for this particular problem. we can proceed to prediction.\nWe can simple use,\n\n    Y_predictions = model.predict(X)"}}