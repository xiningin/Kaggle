{"cell_type":{"fa5126a9":"code","9bdf61cf":"code","d13b1a3c":"code","a8ad7860":"code","136daf15":"code","3f4cb0d7":"code","b06792b3":"code","49bb6b83":"code","37d08862":"code","72437dba":"code","d9a384b3":"code","1274a07b":"code","04bd115b":"code","92109064":"code","4505bc43":"code","668e184e":"code","352520ca":"code","ab0a9d2e":"code","062a05c7":"code","0ad2a1de":"code","1ded9800":"code","ff389ea6":"code","f72baa9c":"code","062767a0":"code","d4bf8e27":"code","1c9e84aa":"code","81eeb4b0":"markdown","007942f8":"markdown","cd9069d6":"markdown","2cbeb304":"markdown","6f161098":"markdown","24c03f4a":"markdown","6a053201":"markdown","c9e803cf":"markdown","a5c8a441":"markdown","cadf1082":"markdown","47b8df1b":"markdown","fd2feadd":"markdown","d5a61965":"markdown","b40167d2":"markdown","08aa41d9":"markdown","ba0a4e84":"markdown","20e7b25c":"markdown","ea3d72b2":"markdown","e943efc9":"markdown","e84dddf3":"markdown","c55d61ca":"markdown","8f82f064":"markdown","355a8337":"markdown","b676238a":"markdown","18ce394b":"markdown","d6b9100f":"markdown","a3472d64":"markdown","6a0cdb26":"markdown","8bfb0c55":"markdown","74ca200d":"markdown","1248a388":"markdown","e0491025":"markdown","2c1518a6":"markdown","a4b3c065":"markdown","58e106cb":"markdown"},"source":{"fa5126a9":"# use to visualize missing value\n!pip install missingno","9bdf61cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n## Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n# clustering algorithms\nfrom sklearn.cluster import KMeans,AgglomerativeClustering,DBSCAN\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.metrics import silhouette_samples, silhouette_score","d13b1a3c":"# load dataset\ncustomer_df = pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","a8ad7860":"customer_df.head()","136daf15":"customer_df.shape","3f4cb0d7":"customer_df.info()","b06792b3":"customer_df.describe(include='all')","49bb6b83":"customer_dtype = customer_df.dtypes\ncustomer_dtype.value_counts()","37d08862":"customer_df.isnull().sum().sort_values(ascending = False).head()","72437dba":"msno.matrix(customer_df)","d9a384b3":"numerical_features = [col for col in customer_df.columns if customer_df[col].dtypes != 'O']\ndiscrete_features = [col for col in numerical_features if len(customer_df[col].unique()) < 10 and col not in ['CustomerID']]\ncontinuous_features = [feature for feature in numerical_features if feature not in discrete_features+['CustomerID']]\ncategorical_features = [col for col in customer_df.columns if customer_df[col].dtype == 'O']\n\nprint(\"Total Number of Numerical Columns : \",len(numerical_features))\nprint(\"Number of discrete features : \",len(discrete_features))\nprint(\"No of continuous features are : \", len(continuous_features))\nprint(\"Number of categorical features : \",len(categorical_features))","1274a07b":"f, axes = plt.subplots(2,2 , figsize=(20, 7), sharex=False)\npos = 1\nfor i, feature in enumerate(continuous_features):\n\n  plt.subplot(1 , 3 , pos)\n  ax = sns.histplot(data=customer_df, x = feature,kde=True,palette=\"husl\") # ax=axes[i%2, i\/\/2]\n  ax.lines[0].set_color('crimson')\n  pos = pos + 1","04bd115b":"# get the features except object types\nnumeric_feats = customer_df.dtypes[customer_df.dtypes != 'object'].index\n\n# check the skew of all numerical features\nskewed_feats = customer_df[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending = False)\nprint('\\n Skew in numberical features: \\n')\nskewness_df = pd.DataFrame({'Skew' : skewed_feats})\nprint(skewness_df.head(10))","92109064":"sns.countplot(x='Gender', data=customer_df, palette=\"Set3\")","4505bc43":"#Pairplot \nsns.pairplot(customer_df, vars=[\"Age\", \"Annual Income (k$)\", \"Spending Score (1-100)\"],  kind =\"reg\", hue = \"Gender\", palette=\"husl\", markers = ['o','D'])","668e184e":"customer_corr = customer_df.corr(method='spearman')\nplt.figure(figsize=(8,8))\nsns.heatmap(customer_corr, cmap=\"icefire\", linewidths=.5) #'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r'","352520ca":"customer_df.drop(columns='CustomerID',axis=1,inplace=True)","ab0a9d2e":"# Generate one-hot dummy columns\ncustomer_df = pd.get_dummies(customer_df).reset_index(drop=True)","062a05c7":"# pre_precessing_pipeline = make_pipeline(RobustScaler()) \n\n# customer_scaled_df = pre_precessing_pipeline.fit_transform(customer_df)\n\n# print(customer_scaled_df.shape)","0ad2a1de":"inertia = []\nrange_val = range(1,15)\nfor i in range_val:\n  kmean = KMeans(n_clusters=i)\n  kmean.fit_predict(pd.DataFrame(customer_df))\n  inertia.append(kmean.inertia_)\nplt.plot(range_val,inertia,'bx-')\nplt.xlabel('Values of K') \nplt.ylabel('Inertia') \nplt.title('The Elbow Method using Inertia') \nplt.show()","1ded9800":"# apply kmeans algorithm\nkmeans_model=KMeans(5)\nkmeans_clusters = kmeans_model.fit_predict(customer_df)\n","ff389ea6":"# apply agglomerative algorithm\nagglo_model = AgglomerativeClustering(linkage=\"ward\",n_clusters=5)\nagglomerative_clusters = agglo_model.fit_predict(customer_df)","f72baa9c":"GaussianMixture_model = GaussianMixture(n_components=5)\ngmm_clusters = GaussianMixture_model.fit_predict(customer_df)","062767a0":"model_dbscan = DBSCAN(eps=3, min_samples=17)\ndbscan_clusters = model_dbscan.fit_predict(customer_df)","d4bf8e27":"def silhouette_method(df,algo,y_pred):\n  print('=================================================================================')\n  print('Clustering ',algo,\" : silhouette score : \",silhouette_score(df,y_pred) )\n\n\nsilhouette_method(customer_df,' : KMeans',kmeans_clusters)\nsilhouette_method(customer_df,' : Agglomerative',agglomerative_clusters)\nsilhouette_method(customer_df,' : GaussianMixture',gmm_clusters)\nprint('=================================================================================')","1c9e84aa":"customer_df[\"label\"] = kmeans_clusters\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n \nfig = plt.figure(figsize=(20,13))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(customer_df.Age[customer_df.label == 0], customer_df[\"Annual Income (k$)\"][customer_df.label == 0], customer_df[\"Spending Score (1-100)\"][customer_df.label == 0], c='blue', s=60)\nax.scatter(customer_df.Age[customer_df.label == 1], customer_df[\"Annual Income (k$)\"][customer_df.label == 1], customer_df[\"Spending Score (1-100)\"][customer_df.label == 1], c='red', s=60)\nax.scatter(customer_df.Age[customer_df.label == 2], customer_df[\"Annual Income (k$)\"][customer_df.label == 2], customer_df[\"Spending Score (1-100)\"][customer_df.label == 2], c='green', s=60)\nax.scatter(customer_df.Age[customer_df.label == 3], customer_df[\"Annual Income (k$)\"][customer_df.label == 3], customer_df[\"Spending Score (1-100)\"][customer_df.label == 3], c='orange', s=60)\nax.scatter(customer_df.Age[customer_df.label == 4], customer_df[\"Annual Income (k$)\"][customer_df.label == 4], customer_df[\"Spending Score (1-100)\"][customer_df.label == 4], c='purple', s=60)\nax.view_init(30, 185)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Annual Income (k$)\")\nax.set_zlabel('Spending Score (1-100)')\nplt.show()","81eeb4b0":"### 6.4. GaussianMixture Model based clustering","007942f8":"## If you find this notebook useful, don't forget to **UPVOTE \u23eb**.\n## Follow me on [Github](https:\/\/github.com\/sidharth178). I used to upload good data science projects.","cd9069d6":"### 6.2. K-Means Clustering","2cbeb304":"# **6. Model Development**","6f161098":"### 4.1.5. Data Type ","24c03f4a":"### 5.3. Normalization","6a053201":"### 4.1.2. Data Shape ","c9e803cf":"### 5.2. Encoding Categorical Features","a5c8a441":"### 4.3.3.2. Check Distribution - Continuous","cadf1082":"- Let's see how gender of customers affects to all other features.","47b8df1b":"### 4.6. Data Correlation","fd2feadd":"- From the above analysis,we saw that \"K-Means\" algorithm has high silhouette score as compared to others. So we'll apply K-Means algorithm for our model.","d5a61965":"- From the above elbow method we see that **K = 5** is the best K value for our clustering","b40167d2":"# **1. Objective** \nThis case requires to develop a customer segmentation to understand customers behavior and sepparate them in different groups or cluster according to their preferences, and once the division is done, this information can be given to marketing team so they can plan the strategy accordingly.\n\n### **Data Description :**\nThe sample Dataset summarizes the usage behavior of about 200 active customers during the last 3 months. The file is at a customer level with 5 behavioral variables.\n\n### **Attribute Information :**\nFollowing is the Data Dictionary for customer dataset :-\n\n**CustomerID** : Unique ID assigned to the customer\n\n**Gender** :Gender of the customer\n\n**Age** : Age of the customer\n\n**Annual Income (k$)** : Annual Income of the customee\n\n**Spending Score** : Score assigned by the mall based on customer behavior and spending nature","08aa41d9":"# **4. Exploratory Data Analysis**\n### 4.1. Data Exploration\n\nFor both train and test dataset, We'll explore following things\n\n- First 5 rows\n- Data shape\n- Data information\n- Statistical description\n- Data types\n- Null value","ba0a4e84":"### 4.1.7. Visualize missing value using **Misingno** - Train Data","20e7b25c":"### 4.1.1. First 5 records","ea3d72b2":"- From the above pairplot we observe that green colour has higher ratio than pink colour as there are more female customers than male.\n\n\n- *True Fact*\n> > **\"I make clothes, women make fashion.\" \u2014Azzedine Ala\u00efa**\n\n","e943efc9":"# **2. Install & Import Libraries**","e84dddf3":"### 6.3. Agglomerative Clustering","c55d61ca":"### 6.7. Cluster Visualization","8f82f064":"### 4.1.3. Data Information","355a8337":"### 4.1.8. Report - Data Exploration\nFrom the above data exploration we saw that\n- There is no missing value present\n- Shape of the dataset is (200, 5)\n- memory usage by dataset : 7.9+ KB\n- There are 4 integer and 1 object type feature present","b676238a":"# **3. Load Datasets**","18ce394b":"In this step we'll apply various clustering algorithms and check which algorithm is best for our dataset. We'r going to use below algorithms.\n\n- Kmeans Clustering\n- Agglomerative Clustering\n- GaussianMixture Model based clustering\n- DBSCAN Clustering\n","d6b9100f":"### 6.1. Hyperparameter tuning \n\nTo find the best K value there are many techniques we can use.\n\n1. **Elbow Method**: The elbow method finds the value of the optimal number of clusters using the total within-cluster sum of square values.\n2. **Average silhouette method** : It is a measure of how well each data point fits its corresponding cluster. This method evaluates the quality of clustering. As a general rule, a high average silhouette width denotes better clustering output.\n3. **Gap statistic method** : It is a measure of the value of gap statistics. Gap statistics is the difference between the total intracluster changes for various values of k compared to their expected values. The optimal number of clusters is the value that maximizes the value of gap statistics.\n\n- Here we'll use **Elbow Method** to find the best K Value.\n\n### 6.2. Elbow Method","a3472d64":"### 4.1.4. Statistical description ","6a0cdb26":"Above distribution shows that:\n- The distribution of continuous features are normally distributed.\n","8bfb0c55":"# <center><b> Customer Segmentation<\/center>","74ca200d":"### 4.3.3.2. Check Distribution - Categorical","1248a388":"### 6.5. DBSCAN Clustering","e0491025":"# **5. Feature Engineering**","2c1518a6":"### 5.1. Drop Columns","a4b3c065":"### 4.1.6. Null Value ","58e106cb":"### 6.6. Cluster Validity Indices\n\nTo compare which clustering algorithm is best for our model, we use various cluster validity indices.\n1. Dunn Index\n2. Davies Bouldin Index\n3. Silhouette Score\n4. Calinski - Harabasz Index\n\n- \"algorithms that produce clusters with high Dunn index are more desirable\" -Wikipedia\n- \"Objects with a high silhouette value are considered well clustered\" -Wikipedia\n- \"clustering algorithm that produces a collection of clusters with the smallest Davies\u2013Bouldin index is considered the best algorithm\" -Wikipedia\n\n- **Here we'll use silhouette score method to choose the best clustering algorithm**"}}