{"cell_type":{"0a693767":"code","cbcf533e":"code","f32f34b6":"code","d2a0454b":"code","86d1cb84":"code","ce0d6f67":"code","5ee3ce68":"code","a03f6743":"code","74f2dfce":"markdown","7ba68b4e":"markdown","56a6615f":"markdown","c43a184f":"markdown","e0b07bcc":"markdown","dab54707":"markdown","a3368a45":"markdown","2c05b6cf":"markdown","c5fe963b":"markdown","aa93804f":"markdown","cfbde38c":"markdown","6eca55ec":"markdown","7a18578a":"markdown","ec3a41db":"markdown","ca18c2ea":"markdown","91494104":"markdown","ff132234":"markdown"},"source":{"0a693767":"import librosa            # Python Audio Manipulation Library\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\n\nimport tensorflow.keras as tk\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import GlobalMaxPooling2D, Dense, Flatten,MaxPooling2D, GlobalAveragePooling2D, Dropout, Input, Concatenate, BatchNormalization, Conv2D\nfrom tensorflow.keras import Model\nimport matplotlib.pyplot as plt","cbcf533e":"train_dir = '..\/input\/birdcall-spectrograms-cornell-birdcall-challenge\/train'\ntrain_csv_dir = '..\/input\/birdsong-recognition\/train.csv'\n\ntest_dir = '..\/input\/birdsong-recognition\/test_audio\/'\ntest_csv_dir = '..\/input\/birdsong-recognition\/test.csv'\n####################################################################################################\/test\/exa_test_......\nexa_test_dir = '..\/input\/birdcall-spectrograms-cornell-birdcall-challenge\/example_test_image\/'\nexa_test_csv_dir = '..\/input\/birdsong-recognition\/example_test_audio_summary.csv'\n\ntrain_df = pd.read_csv(train_csv_dir)\nval_df = pd.read_csv(exa_test_csv_dir)\ntest_df = pd.read_csv(test_csv_dir)\n\n########## [This is Not used for validation, bcz of ebird_code error] ##########\nval_list = []\nval_list_y = []\nval_list_df = pd.DataFrame([])\n\nfor img in os.listdir(exa_test_dir):\n    val_list.append(exa_test_dir +img)\n    img_name = img.split('.')[0]\n    val_list_y.append(' '.join(val_df[val_df['filename_seconds']==img_name]['birds'].astype(str).values))\n\nval_list_df['image'] = val_list\nval_list_df['label'] = val_list_y","f32f34b6":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   validation_split=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size = (150, 150), \n                                                    class_mode = 'categorical',\n                                                    batch_size = 160,\n                                                    shuffle = True,\n                                                    subset= 'training')\n\nval_generator = train_datagen.flow_from_directory(train_dir,\n                                                    target_size = (150, 150), \n                                                    class_mode = 'categorical',\n                                                    batch_size = 160,\n                                                    shuffle = True,\n                                                    subset= 'validation')\n\nprint(train_generator.class_indices)","d2a0454b":"model = Xception(weights='imagenet', include_top=False, input_shape = (150, 150, 3), pooling = 'max')\nfinal_output = Dense(264, activation = 'softmax')(model.output)\nmodel = Model(inputs = model.input, outputs = final_output)","86d1cb84":"from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n\n\n# autosave best Model\nbest_model = ModelCheckpoint(\"model\", monitor='val_accuracy', mode='max',verbose=1, save_best_only=True)\n\nearlystop = EarlyStopping(monitor = 'val_accuracy',\n                          patience = 10,\n                          mode = 'auto',\n                          verbose = 1,\n                          restore_best_weights = True)\n\nacc_thresh = 0.998\n\nclass myCallback(Callback): \n    def on_epoch_end(self, epoch, logs={}): \n        if(logs.get('accuracy') > acc_thresh):   \n          print(\"\\nWe have reached %2.2f%% accuracy, so we will stopping training.\" %(acc_thresh*100))   \n          self.model.stop_training = True\n\ncallbacks = [myCallback(), best_model, earlystop]","ce0d6f67":"model.compile(optimizer='Adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\nhistory = model.fit(train_generator,\n                              epochs = 100,\n                              steps_per_epoch = len(train_generator),\n                              validation_data = val_generator,\n                              validation_steps = len(val_generator),\n                              callbacks = callbacks,\n                              verbose= 1)","5ee3ce68":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='best')\nplt.show()","a03f6743":"import shutil\n\nshutil.make_archive('trained_resnet50', 'zip', '\/kaggle\/working\/resnet50')","74f2dfce":"## Importing Library","7ba68b4e":"Found 17134 images belonging to 264 classes.\n\nFound 4209 images belonging to 264 classes.\n\n{'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4, 'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9, 'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14, 'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19, 'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24, 'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29, 'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34, 'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39, 'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44, 'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49, 'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54, 'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59, 'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64, 'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69, 'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74, 'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79, 'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84, 'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89, 'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94, 'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99, 'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104, 'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109, 'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114, 'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119, 'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124, 'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129, 'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134, 'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139, 'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144, 'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149, 'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154, 'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159, 'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164, 'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169, 'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174, 'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179, 'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184, 'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189, 'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194, 'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199, 'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204, 'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209, 'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214, 'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219, 'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224, 'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229, 'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234, 'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239, 'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244, 'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249, 'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254, 'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259, 'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263}","56a6615f":"## Reference Directory","c43a184f":"![download.png](attachment:download.png)","e0b07bcc":"# Notebook_1 || Part 2\n\nThis Notebook is the **2nd Part** of 'Birdsong_Classifier_Keras_CNN' Notebook for \"Cornell Birdcall Identification\" challenge.\n\nIn the [Previous Part](https:\/\/www.kaggle.com\/timothyalexjohn\/birdsong-classifier-keras-cnn-part-1-notebook-1) of Notebook_1 we converted Audio Files to Spectrogram Image Files using Librosa. \n\nIn this Part we upload last Notebook's Output and train CNN on the extracted features. \n\nIn the [Next part](https:\/\/www.kaggle.com\/timothyalexjohn\/birdsong-classifier-keras-cnn-part-3-notebook-1), trained-model of this notebook and uploaded to next Notebook for prediction.","dab54707":"## Data Generator","a3368a45":"### Please Upvote if you liked this Notebook!! ","2c05b6cf":"## Plotting Accuracy","c5fe963b":"## Model","aa93804f":"## Notebooks in this Challenge:\n\n* Notebook_1:\n       \n > [Part 1](https:\/\/www.kaggle.com\/timothyalexjohn\/birdsong-classifier-keras-cnn-part-1-notebook-1):  Converting Audio Files to Spectrogram Image Files using Librosa\n  \n > [Part 2](https:\/\/www.kaggle.com\/timothyalexjohn\/birdsong-classifier-keras-cnn-part-2-notebook-1):  Running CNN on Extracted Features \/ Training\n \n > [Part 3](https:\/\/www.kaggle.com\/timothyalexjohn\/birdsong-classifier-keras-cnn-part-3-notebook-1): Prediction of test data\n \n\n* Notebook_2:\n        \n >Running LSTM Cells directly on Audio Files\n \n \n...Links will be updated soon!","cfbde38c":"# Important:\n#### 1. Please add the [Previous Notebook's output Dataset](https:\/\/www.kaggle.com\/timothyalexjohn\/birdcall-spectrograms-cornell-birdcall-challenge) to this Notebook.\n\n#### 2. Trained Model (Output) of this Notebook can be found [here](https:\/\/www.kaggle.com\/timothyalexjohn\/xception-birdcall-model). ","6eca55ec":"### ZIP Trained Model","7a18578a":"Epoch 1\/100\n  2\/108 [..............................] - ETA: 2:12 - loss: 6.5205 - accuracy: 0.0031WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.6736s vs `on_train_batch_end` time: 1.8212s). Check your callbacks.\n108\/108 [==============================] - ETA: 0s - loss: 5.3641 - accuracy: 0.0189\nEpoch 00001: val_accuracy improved from -inf to 0.02851, saving model to model\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/training\/tracking\/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/training\/tracking\/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 315s 3s\/step - loss: 5.3641 - accuracy: 0.0189 - val_loss: 6.2479 - val_accuracy: 0.0285\nEpoch 2\/100\n108\/108 [==============================] - ETA: 0s - loss: 3.8983 - accuracy: 0.1499\nEpoch 00002: val_accuracy improved from 0.02851 to 0.14992, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 315s 3s\/step - loss: 3.8983 - accuracy: 0.1499 - val_loss: 4.2193 - val_accuracy: 0.1499\nEpoch 3\/100\n108\/108 [==============================] - ETA: 0s - loss: 2.8843 - accuracy: 0.3127\nEpoch 00003: val_accuracy improved from 0.14992 to 0.22357, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 314s 3s\/step - loss: 2.8843 - accuracy: 0.3127 - val_loss: 3.7993 - val_accuracy: 0.2236\nEpoch 4\/100\n108\/108 [==============================] - ETA: 0s - loss: 2.1958 - accuracy: 0.4508\nEpoch 00004: val_accuracy improved from 0.22357 to 0.28890, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 314s 3s\/step - loss: 2.1958 - accuracy: 0.4508 - val_loss: 3.4109 - val_accuracy: 0.2889\nEpoch 5\/100\n108\/108 [==============================] - ETA: 0s - loss: 1.5313 - accuracy: 0.5903\nEpoch 00005: val_accuracy did not improve from 0.28890\n108\/108 [==============================] - 295s 3s\/step - loss: 1.5313 - accuracy: 0.5903 - val_loss: 3.5206 - val_accuracy: 0.2754\nEpoch 6\/100\n108\/108 [==============================] - ETA: 0s - loss: 1.0594 - accuracy: 0.7054\nEpoch 00006: val_accuracy did not improve from 0.28890\n108\/108 [==============================] - 294s 3s\/step - loss: 1.0594 - accuracy: 0.7054 - val_loss: 4.3753 - val_accuracy: 0.2518\nEpoch 7\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.7886\nEpoch 00007: val_accuracy improved from 0.28890 to 0.31884, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 313s 3s\/step - loss: 0.7211 - accuracy: 0.7886 - val_loss: 3.7360 - val_accuracy: 0.3188\nEpoch 8\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.8638\nEpoch 00008: val_accuracy did not improve from 0.31884\n108\/108 [==============================] - 293s 3s\/step - loss: 0.4494 - accuracy: 0.8638 - val_loss: 4.2238 - val_accuracy: 0.3032\nEpoch 9\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.4121 - accuracy: 0.8782\nEpoch 00009: val_accuracy did not improve from 0.31884\n108\/108 [==============================] - 294s 3s\/step - loss: 0.4121 - accuracy: 0.8782 - val_loss: 4.6084 - val_accuracy: 0.2960\nEpoch 10\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.2398 - accuracy: 0.9269\nEpoch 00010: val_accuracy improved from 0.31884 to 0.33072, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 319s 3s\/step - loss: 0.2398 - accuracy: 0.9269 - val_loss: 4.6120 - val_accuracy: 0.3307\nEpoch 11\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9353\nEpoch 00011: val_accuracy did not improve from 0.33072\n108\/108 [==============================] - 297s 3s\/step - loss: 0.2180 - accuracy: 0.9353 - val_loss: 5.5805 - val_accuracy: 0.2583\nEpoch 12\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.3296 - accuracy: 0.9043\nEpoch 00012: val_accuracy improved from 0.33072 to 0.33262, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 319s 3s\/step - loss: 0.3296 - accuracy: 0.9043 - val_loss: 4.5209 - val_accuracy: 0.3326\nEpoch 13\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9601\nEpoch 00013: val_accuracy did not improve from 0.33262\n108\/108 [==============================] - 297s 3s\/step - loss: 0.1361 - accuracy: 0.9601 - val_loss: 5.1286 - val_accuracy: 0.3165\nEpoch 14\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9551\nEpoch 00014: val_accuracy did not improve from 0.33262\n108\/108 [==============================] - 293s 3s\/step - loss: 0.1570 - accuracy: 0.9551 - val_loss: 4.5522 - val_accuracy: 0.3272\nEpoch 15\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9566\nEpoch 00015: val_accuracy did not improve from 0.33262\n108\/108 [==============================] - 294s 3s\/step - loss: 0.1478 - accuracy: 0.9566 - val_loss: 6.0066 - val_accuracy: 0.2815\nEpoch 16\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9561\nEpoch 00016: val_accuracy did not improve from 0.33262\n108\/108 [==============================] - 296s 3s\/step - loss: 0.1446 - accuracy: 0.9561 - val_loss: 5.0713 - val_accuracy: 0.3184\nEpoch 17\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.0999 - accuracy: 0.9709\nEpoch 00017: val_accuracy improved from 0.33262 to 0.34996, saving model to model\nINFO:tensorflow:Assets written to: model\/assets\n108\/108 [==============================] - 319s 3s\/step - loss: 0.0999 - accuracy: 0.9709 - val_loss: 4.8199 - val_accuracy: 0.3500\nEpoch 18\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9654\nEpoch 00018: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 293s 3s\/step - loss: 0.1214 - accuracy: 0.9654 - val_loss: 5.2382 - val_accuracy: 0.3338\nEpoch 19\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1304 - accuracy: 0.9618\nEpoch 00019: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 293s 3s\/step - loss: 0.1304 - accuracy: 0.9618 - val_loss: 5.9411 - val_accuracy: 0.2965\nEpoch 20\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9515\nEpoch 00020: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 294s 3s\/step - loss: 0.1602 - accuracy: 0.9515 - val_loss: 5.4767 - val_accuracy: 0.3150\nEpoch 21\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9484\nEpoch 00021: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 294s 3s\/step - loss: 0.1765 - accuracy: 0.9484 - val_loss: 4.9656 - val_accuracy: 0.3288\nEpoch 22\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1274 - accuracy: 0.9634\nEpoch 00022: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 295s 3s\/step - loss: 0.1274 - accuracy: 0.9634 - val_loss: 5.3694 - val_accuracy: 0.3355\nEpoch 23\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9810\nEpoch 00023: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 296s 3s\/step - loss: 0.0618 - accuracy: 0.9810 - val_loss: 5.2083 - val_accuracy: 0.2956\nEpoch 24\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9726\nEpoch 00024: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 294s 3s\/step - loss: 0.0993 - accuracy: 0.9726 - val_loss: 5.9058 - val_accuracy: 0.3053\nEpoch 25\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9513\nEpoch 00025: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 295s 3s\/step - loss: 0.1688 - accuracy: 0.9513 - val_loss: 6.2194 - val_accuracy: 0.2853\nEpoch 26\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9619\nEpoch 00026: val_accuracy did not improve from 0.34996\n108\/108 [==============================] - 295s 3s\/step - loss: 0.1251 - accuracy: 0.9619 - val_loss: 5.0750 - val_accuracy: 0.3400\nEpoch 27\/100\n108\/108 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9598\nEpoch 00027: val_accuracy did not improve from 0.34996\nRestoring model weights from the end of the best epoch.\n108\/108 [==============================] - 292s 3s\/step - loss: 0.1368 - accuracy: 0.9598 - val_loss: 5.3347 - val_accuracy: 0.3343\nEpoch 00027: early stopping","ec3a41db":"## Compiling and Training...","ca18c2ea":"## Callback Functions","91494104":"## Reference\n* [How to run Kaggle Dataset in Google Colab](https:\/\/www.kaggle.com\/general\/74235)\n\n\n\n# Note\n\nPart_1 and Part_2 can be combined by directly training with train_on_batch method (without downloading and re-uploading it)\n\nSee You in the [Final Notebook](https:\/\/www.kaggle.com\/timothyalexjohn\/birdsong-classifier-keras-cnn-part-3-notebook-0) of Notebook_1 series!!","ff132234":"## Training in Google Colab"}}