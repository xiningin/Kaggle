{"cell_type":{"c33f246e":"code","c3e53d50":"code","51786512":"code","2bee5bea":"code","9bfa8689":"code","648a95d5":"code","d1fec419":"code","fb6aa976":"code","b2ec46ae":"code","9a4b625b":"code","bf1d1d2e":"code","9bfc0da6":"code","e6ed2372":"code","cec27378":"code","67bbcc72":"code","630580d5":"code","1957137f":"code","e0229fa5":"code","f39be500":"code","8dc06640":"code","55bc049d":"code","364fe1bc":"code","c69c81ba":"code","9d322a04":"code","4d2340f1":"code","551490c6":"code","3afc3704":"code","5c35c0e0":"code","119eb825":"code","e3c175f2":"code","92a950e1":"code","fdfe0a3a":"code","1f8c8021":"code","76c0276b":"code","9b7ae1c2":"code","271f9b6f":"code","89ddd394":"code","80547328":"code","0c5d194b":"code","5daaee2a":"code","1df6c288":"code","a22c70a9":"code","98bc6d9f":"code","96ec0784":"markdown","fecf0e65":"markdown","57d29ca0":"markdown","7e237839":"markdown","bb250cf8":"markdown","6af350dc":"markdown","5e04c03e":"markdown","d128202a":"markdown","d804b102":"markdown","04a102d5":"markdown"},"source":{"c33f246e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3e53d50":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n%matplotlib inline","51786512":"train = pd.read_csv(\"..\/input\/netflix-appetency\/train.csv\")\ntrain.head()","2bee5bea":"train.describe()","9bfa8689":"# As seen by the message displayed, there are too many columns \ntrain.info()","648a95d5":"# Checking for missing data\nprint(\"Missing data for each feature\")\nprint(train.isna().sum(axis=1))","d1fec419":"feature_columns = train.iloc[:, 2:].columns\ntarget_column = 'target'","fb6aa976":"print(train[target_column].value_counts())\ntrain[target_column].value_counts().plot(kind='bar')\nplt.show()\n# From the visualization below, there are more 0s than 1s by a factor of 2.\n# So, the dataset is kind of IMBALANCED. \n# Therefore, at some point, i'll have to find a way to work \n# on an imbalanced dataset","b2ec46ae":"# Getting the numerical columns\nnumerical_columns = train[feature_columns].select_dtypes(np.number).columns\nprint(\"# of numerical columns: \", len(numerical_columns))\n\n# Getting the categorical columns\ncategorical_columns = train[feature_columns].select_dtypes('object').columns\nprint(\"# of categorical columns: \", len(categorical_columns))","9a4b625b":"# Visualizing the numerical columns\n\n# # The text part\n# for n in numerical_columns:\n#     print(\"Feature: \", n)\n#     # The rows having the biggest impact can be determined\n#     print(train[n].value_counts(bins=10, sort=False))\n#     break   # There are too many columns to display the information\n    \n# The graph part\nfor index, column in enumerate(numerical_columns):\n    print(f\"Index: {index}\\tColumn: {column}\")\n    train[column].plot.hist()\n    plt.show()\n    # There are over 400 columns in the numerical columns, as such, \n    # I decided to limit to the first 5 columns.\n    # Also, you can try and get columns within a fixed range, like 100 and 110, ...\n    if index > 5:\n        break","bf1d1d2e":"# VISUALIZING THE CATEGORICAL COLUMNS\nfor index, column in enumerate(categorical_columns):\n    # the indices can be chosen by the user. it could be any two values\n    # Remember there are 92 columns for categorical columns\n    if index > 40 and index < 45:\n        print(f\"Index: {index}\\tColumn: {column}\")\n        train[column].value_counts().plot.hist()\n        plt.show()","9bfc0da6":"# Now we can deal with the missing values.\nfrom sklearn.impute import SimpleImputer\n\n# making a copy of the dataset\ntrain_impute = train.copy()\n\n# i'll try implementing different options using mean, median\n# for the numerical columns, once imputed the number of columns reduces.\n# I don't know why, as such, i'll be indexing to make sure everything fits.\nmean_imputer = SimpleImputer(strategy='mean')\nimputed_mean = mean_imputer.fit_transform(train_impute[numerical_columns])\n\nmedian_imputer = SimpleImputer(strategy=\"median\")\nimputed_median = median_imputer.fit_transform(train_impute[numerical_columns])\n\n\n# then for categorical, i'll be considering the two most used methods\n# constant and most frequent \nconstant_imputer = SimpleImputer(strategy='constant', fill_value='Missing')\nimputed_constant = constant_imputer.fit_transform(train_impute[categorical_columns])\n\nfrequent_imputer = SimpleImputer(strategy='most_frequent', fill_value='Missing')\nimputed_frequent = constant_imputer.fit_transform(train_impute[categorical_columns])","e6ed2372":"# applying some of the imputations into the dataset\n# considering the categorical using constant\ntrain_impute[categorical_columns] = imputed_constant\n\n# based on the shape of the imputed numerical values, i'll apply the need changed\n# imputed_mean.shape\n# imputed_median.shape\ntrain_impute[numerical_columns[:411]] = imputed_mean","cec27378":"# checking for any missing data\nprint(\"Checking for missing values\")\nprint(sum(train_impute[feature_columns].isna().sum(axis=1).values))","67bbcc72":"train_impute[feature_columns].head()","630580d5":"for index, column in enumerate(numerical_columns):\n    if index > 20 and index < 25:\n        print(column)\n        train_impute[column].value_counts().plot.hist()\n        plt.show()","1957137f":"# VISUALIZING THE CATEGORICALS\nfor index, column in enumerate(categorical_columns):\n    if index > 80 and index < 85:\n        print(column)\n        train_impute[column].value_counts().plot.hist()\n        plt.show()","e0229fa5":"# checking for outliers\n# using quantiles\n# q1 = df[c].quantile(0.25)\n# q3 = df[c].quantile(0.75)\n# iqr = q3 - q1\n# dropIndices = df[df[c] > q3 + 1.5*iqr].index\n# df.drop(dropIndices, inplace=True)\n# dropIndices = df[df[c] < q1 - 1.5*iqr].index\n# df.drop(dropIndices, inplace=True)\n# other formulas\n# dropIndices = df[df[c] > df[c].max()*9\/10].index\n# train.drop(dropIndices, inplace=True)\n\n\n# for c in numerical_columns:\n#     q1 = train_impute[c].quantile(0.25)\n#     q3 = train_impute[c].quantile(0.75)\n#     iqr = q3 - q1\n#     dropIndices = train_impute[train_impute[c] > q3 + 1.5*iqr].index\n#     train_impute.drop(dropIndices, inplace=True)\n    \n#     dropIndices = train_impute[train_impute[c] < q1 - 1.5*iqr].index\n#     train_impute.drop(dropIndices, inplace=True)","f39be500":"# # now converting all categoricals into integers \n# for cname in train_impute.select_dtypes('object'):\n#     # factorize() produces array and indices as its outputs, for this case, am just\n#     # looking for the arrays. No need for the index\n#     train_impute[cname], _ = train_impute[cname].factorize()\n    \n\n# #  converting all categoricals into integers using label encoder\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nfor cname in train_impute.select_dtypes('object'):\n    train_impute[cname] = encoder.fit_transform(train_impute[cname])\n    \n    \ntrain_impute[feature_columns].head()","8dc06640":"train_impute[categorical_columns[80:]].corrwith(train[target_column]) >=.05","55bc049d":"positive_features = ['feature_5','feature_6','feature_7','feature_43','feature_48','feature_79','feature_118','feature_127','feature_136','feature_143',\n                    'feature_168','feature_169','feature_173','feature_174','feature_238','feature_259','feature_260','feature_263','feature_264',\n                    'feature_269','feature_294','feature_298','feature_301','feature_302','feature_303','feature_324','feature_325','feature_343',\n                    'feature_344','feature_365','feature_367','feature_369','feature_372','feature_374','feature_376','feature_378','feature_380',\n                    'feature_382','feature_384','feature_386','feature_388','feature_390','feature_392','feature_394','feature_396','feature_398',\n                    'feature_400','feature_402','feature_402','feature_404','feature_406','feature_408','feature_410','feature_412','feature_414',\n                    'feature_416','feature_418','feature_420','feature_424','feature_426','feature_430','feature_432','feature_434','feature_436',\n                    'feature_438','feature_440','feature_442','feature_444','feature_446','feature_448','feature_450','feature_452','feature_454',\n                    'feature_456','feature_458','feature_460','feature_462','feature_464','feature_466','feature_475','feature_477','feature_479',\n                    'feature_481','feature_488','feature_490','feature_492','feature_497','feature_499','feature_502','feature_503','feature_506',\n                    'feature_3','feature_17','feature_163','feature_195','feature_200','feature_204','feature_205','feature_243','feature_247',\n                    'feature_273','feature_281','feature_282','feature_283','feature_284','feature_285','feature_286','feature_289','feature_292',\n                    'feature_293','feature_336']\nprint(\"# positive features with correlation gt 0.05: \", len(positive_features))","364fe1bc":"# Preprocessing the data\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nmm_scaler = MinMaxScaler()\nstd_scaler = StandardScaler()\n\nfeatures = mm_scaler.fit_transform(train_impute[positive_features])","c69c81ba":"from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n\nX_train, X_valid, y_train, y_valid = train_test_split(features, train_impute[target_column], test_size=0.2, random_state=123, shuffle=True)","9d322a04":"# TARGET IMBALANCE\n# print(\"Training set shape: \", train_data.shape)\n\n# print(\"Class 0 samples in the training set: \", sum(train_data[target_column]==0))\n# print(\"Class 1 samples in the training set: \", sum(train_data[target_column]==1))\n\n# print(\"Class 0 samples in the validation set: \", sum(valid_data[target_column]==0))\n# print(\"Class 1 samples in the validation set: \", sum(valid_data[target_column]==1))","4d2340f1":"# from sklearn.utils import shuffle\n\n# class_0_no = train_data[train_data[target_column]==0]\n# class_1_no = train_data[train_data[target_column]==1]\n\n# upsampled_class_1_no = class_1_no.sample(n=len(class_0_no), replace=True, random_state=123)\n\n# train_data = pd.concat([class_0_no, upsampled_class_1_no])\n# train_data = shuffle(train_data)","551490c6":"# print(\"Training set: \", train_data.shape)\n\n# print(\"Class 0 samples in training set: \", sum(train_data[target_column]==0))\n# print(\"class 1 samples in training set: \", sum(train_data[target_column]==1))","3afc3704":"# creating a simple model baseline using Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\ngrad_clf = GradientBoostingClassifier()\n\ngrad_model = grad_clf.fit(X_train, y_train)","5c35c0e0":"from xgboost import XGBClassifier\n\nparams = {'objective':'binary:logistic', 'n_estimators':2, 'use_label_encoder':False}\n\nxgb_clf = XGBClassifier(**params)\nxgb_clf.fit(X_train, y_train,\n           eval_set=[(X_train, y_train), (X_valid, y_valid)],\n           eval_metric='auc',\n           early_stopping_rounds=10)","119eb825":"xgb_clf.evals_result_","e3c175f2":"from sklearn.metrics import classification_report, roc_auc_score","92a950e1":"# making predictions for the training and getting the evaluation\ndef get_predictions(model, ds):\n    return model.predict(ds)\n\n# checking predictions due to gradient Classifier\ngrad_train_preds = get_predictions(grad_model, X_train)\ngrad_valid_preds = get_predictions(grad_model, X_valid)\n\n# checking predictions due to Xgboost\nxgb_train_preds = get_predictions(xgb_clf, X_train)\nxgb_valid_preds = get_predictions(xgb_clf, X_valid)","fdfe0a3a":"print(\"Scikit-learn Gradient Boosted Classifier\")\nprint(classification_report(y_train, grad_train_preds))\n\nprint(\"\\nXgboost classifier\")\nprint(classification_report(y_train, xgb_train_preds))","1f8c8021":"valid_preds = 0.5*(xgb_valid_preds + grad_valid_preds)","76c0276b":"# evaluating the area under the curve\nprint(\"Area under the curve for the training set: \", roc_auc_score(xgb_train_preds, y_train))\nprint(\"AUC for gradient boosted tree: \", roc_auc_score(grad_valid_preds, y_valid))\nprint(\"AUC for xgboost: \", roc_auc_score(xgb_valid_preds, y_valid))\nprint(\"Area under the curve for the ensembled validation set: \", roc_auc_score(valid_preds.astype('int32'), y_valid))","9b7ae1c2":"# getting the dataset\ntest = pd.read_csv(\"..\/input\/netflix-appetency\/test.csv\")\ntest.head()","271f9b6f":"# Applying the changes from the above \n# Imputation for the numerical data\nimpute_test_num = mean_imputer.fit_transform(test[numerical_columns])\n\n# imputation for the categorical data\nimpute_test_cat = constant_imputer.fit_transform(test[categorical_columns])","89ddd394":"# using the shape for the impute_test_num\ntest[numerical_columns[:411]] = impute_test_num\n# applying the changes for the categorical data\ntest[categorical_columns] = impute_test_cat","80547328":"# converting the categoricals into integers\nfor cname in test.select_dtypes('object'):\n    test[cname], _ = test[cname].factorize()\n    \ntest.head()","0c5d194b":"# then preprocessing the data\ntest_features = mm_scaler.fit_transform(test[positive_features])","5daaee2a":"test_predictions = grad_model.predict(test_features)","1df6c288":"sample = pd.read_csv(\"..\/input\/netflix-appetency\/sample_submission.csv\")\n# sample.head()","a22c70a9":"sample['target'] = test_predictions","98bc6d9f":"sample.to_csv('submission.csv', index=False)","96ec0784":"**MODELLING**\n\nInitially, I will be using scikit learn's algorithms for classification","fecf0e65":"**IMPORTING LIBRARIES**","57d29ca0":"**Separating features from targets**\n\nAlso, includes the visualization of the targets","7e237839":"**PREPROCESSING THE DATASET**\n\nFirst, turn categorical data into integers\n\nThere are many ways of encoding categorical data as discussed in the intermediate machine course here on Kaggle.\n\nFor this notebook, I will be using .factorize() and LabelEncoding\n\nSecond, standardize or normalise the data based on the algorithm to be used for modelling","bb250cf8":"**VISUALIZING AFTER HANDLING MISSING DATA**","6af350dc":"**BASIC DESCRIPTION AND STATISTICS OF THE DATA**\n\nThis includes determining the type of data types and missing data too","5e04c03e":"**VISUALIZING CATEGORICAL AND NUMERICAL COLUMNS**\n\nSince there are many columns, I will visualize all of them based on the number of unique values that \n\neach column has.","d128202a":"**DEALING WITH MISSING VALUES**\n\nThere are many ways to deal with missing.\n\nSince there are many columns, dropping columns would be one of them.\n\nAlso, using df.fillna(method='bfill') and df.fillna(method='ffill') can be used. \n\n'bfill' --> back fill,   'ffill' --> forward fill\n\nFor fillna, you might need to repeat like 3 terms to fill all the missing values, I see this as being redundant\n\nTo avoid creating redundancies, I will be using sklearn.impute for both numerical and categorical columns","d804b102":"**Numerical and Categorical Columns**\n\nSince there are objects, integers and floats, its better to separate numbers from non-numbers","04a102d5":"**SUBMISSION**"}}