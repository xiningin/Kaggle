{"cell_type":{"6dc093fb":"code","eb34ae55":"code","db8a70c2":"code","ce68d7d1":"code","29cca56a":"code","0eef5ce0":"code","4b531121":"code","df93c0e0":"code","3c88278a":"code","e3c3ede6":"code","cef5e397":"markdown","6729ecf9":"markdown","f2cd9682":"markdown","14459922":"markdown","2e811768":"markdown","50ad2ecd":"markdown","8b94cc18":"markdown","71a0b488":"markdown","cf47f17d":"markdown","5e2303c6":"markdown","23e74fe5":"markdown"},"source":{"6dc093fb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport time\nimport json\nimport requests\nfrom requests.exceptions import HTTPError\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"KS_ChannelId\")\nsecret_value_1 = user_secrets.get_secret(\"YT_API_Key\")","eb34ae55":"playlistIDs = [] #store the playlist IDs \n\ntry:\n    res = requests.get(f'https:\/\/www.googleapis.com\/youtube\/v3\/playlists?part=id&maxResults=50&channelId={secret_value_0}&key={secret_value_1}')\n    res.raise_for_status()\n    dictPlayList = res.json() #parse the response as a JSON file\/Dictionary\n    \nexcept HTTPError as http_err: #catch errors\n    print(f'HTTP error occurred: {http_err}')\nexcept Exception as err:\n    print(f'Other error occurred: {err}')\n    \nfor i in range(len(dictPlayList['items'])): #extract the playlist IDs and store them in a list 'playlistIDs'\n    if \"id\" in dictPlayList['items'][i]: #check if a playlist IDs exist in each iterated item list\n        playlistIDs.append(dictPlayList['items'][i]['id'])\n\ndictPlayList = {} #empty the JSON file from the drive\nplaylistIDs","db8a70c2":"len(playlistIDs) #check the number of playlist in the channel","ce68d7d1":"videoIDs = [] #here is where the videoIDs are saved\nstart = time.perf_counter() #a counter to check how long the retrieval will take\n\nfor playlistID in playlistIDs: #loop over each playlist ID\n    try:\n        res = requests.get(f'https:\/\/www.googleapis.com\/youtube\/v3\/playlistItems?part=contentDetails&maxResults=50&playlistId={playlistID}&key={secret_value_1}')\n        res.raise_for_status()\n        dictPlayListItems = res.json()\n            \n        while True: #an infinite loop if there is nextPageToken available\n            if \"nextPageToken\" in dictPlayListItems:\n                try:\n                    res = requests.get(f'https:\/\/www.googleapis.com\/youtube\/v3\/playlistItems?part=contentDetails&maxResults=50&playlistId={playlistID}&pageToken={dictPlayListItems[\"nextPageToken\"]}&key={secret_value_1}')\n                    res.raise_for_status()\n                    dictPlayListItems = res.json()\n                \n                except HTTPError as http_err:\n                    print(f'HTTP error occurred: {http_err}')\n                except Exception as err:\n                    print(f'Other error occurred: {err}')\n                    \n                for i in range(len(dictPlayListItems['items'])):\n                    if \"videoId\" in dictPlayListItems['items'][i]['contentDetails']:\n                        videoIDs.append(dictPlayListItems['items'][i]['contentDetails']['videoId'])\n                    \n            if \"nextPageToken\" not in dictPlayListItems: #the while loop breaks if the statement is false ie. last page will have no \"nextPageToken\"\n                break\n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err}')\n    except Exception as err:\n        print(f'Other error occurred: {err}')\n    \n    for i in range(len(dictPlayListItems['items'])):\n        if \"videoId\" in dictPlayListItems['items'][i]['contentDetails']:\n            videoIDs.append(dictPlayListItems['items'][i]['contentDetails']['videoId'])\n    dictPlayListItems = {} #empty\n\nfinish = time.perf_counter()\nprint(f'The videoIDs retrival finised in {round(finish-start, 2)} second(s)')\nprint(f'The total number of videoIDs retrieved are {len(videoIDs)}')","29cca56a":"print(videoIDs)","0eef5ce0":"videoIDs = list(dict.fromkeys(videoIDs))\nlen(videoIDs)","4b531121":"dictStats = {} #here is where I store the final dataset as a dictionary\nstart = time.perf_counter()\n\nindex = 0 #this index optional. I created it as keys for the dictionary that will contain the datasets\n\nfor iD in videoIDs: #loop over each video ID\n    try:\n        res = requests.get(f'https:\/\/www.googleapis.com\/youtube\/v3\/videos?part=snippet,statistics,contentDetails&maxResults=1&id={iD}&key={secret_value_1}')\n        res.raise_for_status()\n        dictResTemp = res.json()\n        if len(dictResTemp['items']) != 0: #some (old) videos may not have any attributes!\n            dictStats[index] = [iD,dictResTemp['items'][0]['snippet']['publishedAt'],dictResTemp['items'][0]['contentDetails']['duration'],dictResTemp['items'][0]['snippet']['title'],\n                                dictResTemp['items'][0]['statistics']['viewCount'],dictResTemp['items'][0]['statistics']['likeCount'],\n                                dictResTemp['items'][0]['statistics']['dislikeCount'],dictResTemp['items'][0]['statistics']['commentCount'],\n                                dictResTemp['items'][0]['snippet']['thumbnails']['default']['url'], dictResTemp['items'][0]['snippet']['channelId']\n                               ]\n        dictResTemp = {} #reset the buffer that stores the responses\n        index +=1\n    \n    except HTTPError as http_err:\n        print(f'HTTP error occurred: {http_err} for iD {iD}')\n    except Exception as err:\n        print(f'Other error occurred: {err} for iD {iD}')\n\nfinish = time.perf_counter()\nprint(f'The Stats retrival finised in {round(finish-start, 2)} second(s)')","df93c0e0":"df = pd.DataFrame.from_dict(dictStats, orient='index', columns=['videoId','publishedAt','duration','title','viewCount','likeCount','dislikeCount','commentCount','thumbnails','channelId'])\ndf","3c88278a":"df.to_csv('YouTubeChannelStats.csv', encoding='utf-8',index=False)","e3c3ede6":"from IPython.display import FileLink\nFileLink(r'YouTubeChannelStats.csv')","cef5e397":"Here is a link to the CSV file. Which I intend to connect to Tableau later on.","6729ecf9":"![YouTube Resource Hierarchy](https:\/\/lh5.googleusercontent.com\/Ss3dapzgLIZ7DW77auB7woMKlxuWpWxRfFZE-KdtlWETFc9BRTHXWqzxH_1bnC-CeZTQnfUSDvMFJ5dIl4gSwvA3FzfD6As3JJkEdffbsks_wn23Uwuk=w1280)\n\nBefore data collection, I needed to first understand YouTube's resource Hierarchy and how I could navigate through them to get to the data I will be collecting. Each user can have zero-to-many channels. Each channel will have one-to-many playlist. There is always a default upload playlist should in case the user decides not to associate the video upload with a user-defined playlist. Playlist items, i.e. videos, can belong to one or more playlists. ","f2cd9682":"To have a better view I transformed the dictionary into a Data Frame using Pandas.","14459922":"# Data Retrieval","2e811768":"# Visualization\n[Link to the Viz](https:\/\/public.tableau.com\/views\/DataAnalysisonKwadwoSheldonsYouTubeChannel\/KwadwoSheldon?:language=en&:display_count=y&:origin=viz_share_link)\n\nThank you. Please leave a comment behind.","50ad2ecd":"In order to use YouTube's API I needed an API Key, which can be obtained using a Google acocunt at [Google Console Developers](https:\/\/console.developers.google.com\/), to contect to the YouTube servers.\nI restricted this analysis to just one channel. I first extracted all the playlist IDs under the channel using this HTTP request link:https:\/\/www.googleapis.com\/youtube\/v3\/playlists and other parameters such as:\n* part = id\n* maxResults = 50\n* channelId = \"your channelId\"\n* key = \"your API Key\"\n<br><br>\nThe Part parameter specifies a comma-seperated list of one or more resources that the API will return, in this case, id (Playlist IDs).","8b94cc18":"Now that I have the playlist IDs, I will get the playlist items from each playlist. The HTTP request link for this retrival is https:\/\/www.googleapis.com\/youtube\/v3\/playlistItems. I set the part parameter to \"contentDetails\" which has the videoIds stored in there. <br>\nThe maximum results per page for each response is 256 so if there more items than 256, the API will generate a \"nextPageToken\" parameter which means I have to iterate over the next pages to get all the videoIds in a playlist.","71a0b488":"Now that I have the video IDs I can do a video search using the HTTP request link https:\/\/www.googleapis.com\/youtube\/v3\/videos and parameters such as\n* part = snippet,statistics,contentDetails\n<br>\nThe snippet will contain attributes like \n* Published Date\n* Thumbnails\n* Channel Id\n* Title etc.<br>\nThe statistics will have attributes like\n* View Count\n* Like Count\n* Dislike Count\n* Comment Count etc. <br>\nThe contentDetails will have the duration attribute.\n","cf47f17d":"The data can be saved as a CSV file.","5e2303c6":"In this analysis, I will be analyzing a YouTube channel that has been growing rapidly since the beginning of 2020. I will extract some data from the channel using YouTube's API, YouTube Data API v3. This API provides access to YouTube data, such as videos, playlists, and channels.","23e74fe5":"Some duplicates may exist as some videos may be saved under more playlists."}}