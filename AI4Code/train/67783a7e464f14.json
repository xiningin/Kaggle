{"cell_type":{"2202736d":"code","252d9be3":"code","3a4283cf":"code","6be65889":"code","e2488ac2":"code","3d07a5ab":"code","cae26e9a":"code","d440c185":"code","08956ad6":"code","fd0046a5":"code","292d8d70":"code","1d25976d":"code","f21bdd80":"code","858212a7":"code","f6a7c20c":"code","b038151e":"code","4f2c2614":"code","b59c1ed2":"code","f32e74dc":"code","2c2cb1a8":"code","8e5c3e92":"code","d12c127c":"code","ed507721":"code","ed317280":"code","e032eeec":"code","72879c16":"code","65a46ae0":"code","40f2197d":"code","06821caf":"code","493d98a3":"code","c85168d7":"code","7d4a398e":"code","7658acd0":"code","ca97ada2":"code","1681d233":"code","7a6501ef":"code","71548b14":"code","88de006e":"code","fdae8ea5":"code","25d20f7b":"code","5a6cfaea":"code","60760801":"code","0dadedc0":"code","688a3e16":"code","96e5ac1d":"code","e05eeb42":"code","08c773fb":"code","5541466c":"code","83b1c519":"code","42876d2b":"code","0bee53c9":"code","8c0f4ffa":"code","1629dd58":"code","646fe487":"markdown","387fda9b":"markdown","59c447ed":"markdown","f77e659f":"markdown","6bf6481a":"markdown","4d964738":"markdown","a3bdc721":"markdown","9aabc6f2":"markdown","ac8f4949":"markdown","5114d415":"markdown","2215ffcc":"markdown","4b33ef6f":"markdown","ebf5bc84":"markdown","dbd354a3":"markdown","46ea7dc6":"markdown","3bee5159":"markdown","d8402533":"markdown","4741f384":"markdown","3c277ba5":"markdown","8444305d":"markdown","939eaca8":"markdown","299484a0":"markdown","d35c359a":"markdown","7f85f737":"markdown","fbed2ff2":"markdown","c434eb20":"markdown","97f056b8":"markdown","4e271271":"markdown","98e92892":"markdown","b10b4843":"markdown","8c888a7f":"markdown","f0169bc6":"markdown","9cfa3b92":"markdown","b3c83314":"markdown","c4c06521":"markdown","9b411e54":"markdown","e9dd4dc3":"markdown","dcd19ebe":"markdown","cc567dc8":"markdown"},"source":{"2202736d":"# packages\n\nimport numpy as np \nimport pandas as pd \n\n# visual\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# missing data\nimport missingno as msno\n\n# data imputation\nfrom sklearn.impute import KNNImputer\n\n# to split dataset to train and test\nfrom sklearn.model_selection import train_test_split\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Data imputation\n!pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE \n\n# ANN\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.neural_network import MLPClassifier\n\n# ML\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# ML classifier model Evaluation\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_auc_score ,roc_curve,auc\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, plot_confusion_matrix\n\n# MISC\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","252d9be3":"# read data\ndf = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","3a4283cf":"df.head(4)","6be65889":"df.info()","e2488ac2":"# missing values\nmissing_value = 100 * df.isnull().sum()\/len(df)\nmissing_value = missing_value.reset_index()\nmissing_value.columns = ['variables','missing values in percentage']\nmissing_value = missing_value.sort_values('missing values in percentage',ascending=False)\n\n# barplot\nfig = px.bar(missing_value, y='missing values in percentage',x='variables',title='Missing values % in each column',\n             template='none',text='missing values in percentage');\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n\n\n\nfig.show()","3d07a5ab":"# stats\nround(df.describe(),2)","cae26e9a":"plot_rain = df[['MinTemp', 'MaxTemp','Rainfall', \n       'Sunshine','WindGustSpeed', 'WindSpeed3pm','Humidity3pm',\n       'Pressure9am', 'Pressure3pm','RainTomorrow']]\n\nsns.pairplot(plot_rain, hue='RainTomorrow' , diag_kind = 'kde');","d440c185":"# skewness of data\n\nfeatures =['MinTemp', 'MaxTemp','Rainfall', 'Evaporation',\n       'Sunshine','WindGustSpeed','WindSpeed9am', 'WindSpeed3pm','Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm']\n\nfor i in features:\n    print(i,'skewness is',df[i].skew())","08956ad6":"features =['MinTemp', 'MaxTemp','Rainfall', 'Evaporation',\n       'Sunshine','WindGustSpeed','WindSpeed9am', 'WindSpeed3pm','Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm']\n\nfor i in features:\n   \n    fig, ax = plt.subplots(1,3,figsize=(12, 3))\n\n    sns.set_style('white')\n    sns.set_context(context = 'notebook',font_scale=1)\n\n    sns.distplot(df[i],bins=10,color='red',kde=False,ax=ax[0]);\n    sns.boxplot(df[i], ax = ax[1],color='#ff4e50');\n    sns.violinplot(df[i],bins=10,color='#1ebbd9',ax=ax[2]);  \n    \n    ax[0].title.set_text(i);\n    ax[1].title.set_text(i);\n    ax[2].title.set_text(i);     \n        \n    ax[0].axvline(df[i].mean(), color='b', linewidth=1)\n    ax[1].axvline(df[i].mean(), color='b', linewidth=1)\n    ax[2].axvline(df[i].mean(), color='b', linewidth=1)\n        \n    ax[0].axvline(df[i].median(), color='r', linewidth=1)\n    ax[1].axvline(df[i].median(), color='r', linewidth=1)\n    ax[2].axvline(df[i].median(), color='r', linewidth=1)\n    \n    plt.tight_layout()    ","fd0046a5":"plt.figure(figsize=(14,9))\nsns.heatmap(df.corr(),annot=True,cmap='Blues',mask=np.triu(df.corr(),+1));\nplt.title('Correlation');\nplt.tight_layout()","292d8d70":"# visual check to ensure null vales are not represented with other values like -999, -1, ?,-111\n\n\n#for feature in df.columns:\n#    print('*******','Column name:',feature,'*******')\n#    print(df[feature].unique())\n#    print('***********-end-***********')\n#    print(' ')","1d25976d":"# to date_time\ndf['Date'] = pd.to_datetime(df['Date'])","f21bdd80":"# checking for duplicated\n\nsim = df.duplicated() \nsim.sum()","858212a7":"# dropping null values of target variable which is just 2.2%\ndf = df[df['RainTomorrow'].notnull()]","f6a7c20c":"# drop null values of Rain Today variable which is just 2.2%\ndf = df[df['RainToday'].notnull()]","b038151e":"# knn imputer\nknn_imputer = KNNImputer(n_neighbors=3)","4f2c2614":"# impute missing values using KNNImputer\n\nlist_impute = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Temp9am',\n       'Temp3pm','Cloud9am', 'Cloud3pm','Evaporation']\n\nfor i in list_impute:\n    \n    df[i] = knn_imputer.fit_transform(df[[i]])","b59c1ed2":"# impute missing values for categorical variables\n\ncat_impute =['WindGustDir', 'WindDir9am', 'WindDir3pm','Location']\n\nfor i in cat_impute:\n    df[i] = df[i].fillna(df[i].mode()[0])","f32e74dc":"# null values check\n# since I decided to drop the Sunshine I am not treating it\ndf.isnull().sum()","2c2cb1a8":"# converting object into the Categorical data type\n\nfor feature in df.columns:\n    if df[feature].dtype =='object':\n        df[feature] = pd.Categorical(df[feature]).codes","8e5c3e92":"# outliers\ndf.plot(kind='box',figsize=(12,6))\nplt.xticks(rotation=70);\nplt.title('Outlier treated in df');","d12c127c":"\ndef remove_outlier(col):\n    sorted(col)\n    Q1,Q3=np.percentile(col,[25,75])\n    IQR=Q3-Q1\n    lower_range= Q1-(1.5 * IQR)\n    upper_range= Q3+(1.5 * IQR)\n    return lower_range, upper_range","ed507721":"# removing outliers\n\nfor column in df[list_impute].columns:\n    lr,ur = remove_outlier(df[column])\n    df[column] = np.where(df[column]>ur, ur,df[column])\n    df[column] = np.where(df[column]<lr,lr,df[column])","ed317280":"# outliers\ndf.plot(kind='box',figsize=(12,6))\nplt.xticks(rotation=70);\nplt.title('Outlier treated in df');","e032eeec":"# dropped 'MaxTemp' because it highly correlated and 'Sunshine' because it has more than 48% values are missing\n\nX = df[['MinTemp', 'Rainfall', 'Evaporation',\n       'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm','WindGustDir', 'WindDir9am', 'WindDir3pm','Location','RainToday']]\n\ny = df.pop('RainTomorrow')\n","72879c16":"# Data split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0,stratify=y)","65a46ae0":"X_train.shape, X_test.shape","40f2197d":"y_train.value_counts(1)","06821caf":"# Scaling data\nsc = MinMaxScaler()\n\nx_train = sc.fit_transform(X_train)\nx_test = sc.transform(X_test)","493d98a3":"NaiveBayse_model = GaussianNB()\n\nNaiveBayse_model.fit(x_train, y_train)\n\ny_predict_NaiveBayse = NaiveBayse_model.predict(x_test)\n\nNaiveBayse_model_score = NaiveBayse_model.score(x_test, y_test)\nNaiveBayse_model_score_train = NaiveBayse_model.score(x_train, y_train)\n\nprint('Accuracy on Test set',NaiveBayse_model_score)\nprint('Accuracy on Train set',NaiveBayse_model_score_train)\n\nprint(confusion_matrix(y_test, y_predict_NaiveBayse))\n\n","c85168d7":"fig, ax = plt.subplots(1,2,figsize=(10, 4))\n\nsns.set_style('dark')\nsns.set_context(context = 'notebook',font_scale=1)\nplot_confusion_matrix(NaiveBayse_model,x_test,y_test,cmap='Blues',normalize='true',ax = ax[0]);\nplot_confusion_matrix(NaiveBayse_model,x_train,y_train,cmap='Blues',normalize='true',ax = ax[1]);\n\nax[0].title.set_text('NaiveBayse on Test');\nax[1].title.set_text('NaiveBayse on Train');\nplt.grid(False)\nplt.tight_layout();","7d4a398e":"# probability\nNB_probs_train = NaiveBayse_model.predict_proba(x_train)\nNB_probs_train = NB_probs_train[:, 1]\n\nNB_probs_test = NaiveBayse_model.predict_proba(x_test)\nNB_probs_test = NB_probs_test[:, 1]\n\n# plot\nplt.style.use('seaborn-whitegrid')\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, NB_probs_train)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, NB_probs_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot([0, 1], [0, 1], linestyle='--');\nax1.plot(fpr_train, tpr_train, marker='.');\nax1.set_title('ROC_Curve on training set for Naive Bayse');\n\nax2.plot([0, 1], [0, 1], linestyle='--');\nax2.plot(fpr_test, tpr_test, marker='.');\nax2.set_title('ROC_Curve on test set for Naive Bayse');\n\nauc_NaiveBayse_test = auc(fpr_test, tpr_test)\nprint('AUC on test set',auc_NaiveBayse_test)","7658acd0":"LR_model = LogisticRegression(max_iter=1000)\nLR_model.fit(x_train, y_train)\n\ny_predict_LR = LR_model.predict(x_test)\n\nLR_model_score = LR_model.score(x_test, y_test)\nLR_model_score_train = LR_model.score(x_train, y_train)\n\nprint('Accuracy on Test set',LR_model_score)\nprint('Accuracy on Train set',LR_model_score_train)\nprint(confusion_matrix(y_test, y_predict_LR))","ca97ada2":"# probability\nprobs_train = LR_model.predict_proba(x_train)\nprobs_train = probs_train[:, 1]\n\nprobs_test = LR_model.predict_proba(x_test)\nprobs_test = probs_test[:, 1]\n\n# plot\nplt.style.use('seaborn-whitegrid')\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, probs_train)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, probs_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot([0, 1], [0, 1], linestyle='--');\nax1.plot(fpr_train, tpr_train, marker='.');\nax1.set_title('ROC_Curve on training set for logistic Regression');\n\nax2.plot([0, 1], [0, 1], linestyle='--');\nax2.plot(fpr_test, tpr_test, marker='.');\nax2.set_title('ROC_Curve on test set for logistic Regression');\n\nauc_NaiveBayse_test = auc(fpr_test, tpr_test)\nprint('AUC on test set',auc_NaiveBayse_test)","1681d233":"fig, ax = plt.subplots(1,2,figsize=(10, 4))\n\nsns.set_style('dark')\nsns.set_context(context = 'notebook',font_scale=1)\nplot_confusion_matrix(LR_model,x_test,y_test,cmap='Blues',normalize='true',ax = ax[0]);\nplot_confusion_matrix(LR_model,x_train,y_train,cmap='Blues',normalize='true',ax = ax[1]);\n\nax[0].title.set_text('logistic Regression on Test');\nax[1].title.set_text('logistic Regression on Train');\nplt.grid(False)\nplt.tight_layout();","7a6501ef":"LDA_model = LinearDiscriminantAnalysis()\nLDA_model.fit(x_train, y_train)\n\ny_pred_LDA = LDA_model.predict(x_test)\nLDA_model_score = LDA_model.score(x_test, y_test)\nLDA_model_score_train = LDA_model.score(x_train,y_train)\n\nprint('Accuracy on Test set',LDA_model_score)\nprint('Accuracy on Train set',LDA_model_score_train)\nprint(confusion_matrix(y_test, y_pred_LDA))","71548b14":"# probability\nprobs_train = LDA_model.predict_proba(x_train)\nprobs_train = probs_train[:, 1]\n\nprobs_test = LDA_model.predict_proba(x_test)\nprobs_test = probs_test[:, 1]\n\n# plot\nplt.style.use('seaborn-whitegrid')\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, probs_train)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, probs_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot([0, 1], [0, 1], linestyle='--');\nax1.plot(fpr_train, tpr_train, marker='.');\nax1.set_title('ROC_Curve on training set for LDA_model');\n\nax2.plot([0, 1], [0, 1], linestyle='--');\nax2.plot(fpr_test, tpr_test, marker='.');\nax2.set_title('ROC_Curve on test set for LDA_model');","88de006e":"fig, ax = plt.subplots(1,2,figsize=(10, 4))\n\nsns.set_style('dark')\nsns.set_context(context = 'notebook',font_scale=1)\nplot_confusion_matrix(LDA_model,x_test,y_test,cmap='Blues',normalize='true',ax = ax[0]);\nplot_confusion_matrix(LDA_model,x_train,y_train,cmap='Blues',normalize='true',ax = ax[1]);\n\nax[0].title.set_text('LDA_model on Test');\nax[1].title.set_text('LDA_model on Train');\nplt.grid(False)\nplt.tight_layout();","fdae8ea5":"RF_model = RandomForestClassifier(random_state=0,max_depth= 10, max_features= 5,min_samples_leaf= 30, min_samples_split= 100, n_estimators= 500)\n\nRF_model.fit(x_train,y_train)\n\ny_pred_RF =RF_model.predict(x_test)\n\nmodel_score_RF = RF_model.score(x_test, y_test)\nmodel_score_RF_train = RF_model.score(x_train, y_train)\n\nprint('Accuracy on Test set',model_score_RF)\nprint('Accuracy on Train set',model_score_RF_train)\nprint(confusion_matrix(y_test,y_pred_RF))","25d20f7b":"# probability\nprobs_train = RF_model.predict_proba(x_train)\nprobs_train = probs_train[:, 1]\n\nprobs_test = RF_model.predict_proba(x_test)\nprobs_test = probs_test[:, 1]\n\n# plot\nplt.style.use('seaborn-whitegrid')\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, probs_train)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, probs_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot([0, 1], [0, 1], linestyle='--');\nax1.plot(fpr_train, tpr_train, marker='.');\nax1.set_title('ROC_Curve on training set for Random Forest');\n\nax2.plot([0, 1], [0, 1], linestyle='--');\nax2.plot(fpr_test, tpr_test, marker='.');\nax2.set_title('ROC_Curve on test set for Random Forest');","5a6cfaea":"fig, ax = plt.subplots(1,2,figsize=(10, 4))\n\n\nsns.set_style('dark')\nsns.set_context(context = 'notebook',font_scale=1)\nplot_confusion_matrix(RF_model,x_test,y_test,cmap='Blues',normalize='true',ax = ax[0]);\nplot_confusion_matrix(RF_model,x_train,y_train,cmap='Blues',normalize='true',ax = ax[1]);\nplt.grid(False)\nax[0].title.set_text('Random Forest on Test');\nax[1].title.set_text('Random Forest on Train');\n\nplt.tight_layout();","60760801":"M_pool = Pool(data=x_train,label = y_train)\n\nmodel_cb = CatBoostClassifier(verbose=False,iterations=500,learning_rate=0.01)\nmodel_cb.fit(M_pool, plot=False,silent=True);\n\nmodel_cb_score = model_cb.score(x_test, y_test)\nmodel_cb_score_train = model_cb.score(x_train, y_train)\n\nprint('Accuracy on Test set',model_cb_score )\nprint('Accuracy on Train set',model_cb_score_train)","0dadedc0":"# probability\nprobs_train = model_cb.predict_proba(x_train)\nprobs_train = probs_train[:, 1]\n\nprobs_test = model_cb.predict_proba(x_test)\nprobs_test = probs_test[:, 1]\n\n# plot\nplt.style.use('seaborn-whitegrid')\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, probs_train)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, probs_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot([0, 1], [0, 1], linestyle='--');\nax1.plot(fpr_train, tpr_train, marker='.');\nax1.set_title('ROC_Curve on training set for Catboost');\n\nax2.plot([0, 1], [0, 1], linestyle='--');\nax2.plot(fpr_test, tpr_test, marker='.');\nax2.set_title('ROC_Curve on test set for Catboost');\n    ","688a3e16":"fig, ax = plt.subplots(1,2,figsize=(10, 4))\n\n\nsns.set_style('dark')\nsns.set_context(context = 'notebook',font_scale=1)\nplot_confusion_matrix(model_cb,x_test,y_test,cmap='Blues',normalize='true',ax = ax[0]);\nplot_confusion_matrix(model_cb,x_train,y_train,cmap='Blues',normalize='true',ax = ax[1]);\nplt.grid(False)\nax[0].title.set_text('Catboost on Test');\nax[1].title.set_text('Catboost on Train');\n\nplt.tight_layout();","96e5ac1d":"# mlpclassifier\nmlpcl = MLPClassifier(hidden_layer_sizes =484,max_iter=5000,\n                   solver='adam',verbose=False, random_state=1,tol=0.001)\n\n# fit the model\nmlpcl.fit(x_train,y_train)","e05eeb42":"mlpcl_train_predict = mlpcl.predict(x_train)\nmlpcl_test_predict = mlpcl.predict(x_test)\n\nmlp_model_score = mlpcl.score(x_test, y_test)\nmlp_model_score_train = mlpcl.score(x_train, y_train)\n\nprint('Accuracy on Test set',mlp_model_score )\nprint('Accuracy on Train set',mlp_model_score_train)","08c773fb":"\n# probability\nmlp_probs_train = mlpcl.predict_proba(x_train)\nmlp_probs_train = mlp_probs_train[:, 1]\n\nmlp_probs_test = mlpcl.predict_proba(x_test)\nmlp_probs_test = mlp_probs_test[:, 1]\n\n# plot\nplt.style.use('seaborn-whitegrid')\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, mlp_probs_train)\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, mlp_probs_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot([0, 1], [0, 1], linestyle='--');\nax1.plot(fpr_train, tpr_train, marker='.');\nax1.set_title('ROC_Curve on training set for MLP');\n\nax2.plot([0, 1], [0, 1], linestyle='--');\nax2.plot(fpr_test, tpr_test, marker='.');\nax2.set_title('ROC_Curve on test set for MLP');\n    ","5541466c":"fig, ax = plt.subplots(1,2,figsize=(10, 4));\n\n\nsns.set_style('dark');\nsns.set_context(context = 'notebook',font_scale=1);\nplt.grid(False)\nplot_confusion_matrix(mlpcl,x_test,y_test,cmap='Blues',normalize='true',ax = ax[0]);\nplot_confusion_matrix(mlpcl,x_train,y_train,cmap='Blues',normalize='true',ax = ax[1]);\n\nax[0].title.set_text('MLP on Test');\nax[1].title.set_text('MLP on Train');\n\nplt.tight_layout();","83b1c519":"# early stopping\nearly_stop = EarlyStopping(mode='max', verbose=1, patience=22)\n\n# ANN\nmodel =  Sequential()\n\n# ANN layers\nmodel.add(Dense(units=21,activation='relu'))\n\nmodel.add(Dense(units=21,activation='relu'))\n\nmodel.add(Dense(units=15,activation='relu'))\n\nmodel.add(Dense(units=5,activation='relu'))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# compile ANN\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n\n# Train ANN\nmodel.fit(x=x_train, \n          y=y_train, \n          epochs=120,\n          validation_data=(x_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )\n","42876d2b":"# model history to df\nloss_plot = pd.DataFrame(model.history.history)\naccuracy_plot = pd.DataFrame(model.history.history)\n\n#  accuracy and loss plot\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(12,4))\n\nax1.plot(loss_plot.loc[:, ['loss']], label='Training loss');\nax1.plot(loss_plot.loc[:, ['val_loss']],label='Validation loss');\nax1.set_title('Training and Validation loss')\nax1.set_xlabel('epochs')\nax1.set_ylabel('Loss')\nax1.legend(loc=\"best\");\n\nax2.plot(accuracy_plot.loc[:, ['accuracy']],label='Training_accuracy');\nax2.plot(accuracy_plot.loc[:, ['val_accuracy']], label='Validation_accuracy');\nax2.set_title('Training_and_Validation_accuracy');\nax2.set_xlabel('epochs')\nax2.set_ylabel('accuracy')\nax2.legend(loc=\"best\");","0bee53c9":"# AUC area under the curve\ny_pred_keras = model.predict(x_test).ravel()\n\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred_keras)\nauc_keras = auc(fpr_keras, tpr_keras)# area under the Curve\n\nprint(\"AUC on test set for ANN\",auc_keras)","8c0f4ffa":"Accuracy_score = pd.DataFrame({'Models':['MLP','Catboost','Random Forest','LDA','Logistic Regression','Naive Bayse'],'Accuracy on Test set':[mlp_model_score,model_cb_score,model_score_RF,LDA_model_score,LR_model_score,NaiveBayse_model_score],'Accuracy on Training set':[mlp_model_score_train, model_cb_score_train, model_score_RF_train,LDA_model_score_train,LR_model_score_train,NaiveBayse_model_score_train],'AUC':[]})\n\nAccuracy_score = Accuracy_score.sort_values('Accuracy on Test set',ascending=False)","1629dd58":"Accuracy_score","646fe487":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n          color:white;\n          text-align:center;\">Index\n<\/p>\n<\/div>","387fda9b":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">EDA\n<\/p>\n<\/div>","59c447ed":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n          color:white;\n          text-align:center;\">Artificial neural network ANN\n<\/p>\n<\/div>","f77e659f":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Data cleaning\n<\/p>\n<\/div>","6bf6481a":"## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">iii. Linear Discriminant Analysis","4d964738":"# Normalize data\n#### <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\"> Scaling is process of standardize the independent features present in the data","a3bdc721":"## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">i. Naive Bayes ","9aabc6f2":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Missing values in data set\n<\/p>\n<\/div>","ac8f4949":"# Outliers\n#### <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">Outliers in data set are extrem values in the variables \n","5114d415":"## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">ii. LogisticRegression","2215ffcc":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Null values imputation\n<\/p>\n<\/div>","4b33ef6f":"# Skewness\n\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\"> Skewness refers to a distortion of data, that deviates from the normal distribution or a bell curve","ebf5bc84":"## <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">Inference\n\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">we can observe that in the above box plot outlier are present in the dataset\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">most of the variable is skewed positively\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">scaling is required for the dataset \n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">The blue line indicates the mean of data and the red line indicate the median of the data and we can observe that few variables normally distributed and rest are skewed","dbd354a3":"# EDA\n","46ea7dc6":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n          color:white;\n          text-align:center;\">Decision Tree based models\n<\/p>\n<\/div>","3bee5159":"## Thanks!!\n## upvote if you like it and feel free to post any suggestions","d8402533":"## <span style=\"font-family: Arial;font-size:1.2em;color:#333333\"> Inference\n\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">Most of the variable is skewed and closely related to each other\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">some of the variabel are  exponential distribution in other words time related ","4741f384":"# Missing values\n\n#### <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">","3c277ba5":"*  <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">MaxTemp variable is highly correlated with Temp9am and with Temp3pm","8444305d":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Outliers\n<\/p>\n<\/div>","939eaca8":"# iii. ANN","299484a0":"## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">ii. Catboost","d35c359a":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n          color:white;\n          text-align:center;\">Packages\n<\/p>\n<\/div>","7f85f737":"## Correlation","fbed2ff2":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Models\n<\/p>\n<\/div>","c434eb20":"## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">i. Random Forest","97f056b8":"* <a href=\"#Packages\">Packages<\/a> \n* <a href=\"#Missing-values\">Missing values<\/a>\n* <a href=\"#EDA\">EDA<\/a>\n* <a href=\"#Data-cleaning\">Data cleaning<\/a>\n* <a href=\"#Null-values-imputation\">Null values imputation<\/a>\n* <a href=\"#Outliers\">Outliers<\/a>\n* <a href=\"#Normalize-data\">Normalize data<\/a>\n* <a href=\"#Models\">Models<\/a>\n* <a href=\"#Decision-Tree-based-models\">Decision Tree based models<\/a>\n* <a href=\"#models-comparison\">models comparison<\/a>","4e271271":"# Packages","98e92892":"### <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">About data set:\n* <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">This data set all about the daily weather observations from many locations across Australia and we need to predict whether it will rain tomorrow or not ","b10b4843":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:250%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Scaling\n<\/p>\n<\/div>","8c888a7f":"### <span style=\"font-family: Arial;font-size:1.2em;color:#333333\"> Inference\n\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">Most of the columns have missing values or NaNs which has more than 40% of most column values are empty or missing\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\"> Sunshine variable has 48% of missing values in the data set\t\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">Either we can use simple imputer to impute the columns with mean and median or we can use KNN \n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">The k-nearest neighbors (KNN) uses the distance metrics to predict the missing value and impute the values\n","f0169bc6":"# Data cleaning\n## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">   \n","9cfa3b92":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           background-color:#189ad3;\n           font-size:200%;\n           font-family:Arial;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n          color:white;\n          text-align:center;\">Do I Need An Umbrella tomorrow?\n<\/p>\n<\/div> ","b3c83314":"# Null values imputation\n    \n    \n#### <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\"> Null values imputation based on the percentage of missing values we can impute or drop some null values in variables\n* <span style=\"font-family: Arial;font-size:1.2em;color:#333333\"> For 'RainToday', 'RainTomorrow' we can drop the null values and it just around 4.4% of the data set\n","c4c06521":"# Models","9b411e54":"### <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">Objective:- \n* <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">Predict next-day rain by training classification models on the target variable RainTomorrow","e9dd4dc3":"# Decision Tree based models","dcd19ebe":"# models comparison","cc567dc8":"## <span style=\"font-family: Arial;font-size:1.2em;color:#0e92ea\">i. Mlpclassifier\n#### <span style=\"font-family: Arial;font-size:1.2em;color:#333333\">A multilayer perceptron (MLP) is a class of feedforward artificial neural network"}}