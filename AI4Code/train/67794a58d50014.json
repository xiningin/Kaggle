{"cell_type":{"4e53032d":"code","23d45361":"code","c0a4bb65":"code","9003dfdf":"code","dae0d0fc":"code","95b69ecd":"code","958ffe40":"code","7d0d69b7":"code","696b1ad8":"code","93c7a4bf":"code","68a2bb6f":"code","57fb0cde":"code","4268de22":"code","17b16719":"code","b5491dba":"code","d774d7b6":"code","d1ef8bd3":"code","dee598e9":"code","89d979ad":"code","dfb58ec9":"code","6af1828f":"code","9152752f":"code","21fa9612":"code","4e246336":"code","5516e7e7":"code","2a00d224":"code","597a591f":"code","5e3e6c84":"code","dfe553d3":"code","96833e8c":"code","588ab7fe":"code","f8e215bc":"code","e2dfd4df":"code","ee490831":"code","8aee4703":"code","e045ff87":"code","a6c93e74":"code","b50c6009":"markdown","9c2b865a":"markdown","4b75e4d9":"markdown","0aeafb3a":"markdown","6c8a5b7c":"markdown","62492d60":"markdown","e4d37184":"markdown","8a79f1f4":"markdown"},"source":{"4e53032d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","23d45361":"# Step one : filter out the record without 'Postion' attributes\nplayer_data = pd.read_csv(\"..\/input\/fifa19\/data.csv\", header=0,  na_values=['.', '??','?', '', ' ', 'NA', 'na', 'Na', 'N\/A', 'N\/a', 'n\/a'])\n\nlegal_index = pd.notnull(player_data['Position'])\n\nplayer_data = player_data.loc[legal_index]\n\nplayer_data.index = range(len(player_data))","c0a4bb65":"player_data.hist(column=['HeadingAccuracy', 'Finishing'])","9003dfdf":"# Step two : add Duty attribute\n\nforward = [\"ST\", \"LW\", \"RW\", \"LF\", \"RF\", \"RS\",\"LS\", \"CF\"]\nmidfielder = [\"CM\",\"RCM\",\"LCM\", \"CDM\",\"RDM\",\"LDM\", \"CAM\", \"LAM\", \"RAM\", \"RM\", \"LM\"]\ndefender = [\"CB\", \"RCB\", \"LCB\", \"LWB\", \"RWB\", \"LB\", \"RB\"]\n\nplayer_data.loc[player_data[\"Position\"] == \"GK\", \"Duty\"] = 0\nplayer_data.loc[player_data[\"Position\"].isin(defender), \"Duty\"] = 1\nplayer_data.loc[player_data[\"Position\"].isin(midfielder), \"Duty\"] = 2\nplayer_data.loc[player_data[\"Position\"].isin(forward), \"Duty\"] = 3\n\nplayer_data['Duty'] = player_data['Duty'].astype('int')","dae0d0fc":"import seaborn as sns\n%pylab inline\nimport random\n# get a subtable\nplayer_data_forward =  player_data.loc[player_data['Duty']==3]\nplayer_data_midfielder = player_data.loc[player_data['Duty']==2]\nplayer_data_defender = player_data.loc[player_data['Duty']==1]\n\nindex = [i for i in range(player_data_forward.shape[0])]\nrandom.shuffle(index)\nplayer_data_forward = player_data_forward.iloc[index]\nplayer_data_midfielder = player_data_midfielder.iloc[index]\nplayer_data_defender = player_data_defender.iloc[index]\nsns.set_style('whitegrid')\nsns.regplot(x='Overall', y='Finishing', data=player_data_forward[:100])\nplt.figure()\nsns.regplot(x='Overall', y='Finishing', data=player_data_midfielder[:100])\nplt.figure()\nsns.regplot(x='Overall', y='Finishing', data=player_data_defender[:100])\nplt.figure()\ndel player_data_forward, player_data_midfielder, player_data_defender","95b69ecd":"# Step three : filter out columns without ability info.\nability = ['Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n       'GKKicking', 'GKPositioning', 'GKReflexes', 'Duty']\n\nplayer_data_filtered = player_data[ability]","958ffe40":"# fill the ability attribute of null with the mean_value of the column. \n# but in fact our dataset is complete enough with no null values now ! so there is no need to fill the null values.\ncol_with_null = [col for col in player_data_filtered.columns\n                if player_data_filtered[col].isnull().any()]\n\nprint(col_with_null)","7d0d69b7":"player_data_filtered.to_csv('\/kaggle\/working\/filtered_data.csv')","696b1ad8":"print(player_data_filtered.shape)\n\nplayer_data_filtered.describe().T.to_csv('\/kaggle\/working\/description.csv')","93c7a4bf":"# standardlize the attributes of players.\n\nX = player_data_filtered.drop(\"Duty\", axis = 1)\n\nfrom sklearn.preprocessing import StandardScaler\n\nScaler = StandardScaler()\n\nX = Scaler.fit_transform(X)\n\nY = player_data_filtered['Duty']","68a2bb6f":"#  Step one :split our traininng and test set.\nx_train = X[:14000]; y_train = Y[:14000]\n\nx_test = X[14000:]; y_test = Y[14000:]\ny_test.index = range(len(x_test))\n\n# Step two : construct our dataset.\nimport torch.utils.data as data\nimport torch\nclass Player_Dataset(data.Dataset):\n    def __init__(self, x, y):\n        self.input = x\n        self.target = y\n    def __getitem__(self, index):\n        return self.input[index], self.target[index]\n    def  __len__(self):\n        return len(self.target)\n    \ntrain_dataset = Player_Dataset(x_train, y_train)\n\ntest_dataset = Player_Dataset(x_test, y_test)\n\n# Step Three : construct our dataloader.\ntrain_iter = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\ntest_iter = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=32, shuffle=True, num_workers=4)","57fb0cde":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass Classifier(nn.Module):\n    def __init__(self, input_size, output_size, dropout):\n        super(Classifier, self).__init__()\n        \n        self.dropout = dropout\n        self.output_size = output_size\n        self.input_size = input_size\n        \n        self.linear_1 = nn.Linear(input_size, 128)\n        self.linear_2 = nn.Linear(128, 128)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_3 = nn.Linear(128, output_size)\n        \n    def forward(self, input):\n#         print(input.shape)\n        temp_result = F.relu(self.linear_1(input))\n#         print(temp_result.shape)\n        temp_result = F.relu(self.linear_2(temp_result))\n#         print(temp_result.shape)\n        temp_result = self.dropout(temp_result)\n#         print(temp_result.shape)\n        result = F.softmax(self.linear_3(temp_result), dim=1)\n#         print(result.shape)\n        return result\n    \nmodel = Classifier(33, 4, 0.5).to(device)\nprint(model)","4268de22":"# define our optimizer and  criterion.\n\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss().to(device)","17b16719":"def train(model, data, optimizer, criterion):\n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(data):\n        Input, target = batch\n        Input = Input.float().to(device); target = target.to(device)\n\n        output = model(Input)\n#         print(\"output\", output)\n#         print(\"target\", target)\n        loss = criterion(output, target)\n#         print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(data)","b5491dba":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  \/ 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs","d774d7b6":"N_epoches = 10\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_epoches):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iter, optimizer, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if train_loss < best_valid_loss:\n        best_valid_loss = train_loss\n        torch.save(model.state_dict(), 'model.pt')\n        \n    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain  Loss: {train_loss: .3f}')","d1ef8bd3":"from sklearn.metrics import classification_report\n\ndef evaluate():\n    result_on_test = []; ground_truth = []\n    model.eval()\n    for i, batch in enumerate(test_iter):\n        features, target = batch\n        ground_truth.extend(list(target))\n        with torch.no_grad():\n            output = model(features.float())\n            output = np.argmax(output, axis=1)\n            result_on_test.extend(output)\n            \n    acc = sum([(ground_truth[i] == result_on_test[i]).item() for i in range(len(result_on_test))]) \/ len(result_on_test)\n    print(f\"The classification accuracy is : {acc*100:.2f} %\\n\")   \n    print(classification_report(ground_truth, result_on_test))","dee598e9":"evaluate()","89d979ad":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\ntree_clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=6)","dfb58ec9":"%%time \n# to record time we used.\n\ntree_clf = tree_clf.fit(x_train, y_train)\n\ny_test_pred = tree_clf.predict(x_test)\n\nscore_decisionTree = metrics.accuracy_score(y_test, y_test_pred)\n\nprint(score_decisionTree)\nprint(classification_report(y_test, y_test_pred))","6af1828f":"%pylab inline\ndef fine_tuning():\n    result = []\n    for i in range(3, 11, 1):\n        tree_clf_tmp = DecisionTreeClassifier(criterion=\"entropy\", \n                                                     max_depth=i)\n        tree_clf_tmp = tree_clf_tmp.fit(x_train, y_train)\n        y_pred_tmp = tree_clf_tmp.predict(x_test)\n        score_decisionTree_tmp = metrics.accuracy_score(y_test, y_pred_tmp)\n        result.append(score_decisionTree_tmp)\n    result = np.array(result)\n    fig = plt.figure(figsize=(8,8))\n    plt.plot(range(3, 11, 1), result, c='blue')\n    plt.xlabel(r\"max_depth\")\n    plt.ylabel(r'accuracy')\n    plt.show()\n    \nfine_tuning()","9152752f":"!pip install pydotplus","21fa9612":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(tree_clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,\n                class_names=['0','1','2','3'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('DecisionTree_Model.png')\nImage(graph.create_png())","4e246336":"X.shape","5516e7e7":"from sklearn.cluster import KMeans\nfrom sklearn import metrics\n\n# using sklearn create KMeans model to do our experiments.\nKMean_model = KMeans(n_clusters=4, random_state=9)\ny_pred = KMean_model.fit_predict(X)\ny_pred.shape","2a00d224":"from sklearn.decomposition import PCA\nfrom sklearn import metrics\n%pylab inline\n\npca = PCA(n_components=2)\nX_decomposition = pca.fit_transform(X)","597a591f":"fig = plt.figure(figsize=(5, 5))\n\nplt.scatter(X_decomposition[:, 0], X_decomposition[:,1], c = y_pred,\n                 cmap = plt.cm.get_cmap(\"tab10\", 10), alpha = 0.5)","5e3e6c84":"print(metrics.calinski_harabasz_score(X, y_pred))","dfe553d3":"list(Y[10:20])","96833e8c":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots(figsize=(10,10))    \n\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n# print(classification_report(list(Y), y_pred))\n\nplot_confusion_matrix(list(Y), y_pred, classes=np.array([\"0\",\"1\",\"2\",\"3\"]), title='Confusion matrix using KMeans')","588ab7fe":"# using pandas built-in plot method to verify our assumption.\n%pylab inline\n\nplt.rcParams['figure.figsize'] = (10.0, 8.0) \nplt.rcParams['image.interpolation'] = 'nearest' # \u8bbe\u7f6e interpolation style\nplt.rcParams['image.cmap'] = 'gray'  \n\n# sns.set_style('whitegrid')\nsns.regplot(x='ShotPower', y='Finishing', data=player_data_filtered[:250])","f8e215bc":"def transform_discrete():\n    player_data_filtered_discrete = player_data_filtered.copy()\n    for column in player_data_filtered_discrete.columns:\n        if column == \"Duty\":\n            continue\n        player_data_filtered_discrete[column] = pd.cut(\n            player_data_filtered_discrete[column], [0, 30, 60, 100], labels=['bottom', 'middle', 'top'])\n    return player_data_filtered_discrete\n\nplayer_data_filter_discrete = transform_discrete()\nplayer_data_filter_discrete.head(3)","e2dfd4df":"player_data_filter_discrete.to_csv('\/kaggle\/working\/player_data_filter_discrete.csv')","ee490831":"Finishing = player_data_filter_discrete['Finishing'].groupby(player_data_filter_discrete['Finishing']).count()\nShotPower = player_data_filter_discrete['ShotPower'].groupby(player_data_filter_discrete['ShotPower']).count()\n\nplt.rcParams['figure.figsize'] = (10.0, 8.0) \nplt.rcParams['image.cmap'] = 'gray' \n# show first 28 picture of the data after filter\nfig, axes = plt.subplots(1, 2, figsize = (14, 7))\n\nplt.subplots_adjust(hspace=0, wspace=0)\n\naxes[0].set_title('Finishing Distribution')\naxes[0].pie(x=Finishing, labels=['bottom', 'medium', 'top'])\naxes[0].set(xticks=[], yticks = [])\naxes[1].set_title('ShotPower Distribution')\naxes[1].pie(x=ShotPower, labels=['bottom', 'medium', 'top'])\naxes[1].set(xticks=[], yticks = [])","8aee4703":"top_Finishing = player_data_filter_discrete['Finishing'].loc[player_data_filter_discrete['Finishing']=='top']\nbottom_Finishing = player_data_filter_discrete['Finishing'].loc[player_data_filter_discrete['Finishing']=='bottom']\n\ntop_ShotPower = player_data_filter_discrete['ShotPower'].loc[player_data_filter_discrete['ShotPower']=='top']\nbottom_ShotPower = player_data_filter_discrete['ShotPower'].loc[player_data_filter_discrete['ShotPower']=='bottom']\n\nboth_top = player_data_filter_discrete.loc[(player_data_filter_discrete['Finishing']=='top') & (player_data_filter_discrete['ShotPower']=='top')]\nboth_bottom = player_data_filter_discrete.loc[(player_data_filter_discrete['Finishing']=='bottom') & (player_data_filter_discrete['ShotPower']=='bottom')]","e045ff87":"P_bothTop = both_top.shape[0] \/ player_data_filter_discrete.shape[0]\n\nP_topFinishing = top_Finishing.shape[0] \/ player_data_filter_discrete.shape[0]\n\nP_topShotPower = top_ShotPower.shape[0] \/ player_data_filter_discrete.shape[0]\n\nprint(f\"Support rate: {P_bothTop*100:.2f}% \\n,\\\nConfidence (top shot power => top Finishing) : {P_bothTop \/ P_topShotPower*100:.2f}%\\n,\\\nConfidence (top Finishing => top shot power) : {P_bothTop \/ P_topFinishing*100:.2f}%\\n,\\\nLift rate : {P_bothTop \/ (P_topFinishing*P_topShotPower) * 100:.2f}%\\n\")","a6c93e74":"P_bothBottom = both_bottom.shape[0] \/ player_data_filter_discrete.shape[0]\n\nP_bottomFinishing = bottom_Finishing.shape[0] \/ player_data_filter_discrete.shape[0]\n\nP_bottomShotPower = bottom_ShotPower.shape[0] \/ player_data_filter_discrete.shape[0]\n\nprint(f\"Support rate: {P_bothBottom*100:.2f}% \\n,\\\nConfidence (bottom shot power => bottom Finishing) : {P_bothBottom \/ P_bottomShotPower*100:.2f}%\\n,\\\nConfidence (bottom Finishing => bottom shot power) : {P_bothBottom \/ P_bottomFinishing*100:.2f}%\\n,\\\nLift rate : {P_bothBottom \/ (P_bottomFinishing*P_bottomShotPower) * 100:.2f}%\\n\")","b50c6009":"## Is Finishing a important skill for a forward or defender or midfielder?\n\n1. get a sub table of all forward.\n\n2. using sns to plot a regplot.","9c2b865a":"## Statistics about the filtered dataset.\n\n1. how many records & how many attributes are there in our filtered dataset.\n\n2. some statistic about each column.","4b75e4d9":"### The first Task \n\nUsing pytorch to build a nerual network classifier to predict the player's duty.\n\n1. build our model \n\n2. define our optimizer and criterion\n\n3. train our model\n\n4. evaluate our model.","0aeafb3a":"## Analysis of Association Rules for Player Ability Values\n\n- We know that the player's ability values are not independent. For example, a player's Vision directly affects the player's LongPassing ability.\n\n- In this experiment, we want to verify the association rules between the player's ability values. The main consideration is an association rule:\n\n\u00a0\u00a0\u00a0\u00a0 -Association rules between players ShotPower and Finishing\n\u00a0\u00a0\u00a0\u00a0\n- We use filtered data for analysis","6c8a5b7c":"## Prepare the data\n\n\n1. fliter out the record without 'Position' attributes.\n\n2. add and duty attribute contain 4 kind of value : (0: \"goalkeeper\", 1: \"defender\", 2: \"midfielder\", 3: \"forward\")\n\n3. filter out columns without ability info.\n\n4. fill the null value in the ability table with the mean value in the table.","62492d60":"## Our Model\n\nFirst Task is to use a Neural Network module to do a claasification on Duty according to the attributes of the player.\n\nThe Second Task is to build a K-Means model to cluster the player ability attribute.\n\nThe last task is to consider the Association Rules between Player's Age and Player's Potential.","e4d37184":"### Task two \n\nwe are going to build a K-Means model to do the cluster task on player's attribute.\n","8a79f1f4":"We also write a Decision Tree model using sklearn. So now Let's begin!"}}