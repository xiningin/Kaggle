{"cell_type":{"70cb816c":"code","b880951c":"code","2b84f1ac":"code","5f14d3ae":"code","3da2846d":"code","b7b7290e":"code","5c928416":"code","a6b26cc9":"code","e321cc19":"code","19558ff0":"code","0b40c339":"code","4c8cc30e":"code","f84246bc":"code","ddf444a9":"code","2d750114":"code","0e3c2ba1":"code","8893d543":"code","17f8c9c9":"code","40659d03":"code","47c4b0fe":"code","eb4449e6":"code","465e9db8":"code","055cfdbf":"code","a8ff57ae":"code","3372a8d7":"code","d6dc13bd":"code","819de811":"code","67279bda":"code","59125b20":"code","c7f24e86":"code","444d4c10":"code","531f70c3":"markdown","4cb862b6":"markdown","8c6b98ab":"markdown","33925995":"markdown","56b45044":"markdown","0cebee89":"markdown","c069c9de":"markdown","05c9946f":"markdown","a1b29f96":"markdown"},"source":{"70cb816c":"from IPython.display import HTML,YouTubeVideo\ndisplay(YouTubeVideo('B2CHNrNmM80', width=600, height=300))","b880951c":"# 1.1 Data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# Dimensionality reduction\nfrom sklearn.decomposition import KernelPCA\n\n# Data transformation classes\nfrom sklearn.preprocessing import OneHotEncoder as ohe\nfrom sklearn.preprocessing import LabelEncoder as le\nfrom sklearn.preprocessing import StandardScaler as ss\n \n#Data splitting\nfrom sklearn.model_selection import TimeSeriesSplit\n\n#Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n#Model\n\nfrom skopt import BayesSearchCV\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import TimeSeriesSplit as tss\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Other small utilities\nfrom sklearn.metrics import make_scorer\nfrom pandas.tseries.offsets import MonthEnd\n\nimport gc\nimport datetime\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os","2b84f1ac":"os.chdir('\/kaggle\/input\/rossmann-store-sales\/')\ntrain = pd.read_csv(\"train.csv\",parse_dates=[2])\ntest = pd.read_csv(\"test.csv\",parse_dates=[3])\nstore = pd.read_csv(\"store.csv\")","5f14d3ae":"train.info()\ndisplay(HTML('<h3>Features in train having null values:<\/h3>'))\ntrain.columns.values[train.isnull().any()]\ndisplay(HTML('<h3>Features wise Minimum-Maximum in training dataset:<\/h3>'))\npd.DataFrame([train.min(),train.max()])","3da2846d":"train['Date']=train.Date.astype('datetime64[D]')\ntrain['Store']=train.Store.astype('category')\ntrain['DayOfWeek'] = train.DayOfWeek.astype('category')","b7b7290e":"train.head()","5c928416":"store.info()\ndisplay(HTML('<h3>Features wise Minimum-Maximum and NaN in Store dataset:<\/h3>'))\npd.DataFrame([store.min(),store.max(),store.isnull().sum(),store.nunique()],index=['Min','Max','Nulls','Unique'])","a6b26cc9":"store.CompetitionDistance.fillna(store.CompetitionDistance.mean(),inplace=True)\nstore[store.Promo2SinceWeek.isnull()].describe(include='all',percentiles=[])","e321cc19":"display(HTML('<h4>From table we get that the rest null values occur due to no promo2 are run on some stores, thus we can put a constant value 0 there'))\nstore.fillna(0,inplace=True)","19558ff0":"store.head()","0b40c339":"X=train.merge(store,on='Store',copy=False)","4c8cc30e":"_=plt.figure(figsize=(20,5))\n_=train.set_index(keys='Date',drop=False).resample('M')['Sales'].sum().plot(fontsize=20)\n_=plt.xlabel('Date', fontsize=20)\n_=plt.ylabel('Sales', fontsize=20)\n_=plt.suptitle('Rossmann Stores Sales over time', fontsize=30)","f84246bc":"_=train.set_index(keys='Date',drop=False).groupby('Store').resample('M')['Sales'].sum().reset_index(level=[0,1])\nf,ax=plt.subplots(10,1,sharex=True)\nax=ax.flatten()\nfor i in range(10):\n    __=_[_.Store==(i+1)].plot(x='Date',y='Sales',legend=False,title='Store'+str(i),ax=ax[i],figsize=(20,20))\ndel _\ndel __\ngc.collect()","ddf444a9":"%%time\ndisplay(HTML('<h4>Now we see week-of-day wise sales for all stores<\/h4>'))\n_=train.groupby('DayOfWeek').agg({'Sales':np.mean}).plot(kind='bar',color='bgyr',legend=[])\n__=_.set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])","2d750114":"display(HTML('The graph above poses 2 big questions:<br>1- Sunday having least sales!Why?<br>2- Whether it is just amount or customer footfall decrease?'))\n\n_=train.plot.scatter('Customers','Sales',s=20,c='DayOfWeek',cmap='rainbow',figsize=(10,5),alpha=0.6)","0e3c2ba1":"train.StateHoliday.replace({0:'0'},inplace=True)\nf,(ax1,ax2)=plt.subplots(1,2,figsize=(20,5))\n_=sns.heatmap(train.groupby(['DayOfWeek','StateHoliday']).agg({'Customers':np.mean}).unstack().fillna(0),cmap='GnBu',ax=ax1,annot=True)\n_=sns.heatmap(train.groupby(['SchoolHoliday','DayOfWeek']).agg({'Customers':np.mean}).unstack().fillna(0),cmap='GnBu',ax=ax2,annot=True)\n_=ax1.set_title('State Holiday Vs Day of Week')\n_=ax2.set_title('School Holiday Vs Day of Week')","8893d543":"_=train.merge(store, on='Store').groupby('Assortment').agg({'Customers':np.mean,'Sales':np.mean})\n_[\"SalesPerCustomer\"]=_.Sales\/_.Customers\n_","17f8c9c9":"f,ax=plt.subplots(1,2,figsize=(20,5))\n_=sns.distplot(train.Customers,ax=ax[0])\n_=sns.distplot(train.Sales,ax=ax[1])\nax[0].set_title('Customers Distribution')\nax[1].set_title('Sales Distribution')","40659d03":"train['TicketSize']= train['Sales'] \/ train['Customers']\nmed_sales= train.groupby('Store')[['Sales', 'Customers', 'TicketSize']].median()\nmed_sales.rename(columns=lambda x: x+'_median', inplace=True)\ntrain.drop(columns=['TicketSize'],inplace=True)\n#med_sales.sample(5)\n\ndef build_features(train):\n    X= train.merge(med_sales,on='Store')\n    X = X.merge(store,on='Store')\n    X['Year'] = X.Date.dt.year\n    X['Month'] = X.Date.dt.month\n    X['Day'] = X.Date.dt.day\n    X['Q_Month'] = (train.Date.dt.month-1)%3+1\n    X['CompOpSinceMonth']=(X.Year-X.CompetitionOpenSinceYear)*12+(X.Month-X.CompetitionOpenSinceMonth)\n    X['LeftDaysInMonth'] = ((X.Date+MonthEnd(0))-X.Date).dt.days\n    \n    #store.PromoInterval.astype('category').cat.categories\n    cat1 = pd.CategoricalDtype(categories=[0, 'Jan,Apr,Jul,Oct', 'Feb,May,Aug,Nov', 'Mar,Jun,Sept,Dec'])\n    X['PromoInterval'] = X.PromoInterval.astype(cat1).cat.codes\n    \n    ##Change types\n    cat_cols = ['Store', 'DayOfWeek', 'Open','Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment','Promo2','Q_Month','PromoInterval']\n    int_cols = ['CompetitionOpenSinceMonth','CompetitionOpenSinceYear', 'Promo2SinceWeek','Promo2SinceYear', 'Year', 'Day', 'CompOpSinceMonth', 'LeftDaysInMonth']\n    for col in cat_cols:\n        X[col] = X[col].astype('category')\n    for col in int_cols:\n        X[col] = X[col].astype('int')\n    return X","47c4b0fe":"X= build_features(train)\ny=X.pop('Sales')\ny = np.log1p(y)","eb4449e6":"def rmspe_log1p(y,yhat):\n    y=np.expm1(y)\n    yhat=np.expm1(yhat)\n    weight=pd.Series([1\/a if a!=0 else 0 for a in y])\n    return np.sqrt(np.mean((weight*(yhat-y))**2))\n\nrmspe_scorer = make_scorer(rmspe_log1p, greater_is_better = False)","465e9db8":"mCat_cols = ['Store','DayOfWeek','StateHoliday','StoreType', 'Assortment','Q_Month','PromoInterval']\nbin_cat_cols = ['Open', 'Promo','SchoolHoliday','Promo2']\nnum_cols = X.select_dtypes('number').columns.to_list()","055cfdbf":"ctt = ColumnTransformer(\n                        [\n                            ('mcat',ohe(),mCat_cols),\n                            ('num',ss(),num_cols)\n                        ])\nX_t=ctt.fit_transform(X)","a8ff57ae":"xgboost_tree = XGBRegressor(\n    n_jobs = -1,\n    n_estimators = 1000,\n    eta = 0.1,\n    max_depth = 2,\n    min_child_weight = 2,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    tree_method = 'exact',\n    reg_alpha = 0.05,\n    random_state = 1023\n)\nxgboost_tree.fit(X_t, y,\n                 eval_metric = rmspe_log1p\n                )","3372a8d7":"rmspe_log1p(y,xgboost_tree.predict(X_t))","d6dc13bd":"import xgboost as xgb\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    \n    weight=pd.Series([1\/a if a!=0 else 0 for a in y])\n    return \"rmspe\", -np.sqrt(np.mean((weight*(yhat-y))**2))\n\ndtrain = xgb.DMatrix(X_t, y)\n\nparams = {\n    'n_estimators': (200, 2000),\n    'max_depth' :(1,8),\n    'eta':(0.01, 0.6, 'log-uniform'),\n    'colsample_bytree':(0.1,0.9,'uniform'),\n    'gamma':(1,10),\n    'alpha':(0,10),\n    'lambda':(1,10),\n    'subsample':(0.1,1.0),\n    'min_child_weight':(0,5)\n}\n\nbayes_cv = BayesSearchCV(\n                        estimator = XGBRegressor(objective= 'reg:linear',\n                                                 booster='gbtree',\n                                                 verbosity=2,\n                                                 tree_method='hist',\n                                                 feval=rmspe_xg\n                                                ),\n                        search_spaces = params, \n                        cv=tss(3),\n                        n_jobs=-1,\n                        n_iter = 100,\n                        verbose=0\n                        )","819de811":"%%time\nbayes_cv.fit(X_t,y.values)","67279bda":"bayes_cv.best_score_","59125b20":"rmspe_log1p(bayes_cv.predict(X_t),y)","c7f24e86":"bayes_cv.best_params_","444d4c10":"rand_stores = np.random.randint(0,1115,5)\n_ = pd.DataFrame(train[['Date','Store','Sales']])\n_['Prediction'] = bayes_cv.predict(X_t)\n_['Prediction'] = np.expm1(_.Prediction)\n_=_.set_index(keys='Date',drop=False).groupby('Store').resample('M')['Sales','Prediction'].sum().reset_index(level=[0,1])\nf,ax=plt.subplots(5,1,sharex=True)\nax=ax.flatten()\nfor i in range(5):\n    __=_[_.Store==rand_stores[i]].plot(x='Date',y='Sales',title='Store'+str(rand_stores[i]),ax=ax[i],figsize=(20,20))\n    __=_[_.Store==rand_stores[i]].plot(x='Date',y='Prediction',title='Store'+str(rand_stores[i]),ax=ax[i])\n    \ndel _\ndel __\ngc.collect()","531f70c3":"It suggests while \"extra\" assortment level stores cater more customers the actual average sales is much lesser than even basic stores. **It suffices the assortment level plays a big role in average sales as well actual footfall**.<br>\nTo know the the actual feed on sales and customers to be given on model we need to know the distribution of the same.","4cb862b6":"The above three plots suggest that there is rather smaller footfall on Sundays and StateHoldays affect the footfall quite adversely while SchoolHolidays have marked only a small dent.<br>\nAnother question is whether big counters actually attract more customers or rather sales...lets see!","8c6b98ab":"# Goal\n<b><font size='5'>T<\/font>HE<\/b> goal of this kernel is to analyse the sales data of <a href='https:\/\/www.kaggle.com\/c\/rossmann-store-sales'>Rossmann Store Sales<\/a> along with stationary data regading its stores and promotions run during whole year and then to predict the sales at a store for further days. These stores are spread in 7 European countries and our task is to predict the sales for upto 6 weeks in advance. The sales may get influenced by numerous factors counting from store location, season, local conditions, running promotion (its total length and time elapsed etc), holiday and other demographic factors.  A robust prediction model will enable managers to plan the resources accoringly to increase productivity. <br>\nThe dataset here is divided into **two** files:-\n - Stores master data for its 1115 stores - store.csv and\n - transaction data - train.csv\/test.csv\n \n## Data fields\n\nMost of the fields are self-explanatory. The following are descriptions for those that aren't.\n\n   - Id - an Id that represents a (Store, Date) duple within the test set\n   - Store - a unique Id for each store\n   - Sales - the turnover for any given day (this is what you are predicting)\n   - Customers - the number of customers on a given day\n   - Open - an indicator for whether the store was open: 0 = closed, 1 = open\n   - StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n   - SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n   - StoreType - differentiates between 4 different store models: a, b, c, d\n   - Assortment - describes an assortment level: a = basic, b = extra, c = extended\n   - CompetitionDistance - distance in meters to the nearest competitor store\n   - CompetitionOpenSince[Month\/Year] - gives the approximate year and month of the time the nearest competitor was opened\n   - Promo - indicates whether a store is running a promo on that day\n   - Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n   - Promo2Since[Year\/Week] - describes the year and calendar week when the store started participating in Promo2\n   - PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew","33925995":"As both of the plots are skewed we can use medians for our model.","56b45044":"Here Store,DayOfWeek,Open,Promo and SchoolHoliday are obvious categorical features but since last 3 are binary we need not encode them furher.","0cebee89":"# Importing required packages\nHere we need various packages ranging from common data processing (numpy\/pandas) to Visualization to machine learning modeling packages like scikit\/mlextend\/prophet etc.","c069c9de":"Now we look into Store-wise monthly sales of first few stores.","05c9946f":"<h3> OBSERVATIONS:<\/h3>\n<ol>\n    <li>CompetionDistance is Null at 3 places:may impute it with mean.<\/li>\n    <li>CompetitionSinceMonth and CompetionSinceYear are Null at 354 samples:Better we drop the columns.<\/li>\n    <li>Promo2 details are uniformly missing in 544 samples:We can look wether these null are common samples or diversed(data missing)<\/li>\n<\/ol>","a1b29f96":"## Evaluation\nTo evaluate the model we will use the loss function **Root Mean Squared Percentage Error** which is calculated as:<br>\n$rmspe=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} {\\left (\\frac{y_i-\\hat{y_i}}{y_i}  \\right )}^{2}}$\n<br>but here we will use log of sales instead of sales itself thus metric may be recalculated as:<br>\n$rmspe-log1p=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} {\\left (\\frac{e^{\\hat{y_i}}-1}{e^{y_i}-1}-1  \\right )}^{2}}$"}}