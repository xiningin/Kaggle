{"cell_type":{"5ba56c4f":"code","2e1aa316":"code","ffa64260":"code","091e2627":"code","98d180c3":"code","728f92ff":"code","f54c96d6":"code","c3853a18":"code","3b5cd7ca":"code","9b86fa66":"code","f1ac74d3":"code","d6c5e353":"code","8abfaec3":"code","65af54ba":"code","bdfee7c0":"code","d9fd87fa":"code","7d22a291":"code","e95515be":"code","86aa9695":"code","39c88d1f":"code","25fe11a7":"code","8eda34bf":"code","731feaa0":"code","7b4b7eaf":"code","5f697131":"code","35b22958":"code","bc22b4a9":"code","bdd60f78":"code","302058d0":"code","b4880460":"code","d1cdc4c2":"code","f9325723":"code","31e172b2":"code","863ce884":"code","7f7a7e6f":"code","1771902b":"code","830a1c79":"code","76793a9b":"code","388c70bf":"code","9fc1dac9":"code","c596e822":"code","9e7fa21b":"code","47765d65":"code","83d8638f":"code","dd8793a6":"code","b9a70c81":"code","7aae2c1d":"code","ba7e3a41":"code","ba58ab6f":"code","341711c1":"code","dac6afe6":"code","cc814147":"code","b97c01b8":"code","b6dc94b5":"code","f64307a4":"code","56a676c8":"code","07b0f50a":"code","30699e68":"code","edea4980":"code","3cb3805f":"code","0f7d927e":"code","b234d485":"code","30766089":"code","a21234ee":"code","b7ad8277":"code","edd76a71":"code","2bc9bf85":"code","4d7c425c":"code","c36f2db1":"code","92ef66d8":"code","ba201f2d":"code","3b3af4ec":"code","a699d62c":"code","4dc73fff":"code","2e1ee99f":"code","8cd7b768":"code","8412c521":"code","6541e674":"code","7ee87691":"code","e70f6a49":"code","ef0daaca":"code","c3b0258f":"code","61037930":"code","ac1763e9":"code","cfebaf4f":"code","bc21c3df":"code","ebc01ba0":"code","01c7c4de":"code","9dd51574":"code","b8d1b808":"code","507a507f":"code","48070faf":"markdown","f0176641":"markdown","6640a9fa":"markdown","b0155544":"markdown","7236a54b":"markdown","db35a7cf":"markdown","82bf78d7":"markdown","224a8b92":"markdown","3cb84e15":"markdown","bf622b26":"markdown","185e7d3d":"markdown","52ae03d4":"markdown","e702acaa":"markdown","b85c6407":"markdown","dbfa773f":"markdown","5f55936d":"markdown","63a001d4":"markdown","8a99b7c7":"markdown","b9f3e0cf":"markdown","e5639227":"markdown","aca498de":"markdown","aba5d2e0":"markdown","b485fe6b":"markdown","cf0f33ac":"markdown","24fc05e3":"markdown","2d57dc18":"markdown","42d1e18e":"markdown","04072db8":"markdown","6e490f25":"markdown","69918633":"markdown","d7905eec":"markdown","3610b54c":"markdown","f03b71ad":"markdown","5454737f":"markdown","c918d437":"markdown","9c0d1162":"markdown","aa7359e5":"markdown","cccdf71e":"markdown","038ba97c":"markdown","d5ba6910":"markdown","86418b02":"markdown","46abfb39":"markdown","0b5829b3":"markdown","413583d5":"markdown","d74eccd1":"markdown","714d2f09":"markdown","8fbeb1a4":"markdown"},"source":{"5ba56c4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# importing stuff\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# a lot of stuff\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Z-score \/ outliers stuff\nfrom scipy import stats\n\n# Rede Neural stuff\nfrom tensorflow.keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import plot_model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e1aa316":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf\n\n## Categoriza\u00e7\u00e3o:\ncategorical = ['animal', 'city', 'furniture']\nnumerical = ['area', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\nordinal = ['rooms', 'bathroom', 'parking spaces', 'floor']\n\n# Tamanho do dataset:\nprint(\"{} linhas\\n{} colunas\/features.\".format(df.shape[0], df.shape[1]))\n\n#infos: \n#df.info()\n\n# Vers\u00e3o bem simples de dataset para servir como exemplo e para conseguir rodar o notebook inteiro\n# df.drop(columns='city', inplace=True)\n# df.drop(columns='floor', inplace=True)\n# df.drop(columns='animal', inplace=True)\n# df.drop(columns='furniture', inplace=True)\n# df.dropna(inplace=True)\n\n# df = aqui vem o dataset processado, SEM ESCALONAMENTO\/NORMALIZADOR\n\n# tabela\n# df.describe()","ffa64260":"df","091e2627":"sns.heatmap(df.corr())","98d180c3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Lidando com os dados ausentes\n# Funcao do Pandas usada para contar o numero de valores vazios de cada coluna\ndata = df.isna().sum(axis=0)\ny = list(range(df.shape[1]))\nx = data.values\n\n# Criamos uma figura\nfig, ax = plt.subplots(figsize=(10, 15))\n\n# Plota as barras\nax.barh(y=y, width=x)\n\n# Adiciona informa\u00e7\u00f5es no gr\u00e1fico\nax.set_yticks(y)\nax.set_yticklabels(df.columns.values)\nax.set_title(\"Quantidade de vari\u00e1veis ausentes por coluna\")\nplt.show()\n","728f92ff":"# Retorna o nro da dados faltantes por coluna, em seguida exibe os valores faltantes totais em cada uma das 13 colunas\nmissing_values_count = df.isnull().sum()\nmissing_values_count[0:13]","f54c96d6":"# Retorna o total de valores faltantes que temos\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n# Porcentagem de dados que est\u00e1 faltando\npercent_missing = (total_missing\/total_cells) * 100\nprint(percent_missing, \"percent of the data is missing\")","c3853a18":"df['animal'].value_counts()","3b5cd7ca":"#animal - Criei uma nova op\u00e7\u00e3o e setei como n\u00e3o informado onde havia dados faltantes. \n\ndf['animal'] = df['animal'].fillna('not informed')\ndf['animal'].value_counts()","9b86fa66":"# furniture - Criei uma nova op\u00e7\u00e3o e setei como n\u00e3o informado onde havia dados faltantes. \n\ndf['furniture'] = df['furniture'].fillna('not informed')\ndf['furniture'].value_counts()","f1ac74d3":"# city - Criei uma nova op\u00e7\u00e3o e setei como n\u00e3o informado onde havia dados faltantes. \n\ndf['city'] = df['city'].fillna('not informed')\ndf['city'].value_counts()\n\n#A principio eu tinha setado como nao informado por\u00e9m conversando com colegas e pesquisnando, vi que n\u00e3o seria muito interessante esse tipo de tratativa\n#ent\u00e3o optei por deletar as linhas que n\u00e3o informavam a cidade.","d6c5e353":"# uso de mascara para esconder os dados inves de dar um drop\ndf = df[df['city'] != 'not informed']","8abfaec3":"df['city'].value_counts()","65af54ba":"df['floor'].unique()","bdfee7c0":"df = df[df['floor'] != '-']\ndf['floor'].unique()","d9fd87fa":"df['floor'].replace('', np.nan, inplace=True)\ndf.dropna(subset=['floor'], inplace=True)\n\ndf['floor'].unique()\n","7d22a291":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Lidando com os dados ausentes\n# Funcao do Pandas usada para contar o numero de valores vazios de cada coluna\ndata = df.isna().sum(axis=0)\ny = list(range(df.shape[1]))\nx = data.values\n\n# Criamos uma figura\nfig, ax = plt.subplots(figsize=(10, 15))\n\n# Plota as barras\nax.barh(y=y, width=x)\n\n# Adiciona informa\u00e7\u00f5es no gr\u00e1fico\nax.set_yticks(y)\nax.set_yticklabels(df.columns.values)\nax.set_title(\"Quantidade de vari\u00e1veis ausentes por coluna\")\nplt.show()","e95515be":"df['rooms'].value_counts()","86aa9695":"#Substiuindo os valores vazios pelo dado mais coerente que \u00e9 a existencia de pelo menos um quarto com certeza.\n#.fillna substitui o argumento nos dados ausentes\ndf['rooms'] = df['rooms'].fillna('1.0')","39c88d1f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Lidando com os dados ausentes\n# Funcao do Pandas usada para contar o numero de valores vazios de cada coluna\ndata = df.isna().sum(axis=0)\ny = list(range(df.shape[1]))\nx = data.values\n\n# Criamos uma figura\nfig, ax = plt.subplots(figsize=(10, 15))\n\n# Plota as barras\nax.barh(y=y, width=x)\n\n# Adiciona informa\u00e7\u00f5es no gr\u00e1fico\nax.set_yticks(y)\nax.set_yticklabels(df.columns.values)\nax.set_title(\"Quantidade de vari\u00e1veis ausentes por coluna\")\nplt.show()","25fe11a7":"df['parking spaces'].value_counts()","8eda34bf":"#Substiuindo os valores vazios pelo dado que aparece mais \n#.fillna substitui o argumento nos dados ausentes\ndf['parking spaces'] = df['parking spaces'].fillna('1.0')","731feaa0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Lidando com os dados ausentes\n# Funcao do Pandas usada para contar o numero de valores vazios de cada coluna\ndata = df.isna().sum(axis=0)\ny = list(range(df.shape[1]))\nx = data.values\n\n# Criamos uma figura\nfig, ax = plt.subplots(figsize=(10, 15))\n\n# Plota as barras\nax.barh(y=y, width=x)\n\n# Adiciona informa\u00e7\u00f5es no gr\u00e1fico\nax.set_yticks(y)\nax.set_yticklabels(df.columns.values)\nax.set_title(\"Quantidade de vari\u00e1veis ausentes por coluna\")\nplt.show()","7b4b7eaf":"df['bathroom'].value_counts()","5f697131":"#Substiuindo os valores vazios pelo dado mais coerente que \u00e9 a existencia de pelo menos um banheiro com certeza.\n#.fillna substitui o argumento nos dados ausentes\ndf['bathroom'] = df['bathroom'].fillna('1.0')","35b22958":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Lidando com os dados ausentes\n# Funcao do Pandas usada para contar o numero de valores vazios de cada coluna\ndata = df.isna().sum(axis=0)\ny = list(range(df.shape[1]))\nx = data.values\n\n# Criamos uma figura\nfig, ax = plt.subplots(figsize=(10, 15))\n\n# Plota as barras\nax.barh(y=y, width=x)\n\n# Adiciona informa\u00e7\u00f5es no gr\u00e1fico\nax.set_yticks(y)\nax.set_yticklabels(df.columns.values)\nax.set_title(\"Quantidade de vari\u00e1veis ausentes por coluna\")\nplt.show()","bc22b4a9":"df_regression = pd.concat([df['area'], df['hoa (R$)'], df['rent amount (R$)'], df['property tax (R$)'], df['fire insurance (R$)'], df['total (R$)']], axis=1)\ndf_regression.head()","bdd60f78":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp_mean = IterativeImputer(random_state=0)\nimp_mean.fit(df_regression.values)\n\nregr_output = imp_mean.transform(df_regression.values)","302058d0":"df['area'] = regr_output[:, 0]\ndf['hoa (R$)'] = regr_output[:, 1]\ndf['rent amount (R$)'] = regr_output[:, 2]\ndf['property tax (R$)'] = regr_output[:, 3]\ndf['fire insurance (R$)'] = regr_output[:, 4]\ndf['total (R$)'] = regr_output[:, 5]\n","b4880460":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## Lidando com os dados ausentes\n# Funcao do Pandas usada para contar o numero de valores vazios de cada coluna\ndata = df.isna().sum(axis=0)\ny = list(range(df.shape[1]))\nx = data.values\n\n# Criamos uma figura\nfig, ax = plt.subplots(figsize=(10, 15))\n\n# Plota as barras\nax.barh(y=y, width=x)\n\n# Adiciona informa\u00e7\u00f5es no gr\u00e1fico\nax.set_yticks(y)\nax.set_yticklabels(df.columns.values)\nax.set_title(\"Quantidade de vari\u00e1veis ausentes por coluna\")\nplt.show()","d1cdc4c2":"df.isna().sum()","f9325723":"selected_columns = numerical \n# + ordinal\ndf[selected_columns].head()","31e172b2":"fig, axes = plt.subplots(ncols=6, figsize=(15, 5))\n\nfor i,col in enumerate(selected_columns):\n    axes[i].boxplot(df[col])\n    axes[i].set_title(col)\n\nplt.tight_layout()","863ce884":"def graph_scatter():\n    fig, ax = plt.subplots()\n\n    ax.scatter(x=df['total (R$)'], y=df['area'])\n    ax.set_ylabel(\"area\")\n    ax.set_xlabel(\"total (R$)\")\n    plt.show()\ngraph_scatter()","7f7a7e6f":"print(\"Tamanho do dataset antes dos filtros: {}\".format(df.shape))\n\nmask = df['area'] < 1250\ndf = df[mask]\nmask = df['total (R$)'] < 30000\ndf = df[mask]\n\nprint(\"Tamanho do dataset depois dos filtros: {}\".format(df.shape))\n\ngraph_scatter()","1771902b":"import plotly.express as px\n\ndata = df[['property tax (R$)', 'rent amount (R$)']].dropna()\nx = data['property tax (R$)'].values\ny = data['rent amount (R$)'].values\n\nmask1 = np.abs(stats.zscore(x)) < 3\nmask2 = np.abs(stats.zscore(y)) < 3\nmask = np.logical_and(mask1, mask2)\n\nfig = px.scatter(df, x=\"property tax (R$)\", y=\"rent amount (R$)\")\n\nfig.add_shape(type=\"rect\",\n    x0=min(x[mask]), y0=min(y[mask]), x1=max(x[mask]), y1=max(y[mask]),\n    line=dict(color=\"Green\", width=2,),\n    opacity=0.2,\n    fillcolor=\"Green\",\n)\n\nfig.show()","830a1c79":"from scipy import stats\n\ncolumns = ['area', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\n\nfig, axes = plt.subplots(nrows=len(columns), ncols=2, figsize=(15,30))\n\nfor i, col in enumerate(columns):\n    # Plotamos o histograma na esquerda\n    axes[i][0].hist(df[col], bins=50)\n    \n    # Obtemos a mascara booleana de poss\u00edveis outliers (de acordo com o valor de Z)\n    mask = np.abs(stats.zscore(df[col].dropna())) < 3\n    dado_filtrado = df[col].dropna()[mask]\n    \n    # Plotamops o histograma na direita, j\u00e1 filtrado\n    axes[i][1].hist(dado_filtrado)\n    \n    # Aproveitamos para plotar no esquerda, onde n\u00f3s \"cortamos\" o histograma, para gerar o hist da direita\n    ymax = axes[i][0].get_yticks()[-1]\n    axes[i][0].vlines(x=max(dado_filtrado), ymin=0, ymax=ymax, color=\"red\")\n    \n    # Damos nomes aos gr\u00e1ficos\n    axes[i][0].set_title(\"{} - sem filtro de outliers\".format(col))\n    axes[i][1].set_title(\"{} - com filtro de outliers\".format(col))","76793a9b":"def graph_scatter():\n    fig, ax1 = plt.subplots()\n\n    ax1.scatter(x=df['hoa (R$)'], y=df['property tax (R$)'])\n    ax1.set_ylabel(\"property tax (R$)\")\n    ax1.set_xlabel(\"hoa (R$)\")\n    plt.show()\ngraph_scatter()","388c70bf":"print(\"Tamanho do dataset antes dos filtros: {}\".format(df.shape))\n# df1 = df\nmask = df['property tax (R$)'] < 6000\ndf = df[mask]\nmask = df['hoa (R$)'] < 10000\ndf = df[mask]\n\nprint(\"Tamanho do dataset depois dos filtros: {}\".format(df.shape))\n\ngraph_scatter()","9fc1dac9":"def graph_scatter():\n    fig, ax1 = plt.subplots()\n\n    ax1.scatter(x=df['rent amount (R$)'], y=df['fire insurance (R$)'])\n    ax1.set_ylabel(\"fire insurance (R$)\")\n    ax1.set_xlabel(\"rent amount (R$)\")\n    plt.show()\ngraph_scatter()","c596e822":"print(\"Tamanho do dataset antes dos filtros: {}\".format(df.shape))\n# df1 = df\nmask = df['fire insurance (R$)'] < 250\ndf = df[mask]\nmask = df['rent amount (R$)'] < 15100\ndf = df[mask]\n\nprint(\"Tamanho do dataset depois dos filtros: {}\".format(df.shape))\n\ngraph_scatter()","9e7fa21b":"df","47765d65":"df1 = df\n\ndf1['totalPerArea'] = df1['total (R$)'] \/ df1['area']\n\ndf1[['totalPerArea', 'total (R$)', 'area']]","83d8638f":"df2 = df\ndf2['firePerRent'] = df2['fire insurance (R$)']\/df['rent amount (R$)']\ndf2[['firePerRent', 'fire insurance (R$)', 'rent amount (R$)']]","dd8793a6":"df['animal'].replace(to_replace='acept', value=1, inplace=True)\ndf['animal'].replace(to_replace='not acept', value=0, inplace=True)\ndf['animal'].replace(to_replace='not informed', value=2, inplace=True)\n\ndf['furniture'].replace(to_replace='furnished', value=1, inplace=True)\ndf['furniture'].replace(to_replace='not furnished', value=0, inplace=True)\ndf['furniture'].replace(to_replace='not informed', value=2, inplace=True)\n\ndf['city'].replace(to_replace='S\u00e3o Paulo', value=1, inplace=True)\ndf['city'].replace(to_replace='Rio de Janeiro', value=2, inplace=True)\ndf['city'].replace(to_replace='Belo Horizonte', value=3, inplace=True)\ndf['city'].replace(to_replace='Porto Alegre', value=4, inplace=True)\ndf['city'].replace(to_replace='Campinas', value=5, inplace=True)","b9a70c81":"# # OneHot Encoding\n# df = df[df['city'].notna()]\n# enc = OneHotEncoder(handle_unknown='ignore')\n# enc.fit(df[['city']])\n# enc_df = pd.DataFrame(enc.transform(df[['city']]).toarray(), columns=enc.get_feature_names(['city']))\n# df.reset_index(drop=True, inplace=True)\n# enc_df.reset_index(drop=True, inplace=True)\n# df = df.join(enc_df)\n\ndf","7aae2c1d":"# Normalizamos os dados de df em uma escala de [0, 1]\n# Estou fazendo isto aqui pois temos que \"desnormalizar\" na hora de gerar os gr\u00e1ficos de R\u00b2\ncolumn_names = df.columns\nscaler = MinMaxScaler()\nscaler.fit(df)\ndf = scaler.transform(df)\ndf = pd.DataFrame(df)\ndf.columns = column_names\n\n# Pegamos o dataset df e separamos em x (entrada) e y (saida), numa separa\u00e7\u00e3o 70% treino e 30% valida\u00e7\u00e3o\ninput_dim = df.shape[1] - 1\nx = df.drop(columns='total (R$)')\ny = df['total (R$)']\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.30, random_state=42)","ba7e3a41":"NEURONIOS_CAMADA_INICIAL = 10\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [20, 30]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","ba58ab6f":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)\n","341711c1":"\ndf_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","dac6afe6":"NEURONIOS_CAMADA_INICIAL = 16\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [64, 32]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","cc814147":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)\n","b97c01b8":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","b6dc94b5":"NEURONIOS_CAMADA_INICIAL = 16\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [64, 32, 16, 32]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","f64307a4":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)\n","56a676c8":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","07b0f50a":"NEURONIOS_CAMADA_INICIAL = 16\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [64, 32, 16, 32]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'tanh'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","30699e68":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)\n","edea4980":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","3cb3805f":"NEURONIOS_CAMADA_INICIAL = 32\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [64, 32, 16, 32]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","0f7d927e":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)","b234d485":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","30766089":"NEURONIOS_CAMADA_INICIAL = 32\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [64, 32, 16, 32]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = 'l1'\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","a21234ee":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)","b7ad8277":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","edd76a71":"NEURONIOS_CAMADA_INICIAL = 32\n\n# N\u00famero de camadas intermedi\u00e1rias e neur\u00f4nios. Tamanho do array s\u00e3o os n\u00fameros de camadas, elementos do array s\u00e3o n\u00fameros de neur\u00f4nios.\n# Ex: [30, 15] = 2 camadas intermedi\u00e1rias com 30 neur\u00f4nios na primeira e 15 neur\u00f4nios na segunda\n# Ex: [] = Nenhuma camada intermedi\u00e1ria\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermedi\u00e1rias, com 10 neur\u00f4nios nas 4 primeiras e 50 neur\u00f4nios na \u00faltima\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [64, 32, 16, 32]\n\n# Usar dropout: True para usar, False para n\u00e3o usar\nUSAR_DROPOUT = True\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = N\u00e3o usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Fun\u00e7\u00e3o de ativa\u00e7\u00e3o: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Defini\u00e7\u00e3o da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermedi\u00e1rias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# \u00daltima camada da RNA (1 sa\u00edda)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que c\u00f3digo tosco!\" Tamb\u00e9m acho... Caso voc\u00ea queira criar sua pr\u00f3pria arquitetura\n# sem usar os par\u00e2metros acima, \u00e9 bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","2bc9bf85":"CALLBACKS = [] # Defini\u00e7\u00e3o dos callbacks a serem utilizados. Isso aqui \u00e9 opcional, mas pode ajudar: https:\/\/keras.io\/api\/callbacks\/early_stopping\/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' \u00e9 o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compila\u00e7\u00e3o do modelo + Defini\u00e7\u00e3o da Fun\u00e7\u00e3o de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)","4d7c425c":"df_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R\u00b2 = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","c36f2db1":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf","92ef66d8":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\nsns.heatmap(df.corr())","ba201f2d":"df.dtypes","3b3af4ec":"df['floor'].unique()","a699d62c":"columns = ['city', 'animal', 'furniture']\n\nfor col in columns:\n    print(df[col].unique())","4dc73fff":"# Comentei isso aqui pq demora batante pra gerar a imagem. Se quiser ver, pode descomentar.\n# sns.pairplot(df[['hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']])","2e1ee99f":"import plotly.express as px\n\ndata = df[['property tax (R$)', 'rent amount (R$)']].dropna()\nx = data['property tax (R$)'].values\ny = data['rent amount (R$)'].values\n\nmask1 = np.abs(stats.zscore(x)) < 3\nmask2 = np.abs(stats.zscore(y)) < 3\nmask = np.logical_and(mask1, mask2)\n\nfig = px.scatter(df, x=\"property tax (R$)\", y=\"rent amount (R$)\")\n\nfig.add_shape(type=\"rect\",\n    x0=min(x[mask]), y0=min(y[mask]), x1=max(x[mask]), y1=max(y[mask]),\n    line=dict(color=\"Green\", width=2,),\n    opacity=0.2,\n    fillcolor=\"Green\",\n)\n\nfig.show()","8cd7b768":"columns = ['area', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\n\nfig, axes = plt.subplots(nrows=len(columns), ncols=2, figsize=(15,30))\n\nfor i, col in enumerate(columns):\n    # Plotamos o histograma na esquerda\n    axes[i][0].hist(df[col], bins=50)\n    \n    # Obtemos a mascara booleana de poss\u00edveis outliers (de acordo com o valor de Z)\n    mask = np.abs(stats.zscore(df[col].dropna())) < 3\n    dado_filtrado = df[col].dropna()[mask]\n    \n    # Plotamops o histograma na direita, j\u00e1 filtrado\n    axes[i][1].hist(dado_filtrado)\n    \n    # Aproveitamos para plotar no esquerda, onde n\u00f3s \"cortamos\" o histograma, para gerar o hist da direita\n    ymax = axes[i][0].get_yticks()[-1]\n    axes[i][0].vlines(x=max(dado_filtrado), ymin=0, ymax=ymax, color=\"red\")\n    \n    # Danos nomes aos gr\u00e1ficos\n    axes[i][0].set_title(\"{} - sem filtro de outliers\".format(col))\n    axes[i][1].set_title(\"{} - com filtro de outliers\".format(col))","8412c521":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ncolumns = ['city', 'animal', 'furniture']\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n\ndf['city'].value_counts().plot.barh(ax=axes[0])\ndf['animal'].value_counts().plot.barh(ax=axes[1])\ndf['furniture'].value_counts().plot.barh(ax=axes[2])\n\naxes[0].set_title(\"city\")\naxes[1].set_title(\"animal\")\naxes[2].set_title(\"furniture\")\nplt.tight_layout()\n","6541e674":"# Para os numeros discretos:\ncolumns = ['rooms', 'bathroom', 'parking spaces', 'floor']\n\ndf_aux = df.copy()\ndf_aux['floor'].replace(to_replace='-', value=0, inplace=True)\ndf_aux['floor'] = pd.to_numeric(df_aux['floor'].values)\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18, 5))\n\nfor i, col in enumerate(columns):\n    axes[i].hist(df_aux[col], bins=df_aux[col].nunique())\n    axes[i].set_title(col)","7ee87691":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf.isna().sum().plot.barh()\nplt.show()","e70f6a49":"df.isna().sum(axis=1).value_counts().plot.barh()","ef0daaca":"df_original = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n# Faremos agora algumas imputa\u00e7\u00f5es de valores ausentes usando regress\u00e3o\n# Para CRIAR o IMPUTADOR, vamos utilizar dados LIMPOS. Ou seja, sem dados ausentes e sem outliers.\ncolumns = ['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']\ndf_regress = df_original[columns].copy()\n\nfor col in columns:\n    value = df_original[col].median()\n    df_regress[col].fillna(value=value, inplace=True)\n\nmask = (np.abs(stats.zscore(df_regress)) < 3).all(axis=1)\ndf_regress = df_regress[mask]\n\ndf_regress.describe()","c3b0258f":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Agora criaremos o regressor com os dados limpos\n# Criamos um objeto que far\u00e1 a Imputa\u00e7\u00e3o por Regress\u00e3o\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regress\u00e3o com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor \"limpo\"\nX = df_original[['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']].values\nregr_output = imp_mean.transform(X)\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in foo.columns:\n    valores_menor_0 = (foo[col]<0).sum()\n    print(\"Existem {} dados com valor < 0 em '{}'\".format(valores_menor_0, col))\n\ndf[columns] = foo[mask]\n    \nfoo[mask].describe()","61037930":"df_original = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n# Faremos agora algumas imputa\u00e7\u00f5es de valores ausentes usando regress\u00e3o\n\ncolumns = ['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']\ndf_regress = df_original[columns].copy()\ndf_regress.dropna(inplace=True)\n\ndf_regress.describe()","ac1763e9":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Agora criaremos o regressor com os dados limpos\n# Criamos um objeto que far\u00e1 a Imputa\u00e7\u00e3o por Regress\u00e3o\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regress\u00e3o com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor \"limpo\"\nX = df_original[['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']].values\nregr_output = imp_mean.transform(X)\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in foo.columns:\n    valores_menor_0 = (foo[col]<0).sum()\n    print(\"Existem {} dados com valor < 0 em '{}'\".format(valores_menor_0, col))\n\ndf[columns] = foo[mask]\n    \nfoo[mask].describe()","cfebaf4f":"df_original = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n\n# Faremos agora algumas imputa\u00e7\u00f5es de valores ausentes usando regress\u00e3o\ncolumns = ['rooms', 'bathroom', 'parking spaces', 'rent amount (R$)']\ndf_regress = df_original[columns].copy()\n\nfor col in columns:\n    value = df_original[col].median()\n    df_regress[col].fillna(value=value, inplace=True)\n\nmask = (np.abs(stats.zscore(df_regress)) < 3).all(axis=1)\ndf_regress = df_regress[mask]\n\n# Criamos um objeto que far\u00e1 a Imputa\u00e7\u00e3o por Regress\u00e3o\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regress\u00e3o com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor\nX = df_original[columns].values\nregr_output = np.round(imp_mean.transform(X))\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in ['rooms', 'bathroom', 'parking spaces']:\n    print(\"{}: {}\".format(col, foo[col].unique()))\n    \n\ndf[columns] = foo[mask]\n\nfoo[mask].describe()","bc21c3df":"df_antes = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\ndf_depois = df_antes.copy()\n\n# Substituindo dados ausentes pela categoria com maior frequ\u00eancia\ndf_depois['animal'].fillna(value=df_depois['animal'].mode()[0], inplace=True)\n\n# Substituindo dados ausentes por \"Indefinido\"\ndf_depois['furniture'].fillna(value=\"Indefinido\", inplace=True)\n\n# Substituindo dados ausentes pelo pr\u00f3ximo dado presente\ndf_depois.fillna(method='ffill', inplace=True)\n\nfig, axes = plt.subplots(3, 1, figsize=(15, 10))\n\nfor i, col in enumerate(['city', 'animal', 'furniture']):\n    axes[i].barh(width=df_depois[col].value_counts(), y=df_depois[col].unique(), label='depois')\n    axes[i].barh(width=df_antes[col].value_counts(), y=df_antes[col].dropna().unique(), label='antes')\n    axes[i].legend()\n    \naxes[0].set_title(\"Imputa\u00e7\u00e3o copiando o dado anterior. Resultado proporcional\", fontsize=18)\naxes[1].set_title(\"Imputa\u00e7\u00e3o usando a moda. Aumenta a despropor\u00e7\u00e3o\", fontsize=18)\naxes[2].set_title(\"Cria\u00e7\u00e3o de uma nova classe\", fontsize=18)\nplt.tight_layout()\nplt.show()","ebc01ba0":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf['animal'].replace(to_replace='acept', value=1, inplace=True)\ndf['animal'].replace(to_replace='not acept', value=0, inplace=True)\n\ndf['furniture'].replace(to_replace='furnished', value=1, inplace=True)\ndf['furniture'].replace(to_replace='not furnished', value=0, inplace=True)\n\ndf","01c7c4de":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\n# OneHot Encoding\ndf = df[df['city'].notna()]\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(df[['city']])\nenc_df = pd.DataFrame(enc.transform(df[['city']]).toarray(), columns=enc.get_feature_names(['city']))\ndf.reset_index(drop=True, inplace=True)\nenc_df.reset_index(drop=True, inplace=True)\ndf = df.join(enc_df)\n\ndf","9dd51574":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf['Feature Nova 1'] = df['area'] ** 2\ndf['Feature Nova 2'] = df['fire insurance (R$)']\/df['rent amount (R$)']\ndf['Feature Nova 3'] = df['rooms'] * df['bathroom']\n\ndf","b8d1b808":"df = pd.read_csv('..\/input\/aula-2-ia-dataset\/CasasParaAlugar.csv', index_col=0)\n\ndf.dropna(inplace=True)\ndf['floor'].replace(to_replace='-', value=0, inplace=True)\ndf['floor'] = pd.to_numeric(df['floor'].values)\n\ncolumns = ['area', 'rooms', 'bathroom', 'parking spaces', 'floor', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)']\nx = df[columns]\ny = df['total (R$)']\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_regression\n\n# k \u00e9 o numero de features que N\u00c3O ser\u00e3o jogadas foras. Vamos primeiro ver os resultados, depois eliminar alguma feature.\nk = x.shape[1]\n# Utilizamos um m\u00e9todo do sklearn para isso, usando a estrat\u00e9gia Chi Squared.\nselector = SelectKBest(f_regression, k=k)\nx_new = selector.fit_transform(x, y)","507a507f":"# Utilizo o log10 pois os valores s\u00e3o ou muito grandes, ou muito pequenos\nscores = -np.log10(selector.pvalues_)\n\nx_plot = list(range(len(scores)))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nplt.bar(x_plot, scores)\nax.set_title(\"Score do m\u00e9todo f-regression para Feature Selection\")\nax.set_xticks(x_plot)\nax.set_xticklabels(columns, rotation=45)\nplt.show()","48070faf":"# An\u00e1lise de caract\u00e9res especiais","f0176641":"# Exemplo de imputa\u00e7\u00e3o dos valores ausentes das vari\u00e1veis discretas:\n#### 'parking spaces', 'rooms', 'bathroom'","6640a9fa":"# Arquitetura da Rede Neural\n#### Criei um c\u00f3digo bem simples pra permitir criar diferentes redes neurais modificando apenas algumas vari\u00e1veis (EM CAPSLOCK),\n#### Mas se quiser criar sua pr\u00f3pria arquitetura mais customizada, fique a vontade","b0155544":"# An\u00e1lise de correla\u00e7\u00e3o entre as vari\u00e1veis\n#### Pode ser interessante na hora de imputa\u00e7\u00e3o","7236a54b":"# Lembrando:\n\n## **Data de entrega:**\n#### - 21\/06\/2021 (segunda-feira)\n\n## **O que ser\u00e1 avaliado de forma objetiva:**\n#### - Se o modelo est\u00e1 ajustado (sem overfit\/underfit). Ou seja, se as curvas de treino (azul) e valida\u00e7\u00e3o (laranja) est\u00e3o suficientemente pr\u00f3ximos.\n#### - A quantidade de dados (linhas de df) e features (colunas de df) foram utilizados.\n#### - O valor de R\u00b2\n\n## **O que ser\u00e1 avaliado de forma subjetiva:**\n#### - Esfor\u00e7o. Ou seja, quando mais explorar as combina\u00e7\u00f5es de pre-processamento + par\u00e2metros de rede, melhor. Para isso, crie novas vers\u00f5es do notebook.\n\n## Observa\u00e7\u00f5es\n#### - O valor de R ser\u00e1 analisado juntamente com o n\u00famero de features e dados usados. Ou seja, um R alto obtido usando poucos dados e features n\u00e3o vale muita coisa. Mas \u00e9 melhor que nada ;)\n#### - Se deixou de utilizar colunas do df = menos pontos. Se fez engenharia de features = mais pontos (pouco). Isto serve para incentivar a utilizar os dados categ\u00f3ricos.\n\n## Dicas:\n#### - Quando alcan\u00e7ar um resultado que queira salvar, clique em Save Version -> Quick Save. Isto far\u00e1 com que o seus resultados atuais sejam mostrados na vers\u00e3o html. Se clicar em Save Version -> Save & Run All (commit), o notebook ser\u00e1 rodado novamente, e isso pode modificar os resultados obtidos.\n#### - No come\u00e7o, KISS (Keep it simple, silly). Comece de forma simples. Comece com menos features e com pouco pre processamento. Recomendo come\u00e7ar usando vari\u00e1veis num\u00e9ricas e filtro de outliers. Isso j\u00e1 deve ser suficiente para obter um R alto. MAS ISSO N\u00c3O \u00c9 SUFICIENTE! Depois que conseguir um modelo com alto R usando um modelo e pre processamento simples, salve a vers\u00e3o do notebook e continue explorando! A meta \u00e9 conseguir utilizar TODAS as features e o maior n\u00famero de dados poss\u00edvel. O objetivo \u00e9 explorar a combina\u00e7\u00e3o de pr\u00e9-processamento de dados + par\u00e2metros da rede neural.\n#### - Eu vou analisar as diferentes vers\u00f5es do notebook (se houver). Ou seja, se na primeira vers\u00e3o obteve um modelo bom usando poucas features, ok... Se na segunda vers\u00e3o obteve um modelo um pouco pior ou melhor usando mais features, melhor ainda. Explore as combina\u00e7\u00f5es! You're a wizard Harry!","db35a7cf":"**Teste 5**","82bf78d7":"**Analisando os dados faltantes:**","224a8b92":"# Dataset\n#### Coloque aqui seu data cleaning e seu pre-processamento e atribua o dataset para um dataframe chamado **df**","3cb84e15":"# Exemplo de imputa\u00e7\u00e3o dos valores ausentes das vari\u00e1veis cont\u00ednuas:\n#### 'hoa', 'rent amount', 'property tax', 'fire insurance', 'total'","bf622b26":"#### Como estamos falando de n\u00famero de vagas de estacionamentos, quartos e banheiros, o esperado \u00e9 que a maior parte dos dados esteja nos valores pequenos, formando uma distribui\u00e7\u00e3o assim\u00e9trica. Ainda assim, existem alguns casos... at\u00edpicos, como por exemplo um im\u00f3vel com 12 quartos. Isso \u00e9 um outlier? Depende! Se for um hotel, 12 quartos pode ser considerado pouco.","185e7d3d":"**Teste 2**","52ae03d4":"# Dataset Original","e702acaa":"# Exemplo de imputa\u00e7\u00e3o dos valores ausentes de vari\u00e1veis categ\u00f3ricas:\n#### 'city', 'animal', 'furniture'","b85c6407":"# Distribui\u00e7\u00e3o das classes e dos n\u00fameros discretos","dbfa773f":"#### \u00c9 percept\u00edvel que existem outliers. Vamos tra\u00e7ar um Scatter Plot de 2 destas vari\u00e1veis usando Plotly para criar um gr\u00e1fico interativo para analisar um pouco melhor alguns desses outliers. Vamos tamb\u00e9m aplicar o Z-Score com z=3 para ter uma no\u00e7\u00e3o da faixa dos valores dos outliers.","5f55936d":"Removendo os dados com - e vazios em floor","63a001d4":"#### \u00c9 esperado que 'city', 'animal' e 'furniture'sejam considerados como tipo 'object', pois nestas colunas existem valores do tipo string. No entanto, 'floor' deveria ser considerado float64. Por que 'floor' est\u00e1 sendo considerado como object?","8a99b7c7":"# Exemplo de Feature Engineering (bem simples)\n#### Criar novas features a partir de features j\u00e1 existentes\n#### Obs: N\u00e3o \u00e9 obrigat\u00f3rio criar novas Features neste trabalho","b9f3e0cf":"# Separa\u00e7\u00e3o dos dados","e5639227":"#### Vamos plotar os Histogramas dessas vari\u00e1veis cont\u00ednuas sem filtros e com filtros de outliers (Z-Score com z=3)","aca498de":"#### \u00c9 poss\u00edvel remover estas vari\u00e1veis outliers conforme indica o gr\u00e1fico. Al\u00e9m disso, o valor de Z \u00e9 ajust\u00e1vel: consideramos z=3, mas caso queira mais rigor, pode diminuir o valor de z ou aument\u00e1-lo, caso queira um maior relaxamento na remo\u00e7\u00e3o de outliers.","aba5d2e0":"# Exemplo Feature Encoding","b485fe6b":"### Imputa\u00e7\u00e3o com dados n\u00e3o tratados:","cf0f33ac":"**Teste 6** - Usando regularizador","24fc05e3":"#### A maioria das LINHAS possuem 1 dado ausente. Boa parte (cerca de 2900, segundo o gr\u00e1fico) n\u00e3o cont\u00e9m NENHUM dado ausente. Outra boa parte dos dados possuem 2 dados ausentes.\n\n#### Com base nisso, existem algumas perguntas:\n - Quais vari\u00e1veis valem a pena imputar os dados ausentes?\n - Quais vari\u00e1veis vale a pena deletar os dados ausentes?\n - Se formos deletar linhas com vari\u00e1veis ausentes, qual seria nossa estrat\u00e9gia? Deletamos linhas com mais de 3 var ausentes ou todas?\n \n#### Existem v\u00e1rias respostas, e todas elas tem pr\u00f3s e contras e dependendem do dataset e do contexto de modelagem.","2d57dc18":"**Detec\u00e7\u00e3o de outliers**","42d1e18e":"#### Olhando os valores presentes em 'floor', percebemos que existe o caract\u00e9r '-' presente. Al\u00e9m disso, n\u00e3o existem valores iguais a 0. Temos algumas possibilidades aqui:\n - Considerar '-' como 0 e fazer esta substitui\u00e7\u00e3o (onde '-' significaria andar 0 ou t\u00e9rreo)\n - Deletar todas as linhas com floor igual a '-'","04072db8":"- Podemos substituir as vari\u00e1veis ausentes pela moda dos valores (o n\u00famero que mais se repete)\n- Podemos substituir o dado ausente pelo dado anterior\/seguinte\n- Podemos criar uma nova categoria \"Indefinido\".","6e490f25":"# Trabalho Final de Est\u00e1gio Docente\n## Estagi\u00e1rio: Douglas Macedo Sgrott\n## Aluno: Eduarda Cristina Rosa\n## Data de entrega: 21\/06\/2021 (segunda-feira)\n## O trabalho est\u00e1 organizado em partes:\n - ### **Dataset: Onde voc\u00ea ir\u00e1 limpar e pre processar o dataset. Atribua a vers\u00e3o final do dataset em um dataframe chamado df.**\n - Separa\u00e7\u00e3o dos dados: Aqui os dados s\u00e3o normalizados e divididos em Treino\/Valida\u00e7\u00e3o. N\u00e3o precisa modificar o c\u00f3digo.\n - ### **Arquitetura da Rede Neural: Onde voc\u00ea vai definir a arquitetura da rede neural.**\n - ### **Par\u00e2metros de otimiza\u00e7\u00e3o da Rede Neural: Onde voc\u00ea vai definir outros par\u00e2metros da rede neural.**\n - Visualiza\u00e7\u00e3o dos resultados: Onde os resultados s\u00e3o obtidos\n - Exemplos: Servir como exemplo de an\u00e1lise, data cleaning e pr\u00e9-processamento.\n","69918633":"#### Vamos aproveitar tamb\u00e9m e verificar os valores de 'city', 'animal' e 'furniture'","d7905eec":"**Teste 3**","3610b54c":"# Feature Selection\n#### \u00datil para decidir qual feature seria mais interessante de descartar. Estou inserindo apenas para fins de completude.","f03b71ad":"# Visualiza\u00e7\u00e3o dos resultados","5454737f":"### Vamos usar o PairPlot do Seaborn para tra\u00e7ar o conjunto de histogramas e scatterplots das vari\u00e1veis cont\u00ednuas:\n#### 'hoa', 'rent amount', 'property tax', 'fire insurance', 'total'","c918d437":"**Feature encoding**","9c0d1162":"**Teste 7** - Usando dropout","aa7359e5":"#### V\u00e1rias estrat\u00e9gias podem ser usadas. Aqui, usaremos uma regress\u00e3o, mas iremos arredondar os n\u00fameros gerados para continuarem discretos.\n\n#### Para esta imputa\u00e7\u00e3o, usaremos 'rooms', 'bathroom' e 'parking spaces' que s\u00e3o valores discretos mas usaremos tamb\u00e9m 'rent amount' pois todas essas vari\u00e1veis tem UM ALTO VALOR DE CORRELA\u00c7\u00c3O, e isso \u00e9 importante na constru\u00e7\u00e3o do regressor.","cccdf71e":"#### Primeiro vamos tratar os dados para criar um regressor com dados \"limpos\". Com base nesse regressor \"limpo\", vamos fazer uma imputa\u00e7\u00e3o dos dados ausentes do dataframe. Vamos tamb\u00e9m comparar o resultado obtido com o resultado da imputa\u00e7\u00e3o usando um regressor \"sujo\", gerado a partir de dados n\u00e3o tratados.\n\n### Imputa\u00e7\u00e3o com dados tratados:","038ba97c":"**Anota\u00e7\u00f5es**\n\nDataframe: Estrutura de duas dimens\u00f5es como uma tabela (linha, coluna). \n\nOs dados podem ser trabalhados tamb\u00e9m utilizando a estrutura Serie, para dados unidimensionais como uma coluna unica de uma tabela por exemplo. ","d5ba6910":"#### Cerca de 10% dos dados de cada COLUNA est\u00e3o ausentes.\n#### Vamos verificar a aus\u00eancia de dados por LINHA:","86418b02":"**Teste 4** - Fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh","46abfb39":"# An\u00e1lise de Outliers","0b5829b3":"# Vari\u00e1veis ausentes\n\n#### Vamos plotar o n\u00famero de dados ausentes por coluna em um Gr\u00e1fico de Barras","413583d5":"**Feature enginnering**","d74eccd1":"# FIM DO TRABALHO \/\\\n# **INICIO DOS EXEMPLOS V**\n","714d2f09":"Os dados restantes ser\u00e3o imputados.","8fbeb1a4":"# Par\u00e2metros de otimiza\u00e7\u00e3o da Rede Neural\n#### Pode alterar os valores das vari\u00e1veis que est\u00e3o EM CAPSLOCK"}}