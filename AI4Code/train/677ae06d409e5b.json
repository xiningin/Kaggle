{"cell_type":{"8f558a31":"code","7d6cc51a":"code","1b1021ea":"code","52fcae0d":"code","f3abe030":"code","2375eda4":"code","39430762":"code","b6bde77a":"code","badd30b7":"code","7f3c1e71":"code","88f844f9":"code","d02dc804":"code","e566dded":"code","7dc12175":"code","0ec3eaeb":"code","a3722099":"code","0e54596e":"code","7b1a3a90":"code","2b7eb115":"code","645616d8":"code","f8bc6993":"code","39d2cf3e":"code","d0400f2c":"code","102526f3":"code","bac153d1":"code","859f0e25":"code","37ff9372":"code","8e06c3f8":"code","83f16a1c":"code","e7c359c8":"code","f49ef8e2":"code","6c1bb9f7":"code","059f0223":"markdown","d4644d07":"markdown","2be773b5":"markdown","106b361c":"markdown","9a76603a":"markdown","4fba0204":"markdown","5d844bca":"markdown","c1515679":"markdown","f49e8f68":"markdown","fc8c84dc":"markdown","f2934be6":"markdown","e30194f3":"markdown","e8867cf0":"markdown","f1467dc9":"markdown","3515348d":"markdown","55cc1b95":"markdown"},"source":{"8f558a31":"import pandas as pd\nimport numpy as np","7d6cc51a":"data=pd.read_csv(\"\/kaggle\/input\/social-network-ads\/Social_Network_Ads.csv\")\ndata.head()","1b1021ea":"data.Purchased.value_counts()","52fcae0d":"data.info()","f3abe030":"import seaborn as sns \nsns.distplot(data.EstimatedSalary)","2375eda4":"sns.distplot(data.Age)","39430762":"sns.boxplot(data.EstimatedSalary)","b6bde77a":"from sklearn.model_selection import train_test_split\nX=data.drop(\"Purchased\",axis=1)\ny=data.Purchased\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=300)","badd30b7":"samplingdf=pd.DataFrame(columns=[\"Model\",\"Sampling\",\"Accuracy\",\"Recall\",\"Precision\"])","7f3c1e71":"def Model_pipeline(X_train,X_test,y_train,y_test,sampling,samplingdf):    \n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    X_trainstand=scaler.fit(X_train).transform(X_train)\n    X_teststand=scaler.transform(X_test)\n    #classification Metrics\n    def metrics(clf,model,sampling,samplingdf):\n        print(\"Model Type:\",model,sampling)\n        from sklearn.metrics import accuracy_score\n        from sklearn.metrics import recall_score\n        from sklearn.metrics import precision_score\n        y_pred_test = clf.predict(X_teststand)\n        print(\"Accuracy for Test set:\")\n        print(accuracy_score(y_test,y_pred_test)) \n        print(\"\\n\")\n        print(\"Recall for Test set:\")\n        print(recall_score(y_test,y_pred_test,pos_label=1))\n        print(\"\\n\")\n        print(\"Precision for Test set:\")\n        print(precision_score(y_test,y_pred_test,pos_label=1))\n        print(\"\\n\")\n        print(\"------------------------------------------------------------------------------\")\n        input1=pd.Series([model,sampling,accuracy_score(y_test,y_pred_test),\n                recall_score(y_test,y_pred_test,pos_label=1),precision_score(y_test,y_pred_test,pos_label=1)], index = samplingdf. columns)\n        samplingdf=samplingdf.append(input1,ignore_index=True)\n        return samplingdf\n    \n    from sklearn.linear_model import LogisticRegression\n    clf = LogisticRegression(random_state=0).fit(X_trainstand, y_train)\n    samplingdf=metrics(clf,\"Logistic Regression\",sampling,samplingdf)\n    \n    from sklearn.tree import DecisionTreeClassifier\n    clf = DecisionTreeClassifier(random_state=0).fit(X_trainstand,y_train)\n    samplingdf=metrics(clf,\"Decision Tree\",sampling,samplingdf)\n    \n    #GBM\n    from sklearn.ensemble import GradientBoostingClassifier\n    clf=GradientBoostingClassifier(random_state=0).fit(X_trainstand,y_train)\n    samplingdf=metrics(clf,\"Gradient Booster\",sampling,samplingdf)\n    \n    #RandomForest\n    from sklearn.ensemble import RandomForestClassifier\n    clf = RandomForestClassifier(random_state=0)\n    clf.fit(X_trainstand, y_train)\n    samplingdf=metrics(clf,\"Random Forest\",sampling,samplingdf)\n    \n    #XGBoost\n    from xgboost import XGBClassifier\n    XGB_model = XGBClassifier(learning_rate=0.05)\n    XGB_model.fit(X_trainstand, y_train)\n    samplingdf=metrics(clf,\"XGBoost\",sampling,samplingdf)\n    \n    #SVM\n    from sklearn.svm import SVC\n    clf = SVC(random_state=0)\n    clf.fit(X_trainstand, y_train)\n    samplingdf=metrics(clf,\"SVM normal\",sampling,samplingdf)\n    \n    #kernal SVM\n    from sklearn.svm import SVC\n    clf = SVC(kernel=\"rbf\")\n    clf.fit(X_trainstand, y_train)\n    samplingdf=metrics(clf,\"SVM Kernal\",sampling,samplingdf)\n    \n    return samplingdf","88f844f9":"samplingdf=Model_pipeline(X_train,X_test,y_train,y_test,\"No sampling\",samplingdf)","d02dc804":"samplingdf","e566dded":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=300)\nX_train, y_train = ros.fit_resample(X_train, y_train)","7dc12175":"samplingdf=Model_pipeline(X_train,X_test,y_train,y_test,\"Over sampling\",samplingdf)","0ec3eaeb":"from imblearn.under_sampling import ClusterCentroids\ncc = ClusterCentroids(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=300)\nX_train, y_train = cc.fit_resample(X_train, y_train)","a3722099":"samplingdf=Model_pipeline(X_train,X_test,y_train,y_test,\"Under sampling\",samplingdf)","0e54596e":"from imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=300)\nX_train, y_train = sm.fit_resample(X_train, y_train)","7b1a3a90":"samplingdf=Model_pipeline(X_train,X_test,y_train,y_test,\"SMOTE\",samplingdf)","2b7eb115":"samplingdf","645616d8":"samplingdf.sort_values(by=[\"Model\",\"Sampling\"])","f8bc6993":"samplingdf.groupby(\"Sampling\").mean()","39d2cf3e":"import seaborn as sns \n\nprint(sns.barplot(data=samplingdf.groupby(\"Sampling\").median().reset_index(),x=\"Sampling\",y=\"Accuracy\"))\n","d0400f2c":"print(sns.barplot(data=samplingdf.groupby(\"Sampling\").median().reset_index(),x=\"Sampling\",y=\"Recall\"))","102526f3":"print(sns.barplot(data=samplingdf.groupby(\"Sampling\").median().reset_index(),x=\"Sampling\",y=\"Precision\"))","bac153d1":"samplingdf[samplingdf.Model==\"Logistic Regression\"]","859f0e25":"sns.barplot(data=samplingdf[samplingdf.Model==\"Logistic Regression\"],x=\"Sampling\",y=\"Accuracy\")","37ff9372":"sns.barplot(data=samplingdf[samplingdf.Model==\"Logistic Regression\"],x=\"Sampling\",y=\"Recall\")","8e06c3f8":"sns.barplot(data=samplingdf[samplingdf.Model==\"Logistic Regression\"],x=\"Sampling\",y=\"Precision\")","83f16a1c":"samplingdf[samplingdf.Model==\"XGBoost\"]","e7c359c8":"sns.barplot(data=samplingdf[samplingdf.Model==\"XGBoost\"],x=\"Sampling\",y=\"Accuracy\")","f49ef8e2":"sns.barplot(data=samplingdf[samplingdf.Model==\"XGBoost\"],x=\"Sampling\",y=\"Recall\")","6c1bb9f7":"sns.barplot(data=samplingdf[samplingdf.Model==\"XGBoost\"],x=\"Sampling\",y=\"Precision\")","059f0223":"Let's look at the importance of class imbalance","d4644d07":"# SMOTE","2be773b5":"We will try and explore the impact of sampling on Accuracy, Precision and Recall. \n\nWe will use 3 sampling techniques:\n* Over Sampling \n* Under Sampling\n* SMOTE(Synthetic Minority Over-sampling Technique)\n\nWe will see which one has the most impact on Metrics. We will also try and see when we need to use Sampling and if it really helps.\n\nHere, we are taking a simple dataset with 2 independent columns, no outliers and no null values.","106b361c":"From the above charts we see that for strong models:\n* Accuracy increases but barely and only when using SMOTE\n* Recall increases significantly,especially when using SMOTE\n* Precision decreases especially when under sampling, but the decrease is insignificant when using SMOTE","9a76603a":"## Let's look at how it affects weak models","4fba0204":"### From all the above I have come to a conclusion that sampling is extremely effective in increasing Recall, which is the popular metric among others. However, when precision is involved, the safest bet is not indulge in any Sampling. \n\n### I believe that the best sampling to use in any circumstance is SMOTE since its has the highest increase in recall in most cases and lowest decrease in precision.","5d844bca":"# Over Sampling","c1515679":"# Conclusion","f49e8f68":"## How about Strong Models?","fc8c84dc":"Recall has increased immensly due to sampling with fair 40-45% increase, any sampling definately increases Recall. Over sampling and Under sampling seem to be giving the same recall ","f2934be6":"# Under sampling","e30194f3":"Considering the aggregate of all models, we can see that sampling does not have much of an effect on accuracy,with a barely 1% difference.\n\nHowever especially over sampling and SMOTE increase the recall considerably with over a 6% increase with SMOTE.\n\nBut we have to keep in mind that precision is effected significantly as we upscale with a reduction of more than 3-4%, this might not be important, for example a cancer prediction will depend on recall. however its important in applications such as youtube recommendations,etc. ","e8867cf0":"There doesn't seem to be much affect on accuracy with a 1%-2% increase due to oversampling, I would not consider this significant.","f1467dc9":"<h1><center>Over Sampling and its effects on Model Metrics<\/center><\/h1>","3515348d":"Precision has taken a hit with 10-12% decrease due to sampling.","55cc1b95":"# Analysing the Results"}}