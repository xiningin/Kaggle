{"cell_type":{"e1620bdb":"code","8a52cba2":"code","b588aca4":"code","cb52a39d":"code","9239ad9c":"code","e2969f3a":"code","b8815a70":"code","5500cb93":"code","039e9dc0":"code","49096e90":"code","309aac26":"code","470d68ee":"code","a8e64b52":"code","9750296e":"code","c99029f7":"code","2ef1b7d7":"code","241e98f3":"code","54db75db":"code","a2895de0":"code","d85596af":"code","53152190":"code","9ee12d31":"code","9c26a734":"code","f61cd129":"code","fcd4ec36":"code","e978948a":"code","0e026d23":"code","2a3c251e":"code","9b6f2c5d":"code","dc9e701a":"code","295852a4":"code","54a2b28e":"code","5845e759":"code","9ee65a73":"code","825eb8b4":"code","26417dfb":"code","fbcfa15b":"code","efc7f8ab":"code","8081f1bf":"code","24054c67":"code","abf8c4a1":"code","330fe8b7":"code","23fda59f":"code","8aeed35a":"code","a28209aa":"code","153737ad":"code","26c77d14":"code","3e60b96a":"code","7020d908":"code","d38538c1":"code","be47e95d":"code","9037e544":"code","060ecb90":"code","09991421":"code","7bdeb91e":"code","fd29f18f":"code","71be0a7f":"code","da753f3f":"code","56c07a29":"code","d9516b21":"code","d8c018c0":"code","04c7f221":"code","72efa6e4":"code","ede40622":"code","aa3ecf6a":"code","99393cc3":"code","77629bbe":"code","252a67d5":"code","bb9d560f":"markdown","0de15d06":"markdown","3b52c64f":"markdown","6ff7aa5e":"markdown","2ec90dbe":"markdown","ee066b98":"markdown","f42ed5a7":"markdown","95aa8a37":"markdown","4f848eb2":"markdown","b47b9859":"markdown","06fbb98e":"markdown"},"source":{"e1620bdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","8a52cba2":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","b588aca4":"train.columns = train.columns.str.lower()","cb52a39d":"train.head()","9239ad9c":"train.shape","e2969f3a":"train.isnull().sum() \/ train.shape[0] * 100","b8815a70":"import missingno as msno\nmsno.bar(train,color = (0.9,0.4,1))","5500cb93":"corr_matrix = train.corr()\nsns.heatmap(corr_matrix,annot=True)","039e9dc0":"# survived ratio\ntrain['survived'].value_counts().plot(kind='pie',autopct='%.2f%%')\nplt.title('survived rate')","49096e90":"train2 = train.copy()\ntrain2['die'] = 1-train['survived']\nfig,(ax1,ax2) = plt.subplots(ncols=2,figsize=(12,8))\ntrain2.groupby('sex').agg('sum')[['survived','die']].plot(kind='bar',ax=ax1)\nax1.set(title = 'survived rate by sex')\ntrain2.groupby('pclass').agg('sum')[['survived','die']].plot(kind='bar',ax=ax2)\nax2.set(title = 'survived rate by pclass')","309aac26":"y_position = 1.02\nfig, axes = plt.subplots(1,2 , figsize = (18,8))\ntrain['pclass'].value_counts().plot.bar(color = ['#CD7F32', '#FFDF00', '#D3D3D3'] , ax = axes[0] )\naxes[0].set_title('Number of passengers by Pclass', y = y_position)\naxes[0].set_ylabel('Count')\nsns.countplot('pclass', hue = 'survived', data = train, ax = axes[1])\naxes[1].set_title('Pclass: Survived vs Dead' , y = y_position)\nplt.show()","470d68ee":"fig, axes = plt.subplots(1,2, figsize = (18,8))\ntrain[['sex','survived']].groupby(['sex'], as_index = True).mean().plot.bar(ax = axes[0])\naxes[0].set_title('Survived vs Sex') \nsns.countplot('sex' , hue = 'survived', data = train, ax = axes[1])\naxes[1].set_title('Sex: Survived vs Dead')\nplt.show ","a8e64b52":"train['embarked'].fillna(train['embarked'].mode()[0],inplace=True) # Replace missing values with mode.\n# Through name , we can predict job and age\ntrain['job'] = train['name'].apply(lambda x: x.split(',')[1].split('.')[0].lstrip())","9750296e":"train['fare'] = train['fare'].apply(np.round)","c99029f7":"train.head()","2ef1b7d7":"train['family_size']= train['sibsp']+train['parch']+1\n# 1 - alone, 2~4 - small , 5~ big\ntrain['family_size'] = pd.cut(train['family_size'],[0,2,4,12],labels = ['alone','small','big'],right=False)","241e98f3":"train['title'] = train['job'].copy()\ntrain['title'] = train['title'].apply(lambda x: x.lower())","54db75db":"# key: old, value:new\ntitle_map={ 'capt':'officer',\n           'col':'officer'\n           ,'major':'officer'\n           ,'dr':'officer'\n           ,'rev':'officer'\n           ,'sir':'officer',\n           'jonkheer': 'royalty',\n          'the countess':'royalty',\n          'dona': 'royalty',\n          'lady': 'royalty',\n          'don': 'royalty',\n          'mr': 'mr',\n          'mrs':'mrs',\n          'ms':'mrs',\n          'mme':'mrs',\n          'miss':'miss',\n          'mlle':'miss',\n          'master':'master'}","a2895de0":"train['title'] = train['title'].map(title_map)","d85596af":"sns.boxplot(data=train,x='title',y='age')","53152190":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 400)\ntmp = train[['age','title']]","9ee12d31":"from sklearn.preprocessing import OneHotEncoder\ndef ohe_trans(data,col):\n    ohe=OneHotEncoder()\n    x= ohe.fit_transform(data[col].values.reshape(-1,1)).toarray()\n    tp = []\n    for i in range(data[col].unique().size):\n        tp.append(col[0]+str(i))\n    ohe_df = pd.DataFrame(x,columns = tp)\n    return ohe_df\ntmp = pd.concat([tmp,ohe_trans(tmp,'title')],axis=1)\ntmp.drop('title',axis=1,inplace=True)","9c26a734":"X = tmp[tmp['age'].notnull()]\ny = tmp[tmp['age'].isnull()]\ntrain_x = X.loc[:,X.columns.difference(['age'])]\ntrain_y = X['age']\ntest_x = y.loc[:,y.columns.difference(['age'])]\ntest_y = y['age']","f61cd129":"rf.fit(train_x,train_y)","fcd4ec36":"pred = rf.predict(test_x)\ntrain['age'][y.index] = pred","e978948a":"def alone(x):\n    if x == 'alone':\n        return 1\n    else:\n        return 0\ntrain['isalone'] = train['family_size'].apply(lambda x: alone(x))","0e026d23":"train['age'] = pd.cut(train['age'],[0,20,40,60,100],labels = [0,1,2,3],right=False)\ntrain['cabin'] = train['cabin'].apply(lambda x: 0 if type(x) == float else 1)","2a3c251e":"size_map={'alone':0,'small':1,'big':2}\ntrain['family_size'] = train['family_size'].map(size_map)","9b6f2c5d":"train.drop(['sibsp','parch','ticket','name','job','passengerid'],axis=1,inplace=True)","dc9e701a":"test.head()","295852a4":"passengerid = test['PassengerId']","54a2b28e":"test.isnull().sum() \/ test.shape[0] * 100","5845e759":"test.columns = test.columns.str.lower()","9ee65a73":"# Add to family_size column\ntest['family_size'] = test['sibsp'] + test['parch'] + 1\npd.cut(test['family_size'],[0,2,4,12],labels = ['alone','small','big'],right=False)\n# Add to title column\ntest['title'] = test['name'].apply(lambda x: x.split(',')[1].split('.')[0].lstrip())\ntest['title'] = test['title'].apply(lambda x: x.lower())\ntest['title'] = test['title'].map(title_map)\n# Add to isalone colum\ntest['isalone'] = test['family_size'].apply(lambda x: alone(x))\n# family_size column tunning\ntest['family_size'].map(size_map)\n# embarked column tunning\ntest['cabin'] = test['cabin'].apply(lambda x: 0 if type(x) == float else 1)\n# Age column missing value processing\ntmp = test[['age','title']]\ntmp = pd.concat([tmp,ohe_trans(tmp,'title')],axis=1)\ntmp.drop('title',axis=1,inplace=True)\nX = tmp[tmp['age'].notnull()]\ny = tmp[tmp['age'].isnull()]\ntrain_x = X.loc[:,X.columns.difference(['age'])]\ntrain_y = X['age']\ntest_x = y.loc[:,y.columns.difference(['age'])]\ntest_y = y['age']\nrf.fit(train_x,train_y)\npred = rf.predict(test_x)\ntest['age'][y.index]=pred\ntest['age'] = pd.cut(test['age'],[0,20,40,60,100],labels = [0,1,2,3],right=False)","825eb8b4":"tmp=[]\nfor i in test['fare']:\n    if i == np.nan:\n        tmp.append(i)\n    else:\n        tmp.append(np.round(i))\ntest['fare'] = tmp","26417dfb":"dataset=pd.concat([train,test],join='inner')\ndataset.head()","fbcfa15b":"# fare column missing value processing\ntmp = dataset[['pclass','fare']]\nX=tmp[tmp['fare'].notnull()]\ny=tmp[tmp['fare'].isnull()]\ntrain_x = X.loc[:,X.columns.difference(['fare'])]\ntrain_y = X['fare']\ntest_x = y.loc[:,y.columns.difference(['fare'])]\ntest_y = y['fare']\nrf.fit(train_x,train_y)\npred = rf.predict(test_x)\ntest['fare'].fillna(pred[0],inplace=True)","efc7f8ab":"test.drop(['sibsp','parch','ticket','name','passengerid'],axis=1,inplace=True)","8081f1bf":"test.head()","24054c67":"# 1. LabelEncoding\ntrain2 = train.copy()\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nencoding = le.fit_transform(train2['title'])\ntrain2['title'] = encoding\nencoding2 = le.fit_transform(train2['embarked'])\ntrain2['embarked'] = encoding2\nencoding3 = le.fit_transform(train2['sex'])\ntrain2['sex'] = encoding3\ncategory_features = ['sex','title','embarked','age','family_size']\nfor i in category_features:\n    train2[i] = train2[i].astype(int)","abf8c4a1":"test2=test.copy()\nencoding=le.fit_transform(test2['title'])\ntest2['title']=encoding\nencoding2=le.fit_transform(test2['embarked'])\ntest2['embarked'] = encoding2\nencoding3 = le.fit_transform(test['sex'])\ntest2['sex'] = encoding3\ncategory_features = ['sex','embarked','title','age','family_size']\nfor i in category_features:\n    test2[i] = test2[i].astype(int)","330fe8b7":"train_x=train2.drop('survived',axis=1)\ntrain_y=train2['survived']\ntest_x=test2","23fda59f":"# model\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n# parameter tunning\nfrom sklearn.model_selection import GridSearchCV","8aeed35a":"rfc = RandomForestClassifier()\nada = AdaBoostClassifier()\net = ExtraTreesClassifier()\ngra = GradientBoostingClassifier()\nxgb = XGBClassifier()\nsvm = SVC()","a28209aa":"rfc_parm={\"max_depth\":[None,5,10],'n_estimators':[200],'max_features':[None,'sqrt','log2']}\nada_parm = {'n_estimators':[200],'random_state':[42,56],'learning_rate':[4.75,5.75]}\net_parm = {'n_jobs': [-1],'n_estimators':[200],\"max_depth\":[None,5,10],\n          'criterion':['gini','entropy'],'max_features':[None,'sqrt','log2']}\ngra_parm = {'n_estimators':[200],'max_depth':[None,5,10]}\nxgb_parm = {'max_depth':[3,5,10],'gamma':[0,0.5,1],'max_features':['auto','log','log2'],'eta':[0.5,0.1,0.3]}\nsvm_parm = {'gamma':['scale','auto'],'degree':[5,10],'kernel':['linear','rbf'],'C':[0.025,0.5]}\n","153737ad":"rfc_cv=GridSearchCV(rf,rfc_parm) \nada_cv=GridSearchCV(ada,ada_parm) \net_cv=GridSearchCV(et,et_parm) \ngra_cv=GridSearchCV(gra,gra_parm) \nxgb_cv=GridSearchCV(xgb,xgb_parm) \nsvm_cv=GridSearchCV(svm,svm_parm) ","26c77d14":"test_y= submission['Survived'].astype(int)","3e60b96a":"model = [rfc_cv,ada_cv,et_cv,gra_cv,xgb_cv,svm_cv,rfc,ada,et,gra,xgb,svm]\nname = ['rfc_cv','ada_cv','et_cv','gra_cv','xgb_cv','svm_cv','rfc','ada','et','gra','xgb','svm']\nscore_table = pd.DataFrame(columns=['Model','score'])\nindex=0\nfor i,k in zip(model,name):\n    i.fit(train_x,train_y)\n    score_table.loc[index,'Model'] = k\n    score_table.loc[index,'score'] = i.score(test_x,test_y)\n    index+=1\nscore_table.sort_values(by='score',ascending=True)","7020d908":"sns.barplot(data=score_table.sort_values(by='score'),x='Model',y='score')\nplt.xticks(rotation=30,ha='right')\nplt.title('Model accuracy score')","d38538c1":"train3 = train.copy()\ntest3 = test.copy()","be47e95d":"ohe_sex = ohe_trans(train3,'sex')\nohe_cabin = ohe_trans(train3,'cabin')\nohe_embarked = ohe_trans(train3,'embarked')\nohe_title = ohe_trans(train3,'title')\nohe_isalone = ohe_trans(train3,'isalone')\ntrain3 = pd.concat([train3,ohe_sex,ohe_cabin,ohe_embarked,ohe_title,ohe_isalone],axis=1)","9037e544":"train3.drop(['sex','cabin','embarked','title','isalone'],axis=1,inplace=True)","060ecb90":"ohe_sex = ohe_trans(test3,'sex')\nohe_cabin = ohe_trans(test3,'cabin')\nohe_embarked = ohe_trans(test3,'embarked')\nohe_title = ohe_trans(test3,'title')\nohe_isalone = ohe_trans(test3,'isalone')\ntest3 = pd.concat([test3,ohe_sex,ohe_cabin,ohe_embarked,ohe_title,ohe_isalone],axis=1)","09991421":"test3.drop(['sex','cabin','embarked','title','isalone'],axis=1,inplace=True)","7bdeb91e":"print(train3.columns)\nprint(test3.columns)\n# Since the number of columns of two data is different, add one column.\nfor i in range(test3.shape[0]):\n    test3.loc[i,'i1'] = 0","fd29f18f":"train_x_ohe=train3.drop('survived',axis=1)\ntrain_y_ohe=train3['survived']\ntest_x_ohe=test3\nmodel = [rfc_cv,ada_cv,et_cv,gra_cv,xgb_cv,svm_cv,rfc,ada,et,gra,xgb,svm]\nname = ['rfc_cv','ada_cv','et_cv','gra_cv','xgb_cv','svm_cv','rfc','ada','et','gra','xgb','svm']\nscore_table2 = pd.DataFrame(columns=['Model','score'])\nindex=0\nfor i,k in zip(model,name):\n    i.fit(train_x,train_y)\n    score_table2.loc[index,'Model'] = k\n    score_table2.loc[index,'score'] = i.score(test_x,test_y)\n    index+=1\nscore_table2.sort_values(by='score',ascending=True)","71be0a7f":"sns.barplot(data=score_table2.sort_values(by='score'),x='Model',y='score')\nplt.xticks(rotation=30,ha='right')\nplt.title('Model accuracy score')","da753f3f":"fig,axes = plt.subplots(ncols=2,figsize=(12,8))\nsns.barplot(data=score_table.sort_values(by='score'),x='Model',y='score',ax=axes[0])\naxes[0].set(title = 'LabelEncoding Model score')\naxes[0].tick_params(labelrotation=30)\nsns.barplot(data=score_table2.sort_values(by='score'),x='Model',y='score',ax=axes[1])\naxes[1].set(title = 'OneHotEncoding Model score')\naxes[1].tick_params(labelrotation=30)","56c07a29":"print('''LabelEncoding mean_score : {}\nOneHotEncoding mean_score : {}'''.format(np.mean(score_table['score']),np.mean(score_table2['score'])))","d9516b21":"from sklearn.model_selection import StratifiedKFold,cross_val_score","d8c018c0":"kfold = StratifiedKFold(n_splits=10)","04c7f221":"general_model = [rfc,ada,et,gra,xgb,svm]\ngeneral_name = ['rfc','ada','et','gra','xgb','svm']\nge_result = []\nge_means = []\ndf=pd.DataFrame(columns = ['model','score'])\n\nfor i in general_model:\n    ge_result.append(cross_val_score(i,train_x,y=train_y,scoring='accuracy',cv=kfold,n_jobs=4))\nfor j in ge_result: \n    ge_means.append(np.mean(j))\nidx = 0\nfor i in range(len(ge_means)): # before parameter tunning\n    df.loc[idx,'score'] = ge_means[i]\n    df.loc[idx,'model'] = general_name[i]\n    idx += 1\n\ncv_model = [rfc_cv,ada_cv,et_cv,gra_cv,xgb_cv,svm_cv]\ncv_name = ['rfc_cv','ada_cv','et_cv','gra_cv','xgb_cv','svm_cv']\n\nfor i in range(len(cv_model)): # after parameter tunning\n    cv_model[i].fit(train_x,train_y)\n    df.loc[idx,'model'] = cv_name[i]\n    df.loc[idx,'score'] = cv_model[i].best_score_\n    idx += 1\nsns.barplot(data=df.sort_values(by='score'), x='model',y='score')\nplt.xticks(rotation=30,ha='right')\nplt.title('Cross validation scores')","72efa6e4":"et_cv_best = et_cv.best_estimator_\ngra_cv_best = gra_cv.best_estimator_\nxgb_cv_best = xgb_cv.best_estimator_\n","ede40622":"# extract more than 0.8\ndf = df.sort_values(by='score')\ndf[df['score']>=0.80].model","aa3ecf6a":"# feature_importances\n# best_estimator_.feature_importances_ <- grid_search\nfinal_model = [rfc,et,ada,xgb,gra,gra_cv_best,et_cv_best,xgb_cv_best]\nfinal_model_name = ['rfc','et','ada','xgb','gra','gra_cv','et_cv','xgb_cv']\nfig,(ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8) = plt.subplots(8,1,figsize=(15,20))\nfor i,j,k in zip(final_model,final_model_name,[ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8]):\n    feat_importance=pd.Series(i.feature_importances_,index=train_x.columns)\n    feat_importance.nlargest(5).plot(kind='barh',color=['r','c','m','y','g'], ax=k)\n    k.set(title='{} feature importances'.format(j))\n    plt.tight_layout()","99393cc3":"# ('xgb',xgb.get_params),('gra',gra.get_params)\n# ('rfc',rfc.estimators_),('et',et.estimators_),('ada',ada.estimators_)","77629bbe":"from sklearn.ensemble import VotingClassifier\nvotingC = VotingClassifier(estimators=[('et_cv', et_cv_best),('xgb_cv',xgb_cv_best),('gbc_cv',gra_cv_best)], voting='soft', n_jobs=4)\nvotingC.fit(train_x,train_y)\ntest_le = pd.Series(votingC.predict(test_x), name=\"Survived\")","252a67d5":"passengerid = submission['PassengerId']\nresults = pd.concat([passengerid,test_le],axis=1)\nresults.to_csv('label_titanic.csv',index=False)","bb9d560f":"#### Reset Category\n- officer : capt,col,major,dr,rev,sir\n- royalty : jonkheer, countess, dona , lady, don\n- mr : mr\n- mrs : mme, ms ,mrs\n- miss : miss, mlle\n- master : master","0de15d06":"Through this box plot, we can know that it is related to title and age.\nSo, I will use RandomForestRegressor to predict missing values.","3b52c64f":" - pclass 1 has a high probability of survived\n - male has a high probability of survived","6ff7aa5e":"I decided to submit it as SVM_CV Model but the score was low.\n\nSo I decided to enhance model tuning.\n\nAnd I saw a lot of different scores and thought that I should write different evaluation indicators.\n\nI will use cross_val_score(accuracy)","2ec90dbe":"# test dataset features engineering","ee066b98":"# features engineering","f42ed5a7":"# Submit my output","95aa8a37":"# EDA","4f848eb2":"Wouldn't it be better to live alone? So I will make column called alone.","b47b9859":"# Modeling(OneHotEncoding)","06fbb98e":"# Modeling(LabelEncoding)"}}