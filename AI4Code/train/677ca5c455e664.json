{"cell_type":{"3a261853":"code","ab63db25":"code","325b52f1":"code","24998501":"code","3fd04ca5":"code","9cf5577e":"code","4f2914c4":"code","c95aa21f":"code","d0326834":"code","4b71993b":"code","0fe35798":"code","3d63c7d9":"code","b3622c64":"code","1e340727":"code","0dfeb6f2":"code","ad3baba8":"code","3374ade6":"code","804e9639":"code","339b3d06":"code","277a29e7":"code","bbea09d4":"code","2be96e83":"code","8e329949":"code","bfd79ccb":"code","38412fe5":"code","b34c79dd":"code","264832ce":"code","ab1d0fcc":"code","60fb13c6":"code","a3fd6e15":"code","6252277a":"code","e0eec8b9":"code","3002723f":"code","38d6b1b5":"code","119a4755":"code","82ee1749":"code","6645c63f":"code","4ec5353b":"code","0f3cf477":"code","4d78f92e":"code","cd0b4b60":"code","78ff32a3":"code","b7d00e6c":"markdown","b0a8fe84":"markdown","822f9e0f":"markdown","e54e43b4":"markdown","7713dee4":"markdown","9ea22438":"markdown","8044545a":"markdown","44c0dbe8":"markdown","7d6da04b":"markdown","8dbb4bac":"markdown","65a13bfc":"markdown","b9e8292b":"markdown","542a2479":"markdown","3bab7eac":"markdown","34422e69":"markdown","11c8faed":"markdown"},"source":{"3a261853":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport gc, datetime, random\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom category_encoders.target_encoder import TargetEncoder\nfrom scipy.stats import norm, skew #for some statistics, if needed\nfrom math import sqrt\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nSEED = 42\nseed_everything(SEED)\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","ab63db25":"def resumetable(df):\n    #print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","325b52f1":"train = pd.read_csv('\/kaggle\/input\/ml-challenge-tr-is-bankasi\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/ml-challenge-tr-is-bankasi\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/ml-challenge-tr-is-bankasi\/sampleSubmission.csv')","24998501":"print(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\nprint(\"Submission shape: \", sample_submission.shape)","3fd04ca5":"summary = resumetable(train)\nsummary","9cf5577e":"musteri_harcama = train.groupby(['CUSTOMER'])['ISLEM_TUTARI'].sum().reset_index()\nmusteri_harcama.head()","4f2914c4":"def davranis_ekle(df):\n    df.loc[df['ISLEM_TUTARI'] <= 5000, 'DAVRANIS'] = 'CAT_1'\n    df.loc[(df['ISLEM_TUTARI'] > 5000) & (df['ISLEM_TUTARI'] <= 100000), 'DAVRANIS'] = 'CAT_2'\n    df.loc[(df['ISLEM_TUTARI'] > 100000) & (df['ISLEM_TUTARI'] <= 200000), 'DAVRANIS'] = 'CAT_3'\n    df.loc[(df['ISLEM_TUTARI'] > 200000) & (df['ISLEM_TUTARI'] <= 300000), 'DAVRANIS'] = 'CAT_4'\n    df.loc[(df['ISLEM_TUTARI'] > 300000) & (df['ISLEM_TUTARI'] <= 400000), 'DAVRANIS'] = 'CAT_5'\n    df.loc[(df['ISLEM_TUTARI'] > 400000) & (df['ISLEM_TUTARI'] <= 500000), 'DAVRANIS'] = 'CAT_6'\n    df.loc[(df['ISLEM_TUTARI'] > 500000) & (df['ISLEM_TUTARI'] <= 600000), 'DAVRANIS'] = 'CAT_7'\n    df.loc[(df['ISLEM_TUTARI'] > 600000) & (df['ISLEM_TUTARI'] <= 700000), 'DAVRANIS'] = 'CAT_8'\n    df.loc[(df['ISLEM_TUTARI'] > 700000) & (df['ISLEM_TUTARI'] <= 800000), 'DAVRANIS'] = 'CAT_9'\n    \n    df.loc[(df['ISLEM_TUTARI'] > 800000) & (df['ISLEM_TUTARI'] <= 900000), 'DAVRANIS'] = 'CAT_10'\n    df.loc[(df['ISLEM_TUTARI'] > 900000) & (df['ISLEM_TUTARI'] <= 1000000), 'DAVRANIS'] = 'CAT_11'\n    \n    df.loc[df['ISLEM_TUTARI'] > 1000000, 'DAVRANIS'] = 'CAT_12'\n\ndavranis_ekle(musteri_harcama)","c95aa21f":"train = pd.merge(train, musteri_harcama[['CUSTOMER','DAVRANIS']], on='CUSTOMER')\ntest = pd.merge(test, musteri_harcama[['CUSTOMER','DAVRANIS']], on='CUSTOMER')","d0326834":"customer_sektor_islem_turu_mean = train.groupby(['CUSTOMER','SEKTOR','ISLEM_TURU']).mean()\ncustomer_sektor_islem_turu_mean['TUTAR_PER_ADET_SCT'] = customer_sektor_islem_turu_mean['ISLEM_TUTARI']\/customer_sektor_islem_turu_mean['ISLEM_ADEDI']\ncustomer_sektor_islem_turu_mean.reset_index(level=customer_sektor_islem_turu_mean.index.names, inplace=True)\ncustomer_sektor_islem_turu_mean = customer_sektor_islem_turu_mean.drop(columns=['YIL_AY','Record_Count','ISLEM_TUTARI','ISLEM_ADEDI'])","4b71993b":"train = pd.merge(train, customer_sektor_islem_turu_mean, how='left', on=['CUSTOMER','SEKTOR','ISLEM_TURU'])\ntest = pd.merge(test, customer_sektor_islem_turu_mean, how='left', on=['CUSTOMER','SEKTOR','ISLEM_TURU'])","0fe35798":"customer_mean = train.groupby('CUSTOMER').mean()\ncustomer_mean=customer_mean[['ISLEM_ADEDI','ISLEM_TUTARI']]\ncustomer_mean=customer_mean.rename(columns={'ISLEM_ADEDI':'ADET_CUSTOMER','ISLEM_TUTARI':'TUTAR_CUSTOMER'})","3d63c7d9":"train = pd.merge(train,customer_mean, how='inner',on='CUSTOMER')\ntest = pd.merge(test,customer_mean, how='inner',on='CUSTOMER')","b3622c64":"test[\"TUTAR_PER_ADET_SCT\"] = test[\"TUTAR_PER_ADET_SCT\"].fillna(test['TUTAR_PER_ADET_SCT'].mean())","1e340727":"#test.isnull().sum().max()","0dfeb6f2":"sektor = train.groupby(['SEKTOR'])['ISLEM_TUTARI'].sum().reset_index().sort_values(by='ISLEM_TUTARI', ascending=False)","ad3baba8":"f, axe = plt.subplots(1,1,figsize=(12,12))\nsns.barplot(x = 'ISLEM_TUTARI', y = 'SEKTOR', data = sektor, ax = axe)\naxe.set_xlabel('Toplam \u0130\u015flem Tutar\u0131', fontsize=14)\naxe.set_ylabel('Sekt\u00f6r', fontsize=14)\naxe.set_xticklabels(axe.get_xticklabels(), rotation=90)\nplt.show()","3374ade6":"def sektor_davranis(df):\n    df.loc[df['SEKTOR'].str.contains('MARKET \/ ALISVERIS MERKEZLERI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_1'\n    \n    df.loc[df['SEKTOR'].str.contains('BENZIN VE YAKIT ISTASYONLARI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_2'\n    df.loc[df['SEKTOR'].str.contains('GIYIM \/ AKSESUAR', na=False), 'SEKTOR_DAVRANIS'] = 'SD_2'\n    df.loc[df['SEKTOR'].str.contains('CESITLI GIDA', na=False), 'SEKTOR_DAVRANIS'] = 'SD_2'\n    df.loc[df['SEKTOR'].str.contains('ELEKTRIK-ELEKTRONIK ESYA \/ BILGISAYAR', na=False), 'SEKTOR_DAVRANIS'] = 'SD_2'\n    \n    df.loc[df['SEKTOR'].str.contains('RESTORAN \/ CATERING', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    df.loc[df['SEKTOR'].str.contains('TURIZM \/ KONAKLAMA', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    df.loc[df['SEKTOR'].str.contains('ARAC BAKIM \/ SERVIS', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    df.loc[df['SEKTOR'].str.contains('MOBILYA \/ DEKORASYON', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    df.loc[df['SEKTOR'].str.contains('HIZMET SEKTORLERI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    df.loc[df['SEKTOR'].str.contains('BIREYSEL EMEKLILIK', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    df.loc[df['SEKTOR'].str.contains('SAGLIK URUNLERI SATISI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_3'\n    \n    df.loc[df['SEKTOR'].str.contains('HAVAYOLLARI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('KITAP-DERGI \/ KIRTASIYE \/ OFIS MALZEMELERI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('KOZMETIK \/ GUZELLIK', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('MUTEAHHIT ISLERI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('KUYUMCULAR', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('DIGER', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('TASIMACILIK', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('DOGRUDAN PAZARLAMA', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('KULUP \/ DERNEK \/ SOSYAL HIZMETLER', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('EGLENCE \/ SPOR \/ HOBI', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n    df.loc[df['SEKTOR'].str.contains('DIJITAL URUNLER', na=False), 'SEKTOR_DAVRANIS'] = 'SD_4'\n\nsektor_davranis(train)\nsektor_davranis(test)","804e9639":"def yil_ekle(df):\n    df.loc[df['YIL_AY'].str.contains('2017', na=False), 'Yil'] = '2017'\n    df.loc[df['YIL_AY'].str.contains('2018', na=False), 'Yil'] = '2018'\n    df.loc[df['YIL_AY'].str.contains('2019', na=False), 'Yil'] = '2019'\n    \ndef ay_ekle(df):\n    df.loc[df['YIL_AY'].str.contains('201801', na=False), 'Ay'] = '1'\n    df.loc[df['YIL_AY'].str.contains('201901', na=False), 'Ay'] = '1'\n    df.loc[df['YIL_AY'].str.contains('02', na=False), 'Ay'] = '2'\n    df.loc[df['YIL_AY'].str.contains('03', na=False), 'Ay'] = '3'\n    df.loc[df['YIL_AY'].str.contains('04', na=False), 'Ay'] = '4'\n    df.loc[df['YIL_AY'].str.contains('05', na=False), 'Ay'] = '5'\n    df.loc[df['YIL_AY'].str.contains('06', na=False), 'Ay'] = '6'\n    df.loc[df['YIL_AY'].str.contains('07', na=False), 'Ay'] = '7'\n    df.loc[df['YIL_AY'].str.contains('08', na=False), 'Ay'] = '8'\n    df.loc[df['YIL_AY'].str.contains('09', na=False), 'Ay'] = '9'\n    df.loc[df['YIL_AY'].str.contains('10', na=False), 'Ay'] = '10'\n    df.loc[df['YIL_AY'].str.contains('11', na=False), 'Ay'] = '11'\n    df.loc[df['YIL_AY'].str.contains('12', na=False), 'Ay'] = '12'","339b3d06":"train['YIL_AY'] = train['YIL_AY'].astype(str)\ntest['YIL_AY'] = test['YIL_AY'].astype(str)\n\nyil_ekle(train)\nyil_ekle(test)\nay_ekle(train)\nay_ekle(test)\n\ntrain['Yil'] = train['Yil'].astype(int)\ntest['Yil'] = test['Yil'].astype(int)\ntrain['Ay'] = train['Ay'].astype(int)\ntest['Ay'] = test['Ay'].astype(int)","277a29e7":"def ceyrek_ekle(df):\n    df.loc[df['Ay'] == 1, 'ceyrek'] = 'Q1'\n    df.loc[df['Ay'] == 2, 'ceyrek'] = 'Q1'\n    df.loc[df['Ay'] == 3, 'ceyrek'] = 'Q1'\n    \n    df.loc[df['Ay'] == 4, 'ceyrek'] = 'Q2'\n    df.loc[df['Ay'] == 5, 'ceyrek'] = 'Q2'\n    df.loc[df['Ay'] == 6, 'ceyrek'] = 'Q2'\n    \n    df.loc[df['Ay'] == 7, 'ceyrek'] = 'Q3'\n    df.loc[df['Ay'] == 8, 'ceyrek'] = 'Q3'\n    df.loc[df['Ay'] == 9, 'ceyrek'] = 'Q3'\n    \n    df.loc[df['Ay'] == 10, 'ceyrek'] = 'Q4'\n    df.loc[df['Ay'] == 11, 'ceyrek'] = 'Q4'\n    df.loc[df['Ay'] == 12, 'ceyrek'] = 'Q4'\n\nceyrek_ekle(train)\nceyrek_ekle(test)","bbea09d4":"# Concatenating train and test data\ntest['ISLEM_TUTARI'] = 'test'\ndf = pd.concat([train, test], axis=0, sort=False)\nprint(\"Data shape:\", df.shape)","2be96e83":"def dolar_alis(df):\n    df.loc[df['YIL_AY'] == \"201711\", 'DOLAR_ALIS'] = 3.88\n    df.loc[df['YIL_AY'] == \"201712\", 'DOLAR_ALIS'] = 3.85\n    df.loc[df['YIL_AY'] == \"201801\", 'DOLAR_ALIS'] = 3.77\n    df.loc[df['YIL_AY'] == \"201802\", 'DOLAR_ALIS'] = 3.78\n    df.loc[df['YIL_AY'] == \"201803\", 'DOLAR_ALIS'] = 3.88\n    df.loc[df['YIL_AY'] == \"201804\", 'DOLAR_ALIS'] = 4.05\n    df.loc[df['YIL_AY'] == \"201805\", 'DOLAR_ALIS'] = 4.41\n    df.loc[df['YIL_AY'] == \"201806\", 'DOLAR_ALIS'] = 4.63\n    df.loc[df['YIL_AY'] == \"201807\", 'DOLAR_ALIS'] = 4.75\n    df.loc[df['YIL_AY'] == \"201808\", 'DOLAR_ALIS'] = 5.73\n    df.loc[df['YIL_AY'] == \"201809\", 'DOLAR_ALIS'] = 6.37\n    df.loc[df['YIL_AY'] == \"201810\", 'DOLAR_ALIS'] = 5.86\n    df.loc[df['YIL_AY'] == \"201811\", 'DOLAR_ALIS'] = 5.37\n    df.loc[df['YIL_AY'] == \"201812\", 'DOLAR_ALIS'] = 5.31\n    df.loc[df['YIL_AY'] == \"201901\", 'DOLAR_ALIS'] = 5.37\n    df.loc[df['YIL_AY'] == \"201902\", 'DOLAR_ALIS'] = 5.26\n\ndolar_alis(df)","8e329949":"dummy_cols = ['DAVRANIS', 'SEKTOR','SEKTOR_DAVRANIS','ceyrek','YIL_AY']","bfd79ccb":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=[dummy_cols[0]],\\\n                          prefix=['DAVRANIS'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","38412fe5":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=[dummy_cols[1]],\\\n                          prefix=['SEKTOR'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","b34c79dd":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=[dummy_cols[2]],\\\n                          prefix=['SEKTOR_DAVRANIS'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","264832ce":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=[dummy_cols[3]],\\\n                          prefix=['CEYREK'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","ab1d0fcc":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=[dummy_cols[4]],\\\n                          prefix=['YIL_AY'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","60fb13c6":"bin_dict = {'PESIN': 1, 'TAKSITLI': 0}\ndf['ISLEM_TURU'] = df['ISLEM_TURU'].map(bin_dict)","a3fd6e15":"X=df[df.columns.difference([\"ISLEM_TUTARI\",\"ID\",\"Record_Count\"])]\nX.head(3)","6252277a":"num_train=len(train)\n\nX_train = X[:num_train]\nX_test = X[num_train:]\ny_train = train[\"ISLEM_TUTARI\"].values\n\n#X_train = X_train.astype(float)\n#y_train = y_train.astype(float)","e0eec8b9":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","3002723f":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'rmse'},\n    'num_leaves': 64,\n    'max_depth': 63,\n    'learning_rate': 0.009,\n    'min_data_in_leaf': 2,\n    'bagging_freq': 1,\n}","38d6b1b5":"%%time\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\ncolumns = X_train.columns\nsplits = folds.split(X_train, y_train)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n\nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_tr, X_val = pd.DataFrame(X_train).loc[train_index], pd.DataFrame(X_train).loc[valid_index]\n    y_tr, y_val = pd.DataFrame(y_train).loc[train_index], pd.DataFrame(y_train).loc[valid_index]\n    \n    dtrain = lgb.Dataset(X_tr, label=y_tr)\n    dvalid = lgb.Dataset(X_val, label=y_val)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=100, early_stopping_rounds=100)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_val)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | RMSE: {sqrt(mean_squared_error(y_val, y_pred_valid))}\")\n    \n    score += sqrt(mean_squared_error(y_val.astype(float), y_pred_valid.astype(float))) \/ NFOLDS\n    y_preds += clf.predict(X_test) \/ NFOLDS\n    \n    del X_tr, X_val, y_tr, y_val\n    gc.collect()\n    \nprint(f\"\\nMean RMSE = {score}\")\nprint(f\"Out of folds RMSE = {sqrt(mean_squared_error(y_train, y_oof))}\")","119a4755":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","82ee1749":"y_pred_train = clf.predict(X_train)\nprint(sqrt(mean_squared_error(y_train, y_pred_train)))","6645c63f":"y_preds = clf.predict(X_test.astype(float))","4ec5353b":"y_preds = np.clip(y_preds, train['ISLEM_TUTARI'].min(), train['ISLEM_TUTARI'].max())","0f3cf477":"sample_submission['Predicted'] = y_preds","4d78f92e":"sample_submission.head()","cd0b4b60":"sample_submission.to_csv('submission.csv', index=False)","78ff32a3":"from IPython.display import FileLink\nFileLink(r'submission.csv')","b7d00e6c":"### <span id=\"15\"><\/span> ** Submission **","b0a8fe84":"### <span id=\"9\"><\/span> ** Y\u0131l\u0131n \u00c7eyrekleri **\n\n12 ayl\u0131k bir y\u0131l\u0131 {Q1, Q2, Q2 Q3, Q4} olmak \u00fczere 4 d\u00f6neme b\u00f6ld\u00fcm. Bir y\u0131l\u0131 \u00e7eyreklerine b\u00f6lmek genel kabul g\u00f6rm\u00fc\u015f yakla\u015f\u0131m. Fakat 2 ayl\u0131k veya 3 ayl\u0131k periyotlara da b\u00f6l\u00fcnebilir, modelin performans\u0131n\u0131 art\u0131ran y\u00f6ntem kullan\u0131labilir. Aylara ay\u0131r\u0131p veriyi analiz etti\u011fimizde daha iyi \u00e7\u0131kar\u0131mlar yap\u0131labilir.","822f9e0f":"## <span id=\"1\"><\/span> ** 1. K\u00fct\u00fcphanelerin Tan\u0131mlanmas\u0131 ve Veriyi Okuma **","e54e43b4":"### <span id=\"14\"><\/span> ** Feature Importance **","7713dee4":"### <span id=\"4\"><\/span> ** M\u00fc\u015fteri Harcamalar\u0131 **\n\nM\u00fc\u015fterileri CUSTOMER \u00fczerinden gruplayarak toplam harcalamalar\u0131n\u0131 hesaplad\u0131m. 27669 unique m\u00fc\u015fterinin high cardinality problemine sebep olaca\u011f\u0131n\u0131 d\u00fc\u015f\u00fcnerek, harcamalar\u0131na g\u00f6re m\u00fc\u015fterileri 12 kategoriye ay\u0131rd\u0131m. Daha fazla veya az kategoriye ay\u0131r\u0131p performansa etkisi \u00f6l\u00e7\u00fclebilir.","9ea22438":"## <span id=\"12\"><\/span> ** 4. Model **","8044545a":"### <span id=\"8\"><\/span> ** Ay ve Y\u0131l **\n\nYIL_AY kolonundaki 15 ayl\u0131k veriyi i\u015fleyip, ay ve y\u0131l olarak iki \u00f6znitelik daha ekledim.","44c0dbe8":"### <span id=\"5\"><\/span> ** M\u00fc\u015fterilerin \u0130\u015flem Adedine G\u00f6re Sekt\u00f6r Harcamas\u0131 **\nKaynak: https:\/\/www.kaggle.com\/unrool\/fatihkykc-xgboost-example-notebook","7d6da04b":"## <span id=\"11\"><\/span> ** 3. Encoding **","8dbb4bac":"### <span id=\"10\"><\/span> ** Dolar Al\u0131\u015f Verisi **\n\nHer aya ait dolar al\u0131\u015f verisini ekledim. Fakat dikkat ederseniz bundan sonraki yeni \u00f6znitelikleri eklerken, test verisini de dahil ediyoruz. \u00c7\u00fcnk\u00fc 2019 \u015fubat ay\u0131ndaki verilere de sahibiz.\n\nKaynak: https:\/\/www.tcmb.gov.tr\/wps\/wcm\/connect\/TR\/TCMB+TR\/Main+Menu\/Istatistikler\/Doviz+Kurlari","65a13bfc":"### <span id=\"6\"><\/span> ** M\u00fc\u015fterilerin Ortalama \u0130\u015flem Adedi ve Harcamas\u0131 **\nKaynak: https:\/\/www.kaggle.com\/unrool\/fatihkykc-xgboost-example-notebook","b9e8292b":"### <span id=\"2\"><\/span> ** Veri A\u00e7\u0131klamas\u0131 **\n\n- YIL_AY: harcamalar\u0131n yap\u0131ld\u0131\u011f\u0131 tarih (YIL ve AY format\u0131nda)\n- ISLEM_TURU: harcamalar\u0131n t\u00fcr\u00fc (pe\u015fin ya da taksitli)\n- SEKTOR: harcamalar\u0131n yap\u0131ld\u0131\u011f\u0131 sekt\u00f6r\n- CUSTOMER: her m\u00fc\u015fterinin unique m\u00fc\u015fteri numaras\u0131\n- ISLEM_ADEDI: ilgili tarih, t\u00fcr ve sekt\u00f6rde m\u00fc\u015fterinin yapt\u0131\u011f\u0131 i\u015flem adedi\n- ISLEM_TUTARI: m\u00fc\u015fterinin ilgili tarih, t\u00fcr ve sekt\u00f6rde yapt\u0131\u011f\u0131 i\u015flemlerin toplam tutar\u0131\n","542a2479":"### <span id=\"7\"><\/span> ** Sekt\u00f6r Harcamalar\u0131 **\n\nSekt\u00f6rlere yap\u0131lan toplam harcamalar\u0131 alt kategorilere ay\u0131rd\u0131m. Bunun i\u00e7in her sekt\u00f6r\u00fcn toplam harcamas\u0131n\u0131 g\u00f6rselle\u015ftirerek tahmini bir ayr\u0131m yapt\u0131m. Ayr\u0131ca tek seferde yap\u0131lan veya ayl\u0131k harcamalara g\u00f6re yeni \u00f6znitelik (feature) da olu\u015fturulabilir.","3bab7eac":"## <span id=\"3\"><\/span> ** 2. \u00d6znitelik \u00c7\u0131kar\u0131m\u0131 (Feature Engineering) **","34422e69":"### <span id=\"13\"><\/span> ** LightGBM **","11c8faed":"[**Tolgahan \u00c7epel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n\n<font color=green>\n* 1. [K\u00fct\u00fcphanelerin Tan\u0131mlanmas\u0131 ve Veriyi Okuma](#1)\n    * [Veri A\u00e7\u0131klamas\u0131](#2)\n* 2. [\u00d6znitelik \u00c7\u0131kar\u0131m\u0131 (Feature Engineering)](#3)\n    * [M\u00fc\u015fteri Harcamalar\u0131](#4)\n    * [M\u00fc\u015fterilerin \u0130\u015flem Adedine G\u00f6re Sekt\u00f6r Harcamas\u0131](#5)\n    * [M\u00fc\u015fterilerin Ortalama \u0130\u015flem Adedi ve Harcamas\u0131](#6)\n    * [Sekt\u00f6r Harcamalar\u0131](#7)\n    * [Ay ve Y\u0131l](#8)\n    * [Y\u0131l\u0131n \u00c7eyrekleri](#9)\n    * [Dolar Al\u0131\u015f Verisi](#10)\n* 3. [Encoding](#11)\n* 4. [Model](#12)\n    * [LightGBM](#13)\n    * [Feature Importance](#13)\n    * [Submission](#14)\n<hr\/>"}}