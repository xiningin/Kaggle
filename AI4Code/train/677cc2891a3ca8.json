{"cell_type":{"ec822daa":"code","b6d2d7dd":"code","9ee4bc57":"code","66b080c7":"code","46a1be3a":"code","48db3225":"code","1059b380":"code","639a39d4":"code","4f3ad91e":"code","de7dbbff":"code","eaac643a":"markdown","d21a6f02":"markdown","c39dac43":"markdown","94b0d660":"markdown"},"source":{"ec822daa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6d2d7dd":"import pandas as pd\nimport numpy as np\n\n# preprocessing \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# modeling\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom xgboost import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error","9ee4bc57":"df = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")\ny = df[\"charges\"]\nX = df.drop(columns=\"charges\")\nX.head()","66b080c7":"df.isna().sum()","46a1be3a":"le = LabelEncoder()\nfor col in X.columns:\n    X[col] = le.fit_transform(X[col])\nX.head()","48db3225":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","1059b380":"# ExtraTreesRegressor\nxt = ExtraTreesRegressor(\n    n_estimators = 200,\n    max_depth = 6,\n    random_state = 100,\n)\nxt_scores = cross_val_score(xt, X_train, y_train)\nprint(f\"ExtraTrees train data R^2 = {np.mean(xt_scores):.3f}\")","639a39d4":"# XGBRegressor\nxgb = XGBRegressor(\n    n_estimators = 200,\n    max_depth = 3,\n    reg_alpha = 50,\n    reg_lambda = 100,\n    random_state = 100,\n)\nxgb_scores = cross_val_score(xgb, X_train, y_train)\nprint(f\"xgboost train data R^2 = {np.mean(xgb_scores):.3f}\")","4f3ad91e":"# ExtraTreesRegressor\nxt.fit(X_train, y_train)\ntest_pred = xt.predict(X_test)\nprint(f\"Test data RMSE = {np.sqrt(mean_squared_error(y_test,test_pred)):.1f}\")\nprint(f\"Test data MAE = {mean_absolute_error(y_test,test_pred):.1f}\")\nprint(f\"Test data R^2 = {r2_score(y_test,test_pred):.3f}\")","de7dbbff":"# XGBRegressor\nxgb.fit(X_train, y_train)\ntest_pred = xgb.predict(X_test)\nprint(f\"Test data RMSE = {np.sqrt(mean_squared_error(y_test,test_pred)):.1f}\")\nprint(f\"Test data MAE = {mean_absolute_error(y_test,test_pred):.1f}\")\nprint(f\"Test data R^2 = {r2_score(y_test,test_pred):.3f}\")","eaac643a":"# **Libraries**","d21a6f02":"# **Preprocessing**","c39dac43":"# **Modeling & Cross Validation**","94b0d660":"# **Model Test**"}}