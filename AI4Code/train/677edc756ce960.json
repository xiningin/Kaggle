{"cell_type":{"d1ac8c99":"code","72d1b925":"code","1ecf04da":"code","aa8dfebe":"code","f1c32ace":"code","7ccff64a":"code","98ad6c01":"code","57a1ba4e":"code","6d23c0b8":"code","488cc30e":"code","baadd4a3":"code","93fd650f":"code","09d4dc36":"code","6c71f45b":"code","b7c4b12e":"code","c7b27bd9":"code","049844ef":"code","0c839b2a":"code","a12b7c45":"code","901069ee":"code","3a051e5c":"markdown"},"source":{"d1ac8c99":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/view-class\/view_helper.py\", dst = \"..\/working\/view_helper.py\")\n\n# import all our functions\nimport view_helper as helper\n\n# Any results you write to the current directory are saved as output.","72d1b925":"print(os.listdir(\"..\/input\/dogs-vs-cats-for-pytorch\/cat_dog_data\/Cat_Dog_data\"))","1ecf04da":"data_dir = '..\/input\/dogs-vs-cats-for-pytorch\/cat_dog_data\/Cat_Dog_data'\n\n# TODO: Define transforms for the training data and testing data\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\n\ntest_transforms = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(data_dir + '\/train', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + '\/test', transform=test_transforms)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=64)","aa8dfebe":"# change this to the trainloader or testloader \ndata_iter = iter(trainloader)\n\nimages, labels = next(data_iter)\nfig, axes = plt.subplots(figsize=(10,10), ncols=4)\nfor ii in range(4):\n    ax = axes[ii]\n    helper.imshow(images[ii], ax=ax, normalize=False)","f1c32ace":"# Use GPU if it's available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = models.densenet121(pretrained=True)\n\n# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n    \nmodel.classifier = nn.Sequential(nn.Linear(1024, 256),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.2),\n                                 nn.Linear(256, 2),\n                                 nn.LogSoftmax(dim=1))\n\ncriterion = nn.NLLLoss()\n\n# Only train the classifier parameters, feature parameters are frozen\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n\nmodel.to(device);","7ccff64a":"torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","98ad6c01":"# epochs = 1\n# steps = 0\n# running_loss = 0\n# print_every = 5\n# for epoch in range(epochs):\n#     for inputs, labels in trainloader:\n#         steps += 1\n#         # Move input and label tensors to the default device\n#         inputs, labels = inputs.to(device), labels.to(device)\n        \n#         optimizer.zero_grad()\n        \n#         logps = model.forward(inputs)\n#         loss = criterion(logps, labels)\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n        \n#         if steps % print_every == 0:\n#             test_loss = 0\n#             accuracy = 0\n#             model.eval()\n#             with torch.no_grad():\n#                 for inputs, labels in testloader:\n#                     inputs, labels = inputs.to(device), labels.to(device)\n#                     logps = model.forward(inputs)\n#                     batch_loss = criterion(logps, labels)\n                    \n#                     test_loss += batch_loss.item()\n                    \n#                     # Calculate accuracy\n#                     ps = torch.exp(logps)\n#                     top_p, top_class = ps.topk(1, dim=1)\n#                     equals = top_class == labels.view(*top_class.shape)\n#                     accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n                    \n#             print(f\"Epoch {epoch+1}\/{epochs}.. \"\n#                   f\"Train loss: {running_loss\/print_every:.3f}.. \"\n#                   f\"Test loss: {test_loss\/len(testloader):.3f}.. \"\n#                   f\"Test accuracy: {accuracy\/len(testloader):.3f}\")\n#             running_loss = 0\n#             model.train()","57a1ba4e":"print(\"Our model: \\n\\n\", model, '\\n')\nprint(\"The state dict keys: \\n\\n\", model.state_dict().keys())","6d23c0b8":"#torch.save(model.state_dict(), 'cat_dog_dense121.pth')","488cc30e":"print(os.listdir(\"..\/input\/dogs-vs-cats-for-pytorch\/cat_dog_data\/Cat_Dog_data\"))\nprint(os.listdir(\"..\/input\/cat-dog-dense121-local\"))","baadd4a3":"ls","93fd650f":"state_dict = torch.load('..\/input\/cat-dog-dense121-local\/cat_dog_dense121_local.pth')\nprint(state_dict.keys())","09d4dc36":"model.load_state_dict(state_dict)\nmodel","6c71f45b":"# change this to the trainloader or testloader \ndata_iter = iter(testloader)\n\nimages, labels = next(data_iter)\nfig, axes = plt.subplots(figsize=(10,10), ncols=4)\nfor ii in range(4):\n    ax = axes[ii]\n    helper.imshow(images[ii], ax=ax, normalize=False)","b7c4b12e":"model.to('cpu')\nmodel.eval()\n\ndata_iter = iter(testloader)\n\nimages, labels = next(data_iter)\nfig, axes = plt.subplots(figsize=(10,10), ncols=4)\n\nfor ii in range(4):\n    ax = axes[ii]\n    helper.imshow(images[ii], ax=ax, normalize=False)","c7b27bd9":"with torch.no_grad():\n    output = model.forward(images)\n\nps = torch.exp(output)","049844ef":"random_img = np.random.randint(64, size=1)[0]\nrandom_img","0c839b2a":"# get the probability\nprobability = ps[random_img].data.numpy().squeeze()\nprobability","a12b7c45":"helper.imshow(images[random_img], normalize=False)","901069ee":"ind = np.arange(2)\nlabels = ['Cat', 'Dog',]\nwidth = 0.35\nlocations = ind\n\nclass_probability = plt.barh(ind, probability, width, alpha=.7, label='Cats vs Dogs')\n\nplt.yticks(np.arange(10))\nplt.title('Class Probability')\nplt.yticks(locations, labels)\n\n#legend\nplt.legend()\nplt.ylim(top=3)\nplt.ylim(bottom=-2)\nplt.show();","3a051e5c":"## Test the Network"}}