{"cell_type":{"52739a21":"code","e39f62df":"code","707e1d2e":"code","81740739":"code","fa184ad3":"code","41f83d82":"code","c927fca6":"code","1d38c1f1":"code","508c582b":"code","83030a58":"code","29984a19":"code","8dd8e5d6":"code","c7d61268":"code","6bba8afa":"code","b4f47d6a":"code","0fdbf35b":"code","1fada510":"code","69cdfcbb":"code","50f54426":"code","ff180626":"code","35ae0f62":"code","49c0fd0c":"code","303c17ad":"code","08f37dc3":"code","30550306":"code","8fad2a94":"code","c5a698aa":"code","fcadb57a":"code","03a6584d":"code","02535ff5":"code","08c6df77":"code","d09668aa":"code","f9f1723c":"code","4880ba3e":"code","26abf455":"code","013c996a":"code","975f3feb":"code","eb541bce":"code","2b81ac34":"code","aa638b50":"code","aa863013":"code","f59e682a":"code","78826ebb":"code","05bbd01f":"code","785913f7":"code","8508b10a":"code","2575b77a":"code","4b19547c":"code","21b69f7e":"code","a508b89c":"code","acd5db8f":"code","f88bfa93":"code","7a60d9e3":"code","cc283d89":"code","01bb43c1":"code","436492c7":"code","f0f61d21":"code","deb838f7":"code","4aeca5ab":"code","28e06432":"code","7d8aa264":"code","fb69337d":"code","0c07c28c":"code","99ab1792":"code","03dfb9ee":"markdown","68fbb961":"markdown","aa1b783b":"markdown","60a5f0a8":"markdown","a8ad6b8e":"markdown","80df650f":"markdown","edd19b1a":"markdown","cfbf1fb9":"markdown","59a4a493":"markdown","ffc55854":"markdown","95fbc1ea":"markdown","6917398f":"markdown"},"source":{"52739a21":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os","e39f62df":"data_path = \"..\/input\/housing.csv\"\ndf = pd.read_csv(data_path)","707e1d2e":"df.describe()","81740739":"df.info()","fa184ad3":"# In the given datasets we have 9 continuous variables and one categorical variable. ML algorithms do not work well with categorical data. \n# So, we will convert the categorical data. \ndf.columns","41f83d82":"df.ocean_proximity.value_counts()","c927fca6":"sns.countplot(df.ocean_proximity)","1d38c1f1":"new_val = pd.get_dummies(df.ocean_proximity)","508c582b":"new_val.head(5)","83030a58":"df[new_val.columns] = new_val","29984a19":"df.describe()","8dd8e5d6":"df.columns","c7d61268":"df = df[['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income'\n       , '<1H OCEAN', 'INLAND',\n       'ISLAND', 'NEAR BAY', 'NEAR OCEAN','median_house_value']]","6bba8afa":"df.describe() ","b4f47d6a":"# Now, let's understand the correlation between variable by plotting correlation plot","0fdbf35b":"df.corr()","1fada510":"plt.figure(figsize=(15,12))\nsns.heatmap(df.corr(), annot=True)","69cdfcbb":"df.corr().sort_values(ascending=False, by = 'median_house_value').median_house_value","50f54426":"df.hist(figsize=(15,12))","ff180626":"df.median_house_value.hist()","35ae0f62":"sns.distplot(df.median_house_value)","49c0fd0c":"# We can see that the median house value is mostly falls between 10,0000 to 30,0000 with few exceptions. ","303c17ad":"# We will need to replace all the null values.\ndf.isna().sum() ","08f37dc3":"# So, we have 207 null values. We can drop the rows with null values or we can replace the null values.\n# 207 is too big a number to drop rows\ndf = df.fillna(df.mean())","30550306":"df.isna().sum() ","8fad2a94":"from sklearn import preprocessing\nconvert = preprocessing.StandardScaler() ","c5a698aa":"df.columns ","fcadb57a":"feature = df.drop(['median_house_value'], axis=1)\nlabel = df.median_house_value","03a6584d":"featureT = convert.fit_transform(feature.values)\nlabelT = convert.fit_transform(df.median_house_value.values.reshape(-1,1)).flatten() ","02535ff5":"featureT","08c6df77":"labelT","d09668aa":"from sklearn.model_selection import train_test_split\nfeature_train, feature_test,label_train, label_test = train_test_split(featureT,labelT, test_size=0.2, random_state=19)                                   ","f9f1723c":"from sklearn import linear_model\nfrom sklearn.metrics import r2_score, mean_squared_error\nlinear_reg = linear_model.LinearRegression()\nlinear_reg.fit(feature_train,label_train)\nr2_score(linear_reg.predict(feature_train),label_train)","4880ba3e":"from sklearn.model_selection import cross_val_score","26abf455":"cross_val_score(linear_reg, feature_train,label_train, cv=10) ","013c996a":"reg_score = r2_score(linear_reg.predict(feature_test),label_test) ","975f3feb":"reg_score","eb541bce":"linear_reg.coef_","2b81ac34":"pd.DataFrame(linear_reg.coef_, index=feature.columns, columns=['Coefficient']).sort_values(ascending=False, by = 'Coefficient')","aa638b50":"df.corr().median_house_value.sort_values(ascending=False) ","aa863013":"ransac_reg = linear_model.RANSACRegressor()","f59e682a":"ransac_reg.fit(feature_train,label_train)\nr2_score(ransac_reg.predict(feature_train),label_train)","78826ebb":"ransac_score = r2_score(ransac_reg.predict(feature_test),label_test)","05bbd01f":"ransac_score","785913f7":"# Ransac regrssor is performing way poorly than Linear Regresson","8508b10a":"ridge_reg = linear_model.Ridge(random_state=19) \nridge_reg.fit(feature_train,label_train) ","2575b77a":"r2_score(ridge_reg.predict(feature_train),label_train)","4b19547c":"ridge_score = r2_score(ridge_reg.predict(feature_test),label_test) ","21b69f7e":"ridge_score","a508b89c":"ridge_reg.coef_","acd5db8f":"pd.DataFrame(ridge_reg.coef_, index=feature.columns, columns=['Coefficient']).sort_values(ascending=False, by = 'Coefficient') ","f88bfa93":"from sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(feature_train,label_train)","7a60d9e3":"r2_score(tree_reg.predict(feature_train),label_train)","cc283d89":"# 99% seems like overfitting. Let's cross validate it.\n\ncross_val_score(tree_reg, feature_train, label_train, cv=10)\n","01bb43c1":"tree_score = r2_score(tree_reg.predict(feature_test),label_test) \ntree_score","436492c7":"from sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()","f0f61d21":"forest_reg.fit(feature_train,label_train)","deb838f7":"r2_score(forest_reg.predict(feature_train),label_train)","4aeca5ab":"cross_val_score(forest_reg, feature_train, label_train, cv=10)","28e06432":"# let's see how well the random forest regressor fits well with the test data\nforest_score = r2_score(forest_reg.predict(feature_test),label_test) ","7d8aa264":"forest_score","fb69337d":"# 76% is not a bad score. We can also use GridSearchCV to find the best paramters for random forest regressor","0c07c28c":"data = [reg_score, ransac_score, ridge_score, tree_score, forest_score]\nindex = ['Linear Regression', 'Ransac Regression', 'Ridge Regression', 'Decision Tree Regressor', 'Random Forest Regressor']\npd.DataFrame(data, index=index, columns=['Scores']).sort_values(ascending = False, by=['Scores'])","99ab1792":"# So, the random forest regressor is winner here out of all the ML Algorithm","03dfb9ee":"**Random Forest Regressor**","68fbb961":"****RANSAC Regression****","aa1b783b":"This kernel is divided into 7 parts.  The most important part of  \"house price prediction\" is knowing the data we are working with. Almost 75% of the prediction effort goes into getting familiar with data, data cleaning and making the data ready for maching learning algorithms. So, broadly you can divide the seven substep into 2 major categories.\n\n**Working with data**\n\n* Get to know your data\n* Data Cleaning(if required)\n* Scaling the data into machine learning readbable format\n* Dividing the data into train\/test\n\n**Working with Machine Learning Algorithms**\n\n* Applying machine learning algorithm\n* Testing the effectiveness of the machine learning Algorithm\n* Trying different Algorithms","60a5f0a8":"**Split the data into train and test**","a8ad6b8e":"**Data Scaling**","80df650f":"**Working with categorical data**","edd19b1a":"**Cross Validation Score**","cfbf1fb9":" **Get to know your data**","59a4a493":"**Data Cleaning**","ffc55854":"**ML Model - Linear Regression**","95fbc1ea":"**Decision Tree Regressor**","6917398f":"**Ridge Regressor**"}}