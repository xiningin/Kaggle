{"cell_type":{"10a20f89":"code","8b415a8e":"code","633df76b":"code","d29ce1db":"code","0f14ec9d":"code","9ec9e854":"code","8e1b863f":"code","b263183e":"code","eabc7431":"code","ced246fc":"code","23572654":"code","bdc9bf15":"code","7546159f":"code","3eab73e1":"code","e4b108d9":"code","66578195":"code","90bdbe80":"code","09017eb9":"code","e20f5796":"code","5e6acbb9":"code","d61c675a":"code","efd05688":"code","0a250303":"code","07307c4e":"code","ecd218f5":"code","7a76369a":"code","d0761669":"code","5614ecbd":"code","4dd85444":"code","9b93857b":"code","37e51dcc":"code","0857964e":"code","211b29d7":"code","2e57cb71":"code","c79193c8":"code","1764a172":"code","12fc736e":"code","1d46a92b":"code","a57a9655":"code","049cd7e9":"code","fda6e5b5":"code","e0392293":"code","544b39b1":"code","df4277f7":"code","f215779a":"code","9d29d5f5":"code","6b5865e1":"code","7284cf59":"code","7777aad8":"code","13c9f2f4":"code","5310732e":"code","eae8061e":"code","8217cb61":"code","51b19d89":"code","6bc9e029":"code","33bf0ac3":"code","400dbafa":"code","5838b1bf":"code","fab01cbb":"code","66293db8":"code","978f52d0":"code","840063f7":"code","16ee360d":"code","5fdd225d":"code","226a23d4":"code","0bbd6859":"code","03e0e97b":"code","f283e7d0":"code","a8a66fdf":"code","4ec48860":"code","efb367d8":"code","1b76647b":"code","c9e954ff":"code","7f637c7f":"code","039561a7":"code","5a655b4a":"code","dd49145f":"code","b8d47722":"code","79d05a4b":"code","d75477b9":"code","0cdae261":"code","fccbff74":"code","7f942635":"code","506f7c92":"code","f919cc6c":"code","42ac9417":"code","f60d8374":"code","c8fcb1c0":"code","61a911f1":"code","c198f499":"code","7b60c6d0":"code","737ab6c0":"code","c7140f6b":"code","8d63b917":"code","d791eb64":"code","9940a4f0":"code","7ac60e4c":"code","5a48285d":"code","b63b6f80":"code","6ab028b1":"code","87c970a7":"code","53ccfe09":"code","f1e89272":"code","def69ab7":"code","dad3b820":"code","070497c9":"code","bc724eba":"code","420f61c0":"code","63102539":"code","8b72c9d0":"code","88c3131c":"code","c87c95b4":"code","272b2644":"code","34d788e9":"code","2596d4fd":"code","400f32a8":"code","2a4a7a81":"markdown","9c8f681b":"markdown","20e30f49":"markdown","1229fa82":"markdown","6c54c145":"markdown","c7a8e499":"markdown","6e657f5e":"markdown","4ca44de8":"markdown","ae1e4173":"markdown","7385f702":"markdown","9a7c20e7":"markdown","20b2c20f":"markdown"},"source":{"10a20f89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b415a8e":"import matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n# np.random.seed(20)","633df76b":"from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, classification_report, confusion_matrix, roc_auc_score, plot_roc_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","d29ce1db":"df = pd.read_csv(\"\/kaggle\/input\/employee-attrition\/HR-Employee-Attrition.csv\"\n)","0f14ec9d":"df_copy = df.copy()","9ec9e854":"df.head()","8e1b863f":"df.info()","b263183e":"# From above information, we can get to know that there are no null values in any of the columns.","eabc7431":"df['Age'].value_counts()","ced246fc":"df['Attrition'].value_counts()","23572654":"df['Attrition'] = np.where(df['Attrition']=='Yes', 1,0)","bdc9bf15":"df['Attrition'].value_counts()","7546159f":"df.head()","3eab73e1":"# We can see that data is imbalanced since for 'Yes' its only 237 and '1233' for NO. We will check on this later.","e4b108d9":"df['BusinessTravel'].value_counts()","66578195":"plt.figure(figsize=(7,7))\nax = sns.countplot(x='Attrition', hue='BusinessTravel', data=df)\nplt.title('Distribution of Truck Configurations')\nplt.xlabel('Attrition')\nplt.ylabel('BusinessTravel')\n\nfor p in ax.patches:\n        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+20))","90bdbe80":"# ppl who travel rarely are less likely to attrition","09017eb9":"df['DailyRate'].value_counts()","e20f5796":"plt.figure(figsize=(16,8))\n\nax = sns.barplot(x='JobRole', y='DailyRate', hue='Attrition', data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")","5e6acbb9":"# We see that healthcare representatives earn the most and managers are more likely to be attrited ","d61c675a":"df['Department'].value_counts()","efd05688":"sns.barplot(x='Department', y='DistanceFromHome', data=df)","0a250303":"# The Sales department members travel the most","07307c4e":"sns.barplot(x=\"Education\", y='JobRole', data=df)","ecd218f5":"# Employees as Research Directors have the max number of degress to their name","7a76369a":"df.head()","d0761669":"df['DistanceFromHome'].value_counts()","5614ecbd":"# sns.barplot(x='DistanceFromHome', y='Attrition', data=df)","4dd85444":"df['Education'].value_counts()","9b93857b":"df['EducationField'].value_counts()","37e51dcc":"sns.countplot(x=\"EducationField\", hue='Attrition', data=df)","0857964e":"# The rate of attrition is highest for employees under LifeSciences and least for HR employees","211b29d7":"df['EmployeeCount'].value_counts()","2e57cb71":"# df = df.drop(columns=['EmployeeCount'])","c79193c8":"df.head()","1764a172":"df['Gender'].value_counts()","12fc736e":"# df['Gender'] = np.where(df['Gender']=='Male', 0, 1)","1d46a92b":"df['Gender'].value_counts()","a57a9655":"df['HourlyRate'].value_counts()","049cd7e9":"df.head()","fda6e5b5":"df['MaritalStatus'].value_counts()","e0392293":"df['MonthlyIncome'].value_counts()","544b39b1":"sns.scatterplot(x='StockOptionLevel', y='MonthlyIncome', data=df)","df4277f7":"# Top Stock option is NOT so widely provided for employees with high salary","f215779a":"df['JobInvolvement'].value_counts()","9d29d5f5":"plt.figure(figsize=(20,10))\nax = sns.countplot(hue='Attrition', x='TotalWorkingYears', data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")","6b5865e1":"# Employees with less experience have higher chance of attrition","7284cf59":"df['JobRole'].value_counts()","7777aad8":"sns.countplot(x='YearsSinceLastPromotion', hue='Attrition', data=df)","13c9f2f4":"df['JobSatisfaction'].value_counts()","5310732e":"df['NumCompaniesWorked'].value_counts()","eae8061e":"df.head()","8217cb61":"df['Over18'].value_counts()","51b19d89":"# df = df.drop(columns = ['Over18'])","6bc9e029":"df['OverTime'].value_counts()","33bf0ac3":"# df['OverTime'] = np.where(df['OverTime']=='Yes', 1, 0)","400dbafa":"df['OverTime'].value_counts()","5838b1bf":"df['StandardHours'].value_counts()","fab01cbb":"# df.drop(columns=['StandardHours'], inplace=True)","66293db8":"df['EmployeeNumber'].value_counts()","978f52d0":"# df.drop(columns=['EmployeeNumber'], inplace=True)","840063f7":"df.shape","16ee360d":"df.head()","5fdd225d":"df.corr()","226a23d4":"y = df['Attrition']","0bbd6859":"df.head()","03e0e97b":"x = df.drop(columns=['Attrition', 'EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'])","f283e7d0":"df_numeric_cols = x.select_dtypes(include=np.number)","a8a66fdf":"df_numeric_cols.head()","4ec48860":"df.columns.difference(df_numeric_cols.columns)","efb367d8":"# df_numeric_cols.head()","1b76647b":"from statsmodels.stats.outliers_influence import variance_inflation_factor","c9e954ff":"vif = pd.DataFrame()","7f637c7f":"vif['Features'] = df_numeric_cols.columns","039561a7":"vif['VIF'] = [variance_inflation_factor(df_numeric_cols.values, i) for i in range(len(df_numeric_cols.columns))]","5a655b4a":"vif","dd49145f":"# We will try to remove the features with high variance after running the model with all the features first\n# Then we can compare the performance after removing each feature with high variance","b8d47722":"x['Gender'] = np.where(x['Gender'] == 'Male', 0, 1)","79d05a4b":"df_cat_cols = x.select_dtypes(include='object')","d75477b9":"df_cat_cols.head()","0cdae261":"len(df_cat_cols.columns)","fccbff74":"\ndf_cat_encoded = pd.get_dummies(df_cat_cols)","7f942635":"df_cat_encoded.head()","506f7c92":"x_final = pd.concat([df_numeric_cols, df_cat_encoded], axis=1)","f919cc6c":"x_final.head()","42ac9417":"x_final.shape","f60d8374":"scaler = StandardScaler()","c8fcb1c0":"scaled_x = scaler.fit_transform(x_final)","61a911f1":"scaled_x_df = pd.DataFrame(data=scaled_x, columns=x_final.columns)","c198f499":"scaled_x_df.head()","7b60c6d0":"trainX, testX, trainY, testY = train_test_split(scaled_x_df,y,test_size=0.3, random_state=220, stratify=y)","737ab6c0":"trainX.head()","c7140f6b":"trainY.head()","8d63b917":"def model_fit(model, testX, testY, trainX, trainY, cv=True, cv_folds=10):\n    model.fit(trainX, trainY)\n    predictY = model.predict(testX)\n    if cv:\n        cv_score = cross_val_score(model, trainX, trainY, cv=cv_folds, scoring='f1_macro')\n    \n    print(\"CV report: Mean %.3g| Std %.3g| Min %.3g | Max %.3g\"%((np.mean(cv_score), \n                                                                  np.std(cv_score), \n                                                                  np.min(cv_score), \n                                                                  np.max(cv_score))))\n    print(\"Classification Report:\\n\",classification_report(testY, predictY))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(testY, predictY))\n    return predictY\n    \n    ","d791eb64":"lr = LogisticRegression()\npredictY = model_fit(lr, testX, testY, trainX, trainY)\n# plot_roc_curve(lr, trainX, trainY)","9940a4f0":"# Getting a recall of 1, need to check","7ac60e4c":"auc = roc_auc_score(testY, predictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, predictY)\nplt.plot(fpr, tpr)","5a48285d":"def knn_model(trainX, trainY, testX, testY, k_value=5, weights='distance', metric='manhattan'):\n#     scaling = StandardScaler()\n#     train_scaled = scaling.fit_transform(trainX)\n#     test_scaled =  scaling.transform(testX)\n    knn = KNeighborsClassifier(n_neighbors=k_value, weights=weights, metric=metric, n_jobs=-1)\n    knn.fit(trainX, trainY)\n    trainpredictY = knn.predict(trainX)\n    testpredictY = knn.predict(testX)\n    return knn, trainpredictY, testpredictY\n    ","b63b6f80":"for k_values in range(3,11):\n    print(\"For k value of {}\".format(k_values))\n    knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, k_values)\n    print(\"Classification report for train\\n\", classification_report(trainY, trainpredictY))\n    print(\"\\n\\n\")\n    print(\"Classification report for test\\n\", classification_report(testY, testpredictY))\n    print(\"\\n\\n\")\n    \n    ","6ab028b1":"# We can select the k value of 5 since it provides the highest accuracy of 86","87c970a7":"for weights in ['uniform', 'distance']:\n    print(\"For weights as {}\".format(weights))\n    knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, weights=weights)\n    print(\"Classification Report for train\\n\", classification_report(trainY, trainpredictY))\n    print(\"\\n\\n\")\n    print(\"Classification Report for test\\n\", classification_report(testY, testpredictY))\n    print(\"\\n\\n\")\n    ","53ccfe09":"# Both uniform and distance weights provide the same accuracy, we will go with distance","f1e89272":"for metrics in [\"manhattan\", \"chebyshev\", \"minkowski\", \"euclidean\"]:\n    print(\"For metric {}\\n\".format(metrics))\n    knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, metric=metrics)\n    print(\"Classification Report for train\\n\", classification_report(trainY, trainpredictY))\n    print(\"\\n\\n\")\n    print(\"Classification Report for test\\n\", classification_report(testY, testpredictY))\n    print(\"\\n\\n\")","def69ab7":"# Manhattan is the metric to be chosen, from the accuracy score obtained.","dad3b820":"knn, trainpredictY, testpredictY = knn_model(trainX, trainY, testX, testY, k_value=7, weights='distance', metric='manhattan')\nprint(\"Classification Report for trained data\\n\", classification_report(trainY, trainpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(trainY, trainpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(trainY, trainpredictY))\ntestY_re = np.array(testY).reshape(-1,1)\n# testpredictY_re = np.array(testpredictY).reshape(-1,1)\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)\n# fpr_re = fpr.reshape(-1,1)\n# tpr_re = tpr.reshape(-1,1)\n# plot_roc_curve(knn, fpr_re, tpr_re)\n","070497c9":"# We need to remove few features that are highly correlated since we are getting a hight recall of 1.0","bc724eba":"nb = GaussianNB()\nnb.fit(trainX, trainY)\ntrainpredictY = nb.predict(trainX)\ntestpredictY = nb.predict(testX)\nprint(\"Confusion Matrix for train data \\n\", confusion_matrix(trainY, trainpredictY))\nprint(\"Classification Report for train data\\n\", classification_report(trainY, trainpredictY))\nprint(\"Confusion Matrix for test data \\n\", confusion_matrix(testY, testpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\ntrainY = np.array(trainY).reshape(-1,1)\ntrainpredictY = np.array(trainpredictY).reshape(-1,1)\ntestY = np.array(testY).reshape(-1,1)\ntestpredictY = np.array(testpredictY).reshape(-1,1)\n\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)\n","420f61c0":"def poly(trainX, trainY, degrees):\n    poly = PolynomialFeatures(degree=degrees)\n    train_poly = poly.fit_transform(trainX)\n    test_poly = poly.fit_transform(testX)\n    return train_poly, test_poly","63102539":"for degrees in [1,2,3]:\n    print(\"For degree {}\".format(degrees))\n    train_poly, test_poly = poly(trainX, trainY, degrees)\n    lr = LogisticRegression()\n    model_fit(lr, test_poly, testY, train_poly, trainY)\n","8b72c9d0":"# No Degrees are required, since degree 1 gave best results in terms of recall, precision, accuracy","88c3131c":"# NO change in accuracy observed with increase in polynomial","c87c95b4":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=5)\nprint(trainX.shape)\nprint(trainY.shape)\ntrainX_oversampled, trainY_oversampled = sm.fit_resample(trainX, trainY)\n\n\nprint(\"The training data is changed to \\n\")\n\nprint((pd.DataFrame(data=trainY, columns=['Attrition'])).value_counts())\nprint((pd.DataFrame(data=trainY_oversampled, columns=['Attrition'])).value_counts())","272b2644":"lr = LogisticRegression()\npredictY = model_fit(lr, testX, testY, trainX_oversampled, trainY_oversampled)\nauc = roc_auc_score(testY, predictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, predictY)\nplt.plot(fpr, tpr)","34d788e9":"# The output from the Smote looks more realistic since 863 entries for Attrition1 and 0 are taken to train the Regression Model","2596d4fd":"knn, trainpredictY, testpredictY = knn_model(trainX_oversampled, trainY_oversampled, testX, testY, k_value=7, weights='distance', metric='manhattan')\nprint(\"Classification Report for trained data\\n\", classification_report(trainY_oversampled, trainpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(trainY_oversampled, trainpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\nprint(\"Confusion Matrix for\\n\", confusion_matrix(testY, testpredictY))\ntestY_re = np.array(testY).reshape(-1,1)\n# testpredictY_re = np.array(testpredictY).reshape(-1,1)\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)","400f32a8":"nb = GaussianNB()\nnb.fit(trainX_oversampled, trainY_oversampled)\ntrainpredictY = nb.predict(trainX_oversampled)\ntestpredictY = nb.predict(testX)\nprint(\"Confusion Matrix for test data \\n\", confusion_matrix(testY, testpredictY))\nprint(\"Classification Report for test data\\n\", classification_report(testY, testpredictY))\n# trainY = np.array(trainY).reshape(-1,1)\n# trainpredictY = np.array(trainpredictY).reshape(-1,1)\n# testY = np.array(testY).reshape(-1,1)\n# testpredictY = np.array(testpredictY).reshape(-1,1)\n\nauc = roc_auc_score(testY, testpredictY)\nprint('auc score {}'.format(auc))\nfpr, tpr, thresholds = roc_curve(testY, testpredictY)\nplt.plot(fpr, tpr)","2a4a7a81":"# Feature Engineering","9c8f681b":"# Logistic Regression","20e30f49":"# Using Logistic Regression with higher degrees","1229fa82":"# Encoding categorical data","6c54c145":"# Using Smote with Logisitc regression, KNN, Gausian NB","c7a8e499":"> Note: Tried removing features with high correlation, but did not see improvement in model performnace, hence the features with high variance are also used to train the model.","6e657f5e":"# > **EDA**","4ca44de8":"# Naive Bayes\n","ae1e4173":"From the above analysis, the recall, precision, accuracy and roc_auc_score is better for Logistic regression with smote applied.","7385f702":"# KNN","9a7c20e7":"# Scaling","20b2c20f":"> Hence Logisitc regression with oversampling(smote) technique for the given imbalanced data provides the best result in terms of accuracy, precision, recall, roc_auc_curve"}}