{"cell_type":{"971bf2f5":"code","3a8a0303":"code","58775f56":"code","492f165a":"code","dae60ee0":"code","2491c8cc":"code","672f9aab":"code","94cebcfb":"code","837beb94":"code","4143b852":"code","656cdfd5":"code","06f662a2":"code","3eb4c414":"code","c2297a67":"code","0191e47f":"code","135f7c67":"code","4e68c029":"code","6d788716":"code","1e9287b2":"code","6365d69b":"code","ba24975a":"code","78504443":"code","d3c032b7":"code","dd21a14b":"code","b288af69":"code","6cbaa68e":"code","dd4680a6":"code","d3427a5e":"code","77dd2282":"code","d784443e":"code","f53a9618":"code","e5eab92e":"code","a46bbf71":"code","e3ed1816":"code","8be0b31b":"code","71916a12":"code","a6a08f33":"code","29f744a3":"code","e4c62f36":"code","912658cb":"code","b1d446d6":"code","0aab2bc9":"code","d8f48982":"code","718cffc1":"code","92a4125f":"code","3b9e25d7":"code","1aebd4b4":"markdown"},"source":{"971bf2f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3a8a0303":"data = pd.read_csv('..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')\n#data = pd.read_csv('..\/input\/all-covid19-vaccines-tweets\/vaccination_all_tweets.csv')\ndata = data.dropna().drop_duplicates()\ndata = data[data.text.str.len()>1]\ndata.head()\n\n#Reading data and removing duplicates\n","58775f56":"data['text'] = data['text'].str.lower()\ndata['text'] = data['text'].str.replace(\"[^A-Za-z0-9^,!?.\\\/'+]\", \" \")\ndata['text'] = data['text'].str.replace(r\"https?.*\", \" \")\ndata['text'] = data['text'].str.replace(r\"\\+\", \" plus \")\ndata['text']= data['text'].str.replace(r\",\", \" \")\ndata['text']= data['text'].str.replace(r\"\\.\", \" \")\ndata['text'] = data['text'].str.replace(r\"!\", \" ! \")\ndata['text'] = data['text'].str.replace(r\"\\?\", \" ? \")\ndata['text'] = data['text'].str.replace(r\"'\", \" \")\ndata['text'] = data['text'].str.replace(r\":\", \" : \")\ndata['text'] = data['text'].str.replace(r\"\\s{2,}\", \" \")\n\ntext = data['text']\ntext.head()\n#Data cleaning","492f165a":"from gensim.models import Word2Vec\nimport multiprocessing\nw2v_model = Word2Vec(min_count=3,window=4,\n                     size=300)\nprint(\"done word2vec\")\n#Defining word2vec model","dae60ee0":"from gensim.models.phrases import Phrases, Phraser\nsent = [row for row in data.text]\nphrases = Phrases(sent, min_count=1, progress_per=50000)\nbigram = Phraser(phrases)\nsentences = bigram[sent]\nsentences[1]\n#converting tweets into bigrams for word2vec","2491c8cc":"sentences2 = []\nfor i in sentences:\n    str(i)\n    i = i.split(' ')\n    sentences2.append(i)\n\n\nfor j in sentences2:\n    for k in j:\n        if(len(k) < 2):\n            j.remove(k)\n#splitting sentences by word to create word2vec vocabulary\n    \n\n","672f9aab":"w2v_model.build_vocab(sentences2, progress_per=50000)\nprint(\"done vocab\")","94cebcfb":"\n\nw2v_model.train(sentences2, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Done Training')\n\nw2v_model.init_sims(replace=True)\n\n","837beb94":"w2v_model.save(\"word2vec.model\")","4143b852":"\n\nword_vectors = Word2Vec.load(\"word2vec.model\").wv\nprint('loaded vectors')\n\n","656cdfd5":"from sklearn.cluster import KMeans\n\n\nmodel = KMeans(n_clusters=2, max_iter=1000, random_state=False, n_init=50).fit(X=word_vectors.vectors.astype('double'))\n\n\n#K means clustering model fit to word2vec model","06f662a2":"word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)","3eb4c414":"word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)\n#These blocks show top 10 words in each cluster to identify the positive one","c2297a67":"\n\npositive_cluster_index = 1\npositive_cluster_center = model.cluster_centers_[positive_cluster_index]\nnegative_cluster_center = model.cluster_centers_[1-positive_cluster_index]\n\n","0191e47f":"words = pd.DataFrame(word_vectors.vocab.keys())\nwords.columns = ['words']\nwords['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\nwords['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\nwords.cluster = words.cluster.apply(lambda x: x[0])\n#fits words to clusters","135f7c67":"\n\nwords['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\nwords['closeness_score'] = words.apply(lambda x: 1\/(model.transform([x.vectors]).min()), axis=1)\nwords['sentiment_coeff'] = words.closeness_score * words.cluster_value\n\n#displays cluster and sentiment score of each word (closeness score multiplied by pos or neg 1)","4e68c029":"words.head(10)","6d788716":"words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)","1e9287b2":"final_file = text","6365d69b":"final_file.head()","ba24975a":"sentiment_map = pd.read_csv('sentiment_dictionary.csv')\nsentiment_map.head()","78504443":"sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))\n#combine words with sentiment scores","d3c032b7":"file_weighting2 = final_file.copy()\nfile_weighting2 = file_weighting2.to_frame()\nfile_weighting2[\"weight\"] = 1\nfile_weighting2 = file_weighting2.rename(columns={\"text\": \"title\", \"weight\": \"rate\"})\nfile_weighting2[['title', 'rate']].to_csv(\"cleaned_dataset.csv\",index=False)\nfile_weighting = pd.read_csv(\"cleaned_dataset.csv\")","dd21a14b":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom IPython.display import display\n\n\n\ntfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\ntfidf.fit(file_weighting.title)\nfeatures = pd.Series(tfidf.get_feature_names())\ntransformed = tfidf.transform(file_weighting.title)\n#prepare tfidf score for adjusted weighting\n#TFIDF takes into account frequency, so the score for a common word like 'the' will be scaled down\n\n","b288af69":"def create_tfidf_dictionary(x, transformed_file, features):\n#create TFIDF dictionary, method from https:\/\/towardsdatascience.com\/unsupervised-sentiment-analysis-a38bf1906483\n    vector_coo = transformed_file[x.name].tocoo()\n    vector_coo.col = features.iloc[vector_coo.col].values\n    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n    return dict_from_coo\n\ndef replace_tfidf_words(x, transformed_file, features):\n    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n    return list(map(lambda y:dictionary[f'{y}'], x.title.split()))","6cbaa68e":"replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)","dd4680a6":"def replace_sentiment_words(word, sentiment_dict):\n    try:\n        out = sentiment_dict[word]\n    except KeyError:\n        out = 0\n    return out","d3427a5e":"\n\nreplaced_closeness_scores = file_weighting.title.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n\n","77dd2282":"\n\nreplacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.title]).T\nreplacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence']\nreplacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\nreplacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n\n\n","d784443e":"!pip install vaderSentiment","f53a9618":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","e5eab92e":"\n\ndef sentiment_analyzer_scores(sentence):\n    score = analyser.polarity_scores(sentence)\n    if score['pos']>score['neg']:\n        return 1\n    return 0\n\n#calculate sentiment with VADER engine to compare to our clustering","a46bbf71":"replacement_df['vader'] = replacement_df.apply(lambda x: sentiment_analyzer_scores(x.loc['sentence']), axis=1)","e3ed1816":"replacement_df","8be0b31b":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\npredicted_classes = replacement_df.prediction\ny_test = replacement_df.vader\n\nconf_matrix = pd.DataFrame(confusion_matrix(replacement_df.vader, replacement_df.prediction))\nprint('Confusion Matrix')\ndisplay(conf_matrix)\n\ntest_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n\nprint('\\n \\n Scores')\nscores = pd.DataFrame(data=[test_scores])\nscores.columns = ['accuracy', 'precision', 'recall', 'f1']\nscores = scores.T\nscores.columns = ['scores']\ndisplay(scores)\n#confusion matrix for comparison","71916a12":"predicted_classes.value_counts()\n#sanity check for number of positive and negative in cluster algorithm + vader\n#a large difference probably means something is wrong","a6a08f33":"y_test.value_counts()","29f744a3":"preds = replacement_df[\"vader\"]\ndata = data.reset_index()\ndata[\"sentiment\"] = preds\ndata","e4c62f36":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ncorrmatrix = data[['favorites', 'retweets','user_verified','user_followers','sentiment']].corr()\nprint(sns.heatmap(corrmatrix))\n#heatmap of correlations between interesting columns","912658cb":"#the zero correlations in sentiment are a bit strange, so this is separating the positive and negative\n#tweets for further analysis\ndata['date'] = pd.to_datetime(data['date'])\npos = data.loc[data['sentiment'] == 1]\nneg = data.loc[data['sentiment'] == 0]\npos.head()","b1d446d6":"#Deeper correlation analysis: comparing tweet engagement and account size on positive vs negative tweets\n\n","0aab2bc9":"#Comparing user followers\nfrom scipy.stats import ttest_ind\ndef correlation_analysis(param):\n    print(\"positive sentiment mean\",np.mean(pos[param]))\n    print(\"positive sentiment standard deviation\",np.std(pos[param]))\n    print(\"negative sentiment mean\",np.mean(neg[param]))\n    print(\"negative sentiment standard deviation\",np.std(neg[param]))\n    ttest,pval = ttest_ind(pos[param],neg[param])\n    print(\"p-value\",pval)\n    plt.plot(pos[param],label=\"positive\")\n    plt.plot(neg[param],label=\"negative\")\n    plt.legend()\n    plt.show()","d8f48982":"correlation_analysis(\"user_followers\")","718cffc1":"correlation_analysis(\"user_friends\")","92a4125f":"correlation_analysis(\"retweets\")","3b9e25d7":"correlation_analysis(\"favorites\")\n#only significant p value here, positive tweets are significantly more likely to get more likes","1aebd4b4":"This is an analysis of covid vaccine tweets, as well as correlations between tweet sentiment and account size\/engagement. An unsupervised clustering algorithm was used based off of this article\nhttps:\/\/towardsdatascience.com\/unsupervised-sentiment-analysis-a38bf1906483?gi=fcf9c329e93d"}}