{"cell_type":{"716f592f":"code","2e3d86bd":"code","7717dc5d":"code","c0027a04":"code","b14d2e63":"code","d3fae501":"code","2f0c0238":"code","5279cdc3":"code","50d6f904":"code","2650a708":"code","0741b372":"code","10f518a8":"code","57958252":"code","91cb4498":"code","22bec9ba":"code","f9a4fd99":"code","98fcd156":"code","9c6ee026":"code","e6fb93a2":"code","5a8b465f":"code","29aa96c2":"code","245ee9a7":"code","1a7f90be":"code","bf025979":"code","810d5a62":"code","b66f59a4":"code","d912c8dc":"code","ffd02cb9":"code","0785a80a":"code","57ceb77f":"code","87e13373":"code","457f7288":"code","8afc55b2":"code","0eb1542c":"code","8d1208dc":"code","333d49bf":"code","a60df278":"code","f7fe6083":"code","5c2d0bdf":"code","89516f1e":"code","835aff47":"code","19267c90":"code","35a8bfda":"code","73e625a9":"code","bfa610f7":"code","8fbb88c4":"code","13219001":"code","7e8a7389":"code","758f3a43":"code","9d1b8a00":"code","4ff5d94a":"code","aa3d4f9a":"code","5633bbb3":"markdown","559179eb":"markdown","6f3e2f4f":"markdown","0bc50b3e":"markdown","5b6aad5d":"markdown","64d72105":"markdown","edf15deb":"markdown","7555ec85":"markdown","4abb45c6":"markdown","235a364e":"markdown","bb29133e":"markdown"},"source":{"716f592f":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn","2e3d86bd":"from sklearn.datasets import load_breast_cancer\nds= load_breast_cancer()\nds","7717dc5d":"type(ds)","c0027a04":"ds.keys()","b14d2e63":"ds['data']","d3fae501":"# malignant or benign value\nds['target']","2f0c0238":"ds['target_names']","5279cdc3":"ds['DESCR']","50d6f904":"ds['feature_names']  # name of features","2650a708":"ds['filename'] # Location\/ path of data file","0741b372":"df=pd.DataFrame(np.c_[ds['data'],ds['target']],columns=\n               np.append(ds['feature_names'],['target']))","10f518a8":"# dataframe to csv\ndf.to_csv('breast_cancer_dataset.csv')","57958252":"df.head()","91cb4498":"df.info()","22bec9ba":"df.describe()","f9a4fd99":"# pairplot of cancer dataframe\n# sn.pairplot(df,hue ='target')","98fcd156":"# pairplot of sample feature\nsn.pairplot(df,hue=\"target\",vars=['mean radius','mean texture','mean perimeter',\n                                  'mean area','mean smoothness','mean compactness'])","9c6ee026":"# count target class\nsn.countplot(df[\"target\"])","e6fb93a2":"# counter plot of feature mean radius\nplt.figure(figsize=(25,10))\nsn.countplot(df['mean radius'])","5a8b465f":"plt.figure(figsize=(25,10))\nsn.heatmap(df)","29aa96c2":"df.corr()","245ee9a7":"# heatmap of correlation matrix of breast canver dataframe\nplt.figure(figsize=(25,25))\nsn.heatmap(df.corr(),annot=True,linewidths=3)","1a7f90be":"# creat second dataframe by droping target\ndf2=df.drop(['target'],axis = 1)","bf025979":"df2.shape","810d5a62":"plt.figure(figsize = (20,10))\nax = sn.barplot(df2.corrwith(df.target).index, df2.corrwith(df.target))\nax.tick_params(labelrotation = 90)","b66f59a4":"# input variable\nX = df.drop(['target'], axis = 1)\nX.head(6)","d912c8dc":"# output variable\ny = df['target']\ny.head(6)","ffd02cb9":"# split dataset into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X,y, test_size=0.2,random_state=5)","0785a80a":"# Feature scaling\n# we need converting different values into one unit\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_sc =scaler.fit_transform(X_train)\nX_test_sc = scaler.transform(X_test)","57ceb77f":"from sklearn.metrics import confusion_matrix, classification_report,accuracy_score","87e13373":"# Support vector classifier\nfrom sklearn.svm import SVC\nsvc_classifier = SVC()\nsvc_classifier.fit(X_train, y_train)\ny_pred_scv = svc_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_scv)","457f7288":"# Train with Standard scaled Data\nsvc_classifier2 = SVC()\nsvc_classifier2.fit(X_train_sc, y_train)\ny_pred_svc_sc = svc_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_svc_sc)","8afc55b2":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr_classifier = LogisticRegression(random_state = 51)\nlr_classifier.fit(X_train, y_train)\ny_pred_lr = lr_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_lr)","0eb1542c":"# K \u2013 Nearest Neighbor Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_classifier.fit(X_train, y_train)\ny_pred_knn = knn_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_knn)","8d1208dc":"# Train with Standard scaled Data\nknn_classifier2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn_classifier2.fit(X_train_sc, y_train)\ny_pred_knn_sc = knn_classifier.predict(X_test_sc)\naccuracy_score(y_test, y_pred_knn_sc)","333d49bf":"# Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\ny_pred_nb = nb_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_nb)","a60df278":"# Train with Standard scaled Data\nnb_classifier2 = GaussianNB()\nnb_classifier2.fit(X_train_sc, y_train)\ny_pred_nb_sc = nb_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_nb_sc)","f7fe6083":"# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 51)\ndt_classifier.fit(X_train, y_train)\ny_pred_dt = dt_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_dt)","5c2d0bdf":"# Train with Standard scaled Data\ndt_classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 51)\ndt_classifier2.fit(X_train_sc, y_train)\ny_pred_dt_sc = dt_classifier.predict(X_test_sc)\naccuracy_score(y_test, y_pred_dt_sc)","89516f1e":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 51)\nrf_classifier.fit(X_train, y_train)\ny_pred_rf = rf_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_rf)","835aff47":"# Train with Standard scaled Data\nrf_classifier2 = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 51)\nrf_classifier2.fit(X_train_sc, y_train)\ny_pred_rf_sc = rf_classifier.predict(X_test_sc)\naccuracy_score(y_test, y_pred_rf_sc)","19267c90":"# Adaboost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nadb_classifier = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', random_state = 200),\n                                    n_estimators=2000,\n                                    learning_rate=0.1,\n                                    algorithm='SAMME.R',\n                                    random_state=1,)\nadb_classifier.fit(X_train, y_train)\ny_pred_adb = adb_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_adb)","35a8bfda":"# Train with Standard scaled Data\nadb_classifier2 = AdaBoostClassifier(DecisionTreeClassifier(criterion = 'entropy', random_state = 200),\n                                    n_estimators=2000,\n                                    learning_rate=0.1,\n                                    algorithm='SAMME.R',\n                                    random_state=1,)\nadb_classifier2.fit(X_train_sc, y_train)\ny_pred_adb_sc = adb_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_adb_sc)","73e625a9":"# XGBoost Classifier\nfrom xgboost import XGBClassifier\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train, y_train)\ny_pred_xgb = xgb_classifier.predict(X_test)\naccuracy_score(y_test, y_pred_xgb)","bfa610f7":"# !pip install xgboost","8fbb88c4":"# Train with Standard scaled Data\nxgb_classifier2 = XGBClassifier()\nxgb_classifier2.fit(X_train_sc, y_train)\ny_pred_xgb_sc = xgb_classifier2.predict(X_test_sc)\naccuracy_score(y_test, y_pred_xgb_sc)","13219001":"# XGBoost classifier most required parameters\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n}\n1\n2\n3\n4\n# Randomized Search\nfrom sklearn.model_selection import RandomizedSearchCV\nrandom_search = RandomizedSearchCV(xgb_classifier, param_distributions=params, scoring= 'roc_auc', n_jobs= -1, verbose= 3)\nrandom_search.fit(X_train, y_train)","7e8a7389":"random_search.best_params_","758f3a43":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=0.3, gamma=0.4,\n       learning_rate=0.3, max_delta_step=0, max_depth=3,\n       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n       nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=None, subsample=1, verbosity=1)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n# training XGBoost classifier with best parameters\nxgb_classifier_pt = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=0.4, gamma=0.2,\n       learning_rate=0.1, max_delta_step=0, max_depth=15,\n       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n       nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=None, subsample=1, verbosity=1)\n \nxgb_classifier_pt.fit(X_train, y_train)\ny_pred_xgb_pt = xgb_classifier_pt.predict(X_test)","9d1b8a00":"# confusion Matrix\ncm = confusion_matrix(y_test, y_pred_xgb_pt)\nplt.title('Heatmap of Confusion Matrix', fontsize = 15)\nsn.heatmap(cm, annot = True)\nplt.show()","4ff5d94a":"print(classification_report(y_test, y_pred_xgb_pt))","aa3d4f9a":"# Cross validation\nfrom sklearn.model_selection import cross_val_score\ncross_validation = cross_val_score(estimator = xgb_classifier2, X = X_train_sc, y = y_train, cv = 10)\nprint(\"Cross validation of XGBoost model = \",cross_validation)\nprint(\"Cross validation of XGBoost model (in mean) = \",cross_validation.mean())\nfrom sklearn.model_selection import cross_val_score\ncross_validation = cross_val_score(estimator = xgb_classifier_pt, X = X_train_sc,y = y_train, cv = 10)\nprint(\"Cross validation accuracy of XGBoost model = \", cross_validation)\nprint(\"\\nCross validation mean accuracy of XGBoost model = \", cross_validation.mean())\n","5633bbb3":"# Heatmap of dataframe ","559179eb":"# Heatmap of a correlation matrix ","6f3e2f4f":"The model is giving 0% type II error and it is best.\n\n### Classification Report of Model","0bc50b3e":"###### Breast Cancer Detection Machine Learning Model Building\nWe have clean data to build the Ml model. But which Machine learning algorithm is best for the data we have to find. The output is a categorical format so we will use supervised classification machine learning algorithms.\n\nTo build the best model, we have to train and test the dataset with multiple Machine Learning algorithms then we can find the best ML model. So let\u2019s try.\n\nFirst, we need to import the required packages.","5b6aad5d":"# create DataFrame ","64d72105":"# Data load form sklearn libraries ","edf15deb":"# Data Visualization","7555ec85":"In the above correlation barplot only feature 'smootheness error' is strongly positively correlated with the target than others. The features 'mean factor dimensionn','texture error',and'symmetry error' are very less positive correlated and others remaining are strongly negatively correlated.","4abb45c6":"## Cross-validation of the ML model\nTo find the ML model is overfitted, under fitted or generalize doing cross-validation.","235a364e":"##### Data preporcessing\nsplit data frame in train and test","bb29133e":"### Breast Cancer Detection using Machine Learning Algo "}}