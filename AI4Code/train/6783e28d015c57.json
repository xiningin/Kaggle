{"cell_type":{"ecace257":"code","324331a4":"code","8739cf83":"code","9dd632bb":"code","0f4e8ae7":"code","e523a07f":"code","3e73195d":"code","1b0f70c0":"code","11d8f03d":"code","e11bc042":"code","d061be13":"code","6af65edb":"code","3b7355f5":"code","01444548":"code","88939f0b":"code","e0bc2a1a":"code","df2b365a":"code","dc2a3c43":"code","1a5fa35c":"code","0ea0ac89":"code","cee2940a":"code","1304ed1f":"code","4ef65eea":"code","99bd62e6":"code","3e07692d":"code","c05f8838":"code","3b9dccf0":"code","ecba3875":"code","ab988ce6":"code","537a65a6":"markdown","02eac397":"markdown","9d2398f8":"markdown","7c7392fd":"markdown","57af0178":"markdown","9d914078":"markdown","8c3b3091":"markdown","5c895811":"markdown","f7148a54":"markdown","2b18be14":"markdown","67b7a9d9":"markdown","b9873862":"markdown","ccdabb5b":"markdown","c99bd03d":"markdown","614b37a5":"markdown","ec8a4b24":"markdown","8344a766":"markdown","95292565":"markdown","6ed66cad":"markdown","a8282101":"markdown","baaff3b8":"markdown","b096421e":"markdown","36204c86":"markdown","8155b0bc":"markdown","c3ec82a1":"markdown","b6bfc985":"markdown","999dcf04":"markdown","065d7d6b":"markdown"},"source":{"ecace257":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom fastai.tabular import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import Lasso\nfrom scipy.stats import norm, skew\nfrom sklearn.ensemble import IsolationForest\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","324331a4":"train = pd.read_csv('..\/input\/train__updated.csv')\ntest = pd.read_csv('..\/input\/test__updated.csv')","8739cf83":"train.describe()","9dd632bb":"train.drop('index',axis=1,inplace=True)\nidx=test['index']","0f4e8ae7":"test.drop('index',axis=1,inplace=True)","e523a07f":"train['platform_type'] = np.where(train['platform_type']==5.0,0,1)\ntrain['sst_measurement_method'] = train['sst_measurement_method'].fillna(value=2.0)\ntrain['sst_measurement_method'] = train['sst_measurement_method'].apply(str)\ntrain['nightday_flag'] = np.where(train['nightday_flag']==1,0,1)\ntrain['national_source_indicator'] = train['national_source_indicator'].fillna(value=0)\ntrain['id_indicator'] = train['id_indicator'].apply(str)\ntrain['wind_speed_indicator'] = train['wind_speed_indicator'].apply(str)\ntrain['deck'] = train['deck'].apply(str)\ntrain['source_id'] = np.where(train['source_id']==103,0,1)\ntrain['ship_course'] = train['ship_course'].fillna(value=9.0)\ntrain['ship_course'] = train['ship_course'].apply(str)\ntrain['ten_degree_box_number'] = train['ten_degree_box_number'].apply(str)\ntrain['characteristic_of_ppp'] = train['characteristic_of_ppp'].fillna(value=9)\ntrain['characteristic_of_ppp'] = train['characteristic_of_ppp'].apply(str)\n\ntest['platform_type'] = np.where(test['platform_type']==5.0,0,1)\ntest['sst_measurement_method'] = test['sst_measurement_method'].fillna(value=2.0)\ntest['sst_measurement_method'] = test['sst_measurement_method'].apply(str)\ntest['nightday_flag'] = np.where(test['nightday_flag']==1,0,1)\ntest['national_source_indicator'] = test['national_source_indicator'].fillna(value=0)\ntest['id_indicator'] = test['id_indicator'].apply(str)\ntest['wind_speed_indicator'] = test['wind_speed_indicator'].apply(str)\ntest['deck'] = test['deck'].apply(str)\ntest['source_id'] = np.where(test['source_id']==103,0,1)\ntest['ship_course'] = test['ship_course'].fillna(value=9.0)\ntest['ship_course'] = test['ship_course'].apply(str)\ntest['ten_degree_box_number'] = test['ten_degree_box_number'].apply(str)\ntest['characteristic_of_ppp'] = test['characteristic_of_ppp'].fillna(value=9)\ntest['characteristic_of_ppp'] = test['characteristic_of_ppp'].apply(str)","3e73195d":"NANwind_speed = train['wind_speed'].median()\nNANsea_level_pressure = train['sea_level_pressure'].mode()[0]\nNANamt_pressure_tend = train['amt_pressure_tend'].mode()[0]\nNANair_temperature = train['air_temperature'].mode()[0]\nNANsea_surface_temp = train['sea_surface_temp'].mode()[0]\nNANswell_height = train['swell_height'].median()\nNANwave_period = train['wave_period'].median()\n\ntrain['wind_speed'] = train['wind_speed'].fillna(-1)\ntrain['sea_level_pressure'] = train['sea_level_pressure'].fillna(NANsea_level_pressure)\ntrain['amt_pressure_tend'] = train['amt_pressure_tend'].fillna(NANamt_pressure_tend)\ntrain['air_temperature'] = train['air_temperature'].fillna(NANair_temperature)\ntrain['sea_surface_temp'] = train['sea_surface_temp'].fillna(NANsea_surface_temp)\ntrain['swell_height'] = train['swell_height'].fillna(5.0)\ntrain['wave_period'] = train['wave_period'].fillna(0)\n\ntest['wind_speed'] = test['wind_speed'].fillna(-1)\ntest['sea_level_pressure'] = test['sea_level_pressure'].fillna(NANsea_level_pressure)\ntest['amt_pressure_tend'] = test['amt_pressure_tend'].fillna(NANamt_pressure_tend)\ntest['air_temperature'] = test['air_temperature'].fillna(NANair_temperature)\ntest['sea_surface_temp'] = test['sea_surface_temp'].fillna(NANsea_surface_temp)\ntest['swell_height'] = test['swell_height'].fillna(5.0)\ntest['wave_period'] = test['wave_period'].fillna(0)","1b0f70c0":"#train.value_counts()","11d8f03d":"train.drop(columns=['wbt_indicator','time_indicator','dup_check','indicator_for_temp','imma_version','attm_count','latlong_indicator','wind_direction_indicator','dpt_indicator','source_exclusion_flags','release_no_primary','release_no_secondary','release_no_tertiary','intermediate_reject_flag','release_status_indicator'],inplace=True)\ntest.drop(columns=['wbt_indicator','time_indicator','dup_check','indicator_for_temp','imma_version','attm_count','latlong_indicator','wind_direction_indicator','dpt_indicator','source_exclusion_flags','release_no_primary','release_no_secondary','release_no_tertiary','intermediate_reject_flag','release_status_indicator'],inplace=True)\n\ntrain.drop(columns=['day','hour','national_source_indicator','year','ship_speed','visibility','present_weather','past_weather','wetbulb_temperature','dewpoint_temperature','total_cloud_amount','lower_cloud_amount','swell_direction','swell_period'],inplace=True)\ntest.drop(columns=['day','hour','national_source_indicator','year','ship_speed','visibility','present_weather','past_weather','wetbulb_temperature','dewpoint_temperature','total_cloud_amount','lower_cloud_amount','swell_direction','swell_period'],inplace=True)","e11bc042":"categoricalCols = ['ten_degree_box_number','characteristic_of_ppp','sst_measurement_method','id_indicator','wind_speed_indicator','deck','ship_course']\nasa = set(train.columns) - set(categoricalCols)\nasb = asa - set(['platform_type','nightday_flag','source_id','dup_status'])\nasb = list(asb)\nasb = set(asb) - set(['wave_height'])\nasb = list(asb)","d061be13":"train = pd.get_dummies(train,columns=['ten_degree_box_number','characteristic_of_ppp','sst_measurement_method','id_indicator','wind_speed_indicator','deck','ship_course'])\ntest = pd.get_dummies(test,columns=['ten_degree_box_number','characteristic_of_ppp','sst_measurement_method','id_indicator','wind_speed_indicator','deck','ship_course'])","6af65edb":"trainer = train.copy()\nfig, axs = plt.subplots(3,4 , figsize=(22, 12), facecolor='w', edgecolor='k')\naxs = axs.ravel()\noutlierPreds = np.ones((len(asb),len(train)))\nfor i, column in enumerate(asb):\n    print (column)\n    isolation_forest = IsolationForest(contamination=0.05,behaviour='old')\n    isolation_forest.fit(train[column].values.reshape(-1,1))\n\n    xx = np.linspace(train[column].min(), train[column].max(), len(train)).reshape(-1,1)\n    anomaly_score = isolation_forest.decision_function(xx)\n    outlier = isolation_forest.predict(xx)\n    \n    outlierPreds[i] = isolation_forest.predict(train[column].values.reshape(-1,1))\n    \n    axs[i].plot(xx, anomaly_score, label='anomaly score')\n    axs[i].fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n                     where=outlier==-1, color='r', \n                     alpha=.4, label='outlier region')\n    axs[i].legend()\n    axs[i].set_title(column)","3b7355f5":"tempArr = outlierPreds.T\n\nfor i in range(0,len(tempArr)):\n  count = 0\n  for j in tempArr[i]:\n    if j == -1:\n      count = count+1\n  if count >= 3:\n    train.drop([i],inplace=True)","01444548":"# for k in asb:\n#     print(k)\n#     plt.hist(train[k])\n#     plt.show()","88939f0b":"skewed_feats = train[asb].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","e0bc2a1a":"skewnessBox = skewness[(skewness.Skew)>0.75]\nskewnessSquare = skewness[(skewness.Skew)<-0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewnessBox.shape[0]))\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewnessSquare.shape[0]))","df2b365a":"from scipy.special import boxcox1p\nskewed_features1 = skewnessBox.index\nskewed_features2 = skewnessSquare.index\nlam = 0.15\nfor feat in skewed_features1:\n    train[feat] = boxcox1p(train[feat], lam)\n    test[feat] = boxcox1p(test[feat], lam)\nfor feat in skewed_features2:\n    train[feat] = np.square(train[feat])\n    test[feat] = np.square(test[feat])","dc2a3c43":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\ntrain[asb] = scaler.fit_transform(train[asb])\ntest[asb] = scaler.transform(test[asb])","1a5fa35c":"plt.subplot(1, 4, 1)\nsns.distplot(train.wave_height, kde=False, fit = norm)\n\nplt.subplot(1, 4, 2)\nsns.distplot(np.log(train.wave_height + 1), kde=False, fit = norm)\nplt.xlabel('Log WaveHeight')\n\nplt.subplot(1, 4, 3)\nsns.distplot(np.sqrt(train.wave_height), kde=False, fit = norm)\nplt.xlabel('Square WaveHeight')\n\n\nplt.subplot(1, 4, 4)\nsns.distplot(np.cbrt(train.wave_height), kde=False, fit = norm)\nplt.xlabel('Cube WaveHeight')","0ea0ac89":"y = train['wave_height']\nyTrue = y.copy()\ntrain['wave_height'] = np.cbrt(y)\ny = train['wave_height']\ntrain.drop(columns=['wave_height'],inplace=True)","cee2940a":"from datetime import datetime\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","1304ed1f":"gdb = GradientBoostingRegressor()\nselector1 = RFECV(gdb, step=1, cv=5,n_jobs =-1,scoring=\"neg_mean_squared_error\",verbose = 2)\nstart_time = timer(None)\nselector1.fit(train, y)\ntimer(start_time)","4ef65eea":"selected1 = train.columns[selector1.support_]\nprint(selected1)","99bd62e6":"trainSelected1 = train[selected1]","3e07692d":"def logit_eval(learning_rate,n_estimators,max_features):\n    params={'penalty':'l1'}\n    params['learning_rate'] = max(learning_rate, 0)\n    params['n_estimators'] = int(n_estimators)\n    params['max_features'] = int(max_features)\n    score = logitModel(params)\n    return score\ndef logitModel(params):\n    reg = GradientBoostingRegressor(learning_rate = params['learning_rate'], n_estimators = params['n_estimators'], max_features = params['max_features'])\n    rocAuc = cross_val_score(reg, trainSelected1, y, cv=5,scoring='neg_mean_squared_error')\n    return rocAuc.mean()\nlgbBO = BayesianOptimization(logit_eval, {'learning_rate': (0.001,1.5),'n_estimators':(10,1000),'max_features':(1,len(trainSelected1.columns)+0.9)})\nlgbBO.maximize(init_points=30, n_iter=300)","c05f8838":"reg_opt = GradientBoostingRegressor(learning_rate = 0.1848, n_estimators = 187, max_features = 9)","3b9dccf0":"reg_opt.fit(trainSelected1,y)\n\npred =reg_opt.predict(test[selected1])\npreds = np.power(pred,3)\n\nsam=pd.read_csv('..\/input\/sample_sub.csv')\n\nsam['wave_height']=pred","ecba3875":"sam.to_csv('sandbox_sub.csv',index=False)","ab988ce6":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe\ncreate_download_link(sam)","537a65a6":"## Preparing Data for feeding into the model","02eac397":"### Loading dataset","9d2398f8":"## SUMMARY OF THE APPROACH - \n\n## EDA\n\n**NAN Handling**\n* The train data provided had a lot of NANs in them.\n* The columns with more than 3\/4 th of the total values (50000) were dropped from the analysis.\n* Different techniques like filling NANs with mean, median, mode, values attained by the model were tried and best choices were made depending on the correlation values with wave_height.\n* The NANs in some of the categorical variables were converted into a new class and then processed.\n\n**Transformations**\n* Some of the continuous features were skewed, i.e, they were Left-tailed or Right-tailed.\n* Appropriate transformations like BoxCox Transformation and Square Transformation were used.\n* The dependent variable \"wave_height\" was also right-tailed.\n* Cube-Root Transformation was used for standardizing it.\n* Predictions were made for the cube-root of the wave heights, which were later raised to the power 3 to get the actual values of the predictions.\n\n## Feature Selection\n\n* Techniques like L1 regularization, L2 regularization, Recursive Feature Elimination were used for optimal feature selection.\n* Using RFECV with Gradient Boosting Regressor for Feature Selection gave the most optimal Cross-Validation results. So, this was used finally for the prediction on the test set.\n\n## Parameter Tuning\n\n* Bayesian Hyper-parameter tuning was used to find the optimal paramters for the learning algorithm.\n\n## Fitting and Prediction\n\n* The models tried out were - \n        1.Elastic Net Regression\n        2.Lasso Regression\n        3.Ridge Regression\n        4.Random Forest Regression\n        5.Support Vector Regression\n        6.Gradient Boosting Regression\n        7.XGBoost Regressor\n        8.LightGBM Regressor\n\n* The model giving the best result was Gradient Boosting Regressor, which was finally used for the prediction on the test set.\n\n## Scope of Improvement\n\n**Further things which I could not try due to time as well as submission constraints - **\n\n* Ensembling the different well performing models to get an even better prediction\n* Meta-Models and Stacking for better predictions.\n\n","7c7392fd":"**Making the categorical variables in the form of integers into categorical variables and making the NAN values as a New Category. **","57af0178":"Used **IsolationForest **for detecting the outliers in the given train set. \nA threshold of 5% outliers was set and IsolationForest model was fit in, which gave indication about which rows were outliers based on individual feature analysis.\n\nThis individaul feature analysis was combined to form a **Tri-variate outlier detection**, which removed the suspected outliers from our train data.","9d914078":"It can be clearly seen that the **cube root transformation** gives back the **most standardized distribution** of the wave height.","8c3b3091":"**Managing NAN values for the Continuous variables depending on how the correlation value varies for different imputing methods like Mean, Median, Mode, a few common values that variable attains.**","5c895811":"## Saving to CSV","f7148a54":"Using **RFECV** for the optimal feature selection process, using \"**Negative of the Mean Squared Error**\" as the metric to maximize, in turn minimizing the RMSE","2b18be14":"Using **Bayesian HyperParamter Optimisation** for finding the best parameter tuning for Gradient Boosting Regressor","67b7a9d9":"**There were many columns which had no information at all contained in them - **\n\n**1.All the rows were having a single value for that categorical variable **\n\n**2.More than 3\/4 th of the values were missing (NANs)**\n\n**3.Very Low correlation with the Dependent variable \"wave_height\"**","b9873862":"**Scaling the Continuous variables for facilitating better learning by the model.**","ccdabb5b":"**One-Hot Encoding certain categorical variables which were not ordinal for better representation in the learning model**","c99bd03d":"**Viewing the continuous variables Histogram plots **","614b37a5":"## Feature Selection","ec8a4b24":"## Fitting the model and Predicting","8344a766":"## Outlier Removal","95292565":"## Hyper-parameter Tuning for optimal learning model","6ed66cad":"Using the Optimal parameter setting obtained from the Bayesian optimisation, defining the optimal Gradient Boosting learner.\n\nThe optimal parameters I found during the training were -** learning_rate = 0.1848, n_estimators = 187, max_features = 9**","a8282101":"### Importing packages","baaff3b8":"## Data PreProcessing","b096421e":"**Separating out columns for the continuous variables for carrying out operations later....**","36204c86":"Visualising the dependent variable \"wave_height\" and its distribution. \n\nIt is quite evident that the **wave_height is right-tailed** and hence, different transformations to fix the skewness were evaluated. The **cube root transformation** was finally used as the graph is the most standard for that transformation.\n\nThen the data was split into X and y for future training.","8155b0bc":"This is the optimal feature selection which i got during training - \n\n**['month', 'latitude', 'longitude', 'wind_speed', 'sea_level_pressure',\n        'air_temperature', 'sea_surface_temp', 'wave_period', 'swell_height',\n       'one_degree_box_number', 'id_indicator_1']**","c3ec82a1":"## Moving on to the code for implementing the above mentioned ideas....","b6bfc985":"## Droping Index column","999dcf04":"It is quite evident that some of the continuous variables are **skewed**,i.e, some are Left-skewed and some are Right-skewed distributions in the train set.\n\nWe set a **threshold of 0.75** for skewness and fix this issue for the  skewed variables using -\n\n1.**BoxCox Transformations** for Right-tailed distributions\n\n2.**Square Transformations** for Left-tailed distributions","065d7d6b":"## Downloading file"}}