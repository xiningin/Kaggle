{"cell_type":{"707e6bdd":"code","02e24345":"code","8202776a":"code","4646147d":"code","12b59e99":"code","52ff03ec":"code","531688e9":"markdown","3acabcc6":"markdown","379b1fb4":"markdown","1ac784fc":"markdown","7e8e3e58":"markdown","4a45da7d":"markdown"},"source":{"707e6bdd":"import os, time\nimport pandas\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom kaggle_datasets import KaggleDatasets\n\n# We'll use a tokenizer for the BERT model from the modelling demo notebook.\n!pip install bert-tensorflow\nimport bert.tokenization\n\nprint(tf.version.VERSION)","02e24345":"SEQUENCE_LENGTH = 128\n\nDATA_PATH =  \"..\/input\/jigsaw-multilingual-toxic-comment-classification\"\nBERT_PATH = \"..\/input\/bert-multi\"\nBERT_PATH_SAVEDMODEL = os.path.join(BERT_PATH, \"bert_multi_from_tfhub\")\n\nOUTPUT_PATH = \"\/kaggle\/working\"","8202776a":"# Training data from our first competition,\n# https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data\nwiki_toxic_comment_data = \"jigsaw-toxic-comment-train.csv\"\n\nwiki_toxic_comment_train = pandas.read_csv(os.path.join(\n    DATA_PATH, wiki_toxic_comment_data))\nwiki_toxic_comment_train.head()","4646147d":"def get_tokenizer(bert_path=BERT_PATH_SAVEDMODEL):\n    \"\"\"Get the tokenizer for a BERT layer.\"\"\"\n    bert_layer = tf.saved_model.load(bert_path)\n    bert_layer = hub.KerasLayer(bert_layer, trainable=False)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    cased = bert_layer.resolved_object.do_lower_case.numpy()\n    tf.gfile = tf.io.gfile  # for bert.tokenization.load_vocab in tokenizer\n    tokenizer = bert.tokenization.FullTokenizer(vocab_file, cased)\n  \n    return tokenizer\n\ntokenizer = get_tokenizer()","12b59e99":"example_sentence = wiki_toxic_comment_train.iloc[37].comment_text[:150]\nprint(example_sentence)\n\nexample_tokens = tokenizer.tokenize(example_sentence)\nprint(example_tokens[:17])\n\nexample_input_ids = tokenizer.convert_tokens_to_ids(example_tokens)\nprint(example_input_ids[:17])","52ff03ec":"def process_sentence(sentence, max_seq_length=SEQUENCE_LENGTH, tokenizer=tokenizer):\n    \"\"\"Helper function to prepare data for BERT. Converts sentence input examples\n    into the form ['input_word_ids', 'input_mask', 'segment_ids'].\"\"\"\n    # Tokenize, and truncate to max_seq_length if necessary.\n    tokens = tokenizer.tokenize(sentence)\n    if len(tokens) > max_seq_length - 2:\n        tokens = tokens[:(max_seq_length - 2)]\n\n    # Convert the tokens in the sentence to word IDs.\n    input_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens + [\"[SEP]\"])\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    pad_length = max_seq_length - len(input_ids)\n    input_ids.extend([0] * pad_length)\n    input_mask.extend([0] * pad_length)\n\n    # We only have one input segment.\n    segment_ids = [0] * max_seq_length\n\n    return (input_ids, input_mask, segment_ids)\n\ndef preprocess_and_save_dataset(unprocessed_filename, text_label='comment_text',\n                                seq_length=SEQUENCE_LENGTH, verbose=True):\n    \"\"\"Preprocess a CSV to the expected TF Dataset form for multilingual BERT,\n    and save the result.\"\"\"\n    dataframe = pandas.read_csv(os.path.join(DATA_PATH, unprocessed_filename),\n                                index_col='id')\n    processed_filename = (unprocessed_filename.rstrip('.csv') +\n                          \"-processed-seqlen{}.csv\".format(SEQUENCE_LENGTH))\n\n    pos = 0\n    start = time.time()\n\n    while pos < len(dataframe):\n        processed_df = dataframe[pos:pos + 10000].copy()\n\n        processed_df['input_word_ids'], processed_df['input_mask'], processed_df['all_segment_id'] = (\n            zip(*processed_df[text_label].apply(process_sentence)))\n\n        if pos == 0:\n            processed_df.to_csv(processed_filename, index_label='id', mode='w')\n        else:\n            processed_df.to_csv(processed_filename, index_label='id', mode='a',\n                                header=False)\n\n        if verbose:\n            print('Processed {} examples in {}'.format(\n                pos + 10000, time.time() - start))\n        pos += 10000\n    return\n  \n# Process the training dataset.\npreprocess_and_save_dataset(wiki_toxic_comment_data)","531688e9":"# BERT Tokenizer\n\nGet the tokenizer corresponding to our multilingual BERT model. See [TensorFlow \nHub](https:\/\/tfhub.dev\/tensorflow\/bert_multi_cased_L-12_H-768_A-12\/1) for more information about the model.","3acabcc6":"# Preprocessing\n\nProcess individual sentences for input to BERT using the tokenizer, and then prepare the entire dataset. The same code will process the other training data files, as well as the validation and test data.","379b1fb4":"# Data processing\n\nThis notebook preprocesses our dataset for compatibility with BERT. You should feel free to investigate other solutions (both models and tokenizers)!","1ac784fc":"We can look at one of our example sentences after we tokenize it, and then again after converting it to word IDs for BERT.","7e8e3e58":"# Examples\n\nLoad and look at examples from [our first competition](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/). These are comments from Wikipedia with a variety of annotations (toxic, obscene, threat, etc).","4a45da7d":"# Set global variables\n\nSet maximum sequence length and path variables."}}