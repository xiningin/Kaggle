{"cell_type":{"d2dfe312":"code","05a51df9":"code","45d6036f":"code","77ae5e39":"code","a9145068":"code","a7d86070":"code","38969162":"code","6b4549a2":"code","e2327997":"code","98803365":"code","e0de4303":"code","8a89e370":"code","7c73cf56":"code","043aa372":"code","b220792f":"code","b44eeace":"code","c18e0ec2":"code","c1bf0a1d":"code","8199d421":"code","73769078":"code","e64356c0":"code","e60663a2":"code","92504781":"code","108b5e7d":"code","bb56144c":"code","fc758655":"code","c35d4794":"code","7d462b51":"code","b8928906":"code","180fd9d2":"code","b6c1dfca":"code","267583d6":"code","b1d54e61":"code","13d8800e":"code","953234d4":"code","c31c4986":"code","6ac17202":"code","dfdb8344":"code","e1817ae0":"code","b6f87f16":"code","c5aa27f3":"code","8f1c40a3":"markdown","badcfab0":"markdown","ea35fa89":"markdown","35a30da1":"markdown","ddc174c6":"markdown","c6ea9517":"markdown","dc378d20":"markdown","e7d1832a":"markdown","a988af54":"markdown","53a8b1d6":"markdown","c4f06f8a":"markdown","4accaeee":"markdown","d7221401":"markdown","42ad0209":"markdown","0e59416a":"markdown","6e9f2b62":"markdown","1b3c76f6":"markdown","39f0b7f6":"markdown","e70263d4":"markdown","b0481c07":"markdown","5c4faddf":"markdown","9bf827ba":"markdown","5de6af61":"markdown","69be9fe5":"markdown","7dbbda8e":"markdown","c2f3fbed":"markdown","533aaadd":"markdown","92f16192":"markdown","e42ad8ef":"markdown","e8f683f7":"markdown","2bb2b4d5":"markdown","b768e01a":"markdown","4e30d37c":"markdown","e0ffa62d":"markdown","f959e74b":"markdown","97ce6302":"markdown","71e75044":"markdown"},"source":{"d2dfe312":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\n\nfrom fastai.tabular.all import *\nimport fastai\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting, enable_iterative_imputer\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.impute import IterativeImputer\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, NeighbourhoodCleaningRule\n\nimport scipy.optimize\nfrom scipy.stats import rankdata\n\nfrom xgboost import XGBClassifier\n\nfrom category_encoders import MEstimateEncoder\n\nfrom tqdm import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05a51df9":"# Config\nN_SPLITS = 5\nSEED = 2022\nUSE_TARGET_ENCODING = False\nUSE_ITERIMP_AS_MODEL = True\nDO_OVERSAMPLING = False\nDO_UNDERSAMPLING = True\n\n# Constants\nTARGET = 'song_popularity'\nFOLD_CRIT = 'fold_crit'\nKFOLDS = 'KFOLDS'\nID = 'id'\nIS_TRAIN = 'is_train'","45d6036f":"train_in = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv').sample(frac=1.0, random_state = SEED).reset_index(drop=True)\ntest_in = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\n\n#train_in.info()","77ae5e39":"train_in[TARGET].value_counts()","a9145068":"features = list(set(train_in.columns)-set([ID, FOLD_CRIT, KFOLDS, TARGET]))\n#features","a7d86070":"def gen_features(df):\n    return (df.assign(\n                     instrumentalness_log = lambda df_: np.log(df_.instrumentalness+0.005),\n                     acousticness_log = lambda df_: np.log(df_.acousticness+0.5),\n                     liveness_log = lambda df_: np.log(df_.liveness)\n                     )\n              .drop(columns=['instrumentalness', 'acousticness','liveness'])\n           )","38969162":"train_in = gen_features(train_in)\ntest_in = gen_features(test_in)\n\n#test_in[['liveness_log']].hist()","6b4549a2":"features = list(set(train_in.columns)-set([ID, FOLD_CRIT, KFOLDS, TARGET, IS_TRAIN]))\nfeatures.sort()","e2327997":"## Baseline imputation:\n\n#fill_vals = train_in[features].median()\n#train=train_in.fillna(fill_vals)\n#test=test_in.fillna(fill_vals)\n#train","98803365":"train_in[IS_TRAIN] = 1\ntest_in[IS_TRAIN] = 0\n\ntt = train_in.append(test_in).reset_index(drop=True)\n","e0de4303":"nan_cols = list(set(tt.columns[np.where(tt.isnull().sum()>0)])-set([TARGET, KFOLDS, FOLD_CRIT]))\nnan_cols.sort()\nnan_cols","8a89e370":"# from https:\/\/www.kaggle.com\/robikscube\/handling-with-missing-data-youtube-stream\ntt_missing_tag_df = tt[nan_cols].isna()\ntt_missing_tag_df.columns = [f\"{c}_is_NaN\" for c in tt_missing_tag_df.columns]\n\ntt = pd.concat([tt, tt_missing_tag_df], axis=1)\n\ntt[\"n_NaN\"] = tt[nan_cols].isna().sum(axis=1)\n\nnan_features = list(tt_missing_tag_df.columns) + ['n_NaN']","7c73cf56":"it_imputer = IterativeImputer(max_iter=10, initial_strategy='median')\ntt_iterimp = it_imputer.fit_transform(tt[features+nan_features])\n# convert array to dataframe\ntt_iterimp = pd.DataFrame(tt_iterimp, columns=features+nan_features)\n\n# merge with tt\ntt_iterimp = tt[list(set(tt.columns)-set(features+nan_features))].merge(tt_iterimp, left_index=True, right_index=True)[tt.columns]\n\n#tt_iterimp","043aa372":"train = tt_iterimp[tt_iterimp[IS_TRAIN]==1].reset_index(drop = True).drop(columns=[IS_TRAIN])\ntest = tt_iterimp[tt_iterimp[IS_TRAIN]==0].reset_index(drop = True).drop(columns=[IS_TRAIN, TARGET])\n\n#test","b220792f":"assert (train.isnull().sum()>0).sum() ==  0\nassert (test.isnull().sum()>0).sum() == 0","b44eeace":"n_components = 2\n\npca_tt = Pipeline(steps=[('scale', StandardScaler()), ('pca', PCA(n_components=n_components, random_state = SEED))]).fit_transform(tt_iterimp[features])\ntt  = pd.DataFrame(pca_tt, columns = ['pca_'+str(i+1) for i in range(n_components)]).merge(tt_iterimp,left_index=True,right_index=True)\n\n#tt.head()","c18e0ec2":"features = list(set(tt.columns)-set([ID, FOLD_CRIT, KFOLDS, TARGET, IS_TRAIN] + nan_features))\nfeatures.sort()","c1bf0a1d":"tt_piv = tt[features + [IS_TRAIN]].melt(id_vars=IS_TRAIN, var_name = 'Feature')\n\ntt_piv","8199d421":"sns.displot(data=tt_piv,\n            x='value', \n            hue=\"is_train\", \n            col=\"Feature\", \n            kind=\"kde\",\n            common_norm = False,\n            col_wrap = 3,\n            fill = True, \n            alpha=0.3, \n            palette=sns.color_palette(\"tab10\", 2),\n            facet_kws=dict(sharey=False, sharex=False))","73769078":"sns.lmplot(data=tt, x='pca_1', y='pca_2',col='is_train', hue='is_train', fit_reg=False)","e64356c0":"sns.scatterplot(data=tt, x='pca_1', y='pca_2', hue='is_train', palette=sns.color_palette(\"tab10\", 2))","e60663a2":"radiusNN = RadiusNeighborsRegressor(radius=0.175, n_jobs=-1).fit(tt[['pca_1','pca_2']].values, np.abs(tt.is_train.values-1))\n\ntt['test_alike'] = radiusNN.predict(tt[['pca_1','pca_2']])\n\ntt.plot.scatter(x='is_train',y='test_alike')","92504781":"no_test_threshold = tt[tt.is_train==0].test_alike.min()\noutlier_threshold = tt[tt.is_train==1].test_alike.max()\n\nprint(f'no_test_threshold: {no_test_threshold:.4f}. outlier_threshold: {outlier_threshold:.4f}')","108b5e7d":"print(f' Points below no_test_threshold: {tt[tt.test_alike<no_test_threshold].shape[0]}')\nprint(f' Points above outlier_threshold: {tt[tt.test_alike>outlier_threshold].shape[0]}')","bb56144c":"sns.scatterplot(data=tt[tt.test_alike<no_test_threshold], x='pca_1', y='pca_2', color = 'green')","fc758655":"sns.scatterplot(data=tt[tt.test_alike>outlier_threshold], x='pca_1', y='pca_2', color = 'red')","c35d4794":"train = tt[(tt[IS_TRAIN]==1)&(tt.test_alike>=no_test_threshold)].reset_index(drop = True).drop(columns=[IS_TRAIN])\ntest = tt[tt[IS_TRAIN]==0].reset_index(drop = True).drop(columns=[IS_TRAIN, TARGET])\n\ntrain.shape, test.shape","7d462b51":"CAT_FEATURES = ['time_signature', 'audio_mode', 'key']","b8928906":"class FastaiNN:\n    \n    def __init__(self, layers=[100,100]):\n        self.layers = layers\n        self.learn = None\n\n        \n    def fit(self, dummy1=None, dummy2=None):\n        val_idx = valid_df.index.values\n        splitter = IndexSplitter(val_idx)(range_of(train))\n\n        to = TabularPandas(train, procs=[Categorify, FillMissing,Normalize],#\n                   #cat_names = CAT_FEATURES,\n                   cont_names = features,\n                   y_names=TARGET,\n                   splits=splitter)\n\n        dls = to.dataloaders(bs=256)\n        \n        self.learn = tabular_learner(dls,\n                        layers=[4,8], \n                        #metrics=[RocAucBinary()],\n                        config={'ps':0.3}, \n                        opt_func=RAdam\n                       )\n        #self.learn.lr_find()\n        with self.learn.no_logging():\n            self.learn.fit_one_cycle(20, 1e-2)\n        \n    \n    def predict(self, df):\n        test_dl = self.learn.dls.test_dl(df)\n        preds,_= self.learn.get_preds(dl = test_dl)\n        \n        return preds.view(-1).cpu().numpy()","180fd9d2":"# define models\nmodels = {\n    'NN' : FastaiNN(),\n    #'LogReg_P': Pipeline(steps=[('scale', StandardScaler()), ('lr', LogisticRegression(n_jobs=-1, random_state=SEED))]),\n    'AdaBoost_P': AdaBoostClassifier(random_state=SEED, n_estimators=100,learning_rate=0.1),\n    'ExtraTrees_P': ExtraTreesClassifier(n_jobs=-1, random_state=SEED, min_samples_leaf=23, class_weight= 'balanced_subsample'),\n    'RF_P': RandomForestClassifier( n_jobs=-1, random_state=SEED, min_samples_leaf=23, class_weight= 'balanced_subsample'),\n    'HGB_P': HistGradientBoostingClassifier(random_state=SEED),\n    'XGB_P': XGBClassifier(use_label_encoder=False,\n                           eval_metric= 'auc',\n                           learning_rate=0.003, #\n                           max_depth=5, #\n                           subsample=0.75, #\n                           reg_lambda = 2, #\n                           n_estimators = 2000, #100\n                           objective= 'binary:logistic', \n                           n_jobs=-1, \n                           random_state=SEED,\n                           tree_method = 'gpu_hist'\n                          ),\n}\n","b6c1dfca":"train[FOLD_CRIT] = train[TARGET].astype(str)\ntrain[FOLD_CRIT] = train[FOLD_CRIT] + train[nan_features].apply(lambda x: '_'.join(x[nan_features].astype(str)), axis=1)\n\n# apply abhisheks splitting technique\nskf = StratifiedKFold(n_splits = N_SPLITS, random_state = None, shuffle = False)\n\ntrain[KFOLDS] = -1\n\nfor f, (train_idx, valid_idx) in enumerate(skf.split(X = train, y = train[FOLD_CRIT].values)):\n    \n    train.loc[valid_idx,KFOLDS] = f\n\ntrain.groupby(KFOLDS)[TARGET].count()","267583d6":"if USE_TARGET_ENCODING:\n    for cf in CAT_FEATURES:\n        train[cf + '_te'] = 0\n        test[cf + '_te'] = 0\n\n    for f in range(N_SPLITS):\n\n        train_df = train[train[KFOLDS] != f]\n        valid_df = train[train[KFOLDS] == f]\n\n        m_estim = MEstimateEncoder(cols=CAT_FEATURES, m=15.0)\n\n        m_estim.fit(train_df[CAT_FEATURES], train_df[TARGET])\n\n        # fill validation fold\n        estim_val_df = m_estim.transform(valid_df[CAT_FEATURES])\n        estim_test_df = m_estim.transform(test[CAT_FEATURES])\n        for cf in CAT_FEATURES:\n            train.loc[train[KFOLDS] == f, cf + '_te'] = estim_val_df[cf]\n\n            # fill test\n            test[cf + '_te'] += estim_test_df[cf] \/ N_SPLITS\n\n    features += [cf + '_te' for cf in CAT_FEATURES]","b1d54e61":"oversampling = False\nundersampling = True\n\nfor (m_name, m) in models.items():\n    print(f'# Model:{m_name}\\n')\n    train[m_name + '_oof'] = 0\n    test[m_name] = 0\n    \n    y_oof = np.zeros(train.shape[0])\n    \n    for f in range(N_SPLITS):\n\n        train_df = train[train[KFOLDS] != f]\n        valid_df = train[train[KFOLDS] == f]\n        \n        if oversampling:\n            smote = SMOTE(random_state=SEED)\n            train_df_X, train_df_y = smote.fit_resample(train_df[features], train_df[TARGET])\n            m.fit(train_df_X, train_df_y)\n        elif undersampling:\n            underSampler = NeighbourhoodCleaningRule(n_neighbors=3, threshold_cleaning=0.9, n_jobs=-1)\n            train_df_X, train_df_y = underSampler.fit_resample(train_df[features], train_df[TARGET])\n            m.fit(train_df_X, train_df_y)\n        else:\n            m.fit(train_df[features], train_df[TARGET])\n        \n        if m_name[-2:] == '_P':\n            oof_preds = m.predict_proba(valid_df[features]) #\n            oof_preds = (oof_preds * np.arange(oof_preds.shape[1])).sum(axis=1) #\n        else:\n            oof_preds = m.predict(valid_df[features])\n        \n        mmScaler = MinMaxScaler()\n        oof_preds = mmScaler.fit_transform(oof_preds.reshape(-1, 1)).reshape(-1)\n        y_oof[valid_df.index] = oof_preds\n        print(f'Fold {f} roc_auc_score: {roc_auc_score(valid_df[TARGET], oof_preds):0.5f}')\n        \n        \n        if m_name[-2:] == '_P':\n            test_preds = m.predict_proba(test[features]) #\n            test_preds = (test_preds * np.arange(test_preds.shape[1])).sum(axis=1) #\n        else:            \n            test_preds = m.predict(test[features])\n            \n        mmScaler = MinMaxScaler()\n        test[m_name] += mmScaler.fit_transform(test_preds.reshape(-1, 1)).reshape(-1) \/ N_SPLITS\n    \n    train[m_name + '_oof'] = y_oof\n    \n    print(f\"\\nTotal roc_auc_score: {roc_auc_score(train[TARGET], train[m_name + '_oof']):0.5f}\\n\")\n\n\noof_cols = [m_name + '_oof' for m_name in models.keys()] #+ ['Iter_P_oof']\npred_cols = list(models.keys()) #+ ['Iter_P']\n\nprint(f\"# ALL Mean ensemble roc_auc_score: {roc_auc_score(train[TARGET], train[oof_cols].mean(axis=1)):0.5f}\\n\")","13d8800e":"# The IterativeImputer can also be used as a model. Let's experiment with that. oof_score ca. 0.555, not too bad.\n\nif USE_ITERIMP_AS_MODEL:\n    impMod = IterativeImputer(max_iter=10, initial_strategy='median')\n\n    temp = train.copy()\n\n    temp[IS_TRAIN]=1\n    temp = temp.append(test.copy()).reset_index(drop=True).assign(is_train = lambda df_: df_[IS_TRAIN].fillna(0))\n\n    temp = impMod.fit_transform(temp[features + nan_features + [TARGET, IS_TRAIN]])\n\n    temp = pd.DataFrame(temp, columns=features + nan_features + [TARGET, IS_TRAIN]).reset_index(drop=True)\n\n    mmScaler = MinMaxScaler()\n    test['Iter_P'] = mmScaler.fit_transform(temp[temp[IS_TRAIN]==0][TARGET].values.reshape(-1, 1)).reshape(-1)\n\n\n    temp.loc[temp[IS_TRAIN]==0,'TARGET2'] = temp[temp[IS_TRAIN]==0][TARGET]\n\n    temp = impMod.transform(temp[features + nan_features + ['TARGET2', IS_TRAIN]])\n\n    temp = pd.DataFrame(temp, columns=features + nan_features + ['TARGET2', IS_TRAIN]).reset_index(drop=True)\n\n    mmScaler = MinMaxScaler()\n    train['Iter_P_oof'] = mmScaler.fit_transform(temp[temp[IS_TRAIN]==1].TARGET2.values.reshape(-1, 1)).reshape(-1)\n\n\n    print(f\"\\nTotal roc_auc_score: {roc_auc_score(train[TARGET], train['Iter_P_oof']):0.5f}\")\n    \n    oof_cols += ['Iter_P_oof']\n    pred_cols += ['Iter_P']\n\n    print(f\"# ALL Mean ensemble roc_auc_score: {roc_auc_score(train[TARGET], train[oof_cols].mean(axis=1)):0.5f}\\n\")","953234d4":"oofs = train[oof_cols]\n\noof_diffs = oofs.copy()\nfor c in oof_cols:\n    oof_diffs[c] = oofs[c]-train[TARGET]\n    oof_diffs[c] = oof_diffs[c]\n\nsns.heatmap(oof_diffs.corr(),vmin=.8, vmax=1)","c31c4986":"cons = ({'type': 'eq', 'fun': lambda x: x.sum()-1})\nbnds = [(0,None)] * len(oof_cols)\n\nw_init = np.ones((len(oof_cols)))\/len(oof_cols)","6ac17202":"fun_ex1 = lambda w: (train[TARGET]-np.matmul(oofs.values, w)).var()\nw_calc = scipy.optimize.minimize(fun_ex1, w_init.copy(), method='SLSQP',  constraints=cons, bounds=bnds).x\n\nprint('Weights for *Min Variance* ensemble:')\nfor mk, wc in zip(list(pred_cols), w_calc):\n    print(f'- {mk}: {wc:.4f}')","dfdb8344":"submission = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\nsubmission[TARGET]=(test[pred_cols]*w_calc).sum(axis=1)\n\nsubmission.to_csv('submission.csv',index=False)\n\nprint(f'OOF roc_auc_score (min var): {roc_auc_score(train[TARGET], (train[oof_cols]*w_calc).sum(axis=1)):.8f}, Var y_oof: {(train[oof_cols]*w_calc).var(axis=1).mean():.5f}, Var y_pred: {(test[pred_cols]*w_calc).var(axis=1).mean():0.5f}')","e1817ae0":"train[TARGET].hist(), submission[TARGET].hist()","b6f87f16":"# roc_auc\nfun_ex2 = lambda w: -1*roc_auc_score(train[TARGET], np.matmul(oofs.values, w))\nw_calc2 = scipy.optimize.minimize(fun_ex2, w_init.copy(), method='SLSQP',  constraints=cons, bounds=bnds).x\n\nprint('Weights for *Max RocAuc* ensemble:')\nfor mk, wc in zip(list(pred_cols), w_calc2):\n    print(f'- {mk}: {wc:.4f}')\n\n# build submission\nsubmission[TARGET]=(test[pred_cols]*w_calc2).sum(axis=1)\nsubmission.to_csv('submission_ra.csv',index=False)\n\n# eval\nprint(f'OOF roc_auc_score (max roc_auc): : {roc_auc_score(train[TARGET], (train[oof_cols]*w_calc2).sum(axis=1)):.8f}, Var y_oof: {(train[oof_cols]*w_calc2).var(axis=1).mean():.5f}, Var y_pred: {(test[pred_cols]*w_calc2).var(axis=1).mean():0.5f}')","c5aa27f3":"preds_ranked = rankdata(test[pred_cols], axis = 0)\n\nsubmission[TARGET]=np.median(preds_ranked, axis=1)\n\nsubmission.to_csv('submission_median_rank.csv',index=False)\n\noofs_ranked = rankdata(train[oof_cols], axis = 0)\n\nprint(f'OOf median of ranked: {roc_auc_score(train[TARGET], np.median(oofs_ranked,axis=1)):.8f}')","8f1c40a3":"## Define constants","badcfab0":"We want outlier_threshold low and no_test_threshold high, but still low enough to have a proper separation. So be carefull while tweaking the `radius` parameter. \n\nThis plot shows the `no_test_threshold` points (from train):","ea35fa89":"# EDA\nThere is a lot of EDA already done in @headsortails [tutorial](https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun), that I won't repeat.\nBut I really got inspired by the overlapping, transparent density plots. So let's try this to catch some insights for the later *adversarial validation*.\n\nFirst thing todo. Pivot the data.","35a30da1":"## Median of ranked","ddc174c6":"Create new flag features for missing values. ","c6ea9517":"The cloud with the train data looks a bit bigger. Let's overlap the clouds.","dc378d20":"How many points are behind the thresholds?","e7d1832a":"## Feature Engineering pt.2\n### PCA\nApply PCA to compress features in a lower dimensional feature space. Hope to catch some signals. Starting out with 2-dimensional PCA to make the results plottable.","a988af54":"Now compare the distributions of test and train for each feature.","53a8b1d6":"Look at this! There is a stronger concentration of test data in the center. It's time to remove some of the train data, that are not test alike.","c4f06f8a":"### Check diversity of models","4accaeee":"## Build KFolds\nStratified-KFolds are build from the target column in combination with the NaN-features.","d7221401":"# Training","42ad0209":"## Target Encoding\nTarget encoding on categorical columns didn't improve the models significantly. Hence, they are turned of by default.","0e59416a":"## Feature Engineering pt.1\nShift values above 0 and apply log-scaling. Drop log-scaled columns.","6e9f2b62":"## Min Variance","1b3c76f6":"Refresh feature list.","39f0b7f6":"# Ensembling\n\nThree types of ensemble are generated.\n\n1. Ensemble: Linear combination of models that minimizes the variance. (see https:\/\/www.kaggle.com\/joatom\/model-allocation)\n2. Ensemble: Linear combination of models that maximizes roc_auc_score.\n3. Ensemble: Median of ranked predictions\n\nDefining constraints for solver and initilizing weights:","e70263d4":"# Intro\n\nThis competition ([Song Popularity Competition](https:\/\/www.kaggle.com\/c\/song-popularity-prediction\/overview)) is about predicting popular songs.\n\n![](https:\/\/joatom.github.io\/ai_curious\/images\/learn_along.png)\n\nIn addition to this competition, there are tutorials available on @abhishek [Youtube channel](https:\/\/www.youtube.com\/c\/AbhishekThakurAbhi). The tutorials are easy to follow and yet contain lots of useful techniques. The notebooks from the videos are also available in the competition's notebook section. Each tutorial covers a certain topic, such as EDA or Imputation.\n\nI will try a new learning strategy in this competition. I'll set up a baseline notebook and than try out the topics of the tutorials *before* watching the videos or studying the tutorial notebooks. I then watch the videos or study the associated notebooks. I'll try out some of the techniques or insights shown in the tutorial and if they improve the results I'll add them to my baseline notebook.\nLet's see how it goes ...\n\n1. Building a baseline model:\n    - Starting with a modified version of my notebook from TPS Aug21-competition ([Model allocation](https:\/\/www.kaggle.com\/joatom\/model-allocation)), incl. oof\n    - Median imputation to get started\n    - Simple sklearn ensemble\n> Lessons Learned: keep an eye on the metric while defining ensemble weights. Negative weights might be usefull on regression but not on probabilistic metrics, such as RocAuc.\n    \n2. EDA:\n    - I tried some transformations\n        - log-transform\n        - *StandardScaler* improved some algos (*RobustScaler* didn't work as good)\n    - Distribution of train vs. test\n    - Distribution preds vs. train target\n> Lessons learned and thoughts: Third competition in a row where distribution of preds doesn't look at all like train targets. Here, it could be because of auc threshold?! But also lately couldn't avoid NN-regressions to get dragged towards mean instead of proper distribution. Also target transformation doesn't work all the time. Got to study this.\n\n3. Watch [EDA](https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun) tutorial pt.1:\n    - Lazy me, didn't added much\n    - Got some insights, simply by watching the video\n> Lessons learned: I really should do more EDA. Plots improve notebooks.\n\n4. Imputation:\n    - Combine train and test\n    - Initialize missing values with median or drop na rows\n    - Flagged rows for missing value\n    - Build training loop to predict missing values for each feature (Classifier for *key*, Regression else)\n> Lessons learned: Fancy imputation can lead to overfitting. Simple median imputation was better on LB.\n\n5. Read [Imputation](https:\/\/www.kaggle.com\/robikscube\/handling-with-missing-data-youtube-stream) Tutorial notebook:\n    - Sum of missing values of a row is a nice feature\n    - Will use *IterativeImputer*. Works great on private notebook. *BaysianRidge* estimator is faster and stronger here, than *HistGradientBoostingRegressor*.\n> Lessons learned: *LGBM* can handle NaNs. Out-of-the box tools (*IterativeImputer*) is often better than home-made.\n\n6. Watch [EDA](https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun) tutorial pt.2:\n    - I liked the overlapping, transparent plots. I'll try them but on some engineered features. It's about time to look at PCA.\n    - in the video the technique *adversarial validation* (AV) was mentioned. I had this in mind as a secrete weapon. But since it was mentioned, I implement it here. \n> Lessons learned: Combining visualization and AV can be a thing!\n\n7. Folding\n    - StartifiedKFold on target and nan-flagging columns\n    \n8. Target Encoding\n    - A [discussion post](https:\/\/www.kaggle.com\/c\/song-popularity-prediction\/discussion\/302827) from @vad13irt reminded me to try out target encoding on categorical features. No improvement. Probably because cats are mostly ordinal.\n\n9. Models\n    - Lot's of tree based from *sklearn*, *FastAI* NN, *XGBoost*.\n    - Best single model: *HistGradientBoostingClassifier*\n    - Also tried *IterativeImputer* as model. Works as well as neural net (roc_auc 0.555).\n    - Hyperparameter tuning: manually.\n\n10. Ensembling\n    - Minimizing variance (see [here](https:\/\/www.kaggle.com\/joatom\/model-allocation))\n    - Max roc_auc_score (see @abhishek's book)\n    - Median of ranked\n\n## Conclusion: Learning experience\nThe accompanying YouTube tutorials were really special. Covering basic topics in a clear format was helpful and motivating for further engagement. I bother to look at many notebooks during the competition because there are always so many. I try, more or less successful, to focus on a few topics. Getting pointed to some high quality content helps to focus.\n\n## References\nBaseline Notebook\n- Ensembling: [Model allocation](https:\/\/www.kaggle.com\/joatom\/model-allocation)\n\nCompetition tutorial notebooks and videos\n- EDA: https:\/\/www.kaggle.com\/headsortails\/song-popularity-eda-live-coding-fun, [Video pt.1](https:\/\/www.youtube.com\/watch?v=JXF-7rCcR1c), [Video pt.2](https:\/\/www.youtube.com\/watch?v=2aE6SvCVOis)\n- Imputation: https:\/\/www.kaggle.com\/robikscube\/handling-with-missing-data-youtube-stream, [Video](https:\/\/www.youtube.com\/watch?v=EYySNJU8qR0)\n\nAdversarial Validation\n- Blog: http:\/\/fastml.com\/adversarial-validation-part-one\/ \n- Where I found out about it: https:\/\/www.kaggle.com\/tunguz\/adversarial-santander\n- My first try on AV: https:\/\/www.kaggle.com\/joatom\/a-test-like-validation-set\n\nTarget Encoding\n- Discussion post: https:\/\/www.kaggle.com\/c\/song-popularity-prediction\/discussion\/302827\n- Kaggle course: https:\/\/www.kaggle.com\/ryanholbrook\/target-encoding\n- Target Encoding and KFolding: https:\/\/medium.com\/rapids-ai\/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784","b0481c07":"# Adversarial Validation\nAdversarial Validation (AV) is a technique to see how similar test and train data are. It basicaly works like this:\n- Combine test and train to one dataset (`tt`)\n- Add a column flagging test and train data (`is_test`)\n- Build a classifier to predict flag column (`is_test`)\n- if roc_auc close to 0.5 then test and train are similar, if not be carefull to not overfit to train.\n\n> Note: Some consider AV as bad practice, because you look at the test data in advance. Kind of cheating, because in real live you barely know the data to predict in advance. A different opinion is, that it is rather foolish if you ignore the test data, if they are available. Because you want to know when the model is not suitable, when test data has a different distribution.\n\nSince I have this great insight from the PCA feature EDA, I am not doing the common classification part, here.\nInstead I am using a neighborhood search and classify with a threshold. Let's approach it thought by thought.\n1. So how can I easily get rid of the orange dots on the PCA plot. First though, SVC, but since the clouds are overlapping, not sure that's gonna work? Or just using three line to seperate the blue from the orange dots? No, to fuzzy.\n2. Nearestneighbors could work. But KNN will probably be a bit random in the center. Nead something with a fixed radius.\n3. ...Quick check on sklearn docs. They provide a *RadiusNeighborsRegressor*. That's what I wan't.","5c4faddf":"Train imputer on original features.","9bf827ba":"## Models","5de6af61":"Build submission.","69be9fe5":"Finaly we remove the not test-alike data from train.","7dbbda8e":"# Start coding ...","c2f3fbed":"Split imputed dataframe back to train and test.","533aaadd":"Check which columns currently have missing values.","92f16192":"## Training loop and oofs\n\nThe oofs and predictions are MinMax-Scaled (from 0 to 1). Due to the metric a model with very centered predictions could skew the results of a broadly spread model, when the models later get ensembled. Another option to counter this, would be to convert the predictions into ranking. But then we would loose the information of the density of predictions in certain areas.","e42ad8ef":"## Max RocAuc Score","e8f683f7":"Flag train and test data and combine the two datasets.","2bb2b4d5":"Looks nice. But not very usefull. Single feature distributions between train and test are pretty simular, besides some peeks for train data.\n\nSo let's have a closer look on the PCA features, since they contain signal of all the other features.","b768e01a":"We can see that a few point of the train data is not at all classified as test (close to 0). Some of the test data is very confidentely classified as test (close to 1). \n\nNext we define two thresholds:\n- `no_test_threshold`: the lower side where the test scale ends. Every point below this threshold we are confident that it is not test-alike and can be removed from train.\n- `outlier_threshold`: the upper side where the train scale ends. Every point above this line might be difficult to predict with the given train data.\n","4e30d37c":"Imputation following @robikscube [tutorial](https:\/\/www.kaggle.com\/robikscube\/handling-with-missing-data-youtube-stream). The NaN values will be filled by sklearns *IterativeImputer*.","e0ffa62d":"# Prepare data\n## Load data","f959e74b":"Check full imputation.","97ce6302":"The `outlier_threshold` points (from test) are here:","71e75044":"## Impute"}}