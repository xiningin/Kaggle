{"cell_type":{"b66a734b":"code","dd1f07b4":"code","3950e662":"code","be2acd01":"code","27be48d2":"code","71eb41e3":"code","36625188":"code","e8120000":"code","b1ea601e":"code","15167c0e":"code","b86f21c9":"code","26774aa1":"markdown","3de2923e":"markdown","b7aaea14":"markdown","fa05726e":"markdown","e6affca4":"markdown","84286725":"markdown","73aca744":"markdown","931c2685":"markdown","f159d986":"markdown"},"source":{"b66a734b":"import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers as L","dd1f07b4":"(x_train, y_train), (x_valid, y_valid) = keras.datasets.cifar10.load_data()\nx_train = x_train\/255.0\nx_valid = x_valid\/255.0\n# x_train = np.expand_dims(x_train, axis=3)\n# x_valid = np.expand_dims(x_valid, axis=3)\ny_train = keras.utils.to_categorical(y_train)\ny_valid = keras.utils.to_categorical(y_valid)","3950e662":"T_EPOCHS = 25\nS_EPOCHS = 20\nIMAGE_SIZE = x_train.shape[1:]\nBATCH_SIZE = 512\nN_CLASSES = y_train.shape[-1]\nIMAGE_SIZE, N_CLASSES","be2acd01":"def nn_callbacks():\n    es = keras.callbacks.EarlyStopping(\n        patience=5, verbose=1, restore_best_weights=True, min_delta=1e-4\n    )\n    rlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n    return [es, rlp]","27be48d2":"d_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nd_valid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n\ndel x_train, x_valid, y_train, y_valid","71eb41e3":"def build_teacher_model(name='teacher'):\n    base_model = keras.applications.VGG19(input_shape=IMAGE_SIZE, include_top=False)\n    base_model.trainable = True\n    return keras.models.Sequential([\n            base_model,        \n            L.GlobalAvgPool2D(),        \n            L.Dense(N_CLASSES)\n        ], name=name\n    )\n        \n\nteacher_model = build_teacher_model()\nteacher_model.summary()","36625188":"def build_student_model(name='student'):\n    return keras.models.Sequential([\n        L.Conv2D(64, 3, input_shape=IMAGE_SIZE, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.MaxPool2D(pool_size=2),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.MaxPool2D(pool_size=2),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.MaxPool2D(pool_size=2),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.MaxPool2D(pool_size=2),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.Conv2D(64, 3, padding='same', activation='relu'),\n        L.MaxPool2D(pool_size=2),\n        L.GlobalAvgPool2D(),\n        L.Dense(N_CLASSES),\n    ],name=name) \n\nstudent_model = build_student_model()\nstudent_model.summary()","e8120000":"teacher_model.compile(\n    optimizer=keras.optimizers.Adam(1e-5), \n    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nhistory = teacher_model.fit(\n    d_train.shuffle(1024, 19).batch(BATCH_SIZE),\n    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n    epochs=T_EPOCHS,\n    callbacks=nn_callbacks(), \n    batch_size=BATCH_SIZE\n)","b1ea601e":"class Distiller(keras.Model):\n    def __init__(self, student, teacher, activation):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n        self.activation = activation\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=10,\n    ):\n        \"\"\" Configure the distiller.\n\n        Args:\n            optimizer: Keras optimizer for the student weights\n            metrics: Keras metrics for evaluation\n            student_loss_fn: Loss function of difference between student\n                predictions and ground-truth\n            distillation_loss_fn: Loss function of difference between soft\n                student predictions and soft teacher predictions\n            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n            temperature: Temperature for softening probability distributions.\n                Larger temperature gives softer distributions.\n        \"\"\"\n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student.compile(optimizer=optimizer, metrics=metrics, loss=student_loss_fn)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n        x, y = data\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            student_predictions = self.student(x, training=True)\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = self.distillation_loss_fn(\n                self.activation(teacher_predictions \/ self.temperature, axis=1),\n                self.activation(student_predictions \/ self.temperature, axis=1),\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.compiled_metrics.update_state(y, student_predictions)\n\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n        )\n        return results\n\n    def test_step(self, data):\n        x, y = data\n        teacher_predictions = self.teacher(x, training=False)\n        student_predictions = self.student(x, training=False)\n        \n        student_loss = self.student_loss_fn(y, student_predictions)\n        distillation_loss = self.distillation_loss_fn(\n            self.activation(teacher_predictions \/ self.temperature, axis=1),\n            self.activation(student_predictions \/ self.temperature, axis=1),\n        )\n        loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss, \"loss\": loss}\n        )\n        return results\n    \n    def call(self, x):\n        return self.student(x)","15167c0e":"distiller = Distiller(student_model, teacher_model, tf.nn.softmax)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.7,\n    temperature=100,\n)\nhistory_distillation = distiller.fit(\n    d_train.shuffle(1024, 19).batch(BATCH_SIZE), \n    validation_data=d_valid.shuffle(1024, 19).batch(BATCH_SIZE),\n    epochs=S_EPOCHS, callbacks=nn_callbacks(), batch_size=BATCH_SIZE\n)","b86f21c9":"import os\n\nprint('Teacher Model:')\nteacher_model.save('teacher.h5')\nteacher_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\nprint(\"File Size is :\", round(os.path.getsize('teacher.h5')\/1024**2, 2), \"MB\")\nprint('Distilled Model:')\nstudent_model.save('student.h5')\nstudent_model.evaluate(d_valid.shuffle(1024, 19).batch(BATCH_SIZE))\nprint(\"File Size is :\", round(os.path.getsize('student.h5')\/1024**2, 2), \"MB\")","26774aa1":"# Data Preparation","3de2923e":"# Training Teacher","b7aaea14":"**Student Model**","fa05726e":"**Teacher Model**","e6affca4":"# Distillation in Action","84286725":"**Reference**\n\n* [Distilling the Knowledge in a Neural Network](https:\/\/arxiv.org\/abs\/1503.02531)\n* [Implementation of classical Knowledge Distillation](https:\/\/keras.io\/examples\/vision\/knowledge_distillation\/)","73aca744":"# Comparison","931c2685":"# Building the Models","f159d986":"# Knowledge Distillation"}}