{"cell_type":{"3b07b123":"code","e33b0c68":"code","d5c6db30":"code","07c559a9":"code","23bd0d66":"code","07d7beb8":"code","eff5e49c":"code","3ebcdebb":"code","3525cca0":"code","7a0d79cc":"code","57dd5617":"code","97a68fc3":"code","87586570":"code","eedb7877":"code","73e00e8e":"code","eaf9f5ca":"code","0dbd1dff":"code","6256a691":"code","0c467892":"code","f70ff69b":"code","271e2e2c":"code","98b91199":"code","d55a6c5f":"code","ea635fc1":"code","7012762f":"code","02401631":"code","6757c52c":"code","97ed2ee7":"code","6cf1d632":"code","10fea966":"code","14773f79":"code","be570a0c":"code","bfa42944":"code","fac795a3":"code","9f0eaef7":"code","fb82c2ea":"code","f6d4a9fc":"code","64a3f367":"code","d243d358":"code","a60becc9":"code","ee8b969f":"code","f8229d88":"code","406dfbd7":"code","509d9d29":"code","7a7efef1":"code","012b6244":"code","9780ad96":"code","8d629edd":"code","b4eff985":"code","f8ce162c":"code","6dc2f0be":"code","07cf0f01":"code","bfa31f39":"code","381d598f":"code","5a619155":"code","83ae327f":"code","696adea4":"code","054169a0":"code","1def3640":"code","77660e76":"code","14ea880f":"code","69f370ec":"code","cce6ab88":"code","3eedeed1":"code","6aa1a717":"code","855ab21a":"code","77d6bbf8":"code","e5f760e0":"code","7af0de8e":"code","156ea3c0":"code","9ed21425":"code","35619cf0":"code","0a7e1bd8":"code","a9223b96":"code","e9b02fe8":"code","7b547a7c":"code","430d9342":"code","6fedb28f":"code","e92d9e87":"code","16accca5":"code","9710dee0":"code","fae77236":"code","0b76e1a6":"code","e19b6ac1":"code","f219570e":"code","34b82f84":"code","7681164e":"code","2ae7062a":"code","035b7709":"code","a97d1ce8":"code","0d27c0f8":"code","6394c5e9":"code","46bee9f0":"markdown","92d39210":"markdown","5245f619":"markdown","d9345a45":"markdown","31641ff4":"markdown","86bf7349":"markdown","ca5fbfb8":"markdown","d32f5b74":"markdown","921d8c98":"markdown"},"source":{"3b07b123":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e33b0c68":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom tqdm import tqdm\nfrom itertools import combinations \nfrom sklearn import ensemble\nfrom sklearn.linear_model import Ridge","d5c6db30":"train = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv')\ntrain_id = train['ID']\ntrain_label = train['target']\n\ntest = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv')\ntest_id = test['ID']","07c559a9":"train.head(10)","23bd0d66":"train.shape","07d7beb8":"test.shape","eff5e49c":"map_types = {}\n\nfor col in train.columns:\n    t = train[col].dtype\n    if t in map_types:\n        li = map_types[t]\n        li.append(col)    \n        map_types[t] = li\n    else:\n        map_types[t] = [col]\n\nfor key, value in map_types.items():\n    print(\"\\ntype {} = {}\".format(key, value))","3ebcdebb":"numeric_features = list(train._get_numeric_data().columns)\ncategorical_features = list(set(train.columns) - set(numeric_features))\nnumeric_features.remove(\"target\")","3525cca0":"len(categorical_features)","7a0d79cc":"print(list(numeric_features))","57dd5617":"len(numeric_features)","97a68fc3":"def show_info(df):\n    for col in df.columns:\n        print(\"Name: {} | Type: {} | Missing: {} - {}% | Unique values: {}\".format(col,\n                                                      df[col].dtype, \n                                                      df[col].isna().sum(), \n                                                      df[col].isna().sum() * 100 \/ len(df), \n                                                      len(df[col].value_counts())))","87586570":"show_info(train[numeric_features])","eedb7877":"show_info(test[numeric_features])","73e00e8e":"show_info(train[categorical_features]) ","eaf9f5ca":"show_info(test[categorical_features])","0dbd1dff":"int_types = ['v38', 'v62', 'v72', 'v129']\ntrain[int_types].describe().T","6256a691":"plt.figure(figsize=(26, 24))\n\nfor i, col in enumerate(int_types):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col] * 10)\n    plt.title(col)","0c467892":"train[int_types].head()","f70ff69b":"temp = int_types + ['target']\n\ncor_matrix = train[temp].corr().round(2)\n# Plotting heatmap \nfig = plt.figure(figsize=(5,5));\nsns.heatmap(cor_matrix, annot=True, center=0, cmap = sns.diverging_palette(100, 10, as_cmap=True), ax=plt.subplot(111));\nplt.show()","271e2e2c":"def analise_concentracao(base, max_vol):\n    import pandas as pd\n    lista = []\n    base_log = []\n    for item in base.columns:\n        try:\n            pct_max = max(base[item].value_counts())\/base[item].count()\n        except:\n            pct_max = 0\n        if pct_max > max_vol:\n            lista.append(item)\n            base_log.append([item,pct_max])\n    base_log = pd.DataFrame(base_log, columns = ['VARIAVEL','CONC_MAX']).sort_values(  ['CONC_MAX'],ascending = False)\n    return lista ","98b91199":"colunas_concentradas = analise_concentracao(train.drop([\"ID\", \"target\"], axis=1), 0.95)","d55a6c5f":"colunas_concentradas","ea635fc1":"train[colunas_concentradas].describe().T","7012762f":"plt.figure(figsize=(26, 24))\n\nfor i, col in enumerate(['v38']):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col] * 10)\n    plt.title(col)","02401631":"temp = ['v38', 'target']\n\ncor_matrix = train[temp].corr().round(2)\n# Plotting heatmap \nfig = plt.figure(figsize=(5,5));\nsns.heatmap(cor_matrix, annot=True, center=0, cmap = sns.diverging_palette(50, 10, as_cmap=True), ax=plt.subplot(111));\nplt.show()","6757c52c":"plt.figure(figsize=(26, 24))\n\nexclude = ['v32', 'v33', 'v34', 'v65', 'v67', 'v68', 'v101', 'v102', 'v103']\n\nfor i, col in enumerate(numeric_features[0:30]):\n    if col not in exclude:\n        plt.subplot(7, 4, i + 1)\n        plt.hist(train[col] * 10)\n        plt.title(col)","97ed2ee7":"plt.figure(figsize=(26, 24))\n\nfor i, col in enumerate(numeric_features[30:60]):\n    if col not in exclude:\n        plt.subplot(7, 4, i + 1)\n        plt.hist(train[col] * 10)\n        plt.title(col)","6cf1d632":"plt.figure(figsize=(26, 24))\n\nfor i, col in enumerate(numeric_features[60:90]):\n    if col not in exclude:\n        plt.subplot(7, 4, i + 1)\n        plt.hist(train[col] * 10)\n        plt.title(col)","10fea966":"plt.figure(figsize=(26, 24))\n\nfor i, col in enumerate(numeric_features[90:115]):\n    if col not in exclude:\n        plt.subplot(7, 4, i + 1)\n        plt.hist(train[col] * 10)\n        plt.title(col)","14773f79":"train[categorical_features].head(10)","be570a0c":"train[categorical_features].describe().T","bfa42944":"sns.countplot(train.target);\nplt.xlabel('PredictedProb');\nplt.ylabel('Ocorrencias');\nplt.show()","fac795a3":"def chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]","9f0eaef7":"for num_f in chunks(numeric_features, 20):\n    num_f_list = list(num_f)\n    num_f_target = num_f_list + [\"target\"]\n    cor_matrix = train[num_f_target].corr().round(2)\n    # Plotting heatmap \n    fig = plt.figure(figsize=(18,18));\n    sns.heatmap(cor_matrix, annot=True, center=0, cmap = sns.diverging_palette(250, 10, as_cmap=True), ax=plt.subplot(111));\n    plt.show()","fb82c2ea":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs_90 = corrs[corrs[0] >= 0.90]\ncorrs_70_90 = corrs[(corrs[0] >= 0.70) & (corrs[0] < 0.90)]\ncorrs_50_70 = corrs[(corrs[0] >= 0.50) & (corrs[0] < 0.70)]\ncorrs_30_50 = corrs[(corrs[0] >= 0.30) & (corrs[0] < 0.50)]\ncorrs_30 = corrs[corrs[0] < 0.30]","f6d4a9fc":"print(len(corrs_90)\/2)\ncorrs_90.tail(30)","64a3f367":"for i in ['v114', 'v25', 'v8', 'v92', 'v116', 'v26', 'v63', 'v95', 'v43', 'v48', 'v60', 'v17', 'v46', 'v69', 'v58']:\n    print(i)\n    train = train.drop(i, axis=1)\n    test = test.drop(i, axis=1)\n    \nnumeric_features = train._get_numeric_data().columns\ncategorical_features = list(set(train.columns) - set(numeric_features))\n\nprint(categorical_features)","d243d358":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs_90 = corrs[corrs[0] >= 0.90]\ncorrs_70_90 = corrs[(corrs[0] >= 0.70) & (corrs[0] < 0.90)]\ncorrs_50_70 = corrs[(corrs[0] >= 0.50) & (corrs[0] < 0.70)]\ncorrs_30_50 = corrs[(corrs[0] >= 0.30) & (corrs[0] < 0.50)]\ncorrs_30 = corrs[corrs[0] < 0.30]","a60becc9":"print(len(corrs_90)\/2)\ncorrs_90.tail(30)","ee8b969f":"print(len(corrs_70_90)\/2)\ncorrs_70_90.tail(10)","f8229d88":"print(len(corrs_50_70)\/2)\ncorrs_50_70.tail(10)","406dfbd7":"print(len(corrs_30_50)\/2)\ncorrs_30_50.tail(10)","509d9d29":"print(len(corrs_30)\/2)\ncorrs_30.tail(10)","7a7efef1":"for c in categorical_features:\n    print(train[c].value_counts())","012b6244":"for c in categorical_features:\n    print(c)\n    le = LabelEncoder()\n    train[c] = train[c].fillna(\"missing\")\n    test[c] = test[c].fillna(\"missing\")\n    le.fit(list(train[c]) + list(test[c]))\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])","9780ad96":"train[categorical_features].describe().T","8d629edd":"corrs = train[categorical_features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","b4eff985":"plt.figure(figsize=(26, 24))\n\nfor i, col in enumerate(categorical_features):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","f8ce162c":"train = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv')\ntrain_id = train['ID']\ntrain_label = train['target']\n\ntest = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv')\ntest_id = test['ID']","6dc2f0be":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","07cf0f01":"train['v71_v75'] = train['v71'] + train['v75'] \ntest['v71_v75'] = test['v71'] + test['v75'] \ntrain = train.drop(['v71', 'v75'], axis=1)\ntest = test.drop(['v71', 'v75'], axis=1)","bfa31f39":"for i in ['v114', 'v25', 'v8', 'v92', 'v116', 'v26', 'v63', 'v95', 'v43', 'v48', 'v60', 'v17', 'v46', 'v69', 'v58']:\n    train = train.drop(i, axis=1)\n    test = test.drop(i, axis=1)","381d598f":"categorical_features_drop = ['v110', 'v125']\ntrain = train.drop(categorical_features_drop, axis=1)\ntest = test.drop(categorical_features_drop, axis=1)","5a619155":"numeric_features = list(train._get_numeric_data().columns)\ncategorical_features = list(set(train.columns) - set(numeric_features))\nprint(categorical_features)\n\nnumeric_features.remove(\"ID\")\nnumeric_features.remove(\"target\")","83ae327f":"for col in numeric_features:\n    train[col] = np.round(train[col], 4)\n    test[col] = np.round(test[col], 4)","696adea4":"numeric_features = list(train._get_numeric_data().columns)\ncategorical_features = list(set(train.columns) - set(numeric_features))\nprint(categorical_features)\nnumeric_features.remove(\"ID\")\nnumeric_features.remove(\"target\")","054169a0":"def mapper_alphabetic(value):\n    if len(value)==1:\n        return ord(value) - ord(\"A\") + 1\n    else:\n        return 0 ","1def3640":"for col in ['v66', 'v79', 'v47', 'v74', 'v24', 'v107', 'v3', 'v31', 'v52', 'v30', 'v91']:\n    train[col] = train[col].fillna('').apply(mapper_alphabetic)\n    test[col] = test[col].fillna('').apply(mapper_alphabetic)\n    categorical_features.remove(col)","77660e76":"for c in tqdm(categorical_features):\n    uniq = len(train[c].unique())\n    if uniq > 120:\n        trn, sub = target_encode(train[c], \n                         test[c], \n                         target=train.target, \n                         min_samples_leaf=100,    \n                         smoothing=10,\n                         noise_level=0.1)\n\n        train[c] = trn\n        test[c] = sub   \n    else:\n        le = LabelEncoder()\n        train[c] = train[c].fillna(\"missing\")\n        test[c] = test[c].fillna(\"missing\")\n        le.fit(list(train[c]) + list(test[c]))\n        train[c] = le.transform(train[c])\n        test[c] = le.transform(test[c])","14ea880f":"def build_categ3(df, v1, v2, v3):\n    df[v1 + v2 + v3] = df[v1].fillna(\"_\").astype(str) + df[v2].fillna(\"_\").astype(str) + df[v3].fillna(\"_\").astype(str)\n    return df\n\ncolumns = categorical_features\n\ncomb = combinations(columns, 2) \n\nscore = dict()\n\nfor v1 in ['v22']:\n    for v2 in comb:\n        name = v1 + v2[0] + v2[1]\n        if 'v22' in v2:\n            continue\n        \n        if name in list(train.columns):\n            continue\n                      \n        train = build_categ3(train, v1, v2[0], v2[1])\n        test = build_categ3(test, v1, v2[0], v2[1])\n                \n        logreg = Ridge()\n        logreg.fit(train[[v1, v2[0], v2[1]]], train_label)\n        y_pred=logreg.predict(train[[v1, v2[0], v2[1]]])\n                         \n        score[name] = log_loss(train_label, y_pred)\n        print(name, \" = \", score[name])","69f370ec":"new_categorical_features = []\n\na1_sorted_keys = sorted(score, key=score.get, reverse=False)\nfor index, r in enumerate(a1_sorted_keys):\n    print(r, score[r])\n    if index >= 10:\n        train = train.drop([r], axis=1)\n        test = test.drop([r], axis=1)\n    else:\n        new_categorical_features.append(r)","cce6ab88":"numeric_features = list(train._get_numeric_data().columns)\ncategorical_features = list(set(train.columns) - set(numeric_features))\nprint(categorical_features)\nnumeric_features.remove(\"ID\")\nnumeric_features.remove(\"target\")","3eedeed1":"for c in train.columns:\n    print(c, \"=\", len(train[c].unique()))","6aa1a717":"for c in tqdm(new_categorical_features):\n    uniq = len(train[c].unique())\n    if uniq > 120:\n        trn, sub = target_encode(train[c], \n                         test[c], \n                         target=train.target, \n                         min_samples_leaf=100,    \n                         smoothing=10,\n                         noise_level=0.1)\n\n        train[c] = trn\n        test[c] = sub   \n    else:\n        le = LabelEncoder()\n        train[c] = train[c].fillna(\"missing\")\n        test[c] = test[c].fillna(\"missing\")\n        le.fit(list(train[c]) + list(test[c]))\n        train[c] = le.transform(train[c])\n        test[c] = le.transform(test[c])","855ab21a":"train.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","77d6bbf8":"train.head()","e5f760e0":"list(train.columns)","7af0de8e":"y = train['target'].values\ndrop_feature = [\n    'ID',\n    'target'\n]\n\ntrain = train.drop(drop_feature,axis=1)\ntest = test.drop(['ID'],axis=1)\n\nprint(train.shape)\nprint(test.shape)","156ea3c0":"X = train\nX_test = test","9ed21425":"NFOLDS = 5\nkfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)","35619cf0":"final_cv_pred = np.zeros(len(test_id))\n\nSIZE = 5\n\nfor s in range(SIZE):\n            \n    clf_etc = ensemble.ExtraTreesClassifier(n_estimators=750, max_features=50, criterion='entropy', min_samples_split=4,\n                        max_depth=35, min_samples_leaf=2, n_jobs=-1, random_state=s, verbose=1)            \n            \n    clf_etc.fit(X, train_label)\n            \n    cv_pred_etc = clf_etc.predict_proba(X_test)[:,1]\n            \n    final_cv_pred += cv_pred_etc\n    \nfinal_cv_pred \/= SIZE","0a7e1bd8":"submission1 = pd.DataFrame({'id': test_id, 'PredictedProb': final_cv_pred})","a9223b96":"plt.subplot(1, 2, 1)\n(submission1[\"PredictedProb\"]).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,1])\nplt.xlabel('PredictedProb+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('PredictedProb Distribution - Test Set', fontsize=17)\nplt.show()","e9b02fe8":"train = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_treino.csv')\ntrain_id = train['ID']\ntrain_label = train['target']\n\ntest = pd.read_csv('\/kaggle\/input\/competicao-dsa-machine-learning-dec-2019\/dataset_teste.csv')\ntest_id = test['ID']","7b547a7c":"numeric_features = train._get_numeric_data().columns\ncategorical_features = list(set(train.columns) - set(numeric_features))\n\nprint(categorical_features)","430d9342":"for i in ['v3', 'v125']:\n    train = train.drop(i, axis=1)\n    test = test.drop(i, axis=1)\n    \nnumeric_features = train._get_numeric_data().columns\ncategorical_features = list(set(train.columns) - set(numeric_features))\n\nprint(categorical_features)","6fedb28f":"for i in ['v115', 'v71', 'v56', 'v22', 'v113']:\n    print(i)\n    train = train.drop(i, axis=1)\n    test = test.drop(i, axis=1)\n    \nnumeric_features = train._get_numeric_data().columns\ncategorical_features = list(set(train.columns) - set(numeric_features))\n\nprint(categorical_features)","e92d9e87":"for c in categorical_features:\n    le = LabelEncoder()\n    train[c] = train[c].fillna(\"missing\")\n    test[c] = test[c].fillna(\"missing\")\n    le.fit(list(train[c]))\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])","16accca5":"if 'v22' in categorical_features:\n    categorical_features.remove('v22')\n    train = train.drop('v22', axis=1)\n    test = test.drop('v22', axis=1) \n    \nfor c in categorical_features:\n    print(\"feature = \" + c)\n    enc = OneHotEncoder(sparse=False, categories='auto')\n    all = pd.concat([train[[c]], test[[c]]], axis=0)\n    enc.fit(all)\n    X = enc.transform(train[[c]])\n    train_cat = pd.DataFrame(X)\n    train_cat = train_cat.add_prefix('ONE_{}_'.format(c))\n\n    test_cat = pd.DataFrame(enc.transform(test[[c]]))\n    test_cat = test_cat.add_prefix('ONE_{}_'.format(c))\n\n    train = pd.concat([train, train_cat], axis=1)\n    test = pd.concat([test, test_cat], axis=1)\n\nfor i in categorical_features:\n    train = train.drop(i, axis=1)\n    test = test.drop(i, axis=1)","9710dee0":"cols = list(numeric_features)\ncols.remove(\"ID\")\ncols.remove(\"target\")\n\nfor feat in cols:\n    train[feat] = np.round(train[feat], 2)\n    test[feat] = np.round(test[feat], 2)","fae77236":"y = train['target'].values\ndrop_feature = [\n    'ID',\n    'target'\n]\n\ntrain = train.drop(drop_feature,axis=1)\ntest = test.drop(['ID'],axis=1)\n\nprint(train.shape)\nprint(test.shape)","0b76e1a6":"X = train\nX_test = test","e19b6ac1":"learning_rate = 0.1\nnum_leaves = 30\nfeature_fraction = 0.6\nnum_boost_round = 10000","f219570e":"params = {\"objective\": \"binary\",\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": learning_rate,\n          \"num_leaves\": num_leaves,\n          \"max_bin\": 256,\n          \"feature_fraction\": feature_fraction,\n          \"verbosity\": 0,\n          \"drop_rate\": 0.1,\n          \"is_unbalance\": False,\n          \"max_drop\": 50,\n          \"min_child_samples\": 10,\n          \"min_child_weight\": 30,\n          \"min_split_gain\": 0,\n          \"subsample\": 0.9\n          }","34b82f84":"NFOLDS = 5\nkfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)","7681164e":"x_score = []\n\nfinal_cv_train = np.zeros(len(train_label))\nfinal_cv_pred = np.zeros(len(test_id))\n\ncv_only = True\n\nSIZE = 12\n\nfor s in range(SIZE):\n    cv_train = np.zeros(len(train_label))\n    cv_pred = np.zeros(len(test_id))\n\n    params['seed'] = s\n\n    if cv_only:\n        kf = kfold.split(X, train_label)\n\n        best_trees = []\n        fold_scores = []\n\n        for i, (train_fold, validate) in enumerate(kf):\n            X_train, X_validate, label_train, label_validate = \\\n                X.iloc[train_fold], X.iloc[validate], train_label[train_fold], train_label[validate]\n            dtrain = lgbm.Dataset(X_train, label_train)\n            dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n            bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, verbose_eval=100,\n                            early_stopping_rounds=100)\n            best_trees.append(bst.best_iteration)\n            cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n            cv_train[validate] += bst.predict(X_validate)\n            \n            score = log_loss(label_validate, cv_train[validate])\n            \n            print(score)\n            fold_scores.append(score)\n\n        cv_pred \/= NFOLDS\n        final_cv_train += cv_train\n        final_cv_pred += cv_pred\n\n        print(\"cv score:\")\n        print(log_loss(train_label, cv_train))\n        print(\"current score:\", log_loss(train_label, final_cv_train \/ (s + 1.)), s+1)\n        print(fold_scores)\n        print(best_trees, np.mean(best_trees))\n\n        x_score.append(log_loss(train_label, cv_train))\n        \nprint(x_score)","2ae7062a":"submission2 = pd.DataFrame({'id': test_id, 'PredictedProb': final_cv_pred \/ SIZE})","035b7709":"plt.subplot(1, 2, 1)\n(submission2[\"PredictedProb\"]).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,1])\nplt.xlabel('PredictedProb+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('PredictedProb Distribution - Test Set', fontsize=17)\nplt.show()","a97d1ce8":"submission_final = pd.DataFrame({'id': test_id})\nsubmission_final['PredictedProb'] = submission1['PredictedProb'] * 0.85 + submission2['PredictedProb'] * 0.15\nsubmission_final.to_csv('submission.csv', index=False)","0d27c0f8":"pd_corr = pd.DataFrame({'id': test_id})\npd_corr['PredictedProb1'] = submission1['PredictedProb']\npd_corr['PredictedProb2'] = submission2['PredictedProb']\npd_corr.corr()","6394c5e9":"plt.subplot(1, 2, 1)\n(submission_final[\"PredictedProb\"]).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,1])\nplt.xlabel('PredictedProb+', fontsize=17)\nplt.ylabel('frequency', fontsize=17)\nplt.tick_params(labelsize=15)\nplt.title('PredictedProb Distribution - Test Set', fontsize=17)\nplt.show()","46bee9f0":"# MODELO 2","92d39210":"# V71 e V75 s\u00e3o altamente correlacionadas. Unindo","5245f619":"# Removendo variaveis com correla\u00e7\u00e3o >= 99%","d9345a45":"# MODELO 1","31641ff4":"Removendo vari\u00e1veis com alta concentra\u00e7\u00e3o de valores","86bf7349":"# Escolhendo as variaveis combinadas de melhor score (via Ridge)","ca5fbfb8":"> Analisando os tipos int: ['v38', 'v62', 'v72', 'v129']\n","d32f5b74":"> Carregando as bases","921d8c98":"# MEDIA DOS MODELOS"}}