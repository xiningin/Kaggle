{"cell_type":{"813ab83b":"code","1c59b3b2":"code","eba77e02":"code","073b921a":"code","8cc9c7a0":"code","180525ee":"code","8f688f24":"code","109e9f94":"code","c29b7683":"code","800efd26":"code","45d08909":"code","35268633":"code","5a213408":"code","86ebe063":"code","f716f240":"code","6e62c561":"code","ddf4eea0":"code","5f42a981":"code","aa76c58d":"code","0319a101":"code","67c4739c":"code","da929676":"code","38ebbca1":"code","6a962a95":"code","2a4668cf":"code","0dcbd11d":"code","d3893741":"code","57ea4614":"code","f29f6153":"code","7f7af458":"code","c76e76d7":"code","bfa3b719":"code","8b23b363":"code","185bf6a9":"code","a425e1c9":"code","3de3da0c":"code","da68259f":"markdown","8dd2b074":"markdown","1729c905":"markdown","2d8399e5":"markdown","dfa757c6":"markdown","e5d12626":"markdown","f457e37e":"markdown","58ec7fe3":"markdown","91c8ef7a":"markdown"},"source":{"813ab83b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# https:\/\/archive.ics.uci.edu\/ml\/datasets\/Communities%2Band%2BCrime\n# Description of this dataset\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nprint('Its alive btw')\n# Any results you write to the current directory are saved as output.","1c59b3b2":"import chardet\ncities = pd.read_json('\/kaggle\/input\/cities.json')\nwith open('\/kaggle\/input\/crimedata.csv', 'rb') as t:\n    res = chardet.detect(t.read())\nres","eba77e02":"df = pd.read_csv('\/kaggle\/input\/crimedata.csv', encoding=res['encoding'])\ndf.columns = ['communityname'] + list(df.columns[1:])\ndf = df.replace(to_replace='?', value=np.nan)","073b921a":"nan_columns = []\nfor col in df.columns:\n    if df[col].isna().sum() > 0.0:\n        nan_columns.append(col)\n        print('dtype={1} uniques={2} nans_percent={3} {0}'.format(col, df[col].dtype, len(df[col].unique()), 100*df[col].isna().sum()\/df.shape[0]))","8cc9c7a0":"for col in nan_columns:\n    try:\n        df.loc[~df[col].isna(), col] = df[~df[col].isna()][col].astype('int')\n    except Exception as e:\n        df.loc[~df[col].isna(), col] = df[~df[col].isna()][col].astype('float')","180525ee":"# Let's make some magic graphics!\n\nimport cartopy.crs as ccrs\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(16,16))\nax=plt.axes(projection=ccrs.PlateCarree())\nax.coastlines()\nplt.show()","8f688f24":"ax=plt.axes(projection=ccrs.Orthographic())\nax.stock_img()\nplt.show()","109e9f94":"# tutorial: https:\/\/data-flair.training\/blogs\/python-geographic-maps-graph-data\/\n%matplotlib inline\nax=plt.axes(projection=ccrs.AlbersEqualArea())\nax.stock_img()\nny_lon, ny_lat=-74.0059413, 40.7127837\ngreen_lon, green_lat=-77.3664, 35.6127\n#plt.figure(figsize=(26,26))\nax.scatter([ny_lon, green_lon], [ny_lat, green_lat], transform=ccrs.Geodetic())\nplt.show()","c29b7683":"import cartopy.feature as fs\n\nax = plt.figure(figsize=(20,20)).add_subplot(1,1,1,projection=ccrs.PlateCarree())\nax.set_extent((-140,-60,10,60))#left right bottom top\nax.add_feature(fs.BORDERS)\nax.add_feature(fs.COASTLINE)\nax.add_feature(fs.RIVERS)\nax.scatter(cities['longitude'], cities['latitude'], s=4, color='red')\nax.stock_img()\nplt.show()","800efd26":"df.head()","45d08909":"print(df['ViolentCrimesPerPop'].min(),\n      df['ViolentCrimesPerPop'].max(),\n      df['ViolentCrimesPerPop'].mean(),\n      df['ViolentCrimesPerPop'].std(),\n      df['ViolentCrimesPerPop'].median())","35268633":"plt.figure(figsize=(13,7))\nplt.hist(df['ViolentCrimesPerPop'].fillna(-1), bins=100)\nplt.show()","5a213408":"ax = plt.figure(figsize=(20,20)).add_subplot(1,1,1,projection=ccrs.PlateCarree())\nax.set_extent((-140,-60,10,60))#left right bottom top\nax.add_feature(fs.BORDERS)\nax.add_feature(fs.COASTLINE)\nax.add_feature(fs.RIVERS)\nax.add_feature(fs.LAKES)\nax.scatter(cities['longitude'], cities['latitude'], s=df['ViolentCrimesPerPop'].fillna(0)\/30, color='red')\nax.stock_img()\nplt.show()","86ebe063":"ax = plt.figure(figsize=(20,20)).add_subplot(1,1,1,projection=ccrs.PlateCarree())\nax.set_extent((-140,-60,10,60))#left right bottom top\nax.add_feature(fs.BORDERS)\nax.add_feature(fs.COASTLINE)\nax.add_feature(fs.RIVERS)\nax.add_feature(fs.LAKES)\ntop10crimes = df[~df['ViolentCrimesPerPop'].isna()]['ViolentCrimesPerPop'].sort_values()[-10:]\ntop10cities = df.iloc[top10crimes.index, 0]\n# removing last 4 letters (Citynamecity --> Cityname)\ntop10cities = list(map(lambda x: x[:-4], top10cities))\n# collecting coordinates of each city\ntop10longitude = pd.Series(); top10latitude = pd.Series()\nfor i, city in enumerate(top10cities):\n    lon = cities[cities['city'] == city]['longitude']\n    lat = cities[cities['city'] == city]['latitude']\n    top10longitude = top10longitude.append(lon)\n    top10latitude = top10latitude.append(lat)\n    ax.text(lon-1+0.2*i, lat+0.1*i, city, transform=ccrs.Geodetic())\nax.scatter(top10longitude, top10latitude, s=30, color='red')\nax.stock_img()\nplt.show()","f716f240":"# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c, \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u043b\u0438 \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430\u043d\u043e\u0432 \u0441 \u0442\u0435\u043c, \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0435 \u0441\u043e\u0431\u0438\u0440\u0430\u043b\u0438\u0441\u044c \u043f\u043e \u0448\u0442\u0430\u0442\u0430\u043c, \u0430 \u043d\u0435 \u043f\u043e \u0433\u043e\u0440\u043e\u0434\u0430\u043c\ndf.head()","6e62c561":"# checking if percentage of nans related to that data was collected by states but not by cities\ntoo_nan = [col for col in df.columns if df[col].isna().sum()\/df.shape[0] > 0.5]\nfor state in df['state'].unique():\n    for col in too_nan:\n        size = df[df['state'] == state][col].shape[0]\n        n_notnan = size - df[df['state'] == state][col].isna().sum()\n        uniques = df[df['state'] == state][col].unique().shape[0]\n        print('Size of state: {0:3.0f}, not NaNs: {1:3.0f}, unique values in current column: {2:3.0f}'.format(size, n_notnan, uniques))\n\n# so if this was true there should be only one not-nan","ddf4eea0":"df.shape","5f42a981":"df_dropped = df.dropna(axis=1, thresh=1400) # drop columns with more than 54% nan values\ndf_dropped.shape","aa76c58d":"for col in df_dropped.columns:\n    try:\n        filler = df_dropped[col].describe()['top']\n    except:\n        filler = df_dropped[col].describe()['mean']\n    df_dropped.loc[:,col] = df_dropped.fillna(filler)","0319a101":"df_dropped.head()","67c4739c":"target = df_dropped['ViolentCrimesPerPop']\ndf_dropped = df_dropped.drop(columns=['communityname', 'state', 'fold', 'ViolentCrimesPerPop', 'nonViolPerPop'])","da929676":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV as rscv\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_dropped, target, random_state=32)\n\nmodel = GradientBoostingRegressor()\nparam_grid = {'learning_rate':[0.001, 0.01, 0.1], 'n_estimators':[50,100,200], 'max_depth':[2,4,7]}\ncv = rscv(model, param_grid, n_iter=6, verbose=1).fit(X_train, y_train)\n\n","38ebbca1":"cv.score(X_train, y_train), cv.score(X_test, y_test)","6a962a95":"df_filled = df.copy()\nfor col in df_filled.columns:\n    try:\n        filler = df_filled[col].describe()['mean']\n    except:\n        filler = df_filled[col].describe()['top']\n    df_filled.loc[:,col] = df_filled.fillna(filler)\ndf_filled.head()","2a4668cf":"target = df_filled['ViolentCrimesPerPop']\ndf_filled = df_filled.drop(columns=['communityname', 'state', 'fold', 'ViolentCrimesPerPop', 'nonViolPerPop'])","0dcbd11d":"X_train, X_test, y_train, y_test = train_test_split(df_filled, target, random_state=32)\n\nmodel = GradientBoostingRegressor()\nparam_grid = {'learning_rate':[0.001, 0.01, 0.1], 'n_estimators':[50,100,200], 'max_depth':[2,4,7]}\ncv = rscv(model, param_grid, n_iter=6, verbose=1).fit(X_train, y_train)","d3893741":"cv.score(X_train, y_train), cv.score(X_test, y_test)","57ea4614":"cv.best_estimator_","f29f6153":"model = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n                          learning_rate=0.1, loss='ls', max_depth=2,\n                          max_features=None, max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=1, min_samples_split=2,\n                          min_weight_fraction_leaf=0.0, n_estimators=100,\n                          n_iter_no_change=None, presort='auto',\n                          random_state=None, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\nmodel.fit(X_train, y_train)\nprint(model.score(X_train, y_train), model.score(X_test, y_test))","7f7af458":"imps = pd.Series(model.feature_importances_)\nfeas = pd.Series(df.drop(columns=['communityname', 'state', 'fold', 'ViolentCrimesPerPop', 'nonViolPerPop']).columns)\nFeasImps = pd.DataFrame(columns=['features', 'importances', 'importances by %'])\nFeasImps['features'] = feas\nFeasImps['importances'] = imps\nFeasImps['importances by %'] = np.array(imps)*100","c76e76d7":"rapes_imp = FeasImps.iloc[129, 1]\nkids_imp = FeasImps[FeasImps['features'] == 'PctKidsBornNeverMar']['importances'].values[0]\nburgl_imp = FeasImps[FeasImps['features'] == 'burglPerPop']['importances'].values[0]\nassault_imp = FeasImps[FeasImps['features'] == 'assaultPerPop']['importances'].values[0]\nFeasImps.sort_values('importances', ascending=False).head(4)","bfa3b719":"corra = []\ncrimes = pd.Series(target)\nfor col in df_filled.columns:\n    try:\n        corra.append(crimes.corr(df_filled[col]))\n    except Exception as e:\n        corra.append(0)\ncorrs = pd.DataFrame(columns=['features', 'correlation'])\ncorrs['features'] = pd.Series(df_filled.columns)\ncorrs['correlation'] = pd.Series(corra)","8b23b363":"crimes.corr(df_filled[df_filled.columns[5]])","185bf6a9":"kids_corr = corrs.iloc[52, 1]\nburgl_corr = corrs.iloc[135, 1]\nassault_corr = corrs.iloc[133, 1]\nrapes_corr = corrs.iloc[129, 1]\ncorrs.sort_values('correlation', ascending=False).head(4)","a425e1c9":"ax = plt.figure(figsize=(20,20)).add_subplot(1,1,1,projection=ccrs.PlateCarree())\nax.set_extent((-140,-60,10,60))#left right bottom top\nax.add_feature(fs.BORDERS)\nax.add_feature(fs.COASTLINE)\nax.add_feature(fs.RIVERS)\n#ax.add_feature(fs.LAKES)\n\ndef find_top_coords(inces):\n    city_list = df.iloc[inces, 0]\n    city_list = list(map(lambda x: x[:-4], city_list))\n    longitude=pd.Series(); latitude=pd.Series()\n    for city in city_list:\n        lon = cities[cities['city'] == city]['longitude']\n        lat = cities[cities['city'] == city]['latitude']\n        longitude = longitude.append(lon)\n        latitude = latitude.append(lat)\n    return longitude, latitude\n\nssize=50\n\n# cities\ntop10crimes = df[~df['ViolentCrimesPerPop'].isna()]['ViolentCrimesPerPop'].sort_values()[-10:]\ntop10crimes_lon, top10crimes_lat = find_top_coords(top10crimes.index)\nax.scatter(top10crimes_lon, top10crimes_lat, s=ssize, c=np.array([99,19,19])\/255, label='Cities')\n\n# kids\ntop10kids = df[~df['PctKidsBornNeverMar'].isna()]['PctKidsBornNeverMar'].sort_values(ascending=False)[:10]\ntop10kids_lon, top10kids_lat = find_top_coords(top10kids.index)\nax.scatter(top10kids_lon, top10kids_lat, s=ssize, c=np.array([171,32,32])\/255,\n           label='Kids {0:.2f} - correlation, {1:.2f} - importance'.format(kids_corr, kids_imp))\n\n# rapes\ntop10rapes = df[~df['rapesPerPop'].isna()]['rapesPerPop'].sort_values()[-10:]\ntop10rapes_lon, top10rapes_lat = find_top_coords(top10rapes.index)\nax.scatter(top10rapes_lon, top10rapes_lat, s=ssize, c=np.array([201,137,40])\/255,\n           label='Rapes {0:.2f} - correlation, {1:.2f} - importance'.format(rapes_imp, rapes_imp))\n\n# burglaries\ntop10burgl = df[~df['burglPerPop'].isna()]['burglPerPop'].sort_values()[-10:]\ntop10burgl_lon, top10burgl_lat = find_top_coords(top10burgl.index)\nax.scatter(top10burgl_lon, top10burgl_lat, s=ssize, c=np.array([184,201,32])\/255,\n           label='Burglaries {0:.2f} - correlation, {1:.2f} - importance'.format(burgl_corr, burgl_imp))\n\n# assaults\ntop10assault = df[~df['assaultPerPop'].isna()]['assaultPerPop'].sort_values()[-10:]\ntop10assault_lon, top10assault_lat = find_top_coords(top10assault.index)\nax.scatter(top10assault_lon, top10assault_lat, s=ssize, c=np.array([77,166,255])\/255,\n           label='Assaults {0:.2f} - correlation, {1:.2f} - importance'.format(assault_corr, assault_imp))\n\nax.stock_img()\nplt.legend()\nplt.show()","3de3da0c":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\ncolors=['rgb(99,19,19)', 'rgb(171,32,32)', 'rgb(201,137,40)', 'rgb(184,201,32)', 'rgb(210,242,0)']\n\ndef merge_labels(name, inces):\n    labels=[]\n    cities = df.iloc[inces,0]\n    states = df.iloc[inces,1]\n    for city, state in zip(cities, states):\n        labels.append(name+' in '+str(city[:-4])+', '+str(state))\n    return labels\n\nstats=((top10crimes_lon, top10crimes_lat, top10crimes, merge_labels('Violent crimes', top10crimes.index), 'Violent crimes'),\n        (top10kids_lon, top10kids_lat, top10kids, merge_labels('Kids', top10kids.index), 'Kids born to never married'),\n        (top10rapes_lon, top10rapes_lat, top10rapes, merge_labels('Rapes', top10rapes.index), 'Rapes'),\n        (top10burgl_lon, top10burgl_lat, top10burgl, merge_labels('Burglaries', top10burgl.index), 'Burglaries'),\n        (top10assault_lon, top10assault_lat, top10assault, merge_labels('Assaults', top10assault.index), 'Assaults'))\n\nfor i, stat in enumerate(stats):\n    fig.add_trace(go.Scattergeo(\n        lon = stat[0], lat = stat[1],\n        #text = stat[2].apply(lambda x: round(x, 3)),\n        text = pd.Series([str(v)+' '+str(d) for v,d in zip(stat[2], stat[3])]),\n        name = stat[4],\n        marker = dict(\n            size = list(20*stat[2].values\/stat[2].max()),\n            color=colors[i],\n            line_width=0\n        )\n    ))\n    \n\nfig.update_layout(\n    title_text = 'Interactive map demonstraiting Violent Crimes and it\\'s reasons (all values calculated per 100k)',\n    geo=dict(\n        resolution=50,\n        scope='north america',\n        showframe=True,\n        showcoastlines=True,\n        showland=True,\n        landcolor='darkgray',\n        countrycolor = 'black',\n        coastlinecolor='black',\n        #projection_type='equirectangular', #-makes a map  more compressed\n        lonaxis_range=[ -120.0, -65.0],\n        lataxis_range=[ 25.0, 55.0],\n        domain = dict(x=[0, 1], y=[0, 1]),\n        bgcolor='rgba(173,217,255, 0.7)'\n    )\n)    \n    \nfig.show()","da68259f":"Use plot.ly to add some interactive<br>\n<a href=\"https:\/\/plot.ly\/python\/choropleth-maps\/\">Example<\/a><br>\nIn every of top 10 cities there rapesPerPop, burglPerPop, assaultPerPop, PctKidsBornNeverMar nust be displayed","8dd2b074":"# Now lets investigate if it better to drop columns with a lot of nans than fill them\n1. drop nan columns or rows, fit the model and tune it parameters\n2. fill nans and do the same\n3. using tree-based models define main founders in ViolentCrimesPerPop\n4. draw these founders on map to make some pretty graphics","1729c905":"## 1. Dropping","2d8399e5":"Well, looks good and representative despite some cities's locations is not real. This may be caused by method which I chose to extract longitude and latitude but still purpose of this notebook is to try out new modules (cartopy and plotly geo stuff). Hope this can help you some way.","dfa757c6":"## 2. Filling","e5d12626":"## 3. Feature importances\nWell, it doesn't matter to drop it or simply fill columns\nLet's define most important features by filled model","f457e37e":"# Bonus","58ec7fe3":"# Final step\nLet's display on map top 10 cities with high rate of violent crimes, PctKidsBornNeverMar, rapesPerPop, assaultPerPop and burglPerPop","91c8ef7a":"Next features have less than 1% of impact on model so it's not neccesary to watch them\nNow let's calculate the most correlated to ViolentCrimesPerPop features"}}