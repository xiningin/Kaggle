{"cell_type":{"79560a7d":"code","4340316b":"code","0f3ca4b2":"code","ede10e0b":"code","108f6ed5":"code","eb148137":"code","fd2f868f":"code","ec0c0d5e":"code","d4e14b1e":"code","bd21829b":"code","49345683":"code","cf8e04bd":"code","9f582e8f":"code","a8798f3f":"code","1c3caf12":"code","3c0b1d30":"code","cb95417e":"code","2d30c6b4":"code","09b86bd1":"markdown","06e35fb4":"markdown","010e1987":"markdown","9511dc69":"markdown","b2578a3e":"markdown","75cc194c":"markdown","70825ab4":"markdown","620238b9":"markdown","8bc4ed12":"markdown","29061d7f":"markdown","959bfa49":"markdown","f751bbad":"markdown","2096306f":"markdown"},"source":{"79560a7d":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/deep-learning-az-ann\/Churn_Modelling.csv\")","4340316b":"# Visualizing the Dataset\ndf.head()","0f3ca4b2":"# Data columns type\ndf.info()","ede10e0b":"df=df.drop(['RowNumber','CustomerId','Surname'],axis='columns')\ndf.describe()","108f6ed5":"# 1. Check out the missing values \ndf.isnull().sum()\n\n# As you see below, we dont have any missing values, so we are moving forward and least bothered \n#about correcting the column values","eb148137":"# Correlation heatmap\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), cmap='BuGn',annot=True)","fd2f868f":"#splitting data into Train, DEV, test\nfrom sklearn.model_selection import train_test_split\ny=df.Exited # pulling values into another array so that we can drop\nX=df.drop(['Exited'],axis='columns')\nX_train, X_Dev, y_train, y_Dev = train_test_split(X,y,test_size=0.3,random_state=0,shuffle=False)\nX_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.2,random_state=0,shuffle=False)\n","ec0c0d5e":"#[Train] divide train data into categories , numerical and binary\n\nbinary_columns=[\"HasCrCard\",\"IsActiveMember\"]\nbinary_df=pd.DataFrame(X_train[binary_columns])\n\nnumerical_columns =[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]\nnumerical_df=pd.DataFrame(X_train[numerical_columns])\n\ncategory_columns=['Geography','Gender']\ncategory_df=pd.DataFrame(X_train[category_columns])\n","d4e14b1e":"#[TRAIN] Encode Categorical Data\n\ncategory_df['Geography'] = category_df['Geography'].astype('category')\ncategory_df['Gender'] = category_df['Gender'].astype('category')\ncategory_df_Final = pd.get_dummies(category_df)","bd21829b":"#[TRAIN] feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumerical_df_train_mean=numerical_df.mean()\nnumerical_df_train_std=numerical_df.std(axis=0)\nnumerical_df_scale =pd.DataFrame(scaler.fit_transform(numerical_df),columns=numerical_columns)","49345683":"# [TRAIN] Concatenate Columns\nX_train = pd.concat([numerical_df_scale, category_df_Final,binary_df], axis=1)","cf8e04bd":"#[DEV] dividing data into binary, number and category\nbinary_columns=[\"HasCrCard\",\"IsActiveMember\"]\nbinary_df=pd.DataFrame(X_Dev[binary_columns])\n\nnumerical_columns =[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]\nnumerical_df=pd.DataFrame(X_Dev[numerical_columns])\n\ncategory_columns=['Geography','Gender']\ncategory_df=pd.DataFrame(X_Dev[category_columns])\n\n# [DEV] Encode Categorical Data\ncategory_df['Geography'] = category_df['Geography'].astype('category')\ncategory_df['Gender'] = category_df['Gender'].astype('category')\ncategory_df_Final = pd.get_dummies(category_df)\n\n# [DEV] feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumerical_df[\"CreditScore\"]=(numerical_df[\"CreditScore\"]-numerical_df_train_mean[\"CreditScore\"]).div(numerical_df_train_std[\"CreditScore\"])\nnumerical_df[\"Age\"]=(numerical_df[\"Age\"]-numerical_df_train_mean[\"Age\"]).div(numerical_df_train_std[\"Age\"])\nnumerical_df[\"Tenure\"]=(numerical_df[\"Tenure\"]-numerical_df_train_mean[\"Tenure\"]).div(numerical_df_train_std[\"Tenure\"])\nnumerical_df[\"Balance\"]=(numerical_df[\"Balance\"]-numerical_df_train_mean[\"Balance\"]).div(numerical_df_train_std[\"Balance\"])\nnumerical_df[\"NumOfProducts\"]=(numerical_df[\"NumOfProducts\"]-numerical_df_train_mean[\"NumOfProducts\"]).div(numerical_df_train_std[\"NumOfProducts\"])\nnumerical_df[\"EstimatedSalary\"]=(numerical_df[\"EstimatedSalary\"]-numerical_df_train_mean[\"EstimatedSalary\"]).div(numerical_df_train_std[\"EstimatedSalary\"])\n\n#[DEV] Concatenate Columns\nX_Dev = pd.concat([numerical_df, category_df_Final,binary_df], axis=1)","9f582e8f":"# [TEST] dividing data into binary, number and category\nbinary_columns=[\"HasCrCard\",\"IsActiveMember\"]\nbinary_df=pd.DataFrame(X_test[binary_columns])\n\nnumerical_columns =[\"CreditScore\",\"Age\",\"Tenure\",\"Balance\",\"NumOfProducts\",\"EstimatedSalary\"]\nnumerical_df=pd.DataFrame(X_test[numerical_columns])\n\ncategory_columns=['Geography','Gender']\ncategory_df=pd.DataFrame(X_test[category_columns])\n\n# [TEST] Encode Categorical Data\ncategory_df['Geography'] = category_df['Geography'].astype('category')\ncategory_df['Gender'] = category_df['Gender'].astype('category')\ncategory_df_Final = pd.get_dummies(category_df)\n\n# [TEST] feature scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnumerical_df[\"CreditScore\"]=(numerical_df[\"CreditScore\"]-numerical_df_train_mean[\"CreditScore\"]).div(numerical_df_train_std[\"CreditScore\"])\nnumerical_df[\"Age\"]=(numerical_df[\"Age\"]-numerical_df_train_mean[\"Age\"]).div(numerical_df_train_std[\"Age\"])\nnumerical_df[\"Tenure\"]=(numerical_df[\"Tenure\"]-numerical_df_train_mean[\"Tenure\"]).div(numerical_df_train_std[\"Tenure\"])\nnumerical_df[\"Balance\"]=(numerical_df[\"Balance\"]-numerical_df_train_mean[\"Balance\"]).div(numerical_df_train_std[\"Balance\"])\nnumerical_df[\"NumOfProducts\"]=(numerical_df[\"NumOfProducts\"]-numerical_df_train_mean[\"NumOfProducts\"]).div(numerical_df_train_std[\"NumOfProducts\"])\nnumerical_df[\"EstimatedSalary\"]=(numerical_df[\"EstimatedSalary\"]-numerical_df_train_mean[\"EstimatedSalary\"]).div(numerical_df_train_std[\"EstimatedSalary\"])\n\n# [TEST] Concatenate Columns\nX_test = pd.concat([numerical_df, category_df_Final,binary_df], axis=1)","a8798f3f":"# assigning NULL to unused variables\ndf=None\nX=None\ny=None\nbinary_columns=None\nbinary_df=None\ncategory_df=None\ncategory_columns=None\ncategory_df_Final=None\nnumerical_df=None\nnumerical_columns=None\nnumerical_df_train_mean=None\nscaler=None\nnumerical_df_train_std=None\nnumerical_df_scale=None\nnull_columns=None","1c3caf12":"#defining and compiling model\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.optimizers import SGD\n\ndef deep_model():\n    classifier = Sequential()\n    classifier.add(Dense(units=3, kernel_initializer='he_uniform',\n                bias_initializer='ones', activation='tanh', input_dim=13))\n    classifier.add(Dense(units=3, kernel_initializer='he_uniform',\n                bias_initializer='ones', activation='tanh'))\n    classifier.add(Dense(units=2, kernel_initializer='he_uniform',\n                bias_initializer='ones', activation='tanh'))\n    #classifier.add(Dense(units=3, kernel_initializer='he_uniform',\n                #bias_initializer='ones', activation='tanh'))\n    #classifier.add(Dense(units=2, kernel_initializer='he_uniform',\n                #bias_initializer='ones', activation='relu'))\n    classifier.add(Dense(units=1,  kernel_initializer='he_uniform',\n                bias_initializer='ones', activation='sigmoid'))\n    classifier.compile(optimizer=Adam(learning_rate=0.01, amsgrad=False), \n    #classifier.compile(optimizer=SGD(learning_rate=0.001, momentum=0.8, nesterov=False), \n    loss='binary_crossentropy', \n    metrics=['accuracy','mae'])\n    return classifier","3c0b1d30":"# fitting the data \nfrom keras.models import load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nclassifier = deep_model()\n# Set callback functions to early stop training and save the best model so far\ncallbacks = [EarlyStopping(monitor='val_loss', patience=2),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\noutput=classifier.fit(X_train, y_train, batch_size=32,callbacks=callbacks ,epochs=100,validation_data=(X_Dev,y_Dev),shuffle=False)\n","cb95417e":"#plotting\nprint(output.history.keys())\nimport matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(output.history['accuracy'])\nplt.plot(output.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(output.history['loss'])\nplt.plot(output.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()","2d30c6b4":"#Calculating Errors\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_absolute_error\n\n\n#Confusion Matric Accuracy\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)*1\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(\"Confusion Matrix Accuracy: \"+ str(accuracy*100)+\"%\")\n\n#F1 score\nrecall=(cm[0][0])\/(cm[0][0]+cm[0][1])\nprecision=(cm[0][0])\/(cm[0][0]+cm[1][0])\nF1=(2*recall*precision)\/(precision+recall)\nprint(\"F1 Score:\"+str(F1))\n\n#MAE\nmae=mean_absolute_error(y_test, y_pred)\nprint(\"MAE:\"+str(mae))","09b86bd1":"Repeat the same for DEV and TEST data","06e35fb4":"**Building Model**\n\n**Training 1:**\n\nHidden Layer(s) : 3\n\nNeurons per Hidden Layer(s): 3 ,3 , 2\n\nActivation function for hidden layer (s) : tanh\n\nOptimizer :Adam\n\nLearning Rate : 0.01\n\nEpochs : 100\n\nBatch size : 32\n\nEarly stopping : True\n\nPatience \/ Tolerance : 2\n\nInitial Weights : uniform distribution within [-limit, limit] where limit is sqrt(6 \/ fan_in) where fan_in is the number of input units\n\nInitial Bias : Ones\n","010e1987":"**Importing Libraries and Dataset**","9511dc69":"We drop \u201cRowNumber\u201d,\u201cSurName\u201d and \u201cCustomerID\u201d columns as \u201cRowNumber\u201d is just a series identifier and \u201cSurName\u201d logically doesn\u2019t have any impact on leaving the bank.\n\n","b2578a3e":"**3. Look for categorial values**\n\nHere we have two categorial columns, \u201cGeography\u201d and \u201cGender\u201d. Machine Learning models deal only with numbers, so let\u2019s convert this string into integer values. We can use any of the below techniques here\n1. Label Encoding\n2. One-Hot Encoding\n\nThe limitation on label encoding is, after encoding, the values in the dataset might confuse the model as if they are somewhat sequential. In our case, both the columns are of some category type, so we would go for \u201cOne-Hot Encoding\u201d.","75cc194c":"**Data Visualization:**\n\nLet us visualize the dataset and its datatypes","70825ab4":"**2. Feature Dropping**\n\nFeatures with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features. I tried finding correlation between all features and found they don\u2019t have any high correlation.","620238b9":"**Data Pre-Processing**\n\nData pre-processing has a sequential flow and it starts as follows\n\n**1. Check out the missing values**","8bc4ed12":"Before we proceed further with building our model, it is recommended to split data into train , DEV and test first and then apply further pre-processing steps on each dataset separately. \n\nThe reason being, lets say, we might have to normalize or standardize our data, if we standardize the complete dataset and then split it, the test dataset might have the **mean** and **standard deviation** of training dataset as well. This might not give us accurate results and our test data already has a sense of our training data and our model starts to overfit.\n\nSo here, we are dividing the dataset into 3 categories. The purpose of dividing is, we validate the accuracy against \u201cDEV\u201d set after each epoch to understand whether the model overfits or underfits. Based on the results,we further tune it and run against test data. This will help us in generating a good model.","29061d7f":"**Objective:**\n\nRick works as Head of Southern TD Canada Bank. Huge number of customers had to leave the bank due to no proper services from Bank. Rick had a hard time solving internal conflicts, streamlining the process and bringing back customers to the bank. He now wants to make sure everything runs smoothly and work on how to retain customers. For this, he wants us to build an application that would predict which of the customers are more likely to leave the bank soon, so that he can work on how to retain the customer. We will be using machine learning algorithms and help Rick in predicting which of the customers are more likely to leave the bank soon.","959bfa49":"I will be using ANN, Multi Layer Perceptron to be precise on the Churn Modelling dataset to predict the outcome. We tried to come to a conclution by tuning all possible hyperparameters","f751bbad":"Let us find out Binary , Numerical and Categorial columns in TRAIN, DEV and TEST datasets and divide each datasets into further small datasets each holding these values. This is just done to perform pre-processing in a better way.\n\nLet us start by diving TRAIN dataset first","2096306f":"**4. Feature Scaling**\n\nWe have few columns in our dataset that are at a different range when compared to others. Below are few among them.\n\nSince they are in different scales, we need to make every column under a common unit. We have two techniques that can help in scaling\n1. Normalization: Data normalization is the process of rescaling one or more attributes to the range of 0 to 1. This means that the largest value for each attribute is 1 and the smallest value is 0.\n2. Standardization: Data standardization is the process of rescaling one or more attributes so that they have a mean value of 0 and a standard deviation of 1\n\nGenerally, standardization is preferred, and we are trying to standardize our data here. However, we will not be standardizing each column. At this point of data pre-processing, we have categorial data, binary and numerical. We standardize only numeric data and ignore binary columns (one-hot encoding produces binary columns).\nNOTE: feature scaling is done on training, testing and DEV data separately to avoid data leaks.\nSo, we first calculate mean and standard deviation of each column of Test data and use the standardization formula on every column of DEV and TEST data on respective columns."}}