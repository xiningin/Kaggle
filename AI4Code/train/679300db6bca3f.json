{"cell_type":{"cfdac148":"code","cf54c508":"code","8bc86cef":"code","2a90bb85":"code","30735384":"code","f4053036":"code","a739beee":"code","44410412":"code","892a39d4":"code","70f717a3":"code","8cfdeebf":"code","d353d00f":"code","5f3f6c2f":"code","45c80379":"code","63d2ebe6":"markdown","33c0b6d4":"markdown","452d8e5f":"markdown","ab1cfb57":"markdown","29614798":"markdown","e2f5d7c6":"markdown","2253fee3":"markdown","2e14ef0c":"markdown","389b7984":"markdown","d8cb6480":"markdown","21ca6eba":"markdown","f12bb834":"markdown","0cee2911":"markdown","94ce1272":"markdown"},"source":{"cfdac148":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","cf54c508":"# Training data\ntrain_data=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\n\n# Test data scaled to lie in [0,1]\ntest_data=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\/255\n\n# Shape and preview\nprint(train_data.shape)\ntrain_data.head()","8bc86cef":"flipped = train_data[train_data['label']== (2 or 4 or 9)]\nflipped_images = np.asarray(flipped.iloc[:,1:].values.reshape((len(flipped),28,28)))\nflipped_images = np.rot90(flipped_images, axes=(1,2))\nflipped_images = flipped_images.reshape((len(flipped_images),28*28))\nflipped['label'] = 10\nflipped.iloc[:,1:] = flipped_images\n\ntrain_data = pd.concat([train_data,flipped])","2a90bb85":"# Figure size\nplt.figure(figsize=(6,6))\n\n# Subplot \nfor i in range(9):\n    img = np.asarray(flipped.iloc[i,1:].values.reshape((28,28))\/255);\n    ax=plt.subplot(3, 3, i+1)\n    ax.grid(False)\n    plt.imshow(img, cmap='gray')\n    \nplt.show()","30735384":"print('Number of null values in training set:',train_data.isnull().sum().sum())\nprint('')\nprint('Number of null values in test set:',test_data.isnull().sum().sum())","f4053036":"# Figure size\nplt.figure(figsize=(6,6))\n\n# Subplot \nfor i in range(9):\n    img = np.asarray(train_data.iloc[i,1:].values.reshape((28,28))\/255);\n    ax=plt.subplot(3, 3, i+1)\n    ax.grid(False)\n    plt.imshow(img, cmap='gray')\n    \nplt.show()","a739beee":"sns.countplot(x='label', data=train_data)\nplt.title('Distribution of labels in training set')","44410412":"# Labels\ny=train_data.label\n\n# One-hot encoding of labels\ny=pd.get_dummies(y)\n\n# Features scaled to lie in [0,1]\nX=train_data.drop('label', axis=1)\/255","892a39d4":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.9,\n                                                             test_size=0.1,random_state=0)","70f717a3":"model = keras.Sequential([\n    \n    # hidden layer 1\n    layers.Dense(units=256, activation='relu', input_shape=[784]),\n    layers.Dropout(rate=0.4),\n    \n    # hidden layer 2\n    layers.Dense(units=256, activation='relu'),\n    layers.Dropout(rate=0.4),\n    \n    # output layer (softmax returns a probability distribution)\n    layers.Dense(units=11, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['categorical_accuracy'])\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.0001,\n    restore_best_weights=True,\n)","8cfdeebf":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=500,\n    epochs=100,\n    callbacks=[early_stopping],\n    verbose=True\n)","d353d00f":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot(title=\"Accuracy\")\n\nprint('Final accuracy on validation set:', \n      history_df.loc[len(history_df)-1,'val_categorical_accuracy'])","5f3f6c2f":"# Predictions\npreds=model.predict(test_data)\n\n# Retrieve most likely classes\npred_classes = np.argmax(preds,axis=1)\n\n# Save predictions to file\noutput = pd.DataFrame({'ImageId': test_data.index+1,\n                       'Label': pred_classes})\n\n# Check format\noutput.head()","45c80379":"output.to_csv('submission.csv', index=False)","63d2ebe6":"# Adding random noise as additional data","33c0b6d4":"**Learning curves**","452d8e5f":"**Explore label distribution**","ab1cfb57":"# Introduction","29614798":"**Labels and features**","e2f5d7c6":"**Preview first few images**","2253fee3":"# Data","2e14ef0c":"# Make predictions","389b7984":"**Check for null values**","d8cb6480":"# Libraries","21ca6eba":"This notebook contains my first attempt at building a neural network! I may update the presentation from time to time but I will leave the original architecture in place so I can use it to compare my progress on computer vision in the future.\n\nThe objective is to classify hand written digits through the famous MNIST dataset. ","f12bb834":"**Break off validation set**","0cee2911":"# Model","94ce1272":"# Train model"}}