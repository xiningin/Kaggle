{"cell_type":{"d83787fe":"code","22c6fc48":"code","69c408da":"code","4bbd1284":"code","cdad1113":"code","40fbb2b5":"code","8e383532":"code","92d5d6b2":"code","ed3def94":"code","5ede5e63":"code","8317319a":"code","cf5b8f82":"code","383fe5c3":"code","45cff845":"code","575327b2":"code","d241a7ae":"code","c74b0b61":"code","8e72e9a4":"code","b1863ad7":"code","b00066a3":"code","42de8912":"code","f4eba777":"code","7760f62a":"code","5b976ba8":"code","4e00ed2a":"code","2c047f9a":"markdown","d64f08af":"markdown","b7707020":"markdown","782aaf1c":"markdown","a5233d6b":"markdown","529ca11e":"markdown","7f2106a4":"markdown","592a4afb":"markdown","8917ab0c":"markdown","02f6b26c":"markdown","f9e6ef5f":"markdown","918fba4c":"markdown","62ebbc86":"markdown","11c7ba99":"markdown"},"source":{"d83787fe":"# Use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","22c6fc48":"# text processing libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport missingno as msno\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplt.rcParams.update(params)\n \nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 10000)","69c408da":"#Training data\ntrain = pd.read_csv('..\/input\/rinatext\/data_rina.csv')\ndf = pd.DataFrame(train)\nprint('Training data shape: ', df.shape)\ndf.head()","4bbd1284":"df = df.drop(columns=['No.', 'Tanggal', 'ATM ID', 'Nomor'])\ndf.head()","cdad1113":"#Missing values in training set\ndf.isnull().sum()","40fbb2b5":"df = pd.DataFrame(df)\ndf.rename(columns={ df.columns[3]: \"P0\",\n                    df.columns[5]: \"P1\"}, inplace = True)","8e383532":"trb_nan_idx = df[pd.isnull(df['P0'])].index.tolist()\ndf.loc[trb_nan_idx, 'P0'] = ' '","92d5d6b2":"trb_nan_idx = df[pd.isnull(df['P1'])].index.tolist()\ndf.loc[trb_nan_idx, 'P1'] = ' '","ed3def94":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # make text lower case\n    text = text.strip() # remove blankspace char\n    text = text.translate(str.maketrans(\"\",\"\",string.punctuation)) #remove punctuation\n    text = re.sub('\\s+',' ',text) # remove multiple whitespace into single whitespace\n#     text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text) # remove repeated char\n\n    return text","5ede5e63":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer_reg = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    #recor = recover_shortened_words(nopunc)\n    tokenized_text = tokenizer_reg.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('indonesian')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n\n# Applying the cleaning function to both test and training datasets\ndf['P0'] = df['P0'].apply(lambda x: text_preprocessing(x))\ndf['P1'] = df['P1'].apply(lambda x: text_preprocessing(x))\n\n# Let's take a look at the updated text\n# train.head()","8317319a":"file1 = open(\"..\/input\/transformation\/stopword.txt\",\"r\")\n# print(file1.read())","cf5b8f82":"!pip install sastrawi\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n\nfactory = StopWordRemoverFactory()\nstopword = factory.get_stop_words()\n\nfor i in file1:\n    stopword.append(i)\n# print(stopword)","383fe5c3":"def remove_stopwords(text, stem=False):\n    tokens = []\n    for token in text.split():\n        if token not in stopword:\n            tokens.append(token)\n    return \" \".join(tokens)\n\ndf[\"P0\"] = df[\"P0\"].apply(lambda x: remove_stopwords(x))\ndf[\"P1\"] = df[\"P1\"].apply(lambda x: remove_stopwords(x))","45cff845":"df.head()","575327b2":"df.to_csv ('data_cleaning.csv', index = False, header=True)","d241a7ae":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P0'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","c74b0b61":"top_word_bigrams=bigrams(df['P0'])[:10]\ntop_word_bigrams","8e72e9a4":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P0'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","b1863ad7":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P0'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","b00066a3":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(4, 4)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P0'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","42de8912":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P1'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","f4eba777":"top_word_bigrams=bigrams(df['P1'])[:10]\ntop_word_bigrams","7760f62a":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P1'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","5b976ba8":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P1'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","4e00ed2a":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(4, 4)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_word_bigrams=bigrams(df['P1'])[:10]\nx,y=map(list,zip(*top_word_bigrams))\nsns.barplot(x=y,y=x)","2c047f9a":"### 3-Gram","d64f08af":"# Exploratory Data Analyze","b7707020":"### 2-Gram","782aaf1c":"### 4-Gram","a5233d6b":"### 4-Gram","529ca11e":"### 1-Gram","7f2106a4":"### 3-Gram","592a4afb":"# Data Preprocessing","8917ab0c":"### 1-Gram","02f6b26c":"# Load Dataset","f9e6ef5f":"# Export Dataframe","918fba4c":"## 2. Penggantian Sparepart","62ebbc86":"### 2-Gram","11c7ba99":"## Pekerjaan Dilakukan"}}