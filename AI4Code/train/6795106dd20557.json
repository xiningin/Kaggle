{"cell_type":{"19812b44":"code","609be839":"code","2b72fa2b":"code","0facf642":"code","b40dbf0c":"code","3dba121c":"code","d0617461":"code","f27cc8f3":"code","4859ba76":"code","f195bf92":"code","93f44e63":"code","b2238008":"code","b912334b":"code","b3d4fa17":"code","5a28b047":"code","5cf2ee6e":"code","60d6758b":"code","6ab9158a":"code","8d9254e0":"code","343f2ef0":"code","8e321b04":"code","9d070bac":"code","4013cff5":"code","d77cd094":"code","720a79c6":"code","2a55c3d8":"code","a1b1b473":"code","96255ef6":"code","1ece6967":"markdown","c456e3e3":"markdown","d87ae7c3":"markdown","52cc71b5":"markdown","f91d21f1":"markdown"},"source":{"19812b44":"# Required packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt","609be839":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","2b72fa2b":"train.head()","0facf642":"# Getting to know data\nprint(f\"Training Set :\\n Number of rows : {train.shape[0]}, Number of Columns : {train.shape[1]}\")\nprint(f\"Test Set :\\n Number of rows : {test.shape[0]}, Number of Columns : {test.shape[1]}\")","b40dbf0c":"# extract numeric data\nnum_cols = train.loc[:,train.dtypes != 'object'].drop(['Id'], axis=1).columns\nnum_train = train[num_cols]\n# extract categorical data\ncat_cols = train.loc[:,train.dtypes == 'object'].columns\ncat_train = train[cat_cols]\n\nprint(\"Total Numerical Cols : \", len(num_cols))\nprint(\"Total Categorical Cols : \", len(cat_cols))","3dba121c":"# summary of numerical variable\ntrain.describe()","d0617461":"# summary of categorial variable\ntrain.info()","f27cc8f3":"# let's clean visualizations :)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","4859ba76":"# Let's take a look at the distribution of the SalePrice \nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","f195bf92":"price = np.log1p(train[\"SalePrice\"])\n \nsns.distplot(price , fit=norm);\n(mu, sigma) = norm.fit(price)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","93f44e63":"# common sales' years\nplt.figure(figsize=(18,10))\nplots = train[\"YrSold\"].value_counts().plot(kind=\"bar\")\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=15, xytext=(0, 8),\n                   textcoords='offset points')\nplt.title(\"Houses Sold over the Years\")\nplt.ylabel(\"Number\")\nplt.show()","b2238008":"# common purchases' types\nplt.figure(figsize=(18,10))\nplots = train[\"SaleType\"].value_counts().plot(kind=\"bar\")\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=15, xytext=(0, 8),\n                   textcoords='offset points')\nplt.title(\"Most purchased Sale Type\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Sale Type\")\nplt.show()","b912334b":"### Feature importances","b3d4fa17":"# fill nan with \"NULL\"\ntmp_df = train.copy()\ntmp_df[cat_cols] = tmp_df[cat_cols].fillna('NULL')\n# label encoding\nfor col in cat_cols:\n    le = LabelEncoder()\n    le.fit(tmp_df[col])\n    tmp_df[col] = le.transform(tmp_df[col])\n# train data\nX_train = tmp_df.drop(['SalePrice', 'Id'], axis=1)\ny_train = tmp_df['SalePrice']\nlgb_train = lgb.Dataset(X_train, y_train)\nparams = {'objective': 'regression', 'metric': 'rmse'}\ngbm = lgb.train(params, lgb_train)\n# create DataFrame\ncols = train.columns.drop(['Id', 'SalePrice'])\nfeat_importances = pd.DataFrame({'importance': gbm.feature_importance()}, index=X_train.columns).sort_values('importance', ascending=False)\nfeat_importances['dtype'] = ['numeric' if feat in num_cols else 'categorical' for feat in cols]\nfeat_importances.head()","5a28b047":"# correlation between numerical values\nplt.figure(figsize=(10, 9))\nsns.heatmap(train.drop('Id', axis=1).corr(), cmap='YlGnBu');","5cf2ee6e":"# correlation coefficient with SalePrice\nax = num_train.corr()['SalePrice'].sort_values().plot(kind='barh', figsize=(14,9))\nax.set_title('Correlation coefficient with SalePrice', fontsize=14)\nax.set_xlabel('correlation coefficient')\nax.set_ylabel('features')","60d6758b":"# plot importance of numeric features\nnum_feat_importances = feat_importances.loc[num_cols[:-1]].sort_values('importance', ascending=False)\nax = num_feat_importances.plot.bar(figsize=(14,5))\nax.set_title('Importance of Numeric Features', fontsize=14)\nax.set_xlabel('features')\nax.set_ylabel('count')","6ab9158a":"# cross validation\ndef rmsle_cv(model):\n    kf = KFold(\n        n_splits=5,\n        shuffle=True,\n        random_state=42).get_n_splits(train_data)\n\n    rmse = np.sqrt(-cross_val_score(\n        model,\n        train_data,\n        price,\n        scoring=\"neg_mean_squared_error\",\n        cv = kf)\n    )\n    return rmse","8d9254e0":"# Numerical columns  (int64\/float64)\ndef get_numerical_cols(threshold=15):\n    num_cols =[]\n    num_df = train.select_dtypes(exclude='O')\n    for num_col in num_df.columns:\n        if num_col != 'SalePrice':\n            if train[num_col].nunique() > 15:\n                num_cols.append(num_col)\n    return num_cols","343f2ef0":"# Category columns\ndef get_cat_cols(type='O', threshold=15):\n    cat = []\n\n    # Feature Selection\n    for col in train.columns:\n        if train[col].dtype == type:\n            if train[col].nunique()<=threshold:\n                cat.append(col)\n    return cat","8e321b04":"# if a column has many missing fields then it has no use \nfield_missing = train['MiscFeature'].isna().sum()\nprint(f'Missing field in MiscFeature column {field_missing} ({field_missing\/len(train):.2f}%)')","9d070bac":"def feature_selection_util(type='O', cat_threshold=15, missing_threshold=100, is_cat=True):\n    features=[]\n    columns = get_cat_cols(type=type, threshold=cat_threshold) if is_cat else get_numerical_cols(threshold=cat_threshold)\n    for feature in columns:\n        if train[feature].isna().sum() <100:\n            features.append(feature)\n    return features","4013cff5":"# string category \nstr_cat_features = feature_selection_util()\nstr_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                      ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n# integer category \nint_cat_features = feature_selection_util(type='int64')\nint_cat_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                      ('ohe', OneHotEncoder(handle_unknown='ignore'))])\n\n# numerical columns \nnumerical_features = feature_selection_util(is_cat=False)\nnumerical_transformer = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                        ('scale',StandardScaler())])\n\n\n# transformation\npreprocessing = ColumnTransformer(transformers=[('str_cat',str_cat_transformer,str_cat_features),\n                                                ('int_cat',int_cat_transformer, int_cat_features),\n                                                ('num_col', numerical_transformer, numerical_features)])\n","d77cd094":"def cross_validate(model,scoring='neg_root_mean_squared_error'):\n    # training features and lables\n    X = train.drop(columns=['SalePrice'])\n    y = train['SalePrice'] \n    # cross validation score\n    cv_score = cross_val_score(model, X, y, scoring=scoring)\n    return cv_score.mean()","720a79c6":"# pipeline combining preprocessing step and modeling\nmodel = RandomForestRegressor(random_state=42)\nmodel_RFR = Pipeline(steps=[('preprocessing', preprocessing),\n                       ('model', model)])\ncv_score_RFR = cross_validate(model_RFR)\n\nprint(cv_score_RFR)","2a55c3d8":"model = XGBRegressor(n_estimators=350, learning_rate=0.05, max_depth=4, subsample = 0.7, colsample_bytree = 0.5)\nmodel_XGB = Pipeline(steps=[('preprocessing', preprocessing),\n                       ('model', model)])\ncv_score_XGB = cross_validate(model_XGB)\n\nprint(cv_score_XGB)","a1b1b473":"X = train.drop(columns=['SalePrice'])\ny = train['SalePrice'] \n\n# train the model\nmodel_XGB.fit(X,y)\n\n# make predictions\npredictions = model_XGB.predict(test)","96255ef6":"output = pd.DataFrame({'Id':test['Id'], 'SalePrice':predictions})\noutput = output.to_csv('submission.csv',index=False)\nprint('done')","1ece6967":"### Numeric features","c456e3e3":"#  **Preprocessing**","d87ae7c3":"#  **Exploratory Data Analysis**","52cc71b5":"#  **Training and Predicting**","f91d21f1":"<h2 style=\"font-weight: bold\">House Prices Competition<\/h2>\n\n<h4>This is my second published notebook on Kaggle, So yeah no wonder it's about the House Prices Competition \ud83d\ude04\ud83d\ude04<br><br>I will be doing a simple then advanced EDA, Data Visualization and Pre-Processing. I also will test different approaches and regression techniques to improve my score.<br><\/h4>\n\n* <h5 style=\"font-weight: 700\">Your feedback is very welcome<\/h5>\n* <h5 style=\"font-weight: 700\">If you find this notebook useful, please don't forget to upvote it!<\/h5>\n"}}