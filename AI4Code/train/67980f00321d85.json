{"cell_type":{"8507e7fe":"code","bf962f8b":"code","10c13be8":"code","bf72093a":"code","c967d7c3":"code","f783828e":"code","c2f2db02":"code","017e49a0":"code","b1f923db":"code","dd0a562a":"code","bd0ddd7c":"code","66b660e4":"code","56c9a64b":"code","96674baf":"code","02b265ed":"code","ded6dbf6":"code","ed5faece":"code","cb327983":"code","01c53127":"code","04e9e025":"code","e026db2b":"markdown","b5850f71":"markdown","57a773f3":"markdown","e413a8a1":"markdown","11f113e5":"markdown","4cefe084":"markdown","a71157cd":"markdown","d7221424":"markdown","b0561977":"markdown","c710043b":"markdown","5c469da4":"markdown","beb9b4c4":"markdown"},"source":{"8507e7fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bf962f8b":"train_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/test.csv')\nsample_sub_df = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/sample_submission.csv')","10c13be8":"train_df.head()","bf72093a":"test_df.head()","c967d7c3":"sample_sub_df.head()","f783828e":"print(f'Shape of training dataset: {train_df.shape}')\nprint(f'Shape of test dataset: {test_df.shape}')","c2f2db02":"train_df.columns","017e49a0":"train_df.isna().sum()","b1f923db":"train_df = train_df.apply(lambda group: group.interpolate(limit_direction='both'))\ntrain_df.isna().sum()","dd0a562a":"for col in train_df:\n    if train_df[col].isna().sum() > 0:\n        train_df[col] = train_df[col].fillna(train_df[col].mode()[0])","bd0ddd7c":"train_df.isna().sum()","66b660e4":"sns.set(rc={'figure.figsize':(13,8)})\nsns.distplot(train_df['target'])\nplt.show()","56c9a64b":"Y_train = train_df['target']\nX_train = train_df.drop(['target', 'id'], axis=1)\nX_test = test_df.drop(['id'], axis=1)","96674baf":"%%time\ncombined_data = pd.concat([X_train, X_test], axis=0, sort=False)\ncombined_data = pd.get_dummies(combined_data, columns=combined_data.columns, drop_first=True, sparse=True)\nX_train = combined_data.iloc[: len(train_df)]\nX_test = combined_data.iloc[len(train_df): ]","02b265ed":"# Delete the dataframe to decrease memory usage\ndel train_df\ndel test_df","ded6dbf6":"print(f'Shape of training dataset: {X_train.shape}')\nprint(f'Shape of test dataset: {X_test.shape}')","ed5faece":"X_train = X_train.sparse.to_coo().tocsr()\nX_test = X_test.sparse.to_coo().tocsr()","cb327983":"class ModelHelper(object):\n    \n    def __init__(self, params, model):\n        self.model = model(**params)\n    \n    def train(self, X_train, Y_train):\n        self.model.fit(X_train, Y_train)\n    \n    def predict(self, X_test):\n        return self.model.predict_proba(X_test)[:, 1]\n    \n    def evaluate(self, Y_true, Y_preds):\n        return roc_auc_score(Y_true, Y_preds)","01c53127":"SPLITS = 10\nkfold = KFold(n_splits=SPLITS, shuffle=False, random_state=666)\n\nlr_params = {\n    'verbose': 100,\n    'max_iter': 600,\n    'C': 0.5,\n    'solver': 'lbfgs'\n}\nmodel_helper = ModelHelper(lr_params, LogisticRegression)\nscores = []\npredictions = np.zeros((SPLITS, X_test.shape[0]))\nfor i, (train_index, test_index) in enumerate(kfold.split(X_train)):\n    X_dev, X_val = X_train[train_index], X_train[test_index]\n    Y_dev,Y_val = Y_train[train_index], Y_train[test_index]\n    \n    model_helper.train(X_dev, Y_dev)\n    preds = model_helper.predict(X_val)\n    roc_score = model_helper.evaluate(Y_val, preds)\n    \n    full_test_preds = model_helper.predict(X_test)\n    scores.append(roc_score)\n    predictions[i, :] = full_test_preds\n\nprint (scores)","04e9e025":"sample_sub_df['target'] = predictions[np.argmax(scores)]\nsample_sub_df.to_csv('submission.csv', index=False)\nsample_sub_df","e026db2b":"Let's first divide the data into features and target variable and remove non-features columns","b5850f71":"So there are no records with NaN in target variable and ~18000 NaNs in columns other than `id`","57a773f3":"Let's first check how many missing values are there in training and testing dataset.","e413a8a1":"### Distribution of target variable","11f113e5":"## Reading the dataset","4cefe084":"Let's try to fill NaN using interpolation first. ","a71157cd":"The `target` is the target variable which we are going to predict for given test dataset.","d7221424":"Now let's convert the entire dataset into one-hot encoding","b0561977":"## Handling the missing values (NaNs)","c710043b":"## Glimpse of the dataset","5c469da4":"Okay, so few features now don't have any missing values at all. To fill other NaNs, Let's use mean and mode of the features (Mean for continuous and mode for categorial features). But here we only have categorial features, so we will use mode.","beb9b4c4":"Now all the missing values are filled!"}}