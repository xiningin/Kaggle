{"cell_type":{"5724cd3e":"code","3c2737db":"code","25ffecc1":"code","eb666f07":"code","69a4c9eb":"code","5422cbad":"code","7f89f354":"code","66b75a5c":"code","5efa0aee":"code","235329f6":"code","7cf35863":"code","dc924f20":"code","0781ea43":"code","767a5b59":"code","6bdc1b81":"code","4a53f017":"code","17fe75e1":"code","7a146370":"code","3fd5b54d":"code","a907cf3f":"code","1abbbc80":"code","42804377":"code","0e799d72":"code","6d707cf8":"markdown","55ce87ff":"markdown","7db32248":"markdown","f8b63ebd":"markdown","fa5de413":"markdown","6585cbfd":"markdown","a49f8420":"markdown"},"source":{"5724cd3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c2737db":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","25ffecc1":"df.info()","eb666f07":"df.describe()","69a4c9eb":"df['Class'].value_counts()","5422cbad":"#Lets check for missing data\ndf.isnull().sum()","7f89f354":"df[\"Class\"].value_counts()","66b75a5c":"#plotting correlation between the top features\n\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(df, 20))","5efa0aee":"#target label, features split\nX = df.drop([\"Class\"], axis=1)\ny = df.Class\ny.head()","235329f6":"X.head()","7cf35863":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","dc924f20":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression(solver='liblinear')\n\nlog_model.fit(X_train, y_train)","0781ea43":"from sklearn.ensemble import RandomForestClassifier\nforest_model = RandomForestClassifier()\n\nforest_model.fit(X_train, y_train)","767a5b59":"from sklearn.tree import DecisionTreeClassifier\ntree_model = DecisionTreeClassifier()\n\ntree_model.fit(X_train, y_train)","6bdc1b81":"from sklearn.metrics import recall_score, precision_score, confusion_matrix\n\nmodel_list = [tree_model, forest_model, log_model]\n\nscores = {}\n\nfor model in model_list:\n    y_preds = model.predict(X_test)\n    score = recall_score(y_test, y_preds)\n    scores[model] = score\n    \nfor model, score in scores.items(): \n    print(\"{}\".format(model) + \" : \" + \"{}\".format(score))    ","4a53f017":"forest_model.get_params().keys()","17fe75e1":"#Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#first we tune the Random Forest Model\nhyperparameter_grid = {\n    'n_estimators' : [100, 150, 200],\n'min_samples_split' : [3, 5, 6],\n'max_leaf_nodes' : [5, 10, 15],\n'max_features' : [\"auto\", \"sqrt\", \"log2\"]\n}\n\n# Set up the random search with cross validation\nrandom_cv = RandomizedSearchCV(estimator=forest_model,\n            param_distributions=hyperparameter_grid,\n            cv=3, n_iter=20,\n            scoring = 'recall',n_jobs = -1,\n            verbose = 2, \n            return_train_score = True,\n            random_state=42)\n\nrandom_cv.fit(X_train,y_train)","7a146370":"random_cv.best_estimator_","3fd5b54d":"best_forest_model = RandomForestClassifier(max_features='sqrt', max_leaf_nodes=15,\n                       min_samples_split=5, n_estimators=150)\nbest_forest_model.fit(X_train, y_train)\n\ny_preds = best_forest_model.predict(X_test)\nscore = recall_score(y_test, y_preds)\n\nprint(\"Recall score: {}\".format(score))","a907cf3f":"#lets try the Decision Tree\n\n#first we tune the Decision Tree Model\nhyperparameter_grid = {\n'min_samples_split' : [3, 5, 6],\n'max_leaf_nodes' : [5, 10, 15],\n'max_features' : [\"auto\", \"sqrt\", \"log2\"]\n}\n\n# Set up the random search with cross validation\nrandom_cv = RandomizedSearchCV(estimator=tree_model,\n            param_distributions=hyperparameter_grid,\n            cv=3, n_iter=20,\n            scoring = 'recall',n_jobs = -1,\n            verbose = 2, \n            return_train_score = True,\n            random_state=42)\n\nrandom_cv.fit(X_train,y_train)","1abbbc80":"random_cv.best_estimator_","42804377":"best_tree_model = DecisionTreeClassifier(max_features='sqrt', max_leaf_nodes=15,\n                       min_samples_split=5)\n\nbest_tree_model.fit(X_train, y_train)\ny_preds = best_tree_model.predict(X_test)\nscore = recall_score(y_test, y_preds)\n\nprint(\"Recall score: {}\".format(score))","0e799d72":"forest_model.fit(X, y)","6d707cf8":"Let us train:\n1. Logistic Regression,\n2. Random Forest\n3. Decision Tree\n\nmodels,","55ce87ff":"The Decision Tree and Random Forest models are performing far much better than the logistic regression model so far","7db32248":"from our class label counts, this is an imbalanced dataset.Therefore , evaluation of our model performance must not be based on accuracy","f8b63ebd":"# Problem Statement\nThis is a binary classification problem where we will build a model to try and predict if fraud occurred within transactions.\n\nThe output label is Class:\n* 1 - Fraudulent\n* 0 - Genuine","fa5de413":"we are going to use Recall as our most important performance metric. We would not want a fraudulent transaction to be\nas genuine.","6585cbfd":"It seems the best performing model so far is the Random Forest Classifier with default values, therefore we will fit that model to the whole dataset.","a49f8420":"It seems like our dataset does not have any missing data"}}