{"cell_type":{"abbbe6db":"code","d11e0398":"code","bca52771":"code","59cab60c":"code","b5cea7f7":"code","035d5b33":"code","fab19646":"code","9f79aea5":"code","55a775a3":"code","964c15a2":"code","a7e52de7":"code","61b484fe":"code","20f2ee01":"code","447c86f4":"code","2da8d7aa":"code","6c5d75c4":"code","dc956ac4":"code","91e89580":"code","2a18011f":"code","823e085b":"code","5db8bb4c":"code","9a2405f1":"code","23aba34f":"code","af68c538":"code","f6f23c3b":"markdown","69f6add7":"markdown","33e6fb08":"markdown","caf99d26":"markdown","4e6ad16b":"markdown","ace1e8ba":"markdown","2f87a3ac":"markdown","93d64023":"markdown","d139481c":"markdown","6dfedb24":"markdown","cc7b24d8":"markdown","1550427b":"markdown","fd25f7ec":"markdown","8cbf4268":"markdown","4c6226e0":"markdown","c812624b":"markdown","138aee9b":"markdown","e2a51ca3":"markdown","01710815":"markdown","6efad236":"markdown","5aea84c3":"markdown","5bc1131e":"markdown","2c7ffffc":"markdown","76674b86":"markdown","9f3855c8":"markdown","cd5fe31d":"markdown"},"source":{"abbbe6db":"#importiamo le librerie necessarie per l'analisi del dataset\nimport pandas as pd\n# pandas \u00e8 una libreria molto utilizzata in data analysis in quanto permette di gestire grosse quantit\u00e0 di dati in maniera veloce e inuitiva\n#inoltre offre molte altre funzionalit\u00e0 integrate, come la creazione di grafici ecc-\nimport numpy as np\n#numpy \u00e8 una libreria utilizzata per un'insieme di operazioni numeriche\nimport matplotlib.pyplot as plt\n#matplotlib ci permette di visualizzare l'andamento dei dati attraverso grafici e tabelle\nimport seaborn as sns\n#seaborn, come matplotlib, permette di visualizzare grafici e creare istogrammi","d11e0398":"#carichiamo il dataset\ntrain= pd.read_csv(\"..\/input\/train.csv\")\n#train = pd.read_csv(\"train.csv\")\ntrain.head()","bca52771":"#per rendere pi\u00f9 facile la lettura, cambiamo i nomi delle colonne in lowercase\ntrain.columns = [x.lower() for x in train.columns]\ntrain.head(20)","59cab60c":"#train.info ci permette di analizzare il dataset e capire quali dati possiede ogni colonna\ntrain.info()","b5cea7f7":"#L'idea principale che si utilizza quando si vanno a riempire valori vuoti, e che i 'segnaposto' che si vanno a inserire \n#non devono togliere o dare informazioni aggiuntive ai dati, ovvero 'snaturare' il dataset: infatti, se noi diamo valori\n#che non c'entrano con quelli reali, \u00e8 molto probabile che la classificazione avvenga non inerente con la realt\u00e0.\n#Inserendo nei valori nulla la media delle et\u00e0, ricaviamo un dato che non aggiunge n\u00e8 toglie valore alla distribuzione dei dati,\n#non alterandone l'andamento.\ntrain['age'] = train['age'].fillna(train['age'].median())\ntrain['fare'] = train['fare'].fillna(train['fare'].median())\n#stessa cosa per embarked: siccome non sappiamo dove sono saliti i passeggeri, poniamo come segnapost la U di 'Unknown'\ntrain['embarked'] = train['embarked'].fillna('S')\n# per le cabine dobbiamo capire come si distribuiscono nel dataset:\ntrain['cabin'] = train['cabin'].fillna('Unknown')","035d5b33":"#per farlo utilizziamo una funzione, denominata def_embarked, e andremo a modificare il dataset attraverso l'operatore lambda\ndef def_embarked(point):\n    if point == \"S\":\n        return 1\n    elif point == \"Q\":\n        return 2\n    elif point == \"C\":\n        return 3\n    else:\n        return 0\n\ntrain[\"embarked_\"] = train.apply(lambda row:def_embarked(row[\"embarked\"]),axis=1)\n\n#per cabin il discorso \u00e8 un po' diverso: ogni cabina \u00e8 divisa in una lettera identificativa di una parte della nave pi\u00f9\n#il numero di stanza reale. \u00e8 necessario dividere in classi in base alla lettera che vi \u00e8 davanti.\n# identifichiamo la posizione di ogni cabina (se la hanno) all'interno della nave\ndef def_position(cabin):\n    return cabin[:1]\ntrain[\"Position\"] = train.apply(lambda row:def_position(row[\"cabin\"]), axis=1)\n#value_counts() ci restituisce quanti valori ci sono all'interno di uan colonna:\ntrain[\"Position\"].value_counts()\n#osserviamo 8 possibili classi, che andremo ad aggiungere al nostro dataset:\ndef def_cabin(pos):\n    if pos == \"C\":\n        return 1\n    elif pos == \"B\":\n        return 2\n    elif pos == \"D\":\n        return 3\n    elif pos == \"E\":\n        return 4\n    elif pos == \"F\":\n        return 5\n    elif pos == \"A\":\n        return 6\n    elif pos == \"G\":\n        return 7\n    else: \n        return 0\ntrain[\"cabin_\"] = train.apply(lambda row:def_cabin(row[\"Position\"]),axis=1)\n#stessa cosa la effettuiamo con male o female\ndef def_sex(sex):\n    if sex==\"male\":\n        return 0\n    else:\n        return 1\ntrain[\"sex_\"] = train.apply(lambda row: def_sex(row[\"sex\"]),axis = 1)","fab19646":"train = train.drop(columns=\"passengerid\")\ntrain = train.drop(columns=\"name\")\ntrain = train.drop(columns = \"embarked\")\ntrain = train.drop(columns = \"cabin\")\ntrain = train.drop(columns= \"Position\")\ntrain = train.drop(columns=\"sex\")\n\ntrain = train.drop(columns=\"ticket\") #drop ma c'\u00e8 da rivedere, perch\u00e8 non capisco come funziona","9f79aea5":"train.info()","55a775a3":"x = []\nfor i in train[\"survived\"]:\n    if(i == 1):\n        x.append(\"Survived\")\n    else:\n        x.append(\"Not Survived\")\ntitanic_target_names = np.asarray(x)\ntitanic_feature_names =  np.asarray(train.columns[1:])\ntrain_ = train.drop(columns=\"survived\")\ntitanic_data = np.asarray(train_.get_values())\ntitanic_target = np.asarray(train[\"survived\"])\n#con train_test_split dividiamo il nostro dataset in due parti: la prima che la utilizzeremo per il training, grande il 75% del totale,\n#mentra la seconda la utilizzeremo per il testing, che \u00e8 grande il 25% del totale. Ovviamente divider\u00e0 anche le etichette relative\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(titanic_data,titanic_target,random_state=1)","964c15a2":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state = 0)\ntree.fit(X_train,y_train)\nprint(\"Accuracy on training set: {}\".format(tree.score(X_train,y_train)))\nprint(\"Accuracy on the testing set: {}\".format(tree.score(X_test,y_test)))","a7e52de7":"#graphviz \u00e8 una libreria che serve per caricare grafici e salvarli. Nel nostro caso andiamo a salvare la raffigurazione\n#dell'albero sopra creato e poi andremo a visualizzarlo nel kernel. C'\u00e8 salvato anche nella cartella\nimport graphviz\nfrom sklearn.tree import export_graphviz\nexport_graphviz(tree,out_file=\"tree.dot\",class_names=[\"Survived\",\"Not Survived\"],feature_names=titanic_feature_names,impurity=False,filled=True)\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","61b484fe":"import matplotlib.pyplot as plt\ndef plot_feature_importances(model):\n    n_features = titanic_data.shape[1]\n    plt.barh(range(n_features),model.feature_importances_,align='center')\n    plt.yticks(np.arange(n_features), titanic_feature_names)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.show()\nplot_feature_importances(tree)","20f2ee01":"results = []\nimportances = []\nmax_ = [0,0]\nfor i in range(1,10):\n    tree = DecisionTreeClassifier(max_depth=i,random_state = 1)\n    tree.fit(X_train,y_train)\n    if (tree.score(X_test,y_test) > max_[0]):\n        max_ = [tree.score(X_test,y_test),i-1]\n    results.append(tree.score(X_test,y_test))\n    importances.append(tree.feature_importances_)","447c86f4":"plt.plot([max_[1]],[max_[0]],marker='o',color=\"red\")\nplt.plot(results)\nplt.title(\"Accuracy on max_depth\")\nplt.ylabel('Accuracy')\nplt.xlabel('max_depth')\nplt.legend([\"Max Accuracy: {0} with {1} depth\".format(round(max_[0],2),max_[1])],loc=(1.04,0.5))\nplt.show()","2da8d7aa":"plt.plot(importances)\nplt.legend(titanic_feature_names,loc=(1.04,0.05))\nplt.title(\"Feature importances through max_depth\")\nplt.ylabel('Accuracy')\nplt.xlabel('max_depth')\n#plt.legend([\"Max Accuracy: {0} with {1} depth\".format(round(max_[0],2),max_[1])],loc=(1.04,0.5))\nplt.show()","6c5d75c4":"plt.plot(importances)\nplt.plot([max_[1]],[max_[0]],marker='o',color=\"red\")\nplt.plot(results, color=\"red\")\nplt.legend(titanic_feature_names,loc=(1.04,0.05))\nplt.show()","dc956ac4":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators=1,random_state=0)\nforest.fit(X_train,y_train)\n\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train,y_train)))\nprint(\"Accuracy on testing set: {:.3f}\".format(forest.score(X_test,y_test)))","91e89580":"plot_feature_importances(forest)","2a18011f":"results_forest = []\nimportances_forest = []\nmax_forest = [0,0]\nfor i in range(1,10):\n    forest = RandomForestClassifier(n_estimators=i,random_state=0)\n    forest.fit(X_train,y_train)\n    if (forest.score(X_test,y_test) > max_forest[0]):\n        max_forest = [forest.score(X_test,y_test),i-1]\n    results_forest.append(forest.score(X_test,y_test))\n    importances_forest.append(forest.feature_importances_)","823e085b":"plt.plot([max_forest[1]],[max_forest[0]],marker='o',color=\"red\")\nplt.plot(results_forest)\nplt.title(\"Accuracy on max_depth\")\nplt.ylabel('Accuracy')\nplt.xlabel('n_estimators')\nplt.legend([\"Max Accuracy: {0} with {1} n_estimators\".format(round(max_forest[0],2),max_forest[1])],loc=(1.04,0.5))\nplt.show()","5db8bb4c":"plt.plot(importances_forest)\nplt.plot([max_forest[1]],[max_forest[0]],marker='o',color=\"red\")\nplt.plot(results_forest, color=\"red\")\nplt.legend(titanic_feature_names,loc=(1.04,0.05))\nplt.show()","9a2405f1":"test = pd.read_csv(\"..\/input\/test.csv\")\ntest1 = pd.read_csv(\"..\/input\/test.csv\")\ntest.head()","23aba34f":"test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())\ntest[\"Fare\"] = test[\"Fare\"].fillna(test[\"Fare\"].median())\ntest[\"Cabin\"] = test[\"Cabin\"].fillna(\"C\")\ntest[\"Embarked\"] = test[\"Embarked\"].fillna(\"U\")\ntest[\"embarked_\"] = test.apply(lambda row:def_embarked(row[\"Embarked\"]),axis=1)\ntest[\"Position\"] = test.apply(lambda row:def_position(row[\"Cabin\"]), axis=1)\ntest[\"cabin_\"] = test.apply(lambda row:def_cabin(row[\"Position\"]),axis=1)\ntest[\"sex_\"] = test.apply(lambda row: def_sex(row[\"Sex\"]),axis = 1)\ntest = test.drop(columns=\"PassengerId\")\ntest = test.drop(columns=\"Name\")\ntest = test.drop(columns = \"Embarked\")\ntest = test.drop(columns = \"Cabin\")\ntest = test.drop(columns= \"Position\")\ntest = test.drop(columns=\"Sex\")\ntest = test.drop(columns=\"Ticket\")\ntest.head()","af68c538":"best_tree = DecisionTreeClassifier(max_depth=2,random_state = 1)\nbest_tree.fit(X_train,y_train)\npred = best_tree.predict(test)\nd =  {'PassengerId' : test1[\"PassengerId\"],'Survived' : pred}\nprediction = pd.DataFrame(d,columns=[\"PassengerId\",\"Survived\"])\nprediction.to_csv(\"Kaggle_first_try.csv\",index=False)","f6f23c3b":"Il test set (come ci immaginavamo) \u00e8 uguale al training set, senza per\u00f2 avere le etichette che dicono se la persona \u00e8 sopravvissuta o meno: quello dobbiamo scoprirlo noi. Applichiamo ogni operazione fatta prima sul nuovo documento.","69f6add7":"La feature principale su cui si \u00e8 basata la classificazione \u00e8 il genere, seguito poi da l'et\u00e0 . Questo conferma l'idea che prima si salvano le donne e i bambini. ","33e6fb08":"Le feature sono un po' diverse da quelle del singolo decision tree (perch\u00e8?)","caf99d26":"Con random forest classifier si intede un classificatore che si basa non su un solo albero, ma su pi\u00f9 di uno, in modo da cercare di combattere l'overfitting.\nRichiamando il classificatore, possiamo inserire il numero di alberi che vogliamo creare: andiamo ad analizzare un po' quello che succede.","4e6ad16b":"### Preprocessing dei dati: valori nulli\nNotiamo come nel dataset sono presenti dei valori nulli: prima di poter visualizzare i dati e prima di creare il nostro classificatore dobbiamo trasformarli in modo da poterli rappresentare.\nLe feature su cui dobbiamo lavorare sono Age, cabin e embraked: tutte le altre feature non hanno valori nulli","ace1e8ba":"Come prima, andiamo a visualizzare accuratezza in base al numero di alberi e percentuale di importanza di ogni feature. L'accuratezza non varia di molto.","2f87a3ac":"### Salvare l'albero e visualizzarlo","93d64023":"#### Le feature all'interno del dataset si distribuiscono come :\n- PASSENGERID:\n    indica l'ID di ogni passeggero\n- PCLASS:\n    \u00e8 il tipo di cabina affidato a ogni passeggero: \u00e8 un indicatore dello stato socio economico dei passeggeri\n    si divide in 1 classe, seconda classe, terza classe. \n- AGE:\n    \u00e8 l'et\u00e0 di ogni passeggero, \u00e8 di tipo float perch\u00e8 quando \u00e8 una stima viene posto come un numero decimale.\n- SIBSP:\n    definisce le relazioni familiari: indica quanti fratelli\/sorelle o mogli\/spose sul Titanic\n- PARCH:\n    definisce le relazioni familiari: indica quanti mamme\/pap\u00e0 o figli sul Titanic\n- TICKET:\n    Indica il numero di ticket \n- FARE:\n    Indica la tariffa\n- Cabin:\n    Indica il numero della cabina\n- Embarked:\n    Indica dove si \u00e8 imbarcato il passeggero (C = Cherbourg, Q = Queenstown, S = Southampton)","d139481c":"Ora che abbiamo pronto il nostro dataset, possiamo entrare nella classificazione. Utilizzeremo quella che ci ha mostrato migliori risultati, ovvero decision tree con grado di profondit\u00e0 2:","6dfedb24":"# TITANIC: MACHINE LEARNING FROM DISASTER\n## A KAGGLE competition\nIn questo progetto si andr\u00e0 ad analizzare un dataset contenente tutte le informazioni relative ai passeggeri del Titanic, nave tristemente famosa in quanto \u00e8 affondata provocando diverse centinaia di morti. Il dataset \u00e8 un dataset pubblico scaricato da Kaggle (https:\/\/www.kaggle.com\/c\/titanic), che \u00e8 stato reso disponibile in quanto oggetto di una delle competizioni iniziali per familiarizzare con il sistema.","cc7b24d8":"### Random Forest Classifier","1550427b":"Si sono utilizzati due tipi di classificatori: DecisionTree e RandomForest. Per entrambi si hanno avuto risultati differenti:\n\n- Best DecisionTree (max_depth = x): \n    - On training set: %\n    - On testing set: %\n- Best RandomForest (n_estimators = x): \n    - On training set: %\n    - On testing set: %\n\nPer entrambi si ha avuta una buona classificazione, con differenze sostanziali individuate sull'importanza data a ogni feature. (PERCHE'?)","fd25f7ec":"Osserviamo come ogni feature ha dato il suo contributo alla classificazione","8cbf4268":"Dall'ultimo grafico notiamo come a max_depth 2 abbiamo la maggior accuratezza, con relativa importanza delle feature","4c6226e0":"### Conclusioni","c812624b":"### Il dataset \u00e8 diviso in due parti: test.csv e train.csv. \nIl primo viene usato come 'ground truth', ovvero come campo di allenamento dove allenare il classificatore, mentre nel secondo si andr\u00e0 a testare il modello ricavato in precedenza per osservare l'accuratezza. \\n Siccome questa \u00e8 una competitions, il testing set \u00e8 proposto senza *labels*, quindi non saar\u00f2 in grado di definire l'accuratezza della classificazione sul testing set. Possiamo ovviare a questo dividendo il training set in due parti, come se fosse l'intero dataset, ma questo avver\u00e0 in un secondo momento.","138aee9b":"### Classificazione: Decision Tree Classifier\nCome primo classificatore utilizziamo DecisionTreeClassifier, che si basa sull'interrogazione del dataset in modo da poterlo dividere sempre pi\u00f9 in maniera specifica in modo da poter classificare meglio i dati. \nPer info: https:\/\/it.wikipedia.org\/wiki\/Albero_di_decisione","e2a51ca3":"Possiamo gestire la 'profondit\u00e0' di un DecisionTree attraverso l'attributo max_depth. Con profondit\u00e0 si intende il numero di domande massimo che il modello fa la dataset: in questo modo si combatte l'overfitting, ovvero la generalizzazione ottenuta solo su dati del dataset.\nOra andiamo a visualizzare l'andamento del grado di importanza di ogni feature in base alla profondit\u00e0 massima.","01710815":"Notiamo come maggiori sono le domande poste dal modello, pi\u00f9 si stabilizzano le percentuali di importanza di ogni feature. Questo perch\u00e8 il modello, all'aumentare della profondit\u00e0, \u00e8 pi\u00f9 soggetto a overfitting.","6efad236":"Notiamo come l'accuratezza vari al variare del numero massimo di interrogazioni al dataset, con accuratezza massima a 2. Come varia l'importanza di ogni feature in base alla profondit\u00e0? ","5aea84c3":"### Preprocessing dei dati: valori utili\nIl dataset contiene dati che potrebbero essere poco utili durante la classificazione: PASSENGERID e NAME probabilmente non sono utili per una classificazione: saranno feature che andremo a cancellare.\nInoltre togliamo tutte quelle feature che sono state trasformate","5bc1131e":"#### Last but not least: inviamo i nostri risultati a Kaggle\nPer inviare a Kaggle i risultati e osservare la bont\u00e0 della nostra classificazione, bisogna riportare tutte le operazioni fatte sul training set al test: osserviamo come \u00e8 strutturato il test set, e vediamo se dobbiamo apportare delle modifiche:","2c7ffffc":"Sul training set abbiamo un'accuratezza alta, circa del 91%, mentre sul testing set abbiamo un 73%. Il valore che ci interessa \u00e8 ovviamente il secondo: vogliamo avere una accuratezza maggiore quando non si hanno il valore delle etichette. Andiamo ad osservare come si distribuisce l'albero e se \u00e8 possibile aumentare il grado di accuratezza.","76674b86":"### Classificazione: Analisi dei risultati ottenuti","9f3855c8":"Prima di poter fare una classificazione, tutti i dati all'interno del dataset dovranno essere di tipo numerico: le feature EMBARKED e  CABIN sono dati testuali: dovremo trasformali. \nUtilizzeremo un metodo 'ad etichetta': daremo un numero ad ogni campo che sar\u00e0 identificativo della sua classe (es. a tutti i passeggeri che saranno saliti a Southampton daremo come valore 1, che \u00e8 identificativo della classe S nel dataset.)\n","cd5fe31d":"### Classificazione: Preparazione\nPrima di effettuare la classificazione, abbiamo necessit\u00e0 di creare i vari vettori contenenti i dati relativi al dataset:\n    - Target Names: \n        - ovvero il nome delle etichette, nel nostro caso sopravvissuto o non sopravvissuto\n    - Feature Names:\n        - Ovvero i nomi delle feature: nel nostro caso pclass, age, sibsp, parch, fare, embarked_,cabin_sex_\n    - Data:\n        - Ovvero i dati relativi a ogni campo del dataset: quindi il valore che ha ogni feature in ogni riga del dataset\n    - Target:\n        - Ovvero l'etichetta di ogni riga del dataset, che pu\u00f2 essere 0 o 1, ovvero sopravvissuto o non sopravvissuto."}}