{"cell_type":{"da42f9fe":"code","c46ed7ee":"code","a95b266e":"code","43a7aeb2":"code","a51816e7":"code","3bc4bca5":"code","d1e0acdc":"code","41e14c60":"code","41cebcf6":"code","2962e203":"code","46be92b2":"code","a2253458":"code","a1f04d69":"code","357de8b8":"code","4af22a71":"code","3c8de282":"code","11a5e680":"code","62061f61":"code","076963b1":"code","5fa0752f":"code","86bf356d":"code","28ac5b20":"code","3ebbef01":"code","1b33db9b":"code","f260c5c9":"code","27174bc3":"code","b12578be":"code","b3a46ec0":"code","32afc999":"code","a0117335":"code","96c4f586":"code","39cb3ead":"code","53420b4d":"code","1303c674":"code","9c81c287":"code","1b9be8a5":"code","4b692130":"code","ac45486c":"code","1f7596de":"code","fa298a61":"markdown","cf783924":"markdown","ddfdb568":"markdown","1b6ebf9d":"markdown","a26f973c":"markdown","69a9db69":"markdown","8cf9eda2":"markdown","4647e88b":"markdown","481c0f93":"markdown","c3fee274":"markdown","acbbeea5":"markdown","0edba0ff":"markdown","39c32ec7":"markdown","5d7037e5":"markdown","9e5a24bc":"markdown","421d2d37":"markdown","496bbdd1":"markdown","1fac1673":"markdown","791109a0":"markdown"},"source":{"da42f9fe":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c46ed7ee":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","a95b266e":"train.columns","43a7aeb2":"# Checking number of rows and columns\nprint('train shape',train.shape)\nprint('test shape',test.shape)","a51816e7":"from numpy import median, mean","3bc4bca5":"# Object and numerical columns\nobj_cols = [col for col in train.columns if 'cat' in col or 'target' in col]\nnum_cols = [col for col in train.columns if 'cont' in col or 'target' in col]","d1e0acdc":"# Categorical variables to SalePrice\nfrom numpy import mean\n\nfig, ax = plt.subplots(5, 2, figsize=(25, 40))\nfor var, subplot in zip(obj_cols, ax.flatten()):\n    sns.barplot(data = train, x=var, y='target', estimator = mean, ax=subplot)","41e14c60":"from scipy import stats","41cebcf6":"scipy.stats.pointbiserialr(train['cat7'].astype('category').cat.codes, train['target'].astype(float))","2962e203":"scipy.stats.pointbiserialr(train['cat6'].astype('category').cat.codes, train['target'].astype(float))","46be92b2":"from kmodes.kmodes import KModes","a2253458":"cluster_col = ['cat0','cat1','cat2',\n               'cat3','cat4','cat5', \n               'cat6', 'cat7', 'cat8', \n               'cat9']","a1f04d69":"# Elbow curve to find optimal K\n\ndf_cluster = train[cluster_col]\n\ncost = []\nK = range(2,5)\nfor num_clusters in list(K):\n    kmode = KModes(n_clusters=num_clusters, init = \"random\", n_init = 5, verbose=1)\n    kmode.fit_predict(df_cluster)\n    cost.append(kmode.cost_)\n    \nplt.plot(K, cost, 'bx-')\nplt.xlabel('No. of clusters')\nplt.ylabel('Cost')\nplt.title('Elbow Method For Optimal k')\nplt.show()","357de8b8":"# Final Cluster\n\nkmode = KModes(n_clusters=3, init =\"random\", n_init =5, verbose=1)\nclusters = kmode.fit_predict(df_cluster)\nclusters","4af22a71":"df_cluster.insert(0, \"cluster\", clusters, True)\ndf_cluster.insert(0, \"target\", train['target'], True)\ndf_cluster","3c8de282":"sns.barplot(data=df_cluster, x='cluster', y='target', estimator=mean)","11a5e680":"sns.boxplot(data=df_cluster, x='cluster', y='target')","62061f61":"# Correlation between categorical features\n\nplt.figure(figsize=(40, 20))\n\ncorr = train[num_cols].corr()\n\nax = sns.heatmap(\n        corr, \n        vmin=-1, vmax=1, center=0,\n        cmap=sns.diverging_palette(20, 220, n=200),\n        square=True,\n        annot=True\n    )\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n    );","076963b1":"# Statistic descriptive\n\ntrain[num_cols].describe()","5fa0752f":"# Boxplot \n\nplt.figure(figsize=(15, 10))\n\nsns.boxplot(data = train[num_cols].drop(['target'],axis=1))","86bf356d":"sns.boxplot(train['target'])","28ac5b20":"def f(row):\n    if row['target'] < 6:\n        val = '< 6'\n    else:\n        val = '>= 6'\n    return val","3ebbef01":"df_explore = train.copy()\n\ndf_explore['target_group'] = df_explore.apply(f, axis=1)","1b33db9b":"plt.figure(figsize=(15, 10))\n\ngroup_cols = ['cont0','cont1','cont2','cont3','cont4',\n             'cont5','cont6','cont7','cont8','cont9',\n             'cont10','cont11','cont12','cont13','target_group']\ndf_explore[group_cols].groupby(['target_group']).mean().unstack(0).plot.barh()","f260c5c9":"df_explore[group_cols].groupby(['target_group']).mean().diff(axis=0).iloc[1]","27174bc3":"from sklearn.decomposition import PCA\n\ndf_pca = train[num_cols]\ndf_pca = df_pca.drop(['target'], axis = 1)","b12578be":"pca = PCA()\nX_pca = pca.fit_transform(df_pca)\n\n# Convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nX_pca.head()","b3a46ec0":"X_pca.shape","32afc999":"pca_result = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix\n    columns=component_names,  # so the columns are the principal components\n    index=df_pca.columns,  # and the rows are the original features\n)\npca_result","a0117335":"pca_1 = ['cont2','cont10','cont12']\n\nplt.figure(figsize=(5, 5))\n\ncorr = train[pca_1].corr()\n\nax = sns.heatmap(\n        corr, \n        vmin=-1, vmax=1, center=0,\n        cmap=sns.diverging_palette(20, 220, n=200),\n        square=True,\n        annot=True\n    )\n\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n    );","96c4f586":"evr = pca.explained_variance_ratio_\nevr","39cb3ead":"plt.figure(figsize=(15, 10))\nsns.barplot(X_pca.columns,evr)","53420b4d":"cv = np.cumsum(evr)\ncv","1303c674":"plt.figure(figsize=(15, 10))\nplt.plot(X_pca.columns, cv, \"o-\")","9c81c287":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","1b9be8a5":"make_mi_scores(X_pca, train['target'], discrete_features=False)","4b692130":"from sklearn.feature_selection import RFECV\nfrom sklearn import preprocessing\n\nfrom xgboost import XGBRegressor","ac45486c":"X = train.drop(['target'], axis =1)\ny = train['target']\n\nordinal_encoder = preprocessing.OrdinalEncoder()\nX[cluster_col] = ordinal_encoder.fit_transform(X[cluster_col])\n\nestimator = XGBRegressor(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\")\n\nselector = RFECV(estimator, step=1, cv=10)\nselector = selector.fit(X, y)\n\nselector.support_","1f7596de":"selector.ranking_","fa298a61":"It seems there is no signal from categorical variables. What if we cluster them?","cf783924":"There is still no significant signal, but cluster 3 shows slightly lower median for target","ddfdb568":"# Step 2: Load the data\n\nNext, load the training and test data.  \n\nSet `index_col=0` in the code cell below to use the `id` column as the index. ","1b6ebf9d":"## RFE-CV","a26f973c":"### Deep dive target < 6 and target >= 6","69a9db69":"The categorical data seems lies in the same\/similar scales between -0.2 to 1.1","8cf9eda2":"# Step 4: Feature Engineering\n\nFinding the most important features to be included in the model in the next step","4647e88b":"There are some outliers for target < 6","481c0f93":"We can see cont12, cont10, cont7, and cont0 show a better signal of difference.","c3fee274":"### Explore numerical features to target","acbbeea5":"* Based on the cumulative variance ratio, PC1 is the highest. However, it's not that informative.\n* Based on mutual information between PC-i and target, PC1 is also the highest but the relationship is weak, ~0.01 near to zero, meaning both variables are independent.","0edba0ff":"# Step 3: Exploratory Data Analysis\n\nNext, understanding the features distribution and also correlation with target variable.","39c32ec7":"Based on RFE-CV, variables that are important are cat1, cat3, cat8, cont1, cont2, cont3, cont4, cont5, cont7, cont8, cont9, cont10, cont11, cont12, cont13","5d7037e5":"no variables show strong relationship to the target","9e5a24bc":"PC1 shows contrast between cont2 with cont10, cont12","421d2d37":"## PCA for Numerical Features","496bbdd1":"It seems 3 will be better as the number of clusters","1fac1673":"### Explore categorical features to target","791109a0":"# Step 1: Import Libraries"}}