{"cell_type":{"93dc027f":"code","239e41f5":"code","2f15d1f5":"code","579e6ca5":"code","5321ae9f":"code","2457ed21":"code","77883fc2":"code","e88ab88f":"code","9f1492b9":"code","0c06f5bf":"code","48511bba":"code","89209c9f":"code","f1dbd322":"markdown","bf312e6d":"markdown","bd707c09":"markdown","876b5315":"markdown","92037a93":"markdown","f99646ab":"markdown","2d3150a8":"markdown","3bc1e0a4":"markdown"},"source":{"93dc027f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","239e41f5":"from sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier, MetricVisualizer, Pool\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.neighbors import KernelDensity\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (20, 10)","2f15d1f5":"training_df = pd.read_csv('\/kaggle\/input\/api-access-behaviour-anomaly-dataset\/supervised_dataset.csv', index_col=False).drop('Unnamed: 0', axis=1)\nevaluation_df = pd.read_csv('\/kaggle\/input\/api-access-behaviour-anomaly-dataset\/remaining_behavior_ext.csv', index_col=False).drop('Unnamed: 0', axis=1)","579e6ca5":"training_df.columns, evaluation_df.columns, len(training_df)","5321ae9f":"print(\"training data class distribution\\n\", training_df['classification'].value_counts())\nprint(\"class distribution based on ALGO-X\\n\", evaluation_df['behavior_type'].value_counts())","2457ed21":"train, test = train_test_split(training_df, test_size=0.4, random_state=1073)\n\nparam_grid = {\n    \"iterations\": [10, 100, 1000], # number of trees\n    \"learning_rate\": [0.1, 0.01],\n    \"custom_loss\": ['CrossEntropy', 'AUC', 'Logloss', ]\n}\ngclf = GridSearchCV(estimator=CatBoostClassifier(), param_grid=param_grid, cv=4)","77883fc2":"gclf.fit(train.drop(['classification', 'source', '_id'], axis=1), train['classification'], cat_features=['ip_type'], verbose=1000)","e88ab88f":"gclf.best_params_","9f1492b9":"evaluation_cols_removed_df= evaluation_df.drop(['behavior_type', 'behavior', 'source', '_id'], axis=1)\ntest_cols_removed_df= test.drop(['source', '_id', 'classification'], axis=1)","0c06f5bf":"prediction=gclf.predict(evaluation_cols_removed_df)\nafter_prediction_pd = evaluation_df[['behavior_type', 'behavior', 'source']].copy()\nafter_prediction_pd['prediction']=prediction\neval_cols_evaluation_set = after_prediction_pd[['behavior_type', 'prediction']]\n\nprediction=gclf.predict(test_cols_removed_df)\ntest_after_prediction_pd = test[['classification']].copy()\ntest_after_prediction_pd['prediction']=prediction\ntest_cols_evaluation_set = test_after_prediction_pd[['classification', 'prediction']]","48511bba":"def calculate_misclassification(eval_cols, predicted_col, orig_col):\n    classes = dict(eval_cols[orig_col].value_counts())\n\n    misclassification_error = 0\n    for class_name in classes:\n        selection = eval_cols[ (eval_cols[orig_col] == class_name)]\n        weight = 1.0\/classes[class_name]\n        misclassifiction_cases = selection[(selection[predicted_col] != class_name)]\n        misclassification_count = len(misclassifiction_cases)\n        print({'class': class_name, 'weight': weight, 'count': len(selection), 'misclassification':misclassification_count})\n\n        misclassification_error += weight*misclassification_count\n    misclassification_error = misclassification_error \/ len(classes)\n    return misclassification_error","89209c9f":"print('Classification of test data')\nprint(\"Misclassification error: \",calculate_misclassification(test_cols_evaluation_set, 'prediction', 'classification'))\n\nprint(\"Classsification of evaluation_df\")\nprint(\"Misclassification error: \",calculate_misclassification(eval_cols_evaluation_set, 'prediction', 'behavior_type'))","f1dbd322":"## <span style=color:blue> Evaluation <\/span>\nEvaluation is performed on test_df as well as evaluationn_df. In the case of **test** dataset we can compare the predicted class label against the class labels manually marked. We can thhen evaluate the classifier.\n\nIn case of evaluation_df we can see if the predicted label is same as behavior_type predicted by ALGO-X ","bf312e6d":"## <span style=color:blue> Weighted misclassification error <\/span>\nThere is class imbalance between normal and outlier classes. Hence we measure the performance of the classifier in terms of a weighted misclassification error. Because there are only 2 classes, we can also analyze in terms of true-positive and false-positive for each class separately.\n<\/span>","bd707c09":"## <span style=color:blue> Using catboost to model the 2 class classifier <\/span>","876b5315":"# <span style=color:blue> Outlier classifier using catboost <\/span>","92037a93":"### <span style=color:blue> Load the training and evaluation datasets <\/span>","f99646ab":"Training data has a skewed distribution of classes with normals > outliers. The evaluation dataset shows that ALGO-X has classified more observations as outliers than as normals. We also have ***bot and attack classes*** which dont find representation in the training dataset. ","2d3150a8":"## Motivation\nWhen users access the application they do so via APIs. A specific sequence of APIs leads to a specific business logic becoming available to users. Sometimes malicious users attempt API access such that it results in a very different sequence of APIs compared to benign users. Depending on what businesss logic is being accessed, there can be multiple sequences of API calls which when aggregated becomes an API call graph for that user. When there are 100s of users, many users generate exact or similar API call graphs.  In such cases users are grouped into a single cluster, all of them having the same graph as per a graph clustering algorithm. These clusters were manually analyzed and each graph was manually classified as ***normal*** or ***outlier*** and this is available in the ***classification*** column. Each row in ***supervised_dataset.csv*** is one such graph. In addition to the class label, some metrics of the graph are also available and these metrics can be used as features. \n\nThe file ***remaining_behavior_ext.csv*** contains the same data as a ***supervised_dataset.csv*** but does not have a ***classification*** column. The goal is to predict the class label for each row of ***remaining_behavior_ext.csv***. However, the column ***behavior_type*** coltains a class label generated by another algorithm called ALGO-X. When we predict the class label using a classsifier trained using ***supervised_dataset.csv***, we can see whether ALGO-X is as good as the trained classifier. It is to be noted that ALGO-X is a heuristic model and is a weak classifier.","3bc1e0a4":"Misclassification error is zero for test data. However for the evaluation dataset we see that the misclassification is high. ALGO-X and the trained classifier seem to agree on the **outlier** class but not on **normal** class. **bot** and **attack** do not have training samples. We will have to handle these sparately (open set learning). normals in the evaluation set need to be re-classified based on classifier output or the classifier itself may need additional samples for training. Using the trained classifier as a send opinion, we can conclude that if ALGO-X classifies as an outlier and the classifier also predicts an outlier, then we could consider the event as a true positive."}}