{"cell_type":{"c65da8b9":"code","423e2f51":"code","256c06d0":"code","2bc2760c":"code","3f40df6a":"code","da6ec9d1":"code","d85d1b5c":"code","dcb889ab":"code","4be4275e":"code","13ceafd7":"code","c6afaa24":"code","e2f2a141":"code","a2ba81ee":"code","615361c9":"code","5cfc314b":"code","15ced216":"code","341e676d":"code","069ee3d6":"code","5a456d31":"code","892cec8c":"code","113c0b65":"code","8ce07d73":"code","5a140729":"code","a9c1e8fe":"code","2b996976":"markdown","e9505108":"markdown","6ec6ee12":"markdown","ce862348":"markdown","bf69b65a":"markdown","744f113e":"markdown","f8f96d6e":"markdown","d0eda218":"markdown","fa8b312d":"markdown","25dd9e89":"markdown","9ec82522":"markdown","c45ec664":"markdown","b95c581f":"markdown","fdb7da81":"markdown","cb40e7e9":"markdown","05e26b28":"markdown","e11650a0":"markdown","572dbb62":"markdown","a832fdf1":"markdown","f84c2f08":"markdown","586f62e8":"markdown","3015a0c1":"markdown","99162a72":"markdown","fc7a92c3":"markdown"},"source":{"c65da8b9":"from IPython.display import HTML\nfrom IPython.display import IFrame\n\nHTML('<iframe width=\"560\" height=\"315\" src=\"\/\/www.youtube.com\/embed\/nBnCsMYm2yQ\" frameborder=\"0\" allowfullscreen><\/iframe>')","423e2f51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","256c06d0":"station_df = pd.read_csv(\"\/kaggle\/input\/aws-open-source-weather-transaction-and-metadata\/2019.csv\", header = 1, \\\n                 names = ['station_code', 'w_date', 'element_type', 'element_value', 'measurement_flag', 'quality_flag', \\\n                          'source_flag', 'obs_time'], \\\n                delimiter = ',')\nstation_df.head()","2bc2760c":"unique_statuion = station_df['element_type'].unique()\nunique_statuion","3f40df6a":"unique_statuion.size","da6ec9d1":"station_df['w_date'].max()","d85d1b5c":"df_grp_dt = station_df.groupby('w_date').count().sort_values(by = 'station_code', ascending = False)\ndf_grp_dt['station_code'].head()","dcb889ab":"station_df[station_df.measurement_flag == 'T'].head()","4be4275e":"station_df[station_df.quality_flag == 'W'].head(5)","13ceafd7":"station_df[station_df.source_flag == 'E'].head(5)","c6afaa24":"import pandas as pd\nmd_station_df = pd.read_csv(\"\/kaggle\/input\/aws-open-source-weather-transaction-and-metadata\/ghcnd-stations.txt\", header = None, sep = '\\s+', \\\n                         names = ['station_id', 'latitude', 'longitude', 'elevation', 'state', 'name', 'gsn_flag', \\\n                                  'hcn_flag', 'wmo_id'])\nmd_station_df.head()\n","e2f2a141":"import folium\nfrom folium import features\n\n\nmd_city_cordinates = md_station_df[md_station_df.state.str.contains(\"BERLIN\", na=False)][['state','latitude', 'longitude']]\nmd_city_cordinates\n\nberlin_location = [md_city_cordinates.iloc[0].latitude, md_city_cordinates.iloc[0].longitude]\n\n#tiles=\"https:\/\/1.base.maps.api.here.com\/maptile\/2.1\/maptile\/newest\/normal.day\/{z}\/{x}\/{y}\/256\/png8?lg=eng&app_id=%s&app_code=%s\"\n\nm = folium.Map(location=berlin_location, zoom_start=11, tiles=\"openstreetmap\", attr=\"HERE.com\")\n\n# mark each station as a point\nfor index, row in md_city_cordinates.iterrows():\n    folium.CircleMarker([row['latitude'], row['longitude']],\n                        radius=15,\n                        popup=row['state'],\n                        fill_color=\"blue\", # divvy color\n                        con_color='white',\n                       ).add_to(m)\n\n    \n\nm","a2ba81ee":"md_station_df[(md_station_df.station_id.str.startswith('GM')) & (md_station_df.state.str.startswith('MUNCHEN'))]","615361c9":"import pandas as pd\nfrom io import StringIO\n\nfile = \"\/kaggle\/input\/aws-open-source-weather-transaction-and-metadata\/ghcnd-countries.txt\"\n\ndef parse_country_file(filename):\n    with open(filename) as f:\n        for line in f:\n            yield line.strip().split(' ', 1)\n\ncountry_df = pd.DataFrame(parse_country_file(file))\ncountry_df.columns=['country_code', 'country_name']\ncountry_df[country_df['country_name'].isin(['Germany', 'Spain', 'Italy', 'France', 'United Kingdom'])]","5cfc314b":"import pandas as pd\nfrom io import StringIO\n\nfile = \"\/kaggle\/input\/aws-open-source-weather-transaction-and-metadata\/ghcnd-states.txt\"\n\ndef parse_country_file(filename):\n    with open(filename) as f:\n        for line in f:\n            yield line.strip().split(' ', 1)\n\nstate_df = pd.DataFrame(parse_country_file(file))\nstate_df.head()","15ced216":"import pandas as pd\nfile = \"\/kaggle\/input\/aws-open-source-weather-transaction-and-metadata\/ghcnd-inventory.txt\"\nghcnd_inventory_df = pd.read_csv(file, sep = '\\s+', header=None, names = ['staion_id','latitude', 'longitude', 'element', 'firstyear', 'lastyear'])\nghcnd_inventory_df.head()","341e676d":"import matplotlib.pyplot as plt\n\nsydney_station_id = md_station_df[md_station_df.state.str.contains('SYDNEY', na=False)].station_id.unique().tolist()\nsydney_station_data = station_df[station_df.station_code.isin(sydney_station_id)]\n\nsydney_data_plot = sydney_station_data[sydney_station_data.element_type.str.contains(\"TAVG\")][['station_code', 'w_date', 'element_type']]\nsydney_data_plot['element_type_celcius'] = sydney_station_data.element_value\/10\n\nsydney_data_pyplot = sydney_data_plot\nsydney_data_pyplot.w_date = pd.to_datetime(sydney_data_pyplot['w_date'], format='%Y%m%d')\nsydney_data_pyplot.set_index(['w_date'],inplace=True)\n\nplt.figure(figsize = (20,4))\nsydney_data_pyplot.plot()\n","069ee3d6":"hamburg_station_id = md_station_df[md_station_df.state.str.contains('ERLANGEN', na=False)].station_id.unique().tolist()\nhamburg_station_data =  station_df[station_df.station_code.isin(hamburg_station_id)]\nhamburg_data_plot = hamburg_station_data[hamburg_station_data.element_type.str.contains(\"TAVG\")][['station_code', 'w_date', 'element_type']]\nhamburg_data_plot['element_type_celcius'] = hamburg_station_data.element_value\/10\n\ntokyo_station_id = md_station_df[md_station_df.state.str.contains('TOKYO', na=False)].station_id.unique().tolist()\ntokyo_station_data =  station_df[station_df.station_code.isin(tokyo_station_id)]\ntokyo_data_plot = tokyo_station_data[tokyo_station_data.element_type.str.contains(\"TAVG\")][['station_code', 'w_date', 'element_type']]\ntokyo_data_plot['element_type_celcius'] = tokyo_station_data.element_value\/10\n\nsydney_station_id = md_station_df[md_station_df.state.str.contains('SYDNEY', na=False)].station_id.unique().tolist()\nsydney_station_data =  station_df[station_df.station_code.isin(sydney_station_id)]\nsydney_data_plot = sydney_station_data[sydney_station_data.element_type.str.contains(\"TAVG\")][['station_code', 'w_date', 'element_type']]\nsydney_data_plot['element_type_celcius'] = sydney_station_data.element_value\/10","5a456d31":"import seaborn as sns\n\n#bts_data_seaplot = bts_data_plot\nhamburg_data_plot.w_date = pd.to_datetime(hamburg_data_plot['w_date'], format='%Y%m%d')\ntokyo_data_plot.w_date = pd.to_datetime(tokyo_data_plot['w_date'], format='%Y%m%d')\nsydney_data_plot.w_date = pd.to_datetime(sydney_data_plot['w_date'], format='%Y%m%d')\n\nplt.figure(figsize = (20,4))\nsns.set(rc={'axes.facecolor':'cyan', 'figure.facecolor':'cornflowerblue'})\n#sns.set_style(\"darkgrid\")\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsns.lineplot(hamburg_data_plot[\"w_date\"],hamburg_data_plot[\"element_type_celcius\"], color = \"red\")\nsns.lineplot(tokyo_data_plot[\"w_date\"],tokyo_data_plot[\"element_type_celcius\"], color = \"darkslategrey\")\nsns.lineplot(sydney_data_plot[\"w_date\"],sydney_data_plot[\"element_type_celcius\"], color = \"steelblue\")\nplt.show()","892cec8c":"station_tran_ms_df = pd.merge(station_df, md_station_df, left_on = ['station_code'], right_on = ['station_id'], \\\n                                how='left')\nstation_tran_ms_df.head()","113c0b65":"station_tran_ms_tmax_df = station_tran_ms_df[(station_tran_ms_df.element_type.str.contains(\"TMAX\" , na=False)) & \n                                            (station_tran_ms_df.station_code.str.startswith('GM', na=False))]\nmax_tmp = station_tran_ms_tmax_df['element_value'].max()\nmax_tmp","8ce07d73":"max_tmp_loc = station_tran_ms_tmax_df[station_tran_ms_tmax_df.element_value == max_tmp][['state','latitude', 'longitude']]\nmax_tmp_loc\n\nplt_location = [max_tmp_loc.iloc[0].latitude, max_tmp_loc.iloc[0].longitude]\n\nmpl = folium.Map(location=plt_location, zoom_start=11)\nfolium.TileLayer('stamenterrain').add_to(mpl)\n\n\n# mark each station as a point\nfor index, row in max_tmp_loc.iterrows():\n    folium.CircleMarker([row['latitude'], row['longitude']],\n                        radius=15,\n                        popup=row['state'],\n                        fill_color=\"blue\", # divvy color\n                        con_color='white',\n                       ).add_to(mpl)\n\n    \n\nmpl","5a140729":"station_tran_snowfall_max_df = station_tran_ms_df[station_tran_ms_df.element_type.str.contains(\"SNOW\" , na=False)]\nmax_snowfall = station_tran_snowfall_max_df['element_value'].max()\nmax_snowfall\n","a9c1e8fe":"max_snowfall_loc = station_tran_snowfall_max_df[station_tran_snowfall_max_df.element_value == max_snowfall][['state','latitude', 'longitude']]\nplt_location = [max_snowfall_loc.iloc[0].latitude, max_snowfall_loc.iloc[0].longitude]\n\nmpl = folium.Map(location=plt_location, zoom_start=11, tiles=\"openstreetmap\", attr=\"HERE.com\")\n\n# mark each station as a point\nfor index, row in max_snowfall_loc.iterrows():\n    folium.CircleMarker([row['latitude'], row['longitude']],\n                        radius=15,\n                        popup=row['state'],\n                        fill_color=\"blue\", # divvy color\n                        con_color='white',\n                       ).add_to(mpl)\n\n    \n\nmpl","2b996976":"The state codes are used in the station identification number. In the table below CODE is the FIPS country code of the country where the station is located.","e9505108":"#### Find maximum temperature in Germany","6ec6ee12":"#### Check station details for a city e.g. M\u00fcnchen","ce862348":"** If you like this notebook, please upvote it **","bf69b65a":"## Country code master data\n","744f113e":"### Plot Average temperature in Sydney for last N months - on availabiliity","f8f96d6e":"### Exploration of data retreived from station\n#### Summary of Date format\nID = 11 character station identification code. Please see ghcnd-stations section below for an explantation  \nYEAR\/MONTH\/DAY = 8 character date in YYYYMMDD format (e.g. 19860529 = May 29, 1986)  \nELEMENT = 4 character indicator of element type  \nDATA VALUE = 5 character data value for ELEMENT  \nM-FLAG = 1 character Measurement Flag  \nQ-FLAG = 1 character Quality Flag  \nS-FLAG = 1 character Source Flag  \nOBS-TIME = 4-character time of observation in hour-minute format (i.e. 0700 =7:00 am)  \n\nThe fields are comma delimited and each row represents one station-day. ","d0eda218":"### History","fa8b312d":"# GHCND inventory master data\n\nThis is the periods of record for each station and element  \n\n### Data structure\n\n- ID = the station identification code. Please see \u201cghcnd-stations.txt\u201d for a complete list of stations and their metadata.  \n- LATITUDE = the latitude of the station (in decimal degrees).  \n- LONGITUDE = the longitude of the station (in decimal degrees).  \n- ELEMENT = the element type. See section III for a definition of elements.  \n- FIRSTYEAR = the first year of unflagged data for the given element.  \n- LASTYEAR = the last year of unflagged data for the given element. ","25dd9e89":"### Compare average temparature of Erlangen, Tokyo and Sydney","9ec82522":"## Exploration of Station meta data","c45ec664":"### Data definition and collection\n\n#### GHCN \n- The Global Historical Climatology Network (GHCN) is an integrated database of climate summaries from land surface stations across the globe. \n- GHCN-Daily contains records from over 100,000 stations in 180 countries and territories.\n- The data are obtained from more than 20 sources. Some data are more than 175 years old.\n- NCEI provides numerous daily variables, including maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about one half of the stations \n  report precipitation only\n  \n##### Data description\nhttps:\/\/www.ncdc.noaa.gov\/ghcn-daily-description\n\n##### Collection\nThe data can be collected from S3 buckets. Here I collected it beforehand and put into aws-data folder for 2019.  \nFor detail information the link is as below:  \nhttps:\/\/docs.opendata.aws\/noaa-ghcn-pds\/readme.html  \nQuestion for data quality should be addressed at noaa.bdp@noaa.gov.","b95c581f":"### Variable and feature details\n\nThese variables have the following definitions:  \n\n- ID = the station identification code.  \n    The first two characters denote the FIPS country code. Details for FIPS country code https:\/\/www.geodatasource.com\/resources\/tutorials\/international-country-code-fips-versus-iso-3166\/     \n    The third character is a network code that identifies the station numbering system used  \n    0 = unspecified (station identified by up to eight alphanumeric characters)  \n    1 = Community Collaborative Rain, Hail,and Snow (CoCoRaHS) based identification number. To ensure consistency with with GHCN \n    Daily, all numbers in the original CoCoRaHS IDs have been left-filled to make them all four digits long. In addition, the \n    characters \u201c-\u201d and \u201c_\u201d have been removed to ensure that the IDs do not exceed 11 characters when preceded by \u201cUS1\u201d. For \n    example, the CoCoRaHS ID \u201cAZ-MR-156\u201d becomes \u201cUS1AZMR0156\u201d in GHCN-Daily  \n- LATITUDE = latitude of the station (in decimal degrees).  \n- LONGITUDE = longitude of the station (in decimal degrees).  \n- STATE = U.S. postal code for the state (for U.S. and Canadian stations only).  \n- NAME = name of the station.  \n- GSN FLAG = flag that indicates whether the station is part of the GCOS Surface Network (GSN). \n- HCN\/CRN FLAG = flag that indicates whether the station is part of the U.S. Historical Climatology Network (HCN). T\n- WMO ID is the World Meteorological Organization (WMO) number for the station. If the station has no WMO number (or one has not  \n    yet been matched to this station), then the field is blank.\n","fdb7da81":"### Projection for all stations in a city\/ town e.g. Berlin","cb40e7e9":"### Check for number of observations per day","05e26b28":"### Q-flag i.e. quality flag  \nQ-FLAG is the measurement quality flag. here are fourteen possible values:  \nBlank = did not fail any quality assurance check  \nD = failed duplicate check  \nG = failed gap check  \nI = failed internal consistency check  \nK = failed streak\/frequent-value check  \nL = failed check on length of multiday period  \nM = failed mega consistency check  \nN = failed naught check  \nO = failed climatological outlier check  \nR = failed lagged range check  \nS = failed spatial consistency check  \nT = failed temporal consistency check  \nW = temperature too warm for snow  \nX = failed bounds check  \nZ = flagged as a result of an official Datzilla Investigation ","e11650a0":"### Retreive unique element_types\n\nThe five core elements are:  \nPRCP = Precipitation (tenths of mm)  \nSNOW = Snowfall (mm)  \nSNWD = Snow depth (mm)  \nTMAX = Maximum temperature (tenths of degrees C)  \nTMIN = Minimum temperature (tenths of degrees C)  ","572dbb62":"### Background\n\nNOAA\u2019s National Centers for Environmental Information (NCEI) hosts and provides access to one of the most significant archives on earth, with comprehensive oceanic, atmospheric, and geophysical data. From the depths of the ocean to the surface of the sun and from million-year-old ice core records to near-real-time satellite images, NCEI is the Nation\u2019s leading authority for environmental information.\n\nThe five \"fundamental activities\" of NOAA are:\n\n- Monitoring and observing Earth systems with instruments and data collection networks.\n- Understanding and describing Earth systems through research and analysis of that data.\n- Assessing and predicting the changes of these systems over time.\n- Engaging, advising, and informing the public and partner organizations with important information.\n- Managing resources for the betterment of society, economy and environment.\n\nDetails of NOAA - https:\/\/en.wikipedia.org\/wiki\/National_Oceanic_and_Atmospheric_Administration","a832fdf1":"## Aditional attributes\n### M-flag i.e. measurement flag  \nMFLAG is the measurement flag. There are ten possible values:  \nBlank = no measurement information applicable  \nB = precipitation total formed from two 12-hour totals  \nD = precipitation total formed from four six-hour totals  \nH = represents highest or lowest hourly temperature (TMAX or TMIN) or the average of hourly values (TAVG)  \nK = converted from knots  \nL = temperature appears to be lagged with respect to reported hour of observation  \nO = converted from oktas  \nP = identified as \u201cmissing presumed zero\u201d in DSI 3200 and 3206  \nT = trace of precipitation, snowfall, or snow depth  \nW = converted from 16-point WBAN code (for wind direction) ","f84c2f08":"## EDA","586f62e8":"## State code master data\n\nThe state codes are used in the station identification number, the table below CODE = is the POSTAL code of the U.S. state\/territory or Canadian province where the station is located.","3015a0c1":"### Merging station weather with master data","99162a72":"### Check station with maximum snowfall","fc7a92c3":"### S-flag i.e. source flag\n\nS-FLAG is the source flag for the observation. There are twenty nine possible values (including blank, upper and lower case letters):\n\nBlank = No source (i.e., data value missing)  \n0 = U.S. Cooperative Summary of the Day (NCDC DSI-3200)  \n6 = CDMP Cooperative Summary of the Day (NCDC DSI-3206)  \n7 = U.S. Cooperative Summary of the Day \u2013 Transmitted via WxCoder3 (NCDC SI-3207)  \nA = U.S. Automated Surface Observing System (ASOS) real-time data (since January 1, 2006)  \na = Australian data from the Australian Bureau of Meteorology  \nB = U.S. ASOS data for October 2000-December 2005 (NCDC DSI-3211)  \nb = Belarus update  \nC = Environment Canada  \nE = European Climate Assessment and Dataset (Klein Tank et al., 2002)  \nF = U.S. Fort data  \nG = Official Global Climate Observing System (GCOS) or other government-supplied data  \nH = High Plains Regional Climate Center real-time data  \nI = International collection (non U.S. data received through personal contacts)  \nK = U.S. Cooperative Summary of the Day data digitized from paper observer forms (from 2011 to present)  \nM = Monthly METAR Extract (additional ASOS data)  \nN = Community Collaborative Rain, Hail,and Snow (CoCoRaHS)  \nQ = Data from several African countries that had been \u201cquarantined\u201d, that is, withheld from public release until permission was granted from the respective meteorological services  \nR = NCEI Reference Network Database (Climate Reference Network and Regional Climate Reference Network)  \nr = All-Russian Research Institute of Hydro-meteorological Information-World Data Center  \nS = Global Summary of the Day (NCDC DSI-9618)NOTE: \u201cS\u201d values are derived from hourly synoptic reports exchanged on the Global    Telecommunications System (GTS). Daily values derived in this fashion may differ significantly from \u201ctrue\u201d daily data, particularly for precipitation (i.e., use with caution).  \ns = China Meteorological Administration\/National Meteorological Information Center\/Climatic Data Center (http:\/\/cdc.cma.gov.cn)  \nT = SNOwpack TELemtry (SNOTEL) data obtained from the U.S. Department of Agriculture\u2019s Natural Resources Conservation Service  \nU = Remote Automatic Weather Station (RAWS) data obtained from the Western Regional Climate Center  \nu = Ukraine update  \nW = WBAN\/ASOS Summary of the Day from NCDC\u2019s Integrated Surface Data (ISD).  \nX = U.S. First-Order Summary of the Day (NCDC DSI-3210)  \nZ = Datzilla official additions or replacements  \nz = Uzbekistan update  \n\n#### Recommendation\nWhen data are available for the same time from more than one source, the highest priority source is chosen according to the following priority order (from highest to lowest): - Z,R,0,6,C,X,W,K,7,F,B,M,r,E,z,u,b,s,a,G,Q,I,A,N,T,U,H,S  \n"}}