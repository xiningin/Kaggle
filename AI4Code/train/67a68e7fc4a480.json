{"cell_type":{"df9f2cfe":"code","5d66a32a":"code","101722c7":"code","ee815a0a":"code","dcfb76fe":"code","c0b3e8a5":"code","fdfe4f5c":"code","af96a7ce":"code","154645cb":"code","699dc6c8":"code","6a64fc94":"code","71628f35":"code","19a66e3b":"code","c4694aa4":"code","b7f07e29":"code","31f6ced0":"code","a39a765d":"code","c87569d7":"code","24eb7163":"code","86f16641":"code","d168ed52":"code","15d0ed9c":"code","b8ecc58e":"code","1eaca692":"code","49247e7a":"code","9cd3b23a":"code","e94a55fe":"code","466beff6":"code","b0ccf3b6":"markdown","728d0851":"markdown","626b4a35":"markdown","dceec136":"markdown","d44357ed":"markdown","88c2b417":"markdown","66c71cce":"markdown","bd90a66a":"markdown","e14c2975":"markdown","bfeeda7c":"markdown","f8581e2c":"markdown","ca1a7797":"markdown"},"source":{"df9f2cfe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport statsmodels.formula.api as smf                # logistic regression\nfrom sklearn.model_selection import train_test_split # train\/test split\nfrom sklearn.linear_model import LogisticRegression  # logistic regression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import confusion_matrix         # confusion matrix\nfrom sklearn.metrics import roc_auc_score            # auc score\nfrom sklearn.neighbors import KNeighborsClassifier   # KNN for classification\nfrom sklearn.preprocessing import StandardScaler     # standard scaler\n\n# CART model packages\nfrom sklearn.tree import DecisionTreeClassifier      # classification trees\nfrom sklearn.tree import export_graphviz             # exports graphics\nfrom six import StringIO                             # saves objects in memory\nfrom IPython.display import Image                    # displays on frontend\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV  # hyperparameter tuning\nfrom sklearn.metrics import make_scorer                 # customizable scorer\n\n# Ensemble Modeling\nfrom sklearn.ensemble import RandomForestClassifier     # random forest\nfrom sklearn.ensemble import GradientBoostingClassifier # gbm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d66a32a":"train_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv', index_col='id')","101722c7":"train_df.head()","ee815a0a":"test_df.head()","dcfb76fe":"# Explore the dataset - row and column\nprint(train_df.shape)\nprint(test_df.shape)","c0b3e8a5":"# Explore the datatype per each column\nprint(train_df.dtypes)","fdfe4f5c":"print(test_df.dtypes)","af96a7ce":"# Show the number of missing values in the dataset.\ntrain_df.isnull().sum(axis = 0)","154645cb":"test_df.isnull().sum(axis = 0)","699dc6c8":"# Show the number of target values in the dataset.\n\ny = train_df[\"target\"]\n\nsns.countplot(y)\n\n\ntarget_temp = train_df.target.value_counts()\n\nprint(target_temp)","6a64fc94":"# Plot data distributions for every variables\nfig, ax = plt.subplots(figsize = (15, 70))\ni = 0\nFEATS = list(train_df.columns)\n\nfor val in FEATS:\n    plt.subplot(21, 5, i + 1)\n    sns.kdeplot(data = train_df, x = val)\n    i = i + 1","71628f35":"# Generate a correlation matrix.\nprint(train_df.corr())","19a66e3b":"# perform a robust scaler transform of the dataset\nfrom sklearn.preprocessing import MinMaxScaler\ntrans = MinMaxScaler()\ntrain_df = trans.fit_transform(train_df)","c4694aa4":"from pandas import DataFrame\n# convert the array back to a dataframe\ntrain_df = DataFrame(train_df)\n# summarize\nprint(train_df.describe())","b7f07e29":"train_df.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n                    'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n                    'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n                    'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n                    'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n                    'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n                    'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n                    'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n                    'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n                    'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'target']","31f6ced0":"# instantiating a logistic model object\nlogistic_full = smf.logit(formula = \"\"\"  target ~ \n                                         f1 + f2 + f3 + f4 + f5 + f6 + f7 + f8 + f9 + f10 +\n                                         f11 + f12 + f13 + f14 + f15 + f16 + f17 + f18 + f19 + f20 + \n                                         f21 + f22 + f23 + f24 + f25 + f26 + f27 + f28 + f29 + f30 +\n                                         f31 + f32 + f33 + f34 + f35 + f36 + f37 + f39 + f40 +\n                                         f41 + f42 + f43 + f44 + f45 + f46 + f47 + f48 + f49 + f50 +\n                                         f51 + f53 + f54 + f55 + f56 + f57 + f58 + f59 + f60 + \n                                         f61 + f62 + f63 + f64 + f65 + f66 + f67 + f68 + f69 + f70 +\n                                         f71 + f73 + f74 + f75 + f76 + f77 + f78 + f79 + f80 +\n                                         f81 + f82 + f83 + f84 + f85 + f86 + f87 + f88 + f89 + f90 +\n                                         f91 + f93 + f94 + f95 + f96 + f97 + f98 + f99 \"\"\",\n                                         data = train_df)\n\n# fitting the model object\nresults_full = logistic_full.fit()\n\n\n# checking the results SUMMARY\nresults_full.summary()","a39a765d":"candidate_dict = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n                  'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n                  'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n                  'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f39', 'f40',\n                  'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n                  'f51', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n                  'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n                  'f71', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n                  'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n                  'f91', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99']\n\ny_train = train_df.iloc[:,-1]\nX_train = train_df.loc[:, candidate_dict]\nX_train = train_df.drop('target', axis=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=1) ","c87569d7":"# INSTANTIATING a logistic regression model\nlogreg = LogisticRegression(solver ='liblinear',\n                            C = 1.0,\n                            warm_start = True,\n                            random_state = 1)\n\n# FITTING the training data\nlogreg_fit = logreg.fit(X_train, y_train)\n\n\n# PREDICTING based on the testing set\nlogreg_pred = logreg_fit.predict(X_val)\n\n\n# SCORING the results\nprint('Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\nprint('Testing  ACCURACY:', logreg_fit.score(X_val, y_val).round(4))\n\n\n# SCORING with AUC\nprint('AUC Score        :', roc_auc_score(y_true  = y_val,\n                                          y_score = logreg_pred).round(decimals = 4))\n\n# unpacking the confusion matrix\nlogreg_tn, \\\nlogreg_fp, \\\nlogreg_fn, \\\nlogreg_tp = confusion_matrix(y_true = y_val, y_pred = logreg_pred).ravel()\n\n\n# printing each result one-by-one\nprint(f\"\"\"\nTrue Negatives : {logreg_tn}\nFalse Positives: {logreg_fp}\nFalse Negatives: {logreg_fn}\nTrue Positives : {logreg_tp}\n\"\"\")\n\n# creating a confusion matrix\nprint(confusion_matrix(y_true = y_val,\n                       y_pred = logreg_pred))","24eb7163":"# INSTANTIATING a classification tree object\npruned_tree = DecisionTreeClassifier(max_depth = 5,\n                                     min_samples_leaf = 17,\n                                     criterion = 'entropy',\n                                     random_state = 1)\n\n# FITTING the training data\npruned_tree_fit  = pruned_tree.fit(X_train, y_train)\n\n\n# PREDICTING on new data\npruned_tree_pred = pruned_tree_fit.predict(X_val)\n\n\n# SCORING the model\nprint('Training ACCURACY:', pruned_tree_fit.score(X_train, y_train).round(4))\nprint('Testing  ACCURACY:', pruned_tree_fit.score(X_val, y_val).round(4))\nprint('AUC Score        :', roc_auc_score(y_true  = y_val,\n                                          y_score = pruned_tree_pred).round(4))\n\n# unpacking the confusion matrix\npruned_tree_tn, \\\npruned_tree_fp, \\\npruned_tree_fn, \\\npruned_tree_tp = confusion_matrix(y_true = y_val, y_pred = pruned_tree_pred).ravel()\n\n\n# printing each result one-by-one\nprint(f\"\"\"\nTrue Negatives : {pruned_tree_tn}\nFalse Positives: {pruned_tree_fp}\nFalse Negatives: {pruned_tree_fn}\nTrue Positives : {pruned_tree_tp}\n\"\"\")\n\n# creating a confusion matrix\nprint(confusion_matrix(y_true = y_val,\n                       y_pred = pruned_tree_pred))","86f16641":"# INSTANTIATING a random forest model with hyperparameters tuned values\nrandom_forest = RandomForestClassifier(n_estimators     = 350,\n                                       criterion        = 'gini',\n                                       max_depth        = 7,\n                                       max_features     = 'auto',\n                                       min_samples_leaf = 1,\n                                       bootstrap        = True,\n                                       warm_start       = True,\n                                       random_state     = 1)\n\n# FITTING the training data\nrandom_forest_fit = random_forest.fit(X_train, y_train)\n\n\n# PREDICTING based on the testing set\nrandom_forest_fit_pred = random_forest_fit.predict(X_val)\n\n\n# SCORING the results\nprint('Training ACCURACY:', random_forest_fit.score(X_train, y_train).round(4))\nprint('Testing  ACCURACY:', random_forest_fit.score(X_val, y_val).round(4))\n\n\n# saving AUC score\nprint('AUC Score        :', roc_auc_score(y_true  = y_val,\n                                          y_score = random_forest_fit_pred).round(4))\n\n# unpacking the confusion matrix\nrf_tn, \\\nrf_fp, \\\nrf_fn, \\\nrf_tp = confusion_matrix(y_true = y_val, y_pred = random_forest_fit_pred).ravel()\n\n\n# printing each result one-by-one\nprint(f\"\"\"\nTrue Negatives : {rf_tn}\nFalse Positives: {rf_fp}\nFalse Negatives: {rf_fn}\nTrue Positives : {rf_tp}\n\"\"\")\n\n# creating a confusion matrix\nprint(confusion_matrix(y_true = y_val,\n                       y_pred = random_forest_fit_pred))","d168ed52":"# INSTANTIATING a Gradient Boosted Machines\ngbm = GradientBoostingClassifier(loss          = 'deviance',\n                                 learning_rate = 0.1,\n                                 n_estimators  = 100,\n                                 criterion     = 'friedman_mse',\n                                 max_depth     = 2,\n                                 warm_start    = False,\n                                 random_state  = 1)\n\n# FITTING the training data\ngbm_fit = gbm.fit(X_train, y_train)\n\n\n# PREDICTING based on the testing set\ngbm_pred = gbm_fit.predict(X_val)\n\n\n# SCORING the results\nprint('Training ACCURACY:', gbm_fit.score(X_train, y_train).round(4))\nprint('Testing ACCURACY :', gbm_fit.score(X_val, y_val).round(4))\nprint('AUC Score        :', roc_auc_score(y_true  = y_val,\n                                          y_score = gbm_pred).round(4))\n\n# unpacking the confusion matrix\ngbm_tn, \\\ngbm_fp, \\\ngbm_fn, \\\ngbm_tp = confusion_matrix(y_true = y_val, y_pred = gbm_pred).ravel()\n\n\n# printing each result one-by-one\nprint(f\"\"\"\nTrue Negatives : {gbm_tn}\nFalse Positives: {gbm_fp}\nFalse Negatives: {gbm_fn}\nTrue Positives : {gbm_tp}\n\"\"\")\n\n# creating a confusion matrix\nprint(confusion_matrix(y_true = y_val,\n                       y_pred = gbm_pred))","15d0ed9c":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout","b8ecc58e":"# Construct the Sequential model\n\nmodel = Sequential()\nmodel.add(Dense(128, activation=\"relu\", input_shape = (X_train.shape[1],))) # Hidden Layer 1 that receives the Input from the Input Layer\n\nmodel.add(Dense(64, activation=\"relu\")) # Hidden Layer 2\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(32, activation=\"relu\")) # Hidden Layer 3\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(16, activation=\"relu\")) # Hidden Layer 4\nmodel.add(Dropout(0.2))\n\n\nmodel.add(Dense(1, activation=\"sigmoid\")) # Outout Layer\n\nmodel.summary()","1eaca692":"# Compile the model\nmodel.compile(optimizer='adam', loss = \"binary_crossentropy\", metrics = ['accuracy'])","49247e7a":"# Fit the model\nmodel.fit(X_train, y_train, batch_size = 64, epochs = 100)","9cd3b23a":"# Validate the model\nvalidation_loss, validation_accuracy = model.evaluate(X_val, y_val, batch_size=32)\nprint(\"Loss: \"+ str(np.round(validation_loss, 3)))\nprint(\"Accuracy: \"+ str(np.round(validation_accuracy, 3)))","e94a55fe":"# Predict the Keras NN model with the test set\ntest_df = trans.fit_transform(test_df)\ntest_df = DataFrame(test_df)\ntest_df.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10',\n                   'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20',\n                   'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\n                   'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40',\n                   'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50',\n                   'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60',\n                   'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70',\n                   'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80',\n                   'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90',\n                   'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99']\n\nX_test = test_df\n\n# test_df[\"test_pred\"] = np.nan\n# y_test = test_df.drop(\"test_pred\", axis=1)\n\ny_predict = model.predict(X_test)\ny_predict = np.ravel(y_predict)","466beff6":"# Save predition to the submission.csv\ntest_df_temp = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\noutput = pd.DataFrame({'id': test_df_temp.id, 'target': y_predict})\noutput = output.loc[:, ['id', 'target']]\n\noutput.to_csv('submission.csv', index = False)","b0ccf3b6":"## Logistic Regression with Default Hyperparameters in scikit-learn","728d0851":"## Exploratory Data Analysis","626b4a35":"## Random Forest","dceec136":"Feature columns : float64 \/ Target column : int","d44357ed":"Train data count: 600,000 \/ Test data count : 540,000","88c2b417":"## Scaling feature values\n\n### MinMaxScaler Transform\n\n<ul>\n<li> Normalization is a rescaling of the data from the original range so that all values are within the new range of 0 and 1. <\/li> \n<li> Data scaling is a recommended pre-processing step when working with many machine learning algorithms. (<a href=\"https:\/\/machinelearningmastery.com\/standardscaler-and-minmaxscaler-transforms-in-python\/\">Jason Brownlee<\/a>, 2020) <\/li> \n<\/ul>","66c71cce":"There's no missing value on the dataframe","bd90a66a":"## Classification Trees (CART Models)","e14c2975":"## Experiment with the validation split\n\nIn the following code cell, you'll see a variable named validation_split, which we've initialized at 0.2. The validation_split variable specifies the proportion of the original training set that will serve as the validation set. \n\nThe following code builds a model, trains it on the training set, and evaluates the built model on both:\n\n- The training set.\n- And the validation set.","bfeeda7c":"## Binary Classification with Keras NN","f8581e2c":"## Import data as dataframe","ca1a7797":"## Gradient Boosted Machines"}}