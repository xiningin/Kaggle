{"cell_type":{"db7fcfef":"code","c5b30643":"code","ceb7d2f2":"code","1ad34d89":"code","9a8de026":"code","8c893677":"code","e3b225e7":"code","b6b7d82d":"code","207c25f5":"code","c55277ad":"code","fe1bab49":"code","c6a797f7":"code","e8a1b7ed":"code","a2b7e197":"code","39f3f532":"code","3505bc10":"code","ee8ffbbb":"code","5e185c49":"code","c25c7068":"code","ae773565":"code","29cb2824":"code","43515236":"code","c76f6b21":"code","265dc78b":"markdown","06c3f6dc":"markdown","0af80f70":"markdown","b3545d7d":"markdown","f386978c":"markdown","16d02439":"markdown","4ccc63c9":"markdown","6f7f9361":"markdown","4d07e07d":"markdown","c2e8645e":"markdown","646b8b6e":"markdown","8595bdad":"markdown","0584e785":"markdown","32cf36d4":"markdown","a92fc2b1":"markdown","83a91979":"markdown","be6ea0ab":"markdown","eace3252":"markdown","28c11bf2":"markdown","a1b58d8b":"markdown","e137fa27":"markdown","38e80572":"markdown","952c76d6":"markdown","8eb597ad":"markdown","e15e90ee":"markdown","4b34bf3b":"markdown"},"source":{"db7fcfef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5b30643":"import matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier","ceb7d2f2":"data= pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\ndata.head()","1ad34d89":"plt.figure(figsize=(10,10))\nsns.heatmap(data.isnull(), yticklabels=False) # no null values in the dataset\"","9a8de026":"x= data.drop(columns='class')\ny= data['class']\nx= pd.get_dummies(x, prefix_sep=\"_\")\nx.head()","8c893677":"x= StandardScaler().fit_transform(x)\ny= LabelEncoder().fit_transform(y)","e3b225e7":"def forest_test(x,y):\n    x_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.3, random_state=101)\n    start= time.process_time()\n    clf= RandomForestClassifier(n_estimators=700).fit(x_train, y_train)\n    print(time.process_time()-start)\n    pred= clf.predict(x_test)\n    print(confusion_matrix(y_test, pred))\n    print(classification_report(y_test, pred))","b6b7d82d":"forest_test(x,y)","207c25f5":"from sklearn.decomposition import PCA\n\npca= PCA(n_components=2)\nx_pca_2= pca.fit_transform(x)\npca_df= pd.DataFrame(data=x_pca_2, columns=[\"PC1\", 'PC2'])\npca_df= pd.concat([pca_df, data['class']], axis=1)\npca_df['class']= LabelEncoder().fit_transform(pca_df['class'])\npca_df.head()","c55277ad":"plt.figure(figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n\nclasses= [1,0]\ncolors= ['r', 'b']\nfor clas, color in zip(classes, colors):\n    plt.scatter(pca_df.loc[pca_df['class'] == clas, 'PC1'],\n               pca_df.loc[pca_df['class'] == clas, 'PC2'],\n               c= color)\n\nplt.xlabel('principal component 1', fontsize= 12)\nplt.ylabel('principal component 2', fontsize= 12)\nplt.title('2D PCA', fontsize= 15)\nplt.legend(['Poisonous', 'Edible'])\nplt.grid()","fe1bab49":"forest_test(x_pca_2, y)","c6a797f7":"pca= PCA(n_components=3, svd_solver='full')\nx_pca= pca.fit_transform(x)\nprint(pca.explained_variance_)\n\nforest_test(x_pca, y)","e8a1b7ed":"import plotly.express as px\npca_df= pd.DataFrame(data=x_pca, columns=[\"PC1\", 'PC2', 'PC3'])\ndf = pd.concat([pca_df, data['class']], axis=1)\nfig = px.scatter_3d(df, x='PC1', y='PC2', z='PC3',\n              color='class',labels= ['Poisonous', 'edible'])\nfig.show()","a2b7e197":"from itertools import product\n\nX_Reduced, X_Test_Reduced, Y_Reduced, Y_Test_Reduced = train_test_split(x_pca_2, y, \n                                                                        test_size = 0.30, \n                                                                        random_state = 101)\ntrainedforest = RandomForestClassifier(n_estimators=700).fit(X_Reduced,Y_Reduced)\n\nx_min, x_max = X_Reduced[:, 0].min() - 1, X_Reduced[:, 0].max() + 1\ny_min, y_max = X_Reduced[:, 1].min() - 1, X_Reduced[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\nZ = trainedforest.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z,cmap=plt.cm.coolwarm, alpha=0.4)\nplt.scatter(X_Reduced[:, 0], X_Reduced[:, 1], c=Y_Reduced, s=20, edgecolor='k')\nplt.xlabel('Principal Component 1', fontsize = 12)\nplt.ylabel('Principal Component 2', fontsize = 12)\nplt.title('Random Forest', fontsize = 15)\nplt.show()","39f3f532":"from sklearn.decomposition import FastICA\n\nica= FastICA(n_components=3)\nx_ica= ica.fit_transform(x)\nforest_test(x_ica, y)","3505bc10":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=1)\n\n# run an LDA and use it to transform the features\nX_lda = lda.fit(x,y).transform(x)\nprint('Original number of features:', x.shape[1])\nprint('Reduced number of features:', X_lda.shape[1])","ee8ffbbb":"forest_test(X_lda, y)","5e185c49":"X_Reduced, X_Test_Reduced, Y_Reduced, Y_Test_Reduced = train_test_split(X_lda, y, \n                                                                        test_size = 0.30, \n                                                                        random_state = 101)\n\nstart = time.process_time()\nlda = LinearDiscriminantAnalysis().fit(X_Reduced,Y_Reduced)\nprint(time.process_time() - start)\npredictionlda = lda.predict(X_Test_Reduced)\nprint(confusion_matrix(Y_Test_Reduced,predictionlda))\nprint(classification_report(Y_Test_Reduced,predictionlda))","c25c7068":"from sklearn.manifold import LocallyLinearEmbedding\n\nembedding = LocallyLinearEmbedding(n_components=3, eigen_solver='dense')\nx_lle= embedding.fit_transform(x)\n\nforest_test(x_lle,y)","ae773565":"from sklearn.manifold import TSNE\nstart= time.process_time()\ntsne= TSNE(n_components=3, verbose= 1, perplexity=40, n_iter=300)\nx_tsne= tsne.fit_transform(x)\n\nprint(time.process_time()-start)","29cb2824":"forest_test(x_tsne, y)","43515236":"from keras.layers import Input, Dense\nfrom keras.models import Model\n\ninput_layer= Input(shape=(x.shape[1],))\nencoded= Dense(3, activation='relu')(input_layer)\ndecoded= Dense(x.shape[1], activation='softmax')(encoded)\nautoencoder= Model(input_layer, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n\nx1, x2, y1, y2= train_test_split(x,x,test_size=0.3, random_state=101)\n\nautoencoder.fit(x1, y1, \n               epochs= 100, \n               batch_size=300, \n               shuffle= True, \n               verbose= 30, \n               validation_data=(x2, y2))\nencoder= Model(input_layer, encoded)\nx_ae= encoder.predict(x)","c76f6b21":"forest_test(x_ae, y)","265dc78b":"# **3 Features PCA**","06c3f6dc":"**We will make function to split the data, train model and caluclate the score**","0af80f70":"# **Why Feature extraction ?**","b3545d7d":"# **Feature extraction**","f386978c":"# **we got 95% score by using just 2 features**","16d02439":"# **Scaling and Encoding**","4ccc63c9":"* Accuracy improvement\n* overfitting risk reduction\n* speed up training\n* improved data visualization\n* to increase explainability of model","6f7f9361":"# **Independent Component Analyasis (ICA)**","4d07e07d":"# it is a most widely used linear dimensionality reduction technique. \n# In PCA we will input the oroiginal features and try to find the combination of features best summarise the original featuresa\n","c2e8645e":"# **1. we will reduce the dataset into only two features**","646b8b6e":"# Principal Component Analysis (PCA)","8595bdad":"**LDA is supervised learning dimensionality reduction technique and machine learning classifier**\n1. It maximize the distance between the mean of each class\n2. minimize the spread within the class ","0584e785":"# **Locally Linear Embedding**","32cf36d4":"# **Importing the required libraries**","a92fc2b1":"# **We got 98% by using 3 features**","83a91979":"# **t-Distributed Stochastic Neighbor Embedding (t-SNE)**","be6ea0ab":"# Autoencoders are family of machine learning algorithms which can be used to reduce the dimensionality of the higher dimensional dataset  ","eace3252":"# **model gives 100% accuracy if we use all the features**","28c11bf2":"it is the dimensionalty reduction method based on manifold learning which is used in case of non lineaer features\n\n","a1b58d8b":"# **Missing values**","e137fa27":"**ICA is linear dimensionality reduction method which takes as input data a mixture of independent components and it try to correctly identify each of them.**","38e80572":"It is non-linear dimenaionality reduction technique whic is typically used to visualize high dimensional datasets","952c76d6":"# **Autoencoders**","8eb597ad":"# **Seperating features (x) and Labels (y)**","e15e90ee":"# **Importing the data**","4b34bf3b":"# **Linear Discriminant Analysis (LDA)**"}}