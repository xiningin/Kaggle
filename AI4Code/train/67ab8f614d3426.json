{"cell_type":{"0386dca9":"code","f9f73c7c":"code","00c03588":"code","d073b268":"code","b40d2f85":"code","c63a8dc4":"code","f1bc8314":"code","21d0757d":"code","81a1dee2":"code","487896e2":"code","f33d28cc":"code","c1e893b8":"code","d68dc1a9":"code","c0760e84":"code","14cd9cd2":"code","704b21cb":"code","abc1285a":"code","bb4c3f5d":"code","7b27feb3":"code","67c331cf":"code","e92b986b":"code","4b332ccb":"code","b005b1fc":"code","dc3905db":"code","48cf6e22":"code","27eb8e8b":"code","7081164e":"markdown","785d2694":"markdown"},"source":{"0386dca9":"import seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O","f9f73c7c":"import warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","00c03588":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d073b268":"train=pd.read_csv('\/kaggle\/input\/advertsuccess\/Train.csv')\ntest=pd.read_csv('\/kaggle\/input\/advertsuccess\/Test.csv')","b40d2f85":"print('total train data: ' + str(train.shape[0]))\nprint('total test data: ' + str(test.shape[0]))","c63a8dc4":"train.describe()","f1bc8314":"train.info()","21d0757d":"train.isnull().sum()","81a1dee2":"from sklearn import preprocessing\n\nle1 = preprocessing.LabelEncoder()\nle1.fit(train['realtionship_status'])\nlist(le1.classes_)\ntrain['realtionship_status'] = le1.transform(train['realtionship_status'])\ntrain.head()","487896e2":"le2 = preprocessing.LabelEncoder()\nle2.fit(train['industry'])\nlist(le2.classes_)\ntrain['industry'] = le2.transform(train['industry']) \ntrain.head()","f33d28cc":"le3 = preprocessing.LabelEncoder()\nle3.fit(train['genre'])\nlist(le3.classes_)\ntrain['genre'] = le3.transform(train['genre']) \ntrain.head()","c1e893b8":"le4 = preprocessing.LabelEncoder()\nle4.fit(train['targeted_sex'])\nlist(le4.classes_)\ntrain['targeted_sex'] = le4.transform(train['targeted_sex']) \ntrain.head()","d68dc1a9":"le5 = preprocessing.LabelEncoder()\nle5.fit(train['airtime'])\nlist(le5.classes_)\ntrain['airtime'] = le5.transform(train['airtime']) \ntrain.head()","c0760e84":"le6 = preprocessing.LabelEncoder()\nle6.fit(train['airlocation'])\nlist(le6.classes_)\ntrain['airlocation'] = le6.transform(train['airlocation']) \ntrain.head()","14cd9cd2":"le7 = preprocessing.LabelEncoder()\nle7.fit(train['expensive'])\nlist(le7.classes_)\ntrain['expensive'] = le7.transform(train['expensive'])\ntrain.head()","704b21cb":"le8 = preprocessing.LabelEncoder()\nle8.fit(train['money_back_guarantee'])\nlist(le8.classes_)\ntrain['money_back_guarantee'] = le8.transform(train['money_back_guarantee'])\ntrain.head()","abc1285a":"le9 = preprocessing.LabelEncoder()\nle9.fit(train['netgain'])\nlist(le9.classes_)\ntrain['netgain'] = le9.transform(train['netgain'])\ntrain.head()","bb4c3f5d":"#Considering all available features for decision tree classifier\nfeatures = ['realtionship_status','industry','genre','targeted_sex','average_runtime(minutes_per_week)','airtime','airlocation','ratings','expensive','money_back_guarantee']\nX = train[features]\ny = train['netgain']","7b27feb3":"# Decision tree classifier and model evaluation using kFold cross validation\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\n\nkfold_mae_train=0\nkfold_mae_test=0\nkfold_f_imp_dic = 0\n\nno_of_folds = 5\n\nkf = KFold(no_of_folds,True,1)\nfor train_index, test_index in kf.split(X):\n    \n    X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n    \n    dt_classifier = DecisionTreeClassifier(random_state=1)\n    dt_classifier.fit(X_train,y_train)\n    \n    mae_train = mean_absolute_error(dt_classifier.predict(X_train),y_train)\n    kfold_mae_train=(kfold_mae_train+mae_train)\n    \n    mae_test = mean_absolute_error(dt_classifier.predict(X_test),y_test)\n    kfold_dt_mae_test = (kfold_mae_test+mae_test)\n    \n    kfold_f_imp_dic = kfold_f_imp_dic + dt_classifier.feature_importances_\n    \nprint('Decision Tree classifier train set mean absolute error =',kfold_mae_train\/no_of_folds)\nprint('Decision Tree classifier test set mean absolute error  =',kfold_dt_mae_test\/no_of_folds)","67c331cf":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error,accuracy_score,mean_squared_error\nfrom sklearn.model_selection import KFold\n\nkfold_mae_train=0\nkfold_mae_test=0\nkfold_mse_train=0\nkfold_mse_test=0\nkfold_f_imp_dic = 0\n\nno_of_folds = 5\nkf = KFold(no_of_folds,True,1)\nfor train_index, test_index in kf.split(X):\n    \n    X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n    \n    rf_classifier = RandomForestClassifier(random_state=1)\n    rf_classifier.fit(X_train,y_train)\n    \n    mae_train = mean_absolute_error(rf_classifier.predict(X_train),y_train)\n    kfold_mae_train=(kfold_mae_train+mae_train)\n    \n    mse_train = mean_squared_error(rf_classifier.predict(X_train),y_train)\n    kfold_mse_train=(kfold_mse_train+mse_train)\n    \n    mae_test = mean_absolute_error(rf_classifier.predict(X_test),y_test)\n    kfold_rf_mae_test = (kfold_mae_test+mae_test)\n    \n    mse_test = mean_squared_error(rf_classifier.predict(X_test),y_test)\n    kfold_mse_test=(kfold_mse_test+mse_test)\n    \n    kfold_f_imp_dic = kfold_f_imp_dic + rf_classifier.feature_importances_\n    \nprint('Random Forest classifier train set mean absolute error =',kfold_mae_train\/no_of_folds)\nprint('Random Forest classifier test set mean absolute error  =',kfold_rf_mae_test\/no_of_folds)\nprint('Random Forest Classifier train set mean Squared error =',kfold_mse_train\/no_of_folds)\nprint('Random Forest Classifier test set mean Squared error =',kfold_mse_test\/no_of_folds)\n#rfc2=RandomForestClassifier()\n#rfc2.fit(X_train,y_train)\n#model on train using all the independent values in df\nrfc_prediction = rf_classifier.predict(X_train)\nrfc_score= accuracy_score(y_train,rfc_prediction)\nprint('Random Forest classifier Train set accuracy score ',rfc_score)\n#model on test using all the indpendent values in df\nrfc_prediction = rf_classifier.predict(X_test)\nrfc_score= accuracy_score(y_test,rfc_prediction)\nprint('Random Forest Classifier Test Set accuracy score ',rfc_score)","e92b986b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error,accuracy_score,mean_squared_error\nfrom sklearn.model_selection import KFold\n\nkfold_mae_train=0\nkfold_mae_test=0\nkfold_mse_train=0\nkfold_mse_test=0\nkfold_f_imp_dic = 0\n\nno_of_folds = 5\nkf = KFold(no_of_folds,True,1)\nfor train_index, test_index in kf.split(X):\n    \n    X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n    \n    lr_classifier = LogisticRegression(random_state=1)\n    lr_classifier.fit(X_train,y_train)\n    \n    mae_train = mean_absolute_error(lr_classifier.predict(X_train),y_train)\n    kfold_mae_train=(kfold_mae_train+mae_train)\n    \n    mse_train = mean_squared_error(lr_classifier.predict(X_train),y_train)\n    kfold_mse_train=(kfold_mse_train+mse_train)\n    \n    mae_test = mean_absolute_error(lr_classifier.predict(X_test),y_test)\n    kfold_rf_mae_test = (kfold_mae_test+mae_test)\n    \n    mse_test = mean_squared_error(lr_classifier.predict(X_test),y_test)\n    kfold_mse_test=(kfold_mse_test+mse_test)\n    \n    kfold_f_imp_dic = kfold_f_imp_dic + rf_classifier.feature_importances_\n    \nprint('Logistic Regression train set mean absolute error =',kfold_mae_train\/no_of_folds)\nprint('Logisitic Regression test set mean absolute error  =',kfold_rf_mae_test\/no_of_folds)\nprint('Logistic Regression train set mean Squared error =',kfold_mse_train\/no_of_folds)\nprint('logistic Regression test set mean Squared error =',kfold_mse_test\/no_of_folds)\n#rfc2=RandomForestClassifier()\n#rfc2.fit(X_train,y_train)\n#model on train using all the independent values in df\nlr_prediction = lr_classifier.predict(X_train)\nlr_score= accuracy_score(y_train,lr_prediction)\nprint('logisitic Regression Train set accuracy score ',rfc_score)\n#model on test using all the indpendent values in df\nlr_prediction = lr_classifier.predict(X_test)\nlr_score= accuracy_score(y_test,lr_prediction)\nprint('logistic Regression Test Set accuracy score ',rfc_score)","4b332ccb":"import xgboost as xgb\nkfold_mae_train=0\nkfold_mae_test=0\nkfold_mse_train=0\nkfold_mse_test=0\nkfold_f_imp_dic = 0\n\nno_of_folds = 5\nkf = KFold(no_of_folds,True,1)\nfor train_index, test_index in kf.split(X):\n    \n    X_train,X_test = X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test = y.iloc[train_index],y.iloc[test_index]\n    xgb_classifier = xgb.XGBClassifier(max_depth=3,n_estimators=300,learning_rate=0.05)\n    \n    xgb_classifier.fit(X_train,y_train)\n    \n    mae_train = mean_absolute_error(xgb_classifier.predict(X_train),y_train)\n    kfold_mae_train=(kfold_mae_train+mae_train)\n    \n    mse_train = mean_squared_error(xgb_classifier.predict(X_train),y_train)\n    kfold_mse_train=(kfold_mse_train+mse_train)\n    \n    mae_test = mean_absolute_error(xgb_classifier.predict(X_test),y_test)\n    kfold_rf_mae_test = (kfold_mae_test+mae_test)\n    \n    mse_test = mean_squared_error(xgb_classifier.predict(X_test),y_test)\n    kfold_mse_test=(kfold_mse_test+mse_test)\n    \n    kfold_f_imp_dic = kfold_f_imp_dic + xgb_classifier.feature_importances_\n    \nprint('XGBoost train set mean absolute error =',kfold_mae_train\/no_of_folds)\nprint('XGBoost test set mean absolute error  =',kfold_rf_mae_test\/no_of_folds)\nprint('XGBoost train set mean Squared error =',kfold_mse_train\/no_of_folds)\nprint('XGBoost test set mean Squared error =',kfold_mse_test\/no_of_folds)\n#rfc2=RandomForestClassifier()\n#rfc2.fit(X_train,y_train)\n#model on train using all the independent values in df\nxgb_prediction = xgb_classifier.predict(X_train)\nxgb_score= accuracy_score(y_train,xgb_prediction)\nprint('XGBoost Train set accuracy score ',xgb_score)\n#model on test using all the indpendent values in df\nxgb_prediction = xgb_classifier.predict(X_test)\nxgb_score= accuracy_score(y_test,xgb_prediction)\nprint('XGBoost Test Set accuracy score ',xgb_score)","b005b1fc":"#Import Gaussian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB\ngnb =GaussianNB()\ngnb.fit(X_train,y_train)\n#model on train using all the independent values in df\ngnb_prediction = gnb.predict(X_train)\ngnb_score= accuracy_score(y_train,gnb_prediction)\nprint('Navie Bayes Train Set accuracy score',gnb_score)\n#model on test using all the independent values in df\ngnb_prediction = gnb.predict(X_test)\ngnb_score= accuracy_score(y_test,gnb_prediction)\nprint('Navie Bayes Test Set accuracy score',gnb_score)","dc3905db":"from sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nestimator = [] \nestimator.append(('LR',  \n                  LogisticRegression(solver ='lbfgs',  \n                                     multi_class ='multinomial',  \n                                     max_iter = 200))) \nestimator.append(('SVC', SVC(gamma ='auto', probability = True))) \nestimator.append(('DTC', DecisionTreeClassifier()))\n\nvc=VotingClassifier(estimators = estimator, voting ='hard') \nvc.fit(X_train,y_train)\n#model on train using all the independent values in df\nvc_prediction = vc.predict(X_train)\nvc_score= accuracy_score(y_train,vc_prediction)\nprint('voting classifier train set accuracy score :',vc_score)\n#model on test using all the independent values in df\nvc_prediction = vc.predict(X_test)\nvc_score= accuracy_score(y_test,vc_prediction)\nprint('voting classifier train set accuracy score :',vc_score)","48cf6e22":"vc=VotingClassifier(estimators = estimator, voting ='soft') \nvc.fit(X_train,y_train)\n#model on train using all the independent values in df\nvc_prediction = vc.predict(X_train)\nvc_score= accuracy_score(y_train,vc_prediction)\nprint('voting classifier train set accuracy score :',vc_score)\n#model on test using all the independent values in df\nvc_prediction = vc.predict(X_test)\nvc_score= accuracy_score(y_test,vc_prediction)\nprint('voting classifier train set accuracy score :',vc_score)","27eb8e8b":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\nf_importance_dic = dict(zip(features,kfold_f_imp_dic\/no_of_folds))\ndf_imp_features = pd.DataFrame(list(f_importance_dic.items()),columns=['feature','score'])\n\nplt.figure(figsize=(30,10))\nplt.bar(df_imp_features['feature'], df_imp_features['score'],color='green',align='center', alpha=0.5)\nplt.xlabel('Mobile features', fontsize=20)\nplt.ylabel('Relative feature score',fontsize=20)\nplt.title('Relative Feature importance in determining price',fontsize=30)","7081164e":"## Advertisement success prediction using label encoding","785d2694":"**(Please upvote if you like)**"}}