{"cell_type":{"a6725ce1":"code","925d172c":"code","4c616a3f":"code","2a52eadb":"code","091ca6d6":"code","95b85bcc":"code","188392f1":"code","cef30815":"code","5b0075a1":"code","d801a2ff":"code","34c54f0f":"code","c9223c3d":"code","d40b903f":"code","bf190fa7":"code","503253ff":"code","8a51cf86":"code","35f51901":"code","84a486f2":"code","e1ab96b4":"code","fa6856a8":"code","6edbd106":"code","48ba7741":"code","2e783c1c":"code","01c0a4af":"code","a089002a":"code","32b71fd2":"code","92c049f2":"code","83f950de":"code","9bac7873":"code","aa542d60":"code","95b66c87":"code","2f4074d1":"code","1d3a3c7b":"code","439a495f":"code","fe7db248":"code","9f23e80c":"code","81b63ee4":"code","9a0c3ff4":"code","b800964e":"code","a03e84a4":"code","18bd5552":"code","3c1be8ea":"code","6a4e844f":"code","c5fcd64a":"code","c8e4907a":"code","49ec0b06":"code","79c223af":"code","8999e6c6":"code","d9b57a9a":"code","f775c35b":"code","3b67ac3e":"code","be60aa69":"code","551533e6":"code","f2ef53fc":"code","f5fe626c":"code","cdbdec4b":"code","1d103b0e":"code","a381bfc7":"code","eb35b03f":"code","fd428906":"code","65fba191":"code","46decec5":"code","c5ead99b":"code","5954a448":"code","e4d750fd":"code","686331b2":"code","ba86d275":"code","4a5deaab":"code","ef447b65":"code","f85c3b30":"code","1c5897a1":"code","c7a15de7":"code","d7721b1a":"code","c9bac579":"code","3ef07696":"code","4fdf4613":"code","de33475e":"code","b2910e92":"code","dfee11fe":"code","0d1063f7":"code","69f98bd0":"code","64371f8c":"code","8fa05db4":"code","b3742a8b":"code","e5a5afde":"code","0509c90d":"code","2923bb9e":"code","358e8f34":"code","6a54a522":"code","0487ddcd":"code","042197ee":"code","b246f964":"code","0b2dbfe3":"code","2f30f20a":"code","51a3508a":"code","4f582f7e":"code","e2846b06":"code","7e1ee7ae":"code","88f87db5":"code","83296839":"code","d895b5dd":"code","fb3020d6":"code","925a66a3":"code","9fdc0bff":"code","acb644d6":"code","bb1b282b":"code","7f6452a5":"code","715323b1":"code","63c4a9bb":"code","e493f7ae":"code","37455436":"code","a34f82cf":"code","386d5f34":"code","d71bef84":"code","b5746e43":"code","d6425d18":"code","4ccf6acf":"code","37009cdf":"code","38e183b7":"code","61ec1678":"code","b90871ca":"code","a77c2c1d":"code","5130dceb":"code","8b28820b":"code","457bc5af":"markdown","ce61ad44":"markdown","ecfbb526":"markdown","3c7c4368":"markdown","3f3f7348":"markdown","29f2fbfa":"markdown","d79b2919":"markdown","16fe91fa":"markdown","b48e4498":"markdown","71a591f3":"markdown","35f1ed04":"markdown","490b29e3":"markdown","b17c1382":"markdown","acbf91d8":"markdown","ee1816d2":"markdown","0c4c9290":"markdown","bee29b5e":"markdown","81cf8322":"markdown","712d76ba":"markdown","11627133":"markdown","fc86f877":"markdown","b0e2122d":"markdown","f9023b36":"markdown","42abbfbc":"markdown","0a2f8a5b":"markdown","c2cc4c60":"markdown","950dc106":"markdown","ee61fe3d":"markdown","f50f4092":"markdown","39b7d2e7":"markdown","c0b440ac":"markdown","312117f5":"markdown","9093349a":"markdown","c13214cb":"markdown","f2bd4823":"markdown","f46f8321":"markdown","1e86c54e":"markdown","50a7d2fb":"markdown","3c204bfa":"markdown","bd7f7e53":"markdown","97304e74":"markdown","6bb076d1":"markdown","e215b25e":"markdown","21510bd7":"markdown","e227a271":"markdown","d85fbfb4":"markdown","3da7de29":"markdown"},"source":{"a6725ce1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","925d172c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB,MultinomialNB\nfrom time import time\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve\nfrom sklearn.tree import plot_tree, export_text\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","4c616a3f":"titanic = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","2a52eadb":"titanic.head()","091ca6d6":"titanic1 = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","95b85bcc":"titanic1.head()","188392f1":"titanic.shape","cef30815":"titanic1.shape","5b0075a1":"titanic.columns","d801a2ff":"titanic1.columns","34c54f0f":"titanic.describe()","c9223c3d":"titanic1.describe()","d40b903f":"titanic.info()","bf190fa7":"titanic.isnull().sum()","503253ff":"titanic1.isnull().sum()","8a51cf86":"titanic['Age'] = titanic['Age'].fillna(titanic['Age'].mean())","35f51901":"titanic1['Age'] = titanic1['Age'].fillna(titanic1['Age'].mean())","84a486f2":"titanic.isnull().sum()","e1ab96b4":"titanic1.isnull().sum()","fa6856a8":"titanic1['Fare'] = titanic1['Fare'].fillna(titanic1['Fare'].median())","6edbd106":"titanic1.isnull().sum()","48ba7741":"titanic.isnull().sum()","2e783c1c":"titanic['Cabin'] = titanic['Cabin'].fillna(titanic['Cabin'].mode()[0])","01c0a4af":"titanic1['Cabin'] = titanic1['Cabin'].fillna(titanic1['Cabin'].mode()[0])","a089002a":"titanic.isnull().sum()","32b71fd2":"titanic1.isnull().sum()","92c049f2":"titanic['Embarked'] = titanic['Embarked'].fillna(titanic['Embarked'].mode()[0])","83f950de":"titanic.isnull().sum()","9bac7873":"titanic['Survived'].value_counts()","aa542d60":"titanic['Survived'].value_counts(normalize = True)","95b66c87":"women = titanic.loc[titanic.Sex == 'female'][\"Survived\"]\nsurvival_rate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", survival_rate_women)","2f4074d1":"men = titanic.loc[titanic.Sex == 'male'][\"Survived\"]\nsurvival_rate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", survival_rate_men)","1d3a3c7b":"sns.countplot(titanic['Survived'])","439a495f":"plt.figure(figsize=(15,8))\ntitanic.boxplot()","fe7db248":"q = int(titanic['Age'].quantile(0.25))\nq = int(titanic['Age'].quantile(0.9))","9f23e80c":"titanic[titanic['Age']>q]['Age'].count()","81b63ee4":"titanic[titanic['Age']<q]['Age'].count()","9a0c3ff4":"titanic['Age'] = np.where(titanic['Age']>q,q,titanic['Age'])\ntitanic['Age'] = np.where(titanic['Age']<q,q,titanic['Age'])","b800964e":"q = int(titanic['SibSp'].quantile(0.25))\nq = int(titanic['SibSp'].quantile(0.9))","a03e84a4":"titanic[titanic['SibSp']>q]['SibSp'].count()","18bd5552":"titanic[titanic['SibSp']<q]['SibSp'].count()","3c1be8ea":"titanic['SibSp'] = np.where(titanic['SibSp']>q,q,titanic['SibSp'])\ntitanic['SibSp'] = np.where(titanic['SibSp']<q,q,titanic['SibSp'])","6a4e844f":"q = int(titanic['Parch'].quantile(0.25))\nq = int(titanic['Parch'].quantile(0.9))","c5fcd64a":"titanic[titanic['Parch']>q]['Parch'].count()","c8e4907a":"titanic[titanic['Parch']<q]['Parch'].count()","49ec0b06":"titanic['Parch'] = np.where(titanic['Parch']>q,q,titanic['Parch'])\ntitanic['Parch'] = np.where(titanic['Parch']<q,q,titanic['Parch'])","79c223af":"q = int(titanic['Fare'].quantile(0.25))\nq = int(titanic['Fare'].quantile(0.99))","8999e6c6":"titanic[titanic['Fare']>q]['Fare'].count()","d9b57a9a":"titanic[titanic['Fare']<q]['Fare'].count()","f775c35b":"titanic['Fare'] = np.where(titanic['Fare']>q,q,titanic['Fare'])\ntitanic['Fare'] = np.where(titanic['Fare']<q,q,titanic['Fare'])","3b67ac3e":"plt.figure(figsize=(15,8))\ntitanic.boxplot()","be60aa69":"## checking how many people survived based on Age\nsns.countplot(y=titanic['Age'],hue=titanic['Survived'])","551533e6":"titanic.groupby('Survived')['Age'].value_counts()","f2ef53fc":"## checking how many people survived based on Pclass\nsns.countplot(y=titanic['Pclass'],hue=titanic['Survived'])","f5fe626c":"titanic.groupby('Pclass')['Survived'].value_counts()","cdbdec4b":"## checking how many people survived based on SibSp\nsns.countplot(y=titanic['SibSp'],hue=titanic['Survived'])","1d103b0e":"titanic.groupby('SibSp')['Survived'].value_counts()","a381bfc7":"## checking how many people survived based on Parch\nsns.countplot(y=titanic['Parch'],hue=titanic['Survived'])","eb35b03f":"titanic.groupby('Parch')['Survived'].value_counts()","fd428906":"## checking how many people survived based on Parch\nsns.countplot(y=titanic['Fare'],hue=titanic['Survived'])","65fba191":"titanic.groupby('Fare')['Survived'].value_counts()","46decec5":"sns.countplot(y=titanic['Sex'],hue=titanic['Survived'])","c5ead99b":"titanic.groupby('Sex')['Survived'].value_counts()","5954a448":"\nle = LabelEncoder()\nfor column in titanic.columns:\n    if titanic[column].dtype == type(object):\n        titanic[column] = le.fit_transform(titanic[column])\n\ntitanic.head(10)","e4d750fd":"X = titanic.drop('Survived',axis=1)\ny = titanic['Survived']","686331b2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n                                                    random_state=0) ","ba86d275":"X_train.head()","4a5deaab":"X_test.head()","ef447b65":"y_train.head()","f85c3b30":"y_test.head()","1c5897a1":"classifier_log = LogisticRegression()\nmodel = classifier_log.fit(X_train,y_train)","c7a15de7":"#predicting on the test data\ny_pred_log = classifier_log.predict(X_test)","d7721b1a":"#checking the accuracy of the model\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred_log,y_test)*100)","c9bac579":"probs_log = classifier_log.predict_proba(X_test)","3ef07696":"probs_log","4fdf4613":"classifier_tree = DecisionTreeClassifier()\nmodel = classifier_tree.fit(X_train,y_train)","de33475e":"# predicting the test data\ny_pred_tree = classifier_tree.predict(X_test)","b2910e92":"print(accuracy_score(y_pred_tree,y_test)*100)","dfee11fe":"features  = X_test.columns\nplt.figure(figsize=(10,10))\nplot_tree(classifier_tree, feature_names=features,filled = True)","0d1063f7":"bnb = BernoulliNB() ","69f98bd0":"print(\"Start training...\")\ntStart = time()\nbnb.fit(X_train, y_train)\ntEnd = time()\nprint(\"Training time: \", round(tEnd-tStart, 3), \"s\")\n\n# making predictions on the testing set \ny_pred = bnb.predict(X_test) \nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred)*100)\n","64371f8c":"CM=confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix is:\", CM, sep='\\n') ","8fa05db4":"CM=confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix is:\", CM, sep='\\n') \n\nprint(\"----------\")\nprint(\"   TN FP\")\nprint(\"   FN TP\")\n\n\n\nprint(\"----------\")\n\nTN = 92\nTP = 49\nFN = 20\nFP = 18\n\nsensitivity = TP\/(TP+FN)\nspecificity = TN\/(TN+FP)\n\nprint(\"The probability of predicting people surviving correctly is \",specificity)\nprint(\"The probability of predicting people not surviving correctly is \",sensitivity)\n","b3742a8b":"y_pred_NB = bnb.predict(X_test)","e5a5afde":"y_pred_NB.shape","0509c90d":"clf=BaggingClassifier(oob_score=True,n_jobs=-1,n_estimators=20,random_state=400,\n                      base_estimator=DecisionTreeClassifier())","2923bb9e":"clf.fit(X_train,y_train)","358e8f34":"#Average number of correct predictions\nclf.score(X_test,y_test)","6a54a522":"clf.oob_score_","0487ddcd":"set(range(100,200,20))","042197ee":"#Parameter tuning\nfor w in range(100,200,20):\n    clf=BaggingClassifier(oob_score=True,n_jobs=-1,n_estimators=w,random_state=400,\n                          base_estimator=DecisionTreeClassifier())\n    clf.fit(X_train,y_train)\n    oob=clf.oob_score_\n    print('For n_estimators = '+str(w))\n    print('OOB score is '+str(oob))\n    print('************************')","b246f964":"#Finalizing on a tree model with 180 trees\nclf=BaggingClassifier(oob_score=True,n_jobs=-1,n_estimators=180,random_state=400,\n                      base_estimator=DecisionTreeClassifier())\nclf.fit(X_train,y_train)","0b2dbfe3":"clf_bagging  = clf.predict(X_test)","2f30f20a":"clf_bagging.shape","51a3508a":"#Average Number of correct predictions\nclf.score(X_test,y_test) ","4f582f7e":"# Feature Importance\nlen(clf.estimators_)","e2846b06":"# We can extract feature importance from each tree then take a mean for all trees\nimport numpy as np\nimp=[]\nfor i in clf.estimators_:\n    imp.append(i.feature_importances_) #feature importance at each tree","7e1ee7ae":"imp","88f87db5":"imp=np.mean(imp,axis=0)\nimp","83296839":"feature_importance=pd.Series(imp,index=X.columns.tolist()).sort_values(ascending=False)\nfeature_importance","d895b5dd":"clf=RandomForestClassifier(n_estimators=80,oob_score=True,n_jobs=-1,random_state=400)\nclf.fit(X_train,y_train)","fb3020d6":"clf.oob_score_","925a66a3":"for w in range(50,300,10):\n    clf=RandomForestClassifier(n_estimators=w,oob_score=True,n_jobs=-1,random_state=400)\n    clf.fit(X_train,y_train)\n    oob=clf.oob_score_\n    print('For n_estimators = '+str(w))\n    print('OOB score is '+str(oob))\n    print('************************')","9fdc0bff":"#Finalize 80 trees\nclf=RandomForestClassifier(n_estimators=80,oob_score=True,n_jobs=-1,random_state=400,max_depth=3,max_features=4)\nclf.fit(X_train,y_train)","acb644d6":"clf.score(X_test,y_test)","bb1b282b":"#Area Under the curve\nfrom sklearn import metrics\nmetrics.roc_auc_score(y_test,clf.predict_proba(X_test)[:,1])","7f6452a5":"clf.feature_importances_","715323b1":"clf_rf = clf.predict(X_test)","63c4a9bb":"clf_rf.shape","e493f7ae":"pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)","37455436":"clf=AdaBoostClassifier(n_estimators=50,random_state=400)\nclf.fit(X_train,y_train)","a34f82cf":"clf.score(X_test,y_test)","386d5f34":"clf=GradientBoostingClassifier(n_estimators=80,random_state=400)\nclf.fit(X_train,y_train)","d71bef84":"clf.score(X_test,y_test)","b5746e43":"from sklearn.model_selection import GridSearchCV\nmod=GridSearchCV(clf,param_grid={'n_estimators':[60,80,100,120,140,160]},cv=3) \nmod.fit(X_train,y_train)","d6425d18":"mod.best_estimator_","4ccf6acf":"clf=GradientBoostingClassifier(n_estimators=80,random_state=400)\nclf.fit(X_train,y_train)","37009cdf":"clf.score(X_test,y_test)","38e183b7":"import xgboost as xg\nxgb=xg.XGBClassifier(objective='binary:logistic',reg_lambda=0.2,max_depth=4,random_state=200) #L2 regularization","61ec1678":"#Grid Search CV: Parameter tuning\nfrom sklearn import model_selection\nxgb=model_selection.GridSearchCV(xgb, param_grid={'max_depth':[2,3,5,9],'n_estimators':[50,100,150,500],'reg_lambda':[0.1,0.2]})\nxgb.fit(X_train,y_train)","b90871ca":"xgb.best_params_","a77c2c1d":"xgb.score(X_test,y_test)","5130dceb":"from sklearn import metrics\nmetrics.roc_auc_score(y_test,xgb.predict_proba(X_test)[:,1])","8b28820b":"from sklearn.ensemble import RandomForestClassifier\n\ny = titanic[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(titanic[features])\nX_test = pd.get_dummies(titanic[features])\n\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nimputed_X = my_imputer.fit_transform(X)\nimputed_X_test = my_imputer.transform(X_test)\n\nmodel = RandomForestClassifier(n_estimators=6, max_depth=20, random_state=1)\nmodel.fit(imputed_X, y)\npredictions = model.predict(imputed_X_test)\n\noutput = pd.DataFrame({'PassengerId': titanic.PassengerId, 'Survived': predictions})\noutput.to_csv('submission1.csv', index=False)\nprint(\"Your submission was successfully saved!\")","457bc5af":"## Checking the percentage of people survived  and the percentage of people did not survive","ce61ad44":"## Checking whether outliers are treated","ecfbb526":"## Reading the Test Dataset","3c7c4368":"## Bivariate Analysis","3f3f7348":"## Importing Necessary Libraries","29f2fbfa":"## XGboost is focused towards\n\n## - Computational Speed\n## - Model performance","d79b2919":"## Filling the missing value in the Fare column of the titanic1 dataframe","16fe91fa":"## Random Forest Classifier: Bootstrap sampling + Feature sampling","b48e4498":"- From the above outputs it is clear that there are missing values in the Age,Cabin,Embarked in df dataframe and df_1 dataframe has missing values in Age,Cabin,Fare columns","71a591f3":"## Decision Tree Classifier","35f1ed04":"## Model Development","490b29e3":"## Having a look at the first five rows of the train dataset","b17c1382":"## Filling the missing values of Age column in both data frames","acbf91d8":"## Outlier Analysis","ee1816d2":"## Encoding the categorical variables (Sex column Label Encoded)","0c4c9290":"- From the above output I can say that out of 891, **549 were not survived where as 342 survived**\n- I can also say that the data is imbalanced","bee29b5e":"- From the above output I can say that **61% of the people did not Survive** and remaining **survived**","81cf8322":"## From the above Bivariate Analysis I can Infer the following:\n\n**- The People who are in the range of 40-50 years the mortality is high**\n\n**- The people who belong to Pclass-2 the mortality is high**\n\n**- The people who have parents and children of maximum upto 2 the mortality is high**\n\n**- The people who bought a ticket for 249 dollars the mortality is high**\n\n**- From the above graph I can say that the mortality is high where sex is Female **","712d76ba":"- From the above output It is clear that all the missing values for both the dataframes are imputed","11627133":"## Train-Test Split","fc86f877":"## Missing Value Analysis","b0e2122d":"- From the above output It is clear that all the outliers have been treated","f9023b36":"## Percentage of women Survived","42abbfbc":"## Bagging Classifier","0a2f8a5b":"## Logistic Regression","c2cc4c60":"## Boosting classifier: Trees are grown sequentially: each tree is grown using information from previously grown trees.","950dc106":"- From the two outputs above I can clearly say that the missing values for Age columns have been imputed","ee61fe3d":"## Reading the Dataset","f50f4092":"- From the above output I can say that 18.8% of men Survived","39b7d2e7":"## Representing the no.of people survived and not survived Graphically","c0b440ac":"- From the above output 71% of the women survived","312117f5":"- From the above output I can say that there are some outliers in the following columns:\n*Age,\nSibSp,\nParch,\nFare.*","9093349a":"## Checking the columns of the two dataframes","c13214cb":"- From the above outputs I can say that there are no unwanted spaces between the names of the attributes","f2bd4823":"## Now checking whether the missing values for Age is imputed or not","f46f8321":"## Univariate Analysis on the Target variable","1e86c54e":"## Now checking whether the missing values of embarked column is imputed or not","50a7d2fb":"- From the above outputs it is clear that there are 2 missing values in Embarked column of titanic dataframe","3c204bfa":"## Now Checking whether the Missing Values for Cabin have been imputed or not","bd7f7e53":"## Filling the Missing Values for Cabin Column","97304e74":"- From the above summary statistics of both train and test datasets I can say that there are some missing values in Age column in titanic dataframe and there are some missing values in Age,Fare columns of titanic1 dataframe","6bb076d1":"## Having a look at the summary statistics","e215b25e":"## Now Checking whether missing values for Fare Column of titanic1 Dataframe is imputed or not","21510bd7":"## Exploratory Data Analysis","e227a271":"## Checking the Datatypes of the train dataset","d85fbfb4":"## Having a look at the first five rows of the test dataset","3da7de29":"## Treating Outliers for Age,SibSp,Parch,Fare columns respectively"}}