{"cell_type":{"87d12b26":"code","ca4ccafe":"code","380de30d":"code","a5c8e91e":"code","ee89a486":"code","520c6210":"code","705f257c":"code","0e34b06e":"code","7fd5d131":"code","b65d2dda":"code","44241783":"code","66eb81a8":"code","baa3ecb3":"code","9534bd11":"code","00ea58d4":"code","e6e01e73":"code","341039d6":"code","dc18012c":"code","b873c6bf":"code","dea364b2":"code","51de3092":"code","8a94c85b":"code","963da9e3":"code","eedd83c8":"code","0d1192da":"code","f492ae07":"code","bbad84bf":"code","422bf055":"code","dbca409d":"code","ec7f04a8":"code","3f3b51e6":"code","1bf9c749":"code","61f191c8":"code","3ef8fd47":"code","828e9cb0":"code","a5dcc369":"code","0ca8c2ac":"code","f21fb444":"code","74ee2616":"code","2849c2a9":"code","79695664":"code","15042074":"code","db21ce88":"code","915c9b33":"code","3fd9aa91":"code","ba225761":"code","ed5c88e3":"code","57795319":"code","9b906c5f":"code","0deae404":"code","4e468a93":"code","daecac32":"code","141c2c98":"code","29932edb":"code","0f632415":"code","076f5e4e":"code","ed0c07da":"code","c9202ded":"code","0e5a8a5f":"code","22886bcc":"code","dfeaaf06":"code","2a60e304":"code","98849fb2":"code","62874ba1":"code","e7643923":"code","d9dab5c6":"code","b6b84d5f":"code","3421a738":"code","274fb492":"code","c78addd7":"code","f7849c39":"code","14634f1b":"code","50fc39f1":"code","d7ee0b5b":"code","9b675a6c":"code","22298702":"code","a015290d":"code","a649a643":"code","ce308fd0":"code","148f1eb6":"code","ca1413ac":"code","42d54249":"code","2554ad9f":"code","08fb88fc":"code","dd841c52":"code","3d958263":"code","a92b8430":"code","44060447":"code","3bd4d6f3":"code","a1efe314":"code","1b24af1e":"code","adb8a513":"code","79304d43":"code","8488fd40":"code","726c45b6":"code","a3c660b3":"code","c9d0c955":"code","600662c4":"code","48ec99c3":"code","3c1ee6ba":"code","a7736c5b":"code","4144a79c":"code","a7229312":"code","3b367c44":"code","05225193":"code","d7b38b6b":"code","fca68fbc":"markdown","f41c0143":"markdown","10da84a6":"markdown","89da1e8d":"markdown","a8e3d71a":"markdown","1417a38a":"markdown","1f77e027":"markdown","0a8e9c5e":"markdown","8c8efa08":"markdown","92e4dd1c":"markdown","e30a96cb":"markdown","71186942":"markdown","eeb34d72":"markdown","635bcd87":"markdown","804a3650":"markdown","2447dc85":"markdown","00de389a":"markdown","008762f1":"markdown","a97f36d9":"markdown","303593c2":"markdown","383cfd95":"markdown","4ccdf529":"markdown","afca4bba":"markdown","2143ae1c":"markdown","30b6bd5f":"markdown","92155795":"markdown","e71b4d8b":"markdown","630c2b87":"markdown","554731e6":"markdown","deebaed7":"markdown","799a6cfe":"markdown","f905f233":"markdown","862339d2":"markdown","e513008c":"markdown","80b4028d":"markdown","55eb2c26":"markdown","4806ca4a":"markdown","7b53a051":"markdown","60664c72":"markdown","4ee5369f":"markdown","a1f8e134":"markdown","cdec3551":"markdown","449f174d":"markdown","62fa0e65":"markdown","6d99afb1":"markdown","0b88e893":"markdown","b8c88a0d":"markdown","23a1c927":"markdown","0ee454e8":"markdown"},"source":{"87d12b26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-dark')\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression, ElasticNet\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.simplefilter('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca4ccafe":"ss = pd.read_csv('..\/input\/av-guided-hackathon\/sample_submission_cxCGjdN.csv')\ntrain = pd.read_csv('..\/input\/av-guided-hackathon\/train.csv')\ntest = pd.read_csv('..\/input\/av-guided-hackathon\/test.csv')","380de30d":"ss.head(10)","a5c8e91e":"train.head(3)","ee89a486":"test.head(3)","520c6210":"ID_COL, TARGET_COL = 'video_id', 'likes'","705f257c":"print(f'\\nTrain data contains {train.shape[0]} samples and {train.shape[1]} variables')\nprint(f'\\nTest data contains {test.shape[0]} samples and {test.shape[1]} variables')\n\nfeatures = [c for c in train.columns if c not in [ID_COL, TARGET_COL]]\nprint(f'\\nThe dataset contains total {len(features)} features')","0e34b06e":"_ = train[TARGET_COL].plot(kind = 'density', title = 'Likes Distribution', fontsize=14, figsize=(10, 6))","7fd5d131":"_ = pd.Series(np.log1p(train[TARGET_COL])).plot(kind = 'density', title = 'Log Likes Distribution', fontsize=14, figsize=(10, 6))","b65d2dda":"train.info()","44241783":"train.nunique()","66eb81a8":"train.columns","baa3ecb3":"num_cols = ['views', 'dislikes', 'comment_count']","9534bd11":"fig, axes = plt.subplots(3, 1, figsize=(8, 9))\nfor i, c in enumerate(num_cols):\n  _ = train[[c]].boxplot(ax=axes[i], vert=False)","00ea58d4":"for c in num_cols + ['likes']:\n  train[c] = np.log1p(train[c]) ","e6e01e73":"fig, axes = plt.subplots(4, 1, figsize=(8, 9))\nfor i, c in enumerate(num_cols + ['likes']):\n  _ = train[[c]].boxplot(ax=axes[i], vert=False)","341039d6":"plt.figure(figsize=(14, 8))\n_ = sns.heatmap(train[num_cols + ['likes']].corr(), annot=True)","dc18012c":"_ = sns.pairplot(train[num_cols + ['likes']], height=5, aspect=24\/16)","b873c6bf":"train.columns","dea364b2":"train['channel_title'].nunique()","51de3092":"cat_cols = ['category_id', 'country_code', 'channel_title']\nfig, axes = plt.subplots(1, 2, figsize=(24, 10))\n\nfor i, c in enumerate(['category_id', 'country_code']):\n    _ = train[c].value_counts()[::-1].plot(kind = 'pie', ax=axes[i], title=c, autopct='%.0f', fontsize=18)\n    _ = axes[i].set_ylabel('')\n    \n_ = plt.tight_layout()","8a94c85b":"sns.set(rc={'figure.figsize':(12.7, 8.27)})\n\ntop_20_channels = train['channel_title'].value_counts()[:20].reset_index()\ntop_20_channels.columns = ['channel_title', 'num_videos']\n\n_ = sns.barplot(data = top_20_channels, y = 'channel_title', x = 'num_videos')\n_ = plt.title(\"Top 20 Channels with maximum number of videos\")","963da9e3":"country_wise_channels = train.groupby(['country_code', 'channel_title']).size().reset_index()\ncountry_wise_channels.columns = ['country_code', 'channel_title', 'num_videos']\ncountry_wise_channels = country_wise_channels.sort_values(by = 'num_videos', ascending=False)\nfig, axes = plt.subplots(4, 1, figsize=(10, 20))\n\nfor i, c in enumerate(train['country_code'].unique()):\n  country = country_wise_channels[country_wise_channels['country_code'] == c][:10]\n  _ = sns.barplot(x = 'num_videos', y = 'channel_title', data = country, ax = axes[i])\n  _ = axes[i].set_title(f'Country Code {c}')\n\nplt.tight_layout()","eedd83c8":"_ = sns.catplot(x=\"category_id\", y=\"likes\", data=train, height=5, aspect=24\/8)","0d1192da":"_ = sns.catplot(x=\"country_code\", y=\"likes\", data=train, height=5, aspect=24\/8)","f492ae07":"_ = train.groupby('country_code')['likes'].mean().sort_values().plot(kind = 'barh')","bbad84bf":"train['publish_date'] = pd.to_datetime(train['publish_date'], format='%Y-%m-%d')\ntest['publish_date'] = pd.to_datetime(test['publish_date'], format='%Y-%m-%d')\ntrain['publish_date']","422bf055":"train['publish_date'].min(), train['publish_date'].max()","dbca409d":"train['publish_date'].dt.year.value_counts()","ec7f04a8":"latest_data_train = train[train['publish_date'] > '2017-11']\nlatest_data_test = test[test['publish_date'] > '2017-11']\n_ = latest_data_train.sort_values(by = 'publish_date').groupby('publish_date').size().rename('train').plot(figsize=(18, 6), title = 'Number of Videos')\n_ = latest_data_test.sort_values(by = 'publish_date').groupby('publish_date').size().rename('test').plot(figsize=(18, 6), title = 'Number of Videos')\n_ = plt.legend()","3f3b51e6":"latest_data = train[train['publish_date'] > '2017-11']\n_ = latest_data.sort_values(by = 'publish_date').groupby('publish_date')['likes'].mean().plot(figsize=(18, 6), title=\"Mean Likes\")","1bf9c749":"tmp = latest_data.groupby(['publish_date', 'country_code']).size().reset_index()\n_ = tmp.pivot_table(index = 'publish_date', columns = 'country_code', values=0).plot(subplots=True, figsize=(20, 20),\n                                                                                           title='Number of Videos by country',\n                                                                                           sharex=False,\n                                                                                           fontsize=20)\nplt.tight_layout()","61f191c8":"tmp = latest_data.groupby(['publish_date', 'country_code'])['likes'].mean().reset_index()\n_ = tmp.pivot_table(index = 'publish_date', columns = 'country_code', values='likes').plot(subplots=True, figsize=(20,20),\n                                                                                           title='Average Number of Likes by country',\n                                                                                           sharex=False,\n                                                                                           fontsize=20)\nplt.tight_layout()","3ef8fd47":"text_cols = ['title', 'tags', 'description']\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwc = WordCloud(stopwords = set(list(STOPWORDS) + ['|']), random_state = 42)\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(text_cols):\n  op = wc.generate(str(train[c]))\n  _ = axes[i].imshow(op)\n  _ = axes[i].set_title(c.upper(), fontsize=24)\n  _ = axes[i].axis('off')\n\n_ = fig.delaxes(axes[3])","828e9cb0":"def plot_countrywise(country_code = 'IN'):\n  country = train[train['country_code'] == country_code]\n  country = country[country['likes'] > 10]\n  fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n  axes = [ax for axes_row in axes for ax in axes_row]\n\n  for i, c in enumerate(text_cols):\n    op = wc.generate(str(country[c]))\n    _ = axes[i].imshow(op)\n    _ = axes[i].set_title(c.upper(), fontsize=24)\n    _ = axes[i].axis('off')\n\n  fig.delaxes(axes[3])\n  _ = plt.suptitle(f\"Country Code: '{country_code}'\", fontsize=30)","a5dcc369":"plot_countrywise(\"US\")","0ca8c2ac":"plot_countrywise(\"GB\")","f21fb444":"plot_countrywise(\"IN\")","74ee2616":"plot_countrywise(\"CA\")","2849c2a9":"train.head(2)","79695664":"def download_preds(preds_test, file_name = 'hacklive_sub.csv'):\n\n  ## 1. Setting the target column with our obtained predictions\n  ss[TARGET_COL] = preds_test\n\n  ## 2. Saving our predictions to a csv file\n\n  ss.to_csv(file_name, index = False)","15042074":"ss = pd.read_csv('..\/input\/av-guided-hackathon\/sample_submission_cxCGjdN.csv')\ntrain = pd.read_csv('..\/input\/av-guided-hackathon\/train.csv')\ntest = pd.read_csv('..\/input\/av-guided-hackathon\/test.csv')","db21ce88":"num_cols = ['views', 'dislikes', 'comment_count']\ncat_cols = ['category_id', 'country_code']\ntext_cols = ['title', 'channel_title', 'tags', 'description']\ndate_cols = ['publish_date']","915c9b33":"train.shape, test.shape","3fd9aa91":"df = pd.concat([train, test], axis=0).reset_index(drop = True)\ndf.shape","ba225761":"df.head(2)","ed5c88e3":"df = pd.get_dummies(df, columns = cat_cols)","57795319":"df = df.fillna(-999)\ndf.isnull().sum().sum()","9b906c5f":"df[num_cols + ['likes']] = df[num_cols + ['likes']].apply(lambda x: np.log1p(x))","0deae404":"df['likes']","4e468a93":"df.head(2)","daecac32":"train_proc, test_proc = df[:train.shape[0]], df[train.shape[0]:].reset_index(drop = True)\nfeatures = [c for c in train_proc.columns if c not in [ID_COL, TARGET_COL]]","141c2c98":"trn, val = train_test_split(train_proc, test_size=0.2, random_state = 420)\n\n###### Input to our model will be the features\nX_trn, X_val = trn[features], val[features]\n\n###### Output of our model will be the TARGET_COL\ny_trn, y_val = trn[TARGET_COL], val[TARGET_COL]\n\n##### Features for the test data that we will be predicting\nX_test = test_proc[features]","29932edb":"from sklearn.metrics import mean_squared_error, mean_squared_log_error\n\ndef rmsle(y_true, y_pred):\n  return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ndef av_metric(y_true, y_pred):\n  return 1000 * np.sqrt(mean_squared_error(y_true, y_pred))","0f632415":"features = [c for c in X_trn.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in text_cols + date_cols]","076f5e4e":"clf = LinearRegression()\n\n_ = clf.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = clf.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","ed0c07da":"clf = DecisionTreeRegressor(random_state=420)\n\n_ = clf.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = clf.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","c9202ded":"regr = RandomForestRegressor(max_depth=6, random_state=42)\n_ = regr.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = regr.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","0e5a8a5f":"\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}\n\nprint(random_grid)\n\n","22886bcc":"forest = RandomForestRegressor(n_jobs=-1)\nrf_random = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='neg_mean_squared_error')\n# Fit the random search model\nsearch = rf_random.fit(train_proc[cat_num_cols], train_proc[TARGET_COL])\nsearch.best_params_","dfeaaf06":"best_params = {'n_estimators': 155,\n 'min_samples_split': 5,\n 'max_features': 'sqrt',\n 'max_depth': 23}","2a60e304":"regr = RandomForestRegressor(**best_params)\n_ = regr.fit(X_trn[cat_num_cols], y_trn)\n\npreds_val = regr.predict(X_val[cat_num_cols])\n\nav_metric_score = av_metric(y_val, preds_val)\n\nprint(f'AV metric score is: {av_metric_score}')","98849fb2":"preds_test = clf.predict(X_test[cat_num_cols])\n\npreds_test = np.expm1(preds_test)\n\ndownload_preds(preds_test, 'regr_num_cat.csv')","62874ba1":"pd.qcut(np.arange(10), 5, labels = False, duplicates='drop')","e7643923":"from sklearn.model_selection import StratifiedKFold\ndef run_clf_kfold(clf, train, test, features):\n\n  N_SPLITS = 5\n\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  target = train[TARGET_COL]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET_COL], 10, labels = False, duplicates='drop')\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ############# Get train, validation and test sets along with targets ################\n  \n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    ############# Scaling Data ################\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n\n\n    ############# Fitting and Predicting ################\n\n    _ = clf.fit(X_trn, y_trn)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = av_metric(y_val, preds_val)\n    print(f'\\nAV metric score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test \/ N_SPLITS\n\n\n  oofs_score = av_metric(target, oofs)\n  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n\n  return oofs, preds","d9dab5c6":"rf_params = best_params = {'n_estimators': 155,\n 'min_samples_split': 5,\n 'max_features': 'sqrt',\n 'max_depth': 23}\n\nclf = RandomForestRegressor(**rf_params)\n        \n\ndt_oofs, dt_preds = run_clf_kfold(clf, train_proc, test_proc, cat_num_cols)","b6b84d5f":"def run_gradient_boosting(clf, fit_params, train, test, features):\n  N_SPLITS = 5\n  oofs = np.zeros(len(train_proc))\n  preds = np.zeros((len(test_proc)))\n\n  target = train[TARGET_COL]\n\n  folds = StratifiedKFold(n_splits = N_SPLITS)\n  stratified_target = pd.qcut(train[TARGET_COL], 10, labels = False, duplicates='drop')\n\n  feature_importances = pd.DataFrame()\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, stratified_target)):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n    ### Training Set\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    ### Validation Set\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    ### Test Set\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n    fold_importance = pd.DataFrame({'fold': fold_ + 1, 'feature': features, 'importance': clf.feature_importances_})\n    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n\n    ### Instead of directly predicting the classes we will obtain the probability of positive class.\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n\n    fold_score = av_metric(y_val, preds_val)\n    print(f'\\nAV metric score for validation set is {fold_score}')\n\n    oofs[val_idx] = preds_val\n    preds += preds_test \/ N_SPLITS\n\n\n  oofs_score = av_metric(target, oofs)\n  print(f'\\n\\nAV metric for oofs is {oofs_score}')\n\n  feature_importances = feature_importances.reset_index(drop = True)\n  fi = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending = False)[:20][::-1]\n  fi.plot(kind = 'barh', figsize=(12, 6))\n\n  return oofs, preds, fi","3421a738":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","274fb492":"def join_df(train, test):\n\n  df = pd.concat([train, test], axis=0).reset_index(drop = True)\n  features = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\n  df[num_cols + ['likes']] = df[num_cols + ['likes']].apply(lambda x: np.log1p(x))\n\n  return df, features\n\ndef split_df_and_get_features(df, train_nrows):\n\n  train, test = df[:train_nrows].reset_index(drop = True), df[train_nrows:].reset_index(drop = True)\n  features = [c for c in train.columns if c not in [ID_COL, TARGET_COL]]\n  \n  return train, test, features","c78addd7":"df, features = join_df(train, test)","f7849c39":"cat_cols = ['category_id', 'country_code', 'channel_title']","14634f1b":"### Label Encoding\n\ndf[cat_cols] = df[cat_cols].apply(lambda x: pd.factorize(x)[0])","50fc39f1":"df['publish_date'] = pd.to_datetime(df['publish_date'], format='%Y-%m-%d')\ndf['publish_date_days_since_start'] = (df['publish_date'] - df['publish_date'].min()).dt.days\n\ndf['publish_date_day_of_week'] = df['publish_date'].dt.dayofweek\ndf['publish_date_year'] = df['publish_date'].dt.year\ndf['publish_date_month'] = df['publish_date'].dt.month","d7ee0b5b":"features = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","9b675a6c":"cat_num_cols","22298702":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])","a015290d":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","a649a643":"df['channel_title_num_videos'] = df['channel_title'].map(df['channel_title'].value_counts())\ndf['publish_date_num_videos'] = df['publish_date'].map(df['publish_date'].value_counts())","ce308fd0":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","148f1eb6":"cat_num_cols","ca1413ac":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","42d54249":"df['channel_in_n_countries'] = df.groupby('channel_title')['country_code'].transform('nunique')\ndf['channel_in_n_countries'].unique()","2554ad9f":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","08fb88fc":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","dd841c52":"df['channel_title_mean_views'] = df.groupby('channel_title')['views'].transform('mean')\ndf['channel_title_max_views'] = df.groupby('channel_title')['views'].transform('max')\ndf['channel_title_min_views'] = df.groupby('channel_title')['views'].transform('min')\n\ndf['channel_title_mean_comments'] = df.groupby('channel_title')['comment_count'].transform('mean')\ndf['channel_title_max_comments'] = df.groupby('channel_title')['comment_count'].transform('max')\ndf['channel_title_min_comments'] = df.groupby('channel_title')['comment_count'].transform('min')","3d958263":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","a92b8430":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","44060447":"cb_preds_t = np.expm1(cb_preds)\ndownload_preds(cb_preds_t, file_name = 'catboost_5_folds.csv')","3bd4d6f3":"df['title_len'] = df['title'].apply(lambda x: len(x))\ndf['description_len'] = df['description'].apply(lambda x: len(x))\ndf['tags_len'] = df['tags'].apply(lambda x: len(x))","a1efe314":"train_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","1b24af1e":"clf = CatBoostRegressor(n_estimators = 3000,\n                       learning_rate = 0.05,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=420,\n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","adb8a513":"from sklearn.feature_extraction.text import CountVectorizer\n?CountVectorizer","79304d43":"TOP_N_WORDS = 50\n\nvec = CountVectorizer(max_features = TOP_N_WORDS)\ntxt_to_fts = vec.fit_transform(df['description']).toarray()\ntxt_to_fts.shape","8488fd40":"c = 'description'\ntxt_fts_names = [c + f'_word_{i}_count' for i in range(TOP_N_WORDS)]\ndf[txt_fts_names] = txt_to_fts\n\ntrain_proc, test_proc, features = split_df_and_get_features(df, train.shape[0])\nfeatures = [c for c in df.columns if c not in [ID_COL, TARGET_COL]]\ncat_num_cols = [c for c in features if c not in ['title', 'tags', 'description', 'publish_date']]","726c45b6":"cat_num_cols","a3c660b3":"clf = CatBoostRegressor(n_estimators = 4000,\n                       learning_rate = 0.06,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=4200,\n                       )\n\nfit_params = {'verbose': 300, 'early_stopping_rounds': 200}\n\ncb_oofs, cb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols) ","c9d0c955":"cb_preds_t = np.expm1(cb_preds)\ndownload_preds(cb_preds_t, file_name = 'catboost_text_cols_bow.csv')","600662c4":"clf = LGBMRegressor(n_estimators = 4000,\n                        learning_rate = 0.04,\n                        colsample_bytree = 0.65,\n                        metric = 'None',\n                        )\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200, 'eval_metric': 'rmse'}\n\nlgb_oofs, lgb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","48ec99c3":"lgb_preds_t = np.expm1(lgb_preds)\ndownload_preds(lgb_preds_t, file_name = 'lgb_text_cols_bow.csv')","3c1ee6ba":"clf = XGBRegressor(n_estimators = 3000,\n                    max_depth = 7,\n                    learning_rate = 0.05,\n                    colsample_bytree = 0.5,\n                    random_state=4200,\n                    )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\nxgb_oofs, xgb_preds, fi = run_gradient_boosting(clf, fit_params, train_proc, test_proc, cat_num_cols)","a7736c5b":"xgb_preds_t = np.expm1(xgb_preds)\n\ndownload_preds(xgb_preds_t, file_name = 'xgb_text_cols_bow.csv')","4144a79c":" av_metric(np.log1p(train[TARGET_COL]), lgb_oofs * 0.7 + cb_oofs * 0.3)","a7229312":"train_new = train[[ID_COL, TARGET_COL]]\ntrain_new[TARGET_COL] = np.log1p(train_new[TARGET_COL])\n\ntest_new = test[[ID_COL]]\n\ntrain_new['lgb'] = lgb_oofs\ntest_new['lgb'] = lgb_preds\n\ntrain_new['cb'] = cb_oofs\ntest_new['cb'] = cb_preds\n\ntrain_new['xgb'] = xgb_oofs\ntest_new['xgb'] = xgb_preds\n\nfeatures = [c for c in train_new.columns if c not in [ID_COL, TARGET_COL]]","3b367c44":"clf = LinearRegression()\n\nens_oofs, ens_preds = run_clf_kfold(clf, train_new, test_new, features)","05225193":"ens_preds_t = np.expm1(ens_preds)\ndownload_preds(ens_preds_t, file_name = 'hacklive_ensemble_final.csv')","d7b38b6b":"pd.read_csv('hacklive_ensemble_final.csv')","fca68fbc":"**Helper Function to Download Test Predictions as CSV**","f41c0143":"**Categorical Columns**","10da84a6":"**Average likes per Country**","89da1e8d":"**Feature Engineering for text data**","a8e3d71a":"**Concatenate train and test data**","1417a38a":"**Let check the datatype of all the columns**","1f77e027":"**Ensembling**","0a8e9c5e":"**One hot Encoding on Categorical columns**","8c8efa08":"**Bag of Words Approach for Text Based Features**","92e4dd1c":"**Likes distribution per Category**","e30a96cb":"**Identify target columns and feature variables**","71186942":"**Numeric Columns**","eeb34d72":"**Bivariate Analysis**","635bcd87":"**Helper Function to run Stratified K-Fold**","804a3650":"**Boosting Algorithm**","2447dc85":"**Correlation heatmaps**","00de389a":"# Analytics Vidhya - Guided Hackathon 2\n\n# Problem Statement\n\nAs YouTube becomes one of the most popular video-sharing platforms, YouTuber is developed as a new type of career in recent decades. YouTubers earn money through advertising revenue from YouTube videos, sponsorships from companies, merchandise sales, and donations from their fans. In order to maintain a stable income, the popularity of videos become the top priority for YouTubers. Meanwhile, some of our friends are YouTubers or channel owners in other video-sharing platforms. This raises our interest in predicting the performance of the video. If creators can have a preliminary prediction and understanding on their videos\u2019 performance, they may adjust their video to gain the most attention from the public.\n\nYou have been provided details on videos along with some features as well. Can you accurately predict the number of likes for each video using the set of input variables?\n\n\n**Refer the dataset :-** https:\/\/www.kaggle.com\/jainpooja\/av-guided-hackathon","008762f1":"**Helper Function**","a97f36d9":"**Country wise number of videos for channels**","303593c2":"**DateTime Variables**","383cfd95":"**CatPlots**","4ccdf529":"**Number of Videos in data datewise**","afca4bba":"**As we see the data is highly right skewed, we will apply log transformation**","2143ae1c":"It is clear from the above output that there are no null values","30b6bd5f":"**Univariate Analysis**","92155795":"**Pair Plots**","e71b4d8b":"**K-Fold on Random Forest**","630c2b87":"**Model on Numerical and Categorical columns**","554731e6":"**Average number of likes by country order by date**","deebaed7":"**Load the dataset**","799a6cfe":"**Let's check the target distribution as it is a regression problem**","f905f233":"**To check results on validation dataset after train the model**","862339d2":"**Split data into train and test**","e513008c":"**Likes Distribution per country**","80b4028d":"**Average likes in data sorted by date**","55eb2c26":"**Split train data into train and validation sets**","4806ca4a":"**Bivariate Analysis**","7b53a051":"**HyperParameter Tuning with Random Search CV**","60664c72":"**Number of videos by country**","4ee5369f":"**Country wise highly liked Youtube videos top words**","a1f8e134":"**Analyze Textual Data**","cdec3551":"**Using Stratified - K_Fold Validation**","449f174d":"**Load the libraries**","62fa0e65":"**Log Transformation of Numerical Columns**","6d99afb1":"**Feature Engineering**","0b88e893":"**Segregate different types of columns**","b8c88a0d":"**Unique values in each variable**","23a1c927":"**Grouping Features**","0ee454e8":"Looks like videos posted in England have an higher average number of likes compared to videos posted in India."}}