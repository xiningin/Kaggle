{"cell_type":{"642a39a4":"code","f39857f7":"code","11029894":"code","8c5018bc":"code","31a13342":"code","a62f2d30":"code","767a8fa2":"code","be9caab7":"code","1ef78311":"code","c2edf3e0":"code","e5467b45":"code","2f6b180b":"code","fb313e7c":"code","3408f09d":"code","1c95791c":"code","d56087a7":"code","c6a9a149":"code","f56b657c":"code","fdf8dc97":"code","08b4ea35":"code","284090e2":"code","d838ea2a":"code","76b896b4":"code","571d8f7d":"code","aa1af849":"code","d988fc35":"code","0062422c":"code","26292e3b":"code","176a7da0":"code","b71b8c2c":"code","577478a5":"code","f7e82d7a":"code","0134a552":"code","843c084e":"code","04e2987c":"code","f5790b40":"code","7e4db97a":"code","0c0c482f":"code","f3b5ffe9":"code","e86ec38c":"code","00bb6067":"code","802a727e":"code","b7773ae3":"code","53d19cb4":"code","8563ff44":"code","405ee689":"code","6f506a47":"code","8c0ef2a8":"code","764ff151":"code","0fdbd30f":"code","47b05708":"code","c11feb4b":"code","60e4f732":"code","a8c7f1bb":"code","7276afba":"code","790bd0be":"code","a5d3329a":"code","c5cbdcba":"code","ba2cd2e5":"code","593b9fdf":"code","9cea4d9d":"code","1f9856ad":"code","d368ebb6":"code","0806874d":"code","b5b2c5a9":"code","1638d1ee":"code","508add28":"code","048ef34a":"code","25085e28":"code","d7f41e6b":"code","a056a9de":"code","9f44d01f":"code","5c837878":"code","75d45a6a":"code","0590d48f":"code","b74a34f0":"code","46bbce60":"markdown","a8b72838":"markdown","72a106cc":"markdown","4ce204f3":"markdown","607d964a":"markdown","31153d7b":"markdown","ece0ffdd":"markdown","0faa6fb5":"markdown","e81edc78":"markdown","1f055476":"markdown","d0a916f9":"markdown","ab46a4b0":"markdown","ee1e747a":"markdown","48ae33ad":"markdown","e4a87cb4":"markdown","a895222c":"markdown","23bf9a19":"markdown","edfcfadc":"markdown","5244febc":"markdown","eac7796f":"markdown","4460a752":"markdown","c31cb627":"markdown","86d671a3":"markdown","89c8c9ba":"markdown","c71c659b":"markdown","4c2f1404":"markdown","90ac7d3f":"markdown"},"source":{"642a39a4":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import boxcox\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport missingno\n\nplt.style.use('dark_background')\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","f39857f7":"train_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain_df.head()","11029894":"test_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest_df.head()","8c5018bc":"train_df.describe()","31a13342":"test_df.describe()","a62f2d30":"missingno.bar(train_df, color='orangered');","767a8fa2":"plt.pie(train_df.Sex.value_counts(), labels=['Male', 'Female'], colors=['orangered', 'lightsalmon'], autopct=\"%1.2f%%\")\nplt.title('Sex Distribution Graph', fontweight='bold', fontsize=18);","be9caab7":"def univariate_graph(title, xlabel, x, y, ylabel='Frequency'):\n    plt.bar(x, y, color='orangered')\n    plt.title(title, fontweight='bold', fontsize=14)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.show();","1ef78311":"univariate_graph(x=['Survived', 'Not Survived'],\n                y=train_df.Survived.value_counts(),\n                title='Survived Distribution',\n                xlabel='Survived')","c2edf3e0":"univariate_graph(x=['Lower', 'Upper', 'Middle'],\n                y=train_df.Pclass.value_counts(),\n                title='Pclass Distribution',\n                xlabel='Pclass')","e5467b45":"plt.hist(train_df.Age, bins=10, color='orangered')\nplt.title('Age Distribution', fontweight='bold', fontsize=14)\nplt.xlabel('Age')\nplt.ylabel('Frequency');","2f6b180b":"univariate_graph(x=train_df.SibSp.value_counts().index,\n                y=train_df.SibSp.value_counts(),\n                title='Sibling\/Spouse Distribution',\n                xlabel='SibSp')","fb313e7c":"univariate_graph(x=train_df.Parch.value_counts().index,\n                y=train_df.Parch.value_counts(),\n                title='Parch Distribution',\n                xlabel='Parch')","3408f09d":"univariate_graph(x=['Southampton', 'Cherbourg', 'Queenstown'],\n                y=train_df.Embarked.value_counts(),\n                title='Embarked Distribution',\n                xlabel='Embarked')","1c95791c":"plt.hist(train_df.Fare, bins=5, color='orangered')\nplt.title('Fare Distribution', fontweight='bold', fontsize=14)\nplt.xlabel('Fare')\nplt.ylabel('Frequency');","d56087a7":"sample_col = [col for col in train_df.columns if pd.api.types.is_numeric_dtype(train_df[col])]\nplt.style.use('dark_background')\ndata = train_df.dropna()\nplt.boxplot(data[sample_col[1:]], patch_artist=True, labels=sample_col[1:])\nplt.title('Outlier Chart', fontsize=24, fontweight='bold');","c6a9a149":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\nax1.boxplot(train_df['SibSp'], patch_artist=True, labels=['SibSp'])\nax1.set_title('SibSp Outlier Chart', fontsize=18, fontweight='bold')\nax2.boxplot(train_df['Parch'], patch_artist=True, labels=['Parch'])\nax2.set_title('Parch Outlier Chart', fontsize=18, fontweight='bold')\ndata = train_df.dropna()\nax3.boxplot(data['Age'], patch_artist=True, labels=['Age'])\nax3.set_title('Age Outlier Chart', fontsize=18, fontweight='bold');","f56b657c":"sns.heatmap(train_df.corr(), annot=True, cmap=\"YlOrBr\");","fdf8dc97":"plt.bar(['female', 'male'], train_df['Sex'][train_df['Survived'] == 1].value_counts(), width=0.3, color='orangered')\nplt.bar(['female', 'male'], train_df['Sex'][train_df['Survived'] == 0].value_counts().sort_values(), bottom=train_df['Sex'][train_df['Survived'] == 1].value_counts(), width=0.3, color='lightsalmon')\nplt.legend(['Survived', 'NotSurvived'])\nplt.title('Sex Survived Relationship', fontsize=18, fontweight='bold')\nplt.show();","08b4ea35":"plt.bar(['Upper', 'Middle', 'Lower'], train_df['Pclass'][train_df['Survived'] == 1].value_counts().sort_values(), color='orangered')\nplt.bar(['Upper', 'Middle', 'Lower'], train_df['Pclass'][train_df['Survived'] == 0].value_counts(), color='lightsalmon', bottom=train_df['Pclass'][train_df['Survived'] == 1].value_counts().sort_values())\nplt.title('Pclass Survived Relationship', fontsize=18, fontweight='bold')\nplt.legend(['Survived', 'NotSurvived'])\nplt.show();","284090e2":"plt.bar([0, 1, 2, 3, 4, 8, 5], train_df['SibSp'][train_df['Survived'] == 1].value_counts(), color='orangered')\nplt.bar([0, 1, 2, 3, 4, 8, 5], train_df['SibSp'][train_df['Survived'] == 0].value_counts(), color='lightsalmon', bottom=train_df['SibSp'][train_df['Survived'] == 1].value_counts())\nplt.title('SibSp Survived Relationship', fontsize=18, fontweight='bold')\nplt.legend(['Survived', 'NotSurvived'])\nplt.show();","d838ea2a":"train_df['train_test'] = 1\ntest_df['train_test'] = 0\ntrain_copy = train_df.drop('Survived', axis=1)\ncombine_df = pd.concat([train_copy, test_df])\ncombine_df.head()","76b896b4":"for col in combine_df.columns:\n    if(combine_df.isna().sum()\/len(combine_df) > 0.0).sum() != 0:\n        if (combine_df[col].isna().sum()\/len(combine_df) > 0.0):\n            print(f\"{col}: {combine_df[col].isna().sum()\/len(combine_df)}\")\n    else:\n        print('No missing value found!!')","571d8f7d":"combine_df.Cabin.fillna('X', inplace=True)\ncombine_df.head()","aa1af849":"data = [f[0] for f in combine_df.Cabin]\ncombine_df['Update_Cabin'] = data\ncombine_df.head()","d988fc35":"combine_df.drop('Cabin', axis=1, inplace=True)","0062422c":"combine_df['Age'] = combine_df['Age'].fillna(combine_df['Age'].median())\ncombine_df['Embarked'] = combine_df['Embarked'].fillna(combine_df['Embarked'].mode()[0])\ncombine_df['Fare'] = combine_df['Fare'].fillna(combine_df['Fare'].median())\ncombine_df['Ticket'] = combine_df['Ticket'].fillna(combine_df['Ticket'].mode()[0])","26292e3b":"for col in combine_df.columns:\n    if(combine_df.isna().sum()\/len(combine_df) > 0.0).sum() != 0:\n        if (combine_df[col].isna().sum()\/len(combine_df) > 0.0):\n            print(f\"{col}: {combine_df[col].isna().sum()\/len(combine_df)}\")\n    else:\n        print('No missing value found!!')\n        break","176a7da0":"combine_df['Update_Age'] = boxcox(combine_df.Age)[0]\ncombine_df['Update_Fare'] = boxcox(combine_df.Fare)[0]","b71b8c2c":"combine_df.head()","577478a5":"quantile_col = ['Parch', 'SibSp', 'Update_Fare']\nfor i in range(len(quantile_col)):\n    q1 = combine_df[quantile_col[i]].quantile(0.25)\n    q3 = combine_df[quantile_col[i]].quantile(0.75)\n    IQR = q3 - q1\n    combine_df[quantile_col[i]] = np.where(combine_df[quantile_col[i]] < q1, q1 - (1.5 * IQR), combine_df[quantile_col[i]])\n    combine_df[quantile_col[i]] = np.where(combine_df[quantile_col[i]] > q3, q3 + (1.5 * IQR), combine_df[quantile_col[i]])","f7e82d7a":"new_combine_df = combine_df.drop(['Age', 'Fare'], axis=1)","0134a552":"sample_col = [col for col in new_combine_df.columns if pd.api.types.is_numeric_dtype(new_combine_df[col])]\nplt.style.use('dark_background')\nplt.boxplot(new_combine_df[sample_col[1:]], patch_artist=True, labels=sample_col[1:])\nplt.title('Outlier Chart', fontsize=24, fontweight='bold');","843c084e":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 5))\nax1.boxplot(new_combine_df['SibSp'], patch_artist=True, labels=['SibSp'])\nax1.set_title('SibSp Outlier Chart', fontsize=18, fontweight='bold')\nax2.boxplot(new_combine_df['Parch'], patch_artist=True, labels=['Parch'])\nax2.set_title('Parch Outlier Chart', fontsize=18, fontweight='bold')\nax3.boxplot(new_combine_df['Update_Age'], patch_artist=True, labels=['Age'])\nax3.set_title('Age Outlier Chart', fontsize=18, fontweight='bold');\nax4.boxplot(new_combine_df['Update_Fare'], patch_artist=True, labels=['Fare'])\nax4.set_title('Fare Outlier Chart', fontsize=18, fontweight='bold');","04e2987c":"new_combine_df.head()","f5790b40":"normal_col = ['Update_Age', 'Update_Fare', 'SibSp', 'Parch', 'Pclass']\nfor col in normal_col:\n    sns.distplot(new_combine_df[col], color='orangered')\n    plt.show();","7e4db97a":"new_combine_df['Family'] = new_combine_df['SibSp'] + new_combine_df['Parch']\nnew_combine_df.head()","0c0c482f":"sns.heatmap(new_combine_df.corr(), annot=True, cmap='YlOrBr');","f3b5ffe9":"new_combine_df.head()","e86ec38c":"value = new_combine_df.groupby('Pclass')['Pclass'].value_counts().to_dict()\nnew_combine_df['Pclass_Count'] = new_combine_df.Pclass.apply(lambda x: value.get((x,x), 0))\n\nvalue = new_combine_df.groupby('Embarked')['Embarked'].value_counts().to_dict()\nnew_combine_df['Embarked_Count'] = new_combine_df.Embarked.apply(lambda x: value.get((x, x), 0))\n\nvalue = new_combine_df.groupby('Sex')['Sex'].value_counts().to_dict()\nnew_combine_df['Sex_Count'] = new_combine_df.Sex.apply(lambda x: value.get((x, x), 0))\n\nvalue = new_combine_df.groupby('Update_Cabin')['Update_Cabin'].value_counts().to_dict()\nnew_combine_df['Cabin_Count'] = new_combine_df.Update_Cabin.apply(lambda x: value.get((x, x), 0))\n\nvalue_list = []\nfor value in new_combine_df.Ticket:\n    if (len(value.split(' ')) > 1):\n        value_list.append(value.split(' ')[0])\n    else:\n        value_list.append('X')\nnew_combine_df['Ticket_Category'] = value_list\n","00bb6067":"new_combine_df.head()","802a727e":"sample_ds = []\nfor value in new_combine_df.Ticket:\n    if(len(value.split(' ')) > 1):\n        if(value.split(' ')[1] == ''):\n            sample_ds.append(np.nan)\n        else:\n            sample_ds.append(value.split(' ')[1])\n    else:\n        sample_ds.append(value)\nnew_combine_df['Update_Ticket'] = sample_ds","b7773ae3":"new_combine_df.fillna(new_combine_df.Update_Ticket.mode()[0], inplace=True)","53d19cb4":"new_combine_df.Update_Ticket = new_combine_df.Update_Ticket.astype('int32')","8563ff44":"min_value = new_combine_df.Update_Ticket.min()\nmax_value = new_combine_df.Update_Ticket.max()\nvalue_ds = []\nfor value in new_combine_df.Update_Ticket:\n    value_ds.append((value - min_value)\/(max_value - min_value))","405ee689":"new_combine_df['Update_Ticket'] = value_ds\nnew_combine_df.head()","6f506a47":"quantile_col = ['Update_Ticket']\nfor i in range(len(quantile_col)):\n    q1 = new_combine_df[quantile_col[i]].quantile(0.25)\n    q3 = new_combine_df[quantile_col[i]].quantile(0.75)\n    IQR = q3 - q1\n    new_combine_df[quantile_col[i]] = np.where(new_combine_df[quantile_col[i]] < q1, q1 - (1.5 * IQR), new_combine_df[quantile_col[i]])\n    new_combine_df[quantile_col[i]] = np.where(new_combine_df[quantile_col[i]] > q3, q3 + (1.5 * IQR), new_combine_df[quantile_col[i]])","8c0ef2a8":"encoder = LabelEncoder()\nvalue = encoder.fit_transform(new_combine_df.Sex)\nnew_combine_df.Sex = value","764ff151":"value = encoder.fit_transform(new_combine_df.Embarked)\nnew_combine_df.Embarked = value","0fdbd30f":"value = encoder.fit_transform(new_combine_df.Update_Cabin)\nnew_combine_df.Update_Cabin = value","47b05708":"value = encoder.fit_transform(new_combine_df.Ticket_Category)\nnew_combine_df.Ticket_Category = value","c11feb4b":"new_combine_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)","60e4f732":"new_combine_df.head()","a8c7f1bb":"new_combine_df['PSib'] = new_combine_df['Pclass']\nnew_combine_df['PSib'] = new_combine_df['PSib'].map(new_combine_df.groupby('Pclass')['SibSp'].mean())","7276afba":"new_combine_df.head()","790bd0be":"new_combine_df['ASex'] = new_combine_df['Ticket_Category']\nnew_combine_df['ASex'] = new_combine_df['ASex'].map(new_combine_df.groupby('Ticket_Category')['Update_Cabin'].mean())","a5d3329a":"new_combine_df['PCabin'] = new_combine_df['Pclass']\nnew_combine_df['PCabin'] = new_combine_df['PCabin'].map(new_combine_df.groupby('Pclass')['Update_Cabin'].mean())","c5cbdcba":"df_train = new_combine_df[new_combine_df['train_test'] == 1]\ndf_test = new_combine_df[new_combine_df['train_test'] == 0]\ndf_train.drop(['train_test'], axis=1, inplace=True)\ndf_test.drop(['train_test'], axis=1, inplace=True)","ba2cd2e5":"df_train.head()","593b9fdf":"df_test.head()","9cea4d9d":"plt.figure(figsize=(10, 10))\nsns.heatmap(df_train.corr(), annot=True);","1f9856ad":"del_col = ['Pclass', 'SibSp', 'Embarked', 'Sex', 'Cabin_Count', 'Parch']\ndf_train.drop(del_col, axis=1, inplace=True)\ndf_test.drop(del_col, axis=1, inplace=True)","d368ebb6":"df_train.head()","0806874d":"np.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(df_train, train_df.Survived, test_size=0.2)\nlen(X_train), len(y_train)","b5b2c5a9":"from sklearn.tree import DecisionTreeClassifier\ndecision_model = DecisionTreeClassifier()\ndecision_model.fit(X_train, y_train)\nplt.barh(X_train.columns, decision_model.feature_importances_)\nplt.title('Feature Importance');","1638d1ee":"col = ['Update_Ticket', 'Sex_Count', 'Update_Fare', 'Update_Age']\ntrain = df_train[col]\nX0, X1, y0, y1 = train_test_split(train, train_df.Survived, test_size=0.2)","508add28":"params = {'ccp_alpha': 0.0,\n 'criterion': 'friedman_mse',\n 'init': None,\n 'learning_rate': 0.1058986682719916,\n 'loss': 'exponential',\n 'max_depth': 5,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_impurity_split': None,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'n_estimators': 100,\n 'n_iter_no_change': None,\n 'random_state': None,\n 'subsample': 1.0,\n 'tol': 0.0001,\n 'validation_fraction': 0.1,\n 'verbose': 0,\n 'warm_start': False}\n\ngradient_model = GradientBoostingClassifier(**params)\ngradient_model.fit(X_train, y_train)\ngradient_model.score(X_test, y_test)","048ef34a":"gradient_model = GradientBoostingClassifier(learning_rate=0.1)\ngradient_model.fit(X_train, y_train)","25085e28":"gradient_model.score(X_test, y_test)","d7f41e6b":"lgbm_model = LGBMClassifier(n_estimators=40)\nlgbm_model.fit(X_train, y_train)","a056a9de":"lgbm_model.score(X_test, y_test)","9f44d01f":"from xgboost import XGBRFClassifier\n\nxg_model = XGBRFClassifier()\nxg_model.fit(X_train, y_train)\nxg_model.score(X_test, y_test)","5c837878":"voting_model = VotingClassifier(estimators=[('gm', gradient_model), \n                                            ('xg', xg_model),\n                                            ('lgbm', lgbm_model)\n                                           ], voting='hard', verbose=True)\nvoting_model.fit(X_train, y_train)","75d45a6a":"voting_model.score(X_test, y_test)","0590d48f":"def submission_file(model, filename='submission.csv'):\n    y_preds = model.predict(df_test)\n    submission = pd.DataFrame(y_preds, columns=['Survived'])\n    submission.index = test_df.PassengerId\n    submission.to_csv(filename)","b74a34f0":"submission = submission_file(voting_model, filename='submission3.csv')\nsubmission","46bbce60":"So, we have outlier value in Fare. We have to see more deeply in SibSp and Parch column but seeing the dataset only we can say that it don't have any outlier.","a8b72838":"# Prepare Training and Testing Data\nIn this section, we seperate the dataset into training and testing which we had combine earlier for preprocessing and transformation purpose.","72a106cc":"# Feature Engineering\nIn this section, we create some new columns from the existing one.","4ce204f3":"Since, we have lots of missing value in `Cabin` column so filling out with some random value doesnot make a good call. So we going to drop out the column from the dataset and fill the rest of the missing column with the help of the EDA.","607d964a":"# Tabular Playground Series - Apr 2021\nIn this notebook, we perform and analyse the `Titanic Dataset` generated using the CTGAN. We need to create the machine learning model that predict the `Survived` field using the 11 different variables. Evaluation is depend upon the `accuracy` of the model. ","31153d7b":"We had deal with all the missing value present in our dataset. Now, its time to perform more EDA to find the normalization and the linear relationship in our dataset. This will help to choose the estimators for training the ml model.","ece0ffdd":"# Data Dictionary\n| Variable | Definition | Key |\n| -------- | ---------- | --- |\n| survival | Survival  |0 = No, 1 = Yes |\n| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n| sex | Sex | |\n| Age | Age in years | |\n| sibsp | # of siblings \/ spouses aboard the Titanic | |\n| parch | # of parents \/ children aboard the Titanic | |\n| ticket | Ticket number | |\n| fare | Passenger fare | |\n| cabin | Cabin number | |\n| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |","0faa6fb5":"# Handling Categorical Datatyes\nIn this section, we transform the categorical data into numerical dataset.","e81edc78":"So most of the passengers are from the age 20 to 40 years. But again, passenger with the age below the 0 or 5 is not possible that they are travelling on the ship. We need to handle such case before fitting the model.","1f055476":"# Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","d0a916f9":"# Submision","ab46a4b0":"# Handle Qunatile in Dataset\nIn this section, we handle the quantile value present in the dataset for making the dataset more consistent.","ee1e747a":"## TODO:\n   * Handle the outliers in columns: (Fare, Age, SibSp, Parch)","48ae33ad":"# More EDA\nIn this section, we perform more EDA to find out the normalization graph in the univariate columns and the linear relationship between the different features in the dataset.","e4a87cb4":"## Univariate","a895222c":"# Model Training\nIn this section, we train the classification model.","23bf9a19":"Yeah!! We found the outlier in the `SibSp`, `Parch` and `Age` when we check these column more closely.","edfcfadc":"## TODO:\n* Handle the Age column and perform normalization.\n* Handle the Fare column and perform normalization.","5244febc":"# Load the Dataset \nIn this section, we import all the useful libraries and load the dataset into the notebook.","eac7796f":"# Perform statistics opertaion\nIn this section, we perform the basic statistics operation like mean, standardization, min, max, etc.","4460a752":"So, most of the passengers on the Titanic are came alone. Somwe of them are come in couple or sibling while some of them come with their family.","c31cb627":"## Bivariate","86d671a3":"# Exploratory Data Analysis\nIn this section, we perform the Exploratory Data Analysis or EDA to understand the dataset and find the useful patterns within the dataset between the different variables.","89c8c9ba":"So most of the passenger are going to the `Southampton`.","c71c659b":"One of the weird observation in the dataset is in the Age column, as it had a minimum age of 0.080 which is really not possible. We need to handle this errorness in the dataset and replace it with something else.","4c2f1404":"So, as we stated above that we are going to drop the `cabin` column from the dataset.","90ac7d3f":"# Handle Missing Value\nIn this section, we handle the missing value present in the dataset. In some case we drop the column from the dataset or in some column we handle using the median and the mode."}}