{"cell_type":{"fda4c1e4":"code","395ffc01":"code","0fab8451":"code","ee7c5ccf":"code","5f0a79f4":"code","bfdb5887":"code","c0822002":"code","d85d3e99":"code","715d9aa7":"code","91dcf4fa":"code","6de20d3b":"code","e52eabb7":"code","9b26a28f":"code","fb789f07":"code","2fc42666":"code","87763a94":"code","799ac8ce":"code","6355ed56":"markdown","818cc70b":"markdown","4542f53d":"markdown","406231e0":"markdown","26f14f41":"markdown"},"source":{"fda4c1e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","395ffc01":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import  MinMaxScaler\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom yellowbrick.cluster import SilhouetteVisualizer\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0fab8451":"data= pd.read_csv('\/kaggle\/input\/unsupervised-learning-on-country-data\/Country-data.csv')","ee7c5ccf":"data.head()","5f0a79f4":"data.describe().T","bfdb5887":"# Visualization of data\nst=MinMaxScaler()\n\npc=PCA(n_components=9)\ndata_pca=pc.fit_transform(st.fit_transform(data.iloc[:,1:]))\n# evaluating two clusters\nsns.scatterplot(x=data_pca[:,0],y=data_pca[:,1])\n","c0822002":"sum(pc.explained_variance_ratio_[0:2])","d85d3e99":"sns.lineplot(x=np.arange(1,10),y=pc.explained_variance_ratio_*100)","715d9aa7":"def train_KMeans(data,cluster):\n    cluster_id,losses,silhoutte,kmean=[],[],[],[]\n    st=MinMaxScaler()\n    dt=st.fit_transform(data.iloc[:,1:].values)\n    # Randomly training 10 models for a cluster to overcome poor random initialization \n    for i in range(10):\n        km=KMeans(n_clusters=cluster,init='random',precompute_distances=True,\n                  random_state=np.random.randint(0,100),n_jobs=-1,)\n        pred=km.fit_predict(dt)\n        cluster_id.append(pred)\n        losses.append(km.inertia_)\n        kmean.append(km)\n \n        silhoutte.append(silhouette_score(dt,pred))\n    return cluster_id,losses,silhoutte,kmean","91dcf4fa":"\nloss=[]\nlabels=[]\nsilh=[]\nkm=[]\nfor i in [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]:\n    cluster_id,losses,silhoutte,kmean=train_KMeans(data,i)\n    loss.append(np.min(losses))\n    km.append(kmean[np.argmax(silhoutte)])\n    labels.append(cluster_id[np.argmin(losses)])\n    silh.append(silhoutte)","6de20d3b":"sns.lineplot(x=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],y=loss);\nplt.title('SSE vs clusters ');\n# The elbow method dosent give a proper elbow but we can estimate 6-8 to be the better option ","e52eabb7":"sns.lineplot(x=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],y=np.mean(np.array(silh),axis=1));\nplt.title('silhouette_score vs clusters ');","9b26a28f":"st=MinMaxScaler()\ndt=st.fit_transform(data.iloc[:,1:].values)\npc=PCA(n_components=2)\ndf_2=pc.fit_transform(dt)\nplt.figure(figsize=(15,15))\nj=1\nfor i in range(4):\n    plt.subplot(4,2,j)\n    vis=SilhouetteVisualizer(km[i],colors = sns.color_palette('hls')[0:i+2],);\n    vis.fit(dt);\n    j+=1\n    plt.subplot(4,2,j)\n    sns.scatterplot(x=df_2[:,0],y=df_2[:,1],hue=labels[i],palette=sns.color_palette('hls')[0:i+2])\n    j+=1","fb789f07":"pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/data-dictionary.csv').style","2fc42666":"for i in range(3):\n    data['cluster']=labels[i]\n    print(data.groupby(['cluster']).mean().T)\n       ","87763a94":"# arranging categories with similar scales\na=[['child_mort','inflation', 'health','total_fer',], ['exports','imports',  'life_expec'],['income',  'gdpp']]\n\nfor i in range(3):\n    data['cluster']=labels[i]\n    print(f'\\n----------------k={i+2}-----------\\n')  \n    for j in a:\n       \n        data.groupby(['cluster']).mean()[j].T.plot(kind='bar')\n        plt.show()\n        \n          \n    ","799ac8ce":"data['cluster']=labels[1]\ndata[data['cluster']==1][['country']]","6355ed56":"##### for k=2, \n\n\nCluster 0 countries have a high child mortality rate of 87, compared to an average of 15 in the rest of the world. Cluster 0 countries also have much lower life expectancy ,income and GDP. These countries also have a tendency to import more and export less than cluster 1 countries.This means that these are the countries with the least developed industrial and manufacturing sectors, resulting in lower employment, increased poverty, and higher living costs.\n\n##### for k=3 \ncluster 1 tends to be the least developed with child mortality of 93(this is much higher than that of cluster 0 in k=2), also lower gdp,income and life expectancy. \n\n##### for k=4 \ncluster 1 tends to be the least developed but countries belongingto this cluster isexactly same asthat in cluster1 ofmodelwith k=3\n","818cc70b":"models 2,3 and 4 clusters shows good average Silhouette score and the individual clusters have a score above average value. Next we will look at countries belonging to these clusters and try to draw some inference. cluster 1 ofk =3 and k=4 are same","4542f53d":"# Silhouette Analysis","406231e0":"###### The CEO of the company should be focusing on countries belonging to cluster 1 of model with K=3","26f14f41":"# Elbow method"}}