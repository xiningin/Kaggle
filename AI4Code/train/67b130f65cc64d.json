{"cell_type":{"e0c206fb":"code","bbe4efaf":"code","4331268c":"code","afc10b2a":"code","9aa97075":"code","a0aa3b31":"code","30ecb6f0":"code","b515769f":"code","efda82c2":"markdown","591d618d":"markdown","4273d3f6":"markdown","13963e95":"markdown","46d83fd5":"markdown","07099af9":"markdown","2871c5e2":"markdown"},"source":{"e0c206fb":"import torchvision.transforms as transforms\nimport torch\nimport torch.utils.data as data\nimport numpy as np\nfrom sklearn import svm\nimport librosa\nimport csv\nimport time\n\nfrom customdatasets import TrainDataset, TestDataset\n\ntoFloat = transforms.Lambda(lambda x: x \/ np.iinfo(np.int16).max)\n\ntrainDataset = TrainDataset(\"..\/input\/oeawai\/train\/kaggle-train\", transform=toFloat)\nprint(len(trainDataset))\n\ntestDataset = TestDataset(\"..\/input\/oeawai\/kaggle-test\/kaggle-test\", transform=toFloat)\nprint(len(testDataset))","bbe4efaf":"familyClassifier = svm.SVC()\n\ntrainLoader = data.DataLoader(trainDataset, batch_size=640, shuffle=True)\nstart = time.time()\nfor samples, instrumentsFamily in trainLoader:\n    familyClassifier.fit(np.array(samples.data)[:, :16000], np.array(instrumentsFamily.data))\n    break # SVM is only fitted to a fixed size of data\nprint(\"Fitting the SVM took \" + str((time.time()-start)\/60) + \"mins\")","4331268c":"batch_size = 32\ntestloader = data.DataLoader(testDataset, batch_size=batch_size, shuffle=False) #!!! Shuffle should be false\n\nfamilyPredictions = np.zeros(len(testDataset), dtype=np.int)\nstart = time.time()\nfor index, samples in enumerate(testloader):\n    familyPredictions[index*batch_size:(index+1)*batch_size] = familyClassifier.predict(np.array(samples.data)[:, :16000])\nprint(\"Classifying took \" + str((time.time()-start)\/60) + \"mins\")","afc10b2a":"import csv\nfamilyPredictionStrings = trainDataset.transformInstrumentsFamilyToString(familyPredictions.astype(int))\n\nwith open('SVM-time-submission.csv', 'w', newline='') as writeFile:\n    fieldnames = ['Id', 'Predicted']\n    writer = csv.DictWriter(writeFile, fieldnames=fieldnames, delimiter=',',\n                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n    writer.writeheader()\n    for index in range(len(testDataset)):\n        writer.writerow({'Id': index, 'Predicted': familyPredictionStrings[index]})\n","9aa97075":"def computeMelspectrogram(numpyArray, sample_rate):\n    S = librosa.feature.melspectrogram(y=numpyArray, sr=sample_rate, n_mels=128, fmax=8000)\n    return np.log(S+1e-4)\nsample_rate = 16000\n\nbatch_size = 16\ntrainLoader = data.DataLoader(trainDataset, batch_size=batch_size, shuffle=True)\nmfccs = np.zeros((batch_size, 128))\nfor samples, instrumentsFamily in trainLoader:\n    for index, sample in enumerate(samples):\n        mfccs[index] = np.mean(computeMelspectrogram(sample.numpy(), sample_rate), axis=1)\n    family = trainDataset.transformInstrumentsFamilyToString(instrumentsFamily.numpy().astype(int))\n    break # SVM is only fitted to a fixed size of data\n\nimport matplotlib.pyplot as plt\n    \nfor i in range(batch_size):\n    plt.plot(mfccs[i])\n    print(family[i])\n    plt.show()","a0aa3b31":"informedFamilyClassifier = svm.SVC()\n\ntrainLoader = data.DataLoader(trainDataset, batch_size=6400, shuffle=True)\nstart = time.time()\nfor samples, instrumentsFamily in trainLoader:\n    mfccs = np.zeros((len(samples), 128))\n    for index, sample in enumerate(samples.data):\n        mfccs[index] = np.mean(computeMelspectrogram(sample.numpy(), sample_rate), axis=1)\n    informedFamilyClassifier.fit(mfccs, instrumentsFamily.numpy())\n    break # SVM is only fitted to a fixed size of data\n    \nprint(\"Fitting the SVM took \" + str((time.time()-start)\/60) + \"mins\")","30ecb6f0":"batch_size = 32\ntestloader = data.DataLoader(testDataset, batch_size=batch_size, shuffle=False) #!!! Shuffle should be false\n\ninformedFamilyPredictions = np.zeros(len(testDataset), dtype=np.int)\nstart = time.time()\nfor index, samples in enumerate(testloader):\n    mfccs = np.zeros((len(samples), 128))\n    for inner_index, sample in enumerate(samples.data):\n        mfccs[inner_index] = np.mean(computeMelspectrogram(sample.numpy(), sample_rate), axis=1)\n    informedFamilyPredictions[index*batch_size:(index+1)*batch_size] = informedFamilyClassifier.predict(mfccs)\n\nprint(\"Classifying took \" + str((time.time()-start)\/60) + \"mins\")","b515769f":"informedFamilyPredictionStrings = trainDataset.transformInstrumentsFamilyToString(informedFamilyPredictions.astype(int))\n\nwith open('SVM-informed-submission.csv', 'w', newline='') as writeFile:\n    fieldnames = ['Id', 'Predicted']\n    writer = csv.DictWriter(writeFile, fieldnames=fieldnames, delimiter=',',\n                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n    writer.writeheader()\n    for index in range(len(testDataset)):\n        writer.writerow({'Id': index, 'Predicted': informedFamilyPredictionStrings[index]})\n","efda82c2":"Now that the model is trained, let's measure how long it takes to classify our test dataset and let's see what performance we get.","591d618d":"This model is also benchmarked and it has a performance of 0.39 F-score. Quite good on 10 classes! And it's a pretty small model! ","4273d3f6":"# Example submission using support vector machines\n\nTo remove some headaches, we provide you with two complete approaches to solve the task. From loading the data to classifying the test data and generating submission files. For this, we use a support vector machine (SVM). \n<img src=\"https:\/\/en.proft.me\/media\/science\/svm_classification.png\" alt=\"SVM example\" width=\"30%\"\/>\n\nSVMs are supervised learning models used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier. Further, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Finally, SVMs are commonly adapted to the multiclass problem. \n\nSVMs are tempting for this classification task. The first problem that we encounter is that SVMs take each data point as a feature, which doesn't make them optimal to classify time-data that is not aligned. Anyhow let's see how they do directly on our sound signals.\n","13963e95":"The training and the testing both were considerably faster. This model could be easily be trained on more samples (which we have!)","46d83fd5":"So now that we have some features that make more sense, let's train a SVM on these. Notice that now we are using the full duration of the signals, but still reduced them from 64k samples to 128 features.","07099af9":"Here, we are going to use a very small trick. We know that most of our signals don't actually last four seconds, and that the complexity of SVMs is determined by the number of features and the number of training examples. So let's reduce the number of features by only using the first second of the signals to classify. Let's also keep a tab of the time it takes to train this model for future reference.","2871c5e2":"This submission is benchmarked in the kaggle competition, it obtains a mean 0.25 F-score, which is pretty good for 10 classes with this little effort. The only 'red light' was the training time, considering that we used a very small portion of the training data. \n\nSo let's try to build a SVM machine with few features that make sense. A way to do this is to measure the amount of energy in different frequency bands for all the signals. The assumption is that instruments from different families will have their energy distributed differently accross frequencies."}}