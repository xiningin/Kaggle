{"cell_type":{"6eabb72b":"code","1eb48d09":"code","b9d27ec5":"code","e541c1e7":"code","b2ba5428":"code","c39568f6":"code","edfb5f16":"code","46863311":"code","a73b170d":"code","3aaa50c7":"code","b0a222e2":"code","64b797d6":"code","4a526d6e":"code","5b3ad3a2":"code","c50219c7":"code","0a07699a":"code","dc435c2f":"code","3db0f71d":"code","ab90a845":"code","a9da034e":"code","bfd21589":"code","54389353":"code","e6e18d26":"code","f78714e6":"code","d93750e7":"code","0cbcb800":"code","450b2346":"code","2f67db31":"code","86cea3d4":"code","5123ef0c":"code","ca60834a":"code","17e2a1f0":"code","14a5de78":"code","92489894":"code","8c57cef2":"code","754c9f5d":"code","bd7b3373":"code","a539f2ce":"code","b73fb4ee":"code","69114736":"code","9b892566":"code","1d1c4e09":"code","10183c06":"code","30ca662a":"code","682f4574":"code","4ca2d1f3":"code","2a5e70f0":"code","899f0ff8":"code","38e3cd3d":"code","8354c9d6":"code","05d1038b":"code","56b8b7ac":"code","8ee4b142":"code","c9466600":"code","3d913288":"code","9503f6a7":"code","ba0491b4":"code","2ab58840":"code","9d28c356":"code","9d4d49c1":"code","a0d0a63d":"code","d3bb86bc":"code","522f24c2":"code","207138bc":"code","07008e21":"code","ba8541b7":"code","9adf0b35":"code","326c9932":"code","90a0d423":"code","f18c1a18":"markdown","b52bd483":"markdown","b197db87":"markdown","c89b5a66":"markdown","da58e4d1":"markdown","5d6fc0d1":"markdown","f1b80138":"markdown","34bbca78":"markdown","ade6fb4e":"markdown","72013dcd":"markdown","05148d5f":"markdown","8a90f7ef":"markdown","4dcfa8d5":"markdown"},"source":{"6eabb72b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n","1eb48d09":"# Loading the Train and Test into Pandas DataFrames. After pre-processing it can be converted to H2OFrame.\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","b9d27ec5":"# Look at the initial rows of train data\ntrain.head(5)","e541c1e7":"#Look at train data statistics\ntrain.describe()","b2ba5428":"# Check the number of null values in each column of train DataFrame\ntrain.isnull().sum()","c39568f6":"#Plot the number of null values in every column.\nplt.figure(figsize=[10, 5])\nsns.heatmap(train.isnull())","edfb5f16":"# Look at the initial rows of test data\ntest.head(5)","46863311":"#Look at test data statistics\ntest.describe()","a73b170d":"# Check the number of null values in each column of test DataFrame\ntest.isnull().sum()","3aaa50c7":"#Plot the number of null values in every column.\nplt.figure(figsize=[10, 5])\nsns.heatmap(test.isnull())","b0a222e2":"# Lets see how much percentage of people survived\nprint(train.Survived.value_counts()*100\/train.shape[0])\n# Here, we can see that only 38.38 percent people survived","64b797d6":"# count plot for Survived column\nplt.figure(figsize=[10, 5])\nsns.countplot(x='Survived',data=train)\n# Below graph shows that number of survivers are less compared to non-survivers.","4a526d6e":"# Lets see how many male and female Survived  \nplt.figure(figsize=[10, 5])\nsns.countplot(x='Survived',data=train, hue='Sex')\n# Below graph shows that more number of females survived than males","5b3ad3a2":"# Lets see how many survivers are there from each class \nplt.figure(figsize=[10, 5])\nsns.countplot(x='Survived',data=train, hue='Pclass')\n# Below graph shows that maximum number of survivers were from class 1 and  \n# the maximum casualties were from class 3","c50219c7":"plt.figure(figsize=[10, 6])\nsns.barplot(x='Pclass', y='Survived', hue='Sex', data=train)\nplt.show()\n# Below figure describes that among those who survived, number of female surviors are more in\n# each of the Pclass. ","0a07699a":"from sklearn.impute import SimpleImputer\nimp_median = SimpleImputer(missing_values=np.nan, strategy='median')","dc435c2f":"train['Age'] = imp_median.fit_transform(train['Age'].values.reshape(-1,1))","3db0f71d":"train.isnull().sum()","ab90a845":"# Dropping Cabin column as there are lot of missing values\ntrain.drop('Cabin',axis=1,inplace=True)","a9da034e":"train.isnull().sum()","bfd21589":"train = train.dropna()","54389353":"train.isnull().sum()","e6e18d26":"plt.figure(figsize=[10, 5])\nsns.heatmap(train.isnull())\n# There are no null values left in the train DataFrame.","f78714e6":"# Dropping the Columns PassengerId,Name,Ticket from the train DataFrame as they are specific to each passenger\n# and do not add any value to our analysis.\ntrain = train.drop(['PassengerId','Name','Ticket'], axis=1)","d93750e7":"train.columns","0cbcb800":"# Putting feature variable to X\nX_train = train.drop('Survived',axis=1)\n# Putting response variable to y\ny_train = train['Survived']","450b2346":"test.isnull().sum()","2f67db31":"test.head()","86cea3d4":"# Imputing the missing age value with the median value\ntest['Age'] = imp_median.fit_transform(test['Age'].values.reshape(-1,1))","5123ef0c":"test.shape","ca60834a":"test.isnull().sum()","17e2a1f0":"# Dropping the Cabin column as it contain large number of missing values.\ntest.drop('Cabin',axis=1,inplace=True)","14a5de78":"test.isnull().sum()","92489894":"# Check if there is any null value left in any column of test DataFrame\nplt.figure(figsize=[10, 5])\nsns.heatmap(test.isnull())","8c57cef2":"test['Fare'].median()","754c9f5d":"test['Fare'].fillna(value=15,inplace=True)","bd7b3373":"# Check if there is any null value left in any column of test DataFrame\nplt.figure(figsize=[10, 5])\nsns.heatmap(test.isnull())","a539f2ce":"# Putting feature variable to X\nX_test = test.drop(['PassengerId', 'Name','Ticket'], axis=1)","b73fb4ee":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","69114736":"train['Sex'].dtype","9b892566":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX_train[\"Sex\"] = le.fit_transform(X_train[\"Sex\"])\nX_train[\"Embarked\"] = le.fit_transform(X_train[\"Embarked\"])\nX_test[\"Sex\"] = le.fit_transform(X_test[\"Sex\"])\nX_test[\"Embarked\"] = le.fit_transform(X_test[\"Embarked\"])","1d1c4e09":"X_train.columns","10183c06":"X_train['Embarked'].dtype","30ca662a":"X_train.head(5)","682f4574":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc = RandomForestClassifier()","4ca2d1f3":"# fit\nrfc.fit(X_train,y_train)","2a5e70f0":"y_predict_default = rfc.predict(X_test)","899f0ff8":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 20, 2)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","38e3cd3d":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","8354c9d6":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"accuracy\")\nplt.legend()\nplt.show()\n","05d1038b":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(100, 1500, 400)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=4)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","56b8b7ac":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","8ee4b142":"# plotting accuracies with n_estimators\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","c9466600":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': range(2, 8, 1)}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=4)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","3d913288":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","9503f6a7":"# plotting accuracies with max_features\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","ba0491b4":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(2, 20, 1)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","2ab58840":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","9d28c356":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","9d4d49c1":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(2, 100, 2)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, y_train)","a0d0a63d":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","d3bb86bc":"# plotting accuracies with min_samples_split\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","522f24c2":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,6,8,10],\n    'min_samples_leaf': [13, 14, 15],\n    'min_samples_split': [38,39,40,41,42,43],\n    'n_estimators': [400,500, 600], \n    'max_features': [4, 5, 6]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","207138bc":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","07008e21":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","ba8541b7":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=4,\n                             min_samples_leaf=13, \n                             min_samples_split=43,\n                             max_features=6,\n                             n_estimators=400)","9adf0b35":"# fit\nrfc.fit(X_train,y_train)","326c9932":"Y_predict = rfc.predict(X_test)","90a0d423":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_predict\n    })\nsubmission.to_csv('titanic.csv', index=False)","f18c1a18":"#### Tuning min_samples_leaf","b52bd483":"#### Null Values Count in train DataFrame:\n      Age column has 177 null values\n      Cabin column has 687 null values\n      Embarked column has 2 null values","b197db87":"#### Tuning min_samples_split\n      Let's now look at the performance of the ensemble as we vary min_samples_split.","c89b5a66":"#### Default Hyperparameters\n     Let's first fit a random forest model with default hyperparameters.","da58e4d1":"Grid Search to Find Optimal Hyperparameters","5d6fc0d1":"### Tuning max_features\n     Let's see how the model performance varies with ```max_features```, which is the maximum numbre of features\n     considered for splitting at a node.","f1b80138":"Exploratory Data Analysis:","34bbca78":"#### Tuning n_estimators\n     Let's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts \n     the overall accuracy. Notice that we'll specify an appropriately low value of max_depth, so that the trees\n     do not overfit.","ade6fb4e":"There is no need to create dummy variables for Pclass, Sex & Embarked columns as it done internally by H2O**","72013dcd":"#### Null Values count in test DataFrame\n      Age column has 86 null values\n      Fare column has 1 null values\n      Cabin column has 327 null values","05148d5f":"### Fitting the final model with the best parameters obtained from grid search.","8a90f7ef":" ### Performing EDA on test DataFrame","4dcfa8d5":"From above result, it can be inferred that Age column has 86 null values and Cabin column has 327 values."}}