{"cell_type":{"1df4ffb4":"code","103d8715":"code","4ce9f3bf":"code","2227e189":"code","61623c97":"code","98e44126":"code","b0ffc5af":"code","fe8a672b":"code","c88f7b6a":"code","9060fbea":"code","9127e08f":"code","05501d0a":"code","91fc980f":"code","9fa05153":"code","87069938":"code","b35e3e11":"code","fc4a1f3a":"code","fb6415f5":"code","55cc65f1":"code","e9cb7232":"code","8c672504":"code","2e755516":"code","3832f52d":"code","80e3e576":"code","81ab6261":"code","3bf4c4a2":"code","d00f8fe8":"code","8127f391":"code","ec7f8f96":"code","1cdfdca7":"code","500a2660":"code","f78bde1b":"code","0927b59d":"code","52af0e4c":"code","68e97da7":"code","9f28842c":"code","3486fcbb":"code","9cb0a089":"code","20498240":"code","736fcdae":"code","e0c268f6":"code","3b6a7b99":"code","ef572d25":"code","ae8b20dc":"code","bc9dcaef":"code","a6349097":"code","64f7a7b5":"code","171a12f0":"code","50eca8a9":"code","4b23ef51":"code","d5495e01":"code","5993f2ef":"code","e3c291bb":"code","6b194cfc":"markdown","6c50efba":"markdown","5a384cb8":"markdown","946de486":"markdown","bb41fbbd":"markdown","54997257":"markdown","b491ff35":"markdown","86980148":"markdown","c72e1a0b":"markdown","96cf6724":"markdown","3d771e74":"markdown","3fcaf663":"markdown","e2959946":"markdown","3bee87fc":"markdown","0c1100b7":"markdown","8fa44db9":"markdown","3659b59e":"markdown","925c2501":"markdown","43356783":"markdown","8addc396":"markdown","c21f2e85":"markdown","db8e0ede":"markdown","2bed557d":"markdown","c14ce57b":"markdown","17473402":"markdown","025efdeb":"markdown","1b7c0100":"markdown","f818292b":"markdown","a230f350":"markdown","7f898f8c":"markdown"},"source":{"1df4ffb4":"import warnings\n\nwarnings.filterwarnings('ignore', 'SettingWithCopyWarning')\nwarnings.filterwarnings('ignore', 'UndefinedMetricWarning')\nwarnings.filterwarnings('ignore', 'ConvergenceWarning')","103d8715":"import os\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display, Markdown, Latex","4ce9f3bf":"# matplotlib\nplt.rc('font', size=15)\nplt.rc('axes', titlesize=18)  \nplt.rc('xtick', labelsize=10)  \nplt.rc('ytick', labelsize=10)\n\n# seaborn\nsns.set(font_scale = 1.2)\nsns.set_style(\"whitegrid\")","2227e189":"class Cfg:\n    RANDOM_STATE = 2021\n    TRAIN_DATA = '..\/input\/tabular-playground-series-dec-2021\/train.csv'\n    TEST_DATA = '..\/input\/tabular-playground-series-dec-2021\/test.csv'\n    SUBMISSION = '..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv'    \n    SUBMISSION_FILE = 'submission.csv'\n    TEST_SIZE = 0.6\n    SAMPLE_FRAC = 0.3\n    INDEX = 'Id'\n    TARGET = 'Cover_Type'\n    \n    @staticmethod\n    def set_seed():\n        random.seed(Cfg.RANDOM_STATE)\n        np.random.seed(Cfg.RANDOM_STATE)\n\nCfg.set_seed()","61623c97":"def read_data(\n    train_file:str=Cfg.TRAIN_DATA, \n    test_file:str=Cfg.TEST_DATA\n) -> (pd.DataFrame, pd.DataFrame):\n    \"\"\"Reads the train and test data files\n    \"\"\"\n    # read csv files\n    train_df = pd.read_csv(train_file).set_index(Cfg.INDEX).astype(np.int32)\n    test_df = pd.read_csv(test_file).set_index(Cfg.INDEX).astype(np.int32)\n    \n    return train_df, test_df","98e44126":"%%time\ntrain_data, test_data = read_data()","b0ffc5af":"train_data","fe8a672b":"test_data","c88f7b6a":"print('Train data: {} rows'.format(len(train_data)))\nprint('Test data: {} rows'.format(len(test_data)))\n\nprint('Train data: {} columns'.format(len(train_data.columns)))","9060fbea":"Cfg.NUM_FEATURES = [\n    'Elevation', \n    'Aspect', \n    'Slope', \n    'Horizontal_Distance_To_Hydrology',\n    'Vertical_Distance_To_Hydrology',\n    'Horizontal_Distance_To_Roadways', \n    'Hillshade_9am',\n    'Hillshade_Noon', \n    'Hillshade_3pm',\n    'Horizontal_Distance_To_Fire_Points', \n]\n    \nCfg.BINARY_FEATURES = [\n    'Wilderness_Area1',\n    'Wilderness_Area2', \n    'Wilderness_Area3', \n    'Wilderness_Area4',\n    'Soil_Type1', \n    'Soil_Type2', \n    'Soil_Type3', \n    'Soil_Type4',\n    'Soil_Type5', \n    'Soil_Type6', \n    'Soil_Type7', \n    'Soil_Type8',\n    'Soil_Type9', \n    'Soil_Type10', \n    'Soil_Type11', \n    'Soil_Type12',\n    'Soil_Type13', \n    'Soil_Type14', \n    'Soil_Type15', \n    'Soil_Type16',\n    'Soil_Type17', \n    'Soil_Type18', \n    'Soil_Type19', \n    'Soil_Type20',\n    'Soil_Type21', \n    'Soil_Type22', \n    'Soil_Type23', \n    'Soil_Type24',\n    'Soil_Type25', \n    'Soil_Type26', \n    'Soil_Type27', \n    'Soil_Type28',\n    'Soil_Type29', \n    'Soil_Type30', \n    'Soil_Type31', \n    'Soil_Type32',\n    'Soil_Type33', \n    'Soil_Type34', \n    'Soil_Type35', \n    'Soil_Type36',\n    'Soil_Type37', \n    'Soil_Type38', \n    'Soil_Type39', \n    'Soil_Type40'\n]\n\nCfg.FEATURES = Cfg.NUM_FEATURES + Cfg.BINARY_FEATURES","9127e08f":"print(f'Numerical Features: {len(Cfg.NUM_FEATURES)}')\nprint(f'Categorical Features: {len(Cfg.BINARY_FEATURES)}')","05501d0a":"pd.DataFrame({\n    'data_set': ['train', 'test'],\n    'missing_values': [\n        train_data.isna().sum().sum(), \n        test_data.isna().sum().sum()\n    ]\n}).set_index('data_set')","91fc980f":"def get_sample_data(\n    data,\n    split_target=True,\n    features=Cfg.FEATURES,\n    target=Cfg.TARGET,\n    frac=Cfg.SAMPLE_FRAC, \n    random_state=Cfg.RANDOM_STATE):\n    \"\"\"Select a sample subset from data\n    \"\"\"\n    idx = data.sample(frac=frac, random_state=random_state).index\n\n    if split_target:\n        X_data = data.iloc[idx][features]\n        y_data = data.iloc[idx][target]\n    \n        return X_data, y_data\n    \n    return train_data.iloc[idx]","9fa05153":"def plot_count(\n    data:pd.DataFrame, \n    feature:str, \n    title='Countplot',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n    \n    sns.countplot(\n        data=train_data,\n        x=feature,\n        palette='Blues_r',\n        ax=ax)\n    \n    ax.set_title(title)\n\n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n\n    return ax","87069938":"fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nplot_count(train_data, Cfg.TARGET, title='Target countplot', ax=ax)\nplt.show()","b35e3e11":"train_data[[Cfg.TARGET]].reset_index().groupby(by='Cover_Type').count()","fc4a1f3a":"def plot_pdf(\n    data:pd.DataFrame, \n    feature:str, \n    title='Histplot',\n    bins=70,\n    ax=None):\n    \"\"\" Plots the estimated pdf. \n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    # plot pdf\n    sns.histplot(\n        data=data[[feature, Cfg.TARGET]],\n        x=feature,\n        hue=Cfg.TARGET,\n        bins=bins,\n        palette='Blues_r',\n        legend=True,\n        kde=False,\n        ax=ax\n    )\n    mean = np.mean(data[feature])\n    ax.vlines(\n        mean, 0, 1, \n        transform=ax.get_xaxis_transform(), \n        color='red', ls=':')\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Feature {}'.format(feature))\n    ax.set_ylabel('Count')\n    \n    return ax","fb6415f5":"def plot_boxplot(\n    data:pd.DataFrame, \n    feature:str, \n    title='Boxplot',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    ax = sns.boxplot(\n        x=Cfg.TARGET, \n        y=feature,\n        palette='Blues_r',\n        data=data\n    )\n    \n    ax.set_title(title)\n    \n    ax.set_xlabel('Target {}'.format(Cfg.TARGET))\n    ax.set_ylabel('Feature {}'.format(feature))\n    \n    return ax","55cc65f1":"stat_data = train_data[Cfg.NUM_FEATURES].describe().drop('count')\nstat_data.loc['var'] = stat_data.T['std']**2\n\nstat_data.T.style.bar(\n    subset=['mean'], \n    color='Bules'\n).background_gradient(subset=['50%'], cmap='Blues')","e9cb7232":"X_data = get_sample_data(train_data, split_target=False)\nfor feature in Cfg.NUM_FEATURES:\n    display(Markdown('### Feature `{}`'.format(feature)))\n \n    info = np.round(train_data[feature].describe(), 4)\n    \n    format_str = '* mean: {}\\n* std: {}\\n* min: {}\\n* 25%: {}' \\\n        + '\\n* 50%: {}\\n* 75%: {}\\n* max: {}'\n        \n    display(Markdown(format_str.format(info['mean'], info['std'], \n        info['min'], info['25%'], info['50%'], info['75%'], info['max'])))\n    \n    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n\n    plot_pdf(X_data, feature, ax=ax[0])\n    plot_boxplot(X_data, feature, ax=ax[1])\n    \n    plt.show()","8c672504":"data = train_data[Cfg.NUM_FEATURES + [Cfg.TARGET]].sample(frac=0.001) \ngrid = sns.pairplot(\n    data, \n    hue=Cfg.TARGET,\n    palette='Blues_r',\n    corner=True)\n\nfor ax in grid.axes.flatten():\n    if ax is not None:\n        ax.set_xlabel(ax.get_xlabel(), rotation=45)\n        ax.set_ylabel('')\n\nplt.tight_layout()\nplt.show()","2e755516":"corr_matrix = train_data[Cfg.NUM_FEATURES].corr()\n\nplt.figure(figsize = (15, 15))\nsns.heatmap(\n    corr_matrix, \n    annot = True, \n    cmap = 'Blues_r', \n    mask = np.triu(corr_matrix), \n    linewidths = 0.1, \n    linecolor = 'white',\n    cbar = True\n)\n\nplt.tight_layout()\nplt.show()","3832f52d":"def plot_stacked_bar(\n    data:pd.DataFrame, \n    feature:str, \n    title='Feature by Target',\n    ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n\n    pd.crosstab(\n        index=data[Cfg.TARGET], \n        columns=data[feature]\n    ).plot(\n        kind=\"bar\", \n        color=['steelblue', 'darkblue'],\n        stacked=True, ax=ax)\n\n    plt.xticks(rotation=0)\n    plt.show()\n\n    ax.set_title(title)\n    return ax","80e3e576":"train_data[Cfg.BINARY_FEATURES].astype(object).describe().drop('count').T","81ab6261":"for feature in Cfg.BINARY_FEATURES:\n    display(Markdown('### Feature `{}`'.format(feature)))\n    \n    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n    plot_stacked_bar(train_data, feature, ax=ax)\n\n    plt.show()","3bf4c4a2":"def plot_feature_importances(feature_imp, feature_names, num=20, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1)\n    \n    df = pd.DataFrame({\n        'feature': feature_names,\n        'value': feature_imp\n    }).sort_values('value', ascending=False).head(num)\n    \n    sns.barplot(\n        x='value', \n        y='feature', \n        palette='Blues_r',\n        data=df,\n        ax=ax\n    ) \n\n    ax.set_title(\"Importance of each feature\")\n    ax.set_xlabel(\"Score\")\n    ax.set_ylabel(\"Features\")\n\n    return ax","d00f8fe8":"from sklearn.ensemble import RandomForestClassifier\n\nX_data, y_data = get_sample_data(train_data, frac=0.01)\nrf = RandomForestClassifier(random_state=Cfg.RANDOM_STATE)\n\nrf.fit(X_data, y_data)\nfeature_imp = rf.feature_importances_","8127f391":"fig, ax = plt.subplots(figsize=(15, 10))\nplot_feature_importances(feature_imp, Cfg.FEATURES, num=30, ax=ax)\n\nfig.tight_layout()\nplt.show()","ec7f8f96":"feature_importance_score = pd.DataFrame({\n    'feature': Cfg.FEATURES,\n    'score': feature_imp\n}).sort_values(by='score', ascending=False).set_index('feature')\n\nfeature_importance_score.head(15)","1cdfdca7":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import confusion_matrix","500a2660":"def split_data(data, frac=Cfg.SAMPLE_FRAC):\n    X_data, y_data = get_sample_data(data, frac=frac)\n\n    # spit data into train and validation data sets\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        X_data,\n        y_data,\n        test_size=Cfg.TEST_SIZE, \n        random_state=Cfg.RANDOM_STATE\n    )\n    return X_train, X_valid, y_train, y_valid","f78bde1b":"from sklearn import metrics\n\ndef plot_confusion_matrix(y_true, y_pred, ax=None):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n\n    labels = range(1, 8)\n    cm = np.round(confusion_matrix(\n        y_true, \n        y_pred, \n        labels=labels,\n        normalize='true'), 2)\n    \n    sns.heatmap(\n        cm, \n        cmap=plt.cm.Blues,\n        annot=True,\n        xticklabels=labels,\n        yticklabels=labels,\n        ax=ax)","0927b59d":"def plot_model_proba(proba, ax=None):\n    \"\"\"\n    \"\"\"\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n\n    sns.histplot(\n        data=proba,\n        palette='Blues_r',\n        stat='probability',\n        legend=True,\n        bins=100,\n        kde=False,\n        ax=ax\n    )\n\n    ax.set_xlabel('Prediction probapility')\n    ax.set_ylabel('Probabitity')","52af0e4c":"def plot_result(y_true, y_pred, y_proba):\n    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n\n    plot_model_proba(y_proba, ax=ax[0])\n    plot_confusion_matrix(y_true, y_pred, ax=ax[1])\n\n    plt.tight_layout()\n    plt.show()    ","68e97da7":"# target encoder\ntarget_encoder = LabelEncoder()\ntarget_encoder.fit(data[Cfg.TARGET])\n\ntarget_encoder.classes_","9f28842c":"def model_result(model, data, target_encoder=target_encoder, frac=Cfg.SAMPLE_FRAC):\n    \"\"\"\n    \"\"\"\n    # split data\n    X_train, X_valid, y_train, y_valid = split_data(data, frac=frac)\n    \n    # train model\n    model.fit(X_train, target_encoder.transform(y_train))\n\n    # make predictions\n    y_pred = target_encoder.inverse_transform(model.predict(X_valid))\n    y_proba = model.predict_proba(X_valid)\n    \n    # display results\n    plot_result(y_valid, y_pred, y_proba)\n    \n    print(classification_report(y_valid, y_pred))\n    \n    # display data size\n    print(f'train size: {X_train.shape[0]} rows')\n    print(f'valid size: {X_valid.shape[0]} rows')","3486fcbb":"def create_preprocessor():\n    num_transformer = make_pipeline(\n        StandardScaler()\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', num_transformer, Cfg.NUM_FEATURES),\n        ], remainder='passthrough')\n    \n    return preprocessor","9cb0a089":"import lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier","20498240":"%%time\n\nmodel = LogisticRegression(solver='saga')\nlog_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(log_model, train_data)","736fcdae":"%%time\n\nmodel = LinearDiscriminantAnalysis(solver='lsqr')\nlda_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(lda_model, train_data)","e0c268f6":"%%time\n\nmodel = DecisionTreeClassifier(max_depth=20)\ndt_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(dt_model, train_data)","3b6a7b99":"%%time\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5)\nrf_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(rf_model, train_data)","ef572d25":"%%time\n\nmodel = AdaBoostClassifier(n_estimators=100)\nada_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(ada_model, train_data)","ae8b20dc":"%%time\n\nmodel = SGDClassifier(loss='log')\nsgd_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(sgd_model, train_data)","bc9dcaef":"%%time\n\nmodel = XGBClassifier(\n    n_estimators=100,\n    eval_metric='rmse',\n    random_state=Cfg.RANDOM_STATE)\n\nxgb_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(xgb_model, train_data)","a6349097":"%%time\n\nmodel = lgb.LGBMClassifier(\n    learning_rate=0.05,\n    n_estimators=100,\n    reg_lambda = 1)\n\nlgbm_model = make_pipeline(\n    create_preprocessor(),\n    model\n)\n\nmodel_result(lgbm_model, train_data)","64f7a7b5":"estimators = [\n    ('dt',  DecisionTreeClassifier(max_depth=10)),\n    ('log', LogisticRegression(solver='saga')),\n    ('lda', LinearDiscriminantAnalysis(solver='lsqr')),\n    ('rf', RandomForestClassifier(n_estimators=100, max_depth=5)),\n    ('ada', AdaBoostClassifier(n_estimators=100)),\n    ('sgd', SGDClassifier(loss='log')),\n    ('lgbm', lgb.LGBMClassifier(\n        learning_rate=0.05,\n        n_estimators=150,\n        reg_lambda = 1)),\n    ('xgb', XGBClassifier(\n        n_estimators=100,\n        eval_metric='rmse'))\n]\n\nweights = [2, 1, 1, 2, 1, 1, 4, 5]","171a12f0":"%%time\n\nmodel = StackingClassifier(\n    estimators=estimators, \n    final_estimator=LogisticRegression(solver='saga'),\n    cv=3,\n    n_jobs=-1,\n    stack_method='predict_proba',\n    verbose=0)\n\nstacking_model = make_pipeline(\n    create_preprocessor(),\n    model)\n\nmodel_result(stacking_model, train_data, frac=0.1)","50eca8a9":"%%time\n\nmodel = VotingClassifier(\n    estimators=estimators, \n    voting='soft',\n    n_jobs=-1,\n    weights=weights)\n\nvoting_model = make_pipeline(\n    create_preprocessor(),\n    model)\n\nmodel_result(voting_model, train_data, frac=0.1)","4b23ef51":"models = {\n    'log': log_model,\n    'lda': lda_model,\n    'dt': dt_model,\n    'rf': rf_model,\n    'ada': ada_model,\n    'sgd': sgd_model,\n    'xgb': xgb_model,\n    'lgbm': lgbm_model, \n    'stack': stacking_model,\n    'voting': voting_model\n}\n\nX_train, X_valid, y_train, y_valid = split_data(train_data, frac=0.2)\n\nscores = []\nfor (k, m) in models.items():\n    y_pred = target_encoder.inverse_transform(m.predict(X_valid))\n    score = accuracy_score(y_pred, y_valid)\n    scores.append(score)\n\n\nmodel_result = pd.DataFrame({\n    'model': [k for (k, m) in models.items()],\n    'accuracy': scores\n}).sort_values(by='accuracy', ascending=False).set_index('model') \n\nmodel_result","d5495e01":"idx = model_result['accuracy'].argmax()\nmodel_name = model_result.iloc[idx].name\nbest_model = models[model_name]\n\nprint(f'Best model: {model_name}')","5993f2ef":"y_pred_submission = target_encoder.inverse_transform(best_model.predict(test_data))\n\nsubmission_data = pd.DataFrame({\n    Cfg.INDEX: test_data.index,\n    Cfg.TARGET: y_pred_submission,\n}).set_index(Cfg.INDEX)\n\nsubmission_data","e3c291bb":"# save submission file\nsubmission_data.to_csv(Cfg.SUBMISSION_FILE)","6b194cfc":"# Read data","6c50efba":"<h4>If you find this notebook useful, support with an upvote.<\/h4>","5a384cb8":"### Notice\n\n* There are no missing values in both data sets.","946de486":"## Target variable","bb41fbbd":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/28009\/logos\/header.png?)","54997257":"## VotingClassifier","b491ff35":"### Files\n\n\n* `train.csv` - the training data with the target ``Cover_Type` column\n* `test.csv` - the test set; you will be predicting the `Cover_Type` for each row in this file (the target integer class)\n* `sample_submission.csv` - a sample submission file in the correct format\n\n","86980148":"# Setup","c72e1a0b":"# Feature importance ","96cf6724":"### XGB","3d771e74":"### Result","3fcaf663":"## Numerical features","e2959946":"### Notes\n\n* The target `Cover_Type` has seven different classes.","3bee87fc":"### AdaBoost","0c1100b7":"## LogisticRegression","8fa44db9":"## LGBMClassifier","3659b59e":"## RandomForest","925c2501":"## Features","43356783":"# Modeling\n","8addc396":"### Notice\n\n* The training data contains 4,000,000 rows.\n\n* The test data contains 1,000,000 rows.\n\n* There are 54 features\n\n    * 10 numerical features\n    * 44 categorical features (All binary - 1\/0).\n\n\n* The target `Cover_Type` is a multi-label variable.","c21f2e85":"# Missing values","db8e0ede":"# Overview\n\n\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. You are asked to predict an integer classification for the forest cover type. The seven types are:\n\n* 1 - Spruce\/Fir\n* 2 - Lodgepole Pine\n* 3 - Ponderosa Pine\n* 4 - Cottonwood\/Willow\n* 5 - Aspen\n* 6 - Douglas-fir\n* 7 - Krummholz\n\nSee: https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data","2bed557d":"## Binary features","c14ce57b":"### Note\n\n* The two features `Soil_Type15` and `Soil_Type7` each have only a single value and can be removed without loss of information.","17473402":"## StackingClassifier","025efdeb":"## DecisionTreeClassifier","1b7c0100":"## LinearDiscriminant","f818292b":"# Exploratory data analysis (EDA)","a230f350":"## SGDClassifier","7f898f8c":"# Submission"}}