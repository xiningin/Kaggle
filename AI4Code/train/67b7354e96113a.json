{"cell_type":{"600a1dcd":"code","e925130d":"code","4096cd38":"code","9dfb0c2e":"code","adcd5ce2":"code","b37edeba":"code","f0e7e701":"code","5ddd0237":"code","493640ef":"code","336f2c49":"code","dfe51d0a":"code","3c3e4a37":"code","fc7a8c5a":"code","e5e7dc74":"code","4fa958e3":"code","9879f226":"code","5a46dc16":"code","14ad7cb0":"code","452281bb":"code","1cbab9c9":"code","6cf0c1db":"code","f992c00c":"code","236d30e0":"code","785091f5":"code","4d404578":"code","6ff097d3":"code","069dc05d":"code","c930fc4f":"code","1bfe013b":"code","ee43ee1d":"code","ac285712":"code","adc844ee":"code","a9b7b0a3":"code","a423a43d":"code","94b4e53c":"code","5e9c7ef5":"code","911313ce":"code","efc64ecb":"code","d29c4c00":"code","279d2c61":"code","69099d17":"code","7b2a857b":"code","671f02f7":"code","e71a2936":"markdown","3b36cd4f":"markdown","c3cf9b95":"markdown","0aaf9407":"markdown","476bd5af":"markdown","cfb4da42":"markdown","58da914b":"markdown","51ce6161":"markdown","f2a261cc":"markdown","5eb3253e":"markdown","b72f9725":"markdown","2b5b309f":"markdown","527d1c89":"markdown","f63c17d7":"markdown","98b35953":"markdown","2b9331b3":"markdown","cb59890d":"markdown","f7bfed7d":"markdown","3db12075":"markdown","4321274f":"markdown","7ab3ddec":"markdown","4e660bc5":"markdown","a91f32f9":"markdown","af4ead42":"markdown","b181807f":"markdown","d336c3c9":"markdown","2675dae2":"markdown","3c7935aa":"markdown","f6f8f4ec":"markdown","946cd17c":"markdown","08ddd279":"markdown","0f60279e":"markdown"},"source":{"600a1dcd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno  #visualize the missing values from the data\nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt #visualization","e925130d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4096cd38":"#Loading the train and test datasets\ntrain_df=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","9dfb0c2e":"train_df.head()","adcd5ce2":"train_df.isnull().sum()","b37edeba":"isna_train = train_df.isnull().sum().sort_values(ascending=False)\nisna_test = test_df.isnull().sum().sort_values(ascending=False)\nplt.subplot(2,1,1)\nplt_1=isna_train.plot(kind='bar')\nplt.ylabel('Train Data')\nplt.subplot(2,1,2)\nisna_test.plot(kind='bar')\nplt.ylabel('Test Data')\nplt.xlabel('Number of features which are NaNs')","f0e7e701":"plt.pie(train_df.Survived.groupby(train_df.Sex).sum(), explode=(0,0.1), labels=[0,1], colors=['green', 'red'],\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","5ddd0237":"#Distribution of Survival Age-wise\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","493640ef":"#Survival of people on gender\ngrid = sns.FacetGrid(train_df, col='Survived', row='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","336f2c49":"#Survival of people on gender\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","dfe51d0a":"Title_train=[]\nfor name in train_df.Name:\n    Title_train.append(name.split('.')[0].split(',')[1])\n    \nTitle_test=[]\nfor name in test_df.Name:\n    Title_test.append(name.split('.')[0].split(',')[1])","3c3e4a37":"train_df['Title']=Title_train\ntest_df['Title']=Title_test\n\nTitle=list(set(Title_train))\nfor title in Title:\n    train_df.loc[train_df[\"Title\"]==title,'Age']=train_df.loc[train_df[\"Title\"]==title,'Age'].fillna(train_df.loc[train_df[\"Title\"]==title,'Age'].median())\nTitle=list(set(Title_test))\nfor title in Title:\n    test_df.loc[test_df[\"Title\"]==title,'Age']=test_df.loc[test_df[\"Title\"]==title,'Age'].fillna(test_df.loc[test_df[\"Title\"]==title,'Age'].median())","fc7a8c5a":"test_df['Fare']=test_df.groupby('Title')['Fare'].transform(lambda x: x.fillna(x.median()))","e5e7dc74":"train_df.head()","4fa958e3":"pd.crosstab([train_df.Embarked,train_df.Pclass],[train_df.Sex,train_df.Survived],margins=True).style.background_gradient(cmap='winter_r')","9879f226":"#Embarked in trian has two missing values we'll drop those rows\ntrain_df['Embarked']=train_df.groupby(['Age','Sex'])['Embarked'].transform(lambda x: x.fillna(x.mode()))\ntest_df['Embarked']=train_df.groupby(['Age','Sex'])['Embarked'].transform(lambda x: x.fillna(x.mode()))","5a46dc16":"def AgeGroup(Age):\n    Age_group=[]\n    for age in Age:\n        if (age)  < 18:\n            Age_group.append(0)\n        elif (age) >= 18 and (age) < 40:\n            Age_group.append(1)\n        elif (age) >= 40 and (age) < 60:\n            Age_group.append(2)\n        elif (age) >= 60 and (age) < 100:\n            Age_group.append(3)\n        else:\n            Age_group.append(2)\n    return Age_group\n\ntrain_df[\"AgeGroup\"]=AgeGroup(train_df['Age'])\ntest_df[\"AgeGroup\"]=AgeGroup(test_df['Age'])","14ad7cb0":"train_df['FamilySize']=train_df['SibSp']+train_df['Parch']+1\ntest_df['FamilySize']=test_df['SibSp']+train_df['Parch']+1","452281bb":"import pylab \nimport scipy.stats as stats\n\nstats.probplot(train_df.Fare, dist=\"norm\", plot=pylab)\npylab.show()","1cbab9c9":"train_df['Fare'].describe()","6cf0c1db":"quartile_1=np.percentile(train_df.Fare, 25)\nquartile_2=np.percentile(train_df.Fare, 50)\nquartile_3=np.percentile(train_df.Fare, 75)","f992c00c":"def Far_Cat(Fare):\n    FarCat=[]\n    for i in Fare:\n        if i >=0 and i< quartile_1:\n            FarCat.append(0)\n        if i >=quartile_1 and i< quartile_2:\n            FarCat.append(1)\n        if i >=quartile_2 and i< quartile_3:\n            FarCat.append(2)\n        if i >=quartile_3:\n            FarCat.append(3)\n    return FarCat\ntrain_df['FarCat']=Far_Cat(train_df.Fare)\ntest_df['FarCat']=Far_Cat(test_df.Fare)","236d30e0":"train_df.head()","785091f5":"test_df['FamilySize']=test_df['FamilySize'].fillna(0)\ntest_df['Age']=test_df['Age'].fillna(0)","4d404578":"test_df.head()","6ff097d3":"pid=test_df.PassengerId\ntrain_df=train_df.drop(['Ticket','Name','Age','Fare','Cabin','PassengerId'],axis=1)\ntest_df=test_df.drop(['Ticket','Name','Age','Fare','Cabin','PassengerId'],axis=1)","069dc05d":"#Finding the columns whether they are categorical or numerical\ncols = train_df.columns\nnum_cols = train_df._get_numeric_data().columns\nprint(\"Numerical Columns\",num_cols)\ncat_cols=list(set(cols) - set(num_cols))\nprint(\"Categorical Columns:\",cat_cols)\n\nfrom sklearn.preprocessing import LabelEncoder\nfor i in cat_cols:\n    train_df[i]=LabelEncoder().fit_transform(train_df[i].astype(str)) \n    test_df[i]=LabelEncoder().fit_transform(test_df[i].astype(str)) ","c930fc4f":"fig,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(train_df.corr(),ax=ax,annot= False,linewidth= 0.02,linecolor='black',fmt='.2f',cmap = 'Blues_r')\nplt.show()\n","1bfe013b":"for i in range(0, len(train_df.columns), 5):\n    sns.pairplot(data=train_df,\n                x_vars=train_df.columns[i:i+5],\n                y_vars=['Survived'])","ee43ee1d":"X=train_df.loc[:,train_df.columns!='Survived']\nY=train_df.Survived","ac285712":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty=\"l1\", C=1).fit(X, Y)\nprediction = lr.predict(test_df)\npred_lr = pd.DataFrame()\npred_lr['PassengerId']=pid\npred_lr['Survived'] = prediction\npred_lr.to_csv(\"..\/working\/submission_lr.csv\", index = False)","adc844ee":"from IPython.display import FileLink\nFileLink(r'submission_lr.csv')","a9b7b0a3":"from sklearn import svm\nsvc = svm.SVC(\n    C=5,\n    kernel=\"rbf\",\n    degree=3,\n    gamma=\"auto\",\n    coef0=0.0,\n    shrinking=True,\n    probability=False,\n    tol=0.001,\n    cache_size=200,\n    class_weight=None,\n    verbose=False,\n    max_iter=-1,\n    decision_function_shape=\"ovr\",\n    random_state=None,\n)\nmodel = svc.fit(X, Y)\nprediction = model.predict(test_df)\npred_svc = pd.DataFrame()\npred_svc['PassengerId']=pid\npred_svc['Survived'] = prediction\npred_svc.to_csv(\"..\/working\/submission_svc.csv\", index = False)","a423a43d":"from sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel=KNeighborsClassifier(n_neighbors=3)\nmodel = model.fit(X,Y)\nprediction = model.predict(test_df)\npred_knn = pd.DataFrame()\npred_knn['PassengerId']=pid\npred_knn['Survived'] = prediction\npred_knn.to_csv(\"..\/working\/submission_svc.csv\", index = False)","94b4e53c":"from sklearn import tree\nclf = tree.DecisionTreeClassifier(\n    criterion=\"gini\",\n    splitter=\"best\",\n    max_depth=5,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=None,\n    random_state=None,\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    class_weight=None,\n    presort=False,\n)\n\nmodel = clf.fit(X,Y)\nprediction = model.predict(test_df)\npred_dt = pd.DataFrame()\npred_dt['PassengerId']=pid\npred_dt['Survived'] = prediction\npred_dt.to_csv(\"..\/working\/submission_svc.csv\", index = False)","5e9c7ef5":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import ensemble\nrf = ensemble.RandomForestClassifier(\n    n_estimators=800,\n    criterion=\"gini\",\n    max_depth=None,\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_features=\"auto\",\n    max_leaf_nodes=None,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    bootstrap=True,\n    oob_score=False,\n    n_jobs=1,\n    random_state=None,\n    verbose=0,\n    warm_start=False,\n    class_weight=None,\n)\nrf.fit(X, Y)","911313ce":"prediction = rf.predict(test_df)\npred_rf = pd.DataFrame()\npred_rf['PassengerId']=pid\npred_rf['Survived'] = prediction\npred_rf.to_csv(\"..\/working\/submission_rf.csv\", index = False)","efc64ecb":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(\n    loss=\"deviance\",\n    learning_rate=0.1,\n    n_estimators=200,\n    subsample=1.0,\n    criterion=\"friedman_mse\",\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_depth=3,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    init=None,\n    random_state=None,\n    max_features=None,\n)\nmodel = clf.fit(X, Y)\n","d29c4c00":"prediction = rf.predict(test_df)\npred_GB = pd.DataFrame()\npred_GB['PassengerId']=pid\npred_GB['Survived'] = prediction\npred_GB.to_csv(\"..\/working\/submission_gb.csv\", index = False)","279d2c61":"import xgboost as xgb\nmodel_xgb = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(X,Y)","69099d17":"prediction = model_xgb.predict(test_df)\npred_xgb = pd.DataFrame()\npred_xgb['PassengerId']=pid\npred_xgb['Survived'] = prediction\npred_xgb.to_csv(\"..\/working\/submission_xgb.csv\", index = False)","7b2a857b":"from statistics import mode\nfinal_pred = np.array([])\nfor i in range(0,len(test_df)):\n    final_pred = np.append(final_pred, mode([pred_lr['Survived'][i],pred_dt['Survived'][i],pred_knn['Survived'][i],pred_svc['Survived'][i],pred_rf['Survived'][i], pred_GB['Survived'][i], pred_xgb['Survived'][i]]))","671f02f7":"prediction = model_xgb.predict(test_df)\npred_ensemble = pd.DataFrame()\npred_ensemble['PassengerId']=pid\npred_ensemble['Survived'] = prediction\npred_ensemble.to_csv(\"..\/working\/submission_ensemble.csv\", index = False)","e71a2936":"**Explanation:**\n\n1) Subgrouping age based on title.\n\n2) Filling the missing values based on median so that distribution of data remains the same after filling Na's","3b36cd4f":"**Using the titles obtained we'll fill the age**","c3cf9b95":"Since age and sex seem's to be a good parameter we'll try to localize the Embarked based on those values and impute them","0aaf9407":"**Don't Forget to upvote :)**","476bd5af":"**Imputing the missing values:**\n\nAs name is nominal column and it has title's for each name (eg.**Mr.** Donald) and it is one of the best key to impute age.","cfb4da42":"## Modelling","58da914b":"## Ensembling","51ce6161":"**Decision Tree**","f2a261cc":"**Finding the missing nature of the data**","5eb3253e":"**XGB Classifier**","b72f9725":"**Key Take Aways**\n\n1) Exploratory Data Analysis\n\n2) Feature Engineering\n\n3) Advanced Machine Learning Techinques\n\nThe important of all is that you will get familiar with how the data science competition works. I hope this kernal will help people to prepare themselves for the data science competitions and people can use this kernal to brush up their skills.","2b5b309f":"**Imputing the Embarked**","527d1c89":"**Feature Selection**","f63c17d7":"**Observation:**\n\n1)From this figure it is clear that missing nature of the data is same in both train and test data \n\n2)We have to figure out a common way to fill the missing data in both train and test. \n\n3)For that let's go for exploratory Data Analysis.","98b35953":"**SVM Classifier**","2b9331b3":"**Gradient Boosting Classifier**","cb59890d":"**Imputing Fare**\n\nThere is a single value missing from fare in test data let's impute that value with mean","f7bfed7d":"Thanks **Manav Sehgal** for your kernel","3db12075":"**Distribution with respect Survived**","4321274f":"**Random Forest Classifier**","7ab3ddec":"**Creating a feature based on Sibsp and parch**\n\nsibsp\t- of siblings \/ spouses aboard the Titanic\t\nparch\t- of parents \/ children aboard the Titanic\t\n\nSince these two features represent Family we create new Column Family","4e660bc5":"**Logistic regression**","a91f32f9":"**Dropping Columns**\n\n**Name**        -     Unique so not needed\n\n**Age**         -     Since we have AgeGroup,we'll delete this.\n\n**Ticket**      -     Unique so not needed\n\n**Fare**        -      Since we have FareCat,we'll delete this \n\n**Cabin**       -       Many Nan so imputing might lead to bais\n\n**PassengerId** - Cannot be categorised","af4ead42":"**Understanding the Survival Nature of Titanic**","b181807f":"## Feature Engineering","d336c3c9":"**Knn**\n","2675dae2":"## About the problem","3c7935aa":"**Fare Category**\n\nSince Fare is From 0 - 512 we'll form a new variable Fare Category based on the quartile's it is distributed\n \n       FareCategory  - Quartile\n            0        -  0-25%\n            1        -  25-50%\n            2        -  50-75%\n            3        -  75-100%","f6f8f4ec":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","946cd17c":"**Creating a new feature based on Age**\n\n1) Since age is from 0-100 we can form a new column based on age\n\n        Age Group    Class\n        0-18      -  Minor 0\n        18-40     -  Adult 1\n        40-60     -  Middle age 2\n        60-100    -  Old 3\n        ","08ddd279":"## Exploratory Data Analysis","0f60279e":"**Label encoding categorical columns**"}}