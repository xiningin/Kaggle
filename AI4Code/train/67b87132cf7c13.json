{"cell_type":{"0c6cf66d":"code","ca9d872c":"code","d1801d81":"code","77d7ca14":"code","c503c9e5":"code","ae21f6f8":"code","f2e027f1":"code","d48875af":"code","664388ea":"code","0fd6a124":"code","58dd82f2":"code","4306ac5c":"code","0fa33b38":"markdown","682de217":"markdown","1756451c":"markdown","7dcbb708":"markdown","f274eb3c":"markdown","018e674e":"markdown","c6c75877":"markdown","2aef91c7":"markdown","779e478e":"markdown","064b2b16":"markdown","109fd6fb":"markdown","04da04ec":"markdown","a06086b0":"markdown","7a637c51":"markdown"},"source":{"0c6cf66d":"from __future__ import print_function \n#%matplotlib inline \nimport argparse \nimport os \nimport random \nimport torch \nimport torch.nn as nn \nimport torch.nn.parallel \nimport torch.backends.cudnn as cudnn \nimport torch.optim as optim \nimport torch.utils.data \nimport torchvision.datasets as dset \nimport torchvision.transforms as transforms \nimport torchvision.utils as vutils \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport matplotlib.animation as animation \nfrom IPython.display import HTML\n\nmanualSeed = 999\nprint(\"Random seed : \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)","ca9d872c":"dataroot = \"..\/input\/cosmos-images\/data\"\n\nworkers = 2\nbatch_size = 128\nimage_size = 64\nnc = 3\nnz = 100\nngf = 64\nndf = 64\nnum_epochs = 500\nlr = 0.0002\nbeta1 = 0.5\nngpu = 1","d1801d81":"dataset = dset.ImageFolder(root=dataroot,\n                           transform=transforms.Compose([\n                               transforms.Resize(image_size),\n                               transforms.CenterCrop(image_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers)\n\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\n# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,16))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","77d7ca14":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","c503c9e5":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)","ae21f6f8":"netG = Generator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.02.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)","f2e027f1":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)","d48875af":"netD = Discriminator(ngpu).to(device)\n\n# Handle multi-gpu if desired\nif (device.type == 'cuda') and (ngpu > 1):\n    netD = nn.DataParallel(netD, list(range(ngpu)))\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)","664388ea":"criterion = nn.BCELoss()\n\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\nreal_label = 1.\nfake_label = 0.\n\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))","0fd6a124":"img_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        output = netD(real_cpu).view(-1)\n\n        errD_real = criterion(output, label)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        \n        output = netD(fake.detach()).view(-1)\n        errD_fake = criterion(output, label)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        netG.zero_grad()\n        label.fill_(real_label)  \n        output = netD(fake).view(-1)\n        errG = criterion(output, label)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1","58dd82f2":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","4306ac5c":"real_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","0fa33b38":"## Weights declaration","682de217":"Thank you for watching \"Tutorial!\n\nI don't think the learning was perfect because it was totally basic.\n\nBut I think this is interesting enough.\n\nI hope you change the base code to create a better image!\n\nGood luck!\ud83d\ude09","1756451c":"## Setting learning parameters.\n* ```workers``` : Number of workers for dataloader\n* ```batch_size``` : during training\n* ```image_size``` : Image size (64 or 128?)\n* ```nc``` : Number of channels in the training images.(For color images this is 3)\n* ```nz``` : Size of z latent vector (i.e. size of generator input)\n* ```ngf``` : Size of feature maps in generator\n* ```ndf``` : Size of feature maps in discriminator\n* ```num_epochs``` : training epochs\n* ```lr``` : Learning rate for optimizers\n* ```beta1``` : for Adam optimizers\n* ```ngpu``` : Number of GPUs available. Use 0 for CPU mode","7dcbb708":"# Generating space images with Pytorch GAN\n* Pytorch GAN Tutorial : [link](https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html)\n* Image source : Pixabay\n* Image count : 3612   ","f274eb3c":"## Discriminator Code\n> A model to check if the image of the universe is real.","018e674e":"## Make a dataset.\nand pring dataset images","c6c75877":"## Training Code\n> 500 epochs","2aef91c7":"## Bring the necessary modules.","779e478e":"## Declaration of optimizer and loss function of each model.","064b2b16":"## Real Image vs Fake Image","109fd6fb":"## Generator Code\n> A model that creates an image of the universe.   \n__After nn.Tanh(), state size : (nc) x 64 x 64__","04da04ec":"## Create the Discriminator","a06086b0":"## Printing Learning graph\n> I don't think I learned it well.......\ud83d\ude05","7a637c51":"## Create the generator model"}}