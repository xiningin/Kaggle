{"cell_type":{"230acd83":"code","8a93113c":"code","02f70162":"code","8f0bfe89":"code","023c3596":"code","5d5a61cb":"code","83abaa76":"code","11c35374":"code","cd177360":"code","eeb55712":"code","dd162875":"code","6cc599c3":"code","ebbbf089":"code","09b212a8":"code","d4b30952":"code","2a3e2fe0":"code","3ceed2ad":"code","a694a4d2":"code","9e9b2520":"code","ad9e32b1":"code","f324ca81":"code","b0662f8d":"code","119ebf1f":"code","8fce127a":"code","b4963299":"code","5706f07e":"markdown","9bda19e9":"markdown","5562b5f3":"markdown","dac6661e":"markdown","da3f9e87":"markdown","792fc543":"markdown","837c9b3f":"markdown"},"source":{"230acd83":"# imporing all the necessary modules \nimport pandas as pd\nimport sklearn\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8a93113c":"# loading the boston dataset\n\nfrom sklearn.datasets import load_boston\nboston = load_boston()","02f70162":"df = pd.DataFrame(boston.data , columns = boston.feature_names)\ndf['target'] = boston.target","8f0bfe89":"X = df.iloc[:,df.columns != 'target']\ny = df.target\n\n\nfrom sklearn.preprocessing import StandardScaler\nse = StandardScaler()\nX = se.fit_transform(X)","023c3596":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 12)\n\n\n# we are converting the training and testing data into xgboost optimized matrix for efficiency\n\ndtrain = xgb.DMatrix(X_train,y_train)\ndtest  = xgb.DMatrix(X_test,y_test)\n","5d5a61cb":"\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\ncv_ = KFold(n_splits=10,random_state=0)\n\nxg_reg = xgb.XGBRegressor(\n)\nscores = cross_val_score(xg_reg, X_train,y_train , scoring = 'neg_root_mean_squared_error',n_jobs = -1,cv = cv_)\nprint(np.mean(scores), np.std(scores))\nprint(scores)","83abaa76":"\n# defing a function for scoring and calculation for the rmse (default for regression )\n# in classification problem auc will be the default \nimport re\ndef return_rmse(params):\n    model = xgb.train(params , dtrain, num_boost_round = 600, evals = [(dtest, 'eval')],\n          early_stopping_rounds=20,verbose_eval = 0)\n    result = model.eval(dtest)\n    result = np.float(re.search(r'[\\d.]+$',result).group(0))\n    print(result)\n    return(result)","11c35374":"return_rmse(study.best_params)","cd177360":"import optuna\nfrom optuna import Trial, visualization\n\nfrom optuna.samplers import TPESampler","eeb55712":"\ndef objective(trial):\n   \n    param = {\n#                 \"n_estimators\" : trial.suggest_int('n_estimators', 0, 500),\n                'max_depth':trial.suggest_int('max_depth', 3, 5),\n                'reg_alpha':trial.suggest_uniform('reg_alpha',0,6),\n                'reg_lambda':trial.suggest_uniform('reg_lambda',0,2),\n                'min_child_weight':trial.suggest_int('min_child_weight',0,5),\n                'gamma':trial.suggest_uniform('gamma', 0, 4),\n                'learning_rate':trial.suggest_loguniform('learning_rate',0.05,0.5),\n                'colsample_bytree':trial.suggest_uniform('colsample_bytree',0.4,0.9),\n                'subsample':trial.suggest_uniform('subsample',0.4,0.9),\n\n                'nthread' : -1\n            }\n    return(return_rmse(param)) # this will return the rmse score \n","dd162875":"# calling the optuna study\nstudy1 = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy1.optimize(objective, n_trials= 1050,show_progress_bar = True)","6cc599c3":"trial = study1.best_trial\nprint('Accuracy: {}'.format(trial.value))","ebbbf089":"### printing the best estimators\nstudy1.best_params","09b212a8":"\n# without using any hyperparameter tuning \n\nparams = {}\nprint(f\"without tuning{return_rmse(params)}\")\nprint(f\"with tuning{return_rmse(study1.best_params)}\")","d4b30952":"optuna.visualization.plot_optimization_history(study1)","2a3e2fe0":"optuna.visualization.plot_slice(study1)\n","3ceed2ad":"\ndef objective(trial):\n   \n    param = {\n#                 \"n_estimators\" : trial.suggest_int('n_estimators', 0, 500),\n                'max_depth':trial.suggest_int('max_depth',4,4),\n                'reg_alpha':trial.suggest_uniform('reg_alpha',0,2),\n                'reg_lambda':trial.suggest_uniform('reg_lambda',0.5,1),\n                'min_child_weight':trial.suggest_int('min_child_weight',1,1),\n                'gamma':trial.suggest_int('gamma',1,1),\n                'learning_rate':trial.suggest_loguniform('learning_rate',0.2,0.4),\n                'colsample_bytree':trial.suggest_uniform('colsample_bytree',0.4,0.6),\n                'subsample':trial.suggest_uniform('subsample',0.4,0.5),\n\n                'nthread' : -1\n            }\n    return(return_rmse(param)) # this will return the rmse score \n","a694a4d2":"# calling the optuna study\nstudy2 = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy2.optimize(objective, n_trials= 1050,show_progress_bar = True)","9e9b2520":"optuna.visualization.plot_optimization_history(study2)","ad9e32b1":"study2.best_params","f324ca81":"params = {}\nprint(f\"without tuning{return_rmse(params)}\")\nprint(f\"with tuning{return_rmse(study2.best_params)}\")","b0662f8d":"from sklearn.model_selection import validation_curve\nparam_range = np.arange(10, 250, 2)\n\n# train_scores, test_scores = validation_curve(xg_reg_base,\n#                                   X, y, param_name=\"n_estimators\", param_range=param_range,\n#                                   cv=cv_shuffle, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n# train_mean = np.mean(train_scores, axis=1)\n# train_std = np.std(train_scores, axis=1)\ntrain_scores = [ ]\ntest_scores = [ ]\nfor i in param_range:\n    xg_reg = xgb.XGBRegressor(**study2.best_params,\n\n        n_estimators = i\n\n)\n    xg_reg.fit(X_train,y_train)\n    train_scores.append(np.sqrt(mean_squared_error(y_train,xg_reg.predict(X_train))))\n    test_scores.append(np.sqrt(mean_squared_error(y_test,xg_reg.predict(X_test))))\n\nimport matplotlib.pyplot as plt\n\nplt.subplots(1, figsize=(7,7))\nplt.plot(param_range, train_scores, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_scores, label=\"Cross-validation score\", color=\"dimgrey\")\n\n# plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n# plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n \nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Error\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","119ebf1f":"##finding the best n_estimators with the early stopping\n\nmodel = xgb.train(study2.best_params , dtrain, num_boost_round = 600, evals = [(dtest, 'eval')],\n          early_stopping_rounds=20,verbose_eval = 1)\n\n","8fce127a":"xgb_regressor = xgb.XGBRegressor(**study2.best_params, n_estimators = 71)\nxgb_regressor_base = xgb.XGBRegressor()","b4963299":"xgb_regressor = xgb.XGBRegressor(**study2.best_params, n_estimators = 71)\nxgb_regressor_base.fit(X_train,y_train)\nscore1 = xgb_regressor_base.score(X_test,y_test)\nxgb_regressor.fit(X_train,y_train)\nscore2 = xgb_regressor.score(X_test,y_test)\nprint(f\"R2 score withouth tuning:{score1} ,R2 score with tuning:{score2}\")","5706f07e":"## visualising the overfitting and underfitting and finding the best estimators","9bda19e9":"#### Creating a scoring funtion with the help of XGBOOST train inbuilt early stopping\n\nhere we are not tuning the n_estimators with the help of optuna \n\n\nwe will tune the n_estimators with the help of early_stopping by using the xgboost.train","5562b5f3":"### final model ","dac6661e":"## Plotting of our search history","da3f9e87":"### Testing a base model ","792fc543":"# Tuning again with more narrower values to get the sweet spot","837c9b3f":"1. **params are the kwargs"}}