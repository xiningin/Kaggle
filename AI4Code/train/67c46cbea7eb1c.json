{"cell_type":{"9e619311":"code","d58443f2":"code","e3b038ca":"code","65ea3df7":"code","e11f30f1":"code","f01d13cd":"code","e3f240ba":"code","fdac28c6":"code","cb6c059e":"code","bcabb4c8":"code","181fccbd":"code","7fc6aadf":"code","c22e31f1":"code","0258d6b5":"code","017c4732":"code","bb7a4c0b":"code","d826d2f6":"code","594bb410":"code","35cc8ef8":"code","79058c9a":"code","76136d9c":"code","b5e50804":"code","4b06640c":"code","eee95b99":"markdown","5760f7f5":"markdown","b3cc67e3":"markdown","0b08bcd3":"markdown"},"source":{"9e619311":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d58443f2":"\n!pip install texthero\n!pip install tweet-preprocessor\n","e3b038ca":"import seaborn as sns \nimport matplotlib.pyplot as plt ","65ea3df7":"#reading the data\ntrain_df= pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ntest_df= pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\nprint(train_df.columns,train_df.shape)\ntarget_col= train_df.columns[2:]\nfeature_col= train_df.columns[1:2]\ntrain_df.head()","e11f30f1":"target_col , feature_col","f01d13cd":"test_df.head()","e3f240ba":"import re \nimport nltk\nfrom wordcloud import WordCloud\nfrom nltk.stem import WordNetLemmatizer \nfrom textblob import TextBlob,Word\nfrom nltk.corpus import words\nnltk.download('words')\nimport texthero as hero\nimport re\nfrom texthero import stopwords\n\nfrom nltk.corpus import wordnet\n\nimport tensorflow as tf\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\n\nimport tensorflow as tf\n\n\ndef lemma_per_pos(sent):\n    '''function to lemmatize according to part of speech tag'''\n    tweet_tokenizer=TweetTokenizer()\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    lemmatized_list = [lemmatizer.lemmatize(w) for w in  tweet_tokenizer.tokenize(sent)]\n    return \" \".join(lemmatized_list)\n\ndef df_preprocessing(df,feature_col):\n    '''\n    Preprocessing of dataframe\n    '''\n    stop = set(stopwords.words('english'))\n    df[feature_col]= (df[feature_col].pipe(hero.lowercase).\n                      pipe(hero.remove_urls).\n                      pipe(hero.remove_digits).\n                      pipe(hero.remove_punctuation).\n                      pipe(hero.remove_html_tags) )\n    # lemmatization\n#     df[feature_col]= [lemma_per_pos(sent) for sent in df[feature_col]]\n    # df[col_name]= hero.remove_stopwords(df[col_name],custom_stopwords)\n    return df","fdac28c6":"with tf.device('\/GPU:0'):\n    proc_train_df= df_preprocessing(train_df,feature_col[0])","cb6c059e":"proc_test_df = df_preprocessing(test_df,feature_col[0])","bcabb4c8":"proc_train_df['len']= proc_train_df[feature_col[0]].str.split().map(lambda x : len(x))\nprint('Max length: {}, Min length: {}, Average Length :{}'.format(max(proc_train_df['len']),min(proc_train_df['len']),int(proc_train_df['len'].mean())))\nproc_train_df.len.hist()\nplt.show()","181fccbd":"# Only taking article length lower than 128\n# proc_train_df= proc_train_df[(proc_train_df.len<=512) & (proc_train_df.len>=10)].reset_index(drop=True)\nprint('Max length: {}, Min length: {}, Average Length :{}'.format(max(proc_train_df['len']),min(proc_train_df['len']),int(proc_train_df['len'].mean())))\n# Now the distribution is \nprint(proc_train_df.shape)\nproc_train_df.len.hist()\nplt.show()","7fc6aadf":"plt.figure(figsize=(10,6))\nplt.hist(x=[proc_train_df.toxic,proc_train_df.severe_toxic,proc_train_df.obscene,proc_train_df.threat,proc_train_df.insult,proc_train_df.identity_hate])\nplt.show()","c22e31f1":"from transformers import AutoTokenizer,TFDistilBertModel, DistilBertConfig\nfrom transformers import TFAutoModel\nimport tensorflow as tf \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport tensorflow_addons as tfa\n\n\n#Creating tokenizer\ndef create_tokenizer(pretrained_weights='distilbert-base-uncased'):\n  '''Function to create the tokenizer'''\n\n  tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n  return tokenizer\n\n#Tokenization of the data\ndef data_tokenization(dataset,feature_col,max_len,tokenizer):\n    '''dataset: Pandas dataframe with feature name is column name \n    Pretrained_weights: selected model \n    RETURN: [input_ids, attention_mask]'''\n\n    tokens = dataset[feature_col].apply(lambda x: tokenizer(x,return_tensors='tf', \n                                                            truncation=True,\n                                                            padding='max_length',\n                                                            max_length=max_len, \n                                                            add_special_tokens=True))\n    input_ids= []\n    attention_mask=[]\n    for item in tokens:\n        input_ids.append(item['input_ids'])\n        attention_mask.append(item['attention_mask'])\n    input_ids, attention_mask=np.squeeze(input_ids), np.squeeze(attention_mask)\n\n\n    return [input_ids,attention_mask]\n\ndef bert_model(pretrained_weights,max_len,learning_rate):\n  '''BERT model creation with pretrained weights\n  INPUT:\n  pretrained_weights: Language model pretrained weights\n  max_len: input length '''\n  print('Model selected:', pretrained_weights)\n  bert=TFAutoModel.from_pretrained(pretrained_weights)\n  \n  # This is must if you would like to train the layers of language models too.\n  for layer in bert.layers:\n      layer.trainable = True\n\n  # parameter declaration\n#   step = tf.Variable(0, trainable=False)\n#   schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [2e-0, 2e-1, 1e-2])\n#   # lr and wd can be a function or a tensor\n#   lr = learning_rate * schedule(step)\n#   wd = lambda:lr * schedule(step)\n#   optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n\n  optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n#   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n  # declaring inputs, BERT take input_ids and attention_mask as input\n  input_ids= Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n  attention_mask=Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')\n\n  bert= bert(input_ids,attention_mask=attention_mask)\n  x= bert[0][:,0,:]\n  x=tf.keras.layers.Dropout(0.1)(x)\n  x= tf.keras.layers.Dense(128)(x)\n  x=tf.keras.layers.Dense(64)(x)\n  x=tf.keras.layers.Dense(32)(x)\n\n  output=tf.keras.layers.Dense(6,activation='sigmoid')(x)\n\n  model=Model(inputs=[input_ids,attention_mask],outputs=[output])\n  # compiling model \n  model.compile(optimizer=optimizer,\n                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE,name='binary_crossentropy'),\n                metrics=['accuracy'])\n  return model","0258d6b5":"pretrained_weights='bert-base-uncased'\nmax_len=256\nepochs=3\nlearning_rate=2e-5\nbatch_size=4","017c4732":"tokenizer= create_tokenizer(pretrained_weights)","bb7a4c0b":"x_train= data_tokenization(proc_train_df,feature_col[0],max_len,tokenizer)","d826d2f6":"y_train= proc_train_df[target_col].values\ny_train","594bb410":"bert=bert_model(pretrained_weights,max_len,learning_rate)\nbert.summary()","35cc8ef8":"with tf.device('\/GPU:0'):\n    bert.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)","79058c9a":"test_ids= proc_test_df['id']\nx_test= data_tokenization(proc_test_df,feature_col[0],max_len,tokenizer)\nx_test","76136d9c":"preds= bert.predict(x_test)\nsubmiss_df= pd.DataFrame(preds, columns= target_col)\nsubmiss_df['id']=test_ids\nsubmiss_df","b5e50804":"submiss_df = submiss_df[['id']+target_col.tolist()]\nsubmiss_df","4b06640c":"submiss_df.to_csv('submissioin.csv', index=False, header=True)","eee95b99":"## Checking distribution of label","5760f7f5":"## Length Statistics","b3cc67e3":"## Data Preprocessing ","0b08bcd3":"# BERT model declaration"}}