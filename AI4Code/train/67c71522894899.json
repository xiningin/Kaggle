{"cell_type":{"9ca23fb8":"code","7ce6ccb3":"code","fffea0b9":"code","5cea320b":"code","4f347f0d":"code","e4af2919":"code","0e9ade5b":"code","4601efff":"code","379312c2":"code","2a732387":"code","fd4e4891":"code","d9d81c90":"code","4b22a003":"code","e1206030":"code","13dff9c7":"code","f043304a":"markdown","0f25b44f":"markdown","77573d6c":"markdown","db77e73e":"markdown","a1ea76ce":"markdown","46fa7acc":"markdown","a9dbf5c1":"markdown","32703bce":"markdown","0d12f772":"markdown","1bcd392d":"markdown","244a2228":"markdown","f92e1413":"markdown"},"source":{"9ca23fb8":"# Trax install\n!pip -q install trax\nimport trax\n\nimport tensorflow as tf\nimport numpy as np\nimport os","7ce6ccb3":"# Create a Transformer model.\n# Pre-trained model config in gs:\/\/trax-ml\/models\/translation\/ende_wmt32k.gin\nmodel = trax.models.Transformer(\n    input_vocab_size=33300,\n    d_model=512, d_ff=2048,\n    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n    max_len=2048, \n    mode='predict')","fffea0b9":"def reset_model_state(model, init_state):\n    \"\"\" Reset model state to previous state \"\"\"\n    model.state = init_state\n    return model\n\n# Initialize using pre-trained weights.\nmodel.init_from_file(file_name='gs:\/\/trax-ml\/models\/translation\/ende_wmt32k.pkl.gz',\n                     weights_only=True)\ninit_model_state = model.state\nprint(\"\\n... MODEL CREATED AND INITIAL STATE SAVED ...\\n\")","5cea320b":"# Tokenize a sentence.\nsentence = 'It is nice to learn new things today!'\ntokenized = np.array(list(trax.data.tokenize(iter([sentence]),  # Operates on streams.\n                                    vocab_dir='gs:\/\/trax-ml\/vocabs\/',\n                                    vocab_file='ende_32k.subword')))\n\nprint(f\"\\nFULL SENTENCE\\n\\t-->{sentence}\\n\")\nprint(f\"TOKENIZED SENTENCE\\n\\t-->{tokenized[0]}\\n\\n\")\n\nprint(f\"{'-'*50}\\n\\tSTRINGS AND THEIR RESPECTIVE TOKENS\\n{'-'*50}\\n\")\nfor s_tok, t_tok in zip(sentence.split(\" \"), tokenized[0]): print(f\"STRING : {s_tok}\\nTOKEN  : {t_tok}\\n\")","4f347f0d":"# Decode from the Transformer.\n\n# NOTE: If we use a higher temperature we will get more diverse results.\ntokenized_translation = trax.supervised.decoding.autoregressive_sample(\n    model, tokenized, temperature=0.0\n)  \n\n# reset for future translations\nmodel = reset_model_state(model, init_model_state)\n\nprint(f\"{'-'*50}\\n\\tTOKENIZED TRANSLATION\\n{'-'*50}\\n\")\nprint(tokenized_translation)","e4af2919":"# De-tokenize the translation\ntokenized_translation = tokenized_translation[0, :-1]  # Remove batch and EOS.\ntranslation = trax.data.detokenize(tokenized_translation,\n                                   vocab_dir='gs:\/\/trax-ml\/vocabs\/',\n                                   vocab_file='ende_32k.subword')\nprint(f\"\\n\\n{'-'*32}\\n\\tENGLISH SENTENCE\\n{'-'*32}\\n\")\nprint(sentence)\n\nprint(f\"\\n\\n{'-'*32}\\n\\tGERMAN SENTENCE\\n{'-'*32}\\n\")\nprint(translation)","0e9ade5b":"def translate_en_de(model, sentence, temp=0.0):\n    \"\"\" Convert an english sentence to German\n    \n    Args:\n        model (): Model to use to perform translation\n        sentence (str): English sentence to be translated\n        temp (float, optional): How diverse the results should be\n    \n    Returns:\n        A string containing the translated text\n    \"\"\"\n    # Tokenize\n    tokenized = np.array(list(trax.data.tokenize(iter([sentence]),  \n                                        vocab_dir='gs:\/\/trax-ml\/vocabs\/',\n                                        vocab_file='ende_32k.subword')))\n    \n    # Translate (ENTOK-2-DETOK)\n    tokenized_translation = trax.supervised.decoding.autoregressive_sample(\n        model, tokenized, temperature=temp, \n    )[0, :-1]  # Remove batch and EOS\n    reset_model_state(model, init_model_state)\n    \n    # Detokenize\n    translation = trax.data.detokenize(tokenized_translation,\n                                       vocab_dir='gs:\/\/trax-ml\/vocabs\/',\n                                       vocab_file='ende_32k.subword')\n    \n    print(f\"\\n\\n{'-'*32}\\n\\tENGLISH SENTENCE\\n{'-'*32}\\n\")\n    print(sentence)\n\n    print(f\"\\n\\n{'-'*32}\\n\\tGERMAN SENTENCE\\n{'-'*32}\\n\\n\")\n    print(translation+\"\\n\\n\")\n    \n\ntranslate_en_de(model, \"It is nice to learn new things today! Temperature is 0.0.\")\ntranslate_en_de(model, \"It is nice to learn new things today! Temperature is 0.5\", temp=0.5)\ntranslate_en_de(model, \"I had to add the hard reset of the model state into this tutorial to make it work... not exactly awesome documentation guys!\")\ntranslate_en_de(model, \"Whadddup my friends. Can this transfomer-bot handle som weirdh things including abbrev. and speling mistakes?\")","4601efff":"from trax.fastmath import numpy as fastnp\ntrax.fastmath.use_backend('jax')  # Can be 'jax' or 'tensorflow-numpy'.\n\n# See how we can make arrays\nmatrix = fastnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(f'\\nmatrix =\\n{matrix}')\n\n# See how we can make ones\/zeros arrays (as normal)\nvector = fastnp.ones((3, 3))\nprint(f'\\nvector =\\n{vector}')\n\n# See how we can calculate dot products\nproduct = fastnp.dot(vector, matrix)\nprint(f'\\nproduct =\\n{product}')\n\n# See how we can calculate functions like tanh (vectorized)\ntanh = fastnp.tanh(product)\nprint(f'\\ntanh(product) =\\n{tanh}')\n\n# See how we can calculate functions like tanh (single)\ntanh = fastnp.tanh(product[0, 0])\nprint(f'\\ntanh(product[0,0]) =\\n{tanh}')","379312c2":"def f(x):\n    \"\"\" Simple f(x) to demonstrate gradient calculation \"\"\"\n    return 4.0*x**3\n\ngrad_f = trax.fastmath.grad(f)\n\n# Note the derivative of 4x^3 is 12x^2\nprint(f'grad(4x^3) at +1 = {grad_f(1.0)}')\nprint(f'grad(4x^3) at -2 = {grad_f(-2.0)}')","2a732387":"from trax import layers as tl\n\n# Create an input tensor x.\nx = np.arange(15)\nprint(f'x = {x}')\n\n# Create the embedding layer.\nembedding = tl.Embedding(vocab_size=20, d_feature=32)\nembedding.init(trax.shapes.signature(x))\n\n# Run the layer -- y = embedding(x).\ny = embedding(x)\nprint(f'shape of y = {y.shape}')","fd4e4891":"model = tl.Serial(\n    tl.Embedding(vocab_size=8192, d_feature=256),\n    tl.Mean(axis=1),  # Average on axis 1 (length of sentence).\n    tl.Dense(2),      # Classify 2 classes.\n)\n\n# You can print model structure.\nprint(model)","d9d81c90":"train_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=True)()\neval_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=False)()\nprint(next(train_stream))  # See one example.","4b22a003":"data_pipeline = trax.data.Serial(\n    trax.data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n    trax.data.Shuffle(),\n    trax.data.FilterByLength(max_length=2048, length_keys=[0]),\n    trax.data.BucketByLength(boundaries=[  32, 128, 512, 2048],\n                             batch_sizes=[512, 128,  32,    8, 1],\n                             length_keys=[0]),\n    trax.data.AddLossWeights()\n  )\ntrain_batches_stream = data_pipeline(train_stream)\neval_batches_stream = data_pipeline(eval_stream)\nexample_batch = next(train_batches_stream)\nprint(f'shapes = {[x.shape for x in example_batch]}')  # Check the shapes.","e1206030":"from trax.supervised import training\n\n# Training task.\ntrain_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.WeightedCategoryCrossEntropy(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=500,\n)\n\n# Evaluaton task.\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],\n    n_eval_batches=20  # For less variance in eval numbers.\n)\n\n# Training loop saves checkpoints to output_dir.\noutput_dir = os.path.expanduser('~\/output_dir\/')\n!rm -rf {output_dir}\ntraining_loop = training.Loop(model,\n                              train_task,\n                              eval_tasks=[eval_task],\n                              output_dir=output_dir)\n\n# Run 2000 steps (batches).\ntraining_loop.run(2000)","13dff9c7":"example_input = next(eval_batches_stream)[0][0]\nexample_input_str = trax.data.detokenize(example_input, vocab_file='en_8k.subword')\nprint(f'example input_str: {example_input_str}')\nsentiment_log_probs = model(example_input[None, :])  # Add batch dimension.\nprint(f'Model returned sentiment probabilities: {np.exp(sentiment_log_probs)}')","f043304a":"### Data\n\nTo train your model, you need data. In Trax, data streams are represented as python iterators, so you can call `next(data_stream)` and get a tuple, e.g., `(inputs, targets)`. Trax allows you to use [TensorFlow Datasets](https:\/\/www.tensorflow.org\/datasets) easily and you can also get an iterator from your own text file using the standard `open('my_file.txt')`.","0f25b44f":"## 1. Run a pre-trained Transformer\n\nHere is how you create an **Engligh-German Translator** in a few lines of code:\n\n* create a Transformer model in Trax with [trax.models.Transformer](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.models.html#trax.models.transformer.Transformer)\n* initialize it from a file with pre-trained weights with [model.init_from_file](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.layers.html#trax.layers.base.Layer.init_from_file)\n* tokenize your input sentence to input into the model with [trax.data.tokenize](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html#trax.data.tf_inputs.tokenize)\n* decode from the Transformer with [trax.supervised.decoding.autoregressive_sample](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.supervised.html#trax.supervised.decoding.autoregressive_sample)\n* de-tokenize the decoded result to get the translation with [trax.data.detokenize](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.data.html#trax.data.tf_inputs.detokenize)\n","77573d6c":"After training the model, run it like any layer to get results.","db77e73e":"Using the `trax.data` module you can create input processing pipelines, e.g., to tokenize and shuffle your data. You create data pipelines using `trax.data.Serial` and they are functions that you apply to streams to create processed streams.","a1ea76ce":"Gradients can be calculated using **`trax.fastmath.grad`**","46fa7acc":"### Models\n\nModels in Trax are built from layers most often using the `Serial` and `Branch` combinators. You can read more about those combinators in the [layers intro](https:\/\/trax-ml.readthedocs.io\/en\/latest\/notebooks\/layers_intro.html) and\nsee the code for many models in `trax\/models\/`, e.g., this is how the [Transformer Language Model](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/models\/transformer.py#L167) is implemented. Below is an example of how to build a sentiment classification model.","a9dbf5c1":"### Supervised training\n\nWhen you have the model and the data, use `trax.supervised.training` to define training and eval tasks and create a training loop. The Trax training loop optimizes training and will create TensorBoard logs and model checkpoints for you.","32703bce":"### Layers\n\nLayers are basic building blocks of Trax models. You will learn all about them in the **[layers intro](https:\/\/trax-ml.readthedocs.io\/en\/latest\/notebooks\/layers_intro.html)** but for now, just take a look at the implementation of one core Trax layer, `Embedding`:\n\n```\nclass Embedding(base.Layer):\n  \"\"\"Trainable layer that maps discrete tokens\/IDs to vectors.\"\"\"\n\n  def __init__(self,\n               vocab_size,\n               d_feature,\n               kernel_initializer=init.RandomNormalInitializer(1.0)):\n    \"\"\"Returns an embedding layer with given vocabulary size and vector size.\n\n    Args:\n      vocab_size: Size of the input vocabulary. The layer will assign a unique\n          vector to each id in `range(vocab_size)`.\n      d_feature: Dimensionality\/depth of the output vectors.\n      kernel_initializer: Function that creates (random) initial vectors for\n          the embedding.\n    \"\"\"\n    super().__init__(name=f'Embedding_{vocab_size}_{d_feature}')\n    self._d_feature = d_feature  # feature dimensionality\n    self._vocab_size = vocab_size\n    self._kernel_initializer = kernel_initializer\n\n  def forward(self, x):\n    \"\"\"Returns embedding vectors corresponding to input token IDs.\n\n    Args:\n      x: Tensor of token IDs.\n\n    Returns:\n      Tensor of embedding vectors.\n    \"\"\"\n    return jnp.take(self.weights, x, axis=0, mode='clip')\n\n  def init_weights_and_state(self, input_signature):\n    \"\"\"Randomly initializes this layer's weights.\"\"\"\n    del input_signature\n    shape_w = (self._vocab_size, self._d_feature)\n    w = self._kernel_initializer(shape_w, self.rng)\n    self.weights = w\n```\n\nLayers with trainable weights like `Embedding` need to be initialized with the signature (shape and dtype) of the input, and then can be run by calling them.\n","0d12f772":"### Tensors and Fast Math\n\nThe basic units flowing through Trax models are *tensors* - multi-dimensional arrays, sometimes also known as numpy arrays, due to the most widely used package for tensor operations -- `numpy`. You should take a look at the [numpy guide](https:\/\/numpy.org\/doc\/stable\/user\/quickstart.html) if you don't know how to operate on tensors: Trax also uses the numpy API for that.\n\nIn Trax we want numpy operations to run very fast, making use of GPUs and TPUs to accelerate them. We also want to automatically compute gradients of functions on tensors. This is done in the `trax.fastmath` package thanks to its backends -- [JAX](https:\/\/github.com\/google\/jax) and [TensorFlow numpy](https:\/\/tensorflow.org).","1bcd392d":"## 3. Walkthrough\n\nYou can learn here how Trax works, how to create new models and how to train them on your own data.","244a2228":"# Trax Quick Intro\n\n[Trax](https:\/\/trax-ml.readthedocs.io\/en\/latest\/) is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the [Google Brain team](https:\/\/research.google.com\/teams\/brain\/). This notebook ([run it in colab](https:\/\/colab.research.google.com\/github\/google\/trax\/blob\/master\/trax\/intro.ipynb)) shows how to use Trax and where you can find more information.\n\n  1. **Run a pre-trained Transformer**: create a translator in a few lines of code\n  1. **Features and resources**: [API docs](https:\/\/trax-ml.readthedocs.io\/en\/latest\/trax.html), where to [talk to us](https:\/\/gitter.im\/trax-ml\/community), how to [open an issue](https:\/\/github.com\/google\/trax\/issues) and more\n  1. **Walkthrough**: how Trax works, how to make new models and train on your own data\n\nWe welcome **contributions** to Trax! We welcome PRs with code for new models and layers as well as improvements to our code and documentation. We especially love **notebooks** that explain how models work and show how to use them to solve problems!\n\n\n","f92e1413":"## 2. Features and resources\n\nTrax includes basic models (like [ResNet](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/models\/resnet.py#L70), [LSTM](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/models\/rnn.py#L100), [Transformer](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/models\/transformer.py#L189) and RL algorithms\n(like [REINFORCE](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/rl\/training.py#L244), [A2C](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/rl\/actor_critic_joint.py#L458), [PPO](https:\/\/github.com\/google\/trax\/blob\/master\/trax\/rl\/actor_critic_joint.py#L209)). It is also actively used for research and includes\nnew models like the [Reformer](https:\/\/github.com\/google\/trax\/tree\/master\/trax\/models\/reformer) and new RL algorithms like [AWR](https:\/\/arxiv.org\/abs\/1910.00177). Trax has bindings to a large number of deep learning datasets, including\n[Tensor2Tensor](https:\/\/github.com\/tensorflow\/tensor2tensor) and [TensorFlow datasets](https:\/\/www.tensorflow.org\/datasets\/catalog\/overview).\n\n\nYou can use Trax either as a library from your own python scripts and notebooks\nor as a binary from the shell, which can be more convenient for training large models.\nIt runs without any changes on CPUs, GPUs and TPUs.\n\n* [API docs](https:\/\/trax-ml.readthedocs.io\/en\/latest\/)\n* [chat with us](https:\/\/gitter.im\/trax-ml\/community)\n* [open an issue](https:\/\/github.com\/google\/trax\/issues)\n* subscribe to [trax-discuss](https:\/\/groups.google.com\/u\/1\/g\/trax-discuss) for news"}}