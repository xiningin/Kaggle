{"cell_type":{"b718a8a9":"code","33f3563a":"code","bc0a8209":"code","58ad319a":"code","bb7c1403":"code","7c41a837":"code","13dd1281":"code","8ecf74ae":"code","f7e63133":"code","124c555f":"code","ea865714":"code","71cad94f":"code","53441fb0":"code","fed0e920":"code","be52dacd":"code","db9f880a":"code","30ae88cf":"code","46dae293":"code","a9872fb4":"code","c38b71a4":"code","0f52689d":"code","a178265a":"code","96324b25":"code","1c9d1418":"code","6f9e473c":"code","5032a3db":"code","b0bf5f80":"code","443f507a":"code","414a8698":"code","e4904291":"code","260b2e60":"code","4855da81":"code","b3a78834":"code","0c2b7212":"code","e248661d":"code","a397656e":"code","45dee873":"code","f47be377":"code","ef1ba06b":"code","2e18ae72":"code","c0bc8358":"code","a8b1e6f7":"code","78127fda":"code","a4835ba1":"code","0be1d68a":"code","2769d646":"code","c23c1611":"code","5968d647":"markdown","b8bf467e":"markdown","adfa1ce5":"markdown","8c6ce7d2":"markdown","32aaf9d1":"markdown","4656cb5a":"markdown","e9055fd1":"markdown","0c1e05f3":"markdown","b9f60f04":"markdown","349875c9":"markdown","62ec11f4":"markdown","b9f7d6d2":"markdown","352fe9bf":"markdown","d5fec3b4":"markdown","b23fbcf9":"markdown","4a9f4615":"markdown","ae17b7bd":"markdown","4ab7ed39":"markdown","0ce990a0":"markdown","f2d9ebac":"markdown","1c8ee037":"markdown","eb165ee4":"markdown","d5218001":"markdown","6e99c591":"markdown","b6b8c9f2":"markdown"},"source":{"b718a8a9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression","33f3563a":"sns.set_style('whitegrid')\nsns.set_context('notebook')","bc0a8209":"# The pre-processed data is now stored here\n!ls \/kaggle\/input\/nb1-linear-regression","58ad319a":"df_train = pd.read_csv('\/kaggle\/input\/weather-postprocessing\/pp_train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/weather-postprocessing\/pp_test.csv', index_col=0)\n\nX_train = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/X_train.csv', index_col=0)\ny_train = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/y_train.csv', index_col=0, squeeze=True)\nX_valid = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/X_valid.csv', index_col=0)\ny_valid = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/y_valid.csv', index_col=0, squeeze=True)\nX_test = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/X_test.csv', index_col=0)","bb7c1403":"def mse(y_true, y_pred):\n    return ((y_true - y_pred)**2).mean()\n\ndef print_scores(model):\n    r2_train = model.score(X_train, y_train)\n    r2_valid = model.score(X_valid, y_valid)\n    mse_train = mse(y_train, model.predict(X_train))\n    mse_valid = mse(y_valid, model.predict(X_valid))\n    print(f'Train R2 = {r2_train}\\nValid R2 = {r2_valid}\\nTrain MSE = {mse_train}\\nValid MSE = {mse_valid}')","7c41a837":"sns.pairplot(\n    df_train[::1000], \n    x_vars=['t2m_fc_mean', 'orog', 'gh_pl500_fc_mean', 'cape_fc_mean', 'ssr_fc_mean', 'sm_fc_mean', 'u10_fc_mean'], \n    y_vars=['t2m_obs']\n);","13dd1281":"from sklearn.tree import DecisionTreeRegressor, plot_tree","8ecf74ae":"dt = DecisionTreeRegressor(max_depth=3)\ndt.fit(X_train, y_train)","f7e63133":"fig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(dt, filled=True, ax=ax, fontsize=12, feature_names=X_train.columns);","124c555f":"y_train.mean()","ea865714":"dt = DecisionTreeRegressor()\n%time dt.fit(X_train, y_train)","71cad94f":"print_scores(dt)","53441fb0":"dt = DecisionTreeRegressor(min_samples_leaf=200)\n%time dt.fit(X_train, y_train)","fed0e920":"print_scores(dt)","be52dacd":"from sklearn.ensemble import RandomForestRegressor","db9f880a":"rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n%time rf.fit(X_train, y_train)","30ae88cf":"print_scores(rf)","46dae293":"rf = RandomForestRegressor(n_estimators=10, n_jobs=-1, min_samples_leaf=100)\n%time rf.fit(X_train, y_train)","a9872fb4":"print_scores(rf)","c38b71a4":"rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=20)\n%time rf.fit(X_train, y_train)","0f52689d":"print_scores(rf)","a178265a":"dt1 = rf.estimators_[0]\nprint_scores(dt1)","96324b25":"plt.figure(figsize=(10, 7))\nplt.barh(X_train.columns, rf.feature_importances_)\nplt.xscale('log')\nplt.tight_layout()","1c9d1418":"X_valid.shape","6f9e473c":"X_pdp = X_valid.copy()","5032a3db":"X_pdp['station_alt'] = 100\nrf.predict(X_pdp).mean()","b0bf5f80":"X_pdp['station_alt'] = 500\nrf.predict(X_pdp).mean()","443f507a":"from sklearn.inspection import plot_partial_dependence","414a8698":"order = np.argsort(rf.feature_importances_)[::-1]","e4904291":"fig, ax = plt.subplots(figsize=(18, 3))\nplot_partial_dependence(rf, X_valid[::1000], order[1:8], feature_names=X_train.columns, grid_resolution=5, n_jobs=-1, n_cols=7, ax=ax)\nplt.tight_layout()","260b2e60":"preds = rf.predict(X_test)\nsub =  pd.DataFrame({'id': range(len(preds)), 'Prediction': preds})\nsub.to_csv('submission.csv', index=False)","4855da81":"df_train = df_train.dropna(subset=['t2m_obs'])","b3a78834":"split_date = '2015-01-01'\nstations_train = df_train.station[df_train.time < split_date]\nstations_valid = df_train.station[df_train.time >= split_date]\nstations_test = df_test.station","0c2b7212":"from tqdm.notebook import tqdm","e248661d":"models = {}\nfor s in tqdm(stations_train.unique()):\n    m = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=10)\n    m.fit(X_train[stations_train == s], y_train[stations_train == s])\n    models[s] = m","a397656e":"preds = np.zeros(len(y_valid))\nfor s in stations_valid.unique():\n    s_idxs = stations_valid == s\n    if s in stations_train.unique():\n        preds[s_idxs] = models[s].predict(X_valid[s_idxs])\n    else:\n        preds[s_idxs] = rf.predict(X_valid[s_idxs])","45dee873":"mse(preds, y_valid)","f47be377":"test_preds = np.zeros(len(X_test))\nfor s in stations_test.unique():\n    s_idxs = stations_test == s\n    if s in stations_train.unique():\n        test_preds[s_idxs] = models[s].predict(X_test[s_idxs])\n    else:\n        test_preds[s_idxs] = rf.predict(X_test[s_idxs])","ef1ba06b":"sub =  pd.DataFrame({'id': range(len(test_preds)), 'Prediction': test_preds})\nsub.head()","2e18ae72":"sub.to_csv('submission_local.csv', index=False)","c0bc8358":"unique_stations = pd.concat([df_train.station, df_test.station]).unique()","a8b1e6f7":"stat2id = {s: i for i, s in enumerate(unique_stations)}","78127fda":"df_train['station'] = df_train.station.apply(lambda x: stat2id[x])\ndf_test['station'] = df_test.station.apply(lambda x: stat2id[x])","a4835ba1":"# Replace missing soil moisture values with mean value\ndf_train.loc[:, 'sm_fc_mean'].replace(np.nan, df_train['sm_fc_mean'].mean(), inplace=True)\n# Same for test dataset, using the training values\ndf_test.loc[:, 'sm_fc_mean'].replace(np.nan, df_train['sm_fc_mean'].mean(), inplace=True)","0be1d68a":"X_train = df_train[df_train.time < split_date].drop(['t2m_obs', 'time'], axis=1)\ny_train = df_train[df_train.time < split_date]['t2m_obs']\n\nX_valid = df_train[df_train.time >= split_date].drop(['t2m_obs', 'time'], axis=1)\ny_valid = df_train[df_train.time >= split_date]['t2m_obs']\n\nX_test  = df_test.drop(['time'], axis=1)","2769d646":"rf = RandomForestRegressor(n_estimators=40, n_jobs=-1, min_samples_leaf=20)\n%time rf.fit(X_train, y_train)","c23c1611":"print_scores(rf)","5968d647":"## Interpretability: Feature importance\n\nSo we have some decent predictions now. But often we would also like to know why the model makes those predictions. Trying to figure this out, falls under the category of ML interpretability which is a hot research topic. \n\nOne of the easiest interpretability methods, that works for every type of ML model is feature importance. This is simply a measure of how important each input feature is for making a good prediction. In other words, without this feature how much worse would the prediction get?\n\n*Note: rf.feature_importances_ != permutation importance. If you want to apply this to your research it is worth reading through these references: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py*","b8bf467e":"Linear regression is a good start but often the relationships we are trying to model are not linear. One way to still use linear regression is to transform the data (e.g. log, square) but this only works if we know what kind of relationship we have. Real world relationships are often much more difficult and unintuitive.\n\nLet's actually look at some relationships in our data.","adfa1ce5":"Each individual tree has quite a lot of overfitting and isn't particularly good. But averaging all predictions (ensembling) makes the prediction much better.","8c6ce7d2":"So how can we prevent overfitting? One way would be to reduce the complexity of the tree.\n\nOne way to do this for a decision tree is to increase the minimum number of samples in each leaf.","32aaf9d1":"# Your turn\n\nAs before, try to open up a new notebook and reproduce the basic steps, looking at this notebook only when necessary.\n\n1. Import all the data\n2. Train a decision tree and visualize it\n3. Train a random forest. Change the parameters and see how this affects your score.\n4. Submit a random forest prediction to Kaggle.\n\nOnce you are done with this, here are some further things to try to improve your score.\n\n1. Train a local model (i.e. one for each station)\n2. Create an ensemble of a linear regression and a random forest. Does this help?\n3. Be creative. ","4656cb5a":"## Feature importance: Partial dependence\n\nPartial dependence plots tell you: \"All else being equal, how does changing one input variable change the output?\"\n\nTo compute this, one starts with the trained model and the validation dataset. To compute, e.g. the partial dependence with respect to the first feature, one progressively changes the value of this feature and checks how this affects the mean prediction.","e9055fd1":"- `n_estimators`: Number of trees\n- `n_jobs`: How many processors to use to train trees in parallel. -1 uses all cores.\n\nTraining random forests can take a few minutes. Add `verbose=1` to get a status report.","0c1e05f3":"- `value`: Mean target of all samples in that node\n- `MSE`: MSE if `value` was used as a prediction for all samples\n\nThe first splits are entirely based on `t2m_fc_mean`. This makes sense since this is by far the most informative variable.","b9f60f04":"## Random forests\n\nThe idea behind random forests is to have many individual decision trees that are all a little different. How are they different. At each split, only a random subset of features and samples is taken into account for deciding the best split. Because the errors of the individual trees will be independent from each other, averaging the prediction will reduce the error.","349875c9":"So lastly, we can increase the number of trees. Of course, this will take much longer. Take a break! If you want to try out several hyper-parameter settings and are tired of waiting, make a copy of the notebook and run several experiments in parallel.","62ec11f4":"Does this fit with our meteorological intuition?\n\nRemember that the model knows nothing about meteorology but only learns correlations in the training dataset.","b9f7d6d2":"So we still get a lot of overfitting. As before, let's make each tree less expressive.","352fe9bf":"Since computing this can take a while we will pick only the most important features but not `t2m_fc_mean` because we know it's a lot more important and the relationship is linear.","d5fec3b4":"# Random forests\n\nNow, we will look at a more sophisticated machine learning algorithm: Random forests.\n\nBefore we start though, we want to carry over some of the work we have done in the first notebook. To carry over the prepared datasets, click `Add or upload data` under `file` and then choose `01-linear_regression`.\n\n<!-- 1. To carry over the functions we defined, click `Add utility script` under `file` and then choose `00-utils`. You can modify this file if you want to add your own functions. For further information: https:\/\/www.kaggle.com\/getting-started\/93133 -->\n<!-- 1. To carry over the prepared datasets, click `Add or upload data` under `file` and then choose `01-linear_regression`. -->","b23fbcf9":"So we can see that a higher station altitude seems to lead to slightly lower temperatures.\n\nWe can use sklearns partial dependence funtion to automate this process for a range of values and features.","4a9f4615":"So its a little better but actually there is a much better way of doing this. Instead of having just one tree, let's build an entire forest.","ae17b7bd":"Hmm. We get a perfect score for our training data but a really bad score for the validation dataset. What happened?\n\nNow is the time to talk about **overfitting**!\n\nhttps:\/\/www.kaggle.com\/raspstephan\/overfitting","4ab7ed39":"## Submit to Kaggle","0ce990a0":"## Outlook: Gradient boosting","f2d9ebac":"So ideally we want an algorithm that can model any kind of relationship. One such algorithm is a random forest. But before we look at forests, let's look at an individual (decision) tree.\n\n## Decision trees\n\nLet's start by building a shallow tree and visualizing it.","1c8ee037":"### Build a full tree\n\nLet's now build a full tree. We will split the samples until only 1 sample is left in each leaf. This is going to take a little bit. [Poll]","eb165ee4":"Let's add some convenience functions we defined in the first notebook.","d5218001":"Finally, Random Forests are pretty robust and good ata awide range of tasks. If you want to do a little better, look at gradient boosting. You could, for example, try out the popular XGBoost package: https:\/\/xgboost.readthedocs.io\/en\/latest\/","6e99c591":"We can actually grab an individual tree and see how good it is.","b6b8c9f2":"## Using station as a feature"}}