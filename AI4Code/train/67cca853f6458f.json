{"cell_type":{"44fd9b6c":"code","675bae5d":"code","8acd6124":"code","b0645ab3":"code","febf1df2":"code","e4fbef29":"code","63bf3752":"code","87051c70":"code","1c39d0dc":"code","b3aec94e":"code","5712ba68":"code","3f3b7d6c":"code","d614563c":"code","0bbd1d6e":"code","fbfe08c8":"code","bb8411d0":"code","1f41301d":"code","391560b3":"code","8ec990b4":"code","32cd09b7":"code","0c4669c7":"code","05fe21d0":"code","74fe1dc8":"code","07badac4":"code","6a432d83":"code","a2a141c1":"code","1120348e":"code","d36f7783":"code","6b69a7fa":"code","0a281045":"code","c5cf44e5":"markdown","1e3d7075":"markdown","4a5bcb5e":"markdown","7101298d":"markdown","29e7dbd1":"markdown","7cb6c2c0":"markdown","15986100":"markdown","533a1060":"markdown","c370f2dc":"markdown","c0b456f1":"markdown","f2cb4934":"markdown","8f67ab6a":"markdown","a79e36a8":"markdown","5758b46b":"markdown","b719d87c":"markdown","1098a5ea":"markdown","d14d4ef1":"markdown","fe6b4d6c":"markdown","2a4d14be":"markdown"},"source":{"44fd9b6c":"#Librerias\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\n\n#Librer\u00edas de PyTorch\nimport torch  #PyTorch library\nfrom torch.utils.data import Dataset, DataLoader, sampler, random_split\nimport torchvision.transforms as tt\n\n#Librerias utiles para plotear\nfrom matplotlib.pyplot import imshow\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n%matplotlib inline \n#plots in the line below the code, inside the notebook ","675bae5d":"#Abro el archivo 95-Cloud Scene IDs, tipo excel\nids95 = pd.read_excel('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/95-Cloud_SceneIDs_lat_lon.xlsx')\nids95.head()","8acd6124":"# Escenas enteras \nprint('La carpeta contiene ' + str(len(os.listdir('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/Entire_Scene_gts'))) + ' im\u00e1genes.')\nes_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/Entire_Scene_gts\/edited_corrected_gts_LC08_L1TP_002053_20160520_20170324_01_T1.TIF')\nplt.imshow(es_img)","b0645ab3":"# Banda azul \nprint('La carpeta contiene ' + str(len(os.listdir('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_blue_additional_to38cloud'))) + ' im\u00e1genes.')\nblue_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_blue_additional_to38cloud\/blue_patch_100_5_by_12_LC08_L1TP_006248_20160820_20170322_01_T1.TIF')\nplt.imshow(blue_img, cmap='gray')","febf1df2":"# Banda verde\nprint('La carpeta contiene ' + str(len(os.listdir('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_green_additional_to38cloud'))) + ' im\u00e1genes.')\ngreen_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_green_additional_to38cloud\/green_patch_100_5_by_12_LC08_L1TP_006248_20160820_20170322_01_T1.TIF')\nplt.imshow(green_img, 'gray')","e4fbef29":"print('La carpeta contiene ' + str(len(os.listdir('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_red_additional_to38cloud'))) + ' im\u00e1genes.')\nred_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_red_additional_to38cloud\/red_patch_100_5_by_12_LC08_L1TP_006248_20160820_20170322_01_T1.TIF')\nplt.imshow(red_img, cmap='gray')","63bf3752":"print('La carpeta contiene ' + str(len(os.listdir('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_nir_additional_to38cloud'))) + ' im\u00e1genes.')\nnir_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_nir_additional_to38cloud\/nir_patch_100_5_by_12_LC08_L1TP_006248_20160820_20170322_01_T1.TIF')\nplt.imshow(nir_img, 'gray')","87051c70":"#Identificaci\u00f2n de la escena a la cual pertenece este recorte\nids95.SceneID == 'LC08_L1TP_006248_20160820_20170322_01_T1'\nprint('Latitud: ' + str(ids95.latitude[2]) + '; Longitud: ' + str(ids95.longitude[2]))","1c39d0dc":"#Imagen de ground truth\n\ngt_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_gt_additional_to38cloud\/gt_patch_100_5_by_12_LC08_L1TP_047011_20160920_20170221_01_T1.TIF')\ngt_array = np.array(gt_img)\nprint('Tama\u00f1o: ' + str(gt_array.shape))\nprint('Maximo = ' + str(gt_array.max()) + ' ; ' + 'Minimo = ' + str(gt_array.min()))\nprint(gt_array)\ngt_img","b3aec94e":"r = np.array(Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_red_additional_to38cloud\/red_patch_368_17_by_16_LC08_L1TP_006248_20160820_20170322_01_T1.TIF'))\nr = r\/r.max()\ng = np.array(Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_green_additional_to38cloud\/green_patch_368_17_by_16_LC08_L1TP_006248_20160820_20170322_01_T1.TIF'))\ng = g\/g.max()\nb = np.array(Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_blue_additional_to38cloud\/blue_patch_368_17_by_16_LC08_L1TP_006248_20160820_20170322_01_T1.TIF'))\nb = b\/b.max()\nrgb = np.zeros([384,384,3])\nrgb[:,:,0] = g\nrgb[:,:,1] = b\nrgb[:,:,2] = r\n\nplt.figure(figsize=(15,15))\n\nplt.subplot(131)\nplt.title('Bandas 2, 3, 4 (visibles), im\u00e1gen RGB')\nplt.imshow(rgb)\nplt.subplot(132)\nplt.title('Banda 5 (NIR)')\nplt.imshow(Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_nir_additional_to38cloud\/nir_patch_368_17_by_16_LC08_L1TP_006248_20160820_20170322_01_T1.TIF'), cmap='gray')\nplt.subplot(133)\nplt.title('M\u00e1scara')\nplt.imshow(Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_gt_additional_to38cloud\/gt_patch_368_17_by_16_LC08_L1TP_006248_20160820_20170322_01_T1.TIF'))\n\nplt.show()","5712ba68":"#Identificaci\u00f2n de la escena a la cual pertenece este recorte\nids95.SceneID == 'LC08_L1TP_006248_20160820_20170322_01_T1'\nprint('Latitud: ' + str(ids95.latitude[2]) + '; Longitud: ' + str(ids95.longitude[2]))","3f3b7d6c":"def data_split(red_dir, train_size, val_size):\n    '''\n    Funci\u00f3n que carga los nombres de todos los archivos de una direcci\u00f3n en una lista, \n    divide la lista en 3 partes de tama\u00f1os train_size la primera, val_size la segunda y tama\u00f1o total -(train_size + val_size) la tercera.\n    Finalmente, convierte todas las direcciones que eran str en objetos tipo Path.\n    Devuelve las 3 listas con las direcciones en formato Path.\n    \n    Parameters:\n    -----------\n    \n    red_dir: str. Direccion de la carpeta de la banda roja\n    train_size: int. Tama\u00f1o del dataset de entrenamiento\n    val_size: int. Tama\u00f1o del dataset de validacion\n    \n    Return:\n    -------\n    (train_list, val_list, test_list)\n    \n    '''\n    \n    if not (isinstance(red_dir, str)):\n        raise AttributeError('red_dir must be a string')\n    \n    a = os.listdir(red_dir)\n    b = np.split(a,[train_size,train_size+val_size,len(a)]) #necesito que siga siendo lista para partirlo\n    train_files = b[0]\n    val_files = b[1]\n    test_files = b[2]\n    \n    train_list = []\n    val_list = []\n    test_list = []\n    \n    r_dir = Path(red_dir)\n\n    for file_name in train_files:\n        train_list.append(r_dir\/file_name)\n        \n    for file_name in val_files:\n        val_list.append(r_dir\/file_name)\n        \n    for file_name in test_files:\n        test_list.append(r_dir\/file_name)\n      \n    return train_list, val_list, test_list ","d614563c":"#Cargo la lista de archivos partida\ntotal_list = data_split('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_red_additional_to38cloud', 15000, 5000)\nlen(total_list)","0bbd1d6e":"#Compruebo las longitudes de cada sub-lista\nprint(len(total_list[0])) #training\nprint(len(total_list[1])) #validation\nprint(len(total_list[2])) #test","fbfe08c8":"# Un objeto de la sub-lista train, el que est\u00e1 en la posici\u00f3n 3\ntotal_list[0][3] ","bb8411d0":"class CloudDataset(Dataset): #Subclase de la clase Dataset\n    \n    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, file_list, pytorch=True): #parametros\n        super().__init__() #para instanciar la clase\n        # Loop through the files in red folder and combine, into a dictionary, the other bands\n        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in file_list if not f.is_dir()]\n        self.pytorch = pytorch #NO SE QUE HACE\n        \n    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n        files = {'red': r_file, \n                 'green': g_dir\/r_file.name.replace('red', 'green'),\n                 'blue': b_dir\/r_file.name.replace('red', 'blue'), \n                 'nir': nir_dir\/r_file.name.replace('red', 'nir'),\n                 'gt': gt_dir\/r_file.name.replace('red', 'gt')}\n        return files\n                                       \n    def __len__(self):\n        return len(self.files)\n     \n    def open_as_array(self, idx, invert=False, include_nir=False):\n        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                            np.array(Image.open(self.files[idx]['green'])),\n                            np.array(Image.open(self.files[idx]['blue'])),\n                           ], axis=2)\n        if include_nir:\n            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n        if invert:\n            raw_rgb = raw_rgb.transpose((2,0,1))\n        return (raw_rgb \/ np.iinfo(raw_rgb.dtype).max)  # normalized\n    \n\n    def open_mask(self, idx, add_dims=False):\n        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n        raw_mask = np.where(raw_mask==255, 1, 0)\n        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n    \n    def __getitem__(self, idx):\n        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=True), dtype=torch.float32)\n        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n        return x, y\n    \n    def open_as_pil(self, idx):\n        arr = 256*self.open_as_array(idx)\n        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n    \n    def __repr__(self):\n        s = 'Dataset class with {} files'.format(self.__len__())\n        return s","1f41301d":"#direccion de la carpeta principal que contiene todas las imagenes\npath_ppal = Path('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud')","391560b3":"#Cargo los archivos del conjunto de entrenamiento en un dataset, es el primer elemento de la lista total_lista\ntrain_dataset = CloudDataset(path_ppal\/'train_red_additional_to38cloud', \n                    path_ppal\/'train_green_additional_to38cloud', \n                    path_ppal\/'train_blue_additional_to38cloud', \n                    path_ppal\/'train_nir_additional_to38cloud',\n                    path_ppal\/'train_gt_additional_to38cloud',\n                    total_list[0])\nlen(train_dataset)","8ec990b4":"#Cargo los archivos del conjunto de validaci\u00f3n en un dataset, es el segundo elemento de la lista total_lista\nval_dataset = CloudDataset(path_ppal\/'train_red_additional_to38cloud', \n                    path_ppal\/'train_green_additional_to38cloud', \n                    path_ppal\/'train_blue_additional_to38cloud', \n                    path_ppal\/'train_nir_additional_to38cloud',\n                    path_ppal\/'train_gt_additional_to38cloud',\n                    total_list[1])\nlen(val_dataset)","32cd09b7":"#Cargo los archivos del conjunto de testeo en un dataset, es el tercer elemento de la lista total_lista\ntest_dataset = CloudDataset(path_ppal\/'train_red_additional_to38cloud', \n                    path_ppal\/'train_green_additional_to38cloud', \n                    path_ppal\/'train_blue_additional_to38cloud', \n                    path_ppal\/'train_nir_additional_to38cloud',\n                    path_ppal\/'train_gt_additional_to38cloud',\n                    total_list[2])\nlen(test_dataset)","0c4669c7":"#Compruebo las formas de los objetos de los datasets\nx, y = train_dataset[0]\nx.shape, y.shape","05fe21d0":"#Abro un archivo como una imagen PIL\nplt.imshow(train_dataset.open_as_pil(13234))","74fe1dc8":"#Abro el mismo archivo como un np.array\narray = train_dataset.open_as_array(13234)\nrgb = np.zeros([384,384,3])\nrgb[:,:,0] = array[:,:,1]\nrgb[:,:,1] = array[:,:,0]\nrgb[:,:,2] = array[:,:,2]\n\nplt.imshow(rgb)\nrgb.max()","07badac4":"#Abro un archivo como array y su correspondiente m\u00e1scara\nfig, ax = plt.subplots(1,2, figsize=(10,9))\nax[0].imshow(train_dataset.open_as_array(3570))\nax[1].imshow(train_dataset.open_mask(3570))","6a432d83":"batch_size = 16","a2a141c1":"train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_dataset, batch_size, shuffle=True, num_workers=4, pin_memory=True)","1120348e":"def show_batch(dl): #OJO CON LOS CANALES!!\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=4).permute(1, 2, 0), cmap='gray')\n        \n        #fig2, ax2 = plt.subplots(figsize=(8, 8))\n        #ax2.set_xticks([]); ax2.set_yticks([])\n        \n        #ax2.imshow(make_grid(labels, nrow=4).permute(1, 2,0))\n        \n        break","d36f7783":"show_batch(train_dl)","6b69a7fa":"show_batch(val_dl)","0a281045":"#Por que usar la banda nir, descripci\u00f3n gr\u00e1fica\ngt2_img = Image.open('..\/input\/95cloud-cloud-segmentation-on-satellite-images\/95-cloud_training_only_additional_to38-cloud\/train_nir_additional_to38cloud\/nir_patch_100_5_by_12_LC08_L1TP_054019_20160820_20170221_01_T1.TIF')\nplt.imshow(gt2_img, cmap='gray')","c5cf44e5":"6. __Mascaras recortadas__ (*train gt*): Son los recortes no superpuestos, de 384x384 pixels de tama\u00f1o, de las m\u00e1scaras de las escenas enteras. Son 26301 en total. ","1e3d7075":"Google Maps\n![image.png](attachment:image.png)","4a5bcb5e":"### Dataloaders\nCreo los dataloaders y defino el tama\u00f1o de batch para cargar el dataset en batches.","7101298d":"Cada carpeta contiene imagenes en formato **TIF**. A continuaci\u00f3n explico qu\u00e9 imagenes contiene cada una y muestro un ejemplo.\n\n1. __Escenas enteras__ (*Entire scene ground truth*): Son las mascaras de las escenas enteras, es decir sin recortar, que provee el Landsat. Son im\u00e1genes de 8000x8000 pixeles de tama\u00f1o, de color 0 \u00f3 255 en escala RGB (negro \u00f3 blanco). No las utilizo en este trabajo (no directamente, si utilizo los recortes).","29e7dbd1":"## Preparaci\u00f3n de los datos\n\n### Partir el conjunto de datos\n\nEl primer paso es partir el conjunto de datos en tres partes:\n\n* **Entrenamiento** (*training*): se utiliza para entrenar el modelo, es decir calcular las perdidas y ajustar los pesos utilizando gradient descent. En este caso contendr\u00e1 15000 im\u00e1genes. \n* **Validaci\u00f3n** (*validation*): se utiliza para evaluar el modelo mientras se est\u00e1 entrenado, ajustar los hiperpar\u00e1metros y elegir el mejor modelo. En este caso, contendr\u00e1 5000 im\u00e1genes, que representan un 25% del tama\u00f1o del conjunto de datos de entrenamiento + validaci\u00f3n (generalmente tener un conjunto con 80% de datos para entrenamiento y 20% para validaci\u00f3n es un buen punto de partida)\n* **Prueba** (*test*): se utiliza una vez finalizado el entrenamiento como un par\u00e1metro para entender qu\u00e9 tan bien entren\u00f3 el modelo. Es decir, concluido el entrenamiento le paso al modelo im\u00e1genes que nunca vi\u00f3, que est\u00e1n en este conjunto, y compruebo qu\u00e9 tan bien puede predecir la prescencia de nubes. En este caso, contendr\u00e1 las 6301 im\u00e1genes restantes.\n\nPara ello cre\u00e9 la siguiente funci\u00f3n que recibe el path de una carpeta y carga los nombres de los archivos en una lista, divide la lista 3 partes de los tama\u00f1os desados y devuelve tres listas con las direcciones de los archivos en objetos tipo Path. ","7cb6c2c0":"3. __Banda verde__ (*train green*):  Son recortes no superpuestos, de 384x384 pixels de tama\u00f1o,  de las escenas enteras en la banda visible verde de Landsat. Notar que cada parche se corresponde espacialmente con el mismo parche del resto de las bandas. ","15986100":"__Explicacion por partes:__\n\n__class__ CloudDataset(Dataset): --> crea la subclase CloudDataset de la clase Dataset\n> \n    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, pytorch=True): \n        super().__init__() #para instanciar la clase\n        \nDefine los parametros que le voy a pasar a la clase, luego instancia la clase. Los par\u00e1metros son:\n1. r_dir: str. Direcci\u00f3n donde est\u00e1n los recortes de im\u00e1genes en la banda visible roja.\n2. g_dir: str. Direcci\u00f3n donde est\u00e1n los recortes de im\u00e1genes en la banda visible verde.\n3. b_dir: str. Direcci\u00f3n donde est\u00e1n los recortes de im\u00e1genes en la banda visible azul.\n4. nir_dir: str. Direcci\u00f3n donde est\u00e1n los recortes de im\u00e1genes en la banda del infrarojo cercano.\n5. gt_dir: str. Direcci\u00f3n donde est\u00e1n los recortes de im\u00e1genes de las m\u00e1scaras.\n6. file_list: lista de los paths de los archivos. \n7. pytorch: Bool. Activa la librer\u00eda PyTorch.\n\nLo siguiente itera a traves de los archivos en la carpeta r_dir y combina cada imagen de este canal (rojo) con la misma imagen (espacial) pero en los otros canales y su correpondiente m\u00e1scara.\n \n     for file in r_dir.iterdir():\n         if not file.is_dir(): \n             self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir)]\n     #self.pytorch = pytorch NO SE QUE HACE\n        \n\nLa funci\u00f3n __file.is_dir()__ Devuelve True si el path apunta a un directorio (carpeta); False si apunta a otro tipo de archivo o el path no existe \u00f3 est\u00e1 roto. \n\n- Funcion __combine_files__\n\n         def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n             files = {'red': r_file, \n                 'green': g_dir\/r_file.name.replace('red', 'green'),\n                 'blue': b_dir\/r_file.name.replace('red', 'blue'), \n                 'nir': nir_dir\/r_file.name.replace('red', 'nir'),\n                 'gt': gt_dir\/r_file.name.replace('red', 'gt')}\n        return files        \nA cada imagen de la carpeta del canal rojo que va abriendo, la combina en un diccionario con la misma imagen espacial pero en otros canales. Para cada elemento, la _clave_ es el nombre del canal y el _valor_ es la direccion (tipo Path) de la imagen. Por eso para los canales que no son el rojo, por ejemplo para la carpeta green abre el mismo archivo pero donde dice red cambia por green.  \n        \n- Funci\u00f3n __len__\n\n        def __len__(self):\n            return len(self.files)\nDevuelve la cantidad de elementos del dataset.\n\n- Funcion __open_as_array__\n\n        def open_as_array(self, idx, invert=False, include_nir=False):\n            raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n                            np.array(Image.open(self.files[idx]['green'])),\n                            np.array(Image.open(self.files[idx]['blue'])),\n                           ], axis=2)\n            if include_nir:\n                nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n                raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n            if invert:\n                raw_rgb = raw_rgb.transpose((2,0,1))\n        return (raw_rgb \/ np.iinfo(raw_rgb.dtype).max)   # normalize       \nAbre las imagenes RGB normalizadas como arrays de Numpy. Adem\u00e1s se puede agregar una dimensi\u00f3n al array que contiene la banda del infrarrojo cercano (claramente si se activa esta funci\u00f3n no es posible graficar la imagen como RGB por ejemplo con plt.imshow).\niinfo param = The kind of integer data type to get information about.\n\n- Funci\u00f3n **open_as_pil**\n\n        def open_as_pil(self, idx):\n        arr = 256*self.open_as_array(idx)\n        return Image.fromarray(arr.astype(np.uint8), 'RGB')\nAbre las im\u00e1genes RGB como im\u00e1genes PIL. ","533a1060":"El dataset contiene una carpeta con todos los datos y la siguiente estructura de directorios:\n\n![image.png](attachment:image.png)","c370f2dc":"5. __Banda infrarrojo cercano__ (*train nir*):  Son recortes no superpuestos, de 384x384 pixels de tama\u00f1o,  de las escenas enteras en la banda del infrarojo cercano de Landsat. Notar que cada parche se corresponde espacialmente con el mismo parche del resto de las bandas. ","c0b456f1":"Los siguientes enlaces muestran mapas que incluyen el sistema de referencia que utiliza el Landsat 8, junto con las latitudes y longitudes. \n\n[World Reference System 2- Daytime, descending](https:\/\/prd-wret.s3.us-west-2.amazonaws.com\/assets\/palladium\/production\/s3fs-public\/styles\/full_width\/public\/thumbnails\/image\/wrs2.gif)\n\n[World Reference System 2- Nightime, ascending](https:\/\/www.usgs.gov\/media\/images\/world-reference-system-2-wrs-2-nightascending)","f2cb4934":"Finalmente, muestro una imagen con las tres bandas combinadas para formar una imagen RGB, y su respectiva m\u00e1scara. Notar que no est\u00e1n normalizadas. Elijo esta en particular porque contiene una superficie con nieve y sin nubes y se puede observar claramente que la m\u00e1scara refleja este hecho.","8f67ab6a":"## Exploraci\u00f3n de los datos","a79e36a8":"4. __Banda roja__ (*train red*):  Son recortes no superpuestos, de 384x384 pixels de tama\u00f1o,  de las escenas enteras en la banda visible roja de Landsat. Notar que cada parche se corresponde espacialmente con el mismo parche del resto de las bandas. ","5758b46b":"**LXSS_LLLL_PPPRRR_YYYYMMDD_yyyymmdd_CC_TX**\n* L = Landsat (constant)\n\n* X = Sensor (C = OLI \/ TIRS, O = OLI-only, T= TIRS-only, E = ETM+, T = TM, M= MSS)\n\n* SS = 08 : Landsat 8\n\n* LLLL = L1TP : Processing level, L1TP es correcci\u00f3n de precisi\u00f3n y terreno (L1TP, L1GT, L1GS)\n\n* PPP = WRS path\n\n* RRR = WRS row\n\n* YYYYMMDD = Acquisition Year (YYYY) \/ Month (MM) \/ Day (DD)\n\n* yyyymmdd = Processing Year (yyyy) \/ Month (mm) \/ Day (dd)\n\n* CC = 01 : Collection number\n* TX= RT for Real-Time, T1 for Tier 1 (highest quality), and T2 for Tier 2\n","b719d87c":"2. __Banda azul__ (*train blue*): Son recortes no superpuestos, de 384x384 pixels de tama\u00f1o, de las escenas enteras en la banda visible azul de LANDSAT8. Notar que cada parche se corresponde espacialmente con el mismo parche del resto de las bandas.","1098a5ea":"A continuaci\u00f3n utilizo esta clase:","d14d4ef1":"## Fuentes:\n* Clase CloudDataset: https:\/\/medium.com\/analytics-vidhya\/how-to-create-a-custom-dataset-loader-in-pytorch-from-scratch-for-multi-band-satellite-images-c5924e908edf","fe6b4d6c":"World Reference System 2- Daytime, descending\n![image.png](attachment:image.png)","2a4d14be":"### Necesidad de la definicion de la clase CloudDataset\n \nPara poder cargar las imagenes en un dataset y que el modelo funcione, a cada parche o recorte debo:\n\n- convertir esa imagen TIF en un array de numpy\n- juntarlo con los parches que le corresponden espacialmete del resto de las bandas (stackear los 4 parches)\n- convertir ese arreglo 4-dimensional en un tensor de PyTorch\n\nPodria hacer esto, guardar todos los archivos como im\u00e1genes PIL y cargarlos con la funci\u00f3n ImageFolder o Datafolder pero tardar\u00eda mas y la memoria no alcanza (Kaggle ofrece 4.9 GB de memoria mientras que las imagenes ocupan 17.29 GB). La soluci\u00f3n es crear una clase que herede de la clase **torch.util.data.Dataset**, una subclase. Esta va a cumplir la misma funci\u00f3n: cargar los datos en un Dataset y devolver los *inputs*, en este caso los recortes de las im\u00e1genes RGB, con sus correspondientes *targets*, en este caso la m\u00e1scara correspondiente a cada recorte, pero adem\u00e1s debemos agregarle otras funcionalidades especialmente adaptadas a este dataset. \n\nLa documentaci\u00f3n de la clase **torch.utils.data.Dataset** dice que todas sus subclases deben sobreescribir el m\u00e9todo __ __getitem__()__ , que es el m\u00e9todo que devuelve los _inputs_ y _targets_. Opcionalmente pueden sobreescribir el metodo __ __len__()__ , del cual se espera que devuelva el tama\u00f1o del dataset."}}