{"cell_type":{"8ec15e45":"code","58d0b0ae":"code","c8506876":"code","721da481":"code","3b9446f7":"code","ff1f3c26":"code","8f0d8eda":"code","48e81113":"code","7e37a8a1":"code","d3b50c07":"code","add750d3":"code","71476698":"code","37fc2c24":"code","87d5f990":"code","ce9b5cdd":"code","4f16dd00":"code","e5877103":"code","4c57894d":"code","48ed0a0b":"markdown","45d5856e":"markdown","e0b1d3ba":"markdown","620147ea":"markdown","e219b455":"markdown","0d3773cb":"markdown","2391ed18":"markdown","bc981951":"markdown"},"source":{"8ec15e45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58d0b0ae":"from tensorflow.keras.preprocessing.text import one_hot","c8506876":"##sentences\nsent = ['the glass of milk',\n       'the glass of juice',\n       'he is a coder',\n       'he is a good devloper',\n       'those videos are good',\n       'nlp is cool',\n       'ai is future']","721da481":"sent","3b9446f7":"voc_size = 10000","ff1f3c26":"onehot_repr = [one_hot(words,voc_size) for words in sent] ","8f0d8eda":"print(onehot_repr)","48e81113":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential","7e37a8a1":"sent_length = 8 #making all senetences 8 words long\nembedded_docs = pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","d3b50c07":"output_dim = 10\ninput_dim = voc_size\ninput_length = sent_length","add750d3":"model = Sequential()\nmodel.add(Embedding(input_dim,output_dim,input_length=input_length))\nmodel.compile('adam','mse')","71476698":"model.summary()","37fc2c24":"model.predict(embedded_docs)","87d5f990":"sent[0]","ce9b5cdd":"embedded_docs[0]","4f16dd00":"first_sent = model.predict(embedded_docs[0])","e5877103":"first_sent.shape","4c57894d":"first_sent","48ed0a0b":"There are also many different things in embedding langauge","45d5856e":"Exploring Whole Transformation for One Sentence","e0b1d3ba":"Padding Sequences : \npad_sequences is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.","620147ea":"ONE HOT REPRESENTATION : Keras provides the one_hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one-hot encoding of the document, which is not the case.\nInstead, the function is a wrapper . The function returns an integer encoded version of the document.","e219b455":"VOCAB SIZE :\n    The vocabulary size (total words) must be specified. This could be the total number of words in the document or more if you intend to encode additional documents that contains additional words. The size of the vocabulary defines the hashing space from which words are hashed","0d3773cb":"The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n\nIt must specify 3 arguments:\n\ninput_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n\n\noutput_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n\n\ninput_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.","2391ed18":"Converting our senetences into embedding layers","bc981951":"#importing one hot library"}}