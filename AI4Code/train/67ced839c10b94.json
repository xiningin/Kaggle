{"cell_type":{"e6cc86c2":"code","7368db59":"code","b3441f61":"code","5ecc729d":"code","c80eb3eb":"code","e12a71fc":"code","1f831ad6":"code","2cb157c7":"code","575443b6":"code","a50b8874":"code","a61d5d4d":"code","ce399bee":"code","ca91c39a":"code","72a5165d":"code","ca34754b":"code","26a7a003":"code","48244643":"code","99a2015d":"code","d1202438":"code","c3f2d2a9":"code","01d688b7":"code","3b13c50b":"code","c0d66cd5":"code","ac995ab8":"code","acbbd7d0":"code","47571b01":"code","d40c95f4":"code","29b1e5c4":"code","027dfb92":"code","c9142e47":"code","53a135aa":"code","6f3ada68":"code","26400310":"code","768e3af5":"code","7fae73ea":"code","551d2035":"code","b095c07d":"code","6c4d62fa":"code","7acc9233":"code","d52055c4":"code","280097f6":"code","3a249c13":"code","d7f15d32":"markdown","a6f36444":"markdown","73b5dd10":"markdown","29f6f512":"markdown","2649a44c":"markdown","8570fb21":"markdown","1b23fd8a":"markdown","a019280f":"markdown","defc30f9":"markdown","6552dd85":"markdown","df2c3ca0":"markdown","98fc80a7":"markdown","faadafe9":"markdown","236f7a90":"markdown","209eb664":"markdown","a7ba33cd":"markdown","2a7adff0":"markdown","eb029184":"markdown","5edb9a7e":"markdown","2571d1de":"markdown","d441bbd7":"markdown","755742f1":"markdown","c4edebd6":"markdown","7be281ae":"markdown","b5bf040a":"markdown","77fa133d":"markdown","4ee43593":"markdown","46449250":"markdown","160f26e7":"markdown","68f6fb28":"markdown","75ec051e":"markdown","ccc31961":"markdown","ddc96c93":"markdown","d4d0a690":"markdown","8da88e03":"markdown","59f19d5a":"markdown","7a05f7dd":"markdown","7700e3d5":"markdown","0b607479":"markdown","e7446f2f":"markdown","5149b8ed":"markdown","9fa01dc5":"markdown","b80ed990":"markdown","f34096a2":"markdown","06d7e6e6":"markdown","e546b9ce":"markdown","0d1c00a0":"markdown","da047427":"markdown","ca72bf83":"markdown","cb7ee4cb":"markdown","a9bc7b6a":"markdown","cf73e5ff":"markdown","ba9e8e55":"markdown","7a92af89":"markdown"},"source":{"e6cc86c2":"import numpy as np \nimport pandas as pd \nimport pandas_profiling as pp\nfrom tqdm import tqdm\nfrom textblob import TextBlob\n\n# Plotly tools\nfrom plotly.offline import init_notebook_mode, iplot\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom sklearn.feature_extraction.text import CountVectorizer","7368db59":"review_df = pd.read_csv('..\/input\/Womens Clothing E-Commerce Reviews.csv')","b3441f61":"# Perform and advanced visualisation using pandas_profiling library\npp.ProfileReport(review_df)","5ecc729d":"# We are going to print some reviews in order to better understand the kind of preprocessing\n# task we will be performing.\n\nsome_reviews_index = [7, 9, 16] \n\n# tqdm is for printing the status bar\nfor rw in tqdm(some_reviews_index):\n    print(review_df['Review Text'].values[rw])\n    print(print(\"=\"*50))","c80eb3eb":"# 1- Remove the Title, Unnamed columns \nreview_df.drop(['Title', 'Unnamed: 0'], axis=1, inplace=True)","e12a71fc":"# 2- Remove the rows where Review Text is missing\nreview_df = review_df[~review_df['Review Text'].isnull()] ","1f831ad6":"# 3- clean Review Text column\n\n# As we can see previously in the six reviews we printed, we will perform many cleaning process\n# Remove special characters\n# remove all digits\n# remove all backslask \n# remove additional spaces\n# remove all the stopwords\n# put all the into lowercase\n\n#----------------------------\n# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","2cb157c7":"# This function will give the decontracted format of the words.\n# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","575443b6":"# ACTION N\u00b01\nprint(\"Before decontraction: \")\nprint(review_df['Review Text'].values[9])\nprint(print(\"=\"*50))\n\n\n# Let check the review n\u00b09 after decontraction\nrew = decontracted(review_df['Review Text'].values[9])\nprint(\"After decontraction: \")\nprint(rew)","a50b8874":"preprocessed_review_text = []\n\nfor review in tqdm(review_df['Review Text'].values):\n    rw = decontracted(review)\n    rw = rw.replace('\\\\r', ' ')\n    rw = rw.replace('\\\\\"', ' ')\n    rw = rw.replace('\\\\n', ' ')\n    rw = re.sub('[^A-Za-z0-9]+', ' ', rw)\n    \n    # https:\/\/gist.github.com\/sebleier\/554280\n    rw = ' '.join(e for e in rw.split() if e.lower() not in stopwords)\n    preprocessed_review_text.append(rw.lower().strip())","a61d5d4d":"# ACTION N\u00b02\n# Action of preprocessing\nprint(\"Before preprocessing: \")\nprint(review_df['Review Text'].values[9])\n\n# Let perform the preprocessing\nprint(\"\\nAfter preprocessing: \")\nprint(preprocessed_review_text[9])","ce399bee":"# 4- Feature for length of review\nreview_df['review_length'] = review_df['Review Text'].astype(str).apply(len) \n\n# 5- Create new feature for the word count of the review \nreview_df['word_count'] = review_df['Review Text'].apply(lambda x: len(str(x).split())) \n\n# 6- Calculate sentiment polarity  \nreview_df['polarity'] = review_df['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)","ca91c39a":"# Let check if our new columns have been added to our set of features.\nreview_df.columns","72a5165d":"# 1- Highest sentiment polarity \nprint('3 random reviews with the highest positive sentiment polarity: \\n')\ncl = review_df.loc[review_df.polarity == 1, ['Review Text']].sample(3).values\nfor c in cl:\n    print(\"review: \",c[0])","ca34754b":"# 2- Neutral sentiment polarity\nprint('3 random reviews with the most neutral sentiment polarity: \\n')\ncl = review_df.loc[review_df.polarity == 0, ['Review Text']].sample(3).values\nfor c in cl:\n    print(\"review: \",c[0])","26a7a003":"# Most negative sentiment polarity (0)\n\n# As there is no review with the value of polarity to -1, we are going to take \n# those have a polarity less than -0.9.\nprint('3 random reviews with the most negative sentiment polarity: \\n')\ncl = review_df.loc[review_df.polarity < -0.90 , ['Review Text']].sample(3).values\nfor c in cl:\n    print(\"review: \",c[0])","48244643":"review_df['Review Text'] = preprocessed_review_text","99a2015d":"# Let check if the operation worked using the previous polarity analysis. \n# We are goin to take only those for negative polarity\nprint('3 random reviews with the most negative sentiment polarity: \\n')\ncl = review_df.loc[review_df.polarity < -0.90 , ['Review Text']].sample(3).values\nfor c in cl:\n    print(\"review: \",c[0])","d1202438":"review_df['polarity'].iplot(\nkind='hist',\n    bins=50,\n    xTitle='Polarity',\n    linecolor='green',\n    yTitle='Count',\n    title='Sentiment Polarity Distribution'\n)","c3f2d2a9":"review_df['Rating'].iplot(\nkind='hist',\n    xTitle='Rating',\n    linecolor='green',\n    yTitle='Count',\n    title='Reviewers rating Distribution'\n)","01d688b7":"review_df['Age'].iplot(\nkind='hist',\n    bins=50,\n    xTitle='Age',\n    linecolor='green',\n    yTitle='Count',\n    title='Reviewers age Distribution'\n)","3b13c50b":"review_df['review_length'].iplot(\nkind='hist',\n    bins=50,\n    xTitle='Review Length',\n    linecolor='green',\n    yTitle='Count',\n    title='Review Length Distribution'\n)","c0d66cd5":"review_df['word_count'].iplot(\nkind='hist',\n    bins=50,\n    xTitle='Word count',\n    linecolor='green',\n    yTitle='Count',\n    title='Review Text Word Count Distribution'\n)","ac995ab8":"review_df.groupby('Class Name').count()['Clothing ID'].sort_values(ascending=False).iplot(kind='bar', \n                                                                                   yTitle='Count', \n                                                                                   linecolor='green', \n                                                                                   opacity=0.8,\n                                                                                   title='Bar chart of Class Name', \n                                                                                   xTitle='Class Name')","acbbd7d0":"review_df.groupby('Department Name').count()['Clothing ID'].sort_values(ascending=False).iplot(kind='bar', \n                                                                                   yTitle='Count', \n                                                                                   linecolor='green', \n                                                                                   opacity=0.8,\n                                                                                   title='Bar chart of Department Name', \n                                                                                   xTitle='Department Name')","47571b01":"review_df.groupby('Division Name').count()['Clothing ID'].sort_values(ascending=False).iplot(kind='bar', \n                                                                                   yTitle='Count', \n                                                                                   linecolor='green', \n                                                                                   opacity=0.8,\n                                                                                   title='Bar chart of Division Name', \n                                                                                   xTitle='Division Name')","d40c95f4":"# function to get TOP n-grams   \n#@corpus: the document to analyse n-gram\n#@n_gram_value: 1(unigram), 2(bigram), 3(trigram), etcetera\n#@n: top n value to get.\n\ndef get_top_n_grams(corpus, n_gram_value, n=None):\n    \n    vector = CountVectorizer(ngram_range=(n_gram_value, n_gram_value)).fit(corpus)\n    \n    # We will used bag of words representation\n    bow = vector.transform(corpus)\n    sum_words = bow.sum(axis=0)\n    \n    # Determine frequency for the chart\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vector.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]","29b1e5c4":"# The final function to plot our N-grams\ndef plot_n_gram(common_words):\n    for word, freq in common_words:\n        print(word, freq)\n        \n    df = pd.DataFrame(common_words, columns=['ReviewText' , 'count'])\n    df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\nkind='bar', yTitle='Count', linecolor='black', title='Top 15 bigrams in review')","027dfb92":"# Get TOP 15 1-gram\ncommon_words_unigram = get_top_n_grams(review_df['Review Text'], 1, 15)\n\n# Plot them\nplot_n_gram(common_words_unigram)","c9142e47":"# Get TOP 15 2-gram\ncommon_words_bigram = get_top_n_grams(review_df['Review Text'], 2, 15)\n\n# Plot them\nplot_n_gram(common_words_bigram)","53a135aa":"# Get TOP 15 3-gram\ncommon_words_trigram = get_top_n_grams(review_df['Review Text'], 3, 15)\n\n# Plot them\nplot_n_gram(common_words_trigram)","6f3ada68":"# Below is the function that will help doing all those analysis  \ndef show_box_plot_from_feature(feature_name, title):\n    y0 = review_df.loc[review_df['Department Name'] == 'Tops'][feature_name]\n    y1 = review_df.loc[review_df['Department Name'] == 'Dresses'][feature_name]\n    y2 = review_df.loc[review_df['Department Name'] == 'Bottoms'][feature_name]\n    y3 = review_df.loc[review_df['Department Name'] == 'Intimate'][feature_name]\n    y4 = review_df.loc[review_df['Department Name'] == 'Jackets'][feature_name]\n    y5 = review_df.loc[review_df['Department Name'] == 'Trend'][feature_name]\n    \n    trace0 = go.Box(\n    y=y0,\n    name = 'Tops',\n    )\n    trace1 = go.Box(\n        y=y1,\n        name = 'Dresses',\n    )\n    trace2 = go.Box(\n        y=y2,\n        name = 'Bottoms',\n    )\n    trace3 = go.Box(\n        y=y3,\n        name = 'Intimate',\n    )\n    trace4 = go.Box(\n        y=y4,\n        name = 'Jackets',\n    )\n    trace5 = go.Box(\n        y=y5,\n        name = 'Trend',\n    )\n\n    data = [trace0, trace1, trace2, trace3, trace4, trace5]\n    layout = go.Layout(\n        title = title\n    )\n    \n    fig = go.Figure(data=data,layout=layout)\n    iplot(fig)","26400310":"show_box_plot_from_feature('review_length', \"Review length Boxplot of Department Name\")","768e3af5":"show_box_plot_from_feature('polarity', \"Sentiment polarity Boxplot of Department Name\")","7fae73ea":"show_box_plot_from_feature('Rating', \"Rating Boxplot of Department Name\")","551d2035":"# The function below will be used to do the analysis    \ndef distribution_by_recommendations(feature, title):\n    axis1 = review_df.loc[review_df['Recommended IND'] == 1, feature]\n    axis0 = review_df.loc[review_df['Recommended IND'] == 0, feature]\n\n    trace1 = go.Histogram(\n        x=axis0, name='Not recommended',\n        opacity=0.75\n    )\n    trace2 = go.Histogram(\n        x=axis1, name = 'Recommended',\n        opacity=0.75\n    )\n\n    data = [trace1, trace2]\n    layout = go.Layout(barmode='overlay', title=title)\n    fig = go.Figure(data=data, layout=layout)\n\n    iplot(fig)","b095c07d":"distribution_by_recommendations('polarity', 'Distribution of Sentiment polarity of reviews based on Recommendation')","6c4d62fa":"distribution_by_recommendations('Rating', 'Distribution of Sentiment polarity of reviews based on Recommendation')","7acc9233":"distribution_by_recommendations('review_length', 'Distribution of review length based on Recommendation')","d52055c4":"# below is the function to plot that jointplot. \ndef joint_plot_of_features(feature1, feature2):\n    trace1 = go.Scatter(\n    x=review_df[feature1], y=review_df[feature2], mode='markers', name='points',\n    marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n    )\n    trace2 = go.Histogram2dContour(\n        x=review_df[feature1], y=review_df[feature2], name='density', ncontours=20,\n        colorscale='Hot', reversescale=True, showscale=False\n    )\n    trace3 = go.Histogram(\n        x=review_df[feature1], name=feature1+' density',\n        marker=dict(color='rgb(102,0,0)'),\n        yaxis='y2'\n    )\n    trace4 = go.Histogram(\n        y=review_df[feature2], name= feature2+' density', marker=dict(color='rgb(102,0,0)'),\n        xaxis='x2'\n    )\n    data = [trace1, trace2, trace3, trace4]\n\n    layout = go.Layout(\n        showlegend=False,\n        autosize=False,\n        width=600,\n        height=550,\n        xaxis=dict(\n            domain=[0, 0.85],\n            showgrid=False,\n            zeroline=False\n        ),\n        yaxis=dict(\n            domain=[0, 0.85],\n            showgrid=False,\n            zeroline=False\n        ),\n        margin=dict(\n            t=50\n        ),\n        hovermode='closest',\n        bargap=0,\n        xaxis2=dict(\n            domain=[0.85, 1],\n            showgrid=False,\n            zeroline=False\n        ),\n        yaxis2=dict(\n            domain=[0.85, 1],\n            showgrid=False,\n            zeroline=False\n        )\n    )\n\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)","280097f6":"joint_plot_of_features('Rating', 'polarity')","3a249c13":"joint_plot_of_features('Age', 'polarity')","d7f15d32":"#### c - Feature: Division Name","a6f36444":"#### a - Feature: Class Name ","73b5dd10":"#### b- age and sentiment polarity","29f6f512":"#### b- Sentiment polarity by department   ","2649a44c":"#### c- Change Review Text values:   \nWe are going to replace the **Review Text** column values by the preprocessed value in order to make them ready for further analyzis. ","8570fb21":"**observation:** All the department have the same median rate exept **Trend** department.  ","1b23fd8a":"#### a - Feature: polarity","a019280f":"### Import modules","defc30f9":"## Start preprocessing  \n\n#### a- Features summary and understanding using pandas_profiling\nWe will be using the **pandas_profiling** library for visualization. Instead of trying to visualize our features one by one with pandas, **pandas_profiling** gives a HTML report about all the variables of our data set. ","6552dd85":"**observation:** a great majority of reviewers wrote a review of more than 500 words. ","df2c3ca0":"#### c- Distribution of review lengths by recommendations","98fc80a7":"# Load the data set for preprocessing","faadafe9":"**observation:** **General** division has the most number of review, and **Initmates** has the least number of review.  ","236f7a90":"#### e - Feature: Review Word count","209eb664":"**observation:** The department which has the most reviews is **Tops** and, **Trend** is the least one.  ","a7ba33cd":"## 2. Bivariate analysis  \nBivariate analysis is similar to univariate analysis. In **bivariate**  expression, **bi** means **two**, so in other hand, the data set is considered to have only two variables. This technic is used to determine if there is a relationship between two sets of features\/variables. \nThere are several options for describing data with univariate analysis, such as: \n\n- Distribution of data   \n- Bar plots  \n- Boxplot","2a7adff0":"#### d.1 Let take the TOP 15 for unigram - bigram - trigram.  ","eb029184":"#### c - Feature: Age","5edb9a7e":"**observation:** The type of clothes that has the most reviews are **Dresses**, **Knits**, **Blouses** and the clothes least represented are **Chemises**, **Casual botto**","2571d1de":"**observation:** we notice that recommended reviews have a higher rating than those from not ","d441bbd7":"##### d.1.1 Unigram ","755742f1":"##### Observation from pandas_profiling and previous review text output: \n\n<ol>\nThe previous results from the pandas_profiling output and the result of the **review text** tell us more about the preprocessing actions to perform. So, we are going to : \n    <li>Remove the **Title** column because it contains a lot (3810) of missing values <\/li>\n    <li>Remove the rows (observations) where the **Review Text** is missing (NaN). We will not remove this column because the missing information is not as much as **Title**<\/li>\n    <li>Clean **Review Text** column to replace abreviations, remove stop words, put words in lowercase, etcetera<\/li>\n    <li>Create new feature for the length of the review<\/li>\n    <li>Create new feature for the word count of the review<\/li>\n    <li>Calculate sentiment polarity which lies in [-1, 1] where -1 means negative polarity and 1 means positive polarity. We will be doing that with TextBlob to do this task<\/li>\n<\/ol>","c4edebd6":"**Observation:** Most of the reviewers are in their 30s to 40s. And the most representative of them are those aged from 38 to 39. ","7be281ae":"# Exploratory Data Analysis with plotly  \nWe are going to use the **plotly** library for our visualization. This is more dynamic than static tools like **pandas**, **seaborn** etcetera ...  \n<ol>\nIn this part, we will perform three technics of analysis for some features: \n    <li>Univariate analysis<\/li>\n    <li>Bivariate analysis<\/li>\n<\/ol>","b5bf040a":"##### d.1.3 Trigram ","77fa133d":"# COMBINATION OF NATURAL LANGUAGE PROCESSING, EXPLORATORY DATA ANALYSIS: WAY TO GENERATE INSIGHT IN A INTUITIVE WAY  \n\n### Introduction\nIn this kernel, we use the women e'commerce clothing review dataset, and try to explore, visualize and understand as much as we can. We will be using plotly and bokeh visualization libraries. We are going to both explore text data and also visualize numeric and categorical features. ","4ee43593":"**observation:** we notice that the median value of **Top** and **Intimate**  are relatively lower than the other departments.  ","46449250":"### 2D Density jointplot of two features   \n","160f26e7":"#### b- Understanding of the output  \nAccording to the output, our data set contains: \n\n- 11 variables \/ features  \/ columns  ==> 5 of them are numeric, 5 are categorical, 1 is boolean. \n- 23486 observations \/ rows   \n\nIn the warning **field**, we can see that:  \n\n- Title variable has 16.2% of missing values.  \n- Review Text has also missing values, but not as much as Title variable.  \n\nHere are just some few details about the output of the library. There others are self explanatory.  \n","68f6fb28":"## End of preprocessing","75ec051e":"**Observation:** We can notice that the ratings are in align with the polarity score, most of the customers have given a rate of 5 and 4, a very few rate of 1 and 2. ","ccc31961":"**Remark:** As we can see, our preprocessing process for the **Review Text** performed very well.  ","ddc96c93":"#### a- sentiment polarity and rating","d4d0a690":"#### b- Distribution of ratings by recommendations","8da88e03":"### Histograms \nA histogram is a type of grapph that provides a visual interpretation of **numerical data** by indicating the number of data points that lie within a range of values. These ranges of values are called **bins** or **classes**, and the frequency of the data that falls in each class is depicted by the use of a bar. The higher that bar is, the greater the frequency of data values in that bin. We will only be analyzing **numerical features**. As given in the overview of **pandas_profiling** output, there are five of them, that means we are going to plot the histogram for 5 + 2 (our two new features: polarity, word_count) features. \n","59f19d5a":"### Bar plot \nA bar plot is like a histogram, but used for **categorical data** in order to compare diffente categories of data.  \nBelow is a nice comparison between Histogram and Barplot.  \n![image.png](attachment:image.png)","7a05f7dd":"**observation:** we notice that the reviews that have higher polarity value are more likely to be recommended.  ","7700e3d5":"<ol>\n**Remark**: We can see that our new columns ('review_length', 'word_count', 'polarity') have been added to the set of initial columns.  \nBelow, we are going to preview if the sentiment polarity score works. To do so, we are going to check three cases: \n    <li>Select three reviews with the highest sentiment polarity (polarity = 1)<\/li>\n    <li>Select three reviews with the most neutral sentiment polarity (polarity = 0)<\/li>\n    <li>Select three reviews with the most negative sentiment polarity (polarity = -1)<\/li>\n<\/ol>","0b607479":"**observation:** There were a quite number of people who like to leave a very long review. ","e7446f2f":"#### d - Feature: Review Text   \nThis feature is not as other categorical features, because it contains text data. To be able to visualize barplot, we need to perform some operations like:  \n\n- N-gram extractions. Those N-grams are used to describe the number of words used as observation points. \nIt is also important to know that :  \n    - unigram means singly-worded  \n    - bigram means 2-worded phrase \n    - trigram means 3-worded phrase. \n\nTo perform that, we will use **CountVectorizer** function from **scikit-learn**.   ","5149b8ed":"**observation:** we notice that recommended reviews tend to be lengthier than those of not recommended reviews.","9fa01dc5":"#### a - Feature: Rating**","b80ed990":"### Distribution of a features by recommendations","f34096a2":"#### a- Review length by department    ","06d7e6e6":"### Boxplot  \nThis analysis give more statistical informations about the features we are analysing. It also show the corrupted values (if they exist) related to the feature. Regarding the statistical informations about quantiles (Q1, Q2, Q3), where\n\nQ1 ==> 25th percentile  \nQ2 ==> median value\/50th percentile  \nQ3 ==> 75th percentile  ","e546b9ce":"**observation:** We can notice that:  \n\n- all the department have reached a highest sentiment polarity, exept **Trend** department.  \n- **Trend** department is also the only one having the lowest median value.  \n- **Tops**  department is the only one having a very bad sentiment polarity.  ","0d1c00a0":"#### d - Feature: Review_length","da047427":"<ul>\n**observation result of preprocessing:** We can notie that the action of preprocessing performed well, because in: \n    <li>ACTION N\u00b01: we performed the decontraction action, which decontracted all the contracted expressions. For example **wasn't** became **was not**, **you're** became **you are** <\/li>\n    <li>ACTION N\u00b02: we applied the final preprocessing action which removed stop words, punctuations, etcetera<\/li>\n<\/ul>","ca72bf83":"## 1. Univariate analysis  \nUnivariate analysis is the simplest form of analyzing data. In **univariate**  expression, **uni** means **one**, so in other hand, the data set is considered to have only one variable. It does not deal with causes or relationships (unlike regression), and it's main goal is to describe. So it takes data, summarizes it, and finds patterns in that data.  \nThere are several options for describing data with univariate analysis, such as: \n\n- Histogram:   \n- Bar plots  \n- Boxplot","cb7ee4cb":"#### a- sentiment polarity of reviews ","a9bc7b6a":"**Observation:** We notice that most of the sentiment polarities are greater than zero, and the most frequent sentiment polarity lies in [0.175 - 0.225]. Than means most of women gave a positive review after buying their clothes online.  ","cf73e5ff":"#### b - Feature: Department Name","ba9e8e55":"#### c- Rating by department   ","7a92af89":"##### d.1.2 Bigram "}}