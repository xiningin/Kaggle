{"cell_type":{"a1b00305":"code","c12347cb":"code","f503591f":"code","0ee8551b":"code","f8e5f6a8":"code","ef8296be":"code","b7eefa27":"code","b441b4ed":"code","4d578887":"code","f8870849":"code","2ff70390":"code","b1d16684":"code","407ffec4":"code","2d22e959":"code","977f9646":"code","aa060cf8":"code","cf96b914":"code","cef28cfe":"code","9d2ca612":"code","fb0fc22d":"code","1630bde2":"code","2aeeac5f":"code","5260a7a1":"code","cc1ff099":"code","8858d145":"code","c4818d8d":"code","efe2822b":"code","ddd50b7e":"code","6808899a":"code","130209a1":"code","10648a11":"code","c5de7b77":"code","8177e5f2":"code","9c36996d":"code","a2568ff8":"code","cf861cb2":"code","1efb308c":"code","138de472":"code","e826ff91":"code","f64c1653":"code","4f645ee4":"code","ebb7a54f":"code","4093ee52":"code","fe783e03":"code","2df97a8e":"code","b5735bb4":"code","c1ab95ca":"code","a68106e1":"code","faea3554":"code","888c261a":"code","e686a49c":"code","b35af413":"code","74a299c7":"code","f048dd1b":"code","9e6a9001":"code","f72c9677":"code","6112f932":"code","df2d068f":"code","703a7cca":"code","11298435":"code","42a93953":"code","f6b73414":"code","9b1fbe74":"code","883996dd":"code","b418b628":"code","8460e18c":"code","baa3139d":"code","94c84364":"markdown","9d79cac8":"markdown","568d3c16":"markdown","04c45a44":"markdown","17497335":"markdown","e4f0aa7f":"markdown","9e0ae8e7":"markdown","8cade837":"markdown","885b75d6":"markdown","929d698e":"markdown","6368e6cf":"markdown","8f169a86":"markdown","6f4eb912":"markdown","cf3c3ca4":"markdown","88c2fe9e":"markdown","22e9fb57":"markdown","e5303711":"markdown","714e3f18":"markdown","2286824e":"markdown","054bed7d":"markdown","8f8ad192":"markdown","741f698d":"markdown","ac2250c6":"markdown","31d71195":"markdown","8d6206d8":"markdown","960e6c01":"markdown","a6db32c2":"markdown","c232c554":"markdown","e29fe237":"markdown","9e943894":"markdown","df23e1ab":"markdown","7106fb9b":"markdown","45dbadd5":"markdown","0123bf61":"markdown","a5aa78b4":"markdown","4a9e61d2":"markdown","340386ce":"markdown","77bd6571":"markdown","b1020ffb":"markdown","7ad314e0":"markdown","1c4d776c":"markdown","b51de5b6":"markdown","d44676b4":"markdown","5c35c89c":"markdown","e5a09f31":"markdown","0c43a803":"markdown","f61100e4":"markdown","40ea12eb":"markdown","8b9202b6":"markdown","e00be48f":"markdown","924060d7":"markdown","9863387f":"markdown","f3e8f10e":"markdown","c11066b5":"markdown","ff0c5714":"markdown","373b7dbc":"markdown"},"source":{"a1b00305":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c12347cb":"#importing libraries and shuffling the data\ndf = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')\ndf = df.sample(frac = 1,random_state = 3)\ndf.head()","f503591f":"#Finding the unqiue values\ndf['origin'].unique()\n\n#here we see there is '?' here.","0ee8551b":"#To check the shape of Dataframe\ndf.shape","f8e5f6a8":"#To check the data-type of the dataframe\ndf.info()","ef8296be":"#To check descriptive statistics of data frame\ndf.describe()","b7eefa27":"df.isna().sum()*100\/398","b441b4ed":"#Removing ?, replacing with mean value and conver the datatype to integer\ndf['horsepower'] = df['horsepower'].replace('?',np.nan)\ndf['horsepower'] = df['horsepower'].astype(float)\ndf['horsepower'] = df['horsepower'].replace(np.nan,df['horsepower'].mean())\ndf['horsepower'] = df['horsepower'].astype(int)\n","4d578887":"df['car name'].value_counts().nlargest(15).plot(kind='bar', figsize=(15,5))\nplt.title(\"Number of vehicles by car brand name\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('car name');","f8870849":"df['mpg'].hist(bins = 5,color = 'red')\nplt.title(\"Miles per gallon of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Miles per gallon');","2ff70390":"df['cylinders'].hist(bins = 5,color = 'red')\nplt.title(\"Number of Cylinders in a vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Numer of Cylinder');","b1d16684":"df['displacement'].hist(bins = 5,color = 'red')\nplt.title(\"Displacement of the vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Displacement');","407ffec4":"df['weight'].hist(bins = 5,color = 'red')\nplt.title(\"Weight of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Weight');","2d22e959":"df['acceleration'].hist(bins = 5,color = 'red')\nplt.title(\"Acceleration of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Acceleration');","977f9646":"df['model year'].hist(bins = 5,color = 'red')\nplt.title(\"Model year of vehicle\")\nplt.ylabel('Number of vehicles')\nplt.xlabel('Year');","aa060cf8":"df[['mpg','cylinders','displacement','weight','acceleration','model year','origin']].hist(figsize=(10,8),bins=6,color='Y')\nplt.tight_layout()\nplt.show()","cf96b914":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"mpg\", y=\"weight\", data=df)","cef28cfe":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"mpg\", y=\"displacement\", data=df)","9d2ca612":"plt.rcParams['figure.figsize']=(23,10)\nax = sns.boxplot(x=\"horsepower\", y=\"weight\", data=df)","fb0fc22d":"sns.scatterplot(x = 'weight',y = 'acceleration',data = df)","1630bde2":"corr = df.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(13,7))\na = sns.heatmap(corr, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","2aeeac5f":"#import library\nimport statsmodels.api as sm","5260a7a1":"#drop car name\ndf = df.drop('car name',axis = 1)","cc1ff099":"#Taking target variable and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","8858d145":"#importing vif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif","c4818d8d":"pd.DataFrame([vif(Xc.values,i) for i in range(Xc.shape[1])],index = Xc.columns,columns=['VIF'])","efe2822b":"#Fitting model\nmodel = sm.OLS(y,Xc).fit()\nmodel.summary()","ddd50b7e":"#Dropping cylinders,horsepower,acceleration and fiting into model\nXc = Xc.drop(['cylinders','horsepower','acceleration'],axis = 1)\nmodel = sm.OLS(y,Xc).fit()\nmodel.summary()","6808899a":"#importing library and creating distplot\nfrom scipy.stats import norm\nnorm.fit(model.resid)\nsns.distplot(model.resid,fit = norm)","130209a1":"#importing library and creating probplot\nimport scipy.stats as st\nst.probplot(model.resid,plot = plt)\nplt.show()","10648a11":"#Jarque Bera Test to check normality\nst.jarque_bera(model.resid)","c5de7b77":"#Applying Transformation\nly = np.log(y)","8177e5f2":"#Building model\nmodel = sm.OLS(ly,Xc).fit()\nmodel.summary()","9c36996d":"#Again creating distplot after applying log\nfrom scipy.stats import norm\nnorm.fit(model.resid)\nsns.distplot(model.resid,fit = norm)","a2568ff8":"#Again creating probplot after applying log\nimport scipy.stats as st\nst.probplot(model.resid,plot = plt)\nplt.show()","cf861cb2":"#Again performing JB Test\nst.jarque_bera(model.resid)","1efb308c":"#Splitting X and y and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","138de472":"model = sm.OLS(y,Xc).fit()\ny_pred = model.predict(Xc)\nresids = model.resid\n\nsns.regplot(x = y_pred,y = resids,lowess=True,line_kws={'color':'red'})","e826ff91":"#Goldfeld Test for Checking Homoscedasticity\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\n\nname = ['F-Statistics','p-value']\ntest = sms.het_goldfeldquandt(model.resid,model.model.exog)\nprint(lzip(name,test))","f64c1653":"#Splitting X and y and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","4f645ee4":"#Fitiing in model and applying autocorrleation\nmodel = sm.OLS(y,Xc).fit()\nimport statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(model.resid,lags = 30,alpha = 0.05)","ebb7a54f":"model.summary()","4093ee52":"#Splitting X and y and adding constant\ny = df['mpg']\nX = df.drop('mpg',axis = 1)\nXc = sm.add_constant(X)","fe783e03":"#Fitting to model and applying linearity\nmodel = sm.OLS(y,Xc).fit()\ny_pred = model.predict(Xc)\nresids = model.resid\n\nsns.regplot(x = y_pred,y = resids,lowess=True,line_kws={'color':'red'})","2df97a8e":"#Fitting to model and applying linearity\nmodel = sm.OLS(y,Xc).fit()\ny_pred = model.predict(Xc)\nresids = model.resid\n\nsns.regplot(x = y,y = y_pred,lowess=True,line_kws={'color':'red'})","b5735bb4":"#Rainbow test for Linearity\nimport statsmodels.api as sm\nsm.stats.diagnostic.linear_rainbow(res = model,frac = 0.5)","c1ab95ca":"#Applying in target variables\ny = df['mpg']\nX = df.drop('mpg',axis = 1)","a68106e1":"#Splitting the dataframe in 70:30(train and test)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=3)","faea3554":"#printing the shape of train and test\nprint(X_train.shape,X_test.shape)","888c261a":"#importing the library and building the model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\ny_test_pred = lr.predict(X_test)\ny_train_pred = lr.predict(X_train)\n\nprint('r-square of Train :',r2_score(y_train,y_train_pred))\nprint('rmse for Train: ',np.sqrt(mean_squared_error(y_train,y_train_pred)))\n\nprint('\\n')\nprint('r-square of Test :',r2_score(y_test,y_test_pred))\nprint('rmse for Test: ',np.sqrt(mean_squared_error(y_test,y_test_pred)))\n","e686a49c":"#importing libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import r2_score,mean_squared_error","b35af413":"#Fitting lr to rfe\nlr = LinearRegression()\nrfe = RFE(lr,n_features_to_select = 8)\nrfe.fit(X,y)","74a299c7":"#Checking the rfe support function\nrfe.support_","f048dd1b":"pd.DataFrame(rfe.ranking_,index = X.columns,columns=['SELECT'])","9e6a9001":"#Applying train test split method and splitting the data in 70:30 ratio\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=3)","f72c9677":"no_of_cols = X_train.shape[1]\nr2score = []\nrmse = []\n\nlr = LinearRegression()  #estimator\nfor i in range(no_of_cols):\n    rfe = RFE(lr,n_features_to_select=i+1)\n    rfe.fit(X_train,y_train)\n    y_test_pred = rfe.predict(X_test)\n    \n    #for r2score\n    r2 = r2_score(y_test,y_test_pred)\n    r2score.append(r2)\n    \n    #for rmse\n    rms = np.sqrt(mean_squared_error(y_test,y_test_pred))\n    rmse.append(rms)","6112f932":"#plot for r2score\nplt.plot(range(1,8),r2score)","df2d068f":"#printing r2score\nr2score","703a7cca":"#plot for rmse\nplt.plot(range(1,8),rmse)","11298435":"#printing rmse\nrmse","42a93953":"from sklearn.model_selection import KFold,GridSearchCV\n\nparams = {'n_features_to_select':list(range(1,8))}\nlr = LinearRegression()\nrfe = RFE(lr)  #estimator\n\nkf = KFold(n_splits=3,random_state=3)\n\ngsearch = GridSearchCV(rfe,param_grid=params,scoring = 'r2',cv = kf,return_train_score=True)","f6b73414":"#to find the best parameter for our data\ngsearch.get_params","9b1fbe74":"#importing the SFS\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nsfs1 = sfs(lr,k_features=7,scoring='r2',cv = 3,verbose = 2)\nsfs1.fit(X,y)","883996dd":"#Making the dataframe as subset and transforming it\nsf = pd.DataFrame(sfs1.subsets_).T\nsf","b418b628":"plt.plot(sf['avg_score'])","8460e18c":"cols = list(sfs1.k_feature_names_)\ncols","baa3139d":"X_train, X_test, y_train, y_test = train_test_split(X[cols], y, test_size=0.3, random_state=3)\n\nlr.fit(X_train, y_train)\ny_test_pred = lr.predict(X_test)\n\nplt.scatter(y_test, y_test_pred)\nplt.plot(y_test, y_test,'r')","94c84364":"Here wee see that cylindes,horsepower,acceleration all three are not significant they are above 0.05 so we will drop it.","9d79cac8":"# Test of Assumptions","568d3c16":"# Forward selection to decide the best number of features to keep in the model(Step Forward Selection)\n","04c45a44":"We can see that all the point are scattered close to the line.","17497335":"## Missing values","e4f0aa7f":"# 4.Test of Linearity","9e0ae8e7":"# Histogram for displacement","8cade837":"# 3.Test of Autocorrelation","885b75d6":"**We can see that as the weight of the car increases, the mileage decreases which is obvious because they are inversely proportional to each other**","929d698e":"p-value very close to zero so reject null hypothesis. Hence JB Test indicate that residuals are not normal.","6368e6cf":"**Maximum number of cars are having dispalcement between 70 to 140**","8f169a86":"# 1. Recursive Feature Elimination","6f4eb912":"# Histogram for weight","cf3c3ca4":"**We can say that most of the cars were made during the year 1970-72, 1975-77 and 1980-82 **","88c2fe9e":"**We see that as the displacement increases,mileage decreases.**","22e9fb57":"# Building Linear Regression Model","e5303711":"# Box plot btw mpg and weight","714e3f18":"# 1. Test of Normality","2286824e":"There is a 4% difference in train and test.","054bed7d":"Ho: residuals exhibit linearity\nH1: residuals exhibit non-linearity\n\nBased on p-value , we say that residuals exhibhit linearity OR there is no non-linearity.","8f8ad192":"# Histogram for cylinders","741f698d":"Ho: Data is normal\n\nH1: Data is not normal","ac2250c6":"Ho: Variance of residuals is constant across the range of data.\nH1: Variance of residuals is not constant across the range of data.\n    \nSince p-value is more than 0.05 we will accept  Ho to conclude that residuals is constatnt across the range of data.","31d71195":"**Maximum number of cars are having cylinder between 4 to 5**","8d6206d8":"# Feature Selection","960e6c01":"# Bivariate Analysis","a6db32c2":"# Step 1 : Importing and Understanding Data","c232c554":"**Max weight of car are between 1600 to 3000  **","e29fe237":"p-value again closed to zero, so reject Ho. Hence Residuals are not normal","9e943894":"**We can clearly see that as the weight increases,the horsepower increases.Otherwise car with heavy weight won't run without greater horsepower. So they are directly proportional**","df23e1ab":"# Building Statistical Model","7106fb9b":"# Correlation Analysis","45dbadd5":"# Histogram for mpg\n","0123bf61":"# Histogram for acceleration","a5aa78b4":"here wee se its almost normal.","4a9e61d2":"### Performing Linear Regression and calculating error terms","340386ce":"**It is showing that lesser the weight of car,more is the acceleration.**","77bd6571":"# Hyperparamter Tuning to find the optimal number of features to keep","b1020ffb":"# 2.Test of Homoscedasticity","7ad314e0":"# Box plot btw horsepower and weight","1c4d776c":"No mising values","b51de5b6":"# Exploratory Data Analysis","d44676b4":"# Cleaning The Data","5c35c89c":"### Splitting data into training and testing sets","e5a09f31":"# Univariate Analysis","0c43a803":" # Box plot btw mpg and displacement","f61100e4":"# Scatter plot for weight of car and acceleration","40ea12eb":"Durbin-Watson statistic for any model will be between 0 and 4. To conclude that there is auto-correlation DW-statistic should be around 2. The DW statistic for the model is 2.028 and this indicate moderate level of autocorrelation.","8b9202b6":"# Removing ? from hp and converting its data type to INT","e00be48f":"? removed and converted into int type","924060d7":"This will give 8 best features with rank 1 and rest features with subsequent ranks.","9863387f":"# Bar Plot for car name","f3e8f10e":"# Histogram for model year","c11066b5":"**Ford Pinto is the car name which has most number of vehicles and Ford Marverick holds the second poisition**","ff0c5714":"**General acceleration of cars are between 15.0 to 18.0**","373b7dbc":"The pattern expected when residuals are plotted against y_pred is a horizontal line around zero(0).\nThe pattern in the graph suggests that model design need to improve(we may need to add square and multiplicative terms to the model).\n\nThe expected pattern when y and y_pred is plotted is a line passing through origin with a slope of 45 degrees.\n\n((#if model is perfect then price = price_predicted))\n\nHowever the model shows a pattern away from expected."}}