{"cell_type":{"ee6c2167":"code","d5e6f778":"code","0b56d9d4":"code","e47675fd":"code","6adbf763":"code","4b1c33f6":"code","47d30ae0":"code","f402d349":"code","801f3ae9":"code","b8a6feb8":"code","3ce8a037":"code","f1bf4c85":"code","fddea2a2":"code","8c3e1591":"code","109d2179":"code","58d4150d":"code","d2fbdf71":"code","803eed5a":"code","54a7ed47":"code","7cd22b13":"code","f7eae01f":"code","dc8bd132":"code","c4f82417":"code","45393982":"code","ebdff7fd":"code","f6eebf44":"code","9762cfe1":"code","bc3b0fab":"code","74140df1":"code","600222fb":"code","9e7335a3":"code","7d666676":"code","31909c9f":"markdown","6a1af6a2":"markdown","29dfc41b":"markdown","27b1e846":"markdown","faa9d6c3":"markdown","fad2c937":"markdown","7dcdf8fa":"markdown","6ceb4d09":"markdown","478970ae":"markdown","8943c66a":"markdown","a25a8722":"markdown","89dd11ea":"markdown","fe2a3494":"markdown","43cbc9d1":"markdown","be3eacac":"markdown","1d11c6bb":"markdown","5d4d13dc":"markdown","0a3f1187":"markdown","d50e0b2b":"markdown","6b0c6ba0":"markdown","d37af5b0":"markdown","ae728a51":"markdown"},"source":{"ee6c2167":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5e6f778":"# Warning Libraries :\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Scientific and Data Manipulation Libraries :\nimport pandas as pd\nimport numpy as np\nimport math\nimport gc\nimport os\nimport category_encoders as ce\n\n\n# ML Libraries :\nfrom sklearn.preprocessing            import LabelEncoder, OneHotEncoder \nfrom sklearn.preprocessing            import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\nfrom sklearn.model_selection          import KFold, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.linear_model             import LogisticRegression\nfrom sklearn                          import tree\nfrom sklearn.ensemble                 import RandomForestClassifier\nfrom sklearn.metrics                  import accuracy_score\nfrom sklearn.metrics                  import f1_score,precision_score\n#from sklearn.metrics                 import jaccard_similarity_score, jaccard_score  \n\n# Boosting Algorithms :\nfrom xgboost                          import XGBClassifier\nfrom lightgbm                         import LGBMClassifier\n\n# Data Visualization Libraries :\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px","0b56d9d4":"np.random.seed(0)","e47675fd":"# Import the data\nstate_lst=['FL']\ndf = pd.read_csv('\/kaggle\/input\/us-accidents\/US_Accidents_Dec20_Updated.csv')\ndf = df[df.State.isin(state_lst)]\ndf.info()\n","6adbf763":"# Display Descriptive Statistics of data :\n\ndf.describe()","4b1c33f6":"# Display No of Unqiue Values and Actual Unique Values :\n\ndef display_unique(data):\n    for column in data.columns :\n        \n        print(\"No of Unique Values in \"+column+\" Column are : \"+str(data[column].nunique()))\n        print(\"Actual Unique Values in \"+column+\" Column are : \"+str(data[column].sort_values(ascending=True,na_position='last').unique() ))\n        print(\"\")\n        \ndisplay_unique(df)","47d30ae0":"# Cast Start_Time to datetime \n\ndf[\"Start_Time\"] = pd.to_datetime(df[\"Start_Time\"])\n\n# Extract year, month, weekday and day\ndf[\"Year\"] = df[\"Start_Time\"].dt.year\ndf[\"Month\"] = df[\"Start_Time\"].dt.month\ndf[\"Weekday\"] = df[\"Start_Time\"].dt.weekday\ndf[\"Day\"] = df[\"Start_Time\"].dt.day\n\n# Extract hour and minute\ndf[\"Hour\"] = df[\"Start_Time\"].dt.hour\ndf[\"Minute\"] = df[\"Start_Time\"].dt.minute\n\ndf.head()","f402d349":"fig = px.parallel_categories(df[[\"Side\", \"City\", \"Weekday\", \"Day\",\"Hour\",\"Minute\", \"Civil_Twilight\",\n                                   \"Severity\"]], \n                             color=\"Severity\", \n                             color_continuous_scale=px.colors.sequential.Aggrnyl  )\nfig.show()","801f3ae9":"fig, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(df.corr(), annot = True);\nplt.show()","b8a6feb8":"features_to_drop = [\"ID\", \"Start_Time\", \"End_Time\", \"End_Lat\", \"End_Lng\",\"Description\", \"Number\", \"Street\", \"County\", \"State\", \"Zipcode\",\n                    \"Country\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\", \"Wind_Chill(F)\", \"Turning_Loop\", \"Sunrise_Sunset\", \"Nautical_Twilight\", \"Astronomical_Twilight\",\"City\",\"Civil_Twilight\",\"Bump\",\"Give_Way\",\"No_Exit\",\"Roundabout\",\"Traffic_Calming\"]\ndf=df.drop(features_to_drop, axis=1)\ndf.head()","3ce8a037":"# Python Method 4 : Removes Data Duplicates while Retaining the First one - Similar to SQL DISTINCT :\n\ndef remove_duplicate(data):\n    \n    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n    data.drop_duplicates(keep=\"first\", inplace=True) \n    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n    \n    return \"Checked Duplicates\"\n\n# Remove Duplicates from data :\n\nremove_duplicate(df)\n","f1bf4c85":"#train Data\nunique_weather = df[\"Weather_Condition\"].unique()\n\nprint(len(unique_weather))\nprint(unique_weather)\n\n","fddea2a2":"df.loc[df[\"Weather_Condition\"].str.contains(\"Thunder|T-Storm\", na=False), \"Weather_Condition\"] = \"Thunderstorm\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Snow|Sleet|Wintry\", na=False), \"Weather_Condition\"] = \"Snow\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Rain|Drizzle|Shower\", na=False), \"Weather_Condition\"] = \"Rain\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Wind|Squalls\", na=False), \"Weather_Condition\"] = \"Windy\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Hail|Pellets\", na=False), \"Weather_Condition\"] = \"Hail\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Fair\", na=False), \"Weather_Condition\"] = \"Clear\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Cloud|Overcast\", na=False), \"Weather_Condition\"] = \"Cloudy\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Mist|Haze|Fog\", na=False), \"Weather_Condition\"] = \"Fog\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Sand|Dust\", na=False), \"Weather_Condition\"] = \"Sand\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"Smoke|Volcanic Ash\", na=False), \"Weather_Condition\"] = \"Smoke\"\ndf.loc[df[\"Weather_Condition\"].str.contains(\"N\/A Precipitation\", na=False), \"Weather_Condition\"] = np.nan\n\nprint(df[\"Weather_Condition\"].unique())","8c3e1591":"df[\"Wind_Direction\"].unique()\n","109d2179":"df.loc[df[\"Wind_Direction\"] == \"CALM\", \"Wind_Direction\"] = \"Calm\"\ndf.loc[df[\"Wind_Direction\"] == \"VAR\", \"Wind_Direction\"] = \"Variable\"\ndf.loc[df[\"Wind_Direction\"] == \"East\", \"Wind_Direction\"] = \"E\"\ndf.loc[df[\"Wind_Direction\"] == \"North\", \"Wind_Direction\"] = \"N\"\ndf.loc[df[\"Wind_Direction\"] == \"South\", \"Wind_Direction\"] = \"S\"\ndf.loc[df[\"Wind_Direction\"] == \"West\", \"Wind_Direction\"] = \"W\"\n\ndf[\"Wind_Direction\"] = df[\"Wind_Direction\"].map(lambda x : x if len(x) != 3 else x[1:], na_action=\"ignore\")\n\ndf[\"Wind_Direction\"].unique()","58d4150d":"# Display the Missing Values in Data :\n\nprint(\"Data : \")\ndisplay(df.isnull().sum())\n","d2fbdf71":"total_size=len(df)\n\ntrain_size=math.floor(0.66*total_size) \ndisplay\n#training dataset\ntrain=df.head(train_size)\n#test dataset\ntest=df.head(len(df) -train_size)\ndisplay('Total Size:',total_size)\ndisplay('Train Size:',train_size)\n\ndisplay('Train Head :',train)\ndisplay('Test Head :',test.head())","803eed5a":"X_train = train[['Side','Wind_Direction','Day','Month','Year','Hour']]\n\ny_train = train['Severity']\ny_train = y_train.to_frame()\n\nX_test = test[['Side','Wind_Direction','Day','Month','Year','Hour']]\ny_test = test['Severity']\ny_test = y_test.to_frame()\n","54a7ed47":"def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n    \n    if encoding_strategy == \"LabelEncoding\":\n        print(\"IF LabelEncoding\")\n        Encoder = LabelEncoder()\n        for column in encoding_columns :\n            print(\"column\",column )\n            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n        \n    elif encoding_strategy == \"OneHotEncoding\":\n        print(\"ELIF OneHotEncoding\")\n        encoding_data = pd.get_dummies(encoding_data)\n        \n    dtypes_list =['float64','float32','int64','int32']\n    encoding_data.astype( dtypes_list[0] ).dtypes\n    \n    return encoding_data","7cd22b13":"# Quote :\n# Applied One Hot Encoding - it will be applied to Object\/Categorical Columns Only :\n# It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. \n# Pandas offers a convenient function called \"get_dummies\" to get one-hot encodings.\n# Many machine learning algorithms cannot operate on label data directly. \n# They require all input variables and output variables to be numeric.\n# This means that categorical data must be converted to a numerical form.\n# a one-hot encoding can be applied to the integer representation. \n# This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n# - Jason Brownlee \n\ndata = [\"Red\",\"Blue\",\"Green\",\"Red\",\"Blue\",\"Blue\"] \n  \ndf = pd.DataFrame(data, columns = ['Color']) \n  \nprint(\"Before One Hot Encoding : \")\ndisplay(df)\nprint(\"\\nAfter One Hot Encoding : \")\ndisplay( pd.get_dummies(df) )","f7eae01f":"encoding_columns  = [ 'Side','Day', 'Week', 'Month', 'Hour' ]\nencoding_strategy = [ \"LabelEncoding\", \"OneHotEncoding\"]\n\nX_train_encode = data_encoding( encoding_strategy[1] , X_train , encoding_columns )\nX_test_encode =  data_encoding( encoding_strategy[1] , X_test  , encoding_columns )\n\n# Display Encoded Train and Test Features :\n\ndisplay(X_train_encode.head())\ndisplay(X_test_encode.head())","dc8bd132":"def data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n    \n    if    scaling_strategy ==\"RobustScaler\" :\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"StandardScaler\" :\n        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MinMaxScaler\" :\n        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n        \n    elif  scaling_strategy ==\"MaxAbsScaler\" :\n        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n        \n    else :  # If any other scaling send by mistake still perform Robust Scalar\n        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n    \n    return scaling_data","c4f82417":"# RobustScaler is better in handling Outliers :\n\nscaling_strategy = [\"RobustScaler\", \"StandardScaler\",\"MinMaxScaler\",\"MaxAbsScaler\"]\nX_train = data_scaling( scaling_strategy[0] , X_train_encode , X_train_encode.columns )\nX_test  = data_scaling( scaling_strategy [0] , X_test_encode  , X_test_encode.columns )\n\n# Display Scaled Train and Test Features :\n\ndisplay(X_train.head())\ndisplay(X_test.head())","45393982":"# Logistic regression with default setting.\n\nclf = LogisticRegression(max_iter=10000,random_state=42)\nclf.fit(X_train, y_train)\naccuracy_train = clf.score(X_train, y_train)\naccuracy_test = clf.score(X_test,y_test)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))","ebdff7fd":"# Calculate the f1 score\n\nlr_cal = clf.predict(X_test)\n\n# Calculate the f1 score\nf1_lr = f1_score(y_test, lr_cal, average='weighted') \nprint(\"F1 Score: %3.4f\" %(f1_lr))\n","f6eebf44":"# Training step, on X_train with y_train\ntree_clf = tree.DecisionTreeClassifier(min_samples_split = 5)\ntree_clf = tree_clf.fit(X_train,y_train)\n\ntree_accuracy_train = tree_clf.score(X_train, y_train)\ntree_accuracy_test = tree_clf.score(X_test,y_test)\nprint(\"Train Accuracy: %.1f%%\"% (tree_accuracy_train*100))\nprint(\"Test Accuracy: %.1f%%\"% (tree_accuracy_test*100))","9762cfe1":"tree_cal = tree_clf.predict(X_test)\n\n# Calculate the f1 score\nf1_tree = f1_score(y_test, tree_cal, average='weighted') \nprint(\"F1 Score: %3.4f\" %(f1_tree))\n\n","bc3b0fab":"%%time\n\nrf_clf=RandomForestClassifier(n_estimators=10)\nrf_clf.fit(X_train,y_train)\n\ntrain_pred =  rf_clf.predict(X_train)\ntest_pred =rf_clf.predict(X_test)\n\nrf_train_accuracy = accuracy_score(y_train, train_pred)\nrf_test_accuracy = accuracy_score(y_test, test_pred)\n\nprint(\"Train Accuracy: %.1f%%\"% (rf_train_accuracy*100))\nprint(\"Test Accuracy: %.1f%%\"% (rf_test_accuracy*100))","74140df1":"rf_cal = tree_clf.predict(X_test)\n\n# Calculate the f1 score\nf1_rf = f1_score(y_test, rf_cal, average='weighted') \nprint(\"F1 Score: %3.4f\" %(f1_rf))\n\n","600222fb":"xgb_clf = XGBClassifier(n_estimators=100)\n\nxgb_clf.fit(X_train,y_train)\n\n# predict the target on the train & test  dataset\npredict_train = xgb_clf.predict(X_train)\npredict_test = xgb_clf.predict(X_test)\n\n# Accuracy Score on train & test dataset\n\nxgb_accuracy_train = accuracy_score(y_train,predict_train)\nxgb_accuracy_test = accuracy_score(y_test,predict_test)\n\nprint('Train Accuracy: %.1f' %(xgb_accuracy_train*100) )\nprint('Test Accuracy:%.1f' %(xgb_accuracy_test*100))","9e7335a3":"xgb_cal = xgb_clf.predict(X_test)\n\n# Calculate the f1 score\nf1_xgb = f1_score(y_test, xgb_cal, average='weighted') \nprint(\"F1 Score: %3.4f\" %(f1_xgb))\n","7d666676":"# Report\ntrain_data=[(accuracy_train*100), (tree_accuracy_train*100), (rf_train_accuracy*100),(xgb_accuracy_train*100)]\ntest_data=[(accuracy_test*100), (tree_accuracy_test*100), (rf_test_accuracy*100),(xgb_accuracy_test*100)]\n\n\nF1_score = [f1_lr,f1_tree,f1_rf,f1_xgb]\n\n    \ndf = {'Algorithm': ['LogisticRegression','Decision Tree','Random Forest','XGBOOST'], \\\n     'Train Data':train_data,'Test Data':test_data,'F1-score': F1_score}\n\nReport = pd.DataFrame(data=df, columns=['Algorithm','Train Data', 'Test Data', 'F1-score'], index=None)\nReport","31909c9f":"## US Accidents - Applied Machine Learning","6a1af6a2":"Created a list to store the state, here I have included the state Florida-FL.Since it is alist we can add multiple states in it.\n\nLet's read the data from data source and check the attribite State, whether it contains the state Florida.And it provides the information of column names","29dfc41b":"**3. Random Forest**","27b1e846":"### Split Train and Test Data","faa9d6c3":"Let's check also the Wind_Direction field:","fad2c937":"### 5. Feature Engineering","7dcdf8fa":"### 1. Understand the Problem Statement & Import Packages and Datasets :","6ceb4d09":"**4. XGBoost**","478970ae":" ### 2. Import the dataset","8943c66a":"**Feature scaling: Split Train Data into Predictors(Independent) & Target(Dependent) :**","a25a8722":"To do so, we are going to replace them with a more generic description:","89dd11ea":"### 4. Check for Duplicate Rows from Data if present :","fe2a3494":"**2. Decision Tree**","43cbc9d1":"## Report\n\nCalculated the F1_score for above algorithms. The final report table of 4 algorithm as below,\n","be3eacac":"### 7. Create Baseline ML Model for Binary Classification Problem :","1d11c6bb":"### Feature selection:\n \n Here is the process of feature selection, in order to select the best features from which  our models can learn.\n\nFrom the observations made with the correlation matrix, we are going to drop some of the features.","5d4d13dc":"**Data Scaling : RobustScaler, StandardScaler, MinMaxScaler, MaxAbsScaler :**","0a3f1187":"### 3.Feature addition\n\nWe decided to decompose the Start_Time feature in year, month, day, weekday, hour and minute, in order to feed them to the models.","d50e0b2b":"If we analyze the weather conditions, we can see that there are lots of them, so it's better to reduce the number of unique conditions.","6b0c6ba0":"As we can see, we can group the values like we did with Weather_Condition:","d37af5b0":"**1. Logistic regression**","ae728a51":"### 6.Data Encoding : Label Encoding, OneHot Encoding"}}