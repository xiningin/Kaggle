{"cell_type":{"27ca1532":"code","9e66ecd1":"code","66aca4fa":"code","e27135f6":"code","8f9a6b8c":"code","6452749b":"code","d8f6a778":"code","841fdaec":"code","1b0227bb":"code","e31689cb":"code","7b8a827b":"code","e05afe0c":"code","7377e1ae":"markdown","6143134c":"markdown","79f41cd0":"markdown","1551edf1":"markdown","22b77b2f":"markdown","b16130cc":"markdown","82098d04":"markdown","bb38850b":"markdown","75b7df8c":"markdown","8bda95e9":"markdown"},"source":{"27ca1532":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nplt.style.use('ggplot')","9e66ecd1":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain['Dataset'] = 'train'\ntest['Dataset'] = 'test'\n\ny = train.SalePrice\ntrain.drop('SalePrice', axis=1, inplace=True)\n\ndf = pd.concat([train, test])\ndf.set_index(['Id', 'Dataset'], inplace=True)","66aca4fa":"missing = df.isnull().sum()\nmissing[missing > 0].index\ndf.drop(missing[missing > 0].index, axis=1, inplace=True)","e27135f6":"df.columns[df.isna().any()]","8f9a6b8c":"categorical = [f for f in df.columns if df.dtypes[f] == 'object']\n\nfor c in categorical:\n    df['cat_' + c] = LabelEncoder().fit_transform(df[c])\n    \ndf.drop(categorical, axis=1, inplace=True)\n\nprint(df.head())","6452749b":"from sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold(0.15)\nselector.fit(df)\n# get support returns True for columns with var > 0.15\nprint('Columns with variance lower than 0.15: ', df.columns[~selector.get_support()])\ndf.drop(df.columns[~selector.get_support()], axis=1, inplace=True)","d8f6a778":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=65)\n\ncategorical = [c for c in df.columns if c.startswith('cat_')]\ntmp = df[df.index.get_level_values(1) == 'train'].copy()\ntmp.loc[:, 'SalePrice'] = y.values\nf = pd.melt(tmp, id_vars=['SalePrice'], value_vars=categorical)\nplt.figure(dpi=100, figsize=(16,8))\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=4, sharex=False, sharey=False)\ng = g.map(boxplot, \"value\", \"SalePrice\")\nplt.show()","841fdaec":"to_binary = ['cat_PavedDrive', 'cat_HeatingQC', 'cat_HouseStyle', 'cat_LotConfig',\n             'cat_LotShape', 'cat_RoofMatl', 'cat_RoofStyle']\none_hot_encoding = ['cat_BldgType', 'cat_ExterQual', 'cat_ExterCond',\n                    'cat_SaleCondition', 'cat_Condition1', 'cat_LandContour',\n                    'cat_Foundation']\n\ndf.loc[:, 'cat_PavedDrive'] = np.where(\n    df.loc[:, 'cat_PavedDrive'].isin([0, 1]), 0, 1\n)\n\ndf.loc[:, 'cat_HeatingQC'] = np.where(\n    df.loc[:, 'cat_HeatingQC'].isin([1, 2, 3, 4]), 0, 1\n)\n\ndf.loc[:, 'cat_HouseStyle'] = np.where(\n    df.loc[:, 'cat_HouseStyle'].isin([0,1,4,6,7]), 0, 1\n)\n\ndf.loc[:, 'cat_LotConfig'] = np.where(\n    df.loc[:, 'cat_LotConfig'].isin([1, 2, 4]), 0, 1\n)\n\ndf.loc[:, 'cat_LotShape'] = np.where(\n    df.loc[:, 'cat_LotShape'].isin([0,1,3]), 0, 1\n)\n\ndf.loc[:, 'cat_RoofStyle'] = np.where(\n    df.loc[:, 'cat_RoofStyle'].isin([1,2,4,5]), 0, 1\n)\n\ndf.loc[:, 'cat_RoofMatl'] = np.where(\n    df.loc[:, 'cat_RoofMatl'].isin([0,2,3,4,5,6]), 0, 1\n)\n\nselector = VarianceThreshold(0.1)\nselector.fit(df)\ndf.drop(df.columns[~selector.get_support()], axis=1, inplace=True)\n\ndf.loc[:, 'cat_BldgType'] = df.loc[:, 'cat_BldgType'].map({\n    1: 0, 2: 0, 3: 0,\n    4: 1,\n    0: 2\n})\n\ndf.loc[:, 'cat_ExterCond'] = df.loc[:, 'cat_ExterCond'].map({\n    1: 0, 3: 0,\n    0: 1,\n    2: 2, 4: 2\n})\n\ndf.loc[:, 'cat_SaleCondition'] = df.loc[:, 'cat_SaleCondition'].map({\n    0: 0, 1: 0, 2: 0, 3: 0,\n    4: 1,\n    5: 2\n})\n\ndf.loc[:, 'cat_Condition1'] = df.loc[:, 'cat_Condition1'].map({\n    1: 0, 5: 0, 6: 0,\n    0: 1, 3: 1, 4: 1, 6: 1, 7: 1, 8: 1,\n    2: 2\n})\n\ndf.loc[:, 'cat_LandContour'] = df.loc[:, 'cat_LandContour'].map({\n    0: 0,\n    1: 1, 2: 1,\n    3: 2\n})\n\ndf.loc[:, 'cat_Foundation'] = df.loc[:, 'cat_Foundation'].map({\n    0: 0, 1: 0,\n    3: 1, 4: 1, 5: 1,\n    2: 2\n})\n\ndf = pd.get_dummies(df, columns=one_hot_encoding)\n\nprint('Columns binirized: {}\\nColumns transformed by One Hot Enconding technique: {}'.format(to_binary, one_hot_encoding))","1b0227bb":"tmp = df[df.index.get_level_values(1) == 'train'].copy()\ntmp.loc[:, 'SalePrice'] = y.values\ntmp.loc[:, 'YearBuilt'] = pd.cut(tmp.YearBuilt, 7, labels=range(7)).astype('int')\nsns.swarmplot(x='YearBuilt', y='SalePrice', hue='YearBuilt',\n              data=tmp)\nplt.show()\n\ndf.GrLivArea = df.GrLivArea.apply(np.log)\ndf.loc[:, 'OverallQual'] = pd.cut(df.OverallQual, 3, labels=[0, 1, 2])\ndf = pd.get_dummies(df, columns=['OverallQual'])\ndf.loc[:, 'TotRmsAbvGrd'] = pd.cut(df.TotRmsAbvGrd, 4, labels=[0, 1, 2, 3])\ndf.loc[:, 'YearRemodAdd'] = pd.cut(df.YearRemodAdd, 3, labels=[0, 1, 2]).astype('int')\ndf = pd.get_dummies(df, columns=['TotRmsAbvGrd'])\ndf.loc[:, '1stFlrSF'] = df['1stFlrSF'].apply(lambda x: np.log(x) if x else x)\ndf.loc[:, '2ndFlrSF'] = df['2ndFlrSF'].apply(lambda x: np.log(x) if x else x)\n# Analyzing YearBuilt we noticed that values > 1982 are very\n# higher than other. Thus, we will transform this column\n# into binary\n# train_.loc[:, 'Year_tmp'] = pd.cut(train_[var], 10) #, labels=range(10))\ndf.loc[:, 'YearBuilt'] = np.where(\n    df.loc[:, 'YearBuilt'] > 1982, 1, 0\n)\n# From here, Neighborhood column looks really useless. Drop it.\ndf.drop(['MSSubClass', 'OpenPorchSF', 'BedroomAbvGr',\n         'LotArea', 'YrSold'], axis=1, inplace=True)","e31689cb":"df.reset_index(inplace=True)\ntrain = df[df.Dataset == 'train'].copy()\ntrain['SalePrice'] = y.copy()\ntest = df[df.Dataset == 'test'].copy()\n\ntrain.drop('Dataset', axis=1, inplace=True)\ntrain.set_index('Id', inplace=True)\ntest.drop('Dataset', axis=1, inplace=True)\ntest.set_index('Id', inplace=True)\n\ntrain.loc[:, 'SalePrice'] = train.SalePrice.apply(np.log)\nX = train.drop('SalePrice', axis=1)\ny = train.SalePrice","7b8a827b":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor(n_estimators=100)\nreg.fit(X, y)\n\ntmp = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': reg.feature_importances_\n})\ntmp.sort_values(by='Importance', ascending=False).head()","e05afe0c":"from sklearn.ensemble import GradientBoostingRegressor\ngb_bestparams = {\n    'loss': 'huber',\n    'max_features': 'log2',\n    'min_samples_split': 10,\n    'n_estimators': 500\n}\nreg = GradientBoostingRegressor(**gb_bestparams)\nreg.fit(X, y)\n\ntest_ = test.copy()\ntest_['SalePrice'] = reg.predict(test)\ntest_['SalePrice'] = test_['SalePrice'].apply(lambda x: np.floor(np.exp(x))) # to submit the predictions we need to transform it back to the original scale.\ntest_.reset_index(inplace=True)\ntest_[['Id', 'SalePrice']].to_csv('submission.csv', index=False)\ntest_[['Id', 'SalePrice']].head()\n","7377e1ae":"Subsequently, we can initially handle categorical values encoding it. ","6143134c":"Now, handling with numerical data, we will normalize some of them, and split the others into intervals. I am plotting the YrSold column as example to show that splitting it into intervals is good alternative to achieve a better explanation of the target SalePrice. ","79f41cd0":"In order to analyze the behavior of thee features, we filter the train dataset and also insert back the SalePrice column. This way, we can get some insights about the dataset plotting some stuff and evaluating statistics measures. We can start by the categorical columns, which now are encoded. ","1551edf1":"By performing this simple step, we can drop columns with low variance.","22b77b2f":"Thus, interpreting these plots we can transform some columns into binary ones, and apply the one hot encoding to the others. Here I'll hard code each column because some of them, for example, present four different values while it can be represented only by two. ","b16130cc":"I will not dive into this analysis, but this is a possible feature selection technique. Going directly to final model we have:","82098d04":"First, let's concat both train and test datasets. We also save the original target for later.","bb38850b":"It is interesent to build a Random Forest to analyze the feature importance provided by the model. ","75b7df8c":"As a first attempt, we drop all columns with missing values.","8bda95e9":"Finally, we could split the dataset into the original train and test to tune some models and validate them. However, to spare time I'll skip the tuning because this already was performed locally. The model that has achieved the best performance was the Gradient Boosting. "}}