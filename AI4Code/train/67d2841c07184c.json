{"cell_type":{"0ba30ec2":"code","053c3f4f":"code","a9c58980":"code","edb67fea":"code","8480463a":"code","247bca48":"code","0d3a8157":"code","7b2e32f3":"code","b44b06e2":"code","6ab1e065":"code","4ab8c82f":"code","b1fc404a":"code","bbf2e302":"code","1699fcdc":"code","9a177b26":"code","edb5c034":"code","fc284a71":"code","040ee9a7":"code","8cc7db49":"code","e9903e27":"code","d21880b4":"code","8a0af18d":"code","ec8a3b27":"code","9a4e411f":"code","9cd10a84":"code","c3858956":"code","13d9b229":"code","ff1f39a7":"code","2ff24651":"code","a66aba6b":"code","b73b58a1":"code","230fdef2":"code","4131989d":"code","2499b980":"code","353f883e":"code","f8e408c5":"code","95fac1a9":"code","8d9609e0":"code","105e2fa8":"code","450e9497":"code","a7a16b36":"code","820bcff1":"code","c65584fc":"code","fae97d8c":"code","4e183b10":"code","a2a223ad":"code","f69cbf5f":"code","95b0299d":"code","8912c966":"code","365d94fe":"code","c287b45a":"code","ea6e944c":"code","228cd553":"code","a05be621":"code","b54d56eb":"code","803f8192":"code","351f7095":"code","7409c944":"code","a561d542":"code","0c6b7fdf":"code","97dc8d75":"code","a3d0648e":"code","3a7037a2":"code","0d83d4f5":"code","22d7bcf3":"code","3e454c34":"code","a245a48e":"code","3ff545b4":"code","316c1b9f":"code","d3c39c58":"code","42285220":"code","563a4cca":"code","c2ec68e5":"code","098a17c7":"code","bbb65342":"code","5587c3d3":"code","8e2ee8ce":"code","c7ac7b3b":"code","48c0c947":"code","5f6a513b":"code","0258507f":"code","c7b6b645":"code","9318cc47":"code","71c9b051":"code","fcc3c939":"code","f74d8669":"code","19e2b32c":"markdown","5b13bdf3":"markdown","173f394f":"markdown","cb6bd13f":"markdown","0d83c14d":"markdown","d0a1b755":"markdown","e8d5d146":"markdown","ace14bda":"markdown","4cd42086":"markdown","6cd01292":"markdown","5f8cfab5":"markdown","01d4eb9a":"markdown","acf3a0de":"markdown","acb7aff9":"markdown","f20702f8":"markdown","92b6e080":"markdown","ef144fc5":"markdown","fa20e6b1":"markdown","e3413e92":"markdown","1d12d2e6":"markdown","221f365c":"markdown","ac42b09b":"markdown","77a4c19c":"markdown","1fe71ca2":"markdown","52969753":"markdown","488641c2":"markdown","d1b6cb89":"markdown","6b8ae49b":"markdown","cedc081c":"markdown","35717588":"markdown","2c80e8d0":"markdown","98d43101":"markdown","8bf1aaab":"markdown","2d3b2dd7":"markdown","59523adf":"markdown","fa0d3c1f":"markdown","cccebea8":"markdown","5a29b938":"markdown","cc10af3b":"markdown","911afa41":"markdown","5e124e4d":"markdown","7aabe869":"markdown","732ab5be":"markdown","f70c2d57":"markdown","358337b2":"markdown","dce106f5":"markdown","2aae5ed1":"markdown","fca01493":"markdown","d0c06b98":"markdown","59885d64":"markdown","3d44a386":"markdown","a9ed1673":"markdown"},"source":{"0ba30ec2":"from IPython.display import Image\nImage(\"..\/input\/infographic\/INFOGRAPHIC.jpg\")","053c3f4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a9c58980":"data = pd.read_csv('..\/input\/diabetes\/diabetic_data.csv')\ndata.shape","edb67fea":"data.columns","8480463a":"data.info()","247bca48":"data.isnull().values.any()","0d3a8157":"data.race.value_counts().plot(kind = 'bar' )","7b2e32f3":"data.payer_code.value_counts().plot(kind = 'bar' )","b44b06e2":"data.medical_specialty.value_counts()","6ab1e065":"data.max_glu_serum.value_counts().plot(kind = 'bar' )","4ab8c82f":"data.A1Cresult.value_counts().plot(kind = 'bar' )","b1fc404a":"data.change.value_counts().plot(kind = 'bar' )","bbf2e302":"data.diabetesMed.value_counts().plot(kind = 'bar' )","1699fcdc":"data.readmitted.value_counts().plot(kind = 'bar' )","9a177b26":"data.age.value_counts().plot(kind = 'bar')","edb5c034":"data=data[data.diabetesMed=='Yes']\ndata.shape","fc284a71":"data=data[data.readmitted=='NO']\ndata.shape","040ee9a7":"data=data[~data.discharge_disposition_id.isin([11,13,14,19,20])]\ndata.shape","8cc7db49":"data = data.drop(['medical_specialty','payer_code','weight'],axis=1)","e9903e27":"data['race']=data.race.replace('?',np.nan)\n","d21880b4":"data['race'].fillna(data['race'].mode()[0], inplace=True)\n","8a0af18d":"data.race.isnull().sum()","ec8a3b27":"data.shape","9a4e411f":"data.columns","9cd10a84":"treatments = data[['encounter_id','metformin', 'repaglinide', 'nateglinide',\n       'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n       'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n       'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n       'insulin', 'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n       'metformin-pioglitazone']].copy()","c3858956":"treatments.head()","13d9b229":"treatments=treatments.replace(['No','Steady','Up','Down'],[0,1,1,1])\ntreatments.set_index('encounter_id',inplace=True)\n","ff1f39a7":"treatments.head()","2ff24651":"treatments.sum(axis=1).value_counts()","a66aba6b":"i1 = treatments[treatments['insulin']==1].sum(axis = 1).replace([1,2,3,4,5,6],['insulin','io','io','io','io','io'])","b73b58a1":"i1.value_counts()","230fdef2":"i0=treatments[treatments['insulin']==0].sum(axis=1).replace([0,1,2,3,4,5,6],['no med','other','other','other','other','other','other'])","4131989d":"i0.value_counts()","2499b980":"treatments=pd.concat([i1,i0])\ntreatments = pd.DataFrame({'treatments':treatments})","353f883e":"treatments.head()","f8e408c5":"data=data.join(treatments,on='encounter_id') #setting index as encounter_id","95fac1a9":"data.head()","8d9609e0":"data = data.drop(['metformin', 'repaglinide', 'nateglinide',\n       'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n       'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n       'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n       'insulin', 'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n       'metformin-pioglitazone'],axis=1)","105e2fa8":"data=data[data.treatments!='other']\ndata.shape","450e9497":"data.columns","a7a16b36":"data = pd.get_dummies(data, columns=['race', 'gender','max_glu_serum', 'A1Cresult', 'change',\n       'diabetesMed', 'readmitted'])","820bcff1":"data.head()","c65584fc":"data.age.value_counts()","fae97d8c":"labels = data['age'].astype('category').cat.categories.tolist()\nreplace_age = {'age' : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n\nprint(replace_age)","4e183b10":"data.replace(replace_age, inplace=True)","a2a223ad":"data.age.value_counts()","f69cbf5f":"data.num_lab_procedures.plot(kind='hist')","95b0299d":"import seaborn as sns\nsns.distplot(data.time_in_hospital)","8912c966":"import matplotlib.pyplot as plt\nage_count = data['age'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(age_count.index, age_count.values, alpha=0.9)\nplt.title('Frequency Distribution of age')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Age', fontsize=12)\nplt.show()","365d94fe":"labels = data['age'].astype('category').cat.categories.tolist()\ncounts = data['age'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","c287b45a":"data.columns","ea6e944c":"data = data.drop(['diag_1','diag_2','diag_3'],axis = 1)","228cd553":"from IPython.display import Image\nImage(\"..\/input\/correlation\/Picture1.png\")","a05be621":"import pandas as pd\nimport numpy as np\nimport scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)","b54d56eb":"data['dummyCat'] = np.random.choice([0, 1], size=(len(data),), p=[0.5, 0.5])\n\ndata.dummyCat.value_counts()","803f8192":"#Initialize ChiSquare Class\ncT = ChiSquare(data)\n\n#Feature Selection\ntestColumns = ['encounter_id', 'patient_nbr', 'age', 'admission_type_id',\n       'discharge_disposition_id', 'admission_source_id', 'time_in_hospital',\n       'num_lab_procedures', 'num_procedures', 'num_medications',\n       'number_outpatient', 'number_emergency', 'number_inpatient','number_diagnoses',\n       'race_AfricanAmerican', 'race_Asian', 'race_Caucasian', 'race_Hispanic',\n       'race_Other', 'gender_Female', 'gender_Male',\n       'max_glu_serum_>200', 'max_glu_serum_>300', 'max_glu_serum_None',\n       'max_glu_serum_Norm', 'A1Cresult_>7', 'A1Cresult_>8', 'A1Cresult_None',\n       'A1Cresult_Norm', 'change_Ch', 'change_No', 'diabetesMed_Yes',\n       'readmitted_NO', 'dummyCat']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"treatments\" ) ","351f7095":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","7409c944":"X = data.drop(['encounter_id','patient_nbr','num_lab_procedures','number_outpatient','number_emergency',\n                      'race_Asian','race_Other','diabetesMed_Yes','max_glu_serum_>200','A1Cresult_>8','A1Cresult_Norm',\n                      'readmitted_NO','dummyCat','treatments'],axis=1)\nY = data['treatments']\nprint(X.shape)\nprint(Y.shape)","a561d542":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","0c6b7fdf":"y_p=[]\nfor i in range(y_test.shape[0]):\n    y_p.append(y_test.mode()[0])#Highest class is assigned to a list which is compared with ytest\nlen(y_p) ","97dc8d75":"y_pred=pd.Series(y_p)","a3d0648e":"print(\"Accuracy : \",accuracy_score(y_test,y_pred))","3a7037a2":"#Logistic Regression\nm1=LogisticRegression()\nm1.fit(X_train,y_train)\ny_pred_lr=m1.predict(X_test)\nTrain_Score_lr = m1.score(X_train,y_train)\nTest_Score_lr = accuracy_score(y_test,y_pred_lr)\n\n\nprint('Training Accuracy is:',Train_Score_lr)\nprint('Testing Accuracy is:',Test_Score_lr)\nprint(classification_report(y_test,y_pred_lr))","0d83d4f5":"m2 = KNeighborsClassifier()\nm2.fit(X_train,y_train)\ny_pred_knn = m2.predict(X_test)\nTrain_Score_knn = m2.score(X_train,y_train)\nTest_Score_knn = accuracy_score(y_test,y_pred_knn)\n\nprint('Training Accuracy is :',Train_Score_knn)\nprint('Testing Accuracy is:',Test_Score_knn)\nprint(classification_report(y_test,y_pred_knn))\n","22d7bcf3":"m3=BernoulliNB()\nm3.fit(X_train,y_train)\ny_pred_bnb=m3.predict(X_test)\nTrain_Score_bnb = m3.score(X_train,y_train)\nTest_Score_bnb = accuracy_score(y_test,y_pred_bnb)\n\nprint('Training Accuracy :',Train_Score_bnb)\nprint('Testing Accuracy  :',Test_Score_bnb)\nprint(classification_report(y_test,y_pred_bnb))","3e454c34":"m4 = DecisionTreeClassifier()\nm4.fit(X_train,y_train)\ny_pred_dt=m4.predict(X_test)\nTrain_Score_dt = m4.score(X_train,y_train)\nTest_Score_dt = accuracy_score(y_test,y_pred_dt)\n\nprint('Training Accuracy :',Train_Score_dt)\nprint('Testing Accuracy :',Test_Score_dt)\nprint(classification_report(y_test,y_pred_dt))","a245a48e":"m5 = RandomForestClassifier()\nm5.fit(X_train,y_train)\ny_pred_rf=m5.predict(X_test)\nTrain_Score_rf = m5.score(X_train,y_train)\nTest_Score_rf = accuracy_score(y_test,y_pred_rf)\n\nprint('Training Accuracy :',Train_Score_rf)\nprint('Testing Accuracy :',Test_Score_rf)\nprint(classification_report(y_test,y_pred_rf))","3ff545b4":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 3\n\n# parameters to build the model on\nparameters = {'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","316c1b9f":"tree.best_params_","d3c39c58":"m6 = DecisionTreeClassifier(criterion='gini',max_depth=5,min_samples_leaf=50,min_samples_split=50)\nm6.fit(X_train,y_train)\ny_pred_tdt=m6.predict(X_test)\nTrain_Score_tdt = m6.score(X_train,y_train)\nTest_Score_tdt = accuracy_score(y_test,y_pred_tdt)\n\nprint('Training Accuracy :',Train_Score_tdt)\nprint('Testing Accuracy  :',Test_Score_tdt)\nprint(classification_report(y_test,y_pred_tdt))\n","42285220":"#Gridsearch CV to find Optimal K value for KNN model\ngrid = {'n_neighbors':np.arange(1,50)}\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,grid,cv=3)\nknn_cv.fit(X_train,y_train)\n\n\nprint(\"Tuned Hyperparameter k: {}\".format(knn_cv.best_params_))","563a4cca":"m7 = KNeighborsClassifier(n_neighbors=19)\nm7.fit(X_train,y_train)\ny_pred_tknn=m7.predict(X_test)\nTrain_Score_tknn = m7.score(X_train,y_train)\nTest_Score_tknn = accuracy_score(y_test,y_pred_tknn)\n\n\nprint('Training Accuracy :',Train_Score_tknn)\nprint('Testing Accuracy  :',Test_Score_tknn)\nprint(classification_report(y_test,y_pred_tknn))","c2ec68e5":"parameter={'n_estimators':np.arange(1,101)}\ngs = GridSearchCV(m5,parameter,cv=3)\ngs.fit(X_train,y_train)\ngs.best_params_\n\n","098a17c7":"m8 = RandomForestClassifier(n_estimators=73)\nm8.fit(X_train,y_train) \ny_pred_trf=m8.predict(X_test)\nTrain_Score_trf = m8.score(X_train,y_train)\nTest_Score_trf = accuracy_score(y_test,y_pred_trf)\n\n\nprint('Training Accuracy :',Train_Score_trf)\nprint('Testing Accuracy  :',Test_Score_trf)\nprint(classification_report(y_test,y_pred_trf))","bbb65342":"data.treatments.replace(['insulin','io'],[0,1],inplace = True)","5587c3d3":"a = data.drop(['age','treatments'],axis=1)\nb = data.treatments","8e2ee8ce":"cate_features_index = np.where(a.dtypes != int)[0]\n","c7ac7b3b":"xtrain,xtest,ytrain,ytest = train_test_split(a,b,train_size=.70,random_state=2)\n","48c0c947":"from catboost import CatBoostClassifier, Pool,cv\n#let us make the catboost model, use_best_model params will make the model prevent overfitting\nmodel = CatBoostClassifier(eval_metric='Accuracy',use_best_model=True,random_seed=42)","5f6a513b":"model.fit(xtrain,ytrain,cat_features=cate_features_index,eval_set=(xtest,ytest))","0258507f":"#show the model test acc, but you have to note that the acc is not the cv acc,\n#so recommend to use the cv acc to evaluate your model!\nprint('the test accuracy is :{:.6f}'.format(accuracy_score(ytest,model.predict(xtest))))\ntest_score_catboost = accuracy_score(ytest,model.predict(xtest))\nprint(\"the train accuracy is :\",model.score(xtrain,ytrain))\ntrain_score_catboost = model.score(xtrain,ytrain)","c7b6b645":"model.predict(xtest)","9318cc47":"Model_Scores=pd.DataFrame({'Models':['Logistic Regression','KNN','Bernauli Naives Bayes','Decision Tree','Random Forest','Tuned Decison Tree','Tuned KNN','Tuned Random Forest','Cat Boost'],\n             'Training Accuracy':[Train_Score_lr,Train_Score_knn,Train_Score_bnb,Train_Score_dt,Train_Score_rf,Train_Score_tdt,Train_Score_tknn,Train_Score_trf,train_score_catboost],\n             'Testing Accuracy':[Test_Score_lr,Test_Score_knn,Test_Score_bnb,Test_Score_dt,Test_Score_rf,Test_Score_tdt,Test_Score_tknn,Test_Score_trf,test_score_catboost],\n                })\n\nModel_Scores.sort_values(by=('Testing Accuracy'),ascending=False)","71c9b051":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","fcc3c939":"from IPython.display import display\n\nfrom sklearn.tree import export_graphviz\nimport graphviz","f74d8669":"dot_data = export_graphviz(\n    m6,\n    out_file=None,\n    feature_names=X.columns,\n    class_names=['insulin', 'Insulin+others'],\n    filled=True,\n    rounded=True,\n    special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","19e2b32c":"**Adding the new feature to the Actual Dataframe**","5b13bdf3":"### We can observe that, Payer code, medical speciality & weight have more than 50% of the missing data, and prefer to drop those features.","173f394f":"### Considering the Domain knowledge, we would like to drop the Columns \"diag_1\" , \"diag_2\" ,\"diag_3\"\n\n##### Since they contain the information about the codes of different types of treatments given to the patient. They don't contribute to the effectiveness of the treat (i.e, our problemm statement)","cb6bd13f":"# Exploratory Data Analysis","0d83c14d":"### UNI VARIATE ANALYSIS","d0a1b755":"## Since the treatments column was created from the 23 Drugs, We will be removing them","e8d5d146":"## Excluding patients who are Dead and are in hospise","ace14bda":"## Decision Trees","4cd42086":"## Filtering patients with Diabetes\n### diabetesMed = Yes","6cd01292":"# Predictive Model Development - Iteration 2","5f8cfab5":"### For KNN","01d4eb9a":"## Random Forest","acf3a0de":"# Feature Identification","acb7aff9":"# Base Model","f20702f8":"### For Random Forest ","92b6e080":"# Here the features which contains numeric values are of type Discrete Quantitative and has a finite set of values. Discrete data can be both Quantitative and Qualitative. So treating outliers in this dataset is not possible","ef144fc5":"## We have seen the individual values of each of the base model. Now let's compare and see which model is performing well for the given problem statement.","fa20e6b1":"## Since we have a combination of Discrete Quantitative Variables and Categorical Variables, we cannot perform general Correlation tests","e3413e92":"## Testing accuracy is a better estimate than training accuracy of out-of-sample performance","1d12d2e6":"**One hot encoding the nominal categorical values**","221f365c":"## Choosing the records with treatments Insulin and Insulin + other ( w.r.t Problem Statement)","ac42b09b":"##### 2. Filling the NaN's with the mode","77a4c19c":"# Predictive Model Development - Iteration 3 ","1fe71ca2":"### Feature Engineering - Creating a new feature \"Treatments\"","52969753":"## Problem Statement:\n### Pedicting effective treatments  for diabetes in turn reducing the readmission into the hospital","488641c2":"## when you evaluate the model we trained we get high scores, this just means how well our model learnt from our training data.","d1b6cb89":"# Patients are Given at max a combination of 6 drugs for treating diabetes","6b8ae49b":"# Model Building\n## Train Test Split","cedc081c":"# Handling Missing Values","35717588":"### For Decision Tree","2c80e8d0":"## CatBoostClassifier","98d43101":"## Baseline Models - Logistic Regression ","8bf1aaab":"# Feature Engineering","2d3b2dd7":"**Missing value Imputation using MODE for Race Feature as most of the people in the Dataset are Caucasian**","59523adf":"## Hyperparameter Tuning","fa0d3c1f":"## Bernoulli Naives Bayes","cccebea8":"Since our target variable is Categorical , We would be importing the required Classification model packages","5a29b938":"## Our Baseline accuracy is 54% \n#### We can set the accuracy as 54% and the models we build should be giving us accuracies greater than 54%","cc10af3b":"**We can observe that the \"Race\" Feature has some missing values**","911afa41":"** Encoding the AGE(ordinal) categorical column**","5e124e4d":"##### 1. Replacing the ? with NaN's","7aabe869":"**2. When the value of Insuin is '0' , creating the classes \"others\" & \"no med\"**","732ab5be":"# Predictive Model Development - Iteration 1 ","f70c2d57":"## KNN","358337b2":"# Model Comparision","dce106f5":"# Data Preparation","2aae5ed1":"## Filtering patients who didn't readmit\n### readmission = NO","fca01493":"## With respect to the problem statement given, the output variable is observed to be the \u201ctreatments\u201d feature\n## The input variables are both Discrete Quantitative and Categorical and our output variable is Categorical\n","d0c06b98":"# Chi-Square Test of Independence","59885d64":"### We will be performing Chi-Square Test of Independence for finding the Correlation btw the variables","3d44a386":"**1. When the value of Insuin is '1' , creating the classes \"insulin\" & \"io\" (insulin + others )********","a9ed1673":" ### Custom encoding for the 23 Drug Features\n"}}