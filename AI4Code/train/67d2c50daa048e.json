{"cell_type":{"ede8c64f":"code","bce6bb3b":"code","ab160dba":"code","451661db":"code","3ba00752":"code","af3229f4":"code","cc57df72":"code","9e6a6362":"code","d31eec71":"code","2171895e":"code","afa14639":"code","053cc0bd":"code","6c1e5614":"code","ce66cdc9":"code","52a2319b":"code","260ea707":"code","67131124":"code","d552a047":"code","63ef38c5":"code","19c0e0da":"code","076b811c":"code","18026db0":"code","bf097bb5":"code","57e95dcd":"code","42f091d6":"code","a3c343ae":"code","617ad4fe":"code","040b6d1c":"code","a80bb09f":"code","e4d5b261":"code","3abd1bee":"code","5b9c0a10":"code","6d1bea5f":"code","734ab8da":"code","9e1f084d":"markdown","9ee4537c":"markdown","8281f360":"markdown","ea8ebbfd":"markdown","8e1c5b3a":"markdown","8916d963":"markdown","5dcf754c":"markdown","053557dc":"markdown","e80ac8d0":"markdown","b4abc48d":"markdown","824cad25":"markdown","ed2e0652":"markdown"},"source":{"ede8c64f":"# Update to transformers 2.8.0\n!pip install -q transformers --upgrade\n!pip install -q pandas --upgrade\n!pip show transformers","bce6bb3b":"import os\nimport pickle\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, average_precision_score, roc_auc_score, f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nimport transformers as trfm\nfrom transformers import AutoTokenizer, TFAutoModel, TFElectraModel, ElectraTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","ab160dba":"def build_reranker(tokenizer, model):\n    tokenizer.enable_padding()\n    \n    def rerank(question, answers):\n        pairs = list(zip([question] * len(answers), answers))\n\n        encs = tokenizer.encode_batch(pairs)\n        input_ids = np.array([enc.ids for enc in encs])\n        scores = model.predict(input_ids).squeeze()\n\n        return scores\n    \n    return rerank","451661db":"def touch_dir(dirname):\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n        print(f\"Created directory {dirname}.\")\n    else:\n        print(f\"Directory {dirname} already exists.\")","3ba00752":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512, enable_padding=False):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \n    ---\n    Inputs:\n        tokenizer: the `fast_tokenizer` that we imported from the tokenizers library\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    if enable_padding:\n        tokenizer.enable_padding(max_length=maxlen)\n    \n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","af3229f4":"def combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=512):\n    \"\"\"\n    Given two arrays of IDs (questions and answers) created by\n    `fast_encode`, we combine and pad them.\n    Inputs:\n        tokenizer: The original tokenizer (not the fast_tokenizer)\n    \"\"\"\n    combined_ids = []\n\n    for i in tqdm(range(q_ids.shape[0])):\n        ids = []\n        ids.append(tokenizer.cls_token_id)\n        ids.extend(q_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend(a_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend([tokenizer.pad_token_id] * (maxlen - len(ids)))\n\n        combined_ids.append(ids)\n    \n    return np.array(combined_ids)","cc57df72":"def encode_qa(questions, answers, tokenizer, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(questions))):\n        q = questions[i]\n        a = answers[i]\n        \n        encs = tokenizer.encode(q, a)\n        all_ids.append(encs.ids)\n        if len(encs.ids) > 512:\n            return q, a\n    \n    return np.array(all_ids)","9e6a6362":"def build_model(transformer, max_len=None):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = L.Input(shape=(max_len, ), dtype=tf.int32)\n    \n    x = transformer(input_ids)[0]\n    x = x[:, 0, :]\n    x = L.Dense(1, activation='sigmoid', name='sigmoid')(x)\n    \n    # BUILD AND COMPILE MODEL\n    model = Model(inputs=input_ids, outputs=x)\n    model.compile(\n        loss='binary_crossentropy', \n        metrics=['accuracy'], \n        optimizer=Adam(lr=1e-5)\n    )\n    \n    return model","d31eec71":"def load_model(sigmoid_dir, transformer_dir='transformer', architecture=\"electra\", max_len=None):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    sigmoid_path = os.path.join(sigmoid_dir,'sigmoid.pickle')\n    \n    if architecture == 'electra':\n        transformer = TFElectraModel.from_pretrained(transformer_dir)\n    else:\n        transformer = TFAutoModel.from_pretrained(transformer_dir)\n    model = build_model(transformer, max_len=max_len)\n    \n    sigmoid = pickle.load(open(sigmoid_path, 'rb'))\n    model.get_layer('sigmoid').set_weights(sigmoid)\n    \n    return model","2171895e":"tokenizer = trfm.ElectraTokenizer.from_pretrained(\"google\/electra-small-discriminator\")\nfast_tokenizer = BertWordPieceTokenizer('\/kaggle\/input\/healthtap-joint-electra-small\/vocab.txt', lowercase=True)","afa14639":"models = {}","053cc0bd":"models['electra_ht_small'] = load_model(\n    sigmoid_dir='\/kaggle\/input\/healthtap-joint-electra-small',\n    transformer_dir='\/kaggle\/input\/healthtap-joint-electra-small\/transformer',\n    architecture='electra',\n    max_len=None\n)\n\nmodels['electra_ht_small'].summary()","6c1e5614":"models['electra_ht_base'] = load_model(\n    sigmoid_dir='\/kaggle\/input\/healthtap-joint-electra-base',\n    transformer_dir='\/kaggle\/input\/healthtap-joint-electra-base\/transformer',\n    architecture='electra',\n    max_len=None\n)\n\nmodels['electra_ht_base'].summary()","ce66cdc9":"models['electra_se_small'] = load_model(\n    sigmoid_dir='\/kaggle\/input\/stackexchange-finetune-electra-small\/transformer',\n    transformer_dir='\/kaggle\/input\/stackexchange-finetune-electra-small\/transformer',\n    architecture='electra',\n    max_len=None\n)\n\nmodels['electra_se_small'].summary()","52a2319b":"models['electra_se_base'] = load_model(\n    sigmoid_dir='\/kaggle\/input\/stackexchange-finetune-electra-base\/transformer',\n    transformer_dir='\/kaggle\/input\/stackexchange-finetune-electra-base\/transformer',\n    architecture='electra',\n    max_len=None\n)\n\nmodels['electra_se_base'].summary()","260ea707":"MAX_LEN = 512\n\ndf = pd.read_csv('\/kaggle\/input\/covidqa\/news.csv')","67131124":"correct_ids = encode_qa(df.question.values.astype(str), df.answer.values.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nwrong_ids = encode_qa(df.question.values.astype(str), df.wrong_answer.values.astype(str), fast_tokenizer, maxlen=MAX_LEN)","d552a047":"input_ids = np.concatenate([correct_ids, wrong_ids])\n\nlabels = np.concatenate([\n    np.ones(correct_ids.shape[0]),\n    np.zeros(correct_ids.shape[0])\n]).astype(np.int32)","63ef38c5":"score_df = pd.concat([df[['source']]]*2)\n\nfor model_name, model in models.items():\n    %time score_df[model_name] = model.predict(input_ids, batch_size=64)","19c0e0da":"score_df['labels'] = labels","076b811c":"score_df.to_csv('news.csv', index=False)","18026db0":"overall = {}\n\nfor model_name in models.keys():\n    result = {}\n    labels = score_df['labels']\n    score = score_df[model_name]\n    pred = score.round().astype(int)\n    result['ap'] = average_precision_score(labels, score).round(4)\n    result['roc_auc'] = roc_auc_score(labels, score).round(4)\n    result['f1_score'] = f1_score(labels, pred).round(4)\n    result['accuracy'] = accuracy_score(labels, pred).round(4)\n    overall[model_name] = result\n\noverall_df = pd.DataFrame(overall)\noverall_df.to_csv(\"overall_results.csv\")\noverall_df","bf097bb5":"print(overall_df.to_latex())","57e95dcd":"print(overall_df.to_markdown())","42f091d6":"all_sources = {}\n\nfor source in df.source.unique():\n    source_results = {}\n    score_source_df = score_df[score_df.source == source]\n\n    for model_name in models.keys():\n        result = {}\n        labels = score_source_df['labels']\n        score = score_source_df[model_name]\n        pred = score.round().astype(int)\n        result['ap'] = average_precision_score(labels, score).round(4)\n        result['roc_auc'] = roc_auc_score(labels, score).round(4)\n        result['f1_score'] = f1_score(labels, pred).round(4)\n        result['accuracy'] = accuracy_score(labels, pred).round(4)\n        \n        source_results[model_name] = result\n    \n    all_sources[source] = pd.DataFrame(source_results)","a3c343ae":"for source, sdf in all_sources.items():\n    print(source)\n    print('-'*40)\n    print(sdf)\n    print('='*40)","617ad4fe":"for source, sdf in all_sources.items():\n    print(source)\n    print('-'*40)\n    print(sdf.to_latex())\n    print('='*40)","040b6d1c":"for source, sdf in all_sources.items():\n    print(source)\n    print('-'*40)\n    print(sdf.to_markdown())\n    print('='*40)","a80bb09f":"ap_df = pd.DataFrame({source: sdf.loc['ap'] for source, sdf in all_sources.items()}).T\nap_df","e4d5b261":"print(ap_df.to_latex())","3abd1bee":"print(ap_df.to_markdown())","5b9c0a10":"micro_df = (sum(all_sources.values()) \/ len(all_sources)).round(4)\nmicro_df","6d1bea5f":"print(micro_df.to_latex())","734ab8da":"print(micro_df.to_markdown())","9e1f084d":"## Load Models","9ee4537c":"## Load Data","8281f360":"## Micro Scores","ea8ebbfd":"### Latex output","8e1c5b3a":"## AP Score by source","8916d963":"### Macro-Average","5dcf754c":"## Compute Scores","053557dc":"### Markdown output","e80ac8d0":"## Compute Prediction Results","b4abc48d":"### Regular output","824cad25":"## By source","ed2e0652":"## Helper functions"}}