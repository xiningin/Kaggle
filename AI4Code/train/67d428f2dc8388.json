{"cell_type":{"fda19483":"code","0c2bcc65":"code","82410865":"code","23633c60":"code","353fcf8b":"code","9743b0f7":"code","73377903":"code","8753c44f":"code","4d5c10f8":"code","72cf9a74":"code","a325ab86":"code","3f4be444":"code","573e081a":"code","56896ab4":"code","8349e56a":"code","a0fb5cdf":"code","afe3b7ab":"code","02430d13":"code","a53f4b49":"code","b54ac0ec":"code","f36e17db":"markdown","d4a2e01a":"markdown","b7e7fbb8":"markdown","b9c76e64":"markdown","866e0786":"markdown","096e825f":"markdown","671fe7c5":"markdown","ef30a55c":"markdown","3a8d9799":"markdown","fc23ed2e":"markdown","1eb6c05e":"markdown","bd032fea":"markdown","362ce771":"markdown","811aef92":"markdown","19c1386e":"markdown","5e6b0138":"markdown","fbafbbd9":"markdown","039a3aa3":"markdown","54f1bf7f":"markdown","ae1ade81":"markdown","c5713757":"markdown","66a3a51b":"markdown","f4d61cb5":"markdown","bbf9b736":"markdown","3e914e32":"markdown","0238cf0e":"markdown","af6ec4a7":"markdown","b6d45995":"markdown"},"source":{"fda19483":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *","0c2bcc65":"train = pd.read_csv('\/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/train.csv')\nprint(train.shape)","82410865":"train.head()","23633c60":"train['pixels'] = [np.fromstring(x, dtype=int, sep=' ').reshape(-1,48,48,1) for x in train['pixels']]","353fcf8b":"pixels = np.concatenate(train['pixels'])\nlabels = train.emotion.values\n\nprint(pixels.shape)\nprint(labels.shape)","9743b0f7":"emotion_prop = (train.emotion.value_counts() \/ len(train)).to_frame().sort_index(ascending=True)\n\nemotion_prop","73377903":"emotions = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']","8753c44f":"palette = ['orchid', 'lightcoral', 'orange', 'gold', 'lightgreen', 'deepskyblue', 'cornflowerblue']\n\nplt.figure(figsize=[12,6])\n\nplt.bar(x=emotions, height=emotion_prop['emotion'], color=palette, edgecolor='black')\n    \nplt.xlabel('Emotion')\nplt.ylabel('Proportion')\nplt.title('Emotion Label Proportions')\nplt.show()","4d5c10f8":"plt.close()\nplt.rcParams[\"figure.figsize\"] = [16,16]\n\nrow = 0\nfor emotion in np.unique(labels):\n\n    all_emotion_images = train[train['emotion'] == emotion]\n    for i in range(5):\n        \n        img = all_emotion_images.iloc[i,].pixels.reshape(48,48)\n        lab = emotions[emotion]\n\n        plt.subplot(7,5,row+i+1)\n        plt.imshow(img, cmap='binary_r')\n        plt.text(-30, 5, s = str(lab), fontsize=10, color='b')\n        plt.axis('off')\n    row += 5\n\nplt.show()","72cf9a74":"X_train, X_valid, y_train, y_valid = train_test_split(\n    pixels, labels, test_size=0.2, stratify=labels, random_state=1\n)\n\n\nprint('X_train Shape:', X_train.shape)\nprint('y_train Shape:', y_train.shape)\nprint()\nprint('X_valid Shape:', X_valid.shape)\nprint('y_valid Shape:', y_valid.shape)","a325ab86":"Xs_train = X_train \/ 255\nXs_valid = X_valid \/ 255","3f4be444":"np.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = Sequential([\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same', input_shape=(48,48,1)),\n    Conv2D(64, (5,5), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n    \n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(256, activation='relu'),\n    Dropout(0.5),\n    BatchNormalization(),\n    Dense(7, activation='softmax')\n])\n\ncnn.summary()","573e081a":"opt = tf.keras.optimizers.Adam(0.001)\ncnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","56896ab4":"%%time \n\nh1 = cnn.fit(\n    Xs_train, y_train, \n    batch_size=256,\n    epochs = 20,\n    verbose = 1,\n    validation_data = (Xs_valid, y_valid)\n)","8349e56a":"history = h1.history\nprint(history.keys())","a0fb5cdf":"epoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","afe3b7ab":"tf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)","02430d13":"%%time \n\nh2 = cnn.fit(\n    Xs_train, y_train, \n    batch_size=256,\n    epochs = 20,\n    verbose = 1,\n    validation_data = (Xs_valid, y_valid)\n)","a53f4b49":"for k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","b54ac0ec":"cnn.save('fer_model_v02.h5')\npickle.dump(history, open(f'fer_v02.pkl', 'wb'))","f36e17db":"We split the data into training and validation sets using a stratified fashion, and scale the pixels values between 0 and 1. ","d4a2e01a":"In order to enhance the performance of the model, we will increase the learning rate to 0.0001. ","b7e7fbb8":"We train for 20 epochs for the first training run. ","b9c76e64":"## Import Packages","866e0786":"As we can see from the distribution of labels, there is a class imbalance within this training set: the emotion happy accounts for about 25% of the data. ","096e825f":"## Label Distribution","671fe7c5":"## Load Training DataFrame","ef30a55c":"We train the model using the Adam optimizer, a learning rate of 0.001, and sparse categorical crossentropy loss. ","3a8d9799":"From the learning curves we can conclude the increased learning rate caused significant overfitting. The training accuracy is ~70%, while the validation accuracy is ~57%. In order to enhance performance, this model may benefit from image augmentation. ","fc23ed2e":"### Training Run 1","1eb6c05e":"## Split, Reshape, and Scale Datasets","bd032fea":"We train for another 20 epochs for the second training run. ","362ce771":"## Build Network","811aef92":"We set the seed in order to produce the same results each training run. We build the convolutional neural network using a series of 2D convolution layers followed by densely-connected layers. Additionally, we incorporate max pooling, dropout, and batch normalization. ","19c1386e":"## Save Model and History","5e6b0138":"The images for this training set are stored as a string. In order to train the model and visualize the images we need to process these strings into a 4D array of pixel values.","fbafbbd9":"### Training Run 2","039a3aa3":"We load the training data and view the first 5 rows. ","54f1bf7f":"## Train Network","ae1ade81":"We save the model and training history for future reference. ","c5713757":"We view a sample of images for each emotion: angry, disgust, fear, happy, sad, surprise, and neutral.","66a3a51b":"Let's view the distribution of labels. ","f4d61cb5":"## View Sample of Images","bbf9b736":"We import all necessary packages. ","3e914e32":"From the leaning curves we can conclude the model is training well, between 55-60%; however, there is room for improvement. There is slight overfitting, and the model could benefit from additional epochs. ","0238cf0e":"# **Facial Expression Recognition**\n## **Week 1 Training Notebook**\n### Alejandro Alemany, Sara Manrriquez, and Benjamin Zaretzky","af6ec4a7":"In this notebook we will build an image classification model to identify the emotion being expressed in the images of human faces.","b6d45995":"## Preprocess Data"}}