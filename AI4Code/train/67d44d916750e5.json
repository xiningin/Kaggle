{"cell_type":{"01ee5ec1":"code","517c02ff":"code","ca659751":"code","c2c7143a":"code","8aef5205":"code","bedcb220":"code","fbd1e45e":"code","e2c397b5":"code","30fba70c":"code","9b19a68d":"code","851cb5fb":"code","46921cb5":"code","dc2b8898":"code","d66655ac":"code","88bd08f6":"code","eba58b72":"code","bdd8b9be":"code","24175b57":"code","4f970b3f":"code","71bf77de":"code","ee65f8d3":"code","386706e8":"code","3dfa02f8":"code","e228187d":"code","4dad47a0":"code","7c920269":"code","1f366df3":"code","8ff85c35":"code","2a5d9bd0":"code","7f3ad4f1":"code","0b62a813":"code","08d39735":"markdown","45d078d2":"markdown","6bd94967":"markdown","2a45e50e":"markdown","5391b7f9":"markdown","505e98a3":"markdown","60a4715b":"markdown","a7463852":"markdown","c26b4146":"markdown","6b246860":"markdown","12154088":"markdown","163291df":"markdown"},"source":{"01ee5ec1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","517c02ff":"# Imports\nimport json\nimport os\nimport glob\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\n\nimport re\nimport string\n\nfrom wordcloud import WordCloud, STOPWORDS","ca659751":"# Take a look at the training data\ntrain_0 = pd.read_json('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/f8b03c87-9d1a-4f20-b76b-cb6c69d447b2.json')\ntrain_csv = pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","c2c7143a":"train_csv","8aef5205":"# Take a look at the title of the first document\ntrain_csv.pub_title[0]","bedcb220":"# Show how many different articles are in the training set, and how many datasets are referenced.\nprint(f'There are {len(train_csv.Id.unique())} different articles and {len(train_csv.cleaned_label.unique())} different datasets.')","fbd1e45e":"sns.countplot(x=train_csv.Id.value_counts())","e2c397b5":"fig = plt.figure(figsize=(13, 6))\nfig.suptitle('Distribution of articles and datasets', fontsize=20)\n\nax0 = plt.subplot2grid((1, 2), (0, 0))\nax1 = plt.subplot2grid((1, 2), (0, 1))\n\nax0.hist(train_csv.Id.value_counts())\nax0.set_xlabel(\"# of linked datasets by article\")\nax0.set_ylabel(\"# of articles\")\n\nax1.hist(train_csv.cleaned_label.value_counts())\nax1.set_xlabel(\"# of occurences of datasets in articles\")\nax1.set_ylabel(\"# of datasets\")\nplt.show()","30fba70c":"sns.kdeplot(x=train_csv.Id.value_counts())","9b19a68d":"sns.kdeplot(x=train_csv.cleaned_label.value_counts())","851cb5fb":"article0 = pd.read_json('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/d0fa7568-7d8e-4db9-870f-f9c6f668c17b.json')","46921cb5":"article0","dc2b8898":"# Display text of the first article\narticle0.text[0]","d66655ac":"# Display the paragraph titles in a different way than above\nfor sentence in article0.section_title:\n    print(''.join(sentence))","88bd08f6":"train_csv = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","eba58b72":"stopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='white',\n                      stopwords=stopwords,\n                      max_words=200,\n                      max_font_size=30,\n                      scale=3,\n                      random_state=1)\n   \ncloud = wordcloud.generate(str(train_csv['dataset_title'].unique()))\n\nfig = plt.figure(figsize=(15, 15))\nplt.axis('off')\nplt.imshow(cloud)\nplt.show()","bdd8b9be":"# Imports, which repeats some of the previous imports, in case we remove that section\nimport json\nimport os\nimport re\n\nimport numpy\nimport pandas\nfrom fuzzywuzzy import fuzz\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords","24175b57":"# Retrieve the training data and sample submission\ndirectory = r\"..\/input\/coleridgeinitiative-show-us-the-data\/\"\n\ntrain_csv = pandas.read_csv(directory + \"\/train.csv\")\nsample_submission = pandas.read_csv(directory + \"\/sample_submission.csv\")","4f970b3f":"def retrieve_text(filename, type):\n    json_path = os.path.join(directory, type, filename + \".json\")\n\n    section_title = []\n    contents = []\n    with open(json_path, mode='r') as recurse:\n        json_contents = json.load(recurse)\n\n        for data in json_contents:\n            contents.append(data.get('section_title'))\n            contents.append(data.get('text'))\n\n        # section_title = data_cleaning(\" \".join(section_title))\n        contents = data_cleaning(\" \".join(contents))\n\n    return contents","71bf77de":"def data_cleaning(text):\n    text = re.sub('[^A-Za-z0-9]+', \" \", text)\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    cleaned_text = emoji_pattern.sub(r'', text)\n\n    return cleaned_text.lower()","ee65f8d3":"def load_json():\n    train_csv['json-content'] = train_csv['Id'].apply(retrieve_text, args=('train',))\n    test_set['json-content'] = sample_submission['Id'].apply(retrieve_text, args=('test',))\n    # train_csv['acronym'] = train_csv['dataset_title'].progress_apply(create_patterns)\n    # train_csv['fuzzy-ratio'] = train_csv.progress_apply(get_fuzzy_score, axis=1)","386706e8":"test_set = pandas.DataFrame()\ntest_set['Id'] = sample_submission['Id']\nload_json()","3dfa02f8":"# Define the preprocessing function\ndef preprocess_data(dataframe):\n    unique_dataset_titles = dataframe['dataset_title'].unique()\n\n    for dataset_title in unique_dataset_titles:\n        try:\n            if '(' in str(dataset_title):\n                tmp_title = str(dataset_title).split(\" \")\n                \n                tmp_title_without_braces = str(dataset_title).replace(\"(\", \"\")\n                tmp_title_without_braces = tmp_title_without_braces.replace(\")\", \"\").lower()\n                tmp_title_without_braces = re.sub('[^A-Za-z]+', \" \", tmp_title_without_braces)\n                    \n                for word in tmp_title:\n                    if '(' in word:\n                        acronyms_dict[str(word[1: -1]).lower()] = tmp_title_without_braces\n\n            else:\n                text = re.sub('[^A-Za-z]+', \" \", str(dataset_title))\n                clean_text = text.lower().split()\n                clean_text = [clean_word for clean_word in clean_text if not clean_word in set(stop_words)]\n\n                acronym_text = []\n                for word in clean_text:\n                    acronym_text.append(word[0: 1])\n\n                acronyms_dict[\"\".join(acronym_text)] = str(dataset_title).lower()\n\n            tmp_title = str(dataset_title)\n            tmp_title_without_braces = str(dataset_title).lower().split(\" \")\n            tmp_title = re.sub('[^A-Za-z0-9]+', \" \", tmp_title).lower()\n            tmp_title_without_braces = [word for word in tmp_title_without_braces\nif not '(' in word]\n            tmp_title_without_braces = re.sub('[^A-Za-z0-9]+', \" \", str(tmp_title_without_braces)).lower()\n\n            titles_prior1.add(tmp_title.strip())\n            \n            if tmp_title_without_braces.strip() not in titles_prior1:\n                titles_prior2.add(tmp_title_without_braces.strip())\n                titles_dict[tmp_title_without_braces.strip()] = tmp_title.strip()\n\n        except:\n            print(\"exception occurred for title: \", dataset_title)\n            continue\n\n    return acronyms_dict, titles_dict, titles_prior1, titles_prior2                                        ","e228187d":"stop_words = stopwords.words('english')\nacronyms = set()\ntitles_prior1 = set()\ntitles_prior2 = set()\nacronyms_dict = {}\ntitles_dict = {}\nacronyms_dict, titles_dict, titles_prior1, titles_prior2 = preprocess_data(train_csv)","4dad47a0":"# Display the acronyms dictionary\nacronyms_dict","7c920269":"titles_prior1 = list(sorted(titles_prior1, key=len, reverse=True))\ntitles_prior2 = list(sorted(titles_prior2, key=len, reverse=True))\nunique_cleaned_matches = train_csv['cleaned_label'].unique()","1f366df3":"print(titles_prior1)","8ff85c35":"acronyms = acronyms_dict.keys()\nmatch_out = []\nfor json_data in test_set['json-content']:\n    match = ''\n    tmp_set = set()\n\n    for word in json_data.split():\n        tmp_set.add(word)\n    \n    for clean_text in unique_cleaned_matches:\n        if clean_text in str(json_data) and clean_text not in match:\n            match += ('|' + clean_text if len(match) > 0 else clean_text)\n            \n    for query_prior1 in titles_prior1:\n        query_text = str(query_prior1).lower()\n\n        if query_text in str(json_data) and query_text not in match:\n            match += ('|' + query_text if len(match) > 0 else query_text)\n\n    for query_prior2 in titles_prior2:\n        query_text = str(query_prior2).lower()\n\n        if query_text in str(json_data) and query_text not in match:\n            match += ('|' + query_text if len(match) > 0 else query_text)\n\n    for query_text in acronyms:\n        if len(query_text) > 3 and query_text in tmp_set and query_text not in match:\n            match += ('|' + query_text if len(match) > 0 else query_text)\n\n    match_out.append(match)","2a5d9bd0":"print(match_out)","7f3ad4f1":"# Test set results\nresult = pandas.DataFrame()\nresult['Id'] = test_set['Id']\nresult['PredictionString'] = match_out\nresult.to_csv('submission.csv', index=False)","0b62a813":"# Training set results, not required for this competition\n# result = pandas.DataFrame()\n# result['Id'] = train_csv['Id']\n# result['title'] = train_csv['dataset_title']\n# result['clean'] = train_csv['cleaned_label']\n# result['PredictionString'] = match_out\n# result.to_csv('submission.csv', index=False)","08d39735":"Let's visualize the training data","45d078d2":"**Conclusion:** The distribution of linked datasets by articles is much less smoother than that of articles by datasets.","6bd94967":"# **Competition Objective**\nThis competition (The Coleridge Competition) seeks to predict the mentioning of datasets in scientific articles. The use of these predictions is to tie it to policymaking driven by data. \n\nSubmissions for this competition need to include the id of the article and a string justifying the prediction (cleaned using the clean_text() function that is provided. Our model needs to predict this string.","2a45e50e":"# Data Retrieval and Cleaning","5391b7f9":"Distribution of publications and datasets","505e98a3":"# **Notes about the Training data**\n**train.csv** is a file linking the publication and datasets referenced. We find the publication texts in the individual .json files. As for the test data, we have 4 articles to parse and link to datasets. From what I understand now, we have to find strings within the articles' text that we find are likely to be references to datasets.","60a4715b":"Let's look at the format of the articles","a7463852":"We can see above that this first article has 18 \"section titles\". The text from that section title is listed above to left of each section title.","c26b4146":"# **Exploratory Data Analysis (EDA)**","6b246860":"# Predict the results, and save it to a submission file","12154088":"Preprocess the data for NLP","163291df":"The above wordcloud looks at the top 200 words in the corpus after stopwords (placeholder words) have been removed. We can see that there is a depth of various kinds of words, both in the frequency and the topics. This is what we might expect to find."}}