{"cell_type":{"46602207":"code","87c47a88":"code","c6b52ff2":"code","b39e3a5d":"code","1c4a5b8b":"code","27a63f5e":"code","d92cb769":"code","28e9fdf5":"code","77d4e210":"code","6efdd47a":"code","8d2e1bce":"code","c7315bbb":"code","7ff762c4":"code","f608d88a":"code","d0283cba":"code","0f26c78a":"code","131f9567":"code","6d63c2c4":"code","8e5d8243":"code","3d8d3abb":"code","4ea78895":"markdown","de3636e1":"markdown","657045bc":"markdown","aa98dd2f":"markdown","bbdfe2b7":"markdown","3c7dd865":"markdown","2b368f03":"markdown","212ff259":"markdown","cfe3895d":"markdown","442c9ec8":"markdown","9ac6f1a2":"markdown","80f72f53":"markdown","1b72addf":"markdown","d734c0bb":"markdown","36a38da4":"markdown","b544b4e4":"markdown","f80da5e4":"markdown","02185204":"markdown"},"source":{"46602207":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport imageio\nimport os\nfrom statsmodels.graphics.tsaplots import plot_acf","87c47a88":"#Data consists of two columns: date-time and consumption for one hour.\n\nenergy_hourly = pd.read_csv('..\/input\/hourly-energy-consumption\/PJME_hourly.csv', \n                            index_col=[0], parse_dates=[0])\n\n#Indices are not sorted - order the readings\nenergy_hourly.sort_index(inplace=True)\n\n#PJME_MW - MW per hour in PJM East Area\nenergy_hourly.head(3)","c6b52ff2":"def split_data(data, split_date):\n    return data[data.index <= split_date].copy(), \\\n           data[data.index >  split_date].copy()","b39e3a5d":"train, test = split_data(energy_hourly, '01-Jan-2015')\n\nplt.figure(figsize=(15,5))\nplt.xlabel('time')\nplt.ylabel('energy consumed')\nplt.plot(train.index,train)\nplt.plot(test.index,test)\nplt.show()","1c4a5b8b":"def create_features(df):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    df['date'] = df.index\n    df['hour'] = df['date'].dt.hour\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    return X","27a63f5e":"X_train, y_train = create_features(train), train['PJME_MW']\nX_test, y_test   = create_features(test), test['PJME_MW']\n\nX_train.shape, y_train.shape","d92cb769":"reg = xgb.XGBRegressor(n_estimators=1000)\nreg.fit(X_train, y_train,\n        eval_set=[(X_train, y_train), (X_test, y_test)],\n        early_stopping_rounds=50, #stop if 50 consequent rounds without decrease of error\n        verbose=False) # Change verbose to True if you want to see it train","28e9fdf5":"xgb.plot_importance(reg, height=0.9)","77d4e210":"def plot_performance(base_data, date_from, date_to, title=None):\n    plt.figure(figsize=(15,3))\n    if title == None:\n        plt.title('From {0} To {1}'.format(date_from, date_to))\n    else:\n        plt.title(title)\n    plt.xlabel('time')\n    plt.ylabel('energy consumed')\n    plt.plot(energy_hourly.index,energy_hourly, label='data')\n    plt.plot(X_test.index,X_test_pred, label='prediction')\n    plt.legend()\n    plt.xlim(left=date_from, right=date_to)","6efdd47a":"X_test_pred = reg.predict(X_test)\n    \nplot_performance(energy_hourly, energy_hourly.index[0].date(), energy_hourly.index[-1].date(),\n                 'Original and Predicted Data')\n\nplot_performance(y_test, y_test.index[0].date(), y_test.index[-1].date(),\n                 'Test and Predicted Data')\n\nplot_performance(y_test, '01-01-2015', '02-01-2015', 'January 2015 Snapshot')\n\nplt.legend()\n\nplt.show()","8d2e1bce":"random_weeks = X_test[['year', 'weekofyear']].sample(10)\nfor week in random_weeks.iterrows():\n    index = (X_test.year == week[1].year) & \\\n            (X_test.weekofyear == week[1].weekofyear)\n    data = y_test[index]\n    plot_performance(data, data.index[0].date(), data.index[-1].date())","c7315bbb":"mean_squared_error(y_true=y_test,\n                   y_pred=X_test_pred)","7ff762c4":"mean_absolute_error(y_true=y_test,\n                   y_pred=X_test_pred)","f608d88a":"def mean_absolute_percentage_error(y_true, y_pred): \n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","d0283cba":"mean_absolute_percentage_error(y_test,X_test_pred)","0f26c78a":"error_by_week = []\nrandom_weeks = X_test[['year', 'weekofyear']].sample(10)\nfor week in random_weeks.iterrows():\n    index = (X_test.year == week[1].year) & \\\n            (X_test.weekofyear == week[1].weekofyear)\n    error_by_week.append(mean_absolute_percentage_error(y_test[index], X_test_pred[index]))\npd.Series(error_by_week, index=random_weeks.index)","131f9567":"X_test['PJME_MW'] = y_test\nX_test['MW_Prediction'] = X_test_pred\nX_test['error'] = y_test - X_test_pred\nX_test['abs_error'] = X_test['error'].apply(np.abs)\nerror_by_day = X_test.groupby(['year','month','dayofmonth']) \\\n   .mean()[['PJME_MW','MW_Prediction','error','abs_error']]\n\nerror_by_day.sort_values('error', ascending=True).head(10)","6d63c2c4":"# Best predicted days\nerror_by_day.sort_values('abs_error', ascending=True).head(10)","8e5d8243":"series = pd.Series.from_csv('..\/input\/hourly-energy-consumption\/PJME_hourly.csv', header=0)\nplot_series = series[series.index<pd.Timestamp(series.index[1].date())]\nplot_acf(series[0:24])","3d8d3abb":"datetime.datetime","4ea78895":"## Create Time Series Features\nThey give more view points of the data: XGBoost will use them to find patterns not only from hour to hour, but by week, by season and so on.","de3636e1":"## Forecast on Test Set\nShow algorithm forecast for the whole test period and for january 2015.","657045bc":"## Goal: More accute predictions using PACF\n\n* Box-Jenkins\/ARIMA modeling approach to determine how past and future data points are related in a time series.\n\n* The partial autocorrelation function (PACF) can be thought of as the correlation between two points that are separated by some number of periods n, BUT with the effect of the intervening correlations removed.","aa98dd2f":"## Data\nWe will explore the openly avaliable power consumption data of PJM Grid. PJM Interconnection coordinates the continuous buying, selling, and delivery of wholesale electricity through the Energy Market from suppliers to customers in the reagon of South Carolina, USA.\n\nThe data is gathered from monitoring companies that are part of PJM Grid. These companies are regional, so we don't expect change of the number of consumers.\n\nThe data consists of power in MW, consumed by each company for every hour of its participation in PJM grid. We have also division btw. PJM East (PJME_hourly.csv) and PJM West (PJMW_hourly.csv)","bbdfe2b7":"## How Power Consumption Data is Gathered\nElectrical power consumption measurement is a very broad topic and a very interesting one. Most common approach is a meter reading device on the point at distribution and consumer electrical circuits.\n\nThese devices need to be reliable, errorprone and cost efficient. Specific organs are present to properly certify them. These devices measure the electricity that flows trough them and record it. The maximum error allowed by PJM Grid for meter reading devices is 2 (!) percent.<br>\n[Source](https:\/\/www.pjm.com\/-\/media\/committees-groups\/subcommittees\/ders\/20180629\/20180629-item-05a-demand-response-metering-requirements.ashx)\n\nNow the fun part comes when data has to be collected. \n* Simplest approach is for a technician to collect it at specific time.\n> today's measurement - last measurement = consumed electricity for that time.\n* But we can estimate for half an year based on average consumption e.g. and send technitian only twice a year - he can collect the data and with it a correction will be made.\n* And if we want accurate result(frequently gathering) that is also a cheap one(don't have army of collecting technitians), there is a device with SIM card that can send hourly reports.<br>\n[Source](https:\/\/www.utilityproducts.com\/articles\/print\/volume-6\/issue-4\/product-focus\/amr-ami\/automatic-meter-reading-and-the-advanced-metering-infrastructure.html)\n\nThe meter reading device can be privately owned(even in Bulgaria), but devices are most comonly supplier-owned. That means they are cheap and only regulation away from unfavorable to the customer. That means a natural **bias** towards larger electrical bills, aka. mesurement of larger that actual consumption.\n\nThe **noise** data suffers from  inevitable device malfunctions, device permitted deviation, technition measurement errors and losses of data. Also from people who don't pay much attention when cultivating the data.\n\nIn this field as in any other, when we dig deep enough we find crushing amounts of **variance**. You will see some findings at the end of this document that tell us the importance of features, not included in the dataframe. Another example is the weather - heating or cooling consumes electricity.\n\nIn conclusion, data was crushed and squashed, and we will crush and squash it once more. And like a reporters, if something goes wrong we can say: \"The data said so.\"","3c7dd865":"![](https:\/\/cdn-images-1.medium.com\/max\/1500\/1*8T4HEjzHto_V8PrEFLkd9A.png)\n#### Categorization of ensembles<br>\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*PaXJ8HCYE9r2MgiZ32TQ2A.png)\n#### Comparison btw. bagging and boosting ensemble techniques","2b368f03":"## Feature Importances\n> Feature importance is a great way to get a general idea about which features the model is relying on most to make the prediction. This is a metric that simply sums up how many times each feature is split on.\n> \n> We can see that the day of year was most commonly used to split trees, while hour and year came in next. Quarter has low importance due to the fact that it could be created by different dayofyear splits.\n\n*by robikscube*\n","212ff259":"The best predicted days seem to be a lot of october (not many holidays and mild weather) Also early may","cfe3895d":"## Create and Train XGBoost Model","442c9ec8":"> Notice anything about the over forecasted days?\n> \n> 1. worst day - July 4th, 2016 - is a holiday.\n> 3. worst day - December 25, 2015 - Christmas\n> 5. worst day - July 4th, 2016 - is a holiday.\n> Looks like our model may benefit from adding a holiday indicator.\n\nApperantly americans use less electricity during the holidays.\n\nThe best predicted days seem to be a lot of october (not many holidays and mild weather) Also early may","9ac6f1a2":"## Conclusion\n\nWe saw the ease of implementing an entirely new machine learning algorithm(at least new for me).  Understanding only sketchily XGBoost we can deduce its stong and weak sides. Choosing the best approach is looking more like reaching to the toy basket.\n\nIt is used to predict air quality in China. For forecasting markets. Even to prevent mushroom poisoning.\n\nSo... We got results, we got excited and now what's next? To use the newly acquired tools to gain something that our species is just not wired to understand on their own?","80f72f53":"## Related Literature:\n\nExplanation of XGBoost Algorithm<br>\nhttps:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d\n\nXGBoost Features<br>\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nHourly Time Series Forecasting using XGBoost<br>\nhttps:\/\/www.kaggle.com\/robikscube\/hourly-time-series-forecasting-with-xgboost\n\nExample applications:<br>\nhttp:\/\/iopscience.iop.org\/article\/10.1088\/1755-1315\/113\/1\/012127<br>\nhttps:\/\/www.r-bloggers.com\/forecasting-markets-using-extreme-gradient-boosting-xgboost\/\nhttps:\/\/www.linkedin.com\/pulse\/application-xgboost-algorithm-prevent-mushroom-poisoning-yi-kang-wu\n\nPACF Approach:<br>\n[A Gentle Introduction Autocorrelation Partial Autocorrelation](https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/)","1b72addf":"# XGBoost on time-based data - Performance evaluation\n\n## Introduction\nFrom flashing police cars to games and TVs, boys grow with their toys. Now, we can make our favorite toy think. Need I say more?\n\nA lot can be learned from Machine Learning techniques. There are a lot of impressive statements to comprehend and it is a beautiful field because of what Iordan showed us I feel like I can touch it, I can play with it(the field)\n\nHere is what I had time to do for - a tiny demo of a previously unknown algorithm for me and how 5 hours are enough to put a new, powerful tool in the box.\n\nThis notebook is based on kaggle hourly-time-series-forecasting-with-xgboost from robikscube, where he demonstrates the ability of XGBoost to predict power consumption data from PJM - an american power distribution company.\n\nEnjoy!","d734c0bb":"# MAPE of prediction","36a38da4":"## Let's measure the error!\nWhat it means: *To Be Continued*","b544b4e4":"## Percent of Mean Error for ten Random Weeks","f80da5e4":"## Theory of XGBoost\n\n**gradient boosting algorithm** is a machine learning algorithm:\n1. is a supervised algorithm - defines a loss function and minimizes it; trains by being presented training data and answer to match\n2. uses an ensemble - a collection of predictors which come together to give an answer\n3. uses boosting rather than bagging(many independent predictors - e.g. Random Tree Forest) - in boosting the predictors are made sequantially, not independently\n\n**XGBoost** is gradient boosting algorithm. It:\n1. is also known as \u2018regularized boosting\u2018 technique - seeks a goot bias-variant trade-off to reduce overfitting, (which its big compatitor Standard GBM lacks)\n2. allows cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run (it sounds good, so it should be good)","02185204":"## Show forcast for ten random weeks"}}