{"cell_type":{"e4b278d9":"code","885f6122":"code","ab7f0a67":"code","69a63a6d":"code","9e34cb6c":"code","7ac6a55f":"code","971b00a9":"code","836d8458":"code","821179bf":"code","125e2a07":"code","21dd1cbd":"code","3e5cebf5":"code","2b1e40d5":"code","7c862fa2":"code","3f747106":"code","a39466d6":"code","fa66180f":"code","05a7b6af":"code","5fa1621f":"code","d516e3fa":"code","f2ba1a78":"code","a413e45f":"code","d63b6d2c":"markdown"},"source":{"e4b278d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt","885f6122":"\nimport cv2\nimport numpy as np\n\nnet = cv2.dnn.readNet('..\/input\/yolo-coco-data\/yolov3.weights', '..\/input\/yolo-coco-data\/yolov3.cfg')\ncap = cv2.VideoCapture('..\/input\/test-video\/test.mp4')\nclasses = []\nwith open(\"..\/input\/yolo-coco-data\/coco.names\", \"r\") as f:\n    classes = f.read().splitlines()\n\n\nfont = cv2.FONT_HERSHEY_PLAIN\ncolors = np.random.uniform(0, 255, size=(100, 3))","ab7f0a67":"img = cv2.imread('..\/input\/input-2\/Half-Page-Size-Laurel-Restricted.jpg')\n","69a63a6d":"len(classes)# number of label\n","9e34cb6c":"#getting the op layers name and then using forward to extract the op in the layers","7ac6a55f":"#modifing the input and the RGB channels\nheight,width,_  = img.shape\nprint(height,width)","971b00a9":"#BGR mode for image \nplt.imshow(img)","836d8458":"#after converting the channels from BGR to BGR (real channels for image)","821179bf":"plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))","125e2a07":"#getting blob from input image\nblob = cv2.dnn.blobFromImage(img,1\/255,(416,416),(0,0,0),swapRB = True,crop = False)\nprint(blob.shape)#1 smaple  and 3 channels and width ans height ","21dd1cbd":"import time","3e5cebf5":"start = time.time()\nnet.setInput(blob)\n#getting the op layer and then extract the stuff from it \nop_layer_name  = net.getUnconnectedOutLayersNames()\nlayer_op = net.forward(op_layer_name)\nend = time.time()\nprint('YOLO v3 took {:.5f} seconds'.format(end - start))\n","2b1e40d5":"# Preparing lists for detected bounding boxes, obtained confidences and class's number\nbounding_boxes = []\nconfidences = []\nclass_numbers = []","7c862fa2":"import numpy as np","3f747106":"h = height\nw = width","a39466d6":"for result in layer_op:\n    # Going through all detections from current output layer\n    for detection in result:\n        # Getting class for current object\n        scores = detection[5:]\n        class_current = np.argmax(scores)\n\n        # Getting confidence (probability) for current object\n        confidence_current = scores[class_current]\n\n        # Eliminating weak predictions by minimum probability\n        if confidence_current > 0.5:\n            # Scaling bounding box coordinates to the initial image size\n            # YOLO data format keeps center of detected box and its width and height\n            # That is why we can just elementwise multiply them to the width and height of the image\n            box_current = detection[0:4] * np.array([w, h, w, h])\n\n            # From current box with YOLO format getting top left corner coordinates\n            # that are x_min and y_min\n            x_center, y_center, box_width, box_height = box_current.astype('int')\n            x_min = int(x_center - (box_width \/ 2))\n            y_min = int(y_center - (box_height \/ 2))\n\n            # Adding results into prepared lists\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)","fa66180f":"len(bounding_boxes)#52 object has been detecting ","05a7b6af":"results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, 0.5, 0.4)","5fa1621f":"print(results.flatten())","d516e3fa":"for i in results.flatten():\n    x,y,w,h = bounding_boxes[i]\n    label = str(classes[class_numbers[i]])\n    confidence = str(round(confidences[i],2))\n    color = colors[i] #random color\n    #drawing rectangle and putting the text\n    cv2.rectangle(img,(x,y),(x+w,y+h),color,2)\n    cv2.putText(img,label+\" \"+confidence,(x,y+20),font,2,(255,255,255),2)\n    ","f2ba1a78":"import matplotlib.pyplot as plt\nimport cv2","a413e45f":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.show()\n","d63b6d2c":"# **two parts of code first using for photo object detection then using in vedio**"}}