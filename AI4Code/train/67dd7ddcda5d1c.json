{"cell_type":{"a0862f6a":"code","480d2b77":"code","131d3b44":"code","b6c02a7d":"code","f8d5553d":"code","7169a93d":"code","0b94a161":"code","9fadca5f":"code","deca31e3":"code","6290522a":"code","3bf5ec6a":"code","e019d9ce":"code","dac78282":"code","b2ff4309":"code","9beaeafa":"code","ed6ff612":"code","59d43993":"code","865f935c":"markdown","3ce5919c":"markdown","f3034ed1":"markdown","2e727dc2":"markdown","91933509":"markdown","b819030e":"markdown","c8a25745":"markdown","e0c4a73e":"markdown","a4802031":"markdown","c3b56d54":"markdown","3e2551bb":"markdown","e702d213":"markdown","91a62094":"markdown","e954e4bb":"markdown","e96dee35":"markdown","3ac9d26a":"markdown","db34e607":"markdown","4daea7f3":"markdown","7210c1f2":"markdown"},"source":{"a0862f6a":"sentences = [\"We are going to fine-tune RoBERTa with MaskedLM task\",\n             \"You can apply the same technique to other models, such as BERT\",\n             \"But you need to pay a close attention to the special tokens in differet models. Different models have different vocabulary\",\n             \"The final model will hopefully has more knowledge about your specific domain\",\n             \"If you don't have a large amount of data, don't fine tune a pre-trained model with a maskedLM task\",\n             \"In general, be very cautious about doing maskedLM\"\n             \"Assume that the length of these sentences is around 512 tokens\",\n             \"DO NOT forget that your sentence is better to be very lengthy, like all 512 tokens are occupied!\",\n             \"If you don't have that lengthy paragraphs, you can combine these sentences together and generate longer sentences\",\n             \"and this is the last sentence!\",\n             \"You must have much much more data, much much more!\"\n]","480d2b77":"from sklearn.model_selection import train_test_split\ntrain_sentences, val_sentences = train_test_split(sentences, test_size=0.2, random_state=12345)\nprint(len(train_sentences))\nprint(len(val_sentences))","131d3b44":"import torch\nimport numpy as np\nimport random\nfrom transformers import RobertaTokenizer, RobertaForMaskedLM # BertTokenizer, BertForMaskedLM\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')","b6c02a7d":"def tokenize_sentences(sentences):\n    # Tokenize all of the sentences and map the tokens to thier word IDs.\n    input_ids = []\n    attention_masks = []\n    masked_lm_labels = []\n    mask_token_id = tokenizer.convert_tokens_to_ids(['<mask>'])[0]\n    repeat_random_sampling = 3 # times. Given 1 sentence we repeat the masking 3 times, so, we finally have 3 sentences per 1 raw sentence.\n\n    # For every sentence...\n    for sent in sentences:\n        # `encode_plus` will:\n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        len_sent = len(sent.split())  \n\n        for times in range(repeat_random_sampling):\n            encoded_dict = tokenizer.encode_plus(\n                                sent,                      # Sentence to encode.\n                                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                                max_length = 512,           # Pad & truncate all sentences.\n                                pad_to_max_length = True,\n                                return_attention_mask = True,   # Construct attn. masks.\n                                return_tensors = 'pt',     # Return pytorch tensors.\n                           )\n            masked_lm_labels.append(encoded_dict.input_ids.detach().clone())\n            # Randomly mask tokens\n            # We don't want to mask special tokens, so, we define the following conditions:\n            special_tokens_condition = (encoded_dict.input_ids != 0) * \\\n                                       (encoded_dict.input_ids != 1) * \\\n                                       (encoded_dict.input_ids != 2)\n            rand = torch.rand(512) # generate a list of 512 random numbers. \n            mask_arr = (rand < 0.15) * special_tokens_condition # mark those that are smaller than 0.15 and are not special tokens (we want to mask 15% of all tokens)\n#             mask_arr = (rand < random.choice([0.15, 0.2])) * special_tokens_condition # if you want to use various thresholds use this line instead, not recommended though.\n            selection = torch.flatten((mask_arr[0]).nonzero()).tolist()\n\n            for sel in selection:\n                if np.random.rand(1)[0] < 0.80: # 80% of the time use mask token\n                    encoded_dict.input_ids[0, sel] = mask_token_id\n                else:\n                    if np.random.rand(1)[0] < 0.50: # other wise 50% of them time replace with the original token\n                        encoded_dict.input_ids[0, sel] = masked_lm_labels[-1][0][sel]\n                    else: ## replace with a randomly selected token\n                        encoded_dict.input_ids[0, sel] = random.randint(4, 50263) # we don;t want to replace with special tokens\n\n            # Add the encoded sentence to the list.    \n            input_ids.append(encoded_dict['input_ids'])\n\n            # And its attention mask (simply differentiates padding from non-padding).\n            attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    masked_lm_labels = torch.cat(masked_lm_labels, dim=0)\n    return input_ids, attention_masks, masked_lm_labels ","f8d5553d":"def combine_sentences(train_sentences, repeats = 1):\n    \n    random.seed(12345)\n    len_train_sentences = len(train_sentences)\n\n    max_length = 512\n    i = 0\n    concat_sent = \"\"\n    current_length = 0\n    train_sentences_mixed = []\n    seq = 0\n\n    for i in range(repeats):\n        random.shuffle(train_sentences)\n\n        while i < len_train_sentences:\n            sent = str(train_sentences[i])\n            current_length += len(tokenizer.tokenize(sent))\n            if current_length > max_length + random.randint(-50, 0): # randomly cap the length between 512-50 and 512.\n                if seq>0: \n                    i -= 1\n                if sent.strip()[-1] in ['.', '!', '?', ';']:\n                    concat_sent  += sent\n                else:\n                    concat_sent  += sent + ' . '\n                    current_length += 1 # for the separator\n                train_sentences_mixed.append(concat_sent)\n                seq = 0\n                current_length = 0\n                concat_sent = \"\"\n            else:\n                if sent.strip()[-1] in ['.', '!', '?', ';']:\n                    concat_sent  += sent\n                else:\n                    concat_sent  += sent + ' . '\n                    current_length += 1 # for the separator\n                seq += 1   \n            i += 1\n    return train_sentences_mixed","7169a93d":"# If you want to combine the sentences use the following two lines otherwise skip to the next two lines\n# train_sentences_mixed = combine_sentences(train_sentences, repeats = 1)\n# valid_sentences_mixed = combine_sentences(val_sentences  , repeats = 1)\n\ntrain_sentences_mixed = train_sentences\nvalid_sentences_mixed = val_sentences","0b94a161":"train_input_ids, train_attention_masks, train_masked_lm_labels = tokenize_sentences(train_sentences_mixed)\nval_input_ids, val_attention_masks, val_masked_lm_labels = tokenize_sentences(valid_sentences_mixed)","9fadca5f":"print('Train Size = '  , train_input_ids.shape[0])\nprint('Val Size   = '  , val_input_ids.shape[0])","deca31e3":"from torch.utils.data import TensorDataset\n\n# Combine the training inputs into a TensorDataset.\ntrain_dataset = TensorDataset(train_input_ids, train_attention_masks, train_masked_lm_labels)\nval_dataset   = TensorDataset(val_input_ids, val_attention_masks, val_masked_lm_labels)","6290522a":"torch.manual_seed(12345)\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# The DataLoader needs to know our batch size for training, so we specify it \n# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n# size of 16 or 32.\nbatch_size = 8 # because of the limited GPU memory I picked 8\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","3bf5ec6a":"from transformers import RobertaTokenizer, RobertaForMaskedLM, AdamW, BertConfig\n\n# Load the model\nmodel = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n\n# Tell pytorch to run this model on the GPU.\nmodel.cuda();","e019d9ce":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr  = 1e-5, # args.learning_rate - default is 5e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8. ### ROBERTA PAPER USED 1e-6\n                )","dac78282":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 3\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","b2ff4309":"import time, datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","9beaeafa":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","ed6ff612":"# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    save_folder = \".\/pre-trained-models\/roberta-BioPage_Large-LR_moreRepeats_epoch-\" + str(epoch_i+1) + \"\/\"\n    print('Save this epoch model to: ', save_folder)\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n    total_train_loss_for_report_only = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            print()\n            print('Average Loss in the past 40 steps = ', total_train_loss_for_report_only\/40)\n            total_train_loss_for_report_only = 0\n            print()\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        # It returns different numbers of parameters depending on what arguments\n        # arge given and what flags are set. For our useage here, it returns\n        # the loss (because we provided labels) and the \"logits\"--the model\n        # outputs prior to activation.\n        \n        #You may need to change this part based on the transformer version you have. \n        # Check out the input\/output format of the version you have\n        model_output  = model(input_ids        = b_input_ids, \n                              attention_mask   = b_input_mask,\n                              labels = b_labels)\n        loss = model_output['loss']\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += loss.item()\n        total_train_loss_for_report_only += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n#         scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n    \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n#     total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            model_output = model(input_ids        = b_input_ids, \n                                   attention_mask   = b_input_mask,\n                                   labels = b_labels)\n            loss = model_output['loss']\n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n#             'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","59d43993":"# model.save_pretrained(\".\/PATH\/TO\/SAVE\/\")","865f935c":"# Load Data","3ce5919c":"#### Create TensorDataset and dataloader ","f3034ed1":"# Save the Model","2e727dc2":"# Introduction","91933509":"Define the device we want to run the model","b819030e":"#### Setup the optimizer and scheduler","c8a25745":"### If you have reached up to this point, you probably enjoyed it. Please support this notebook by upvoting. Thanks!","e0c4a73e":"### Load the model","a4802031":"The following function tokenizes the sentences and generates the required tensors, \"input_ids, attention_masks, masked_lm_labels\" for RoBERTa model. If you are training a different model you need to modify it accordingly. ","c3b56d54":"First we need to define the tokenizer:","3e2551bb":"One idea that you might already think of is to fine-tune a pre-trained transformer models on your own dataset to add extra domain knowledge to the model weights. You may do some extra pre-training steps using masked language modeling (MaskedLM) to updates the weights of the encoder (the features extractor) in Transformers. Most of the time, we only have a few number of labled data but we may have much more textual unlabeled data. We may use the unlabled data to add additional domain-specific language to the model. But you should be very cautious if you don't have a very large large dataset (in the order of 10 Million words or more). \nIf you do not have a large and diverse dataset, the original pre-trained model will lose it universality as well as its English langugage skills. Try to add more data to the train set other than those that are being used in the downstream task. There is a high chance that by fine-tunning the model using a small dataset, the model forget some of the knowledge it gained from the original pre-training and ONLY focuses on your dataset which is not good for generalization (See [this paper](https:\/\/arxiv.org\/pdf\/1910.07117.pdf)). If you decide to fine-tune a pre-trained model with the maskedLM, one way to avoid\/reduce the chance of overfitting is to freeze the feature extractor layers in the downstream task and let the model only update the classifier\/regressor. In general, I do not recommand to do extra pre-training steps before a downstream task.\n\nBut still the maskedLM training can be beneficial if you want to get the sentence embedding. The sentence embedding can be used later for clustering or getting the similarity score. In this notebook, I am going to show how to execute extra pre-training steps using maskedLM task. I mostly followed this [post](https:\/\/towardsdatascience.com\/masked-language-modelling-with-bert-7d49793e5d2c) and [this one](https:\/\/mccormickml.com\/2019\/07\/22\/BERT-fine-tuning\/) with slight updates in the way we generate masked tokens. I tried to follow the same\/similar steps that BERT and\/or RoBERTa developers took to generate (semi-)randomized masked tokens. \n\nIn BERT pre-training task, 15% of the tokens are selected to be masked. 80% of the time the masked token is replaced by [MASK] token, 10% of the time it is replaced by a random\/incorrect token (other than the original token) and 10% of the time it is replaced by the original token. BERT developers found that with this technique BERT learns the language much better. \n\n\n![](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1644252\/2699761\/BERT-Masking_Strategy.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211012%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211012T200010Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=4101079db3c67d47698ee1a75fd4dcbe338497260823d1145b519e31def95d9afbe628f0bea114802140d911a197bc53a99457314638ce0398af95b250be11a123bb0f4c4241812e2cf6624d7f1a2b36287fcb6b34a9bda4934f07595fae94dcc26e6955eb28f2fe0c6a6908f003c77a95d40889075e19a1d66ad95b9315c2efe46580c77ad23484da3e5dc1850b8c51f0a80b885870bca20e77826064764e57292354b1079a6f515a702b27368816f7fbb5f9d3f3179fa5b6cbe1ae773ea310ec392d7ed76534f556c69cdf841423757fda4791e1a68d7b30deaa13f6a79bf4c7cccdc060448784df5ad341f25ef43a8fed10aaab05b246b30cd0d15dd79619)\n\n\"Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and fine-tuning, as the [MASK] symbol never appears during the fine-tuning stage.\" ([BERT PAPER](https:\/\/arxiv.org\/pdf\/1810.04805.pdf))\n\nThe other critical point is to have sentences with the full length of 512 (or in the range of 512) tokens. If your sentences are not in that range it will downgrade the model performance significantly (it will negatively impact the long-distance connections capability). If your sentences\/paragraphs are smaller than ~512 tokens try to concatenate them to generate longer texts (don't refer to the hypothetical sentences I generated below, assume that their length is around 512 tokens).\n","e702d213":"### Tokenize Train and Val dataset","91a62094":"## Training","e954e4bb":"The goal of this notebook is not how to load and clean data. I assume that your textual data is already in a \"list\". So, I'll create a hypothetical list of texts.","e96dee35":"# Tokenization","3ac9d26a":"#### Timer helper function","db34e607":"### Random split Train and Val datset","4daea7f3":"I made this function to combine\/concatenate sentences to make larger sentences (As discussed above)","7210c1f2":"# Train MaskedLM Model"}}