{"cell_type":{"9d2fa30a":"code","c6fd8819":"code","df74f62f":"code","524ccd65":"code","0ca76b0b":"code","58918269":"code","29bfe513":"code","85e83bf0":"code","c91f35a7":"code","f356508c":"code","61b7b34b":"code","e49427e1":"code","9e059535":"code","0413094a":"code","1937f521":"code","ad65459e":"code","6bf1aa85":"code","6ee105a1":"code","6e93ba16":"code","2ee27d82":"code","4af68451":"code","136ede43":"code","6ff5c2af":"code","0c290134":"code","b65deafd":"code","18ea6316":"code","c8a5fae6":"code","c7c88540":"code","5da98dd1":"code","bf12d9c4":"code","4d37a56e":"code","e910c9e5":"code","eebe64ca":"code","c182fd8c":"code","2146248b":"code","e371d9b6":"code","2437f4a1":"code","e296f357":"code","afa862c3":"code","cf1418e4":"code","0546d9eb":"markdown","72cb7861":"markdown","7be44e5e":"markdown","c6f7e6ec":"markdown","299b4e0c":"markdown","9b1d89a6":"markdown","f32e64ed":"markdown","3a0c025e":"markdown","336ec79d":"markdown","43648c68":"markdown","fa8e063b":"markdown","6c84ebb8":"markdown","0f78178e":"markdown","82c2f297":"markdown","b0da6893":"markdown","dd79d9ab":"markdown","50677891":"markdown","98b6f8d7":"markdown","77299e05":"markdown","cd9a25b6":"markdown","886d7cdc":"markdown","4e2724ee":"markdown","128ce014":"markdown","30719106":"markdown","34c33c6c":"markdown","d47ecbba":"markdown"},"source":{"9d2fa30a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6fd8819":"import numpy as np\nfrom pandas import read_csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jupyter","df74f62f":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","524ccd65":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","0ca76b0b":"df_train=read_csv(\"..\/input\/kepler-labelled-time-series-data\/exoTrain.csv\")   # sur kaggle\ndf_test = read_csv(\"..\/input\/kepler-labelled-time-series-data\/exoTest.csv\") \n# df_train = read_csv('exoTrain.csv')    # depuis l'ordi (le fichier ipynb et les csv doivent \u00eatre dans le m\u00eame dossier)\n# df_test = read_csv('exoTest.csv')\ndf_train","58918269":"df_test","29bfe513":"X_train = df_train.drop(['LABEL'], axis=1)\ny_train = df_train['LABEL']\nX_test = df_test.drop(['LABEL'], axis=1)\ny_test = df_test['LABEL']","85e83bf0":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","c91f35a7":"# Importation des m\u00e9thodes de mesure de performances\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score","f356508c":"rf_score = accuracy_score(y_test, y_rf)\nprint(rf_score)","61b7b34b":"pd.crosstab(y_test, y_rf, rownames=['Reel'], colnames=['Prediction'], margins=True)","e49427e1":"# Sous Jupyter, si xgboost n'est pas d\u00e9j\u00e0 install\u00e9\n# !pip install xgboost","9e059535":"import xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)\nprint(classification_report(y_test, y_xgb))","0413094a":"df_train.LABEL.value_counts()","1937f521":"df_test.LABEL.value_counts()","ad65459e":"X_train = df_train.drop(['LABEL'], axis=1)\ny_train = df_train['LABEL']\nX_test = df_test.drop(['LABEL'], axis=1)\ny_test = df_test['LABEL']","6bf1aa85":"from imblearn.under_sampling import RandomUnderSampler \n\nrus = RandomUnderSampler()\nX_train, y_train = rus.fit_resample(X_train, y_train)\nX_test, y_test = rus.fit_resample(X_test, y_test)","6ee105a1":"y_train.value_counts()","6e93ba16":"y_test.value_counts()","2ee27d82":"rf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\n\nprint(classification_report(y_test, y_rf))\n\ncm = confusion_matrix(y_test, y_rf)\nprint(cm)","4af68451":"print(accuracy_score(y_test, y_rf))","136ede43":"X_train = df_train.drop(['LABEL'], axis=1)\ny_train = df_train['LABEL']\nX_test = df_test.drop(['LABEL'], axis=1)\ny_test = df_test['LABEL']","6ff5c2af":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(k_neighbors = 2)\nX_train, y_train = smote.fit_resample(X_train, y_train)\nX_test, y_test = smote.fit_resample(X_test, y_test)   # accuracy_score = 0.5 si on resample X_test, y_test\n                                                      # accuracy_score = 0.99 sinon","0c290134":"y_train.value_counts()","b65deafd":"y_test.value_counts()","18ea6316":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","c8a5fae6":"print(classification_report(y_test, y_rf))","c7c88540":"print(accuracy_score(y_test, y_rf))","5da98dd1":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","bf12d9c4":"plot_learning_curve(rf, X_train, y_train)","4d37a56e":"plot_roc_curve(rf,X_test,y_test-1)  # j'ai rajout\u00e9 -1 car y prend ses valeurs dans {1, 2} et la courbe\n                                    # ROC veut des valeurs dans {0, 1}","e910c9e5":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\nprint(xgb.score(X_test,y_test))","eebe64ca":"y_xgb = xgb.predict(X_test)\n\nprint(classification_report(y_test, y_xgb))\n\ncm = metrics.confusion_matrix(y_test, y_xgb)\nprint(cm)","c182fd8c":"plot_learning_curve(xgb, X_train, y_train)\nplot_roc_curve(xgb,X_test,y_test-1) # j'ai rajout\u00e9 -1 car y prend ses valeurs dans {1, 2} et la courbe\n                                    # ROC veut des valeurs dans {0, 1}","2146248b":"print(classification_report(y_test, y_xgb))","e371d9b6":"X_train = df_train.drop(['LABEL'], axis=1)\ny_train = df_train['LABEL']\nX_test = df_test.drop(['LABEL'], axis=1)\ny_test = df_test['LABEL']","2437f4a1":"y_train.value_counts()","e296f357":"from xgboost import XGBClassifier\nxgb = XGBClassifier(scale_pos_weight=5050\/37) # car il y a 5050 \u00e9toiles sans exoplan\u00e8te et 37 \u00e9toiles avec exoplan\u00e8te dans df_train\n# xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_xgb = xgb.predict(X_test)","afa862c3":"print(classification_report(y_test, y_xgb))\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)","cf1418e4":"plot_learning_curve(xgb, X_train, y_train)\nplot_roc_curve(xgb,X_test,y_test-1) # j'ai rajout\u00e9 -1 car y prend ses valeurs dans {1, 2} et la courbe\n                                    # ROC veut des valeurs dans {0, 1}","0546d9eb":"## RandomForest","72cb7861":"## Extreme Gradient Boosting : XGBoost avec sur\u00e9chantillonage SMOTE","7be44e5e":"On va r\u00e9\u00e9quilibrer le dataset en sur-\u00e9chantillonnant la classe minoritaire :","c6f7e6ec":"<img src=\"https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png\">","299b4e0c":"On va garder autant d'\u00e9toiles avec exoplan\u00e8te que d'\u00e9toiles sans exoplan\u00e8te dans l'ensemble d'apprentissage (X_train), en tirant al\u00e9atoirement ceux qu'on va garder. On dit qu'on \"sous-\u00e9chantillonne la classe majoritaire\".","9b1d89a6":"On teste les for\u00eats al\u00e9atoires avec les donn\u00e9es sur\u00e9chantillonn\u00e9es :","f32e64ed":"On obtient toujours 0.5 d'accuracy_score, ce qui est le pire pour un choix \u00e0 deux possibilit\u00e9s. XGBoost ne fonctionne donc pas non plus (comme l'algorithme de RandomForest) avec le sur-\u00e9chantillonnage de nos donn\u00e9es.","3a0c025e":"On a moins de donn\u00e9es d'apprentissage, mais les r\u00e9sultats sont plut\u00f4t meilleurs ...\nIls sont plus coh\u00e9rents car on obtient un r\u00e9sultat de 62%, 86%, 62%, 92% (on a fait tourner plusieurs fois le programme) m\u00eame s'ils ne sont pas tr\u00e8s r\u00e9guliers et pas toujours tr\u00e8s bons car dans un choix entre \"exoplan\u00e8te\" ou \"pas d'exoplan\u00e8te\", le pire r\u00e9sultat possible est de 50%.\n\nOn peut l'expliquer par le fait qu'il n'y a que tr\u00e8s peu de valeurs avec le sous-chantilonnage.","336ec79d":"Fonction pour tracer les courbes d'apprentissage sur l'ensemble d'apprentissage et l'ensemble de validation :","43648c68":"## Xgboost","fa8e063b":"## Sous-\u00e9chantillonage","6c84ebb8":"On applique les for\u00eats al\u00e9atoires sur le nouvel ensemble d'apprentissage","0f78178e":"## XGBoost pond\u00e9r\u00e9","82c2f297":"L'algorithme pr\u00e9dit toujours qu'il n'y a pas d'exoplan\u00e8tes et on obtient 0.5 d'accuracy si on resample X_train, y_train, X_test et y_test aussi, c'est le pire r\u00e9sultat possible car il n'y a que deux choix, \"exoplan\u00e8te\" ou \"pas d'exoplan\u00e8te\". Si on ne resample pas X_test et y_test, on obtient 0.99 d'accuracy_score. La m\u00e9thode de sur-\u00e9chantillonnage ne fonctionne pas ici, c'est peut-\u00eatre d\u00fb au fait que l'on a cr\u00e9e des nouvelles valeurs \u00e0 partir de trop peu de valeur, ainsi, toutes les donn\u00e9es se ressemblaient et l'apprentissage n'a pas \u00e9t\u00e9 efficace.","b0da6893":"Le dataset \u00e9tant tr\u00e8s d\u00e9s\u00e9quilibr\u00e9 car il y a peu d'exoplan\u00e8tes identifi\u00e9es, on peut avoir 98% d'accuracy en pr\u00e9disant toujours qu'il n'y a pas d'exoplan\u00e8te.","dd79d9ab":"La matrice de confusion montre que l'algorithme pr\u00e9dit toujours la m\u00eame chose, l'apprentissage n'a pas fonctionn\u00e9.","50677891":"Fonction pour tracer la courbe ROC :","98b6f8d7":"On va utiliser une am\u00e9lioration de la m\u00e9thode XGBoost, sans sur-\u00e9chantillonage","77299e05":"La m\u00e9thode SMOTE (Synthetic Minority Oversampling TEchnique) consiste \u00e0 synth\u00e9tiser des \u00e9l\u00e9ments pour la classe minoritaire, \u00e0 partir de ceux qui existent d\u00e9j\u00e0. Elle fonctionne en choisissant au hasard un point de la classe minoritaire et en calculant les k-voisins les plus proches pour ce point. Les points synth\u00e9tiques sont ajout\u00e9s entre le point choisi et ses voisins.","cd9a25b6":"## Sur\u00e9chantillonnage","886d7cdc":"Il y a beaucoup moins d'\u00e9toiles avec exoplan\u00e8te que d'\u00e9toiles sans exoplan\u00e8te :","4e2724ee":"On v\u00e9rifie qu'on a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage :","128ce014":"On utilise le param\u00e8tre scale_pos_weight pour donner plus d'impact aux erreurs commises sur la classe minoritaire :","30719106":"On reconstitue les jeux de donn\u00e9es sans \u00e9chantillonnage :","34c33c6c":"On a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage (en \"ajoutant\" des donn\u00e9es) :","d47ecbba":"On cr\u00e9e donc de \"fausses donn\u00e9es\" (mais \"vraisemblables\") pour l'apprentissage"}}