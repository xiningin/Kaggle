{"cell_type":{"9cc968fa":"code","176be6c0":"code","07490055":"code","f5a37032":"code","ede09b17":"code","0eff7e58":"code","acd0a33d":"code","72a9aa4c":"code","4bfdec64":"code","ca11933c":"code","74f3de5b":"code","fda770e9":"code","72b0211f":"code","a83d75dd":"code","4066013f":"code","9f483cd1":"code","c2514f4e":"code","e3922f3d":"code","c9dcbc30":"code","40428f93":"code","94f3d4cc":"code","cd1077a5":"code","ff7a04cc":"code","5afe0057":"code","492e25ab":"code","da53d69c":"code","f6f39855":"code","2f6aad0e":"code","d1d04f21":"code","5279e511":"code","94da5e71":"code","a5bdcfe9":"code","222dcd1d":"code","c227f54b":"code","a3fe653c":"code","49fde105":"code","a0f3aca2":"code","a78c9b1b":"code","c1ac98b2":"code","6e83a7e4":"code","a53de60f":"code","0e404682":"code","07d2f742":"code","d9e4629c":"code","962698d6":"code","4da3110e":"code","15914301":"code","2b3f2d0f":"code","18e3e2fe":"code","cf861feb":"code","a2e5ce51":"markdown","9151b493":"markdown","86698efe":"markdown","c412fb9f":"markdown","dc02cafb":"markdown","774c8647":"markdown","f0c589d7":"markdown","0073c2ef":"markdown","8c79416b":"markdown","0a4ea1d8":"markdown"},"source":{"9cc968fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","176be6c0":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\nfrom warnings import filterwarnings\nfrom collections import Counter\n\n# Visualizations Libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\nimport plotly\nimport plotly.offline as pyo\nimport plotly.express as px\nimport plotly.graph_objs as go\npyo.init_notebook_mode()\nimport plotly.figure_factory as ff\nimport missingno as msno\n\n# Generating Random Floating Point Values\nfrom numpy.random import seed\nimport random\nfrom random import randrange\n\n# System Operations\nimport sys\nimport os\n\npd.options.mode.chained_assignment = None\n\n# Data Pre-processing Libraries\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Modelling Libraries\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDClassifier,PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier,NearestCentroid\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\n    roc_auc_score, confusion_matrix, classification_report, plot_roc_curve\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, \\\n    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier, StackingClassifier\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, train_test_split\n\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Evaluation & CV Libraries\nfrom sklearn.metrics import precision_score,accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold","07490055":"df=pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')\ndf.head(20)","f5a37032":"def check_df(dataframe, head=5):\n    \n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    \n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    \n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    \n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    \n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    \n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","ede09b17":"check_df(df, head=5)","0eff7e58":"df.describe()","acd0a33d":"df.corr()","72a9aa4c":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")","4bfdec64":"fig = px.histogram(df, x=\"ph\", color=\"Potability\")\nfig.update_layout(title_text='The pH values of water according to the potability category.', # title of plot\n    xaxis_title_text='pH Range', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show()","ca11933c":"df.columns","74f3de5b":"index_vals = df['Potability'].astype('category').cat.codes\ntextd = ['Non-potability' if cl==0 else 'Potability' for cl in df['Potability']]\n\nfig = go.Figure(data=go.Splom(\n                dimensions=[dict(label='ph', values=df['ph']),\n                              dict(label='Hardness', values=df['Hardness']),\n                              dict(label='Solids', values=df['Solids']),\n                              dict(label='Chloramines', values=df['Chloramines']),\n                              dict(label='Sulfate', values=df['Sulfate']),\n                              dict(label='Conductivity', values=df['Conductivity']),\n                              dict(label='Organic_carbon', values=df['Organic_carbon']),\n                              dict(label='Trihalomethanes', values=df['Trihalomethanes']),\n                              dict(label='Turbidity', values=df['Turbidity']),\n                              dict(label='Potability', values=df['Potability'])],\n                text=textd,\n                marker=dict(color=df['Potability'],\n                            size=10,colorscale='Bluered',line=dict(width=0.5,color='rgb(230,230,230)'))))\n\n\nfig.update_layout(\n    title='Scatter Matrix for Water Quality Dataset',\n    dragmode='select',\n    width=1500,\n    height=1500,\n    hovermode='closest'\n)\n\n\nfig.show()","fda770e9":"df.loc[(df['Hardness'] >= 10) & (df['Hardness'] < 50), 'New_Hardness'] = 'Soft water'\ndf.loc[(df['Hardness'] >= 50) & (df['Hardness'] < 100), 'New_Hardness'] = 'Slightly hard water'\ndf.loc[(df['Hardness'] >= 100) & (df['Hardness'] < 200), 'New_Hardness'] = 'Hard water'\ndf.loc[(df['Hardness'] >= 200), 'New_Hardness'] = 'Very hard water'","72b0211f":"df.head()","a83d75dd":"\"\"\"df[\"ph\"] = df[\"ph\"].fillna(df.groupby(\"New_Hardness\")[\"ph\"].transform(\"mean\"))\ndf[\"Sulfate\"] = df[\"Sulfate\"].fillna(df.groupby(\"New_Hardness\")[\"Sulfate\"].transform(\"mean\"))\ndf[\"Trihalomethanes\"] = df[\"Trihalomethanes\"].fillna(df.groupby(\"New_Hardness\")[\"Trihalomethanes\"].transform(\"mean\"))\"\"\"","4066013f":"fig = px.histogram(df, x=\"ph\", color=\"Potability\")\nfig.update_layout(title_text='The pH values of water according to the potability category.', # title of plot\n    xaxis_title_text='pH Range', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show()","9f483cd1":"df = df.fillna(method='ffill').fillna(method='bfill')","c2514f4e":"\"\"\"df[\"ph\"] = df[\"ph\"].fillna(df.groupby(\"New_Hardness\")[\"ph\"].transform(\"median\"))\ndf[\"Sulfate\"] = df[\"Sulfate\"].fillna(df.groupby(\"New_Hardness\")[\"Sulfate\"].transform(\"median\"))\ndf[\"Trihalomethanes\"] = df[\"Trihalomethanes\"].fillna(df.groupby(\"New_Hardness\")[\"Trihalomethanes\"].transform(\"median\"))\"\"\"","e3922f3d":"\"\"\"df[\"ph\"] = df[\"ph\"].fillna(value=df[\"ph\"].median())\ndf[\"Sulfate\"] = df[\"Sulfate\"].fillna(value=df[\"Sulfate\"].median())\ndf[\"Trihalomethanes\"] = df[\"Trihalomethanes\"].fillna(value=df[\"Trihalomethanes\"].median())\"\"\"","c9dcbc30":"df.loc[(df['ph'] < 7), 'New_ph'] = 'Acid'\ndf.loc[(df['ph'] == 7), 'New_ph'] = 'Neutral'\ndf.loc[(df['ph'] > 7), 'New_ph'] = 'Alkaline'","40428f93":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")","94f3d4cc":"df.head(30)","cd1077a5":"fig = px.histogram(df, x=\"ph\", color=\"Potability\")\nfig.update_layout(title_text='The pH values of water according to the potability category.', # title of plot\n    xaxis_title_text='pH Range', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show()","ff7a04cc":"# distribution checks.\n\nfor col in df.columns:\n    fig = px.histogram(df, x=col, color=\"Potability\")\n    fig.update_layout(title_text='The ' +col+ ' values of water according to the potability category.', # title of plot\n                      xaxis_title_text=col, # xaxis label\n                      yaxis_title_text='Count', # yaxis label\n                      bargap=0.2, # gap between bars of adjacent location coordinates\n                      bargroupgap=0.1) # gap between bars of the same location coordinates\n    fig.show()\n#     sns.histplot(x = col, bins = 50, hue='Potability', data = df)\n#     plt.show()","5afe0057":"def outlier_winsorize(df, col):\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    upper = Q3 + 1.5 * IQR\n    lower = Q1 - 1.5 * IQR\n    df.loc[(df[col] < lower), col] = lower\n    df.loc[(df[col] > upper), col] = upper","492e25ab":"def overview_dataset(dataframe, cat_th=10, car_th=20):\n    \n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    \n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car","da53d69c":"cat_cols, num_cols, cat_but_car = overview_dataset(df)\nnum_cols = [col for col in num_cols if \"Potability\" not in col]","f6f39855":"num_cols","2f6aad0e":"# outlier detection, i will leave it as it is.\n\nfor col in num_cols:\n    fig = px.box(df, y=col)\n    fig.show()\n#     sns.boxplot(x = col, data = df)\n#     plt.show()","d1d04f21":"for col in num_cols:\n    outlier_winsorize(df,col)","5279e511":"df = pd.get_dummies(df, columns=['New_Hardness', 'New_ph'])\ndf","94da5e71":"check_df(df)","a5bdcfe9":"X = df.drop([\"Potability\"], axis=1)\ny = df[\"Potability\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)","222dcd1d":"print(X_train.shape)  \nprint(X_test.shape) ","c227f54b":"# dropout method.\n\nrf_model = RandomForestClassifier(random_state=1).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)","a3fe653c":"def plot_confusion_matrix(y, y_pred):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n    \nplot_confusion_matrix(y_test, y_pred)","49fde105":"# ACCURACY\naccuracy_score(y_test, y_pred)","a0f3aca2":"# PRECISION\nprecision_score(y_test, y_pred)","a78c9b1b":"# RECALL\nrecall_score(y_test, y_pred)","c1ac98b2":"# F1\nf1_score(y_test, y_pred)","6e83a7e4":"# ROC CURVE\nplot_roc_curve(rf_model, X_test, y_test)\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.show()","a53de60f":"# AUC\ny_prob = rf_model.predict_proba(X_test)[:, 1]\nroc_auc_score(y_test, y_prob)","0e404682":"plot_confusion_matrix(y_test, y_pred)\nprint(classification_report(y_test, y_pred))","07d2f742":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(rf_model, X_train)","d9e4629c":"def run_single_model():\n    models = [\n              ('RF', RandomForestClassifier()),\n              ('GBM', GradientBoostingClassifier()),\n              (\"XGBoost\", XGBClassifier(objective=\"binary:logistic\")),\n              (\"LightGBM\", LGBMClassifier()),\n              (\"CatBoost\", CatBoostClassifier(verbose=False)),\n              (\"AdaBoost\", AdaBoostClassifier()),\n              (\"Bagging\", BaggingClassifier()),\n              (\"ExtraTrees\", ExtraTreesClassifier()),\n              (\"HistGradient\", HistGradientBoostingClassifier())\n            ]\n\n    global output_df\n    output_df = pd.DataFrame(models, columns=[\"MODEL_NAME\", \"MODEL_BASE\"])\n    output_df.drop('MODEL_BASE', axis=1, inplace=True)\n    for name, classifier in models:\n        recall_cv = np.mean(cross_val_score(classifier, X, y, cv=3, scoring=\"recall\"))\n        print(f\"recall_cv: {round(recall_cv, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RECALL_BASE_CV_ALL\"] = recall_cv\n\n    ## HYPER PARAMETERS TUNNING\n    rf_params = {\"max_depth\": [5, 15, 20, None],\n                 \"max_features\": [5, 7, 9,  \"auto\"],\n                 \"min_samples_split\": [6, 8, 15],\n                 \"n_estimators\": [150, 200, 300]}\n\n    xgboost_params = {\"learning_rate\": [ 0.01, 0.05, 0.1, 0.15],\n                      \"max_depth\": [3, 5, 8],\n                      \"n_estimators\": [100, 200, 300],\n                      \"colsample_bytree\": [0.3, 0.5, 0.8]}\n\n    lightgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1],\n                       \"n_estimators\": [100, 300, 500],\n                       \"colsample_bytree\": [0.1, 0.3, 0.7, 1]}\n\n    extraTrees_params = {\n                        'n_estimators': [10, 50, 100],\n                        'max_depth': [2, 16, 50],\n                        'min_samples_split': [2, 6],\n                        'min_samples_leaf': [1, 2],\n                        'max_features': ['auto', 'sqrt', 'log2'],\n                        'bootstrap': [True, False],\n                        'warm_start': [True, False],\n                        }\n\n    HistGradient_params = {\"learning_rate\": [0.01, 0.05],\n                       \"max_iter\": [20, 100],\n                       \"max_depth\": [None, 25],\n                       \"l2_regularization\": [0.0, 1.5],\n                           }\n\n    classifiers = [\n                  (\"RF\", RandomForestClassifier(), rf_params),\n                  ('XGBoost', XGBClassifier(objective='reg:squarederror'), xgboost_params),\n                  ('LightGBM', LGBMClassifier(), lightgbm_params),\n                  ('ExtraTrees', ExtraTreesClassifier(), extraTrees_params),\n                  ('HistGradient', HistGradientBoostingClassifier(), HistGradient_params)\n                  ]\n    global best_models\n    best_models = {}\n\n    for name, classifier, params in classifiers:\n        #GridSearch\n        gs_best = GridSearchCV(classifier, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n        final_model = classifier.set_params(**gs_best.best_params_)\n        recall_cv = np.mean(cross_val_score(final_model, X, y, cv=3, scoring=\"recall\"))\n        print(f\"recall_cv (After): {round(recall_cv, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"ReCALL_TUNED_CV_ALL\"] = recall_cv\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"BEST_PARAMS\"] = str(gs_best.best_params_)\n\n        best_models[name] = final_model\n\n","962698d6":"def run_multiple_model():\n    ######################################################\n    # # Stacking & Ensemble Learning\n    ######################################################\n    best_models\n\n    model_name = 'voting_reg_RF_LGBM'\n    model = VotingClassifier(estimators=[('RF', best_models[\"RF\"]), ('LightGBM', best_models[\"LightGBM\"])])\n    model.fit(X, y)\n    recall_cv = np.mean(cross_val_score(model, X, y, cv=5, scoring=\"recall\"))\n    print(f\"recall_cv ({model_name}): {round(recall_cv, 4)} \")\n    global output_df\n    output_df = output_df.append({'MODEL_NAME': {model_name}}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == {model_name}, \"ReCALL_TUNED_CV_ALL\"] = recall_cv\n    best_models[model_name] = model\n\n\n\n    estimators = [('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"])]\n    stacking_reg = StackingClassifier(estimators=estimators, final_estimator=best_models[\"LightGBM\"])\n    stacking_reg.fit(X, y)\n    stacking_reg_recall = np.mean(cross_val_score(stacking_reg, X, y, cv=5, scoring=\"recall\"))\n    print(f\"RECALL (stacking_reg_RF_XG): {round(stacking_reg_recall, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"stacking_reg_rmse\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"stacking_reg_rmse\", \"ReCALL_TUNED_CV_ALL\"] = stacking_reg_recall\n    best_models['stacking_reg_RF_XG'] = stacking_reg\n\n    voting_reg_RF_XG_LGBM = VotingClassifier(estimators=[('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"]), ('LightGBM', best_models[\"LightGBM\"])])\n    voting_reg_RF_XG_LGBM.fit(X, y)\n    voting_reg_recall = np.mean(cross_val_score(voting_reg_RF_XG_LGBM, X, y, cv=5, scoring=\"recall\"))\n    print(f\"RECALL (voting_reg_RF_XG_LGBM): {round(voting_reg_recall, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"voting_reg_RF_XG_LGBM\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"voting_reg_RF_XG_LGBM\", \"ReCALL_TUNED_CV_ALL\"] = voting_reg_recall\n    best_models['voting_reg_RF_XG_LGBM'] = voting_reg_RF_XG_LGBM","4da3110e":"run_single_model()","15914301":"run_multiple_model()","2b3f2d0f":"output_df.sort_values([\"ReCALL_TUNED_CV_ALL\", \"RECALL_BASE_CV_ALL\"])","18e3e2fe":"# final_model = best_model[\"A\"].fit(X, y)","cf861feb":"# pickle.dump(final_model, open(\"xgb_tuned_model.pkl\", 'wb'))","a2e5ce51":"<div>\n    <h1><center>Water Quality Analysis<\/center><\/h1>\n<\/div>","9151b493":"PH of the water\n\nThe negative logarithm of the hydrogen ion concentration in water to base 10 is defined as the pH value. Waters with pH = 7 are known as Neutral waters. Such waters do not have acid and alkaline reactions. With the increase of the H+ ion concentration, the pH value drops below 7 and the water becomes acidic. With the increase in the OH- ion concentration, the pH becomes above 7 and the water has a basic character. pH values vary between 0-14. In general, groundwater is water with a pH of less than 7 and has acid properties. In surface waters, they are generally basic waters with a pH greater than 8. The pH value in drinking water is considered appropriate between 6.5 and 8.5. The pH value in groundwater varies depending on the balance between dissolved carbon dioxide and other carbonate and bicarbonate compounds. This balance changes easily according to temperature and pressure changes. For example, since the pressure will drop with the decrease that occurs during pumping in a well, some of the dissolved carbon dioxide is released. Thus, the pH value of the sample taken from flowing or open water is not the same as the pH value of the water in the source.","86698efe":"<div>\n    <h1><left>Introduction<\/left><\/h1>\n<\/div>","c412fb9f":"All living things depend on water. Water, which people have been fighting for ages to find and which is the main factor in the development of societies, has also caused many civilizations to collapse and disappear in the past.\n\nWater resources are gradually decreasing, and the proportion of societies facing water problems is increasing. The use of the word scarce for a substance that covers about three quarters of the world can be strange. Gradually, the level of the underground water table is decreasing and the pollution rate of surface and underground water resources is increasing.\n\nThe main purpose of the criteria related to water quality is to purify the water from some negativities that may endanger public health. Purification of water from some substances harmful to health is especially important in order to prevent consequences that may endanger public health. It is especially emphasized in many international sources that national risk-reward analyzes should be taken as a basis in determining the criteria related to water quality.","dc02cafb":"1. **ph**: pH of 1. water (0 to 14).\n\n2. **Hardness**: Capacity of water to precipitate soap in mg\/L.\n\n3. **Solids**: Total dissolved solids in ppm.\n\n4. **Chloramines**: Amount of Chloramines in ppm.\n\n5. **Sulfate**: Amount of Sulfates dissolved in mg\/L.\n\n6. **Conductivity**: Electrical conductivity of water in \u03bcS\/cm.\n\n7. **Organic_carbon**: Amount of organic carbon in ppm.\n\n8. **Trihalomethanes**: Amount of Trihalomethanes in \u03bcg\/L.\n\n9. **Turbidity**: Measure of light emiting property of water in NTU.\n\n10. **Potability**: Indicates if water is safe for human consumption. Potable - 1 and Not potable - 0","774c8647":"<div>\n    <h3><left>Feature Description<\/left><\/h3>\n<\/div>","f0c589d7":"<div>\n    <h1><left>-----------------------------------------------------------------------------------------------------------------<\/left><\/h1>\n<\/div>","0073c2ef":"<div>\n    <h1><center>---------------------------------------------------------------------------------------------------------------<\/center><\/h1>\n<\/div>","8c79416b":"<div>\n    <h1><left>Thanks!<\/left><\/h1>\n<\/div>","0a4ea1d8":"<div>\n    <h1><center>---------------------------------------------------------------------------------------------------------------<\/center><\/h1>\n<\/div>"}}