{"cell_type":{"c386bf19":"code","b5edb343":"code","14ad80d2":"code","2f593ca3":"code","edb08b5c":"code","27285afb":"code","7d1b3d12":"code","a48a205d":"code","77dd0bb5":"code","25b4a036":"code","9baa040d":"code","f2839412":"code","44350e01":"code","6a4eec11":"code","360e973d":"code","d251ee5e":"code","087ff2d7":"code","4510df5b":"code","3a7ecb96":"code","85f033f1":"code","3f862219":"code","86936e07":"markdown"},"source":{"c386bf19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5edb343":"from matplotlib import pyplot as plt\n%matplotlib inline ","14ad80d2":"base_dir='\/kaggle\/input\/facial-keypoints-detection\/'\ntrain_dir_zip=base_dir+'training.zip'\ntest_dir_zip=base_dir+'test.zip'","2f593ca3":"from zipfile import ZipFile\nwith ZipFile(train_dir_zip,'r') as zipObj:\n    zipObj.extractall('.')\n    print(\"Train Archive unzipped\")\nwith ZipFile(test_dir_zip,'r') as zipObj:\n    zipObj.extractall('.')\n    print(\"Test Archive unzipped\")","edb08b5c":"train_dir='.\/training.csv'\ntest_dir='.\/test.csv'\ntrain=pd.read_csv(train_dir)\ntest=pd.read_csv(test_dir)","27285afb":"train.head()","7d1b3d12":"print(\"Size of dataframe: \"+str(len(train))+'x'+str(len(train.columns))+'\\n')\nprint(train.info())","a48a205d":"train=train.dropna()\ntrain=train.reset_index(drop=True)\nprint(\"After droppping all the rows with any NA in column\\n\")\nprint(\"Size = \"+str(len(train))+'x'+str(len(train.columns)))","77dd0bb5":"X=[]\nY=[]\n\nfor img in train['Image']:\n    X.append(np.asarray(img.split(),dtype=float).reshape(96,96,1))\nX=np.reshape(X,(-1,96,96,1))\nX = np.asarray(X).astype('float32')\n    \nfor i in range(len((train))):\n    Y.append(np.asarray(train.iloc[i][0:30].to_numpy()))\nY = np.asarray(Y).astype('float32')","25b4a036":"print(X.shape)\nprint(Y.shape)","9baa040d":"disp=8\n\nfig,axes=plt.subplots((disp+3)\/\/4,4,figsize=(15,10))\n                    \nfor i in range(disp):\n    axes[i\/\/4,i%4].imshow(X[i].reshape(96,96),cmap='gray')\n    axes[i\/\/4,i%4].scatter([train[train.columns[2*j]][i] for j in range(15)],[train[train.columns[2*j+1]][i] for j in range(15)],s=10,c='r')","f2839412":"import tensorflow\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import BatchNormalization, Flatten, Dense, Dropout, Conv2D, MaxPool2D, LeakyReLU","44350e01":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)","6a4eec11":"model = Sequential()\n\nmodel.add(Conv2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3,3),padding='same', use_bias=False))\n# model.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","360e973d":"model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\nhistory=model.fit(X_train, Y_train, epochs=500,batch_size=32,validation_data=(X_test,Y_test))","d251ee5e":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs Epoch')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","087ff2d7":"Test_Data=[]\nfor img in test['Image']:\n    Test_Data.append(np.asarray(img.split(),dtype=float).reshape(96,96,1))\nTest_Data=np.reshape(Test_Data,(-1,96,96,1))\nTest_Data = np.asarray(Test_Data).astype('float32')","4510df5b":"Pred=model.predict(Test_Data)","3a7ecb96":"disp=8\n\nfig,axes=plt.subplots((disp+3)\/\/4,4,figsize=(15,10))\n                    \nfor i in range(disp):\n    axes[i\/\/4,i%4].imshow(Test_Data[i].reshape(96,96),cmap='gray')\n    axes[i\/\/4,i%4].scatter([Pred[i][2*j] for j in range(15)],[Pred[i][2*j+1] for j in range(15)],s=10,c='r')","85f033f1":"idtable=pd.read_csv(base_dir+'IdLookupTable.csv')\nrowId=list(idtable['RowId'])\nimageId=list(idtable['ImageId'])\nfeatureHead=list(train.columns.values)\nfeatureIndex=[featureHead.index(feature) for feature in idtable['FeatureName']]\n\nloc=[]\nfor index,imgId in zip(featureIndex,imageId):\n    loc.append(Pred[imgId-1][index])\nsubm=pd.DataFrame({'RowId':rowId,'Location':loc})\nsubm.head()","3f862219":"subm.to_csv('submission.csv',index = False)","86936e07":"The Notebook presents a runthrough the Facial Keypoint Detection Challenge hosted on Kaggle"}}