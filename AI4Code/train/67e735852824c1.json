{"cell_type":{"6e116bf1":"code","6691a38c":"code","84ef60fe":"code","e30186cf":"code","e93b2826":"code","655e56e9":"code","e1cf2b0a":"code","a667b203":"code","d332928a":"code","8d8ba12e":"code","ac28bb77":"code","2c25b06c":"code","09118d94":"code","f814fc2c":"code","f4b3341d":"code","6e1d03ff":"code","2d4ab3ee":"code","abf538cc":"code","7f35948b":"code","b1b5308d":"code","5b47cc24":"code","90ce5e4f":"code","13a296ee":"code","10548597":"code","32c06a82":"code","97301b3e":"code","9248de49":"code","41a17b57":"code","b1c610b9":"code","e0ec08bb":"code","503e3cc5":"code","488f4563":"code","825e4593":"code","97f38cf0":"code","a4bc153a":"code","bc1928fa":"code","8e6892a2":"code","f4b4bac1":"code","93c4bbe2":"code","0e46ad91":"code","2243177d":"code","2cea8f7d":"code","aba30a9a":"code","e3d4c0d2":"code","2d4db3a8":"code","31446f59":"code","2130b165":"code","d3aaeb0d":"code","acc2d178":"code","9c39ddfe":"code","67435f98":"code","26408335":"code","6a22c081":"code","1fdd0638":"code","405728de":"code","02273f5c":"code","63406986":"code","33c78fe3":"code","49e9d556":"code","b82c3b11":"code","9630f810":"code","87485b4e":"markdown","1aa5ef9b":"markdown"},"source":{"6e116bf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6691a38c":"import numpy as np\nimport os\nimport pandas as pd\nimport re","84ef60fe":"pwd","e30186cf":"news_category = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n\nrow_doc = \"..\/input\/bbc-news-summary\/BBC News Summary\/News Articles\/\"\nsummary_doc = \"..\/input\/bbc-news-summary\/BBC News Summary\/Summaries\/\"\n\ndata={\"articles\":[], \"summaries\":[]}","e93b2826":"directories = {\"news\": row_doc, \"summary\": summary_doc}\nrow_dict = {}\nsum_dict = {}\n\nfor path in directories.values():\n    if path == row_doc:\n        file_dict = row_dict\n    else:\n        file_dict = sum_dict\n    dire = path\n    for cat in news_category:\n        category = cat\n        files = os.listdir(dire + category)\n        file_dict[cat] = files","655e56e9":"\nrow_data = {}\nfor cat in row_dict.keys():\n    cat_dict = {}\n    # row_data_frame[cat] = []\n    for i in range(0, len(row_dict[cat])):\n        filename = row_dict[cat][i]\n        path = row_doc + cat + \"\/\" + filename\n        with open(path, \"rb\") as f:                \n            text = f.read()\n            cat_dict[filename[:3]] = text\n    row_data[cat] = cat_dict\n","e1cf2b0a":"sum_data = {}\nfor cat in sum_dict.keys():\n    cat_dict = {}\n    # row_data_frame[cat] = []\n    for i in range(0, len(sum_dict[cat])):\n        filename = sum_dict[cat][i]\n        path = summary_doc + cat + \"\/\" + filename\n        with open(path, \"rb\") as f:                \n            text = f.read()\n            cat_dict[filename[:3]] = text\n    sum_data[cat] = cat_dict","a667b203":"news_business = pd.DataFrame.from_dict(row_data[\"business\"], orient=\"index\", columns=[\"row_article\"])\nnews_business.head()","d332928a":"news_category = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\nnews_entertainment = pd.DataFrame.from_dict(row_data[\"entertainment\"], orient=\"index\", columns=[\"row_article\"])\nnews_politics = pd.DataFrame.from_dict(row_data[\"politics\"], orient=\"index\", columns=[\"row_article\"])\nnews_sport = pd.DataFrame.from_dict(row_data[\"sport\"], orient=\"index\", columns=[\"row_article\"])\nnews_tech = pd.DataFrame.from_dict(row_data[\"tech\"], orient=\"index\", columns=[\"row_article\"])\n","8d8ba12e":"# summary data\nsummary_business = pd.DataFrame.from_dict(sum_data[\"business\"], orient=\"index\", columns=[\"summary\"])\nsummary_entertainment = pd.DataFrame.from_dict(sum_data[\"entertainment\"], orient=\"index\", columns=[\"summary\"])\nsummary_politics = pd.DataFrame.from_dict(sum_data[\"politics\"], orient=\"index\", columns=[\"summary\"])\nsummary_sport = pd.DataFrame.from_dict(sum_data[\"sport\"], orient=\"index\", columns=[\"summary\"])\nsummary_tech = pd.DataFrame.from_dict(sum_data[\"tech\"], orient=\"index\", columns=[\"summary\"])","ac28bb77":"summary_business.head()","2c25b06c":"business = news_business.join(summary_business, how='inner')\nentertainment = news_entertainment.join(summary_entertainment, how='inner')\npolitics = news_politics.join(summary_politics, how='inner')\nsport = news_sport.join(summary_sport, how='inner')\ntech = news_tech.join(summary_tech, how='inner')","09118d94":"business = news_business.join(summary_business, how='inner')","f814fc2c":"business.head()","f4b3341d":"print(\"row\", len(business.iloc[0,0]))\nprint(\"sum\", len(business.iloc[0,1]))","6e1d03ff":"list_df = [business, entertainment, politics, sport, tech]\nlength = 0\nfor df in list_df:\n    length += len(df)","2d4ab3ee":"print(\"length of all data: \", length)","abf538cc":"bbc_df = pd.concat([business, entertainment, politics, sport, tech], ignore_index=True)\nlen(bbc_df)","7f35948b":"def cleantext(text):\n    text = str(text)\n    text=text.split()\n    words=[]\n    for t in text:\n        if t.isalpha():\n            words.append(t)\n    text=\" \".join(words)\n    text=text.lower()\n    text=re.sub(r\"what's\",\"what is \",text)\n    text=re.sub(r\"it's\",\"it is \",text)\n    text=re.sub(r\"\\'ve\",\" have \",text)\n    text=re.sub(r\"i'm\",\"i am \",text)\n    text=re.sub(r\"\\'re\",\" are \",text)\n    text=re.sub(r\"n't\",\" not \",text)\n    text=re.sub(r\"\\'d\",\" would \",text)\n    text=re.sub(r\"\\'s\",\"s\",text)\n    text=re.sub(r\"\\'ll\",\" will \",text)\n    text=re.sub(r\"can't\",\" cannot \",text)\n    text=re.sub(r\" e g \",\" eg \",text)\n    text=re.sub(r\"e-mail\",\"email\",text)\n    text=re.sub(r\"9\\\\\/11\",\" 911 \",text)\n    text=re.sub(r\" u.s\",\" american \",text)\n    text=re.sub(r\" u.n\",\" united nations \",text)\n    text=re.sub(r\"\\n\",\" \",text)\n    text=re.sub(r\":\",\" \",text)\n    text=re.sub(r\"-\",\" \",text)\n    text=re.sub(r\"\\_\",\" \",text)\n    text=re.sub(r\"\\d+\",\" \",text)\n    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n    \n    return text","b1b5308d":"for col in bbc_df.columns:\n    bbc_df[col] = bbc_df[col].apply(lambda x: cleantext(x))","5b47cc24":"bbc_df.head()","90ce5e4f":"df.head()","13a296ee":"len_list =[]\nfor article in df.row_article:\n    words = article.split()\n    length = len(words)\n    len_list.append(length)\nmax(len_list)","10548597":"import numpy as np\nimport os\nimport pandas as pd\nimport re","32c06a82":"articles = list(bbc_df.row_article)\nsummaries = list(bbc_df.summary)","97301b3e":"articles","9248de49":"# from sklearn.model_selection import train_test_split\n# art_train, art_test, sum_train, sum_test = train_test_split(pad_art_sequences, pad_sum_sequences, test_size=0.2)","41a17b57":"from keras.preprocessing.text import Tokenizer\nVOCAB_SIZE = 1999\ntokenizer = Tokenizer(num_words=VOCAB_SIZE)\ntokenizer.fit_on_texts(articles)\narticle_sequences = tokenizer.texts_to_sequences(articles)\nart_word_index = tokenizer.word_index\nlen(art_word_index)","b1c610b9":"\nprint(article_sequences[0][:20])\nprint(article_sequences[1][:20])\nprint(article_sequences[2][:20])","e0ec08bb":"# Vocabraly: article and summary 15000 words\nart_word_index_1500 = {}\ncounter = 0\nfor word in art_word_index.keys():\n    if art_word_index[word] == 0:\n        print(\"found 0!\")\n        break\n    if art_word_index[word] > VOCAB_SIZE:\n        continue\n    else:\n        art_word_index_1500[word] = art_word_index[word]\n        counter += 1\n","503e3cc5":"counter","488f4563":"tokenizer.fit_on_texts(summaries)\nsummary_sequences = tokenizer.texts_to_sequences(summaries)\nsum_word_index = tokenizer.word_index\nlen(sum_word_index)","825e4593":"sum_word_index_1500 = {}\ncounter = 0\nfor word in sum_word_index.keys():\n    if sum_word_index[word] == 0:\n        print(\"found 0!\")\n        break\n    if sum_word_index[word] > VOCAB_SIZE:\n        continue\n    else:\n        sum_word_index_1500[word] = sum_word_index[word]\n        counter += 1","97f38cf0":"\ncounter","a4bc153a":"#Padding: pad_sequences 1000 max_len\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_LEN = 400\npad_art_sequences = pad_sequences(article_sequences, maxlen=MAX_LEN, padding='post', truncating='post')","bc1928fa":"print(len(article_sequences[1]), len(pad_art_sequences[1]))","8e6892a2":"pad_sum_sequences = pad_sequences(summary_sequences, maxlen=MAX_LEN, padding='post', truncating='post')","f4b4bac1":"print(len(summary_sequences[1]), len(pad_sum_sequences[1]))","93c4bbe2":"pad_art_sequences.shape","0e46ad91":"pad_art_sequences","2243177d":"# Reshape: manual max_len * one-hot matrix\n\"\"\"\nencoder_inputs = np.zeros((2225, 400), dtype='float32')\nencoder_inputs.shape\n\ndecoder_inputs = np.zeros((2225, 400), dtype='float32')\ndecoder_inputs.shape\n\nfor i, seqs in enumerate(pad_art_sequences):\n    for j, seq in enumerate(seqs):\n        encoder_inputs[i, j] = seq\n        \nfor i, seqs in enumerate(pad_sum_sequences):\n    for j, seq in enumerate(seqs):\n        decoder_inputs[i, j] = seq\n\"\"\"","2cea8f7d":"decoder_outputs = np.zeros((2225,400, 2000), dtype='float32')\ndecoder_outputs.shape","aba30a9a":"for i, seqs in enumerate(pad_sum_sequences):\n    for j, seq in enumerate(seqs):\n        decoder_outputs[i, j, seq] = 1.","e3d4c0d2":"decoder_outputs.shape","2d4db3a8":"embeddings_index = {}\nwith open('..\/input\/glove6b50d\/glove.6B.50d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","31446f59":"def embedding_matrix_creater(embedding_dimention, word_index):\n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n          # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","2130b165":"art_embedding_matrix = embedding_matrix_creater(50, word_index=art_word_index_1500)\nart_embedding_matrix.shape","d3aaeb0d":"sum_embedding_matrix = embedding_matrix_creater(50, word_index=sum_word_index_1500)\nsum_embedding_matrix.shape","acc2d178":"from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nencoder_embedding_layer = Embedding(input_dim = 2000, \n                                    output_dim = 50,\n                                    input_length = MAX_LEN,\n                                    weights = [art_embedding_matrix],\n                                    trainable = False)","9c39ddfe":"decoder_embedding_layer = Embedding(input_dim = 2000, \n                                    output_dim = 50,\n                                    input_length = MAX_LEN,\n                                    weights = [sum_embedding_matrix],\n                                    trainable = False)","67435f98":"sum_embedding_matrix.shape","26408335":"pip install chart-studio","6a22c081":"# Building Encoder-Decoder Model\nfrom numpy.random import seed\nseed(1)\n\n\nfrom sklearn.model_selection import train_test_split\nimport logging\n\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pydot\n\n\nimport keras\nfrom keras import backend as k\nk.set_learning_phase(1)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras import initializers\nfrom keras.optimizers import RMSprop\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense,LSTM,Dropout,Input,Activation,Add,concatenate, Embedding, RepeatVector\nfrom keras.layers.advanced_activations import LeakyReLU,PReLU\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.optimizers import Adam","1fdd0638":"# Hyperparams\n\nMAX_LEN = 400\nVOCAB_SIZE =1999\nEMBEDDING_DIM = 50\nHIDDEN_UNITS = 200\nVOCAB_SIZE = VOCAB_SIZE + 1\n\nLEARNING_RATE = 0.9\nBATCH_SIZE = 128\nEPOCHS =20","405728de":"# encoder\nencoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\nencoder_embedding = encoder_embedding_layer(encoder_inputs)\nencoder_LSTM = LSTM(HIDDEN_UNITS)(encoder_embedding)\n# decoder\ndecoder_inputs = Input(shape=(MAX_LEN, ))\ndecoder_embedding = decoder_embedding_layer(decoder_inputs)\ndecoder_LSTM = LSTM(200)(decoder_embedding)\n# merge\nmerge_layer = concatenate([encoder_LSTM, decoder_LSTM])\ndecoder_outputs = Dense(units=VOCAB_SIZE+1, activation=\"softmax\")(merge_layer) # SUM_VOCAB_SIZE, sum_embedding_matrix.shape[1]\n\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","02273f5c":"#Training your model and Validate it\nimport numpy as np\nnum_samples = len(pad_sum_sequences)\ndecoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"int32\")","63406986":"# output\nfor i, seqs in enumerate(pad_sum_sequences):\n    for j, seq in enumerate(seqs):\n        if j > 0:\n            decoder_output_data[i][j][seq] = 1","33c78fe3":"art_train, art_test, sum_train, sum_test = train_test_split(pad_art_sequences, pad_sum_sequences, test_size=0.2)","49e9d556":"train_num = art_train.shape[0]\ntrain_num","b82c3b11":"target_train = decoder_output_data[:train_num]\ntarget_test = decoder_output_data[train_num:]","9630f810":"history = model.fit([art_train, sum_train], \n                     target_train, \n                     epochs=EPOCHS, \n                     batch_size=BATCH_SIZE,\n                     validation_data=([art_test, sum_test], target_test))","87485b4e":"# 2-2. Tokenizer\n    Tokenize and One-Hot : Tokenizer\n    Vocabraly: article and summary 15000 words\n    Padding: pad_sequences 1000 max_len\n    Reshape: manual max_len * one-hot matrix","1aa5ef9b":"Step 2. Preprocessing Text Data.\n\n    Clean Text\n    Tokenize\n    Vocabrary\n    Padding\n    One-Hot Encoding\n    Reshape to (MAX_LEN, One-Hot Encoding DIM)"}}