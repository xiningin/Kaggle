{"cell_type":{"a799b625":"code","f525b34d":"code","45a1f4a3":"code","195bc8ef":"code","f948221b":"code","2897e80e":"markdown","c8fe8672":"markdown","7387af0b":"markdown","0ad78893":"markdown","3042be8b":"markdown","a98efd4a":"markdown","077b48e2":"markdown","27772194":"markdown","2710570c":"markdown","b0f66013":"markdown","c9a9b7a7":"markdown"},"source":{"a799b625":"import pandas as pd\nfrom IPython.display import Image\nimport numpy as np\nfrom sklearn import preprocessing\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nmatplotlib.style.use('ggplot')","f525b34d":"np.random.seed(1)\ndf = pd.DataFrame({\n    'x1': np.random.normal(0, 2, 10000),\n    'x2': np.random.normal(5, 3, 10000),\n    'x3': np.random.normal(-5, 5, 10000)\n})\n\nscaler = preprocessing.StandardScaler()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\n\nax1.set_title('Before Scaling')\nsns.kdeplot(df['x1'], ax=ax1)\nsns.kdeplot(df['x2'], ax=ax1)\nsns.kdeplot(df['x3'], ax=ax1)\nax2.set_title('After Standard Scaler')\nsns.kdeplot(scaled_df['x1'], ax=ax2)\nsns.kdeplot(scaled_df['x2'], ax=ax2)\nsns.kdeplot(scaled_df['x3'], ax=ax2)\nplt.show()","45a1f4a3":"df = pd.DataFrame({\n    # positive skew\n    'x1': np.random.chisquare(8, 1000),\n    # negative skew \n    'x2': np.random.beta(8, 2, 1000) * 40,\n    # no skew\n    'x3': np.random.normal(50, 3, 1000)\n})\n\nscaler = preprocessing.MinMaxScaler()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=['x1', 'x2', 'x3'])\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(6, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(df['x1'], ax=ax1)\nsns.kdeplot(df['x2'], ax=ax1)\nsns.kdeplot(df['x3'], ax=ax1)\nax2.set_title('After Min-Max Scaling')\nsns.kdeplot(scaled_df['x1'], ax=ax2)\nsns.kdeplot(scaled_df['x2'], ax=ax2)\nsns.kdeplot(scaled_df['x3'], ax=ax2)\nplt.show()","195bc8ef":"x = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(20, 1, 1000), np.random.normal(1, 1, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(30, 1, 1000), np.random.normal(50, 1, 25)]),\n})\n\nscaler = preprocessing.RobustScaler()\nrobust_scaled_df = scaler.fit_transform(x)\nrobust_scaled_df = pd.DataFrame(robust_scaled_df, columns=['x1', 'x2'])\n\nscaler = preprocessing.MinMaxScaler()\nminmax_scaled_df = scaler.fit_transform(x)\nminmax_scaled_df = pd.DataFrame(minmax_scaled_df, columns=['x1', 'x2'])\n\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(9, 5))\nax1.set_title('Before Scaling')\nsns.kdeplot(x['x1'], ax=ax1)\nsns.kdeplot(x['x2'], ax=ax1)\nax2.set_title('After Robust Scaling')\nsns.kdeplot(robust_scaled_df['x1'], ax=ax2)\nsns.kdeplot(robust_scaled_df['x2'], ax=ax2)\nax3.set_title('After Min-Max Scaling')\nsns.kdeplot(minmax_scaled_df['x1'], ax=ax3)\nsns.kdeplot(minmax_scaled_df['x2'], ax=ax3)\nplt.show()","f948221b":"from mpl_toolkits.mplot3d import Axes3D\n\ndf = pd.DataFrame({\n    'x1': np.random.randint(-100, 100, 1000).astype(float),\n    'y1': np.random.randint(-80, 80, 1000).astype(float),\n    'z1': np.random.randint(-150, 150, 1000).astype(float),\n})\n\nscaler = preprocessing.Normalizer()\nscaled_df = scaler.fit_transform(df)\nscaled_df = pd.DataFrame(scaled_df, columns=df.columns)\n\nfig = plt.figure(figsize=(9, 5))\nax1 = fig.add_subplot(121, projection='3d')\nax2 = fig.add_subplot(122, projection='3d')\nax1.scatter(df['x1'], df['y1'], df['z1'])\nax2.scatter(scaled_df['x1'], scaled_df['y1'], scaled_df['z1'])\nplt.show()","2897e80e":"Note that the points are all brought within a sphere that is at most 1 away from the origin at any point. Also, the axes that were previously different scales are now all one scale.","c8fe8672":"All features are now on the same scale relative to one another.","7387af0b":"Notice that the skewness of the distribution is maintained but the 3 distributions are brought into the same scale so that they overlap.","0ad78893":"**Normalizer**\nThe normalizer scales each value by dividing each value by its magnitude in n-dimensional space for n number of features.\n\nSay your features were x, y and z Cartesian co-ordinates your scaled value for x would be:\n\nxi\/\u221ax2i+y2i+\nEach point is now within 1 unit of the origin on this Cartesian co-ordinate system.","3042be8b":"**Standard Scaler**\nThe StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1.\n\nThe mean and standard deviation are calculated for the feature and then the feature is scaled based on:\n\n**xi\u2013mean(x)\/stdev(x)**\nIf data is not normally distributed, this is not the best scaler to use.","a98efd4a":"Let\u2019s take a look at it in action:","077b48e2":"Notice that after Robust scaling, the distributions are brought into the same scale and overlap, but the outliers remain outside of bulk of the new distributions.\n\nHowever, in Min-Max scaling, the two normal distributions are kept seperate by the outliers that are inside the 0-1 range.","27772194":"**Min-Max Scaler**\nThe MinMaxScaler is the probably the most famous scaling algorithm, and follows the following formula for each feature:\n\n**xi\u2013min(x)\/max(x)\u2013min(x)**\nIt essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values).\n\nThis scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.\n\nHowever, it is sensitive to outliers, so if there are outliers in the data, you might want to consider the Robust Scaler below.","2710570c":"**Robust Scaler**\nThe RobustScaler uses a similar method to the Min-Max scaler but it instead uses the interquartile range, rathar than the min-max, so that it is robust to outliers. Therefore it follows the formula:\n\n**xi\u2013Q1(x)\/Q3(x)\u2013Q1(x)**\nFor each feature.\n\nOf course this means it is using the less of the data for scaling so it\u2019s more suitable for when there are outliers in the data.\n\nLet\u2019s take a look at this one in action on some data with outliers","b0f66013":"For now, let\u2019s see the min-max scaler in action","c9a9b7a7":"In this kernel we explore 3 methods of feature scaling that are implemented in scikit-learn:\n\n1. StandardScaler\n2. MinMaxScaler\n3. RobustScaler\n4. Normalizer"}}