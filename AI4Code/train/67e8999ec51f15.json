{"cell_type":{"1081ba05":"code","9496322c":"code","7a55bcf3":"code","e6767a18":"code","28db8519":"code","9d6da57f":"code","b58ce10e":"code","c33d1a1d":"code","9edba205":"code","1798012f":"code","0dadc252":"code","b7224cc0":"code","a34d31bf":"code","b63fb232":"code","1b99d2bb":"code","9f9db621":"code","2347c60e":"code","465a46f0":"code","a2dea3ed":"code","b52c23dc":"code","c70b6ab0":"code","a375845e":"code","6e831e13":"code","05b6d0dc":"code","aea7d3a2":"code","67195964":"code","e356bacc":"code","6f988d68":"code","767be8f1":"code","601cb9c5":"code","19b7064c":"code","c926e2d0":"code","893af294":"code","4c19e294":"code","29a991b1":"code","a10c05b7":"code","b26757c4":"code","1c83d87d":"code","c1a5b85e":"code","de5093ea":"code","699dc148":"code","ce39b1cd":"code","a64b862a":"code","55ea7a19":"code","078db473":"code","93f43748":"code","bc944431":"code","00d8574a":"code","07e97b8a":"code","139ebc67":"code","744b72d9":"code","6e0eceb8":"code","fe3f5335":"code","d0c2d259":"code","56f90c6b":"code","921f7904":"code","d13fe80f":"code","2f0d796e":"code","22d0058c":"code","074c045e":"markdown","a6b039a8":"markdown","f9e26067":"markdown","c18dd3dd":"markdown","22e4fe10":"markdown","4fd288e3":"markdown","92c06c0b":"markdown","66771e1c":"markdown","70317fbc":"markdown","0952f4ce":"markdown","ce9137a1":"markdown","86a9e216":"markdown","112cf563":"markdown","c23451b9":"markdown","ac18b257":"markdown","52ff0482":"markdown","3eadcf66":"markdown","6d18192f":"markdown","6eb8745f":"markdown","bbb7f776":"markdown","6467600b":"markdown","f0c598f2":"markdown","756c902f":"markdown","0013eded":"markdown","550b6a96":"markdown","7e6fa64f":"markdown","0dd7a23c":"markdown","4774490b":"markdown","758f1214":"markdown"},"source":{"1081ba05":"import os\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom gensim.models import Word2Vec\n\nfrom tqdm import tqdm\n\npd.set_option(\"display.max_columns\", 300)\npd.set_option(\"display.max_rows\", 300)","9496322c":"train = pd.read_csv('..\/input\/data-science-winter-osaka2\/train.csv')\ntest = pd.read_csv('..\/input\/data-science-winter-osaka2\/test.csv')","7a55bcf3":"train.head(10)","e6767a18":"type(train['popular_tags'][0])","28db8519":"def eval_data(row):\n    if row == row:\n        return eval(row)\n    else:\n        return np.nan\n\nfor column in ['popular_tags', 'categories', 'minimum_requirements', 'recommended_requirements']:\n    train[column] = train[column].apply(eval_data)\n    test[column] = test[column].apply(eval_data)","9d6da57f":"type(train['popular_tags'][0]), type(train['categories'][0]), type(train['minimum_requirements'][0]), type(train['recommended_requirements'][0])","b58ce10e":"train.info()","c33d1a1d":"train['release_date'].tail(50)","9edba205":"train['description'].tolist()[0]","1798012f":"train['user_reviews'].hist()","0dadc252":"from torchvision.io import read_image\n\nfor i in range(10):\n    img = read_image(os.path.join('..\/input\/data-science-winter-osaka2\/train\/train', train['img_path'][i]))\n    print(img.shape)\n    plt.imshow(img.permute(1, 2, 0))\n    plt.show()","b7224cc0":"train['is_train'] = True\ntest['user_reviews'] = np.nan\ntest['is_train'] = False\n\ndf = pd.concat([train, test], axis=0)\ndf = df.reset_index(drop=True)","a34d31bf":"def category_numeric_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    # \u30bf\u30b0\u306e\u6570\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    df['popular_tags_len'] = df['popular_tags'].fillna(\"\").apply(len)\n    df['categories_len'] = df['categories'].fillna(\"\").apply(len)\n    \n    # \u5024\u6bb5\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    df = price_preprocess(df)\n    \n    # developer\u3068publisher\u3092label encode\u3059\u308b\u3002\n    for column in ['developer', 'publisher']:\n        df = apply_label_encode(df, column)\n    \n    # \u5e74\u306b\u3088\u3063\u3066\u8a55\u4fa1\u304c\u5909\u308f\u308b\uff1f\n    # \u6b63\u898f\u8868\u73fe\u3067\u5e74\u3092\u62bd\u51fa\u3059\u308b\u3002\n    df['year'] = df['release_date'].str.extract(r'(\\d{4})')\n    df['year'] = df['year'].astype(float)\n    return df\n\ndef price_preprocess(df: pd.DataFrame) -> pd.DataFrame:\n    df['price'] = df['price'].replace({'free':0})\n    price_null_index = df[df['price']==''].index\n    df.loc[price_null_index, 'price'] = -1\n    df['price'] = df['price'].astype(float)\n    return df\n\ndef apply_label_encode(df:pd.DataFrame, column:str)-> pd.DataFrame:\n    label_df = df[~df[column].isnull()]\n    le = LabelEncoder()\n    le_data = le.fit_transform(label_df[column])\n    label_df_index = label_df.index\n    df.loc[label_df_index, f'{column}_label_encoding'] = le_data\n    return df","b63fb232":"df = category_numeric_preprocess(df)","1b99d2bb":"def tfidf_preprocess(df: pd.DataFrame, column: str, embedding_dim: int=20)-> pd.DataFrame:\n    # tfidf\u3067\u5909\u63db\u3057\u305f\u5f8c\u306b\u3001SVD\u3067\u6642\u9650\u524a\u6e1b\u3059\u308b\u3002\n    tfidf_vec = TfidfVectorizer(ngram_range=(1,2))\n    text_tfidf = tfidf_vec.fit_transform(df[column].fillna('').values.tolist())\n    svd = TruncatedSVD(n_components=embedding_dim, algorithm='arpack',random_state=9999)\n    return svd.fit_transform(text_tfidf)\n\nclass W2VSWEM(object):\n    def __init__(self, word_list: List[str], embedding_dim:int=20):\n        # word2vec\u3067\u7279\u5fb4\u91cf\u3092\u4f5c\u3063\u3066\u3001wrod2vec\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\u3059\u308b\u3002\n        self.word_list = word_list\n        self.embedding_dim = embedding_dim\n        self.__validation_type()\n        self.__train_w2v()\n        \n    def __validation_type(self):\n        if isinstance(self.word_list, pd.Series):\n            self.word_list = self.word_list.tolist()\n        if not isinstance(self.word_list, list):\n            raise TypeError(f'you should use list object, however you are using {type(word_list)}.')\n        \n    def __train_w2v(self):\n        self.w2v_model = Word2Vec(self.word_list, vector_size=self.embedding_dim, workers=1, seed=71)\n        self.vocab = self.w2v_model.wv.key_to_index\n        \n    def _get_single_column_vec(self, words) -> np.array:\n        result = []\n        \n        for word in words:\n            if word in self.vocab:\n                \n                vector = self.w2v_model.wv[word]\n                result.append(vector)\n        result = np.array(result)\n        return result\n    \n    def get_result(self):\n        self.swem_result = np.zeros([len(self.word_list), self.embedding_dim])\n        for num, i in enumerate(self.word_list):\n            swem = self._get_single_column_vec(i)\n            if len(swem):\n                swem = np.mean(swem, axis=0)\n            else:\n                swem = np.zeros(self.embedding_dim)\n            self.swem_result[num, :] = swem    ","9f9db621":"# word2vec\u3067tags\u3092encode\u3059\u308b\ntags_df = []\nfor column in ['categories', 'popular_tags']:\n    df[column] = df[column].fillna('')\n    w2v_swem = W2VSWEM(df[column].tolist())\n    w2v_swem.get_result()\n    result = w2v_swem.swem_result\n    _df = pd.DataFrame(result, columns=[f'w2v_{column}_{x}' for x in range(result.shape[1])])\n    tags_df.append(_df)\ntags_df = pd.concat(tags_df, axis=1)","2347c60e":"tags_df","465a46f0":"# \u30b2\u30fc\u30e0\u306e\u8aac\u660e\u3092tfidf\u3067\u7279\u5fb4\u91cf\u306b\u3059\u308b\ntext_svd = tfidf_preprocess(df, 'description')\ntext_svd_df = pd.DataFrame(text_svd, columns=[f'text_svd_{x}' for x in range(text_svd.shape[1])])","a2dea3ed":"#\u2605\"developer\"\u306e\u6b20\u6e2c\u5024\u3092\u57cb\u3081\u308b\u3002\u4ed6\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u5834\u5408\u3082\u6b20\u6e2c\u5024\u304c\u3042\u308b\u5834\u5408\u3082\u9069\u5f53\u306a\u5024\u3067\u306e\u57cb\u3081\u304c\u5fc5\u8981\u3067\u3059\ndf[\"developer\"].fillna(\"aaa\", inplace=True)\n#\u2605\"release_date\"\u306e\u6b20\u6e2c\u5024\u3092\u57cb\u3081\u308b\u3002\u6708\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u308b\u7528\u3067\u3059\u3002\u901a\u5e38\u306eBOW\u3067\u3082\u4f7f\u3048\u307e\u3059\ndf[\"release_date\"].fillna(\"aaa\", inplace=True)","b52c23dc":"#\u2605BOW\u3067\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u524d\u51e6\u7406\ndef str_normalize(ds):\n    # \u6587\u5b57\u30b3\u30fc\u30c9\u306e\u6b63\u898f\u5316(Unicode\u6b63\u898f\u5316)\n    ds = ds.str.normalize(\"NFKC\")\n    # \u5c0f\u6587\u5b57\u5316\uff08\u2605\u4f8b\u3048\u3070upper\u3060\u3068\u5168\u90e8\u3001\u5927\u6587\u5b57\u306b\u306a\u308a\u307e\u3059\uff01\uff09\n    ds = ds.apply(lambda x: x.lower())\n    # \u6539\u884c\u3092\u9664\u53bb\n    ds = ds.str.replace(\"\\r\\n\", \" \")\n    ds = ds.str.replace(\"\\n\", \" \")\n    ds = ds.str.replace(\"\\r\", \" \")\n    ds = ds.str.replace(\"\\u3000\", \" \")  # \u5168\u89d2\u30b9\u30da\u30fc\u30b9\n    # \u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u3068\u6570\u5b57\u306e\u307f\u306b\u3059\u308b\uff08\u2605\u4f8b\u3048\u30700-9\u3092\u7121\u304f\u3059\u3068\u3001\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u306e\u307f\u306b\u306a\u308a\u307e\u3059\uff01\uff09\n    ds = ds.str.replace(\"[^a-zA-Z0-9]+\", \" \", regex=True)\n    # \u524d\u5f8c\u306e\u7a7a\u767d\u3092\u524a\u9664\n    ds = ds.apply(lambda x: x.strip())\n    return ds\n\ndf['developer_norm'] = str_normalize(df['developer'])\ndf['release_date_norm'] = str_normalize(df['release_date'])","c70b6ab0":"#\u2605\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u5f62\u614b\u7d20\u89e3\u6790\u3067\u3059\u3002\u3053\u308c\u306f\u3088\u304f\u7406\u89e3\u3067\u304d\u3066\u307e\u305b\u3093\nimport nltk\nnltk.download('punkt')  # \u5206\u304b\u3061\u66f8\u304d\uff08word_tokenize\uff09\nnltk.download('averaged_perceptron_tagger')  # \u54c1\u8a5e\uff08pos_tag\uff09\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nlemmatizer = nltk.stem.WordNetLemmatizer()  # \u898b\u51fa\u3057\u8a9e\nstemmer = nltk.stem.PorterStemmer()         # \u8a9e\u5e79\u5316\nstopwords = nltk.corpus.stopwords.words('english')  #stopword\nstopwords_dict = {k:1 for k in stopwords}\n\n#--- \u5f62\u614b\u7d20\u89e3\u6790\u7528\u306e\u95a2\u6570\ndef _morphological_analysis(text):\n    words = []\n    \n    # \u5358\u8a9e\u306b\u5206\u89e3\n    morph = nltk.word_tokenize(text)\n    for word, tag in nltk.pos_tag(morph):\n\n        # 2\u6587\u5b57\u306e\u5358\u8a9e\u306f\u9664\u5916\uff08\u2605\u3053\u3053\u306f\u8272\u3005\u5909\u3048\u3066\u898b\u307e\u3057\u305f\uff01\uff09\n        if len(word) <= 2:\n            continue\n\n        # stopword \u306b\u767b\u9332\u3055\u308c\u3066\u3044\u308b\u5358\u8a9e\u306f\u9664\u5916\n        if word in stopwords_dict:\n            continue\n        # \u898b\u51fa\u3057\u8a9e\n        if tag.startswith('NN'):  # \u540d\u8a5e\n            word = lemmatizer.lemmatize(word, pos=\"n\")\n        elif tag.startswith('VB'):  # \u52d5\u8a5e\n            word = lemmatizer.lemmatize(word, pos=\"v\")\n        elif tag.startswith('JJ'):  # \u5f62\u5bb9\u8a5e\n            word = lemmatizer.lemmatize(word, pos=\"a\")\n        elif tag.startswith('RB'):  # \u526f\u8a5e\n            word = lemmatizer.lemmatize(word, pos=\"r\")\n        \n        # \u8a9e\u5e79\u5316\n        word = stemmer.stem(word)\n\n        words.append(word)\n        \n        # \u30b9\u30da\u30fc\u30b9\u533a\u5207\u308a\u3067\u51fa\u529b\n    return \" \".join(words)\n\n# \u5b9f\u884c\ndf[\"developer_token\"] = [_morphological_analysis(text) for text in tqdm(df[\"developer_norm\"])]\ndf[\"release_date_token\"] = [_morphological_analysis(text) for text in tqdm(df[\"release_date_norm\"])]","a375845e":"# \u5168\u5358\u8a9e\u306e\u500b\u6570\u3092\u6570\u3048\u308b\u3002\u2605\u3053\u3053\u3092\u8272\u3005\u5909\u3048\u3066\u898b\u307e\u3057\u305f\nword_count = {}\nfor text in df[\"developer_token\"].values:\n    for word in text.split(\" \"):\n        if word not in word_count:\n            word_count[word] = 0\n        word_count[word] += 1\nsorted_word_count = sorted(word_count.items(), key=lambda x:x[1])\n\n# stopword\ndef _stopword_remove(text):\n    words = []\n    for word in text.split(\" \"):\n\n        # \u51fa\u73fe\u56de\u6570\u304cn\u56de\u4ee5\u4e0b\u306f\u9664\u5916\uff08\u2605\u3053\u308c\u3092\u5909\u3048\u3066\u51fa\u73fe\u56de\u6570\u304c\u591a\u3044\uff1d\u4f55\u304b\u3057\u3089\u5f71\u97ff\u304c\u3042\u308b\uff1f\u306e\u5224\u65ad\u3057\u3066\u307e\u3057\u305f\uff09\n        if word_count[word] <= 500:\n            continue\n\n        words.append(word)\n\n    return \" \".join(words)\n\ndf[\"developer_token_stop\"] = df[\"developer_token\"].apply(_stopword_remove)","6e831e13":"#\u2605\u5168\u5358\u8a9e\u306e\u500b\u6570\u3092\u6570\u3048\u308b\u3002\u2605\u3053\u3053\u3092\u8272\u3005\u5909\u3048\u3066\u898b\u307e\u3057\u305f\nword_count = {}\nfor text in df[\"release_date_token\"].values:\n    for word in text.split(\" \"):\n        if word not in word_count:\n            word_count[word] = 0\n        word_count[word] += 1\nsorted_word_count = sorted(word_count.items(), key=lambda x:x[1])\n\n# stopword\ndef _stopword_remove(text):\n    words = []\n    for word in text.split(\" \"):\n\n        # \u51fa\u73fe\u56de\u6570\u304cn\u56de\u4ee5\u4e0b\u306f\u9664\u5916\uff08\u2605\u3053\u308c\u3092\u5909\u3048\u3066\u51fa\u73fe\u56de\u6570\u304c\u591a\u3044\uff1d\u4f55\u304b\u3057\u3089\u5f71\u97ff\u304c\u3042\u308b\uff1f\u306e\u5224\u65ad\u3057\u3066\u307e\u3057\u305f\uff09\n        if word_count[word] <= 500:\n            continue\n\n        words.append(word)\n\n    return \" \".join(words)\n\ndf[\"release_date_token_stop\"] = df[\"release_date_token\"].apply(_stopword_remove)","05b6d0dc":"#\u2605developer_token_stop\u3092BOW\u3092\u7528\u3044\u3066\u30ab\u30a6\u30f3\u30c8\u3059\u308b\nimport sklearn.feature_extraction\nbow = sklearn.feature_extraction.text.CountVectorizer()\ndeveloper_bow = bow.fit_transform(df.developer_token_stop)","aea7d3a2":"#\u2605DataFlame\u5f62\u5f0f\u306b\u5909\u63db\ndeveloper_bow = pd.DataFrame(developer_bow.toarray(), columns=bow.get_feature_names())","67195964":"#\u2605recommended_requirements_token_stop\u3092BOW\u3092\u7528\u3044\u3066\u30ab\u30a6\u30f3\u30c8\u3059\u308b\nimport sklearn.feature_extraction\nbow = sklearn.feature_extraction.text.CountVectorizer()\nrelease_date_bow = bow.fit_transform(df.release_date_token_stop)","e356bacc":"#\u2605DataFlame\u5f62\u5f0f\u306b\u5909\u63db\nrelease_date_bow = pd.DataFrame(release_date_bow.toarray(), columns=bow.get_feature_names())","6f988d68":"#\u2605\u7b97\u51fa\u306b\u4f7f\u3063\u305f\u4e0d\u8981\u30ab\u30e9\u30e0\u3092Drop\ndf.drop(['developer','developer_norm','developer_token','developer_token_stop'], axis=1, inplace=True)\ndf.drop(['release_date','release_date_norm','release_date_token','release_date_token_stop'], axis=1, inplace=True)","767be8f1":"#\u2605\u30ab\u30e9\u30e0\u540d\u306b_dev,_dat\u3092\u8ffd\u52a0\uff08\u4ed6\u306eBOW\u3068\u540c\u4e00\u30ab\u30e9\u30e0\u3092\u5206\u3051\u308b\u305f\u3081\uff09\ndeveloper_bow = developer_bow.rename(columns=lambda x: x + '_bow_dev')\nrelease_date_bow = release_date_bow.rename(columns=lambda x: x + '_bow_dat') ","601cb9c5":"#\u2605developer\u304b\u3089\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u3067\u3059\ndeveloper_bow.head(2)","19b7064c":"#\u2605release_date\u304b\u3089\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u3067\u3059\u3002\u3053\u306e\u30ea\u30b9\u30c8\u3092\u898b\u3066\u4ee5\u4e0b\u306e\u6708\u306b\u632f\u308a\u5206\u3051\u308b\u7279\u5fb4\u91cf\u3092\u6c7a\u3081\u307e\u3057\u305f\nrelease_date_bow.head(2)","c926e2d0":"#\u2605\u6708\u3092Month\u7279\u5fb4\u91cf\u306b\u307e\u3068\u3081\u308b\nconditions = [(release_date_bow['apr_bow_dat'] == 1),(release_date_bow['aug_bow_dat'] == 1),\n              (release_date_bow['dec_bow_dat'] == 1),(release_date_bow['feb_bow_dat'] == 1),\n              (release_date_bow['jan_bow_dat'] == 1),(release_date_bow['jul_bow_dat'] == 1),\n              (release_date_bow['jun_bow_dat'] == 1),(release_date_bow['mar_bow_dat'] == 1),\n              (release_date_bow['may_bow_dat'] == 1),(release_date_bow['nov_bow_dat'] == 1),\n              (release_date_bow['oct_bow_dat'] == 1),(release_date_bow['sep_bow_dat'] == 1)]\nchoices = ['Apr','Aug','Dec','Feb','Jan','Jul','Jun','Mar','May','Nov','Oct','Sep']\nrelease_date_bow['Month'] = np.select(conditions, choices, default = 'No_Season')","893af294":"#\u2605Month\u7279\u5fb4\u91cf\u3060\u3051\u3092\u53d6\u308a\u51fa\u3057\u3066\u307e\u3059\nMonth = release_date_bow['Month']","4c19e294":"#Starter\u306e\u307e\u307e\nnlp_df = pd.concat([tags_df, text_svd_df], axis=1)","29a991b1":"#Starter\u306e\u307e\u307e\nuse_df = df.copy()\nuse_columns = ['name', 'price', 'popular_tags_len', 'categories_len', 'year', 'developer_label_encoding', 'publisher_label_encoding', 'user_reviews', 'is_train']\nuse_df = use_df[use_columns]\n","a10c05b7":"#\u2605developer_bow\u3001Month\u3092\u8ffd\u52a0\u3057\u3066\u307e\u3059\u3002release_date_bow\u3092\u8ffd\u52a0\u3059\u308b\u5834\u5408\u306fMonth\u3092\u8ffd\u52a0\u3057\u306a\u3044\u3067\u304f\u3060\u3055\u3044\uff08\u30ab\u30e9\u30e0\u304c\u3060\u3076\u308b\u304b\u3089\uff09\nuse_df = pd.concat([use_df, nlp_df, developer_bow, Month], axis=1)","b26757c4":"#\u2605developer_bow\u3001Month\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u3066\u304f\u3060\u3055\u3044\nuse_df ","1c83d87d":"#\u2605category_encoders\u306b\u3066CountEncoder\uff08\u767b\u5834\u56de\u6570\u306b\u7f6e\u63db\uff09\u3000mugen88\u306e\u516c\u958b\u30b3\u30fc\u30c9\u3088\u308a\nfrom category_encoders import OrdinalEncoder, CountEncoder, OneHotEncoder\ncol = 'Month' #\u3053\u3053\u3092\u5909\u66f4\u3057\u3066\u307f\u3066\u4e0b\u3055\u3044\u3002 \nencoder = CountEncoder()\nencoder.fit(use_df[col])\nuse_df['%s_count'%col] = encoder.transform(use_df[col])","c1a5b85e":"#\u2605\u4e0d\u8981\u306b\u306a\u3063\u305f'Month'\u3092Drop\nuse_df.drop(['Month'], axis=1, inplace=True)","de5093ea":"#\u2605\u3061\u3083\u3093\u3068\u51fa\u6765\u305f\u304b\u3001\u78ba\u8a8d\nuse_df","699dc148":"#Groupby\u3067\u96c6\u7d04\u306e\u5143\u306b\u306a\u308b'year'\u30ab\u30e9\u30e0\u3092\u30ab\u30c6\u30b4\u30ea\u5909\u6570\uff08str\uff09\u306b\u3059\u308b\u3002\u6570\u5024\u5909\u6570\uff08float\uff09\u306e\u307e\u307e\u3060\u3068\u3067\u304d\u306a\u3044\n\nuse_df['year_str'] = use_df['year'].astype(object)","ce39b1cd":"#Year\u6bce\u306e\u5e73\u5747\u4fa1\u683c\u306e\u7279\u5fb4\u91cf\u8ffd\u52a0\n#'mean'\u306e\u6240\u3092\u5909\u66f4\u3059\u308b\u3068\u69d8\u3005\u306a\u7279\u5fb4\u91cf\u306e\u96c6\u7d04\u30ab\u30e9\u30e0\u3092\u4f5c\u308c\u307e\u3059\u3002\u8907\u6570\u306e\u7279\u5fb4\u91cf\u304c\u8ffd\u52a0\u3067\u304d\u307e\u3059\n#'median'\u306f\u30e1\u30c7\u30a3\u30a2\u30f3\u3001'max'\u306f\u6700\u5927\u3001'min'\u306f\u6700\u5c0f\u3001'sum'\u306f\u5408\u8a08\u3001'std'\u306f\u6a19\u6e96\u504f\u5dee\u3001'var'\u306f\u5206\u6563\u3067\u3059\n\nuse_df['YearPriceMean'] = use_df.groupby(\"year_str\")[\"price\"].transform('mean')","a64b862a":"#'year_str'\u306f\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306a\u306e\u3067\u305d\u306e\u307e\u307e\u4f7f\u3048\u306a\u3044\u306e\u3067\u3001Drop\u3057\u3066\u304a\u304d\u307e\u3059\nuse_df = use_df.drop(columns=['year_str'])","55ea7a19":"#\u4e00\u756a\u53f3\u306b\u3001'YearPriceMean'\u306e\u30ab\u30e9\u30e0\u304c\u8ffd\u52a0\u3055\u308c\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\nuse_df.head(5)","078db473":"train = use_df[use_df['is_train']==True].reset_index(drop=True)\ntest =  use_df[use_df['is_train']!=True].reset_index(drop=True)","93f43748":"train = train.drop(columns=['is_train'])\ntest = test.drop(columns=['is_train', 'user_reviews'])","bc944431":"train.shape, test.shape","00d8574a":"X = train.copy().drop(columns='user_reviews')\ny = train['user_reviews']","07e97b8a":"y = y.map({'c0':0, 'c1':1, 'c2':2})","139ebc67":"from typing import Optional, List\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\n\nclass TreeModel:\n    \"\"\"LGB\/XGB\u306eAPI\u3092\u7d71\u4e00\u3057\u305fwrapper\"\"\"\n\n    def __init__(self, model_type: str):\n        self.model_type = model_type\n        self.trn_data = None\n        self.val_data = None\n        self.model = None\n\n    def train(self,\n              params: dict,\n              X_train: pd.DataFrame,\n              y_train: np.ndarray,\n              X_val: pd.DataFrame,\n              y_val: np.ndarray,\n              train_weight: Optional[np.ndarray] = None,\n              val_weight: Optional[np.ndarray] = None,\n              train_params: dict = {}):\n        if self.model_type == \"lgb\":\n            self.trn_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n            self.val_data = lgb.Dataset(X_val, label=y_val, weight=val_weight)\n            self.model = lgb.train(params=params,\n                                   train_set=self.trn_data,\n                                   valid_sets=[self.trn_data, self.val_data],\n                                   **train_params)\n        elif self.model_type == \"xgb\":\n            self.trn_data = xgb.DMatrix(X_train, y_train, weight=train_weight, enable_categorical=True)\n            self.val_data = xgb.DMatrix(X_val, y_val, weight=val_weight, enable_categorical=True)\n            self.model = xgb.train(params=params,\n                                   dtrain=self.trn_data,\n                                   evals=[(self.trn_data, \"train\"), (self.val_data, \"val\")],\n                                   **train_params)\n        else:\n            raise NotImplementedError\n        return self.model\n\n    def predict(self, X: pd.DataFrame):\n        if self.model_type == \"lgb\":\n            return self.model.predict(X, num_iteration=self.model.best_iteration)  # type: ignore\n        elif self.model_type == \"xgb\":\n            X_DM = xgb.DMatrix(X)\n            return self.model.predict(X_DM)  # type: ignore\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_names_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_name()\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").keys())\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_importances_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_importance(importance_type=\"gain\")\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").values())\n        else:\n            raise NotImplementedError","744b72d9":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\n\n\nfeature_importances = pd.DataFrame()\nscores=0.0\nnum_fold=5\noof = pd.DataFrame()\n    \nskf = StratifiedKFold(n_splits=num_fold, random_state=1213, shuffle=True)\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(\"*\" * 100)\n    print(f\"Fold: {fold}\")\n\n    X_trn = X.iloc[trn_idx].drop(columns='name')\n    X_val = X.iloc[val_idx].drop(columns='name')\n    y_trn = y.iloc[trn_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)\n        \n\n    model = TreeModel(model_type='lgb')\n    params = {\n                \"objective\": \"softmax\",\n                \"boosting\": \"gbdt\",\n                \"num_class\":3,\n                \"learning_rate\": 0.1,\n                'metric': 'multi_logloss',\n                'num_leaves': 5,\n                'verbose': 1,\n                'lambda_l2': 3,\n                'random_state': 71,\n            }\n    model.train(params=params,\n                X_train=X_trn,\n                y_train=y_trn.values.astype('float32'),\n                X_val=X_val,\n                y_val=y_val.values.astype('float32'),\n                train_params={\n                                \"num_boost_round\": 20000,\n                                \"early_stopping_rounds\": 100,\n                                \"verbose_eval\": 1000,\n                })\n    fi_tmp = pd.DataFrame()\n    fi_tmp[\"feature\"] = model.feature_names_\n    fi_tmp[\"importance\"] = model.feature_importances_\n    fi_tmp[\"fold\"] = fold\n    feature_importances = feature_importances.append(fi_tmp)\n\n    val_pred = model.predict(X_val)\n    score = log_loss(y_val, val_pred)\n    scores += score \/ num_fold\n\n    pred = model.predict(test.drop(columns='name'))\n    if fold == 0:\n        prediction = np.copy(pred) \/ num_fold\n    else:\n        prediction += pred \/ num_fold\n    print(f\"score: {score:.5f}\")\n    oof = oof.append(pd.DataFrame({\"name\": X.loc[val_idx, \"name\"], \"preds_c0\": val_pred[:, 0], \n                                   \"preds_c1\": val_pred[:, 1],\"preds_c2\": val_pred[:, 2]}))\nprint(f'average_score: {scores:.5f}')","6e0eceb8":"oof['prediction'] = np.argmax(np.array(oof.iloc[:, 1:]), axis=1)","fe3f5335":"from sklearn.metrics import accuracy_score\naccuracy_score(y, oof.sort_index()['prediction'])","d0c2d259":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 30))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(f\"LGBM importance, average auc score {score}\")\nplt.tight_layout()","56f90c6b":"#\u4e88\u6e2c\u306e\u4e2d\u8eab\u3092\u78ba\u8a8d\u3057\u307e\u3059\nprediction","921f7904":"#\uff11\u884c\u306e3\u3064\u306e\u6570\u5b57\u304b\u3089\u4e00\u756a\u5927\u304d\u306a\u3082\u306e\u3092\u9078\u629e\ntest_prediction = np.argmax(prediction, axis=1)\n#DataFlame\u5f62\u5f0f\u306b\u5909\u3057\u307e\u3059\npred_df = pd.DataFrame(test_prediction) \n#\u305d\u306e\u7d50\u679c\u3067C0\u3001C1\u3001C2\u304c\u6c7a\u307e\u3063\u3066\u3044\u308b\npred_df = pred_df[0].map({0:'c0', 1:'c1', 2:'c2'})","d13fe80f":"pred_df","2f0d796e":"sub = pd.read_csv('..\/input\/data-science-winter-osaka2\/sample_submission.csv')\nsub['user_reviews'] = pred_df.tolist()\nsub","22d0058c":"sub.to_csv('submission.csv', index=False)","074c045e":"\"developer\"\u3001\"release_date\"\u306e\u30ab\u30a6\u30f3\u30c8\u3092\uff22\uff2f\uff37\u3067\u5b9f\u65bd\u3057\u3001\"Month\"\u306e\u7279\u5fb4\u91cf\u3092\u8ffd\u52a0\u3057\u305f\u4e8b\u4f8b\u3067\u3059","a6b039a8":"groupby\u3067\u306f\u3042\u308b\u96c6\u7d04\u5143\u5909\u6570\u3067\u306e\u7d71\u8a08\u91cf\u3092\u7b97\u51fa\u3067\u304d\u307e\u3059\n\u4f8b\u3048\u3070\u3001\u8ca9\u58f2\u5e74\u6bce\u306e\u5e73\u5747\u4fa1\u683c\u3092\u7b97\u51fa\u3059\u308b\u30b3\u30fc\u30c9\u3092\u4ee5\u4e0b\u306b\u66f8\u304d\u307e\u3057\u305f\n\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u5143\u306b\u6570\u5024\u30c7\u30fc\u30bf\u306e\u96c6\u7d04\u5909\u6570\u3092\u7b97\u51fa\u3057\u305f\u3044\u5834\u5408\u306f\u4fbf\u5229\u3060\u3068\u601d\u3044\u307e\u3059","f9e26067":"\u4ed6\u306b\u3082\u3001developer, publisher\u306f\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3001price\u306f\u6570\u5024\u5909\u6570\u3001popular_tags, categories\u306f\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u30ea\u30b9\u30c8\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\n\u305d\u3053\u3067\u3001\u3053\u308c\u3089\u306e\u524d\u51e6\u7406\u3092\u3053\u306enotebook\u3067\u306f\u884c\u3044\u307e\u3059\u3002","c18dd3dd":"## CV\u306eaccuracy\u3092\u51fa\u3059","22e4fe10":"\u5024\u6bb5\u3068publisher,\u5e74\u304c\u5927\u304d\u306a\u5f71\u97ff\u3092\u53ca\u307c\u3057\u3066\u3044\u307e\u3059\u3002","4fd288e3":"\u6b20\u640d\u3057\u3066\u306a\u3044\u306e\u306f\u3001\u540d\u524d\u3068\u3001categories, \u753b\u50cf\u3001\u6b63\u89e3\u30c7\u30fc\u30bf\u306e\u307f\u3067\u3001\u305d\u308c\u4ee5\u5916\u306f\u6b20\u640d\u304c\u3042\u308b\u3068\u5206\u304b\u308a\u307e\u3059\u3002<br>\n\nrelease_date\u306e\u6700\u5f8c\u306e\u65b9\u3092\u898b\u3066\u307f\u308b\u3068\u3001","92c06c0b":"Target encodeing\u3092\u3057\u306a\u3044\u306e\u3067\u3001dataframe\u3092\u7d50\u5408\u3057\u3066\u524d\u51e6\u7406\u3092\u3057\u307e\u3059\u3002","66771e1c":"call of duty\u306e\u8aac\u660e\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u8aac\u660e\u306e\u6587\u5b57\u5217\u304c\u5165\u3063\u3066\u3044\u308b\u306e\u3067\u3001NLP\u306e\u524d\u51e6\u7406\u304c\u9069\u5fdc\u3067\u304d\u305d\u3046\u3067\u3059\u3002","70317fbc":"## \u2606\u3053\u3053\u304b\u3089\u306f[Strater with groupby]\u3067\u516c\u958b\u6e08\u307f","0952f4ce":"\u4ee5\u4e0a\u3067\u30c7\u30fc\u30bf\u304c\u5143\u306e\u5f62\u3067\u8aad\u307f\u8fbc\u3081\u307e\u3057\u305f\u3002\u6b20\u640d\u30c7\u30fc\u30bf\u3092\u78ba\u304b\u3081\u307e\u3059\u3002","ce9137a1":"## \u8a00\u8a9e\u7cfb\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u308b","86a9e216":"\u3068\u6587\u5b57\u5217\u306e\u307f\u304c\u5165\u3063\u3066\u3044\u308b\u3082\u306e\u3084\u3001\u5927\u96d1\u628a\u306a\u5b63\u7bc0\u3001\u5e74\u306e\u307f\u304c\u5165\u3063\u3066\u3044\u308b\u3082\u306e\u306a\u3069\u5168\u3066\u306b\u65e5\u4ed8\u304c\u5165\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u3053\u3068\u304c\u5206\u304b\u308a\u307e\u3059\u3002<br>\n\u65e5\u4ed8\u306b\u3064\u3044\u3066\u306f\u524d\u51e6\u7406\u3092\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002<br>","112cf563":"description\u3082\u898b\u3066\u307f\u307e\u3059","c23451b9":"## \u2606\u3053\u3053\u307e\u3067","ac18b257":"## \u30e2\u30c7\u30eb\u4f5c\u6210\naccuracy\u304cmetric\u3067\u3059\u304c\u3001\u4e88\u6e2c\u78ba\u7387\u3092\u51fa\u3057\u30665 fold\u306e\u5e73\u5747\u3092\u53d6\u308a\u305f\u3044\u306e\u3067\u3001log_loss\u3067\u30e2\u30c7\u30eb\u3092\u4e00\u5ea6\u4f5c\u6210\u3057\u307e\u3059\u3002","52ff0482":"0\u304c\u30dd\u30b8\u30c6\u30a3\u30d6\u3001\uff11\u304c\u30cd\u30ac\u30c6\u30a3\u30d6\u30012\u304c\u30ec\u30d3\u30e5\u30fc\u306a\u3057\u306a\u306e\u3067\u3001\u30ec\u30d3\u30e5\u30fc\u304c\u306a\u3044\u306e\u304c\u6700\u3082\u591a\u3088\u3046\u3067\u3059\u3002<br>\n\u753b\u50cf\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002","3eadcf66":"# Submisson file\u3092\u4f5c\u308b","6d18192f":"## \u2605\u2605\u3053\u3053\u307e\u3067\u306b\u306a\u308a\u307e\u3059","6eb8745f":"popular_tags\u3068categories\u3001minimum_requirements\u3001recommended_requirements\u306flist\u3068dict\u30c7\u30fc\u30bf\u3063\u307d\u3044\u3002","bbb7f776":"# \u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u8efd\u304fEDA\u3092\u3057\u3066\u307f\u308b","6467600b":"# \u2605\u5370\u307e\u3067\u306fStarter\u306e\u307e\u307e\u3067\u3059","f0c598f2":"# \u2605\u3053\u3053\u304b\u3089\u2605\u2605\u307e\u3067\u8ffd\u52a0\u3057\u307e\u3057\u305f","756c902f":"\u5148\u65e5\u306e\u4e2d\u9593\u78ba\u8a8d\u4f1a\u3067\u5c71\u672c\u5148\u751f\u304c\u53e4\u5178\u7684\u306a\u65b9\u6cd5\u3082\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306a\u3069\u306e\u6570\u5024\u5316\u306b\u6709\u52b9\u306a\u5834\u5408\u304c\u3042\u308b\u3068\u306e\u3053\u3068\u306a\u306e\u3067\u3001\uff22\uff2f\uff37\u3067\u306e\u7279\u5fb4\u91cf\u8ffd\u52a0\u3068\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u306e\u751f\u6210\u30b3\u30fc\u30c9\u3092\u5171\u6709\u3057\u307e\u3059\u3002\n\uff37\uff12\uff36\u306e\u30e9\u30f3\u30c0\u30e0\u8981\u7d20\u306f\u56fa\u5b9a\u3057\u3066\u3044\u307e\u3059","0013eded":"\u77e5\u3063\u3066\u308b\u30b2\u30fc\u30e0\u304c\u51fa\u3066\u304f\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\uff08\u79c1\u306fcall of duty\u3068fallout\u306f\u77e5\u3063\u3066\u3044\u307e\u3057\u305f\uff09<br>\n\u3053\u308c\u3089\u306e\u753b\u50cf\u30c7\u30fc\u30bf\u306f215x460\u306e\u30b5\u30a4\u30ba\u3067\u3042\u308b\u3068\u5206\u304b\u308a\u307e\u3059\u3002<br>\n\u753b\u50cf\u3092\u6570\u5024\u5316\u3059\u308b\u3053\u3068\u3067\u3001\u4f55\u304b\u5bc4\u4e0e\u3059\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","550b6a96":"# \u524d\u51e6\u7406\u3092\u884c\u3046\n## \u30ab\u30c6\u30b4\u30ea\u3001\u6570\u5024","7e6fa64f":"target\u306e\u5206\u5e03\u3092\u898b\u3066\u307f\u307e\u3059","0dd7a23c":"# \u2605\u2605\u307e\u3067\u8ffd\u52a0\u3057\u307e\u3057\u305f","4774490b":"\u5143\u3005list\u30c7\u30fc\u30bf\u3060\u3063\u305f\u306e\u304c\u3001csv\u306b\u3057\u305f\u3053\u3068\u3067\u6587\u5b57\u5217\u306b\u5909\u63db\u3055\u308c\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002<br>\n\u5143\u3005\u306e\u30c7\u30fc\u30bf\u306b\u623b\u3059\u305f\u3081\u306b\u3001eval\u95a2\u6570\u3092\u9069\u5fdc\u3057\u3066\u3044\u304d\u307e\u3059\u3002","758f1214":"# \u5b66\u7fd2\u3055\u305b\u308b"}}