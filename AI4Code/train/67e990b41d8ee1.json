{"cell_type":{"72ce3d21":"code","068cf1c4":"code","cb96ba65":"code","78c59254":"code","9f40b2cf":"code","34747cae":"code","bc8b27a6":"code","b6de7981":"code","407aedcd":"code","0031b833":"code","52fc68ea":"code","16de7ddf":"code","82278493":"code","ae670eb0":"code","cf5db4c2":"code","d124cd8d":"code","5135ee46":"code","05a6c514":"code","5a5eb9a0":"code","e869a1cb":"code","2e803975":"code","2f69fefb":"code","4d10c391":"code","b91ee746":"code","013a8e24":"code","78de47fa":"code","d684ee6b":"code","b785c4e3":"code","7cc6f795":"code","d25656fb":"code","dc135d84":"code","34e3badc":"code","053d93c5":"code","6a4e6b16":"code","1141c117":"code","8ff9bf9b":"code","92017d44":"code","ec327da2":"code","cb986f69":"code","3abaf3a7":"code","1ee8fa62":"code","24a598bc":"code","f1d35047":"code","ef28e9c1":"code","d39c1b0e":"code","aa8e855f":"code","fbe61a6d":"code","0eb3f9f2":"code","fcaebcd2":"code","16962489":"code","b8499ff9":"code","e3291187":"code","c5e7a749":"code","17fea9d0":"code","3f3e5777":"code","fdb7e831":"code","19a9f71c":"code","8ef9e491":"code","3dd2ede5":"code","fa554c11":"code","288cd545":"code","e1188541":"code","b6797ea4":"code","0816d682":"code","e4f6a039":"code","fae49abc":"code","720a9e2f":"code","f3a8a076":"code","e7dd59c7":"code","d4f6794d":"code","3c3e0ca5":"code","9b2feb0b":"code","c68b94e0":"code","c27e7c68":"code","866c3e9d":"code","82db363a":"code","a8d816ca":"code","890227ba":"code","73c24489":"code","40c5c25f":"markdown","4f9c604d":"markdown","c3cf3a6d":"markdown","8d5d54d9":"markdown","898d15f9":"markdown","ea08f32d":"markdown","cdf29b24":"markdown","414ec76b":"markdown","735a3c85":"markdown","840742ad":"markdown","51f974c5":"markdown","f747824b":"markdown","b6c9e683":"markdown","77785d13":"markdown","979a0131":"markdown","6d97549e":"markdown","fe09f363":"markdown","66d9bfb8":"markdown","b3a30a2f":"markdown","d5e5e00a":"markdown","91592642":"markdown","b1585934":"markdown"},"source":{"72ce3d21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # l|inear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","068cf1c4":"import tqdm\nfrom tqdm import tqdm\nfrom keras_tqdm import TQDMNotebookCallback\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf","cb96ba65":"from keras.models import Model,Sequential\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM \nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam ,Adagrad ,RMSprop\nfrom keras.losses import sparse_categorical_crossentropy","78c59254":"train_d=pd.read_csv('..\/input\/english-to-french\/small_vocab_en.csv',sep=\"\\t\",header=None,names=['Text'])\ntrain_d=train_d.rename(columns={'0':'En_text'})\ntest_d=pd.read_csv('..\/input\/english-to-french\/small_vocab_fr.csv',sep=\"\\t\",header=None,names=['Text'])\ntest_d=test_d.rename(columns={'0':'Fr_text'})","9f40b2cf":"test_d.head(10)","34747cae":"def remove_stopwords(sentence,language='english'):\n    \"\"\"\n    Parameters\n    ----------\n    sentence : string\n        String containing text\n    language : str, optional\n        The language code to be used for filtering stopwords. The default is 'english'.\n\n    Raises\n    ------\n    TypeError\n        When wrong input to the fucntion.\n\n    Returns\n    -------\n    filtered_sentence : str\n        The filtered sentence\n\n    \"\"\"\n\n    if sentence is type(str):\n            import nltk\n            nltk.download('stopwords')\n            from nltk.corpus import stopwords\n            from nltk import word_tokenize\n            stop_words = stopwords.words(language)\n            word_tokens = word_tokenize(sentence) \n            filtered_sentence =' '.join(map(str,[w for w in word_tokens if not w in stop_words]))\n            return filtered_sentence\n    else:\n        raise TypeError(\"Expected type str\")\n    ","bc8b27a6":"def words_distribution(dataframe,topx=10,stopwords=False,stopwordslist=None):\n    \"\"\"\n    \n\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        The dataframe containing text.\n    topx : int, optional\n        Display the top n words in the given distribution. The default is 10.\n    stopwords : bool, optional\n        Stopwords in various lagnuages. The default is False.\n    stopwordslist : TYPE, optional\n        DESCRIPTION. The default is None.\n\n    Raises\n    ------\n    TypeError\n        When wrong type of iterable is entered in the given fucntion\n\n    Returns\n    -------\n    None.\n\n    \"\"\"\n    \n    if  isinstance(dataframe,pd.DataFrame):\n        if topx is not type(int):\n            from nltk import FreqDist\n            from nltk import word_tokenize\n            sens=[sen[0] for sen in dataframe.values if sen!=']' or sen!='[']\n            sens=[''.join(sen[0].lower()) for sen in dataframe.values if sen!=']' or sen!='[']\n            words=word_tokenize(str([sens[i] for i in range(len(sens))]))\n            freq=FreqDist(words)\n            freq.plot(topx)\n        else:\n                 raise TypeError(\"Expected type int got type {0}\".format(type(topx)))\n                \n    else:\n        raise TypeError(\"Expected type pd.DataFrame got type {0}\".format(type(dataframe)))          ","b6de7981":"\n#words_distribution(dataframe=test_d)","407aedcd":"def tokenize(dataframe,char_level=False):\n    \"\"\"\n    Parameters\n    ----------\n    dataframe : pd.Dataframe\n        Dataframe to generate tokens\n    char_level : bool\n        Create character level or word level tokens. The default is False.\n\n    Raises\n    ------\n    TypeError\n        When character level is not type bool\n\n    Returns\n    -------\n    text_sequences : list\n        A list containing the tokenized vocabulary\n    tk : keras_preprocessing.text.Tokenizer\n       Keras tokenizer\n\n    \"\"\"\n    if  isinstance(dataframe,pd.DataFrame):\n        data=np.array(dataframe.values).ravel()\n        if char_level is not type(bool):\n                from keras.preprocessing.text import Tokenizer\n                if char_level==False:\n                    tk=Tokenizer(lower=True ,char_level=False)\n                    tk.fit_on_texts(data)\n                    text_sequences=tk.texts_to_sequences(data)\n                else:\n                    tk=Tokenizer(lower=True ,char_level=True)\n                    tk.fit_on_texts(data)\n                    text_sequences=tk.texts_to_sequences(data)  \n                return text_sequences,tk\n        else:\n            TypeError(\"Expected type bool got type {0}\".format(type(topx)))\n    else:\n        raise TypeError(\"Expected type pd.DataFrame got type {0}\".format(type(dataframe))) ","0031b833":"def pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    from keras.preprocessing.sequence import pad_sequences\n    return pad_sequences(x, maxlen=length, padding='post',value=0.0) #Pad at the end","52fc68ea":"sequences,tokenizer=tokenize(test_d,char_level=False)","16de7ddf":"len(tokenizer.word_index)","82278493":"padseq=pad(sequences,10)","ae670eb0":"def preprocess(x, y):\n    \"\"\"\n    Preprocess x and y\n    :param x: Feature List of sentences\n    :param y: Label List of sentences\n    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n    \"\"\"\n    preprocess_x, x_tk = tokenize(x)\n    preprocess_y, y_tk = tokenize(y)\n    \n    preprocess_x = pad(preprocess_x)\n    preprocess_y = pad(preprocess_y)\n\n    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n\n    return preprocess_x, preprocess_y, x_tk, y_tk","cf5db4c2":"preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n    preprocess(train_d,test_d)","d124cd8d":"preproc_french_sentences.shape[::]","5135ee46":"english_tokenizer.word_index","05a6c514":"def logits_to_text(logits, tokenizer):\n    \"\"\"\n    Turn logits from a neural network into text using the tokenizer\n    :param logits: Logits from a neural network\n    :param tokenizer: Keras Tokenizer fit on the labels\n    :return: String that represents the text of the logits\n    \"\"\"\n    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n    index_to_words[0] = '<PAD>'\n\n    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])","5a5eb9a0":"padding_english_sent = pad(preproc_english_sentences,preproc_french_sentences.shape[1])\npadding_english_sent= padding_english_sent.reshape((-1, preproc_french_sentences.shape[-2], 1)) #Both should be of same dimensions","e869a1cb":"padding_english_sent.shape","2e803975":"english_vocab_size = len(english_tokenizer.word_index)\nfrench_vocab_size = len(french_tokenizer.word_index)\n#print(\"Prediction:\")\n#print(logits_to_text(model.predict(np.ndarray('How are you doing'), french_tokenizer)))\n#padding_english_sent.shape\npreproc_french_sentences.shape\n#padding_english_sent[:1][0]\n","2f69fefb":"def gru_rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n        model = Sequential()\n        model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n        model.add(TimeDistributed(Dense(1024, activation='tanh')))\n        model.add(Dropout(0.5))\n        model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n        # Compile model\n        model.compile(loss=sparse_categorical_crossentropy,\n                      optimizer=Adam(0.0054),\n                      metrics=['accuracy','sparse_categorical_crossentropy'])\n        return model","4d10c391":"\nmodel = gru_rnn_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nprint(model.summary())\n\n","b91ee746":"padding_english_sent.shape","013a8e24":"from keras.callbacks.callbacks import EarlyStopping\nes=EarlyStopping(monitor='val_accuracy',mode='max',patience=5) #Stop when model loss cannot reach a greater minmum value","78de47fa":"history=model.fit(padding_english_sent, preproc_french_sentences, batch_size=1024, epochs=30,validation_split=0.2,callbacks=None)","d684ee6b":"def plot_model_performance(*criteria):\n    for c in criteria:\n            fig, axes= plt.subplots()\n            axes.plot(history.history[''+c])\n            axes.plot(history.history['val_'+c])\n            axes.set_title('Model '+c)\n            axes.set_ylabel(''+c)\n            axes.set_xlabel('Epoch')\n            axes.legend(['Train', 'Test'], loc='upper right')\n    return axes\n          \n\n","b785c4e3":"gru=plot_model_performance('accuracy','loss')","7cc6f795":"model.save('my_model.h5')","d25656fb":"def lstm_rnn_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n    model = Sequential()\n    model.add(LSTM(256, input_shape=input_shape[1:], return_sequences=True))\n    model.add(TimeDistributed(Dense(1024, activation='relu')))\n    #model.add(Dropout(0.5))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.0054),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model","dc135d84":"model = lstm_rnn_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nmodel.summary()","34e3badc":"history=model.fit(padding_english_sent, preproc_french_sentences, batch_size=1024, epochs=30,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","053d93c5":"axes=plot_model_performance('accuracy','loss')","6a4e6b16":"padding_english_sent= pad(preproc_english_sentences, preproc_french_sentences.shape[1])\npadding_english_sent.shape","1141c117":"def gru_embedding_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size+1,output_dim=128,input_length=input_shape[1:][0]))\n    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True,recurrent_dropout=True))\n    model.add(TimeDistributed(Dense(1024, activation='tanh')))\n    model.add(Dropout(0.6))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.001),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model","8ff9bf9b":"modele = gru_embedding_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nmodele.summary()","92017d44":"history=modele.fit(padding_english_sent,preproc_french_sentences, batch_size=1024, epochs=35,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","ec327da2":"axes=plot_model_performance('accuracy','loss')","cb986f69":"weights=np.asarray(modele.layers[0].get_weights())\nW=weights.reshape(200,128)","3abaf3a7":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nWnorm=sc.fit_transform(W)","1ee8fa62":"l=list(english_tokenizer.word_index.keys())","24a598bc":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig, ax = plt.subplots(figsize=(40,30))  \nax.set_yticklabels(english_tokenizer.word_index.keys()) \nax.set_xticklabels(np.arange(0,128))\nsns.heatmap(Wnorm,cmap=\"Accent_r\",linewidths=1,square=True,ax=ax)\n\n","f1d35047":"import matplotlib.pyplot as plt\nsns.clustermap(W,cmap='Accent',metric=\"cosine\")\n","ef28e9c1":"from mpl_toolkits.mplot3d import axes3d\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\ntsne=TSNE(n_components=2, perplexity=5,  learning_rate=200.0, n_iter=10000, metric='cosine', verbose=1,random_state=24)\nX=tsne.fit_transform(W)\nwords = list(english_tokenizer.word_index.keys())\n","d39c1b0e":"%matplotlib widget","aa8e855f":"from sklearn.manifold import TSNE\nimport seaborn as sns\ntsne=TSNE(n_components=3, perplexity=5,  learning_rate=200.0, n_iter=10000, metric='cosine', verbose=1,random_state=24)\nX=tsne.fit_transform(W)\nwords = list(english_tokenizer.word_index.keys())\n","fbe61a6d":"from mpl_toolkits import mplot3d\nax = plt.axes(projection='3d')\nx=X[:,0]\ny=X[:,1]\nz=X[:,2]\n\nax.scatter(x,y,z,c=z,cmap='hsv')\nfor x,y,z,i in zip(x,y,z,range(len(words))):\n    ax.text(x,y,z,words[i])\n#for i, word in enumerate(words):\n#    plt.annotate(word, xy=(X[i, 0], X[i, 1]))\n#plt.show()","0eb3f9f2":"FrW=modele.layers[4].weights[0]\nW=FrW.numpy()\nWfr=W.reshape(344,1024)","fcaebcd2":"%matplotlib inline","16962489":"import matplotlib.pyplot as plt\nfig, axes = plt.subplots(figsize=(80,70))  \nsns.heatmap(Wfr,linewidths=1,square=True,ax=axes)\naxes.set_yticklabels(list(french_tokenizer.word_index.keys())) \n\n\n","b8499ff9":"import matplotlib.pyplot as plt\nsns.clustermap(Wfr,cmap='Accent',metric=\"cosine\")","e3291187":"tsne=TSNE(n_components=3, perplexity=5,  learning_rate=200.0, n_iter=10000, metric='cosine', verbose=1,random_state=24)\nXfr=tsne.fit_transform(Wfr)","c5e7a749":"%matplotlib widget","17fea9d0":"ax = plt.axes(projection='3d')\nx=Xfr[:,0]\ny=Xfr[:,1]\nz=Xfr[:,2]\nwordsfr=list(french_tokenizer.word_index.keys())\nax.scatter(x,y,z,c=z,cmap='hsv')\nfor x,y,z,i in zip(x,y,z,range(len(wordsfr))):\n    ax.text(x,y,z,wordsfr[i])","3f3e5777":"from gensim.models.keyedvectors import KeyedVectors\nfilename = '\/kaggle\/input\/googlevec\/GoogleNews-vectors-negative300.bin'\nmodel = KeyedVectors.load_word2vec_format(filename, binary=True)","fdb7e831":"def get_Vector(str):\n    if str in model:\n             return model[str][:256]\n    else:\n        return None","19a9f71c":"s=len(english_tokenizer.word_index)+1","8ef9e491":"embedding_matrix = np.zeros((s,256))\nfor word, i in english_tokenizer.word_index.items():\n         embedding_vector = get_Vector(word)\n         if embedding_vector is not None:\n                 embedding_matrix[i] = embedding_vector\n        \n            ","3dd2ede5":"from keras.initializers import Constant","fa554c11":"def gru_pretrained_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size+1,output_dim=256,input_length=input_shape[1:][0],embeddings_initializer=Constant(embedding_matrix),trainable=False))\n    model.add(GRU(256, input_shape=input_shape[1:], return_sequences=True))\n    model.add(TimeDistributed(Dense(1024, activation='tanh')))\n    model.add(Dropout(0.6))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.001),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model","288cd545":"modele = gru_pretrained_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\nmodele.summary()","e1188541":"history=modele.fit(padding_english_sent,preproc_french_sentences, batch_size=1024, epochs=35,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","b6797ea4":"res=logits_to_text(modele.predict(padding_english_sent[:1])[0], french_tokenizer).replace('<PAD>','').strip()","0816d682":"res","e4f6a039":"print(padding_english_sent[:1][0],end=\"\\n\\n\")\nprint(preproc_french_sentences[:1][0].reshape(1,21))","fae49abc":"logits_to_text(preproc_french_sentences[:1][0].reshape(1,21),french_tokenizer)","720a9e2f":"axes=plot_model_performance('accuracy','loss')","f3a8a076":"#weights=np.asarray(modele.layers[0].get_weights())\n#W2Vec=weights.reshape(200,128)\n#from sklearn.preprocessing import StandardScaler\n#sc=StandardScaler()\n#W2Vecnorm=sc.fit_transform(W2Vec)","e7dd59c7":"#import matplotlib.pyplot as plt\n#import seaborn as sns\n#fig, ax = plt.subplots(figsize=(40,30))  \n#sns.heatmap(W2Vecnorm,cmap=\"Accent_r\",linewidths=1,square=True,ax=ax)\n#ax.set_yticklabels(list(english_tokenizer.word_index.keys())) \n#ax.set_xticklabels(np.arange(0,128))","d4f6794d":"#from sklearn.metrics.pairwise import cosine_similarity as cosine\n#list=[]\n#for i in tqdm(range(0,200)):\n#    list.append(cosine(Wnorm[i].reshape(1,128),W2Vecnorm[i].reshape(1,128)))","3c3e0ca5":"\"\"\"def bidirectional_gru_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):   \n    model = Sequential()\n    model.add(Embedding(input_dim=english_vocab_size+1,output_dim=128,input_length=input_shape[1:][0]))\n    model.add(Bidirectional(LSTM(256, input_shape=input_shape[1:], return_sequences=True,recurrent_dropout=0.2)))\n    model.add(TimeDistributed(Dense(1024, activation='tanh')))\n    model.add(Dropout(0.6))\n    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax'))) \n\n    # Compile model\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(0.001),\n                  metrics=['accuracy','sparse_categorical_crossentropy'])\n    return model\"\"\"","9b2feb0b":"#modelBi=bidirectional_gru_model(padding_english_sent.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index),len(french_tokenizer.word_index))\n#modelBi.summary()","c68b94e0":"#history=modelBi.fit(padding_english_sent,preproc_french_sentences, batch_size=1024, epochs=35,validation_split=0.2,callbacks=[TQDMNotebookCallback()])","c27e7c68":"#axes=plot_model_performance('accuracy','loss')","866c3e9d":"#modelBi.save('BILSTMRNN.h5')","82db363a":"\nimport tensorflow\ntensorflow.random.set_seed(1)\n\nfrom numpy.random import seed\nseed(1)\n","a8d816ca":"\ndef encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n    embedding_size = 128\n    rnn_cells = 200\n    dropout = 0.0\n    learning_rate = 1e-3\n    from keras.layers import LSTM\n    encoder_input_seq = Input(shape=input_shape[1:], name=\"enc_input\")\n \n    # Encoder (Return the internal states of the RNN -> 1 hidden state for GRU cells, 2 hidden states for LSTM cells))\n    encoder_output, state_t = GRU(units=rnn_cells, \n                                  dropout=dropout,\n                                  return_sequences=False,\n                                  return_state=True,\n                                  name=\"enc_rnn\")(encoder_input_seq)\n          #or for LSTM cells: encoder_output, state_h, state_c = LSTM(...)\n        \n    # Decoder Input   \n    decoder_input_seq = RepeatVector(output_sequence_length)(encoder_output)\n\n    # Decoder RNN (Take the encoder returned states as initial states)\n    decoder_out = GRU(units=rnn_cells,\n                      dropout=dropout,\n                      return_sequences=True,\n                      return_state=False)(decoder_input_seq, initial_state=state_t)\n                                         #or for LSTM cells: (decoder_input_seq, initial_state=[state_h, state_c])\n    \n    # Decoder output \n    logits = TimeDistributed(Dense(units=french_vocab_size))(decoder_out) \n    \n    # Model\n    model = Model(encoder_input_seq, Activation('softmax')(logits))\n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(lr=learning_rate),\n                  metrics=['accuracy'])\n     \n    return model    ","890227ba":"tmp_x = pad(preproc_english_sentences,preproc_french_sentences.shape[1])\ntmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\nencdec_rnn_model = encdec_model(input_shape = tmp_x.shape,\n                                output_sequence_length =preproc_french_sentences.shape[1],\n                                english_vocab_size = english_vocab_size+1,\n                                french_vocab_size = french_vocab_size+1)","73c24489":"encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)","40c5c25f":"# Part III \nEncoder-Decoder models .\nConsisting of a rnn acting as encoder and another acting as decoder","4f9c604d":"Created a list of sequences with number\n\nCreates dictionary of** * Word *:*Frequency***\n\n**Note:0 is reserved for Padding**\n","c3cf3a6d":"THIS PADDING STEP IS A BIT DIFFERENT FROM  OTHER MODELS","8d5d54d9":"Basically trying to interpret the model by visualizing the last layer","898d15f9":"# Now final layer weights","ea08f32d":"**GRU PLOT**\n![](http:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/12dd26f9c68a2dc8aab60bb9627d9440d4e6952b)\n**Original Paper**\n                    [https:\/\/arxiv.org\/pdf\/1406.1078.pdf](http:\/\/)","cdf29b24":"Add multiprocessing\/threading to make this nigga faster\n","414ec76b":"** Model with Embedding**\n","735a3c85":"Embedding Layer Visualized","840742ad":"**Trying Early Stopping**\n\nHowever,has no effect since loss is high and also not overfitting as much. An overall bad model tbh.","51f974c5":"Visualize the English vocabulary","f747824b":"Still use tanh ","b6c9e683":"# Word Embeddings Visualized","77785d13":"**LSTM PLOT**\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n\nAlso note GRU is faster to train than LSTM due to reduced gates and also less number of Parameters(813K vs 879K)","979a0131":"Steps to be performed :\n**Tokenizing**\n**Paddding to prevent length issues.**","6d97549e":"Description of Pre-Trained Embeddings\n> The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors.\n\nI try to use Word2Vec pretrained embeddings from google in custom GRU model. Then compare these Word2Vec embeddings to the embeddings created by older GRU model","fe09f363":"Usage defined below","66d9bfb8":"# PreTrained Word Embeddings","b3a30a2f":"Thx to KerasDocs for explaining how tio load them .\nWas particulary keen on Gensim having used them b4","d5e5e00a":"# PART II\n\nAs we can clearly see that the loss metrics for both the GRU and LSTM RNN's are really high despite an approx accuracy around 82.xx%\nWith this aim I try and explore other models which can further reduce the SPRSCATCRENTRPY Loss\n\n## BiGRU","91592642":"This honestly made no sense.\n- So let us try and visualize them in 3D","b1585934":"Verifying padding"}}