{"cell_type":{"580c432b":"code","53c79810":"code","fb13c898":"code","692cf09f":"code","e0d26b5a":"code","133aa275":"code","111a86e7":"code","4c324b20":"code","a64b24c5":"code","810d124e":"code","ff69e2c4":"code","5bbbd167":"code","d9b4b254":"code","2b77e191":"code","5e2daef1":"code","8fdf4b3c":"code","98322a3d":"code","9c374d4d":"markdown","d4836a21":"markdown","69d1950e":"markdown","285f1875":"markdown","ea671b2d":"markdown"},"source":{"580c432b":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd  \nimport seaborn as sns \nfrom sklearn.datasets import load_boston\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","53c79810":"train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)","fb13c898":"# found that one of the test data is in the train set and removed it\n# Dropping the last row which is 2011-01-01 00:00:00\ntrain=train.loc[~(train['date_time']=='2011-01-01 00:00:00')].reset_index(drop=True)","692cf09f":"print('train shape:',train.shape)","e0d26b5a":"all_data = pd.concat([train, test])\n# convert to datatime format\nall_data['date_time'] = pd.to_datetime(all_data['date_time'])\nall_data.head()","133aa275":"all_data.isnull().sum()","111a86e7":"import math\n\ndef pb_add(X):\n    X['day'] = X.date_time.dt.weekday\n    is_odd = (X['sensor_4'] < 646) & (X['absolute_humidity'] < 0.238)\n    X['is_odd'] = is_odd\n    diff = X['date_time'] - min(X['date_time'])\n    trend = diff.dt.days\n    X['f1s'] = np.sin(trend * 2 * math.pi \/ (365 * 1)) \n    X['f1c'] = np.cos(trend * 2 * math.pi \/ (365 * 1))\n    X['f2s'] = np.sin(2 * math.pi * trend \/ (365 * 2)) \n    X['f2c'] = np.cos(2 * math.pi * trend \/ (365 * 2)) \n    X['f3s'] = np.sin(2 * math.pi * trend \/ (365 * 3)) \n    X['f3c'] = np.cos(2 * math.pi * trend \/ (365 * 3)) \n    X['f4s'] = np.sin(2 * math.pi * trend \/ (365 * 4)) \n    X['f4c'] = np.cos(2 * math.pi * trend \/ (365 * 4)) \n    X['fh1s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 1))\n    X['fh1c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 1))\n    X['fh2s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 2))\n    X['fh2c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 2))\n    X['fh3s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 3))\n    X['fh3c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 3))\n    \n    sensor_features = [\n        'deg_C', \n        'relative_humidity', 'absolute_humidity', \n        'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5' ]\n    \n    lags = [-1, -4, -24, -7 * 24]  # last 4 hours, last 2 days, last 4 weeks\n    for sensor_feature in sensor_features:\n        this = X[sensor_feature]\n        # look back\n        for lag in lags:\n            feature = f'{sensor_feature}_{abs(lag)}b'\n            this_f = X[sensor_feature].shift(lag)\n            X[feature] = (this_f - this).fillna(0)\n        # look forwards\n        for lag in lags:\n            feature = f'{sensor_feature}_{abs(-lag)}f'\n            this_f = X[sensor_feature].shift(-lag)\n            X[feature] = (this_f - this).fillna(0)\n            \n    return X","4c324b20":"# The months will be used for folds split\nmonths = all_data[\"date_time\"].dt.month[:len(train)]\n## New idea\nall_data[\"hour\"] = all_data[\"date_time\"].dt.hour\nall_data[\"working_hours\"] =  all_data[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\nall_data[\"is_weekend\"] = (all_data[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\nall_data['hr'] = all_data.date_time.dt.hour*60+all_data.date_time.dt.minute\nall_data['satday'] = (all_data.date_time.dt.weekday==5).astype(\"int\")\nall_data[\"SMC\"] = (all_data[\"absolute_humidity\"] * 100) \/ all_data[\"relative_humidity\"]\nall_data.drop(columns = 'hour', inplace = True)\n\n\n# convert datetime to timestamp(s)\n# included::: all_data['time'] = all_data['date_time'].astype(np.int64)\/\/10**9\n\nall_data = pb_add(all_data.copy())\n\nall_data.drop(columns = 'date_time', inplace = True)","a64b24c5":"X=all_data[:len(train)].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\ny=all_data[:len(train)][['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\ny_log=np.log(y)\nX_test=all_data[len(train):].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\nprint('X_train shape:', X.shape)\nprint('y_train shape:', y.shape)\nprint('X_test shape:', X_test.shape)","810d124e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)\nX_scaled = scaler.transform(X)\nX_test_scaled = scaler.transform(X_test)","ff69e2c4":"X_scaled=X.values\nX_test_scaled=X_test.values","5bbbd167":"X_scaled","d9b4b254":"# load submission\npreds = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","2b77e191":"# Sets of hyperparameters optimized by Optuna for each target\ncb_params = [\n                {'learning_rate': 0.010169009412219588,\n                 'l2_leaf_reg': 8.908337085912136,\n                 'bagging_temperature': 8.384477224270551,\n                 'random_strength': 1.950237493637981,\n                 'depth': 6,\n                 'grow_policy': 'Lossguide',\n                 'leaf_estimation_method': 'Newton'},\n                {'learning_rate': 0.166394867169309,\n                 'l2_leaf_reg': 8.704675157564441,\n                 'bagging_temperature': 3.340826164726799,\n                 'random_strength': 1.538518016574368,\n                 'depth': 2,\n                 'grow_policy': 'Depthwise',\n                 'leaf_estimation_method': 'Newton'},\n                {'learning_rate': 0.028141156076957437,\n                 'l2_leaf_reg': 3.116523267336638,\n                 'bagging_temperature': 4.420661209459851,\n                 'random_strength': 1.8011752694610028,\n                 'depth': 6,\n                 'grow_policy': 'Depthwise',\n                 'leaf_estimation_method': 'Newton'},\n            ]","5e2daef1":"%%time\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom catboost import CatBoostRegressor\n\nall_fi = []\nsplits = 10\ntarget_names=y_log.columns\n\nfor i, target in enumerate(target_names):\n    print(f\"\\nTraining for {target}...\")\n    logo = LeaveOneGroupOut()\n    oof_preds = np.zeros((X_scaled.shape[0],))\n    model_preds = 0\n    model_fi = 0\n    for num, (train_idx, valid_idx) in enumerate(logo.split(X_scaled, y_log, months)):\n        X_train, X_valid = X_scaled[[train_idx]], X_scaled[[valid_idx]]\n        y_train, y_valid = y_log.loc[train_idx, target], y_log.loc[valid_idx, target]\n        model = CatBoostRegressor(random_state=42,\n                                 thread_count=4,\n                                 verbose=False,\n                                 loss_function='RMSE',\n                                 eval_metric='RMSE',\n                                 od_type=\"Iter\",\n                                 early_stopping_rounds=500,\n                                 use_best_model=True,\n                                 iterations=10000,\n                                 **cb_params[i])\n        model.fit(X_train, y_train,\n                  eval_set=(X_valid, y_valid),\n                  verbose=False)\n        model_preds += np.exp(model.predict(X_test_scaled)) \/ splits\n        model_fi += model.feature_importances_\n        oof_preds[valid_idx] = np.exp(model.predict(X_valid))\n        print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.exp(y_valid), oof_preds[valid_idx]))}\")\n    print(f\"\\nOverall RMSLE: {np.sqrt(mean_squared_log_error(np.exp(y_log[target]), oof_preds))}\")    \n    preds[target] = model_preds\n    all_fi.append(dict(zip(X.columns, model_fi)))","8fdf4b3c":"preds.head()","98322a3d":"preds.to_csv('submission.csv', index=False)","9c374d4d":"# Copied from CatBoost_14Feature_Cross_Validation\n\nAlmost no work by me - just a bit of FE.\n\nsee https:\/\/www.kaggle.com\/andy6804tw\/catboost-13feature-cross-validation\n\nRemoved the time feature: all_data['date_time'].astype(np.int64)\/\/10**9\n\nAdded some lags and fourier features (to try to pick up the season component).\n","d4836a21":"## original CV\n\n* Training for target_carbon_monoxide...\n* * Fold 0 RMSLE: 0.0778258850819034\n* * Fold 1 RMSLE: 0.09567414611737445\n* * Fold 2 RMSLE: 0.12458770150082436\n* * Fold 3 RMSLE: 0.11865213752846315\n* * Fold 4 RMSLE: 0.10702740862606132\n* * Fold 5 RMSLE: 0.1022360313603644\n* * Fold 6 RMSLE: 0.09396926069065796\n* * Fold 7 RMSLE: 0.08563271122743246\n* * Fold 8 RMSLE: 0.1750379107364064\n* * Fold 9 RMSLE: 0.278176029653778\n\n* Overall RMSLE: 0.1398755852324709\n\n* Training for target_benzene...\n* * Fold 0 RMSLE: 0.10108794639781213\n* * Fold 1 RMSLE: 0.08710269182125585\n* * Fold 2 RMSLE: 0.09114492935249197\n* * Fold 3 RMSLE: 0.08179494264044823\n* * Fold 4 RMSLE: 0.08300192021616676\n* * Fold 5 RMSLE: 0.08387077658443987\n* * Fold 6 RMSLE: 0.09327116316158779\n* * Fold 7 RMSLE: 0.08067460865192715\n* * Fold 8 RMSLE: 0.09538156034307499\n* * Fold 9 RMSLE: 0.11808006284554567\n\n* Overall RMSLE: 0.09190958831600804\n\n* Training for target_nitrogen_oxides...\n* * Fold 0 RMSLE: 0.18152157734387575\n* * Fold 1 RMSLE: 0.2140913436184166\n* * Fold 2 RMSLE: 0.19964470707304033\n* * Fold 3 RMSLE: 0.24475069072351055\n* * Fold 4 RMSLE: 0.2232802232637754\n* * Fold 5 RMSLE: 0.49218598179725737\n* * Fold 6 RMSLE: 0.4173263400059904\n* * Fold 7 RMSLE: 0.2017441021227966\n* * Fold 8 RMSLE: 0.2208801570446175\n* * Fold 9 RMSLE: 0.5116902845744751\n\n* Overall RMSLE: 0.3194398990409419","69d1950e":"## Save Predict File","285f1875":"## LeaveOneGroupOut\ncat_features=[\"working_hours\",\"is_weekend\",\"satday\"] 8\u30019\u300111\n\nPublic Score: \n- 0.19336 with 13 feature \n - ['deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5', 'working_hours', 'is_weekend', 'time']\n\ninside test error:\n- 0.18441005 with 13 features (0.14049546445212416+0.09160805296249219+0.3211266334764782)","ea671b2d":"## Train Model"}}