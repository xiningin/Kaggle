{"cell_type":{"9a1ab86c":"code","1800254e":"code","bbd58ef8":"code","6f897931":"code","b1d5767b":"code","846a3d4f":"code","7baed94b":"code","5f7c1bcd":"code","66693908":"code","eedd5e96":"code","8c85448c":"code","cd3c4e32":"code","a3345e6f":"code","e407ab4f":"code","759f627e":"code","6cb5fa06":"code","bcdd455f":"code","6f25cf2a":"code","829cd9f4":"code","bc5a3a78":"code","ae78d4ea":"code","1f73fa2c":"code","a53b0a3c":"code","9cc66d29":"markdown","fc3cee44":"markdown","6ecd350d":"markdown","f7c4ce8c":"markdown","c4ff2bbe":"markdown","de27d55b":"markdown","fee49bd4":"markdown","d4eaad22":"markdown","3fa9bd77":"markdown","ca0e3f7a":"markdown","262b143f":"markdown","2062e056":"markdown","85374267":"markdown","a0472471":"markdown","cfd4704a":"markdown","5b22a151":"markdown","b635e485":"markdown"},"source":{"9a1ab86c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1800254e":"import pandas as pd\n\ndf = pd.read_csv('..\/input\/donorsprediction\/Raw_Data_for_train_test.csv')\n\ndf.head()","bbd58ef8":"df.columns[df.isnull().any()]","6f897931":"# Fill numeric rows with the median\nfor label, content in df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Fill missing numeric values with median since it's more robust than the mean\n            df[label] = content.fillna(content.median())\n            \ndf.columns[df.isnull().any()]","b1d5767b":"df.info()","846a3d4f":"# Turn categorical variables into numbers\nfor label, content in df.items():\n    # Check columns which aren't numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # print the columns that are objectt type \n        print(label)\n        df[label] = pd.Categorical(content).codes+1","7baed94b":"df.head()","5f7c1bcd":"df = df.drop('TARGET_D', axis=1)\ndf.head()","66693908":"# input features\nx = df.drop('TARGET_B', axis=1)\n\n# Target variable\ny = df['TARGET_B']\n\nx.head()","eedd5e96":"y.head()","8c85448c":"# Import standard scaler\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\n# apply scaler\nx = ss.fit_transform(x)\n\nx","cd3c4e32":"from sklearn.model_selection import train_test_split\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# define and configure the model\nmodel = KNeighborsClassifier()\n\n# fit the model\nmodel.fit(xtrain, ytrain)\n\n# evaluate the model\npreds = model.predict(xtest)\naccuracy_score(ytest, preds)","a3345e6f":"from sklearn.ensemble import RandomForestClassifier\n\n# define and configure the model\nmodel = RandomForestClassifier()\n\n# fit the model\nmodel.fit(xtrain, ytrain)\n\n# evaluate the model\npreds = model.predict(xtest)\naccuracy_score(ytest, preds)","e407ab4f":"from xgboost import XGBClassifier\n\n# define and configure the model\nmodel = XGBClassifier()\n\n# fit the model\nmodel.fit(xtrain, ytrain)\n\n# evaluate the model\npreds = model.predict(xtest)\naccuracy_score(ytest, preds)","759f627e":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# different randomforestregressor hyperperameters\nrf_grid = {'n_estimators' : np.arange(10, 100, 10),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split' : np.arange(2, 20, 2),\n           'min_samples_leaf': np.arange(1, 20, 2),\n            'max_features' : [0.5, 1, 'sqrt', 'auto']}\n\n# instentiate randomizedsearchcv model\nrs_model= RandomizedSearchCV(RandomForestClassifier(n_jobs = -1, \n                                                  random_state=42),\n                                                  param_distributions = rf_grid,\n                                                  n_iter = 90,\n                                                  cv=5,\n                                                  verbose=True)\n\nrs_model.fit(xtrain, ytrain)","6cb5fa06":"rs_model.best_params_","bcdd455f":"ideal_model = RandomForestClassifier(n_estimators= 70,\n                                     min_samples_split = 8,\n                                     min_samples_leaf = 1,\n                                     max_features = 'auto',\n                                     max_depth = 10)\n\n# fit the model\nideal_model.fit(xtrain, ytrain)\n\n# evaluate the model\npreds = ideal_model.predict(xtest)\naccuracy_score(ytest, preds)","6f25cf2a":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = ideal_model.predict_proba(xtest)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(ytest, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","829cd9f4":"test_df = pd.read_csv('..\/input\/donorsprediction\/Predict_donor.csv')\ntest_df.head()","bc5a3a78":"# Fill numeric rows with the median\nfor label, content in test_df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Fill missing numeric values with median since it's more robust than the mean\n            test_df[label] = content.fillna(content.median())\n","ae78d4ea":"# Turn categorical variables into numbers\nfor label, content in test_df.items():\n    # Check columns which aren't numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # print the columns that are object type \n        print(label)\n        test_df[label] = pd.Categorical(content).codes+1","1f73fa2c":"Target = ideal_model.predict(test_df)\nTarget","a53b0a3c":"PREDICTED_df = pd.DataFrame()\nPREDICTED_df['TARGET_B'] = Target\nPREDICTED_df['CONTROL_NUMBER'] = test_df['CONTROL_NUMBER']\nPREDICTED_df.head()","9cc66d29":"**To handle these NAN values we'll replace these values with the median.**","fc3cee44":"**We can see Random forest perfomed best. So let's perform hyperperameter tuning for Random forest**","6ecd350d":"**OK so TARGET_B is out target variable and others are my feature variables.**\n\n**Let's See weather our dataset is having null value or not. And if it's having some null values we have to clean our dataset.**\n","f7c4ce8c":"**Now our dataset is cleaned. We can easily predict the Target variable for our dataset.**","c4ff2bbe":"**So it's an ideal model for our dataset. Now Let's plot a ROC curve to visulaize the performence of our model.**","de27d55b":"**We got the best parameters for our model. Now Let's create an ideal model that have these as it's parameters.**","fee49bd4":"**So here also we can see the NAN values that we have to fix and then we'll convert Categorical features into numerical ones.**","d4eaad22":"# Donors Prediction\n\n**In this notebook we are going to predict that weather a person is going to donate or not. \n\n**The dataset is a list of new contacts a client is interested in reaching out to in the next campaign they intend to carry out soon.**\n\n**So it's a classification problem.**\n\n**Let's begin**\n\n**Firstly we'll take a look of our dataset.**","3fa9bd77":"### Random Forest Classifier","ca0e3f7a":"**So now our data have no missing values.**\n\n**Let's now check how many columns are categorical. If there are categorical columns we'll turn them to numerical columns.**","262b143f":"**Now since we have a good model to predict. Let's Predict wheather a person donates or not for our Test data**\n\n**Let's first import the data**","2062e056":"**Great!!  Now our data needs just one thing more to be ready for modelling.**\n\n**We have to change data values all into one range by using **Standardization** so that our model can easily predict.**\n\n**For this we have to do following steps.**\n\n* Split data into x (input features) & y(target variable)\n* Then use Standard Scaler to to change data values of 'x' into one range.","85374267":"**Yehh! our data is ready for modelling and then predicting wheather a persor will donate or not. \ud83d\ude00**\n\n## Modelling\n\n**We'll use following models and then evaluate them to find which model works well:**\n\n* KNN\n* Random Forest\n* XGBoost Classifier\n\n### KNN","a0472471":"**Hence we have sucessfully predict weather a person will donate or not.**\n\n**PLEASE UPVOTE MY NOTEBOOK IF IT HELPED YOU \ud83d\ude0a\ud83d\ude0a**","cfd4704a":"**There's no need of Target_D column. As we are taking TARGET_B as our target variable. So we can drop this **\n ","5b22a151":"### XGBoostClassifier","b635e485":"**Hence our data is cleaned. We have replaced all null values as well as there's no categorical data now.\nLet's take a look of our cleaned dataset**"}}