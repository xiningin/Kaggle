{"cell_type":{"4fc3f7ca":"code","b37f373a":"code","368b4cfe":"code","e2bc53f9":"code","44672c22":"code","9999b4e1":"code","9aca48b7":"code","249a8e38":"code","79782b2d":"code","6150d193":"code","3c12b737":"code","2b7af389":"code","f16ce324":"code","58d1771a":"code","fb5b1ed6":"code","316dbb2b":"markdown","5a370846":"markdown","9f66f395":"markdown","51ce0341":"markdown"},"source":{"4fc3f7ca":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers as ppb\n\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/tweet-sentiment-extraction\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.simplefilter('ignore')","b37f373a":"def label_encoding(sentiment):\n    #label encoding\n    if sentiment == 'positive':\n        return 0\n    elif sentiment == 'negative':\n        return 1\n    else:\n        return 2","368b4cfe":"def tokenize_fn(text):\n    #BERT model\u5ba3\u8a00\n    tokenizer = ppb.BertTokenizerFast.from_pretrained('\/kaggle\/input\/bert-base-uncased\/')\n    if str(text) == 'nan':\n        text = ' '\n    text = text.lower()\n    #tokenize using BERTtokenizer\n    text = tokenizer.encode(text,do_lower_case=True)\n    \n    text = np.pad(text,[0,110-len(text)],'constant')\n    return text","e2bc53f9":"class Mydatasets(torch.utils.data.Dataset):\n    def __init__(self, path):\n        self.data = pd.read_csv(path)\n        self.tokenized = torch.empty(self.data.shape[0], 110)\n        self.labels = torch.empty(self.data.shape[0])\n        \n        for i in tqdm(range(self.data.shape[0])):\n            self.tokenized[i] = torch.from_numpy(tokenize_fn(self.data['text'][i]))\n            self.labels[i] = label_encoding(self.data['sentiment'][i])\n        self.len = self.tokenized.shape[0]\n        \n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, idx):\n        out_data = self.tokenized[idx]\n        out_label = self.labels[idx]\n\n        return out_data, out_label","44672c22":"trainset = Mydatasets('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntestset = Mydatasets('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')","9999b4e1":"train_size = int(0.8 * len(trainset))\ntest_size = len(trainset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(trainset, [train_size, test_size])","9aca48b7":"BATCH_SIZE = 16\n\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\ntestloader = torch.utils.data.DataLoader(testset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)\nvalloader = torch.utils.data.DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False, num_workers = 4)","249a8e38":"class BERTmodel(nn.Module):\n    def __init__(self,bert):\n        super(BERTmodel,self).__init__()\n        self.bert = bert\n        #in_feature=768 depend on output size of BERT model\n        self.cls = nn.Linear(in_features=768,out_features=3)\n    \n    def forward(self,x,token_type_ids=None,attention_mask=None):\n        encoded_layers,_ = self.bert(x,token_type_ids,attention_mask)\n        word_vec = encoded_layers[:,0,:].view(-1,768)\n        out = self.cls(word_vec)\n        return out","79782b2d":"def train(net,optimizer,criterion,epochs,batch_size,trainset,valset):\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print('device:',device)\n    print('---start---')\n    net.to(device)\n    torch.backends.cudnn.benchmark = True\n    \n    train_acc_list,train_loss_list = [],[]\n    val_acc_list,val_loss_list = [],[]\n    \n    for epoch in range(epochs):\n        for phase in ['train','val']:\n            print('---%s_%d---' % (phase,epoch))\n            if phase == 'train':\n                dataset = trainset\n                net.train()\n            else:\n                dataset = valset\n                net.eval()\n            epoch_loss = 0.0\n            epoch_corrects = 0\n            total = 0\n            \n            for inputs,labels in tqdm(dataset):\n                inputs = inputs.long().to(device)\n                labels = labels.long().to(device)\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase=='train'):\n                    outputs = net(inputs)\n                    loss = criterion(outputs,labels)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        \n                    pred = torch.argmax(outputs,dim=1)\n                    total += labels.size(0)\n                epoch_corrects += (pred == labels).sum().item()\n                epoch_loss += loss.item()\n                \n            if phase == 'train':\n                train_loss_list.append(epoch_loss\/len(dataset))\n                train_acc_list.append(100 * epoch_corrects \/ total)\n            else:\n                val_loss_list.append(epoch_loss\/len(dataset))\n                val_acc_list.append(100 * epoch_corrects \/ total)\n            print('Loss: %f' % (epoch_loss\/len(dataset)))\n            print('Accuracy: %f' % (epoch_corrects\/total))\n            \n    return (train_acc_list,train_loss_list),(val_acc_list,val_loss_list)","6150d193":"EPOCHS=20\n\nbert = ppb.BertModel.from_pretrained('\/kaggle\/input\/bert-base-uncased\/')\nnet = BERTmodel(bert)\n\n# No gradient calculation for the 1st to 11th transformer. \n# It should be all calculated, but to reduce the cost of calculation.\nfor name, param in net.named_parameters():\n    param.requires_grad = False\n    \n# 12th transformer with gradient calculation.\nfor name, param in net.bert.encoder.layer[-1].named_parameters():\n    param.requires_grad = True\n\n#dense layer with gradient calculation.(fine-tuning)\nfor name, param in net.cls.named_parameters():\n    param.requires_grad = True\n\n#param update\noptimizer = torch.optim.Adam([{'params': net.bert.encoder.layer[-1].parameters(),'lr':5e-5},\n                        {'params': net.cls.parameters(),'lr': 5e-5}], betas=(0.9,0.999))\n\n#loss function\ncriterion = nn.CrossEntropyLoss() ","3c12b737":"train_result,val_result = train(net,optimizer,criterion,EPOCHS,BATCH_SIZE,trainloader,valloader)","2b7af389":"def plot_history(index,train_data,val_data,title):\n    # Plot the loss in the history\n    plt.plot(index, train_data, label='train')\n    plt.plot(index, val_data, label='val')\n    plt.title(title)\n    plt.xlabel('epoch')\n    plt.ylabel(title)\n    plt.legend()","f16ce324":"index = np.arange(20)\nplot_history(index,train_result[0],val_result[0],'acc')","58d1771a":"plot_history(index,train_result[1],val_result[1],'loss')","fb5b1ed6":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.benchmark = True\nnet.eval()\nepoch_corrects = 0\ntotal = 0\n\nfor inputs,labels in tqdm(testloader):\n    inputs = inputs.long().to(device)\n    labels = labels.long().to(device)\n    optimizer.zero_grad()\n    with torch.set_grad_enabled(False):\n        outputs = net(inputs)\n        loss = criterion(outputs,labels)\n        pred = torch.argmax(outputs,dim=1)\n        total += labels.size(0)\n    epoch_corrects += (pred == labels).sum().item()\n\nprint('Accuracy: %f' % (epoch_corrects\/total))","316dbb2b":"# 2. Create sentiment prediction model using BERT","5a370846":"## BERT hyper-parameters","9f66f395":"# 1.Make a Dataset and DataLoaders","51ce0341":"# 3. Results"}}