{"cell_type":{"7cce9ca1":"code","bd8d21c7":"code","1fae758c":"code","170e09c3":"code","07ee40a5":"code","e0b2285d":"code","709611a9":"code","3ee90e96":"code","4f4d411f":"code","081fa4bc":"code","4b73174a":"code","8d4adb90":"code","abc2c6ff":"code","c836c43a":"code","2457cad9":"code","126fac5a":"code","36fa8f40":"code","17947bbb":"code","59e1a648":"code","4034c74f":"markdown","41131496":"markdown","1cc24dfb":"markdown","34d13668":"markdown","d61cc33a":"markdown","221ee7e8":"markdown","4c784a6a":"markdown","43d74e0f":"markdown","be2d1f0d":"markdown","dcb03ef6":"markdown"},"source":{"7cce9ca1":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 12\n\n# Don't change these\nNUM_CORES = 8\nNUM_CLASSES = 10\nINPUT_SIZE = 28,28,1","bd8d21c7":"# General Imports\nimport numpy as np\nimport pandas as pd\nimport time\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n# Logging\/Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Tensorflow\/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Evaluation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Load data\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nfeatures = [x for x in train.columns if x != 'label']\n\nprint(\"Train Shape:\", train.shape)\nprint(\"Test Shape:\", test.shape)","1fae758c":"# Model 1 Parameters\nBATCH_SIZE = 32 * NUM_CORES\nEPOCHS = 50\nEARLY_STOP = 3\nVERBOSE = 0","170e09c3":"def train_islr_model():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Flatten(),\n                layers.Dense(256, activation=\"relu\"),\n                layers.Dropout(0.4),\n                layers.Dense(128, activation=\"relu\"),\n                layers.Dropout(0.3),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.RMSprop(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n            \n        start = time.time()\n\n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        time.sleep(0.5)\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","07ee40a5":"islr_scores, islr_preds, islr_oof = train_islr_model()\n\nsubmission['Label'] = islr_preds\nsubmission.to_csv('islr_submission.csv', index=False)","e0b2285d":"# Model 2 Parameters\nBATCH_SIZE = 32 * NUM_CORES\nEPOCHS = 50\nEARLY_STOP = 3\nVERBOSE = 0","709611a9":"def train_ebook_model():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Flatten(),\n                layers.Dense(512, activation=\"relu\"),\n                layers.Dropout(0.2),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.Adam(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n            \n        start = time.time()\n\n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        \n        time.sleep(0.5)\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","3ee90e96":"ebook_scores, ebook_preds, ebook_oof = train_ebook_model()\n\nsubmission['Label'] = ebook_preds\nsubmission.to_csv('ebook_nn_submission.csv', index=False)","4f4d411f":"# Model 3 Parameters\nBATCH_SIZE = 32 * NUM_CORES\nEPOCHS = 200\nEARLY_STOP = 10\nVERBOSE = 0","081fa4bc":"def train_ebook_cnn():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Flatten(),\n                layers.Dense(64, activation=\"relu\"),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.Adam(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n\n        start = time.time()\n        \n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        \n        time.sleep(0.5)\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","4b73174a":"ebook_cnn_scores, ebook_cnn_preds, ebook_cnn_oof = train_ebook_cnn()\n\nsubmission['Label'] = ebook_cnn_preds\nsubmission.to_csv('ebook_cnn_submission.csv', index=False)","8d4adb90":"# Model 4 Parameters\nBATCH_SIZE = 128 * NUM_CORES\nEPOCHS = 200\nEARLY_STOP = 10\nVERBOSE = 0","abc2c6ff":"def train_keras_cnn():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Flatten(),\n                layers.Dropout(0.5),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.Adam(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n\n        \n        start = time.time()\n        \n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","c836c43a":"keras_scores, keras_preds, keras_oof = train_keras_cnn()\n\nsubmission['Label'] = keras_preds\nsubmission.to_csv('keras_doc_submission.csv', index=False)","2457cad9":"# Helper function to plot 6 misclassified examples\ndef plot_misclassified(preds):\n    train['guess'] = preds\n    samples = train[train['label'] != preds].sample(n = 6, random_state = RANDOM_SEED)\n    y_true = samples['label'].to_numpy()\n    y_pred = samples['guess'].to_numpy()\n    samples = samples[features].to_numpy()\n    fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(12, 9))\n    for row in range(2):\n        for col in range(3):\n            idx = 3*row + col\n            ax[row,col].imshow(samples[idx].reshape(28, 28), cmap=plt.get_cmap('gray'))\n            ax[row,col].set_title(f'Actual - {y_true[idx]}\\n Predicted - {int(y_pred[idx])}')","126fac5a":"plot_misclassified(islr_oof)","36fa8f40":"plot_misclassified(ebook_oof)","17947bbb":"plot_misclassified(ebook_cnn_oof)","59e1a648":"plot_misclassified(keras_oof)","4034c74f":"# Model 2: 4-Layer Neural Network (AMD)\n\nThe second model we consider has the following structure:\n\n* A Flatten layer to reshape the input\n* A Dense layer with 512 units and a ReLu activation function\n* A Dropout layer with a dropout rate of 0.5\n* A Dense layer with 10 units an a softmax activation","41131496":"## Model 2: 4-Layer Neural Network","1cc24dfb":"# Model 4: Convolutional Neural Network with Dropout\n\nThe final model we consider is the convolutional neural network from the Keras documentation. This model utilizes two convolution layers and dropout regularization.","34d13668":"## Model 1: Simple Neural Network","d61cc33a":"# Model 3: Convolutional Neural Network\n\nThe third model we consider is a convolutional neural network with 3 convolution layers (each followed by max pooling) and two dense layers.","221ee7e8":"# Misclassification\n\nWe take a look at a few misclassified training examples using the out-of-fold predictions to get an idea of the examples which our neural networks failed to classify correctly.","4c784a6a":"## Model 3: Convolutional Neural Network","43d74e0f":"# Digit Recognition\n\nIn this notebook, we adapt example code from various sources for use with this competition. In particular, we look at the following examples:\n\n* A 5-layer NN from the book [Introduction to Statistical Learning (2nd Edition)](https:\/\/www.statlearning.com\/).\n* A 4-layer NN from the ebook [Artificial Intelligence, Machine Learning and Deep Learning](http:\/\/www.merclearning.com\/titles\/Artificial%20Intelligence-Machine-Learning-and-Deep-Learning.html).\n* A CNN from the same ebook.\n* The CNN from the [Keras documentation](https:\/\/keras.io\/examples\/vision\/mnist_convnet\/). \n\n\nWe make the following adjustments from the code given at the original sources, namely:\n\n* Use pandas and scikit-learn for loading\/processing the data.\n* Enabling GPU to speedup training.\n* Vary the number of epochs and enable [early stopping](https:\/\/keras.io\/api\/callbacks\/early_stopping\/).\n* Use cross-validation to ensemble test predictions and average validation accuracy.","be2d1f0d":"# Model 1: Simple Neural Network (ISL)\n\nThe first model we consider has the following structure:\n\n* A Flatten Layer to reshape our input\n* Dense Layer with 256 units and ReLu activation function\n* Dropout Layer with 0.4 dropout rate\n* Dense Layer with 128 unit and ReLu activation function\n* Dropout Layer with 0.3 dropout rate\n* Dense Layer with 10 units and softmax activation","dcb03ef6":"## Model 4: Convolutional Neural Network with Dropout"}}