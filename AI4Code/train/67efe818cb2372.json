{"cell_type":{"59b1deb9":"code","68dd5860":"code","d29d407f":"code","7cc7b9ef":"code","4588124d":"code","aad34d73":"code","e4782f35":"markdown","bcf4d8ee":"markdown","75c1f8e7":"markdown","fe2f02bb":"markdown","e54d32be":"markdown","e6f01c13":"markdown","8e8dc12d":"markdown","b4149982":"markdown"},"source":{"59b1deb9":"# Imports for this block \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n\n# Load training data into an ndarray (skip the col headers - the first line in the csv file)\ntrain = np.genfromtxt(\"..\/input\/training\/train.csv\", skip_header=1, delimiter=\",\")\n\n# Separate labels from data (for the train file): Labels are the first column\nY_train = train[:,0:1]\nX_train = train[:,1:]\ndel train\n\n# normalize integers (0-255) into floats (0-1)\nX_train = X_train\/255.0\n\n# Reshape the ndarray from 42000,784,1 -> 42000,28,28,1 (the ,1 is the channel number - needed for conv nets)\nX_train = X_train.reshape([-1,28,28,1])\n\n# Encode labels to one hot vectors (eg : 2 -> [0,0,1,0,0,0,0,0,0,0] etc)\nY_train = to_categorical(Y_train, num_classes = 10)\n\n# Split the training set (using sklearn utility)\nrandom_seed = 2\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)\n\nprint('X_train shape = ', X_train.shape)\nprint('Y_train shape = ', Y_train.shape)\nprint('X_val shape = ', X_val.shape)\nprint('Y_val shape = ', Y_val.shape)\n","68dd5860":"# New imports for this block\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n\n# Build a quick conv net\nmodel = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3), activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D((2,2)))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\n\n# Let's see what the layers look like\nprint(model.summary())\n","d29d407f":"# New imports for this block\nfrom keras.optimizers import RMSprop\n\n# Compile the model\nmodel.compile(optimizer=RMSprop(lr=2e-5), loss='binary_crossentropy', metrics=['acc'])\n\n# Fit the model (running the validation test as we go and recording loss and accuracy on 'unseen' images)\nhistory = model.fit(X_train, Y_train, epochs=30, batch_size=100, validation_data=(X_val, Y_val))\n","7cc7b9ef":"# New imports for this block\nimport matplotlib.pyplot as plt\n\n# Get the loss and accuracy info from the history history dictionary\ntraining_losses = history.history['loss']\nvalidation_losses = history.history['val_loss']\ntraining_acc = history.history['acc']\nvalidation_acc = history.history['val_acc']\n\n# label epochs from 1 not 0\nepochs = range(1, 31)\n\nplt.plot(epochs, training_losses, 'bo', label='Training loss')\nplt.plot(epochs, validation_losses, 'b', label='Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()\n\nplt.clf()\n\nplt.plot(epochs, training_acc, 'bo', label='Training accuracy')\nplt.plot(epochs, validation_acc, 'b', label='Validation accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n","4588124d":"# Save the net we have\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n","aad34d73":"# Load training data into an ndarray (skip the col headers - the first line in the csv file)\ntest = np.genfromtxt(\"..\/input\/testing\/test.csv\", skip_header=1, delimiter=\",\")\n\n# Normalize and reshape the data into 28x28x1 tensors\ntest = test\/255.0\ntest = test.reshape([-1,28,28,1])\n\n# Make a ndarray of digit probabilities\npredictions = model.predict(test)\n\n# Find the most probable digit in each row of the array\ndigit_predictions = np.argmax(predictions, axis=1)\n\n# Write to file etc...\n\nprint('Done')\n\n","e4782f35":"**Prepare the training data**\n1. Read the csv file into a ndarray.\n2. Separate the labels from the vectors of pixel data.\n3. Normalize pixel values from {0,255} to {0,1}\n4.  Reshape the input vectors from rows (elements) of 784 values to tensors 28x28 values with 1 channel (greyscale)\n5. Recode the labels to a 10 element vector (corresponding to the 10 output neurons we will have in the output layer - one hot encoding) \n6. Separate (randomly) the training tensors into training and validation sets (9:1 split)\n","bcf4d8ee":"**First Attempt  (VERY Naive)**\n\nVery much a first attempt; simple convnet, no data augmentation, no dropout, no weight regularization in the dense layers etc.","75c1f8e7":"OK:  about 99.47% accuracy in the validation set.\nLet's plot the trends on the  training and validation sets and see what our trends look like.","fe2f02bb":"The summary above tells us a lot about what the net is going to do. It comprises 4 kinds of layers.\n\n1. Conv2D : this layer is a 'features finder'. It seeks translation invarient features in the input. Using the first layer as an example; it takes an input tensor (in the first layer this is a tensor of size 28x28x1) and passes a sliding window (3x3 in the first layer) over it, discovering features. The output is a new tensor of size 26x26x30. The number of filters (channels) 30 is indicitive of the fact that we have created 30 'views' of the input data showing the existence of particular features. Both the input and output tensors can be described as 'feature maps'. The X,Y size of the output feature map is smaller (28 -> 26) because not all (the outermost) pixels can be validly represented in the sliding windows. (See Keras docs for details {ref here}).\n2. MaxPooling2D : this layer is an aggressive downscaling of the input tensor. It slides a 2x2 window over its input tensor with no overlap looking for max val. What we get is a new tensor with exactly half X,Y dimentions and the same number of  filters (channels). In our first MaxPooling2D layer we take a 26x26x30 input tensor and out put a new representation; a 13x13x30 tensor. It's all about taking what we've found so far and reducing its dimentionality.\n3. Flatten : What we've got so far is OK but at some point we need to make a decision about things. The decision is going to be made by some feed-forward, back-propogation (of error) layers. These (Dense) layers take a 1D tensor (vector) as input and this is the job of the Flatten layer. In this case a 3x3x64 tensor is simply turned into a vector comprising 576 elements.\n4. Dense : the 3 Dense layers take this 576 input vector and output a 10 element vector which can be used as input against an error function.\n\nLet's just run the model, I'm setting GPU on! 30 epocs will run in under 3 mins on GPUs. On CPUs the same run would take much longer","e54d32be":"That's it. A submission from this 'only' scores 0.968 accuracy on the 28000 test samples in the MNIST competition. However, it's a very first attempt and there are many modifications and optimisations that can be applied. A decent start I hope?","e6f01c13":"**Introduction**\n\nI have some experience in classification neural nets from the 1990s (feed-forward backprops, adaptive resonance etc - all written in C or C++ from scratch) which took vectors as inputs. \nI'm completely new to both Python (as a programming language) and the new net architectures (CNN etc) and frameworks (tensorflow, keras, sklearn) so armed with a couple of books and the on line documentation here's my first attempt at image classification using keras.\n","8e8dc12d":"So, what we're not seeing is overfitting. Normally, we'd like to overfit and then generalize the net using dropout, layer removal or a bit of general tweaking. Additionally; if you look at the leader board for the MNIST competition you'll see that 0.994 won't get you into the top 50%. We need at least 3 9s in the validation set to be remotely competitive. If we wanted to save the net though, we'd do the folowing. ","b4149982":"and then use the model to make some predictions"}}