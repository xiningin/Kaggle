{"cell_type":{"107259a7":"code","812236ce":"code","6c988e15":"code","19da0b9d":"code","84106e47":"code","82c3d3e5":"code","aab78923":"code","770b346e":"code","d1072ba4":"code","0d098293":"code","c6beda1f":"code","f355d807":"code","df95c02b":"code","6b49c5d8":"code","4ce0b0fd":"code","920ad276":"code","ea031a93":"code","32e4bbd4":"markdown","b9ad37e2":"markdown","85c8b2d1":"markdown"},"source":{"107259a7":"from functools import partial\nfrom itertools import combinations\nfrom textwrap import wrap\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport gensim.downloader as api\nimport plotly.graph_objs as go\nimport spacy\nimport umap\nfrom gensim.corpora import Dictionary\nfrom gensim.matutils import softcossim\nfrom plotly.offline import init_notebook_mode, iplot","812236ce":"%%capture\n\nnlp = spacy.load(\"en\")\n\nmodel = api.load(\"glove-wiki-gigaword-50\");\nmodel.init_sims(replace=True)","6c988e15":"scripts = pd.read_csv(\"..\/input\/scripts.csv\", index_col=0)","19da0b9d":"scripts.head()","84106e47":"scripts.tail()","82c3d3e5":"character = \"KRAMER\"\ncharacter_script = scripts[(scripts[\"Character\"] == character)]\nprint(f\"original n lines: {len(scripts)}, character n lines {len(character_script)}\")","aab78923":"%%time\n\ndialogues = character_script[\"Dialogue\"].astype(str).tolist()\n\ntokenized_docs = []\nfor i, doc in enumerate(nlp.pipe(dialogues, n_threads=-1)):\n    tokens = [token.lower_ for token in doc if token.lower_ in model.vocab and token.is_alpha]\n    if len(tokens) > 0:\n        tokenized_docs.append((i, tokens))\n\nd_index, d_tokenized = zip(*tokenized_docs)\nkept_dialogues = [dialogues[i] for i in d_index]\nprint(f\"original: {len(dialogues)}, reduced: {len(kept_dialogues)}\")","770b346e":"tfidf_vectors = TfidfVectorizer(analyzer=lambda x: x).fit_transform(d_tokenized)\ncos_dist = 1 - (tfidf_vectors.toarray() * tfidf_vectors.T)","d1072ba4":"tfidf_embedding = umap.UMAP(metric=\"precomputed\", random_state=666).fit_transform(cos_dist)\nembedding_df = pd.DataFrame(tfidf_embedding, columns=[\"dim0\", \"dim1\"])\nsentence_text_series = pd.Series(kept_dialogues, name=\"text\")\nsentence_token_series = pd.Series(d_tokenized, name=\"tokens\")\ntfidf_df = pd.concat([sentence_text_series, sentence_token_series, embedding_df], axis=1)","0d098293":"def build_tooltip(row):\n    text = \"<br>\".join(wrap(row[\"text\"], 40))\n    tokens = \"<br>\".join(wrap(\", \".join(row[\"tokens\"]), 40))\n    full_string = [\n        \"<b>Text:<\/b> \",\n        text,\n        \"<br>\",\n        \"<b>Tokens:<\/b> \",\n        tokens\n    ]\n    return \"\".join(full_string)\n\ntfidf_df[\"tooltip\"] = tfidf_df.apply(build_tooltip, axis=1)","c6beda1f":"init_notebook_mode(connected=True)\n\ntrace = go.Scatter(\n    x = tfidf_df[\"dim0\"],\n    y = tfidf_df[\"dim1\"],\n    name = \"TFIDF Embedding\",\n    mode = \"markers\",\n    marker = dict(\n        color = \"rgba(49, 76, 182, .8)\",\n        size = 5,\n        line = dict(width=1)),\n    text=tfidf_df[\"tooltip\"])\n\nlayout = dict(title=\"2D Embeddings - TFIDF\",\n             yaxis = dict(zeroline=False),\n             xaxis = dict(zeroline=False),\n             hovermode = \"closest\")\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","f355d807":"%%time\n\ndictionary = Dictionary(d_tokenized)\ncorpus = [dictionary.doc2bow(document) for document in d_tokenized]\nsimilarity_matrix = model.similarity_matrix(dictionary)\ncorpus_softcossim = partial(softcossim, similarity_matrix=similarity_matrix)","df95c02b":"%%time\n\nsentence_pairs = combinations(corpus, 2)\nscs_sims = [corpus_softcossim(d1, d2) for d1, d2 in sentence_pairs]","6b49c5d8":"n_sentences = len(corpus)\nscs_empty = np.zeros((n_sentences, n_sentences))\nupper_indices = np.triu_indices(n_sentences, 1)\nscs_empty[upper_indices] = scs_sims\nscs_sim = np.triu(scs_empty, -1).T + scs_empty\nnp.fill_diagonal(scs_sim, 1)\nscs_dist = 1 - scs_sim","4ce0b0fd":"scs_embedding = umap.UMAP(metric=\"precomputed\", random_state=666).fit_transform(scs_dist)\nscs_embedding_df = pd.DataFrame(scs_embedding, columns=[\"dim0\", \"dim1\"])\nscs_df = pd.concat([sentence_text_series, sentence_token_series, scs_embedding_df], axis=1)","920ad276":"scs_df[\"tooltip\"] = scs_df.apply(build_tooltip, axis=1)","ea031a93":"trace = go.Scatter(\n    x = scs_df[\"dim0\"],\n    y = scs_df[\"dim1\"],\n    name = \"SCS Embedding\",\n    mode = \"markers\",\n    marker = dict(\n        color = \"rgba(49, 76, 182, .8)\",\n        size = 5,\n        line = dict(width=1)),\n    text=scs_df[\"tooltip\"])\n\nlayout = dict(title=\"2D Embeddings - SCS\",\n             yaxis = dict(zeroline=False),\n             xaxis = dict(zeroline=False),\n             hovermode = \"closest\")\n\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","32e4bbd4":"## Soft Cosine Similarity","b9ad37e2":"# Semantic Maps for Documents\n\nThis is an approach I take often to better understand the semantic landscape of a corpus. Using Soft Cosine Similarity with word embeddings, this process works particularly well when the corpus is a collection of short texts like reviews, comments, or tweets, where approaches like TFIDF fall short.  For this notebook we'll apply this approach to better understand Kramer's dialogue throughout Seinfeld.\n\nThe steps are as follows:\n\n1. Process all the documents so each document is a collection of tokens that are in the word embedding vocabulary. In our case, we're using the `glove-wiki-gigaword-50` corpus, where all tokens are lowercase, so we'll lowercase all the words. Remove documents that don't have any words in the vocabulary.\n2. Calculate a TFIDF matrix and use this to calculate the cosine distance between each document's TFIDF vector. \n3. Using UMAP (or alternatively, t-SNE), take the precomputed distance matrix and project down to 2 dimensions.\n4. Combine the supporting information for each document into a DataFrame. This includes the original documents, the tokenized documents, and the embeddings (and optionally labels or other attributes that might be interesting to plot).\n5. Create a 2D scatterplot of the projection. The value here comes from interactive hover tooltips so that you can quickly explore why documents are in a similar location in the projection. We use `plotly` for our visualization, which can create HTML tooltips, so we create those for each point. \n6. Repeat steps 3-5 for other similarity measures. In this notebook, we use Soft Cosine Distance, which takes a long time to calculate, but is able to use the word embeddings to capture more semantic information about the documents. Comparatively, TFIDF-cosine distance only captures overlap in tokens.\n\nBeyond these visualizations, there are a lot of other places you can take this. One possibility would be to add labels as colors to the plot -- this will actually give you a good idea of how easy a classification machine learning task would be by leveraging the topological properties of the data. If your color labels are distinct in the 2D projection, generally models will have an easier time distinguishing classes when those features are used. Another possibility is to perform clustering (using something like HDBSCAN) to find collections of documents that have similar semantic information and use those clusters like an informal topic model.\n\nFinally, my process is iterative going between the scatterplots and the processing steps, but this isn't indicated in the notebook. Often times the outliers in the scatterplots can indicate additional areas to improve on the text preprocessing.","85c8b2d1":"## TFIDF - Cosine Similarity"}}