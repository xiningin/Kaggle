{"cell_type":{"40d9c0c5":"code","ba82ed5d":"code","6f3643c0":"code","1b8c9857":"code","c055c8b3":"code","2aefe47a":"code","89ecd6a0":"code","e6ad9b8b":"code","8b4cda6b":"code","fdab376e":"code","f5c9b5e8":"code","e598fb65":"code","787a0de9":"code","0179e035":"code","f0eb6d2b":"code","05ac6427":"code","1337c16b":"code","7777e2b5":"code","a2b4966a":"code","568a0a9e":"code","7e1f29b4":"code","b9b6ef8c":"code","36070227":"code","daa9b285":"code","a8523169":"code","15f976ff":"code","082b7da5":"code","c4e8aa20":"code","a892765a":"code","eda5d4f3":"code","bbb5c1c2":"code","6bb12469":"code","2234d826":"code","ef570684":"code","ccdb965c":"code","60d7fc9b":"code","8369bf45":"code","bca68e2b":"code","86d3b6d5":"code","2c1978d8":"code","321c0c30":"code","82e2efb9":"code","5add8ef6":"code","8682239b":"code","b295bddb":"code","764feb79":"code","8a45a3e2":"code","17bd1a72":"code","28823548":"code","b19aebb3":"code","aff87cfb":"code","14141da1":"code","c82ab404":"code","920c7cec":"code","e4928a87":"code","e8606b50":"code","280ef41e":"code","6061d5c6":"code","6bf1fe7c":"code","c04f8747":"code","f65e4e3c":"code","523d3286":"code","c88df79b":"code","f68d24cb":"code","93de49f7":"code","20772473":"code","81b33643":"code","ccb31ff4":"code","450bc964":"code","bd2c502a":"code","fd22c408":"code","dbd7f5e6":"code","fabb96f7":"code","605962b6":"code","14efc7b0":"code","18be30a6":"code","d4967a82":"code","670afae7":"code","b1ac997f":"code","c21cf8b9":"code","c2da61aa":"code","f5c5a5bd":"code","6095945c":"code","9df5f1da":"code","00915d57":"code","44b19ccb":"code","38ac6242":"code","8a182247":"code","f37ae72a":"code","6130693c":"code","24811059":"code","118e1749":"code","8de78a74":"code","36ed8cc1":"code","9dca9cbf":"code","12aedea1":"code","d2dee6aa":"code","22e8a405":"code","50b6bf67":"code","b7ee8567":"code","e0237011":"code","c292f336":"code","23d94bef":"code","ffbeea9f":"code","89c60915":"code","78658648":"markdown","5eb95bfe":"markdown","3a984140":"markdown","266f8ea8":"markdown","686e6a23":"markdown","cd247bde":"markdown","f34d329b":"markdown","1eb00adf":"markdown","9c46b426":"markdown","090b8dfa":"markdown","173912e8":"markdown","d25e62e4":"markdown","a62f008a":"markdown","f3864e24":"markdown","86f7eb0e":"markdown","05d25052":"markdown","2fd4e466":"markdown","4d0a2508":"markdown","5726b2b2":"markdown","0b9457b7":"markdown","7245426a":"markdown","584a32a3":"markdown","0d0a93b3":"markdown","e524711a":"markdown","9895f83b":"markdown","4c9b310d":"markdown","16372032":"markdown","5424a416":"markdown"},"source":{"40d9c0c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba82ed5d":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns","6f3643c0":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","1b8c9857":"df.head()","c055c8b3":"df.shape","2aefe47a":"df.info()","89ecd6a0":"def tofloat(x):\n  try:\n    a = float(x)\n    return a\n  except:\n    return np.nan","e6ad9b8b":"df.TotalCharges = df.TotalCharges.apply(tofloat)","8b4cda6b":"category_cols = []\nfor x in df.columns:\n  if df[x].dtypes=='O':\n    print(x,df[x].unique())\n    print()","fdab376e":"df.replace('No internet service','No',inplace=True)\ndf.replace('No phone service','No',inplace=True)","f5c9b5e8":"category_cols = []\nfor x in df.columns:\n  if df[x].dtypes=='O':\n    print(x,df[x].unique())\n    print()","e598fb65":"df.isna().sum()","787a0de9":"round(df.TotalCharges.median(),2)","0179e035":"df.TotalCharges.fillna(round(df.TotalCharges.median(),2),inplace=True)","f0eb6d2b":"df.plot(kind='box')","05ac6427":"df.head()","1337c16b":"df.Churn.value_counts()","7777e2b5":"Y = df.Churn","a2b4966a":"Y","568a0a9e":"Y = Y.map({'Yes':1,'No':0})","7e1f29b4":"# Input Variables\nX = df.drop('Churn',axis=1)","b9b6ef8c":"cols = []\nfor x in X.columns:\n  if X[x].dtypes == 'O':\n    cols.append(x)\ncols","36070227":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor x in cols:\n  X[x] = le.fit_transform(X[x])","daa9b285":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=1)","a8523169":"ind = X_train.index","15f976ff":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","082b7da5":"X_train = pd.DataFrame(X_train,columns=X.columns,index=ind)","c4e8aa20":"Y_train = pd.DataFrame(Y_train)","a892765a":"X_train","eda5d4f3":"Y_train","bbb5c1c2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","6bb12469":"X.shape[1]","2234d826":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=X.shape[1]))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","ef570684":"model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")","ccdb965c":"trained_model = model.fit(X_train, Y_train, epochs=150,batch_size=20)","60d7fc9b":"plt.plot(trained_model.history[\"loss\"])","8369bf45":"Y_pred = model.predict(X_test)","bca68e2b":"Y_pred = np.where(Y_pred >= 0.5,1,0)","86d3b6d5":"from sklearn.metrics import classification_report,recall_score,f1_score,precision_score,accuracy_score\nprint(classification_report(Y_test,Y_pred))\nactual_acc = accuracy_score(Y_test,Y_pred)\nactual_rec = recall_score(Y_test,Y_pred)\nactual_p = precision_score(Y_test,Y_pred)\nactual_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',actual_rec)\nprint('Precision ->',actual_rec)\nprint('F1 Score ->',actual_f1)\nprint('Accuracy ->',actual_acc)","2c1978d8":"# Import the resampling package\nfrom sklearn.utils import resample","321c0c30":"# Split into training and test sets\n#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)","82e2efb9":"# Returning to one dataframe\ntraining_set = pd.concat([X_train, Y_train], axis=1)","5add8ef6":"training_set","8682239b":"# Separating classes\nchurn = training_set[training_set.Churn == 1]\nno_churn = training_set[training_set.Churn == 0]","b295bddb":"len(churn)","764feb79":"len(no_churn)","8a45a3e2":"undersample = resample(no_churn,replace=True,n_samples=len(churn),random_state=10)","17bd1a72":"len(undersample) # len of no_churn is reduced to len of churn\n# both are equal now","28823548":"# Returning to new training set\nundersample_train = pd.concat([churn, undersample])","b19aebb3":"undersample_train.Churn.value_counts()","aff87cfb":"# Separate undersampled data into X and y sets\nundersample_x_train = undersample_train.drop('Churn', axis=1)\nundersample_y_train = undersample_train.Churn","14141da1":"undersample_x_train.shape","c82ab404":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=20))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","920c7cec":"model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")","e4928a87":"trained_model = model.fit(undersample_x_train, undersample_y_train, epochs=150,batch_size=20)","e8606b50":"plt.plot(trained_model.history[\"loss\"])","280ef41e":"Y_pred = model.predict(X_test)","6061d5c6":"Y_pred = np.where(Y_pred >= 0.5,1,0)","6bf1fe7c":"print(classification_report(Y_test,Y_pred))\nunder_acc = accuracy_score(Y_test,Y_pred)\nunder_rec = recall_score(Y_test,Y_pred)\nunder_p = precision_score(Y_test,Y_pred)\nunder_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',under_rec)\nprint('Precision ->',under_rec)\nprint('F1 Score ->',under_f1)\nprint('Accuracy ->',under_acc)","c04f8747":"oversample = resample(churn,replace=True,n_samples=len(no_churn),random_state=10)","f65e4e3c":"# Returning to new training set\noversample_train = pd.concat([no_churn, oversample])","523d3286":"oversample_train.Churn.value_counts()\n","c88df79b":"# Separate oversampled data into X and y sets\noversample_x_train = oversample_train.drop('Churn', axis=1)\noversample_y_train = oversample_train.Churn","f68d24cb":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=20))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","93de49f7":"model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")","20772473":"trained_model = model.fit(oversample_x_train, oversample_y_train, epochs=150,batch_size=20)","81b33643":"plt.plot(trained_model.history[\"loss\"])","ccb31ff4":"Y_pred = model.predict(X_test)\nY_pred = np.where(Y_pred >= 0.5,1,0)","450bc964":"print(classification_report(Y_test,Y_pred))\nover_acc = accuracy_score(Y_test,Y_pred)\nover_rec = recall_score(Y_test,Y_pred)\nover_p = precision_score(Y_test,Y_pred)\nover_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',over_rec)\nprint('Precision ->',over_rec)\nprint('F1 Score ->',over_f1)\nprint('Accuracy ->',over_acc)","bd2c502a":"# Import the SMOTE package\nfrom imblearn.over_sampling import SMOTE","fd22c408":"# Synthesize minority class datapoints using SMOTE\nsm = SMOTE(random_state=42)\nsmote_x_train, smote_y_train = sm.fit_resample(X_train, Y_train)","dbd7f5e6":"# Separate into training and test sets\nsmote_x_train = pd.DataFrame(smote_x_train, columns = X_train.columns)\nsmote_y_train = pd.DataFrame(smote_y_train, columns = ['Churn'])","fabb96f7":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=20))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","605962b6":"model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")","14efc7b0":"trained_model = model.fit(smote_x_train, smote_y_train, epochs=150,batch_size=20)","18be30a6":"plt.plot(trained_model.history[\"loss\"])","d4967a82":"Y_pred = model.predict(X_test)\nY_pred = np.where(Y_pred >= 0.5,1,0)","670afae7":"print(classification_report(Y_test,Y_pred))\nsmote_acc = accuracy_score(Y_test,Y_pred)\nsmote_rec = recall_score(Y_test,Y_pred)\nsmote_p = precision_score(Y_test,Y_pred)\nsmote_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',smote_rec)\nprint('Precision ->',smote_rec)\nprint('F1 Score ->',smote_f1)\nprint('Accuracy ->',smote_acc)","b1ac997f":"smote_y_train.value_counts()","c21cf8b9":"from imblearn.over_sampling import ADASYN\n\n# Synthesize minority class datapoints using SMOTE\nada = ADASYN(random_state=42)\nada_x_train, ada_y_train = ada.fit_resample(X_train, Y_train)","c2da61aa":"# Separate into training and test sets\nada_x_train = pd.DataFrame(ada_x_train, columns = X_train.columns)\nada_y_train = pd.DataFrame(ada_y_train, columns = ['Churn'])","f5c5a5bd":"ada_y_train.value_counts()","6095945c":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=20))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","9df5f1da":"model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")","00915d57":"trained_model = model.fit(ada_x_train, ada_y_train, epochs=150,batch_size=20)","44b19ccb":"plt.plot(trained_model.history[\"loss\"])","38ac6242":"Y_pred = model.predict(X_test)\nY_pred = np.where(Y_pred >= 0.5,1,0)","8a182247":"print(classification_report(Y_test,Y_pred))\nada_acc = accuracy_score(Y_test,Y_pred)\nada_rec = recall_score(Y_test,Y_pred)\nada_p = precision_score(Y_test,Y_pred)\nada_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',ada_rec)\nprint('Precision ->',ada_rec)\nprint('F1 Score ->',ada_f1)\nprint('Accuracy ->',ada_acc)","f37ae72a":"from imblearn.combine import SMOTETomek","6130693c":"# oversampling using SMOTE+TOMEK\nstom = SMOTETomek(random_state=42)\nstom_x_train, stom_y_train = stom.fit_resample(X_train, Y_train)","24811059":"# Separate into training and test sets\nstom_x_train = pd.DataFrame(stom_x_train, columns = X_train.columns)\nstom_y_train = pd.DataFrame(stom_y_train, columns = ['Churn'])","118e1749":"stom_y_train.value_counts()","8de78a74":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=20))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n\ntrained_model = model.fit(stom_x_train, stom_y_train, epochs=150,batch_size=20)","36ed8cc1":"plt.plot(trained_model.history[\"loss\"])","9dca9cbf":"Y_pred = model.predict(X_test)\nY_pred = np.where(Y_pred >= 0.5,1,0)\n\n\nprint(classification_report(Y_test,Y_pred))\nsmtom_acc = accuracy_score(Y_test,Y_pred)\nsmtom_rec = recall_score(Y_test,Y_pred)\nsmtom_p = precision_score(Y_test,Y_pred)\nsmtom_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',smtom_rec)\nprint('Precision ->',smtom_rec)\nprint('F1 Score ->',smtom_f1)\nprint('Accuracy ->',smtom_acc)","12aedea1":"from imblearn.combine import SMOTEENN","d2dee6aa":"# oversampling using SMOTE+ENN\nsenn = SMOTEENN(random_state=42)\nsenn_x_train, senn_y_train = senn.fit_resample(X_train, Y_train)","22e8a405":"# Separate into training and test sets\nsenn_x_train = pd.DataFrame(senn_x_train, columns = X_train.columns)\nsenn_y_train = pd.DataFrame(senn_y_train, columns = ['Churn'])","50b6bf67":"senn_y_train.value_counts()","b7ee8567":"model = Sequential()\nmodel.add(Dense(8, activation=\"relu\", input_dim=20))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n\ntrained_model = model.fit(senn_x_train, senn_y_train, epochs=150,batch_size=20)","e0237011":"plt.plot(trained_model.history[\"loss\"])","c292f336":"Y_pred = model.predict(X_test)\nY_pred = np.where(Y_pred >= 0.5,1,0)\n\n\nprint(classification_report(Y_test,Y_pred))\nsmenn_acc = accuracy_score(Y_test,Y_pred)\nsmenn_rec = recall_score(Y_test,Y_pred)\nsmenn_p = precision_score(Y_test,Y_pred)\nsmenn_f1 = f1_score(Y_test,Y_pred)\nprint('Recall ->',smenn_rec)\nprint('Precision ->',smenn_rec)\nprint('F1 Score ->',smenn_f1)\nprint('Accuracy ->',smenn_acc)","23d94bef":"performance = pd.DataFrame([[actual_acc,under_acc,over_acc,smote_acc,ada_acc,smtom_acc,smenn_acc],\n              [actual_rec,under_rec,over_rec,smote_rec,ada_rec,smtom_rec,smenn_rec],\n              [actual_p,under_p,over_p,smote_p,ada_p,smtom_p,smenn_p],\n              [actual_f1,under_f1,over_f1,smote_f1,ada_f1,smtom_f1,smenn_f1]]).T\n\nperformance.columns=['Accuracy','Recall','Precision','F1 Score']\nperformance.index=['Actual','Under_Sampling','Over_Sampling','SMOTE','ADASYN','SMOTE+TOMEK','SMOTE+ENN']","ffbeea9f":"performance","89c60915":"plt.figure(figsize=(20,12))\nplt.subplot(2,2,1)\nsns.barplot(x=performance.Accuracy.values,y=performance.Accuracy.index)\nplt.title('Accuracy')\n\nplt.subplot(2,2,2)\nsns.barplot(x=performance.Recall.values,y=performance.Recall.index)\nplt.title('Recall')\n\nplt.subplot(2,2,3)\nsns.barplot(x=performance.Precision.values,y=performance.Precision.index)\nplt.title('Precision')\n\nplt.subplot(2,2,4)\nsns.barplot(x=performance['F1 Score'].values,y=performance['F1 Score'].index)\nplt.title('F1 Score')\n\nplt.show()","78658648":"# Data Inspection","5eb95bfe":"# Neural Network","3a984140":"No Outliers in the dataset.","266f8ea8":"# Prediction","686e6a23":"Building Neural Network Models with following re-sampling techniques and observing the recall value:\n\n1.   Without Re-sampling Techniques (Actual)\n2.   Under Sampling\n3.   Over Sampling\n4.   SMOTE\n5.   ADASYN\n6.   SMOTE+TOMEK\n7.   SMOTE+ENN\n\n","cd247bde":"4. SMOTE (synthetic minority oversampling technique)","f34d329b":"2. Under Sampling","1eb00adf":"Handling Missing Values","9c46b426":"Under sampling the majority","090b8dfa":"There is some string in TotalCharges column, converting that string to missing values and other string to float.","173912e8":"# Split the Data","d25e62e4":"# Label Encoding of X","a62f008a":"3. Over Sampling the minority","f3864e24":"1. Without Re-sampling Techniques\n","86f7eb0e":"# Evaluation","05d25052":"No Null Values now. Data is clean.","2fd4e466":"# Checking Null Values","4d0a2508":"Looking unique values","5726b2b2":"# Import Libraries","0b9457b7":"# Scaling of X","7245426a":"# Target Variable","584a32a3":"Yes = 1 = Customer will stop the service \\\nNo  = 0 = Customer will continue the service \\\n\nFALSE NEGATIVE IS CRITICAL IN THIS CASE.\nRECALL VALUE SHOULD BE CLOSE TO 1","0d0a93b3":"7. Hybridization: SMOTE + ENN\n","e524711a":"6. Hybridization: SMOTE + Tomek Links\n","9895f83b":"Replacing No internet service and No phone service with No\n","4c9b310d":"# Read Data","16372032":"5. ADASYN: Adaptive Synthetic Sampling Approach","5424a416":"# Checking Outliers"}}