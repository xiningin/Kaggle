{"cell_type":{"bc9853b2":"code","5fa85fb9":"code","643fb67c":"code","da9c1b8e":"code","7902792a":"code","499a0754":"code","15f07321":"code","3e3bc2a7":"code","de26265e":"code","83d11fd6":"code","ac739914":"code","84e6ec5c":"code","976b46eb":"code","1e601c99":"code","c82573d6":"code","a57df460":"code","1a568f26":"code","cf8b5069":"code","2617a244":"code","219d0da4":"code","605688ca":"code","eb79ed82":"code","99ed31a1":"code","206097cb":"code","bb40d196":"code","db0151c6":"code","7d273d57":"code","d6e72853":"code","55c70210":"code","28cd7adf":"code","6605b4ab":"code","5ba6aca2":"code","5cb7a493":"code","b7c8ef24":"code","6a1af056":"code","ae5f1384":"code","7d00107f":"code","ee8c1a24":"code","53ca08a1":"code","1cf6e8ec":"code","15f415ec":"code","8ce1fd2c":"code","a6ea0fed":"code","754f62c8":"code","b3824817":"code","3d48c619":"code","54b14b16":"code","7510a2c1":"code","015cae9d":"code","3945d784":"code","e6791eb2":"code","85825711":"code","d12e82ae":"code","790b9c18":"code","4c0d5593":"code","6b8fd54d":"code","34d631fa":"code","731f8369":"markdown","80894511":"markdown","e7ca17e0":"markdown","a49237bd":"markdown","4a9a0e20":"markdown","a235897a":"markdown","adcff4d6":"markdown","02450e8b":"markdown","764a1f53":"markdown","9ebb805e":"markdown","3ece7b70":"markdown","09266a1e":"markdown","db3e2f84":"markdown","d4420092":"markdown","57e5700c":"markdown","a5fcdbeb":"markdown","9062a43f":"markdown","e4265619":"markdown","16d2b313":"markdown","bd4754f5":"markdown","3ed8f9e1":"markdown","204c49c7":"markdown","5421aef7":"markdown","3f531bd1":"markdown","0ba0b921":"markdown","533a0ece":"markdown","a88d4974":"markdown","e8e90a8b":"markdown","4f018e03":"markdown","f802caef":"markdown","3b222d96":"markdown","ff801bae":"markdown","29f8f8c6":"markdown","a29a6b31":"markdown","ee194778":"markdown","702ad2ed":"markdown","9387eca9":"markdown","3ec69489":"markdown","da7be1f7":"markdown","3421f964":"markdown","ff9562bf":"markdown","f1563db0":"markdown","b6a3aa3d":"markdown","79afb8a1":"markdown","df1d2730":"markdown","1dec1379":"markdown","2e040559":"markdown","f58b0f93":"markdown","6cfc7eaf":"markdown","e5dd2ab6":"markdown","cb357166":"markdown","b0ae8e7e":"markdown","2a27052b":"markdown","db0a8c28":"markdown"},"source":{"bc9853b2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph.\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","5fa85fb9":"data = pd.read_csv('..\/input\/train.csv')\ndata.head(5)","643fb67c":"data.shape","da9c1b8e":"data.info()","7902792a":"data.describe()","499a0754":"import pandas_profiling\npandas_profiling.ProfileReport(data)","15f07321":"%matplotlib inline\ndata.hist(figsize=(12,8))\nplt.figure()","3e3bc2a7":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"Survived\"\ngrouped = data[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\nlayout = {'title': 'Survived(0 = No, 1 = Yes)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","de26265e":"col = \"Sex\"\ngrouped = data[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\nlayout = {'title': 'Sex(male, female)'}\nfig = go.Figure(data = [trace], layout = layout)\nfig.layout.template='presentation'\niplot(fig)","83d11fd6":"x=data\nd1=x[x['Survived']==0]\nd2=x[x['Survived']==1]","ac739914":"col='Sex'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#6ad49b\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict())\ny = [trace1, trace2]\nlayout={'title':\"surviving rate male vs female\",'xaxis':{'title':\"Sex\"}}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='plotly'\niplot(fig)","84e6ec5c":"col='Pclass'\nv2=x[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"Emb\",  marker=dict(color=\"#9467bd\"))\nlayout={'title':\"Pclass count\",'xaxis':{'title':\"pclass\"}}\nfig = go.Figure(data=[trace1], layout=layout)\nfig.layout.template='presentation'\niplot(fig)","976b46eb":"col='Pclass'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#d62728\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict(color='#6ad49b'))\ny = [trace1, trace2]\nlayout={'title':\"surviving rate in Pclass\",'xaxis':{'title':\"Pclass\"},'barmode': 'relative'}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='presentation'\niplot(fig)","1e601c99":"col='Embarked'\nv2=x[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"Emb\",  marker=dict(color=\"#bcbd22\"))\nlayout={'title':\"Embarked Count\",'xaxis':{'title':\"Embarked\"}}\nfig = go.Figure(data=[trace1], layout=layout)\nfig.layout.template='plotly_dark'\niplot(fig)","c82573d6":"col='Embarked'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#bcbd22\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict(color='#8c564b'))\ny = [trace1, trace2]\nlayout={'title':\"surviving rate in Embarked\",'xaxis':{'title':\"Embarked\"}}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='presentation'\niplot(fig)","a57df460":" col='SibSp'\nv2=x[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"Emb\",  marker=dict(color=\"#e377c2\"))\nlayout={'title':\"siblings\/spouse Count\",'xaxis':{'title':\"SibSp\"}}\nfig = go.Figure(data=[trace1], layout=layout)\nfig.layout.template='presentation'\niplot(fig)","1a568f26":"col='SibSp'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#17becf\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict(color='#ff7f0e'))\ny = [trace1, trace2]\nlayout={'title':\"surviving rate in SibSp\",'xaxis':{'title':\"SibSp\"}}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='presentation'\niplot(fig)","cf8b5069":"col='Age'\nv1=x[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"rgb(63, 72, 204)\"),text= data.Name)\ny = [trace1]\nlayout={'title':\"Age count with name\",'xaxis':{'title':\"Age\"}}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='presentation'\niplot(fig)","2617a244":"print('Oldest Passenger was of:',data['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data['Age'].min(),'Years')\nprint('Average Age on the ship:',data['Age'].mean(),'Years')","219d0da4":"col='Age'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#d62728\"),text= data.Name)\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict(color='#bcbd22'),text= data.Name)\ny = [trace1, trace2]\nlayout={'title':\"surviving rate on the basic of Age with their names\",'xaxis':{'title':\"Age\"}}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='plotly_dark'\niplot(fig)","605688ca":" col='Parch'\nv2=x[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"Emb\",  marker=dict(color=\"#a678de\"))\nlayout={'title':\"Parch Count\",'xaxis':{'title':\"Parch\"}}\nfig = go.Figure(data=[trace1], layout=layout)\nfig.layout.template='presentation'\niplot(fig)","eb79ed82":"col='Parch'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#17becf\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict(color='rgb(63, 72, 204)'))\ny = [trace1, trace2]\nlayout={'title':\"surviving rate on the basic of Parch\",'xaxis':{'title':\"Parch\"},'barmode': 'relative'}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='presentation'\niplot(fig)","99ed31a1":"col='Fare'\nv1=x[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=\"fare\", marker=dict(color=\"#9467bd\"))\ny = [trace1]\nlayout={'title':\"Farecount\",'xaxis':{'title':\"Fare\"}}\nfig = go.Figure(data=y, layout=layout)\niplot(fig)","206097cb":"col='Fare'\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=\"0\", marker=dict(color=\"#17becf\"),text= data.Name)\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=\"1\", marker=dict(color='#bcbd22'),text= data.Name)\ny = [trace1, trace2]\nlayout={'title':\"surviving rate on the basic of fare with their names\",'xaxis':{'title':\"Fare\"}}\nfig = go.Figure(data=y, layout=layout)\nfig.layout.template='plotly_dark'\niplot(fig)","bb40d196":"plt.style.use('fivethirtyeight')\ngrid = sns.FacetGrid(data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', bins=20)\n","db0151c6":"plt.figure(num=None, figsize=(10, 6), dpi=80, facecolor='w', edgecolor='k')\n\n# specify hue=\"categorical_variable\"\nsns.boxplot(y='Age', x='Survived', hue=\"Pclass\", data=data)\nplt.show()","7d273d57":"grid = sns.FacetGrid(data, col='Survived', row='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', bins=15)","d6e72853":"plt.figure(num=None, figsize=(10, 6), dpi=80, facecolor='w', edgecolor='k')\n\n# specify hue=\"categorical_variable\"\nsns.boxplot(y='Age', x='Survived', hue=\"Sex\", data=data)\nplt.show()","55c70210":"grid = sns.FacetGrid(data, col='Survived', row='Embarked', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', bins=15)","28cd7adf":"plt.figure(num=None, figsize=(10, 6), dpi=80, facecolor='w', edgecolor='k')\n\n# specify hue=\"categorical_variable\"\nsns.boxplot(y='Age', x='Survived', hue=\"Embarked\", data=data)\nplt.show()","6605b4ab":"f,ax=plt.subplots(1,2,figsize=(12,6))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","5ba6aca2":"f,ax=plt.subplots(2,2,figsize=(12,6))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","5cb7a493":"data.corr()","b7c8ef24":"data.groupby('Pclass',as_index=False)['Survived'].mean()","6a1af056":"data.groupby('Sex',as_index=False)['Survived'].mean()","ae5f1384":"data.groupby('Embarked',as_index=False)['Survived'].mean()","7d00107f":"data.groupby('SibSp',as_index=False)['Survived'].mean().sort_values(by='Survived',ascending=False)","ee8c1a24":"data.groupby('Parch',as_index=False)['Survived'].mean().sort_values(by='Survived',ascending=False)","53ca08a1":"fig,ax = plt.subplots(figsize=(8,7))\nax = sns.heatmap(data.corr(), annot=True,linewidths=.5,fmt='.1f')\nplt.show()","1cf6e8ec":"data['NewSibSp'] = 5 # let something more than 5 be 5 (others)\n\ndata.loc[(data.SibSp.values == 0),'NewSibSp']= 0\ndata.loc[(data.SibSp.values == 1),'NewSibSp']= 1\ndata.loc[(data.SibSp.values == 2),'NewSibSp']= 2\ndata.loc[(data.SibSp.values == 3),'NewSibSp']= 3\ndata.loc[(data.SibSp.values == 4),'NewSibSp']= 4","15f415ec":"data['NewParch'] = 3 # let something more than 3 be 3 (others)\n\ndata.loc[(data.Parch.values == 0),'NewParch']= 0\ndata.loc[(data.Parch.values == 1),'NewParch']= 1\ndata.loc[(data.Parch.values == 2),'NewParch']= 2","8ce1fd2c":"data['Agroup'] = 1\n\ndata.loc[(data.Age.values < 24.0),'Agroup']= 0\ndata.loc[(data.Age.values > 30.0),'Agroup']= 2\n\ndata.head()","a6ea0fed":"for i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')\n\ndata.head()","754f62c8":"def survpct(a):\n  return data.groupby(a).Survived.mean()\n\nsurvpct('Initial')","b3824817":"total=data.isnull().sum()\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","3d48c619":"data['Age'] = data.groupby('Initial')['Age'].apply(lambda x: x.fillna(x.mean()))\ndata.head()","54b14b16":"data['Fare']=np.log1p(data[\"Fare\"])","7510a2c1":"data.drop(['PassengerId','Name','Age','Parch','Ticket','Cabin'],axis=1,inplace=True)","015cae9d":"data=pd.get_dummies(data)\ndata.head()","3945d784":"data.shape","e6791eb2":"X = data.drop(['Survived'], axis = 1)\ny = data.Survived.values","85825711":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)","d12e82ae":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.svm import SVC","790b9c18":"#LogisticRegression\nlr_c=LogisticRegression(random_state=0)\nlr_c.fit(X_train,y_train)\nlr_pred=lr_c.predict(X_test)\nlr_cm=confusion_matrix(y_test,lr_pred)\nlr_ac=accuracy_score(y_test, lr_pred)\n\n#SVM classifier\nsvc_c=SVC(kernel='linear',random_state=0)\nsvc_c.fit(X_train,y_train)\nsvc_pred=svc_c.predict(X_test)\nsv_cm=confusion_matrix(y_test,svc_pred)\nsv_ac=accuracy_score(y_test, svc_pred)\n\n#SVM regressor\nsvc_r=SVC(kernel='rbf')\nsvc_r.fit(X_train,y_train)\nsvr_pred=svc_r.predict(X_test)\nsvr_cm=confusion_matrix(y_test,svr_pred)\nsvr_ac=accuracy_score(y_test, svr_pred)\n\n#RandomForest\nrdf_c=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\nrdf_c.fit(X_train,y_train)\nrdf_pred=rdf_c.predict(X_test)\nrdf_cm=confusion_matrix(y_test,rdf_pred)\nrdf_ac=accuracy_score(rdf_pred,y_test)\n\n# DecisionTree Classifier\ndtree_c=DecisionTreeClassifier(criterion='entropy',random_state=0)\ndtree_c.fit(X_train,y_train)\ndtree_pred=dtree_c.predict(X_test)\ndtree_cm=confusion_matrix(y_test,dtree_pred)\ndtree_ac=accuracy_score(dtree_pred,y_test)\n\n#KNN\nknn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train,y_train)\nknn_pred=knn.predict(X_test)\nknn_cm=confusion_matrix(y_test,knn_pred)\nknn_ac=accuracy_score(knn_pred,y_test)","4c0d5593":"print('LogisticRegression_accuracy:\\t',lr_ac)\nprint('SVM_regressor_accuracy:\\t\\t',svr_ac)\nprint('RandomForest_accuracy:\\t\\t',rdf_ac)\nprint('DecisionTree_accuracy:\\t\\t',dtree_ac)\nprint('KNN_accuracy:\\t\\t\\t',knn_ac)\nprint('SVM_classifier_accuracy:\\t',sv_ac)","6b8fd54d":"plt.figure(figsize=(20,10))\nplt.subplot(2,3,1)\nplt.title(\"LogisticRegression_cm\")\nsns.heatmap(lr_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,3,2)\nplt.title(\"SVM_regressor_cm\")\nsns.heatmap(sv_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,3,3)\nplt.title(\"RandomForest\")\nsns.heatmap(rdf_cm,annot=True,cmap=\"Oranges\",fmt=\"d\",cbar=False)\nplt.subplot(2,3,4)\nplt.title(\"SVM_classifier_cm\")\nsns.heatmap(svr_cm,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\nplt.subplot(2,3,5)\nplt.title(\"DecisionTree_cm\")\nsns.heatmap(dtree_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,3,6)\nplt.title(\"kNN_cm\")\nsns.heatmap(knn_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.show()","34d631fa":"model_accuracy = pd.Series(data=[lr_ac,sv_ac,svr_ac,rdf_ac,dtree_ac,knn_ac], \n                index=['LogisticRegression','SVM_classifier','SVM_regressor',\n                                      'RandomForest','DecisionTree_Classifier','KNN'])\nfig= plt.figure(figsize=(10,6))\nmodel_accuracy.sort_values().plot.barh()\nplt.title('Model Accracy')","731f8369":"there seems to have little corelation among survived and Pclass.Higher class passangers are more likely to survive.","80894511":"Maximum passenegers boarded from S. Majority of them being from Pclass3.The Passengers from C look to be lucky as a good proportion of them survived.Port Q had almost 95% of the passengers were from Pclass3, passengers from Pclass3 around 81% didn't survive.","e7ca17e0":"* That looks amazing. It is usually said that Money can\u2019t buy Everything, But it is clearly seen that passengers of Class 1 are given high priority while Rescue. There are greater number of passengers in Class 3 than Class 1 and Class 2 but very few, almost 25% in Class 3 survived. In Class 2, survival and non-survival rate is 49% and 51% approx. While in Class 1 almost 68% people survived. So money and status matters here.\n* it is clear that women survival rate in Class 1 is about 95\u201396%, as only 3 out of 94 women died. So, it is now more clear that irrespective of Class, women are given first priority during Rescue. Because survival rate for men in even Class 1 is also very low. From this conclusion, PClass is also a important feature","a49237bd":"# Summary\nWe started with the data exploration where we got a feeling for the dataset, checked about missing data and learned which features are important. During this process we used seaborn and matplotlib to do the visualizations. During the data preprocessing part, we computed missing values, converted features into numeric ones, grouped values into categories and created a few new features. Afterwards we started training  machine learning models, and applied cross validation on it. \nOf course there is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. Another thing that can improve the overall result on the kaggle leaderboard would be a more extensive hyperparameter tuning on several machine learning models. You could also do some ensemble learning.Lastly, we looked at it\u2019s confusion matrix and computed the models precision.","4a9a0e20":" **>>** You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n \n **>>** For men the probability of survival is very low between the age of 5 and 18, but that isn\u2019t true for women. Another thing to note is that infants also have a little bit higher probability of survival.","a235897a":"### Age violinplot based on Pclass,Survived\n","adcff4d6":"# Import libraries","02450e8b":"There are many interesting facts with this feature. Above plot shows that if a passanger is alone in ship with no siblings, survival rate is 34.5%. The graph decreases as no of siblings increase. This is interesting because, If I have a family onboard, I will save them instead of saving myself. But there\u2019s something wrong, the survival rate for families with 5\u20138 members is 0%. Is this because of PClass? Yes this is PClass, The crosstab shows that Person with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass3(>3) died.","764a1f53":"* The training-set has 891 examples and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects. \n* Through this data we would like to find the effect of various factors such as age, sex, station of Embarkment,their class and no. of relatives present on survival chances of passangers.our cabin column has lots of null values.so we would not like to modify it much. there is only 2 entries in embarked column having null values,so we will replace it with mode value of point of embarktion","9ebb805e":"Persons with more parents or children seem to have low chances.Let's divide our age data into bands and let's see if there is some corelation.","3ece7b70":"From the above plots, I found the following observations\n(1) First priority during Rescue is given to children and women, as the persons<5 are save by large numbers\n(2) The oldest saved passenger is of 80\n(3) The most deaths were between 30\u201340\n>  The no of children is increasing from Class 1 to 3, the number of children in Class 3 is greater than other two. 2) Survival rate of children, for age 10 and below is good irrespective of Class 3) Survival rate between age 20\u201330 is well and is quite better for women.","09266a1e":"# Data cleaning","db3e2f84":"### Plotting the Accuracy of the models\nHere we plot the performance or the accuracy of the different machine learning model, in this plot we observe that the different models have diffrent performence.","d4420092":"The 3 embarked category in the dataset, among them 168 belongs to category C, 77 belongs to category Q and 644 belongs to category s.","57e5700c":"But, what exactly is the best hyperplane? For SVM, it\u2019s the one that maximizes the margins from both tags. In other words: the hyperplane (remember it\u2019s a line in this case) whose distance to the nearest element of each tag is the largest.","a5fcdbeb":"that's pretting amazing correlation. females are 4 times more likely to survive","9062a43f":"Look at the above figure. Sex seems to be very interesting feature. The survival rate of men is much less than that of women. Only 81 women died out of 344. But 109 men survived out of 577. It means that women were given high priority while Rescue.","e4265619":"### Age histogram based on Embarked,Survived","16d2b313":"# Data Exploration\/Analysis","bd4754f5":"Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the \u2018Age\u2019 feature.","3ed8f9e1":"# Getting the data","204c49c7":"### Confusion matrix\nThe first row is about the not-survived-predictions: 364 passengers were correctly classified as not survived (called true negatives) and 60 where wrongly classified as not survived (false positives).\n\nThe second row is about the survived-predictions: 96passengers where wrongly classified as survived (false negatives) and 192re correctly classified as survived (true positives).\n\nA confusion matrix gives you a lot of information about how well your model does, but theres a way to get even more, like computing the classifiers precision.","5421aef7":"SibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it above and also a feature that sows if someone is not alone","3f531bd1":"# I hope this kernel is helpfull for you  --> upvote will appreciate me for further work.\n","0ba0b921":"A support vector machine takes these data points and outputs the hyperplane (which in two dimensions it\u2019s simply a line) that best separates the tags. This line is the decision boundary: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red.","533a0ece":"# Visulaly analyzing","a88d4974":"It is clear that the no of people survived is less than the number of people who died.\n> From the total passenger on the titanic 61.6% people died and 31.6% survived. If we assume all the passenger died in that infamous incident then our accuracy is about to 62%. But we have to analyse more and train our model to predict more accurate value.","e8e90a8b":"Embarked seems to be correlated with survival, depending on the gender.\n* Women on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S","4f018e03":"64.8% of the total passenger are male and 35.2% are female.","f802caef":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. ","3b222d96":"From the table above, we can note a few things. First of all, that we need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.","ff801bae":"Having more sibling can be corelated to less survival rate","29f8f8c6":"# Creating Dummy Variables","a29a6b31":"## Let's see feature corelation","ee194778":"#### >> we can clearly see that women had very higher survival rate.","702ad2ed":"Whether a passenger is alone or have family, from the plot we see that most of the person are alone and there's a family which have 6 members.","9387eca9":"### Age Histogram based on Gender and Survived","3ec69489":"There are 3 classes of passengers. Class1 216 passangers Class2 184 passangers and Class3 491 passangers, here we see that the no. of passangers in the class 3 is more than class 1 and class 3. ","da7be1f7":"# How does SVM work?\nThe basics of Support Vector Machines and how it works are best understood with a simple example. Let\u2019s imagine we have two tags: red and blue, and our data has two features: x and y. We want a classifier that, given a pair of (x,y) coordinates, outputs if it\u2019s either red or blue. We plot our already labeled training data on a plane:","3421f964":"<img src=\"https:\/\/monkeylearn.com\/blog\/wp-content\/uploads\/2017\/06\/plot_hyperplanes_annotated.png\" style=\"width: 350px;\"\/>","ff9562bf":"> we see that more no. of passangers survived in upper classes and also children were almost certain to survive if they belonged to higher classes.","f1563db0":"<img src=\"https:\/\/monkeylearn.com\/blog\/wp-content\/uploads\/2017\/06\/plot_hyperplanes_2.png\" style=\"width: 350px;\"\/>","b6a3aa3d":"And that\u2019s the basics of Support Vector Machines!\n1.A support vector machine allows you to classify data that\u2019s linearly separable.\n2.If it isn\u2019t linearly separable, you can use the kernel trick to make it work.\nHowever, for text classification it\u2019s better to just stick to a linear kernel.\nCompared to newer algorithms like neural networks, they have two main advantages: higher speed and better performance with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems, where it\u2019s common to have access to a dataset of at most a couple of thousands of tagged samples.","79afb8a1":"**>>** For males, the survival chances decreases with an increase in age.Survival chances for Womens Passenegers aged 20-50 from Pclass1 is high,As we had seen earlier, the Age feature has 177 null values.","df1d2730":"# Correlation\nIt is used to describe the linear relationship between two continuous variables (e.g., height and weight). In general, correlation tends to be used when there is no identified response variable. It measures the strength (qualitatively) and direction of the linear relationship between two or more variables.","1dec1379":"* Survived: The Survived variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. All other variables are potential predictor or independent variables.\n* PassengerID and Ticket: The PassengerID and Ticket variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n* Pclass: The Pclass variable is an ordinal datatype for the ticket class, a proxy for socio-economic status (SES), representing 1 = upper class, 2 = middle class, and 3 = lower class.\n* Name: The Name variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size from surname, and SES from titles like doctor or master. Since these variables already exist, we'll make use of it to see if title, like master, makes a difference.\n* Sex and Embarked: The Sex and Embarked variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n* Age and Fare: The Age and Fare variable are continuous quantitative datatypes.\n* SibSp: The SibSp represents number of related siblings\/spouse aboard and Parch represents number of related parents\/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.\n* Cabin: The Cabin variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred. However, since there are many null values, it does not add value and thus is excluded from analysis.","2e040559":"# Building Machine Learning Models &  Train Data\nNow we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other. Later on, we will use cross validation.","f58b0f93":"<img src=\"https:\/\/monkeylearn.com\/blog\/wp-content\/uploads\/2017\/06\/plot_original.png\" style=\"width: 350px;\"\/>","6cfc7eaf":"### Survived","e5dd2ab6":"# Introduction\n\nTitanic movie is one of the most beautiful Love story movie I have ever seen. Love affair of Jack and Rose start in Ship and they enjoy the company of each other. Jack was a poor boy while Rose belong to a rich family and engaged to some other person Caledon. The ship drowns by bumping with a Iceberg.\n* > The sinking of the Titanic is one of the most disgraceful shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This tragedy shocked the international community and led to better safety regulations for ships.\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew, Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\nIn this notebook, I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival.\nIn this challenge, we are asked to predict whether a passenger on the titanic would have been survived or not.","cb357166":"Embarked seems to be correlated with survival, depending on the gender.\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.\nPclass also seems to be correlated with survival. We will generate another plot of it below.","b0ae8e7e":" total number of survived passanger ","2a27052b":"# Titanic\n<img src=\"https:\/\/res.cloudinary.com\/dk-find-out\/image\/upload\/q_80,w_1920,f_auto\/MA_00079563_yvu84f.jpg\" style=\"width: 650px;\"\/>","db0a8c28":"# What is Support Vector Machine?\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N\u200a\u2014\u200athe number of features) that distinctly classifies the data points.\nSupport Vector Machine (SVM) is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. SVM supports both regression and classification tasks and can handle multiple continuous and categorical variables. For categorical variables a dummy variable is created with case values as either 0 or 1."}}