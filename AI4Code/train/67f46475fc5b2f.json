{"cell_type":{"114f6dc8":"code","9c9a3d75":"code","0779a766":"code","e4f486c2":"code","6de77186":"code","b7167ac3":"code","465c35dc":"code","80370dea":"code","81121366":"code","fc54a416":"code","6e4794b3":"code","a00e8832":"code","55d7b76f":"code","dfb1233a":"code","5d291de5":"code","2e06c67a":"code","3355543e":"code","4c7b94f3":"code","65fb472a":"code","12ee6106":"code","8d8a171c":"code","633a313b":"code","019d9a15":"code","c83f34cb":"code","5411af81":"code","3fb1000a":"code","655a95c7":"code","6d097cea":"code","e7d54ee6":"code","e217733a":"code","615c5cb2":"code","068bf2d6":"code","7c904f3e":"code","189f9e9b":"markdown","fead30ec":"markdown","d712eba6":"markdown","90a0bbd3":"markdown","5e51a118":"markdown","0bd7b7e5":"markdown","86e44cd6":"markdown","daa7f7ad":"markdown","6c591dbd":"markdown","51068f43":"markdown","a0006a44":"markdown","d35a1e57":"markdown","11b71b09":"markdown","350e9dc8":"markdown","d8c1642e":"markdown","10ef4a1a":"markdown","a3aec4f5":"markdown","c9ff37d0":"markdown","7d2786e7":"markdown","3d0d1ab9":"markdown"},"source":{"114f6dc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c9a3d75":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","0779a766":"train_og = train.copy()\ntest_og = test.copy()\n\ntrain.head()","e4f486c2":"test.head()","6de77186":"train.shape","b7167ac3":"test.shape","465c35dc":"\"\"\"\nCombining train and test data will allow us to manipulate both sets at the same time\n\"\"\"\ndata = pd.concat([train, test], keys=('x', 'y'))\ndata = data.drop([\"Id\"], axis = 1)\n\n\"\"\"\nSort columns by overall and relative amounts of null data\n\"\"\"\nnull_data = data.isnull().sum().sort_values(ascending=False)\n\nnull_percentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([null_data, null_percentage], axis= 1, keys= [\"Total\", \"Percentage\"])\nmissing_data.head(20)","80370dea":"\"\"\"\nWe will remove these columns completely because of high instances of null data\n\"\"\"\ndata = data.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"LotFrontage\"],\n                 axis = 1)","81121366":"\"\"\"\nSeparate numeric and categorical variables before filling in null values\n\"\"\"\nnum_data = data._get_numeric_data().columns.tolist()\ncat_data = set(data.columns) - set(num_data)\n\n\"\"\"\nFill in null values for numeric and categorical variables with mean and mode respectively\n\"\"\"\nfor col in num_data:\n    data[col].fillna(data[col].mean(), inplace=True)\n    \nfor col in cat_data:\n    data[col].fillna(data[col].mode()[0], inplace=True)\n    \n\"\"\"\nCheck to see if all null values have been filled\n\"\"\"\ndata[num_data].isnull().sum()","fc54a416":"data[cat_data].isnull().sum()","6e4794b3":"\"\"\"\nNow take a look at the categorical data. Remove any variables dominated by a single class of data \nbecause they will not be very useful for training our algorithms. To make it easy we will use an 80% cutoff.\n(remove columns with >80% single class of data)\n\"\"\"\nfor i in cat_data:\n    print(data[i].value_counts())","a00e8832":"data = data.drop([\"LandSlope\", \"Condition2\", \"LandContour\", \"Street\", \"ExterCond\",\n                  \"Condition1\", \"Functional\", \"Electrical\", \"CentralAir\",\n                  \"Heating\", \"GarageQual\", \"RoofMatl\", \"BsmtCond\", \"PavedDrive\",\n                  \"Utilities\", \"GarageCond\", \"BsmtFinType2\"], axis= 1)","55d7b76f":"plt.figure(figsize=(12,10))\n\nsns.set_style(\"darkgrid\")\nsns.histplot(data=train, x=\"SalePrice\", bins=50, cbar=True)","dfb1233a":"train['SalePrice'] = np.log1p(train['SalePrice'])\ndata['SalePrice'] = np.log1p(data['SalePrice'])\n\nplt.figure(figsize=(12,10))\nsns.set_style(\"darkgrid\")\nsns.histplot(data=train, x=\"SalePrice\", bins=50, cbar=True, color='black')","5d291de5":"\"\"\"\nRank variables based on correlation with sale price\n\"\"\"\ncorr = train.corr()\ncorr_rank = corr[\"SalePrice\"].sort_values(ascending = False)\ncorr_rank","2e06c67a":"\"\"\"\nDelete variables with correlation of absolute value less than 0.1\n\"\"\"\ndata = data.drop([\"PoolArea\", \"MoSold\", \"3SsnPorch\", \"BsmtFinSF2\", \"BsmtHalfBath\",\n                  \"MiscVal\", \"LowQualFinSF\", \"YrSold\", \"OverallCond\", \"MSSubClass\"],\n                 axis = 1)","3355543e":"top_features = corr.index[abs(corr[\"SalePrice\"]>0.5)]\nplt.figure(figsize = (9,9))\nheat_map = sns.heatmap(data[top_features].corr(), annot=True, cmap=\"RdYlGn\")","4c7b94f3":"corr_rank = corr_rank.drop([\"SalePrice\"])\nsorted_corr = corr_rank.index.tolist()\nfig, axes = plt.subplots(4, 3, figsize=(20,10), sharey= True)\nfig.suptitle(\"Highest Correlation with Sale Price\", fontsize= 20)\nplt.subplots_adjust(hspace = 0.7, wspace=0.1)\nfor i,col in zip(range(12), sorted_corr):\n    sns.scatterplot(y=data['SalePrice'], x=data[col],ax=axes[i\/\/3][i%3])\n    axes[i\/\/3][i%3].set_title('SalesPrice with '+col)","65fb472a":"n_features = data.select_dtypes(exclude = [\"object\"]).columns\nn_features","12ee6106":"data_outliers = data[[\"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\",\n                \"1stFlrSF\", \"2ndFlrSF\", \"GrLivArea\", \"GarageArea\", \"WoodDeckSF\",\n                \"OpenPorchSF\"]]","8d8a171c":"def mod_outliers(data):\n    df1 = data.copy()\n    data = data[[\"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\",\n                \"1stFlrSF\", \"2ndFlrSF\", \"GrLivArea\", \"GarageArea\", \"WoodDeckSF\",\n                \"OpenPorchSF\"]]\n    \n    q1 = data.quantile(0.25)\n    q3 = data.quantile(0.75)\n    \n    iqr = q3 - q1\n    \n    lower_bound = q1 - (1.5 * iqr)\n    upper_bound = q3 + (1.5 * iqr)\n    \n    for col in data.columns:\n        for i in range(0, len(data[col])):\n            if data[col][i] < lower_bound[col]:\n                data[col][i] = lower_bound[col]\n                \n            if data[col][i] > upper_bound[col]:\n                data[col][i] = upper_bound[col]\n                \n    for col in data.columns:\n        df1[col] = data[col]\n        \n    return(df1)","633a313b":"data_outliers = mod_outliers(data_outliers)\ndata = mod_outliers(data)","019d9a15":"\"\"\"\nPrint box plots of each modified variable to check if outliers were indeed removed\n\"\"\"\nfor i in data_outliers:\n    sns.boxplot(x=data_outliers[i])\n    plt.show()","c83f34cb":"data = pd.get_dummies(data)\n\ntrain = data.loc[\"x\"]\ntest = data.loc[\"y\"]\ntest = test.drop([\"SalePrice\"], axis = 1)\n\ny = train[\"SalePrice\"]\ntrain_x = train.drop([\"SalePrice\"], axis = 1)\ntest_x = test","5411af81":"train_x.shape","3fb1000a":"test_x.shape","655a95c7":"y.shape","6d097cea":"\"\"\"\nUsing the cross_val_score() function, we will measure all of our models with 5-fold cross validation and return\nroot mean squared error to compare the accuracy of each model.\n\"\"\"\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, train_x, y, scoring = \"neg_mean_squared_error\",\n                                    cv = 5))\n    return(rmse)\nridge = RidgeCV(alphas = [0.05, 0.1, 0.3, 1, 5, 10, 15, 30, 50, 75]).fit(train_x, y)\nrmse_cv(ridge).mean()","e7d54ee6":"lasso = LassoCV(alphas = [1, 0.1, 0.01, 0.001, 0.0001]).fit(train_x, y)\nrmse_cv(lasso).mean()","e217733a":"model_xgb = xgb.XGBRegressor(n_estimators = 360, max_depth = 2, learning_rate = 0.1)\nmodel_xgb.fit(train_x, y)","615c5cb2":"lasso_preds = np.expm1(lasso.predict(test_x))\nxgb_preds = np.expm1(model_xgb.predict(test_x))\npreds = 0.7*lasso_preds + 0.3*xgb_preds","068bf2d6":"submission = pd.DataFrame({\"id\": test_og.Id, \"SalePrice\": preds})\nsubmission.head(5)","7c904f3e":"submission.to_csv(\"submission.csv\", index = False)","189f9e9b":"### Dealing with Null Data\n\nNow that we've imported and inspected our data, it's time to begin cleaning. First we will be dealing with null values. We will want to delete columns with an excessive amount of null values and fill in the null values for the rest of the columns. This will make it easier for us to run machine learning algorithms later on in order to make our final predictions about sale price.","fead30ec":"# Apply Machine Learning Algorithms and Make Final Prediction\n\nThe final step is to apply some regression algorithms to our data and make a final prediction. Since we have a high number of dimensions we are going to skip simple linear regression. We will start by cross validating the ridge and lasso algorithms in order to find the best tuning parameters for our data. Lasso and Ridge regression are both designed to reduce the coefficients of non-important variables. The alpha parameter determines the degree to which the coefficients are reduced. Then we will use XGBoost to boost the better performing algorithm between lasso and ridge and save our final prediction.","d712eba6":"Using lasso regression we were able to achieve a lower rmse and therefore we will use XGBoost to boost our lasso model and make our final prediction. Although lasso and ridge both reduce the coefficients of variables based on alpha, it should be noted that only lasso can completely remove variables by reducing their coefficients to zero. Since this dataset has high dimensionality, this could be a reason why lasso was able to perform slightly better than ridge.","90a0bbd3":"# Clean the Data and Perform Exploratory Analaysis","5e51a118":"According to our correlation heatmap, overall quality (OverallQUal) and above ground square footage (GrLivArea) are key variables which have the highest correlation with sale price. We can see that GarageCars and GarageArea also have a high correlation with sale price, but since they essentially measure the same thing we don't have to consider them individually. First floor and basement square footage also have a relatively high correlation with sale price. Based on this analysis it seems like variables dealing with living area have a high impact on the sale price.","0bd7b7e5":"After the log transformation the data now has a relatively normal distribution. We can now continue with our data analysis. However, since we transformed our dependent variable data we will need to remember to undo the log transformation on our predicted values at the end of our project.","86e44cd6":"Using scatterplots we can take a closer look at the variables with the highest correlation to sale price. You can see that there is a significant upward trajectory in sale price once the overall quality reaches 6 and above. We can also see a clear trend as sale price increases with greater square footage in above ground living area, garage area, basement square footage, as well as 1st floor sqare footage. However, we can also see that there are a lot of outliers in our data. Next we will create a function to remove the outliers in our numerical data.","daa7f7ad":"### Prepare Data for Modeling\n\nNow that were have cleaned and analyzed our data, we can now prepare the data for modeling. First we have to create dummy variable for all of our categorical variables. Next, separate the test and training sets which we combined earlier for cleaner processing. Then separate sale price as its own series and remove it from the training set. You should end up with training \"X\", test \"X\", and training\/test \"Y\" datasets in order to start modeling.","6c591dbd":"### Lasso Regression","51068f43":"# Easy to Follow EDA and Machine Learning Using Python\n\n*Sam Park*\n\n*16 July 2021*","a0006a44":"Before jumping right into data cleaning and analysis, make a copy of your original data for reference later on. Also, take a look at how the data is organized and the overall shape of the datasets.","d35a1e57":"### Using XGBoost to Boost our Lasso Model","11b71b09":"### Prepare Predictions for Submission","350e9dc8":"### Removing Outliers\n\nRemoving outliers in our data will prevent unnecessary biases from arising for our numerical variables. First choose which numerical variables need to have outliers removed. I would recommend leaving out variables such as year built and number of rooms as these are essentially categorical variables with numeric values. Then write a function to convert all outliers to the maximum and minimum values based on interquartile range.","d8c1642e":"### Correlation Analysis\n\nOur next step is to find out which variables are correlated with sale price. Drop any variables that have little to no correlation with sale price and take note of the variables which have high correlation. This will give us an idea of which variables will be weighted heavily in our predictive algorithms.","10ef4a1a":"Pandas and numpy are the most important libraries to be familiar with and will be used for most data projects. If you are confused or have questions about any of these libraries, take a second to google each one in order to get a better understanding before reading the rest of the notebook.","a3aec4f5":"It appears that our dependent variable has a pretty severe right skew. We will attempt to adjust the distribution by performing a log transformation on the data in our selected column.","c9ff37d0":"# Import Libraries and Data","7d2786e7":"### Check for Normality of Dependent Variable\n\nIt is important that our dependent variable has a relatively normal distribution. This will allow us to identify accurate correlations with other variables and make better predictions using machine learning algorithms later on.","3d0d1ab9":"### Ridge Regression"}}