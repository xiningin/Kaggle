{"cell_type":{"0f42e3b8":"code","1f094153":"code","fe861d26":"code","15db8413":"code","befd0279":"code","398965b6":"code","fe7d35df":"code","9d7caf9f":"code","9cef257f":"code","7154dfae":"code","b20089c4":"code","5cb8144c":"code","9029c94c":"code","6254f0e3":"code","1287e7cf":"code","8c7e963b":"code","7c8fe814":"code","3ee4359d":"code","b613201b":"markdown","5347d788":"markdown","1a2828cc":"markdown","0b85c35b":"markdown","ee9a40af":"markdown","84e7150c":"markdown"},"source":{"0f42e3b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1f094153":"# Read and upload data\ndata = pd.read_csv(\"..\/input\/data.csv\")","fe861d26":"# We don't need id and NaN data.\ndata.drop([\"Unnamed: 32\", \"id\"], axis = 1, inplace = True)\ndata.head()","15db8413":"data[\"diagnosis\"].value_counts()\n\n# We have 357 B and 212 M labelled data","befd0279":"# For clustering we do not need labels. Because we'll identify the labels.\n\ndataWithoutLabels = data.drop([\"diagnosis\"], axis = 1)\ndataWithoutLabels.head()","398965b6":"dataWithoutLabels.info()","fe7d35df":"# radius_mean and texture_mean features will be used for clustering. Before clustering process let's check  how our data looks.\n\nsns.pairplot(data.loc[:,['radius_mean','texture_mean', 'diagnosis']], hue = \"diagnosis\", height = 5)\nplt.show()","9d7caf9f":"# Our data looks like below plot without diagnosis label\n\nplt.figure(figsize = (10, 10))\nplt.scatter(dataWithoutLabels[\"radius_mean\"], dataWithoutLabels[\"texture_mean\"])\nplt.xlabel('radius_mean')\nplt.ylabel('texture_mean')\nplt.show()","9cef257f":"from sklearn.cluster import KMeans\nwcss = [] # within cluster sum of squares\n\nfor k in range(1, 15):\n    kmeansForLoop = KMeans(n_clusters = k)\n    kmeansForLoop.fit(dataWithoutLabels)\n    wcss.append(kmeansForLoop.inertia_)\n\nplt.figure(figsize = (10, 10))\nplt.plot(range(1, 15), wcss)\nplt.xlabel(\"K value\")\nplt.ylabel(\"WCSS\")\nplt.show()","7154dfae":"# Elbow point starting from 2 \n\ndataWithoutLabels = data.loc[:,['radius_mean','texture_mean']]\nkmeans = KMeans(n_clusters = 2)\nclusters = kmeans.fit_predict(dataWithoutLabels)\ndataWithoutLabels[\"type\"] = clusters\ndataWithoutLabels[\"type\"].unique()","b20089c4":"# Plot data after k = 2 clustering\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutLabels[\"radius_mean\"][dataWithoutLabels[\"type\"] == 0], dataWithoutLabels[\"texture_mean\"][dataWithoutLabels[\"type\"] == 0], color = \"red\")\nplt.scatter(dataWithoutLabels[\"radius_mean\"][dataWithoutLabels[\"type\"] == 1], dataWithoutLabels[\"texture_mean\"][dataWithoutLabels[\"type\"] == 1], color = \"green\")\nplt.xlabel('radius_mean')\nplt.ylabel('texture_mean')\nplt.show()","5cb8144c":"# Data centroids middle of clustered scatters\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutLabels[\"radius_mean\"], dataWithoutLabels[\"texture_mean\"], c = clusters, alpha = 0.5)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color = \"red\", alpha = 1)\nplt.xlabel('radius_mean')\nplt.ylabel('texture_mean')\nplt.show()","9029c94c":"dataWithoutDiagnosis = data.drop([\"diagnosis\"], axis = 1)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar, kmeans)\npipe.fit(dataWithoutDiagnosis)\nlabels = pipe.predict(dataWithoutDiagnosis)\ndf = pd.DataFrame({'labels': labels, \"diagnosis\" : data['diagnosis']})\nct = pd.crosstab(df['labels'], df['diagnosis'])\nprint(ct)","6254f0e3":"dataWithoutTypes = dataWithoutLabels.drop([\"type\"], axis = 1)\ndataWithoutTypes.head()","1287e7cf":"from scipy.cluster.hierarchy import linkage,dendrogram\nmerg = linkage(dataWithoutTypes, method = \"ward\")\ndendrogram(merg, leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n","8c7e963b":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 2, affinity = \"euclidean\", linkage = \"ward\")\ncluster = hc.fit_predict(dataWithoutTypes)\ndataWithoutTypes[\"label\"] = cluster","7c8fe814":"dataWithoutTypes.label.value_counts()","3ee4359d":"# Data after hierarchical clustering\n\nplt.figure(figsize = (15, 10))\nplt.scatter(dataWithoutTypes[\"radius_mean\"][dataWithoutTypes.label == 0], dataWithoutTypes[\"texture_mean\"][dataWithoutTypes.label == 0], color = \"red\")\nplt.scatter(dataWithoutTypes[\"radius_mean\"][dataWithoutTypes.label == 1], dataWithoutTypes[\"texture_mean\"][dataWithoutTypes.label == 1], color = \"blue\")\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.show()","b613201b":"<a id=\"1\"><\/a> \n**DATA EXPLORATION**","5347d788":"# K-Means and Hierarchical Clustering Implementation\n\n* Clustering algorithms is being used for unlabelled datasets.\n* This is an implementation example of clustering algorithms.  We'll use K-Means an Hierarchical clustering algorithms for seperate the cancer data by \"radius_mean\" and \"texture_mean\"\n\n## Index of contents\n\n* [DATA EXPLORATION](#1)\n* [K-MEANS CLUSTERING](#2)\n* [HIERARCHICAL CLUSTERING](#3)","1a2828cc":"<a id=\"3\"><\/a> \n**HIERARCHICAL CLUSTERING**\n* Each data point is transformed to cluster,\n* Create cluster using closest 2 data point,\n* Create cluster using closest 2 cluster,\n* Repeat step 3.\n\n![dendogram.PNG](http:\/\/i66.tinypic.com\/97piew.png)\n\n* Above mentioned distances are euclidean distances\n* Dendogram is used for n_clusters value detection.","0b85c35b":"# Conclusion","ee9a40af":"<a id=\"2\"><\/a> \n**K-MEANS CLUSTERING**\n* Define **K** centers and cluster data,\n* Assign random centroids,\n* Cluster data points according to distance from centroids (euclidean distance),\n* Repeat step 3 until centroid positions start not to change.\n\n![KMeansClustering.png](http:\/\/i67.tinypic.com\/sdpueu.png)\n* WCSS is a metric used for k value selection process. After this operation elbow rule is used for k value.\n\n![KMeansElbow.png](http:\/\/i64.tinypic.com\/2dw7ztg.png)","84e7150c":"*We used unsupervised learning algorithms for clustering cancer data. Thank you for reading my kernel. Please do not hesitate to leave comments.*"}}