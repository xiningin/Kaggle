{"cell_type":{"d58baaf6":"code","b081aa9b":"code","d50d6ece":"code","c3c78f88":"code","5a42b604":"code","5fefd91f":"code","bffa9b63":"code","30c9e816":"code","cc589e5f":"code","f0049e68":"code","f8af748e":"code","e9dd4950":"code","ab9ff9f8":"code","8905a298":"code","fcd314fa":"code","c77c6feb":"code","35837fc0":"code","39485cc2":"code","60cd8db7":"code","1654f781":"code","aa47effc":"markdown"},"source":{"d58baaf6":"import warnings, sys\nwarnings.filterwarnings(\"ignore\")\n\n# Thanks to Chris's RAPIDS dataset, it only takes around 1 min to install offline\n!cp ..\/input\/rapids\/rapids.0.15.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","b081aa9b":"import cudf as gd\nimport cupy as cp\nfrom cuml.svm import SVC\nfrom cuml.preprocessing import LabelEncoder\nfrom cuml.linear_model import LogisticRegression\nfrom numba import cuda\nfrom cuml.metrics import log_loss, roc_auc_score\nfrom tqdm import tqdm","d50d6ece":"path = '..\/input\/lish-moa'","c3c78f88":"%%time\ntrain = gd.read_csv(f'{path}\/train_features.csv')\ntest = gd.read_csv(f'{path}\/test_features.csv')\nfea_cols = train.columns.values[1:]\nprint(train.shape, test.shape)\ntrain.head()","5a42b604":"%%time\n\nlbl = LabelEncoder()\ntrain['cp_type'] = lbl.fit_transform(train['cp_type'])\ntest['cp_type'] = lbl.transform(test['cp_type'])\ntrain['cp_type'].value_counts()\nprint('0 means control group')","5fefd91f":"%%time\n\nprint(train['cp_time'].value_counts())\nprint(test['cp_time'].value_counts())\ntrain['cp_time'] = train['cp_time']\/24 - 2\ntest['cp_time'] = test['cp_time']\/24 - 2","bffa9b63":"%%time\n\nlbl = LabelEncoder()\ntrain['cp_dose'] = lbl.fit_transform(train['cp_dose'])\ntest['cp_dose'] = lbl.transform(test['cp_dose'])\ntrain['cp_dose'].value_counts()","30c9e816":"%%time\n\n# confirm there is no missing values\n\nfor col in train.columns:\n    nasum = train[col].isnull().sum() + test[col].isnull().sum()\n    if nasum: print(col, nasum)       ","cc589e5f":"%%time\n\n# normalize \n\nfor col in train.columns[4:]:\n    mean, std = train[col].mean(), train[col].std()\n    train[col] = (train[col] - mean)\/std\n    test[col] = (test[col] - mean)\/std\ntrain.head()","f0049e68":"%%time\n\ntrain_targets = gd.read_csv(f'{path}\/train_targets_scored.csv')\nprint(train_targets.shape)\ntrain_targets.head()","f8af748e":"%%time\n\ntrain = train.merge(train_targets, on='sig_id', how='left')\nprint(train.shape)\ntrain.head()","e9dd4950":"%%time\n\n# confirm control group has all 0 targets\n\nmask = train.cp_type == 0\ntcols = train_targets.columns[1:].values\nycontrol = train.loc[mask, tcols].values\nycontrol.max()","ab9ff9f8":"%%time\nprint(train.shape)\ntrain = train.loc[train.cp_type>0]\ntrain.shape","8905a298":"%%time\n\nX = train[fea_cols].values\nXt = test[fea_cols].values","fcd314fa":"class StratifiedKFold_gpu:\n    \n    def __init__(self,n_splits=3,shuffle=True,random_state=42,tpb=32,mode='relax'):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.seed = random_state\n        self.tpb = tpb # threads per thread block\n        self.mode = mode\n        \n    def get_n_splits(self, X=None, y=None):\n        return self.n_splits\n              \n    def split(self,x,y):\n        \n        assert x.shape[0] == y.shape[0]\n        df = gd.DataFrame()\n        ids = cp.arange(x.shape[0])\n        \n        if self.shuffle:\n            cp.random.seed(self.seed)\n            cp.random.shuffle(ids)\n            x = x[ids]\n            y = y[ids]\n        \n        cols = []\n        df['y'] = y\n        df['ids'] = ids\n    \n        grpby = df.groupby(['y'])\n        if self.mode == 'sklearn':\n            dg = grpby.agg({'y':'count'})\n            #print(dg.columns)\n            col = dg.columns[0]\n            msg = 'n_splits=%d cannot be greater than the number of members in each class.'%self.n_splits\n            assert dg[col].min()>=self.n_splits,msg\n\n        def get_order_in_group(y,ids,order):\n            for i in range(cuda.threadIdx.x, len(y), cuda.blockDim.x):\n                order[i] = i\n\n        got = grpby.apply_grouped(get_order_in_group,incols=['y','ids'],\n                                  outcols={'order': 'int32'},\n                                  tpb=self.tpb)\n\n        got = got.sort_values('ids')\n        \n        for i in range(self.n_splits):\n            mask = got['order']%self.n_splits==i\n            train = got.loc[~mask,'ids'].values\n            test = got.loc[mask,'ids'].values\n            if len(test)==0:\n                break\n            yield train,test","c77c6feb":"%%time\n\ndef calibrate(y, mean, eps = 1e-4):\n    ymean = y.mean()\n    eps = min(eps, ymean)\n    y = y - ymean + mean\n    return cp.clip(y, eps, 1-eps)\n    \ndef cv(X, y, Xt, folds = 4):\n    skf = StratifiedKFold_gpu(n_splits=folds)#, random_state=None, shuffle=False)\n\n    scores = 0\n    ysub = 0\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        model = LogisticRegression(C=0.01)\n        model.fit(X_train, y_train)\n        yp = model.predict_proba(X_test)[:,1]\n        yp = cp.asarray(yp, order='C')\n        yp = calibrate(yp, y_train.mean())\n        \n        yps = model.predict_proba(Xt)[:,1]\n        yps = cp.asarray(yps, order='C')\n        yps = calibrate(yps, y_train.mean())\n        ysub += yps\n        \n        score = log_loss(y_test, yp)\n\n        scores += score\n    return scores\/folds, ysub\/folds","35837fc0":"%%time\n\nscores = []\nmean_scores = []\ntargets = train_targets.columns.values[1:]\n\nsub = test[['sig_id', 'cp_type']]\n\nfor ycol in tqdm(targets, total=len(targets)):\n    y = train[ycol].values\n    try:\n        score, ysub = cv(X, y, Xt, folds=8)\n    except:\n        ysub = cp.ones(Xt.shape[0])*y.mean()\n        score = log_loss(y, cp.ones_like(y)*y.mean())\n    mean_score = log_loss(y, cp.ones_like(y)*y.mean())\n    sub[ycol] = ysub #if score < mean_score else y.mean()\n\n    scores.append(score)\n    mean_scores.append(mean_score)\n\nscores = gd.DataFrame({'model_score': scores, 'target': targets, 'mean_score':mean_scores})\nscores['best'] = cp.minimum(scores['model_score'].values, scores['mean_score'].values)\n\nscores = scores.sort_values(by='model_score', ascending=False)\nprint(f\"best:{scores['best'].mean():.5f}, mean:{scores['mean_score'].mean():.5f}, model:{scores['model_score'].mean():.5f}\")\nscores.head()","39485cc2":"%%time\n\nmask = sub['cp_type'] == 0\ncontrol_sum = mask.sum()\nprint(sub.shape[0], control_sum)\n\nfor col in targets:\n    ys = sub[col].values.copy()\n    sub.loc[mask, col] = 0\n    assert (ys != sub[col].values).sum() == control_sum","60cd8db7":"sub = sub.drop('cp_type', axis=1)\nsub.head()","1654f781":"sub.to_csv('submission.csv', index=False)","aa47effc":"This notebook is inspired by [Yirun Zhang's great rapids svm kernel](https:\/\/www.kaggle.com\/gogo827jz\/rapids-svm-on-gpu-6000-models-in-1-hour).\n\nWe go a step further to use rapids libraries exclusively for the entire pipeline including data preprocessing, training and scoring. To make a point, we don't even import `numpy`, `pandas` and `sklearn`. We use simple `LogisticRegression` models and **in less than 10 mins** it gets a better score than previous best rapids demo. "}}