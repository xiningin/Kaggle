{"cell_type":{"337b7410":"code","9b1b350c":"code","77a8031e":"code","fba02588":"code","ef475a40":"code","5f96906d":"code","5e2c4853":"code","e6db3cff":"code","42c00d2a":"code","45382374":"code","34697c35":"code","2308587d":"code","f506345f":"code","8b222e17":"code","61f70902":"code","f53a56e6":"code","20326717":"code","1446d4d6":"code","bb0dd581":"code","5ee17b25":"code","2ce2de95":"code","a064ecf4":"code","3044dfd4":"code","811cdaca":"code","c2111879":"code","c095470f":"code","745dfd1e":"code","e8ad3a7c":"code","97cf2c1b":"code","9cbf2150":"code","b2aedc20":"code","aa853d42":"code","50c20500":"code","f591167d":"code","7d03b8ca":"code","96650d9d":"code","f09f2664":"code","24e184f1":"code","ca8a8ebc":"code","0a4dec31":"code","42797205":"markdown","215c4377":"markdown","48bb7cb6":"markdown","d306a341":"markdown","5ed13dd0":"markdown","e21e29a7":"markdown","b58bc34c":"markdown","8462618b":"markdown","1aa46328":"markdown","f7f422dc":"markdown","f637089e":"markdown","f0e1b0bc":"markdown","c8e6ef4b":"markdown","ee9efaa7":"markdown","40e619a7":"markdown","4fac5b4f":"markdown","b61225c3":"markdown","f7209316":"markdown","22a0be78":"markdown","b86296c0":"markdown","3ede0848":"markdown","95c47c6a":"markdown","1eb65320":"markdown","0571b38d":"markdown","aa254abd":"markdown","65b4c5e0":"markdown","63d307e3":"markdown","007c73a0":"markdown","61dbbcdf":"markdown"},"source":{"337b7410":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","9b1b350c":"driver_data= pd.read_csv(\"..\/input\/driver-dataset\/driver_data.csv\")","77a8031e":"driver_data.head(5)","fba02588":"driver_data.describe().T","ef475a40":"print(driver_data.shape)\nprint(driver_data.info())","5f96906d":"for k, v in driver_data.items():\n    q1 = v.quantile(0.25)\n    q3 = v.quantile(0.75)\n    irq = q3 - q1\n    v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n    perc = np.shape(v_col)[0] * 100.0 \/ np.shape(driver_data)[0]\n    print(\"Column %s outliers = %.2f%%\" % (k, perc))","5e2c4853":"plt.figure(figsize=(8,4))\ndriver_data.boxplot(patch_artist=True,vert=False)","e6db3cff":"plt.figure(figsize=(15,7))\nsns.scatterplot(x=\"Speeding_Feature\", y=\"Distance_Feature\", data=driver_data, markers='o', edgecolors='black', s=150)\nplt.ylim(0,)","42c00d2a":"sns.jointplot(x=\"Speeding_Feature\", y=\"Distance_Feature\", data=driver_data, kind=\"reg\");","45382374":"from sklearn.preprocessing import StandardScaler","34697c35":"std_scale = StandardScaler().fit(driver_data) ","2308587d":"driver_data_scaled = std_scale.transform(driver_data)","f506345f":"driver_data_scaled[:5,]","8b222e17":"from sklearn.cluster import KMeans \nimport sklearn.cluster as cluster # Importing K-Means","61f70902":"ssq = []\nfor K in range(1,11):\n    kmeans_model = KMeans(n_clusters=K, random_state=123)\n    kmeans_model.fit(driver_data_scaled)\n    ssq.append(kmeans_model.inertia_)","f53a56e6":"plt.figure(figsize=(15,7))\nplt.plot(range(1,11), ssq, marker='o')\nplt.xlabel(\"Number of clusters(k)\")\nplt.ylabel(\"Within-cluster SSQ(Inertia)\")\nplt.title(\"Scree Plot\")\nplt.plot([4]*3000, range(1,3001), \":\")\nplt.text(4.3, 3000, \"optimal number of clusters = 4\")\nplt.show()","20326717":"km = KMeans(n_clusters=4, random_state=123)\nkm.fit(driver_data_scaled)","1446d4d6":"predicted_cluster = km.predict(driver_data_scaled)","bb0dd581":"predicted_cluster[:15]","5ee17b25":"import sklearn.metrics as metrics\nlabels=cluster.KMeans(n_clusters=7,random_state=200).fit(driver_data_scaled).labels_","2ce2de95":"metrics.silhouette_score(driver_data_scaled,labels,metric=\"euclidean\",sample_size=4000,random_state=200)","a064ecf4":"for i in range(2,8):\n    labels=cluster.KMeans(n_clusters=i,random_state=123).fit(driver_data_scaled).labels_\n    print (\"Silhoutte score for k= \"+str(i)+\" is \"+str(metrics.silhouette_score(driver_data_scaled,labels,metric=\"euclidean\",random_state=200)))","3044dfd4":"f, ax = plt.subplots(figsize=(15,8))\nplt.scatter(driver_data_scaled[predicted_cluster==0,0], driver_data_scaled[predicted_cluster==0, 1], s=50, c='green',\\\n           marker='s', edgecolors='black', label='cluster 1')\nplt.scatter(driver_data_scaled[predicted_cluster==1,0], driver_data_scaled[predicted_cluster==1, 1], s=50, c='orange',\\\n           marker='o', edgecolors='black', label='cluster 2')\nplt.scatter(driver_data_scaled[predicted_cluster==2,0], driver_data_scaled[predicted_cluster==2, 1], s=50, c='blue',\\\n           marker='v', edgecolors='black', label='cluster 3')\nplt.scatter(driver_data_scaled[predicted_cluster==3,0], driver_data_scaled[predicted_cluster==3, 1], s=50, c='pink',\\\n           marker='v', edgecolors='black', label='cluster 4')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], s=250, c='red',\\\n           marker='*', edgecolors='black', label='centroids')\nplt.legend(scatterpoints=1)\nplt.xlabel(\"Distance_Feature\")\nplt.ylabel(\"Speeding_Feature\")\nplt.title(\"Clustering Output\")\nplt.show()","811cdaca":"print(km.cluster_centers_)","c2111879":"predictions_relabelled = np.where(predicted_cluster==0, \"Withincity & Slow\", np.where(predicted_cluster==1,\\\n    \"Outskirts & Slow\", np.where(predicted_cluster==2,\"Withincity & Fast\",\"Outskirts & Fast\")))\ndriver_data['category'] = pd.Series(predictions_relabelled, index=driver_data.index)\ndriver_data.index.name = \"Fleet Driver No.\"","c095470f":"pd.DataFrame(driver_data).head()","745dfd1e":"from sklearn.metrics import silhouette_score","e8ad3a7c":"silhouette_score(driver_data_scaled,predicted_cluster)","97cf2c1b":"#driver_data.to_csv(\"fleet_segementation_output.csv\", index=False)","9cbf2150":"from scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\n%matplotlib inline","b2aedc20":"from scipy.cluster.hierarchy import set_link_color_palette\nset_link_color_palette(['black'])","aa853d42":"link = linkage(driver_data_scaled, method='ward', metric='euclidean')","50c20500":"plt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.ylabel(\"Euclidean distance\")\ndendrogram(\n    link,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()","f591167d":"from sklearn.cluster import AgglomerativeClustering","7d03b8ca":"model = AgglomerativeClustering(n_clusters=4)","96650d9d":"model.fit(driver_data_scaled)","f09f2664":"model.labels_","24e184f1":"silhouette_score(driver_data_scaled, model.labels_)","ca8a8ebc":"import seaborn as sns","0a4dec31":"plt.figure(figsize=(15,8))\nsns.scatterplot(x='Distance_Feature', y='Speeding_Feature', data=driver_data, hue=model.labels_, s=100)\nplt.show()","42797205":"#### Objective 3: Compare the results from your K-Means model and your Hierarchical clustering model using silhouette scores. Which model is superior?","215c4377":"#### The above shown plot describes Hierarchical Clustering Dendrograms> This is a Bottom-Up Approach and we select the number of clusters by looking at Height of each clades. The more the height of clades(Vertical) we select that many clusters","48bb7cb6":"* In the below code snippet we use method as Ward's.\n* **Ward's minimum variance criterion** minimizes the total within-cluster variance. To implement this method, at each step we find the pair of clusters that leads to minimum increase in total within-cluster variance after merging. This increase is a **weighted squared distance** between cluster centers. At the initial step, all clusters are singletons (clusters containing a single point). To apply a recursive algorithm under this objective function, the initial distance between individual objects must be (proportional to) squared Euclidean distance and so Metric used is **Euclidean**","d306a341":"Comparing results of KMeans and Hierarchical Clustering by silhouette scores we can see, The Scores of K-means is much better than Hierarchical Clustering. So for this dataset KMeans seems superior model as its scores are higher than Hierarchical.","5ed13dd0":"#### The above Shown Outlier Percentage in data are not actually an outliers. Because 20% of values in data we will not consider it as outliers.\n#### Below is the Box Plot of the features","e21e29a7":"#### Feature Scaling the Data by Using StandardScaler such that  Mean = 0  &  Standard Deviation = 1 ","b58bc34c":"### Data Preprocessing","8462618b":"#### Scatter Plot between  Speeding_Feature &  Distance Feature. On abscissa Speeding_Feature is plotted and on ordinate Distance_Feature is plotted to show the datapoint distribution and also to get rough idea what could be the No. of Clusters (Manual Vizualization)","1aa46328":" Importing Dendrogram and Linkages from Cluster.hierarchy from Scipy ","f7f422dc":"#### Running multiple iteration of KMeans algorithm to find an optimal number of clusters using elbow method and also using Silhoutte Score.","f637089e":"### Dataset of delivery fleet driver data. For each driver we have two features: mean distance driven per day and the mean percentage of time a driver was >5 mph over the speed limit.\n\nAbout Dataset: This dataset has two columns Distance_Feature Columns and Speeding_Feature. Now as per my understanding The distance column says the mean distance driven by driver per day & Speeding feature says the mean percentage of time the driver was 5MPH more than its speed limit.\n\nLets Understand by an example:\n* Suppose Speed Limit is : 40km\/hr\n* Gr.than Speed Limit 5 MPH = 8km\/hr\n* Overspeed: 48km\/hr\n* Total Driving Hours: 8hrs.\n* If a data point is given that Speeding Feature=28% then ((28 * 8)\/100)=2.24 hrs.\n* So, for 2.24 hr out of 8 hrs driving. The person drives vehicle over The speed limit > 5MPH. (i.e 48km\/hr)","f0e1b0bc":"#### From the above Silhoutte Score we can see that Selecting K=4 using elbow method is correct decision. So K=4 gives better score. Let's Plot the Output ","c8e6ef4b":"#### Importing Silhouette Score (For K-Means)","ee9efaa7":"### Objective: Run the K-Means algorithm on this dataset and find the optimal number of clusters. What insights can you gather from these clusters also Compare the results from your K-Means model and your Hierarchical clustering model using silhouette scores.","40e619a7":"**Dataset of delivery fleet driver data. For each driver we have two features: mean distance driven per day and the mean percentage of time a driver was >5 mph over the speed limit.**\n\n***1. Run the K-Means algorithm on this dataset and find the optimal number of clusters. What insights can you gather from these clusters?***\n\n***2. Perform Agglomerative clustering on the same dataset with an appropriate number of clusters. Plot the dendrogram.***\n\n***3. Compare the results from your K-Means model and your Hierarchical clustering model using silhouette scores. Which model is superior?***\n\n***4. Document your work and explain your reasoning***","4fac5b4f":"## Hierarchical Clustering using Scikit-Learn","b61225c3":"### Dataset of delivery fleet driver data. For each driver we have two features: mean distance driven per day and the mean percentage of time a driver was >5 mph over the speed limit.\n\n### Objective: Perform Agglomerative clustering on the fleet driver data dataset with an appropriate number of clusters. Plot the dendrogram and Compare the results from your K-Means model and your Hierarchical clustering model using silhouette scores.","f7209316":"## K-Means Clustering using Scikit-Learn","22a0be78":"From the result we see that Four should be the optimal number of clusters for the given data as After 4th Cluster the change in number of Clusters won't give that effective result. \n\n* So Let's consider No. of Clusters = 4","b86296c0":"#### The Silhouette_score for Agglomerative Clustering ","3ede0848":"#### The above silhouette Score represents that 78.4% of the data within its own cluster (cohesion) are similar as compared to other clusters (separation).\n\nBelow is the code for saving segmented data to CSV","95c47c6a":"### Sumary Statistics about Data","1eb65320":"* Silhouette Score of K-Means: **0.7843**\n* Silhouette Score of Hierarchical Clustering: **0.7788**","0571b38d":"#### Checking and Determining the No. of Clusters taken by using Silhoutte Score","aa254abd":"*Make predictions for the clusters of the given dataset*","65b4c5e0":"### K-Means Clustering","63d307e3":"### EDA ","007c73a0":"From the above Clusterring Output we can see Four Clusters these are **Centre Based Clusters** the centre with red star represents **Centroid**. The intra Cluster distance in Cluster No. 1,2,3 are Low which is Good for analytics point, decision making and Making Inferences. But In Cluster 4 we can see that Intra cluster distance is very High which affects the model as datapoints are widely spread. \n\nThe intercluster distance between 1-2 & 1-4 is also high so its a good Sign but Intercluster distance between Cluster-1 7 Cluster 3 is very very low.","61dbbcdf":"#### The Below shown Jointplot is a Plot between Speeding_Feature and Distace_Feature From the plot we can see that datapoints in Both the features are Right Skewed and regression plot shows a Positive Slope i.e. Speeding_Feature(Time > 5 MPH Overspeeding) increases the distance increases which is obvious."}}