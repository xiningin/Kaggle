{"cell_type":{"a1ed07ab":"code","886c90e8":"code","9861086d":"code","0b00555e":"code","7ce199b5":"code","118fe063":"code","3ece5864":"markdown","0ff795c8":"markdown","d9d130b3":"markdown","1f0fff5a":"markdown","baf33af4":"markdown","faf31700":"markdown","36f5125e":"markdown"},"source":{"a1ed07ab":"!pip install -q efficientnet\n!pip install tensorflow_addons\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets","886c90e8":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","9861086d":"ROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0\n\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\ndef transform(image, DIM=256):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM\/\/2, -DIM\/\/2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM\/\/2, DIM\/\/2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM\/\/2+XDIM+1, DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM, DIM,3])","0b00555e":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_PATH = KaggleDatasets().get_gcs_path('shopee-tf-records-512-stratified')\n\n# Configuration\nEPOCHS = 15\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Learning rate\nLR = 0.001\n# Verbosity\nVERBOSE = 2\n# Number of classes\nN_CLASSES = 11014\n# Number of folds\nFOLDS = 5\n\n# Training filenames directory\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/*.tfrec')","7ce199b5":"# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \ndef arcface_format(posting_id, image, label_group, matches):\n    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n\n# Data augmentation function\ndef data_augment(posting_id, image, label_group, matches):\n    dim = image.shape[0]\n    if tf.random.uniform([1])[0] < 0.4:\n        image = tf.image.central_crop(image, 0.9)\n        image = tf.image.resize_with_pad(image, dim, dim)\n    if tf.random.uniform([1])[0] <= 0.7:\n        image = tf.image.random_crop(image,[int(dim*0.8), int(dim*0.8), 3])\n        image = tf.image.resize_with_pad(image, dim, dim)\n    if tf.random.uniform([1])[0] < 0.5:\n        image = transform(image, DIM=dim)\n    if tf.random.uniform([1])[0] < 0.5:\n        image = tf.image.random_flip_left_right(image)\n#     if tf.random.uniform([1])[0] < 0.5:\n#         image = tf.image.random_flip_up_down(image)\n    if tf.random.uniform([1])[0] < 0.5:\n        image = tf.image.random_hue(image, 0.01)\n    if tf.random.uniform([1])[0] < 0.5:\n        image = tf.image.random_saturation(image, 0.7, 1.3)\n    if tf.random.uniform([1])[0] < 0.5:\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n    if tf.random.uniform([1])[0] < 0.5:\n        image = tf.image.random_brightness(image, 0.1)\n    return posting_id, image, label_group, matches\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image\n\n# This function parse our images and also get the target variable\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n        \"matches\": tf.io.FixedLenFeature([], tf.string)\n    }\n\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    posting_id = example['posting_id']\n    image = decode_image(example['image'])\n#     label_group = tf.one_hot(tf.cast(example['label_group'], tf.int32), depth = N_CLASSES)\n    label_group = tf.cast(example['label_group'], tf.int32)\n    matches = example['matches']\n    return posting_id, image, label_group, matches\n\n# This function loads TF Records and parse them into tensors\ndef load_dataset(filenames, ordered = False):\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False \n        \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n    return dataset\n\n# This function is to get our training tensors\ndef get_training_dataset(filenames, ordered = False):\n    dataset = load_dataset(filenames, ordered = ordered)\n    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# This function is to get our validation tensors\ndef get_validation_dataset(filenames, ordered = True):\n    dataset = load_dataset(filenames, ordered = ordered)\n    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) \n    return dataset\n\n# Function to count how many photos we have in\ndef count_data_items(filenames):\n    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nprint(f'Dataset: {NUM_TRAINING_IMAGES} training images')","118fe063":"# Function for a custom learning rate scheduler with warmup and decay\ndef get_lr_callback():\n    lr_start   = 0.000001\n    lr_max     = 0.000005 * BATCH_SIZE\n    lr_min     = 0.000001#0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start   \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max    \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n    return lr_callback\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n# from tensorflow.keras.applications.xception import Xception\n# from tensorflow.keras.applications.inception_v3 import InceptionV3\n# from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n# from tensorflow.keras.applications.vgg16 import VGG16\n# from tensorflow.keras.applications.vgg19 import VGG19\n# from tensorflow.keras.applications.resnet import ResNet50, ResNet101, ResNet152\n# from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\n\ndef get_model():\n\n    with strategy.scope():\n\n        margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5,\n#             ls_eps = 0.05,\n            name='head\/arc_margin', \n            dtype='float32'\n            )\n\n        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n        x = efn.EfficientNetB3(weights = 'imagenet', include_top = False)(inp)\n#         x = ResNet101(weights = 'imagenet', include_top = False)(inp)\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x = margin([x, label])\n        \n        output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n\n        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n\n        model.compile(\n            optimizer = opt,\n            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n            ) \n        \n        return model\n\ndef train_and_evaluate():\n\n    # Seed everything\n    seed_everything(SEED)\n    \n    print('\\n')\n    print('-'*50)\n    train, valid = train_test_split(TRAINING_FILENAMES, shuffle = True, random_state = SEED, train_size = 0.35)\n    train_dataset = get_training_dataset(train, ordered = False)\n    train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n    val_dataset = get_validation_dataset(valid, ordered = True)\n    val_dataset = val_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n    STEPS_PER_EPOCH = count_data_items(train) \/\/ BATCH_SIZE\n    K.clear_session()\n    model = get_model()\n    # Model checkpoint\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'model_{IMAGE_SIZE[0]}_{SEED}.h5', \n                                                    monitor = 'val_loss', \n                                                    verbose = VERBOSE, \n                                                    save_best_only = True,\n                                                    save_weights_only = True, \n                                                    mode = 'min')\n\n    history = model.fit(train_dataset,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        epochs = EPOCHS,\n                        callbacks = [checkpoint, get_lr_callback()], \n                        validation_data = val_dataset,\n                        verbose = VERBOSE)\n\n    \ntrain_and_evaluate()","3ece5864":"Evaluated with : (https:\/\/www.kaggle.com\/chankhavu\/keras-layers-arcface-cosface-adacos)\n- 3 augs\n- EffNet B3\n- 1\/3 train data\n- 15 epochs\n\n| Weight  | Val loss |\n|---------|----------|\n| ArcFace |   14.20  |\n| CosFace |          |\n| AdaCos  |          |","0ff795c8":"Evaluated with :\n- 3 augs\n- EffNet B3\n- 1\/3 train data\n- 15 epochs\n\n| s  | m   | Val loss |\n|----|-----|----------|\n| 30 | 0.2 |    8.46  |\n| 30 | 0.3 |   10.40  |\n| 30 | 0.4 |   12.32  |\n| 30 | 0.5 |   14.20  |\n| 30 | 0.6 |   16.12  |\n\n| s  | m   | Val loss |\n|----|-----|----------|\n| 20 | 0.5 |   10.84  |\n| 25 | 0.5 |   12.49  |\n| 30 | 0.5 |   14.20  |\n| 35 | 0.5 |   15.99  |\n\n| Margin      | Val loss |\n|-------------|----------|\n| default     |   14.20  |\n| easy_margin |   10.57  |\n\n| ls_eps  | Val loss |\n|---------|----------|\n| 0.0     |   14.20  |\n| 0.001   |   14.23  |\n| 0.01    |   14.14  |\n| 0.05    |   13.72  |\n| 0.1     |   13.26  |","d9d130b3":"Evaluated with :\n- EffNet B3\n- img_size 512\n- 1\/3 train data\n- 15 epochs\n\n|                   Augmentations                  | Val loss |\n|:------------------------------------------------:|:--------:|\n| baseline augs                                    | 14.50    |\n| no augs                                          | 14.45    |\n| central_crop(image, 0.9)                         | 14.32    |\n| random_crop(image,[int(dim0.8), int(dim0.8), 3]) | 14.23    |\n| transform(image, DIM=dim)                        | 14.52    |\n| random_flip_left_right(image)                    | 14.41    |\n| random_flip_up_down(image)                       | 14.52    |\n| random_hue(image, 0.01)                          | 14.42    |\n| random_saturation(image, 0.7, 1.3)               | 14.39    |\n| random_contrast(image, 0.8, 1.2)                 | 14.41    |\n| random_brightness(image, 0.1)                    | 14.42    |\n| central_crop+random_crop+flip_left_right         | 14.20    |","1f0fff5a":"Evaluated with :\n- no augmentations\n- img_size 512\n- 1\/3 train data\n- 15 epochs\n\n| EffNet w\/o aug | Val loss |\n|----------------|----------|\n| B0             |  14.70   |\n| B1             |  14.58   |\n| B2             |  14.47   |\n| B3             |  14.45   |\n| B4             |  14.43   |\n| B5             |  14.40   |\n| B6             |  14.37   |\n| B7             |  14.24   |","baf33af4":"Evaluated with :\n- 3 augs\n- EffNet B3\n- 1\/3 train data\n- 15 epochs\n\n| Weight        | Val loss |\n|---------------|----------|\n| imagenet      |   14.20  |\n| noisy-student |   14.25  |","faf31700":"Evaluated with :\n- no augmentations\n- EffNet B3\n- 1\/3 train data\n- 15 epochs\n\n| Image size | Val loss |\n|------------|----------|\n| 256        |  14.95   |\n| 384        |  14.55   |\n| 512        |  14.45   |","36f5125e":"Evaluated with :\n- 3 augs\n- 1\/3 train data\n- 15 epochs\n\n| Model       | Val loss |\n|-------------|----------|\n| EffNet B3   |   14.20  |\n| Xception    |   15.94  |\n| InceptionV3 |   16.70  |\n| MobileNetV2 |   22.29  |\n| VGG16       |   15.10  |\n| VGG19       |   24.26  |\n| ResNet50    |   23.37  |\n| ResNet101   |   22.85  |\n| DenseNet121 |   17.07  |\n| DenseNet169 |   16.59  |\n| DenseNet201 |   16.30  |"}}