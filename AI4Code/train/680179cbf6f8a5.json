{"cell_type":{"2570b4e8":"code","af0fe3bc":"code","a71ad10a":"code","4caa82f1":"code","75e50cde":"code","6c411d72":"code","293c3dc3":"code","7b0bef1c":"code","43975019":"code","5f0028aa":"code","9afd4854":"code","4b82a0ac":"code","c41a453c":"code","8d301248":"code","502d695e":"code","5b5132f7":"code","27d84875":"code","5028969e":"code","d75915ac":"code","e7bbda1e":"code","29477855":"code","f86cf99e":"code","329115e4":"code","a53a43d0":"code","c8d7bf23":"code","e8006d4c":"code","b4b29d28":"code","359ff5ec":"code","1873d314":"code","53917164":"code","79bda1af":"code","18ca0fd9":"code","f8765901":"code","bcebaab4":"code","7d5c68f5":"code","6b7a5a77":"code","0ecef64c":"code","756bc3de":"code","10e884ea":"code","5b63f6c5":"code","9c5b860a":"code","c80ab55e":"code","ab6fd9df":"code","8858e0e8":"code","64bf559a":"code","a79d1e02":"code","f254b64c":"code","66d16c8c":"code","03ab0cbd":"code","92214b95":"code","eb91800c":"code","668f0c02":"code","28250cc5":"code","2b8451e1":"code","df5db10b":"code","faa1a928":"code","6f2060c8":"code","44ca337d":"code","004a8b07":"code","c47f8a87":"code","10e0281e":"code","49605eeb":"code","d9ae7898":"code","4b3f46de":"code","9240c39a":"code","996a8d1a":"code","9ed2be7a":"markdown","00565028":"markdown","c15ac6a7":"markdown","8b15c972":"markdown","984f5c01":"markdown","20bb51dd":"markdown","d7142d67":"markdown","1ec541ab":"markdown","27eadb99":"markdown","1a76b3b3":"markdown","f8281309":"markdown","3030d6e9":"markdown","5a28cdd1":"markdown","79c0c6e4":"markdown","c7f6dbf8":"markdown","66c13963":"markdown","5d9e402e":"markdown","ec36d3bd":"markdown","14fe17d9":"markdown","e57e14b2":"markdown"},"source":{"2570b4e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af0fe3bc":"#!pip install glob2","a71ad10a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nfrom sklearn.feature_selection import chi2;\nfrom scipy import stats\nimport datetime as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom geopy.geocoders import Nominatim\nimport folium\nfrom folium.plugins import HeatMap\nfrom folium.plugins import FastMarkerCluster\nfrom plotly import tools\nimport re\nfrom plotly.offline import init_notebook_mode, plot, iplot\nfrom wordcloud import WordCloud, STOPWORDS \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport missingno as msno\nimport glob","4caa82f1":"\n\n\npath = r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' # use your path\nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split('\/')[5].split('.')[0]\n    df['district_id'] = district_id\n    li.append(df)\n\ndf_eng = pd.concat(li, axis=0, ignore_index=True)","75e50cde":"#df_eng.to_csv('df_enga.csv')\ndf_eng","6c411d72":"df_eng.dtypes","293c3dc3":"os.chdir(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\")\ndf_dist = pd.read_csv(\"districts_info.csv\")","7b0bef1c":"df_dist","43975019":"df_dist.dtypes","5f0028aa":"df_prod = pd.read_csv(\"products_info.csv\")\ndf_prod","9afd4854":"df_prod.dtypes","4b82a0ac":"# how many missing values exist or better still what is the % of missing values in the dataset?\ndef percent_missing(df):\n\n    # Calculate total number of cells in dataframe\n    totalCells = np.product(df.shape)\n\n    # Count number of missing values per column\n    missingCount = df.isnull().sum()\n\n    # Calculate total number of missing values\n    totalMissing = missingCount.sum()\n\n    # Calculate percentage of missing values\n    print(\"The Data  contains\", round(((totalMissing\/totalCells) * 100), 2), \"%\", \"missing values.\")\n\n","c41a453c":"percent_missing(df_eng)","8d301248":"percent_missing(df_prod)","502d695e":"percent_missing(df_dist)","5b5132f7":"# Function to calculate missing values by column\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n\n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n\n    # dtype of missing values\n    mis_val_dtype = df.dtypes\n\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)\n\n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})\n\n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n\n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n\n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns\n","27d84875":"missing_values_table(df_eng)","5028969e":"missing_values_table(df_dist)","d75915ac":"missing_values_table(df_prod)","e7bbda1e":"#fix the missing values\ndef fix_missing_ffill(df, col):\n    df[col] = df[col].fillna(method='ffill')\n    return df[col]\n\ndef fix_missing_bfill(df, col):\n    df[col] = df[col].fillna(method='bfill')\n    return df[col]\n","29477855":"clean_eng = df_eng.copy()\n\nclean_eng['engagement_index'] = clean_eng['engagement_index'].fillna(clean_eng['engagement_index'].mean())\nclean_eng['pct_access'] = clean_eng['pct_access'].fillna(clean_eng['pct_access'].mean())\nclean_eng['lp_id'] = fix_missing_ffill(clean_eng, 'lp_id')\n\n\n","f86cf99e":"missing_values_table(clean_eng)","329115e4":"clean_dist = df_dist.copy()\n#remove columns with 30% of missing values\nclean_dist = clean_dist.drop(['pp_total_raw','pct_free\/reduced','county_connections_ratio'],axis=1)\nclean_dist['state'] = fix_missing_ffill(clean_dist, 'state')\n\nclean_dist['locale'] = fix_missing_bfill(clean_dist, 'locale')\nclean_dist['pct_black\/hispanic'] = fix_missing_bfill(clean_dist, 'pct_black\/hispanic')\n\n","a53a43d0":"missing_values_table(clean_dist)","c8d7bf23":"clean_prod = df_prod.copy()\nclean_prod['Sector(s)'] = fix_missing_ffill(clean_prod, 'Sector(s)')\n\nclean_prod['Primary Essential Function'] = fix_missing_ffill(clean_prod, 'Primary Essential Function')\nclean_prod['Provider\/Company Name'] = fix_missing_ffill(clean_prod, 'Provider\/Company Name')","e8006d4c":"missing_values_table(clean_prod)","b4b29d28":"clean_eng['district_id'] =clean_eng['district_id'].astype(int)\n\nclean_eng.dtypes","359ff5ec":"df_merge1 = pd.merge(clean_eng, clean_dist, on=\"district_id\")","1873d314":"df_merge1","53917164":"missing_values_table(df_merge1)","79bda1af":"#Change the name of the column\nclean_prod = clean_prod.rename(columns={'LP ID': 'lp_id'})\nclean_prod['lp_id'] =clean_prod['lp_id'].astype('float')\nclean_prod.head(3)","18ca0fd9":"df_merge2 = pd.merge( clean_prod,df_merge1, on=\"lp_id\")\n","f8765901":"missing_values_table(df_merge2)","bcebaab4":"df_merge2","7d5c68f5":"df_merge2.dtypes","6b7a5a77":"df_merge2.describe()","0ecef64c":"mode1= df_merge2.mode()\nprint(mode1)","756bc3de":"def plot_count(df:pd.DataFrame, column:str) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.countplot(data=df, x=column)\n    plt.title(f'Distribution of {column}', size=20, fontweight='bold')\n    plt.show()","10e884ea":"plot_count(df_merge2, \"Sector(s)\")","5b63f6c5":"df_merge2.columns","9c5b860a":"plot_count(df_merge2, \"pct_black\/hispanic\")","c80ab55e":"#splitting date column into day_name,month,weekdef features_create(data):\ndef features_create(data): \n    data['year']=data['time'].dt.year\n    data['month']=data['time'].dt.month\n    data['day_name']=data['time'].dt.day_name()\n    return data\n","ab6fd9df":"df_merge2['time'] = pd.to_datetime(df_merge2['time'])\n","8858e0e8":"features_create(df_merge2)","64bf559a":"plot_count(df_merge2, \"day_name\")","a79d1e02":"plot_count(df_merge2, \"month\")","f254b64c":"plt.figure(figsize=(14,10))\nfor i, column in enumerate(df_merge2[['Sector(s)']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index);","66d16c8c":"plt.figure(figsize=(14,10))\nfor i, column in enumerate(df_merge2[['pct_black\/hispanic']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","03ab0cbd":"labels = list(df_merge2.state.value_counts().index)\nvalues = df_merge2['state'].value_counts()\n# colors = ['mediumslateblue', 'darkorange']\nfig = go.Figure(data=[go.Pie(labels=labels,\n                             values=values,hole=.3)])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,\n                  marker=dict( line=dict(color='#000000', width=3)))\nfig.update_layout(title=\"State Distribution \",\n                  titlefont={'size': 30},      \n                  )\nfig.show()","92214b95":"plt.figure(figsize=(10,70))\nfor i, column in enumerate(df_merge2[['Provider\/Company Name']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","eb91800c":"plt.figure(figsize=(10,25))\nfor i, column in enumerate(df_merge2[['Primary Essential Function']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","668f0c02":"def bar_p(df:pd.DataFrame, column:str,title:str):\n    plt.figure(figsize=(15, 9))\n    sns.countplot(y=column, data=df_merge2, order=df[column].value_counts().head(10).index,color = \"red\")\n    plt.title(title,font=\"Serif\", size=15)\n    plt.show()","28250cc5":"bar_p(df_merge2,'Product Name','Distribution of the best Product Name')","2b8451e1":"plt.figure(figsize=(10,8))\nfor i, column in enumerate(df_merge2[['locale']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","df5db10b":"def plot_hist(df:pd.DataFrame, column:str, color:str)->None:\n    plt.figure(figsize=(12, 7))\n    sns.displot(data=df, x=column, color=color, bins = 100, kde=True, height=7, aspect=2)\n    plt.title(f'Distribution of {column}', size=20, fontweight='bold')\n    plt.show()\ndef plot_correlation(df:pd.DataFrame, title:str) -> None:\n    f = plt.figure(figsize=(19, 15))\n    plt.matshow(df.corr(), fignum=f.number)\n    plt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\n    plt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title('Correlation Matrix', fontsize=16)","faa1a928":"plot_hist(df_merge2[df_merge2['pct_access'] <= df_merge2['pct_access'].quantile(0.95)], 'pct_access', 'blue')","6f2060c8":"plot_hist(df_merge2[df_merge2['engagement_index'] <= df_merge2['engagement_index'].quantile(0.95)], 'engagement_index', 'blue')","44ca337d":"df_merge2.columns","004a8b07":"data1= df_merge2[['state','pct_access','engagement_index']]\ndata2 = data1.groupby(['state']).mean()\ndata2.sort_values(\"engagement_index\", ascending=False).head(6)\n#data1.head(6)","c47f8a87":"most_engaged = data2.sort_values('engagement_index', ascending = False).head(10).reset_index()\nplt.figure(figsize=(15,9))\nplt.title(\"Distribution of percentage of page loads per district\")\nsns.barplot(x = most_engaged['state'], y = most_engaged['engagement_index'])\nplt.xticks(rotation=60)","10e0281e":"data2.sort_values(\"pct_access\", ascending=False).head(6)","49605eeb":"data1= df_merge2[['locale','pct_access','engagement_index']]\ndata2 = data1.groupby(['locale']).mean()\ndata2.sort_values(\"engagement_index\", ascending=False).head(6)","d9ae7898":"data1= df_merge2[['Provider\/Company Name','pct_access','engagement_index']]\ndata2 = data1.groupby(['Provider\/Company Name']).mean()\ndata2.sort_values(\"engagement_index\", ascending=False).head(6)","4b3f46de":"data1= df_merge2[['Product Name','pct_access','engagement_index']]\ndata2 = data1.groupby(['Product Name']).mean()\ndata2.sort_values(\"engagement_index\", ascending=False).head(6)","9240c39a":"state_available = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\ndf_merge2['state_available'] = df_merge2['state'].map(state_available)","996a8d1a":"fig = go.Figure()\nlayout = dict(\n    title_text = \"Districts in the available States\",\n    title_font = dict(\n            family = \"monospace\",\n            size = 30,\n            color = \"black\"\n            ),\n    geo_scope = 'usa'\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations = df_merge2['state_available'].value_counts().to_frame().reset_index()['index'],\n        zmax = 1,\n        z = df_merge2['state_available'].value_counts().to_frame().reset_index()['state_available'],\n        locationmode = 'USA-states',\n        marker_line_color = 'white',\n        geo = 'geo',\n        colorscale = \"cividis\", \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","9ed2be7a":"**We can notice with the table above , the first  state which has the  greatest mean of Percentage of students in the district which at least one page-load event  was North Dakota followed by   Arizona  during the period of Covid  in 2020.**","00565028":"## **Bivariate analysis**","c15ac6a7":"**People are more engaged to use  product coming from Instructure ,Inc. than Google and others Provider**","8b15c972":"**With plot above Google docs has high engagement, and also the 3 first place wre taken by Google services**","984f5c01":"## Clean data","20bb51dd":"**The plot above showed  where the schools districts are located in the state. The 3 states Connecticus,Utah,Massachussettes are the high engaged states** ","d7142d67":"### **Mapping**","1ec541ab":"##                                  <B> DIGITAL LEARNING <B>\n    \n<b>Problem Statement<b>\n    \nThe COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow.\n    \n<b>Business Need<b>\n    \nWhat is the state of digital learning in 2020? And how does the engagement of digital learning relate to factors such as district demographics, broadband access, and state\/national level policies and events?\n\n<b>Basic information<b>\n    \n    \n<b>Engagement data<b>\n    \nThe engagement data are aggregated at school district level, and each file in the folder engagement_data represents data from one school district. The 4-digit file name represents district_id which can be used to link to district information in district_info.csv. The lp_id can be used to link to product information in product_info.csv.\n\n<b>Name :<\/b>Description\n    \n<b>time :<\/b>date in \"YYYY-MM-DD\"\n    \n<b>lp_id:<\/b>The unique identifier of the product\n    \n<b>pct_access:<\/b>Percentage of students in the district have at least one page-load event of a given product and on a given day\n    \n<b>engagement_index:<\/b>Total page-load events per one thousand students of a given product and on a given day\n    \n    \n <b>District information data<b>\n    \nThe district file districts_info.csv includes information about the characteristics of school districts, including data from NCES (2018-19), FCC (Dec 2018), and Edunomics Lab. In this data set, we removed the identifiable information about the school districts. We also used an open source tool ARX (Prasser et al. 2020) to transform several data fields and reduce the risks of re-identification. For data generalization purposes some data points are released with a range where the actual value falls under. Additionally, there are many missing data marked as 'NaN' indicating that the data was suppressed to maximize anonymization of the dataset.\n\n<b>Name<\/b>:Description\n    \n<b>district_id:<\/b>The unique identifier of the school district\n    \n<b>state:<\/b>The state where the district resides in\n    \n<b>locale:<\/b>NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See Locale Boundaries User's Manual for more information.\n    \n<b>pct_black\/hispanic:<\/b>Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data\n    \n<b>pct_free\/reduced:<\/b>Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data\n    \n<b>countyconnectionsratio:<\/b>ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version). See FCC data for more information.\n    \n<b>pptotalraw:<\/b>Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district.\n    \n<b>Product information data<\/b>\n\nThe product file products_info.csv includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy. Data were labeled by our team. Some products may not have labels due to being duplicate, lack of accurate url or other reasons.\n\n<b>Name:<\/b>Description\n\n<b>LP ID:<\/b>The unique identifier of the product\n\n<b>URL:<\/b>Web Link to the specific product\n\n<b>Product Name:<\/b>Name of the specific product\n\n<b>Provider\/Company Name:<\/b>Name of the product provider\n\n<b>Sector(s):<\/b>Sector of education where the product is used\n\n<b>Primary Essential Function:<\/b>The basic function of the product. There are two layers of labels here. \nProducts are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled\n    \n <b>Objectives<\/b>\n \nUncover trends in digital learning\n\nVisualize the trends of digital connectivity and engagement in 2020\n\nUnderstand and measure the scope and impact of the pandemic on digital learning\n\nHow does student engagement with different types of education technology change over the course of the pandemic?\n\nHow does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race\/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?\nDo certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?","27eadb99":"**Here the first step of our analysis was to compute a mean of engagement and pct access per state in order to know how they were distributed and also which state  had  a greatest mean  of those variable.** ","1a76b3b3":"\n**We can notice with the table above , the first  state which has the  greatest mean engagement was Arizona followed by North Dakota  during the period of Covid  in 2020.**","f8281309":"## Data exploration","3030d6e9":"**With the plot above , Google LLC was the the provider company that offered services significantly**","5a28cdd1":"**Most of the people  who are leaning are more engaged on the Rural locale** ","79c0c6e4":"### Merge data","c7f6dbf8":"# Now which column(s) has missing values\n","66c13963":"## Data prepocessing","5d9e402e":"**From the graph above we see LC DIgital learning platform was most engaged in this case**","ec36d3bd":"### **Univariate analysis**","14fe17d9":"**People are more engaged to use Service provided by Google . Google doc are the head of the classement   due to engagement .**","e57e14b2":"The most important thing is this part is to know how to deal with missin data. Here we decide to  remove columns from the dataset  which contains more than 30% of missing values and fix those which is less than 30% by using methode such  **forward fill**,  **backward fill**, **fill by the mode, mean etc....**\n"}}