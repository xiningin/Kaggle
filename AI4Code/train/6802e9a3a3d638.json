{"cell_type":{"18f37834":"code","d43c43be":"code","6a86e42e":"code","ddbf56c6":"code","e6d3ee50":"code","d8c0ef75":"code","d3b707d4":"code","424c5498":"code","03a18a13":"code","f6be64f4":"code","47bc8403":"code","7ce90fda":"code","4463119f":"code","69d63065":"code","752cd7b2":"code","17803871":"code","be12ca54":"code","e021e51b":"code","fbacdde5":"code","45a14676":"code","ab9b6122":"code","60fa8629":"code","ca8e6a33":"code","557021a0":"code","6d9bc1f5":"code","ba47ecd1":"code","2ad95252":"code","d38eb241":"code","c8ce46f4":"code","c2799ac0":"code","6bf0ea65":"code","9e274650":"code","7f75b62f":"code","bd301fd8":"code","58e4122b":"code","fb36de6f":"code","53824d69":"code","b4b0a0e6":"code","d1c84e78":"code","6e38a7cc":"code","85341d96":"code","fa533c46":"code","c68d7021":"code","478bb1e4":"code","18cc1f76":"code","7e6a2974":"code","f214b7c0":"code","7295c8e3":"code","506c9e5d":"code","7b087366":"code","88bab883":"code","4aea144e":"code","7746d1a0":"code","5da15040":"code","72c9821f":"code","0b86e2d4":"code","8b9e0234":"code","9bd38b09":"code","189a05f3":"code","b5b13337":"code","e303dbce":"code","b3ec343a":"code","93e0715b":"code","076f55f6":"code","c8225bb8":"code","12b16035":"code","8b0a0ae6":"code","604a601d":"code","80783a0c":"code","46343e2f":"code","59a879fc":"code","ec3c0e96":"code","7a291d5b":"code","162aa5c2":"code","d3fc4ba7":"code","b4031f70":"code","0c8759e3":"code","ed10215b":"code","5df3b356":"code","b6e0f717":"code","2174ae75":"code","73db5ea0":"code","70b75f19":"code","d0fb2fad":"code","2a85e99a":"code","839dc056":"code","a4d93ceb":"code","89b435bd":"code","74ee4b90":"code","c71fd474":"code","66ee1749":"code","9d1ed63a":"code","7d6c95d5":"code","67ba2b05":"code","cb32b709":"code","da17ce0e":"code","4de00777":"code","96934d00":"code","a8b0fa86":"code","bb6fe461":"code","6abe024e":"code","80a9213c":"code","5c12aba4":"code","6d627b99":"markdown","b76690e3":"markdown","0c4926a0":"markdown","19730880":"markdown","e2bd895a":"markdown","6624448c":"markdown","9d25da49":"markdown","9b5894d9":"markdown","6bea21f3":"markdown","acbd1ae7":"markdown","394225ac":"markdown","f7191e77":"markdown","d8d932a0":"markdown","0d6b57cb":"markdown","47065bbb":"markdown","96e2908e":"markdown","89db3471":"markdown","b944890e":"markdown","c8b58804":"markdown","d2780497":"markdown","5efe8d99":"markdown","d4879ff0":"markdown","f4d25020":"markdown","0e6f692b":"markdown","a6e41edc":"markdown","2e7421d8":"markdown"},"source":{"18f37834":"import copy\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\nfrom mpl_toolkits import mplot3d\nfrom scipy import stats\n%matplotlib inline\n\n# WordCloud\nfrom wordcloud import WordCloud\n\n# map visualization\nimport folium \nfrom folium import plugins\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nimport pandas_profiling as pp\n\n# models\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, SGDRegressor, RidgeCV\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor \nfrom sklearn.ensemble import BaggingRegressor\nimport sklearn.model_selection\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom scipy.stats import pearsonr\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d43c43be":"features = ['price','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view',\n            'condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated',\n            'zipcode','lat','long','sqft_living15','sqft_lot15']","6a86e42e":"valid_part = 0.3\npd.set_option('max_columns',100)","ddbf56c6":"train0 = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ntrain0 = train0[features]\ntrain0.head(5)","e6d3ee50":"train0.info()","d8c0ef75":"train0 = train0.dropna()\ntrain0.head(5)","d3b707d4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train0.columns.values.tolist()\nfor col in features:\n    if train0[col].dtype in numerics: continue\n    categorical_columns.append(col)\n# Encoding categorical features\nfor col in categorical_columns:\n    if col in train0.columns:\n        le = LabelEncoder()\n        le.fit(list(train0[col].astype(str).values))\n        train0[col] = le.transform(list(train0[col].astype(str).values))","424c5498":"train0['price'] = (train0['price']).astype(int)\ntrain0['floors'] = (train0['floors']).astype(int)\ntrain0['bedrooms'] = (train0['bedrooms']).astype(int)","03a18a13":"train0.head(10)","f6be64f4":"train0.info()","47bc8403":"train0['price'].value_counts()","7ce90fda":"train0.corr()","4463119f":"#Thanks to: https:\/\/www.kaggle.com\/arthurtok\/feature-ranking-rfe-random-forest-linear-models\nstr_list = [] # empty list to contain columns with strings (words)\nfor colname, colvalue in train0.iteritems():\n    if type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion            \nnum_list = train0.columns.difference(str_list) \n# Create Dataframe containing only numerical features\nhouse_num = train0[num_list]\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation of features')\n# Draw the heatmap using seaborn\nsns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"cubehelix\", linecolor='k', annot=True)","69d63065":"# FINDING CORRELATION\n# Thanks to: https:\/\/www.kaggle.com\/sid321axn\/house-price-prediction-gboosting-adaboost-etc\n# As id and date columns are not important to predict price so we are discarding it for finding correlation\nfeaturess = train0.iloc[:,3:].columns.tolist()\ntarget = train0.iloc[:,0].name","752cd7b2":"# Finding Correlation of price woth other variables to see how many variables are strongly correlated with price\ncorrelations = {}\nfor f in featuress:\n    data_temp = train0[[f,target]]\n    x1 = data_temp[f].values\n    x2 = data_temp[target].values\n    key = f + ' vs ' + target\n    correlations[key] = pearsonr(x1,x2)[0]","17803871":"# Printing all the correlated features value with respect to price which is target variable\ndata_correlations = pd.DataFrame(correlations, index=['Value']).T\ndata_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]","be12ca54":"sns.distplot(train0['yr_built'])","e021e51b":"train0.describe()","fbacdde5":"# As zipcode is negatively correlated with sales price , so we can discard it for sales price prediction.\ndrop_columns = ['zipcode', 'view', 'waterfront', 'yr_renovated']\ntrain0 = train0.drop(columns = drop_columns)","45a14676":"train0.describe(percentiles=[.01, .05, .1, .5, .9, .92, .93, .94, .96, .97, .99])","ab9b6122":"(train0['condition'] > 2.5).value_counts()","60fa8629":"(train0['grade'] == 4).value_counts()","ca8e6a33":"train0 = train0[(\n                (train0['price'] <= 1000000) & \n                (train0['price'] > 170000) & \n                (train0['bathrooms'] <= 4) & \n                (train0['condition'] > 2.5) & \n                (train0['grade'] != 4) &\n                (train0['sqft_lot15'] > 1300) &\n                (train0['sqft_lot15'] < 44000) &\n                (train0['sqft_lot'] > 1500) &\n                (train0['sqft_lot'] < 70000) &\n                (train0['sqft_living'] > 700) & \n                (train0['yr_built'] > 1925) & \n                (train0['bedrooms'] > 0) & \n                (train0['bedrooms'] < 7) \n                )]","557021a0":"train0.info()","6d9bc1f5":"pp.ProfileReport(train0)","ba47ecd1":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ndef plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(15,10))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    \n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    \nplotting_3_chart(train0, 'price')","2ad95252":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ny = np.array(train0.price)\nplt.subplot(131)\nplt.plot(range(len(y)),y,'.');plt.ylabel('price');plt.xlabel('index');\nplt.subplot(132)\nsns.boxplot(y=train0.price)","d38eb241":"#Thanks to https:\/\/towardsdatascience.com\/an-easy-introduction-to-3d-plotting-with-matplotlib-801561999725\nfig = plt.figure(figsize=(10,10))\nax = plt.axes(projection=\"3d\")\n\nz_points = train0['price']\nx_points = train0['condition']\ny_points = train0['yr_built']\nax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');\n\nax.set_xlabel('condition')\nax.set_ylabel('yr_built')\nax.set_zlabel('price')\n\nplt.show()","c8ce46f4":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\nfig=plt.figure(figsize=(19,12.5))\nax=fig.add_subplot(2,2,2, projection=\"3d\")\nax.scatter(train0['floors'],train0['bedrooms'],train0['sqft_living'],c=\"darkgreen\",alpha=.5)\nax.set(xlabel='\\nFloors',ylabel='\\nBedrooms',zlabel='\\nsqft Living')\nax.set(ylim=[0,12])\nplt.show()","c2799ac0":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\ngrpby_bedrooms_df = train0[[\"price\", \"bedrooms\"]].groupby(by = \"bedrooms\", as_index = False)\ngrpby_bedrooms_df = grpby_bedrooms_df.mean().astype(int)\ngrpby_bedrooms_df.head()","6bf0ea65":"# Thanks to: https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\nax1.set(yscale = \"log\")\nsns.stripplot(x = \"bedrooms\", y = \"price\", data = train0, ax = ax1, jitter=True, palette=\"Blues_d\")\nsns.barplot(x = \"bedrooms\", y = \"price\", data = grpby_bedrooms_df, ax = ax2, palette=\"Blues_d\")\nplt.show()","9e274650":"# Thanks to: https:\/\/www.kaggle.com\/fulrose\/copy-fix-house-price-prediction-1-2\nhouses_map = folium.Map(location = [train0['lat'].mean(), train0['long'].mean()], zoom_start = 10)\nlat_long_data = train0[['lat', 'long']].values.tolist()\nh_cluster = folium.plugins.FastMarkerCluster(lat_long_data).add_to(houses_map)\nhouses_map","7f75b62f":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Grade Frame\ngradeframe = pd.DataFrame({\"Grades\":train0.grade.value_counts().index,\"House_Grade\":train0.grade.value_counts().values})\ngradeframe[\"Grades\"] = gradeframe[\"Grades\"].apply(lambda x : \"Grade \" + str(x))\ngradeframe.set_index(\"Grades\",inplace=True)\n#gradeframe","bd301fd8":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = gradeframe.index,values = gradeframe.House_Grade,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Grade Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","58e4122b":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Bedrooms Frame\nbedroomsframe = pd.DataFrame({\"Bedrooms\":train0.bedrooms.value_counts().index,\"House_bedrooms\":train0.bedrooms.value_counts().values})\nbedroomsframe[\"Bedrooms\"] = bedroomsframe[\"Bedrooms\"].apply(lambda x : \"Bedrooms \" + str(x))\nbedroomsframe.set_index(\"Bedrooms\",inplace=True)\n#bedroomsframe","fb36de6f":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = bedroomsframe.index,values = bedroomsframe.House_bedrooms,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Bedrooms Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","53824d69":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Floors Frame\nfloorsframe = pd.DataFrame({\"Floors\":train0.floors.value_counts().index,\"House_floors\":train0.floors.value_counts().values})\nfloorsframe[\"Floors\"] = floorsframe[\"Floors\"].apply(lambda x : \"Floors \" + str(x))\nfloorsframe.set_index(\"Floors\",inplace=True)\n#floorsframe","b4b0a0e6":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = floorsframe.index,values = floorsframe.House_floors,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Floors Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","d1c84e78":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\n#Create Condition Frame\nconditionframe = pd.DataFrame({\"Condition\":train0.condition.value_counts().index,\"House_condition\":train0.condition.value_counts().values})\nconditionframe[\"Condition\"] = conditionframe[\"Condition\"].apply(lambda x : \"Condition \" + str(x))\nconditionframe.set_index(\"Condition\",inplace=True)\n#conditionframe","6e38a7cc":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\np1 = [go.Pie(labels = conditionframe.index,values = conditionframe.House_condition,hoverinfo=\"percent+label+value\",hole=0.1,marker=dict(line=dict(color=\"#000000\",width=2)))]\nlayout4 = go.Layout(title=\"Condition Pie Chart\")\nfig4 = go.Figure(data=p1,layout=layout4)\niplot(fig4)","85341d96":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\nbuiltyear = pd.DataFrame({\"Years\":train0.yr_built})\nbuiltyear[\"Years\"] = builtyear[\"Years\"].apply(lambda x: \"y\" + str(x)) #I can't use wordcloud with integers so I put y on head\n#builtyear[\"Years\"].head()","fa533c46":"# Thanks to: https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization\nplt.subplots(figsize=(8,8))\nwcloud  = WordCloud(background_color=\"white\",width=500,height=500).generate(\",\".join(builtyear[\"Years\"]))\nplt.imshow(wcloud)\nplt.title(\"Years for Most Built Homes\",fontsize=40)\nplt.axis(\"off\")\nplt.show()","c68d7021":"# Clone data for FE \ntrain_fe = copy.deepcopy(train0)\ntarget_fe = train_fe['price']\ndel train_fe['price']","478bb1e4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nX = train_fe\nz = target_fe","18cc1f76":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","7e6a2974":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50,verbose_eval=10, valid_sets=valid_set)","f214b7c0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","7295c8e3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score = pd.DataFrame(train_fe.columns, columns = ['feature']) \nfeature_score['score_lgb'] = modelL.feature_importance()","506c9e5d":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","7b087366":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:squarederror',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","88bab883":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","4aea144e":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\nfeature_score","7746d1a0":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Standardization for regression model\ntrain_fe = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(train_fe),\n    columns=train_fe.columns,\n    index=train_fe.index\n)","5da15040":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train_fe, target_fe)\ncoeff_logreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","72c9821f":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# the level of importance of features is not associated with the sign\ncoeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","0b86e2d4":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n# Linear Regression\n\nlinreg = LinearRegression()\nlinreg.fit(train_fe, target_fe)\ncoeff_linreg = pd.DataFrame(train_fe.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","8b9e0234":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\ncoeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","9bd38b09":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Thanks to: https:\/\/www.kaggle.com\/nanomathias\/feature-engineering-importance-testing\n# MinMax scale all importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\n\n# Plot the feature importances\nfeature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 10))","189a05f3":"feature_score.sort_values('mean', ascending=False)","b5b13337":"# Thanks to: Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n# Create total column with different weights\nfeature_score['total'] = 0.5*feature_score['score_lgb'] + 0.3*feature_score['score_xgb'] \\\n                       + 0.1*feature_score['score_logreg'] + 0.1*feature_score['score_linreg']\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","e303dbce":"feature_score.sort_values('total', ascending=False)","b3ec343a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ntarget_name = 'price'\ntrain_target0 = train0[target_name]\ntrain0 = train0.drop([target_name], axis=1)","93e0715b":"# Synthesis test0 from train0\ntrain0, test0, train_target0, test_target0 = train_test_split(train0, train_target0, test_size=0.2, random_state=0)","076f55f6":"# For boosting model\ntrain0b = train0\ntrain_target0b = train_target0\n# Synthesis valid as test for selection models\ntrainb, testb, targetb, target_testb = train_test_split(train0b, train_target0b, test_size=valid_part, random_state=0)","c8225bb8":"#For models from Sklearn\nscaler = StandardScaler()\ntrain0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)","12b16035":"train0.head(3)","8b0a0ae6":"len(train0)","604a601d":"# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(train0, train_target0, test_size=valid_part, random_state=0)","80783a0c":"train.head(3)","46343e2f":"test.head(3)","59a879fc":"train.info()","ec3c0e96":"test.info()","7a291d5b":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nacc_train_r2 = []\nacc_test_r2 = []\nacc_train_d = []\nacc_test_d = []\nacc_train_rmse = []\nacc_test_rmse = []","162aa5c2":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","d3fc4ba7":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_boosting_model(num,model,train,test,num_iteration=0):\n    # Calculation of accuracy of boosting model by different metrics\n    \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    if num_iteration > 0:\n        ytrain = model.predict(train, num_iteration = num_iteration)  \n        ytest = model.predict(test, num_iteration = num_iteration)\n    else:\n        ytrain = model.predict(train)  \n        ytest = model.predict(test)\n\n    print('target = ', targetb[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(targetb, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(targetb, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(targetb, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_testb[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_testb, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_testb, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_testb, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","b4031f70":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\ndef acc_model(num,model,train,test):\n    # Calculation of accuracy of model from Sklearn by different metrics   \n  \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    ytrain = model.predict(train)  \n    ytest = model.predict(test)\n\n    print('target = ', target[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(target, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(target, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(target, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_test[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_test, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_test, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_test, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","0c8759e3":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Random Forest\n\n#random_forest = GridSearchCV(estimator=RandomForestRegressor(), param_grid={'n_estimators': [100, 1000]}, cv=5)\nrandom_forest = RandomForestRegressor()\nrandom_forest.fit(train, target)\nacc_model(1,random_forest,train,test)","ed10215b":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nxgb_clf = xgb.XGBRegressor({'objective': 'reg:squarederror'}) \nparameters = {'n_estimators': [60, 100, 120, 140], \n              'learning_rate': [0.01, 0.1],\n              'max_depth': [5, 7],\n              'reg_lambda': [0.5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=5, n_jobs=-1).fit(trainb, targetb)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\nacc_boosting_model(2,xgb_reg,trainb,testb)","5df3b356":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","b6e0f717":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': False,\n        'seed':0,        \n    }\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=10000,\n                   early_stopping_rounds=8000,verbose_eval=500, valid_sets=valid_set)","2174ae75":"acc_boosting_model(3,modelL,trainb,testb,modelL.best_iteration)","73db5ea0":"fig =  plt.figure(figsize = (5,5))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()","70b75f19":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Bagging Regressor\n\nbagging = BaggingRegressor()\nbagging.fit(train, target)\nacc_model(4,bagging,train,test)","d0fb2fad":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Extra Trees Regressor\n\netr = ExtraTreesRegressor()\netr.fit(train, target)\nacc_model(5,etr,train,test)","2a85e99a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\nmodels = pd.DataFrame({\n    'Model': ['Random Forest', 'XGB', 'LGBM', 'BaggingRegressor', 'ExtraTreesRegressor'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })","839dc056":"pd.options.display.float_format = '{:,.2f}'.format","a4d93ceb":"print('Prediction accuracy for models by R2 criterion - r2_test')\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)","89b435bd":"print('Prediction accuracy for models by relative error - d_test')\nmodels.sort_values(by=['d_test', 'd_train'], ascending=True)","74ee4b90":"print('Prediction accuracy for models by RMSE - rmse_test')\nmodels.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)","c71fd474":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('R2-criterion for 5 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('R2-criterion, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","66ee1749":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for 5 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","9d1ed63a":"# Thanks to: https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n# Plot\nplt.figure(figsize=[15,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('RMSE for 5 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('RMSE, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","7d6c95d5":"test0.info()","67ba2b05":"test0.head(3)","cb32b709":"#For models from Sklearn\ntestn = pd.DataFrame(scaler.transform(test0), columns = test0.columns)","da17ce0e":"#Linear Regression model for basic train\nlinreg.fit(train0, train_target0)\nlinreg_predict = linreg.predict(testn)\nlinreg_predict[:3]","4de00777":"#Random Forest model for basic train\nrandom_forest.fit(train0, train_target0)\nrf_predict = random_forest.predict(testn)\nrf_predict[:3]","96934d00":"#Bagging Regressor model for basic train\nbagging.fit(train0, train_target0)\nbg_predict = bagging.predict(testn)\nbg_predict[:3]","a8b0fa86":"# XGB Regression model for basic train\nxgb_reg.fit(train0, train_target0)\nxgb_predict = xgb_reg.predict(testn)\nxgb_predict[:3]","bb6fe461":"# LGB Regression model for basic train\nlgb_predict = modelL.predict(test0)\nlgb_predict[:3]","6abe024e":"# Extra Trees Regressor model for basic train\netr.fit(train0, train_target0)\netr_predict = etr.predict(testn)\netr_predict[:3]","80a9213c":"# Thanks to: https:\/\/www.kaggle.com\/dnzcihan\/house-sales-prediction-and-eda\nfinal_df = test_target0.values\nfinal_df = pd.DataFrame(final_df,columns=['Real_price'])\nfinal_df['predicted_prices'] = lgb_predict.astype(int)\nfinal_df['difference'] = abs(final_df['Real_price'] - final_df['predicted_prices']).astype(int)\nfinal_df.head(20)","5c12aba4":"mean_real_price = round(final_df['Real_price'].mean(), 0)\nmean_predicted_prices = round(final_df['predicted_prices'].mean(), 0)\nmean_difference = round(final_df['difference'].mean(), 0)\n# Create and append mean values to DataFrame \nmean_val = []\nmean_val.append(('real_price', mean_real_price))\nmean_val.append(('predicted_prices', mean_predicted_prices))\nmean_val.append(('difference', mean_difference))\npd.DataFrame(mean_val, columns = ('Name', 'Average'))","6d627b99":"<a class=\"anchor\" id=\"6\"><\/a>\n## 6. Dada for modeling\n##### [Back to Table of Contents](#0.1)","b76690e3":"![](https:\/\/images.pexels.com\/photos\/106399\/pexels-photo-106399.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=650&w=940)\n###### Thanks for the photo Binyamin Mellish: https:\/\/www.pexels.com\/photo\/home-real-estate-106399\/","0c4926a0":"<a class=\"anchor\" id=\"8.2\"><\/a>\n### 8.2 XGB\n##### [Back to Table of Contents](#0.1)","19730880":"<a class=\"anchor\" id=\"8.1\"><\/a>\n### 8.1 Random Forest\n##### [Back to Table of Contents](#0.1)","e2bd895a":"<a class=\"anchor\" id=\"8.3\"><\/a>\n### 8.3 LGBM\n##### [Back to Table of Contents](#0.1)","6624448c":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## **Table of Contents**\n1. [Import libraries](#1)\n2. [Download datasets](#2)\n3. [EDA](#3)\n4. [FE: building the feature importance diagrams](#4)\n  -  [LGBM](#4.1)\n  -  [XGB](#4.2)\n  -  [Logistic Regression](#4.3)\n  -  [Linear Reagression](#4.4)\n5. [Comparison of the all feature importance diagrams](#5)\n6. [Dada for modeling](#6)\n7. [Preparing to modeling](#7)\n8. [Tuning models](#8)\n  -  [Random Forest](#8.1)\n  -  [XGB](#8.2)\n  -  [LGBM](#8.3)\n  -  [BaggingRegressor](#8.4)\n  -  [ExtraTreesRegressor](#8.5)\n9. [Models comparison](#9)\n10. [Prediction](#10)","9d25da49":"## **Acknowledgements**\n#### This kernel uses such good kernels:\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-fe-eda-with-3d\n   - https:\/\/www.kaggle.com\/vbmokin\/used-cars-price-prediction-by-15-models\n   - https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\\\n   - https:\/\/www.kaggle.com\/shanroy1999\/house-price-prediction-using-linear-regression\n   - https:\/\/www.kaggle.com\/fulrose\/copy-fix-house-price-prediction-1-2\n   - https:\/\/www.kaggle.com\/sid321axn\/house-price-prediction-gboosting-adaboost-etc\n   - https:\/\/www.kaggle.com\/dnzcihan\/house-sales-prediction-and-eda\n   - https:\/\/www.kaggle.com\/darkcore\/house-sales-visualization","9b5894d9":"<a class=\"anchor\" id=\"4.3\"><\/a>\n### 4.3 Logistic Regression\n##### [Back to Table of Contents](#0.1)","6bea21f3":"<a class=\"anchor\" id=\"4\"><\/a>\n## 4. FE: building the feature importance diagrams\n##### [Back to Table of Contents](#0.1)","acbd1ae7":"<a class=\"anchor\" id=\"8\"><\/a>\n## 8. Tuning models\n##### [Back to Table of Contents](#0.1)","394225ac":"<a class=\"anchor\" id=\"3\"><\/a>\n## 3. EDA\n##### [Back to Table of Contents](#0.1)","f7191e77":"<a class=\"anchor\" id=\"8.5\"><\/a>\n### 8.5 Extra Trees Regressor\n##### [Back to Table of Contents](#0.1)","d8d932a0":"<a class=\"anchor\" id=\"10\"><\/a>\n### 10 Prediction\n##### [Back to Table of Contents](#0.1)","0d6b57cb":"# **House Price Prediction by 5 models + EDA & FE**","47065bbb":"<a class=\"anchor\" id=\"5\"><\/a>\n## 5. Comparison of the all feature importance diagrams \n##### [Back to Table of Contents](#0.1)","96e2908e":"<a class=\"anchor\" id=\"8.4\"><\/a>\n### 8.4 Bagging Regressor\n##### [Back to Table of Contents](#0.1)","89db3471":"<a class=\"anchor\" id=\"4.1\"><\/a>\n### 4.1 LGBM ","b944890e":"<a class=\"anchor\" id=\"2\"><\/a>\n## 2. Download datasets\n##### [Back to Table of Contents](#0.1)","c8b58804":"### **Below screenshot explains what all headers mean.**","d2780497":"![](https:\/\/storage.googleapis.com\/kaggle-forum-message-attachments\/479761\/11440\/Screenshot%202019-02-27%20at%205.26.24%20PM.png)\n###### Thanks for the screenshot: https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction\/discussion\/82135","5efe8d99":"###### These wonderful charts are taken in [vbmokin](https:\/\/www.kaggle.com\/vbmokin)\n###### Thanks to: https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg","d4879ff0":"<a class=\"anchor\" id=\"9\"><\/a>\n### 9 Models comparison\n##### [Back to Table of Contents](#0.1)","f4d25020":"<a class=\"anchor\" id=\"4.2\"><\/a>\n### 4.2 XGB\n##### [Back to Table of Contents](#0.1)","0e6f692b":"<a class=\"anchor\" id=\"4.4\"><\/a>\n### 4.4 Linear Regression\n##### [Back to Table of Contents](#0.1)","a6e41edc":"<a class=\"anchor\" id=\"1\"><\/a>\n## 1. Import libraries \n##### [Back to Table of Contents](#0.1)","2e7421d8":"<a class=\"anchor\" id=\"7\"><\/a>\n## 7. Preparing to modeling\n##### [Back to Table of Contents](#0.1)"}}