{"cell_type":{"21d74f2f":"code","fa529f7f":"code","a4d75904":"code","7a3bd0b8":"code","412fd3a9":"code","ecd000bb":"code","7224528b":"code","21734962":"code","a040d174":"code","ebad41d9":"code","890949bd":"code","781cdbcf":"code","0bd2a09f":"code","17eec1e6":"code","404d52c9":"code","861baf7a":"code","1b19b827":"code","2301bf8e":"code","4d21d3b2":"code","df36d41a":"code","e382c251":"code","77bb5882":"code","c6efe48f":"code","e4422423":"code","0b459863":"code","917f3c6c":"code","9c412037":"code","f349cdcf":"code","994d8e0e":"code","153b182a":"code","d4af0087":"code","1f44d496":"code","fe213f59":"code","0228fb90":"code","4c60a072":"code","07c260ea":"code","a6ec0f89":"code","7f4dc3ea":"code","11aa15fb":"code","f2494e04":"code","24f2a269":"code","0eb5fc17":"code","7169a63f":"code","ce30c97f":"code","0e3b3406":"code","89702324":"code","58fdb864":"code","66917770":"code","10c91c86":"code","54b056ce":"code","491d71c7":"code","70f1b8ac":"code","7a860447":"code","d3a1dd62":"code","e0c0fd38":"code","d0f5315c":"code","1f58e18b":"code","3f6eb3c3":"code","a04f00cc":"code","a9c1edc9":"code","72c93f5e":"code","da740e27":"code","48835435":"code","594283bb":"code","12f9f935":"code","807a1366":"code","9859ecca":"code","a19a2888":"code","b7c8b7c8":"code","3ab69b79":"code","01b3ff58":"code","0857641a":"code","b0c461b7":"code","16d9e0b8":"code","a4688ca1":"code","287ec6cc":"code","48c1d00f":"code","c5d06b3f":"code","344eb260":"code","dfab2385":"code","a350fd2d":"code","15b4e0c7":"code","e46a4dc4":"code","0b6147d2":"code","a21fa6a3":"code","b49401ee":"code","91106ae7":"code","55703297":"code","80d29d84":"code","9459d2e6":"code","c5dbb77b":"code","9b9d420f":"code","bc3fea66":"code","418618e3":"code","f013f0ef":"code","c0afecde":"code","b23a370f":"code","99918b47":"code","00f61b75":"code","3fdc00d8":"code","39c70bb1":"markdown","bf4f0b99":"markdown","6baf1869":"markdown","dad0eba5":"markdown","9688f1f4":"markdown","b68f307f":"markdown","1c497c03":"markdown","2d4cb828":"markdown","dd471a62":"markdown","186b95fd":"markdown","19e4a1a8":"markdown","335dd9e8":"markdown","4f2e936e":"markdown","051ed5c9":"markdown","45a94583":"markdown","55eb7be8":"markdown","089c87ce":"markdown","545d4f20":"markdown","35f02e9c":"markdown","9430df8f":"markdown","511bc84d":"markdown","b276510f":"markdown","03c80620":"markdown","b90e64d7":"markdown","8e7e84af":"markdown","b3c0e44c":"markdown","1c0ea15c":"markdown","379c29c8":"markdown","4868e828":"markdown","587f678d":"markdown"},"source":{"21d74f2f":"#import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom scipy.stats import norm\nfrom matplotlib import gridspec\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\n!pip install feature_engine\nimport feature_engine.transformation as vt\nfrom feature_engine.outliers import Winsorizer\nfrom sklearn.model_selection import train_test_split\nsns.set_style('whitegrid')\nimport warnings\nwarnings.filterwarnings('ignore')","fa529f7f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a4d75904":"#import dataset\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","7a3bd0b8":"df.head()","412fd3a9":"df.info()","ecd000bb":"import missingno as msno\nmsno.matrix(df)","7224528b":"msno.bar(df)","21734962":"# delete the \"Unnamed: 32\" column sine it contains no values\ndel df['Unnamed: 32']","a040d174":"df.drop('id',axis = 1).describe().T","ebad41d9":"df[df['diagnosis'] == 'B'].describe().T","890949bd":"df[df['diagnosis'] == 'M'].describe().T","781cdbcf":"sns.set_theme(style=\"whitegrid\")\nfig,ax = plt.subplots(figsize = (6,6))\nsns.countplot(x=\"diagnosis\", data=df)\n#annotatinos\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2., height + 1,height ,ha=\"center\")","0bd2a09f":"def plotHistBox(col,hexColor = '#00aeff'):\n    fig,ax = plt.subplots(ncols = 2,figsize = (15,4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1.7])\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n    #set figure title\n    ax0.set_title('Histplot with KDE estimates')\n    ax1.set_title('Boxplot')\n    sns.histplot(df[col], kde=True,color = hexColor,ax = ax0)\n    #sns.kdeplot(df[col], color=\"red\")\n    sns.boxplot(x=df[col],color = hexColor,ax = ax1)\n    fig.suptitle(f'Column : {col}', fontsize=16)\n    ","17eec1e6":"plotHistBox('radius_mean')","404d52c9":"for col in df.columns[3:]:\n    plotHistBox(col)","861baf7a":"kurtSkewDict = {\n    \"Skewness\" : df.skew()[1:].values,\n    \"Kurtosis\" : df.kurt()[1:].values,\n}\n","1b19b827":"#create a dataFrame for the Skewness and Kurtosis\nkurtSkewFrame = pd.DataFrame(data=kurtSkewDict,index = df.columns[2:])\nkurtSkewFrame","2301bf8e":"def plotHistBoxBi(col):\n    fig,ax = plt.subplots(ncols = 2,figsize = (15,4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1.5])\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n    #set figure title\n    ax0.set_title('Histplots')\n    ax1.set_title('Boxplots')\n    sns.histplot(data = df,x = col,hue= 'diagnosis',ax = ax0)\n    #sns.kdeplot(df[col], color=\"red\")\n    sns.boxplot(data =df,x=col,y = 'diagnosis',hue= 'diagnosis',ax = ax1)\n    fig.suptitle(f'Column : {col}', fontsize=16)","4d21d3b2":"plotHistBoxBi('radius_mean')","df36d41a":"for col in df.columns[3:]:\n    plotHistBoxBi(col)","e382c251":"#create a class to return the top correlated features and also plot them\nclass correlationInfo():\n    \n    def __init__(self,col):\n        self.col = col\n        self.corrFrame = df[df.columns[2:]].corr()[self.col].sort_values(ascending = False)[1:].head(9)\n        \n    def corrVal(self):\n        corrFrame = self.corrFrame.to_frame()\n        corrFrame.columns = ['Correlation_values']\n        print(f'Top nine features most correlatd to {col}')\n        return corrFrame\n    \n    def correlationPlot(self):\n        #grab the top nine most correlated attributes with the col\n        corrCol = self.corrFrame.index\n        fig,ax = plt.subplots(nrows = 3,ncols = 3,figsize = (15,12))\n        nrow = 0\n        ncol = 0\n        for cor_col in corrCol:\n            sns.scatterplot(data=df, x=self.col, y=cor_col, hue=\"diagnosis\", size=\"diagnosis\",ax = ax[nrow,ncol])\n            ncol += 1\n            if ncol ==3:\n                nrow +=1\n                ncol =0\n        fig.suptitle(f'Top 9 most correated features with {self.col}',size = 16)        \n    \n        ","77bb5882":"correlationInfo('radius_mean').corrVal()","c6efe48f":"correlationInfo('radius_mean').correlationPlot()","e4422423":"correlationInfo('symmetry_worst').corrVal()","0b459863":"correlationInfo('symmetry_worst').correlationPlot()","917f3c6c":"#change the label of the target variables\ndf['diagnosis_e'] = df['diagnosis'].replace(['M'],1)\ndf['diagnosis_e'] = df['diagnosis_e'].replace(['B'],0)","9c412037":"print('The top nine features most correlated with Malignant tumor is ')\ndf.corr()['diagnosis_e'].sort_values(ascending = False)[1:].head(9)","f349cdcf":"print(\"Pair plot of the top features with most correlation with the target (part-1)\")\nsns.pairplot(df,vars = df.corr()\n             ['diagnosis_e'].sort_values(ascending = False)[1:].head(4).index,\n             hue=\"diagnosis\",diag_kind=\"hist\",height=3,markers=['v', '^'])","994d8e0e":"print(\"Pair plot of the top features with most correlation with the target (part-2)\")\nsns.pairplot(df,vars = df.corr()\n             ['diagnosis_e'].sort_values(ascending = False)[1:][4:9].index,\n             hue=\"diagnosis\",diag_kind=\"hist\",markers=['v', '^'])","153b182a":"# Separate into train and test sets\nX_train, X_test, y_train, y_test =  train_test_split(\n            df.drop(['id', 'diagnosis','diagnosis_e'], axis=1),\n            df['diagnosis_e'], test_size=0.3, random_state=69)","d4af0087":"var_cols = df.drop(['id','diagnosis','diagnosis_e'],axis = 1).columns.to_list()","1f44d496":"windsoriser = Winsorizer(capping_method='iqr', # choose iqr for IQR rule boundaries or gaussian for mean and std\n                          tail='both', # cap left, right or both tails \n                          fold=1.5,\n                          variables = var_cols)\n\nwindsoriser.fit(X_train)","fe213f59":"#transform only the test set as it might lead to overfitting.\nX_train = windsoriser.transform(X_train)","0228fb90":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX_train = rs.fit_transform(X_train)\nX_test = rs.transform(X_test)","4c60a072":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom scipy.stats import loguniform\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","07c260ea":"def plot_condution_metrics(y_test,predictions):\n    #condusion metrics\n    cm = metrics.confusion_matrix(y_test, predictions)\n    score = np.mean([y_test == predictions])\n    #plot\n    sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=1, square = True,cbar = False);\n    plt.ylabel('Actual label');\n    plt.xlabel('Predicted label');\n    all_sample_title = 'TEST ACCURACY SCORE: {0}'.format(score)\n    plt.title(all_sample_title, size = 15);","a6ec0f89":"print(\"Training Shape\",X_train.shape)\nprint(\"Testing Shape\",X_test.shape)","7f4dc3ea":"from sklearn.linear_model import LogisticRegression","11aa15fb":"parameters = {'penalty': ['l1'],\"fit_intercept\":[True,False],\"C\" :np.logspace(-4,4,16),'solver' : ['liblinear'],\n              'max_iter' : [500,1000]\n             }","f2494e04":"gridsearch = GridSearchCV(LogisticRegression(), parameters)\ngridsearch.fit(X_train, y_train)","24f2a269":"gridsearch.best_params_","0eb5fc17":"score  = cross_val_score(gridsearch, X_train, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f'The Training Score is {gridsearch.score(X_train, y_train)}')","7169a63f":"test_pred = gridsearch.predict(X_test)\nauc = roc_auc_score(y_test, test_pred)\nprint(f\"The score on the Test-dataset is {gridsearch.score(X_test, y_test)}\")\nprint(f\"The ROC_AUC score on the Test-dataset is {auc}\")","ce30c97f":"plot_condution_metrics(y_test,test_pred)","0e3b3406":"model_performance = {}\nauc_socre = {}\nmodel_performance['Logistic Regression(Lasso)'] = gridsearch.score(X_test, y_test)\nauc_socre['Logistic Regression(Lasso)'] = auc","89702324":"parameters = {'penalty': ['l2'],\"fit_intercept\":[True,False],\"C\" : np.logspace(-4,4,16),\n              'solver' : ['newton-cg','liblinear'],\n              'max_iter' : [100,500]\n             }","58fdb864":"gridsearch = GridSearchCV(LogisticRegression(), parameters)\ngridsearch.fit(X_train, y_train)","66917770":"gridsearch.best_params_","10c91c86":"score  = cross_val_score(gridsearch, X_train, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f'The Training Score is {gridsearch.score(X_train, y_train)}')\n","54b056ce":"test_pred = gridsearch.predict(X_test)\nauc = roc_auc_score(y_test, test_pred)\nprint(f\"The score on the Test-dataset is {gridsearch.score(X_test, y_test)}\")\nprint(f\"The ROC_AUC score on the Test-dataset is {auc}\")","491d71c7":"plot_condution_metrics(y_test,test_pred)","70f1b8ac":"model_performance['Logistic Regression(Ridge)'] = gridsearch.score(X_test, y_test)\nauc_socre['Logistic Regression(Ridge)'] = auc","7a860447":"from sklearn.preprocessing import PolynomialFeatures\nparameters = {'penalty': ['l1','l2'],\"fit_intercept\":[True,False],\"C\" :np.logspace(-4,4,16),'solver' : ['liblinear'],\n              'max_iter' : [500,1000]\n             }","d3a1dd62":"poly_features = PolynomialFeatures(degree = 2)\n\ntrain_poly = poly_features.fit_transform(X_train)\ntest_poly = poly_features.transform(X_test)\n\ngridsearch = GridSearchCV(LogisticRegression(), parameters)\ngridsearch.fit(train_poly, y_train)","e0c0fd38":"gridsearch.best_params_","d0f5315c":"score  = cross_val_score(gridsearch, train_poly, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f'The Training Score is {gridsearch.score(train_poly, y_train)}')","1f58e18b":"test_pred = gridsearch.predict(test_poly)\nauc = roc_auc_score(y_test, test_pred)\nprint(f\"The score on the Test-dataset is {gridsearch.score(test_poly, y_test)}\")\nprint(f\"The ROC_AUC score on the Test-dataset is {auc}\")","3f6eb3c3":"plot_condution_metrics(y_test,test_pred)","a04f00cc":"model_performance['Polynomial Logistic Regression'] = gridsearch.score(test_poly, y_test)\nauc_socre['Polynomial Logistic Regression'] = auc","a9c1edc9":"from sklearn.neighbors import KNeighborsClassifier","72c93f5e":"parameters = {\n    'n_neighbors' :np.arange(1,50),\n    \"weights\" : ['uniform','distance'],\n    \"p\" : [1,2],\n    \n}","da740e27":"gridsearch = GridSearchCV(KNeighborsClassifier(), parameters)\ngridsearch.fit(X_train, y_train)","48835435":"gridsearch.best_params_","594283bb":"score  = cross_val_score(gridsearch, X_train, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f'The Training Score is {gridsearch.score(X_train, y_train)}')\n","12f9f935":"test_pred = gridsearch.predict(X_test)\nauc = roc_auc_score(y_test, test_pred)\nprint(f\"The score on the Test-dataset is {gridsearch.score(X_test, y_test)}\")\nprint(f\"The ROC_AUC score on the Test-dataset is {auc}\")","807a1366":"plot_condution_metrics(y_test,test_pred)","9859ecca":"model_performance['KNN Classifier'] = gridsearch.score(X_test, y_test)\nauc_socre['KNN Classifier'] = auc","a19a2888":"from sklearn.naive_bayes import GaussianNB","b7c8b7c8":"gnb = GaussianNB()\ntest_pred = gnb.fit(X_train, y_train).predict(X_test)","3ab69b79":"score  = cross_val_score(gnb, X_train, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f'The Training Score is {gnb.score(X_train, y_train)}')","01b3ff58":"test_pred = gnb.predict(X_test)\nauc = roc_auc_score(y_test, test_pred)\nprint(f\"The score on the Test-dataset is {gnb.score(X_test, y_test)}\")\nprint(f\"The ROC_AUC score on the Test-dataset is {auc}\")","0857641a":"plot_condution_metrics(y_test,test_pred)","b0c461b7":"model_performance['Naive Bayes (gaussian)'] = gnb.score(X_test, y_test)\nauc_socre['Naive Bayes (gaussian)'] = auc","16d9e0b8":"from sklearn.svm import SVC","a4688ca1":"gamma =list(np.logspace(-4,4,16))\ngamma.append('scale')\nparam_grid = [\n    {\n        \"C\" :np.logspace(-4,4,16),\n        \"gamma\" : gamma,\n        \"kernel\" : ['rbf','linear']\n        \n    },\n]","287ec6cc":"gridsearch = GridSearchCV(SVC(), param_grid)\ngridsearch.fit(X_train, y_train)","48c1d00f":"gridsearch.best_params_","c5d06b3f":"score  = cross_val_score(gridsearch, X_train, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f'The Training Score is {gridsearch.score(X_train, y_train)}')\n","344eb260":"test_pred = gridsearch.predict(X_test)\nauc = roc_auc_score(y_test, test_pred)\nprint(f\"The score on the Test-dataset is {gridsearch.score(X_test, y_test)}\")\nprint(f\"The ROC_AUC score on the Test-dataset is {auc}\")","dfab2385":"plot_condution_metrics(y_test,test_pred)","a350fd2d":"model_performance['Support Vector Machine'] = gridsearch.score(X_test, y_test)\nauc_socre['Support Vector Machine'] = auc","15b4e0c7":"from sklearn.tree import DecisionTreeClassifier,plot_tree\nclf_dt = DecisionTreeClassifier(random_state=42)\n#build a preliminary tree\nclf_dt.fit(X_train, y_train)","e46a4dc4":"fig,ax = plt.subplots(figsize = (25,12))\nax = plot_tree(\n    clf_dt,\n    filled = True,\n    rounded = True,\n    class_names = ['Benign',\"Malignant\"],\n    feature_names = var_cols\n    \n)","0b6147d2":"score  = cross_val_score(clf_dt, X_train, y_train, cv=9,scoring='accuracy')\nprint(f'The mean Cross-Valiation Score is {score.mean()}')\nprint(f\"The score on the Train-dataset is {clf_dt.score(X_train, y_train)}\")","a21fa6a3":"path = clf_dt.cost_complexity_pruning_path(X_train,y_train)\nccp_alphas = path.ccp_alphas\nccp_alphas = ccp_alphas[:-1]\n\ncct_dts = []\n\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=42,ccp_alpha = ccp_alpha)\n    clf_dt.fit(X_train,y_train)\n    cct_dts.append(clf_dt)","b49401ee":"train_scores = [clf.score(X_train, y_train) for clf in cct_dts]\ntest_scores = [clf.score(X_test, y_test) for clf in cct_dts]\n\nfig, ax = plt.subplots(figsize = (15,6))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","91106ae7":"#looking at the figure and 'eye-balling' we see the alpha of 0.005 could be a bette value\n# using K-fold CV\nclf_dt = DecisionTreeClassifier(random_state=42,ccp_alpha = 0.005)\nscores = cross_val_score(clf_dt, X_train, y_train, cv=9,scoring='accuracy')\n#plot \ndf_cv = pd.DataFrame(data = {'tree' : range(9),'accuracy':scores})\ndf_cv.plot(x = 'tree',y = 'accuracy',marker = 'o',linestyle = '--')","55703297":"alpha_loop_values = []\n\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=42,ccp_alpha = ccp_alpha)\n    scores = cross_val_score(clf_dt, X_train, y_train, cv=9,scoring='accuracy')\n    alpha_loop_values.append([ccp_alpha,np.mean(scores),np.std(scores)])\n    \n#storing in a pandas datframe\nalpha_df = pd.DataFrame(alpha_loop_values,columns = ['alpha','mean_Score','std_score'])\n\n#plot df\nalpha_df.plot(x = 'alpha',y = 'mean_Score',marker = 'o',linestyle = '--')","80d29d84":"print('alpha values with cv score > .90')\nalpha_df[alpha_df['mean_Score'] > .9 ].sort_values(by = 'mean_Score',ascending = False)","9459d2e6":"ideal_alpha = 0.003350","c5dbb77b":"clf_dt_prune = DecisionTreeClassifier(random_state=42,ccp_alpha = ideal_alpha)\nclf_dt_prune.fit(X_train, y_train)","9b9d420f":"fig,ax = plt.subplots(figsize = (25,9))\nax = plot_tree(\n    clf_dt_prune,\n    filled = True,\n    rounded = True,\n    class_names = ['Benign',\"Malignent\"],\n    feature_names = var_cols\n    \n)","bc3fea66":"print(f\"The score on the Test-dataset is {clf_dt_prune.score(X_test, y_test)}\")\nprint(f\"The score on the Train-dataset is {clf_dt_prune.score(X_train, y_train)}\")","418618e3":"test_pred = clf_dt_prune.predict(X_test)\nauc = roc_auc_score(y_test, test_pred)\nplot_condution_metrics(y_test,test_pred)","f013f0ef":"model_performance['Decision Tree'] = clf_dt_prune.score(X_test, y_test)\nauc_socre['Decision Tree'] = auc","c0afecde":"model_performance","b23a370f":"auc_socre","99918b47":"acc_df = pd.DataFrame.from_dict(model_performance,orient = 'index',columns = ['Test Accuracy'])\nauc_df = pd.DataFrame.from_dict(auc_socre,orient = 'index',columns = ['Test AUC'])\nmodel_df = acc_df.join(auc_df)\nmodel_df = model_df.sort_values(by ='Test AUC',ascending = False)\nmodel_df","00f61b75":"fig,ax = plt.subplots(figsize = (9,4.5))\nplt.style.use('fivethirtyeight')\nax = sns.barplot(x=\"Test Accuracy\", y=model_df.index, data=model_df,color = '#1ecfd9')\n# Annotate every single Bar with its value, based on it's width           \nfor p in ax.patches:\n    ax.annotate(\"%.4f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")\nsns.despine()    ","3fdc00d8":"fig,ax = plt.subplots(figsize = (9,4.5))\nax = sns.barplot(x=\"Test AUC\", y=model_df.index, data=model_df,color = '#fc6423')\n# Annotate every single Bar with its value, based on it's width           \nfor p in ax.patches:\n    ax.annotate(\"%.4f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")\nsns.despine() ","39c70bb1":"#### Check the distribution of the variables\n<br>","bf4f0b99":"#### Support Vector Machine\n<br>","6baf1869":"### Model Fitting\n<br>","dad0eba5":"\n### Bivariate Analysis\n<br>","9688f1f4":"##### Descriptive statistics of the \"Malignant\" tumor  \n<br>","b68f307f":"#### Scalling the data","1c497c03":"##### Descriptive statistics of the \"Benign\" tumor\n<br>","2d4cb828":"##### Visualizing the model\n<br>","dd471a62":"#### Logistic Regression (Ridge)\n<br>","186b95fd":"#### Check the correlation\n<br>","19e4a1a8":"#### Check for missing values","335dd9e8":"#### <li>About 37.2 % (212) of the total patients (569) are malignant tumor.","4f2e936e":"The Gaussian distibution has a Skewness of 0 and Kurtosis of 3<br>\nLet's check the skewness and kurtosis of the variables","051ed5c9":"#### Check the distribution of the variables with respect to target \n<br>","45a94583":"#### Logistic Regression (Lasso)\n<br>","55eb7be8":"###### Cost-complexity prunning","089c87ce":"##### You can plot as check for as many features you want!!!\n<br>","545d4f20":"##### As we can clearly see our tree overfits the data.\n<br>","35f02e9c":"### Univariate Data Analysis\n<br>","9430df8f":"### Multivariat Data Analysis\n<br>","511bc84d":"#### Decision Trees\n<br>","b276510f":"#### Check the skewness and kurtosis of the columns\n<br>","03c80620":"Using cross-validation to find the optimal value of alpha\n<br>","b90e64d7":"#### Take a general look of the data","8e7e84af":"#### Outlier Handling\n<br>\n* We cap the points outside the 1.5 * IQR range.","b3c0e44c":"##### Descriptive statistics of the whole dataset\n<br>","1c0ea15c":"#### KNN Classification\n<br>","379c29c8":"#### Polynomial Logistic Regression\n<br>","4868e828":"#### Column : diagnosis\n<br>","587f678d":"#### Naive Bayes\n<br>"}}