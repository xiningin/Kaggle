{"cell_type":{"6adadfef":"code","82fa21fb":"code","a117f709":"code","91e7c89f":"code","341c2786":"code","084ce330":"code","c5dde0d2":"code","76ce2b4a":"code","a9f8e1b7":"code","df5afa76":"code","06f15e10":"code","2b960d4f":"code","ea081d5d":"code","02d75407":"code","d094e748":"code","38ef781f":"code","85d6516f":"code","7a0d35a6":"code","d3b5d9d0":"code","be9bdcd5":"code","428576f6":"code","d9f4615e":"code","6649758e":"code","9c929c30":"code","6ff84eee":"code","f44c4ca2":"code","ce316aa2":"code","52684832":"code","260eb775":"markdown","f450765f":"markdown","e3edf749":"markdown","fd600122":"markdown","a5dc8d3f":"markdown","934b47d0":"markdown","0978425d":"markdown","5bc4deda":"markdown"},"source":{"6adadfef":"import pandas as pd\nimport numpy as np\nfrom datetime import timedelta\nfrom tqdm import tqdm\nimport gc\nfrom functools import reduce\nfrom sklearn.model_selection import StratifiedKFold","82fa21fb":"def make_df(df, col, bool_in=False):\n    tp = df.loc[ ~df[col].isnull() ,[col]].copy()\n    df.drop(col, axis=1, inplace=True)\n    \n    tp[col] = tp[col].str.replace(\"null\",'\"\"')\n    if bool_in:\n        tp[col] = tp[col].str.replace(\"false\",'\"False\"')\n        tp[col] = tp[col].str.replace(\"true\",'\"True\"')\n    tp[col] = tp[col].apply(lambda x: eval(x) )\n    a = tp[col].sum()\n    gc.collect()\n    return pd.DataFrame(a)","a117f709":"ROOT_DIR = \"..\/input\/mlb-player-digital-engagement-forecasting\"","91e7c89f":"#=======================#\ndef flatten(df, col):\n    du = (df.pivot(index=\"playerId\", columns=\"EvalDate\", \n               values=col).add_prefix(f\"{col}_\").\n      rename_axis(None, axis=1).reset_index())\n    return du\n#============================#\ndef reducer(left, right):\n    return left.merge(right, on=\"playerId\")\n#========================","341c2786":"TGTCOLS = [\"target1\",\"target2\",\"target3\",\"target4\"]\ndef train_lag(df, lag=1):\n    dp = df[[\"playerId\",\"EvalDate\"]+TGTCOLS].copy()\n    dp[\"EvalDate\"]  =dp[\"EvalDate\"] + timedelta(days=lag) \n    df = df.merge(dp, on=[\"playerId\", \"EvalDate\"], suffixes=[\"\",f\"_{lag}\"], how=\"left\")\n    return df\n#=================================\ndef test_lag(sub):\n    sub[\"playerId\"] = sub[\"date_playerId\"].apply(lambda s: int(  s.split(\"_\")[1]  ) )\n    assert sub.date.nunique() == 1\n    dte = sub[\"date\"].unique()[0]\n    \n    eval_dt = pd.to_datetime(dte, format=\"%Y%m%d\")\n    dtes = [eval_dt + timedelta(days=-k) for k in LAGS]\n    mp_dtes = {eval_dt + timedelta(days=-k):k for k in LAGS}\n    \n    sl = LAST.loc[LAST.EvalDate.between(dtes[-1], dtes[0]), [\"EvalDate\",\"playerId\"]+TGTCOLS].copy()\n    sl[\"EvalDate\"] = sl[\"EvalDate\"].map(mp_dtes)\n    du = [flatten(sl, col) for col in TGTCOLS]\n    du = reduce(reducer, du)\n    return du, eval_dt\n    #\n#===============","084ce330":"%%time\n#tr = pd.read_csv(f\"{ROOT_DIR}\/train.csv\")\ntr = pd.read_csv(\"..\/input\/mlb-data\/target.csv\")\nprint(tr.shape)\ngc.collect()","c5dde0d2":"tr[\"EvalDate\"] = pd.to_datetime(tr[\"EvalDate\"])\ntr[\"EvalDate\"] = tr[\"EvalDate\"] + timedelta(days=-1)\ntr[\"EvalYear\"] = tr[\"EvalDate\"].dt.year","76ce2b4a":"MED_DF = tr.groupby([\"playerId\",\"EvalYear\"])[TGTCOLS].median().reset_index()\nMEDCOLS = [\"tgt1_med\",\"tgt2_med\", \"tgt3_med\", \"tgt4_med\"]\nMED_DF.columns = [\"playerId\",\"EvalYear\"] + MEDCOLS","a9f8e1b7":"MED_DF.head()","df5afa76":"LAGS = list(range(1,31))\nFECOLS = [f\"{col}_{lag}\" for lag in reversed(LAGS) for col in TGTCOLS]","06f15e10":"%%time\nfor lag in tqdm(LAGS):\n    tr = train_lag(tr, lag=lag)\n    gc.collect()\n#===========\ntr = tr.sort_values(by=[\"playerId\", \"EvalDate\"])\nprint(tr.shape)\ntr = tr.dropna()\nprint(tr.shape)\ntr = tr.merge(MED_DF, on=[\"playerId\",\"EvalYear\"])\ngc.collect()","2b960d4f":"tr.head()","ea081d5d":"X = tr[FECOLS+MEDCOLS].values\ny = tr[TGTCOLS].values\ncl = tr[\"playerId\"].values","02d75407":"NFOLDS = 5\nskf = StratifiedKFold(n_splits=NFOLDS)\nfolds = skf.split(X, cl)\nfolds = list(folds)","d094e748":"X.shape","38ef781f":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping","85d6516f":"def make_model(n_in):\n    inp = L.Input(name=\"inputs\", shape=(n_in,))\n    x = L.Dense(50, activation=\"relu\", name=\"d1\")(inp)\n    x = L.Dense(50, activation=\"relu\", name=\"d2\")(x)\n    preds = L.Dense(4, activation=\"linear\", name=\"preds\")(x)\n    \n    model = M.Model(inp, preds, name=\"ANN\")\n    model.compile(loss=\"mean_absolute_error\", optimizer=\"adam\")\n    return model","7a0d35a6":"net = make_model(X.shape[1])\nprint(net.summary())","d3b5d9d0":"oof = np.zeros(y.shape)\nnets = []\nEPOCHS  = 15\nfor idx in range(NFOLDS):\n    print(\"FOLD:\", idx)\n    tr_idx, val_idx = folds[idx]\n    ckpt = ModelCheckpoint(f\"w{idx}.h5\", monitor='val_loss', verbose=1, save_best_only=True,mode='min')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.0005)\n    es = EarlyStopping(monitor='val_loss', patience=6)\n    reg = make_model(X.shape[1])\n    reg.fit(X[tr_idx], y[tr_idx], epochs=EPOCHS, batch_size=30_000, \n            validation_data=(X[val_idx], y[val_idx]),\n            verbose=1, callbacks=[ckpt, reduce_lr, es])\n    reg.load_weights(f\"w{idx}.h5\")\n    oof[val_idx] = reg.predict(X[val_idx], batch_size=50_000, verbose=1)\n    nets.append(reg)\n    gc.collect()\n    #\n#","be9bdcd5":"mae = mean_absolute_error(y, oof)\nmse = mean_squared_error(y, oof, squared=False)\nprint(\"mae:\", mae)\nprint(\"mse:\", mse)","428576f6":"# Historical information to use in prediction time\nbound_dt = pd.to_datetime(\"2021-01-01\")\nLAST = tr.loc[tr.EvalDate>bound_dt].copy()","d9f4615e":"LAST_MED_DF = MED_DF.loc[MED_DF.EvalYear==2021].copy()\nLAST_MED_DF.drop(\"EvalYear\", axis=1, inplace=True)\ndel tr","6649758e":"LAST.shape, LAST_MED_DF.shape, MED_DF.shape","9c929c30":"#nets[0].summary()","6ff84eee":"#\"\"\"\nimport mlb\nFE = []; SUB = [];\nenv = mlb.make_env() # initialize the environment\niter_test = env.iter_test() # iterator which loops over each date in test set\n\nfor (test_df, sub) in iter_test:\n    # Features computation at Evaluation Date\n    sub = sub.reset_index()\n    sub_fe, eval_dt = test_lag(sub)\n    sub_fe = sub_fe.merge(LAST_MED_DF, on=\"playerId\", how=\"left\")\n    sub_fe = sub_fe.fillna(0.)\n    \n    _preds = 0.\n    for reg in nets:\n        _preds += reg.predict(sub_fe[FECOLS + MEDCOLS]) \/ NFOLDS\n    sub_fe[TGTCOLS] = np.clip(_preds, 0, 100)\n    sub.drop([\"date\"]+TGTCOLS, axis=1, inplace=True)\n    sub = sub.merge(sub_fe[[\"playerId\"]+TGTCOLS], on=\"playerId\", how=\"left\")\n    sub.drop(\"playerId\", axis=1, inplace=True)\n    sub = sub.fillna(0.)\n    # Submit\n    env.predict(sub)\n    # Update Available information\n    sub_fe[\"EvalDate\"] = eval_dt\n    #sub_fe.drop(MEDCOLS, axis=1, inplace=True)\n    LAST = LAST.append(sub_fe)\n    LAST = LAST.drop_duplicates(subset=[\"EvalDate\",\"playerId\"], keep=\"last\")\n#\"\"\"","f44c4ca2":"sub.head()","ce316aa2":"LAST.shape, sub_fe.shape","52684832":"#df_tr[\"dte\"] = pd.to_datetime(df_tr[\"date\"], format='%Y%m%d')","260eb775":"## Neural Net Training\n\n![image.png](attachment:270a4bbf-5fe5-4964-8ff9-b156cc07502a.png)","f450765f":"![image.png](attachment:ad0080b7-8364-4c8b-b1d3-353427648498.png)","e3edf749":"## UTILITY FUNCTIONS","fd600122":"I think this model is simple. but its effective.","a5dc8d3f":"\u7814\u7a76\u3057\u3066\u3044\u308b\u306e\u3067\u3057\u3070\u3089\u304f\u304a\u5f85\u3061\u304f\u3060\u3055\u3044\u3002\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u3092\u691c\u7d22\u3059\u308b\u3068\u3001\u534a\u5e74\u3067\u91d1\u30e1\u30c0\u30eb\u3068\u308a\u307e\u3057\u305f\u3001\u307f\u305f\u3044\u306a\u8a18\u4e8b\u304c\u3044\u308d\u3044\u308d\u3067\u3066\u304f\u308b\u306e\u3067\u3059\u304c\u3002\u3002\u3002<br>\n\u79c1\u306b\u306f\u307e\u3063\u305f\u304f\u624b\u3054\u305f\u3048\u304c\u3042\u308a\u307e\u305b\u3093\u3002<br>\n\n\u30b3\u30f3\u30da\u306e\u4e00\u756a\u512a\u3057\u3044\u3082\u306e\u3092\u9078\u629e\u3057\u3066\u305d\u308c\u306b\u96c6\u4e2d\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u307e\u3059\u3002<br>\n\u3057\u304b\u3057\u3001\u5168\u90e8\u30e0\u30ba\u30ab\u30b7\u30a4\u3002\u3069\u3046\u3057\u3088\u3046\u3082\u306a\u3044\u306e\u3067\u3001\u5168\u90e8\u624b\u3092\u3060\u3057\u3066\u7d5e\u3063\u3066\u3044\u3053\u3046\u3068\u304a\u3082\u3063\u3066\u3044\u307e\u3059\u3002<br>\n\nMLB\u306e\u30b3\u30f3\u30da\u306f\u3053\u308c\u306f\uff12\u3064\u76ee\u306e\u30b3\u30fc\u30c9\u306a\u306e\u3067\u3059\u304c\u3001\u624b\u6cd5\u304c\u9055\u3046\u3088\u3046\u3067\u3059\u3002<br>\n\u3053\u306e\u30b3\u30fc\u30c9\u306f\u3001\u3057\u3070\u3089\u304f\u304a\u3044\u3066\u304a\u3053\u3046\u304b\u306a<br>\n\u611a\u75f4\u66f8\u3044\u3066\u3001\u4eca\u65e5\u306f\u7d42\u308f\u308a\u3068\u3044\u3046\u3053\u3068\u3067\u3002\n","934b47d0":"\u30df\u30c3\u30b7\u30e7\u30f3\uff1a\u3053\u306e\u30b3\u30f3\u30c6\u30b9\u30c8\u3067\u306f\u3001\u5c06\u6765\u306e\u65e5\u4ed8\u7bc4\u56f2\u3067\u30d5\u30a1\u30f3\u304cMLB\u30d7\u30ec\u30fc\u30e4\u30fc\u306e\u30c7\u30b8\u30bf\u30eb\u30b3\u30f3\u30c6\u30f3\u30c4\u306b\u6bce\u65e5\u3069\u306e\u3088\u3046\u306b\u95a2\u4e0e\u3059\u308b\u304b\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002 \u30d7\u30ec\u30fc\u30e4\u30fc\u306e\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u30c7\u30fc\u30bf\u3001\u30bd\u30fc\u30b7\u30e3\u30eb\u30e1\u30c7\u30a3\u30a2\u30c7\u30fc\u30bf\u3001\u5e02\u5834\u898f\u6a21\u306a\u3069\u306e\u30c1\u30fc\u30e0\u8981\u56e0\u306b\u30a2\u30af\u30bb\u30b9\u3067\u304d\u307e\u3059\u3002 \u6210\u529f\u3057\u305f\u30e2\u30c7\u30eb\u306f\u3001\u3069\u306e\u30b7\u30b0\u30ca\u30eb\u304c\u30a8\u30f3\u30b2\u30fc\u30b8\u30e1\u30f3\u30c8\u3068\u6700\u3082\u5f37\u304f\u76f8\u95a2\u3057\u3001\u5f71\u97ff\u3092\u4e0e\u3048\u308b\u304b\u306b\u3064\u3044\u3066\u306e\u65b0\u3057\u3044\u6d1e\u5bdf\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002","0978425d":"### CREDITS\nhttps:\/\/www.kaggle.com\/ulrich07\/mlb-ann-with-lags-tf-keras\nthanks. I copied your code.","5bc4deda":"![image.png](attachment:5f5bd586-49b8-4d97-a5de-b6cfb0402353.png)"}}