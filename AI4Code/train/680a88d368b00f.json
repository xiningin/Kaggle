{"cell_type":{"f3ec5f81":"code","cb870afa":"code","afd46ee7":"code","216c100b":"code","94976fd3":"code","6baf632f":"code","9035f28c":"code","a0fa7b5e":"code","6048edf5":"code","40ca2c47":"code","158a1108":"code","a576cefc":"code","4f1b65ea":"code","7de9c5eb":"code","07049656":"code","f9f0803c":"code","3a830e60":"code","99b07942":"code","4b1c2342":"code","d27f027d":"code","48f6a8e3":"code","d26ad305":"code","59c707b7":"code","869c54bf":"code","b843c412":"code","7f09c81c":"code","0cac6550":"code","7df36143":"code","e0d71102":"code","1b003304":"code","f7eecc38":"code","0086200a":"code","a227a231":"code","a5a6cecb":"code","857369e0":"code","6f4a3069":"code","0281f5c7":"code","570e3924":"code","09cbac13":"code","704ead7a":"code","6fb0b094":"code","aa92ad91":"code","93faf425":"code","16a46b33":"code","c124f759":"code","1c0a48d3":"code","71f7131e":"code","80ec8def":"code","b4408d94":"code","73e19893":"code","874f2fd3":"code","da908c23":"code","0d64cf11":"code","c99a11a3":"markdown","2e6d68ae":"markdown","c14f07a1":"markdown","c863849d":"markdown","66592f48":"markdown","b7a0f54a":"markdown","b3077118":"markdown","33b20b7c":"markdown","f0e3e270":"markdown","da766a3d":"markdown","57d2bd84":"markdown","7a2e4fdf":"markdown","c9beaeec":"markdown","27ab388a":"markdown","61616cad":"markdown","57dd60d4":"markdown","b0fc856c":"markdown","506ddb2b":"markdown","66e4b798":"markdown","93840d9c":"markdown","d46a4efd":"markdown","498f2030":"markdown","0084dbf1":"markdown","3ff7ccec":"markdown","5f28c32c":"markdown","ac6d0815":"markdown","105c342b":"markdown","0aa49479":"markdown","37c93682":"markdown","25908834":"markdown"},"source":{"f3ec5f81":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score \nfrom sklearn.model_selection import cross_validate, cross_val_predict\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn import metrics, utils\n\nimport xgboost as xgb\n\nimport pandas_profiling\n\n%matplotlib inline\n\nseed = 42\n\nimport warnings\nwarnings.filterwarnings('ignore')","cb870afa":"red = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","afd46ee7":"red.head()","216c100b":"red.profile_report()","94976fd3":"bins = [2, 5.5, 8]\nlabels = [\"bad\", \"good\"]\nred['quality_cat'] = pd.cut(red['quality'], bins=bins, labels=labels)","6baf632f":"red[\"quality_cat\"].value_counts()","9035f28c":"print(f\"Percentage of quality scores\")\nred[\"quality_cat\"].value_counts(normalize=True)*100","a0fa7b5e":"red[\"quality_cat\"].value_counts().plot.pie(autopct='%1.2f%%');","6048edf5":"predict_columns = red.columns[:-2]\npredict_columns","40ca2c47":"X = red[predict_columns]\ny = red[\"quality_cat\"]","158a1108":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    random_state=42, \n                                                    test_size=0.2)","a576cefc":"clf_dummy = DummyClassifier(strategy=\"uniform\", random_state=seed) # Random prediction\nclf_dummy.fit(X_train, y_train)","4f1b65ea":"cross_val_score(clf_dummy, X_train, y_train, cv=3, \n                scoring=\"accuracy\", n_jobs=-1).mean()","7de9c5eb":"# Always predict the most frequent class\nclf_dummy = DummyClassifier(strategy=\"most_frequent\", random_state=seed) \nclf_dummy.fit(X_train, y_train)","07049656":"cross_val_score(clf_dummy, X_train, y_train, cv=3, scoring=\"accuracy\", n_jobs=-1).mean()","f9f0803c":"preds = cross_val_predict(clf_dummy, X_train, y_train, cv=3, n_jobs=-1)","3a830e60":"conf_mx = metrics.confusion_matrix(y_train, preds)\nconf_mx","99b07942":"pd.crosstab(y_train, preds, rownames = ['Actual'], colnames =['Predicci\u00f3n'])","4b1c2342":"fig = plt.figure(figsize=(6,5))\nax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", \n                 xticklabels=clf_dummy.classes_,\n                 yticklabels=clf_dummy.classes_,)","d27f027d":"accuracy_base = metrics.accuracy_score(y_train, preds)\nprecision_base = metrics.precision_score(y_train, preds, \n                                         average='weighted', \n                                         zero_division=0)\nrecall_base = metrics.recall_score(y_train, preds, \n                                   average='weighted')\nf1_base = metrics.f1_score(y_train, preds, \n                           average='weighted')\nprint(f\"Accuracy: {accuracy_base}\")\nprint(f\"Precision: {precision_base}\")\nprint(f\"Recall: {recall_base}\")\nprint(f\"f1: {f1_base}\")","48f6a8e3":"print(metrics.classification_report(y_train, preds, zero_division=0))","d26ad305":"def evaluate_model(estimator, X_train, y_train, cv=5, verbose=True):\n    \"\"\"Print and return cross validation of model\n    \"\"\"\n    scoring = {\"accuracy\": \"accuracy\",\n               \"precision\": \"precision_weighted\",\n               \"recall\": \"recall_weighted\",\n               \"f1\": \"f1_weighted\"}\n    scores = cross_validate(estimator, X_train, y_train, cv=cv, scoring=scoring)\n    \n    accuracy, accuracy_std = scores['test_accuracy'].mean(), \\\n                                scores['test_accuracy'].std()\n    \n    precision, precision_std = scores['test_precision'].mean(), \\\n                                scores['test_precision'].std()\n    \n    recall, recall_std = scores['test_recall'].mean(), \\\n                                scores['test_recall'].std()\n    \n    f1, f1_std = scores['test_f1'].mean(), scores['test_f1'].std()\n\n    \n    result = {\n        \"Accuracy\": accuracy,\n        \"Accuracy std\": accuracy_std,\n        \"Precision\": precision,\n        \"Precision std\": precision_std,\n        \"Recall\": recall,\n        \"Recall std\": recall_std,\n        \"f1\": f1,\n        \"f1 std\": f1_std,\n    }\n    \n    if verbose:\n        print(f\"Accuracy: {accuracy} - (std: {accuracy_std})\")\n        print(f\"Precision: {precision} - (std: {precision_std})\")\n        print(f\"Recall: {recall} - (std: {recall_std})\")\n        print(f\"f1: {f1} - (std: {f1_std})\")\n\n    return result","59c707b7":"models = [GaussianNB(), KNeighborsClassifier(), RandomForestClassifier(random_state=seed),\n          DecisionTreeClassifier(random_state=seed), ExtraTreeClassifier(random_state=seed), \n          AdaBoostClassifier(random_state=seed), GradientBoostingClassifier(random_state=seed), \n          xgb.XGBClassifier()]\n\nmodel_names = [\"Naive Bayes Gaussian\", \"K Neighbors Classifier\", \"Random Forest\",\n               \"Decision Tree\", \"Extra Tree\", \"Ada Boost\", \n               \"Gradient Boosting\", \"XGBoost\"]","869c54bf":"accuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nfor model in range(len(models)):\n    print(f\"Step {model+1} de {len(models)}\")\n    print(f\"...running {model_names[model]}\")\n    \n    clf_scores = evaluate_model(models[model], X_train, y_train)\n    \n    accuracy.append(clf_scores[\"Accuracy\"])\n    precision.append(clf_scores[\"Precision\"])\n    recall.append(clf_scores[\"Recall\"])\n    f1.append(clf_scores[\"f1\"])","b843c412":"df_result = pd.DataFrame({\"Model\": model_names,\n                          \"accuracy\": accuracy,\n                          \"precision\": precision,\n                          \"recall\": recall,\n                          \"f1\": f1})\ndf_result.sort_values(by=\"f1\", ascending=False)","7f09c81c":"metrics_list = [\"f1\", \"accuracy\", \"precision\", \"recall\"]\n\nfor metric in metrics_list:\n    df_result.sort_values(by=metric).plot.barh(\"Model\", metric)\n    plt.title(f\"Model by {metric}\")\n    plt.show()","0cac6550":"clf_rf = RandomForestClassifier(random_state=seed)\npreds = cross_val_predict(clf_rf, X_train, y_train, cv=5, n_jobs=-1)","7df36143":"clf_rf.get_params()","e0d71102":"pd.crosstab(y_train, preds, rownames = ['Real'], colnames =['Predicted'])","1b003304":"print(metrics.classification_report(y_train, preds, zero_division=0))","f7eecc38":"param_grid = [\n    {\"n_estimators\": range(20, 200, 20), \n     \"bootstrap\": [True, False],\n     \"criterion\": [\"gini\", \"entropy\"],   \n     \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None],\n     \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n     \"min_samples_split\": [2, 5, 10],\n     \"min_samples_leaf\": [1, 2, 4],\n     }\n]\n\nclf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)","0086200a":"clf_random = RandomizedSearchCV(clf_rf, param_grid, n_iter = 200, cv = 5, \n                                scoring=\"f1_weighted\", verbose=2, \n                                random_state=seed, n_jobs = -1)","a227a231":"clf_random.fit(X_train, y_train)","a5a6cecb":"clf_random.best_params_","857369e0":"preds = cross_val_predict(clf_random.best_estimator_, \n                          X_train, y_train, \n                          cv=5, n_jobs=-1)\nprint(metrics.classification_report(y_train, preds, zero_division=0))","6f4a3069":"param_grid = [\n    {\"n_estimators\": range(20, 80, 10), \n     \"bootstrap\": [True, False],\n     \"criterion\": [\"gini\", \"entropy\"],   \n     \"max_depth\": [2, 4, 6, 8, 10, 12, 14, None],\n     \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n     \"min_samples_split\": [2, 5, 10],\n     \"min_samples_leaf\": [1, 2, 4],\n     }\n]\n\nclf_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)","0281f5c7":"grid_search = GridSearchCV(clf_rf, param_grid, cv=5,\n                           scoring=\"f1_weighted\", verbose=2, n_jobs=-1)","570e3924":"grid_search.fit(X_train, y_train)","09cbac13":"grid_search.best_params_","704ead7a":"final_model = grid_search.best_estimator_\npreds = cross_val_predict(final_model, X_train, y_train, cv=5, n_jobs=-1)","6fb0b094":"pd.crosstab(y_train, preds, rownames = ['Real'], colnames =['Predicted'])","aa92ad91":"print(metrics.classification_report(y_train, preds))","93faf425":"y_pred = final_model.predict(X_test)","16a46b33":"pd.crosstab(y_test, y_pred, rownames = ['Real'], colnames =['Predicted'])","c124f759":"print(metrics.classification_report(y_test, y_pred, zero_division=0))","1c0a48d3":"conf_mx = metrics.confusion_matrix(y_test, y_pred)","71f7131e":"fig = plt.figure(figsize=(8,8))\nax = sns.heatmap(conf_mx, annot=True, fmt=\"d\", \n                 xticklabels=final_model.classes_,\n                 yticklabels=final_model.classes_,)","80ec8def":"feature_importances = final_model.feature_importances_\nfeature_importances","b4408d94":"sorted(zip(feature_importances, X_test.columns), reverse=True)","73e19893":"feature_imp = pd.Series(feature_importances, index=X_train.columns).sort_values(ascending=False)\nfeature_imp.plot(kind='bar')\nplt.title('Feature Importances');","874f2fd3":"selector = RFECV(final_model, step=1, cv=StratifiedKFold())\nselector = selector.fit(X_train, y_train)\npd.DataFrame({\"Feature\": predict_columns, \"Support\": selector.support_})","da908c23":"plt.figure()\nplt.xlabel(\"No. of features selected\")\nplt.ylabel(\"Cross validation scores\")\nplt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\nplt.show()","0d64cf11":"selector.grid_scores_","c99a11a3":"# Fine-Tune\n\nWe are going to do a hyperparameter adjustment to see if any improvement is achieved.","2e6d68ae":"# Select and train models","c14f07a1":"We are going to use RFECV to determine the number of valid features with cross-validation.","c863849d":"The conclusion is that all the variables are important for the model, since the maximum score is obtained with the 10 selected features.","66592f48":"Our model is correct 81% of the time (precision) and detects 81% of the actual scores (recall). The F1 score is 0.81. Well, it has improved our baseline significantly (remember, precision = 28%, recall = 53%, and F1 = 0.37).","b7a0f54a":"The best performing model is Random Forest. Let's examine the execution of Random Forest a little more in detail:","b3077118":"We are not going to delve into data exploration, as we already did in the [first part of this analysis](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-regression). The only preprocessing that we are going to perform is to convert the target variable `quality` to categorical, indicating whether the score is good or not: `bad`: scores less than or equal to 5. `good`: scores greater than or equal to 6.","33b20b7c":"The goal of this phase is to train many models quickly and unrefined, of different categories (i.e. Random Forests, AdaBoost, Extra Tree, etc.) using the standard parameters. The idea is to have a quick overview of which models are most promising. Measure and compare the performance of all of them. Select the best models.\n\nCreate the predictor dataset and the dataset with the target variable:","f0e3e270":"As we can see, a classifier that predicts randomly obtains an accuracy of 53%.","da766a3d":"## Conclusions\n\nThis analysis has addressed the issue as a problem of supervised learning binary classification. Our starting baseline, obtained from a classifier that always predicts the most frequent class, is the following:\n\n+ Precision: **28%**\n+ Recall: **53%**\n+ Accuracy: **53%**\n+ f1: **0.37**\n\nAfter training various models, the one that has provided the best results is RandomForest. After fine-tuning the hyperparameters we obtain the following metrics:\n\n+ Precision: **82%**\n+ Recall: **82%**\n+ Accuracy: **82%**\n+ f1: **0.82**\n\nThe performance in the test set is as follows:\n\n+ Precision: **78%**\n+ Recall: **78%**\n+ Accuracy: **78%**\n+ f1: **0.78**\n\nAll predictor variables are relevant to the model. The three that most affect prediction are the following:\n\n+ alcohol\n+ sulphates\n+ volatile acidity","57d2bd84":"## Initial fine-tune with Randomized Search\n\nFirst we do a random quick sweep:","7a2e4fdf":"![](https:\/\/cdn.pixabay.com\/photo\/2016\/03\/09\/11\/53\/wine-glasses-1246240_1280.jpg)","c9beaeec":"# Feature Selection","27ab388a":"# Explore the data","61616cad":"In [the first part of this analysis](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-regression) we approach the problem as supervised learning - regression. The resulting model was not satisfactory to us. [In a second analysis](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-multiclass-classification), we approach the problem as supervised learning - multiclass classification. Given that the data we had was so unbalanced (the quality scores were concentrated on scores 5 or 6) the performance of our best model (RandomForest) was not quite good. In this third and final part, we will focus our analysis on a **supervised learning - binary classification** problem.","57dd60d4":"## Baseline\n\nFirst, we are going to train a dummy classifier that we will use as a baseline with which to compare.","b0fc856c":"# Imports","506ddb2b":"# Wine Quality Prediction - Part 3 - Binary Classification","66e4b798":"It's correct 78% of the time (precision) and detects 78% of the actual scores (recall). The F1 score is 0.78. Which significantly improves our baseline (remember, precision = 28%, recall = 53%, and F1 = 0.37).","93840d9c":"This notebook is part of a trilogy in which I will approach the wine quality dataset from several different approaches:\n\n+ [Part 1: Supervised Learning - Regression](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-regression)\n+ [Part 2: Supervised Learning - Multiclass Classification](https:\/\/www.kaggle.com\/sgtsteiner\/red-wine-quality-multiclass-classification)\n+ Part 3: Supervised Learning - Binary Classification","d46a4efd":"A classifier that always predicts the most frequent class (in our case the `good` quality) also obtains an accuracy of 53%. We are going to take the prediction of this dummy classifier as our baseline.","498f2030":"Create the training and test datasets:","0084dbf1":"# Feature importances","3ff7ccec":"Let's see the performance of each of them:","5f28c32c":"After adjusting hyperparameters, a very slight improvement is achieved over the default hyperparameters. It's correct 82% of the time (precision) and detects 82% of the actual scores (recall). The F1 score is 0.82. Which significantly improves our baseline (remember, precision = 28%, recall = 53%, and F1 = 0.37).\n\nFinally let's see how it runs on the test set:","ac6d0815":"Our dummy classifier is correct only 28% of the time (precision) and detects 53% of the actual scores (recall). It is often convenient to combine precision and sensitivity into a single metric called the F1 score, particularly if we need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and sensitivity. While the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only score high on F1 if both sensitivity and precision are high. In our case, F1 = 0.37. Okay, let's take these three metrics as our initial baseline.","105c342b":"We are going to visualize the comparison of the different models \/ metrics:","0aa49479":"## Final fine-tune with GridSearch","37c93682":"# Get the data","25908834":"## Shortlist Promising Models"}}