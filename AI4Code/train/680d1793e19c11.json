{"cell_type":{"52d4769b":"code","1b60f6e1":"code","6520b47b":"code","7cb42844":"code","9a6404a1":"code","bcccfdd8":"code","8d8f299a":"code","32717a3d":"code","308b834e":"code","a384d5c2":"code","af4810bd":"code","c7d5329f":"code","324910f8":"code","bb9844bc":"code","62203026":"code","29b68348":"code","1b23cf95":"code","aab473a4":"code","b41d03ba":"code","7d3bebe5":"code","9fb6d3f6":"code","3fb8600a":"code","f1a769a9":"code","d49bad24":"code","050a2cd9":"code","d6b54783":"code","1ca5b1fe":"code","bf504e78":"code","6150ecdf":"code","afd165e7":"code","350f710b":"code","9314369b":"code","d1223400":"code","216d7b2c":"code","1c709b5f":"code","02ed500d":"code","ffb994b8":"code","a8272a2f":"code","a29fb078":"code","501da2b3":"code","8f20d62e":"code","24219c85":"code","7be6baa2":"code","6b3f5ed7":"code","30bcfd29":"code","d5df2e81":"code","843d5cba":"code","945bde6d":"code","fd51d6d8":"code","98766a19":"markdown","91ce1b84":"markdown","122976e0":"markdown","6d24aca1":"markdown","eb0456f2":"markdown","0ece7c49":"markdown","cb3fbf7b":"markdown","d3a08828":"markdown","aee50b05":"markdown","906442c0":"markdown","4ef35d08":"markdown","351e1c1f":"markdown","6ba526db":"markdown","5d291284":"markdown","c14bcc55":"markdown","ac07090b":"markdown","21957175":"markdown","f6cca37b":"markdown","c02ee928":"markdown","95b2b5e8":"markdown","cfdc589b":"markdown","8deda337":"markdown","31b25d19":"markdown","773bb265":"markdown","4770c5d5":"markdown","1343ba3d":"markdown","8a79c99f":"markdown","9fa96bcc":"markdown","a7b4e102":"markdown","7408ce65":"markdown","6c984a0c":"markdown","72023fc6":"markdown","9ba2bf80":"markdown","7505c8c3":"markdown","1fd14986":"markdown","d41c9827":"markdown","8bbc0940":"markdown","9c316655":"markdown","5aaf978a":"markdown","d4d17de2":"markdown","8c9197db":"markdown","3e814c93":"markdown","30d3194c":"markdown","3a5bcfcf":"markdown"},"source":{"52d4769b":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Import all important libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","1b60f6e1":"#Load the dataset and check initial entries of the dataset\ndf=pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\ndf.head()","6520b47b":"#Shape of the dataset\ndf.shape","7cb42844":"#Information Summary of the dataset\ndf.info()","9a6404a1":"#Checking for Null Values\ndf.isnull().sum()","bcccfdd8":"fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\nfig.suptitle('Outlier Analysis')\nsns.boxplot(ax=axes[0, 0], data=df['age'])\naxes[0, 0].set_title('Age')\nsns.boxplot(ax=axes[0, 1], data=df['bmi'])\naxes[0, 1].set_title('BMI')\nsns.boxplot(ax=axes[1, 0], data=df['children'])\naxes[1, 0].set_title('Children')\nsns.boxplot(ax=axes[1, 1], data=df['charges'])\naxes[1, 1].set_title('Charges')\n\nplt.show()","8d8f299a":"plt.figure(figsize=(10,4))\nplt.title('Effect of Sex on Charges')\nsns.boxplot(x='sex',y='charges',data=df)\nplt.show()","32717a3d":"plt.figure(figsize=(10,4))\nplt.title('Effect of Age on Charges')\nsns.scatterplot(x='age',y='charges',data=df)\nplt.show()","308b834e":"plt.figure(figsize=(10,4))\nplt.title('Effect of Smokers on Charges')\nsns.boxplot(x='smoker',y='charges',data=df,hue='sex',palette='viridis')\nplt.show()","a384d5c2":"plt.figure(figsize=(10,4))\nplt.title('Effect of Regions and sex on Charges')\nsns.boxplot(x='region',y='charges',data=df,hue='sex',palette='viridis')\nplt.show()","af4810bd":"plt.figure(figsize=(10,4))\nplt.title('Effect of Regions and smokers on Charges')\nsns.boxplot(x='region',y='charges',data=df,hue='smoker',palette='viridis')\nplt.show()","c7d5329f":"# Lets check the correlation of different variables\nsns.pairplot(df)\nplt.show()","324910f8":"#Heatmap of variables to check correlation between variables\ncorrMatt = df.corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(corrMatt, mask=mask,cmap='viridis', square=True,annot=True)\nplt.show()","bb9844bc":"#First few lines of data\ndf.head()","62203026":"# Lets convert Sex and Smoker as binary categorical variables(Male: 1, Female: 0  & Smoker_yes: 1 , Smoker_no: 0)\ndf.sex=df.sex.apply(lambda x: 1 if x=='male' else 0)\ndf.smoker=df.smoker.apply(lambda x: 1 if x=='yes' else 0)\ndf.head()","29b68348":"# Lets convert region as dummy variables\nregion = pd.get_dummies(df['region'], drop_first = True,prefix='region')\ndf = pd.concat([df, region], axis = 1)\n\n#Dropping season variable\ndf.drop('region',axis=1,inplace=True)\ndf.head()","1b23cf95":"# Lets split the data into Training and testing sets (70%-30% combination)\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","aab473a4":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['charges', 'age', 'bmi']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","b41d03ba":"y_train = df_train.pop('charges')\nX_train = df_train","7d3bebe5":"# Importing LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\n# Fitting LinearRegression onto the train data\nlm = LinearRegression()\nlm.fit(X_train, y_train)","9fb6d3f6":"num_vars = ['charges', 'age', 'bmi']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","3fb8600a":"y_test = df_test.pop('charges')\nX_test = df_test","f1a769a9":"# Making predictions using the third model\ny_pred = lm.predict(X_test)","d49bad24":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\n# Plot heading \nfig.suptitle('y_test vs y_pred', fontsize = 20) \n# X-label\nplt.xlabel('y_test', fontsize = 18) \n# y-label\nplt.ylabel('y_pred', fontsize = 16)\nplt.show()","050a2cd9":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","d6b54783":"# Lets split the data into Training and testing sets (70%-30% combination)\nfrom sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","1ca5b1fe":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['charges', 'age', 'bmi']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","bf504e78":"y_train = df_train.pop('charges')\nX_train = df_train","6150ecdf":"#Lets have a backup of X_train data\nX_train_bc=X_train.copy()","afd165e7":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm.summary())","350f710b":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9314369b":"#dropping sex variable\nX_train_bc = X_train_bc.drop(['sex'], axis=1)","d1223400":"#Rebuilding second model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm2 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm2.summary())","216d7b2c":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1c709b5f":"#dropping region_northwest variable\nX_train_bc = X_train_bc.drop(['region_northwest'], axis=1)","02ed500d":"#Rebuilding third model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm3 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm3.summary())","ffb994b8":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a8272a2f":"#dropping region_southeast variable\nX_train_bc = X_train_bc.drop(['region_southeast'], axis=1)","a29fb078":"#Rebuilding fourth model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm4 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm4.summary())","501da2b3":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8f20d62e":"#dropping region_southwest variable\nX_train_bc = X_train_bc.drop(['region_southwest'], axis=1)","24219c85":"#Rebuilding fifth model\n\n# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_new = sm.add_constant(X_train_bc)\n\n# Running the linear model\nlm5 = sm.OLS(y_train,X_train_new).fit()\n\n#Let's see the summary of our linear model\nprint(lm5.summary())","7be6baa2":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_bc\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6b3f5ed7":"num_vars = ['charges', 'age', 'bmi']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])\n\ndf_test.describe()","30bcfd29":"y_test = df_test.pop('charges')\nX_test = df_test","d5df2e81":"# Adding constant variable to test dataframe\nimport statsmodels.api as sm \nX_test_m5 = sm.add_constant(X_test)\n\n# Creating X_test_m5 dataframe by dropping variables from X_test_m5\nX_test_m5 = X_test_m5.drop(['sex', 'region_northwest', 'region_southeast', 'region_southwest'], axis = 1)\n\n# Making predictions using the fifth model\ny_pred = lm5.predict(X_test_m5)","843d5cba":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\n# Plot heading \nfig.suptitle('y_test vs y_pred', fontsize = 20) \n# X-label\nplt.xlabel('y_test', fontsize = 18) \n# y-label\nplt.ylabel('y_pred', fontsize = 16)\nplt.show()","945bde6d":"#Model parameters\nround(lm5.params,3)","fd51d6d8":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","98766a19":"**Observation:** After leaving out `region_southwest` variable, we now have a model which is free of multicolinearity (all VIFs are less than 5) and all the remaining variables are significant (p-values are less than 0.03). We can also observe that Adj. R-squared has only fractionally come down(by 0.001). Now we have a efficient model. Lets use this as final model and predict the charges for test data.","91ce1b84":"### 3.1 Bivariate Analysis","122976e0":"**Observation:** We can observe some kind of linear relationship between `age` and `charges`","6d24aca1":"**Observation:** We now have a model with r2_score of **77.7%** which is not bad. Lets now try to check the possibility of building mode efficient model(devoid of insignificant variables and multicolinearity)","eb0456f2":"We see that charges, age and BMI variables are in larger scale compared to other. Lets scale Training data sets using minmax scaler","0ece7c49":"### 4.4 Dividing into X and Y sets for the model building","cb3fbf7b":"### 6.1 Splitting the Data into Training and Testing Sets","d3a08828":"**Observation:** \n- Smokers are spending more in hospital. \n- Majority of spendings by non-smokers between males and females are in the similar rages. Females spend fractionally higher\n- However among smokers males endup spending more in hospital","aee50b05":"## 6. Building the Model - Considering significant variables and avoiding Multicolinearity if any","906442c0":"Lets use stats model library for its great statistical output","4ef35d08":"### 3.2 Multivariate Analysis","351e1c1f":"## 4. Preparation of dataset for Model Building","6ba526db":"**Observation:** We can see that after dropping `sex` we have no alarming multicolinearity in the model(all VIFs are less than 5). We can also see that Adj. R-squared has not changed. We still have `region_northwest` & `region_southeast` as insignificant (p-value more than 0.03). We shall drop `region_northwest` and rebuild the model","5d291284":"**Observation:** \n- People in Southeast spend more on healthcare compared to other regions\n- Irrespective of regions generally its males who are spending more in hospitals\n- Again irrespective of regions its smokers who spend heavily in hospitals. Here as well Southeast region has upper hand over other regions","c14bcc55":"### 4.2 Splitting the Data into Training and Testing Sets","ac07090b":"### 6.6 Model Evaluation","21957175":"### 2.1 Checking for Outliers","f6cca37b":"## Introduction\nTo make profit, insurance companies should collect higher premium than the amount paid to the insured person. Due to this, insurance companies invests a lot of time, effort, and money in creating models that accurately predicts health care costs. In this kernel, I will try to build the most accurate model as possible but at the same time I would keep everything simple.","c02ee928":"# Insurance Forecast by using Linear Regression - Ganesh Nagappa Shetty","95b2b5e8":"### 4.1 Handling Categorical Variables","cfdc589b":"#### 5.2.3 Model Evaluation","8deda337":"### 6.3  Dividing into X and Y sets for the model building","31b25d19":"### 5.1 Fitting Linear regression model onto Train data","773bb265":"#### 6.5.2 Dividing into X_test and y_test","4770c5d5":"**Observation:** There no missing values in the dataset. Lets check for outliers in the dataset","1343ba3d":"**Observation:** We can see that after dropping `region_southeast` we have no alarming multicolinearity in the model(all VIFs are less than 5). We can also see that Adj. R-squared has not changed. We still have `region_southwest` as insignificant (p-value more than 0.03). We shall drop `region_southwest` and rebuild the model","8a79c99f":"## 1. Reading and Understanding the Data\n\nLet us first import necessary libraries, dataset and try to understand the data","9fa96bcc":"## 3. Exploratory Data Analysis","a7b4e102":"**Observation:** Males are spending more than females for healthcare. Another obvious observation is healthcare expenditure is continuously increasing with age","7408ce65":"### 6.5 Making Predictions Using the Final Model","6c984a0c":"**Observation:** We can see that after dropping `region_northwest` we have no alarming multicolinearity in the model(all VIFs are less than 5). We can also see that Adj. R-squared has not changed. We still have `region_southeast` as insignificant (p-value more than 0.03). We shall drop `region_southeast` and rebuild the model","72023fc6":"### 6.2 Rescaling the Features","9ba2bf80":"Lets build the model with all variables first and then compare the performance with the model with eliminated features","7505c8c3":"## 2. Cleaning the data","1fd14986":"### 4.3 Rescaling the Features","d41c9827":"## 5. Building the Model - Considering all variables","8bbc0940":"**Observation:** There are no outliers in the numerical variables of the dataset. The datapoints beyond 75th percentile in Charges and BMI are continuous in nature. This is quite clean dataset without outliers.","9c316655":"### 6.4 Fitting Linear regression model onto Train data","5aaf978a":"**Conclusion:** The model now has fewer variables (4 insignificant variables are left out). We now have a R2_squared value of **78%**. \n\nThis model has a R2_square value marginally better than previous model (With all variables). Now the model has no insignificant variables hence making it cost and time effective.\n\nThe final equation of the model is\n\n$ charges = -0.043 + 0.191  \\times  age + 0.165  \\times  bmi + 0.007 \\times children + 0.383 \\times smoker $ \n\n**`Smoker`** turned out to be the most significant variable in deciding hospital charges","d4d17de2":"**Observation:** As we can see `sex` with p-value of 1.00 (much higher than 0.03) is highly insignificant. Lets drop this variable and rebuild the model again","8c9197db":"#### 6.5.1 Applying the scaling on the test sets. Only transforming not fitting","3e814c93":"#### 6.5.3 Preparing the test dataset","30d3194c":"#### 5.2.2 Dividing into X_test and y_test","3a5bcfcf":"### 5.2 Making Predictions Using the  Model\n#### 5.2.1 Applying the scaling on the test sets. Only transforming not fitting"}}