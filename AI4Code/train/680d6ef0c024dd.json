{"cell_type":{"08bcffd0":"code","1b9d02fc":"code","2515cd05":"code","8f084ab7":"code","034b7be3":"code","66ef4f5a":"code","37296abb":"code","658886eb":"code","a51b3e1f":"code","bb9571d0":"code","56aec2ed":"code","ddfe8f29":"code","8551ca30":"code","d7f10727":"code","4df62a71":"code","9e7cb95e":"code","b829cd48":"code","d2fa76cf":"code","0585d494":"code","a60f6d8d":"code","cd623952":"code","fa633147":"code","b2463532":"code","bc906be3":"code","8daf1f53":"code","1ab1de2d":"code","90de7d69":"code","c2993216":"code","b0cbc826":"code","be289d05":"code","e3705e01":"code","7e803e63":"code","1033583b":"code","d4f13e9b":"code","9b89453b":"code","5962859c":"code","fe8cbb66":"code","79123ac2":"code","5a84a773":"markdown","7bb559fd":"markdown","c0a091dc":"markdown"},"source":{"08bcffd0":"import pandas as pd\nimport numpy as np\nimport pandas_profiling\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\nimport os\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","1b9d02fc":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","2515cd05":"train_df_copy = train_df.copy()\ntest_df_copy = test_df.copy()","8f084ab7":"train_df.head(2)","034b7be3":"test_df.head(2)","66ef4f5a":"print('Training Data')\nprint('\\tRows\\t',train_df.shape[0])\nprint('\\tColumns\\t',train_df.shape[1])\n#Train Samples\nfor col in train_df:\n    print(col)\n    print('\\tDatatype\\t:%s '%(train_df[col].dtype))\n    print('\\tUnique values\\t:%s '%(np.size(train_df[col].unique())))  \n    print('\\tMissing values\\t:%d '%(train_df[col].isnull().sum()))","37296abb":"print('Test Data')\nprint('\\tRows\\t',test_df.shape[0])\nprint('\\tColumns\\t',test_df.shape[1])\n#Test Samples\nfor col in test_df:\n    print(col)\n    print('\\tDatatype\\t:%s '%(test_df[col].dtype))\n    print('\\tUnique values\\t:%s '%(np.size(test_df[col].unique())))  \n    print('\\tMissing values\\t:%d '%(test_df[col].isnull().sum()))","658886eb":"#Impute age by median\nimpute_age = train_df['Age'].median()\ntrain_df['Age'].fillna(impute_age,inplace = True)\ntest_df['Age'].fillna(impute_age,inplace = True)\n\n#Impute Embarked by mode\nimpute_embarked = train_df['Embarked'].mode().values[0]\ntrain_df['Embarked'].fillna(impute_embarked,inplace = True)\ntest_df['Embarked'].fillna(impute_embarked,inplace = True)\n\n#Cabin can be dropped, for now lets create a feature to hold if cabin is available or not\ntrain_df['hasCabin'] = train_df['Cabin'].isnull().astype(int)\ntest_df['hasCabin'] = test_df['Cabin'].isnull().astype(int)\n\n#fare = 0 means missing value hence needs to be imputed . \n#Fare is based on passanger class. So that can be imputed by median value of fare per class\n#Median value wont be impacted by outliers as compared to mean\nimpute_fare1 = train_df[train_df['Pclass'] == 1]['Fare'].median()\nimpute_fare2 = train_df[train_df['Pclass'] == 2]['Fare'].median()\nimpute_fare3 = train_df[train_df['Pclass'] == 3]['Fare'].median()\ntrain_df.loc[((train_df['Fare'] == 0) | (train_df['Fare'].isnull())) & (train_df['Pclass'] == 1),'Fare'] = impute_fare1\ntrain_df.loc[((train_df['Fare'] == 0) | (train_df['Fare'].isnull())) & (train_df['Pclass'] == 2),'Fare'] = impute_fare2\ntrain_df.loc[((train_df['Fare'] == 0) | (train_df['Fare'].isnull())) & (train_df['Pclass'] == 3),'Fare'] = impute_fare3\ntest_df.loc[((test_df['Fare'] == 0) | (test_df['Fare'].isnull())) & (test_df['Pclass'] == 1),'Fare'] = impute_fare1\ntest_df.loc[((test_df['Fare'] == 0) | (test_df['Fare'].isnull())) & (test_df['Pclass'] == 2),'Fare'] = impute_fare2\ntest_df.loc[((test_df['Fare'] == 0) | (test_df['Fare'].isnull())) & (test_df['Pclass'] == 3),'Fare'] = impute_fare3","a51b3e1f":"#Univariate Analysis\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","bb9571d0":"_ = sns.distplot(train_df['Age'], bins=15, kde=True)","56aec2ed":"_ = sns.distplot(train_df['Fare'], bins=15, kde=True)","ddfe8f29":"#Bivariate\n_ = sns.lmplot(x='Pclass',y='Survived',data=train_df,col='Embarked',hue='Sex')","8551ca30":"_ = sns.countplot(x='Survived',data=train_df,hue='Sex')","d7f10727":"_ = sns.countplot(x='Survived',data=train_df,hue='Pclass')","4df62a71":"_ = sns.countplot(x='Survived',data=train_df,hue='SibSp')","9e7cb95e":"#Outliers\nfrom matplotlib import style\nstyle.use('ggplot')\nsns.boxplot(x='Age',data=train_df)","b829cd48":"sns.boxplot(x='Fare',data=train_df)","d2fa76cf":"sns.boxplot(x='SibSp',data=train_df)","0585d494":"sns.boxplot(x='Parch',data=train_df)","a60f6d8d":"def detect_outliers(features,n):\n    outlier_indices=[]\n    for col in features:\n        Q1 = np.percentile(train_df[col],25)\n        Q3 = np.percentile(train_df[col],75)\n        IQR = Q3 - Q1\n    \n        outlier_indices.extend(train_df[(train_df[col] > (Q3 + (IQR * 1.5)) ) | (train_df[col] < (Q1 - (IQR * 1.5)) )].index)\n    \n    outlier_indices = Counter(outlier_indices)        \n    #multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    multiple_outliers = []\n    for k,v in outlier_indices.items():\n        if v > n:\n            multiple_outliers.append(k)\n        \n    return multiple_outliers   \n","cd623952":"outliertodrop = detect_outliers([\"Age\",\"SibSp\",\"Parch\",\"Fare\"],2)","fa633147":"#Standard scaling of features Age and Fare\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(train_df[['Age']])\n\ntrain_df['AgeSc'] = sc.transform(train_df[['Age']])\ntest_df['AgeSc'] = sc.transform(test_df[['Age']])\n\nsc1 = StandardScaler()\nsc1.fit(train_df[['Fare']])\ntrain_df['FareSc'] = sc1.transform(train_df[['Fare']])\ntest_df['FareSc'] = sc1.transform(test_df[['Fare']])","b2463532":"#One hot encoding for Sex, Pclass, Embarked\noheSex = OneHotEncoder(categories='auto')\noheSex.fit(train_df[['Sex']])\nf_train = oheSex.transform(train_df[['Sex']]).toarray()\ndfSex_train = pd.DataFrame(f_train.astype(int),columns=oheSex.get_feature_names())\ntrain_df = pd.concat([train_df,dfSex_train],axis=1)\nf_test = oheSex.transform(test_df[['Sex']]).toarray()\ndfSex_test = pd.DataFrame(f_test.astype(int),columns=oheSex.get_feature_names())\ntest_df = pd.concat([test_df,dfSex_test],axis=1)\n\n#One hot encoding for Pclass\nohePclass = OneHotEncoder(categories='auto')\nohePclass.fit(train_df[['Pclass']])\nf_train = ohePclass.transform(train_df[['Pclass']]).toarray()\ndfPclass_train = pd.DataFrame(f_train.astype(int),columns=['Pclass_1','Pclass_2','Pclass_3'])\ntrain_df = pd.concat([train_df,dfPclass_train],axis=1)\nf_test = ohePclass.transform(test_df[['Pclass']]).toarray()\ndfPclass_test = pd.DataFrame(f_test.astype(int),columns=['Pclass_1','Pclass_2','Pclass_3'])\ntest_df = pd.concat([test_df,dfPclass_test],axis=1)\n\n#One hot encoding for Embarked\noheEmbarked = OneHotEncoder(categories='auto')\noheEmbarked.fit(train_df[['Embarked']])\nf_train = oheEmbarked.transform(train_df[['Embarked']]).toarray()\ndfEmbarked_train = pd.DataFrame(f_train.astype(int),columns=['EMB_C','EMB_Q','EMB_S'])\ntrain_df = pd.concat([train_df,dfEmbarked_train],axis=1)\nf_test = oheEmbarked.transform(test_df[['Embarked']]).toarray()\ndfEmbarked_test = pd.DataFrame(f_test.astype(int),columns=['EMB_C','EMB_Q','EMB_S'])\ntest_df = pd.concat([test_df,dfEmbarked_test],axis=1)","bc906be3":"#Features for model\nX_ml = train_df[['SibSp' , 'Parch' , 'Fare' , 'hasCabin' , 'AgeSc' , 'FareSc' , 'x0_female' , 'x0_male' , 'Pclass_1' , 'Pclass_2' , 'Pclass_3' , 'EMB_C' , 'EMB_Q' , 'EMB_S']]\ny_ml = train_df ['Survived']\nX_test1 = test_df[['SibSp' , 'Parch' , 'Fare' , 'hasCabin' , 'AgeSc' , 'FareSc' , 'x0_female' , 'x0_male' , 'Pclass_1' , 'Pclass_2' , 'Pclass_3' , 'EMB_C' , 'EMB_Q' , 'EMB_S']]","8daf1f53":"#Train Test Split\nX_train,X_test,y_train,y_test = train_test_split(X_ml,y_ml)","1ab1de2d":"#kfold cross validation with Logistic Regression with default parameters\nn_splits = 10\nkfold = StratifiedKFold(n_splits=n_splits,shuffle=False,random_state=420)\nscores = np.zeros(n_splits)\nfor k , (train, test) in enumerate(kfold.split(X_train,y_train)):\n    #print(train)\n    clf_lr = LogisticRegression(penalty='l2')\n    clf_lr.fit(X_train.iloc[train],y_train.iloc[train])\n    scores[k] = clf_lr.score(X_train.iloc[test],y_train.iloc[test])\n    \nprint('Training Score-->%s'%(scores.mean()))\nprint('Training Standard Deviation-->%s'%(scores.std()))\nprint('Logistic Regression Test Accuracy Score --> %s'%(accuracy_score(y_test,clf_lr.predict(X_test)))) ","90de7d69":"#parameter tuning for classifier for parameter C\nparam_range = [0.001,0.01,0.1,1.0,10.0,100.0]\ntrain_score,test_score = validation_curve(clf_lr, \n                                          X = X_train, \n                                          y = y_train, \n                                          param_name = 'C', \n                                          param_range = param_range,  \n                                          cv=10, \n                                          n_jobs=1\n                                         )\n\ntrain_mean = np.mean(train_score,axis = 1)\ntest_mean = np.mean(test_score,axis = 1)\n\nplt.plot(param_range,train_mean,color = 'blue', marker = 'o',label='training accuracy')\nplt.plot(param_range,test_mean,color = 'green', marker = 's',label='validation accuracy')\nplt.legend(loc = 'best')\nplt.xscale('log')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')","c2993216":"#kfold cross validation with Logistic Regression with Tuned parameters\nn_splits = 10\nkfold = StratifiedKFold(n_splits=n_splits,shuffle=False,random_state=420)\nscores = np.zeros(n_splits)\nfor k , (train, test) in enumerate(kfold.split(X_train,y_train)):\n    #print(train)\n    clf_lr = LogisticRegression(penalty='l2',C = 10)\n    clf_lr.fit(X_train.iloc[train],y_train.iloc[train])\n    scores[k] = clf_lr.score(X_train.iloc[test],y_train.iloc[test])\n    \nprint('Training Score-->%s'%(scores.mean()))\nprint('Training Standard Deviation-->%s'%(scores.std()))\nprint('Logistic Regression Test Accuracy Score --> %s'%(accuracy_score(y_test,clf_lr.predict(X_test)))) ","b0cbc826":"#Dignose Bias and variance with Learning curves against multiple samples\ntrain_sizes,train_score,test_score = learning_curve(estimator = clf_lr, \n                                                    X = X_train, \n                                                    y = y_train, \n                                                    train_sizes=np.linspace(0.1  , 1, 10), \n                                                    cv=10, \n                                                    n_jobs=1, pre_dispatch='all', \n                                                    random_state=420)\n\ntrain_mean = np.mean(train_score,axis = 1)\ntest_mean = np.mean(test_score,axis = 1)\n\nplt.plot(train_sizes,train_mean,color = 'blue', marker = 'o',label='training accuracy')\nplt.plot(train_sizes,test_mean,color = 'green', marker = 's',label='validation accuracy')\nplt.legend(loc = 'best')\nplt.xlabel('Number of samples')\nplt.ylabel('Accuracy')","be289d05":"#kfold cross validation with Random Forest Classifier\nn_splits = 10\nkfold = StratifiedKFold(n_splits=n_splits,shuffle=False,random_state=420)\nscores = np.zeros(n_splits)\nfor k , (train, test) in enumerate(kfold.split(X_train,y_train)):\n    #print(train)\n    clf_rf = RandomForestClassifier()\n    clf_rf.fit(X_train.iloc[train],y_train.iloc[train])\n    scores[k] = clf_rf.score(X_train.iloc[test],y_train.iloc[test])\n    \nprint('Training Score-->%s'%(scores.mean()))\nprint('Training Standard Deviation-->%s'%(scores.std()))\nprint('Random Forest Test Accuracy Score --> %s'%(accuracy_score(y_test,clf_rf.predict(X_test)))) ","e3705e01":"#parameter tuning for classifier for parameter criterion\nparam_range = ['gini','entropy']\ntrain_score,test_score = validation_curve(clf_rf, \n                                          X = X_train, \n                                          y = y_train, \n                                          param_name = 'criterion', \n                                          param_range = param_range,  \n                                          cv=10, \n                                          n_jobs=1\n                                         )\n\ntrain_mean = np.mean(train_score,axis = 1)\ntest_mean = np.mean(test_score,axis = 1)\n\n","7e803e63":"plt.plot(param_range,train_mean,color = 'blue', marker = 'o',label='training accuracy')\nplt.plot(param_range,test_mean,color = 'green', marker = 's',label='validation accuracy')\nplt.legend(loc = 'best')\nplt.xlabel('Criterion')\nplt.ylabel('Accuracy')","1033583b":"#parameter tuning for classifier for parameter max_depth\nparam_range = [3,4,5,6,7]\ntrain_score,test_score = validation_curve(clf_rf, \n                                          X = X_train, \n                                          y = y_train, \n                                          param_name = 'max_depth', \n                                          param_range = param_range,  \n                                          cv=10, \n                                          n_jobs=1\n                                         )\n\ntrain_mean = np.mean(train_score,axis = 1)\ntest_mean = np.mean(test_score,axis = 1)","d4f13e9b":"plt.plot(param_range,train_mean,color = 'blue', marker = 'o',label='training accuracy')\nplt.plot(param_range,test_mean,color = 'green', marker = 's',label='validation accuracy')\nplt.legend(loc = 'best')\nplt.xlabel('Depth')\nplt.ylabel('Accuracy')","9b89453b":"#kfold cross validation with RAndom Forest Classifier with tuned parameters\nn_splits = 10\nkfold = StratifiedKFold(n_splits=n_splits,shuffle=False,random_state=420)\nscores = np.zeros(n_splits)\nfor k , (train, test) in enumerate(kfold.split(X_train,y_train)):\n    #print(train)\n    clf_rf = RandomForestClassifier(criterion='entropy',\n                                         n_estimators=70,\n                                         max_depth=7,\n                                         min_samples_split=15,\n                                         min_samples_leaf=5, \n                                         max_features='auto', \n                                         oob_score=True, \n                                         random_state=420,\n                                         n_jobs=-1\n                                        )\n    clf_rf.fit(X_train.iloc[train],y_train.iloc[train])\n    scores[k] = clf_rf.score(X_train.iloc[test],y_train.iloc[test])\n    \nprint('Training Score-->%s'%(scores.mean()))\nprint('Training Standard Deviation-->%s'%(scores.std()))\nprint('Random Forest Test Accuracy Score --> %s'%(accuracy_score(y_test,clf_rf.predict(X_test)))) ","5962859c":"#Dignose Bias and variance with Learning curves against multiple samples\ntrain_sizes,train_score,test_score = learning_curve(estimator = clf_rf, \n                                                    X = X_train, \n                                                    y = y_train, \n                                                    train_sizes=np.linspace(0.1  , 1, 10), \n                                                    cv=10, \n                                                    n_jobs=1, pre_dispatch='all', \n                                                    random_state=420)\n\ntrain_mean = np.mean(train_score,axis = 1)\ntest_mean = np.mean(test_score,axis = 1)\n\nplt.plot(train_sizes,train_mean,color = 'blue', marker = 'o',label='training accuracy')\nplt.plot(train_sizes,test_mean,color = 'green', marker = 's',label='validation accuracy')\nplt.legend(loc = 'best')\nplt.xlabel('Number of samples')\nplt.ylabel('Accuracy')","fe8cbb66":"y_pred = clf_rf.predict(X_test1)","79123ac2":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = test_df['PassengerId']\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","5a84a773":"# Logistic Regression","7bb559fd":"#Model Selection, evaluation, Hyperparameter tuning","c0a091dc":"# RandomForest"}}