{"cell_type":{"4713a95c":"code","4a10979f":"code","89743b91":"code","74088e35":"code","68702b40":"code","14019ac9":"code","185960e6":"code","bbe1638c":"code","474161b4":"code","797940cd":"code","c908a200":"code","0feb644d":"code","e9143b48":"code","f12dcf83":"code","1794e4a7":"code","8be618d1":"code","c3e1fa1e":"code","869ccfc8":"code","ddd6f6d6":"code","98441665":"code","7389a326":"code","1810ea1e":"code","9597ee36":"code","a20b5be7":"code","b423495e":"code","401f7953":"markdown","7c338b21":"markdown","6aca8ea8":"markdown","3031f8d1":"markdown","51e9b9b6":"markdown","89964255":"markdown","29d608ee":"markdown","9454a81a":"markdown","b954f77b":"markdown","32077b1d":"markdown","f822854b":"markdown","aadd788e":"markdown","256c7774":"markdown","c66c31c0":"markdown","5579af01":"markdown","e5f6aa63":"markdown","2ea191d5":"markdown","0573ef70":"markdown","a20a50ae":"markdown","18f464b2":"markdown"},"source":{"4713a95c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under\n# the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as\n# output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the\n# current session\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin \nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom category_encoders import BinaryEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest","4a10979f":"# Functions\n\ndef get_titles(df):\n    '''Get the set of titles from the Name field'''\n    titles=set()\n    for name in df:\n        if name.find('.'):\n            title = name.split('.')[0].split()[-1]\n            titles.add(title)\n    return titles\n\n\ndef get_family_name(df):\n    '''Get the set of family names from Name field'''\n    family_names=set()\n    for name in df:\n        if name.find(','):\n            family_name = name.split(',')[0].split()[-1]\n            family_names.add(family_name)\n    return family_names\n\ndef get_cabin_chars(df):\n    '''Get the set of chars used in Cabin field'''\n    cabin_chars = set()\n    for word in df:\n        cabin_chars.add(str(word)[0]) \n    return cabin_chars\n\ndef save_file (predictions):\n    \"\"\"Save submission file.\"\"\"\n    # Save test predictions to file\n    output = pd.DataFrame({'PassengerId': sample_sub_file.PassengerId,\n                       'Survived': predictions})\n    output.to_csv('submission.csv', index=False)\n    print (\"Submission file is saved\")\n    \nprint(\"Functions loaded\")","89743b91":"# Loading data\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\nsample_sub_file = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\n# Make a copy to avoid changing original data\nX = train_data.copy()\ny = X.Survived\nX_test = test_data.copy()\n\n# Remove target from predictors\nX.drop(['Survived'], axis=1, inplace=True)\nprint('\"Survived\" column dropped from training data!')\n\n# Remove ticket column. We will not use it.\nX.drop(\"Ticket\",axis = 1, inplace = True)\nX_test.drop(\"Ticket\",axis = 1, inplace = True)\nprint('\"Ticket\" column dropped from both training and test data!')\n\nprint(\"\\nShape of training data: {}\".format(X.shape))\nprint(\"Shape of target: {}\".format(y.shape))\nprint(\"Shape of test data: {}\".format(X_test.shape))\nprint(\"Shape of submission data: {}\".format(sample_sub_file.shape))\n\n# Split the data for validation\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, random_state=2)\n\nprint(\"\\nShape of X_train data: {}\".format(X_train.shape))\nprint(\"Shape of X_valid: {}\".format(X_valid.shape))\nprint(\"Shape of y_train: {}\".format(y_train.shape))\nprint(\"Shape of y_valid: {}\".format(y_valid.shape))\n\nprint(\"\\nFiles Loaded\")","74088e35":"X.head()","68702b40":"X.info()","14019ac9":"# Make Lists\n# Get missing values\nmissing_values = [col for col in X_train.columns if X_train[col].isnull().sum()]\nprint(\"Cols with missing: {}\".format(missing_values))\n\n# get numerical and categorical columns\nnumerical_columns = [col for col in X.columns if X[col].dtype in ['int64',\n                                                                  'float64']]\nprint(\"Numerical columns: {}\".format(numerical_columns))\ncategorical_columns = [col for col in X.columns if X[col].dtype==\"object\"]\nprint(\"Categorical columns: {}\".format(categorical_columns))\n\n# Get titles from the name column\ntitles = list(get_titles(X[\"Name\"]))\nprint(\"\\nTitles: {}\".format(titles))\n\n# Get chars from the cabin column\nchars = list(get_cabin_chars(X[\"Cabin\"]))\nprint(\"\\nChars in Cabin column: {}\".format(chars))","185960e6":"# Get surnames from the name column\nsurname = list(get_family_name(X[\"Name\"]))\nprint(\"Surnames of the passengers: {}\".format(surname))","bbe1638c":"# Custom transformer classes\nclass NameColumnTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    a general class for transforming Name, SibSp and Parch columns of Titanic dataset\n    for using in the machine learning pipeline\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        constructor\n        \"\"\"\n        # Will be used for fitting data\n        self.titles_set = set()\n        self.surname_set = set()\n        # Titles captured from train data\n        self.normal_titles_list = [\"Mr\", \"Mrs\", \"Mme\", \"Miss\", \"Mlle\", \"Ms\", \"Master\", \"Dona\"]\n        self.titles_dict = {\"Mr\": ['Mr', 'Major', 'Jonkheer', 'Capt', 'Col', 'Don', 'Sir',\n                                   'Rev'],\n                            \"Mrs\": ['Mrs', 'Mme', 'Lady','Countess', 'Dona'],\n                            \"Miss\": ['Miss', 'Mlle', 'Ms'],\n                            \"Master\": ['Master'],\n                            \"Dr\": ['Dr']}\n\n    def fit(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to fit the step and to learn by examples\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: self: the class object - an instance of the transformer - Transformer\n        \"\"\"\n        '''Fits the titles, family and rank from Names Column'''\n        \n        # Make a copy to avoid changing original data\n        X_temp = X.copy()\n        # Create Titles column\n        if \"Title\" in X_temp.columns:\n            X_temp.drop(\"Title\", axis=1, inplace=True)\n        else:\n            pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Title\",\"\",False)  \n            \n        # Get the index values\n        index_values=X_temp.index.values.astype(int)\n        \n        # Set state (Add to: {titles_set, surname_set} attributes) of the object\n        for i in index_values:\n            \n            # Get the name for the ith index\n            name = X_temp.loc[i,'Name']\n            # Get the number of followers for the ith index\n            number_of_followers = X_temp.loc [i, 'SibSp'] + X_temp.loc [i, 'Parch']\n            \n            # Split the title from name\n            if name.find('.'):\n                title = name.split('.')[0].split()[-1]\n                if title in self.titles_dict.keys():\n                    X_temp.loc[i, 'Title'] = title\n                else:\n                    X_temp.loc[i, 'Title'] = np.NaN\n                # Add title to titles_set to use in transform method\n                self.titles_set.add(title)\n            \n            # Split the surname from name\n            if name.find(','):\n                surname = name.split(',')[0].split()[-1]\n                # Add surname to surname_set to use in transform method\n                if number_of_followers > 0:\n                    self.surname_set.add(surname)\n                    X_temp.loc[i,\"Family\"]=surname\n                \n        # Title Encoding\n        \n        # Drop missing Title rows (Hi rank columns that are mapped to titles_dict keys)\n        # so that no 'Title_' columns will appear in transform \n        X_temp.dropna(axis = \"index\", subset=['Title'], inplace=True)\n        \n        # Apply one-hot encoding to the Title column.\n        self.OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n        # Get column names to use in transform.\n        self.OH_encoder = self.OH_encoder.fit(X_temp[['Title']])\n        self.title_columns = self.OH_encoder.get_feature_names(['Title'])\n\n        # Family Encoding\n        \n        # Drop missing Family rows\n        # so that no 'Family_' columns will appear in transform \n        X_temp.dropna(axis = \"index\", subset=['Family'], inplace=True)\n        \n        # Apply binary encoding to the Family column.\n        self.binary_encoder = BinaryEncoder(cols =['Family'])\n        self.binary_encoder = self.binary_encoder.fit(X_temp[['Family']])\n        \n        return self\n\n    def transform(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to transform according to what happend in the fit method\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        '''Transforms the titles and family from Names Column'''\n        \n        # Make a copy to avoid changing original data\n        X_temp = X.copy()\n        \n        # Create Titles column    \n        pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Title\",\"\",False)    \n        # Create Family column\n        pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Family\",\"\",False)          \n        # Create Rank column\n        pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Rank\",\"\",False)\n        # Create Followers column\n        pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Followers\",\"\",False)\n        \n        # Get the index values\n        index_values=X_temp.index.values.astype(int)\n        \n        for i in index_values:\n            # Get the name for the ith index\n            name = X_temp.loc[i,'Name']\n            # Get the number of followers for the ith index\n            number_of_followers = X_temp.loc [i, 'SibSp'] + X_temp.loc [i, 'Parch']\n            X_temp.loc[i, 'Followers'] = number_of_followers\n            \n            # Split the title from name\n            if name.find('.'):\n                title = name.split('.')[0].split()[-1]\n                if title in self.titles_set:\n                    for key in self.titles_dict:\n                        # Insert title\n                        if title in self.titles_dict[key]:\n                            X_temp.loc[i, 'Title'] = key \n                        \n                        # Insert rank\n                        if title in self.normal_titles_list:\n                            X_temp.loc[i, 'Rank'] = \"Normal\"\n                        else:\n                            X_temp.loc[i, 'Rank'] = \"High\"\n                else:\n                    X_temp.loc[i, 'Title'] = \"Other\"\n                    X_temp.loc[i, 'Rank'] = \"Normal\"\n                    \n                    \n            # Split the surname from name\n            if name.find(','):\n                surname = name.split(',')[0].split()[-1]\n                if surname in self.surname_set and number_of_followers > 0:\n                    X_temp.loc[i, 'Family'] = surname                 \n                else:\n                    X_temp.loc[i, 'Family'] = \"NA\"                    \n        \n        # Encoding Title\n        encoded = self.OH_encoder.transform(X_temp[['Title']])\n        # convert arrays to a dataframe \n        encoded = pd.DataFrame(encoded) \n        # One-hot encoding removed index; put it back\n        encoded.index = X_temp.index\n        # Insert column names\n        encoded.columns = self.title_columns\n        encoded = encoded.astype('int64')\n        # concating dataframes  \n        X_temp = pd.concat([X_temp, encoded], axis = 1)\n        \n        # Encoding Family\n        bin_encoded = self.binary_encoder.transform(X_temp[['Family']])\n        # convert arrays to a dataframe \n        bin_encoded = pd.DataFrame(bin_encoded) \n        # One-hot encoding removed index; put it back\n        bin_encoded.index = X_temp.index\n        bin_encoded = bin_encoded.astype('int64')\n        # concating dataframes  \n        X_temp = pd.concat([X_temp, bin_encoded], axis = 1)\n        # We do not need Family any more\n        X_temp.drop(\"Family\", axis = 1, inplace=True) \n        \n        # Encoding Rank\n        X_temp['Rank'] = X_temp['Rank'].apply(lambda x: 1 if x =='Normal' else (0 if x =='High' else None))\n        # We do not need Name any more\n        X_temp.drop(\"Name\", axis = 1, inplace=True)\n\n        return X_temp\n\n    def fit_transform(self, X, y=None, **kwargs):\n        \"\"\"\n        perform fit and transform over the data\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        self = self.fit(X, y)\n        return self.transform(X, y)\nprint(\"NameColumnTransformer loaded\")","474161b4":"# Custom transformer classes\nclass AgeColumnTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    a general class for transforming age column of Titanic dataset for using in the machine learning pipeline\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        constructor\n        \"\"\"\n        # Will be used for fitting data\n        self.titles_set = set()\n        self.titles_dict = {}\n\n\n    def fit(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to fit the step and to learn by examples\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: self: the class object - an instance of the transformer - Transformer\n        \"\"\"\n        '''Fits the titles, family and rank from Names Column'''\n \n        # Make a copy to avoid changing original data\n        X_temp = X.copy()\n    \n        # Get the index values\n        index_values = X_temp.index.values.astype(int)\n        \n        # Get all the titles from dataset\n        for i in index_values:\n            title = X_temp.loc [i, 'Title']\n            self.titles_set.add(title)\n        \n        # Calculate mean for all titles\n        for title in self.titles_set:\n            mean = self.calculate_mean_age(title, X_temp)\n            self.titles_dict[title] = mean\n            #print(\"Avarage age for title '{}' is {}\".format(title, mean))\n       \n        return self\n\n    def transform(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to transform according to what happend in the fit method\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        '''Transforms the titles and family from Names Column'''\n            \n        # Make a copy to avoid changing original data\n        X_temp = X.copy()        \n        \n        # Get the index values\n        index_values = X_temp.index.values.astype(int)\n        \n        # If a passangers age is Nan replace it with the avarage value\n        # of that title class. e.g. if that passanger is master use the\n        # mean value calculated for the masters.\n        for i in index_values:\n            age = X_temp.at[i, 'Age'].astype(float)\n            if np.isnan(age):\n                title = X_temp.loc [i, 'Title']\n                X_temp.loc[i,'Age'] =  round(self.titles_dict.get(title),2)\n\n        # We do not need Title any more\n        X_temp.drop(\"Title\", axis = 1, inplace=True)\n        \n        return X_temp\n\n    def fit_transform(self, X, y=None, **kwargs):\n        \"\"\"\n        perform fit and transform over the data\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        self = self.fit(X, y)\n        \n        return self.transform(X, y)\n                   \n    def calculate_mean_age(self, title, X):\n        \n        # Make a copy to avoid changing original data\n        X_temp = X.copy()  \n        \n        title_X = X_temp[[title in x for x in X_temp['Title']]][X_temp[\"Age\"].notnull()]\n        return title_X[\"Age\"].mean()\n\n    print(\"AgeColumnTransformer loaded\")","797940cd":"# Custom transformer classes\nclass CabinColumnTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    a general class for transforming cabin column of Titanic dataset for using in the machine \n    learning pipeline\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        constructor\n        \"\"\"\n        # Will be used for fitting data\n        self.cabin_set = set()\n\n    def fit(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to fit the step and to learn by examples\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: self: the class object - an instance of the transformer - Transformer\n        \"\"\"\n        '''Fits the titles, family and rank from Names Column'''\n         \n        # Make a copy to avoid changing original data\n        X_temp = X.copy()\n        \n        # Imputation on X_temp_imputed\n        imputer = SimpleImputer(strategy='constant', fill_value=\"NaN\")\n        X_temp_imputed = pd.DataFrame(imputer.fit_transform(X_temp[['Cabin']]))\n        # Imputation removed column names; put them back\n        X_temp_imputed.columns = X_temp[['Cabin']].columns\n        X_temp_imputed.index = X_temp[['Cabin']].index\n    \n        # Get the index values\n        index_values = X_temp.index.values.astype(int)\n        \n        # For each cabin\n        for i in index_values:\n            cabin = X_temp_imputed.loc[i, 'Cabin']\n            X_temp_imputed.loc[i, 'Cabin'] = cabin[0]\n            self.cabin_set.add(cabin[0])\n        \n        # Cabin Encoding\n        \n        # Apply one-hot encoding to the Cabin column.\n        self.OH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\n        # Get column names to use in transform.\n        self.OH_encoder = self.OH_encoder.fit(X_temp_imputed[['Cabin']])\n        self.cabin_columns = self.OH_encoder.get_feature_names(['Cabin'])\n      \n        return self\n\n    def transform(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to transform according to what happend in the fit method\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        '''Transforms the titles and family from Names Column'''\n\n        # Make a copy to avoid changing original data\n        X_temp = X.copy()        \n        \n        # Get the index values\n        index_values = X_temp.index.values.astype(int)\n        \n        # Imputation on X_imputed\n        imputer = SimpleImputer(strategy='constant', fill_value=\"NaN\")\n        X_imputed = pd.DataFrame(imputer.fit_transform(X_temp[['Cabin']]))\n        # Imputation removed column names; put them back\n        X_imputed.columns = X_temp[['Cabin']].columns\n        X_imputed.index = X_temp[['Cabin']].index\n        \n        for i in index_values:\n            cabin = X_imputed.loc[i, 'Cabin']\n            if cabin[0] in self.cabin_set:\n                X_imputed.loc [i, 'Cabin'] = cabin[0]\n            else:\n                X_imputed.loc [i, 'Cabin'] = \"N\"\n        X_temp.drop(\"Cabin\",axis = 1, inplace = True)\n\n        # concating dataframes  \n        X_temp = pd.concat([X_temp, X_imputed], axis = 1)\n               \n        # Encoding Cabin\n        encoded = self.OH_encoder.transform(X_imputed[['Cabin']])\n        # convert arrays to a dataframe \n        encoded = pd.DataFrame(encoded) \n        # One-hot encoding removed index; put it back\n        encoded.index = X_imputed.index\n        # Insert column names\n        encoded.columns = self.cabin_columns\n        encoded = encoded.astype('int64')\n        # concating dataframes  \n        X_temp = pd.concat([X_temp, encoded], axis = 1)\n\n        X_temp.drop(\"Cabin\",axis = 1, inplace = True)\n        \n        return X_temp\n\n    def fit_transform(self, X, y=None, **kwargs):\n        \"\"\"\n        perform fit and transform over the data\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        self = self.fit(X, y)\n        return self.transform(X, y)\n\n    print(\"CabinColumnTransformer loaded\")","c908a200":"X.head()","0feb644d":"# Apply NameColumnTransformer\nname = NameColumnTransformer()\nX_name=name.fit_transform(X) \n\n# Apply AgeColumnTransformer\nage=AgeColumnTransformer()\nX_age=age.fit_transform(X_name) \n\n# Apply CabinColumnTransformer\ncabin=CabinColumnTransformer()\nX_cabin=cabin.fit_transform(X_age) \n\nX_cabin.head()","e9143b48":"# Define the custom transformers for the pipeline\nname_transformer = NameColumnTransformer()\nage_transformer = AgeColumnTransformer()\ncabin_transformer = CabinColumnTransformer()","f12dcf83":"# Define the columns that will be handled by the column transformer\nnumerical_cols = ['Pclass', 'Fare']\ncategorical_cols = ['Sex', 'Embarked']","1794e4a7":"# Define transformers for numerical columns using a pipeline\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\n\n# Define transformers for categorical columns using a pipeline\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n# Define column transformer for numerical and categorical data\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('numerical', numerical_transformer, numerical_cols),\n        ('categorical', categorical_transformer, categorical_cols)\n    ], remainder='passthrough')","8be618d1":"# This dataset is way too high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=12)\n\n# Define union\nfeature_union = FeatureUnion([('pca', pca), ('select', selection)])","c3e1fa1e":"# Define the model\nmy_model = XGBClassifier(\n learning_rate =0.01,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n seed=42)","869ccfc8":"# Define preprocessor\npreprocessor = Pipeline(steps=[('name', name_transformer),\n                              ('age', age_transformer),\n                              ('cabin', cabin_transformer),\n                              ('column', column_transformer),\n                              ('union', feature_union)])\n\n# Make a copy to avoid changing original data \nX_valid_eval=X_valid.copy()\n\n# Preprocessing of validation data\nX_valid_eval = preprocessor.fit(X_train, y_train).transform (X_valid_eval)","ddd6f6d6":"# Display the number of remaining columns after transformation \nprint(\"We have\", X_valid_eval.shape[1], \"features left\")","98441665":"# Define XGBoostClassifier fitting parameters for the pipeline\nfit_params = {\"model__early_stopping_rounds\": 50,\n              \"model__eval_set\": [(X_valid_eval, y_valid)],\n              \"model__verbose\": True,\n              \"model__eval_metric\" : \"error\"}","7389a326":"# Create and Evaluate the Pipeline\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', my_model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train, **fit_params)\n\n# Get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = accuracy_score(y_valid, preds)\n\nprint(\"Score: {}\".format(score))","1810ea1e":"cv_model = XGBClassifier(learning_rate =0.01,\n                         n_estimators=40,\n                         max_depth=5,\n                         min_child_weight=1,\n                         gamma=0,\n                         subsample=0.8,\n                         colsample_bytree=0.8,\n                         seed=42)\n\ncv_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', cv_model)\n                             ])\n\n    \nscores = cross_val_score(cv_pipeline, X_train, y_train,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy of the folds:\\n\", scores)\nprint(\"\\nmean:\\n\", scores.mean())\nprint(\"std:\\n\", scores.std())","9597ee36":"# Preprocessing of training data, fit model \ncv_pipeline.fit(X_train, y_train)\n\n# Get predictions\npreds = cv_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = accuracy_score(y_valid, preds)\n\nprint(\"Score: {}\".format(score))","a20b5be7":"# Preprocessing of training data, fit model \ncv_pipeline.fit(X, y)\n\n# Get predictions\npreds = cv_pipeline.predict(X_test)","b423495e":"# Save submission file\nsave_file(preds)","401f7953":"## Implementing Column Transformer<a id='implementingcolumntransformer'><\/a>\n\nEverything looks good. Now we can transform the other columns ('Pclass', 'Fare', 'Sex', 'Embarked') by using ColumnTransformer. However, there is a trick here. We need to set the remainder parameter of the column transformer as 'passthrough'. Otherwise, the remaining columns of our data will not be available after the transform.","7c338b21":"# Transformers <a id='transformers'><\/a>\n\nWe can separate machine learning workflow in different pieces. A pipeline generally contains a  pre-processing step that transforms\/imputes the data and a final predictor that predicts target values. Similar to a regular estimator a pipeline can be fitted\/predicted. Pre-processors and transformers stick to the same API as the estimator objects (same BaseEstimator class). The transformer objects don\u2019t have a predict method as predictors have, however they implement a transform method that outputs a newly transformed sample matrix X.\n\nA fit method grasps model parameters (e.g. mean and standard deviation for normalization) from a training set, and a transform method applies this transformation model to unseen data. fit_transform may be more convenient and efficient for modelling and transforming the training data simultaneously.\n\n## Custom Transformers   <a id='customtransformers'><\/a> \n\nIf you search the net you can find several blogs\/posts that show how to implement a custom transformer. There are some different ways to implement one. You can even use Function Transformers of Scikit-learn library. I have chosen the one below which you can find in this [post](https:\/\/g-stat.com\/using-custom-transformers-in-your-machine-learning-pipelines-with-scikit-learn\/).\n\n```\nclass CustomTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    a general class for creating a machine learning step in the machine\n    learning pipeline\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        constructor\n        \"\"\"\n        super(CustomTransformer, self).__init__()\n\n    def fit(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to fit the step and to learn by\n        examples\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: self: the class object - an instance of the transformer\n        - Transformer\n        \"\"\"\n        pass\n\n    def transform(self, X, y=None, **kwargs):\n        \"\"\"\n        an abstract method that is used to transform according to what happend\n        in the fit method\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        pass\n\n    def fit_transform(self, X, y=None, **kwargs):\n        \"\"\"\n        perform fit and transform over the data\n        :param X: features - Dataframe\n        :param y: target vector - Series\n        :param kwargs: free parameters - dictionary\n        :return: X: the transformed data - Dataframe\n        \"\"\"\n        self = self.fit(X, y)\n        return self.transform(X, y)\n```\n\n## Column Transformers   <a id='columntransformers'><\/a>\n\nDatasets generaly consists of different data types (floats, dates,strings, etc.). We need to deal with those datatypes seperately. \n\nBy using ColumnTransformer different transformations can be made for different columns of the data within a Pipeline that is safe from data leakage and that can be parametrized. ColumnTransformer works on arrays, sparse matrices, and pandas DataFrames.\n\nTo each column, a different transformation can be applied, such as preprocessing or a specific feature extraction method. Below you can find a column ColumnTransformer example from [Intermediate Machine Learning Course](https:\/\/www.kaggle.com\/alexisbcook\/pipelines) on [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview)\n\n```\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n```\n\n## Feature Unions   <a id='featureunions'><\/a>\n\nFeatureUnion combines several transformer objects into a new transformer that combines their output. A FeatureUnion takes a list of transformer objects. During fitting, each of these is fit to the data independently. The transformers are applied in parallel, and the feature matrices they output are concatenated side-by-side into a larger matrix.\n\nFeatureUnion serves the same purposes as Pipeline - convenience and joint parameter estimation and validation.\n\nFeatureUnion and Pipeline can be combined to create complex models.\n\nA FeatureUnion is built using a list of (key, value) pairs, where the key is the name you want to give to a given transformation (an arbitrary string; it only serves as an identifier) and value is an estimator object Below you can find an example taken from the [Scikit-learn official page](https:\/\/scikit-learn.org\/stable\/index.html).\n\n```\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\nestimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]\ncombined = FeatureUnion(estimators)\n\n```\n\nNow that since we learned all we need we can make a pipeline structure for our data. According to the workflow above our pipeline structure will be as follows:\n\n![Pipeline%20Structure.png](attachment:Pipeline%20Structure.png)","6aca8ea8":"# Submission   <a id='submission'><\/a>\n\nWe will use our utility function to create submission file.","3031f8d1":"## Implementing AgeColumnTransformer <a id='agecolumntransformer'><\/a>\n\nIn the AgeColumnTransformer class we will:\n\n* Impute the missing values in the age column by using the mean values of each title. As a result no master will be in the age of 30.\n\nYou can read the comments to catch what is going on.","51e9b9b6":"## Implementing the Model<a id='implementingthemodel'><\/a>\n\nWe will use XGBClassifier from sklearn library.","89964255":"Cross-validation seems reasonable. We can now check our validation set with our cross-validated model. That is the same value with the early_stopping_rounds example above as expected. ","29d608ee":"## Implementing Feature Union<a id='implementingfeatureunion'><\/a>\n\nThe feature union is directly taken from the [Scikit-learn official page](https:\/\/scikit-learn.org\/stable\/index.html). The main purpose is how to use a feature union within a pipeline.","9454a81a":"## Getting Started   <a id='gettingstarted'><\/a>   \n\nSince we all know the renowned Titanic dataset there isn't much to tell at this stage. As previously stated we have already dropped Ticket column since we will not use it. We have 9 columns remaining.","b954f77b":"## Validation<a id='validation'><\/a>\n\nWe have one final trick at the end. If we want to use early_stopping_rounds with our pipeline we cannot use them directly. This is because sklearn pipelines do not process the eval_set used with early_stopping_rounds. As a result we need to process our validation set before using early_stopping_rounds.\n\nWe can do this by chaining all our transformers (except the model) in a pipeline (only transformers) and using it to transform validation set as below.","32077b1d":"## Result of Applying Custom Transformers<a id='customtransformerresult'><\/a>\n\nOK, I guess we all wonder what the result of applying those custom transformers to our data set is. But we must be careful at this stage. We have used the results of NameColumnTransformer at AgeColumnTransformer. As a result, we need to transform our data using NameColumnTransformer at first. Then we can use AgeColumnTransformer. Let's remember our training set and then apply custom transformations and see the result.","f822854b":"## Prediction   <a id='prediction'><\/a>\n\nEverything looks cool. We can now use our full dataset and submit our predictions.","aadd788e":"## Loading Data   <a id='loading'><\/a>   \n\nWe will first load the data. Next we will drop the ticket column since we do not need it. Finally we will split our data into training and test data sets. ","256c7774":"# Implementation <a id='implementation'><\/a>\n\nTime for Implementation of the pipeline. We will start with the custom transformers. We will implement the constructor, fit, transform and fit_transform methods.\n\nIn the fit method we will learn from the data and store necessary information in the the object variables. \n\nIn the transform method we will transform the data at hand by using object variables.\n\nIn the fit_transform method we will call fit and transform methods successively.\n\n## Implementing NameColumnTransformer <a id='namecolumntransformer'><\/a>\n\nIn the NameColumnTransformer class we will:\n\n* Extract the title from the name column,\n* Extract the Rank from the name column,\n* Sum SibSp and Parch to get the number of followers,\n* Extract the Surname from Name column for people that have families on board (we will use binary encoding).\n\nYou can read the comments to catch what is going on.","c66c31c0":"## Implementing CabinColumnTransformer <a id='cabincolumntransformer'><\/a>\n\nIn the CabinColumnTransformer class we will:\n\n* Extract the first char of the Cabin column values.\n\nYou can read the comments to catch what is going on.","5579af01":"# Introduction   <a id='introduction'><\/a>\n\nThis notebook is designed to be an introductory level tutorial for using pipelines from the scikit-learn library. We will be using the well known Titanic dataset for this purpose. Although we will submit a solution in the end, I have no intention to get a good accuracy score from our model. The model will be used for learning purposes only and it will probably overfit the data.\n\nBy the end of this notebook we will gain an understanding of how to make pipelines by chaining multiple estimators, how to use column transformers and feature unions for parallel processing within pipelines, and how to implement derived custom transformer classes for series processing within pipelines.\n\nWe will also use encoders and imputers within our pipelines and understand fit-transform mechanism. To do all of these jobs we will use a very complex model which will definetely overfit the data. We will see this by comparing our cross-validation results and test score.\n\nPlease feel free to correct me if I've got mistakes, and also if you face any problem or cannot implement the code just let me know via the comments. I will try to answer as soon as possible. Thank you for reading.\n\n\n![jj-ying-4XvAZN8_WHo-unsplash.jpg](attachment:jj-ying-4XvAZN8_WHo-unsplash.jpg)\n\nPhoto by Danil Sorokin on Unsplash\n\n# Table of Contents\n* [Introduction](#introduction)\n* [Pipelines](#pipelines)\n    * [The Purpose of a pipeline](#purpose)\n    * [Helper Functions](#functions)\n    * [Loading Data](#loading)\n    * [Getting Started](#gettingstarted)\n* [Transformers](#transformers) \n    * [Custom Transformers](#customtransformers)\n    * [Column Transformers](#columntransformers)\n    * [Feature Unions](#featureunions)\n* [Implementation](#implementation) \n    * [Implementing NameColumnTransformer](#namecolumntransformer)\n    * [Implementing AgeColumnTransformer](#agecolumntransformer)\n    * [Implementing CabinColumnTransformer](#cabincolumntransformer)\n    * [Result of Applying Custom Transformers](#customtransformerresult)\n    * [Implementing Column Transformer](#implementingcolumntransformer)\n    * [Implementing Feature Union](#implementingfeatureunion)\n    * [Implementing the Model](#implementingthemodel)\n    * [Validation](#validation)\n    * [Cross-Validation](#crossvalidation)\n    * [Prediction](#prediction)\n* [Submission](#submission) \n* [Conclusion](#conclusion) \n* [References](#references)\n\n\n# Pipelines <a id='pipelines'><\/a>\n\nA sklearn pipeline combines and links multiple estimators into one single estimator. Estimators are objects that learn from data; as a result model algorithms (classification, regression, clustering) or a transformer that extracts\/filters useful features from raw data are counted as estimators (they inherit from the same BaseEstimator class). Imputers and encoders are also estimators. Estimators must have a fit method. While models also implement predict method, transformers implement transform method as we will see below.\n\nThe Pipeline can be constructed using a list of (step_name, estimator) pairs. Below you can find an example taken from the [Scikit-learn official page](https:\/\/scikit-learn.org\/stable\/index.html).\n\n```\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nestimators = [('reduce_dim', PCA()), ('clf', SVC())]\npipe = Pipeline(estimators)\n```\n\n## The Purpose of a pipeline <a id='purpose'><\/a>\n\nWhy is binding multiple estimators useful? There is often a fixed sequence of steps in processing the data and by using pipelines we combine those into one single step:\n\n* We encapsulate the data by calling fit and predict only once on our data, this helps us writing cleaner code and as a result less bugs are intruduced.\n\n* We can make common parameter selection by using grid search in the pipeline. That is we will use grid search once and call all the parameters from all of the estimators altogether.\n\n* Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors (avoid data leakage). \n\nOne of the most important features of a pipeline is that all estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.). In this notebook we will use both a pipeline ending with a model and pipelines consists of transformers only.\n\nNow lets check the data and begin our pipeline design... \n","e5f6aa63":"# Conclusion   <a id='conclusion'><\/a>\n\nOur submission score came to be as 0,78708. As we all expected, it is far too small than our validation score which suggests an overfit. On the other hand, maybe our model was not good enough to be generalized. Anyway as I have already stated, the main intention of this Notebook is to introduce the reader with scikit-learn pipelines.\n\nTo summarize we have learned the following concepts throughout this notebook:\n\n* Pipelines,\n* Estimators,\n* fit and transform mechanism,\n* Custom Transformers,\n* Column Transformers,\n* Feature Unions,\n* Implementing a scikit-learn pipeline from start to end with tabular data.\n\nThank you all for reading to the end. That's all folks!","2ea191d5":"## Helper Functions   <a id='functions'><\/a>   \n\nWe will use some helper functions throughout the notebook. Collecting them in one place is a good idea. It makes the code more organized.","0573ef70":"## Cross-Validation<a id='crossvalidation'><\/a>\n\nWe can also cross-validate our pipeline.","a20a50ae":"We have 5 numerical columns and 4 categorical columns. We have also 3 columns with missing values. One of them is numerical and two of them are categorical. \n\nOK, what can we do with these 9 columns?\n\nWe need to impute the missing columns and encode categorical data. \n\nWe can sum SibSp and Parch columns to see that if the related person has a family on board.\n\nWe have a Name column that we can extract titles and surnames. There will be tons of surnames. We may reduce them a bit by only counting the families (```SibSp + Parch > 0```) in the ship. Maybe the fate of the family members was alike. But we have to find a suitable encoder for them. One-hot encoding will bring too many columns. We can even differentiate high-rank society members such as Countess or Jonkheer from others. Maybe they have been favored more than the others.\n\nThe Cabin column has lots of null values. The valid values for this column start with a character and continue with numbers. Maybe we can extract those starting letters and use them.\n\nNow we can make the initial design but before starting I want to emphasize again that our goal in this notebook is not to solve the Titanic challenge. I am aware that we will overfit the data by doing all of those things we described above. The one and only purpose of this notebook is to introduce the reader to the Sklearn pipelines. So we do not care about overfitting the data.\n\nOur workflow will be as follows:\n\n![Titanic%20flowchart.png](attachment:Titanic%20flowchart.png)\n\n","18f464b2":"# References    <a id='references'><\/a>  \n* [Scikit-Learn Pipelines with Custom Transformer \u2014 A Step by Step Guide](https:\/\/medium.com\/analytics-vidhya\/scikit-learn-pipelines-with-custom-transformer-a-step-by-step-guide-9b9b886fd2cc)\n* [Using Custom Transformers in Your Machine Learning Pipelines with scikit-learn](https:\/\/g-stat.com\/using-custom-transformers-in-your-machine-learning-pipelines-with-scikit-learn\/)\n* [Dataset Transformations - Scikit-learn](https:\/\/scikit-learn.org\/stable\/data_transforms.html)\n* [Tuning the hyper-parameters of an estimator](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)\n* [Using scikit-learn Pipelines and FeatureUnions](http:\/\/zacstewart.com\/2014\/08\/05\/pipelines-of-featureunions-of-pipelines.html)\n* [Intermediate Machine Learning Course - Pipelines](https:\/\/www.kaggle.com\/alexisbcook\/pipelines)\n* [Feature Encoding Techniques](https:\/\/www.geeksforgeeks.org\/feature-encoding-techniques-machine-learning\/) \n* [@alexisbcook](https:\/\/www.kaggle.com\/alexisbcook)    \n* [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview)"}}