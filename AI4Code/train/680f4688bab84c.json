{"cell_type":{"9e617d74":"code","34c8fa82":"code","a0a79c04":"code","6ea8103b":"code","2a35389d":"code","3b1ae723":"code","013754f1":"code","d240c4d7":"code","4f8e7507":"code","f63993c1":"code","e17f61fc":"code","7641937f":"code","bbaae06a":"markdown","902dc1bb":"markdown","4d45133f":"markdown","cb2e56c5":"markdown","efc4c258":"markdown","3d3a9efd":"markdown","8e7dc7ac":"markdown","9b14236e":"markdown"},"source":{"9e617d74":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport optuna\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\nimport sklearn\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","34c8fa82":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf1 = pd.read_csv(\"..\/input\/fin-stack\/train_pred_XGB.csv\")\ndf1.columns = [\"id\", \"pred_1\"]\ndf2 = pd.read_csv(\"..\/input\/blending-n\/train_pred_XGB1(1).csv\")\ndf2.columns = [\"id\", \"pred_2\"]\ndf3 = pd.read_csv(\"..\/input\/fin-stack\/train_pred_XGB3.csv\")\ndf3.columns = [\"id\", \"pred_3\"]\ndf4 = pd.read_csv(\"..\/input\/blending-n\/train_pred_LGB(1).csv\")\ndf4.columns = [\"id\", \"pred_4\"]\ndf5 = pd.read_csv(\"..\/input\/catboost-nik\/train_pred_CatB.csv\")\ndf5.columns = [\"id\", \"pred_5\"]\n\ndf_test1 = pd.read_csv(\"..\/input\/fin-stack\/test_pred_XGB.csv\")\ndf_test1.columns = [\"id\", \"pred_1\"]\ndf_test2 = pd.read_csv(\"..\/input\/blending-n\/test_pred_XGB1(1).csv\")\ndf_test2.columns = [\"id\", \"pred_2\"]\ndf_test3 = pd.read_csv(\"..\/input\/fin-stack\/test_pred_XGB3.csv\")\ndf_test3.columns = [\"id\", \"pred_3\"]\ndf_test4 = pd.read_csv(\"..\/input\/blending-n\/test_pred_LGB(1).csv\")\ndf_test4.columns = [\"id\", \"pred_4\"]\ndf_test5 = pd.read_csv(\"..\/input\/catboost-nik\/test_pred_CatB.csv\")\ndf_test5.columns = [\"id\", \"pred_5\"]\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")\n\ndf.head()","a0a79c04":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\",\"pred_5\"]\ndf_test = df_test[useful_features]\n\n\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n        reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n        \n        model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=9000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth)\n        \n        \n        model.fit(xtrain, ytrain, early_stopping_rounds=300,eval_set=[(xvalid, yvalid)],  verbose=1000)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","6ea8103b":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n\n    params = {'learning_rate': 0.031354783240695655,\n 'reg_lambda': 0.004484898434507873,\n 'reg_alpha': 3.429535691214872e-07,\n 'subsample': 0.14371213599791083,\n 'colsample_bytree': 0.562816437109489,\n 'max_depth': 2}\n    \n    model = XGBRegressor(\n        n_jobs=-1,\n        \n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=9000,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"level1_test_pred_1.csv\", index=False)","2a35389d":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\n\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        params = {'n_estimators':trial.suggest_int(\"n_estimators\", 100,105),\n              'min_weight_fraction_leaf':trial.suggest_float(\"min_weight_fraction_leaf\", 0.01,0.05),\n                        'min_samples_leaf':trial.suggest_int(\"min_samples_leaf\", 2,7),\n                        'max_depth': trial.suggest_int(\"max_depth\", 55, 65),\n                        'min_samples_split': trial.suggest_int(\"min_samples_split\", 10,15),\n                        'n_jobs':-1,\n                        'max_features':'sqrt',\n                        'oob_score':False,\n                        'verbose':False,\n                        'random_state':7, \n                        'warm_start':False, 'bootstrap':True,\n                        'max_leaf_nodes' : None,\n                        'min_impurity_split':None}\n        \n        \n        \n        \n        model = RandomForestRegressor(**params)\n        model.fit(xtrain, ytrain)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","3b1ae723":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    params = {'n_estimators': 102,\n 'min_weight_fraction_leaf': 0.014050550614029426,\n 'min_samples_leaf': 3,\n 'max_depth': 64,\n 'min_samples_split': 13,'n_jobs':-1,\n                        'max_features':'sqrt',\n                        'oob_score':False,\n                        'verbose':False,\n                        'random_state':7, \n                        'warm_start':False, 'bootstrap':True,\n                        'max_leaf_nodes' : None,\n                        'min_impurity_split':None}\n    \n    model = RandomForestRegressor(**params)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_2.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_2\"]\nsample_submission.to_csv(\"level1_test_pred_2.csv\", index=False)","013754f1":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\n\n\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n        model = CatBoostRegressor(**cat_parameters_1,task_type = \"GPU\")\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","d240c4d7":"useful_features = [\"pred_2\", \"pred_3\", \"pred_4\", \"pred_1\", \"pred_5\"]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    params = {'iterations': 7126,\n 'learning_rate': 0.2433541796869192,\n 'l2_leaf_reg': 177,\n 'random_strength': 4.029534673251209,'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n    \n    model = CatBoostRegressor(**params, task_type = \"GPU\")    \n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"level1_train_pred_3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_3\"]\nsample_submission.to_csv(\"level1_test_pred_3.csv\", index=False)","4f8e7507":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf1 = pd.read_csv(\".\/level1_train_pred_1.csv\")\ndf2 = pd.read_csv(\".\/level1_train_pred_2.csv\")\ndf3 = pd.read_csv(\".\/level1_train_pred_3.csv\")\n\ndf_test1 = pd.read_csv(\".\/level1_test_pred_1.csv\")\ndf_test2 = pd.read_csv(\".\/level1_test_pred_2.csv\")\ndf_test3 = pd.read_csv(\".\/level1_test_pred_3.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\n\ndf.head()","f63993c1":"useful_features = [\"pred_2\", \"pred_3\", \"pred_1\"]\n\ndef run(trial):\n    \n    \n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain.target\n        yvalid = xvalid.target\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        model = CatBoostRegressor(**cat_parameters_1, task_type = \"GPU\")\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n\n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","e17f61fc":"useful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]\n\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    cat_parameters_1 = {'iterations': 5958,\n 'learning_rate': 0.05235006201456179,\n 'l2_leaf_reg': 140,\n 'random_strength': 2.2544196494134883,\n                        'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n    model = CatBoostRegressor(**cat_parameters_1, task_type = \"GPU\")\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","7641937f":"sample_submission.target = np.mean(np.column_stack(final_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","bbaae06a":"# **Importing Blended Files Generated from :**\n## https:\/\/www.kaggle.com\/snikhil17\/tuning-blending-of-xgb-catboost-lightgbm\/edit","902dc1bb":"# **Model 2 tuning (RandomForestRegressor)**","4d45133f":"# Hpertuning Final Model","cb2e56c5":"# **Tuning 3rd Model**","efc4c258":"# **HyperParamerter Tuning of XGBRegressor (Model 1)**","3d3a9efd":"# Final Model","8e7dc7ac":"# **Model 2: RandomForestRegressor**","9b14236e":"# **Model 3: CatBoost**"}}