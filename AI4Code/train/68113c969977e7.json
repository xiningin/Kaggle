{"cell_type":{"8ea4e030":"code","c57edc61":"code","5f605ce6":"code","7d05b052":"code","9fa3f999":"code","f4610144":"code","9bc02ec9":"code","b26bfb9e":"code","b0d83879":"code","b85375e8":"code","a7803182":"code","978cec94":"code","e3154d5d":"code","7c17686f":"code","9c49b165":"markdown","fd75d281":"markdown","3e1caa73":"markdown","be336ced":"markdown","1a552d27":"markdown","23c2f888":"markdown","97d7dc75":"markdown"},"source":{"8ea4e030":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","c57edc61":"creditcard_data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","5f605ce6":"#describe the dara\ncreditcard_data.describe()","7d05b052":"#printing the information like column and row counts\n\ncolumns = creditcard_data.columns\n\nprint('Various columns are : ')\nfor name in columns:\n    print(name)","9fa3f999":"#finding about the missing data\n\ncreditcard_data.isna()","f4610144":"#let's just have a look onto the overall data\n\ncreditcard_data.head(10)","9bc02ec9":"plt.figure(figsize = (10, 10))\nsb.lineplot(data = creditcard_data['Amount'])","b26bfb9e":"plt.figure(figsize = (10, 10))\nsb.lineplot(data = [creditcard_data['Time'], creditcard_data['Amount']])","b0d83879":"data = creditcard_data[['Time', 'Amount']]\nlabels = creditcard_data['Class']","b85375e8":"plt.figure(figsize = (10, 10))\nsb.heatmap(data.corr(), cmap = 'rainbow', annot = True)","a7803182":"model = keras.Sequential([\n    layers.BatchNormalization(),\n    \n    layers.Dense(units = 2, activation = 'relu'),\n    layers.Dropout(0.2),\n    \n    layers.Dense(units = 1, activation = 'sigmoid'),\n    layers.Dropout(0.2)\n]\n)","978cec94":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])","e3154d5d":"history = model.fit(data, labels, epochs = 10, validation_split = 0.7, shuffle = True)","7c17686f":"history_df = pd.DataFrame(history.history)\n\nprint('Best validation loss : ' + str(history_df['val_loss'].min()))\nprint('Best Accuray  : ' + str(history_df['binary_accuracy'].max()))","9c49b165":"## Data visualization\n### Now that we have the data we can view the various aspects of the data","fd75d281":"## importing required libraries like\n    tensorflow for creating model\n    pandas to read the data\n    matplotlib and seaborn for visualization","3e1caa73":"### plot of Time vs Amount","be336ced":"#### as we do not have any seperate dataset, we have to split our current data in training and validation purpose\n\n\n\n#### We will build a sequential model with follwing parameters\n\n    1. Batach normalization layer\n    2. Dense layer with two units and relu activation\n    3. Dropout layer\n    4. Dense layer with one unit with sigmoid activation\n    5. Dropout layer","1a552d27":"### We have the data that we will need in a CSV file so in order to access the data we need to use pandas library","23c2f888":"### Variations in amount","97d7dc75":"#### From the above correlation, it's pretty clear that we cannot use regression analysis for this problem as no feature is related to the class\n\n#### So we will have to try buiding our own neural network"}}