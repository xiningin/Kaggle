{"cell_type":{"81d8ad28":"code","c8566cd9":"code","cc465b04":"code","204d279c":"code","8f6a00fc":"code","debf1d4e":"code","2f0696eb":"code","71b1045f":"code","bd3821f0":"code","39f4f44d":"code","d69a7fe1":"code","a81f16cc":"code","13ca8560":"code","d2b17ef3":"code","9868ecd4":"markdown","b997cc7e":"markdown","cf8cb1cc":"markdown","7bd4d1ec":"markdown","53b595c5":"markdown","96531f46":"markdown","cb61c60e":"markdown","65be9216":"markdown","8f372fcc":"markdown","02a68228":"markdown","6fa682e3":"markdown"},"source":{"81d8ad28":"import numpy as np\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# import sklearn\n# from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n# from sklearn import model_selection\n# from sklearn import metrics\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import train_test_split\n# from sklearn.model_selection import GridSearchCV\n# Going to use these 5 base models \n# from sklearn.ensemble import (RandomForestClassifier, \n#                               GradientBoostingClassifier, ExtraTreesClassifier)\n\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import KFold, StratifiedKFold\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n\n# from xgboost import XGBClassifier \n# import xgboost as xgb \n\n","c8566cd9":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\n#  Numerical Features: Age (Continuous), Fare (Continuous), SibSp (Discrete), Parch (Discrete)\n#  Categorical Features: Survived, Sex, Embarked, Pclass\n#  Alphanumeric Features: Ticket, Cabin\n# There are 567 tickets in the training set with 712 examples. It may be because someone may be with children or other commpanies. So 'Alone' feature could be extracted\n\n\nCATEGORICAL_COLUMNS = ['Sex', 'SibSp', 'Parch', 'Pclass',\n                       'Embarked']\nNUMERIC_COLUMNS = ['Age', 'Fare']\nTARGET = 'Survived'\n\n\ntrain_df= train_df[CATEGORICAL_COLUMNS+NUMERIC_COLUMNS+[TARGET]]\ntest_df= test_df[CATEGORICAL_COLUMNS+NUMERIC_COLUMNS]\nprint(train_df.isnull().sum())\nprint(\"-\"*10)\nprint(test_df.isnull().sum())","cc465b04":"\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)\n\ntest_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntest_df['Embarked'].fillna(test_df['Embarked'].mode()[0], inplace = True)\ntest_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)","204d279c":"train_df, eval_df = train_test_split(train_df, test_size=0.2)\ny_train = train_df.pop('Survived')\ny_eval = eval_df.pop('Survived')","8f6a00fc":"#Discrete variables\n# train_df['FamilySize'] = train_df ['SibSp'] + train_df['Parch'] + 1\n# train_df['IsAlone'] = 1 #initialize to yes\/1 is alone\n# train_df['IsAlone'].loc[train_df['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n# train_df['FareBin'] = pd.qcut(train_df['Fare'], 4)\n# train_df['AgeBin'] = pd.cut(train_df['Age'].astype(int), 5)\n\n\n# train_df['Title'] = train_df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n# stat_min = 10 \n# #this will create a true false series with title name as index\n# title_names = (df_train['Title'].value_counts() < stat_min) \n\n\n# df_train['Title'] = df_train['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n# #this will create a true false series with title name as index\n# title_names = (df_test['Title'].value_counts() < stat_min) \n\n\n# df_test['Title'] = df_test['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n# print(df_train['Title'].value_counts())\n# print(df_test['Title'].value_counts())","debf1d4e":"train_df.head()","2f0696eb":"# #code categorical data\n# label = LabelEncoder()\n# for dataset in data:    \n#     dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n#     dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n#     dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n#     dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n#     dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n# df_train.head(1)","71b1045f":"print(train_df.values.shape)\nprint(y_train.values.shape)\nprint(test_df.values.shape)\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ntrain_df['Sex'] = label.fit_transform(train_df['Sex'])\ntrain_df['Embarked'] = label.fit_transform(train_df['Embarked'])\neval_df['Sex'] = label.fit_transform(eval_df['Sex'])\neval_df['Embarked'] = label.fit_transform(eval_df['Embarked'])\ntest_df['Sex'] = label.fit_transform(test_df['Sex'])\ntest_df['Embarked'] = label.fit_transform(test_df['Embarked'])","bd3821f0":" y_train.values.shape","39f4f44d":"model = tf.keras.Sequential([\n    layers.Dense(20, activation='relu'),\n    layers.Dense(1, activation='relu')\n])\n\nmodel.compile(loss='binary_crossentropy',\n                  optimizer=tf.optimizers.Adam(),\n                  metrics=['acc'])\n    \nmodel.fit(train_df.values, y_train.values,\n              epochs=10,\n              batch_size=2,\n              verbose=1,\n              shuffle=True,\n              validation_data=(eval_df.values, y_eval.values))\n    ","d69a7fe1":"# Cross validate model with Kfold stratified cross val\n# \u5bf9\u4e8e\u5206\u7c7b\u95ee\u9898\uff0c\u5e94\u8be5\u4f7f\u7528\u5206\u5c42\u62bd\u6837\uff08stratified sampling\uff09\u6765\u751f\u6210\u6570\u636e\uff0c\u4fdd\u8bc1\u6b63\u8d1f\u4f8b\u7684\u6bd4\u4f8b\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6bd4\u4f8b\u76f8\u540c\nkfold = StratifiedKFold(n_splits=10, shuffle=False)\n\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, df_train[features], df_train[Target], cv=kfold, scoring='accuracy')\n    k_scores.append(scores.mean())\n\nprint(k_scores)","a81f16cc":"# g = sns.scatterplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\")","13ca8560":"import warnings\nwarnings.filterwarnings(\"ignore\")\nknn.fit(df_train[features],df_train[Target])\n","d2b17ef3":"\ntest_Survived = pd.Series(knn.predict(df_test[features]), name=\"Survived\")\nresults = pd.concat([IDtest,test_Survived],axis=1)\nfile_name = \"KNN.csv\"\nresults.to_csv(file_name,index=False)","9868ecd4":"# **Introduction**\nThis notebook is a very basic and simple introductory primer to understand how to go from raw datasets to accurate predictions. It covers all the steps that help in dealing with the data at hand, Efficiently!\n\nThe Titanic dataset is a prime candidate for introducing this as many newcomers to Kaggle start out here. ","b997cc7e":"## **Final Prediction on Test set**","cf8cb1cc":"## Exploratory Data ","7bd4d1ec":"**Some Observations:**\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","53b595c5":"### Training models on the entire train dataset and plotting their feature importance","96531f46":"## **Import Libraries**","cb61c60e":"## **Feature Engineer**","65be9216":"### Stratified KFold  Cross validation on different models","8f372fcc":"## **Load Data**","02a68228":"## Neural Network","6fa682e3":"### Convert objects to category using Label Encoder for train and test dataset\n"}}