{"cell_type":{"89b50ddb":"code","9bb858e6":"code","999f7e1d":"code","80ff9651":"code","f7213fb9":"code","2560e06d":"code","08a48ea1":"code","898ff499":"code","1f17afa2":"code","b0c72246":"code","a2d519fa":"code","836c8c54":"code","fcb87cd4":"markdown","b4b8a2bf":"markdown","17ac76b3":"markdown","8f0ec07d":"markdown","84f7df72":"markdown","53935bb6":"markdown","9dae814a":"markdown","beb2bfa4":"markdown","de0ff633":"markdown","736f4504":"markdown","ad75642a":"markdown","e3a81e12":"markdown","da5e7504":"markdown","7feb2abc":"markdown"},"source":{"89b50ddb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nimport seaborn as sns\nfrom catboost import Pool, CatBoostClassifier","9bb858e6":"#Plotting ROC-curves for 2 classes (binary classification):\ndef plot_roc(pred1,y_cat):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(1):\n        fpr[i], tpr[i], _ = roc_curve(y_cat[:, i], pred1[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    for i in range(1):\n        plt.figure()\n        plt.plot(fpr[i], tpr[i], label='ROC-curve (area = %0.2f)' % roc_auc[i])\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'ROC-curve for val_split predictions')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n\n#Plotting confusion matrix\ndef plot_confusion_matrix(cm, names):\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt = 'g',cmap=\"YlGnBu\"); #annot=True to annotate cells\n    # labels, title and ticks\n    ax.set_xlabel('Predicted values', fontsize=10)\n    ax.xaxis.set_label_position('bottom')\n    plt.xticks(rotation=0)\n    ax.xaxis.set_ticklabels(names, fontsize = 10)\n    ax.xaxis.tick_bottom()\n    ax.set_ylabel('True values', fontsize=10)\n    ax.yaxis.set_ticklabels(names, fontsize = 10)\n    plt.yticks(rotation=0)\n    plt.title('confusion matrix', fontsize=20)\n    plt.tight_layout()\n    plt.show()\n","999f7e1d":"train_file_path = \"..\/input\/titanic\/train.csv\"\nX = pd.read_csv(train_file_path)\n\nlearnoutput = X.Survived\n\nfeatures = [\"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\nX = pd.get_dummies(X[features], columns=[\"Pclass\", \"Sex\", \"Parch\", \"SibSp\"], prefix=[\"Pclass\", \"Sex\", \"Parch\", \"SibSp\"])\nX.fillna(-99999, inplace=True)\n\nX['Cabin'] = X['Cabin'].apply(lambda x: x[0] if x != -99999 else x)\nlearninput = X\n\ncat_features_index = np.where(X.dtypes != float)[0]\n# fig = xp.scatter(X, x=\"Age\", y=\"Survived\",color=\"Survived\",width=1200, height=500,color_discrete_sequence=cm)\n# fig.show()\n\ntest_file_path = \"..\/input\/titanic\/test.csv\"\nY = pd.read_csv(test_file_path)\nPassId = Y.PassengerId\nY = pd.get_dummies(Y[features], columns=[\"Pclass\", \"Sex\", \"Parch\", \"SibSp\"], prefix=[\"Pclass\", \"Sex\", \"Parch\", \"SibSp\"])\nY.fillna(-99999, inplace=True)\n\nY['Cabin'] = X['Cabin'].apply(lambda x: x[0] if x != -99999 else x)\nY.drop('Parch_9', axis=1, inplace=True)\ntestinput = Y","80ff9651":"X_train, X_test, y_train, y_test = train_test_split(learninput, learnoutput, test_size=0.05, shuffle=True, random_state=0)","f7213fb9":"pool = Pool(X_train, y_train, cat_features=cat_features_index, feature_names=list(X.columns))","2560e06d":"params = {\n            'depth': 8,\n            'iterations': 1000,\n            'l2_leaf_reg': 25,\n            'learning_rate': 0.05,\n            'random_strength': 0.5,\n            'random_state': 0\n    }","08a48ea1":"model_cat = CatBoostClassifier(**params,\n                               loss_function='Logloss',\n                               eval_metric='Accuracy',\n                               nan_mode='Min',\n                               task_type='CPU',\n                               verbose=True)\n\nmodel_cat.fit(pool,\n              eval_set=(X_test, y_test),\n              verbose_eval=300,\n              early_stopping_rounds=500,\n              use_best_model=True,\n              )\n\nprint(\"Train Accuracy : %.2f\"%model_cat.score(X_train, y_train))","898ff499":"model_cat.plot_tree(\n    tree_idx=0,\n    pool=pool\n)","1f17afa2":"val_predictions = model_cat.predict(X_test)\n\ny_test = y_test.to_numpy()\n\ncm = confusion_matrix(val_predictions, y_test)\nproducts=['Dead','Alive']\nplot_confusion_matrix(cm, products)","b0c72246":"print(\"Test  Accuracy : %.2f\"%model_cat.score(X_test, y_test))","a2d519fa":"y_test = np.reshape(y_test, (len(y_test), 1))\nval_predictions = np.reshape(val_predictions, (len(val_predictions), 1))\n\n\nplot_roc(val_predictions, y_test)","836c8c54":"predictions = model_cat.predict(testinput)\noutput = pd.DataFrame({'PassengerId': PassId.values, \"Survived\": predictions})\noutput.to_csv(\".\/my_submission.csv\", index=False)","fcb87cd4":"# Split our train dataset on train and test split:\nThis step is needed to:\n\nControl the distribution of validation sample\n\nI use validation sampling to evaluate accuracy of decision trees algorithm and build ROC-curve and confusion matrix (random_state=0 for stability)","b4b8a2bf":"# Main cast:","17ac76b3":"# Set parameters for CatBoostClassifie\n1. I create an object of the class CatBoostClassifie pass parameters to it:\n* loss_function='Logloss' - binary classification\n* eval_metric='Accuracy' - metric for evaluating\n\n2. The fit function is needed to train the model\n* verbose_eval=300 - log info with\n* early_stopping_rounds to prevent overfitting\n\n\nI tried to use plot=True, but it doesn't work ","8f0ec07d":"# Some transformations of data for building confusion matrix and ROC-curve + AUC of test split from train_test_split\n\n1. The model has already been trained on the training split sample, I use it to predict the values of those who survived in the test split sample from train_test_split\n1. Build confusion matrix","84f7df72":"# Model parameters\nAt this step, I determine the optimal parameters of the model, some of them I selected manually, and some I spied on in the documentation\n\nrandom_state=0 for stability of results","53935bb6":"I output the recognition accuracy of the training sample to the console","9dae814a":"# Saving results of prediction to csv-file","beb2bfa4":"# Creating Pool-class\n\nI create Pool-class to pass it to the fit and plot_tree functions","de0ff633":"# Extracting data from .csv to DataFrames  + feature engineering\nExtracting data from train and test datasets\n\nI define the features that I will use further. \n\n\"Pclass\", \"Sex\", \"Parch\", \"SibSp\" -- transform to onehot vector and add to input dataset\n\nIf you find the average of empty cells using SimpleImputer() from sklearn.impute, then the training result is worse than replacing empty and zero values with -99999. So I used this option to increase accuracy\n\ncat_features_index -- determine the numbers of the columns in which the format data is not float and put it in a separate array for feeding into the class object pool\n\nI also removed the numbers of the cabins, leaving only their first letter. Like C, G, E etc...","736f4504":"# Plotting a tree number 0\nI used it to watch how it works","ad75642a":"Build ROC-curve and AUC","e3a81e12":"# I was inspired to start learning ML and feature engineering thanks to this video:\n# <a href=\"https:\/\/youtu.be\/dQw4w9WgXcQ\">Machine Learning \u2014 Andrew Ng, Stanford University<\/a>\nP.S. I hear the voice of fate, speaking my name in humble supplication.","da5e7504":"# Functions for plotting ROC-curves and confusion matrix\nFunctions configured on binary classification, but it can be multiclass classification.","7feb2abc":"# Greetings, my friends!\n\nHere u can see my solution of titanic task:"}}