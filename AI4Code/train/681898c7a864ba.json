{"cell_type":{"7238434a":"code","9a36169d":"code","0272248a":"code","cbfad307":"code","fd4b28df":"code","be92c627":"code","140ba76b":"code","1118741d":"code","36a63386":"code","808b4ce2":"code","70839063":"code","57cd0c70":"code","904f22aa":"markdown","29f56835":"markdown","abfcef80":"markdown","5efe1ed8":"markdown","2cfbb317":"markdown","3c931d19":"markdown","dbd40002":"markdown","88545773":"markdown","74af7503":"markdown","8a71cf72":"markdown","b40e991c":"markdown","23ff4c15":"markdown","d6456862":"markdown","5ce8907b":"markdown","03882605":"markdown"},"source":{"7238434a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a36169d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix , classification_report\nfrom sklearn.model_selection import KFold , cross_val_score\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.model_selection import GridSearchCV","0272248a":"df = pd.read_csv('..\/input\/diabetes-uci-dataset\/diabetes.csv')\ndf.head()","cbfad307":"df = df.replace(['Male','Female','Yes','No','Positive','Negative'],(1,0,1,0,1,0))\ndf.head()","fd4b28df":"cor_mat = df.corr()\nplt.figure(figsize=(15,10))\ntop_corr_features = cor_mat.index\nsns.heatmap(cor_mat[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","be92c627":"X = df.drop(['class'],axis=1)\ny = df['class']","140ba76b":"bestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['features','Score']\nprint(featureScores.nlargest(16,'Score'))","1118741d":"X = df[['Polydipsia','Polyuria','sudden weight loss','partial paresis','Gender','Irritability','Polyphagia','Alopecia']]\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=17,shuffle=True) ","36a63386":"rf = RandomForestClassifier()\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\n\nscores = cross_val_score(rf, X_train, y_train, scoring='f1', cv=cv, n_jobs=-1)\nprint('Accuracy : '+ str(mean(scores))+' Std Deviation :'+str(std(scores)))\nprint(scores)","808b4ce2":"parameters = [{'n_estimators':[10,50,100,150,200,250],'criterion':['gini','entropy'],'max_features':['auto','sqrt','log2']}]\n\nclf = GridSearchCV(rf,parameters,scoring='f1')\nclf.fit(X_train,y_train)","70839063":"y_pred = clf.predict(X_test)\nprint(classification_report(y_test,y_pred))","57cd0c70":"#false negatives have to be kept low as we can't risk predicting positive patients as negative on the other hand false positives are acceptable as they can be corrected\ncm = confusion_matrix(y_test,clf.predict(X_test)) \ncm","904f22aa":"## Reading the dataset into a dataframe","29f56835":"- Selecting features with chi2 test","abfcef80":"- Separating our independent and dependent variables","5efe1ed8":"- Visualizing the correlation matrix in the form of heatmap","2cfbb317":"## Confusion matrix and Analysis","3c931d19":"# Data Preparation","dbd40002":"## Importing the required libraries","88545773":"- Evaluating the model using k-fold cross validation","74af7503":"# Model Selection","8a71cf72":"- Encoding the categorical features","b40e991c":"- Splitting the dataframe into training and testing","23ff4c15":"# Content\n- [Importing the required libraries](#Importing-the-required-libraries)\n- [Reading the dataset into a dataframe](#Reading-the-dataset-into-a-dataframe)\n- [Data Preparation](#Data-Preparation)\n    - [Encoding the categorical features](#Encoding-the-categorical-features)\n    - [Visualizing the correlation matrix in the form of heatmap](#Visualizing-the-correlation-matrix-in-the-form-of-heatmap)\n    - [Separating our independent and dependent variables](#Separating-our-independent-and-dependent-variables)\n    - [Selecting features with chi2 test](#Selecting-features-with-chi2-test)\n    - [Splitting the dataframe into training and testing](#Splitting-the-dataframe-into-training-and-testing)\n- [Model Selection](#Model-Selection)\n    - [Evaluating the model using k-fold cross validation](#Evaluating-the-model-using-k-fold-cross-validation)\n    - [Finding the best parameters using GridSearchCV](#Finding-the-best-parameters-using-GridSearchCV)\n    - [Printing the results](#Printing-the-results)\n- [Confusion matrix and Analysis](#Confusion-matrix-and-Analysis)","d6456862":"- Finding the best parameters using GridSearchCV","5ce8907b":"- Printing the results","03882605":"# The End\n`If you liked the notebook then don't forget to upvote and suggestions are always welcomed.`\n`Follow me on Linkedin :` __[Atharva_Dumbre](https:\/\/www.linkedin.com\/in\/atharva-dumbre-208b5716b)__"}}