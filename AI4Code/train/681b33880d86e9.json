{"cell_type":{"94175e61":"code","f0d3a52e":"code","d889b47e":"code","46c8b895":"code","c465cf65":"code","121c02c7":"code","c1c48788":"code","7e1e761b":"code","558cf2fd":"code","210d4305":"code","b902de35":"code","864ceed1":"code","1bd04a59":"code","292a77bc":"code","d8cda4a1":"code","3489b066":"code","e607512c":"code","261442f5":"code","2f48f66c":"code","041f9f55":"code","77b487ed":"code","fcf47016":"code","5d7348af":"code","00166404":"code","0d71e312":"code","8b1b9e1c":"code","ff7be7b4":"code","95194640":"code","97253e35":"code","5b59f68d":"code","edbc304a":"code","dafc1bec":"code","feba2e7b":"code","08ef1f07":"code","0373c955":"code","85b336da":"code","b31352c4":"markdown","260470bf":"markdown","8464034b":"markdown","0388f023":"markdown","4d9c8518":"markdown","1620f3e8":"markdown","c88ca50c":"markdown","5d6034a1":"markdown","f40db0bb":"markdown","dce94d34":"markdown","dc4274bf":"markdown","1f6dbddb":"markdown","73f11505":"markdown","cd49dc87":"markdown","ffca67a6":"markdown","5d1a8d52":"markdown"},"source":{"94175e61":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0d3a52e":"import re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom sklearn.metrics import plot_roc_curve\nfrom numpy import interp\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc","d889b47e":"df = pd.read_json('..\/input\/isnacoronanews\/data.json', encoding='utf8')\ndf.head()","46c8b895":"from urllib.parse import urlparse\ndf['url_domain'] = df['link'].apply(lambda x: urlparse(x).netloc)","c465cf65":"df['url_domain'].value_counts().head()","121c02c7":"! pip install pandarallel","c1c48788":"#Code by Dimitrina Zlatkova https:\/\/www.kaggle.com\/didizlatkova\/task-1-source-filtering\n\nimport bs4 as bs\nimport urllib.request\nfrom pandarallel import pandarallel\npandarallel.initialize(progress_bar=True)\n\ndef get_url_title(url):\n    try:\n        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla\/5.0'})\n        source = urllib.request.urlopen(req).read()\n        soup = bs.BeautifulSoup(source,'lxml')\n        if not soup.title:\n            print('No title')\n            print(url)\n            return \"\"\n        return soup.title.text\n    except urllib.error.HTTPError as e:\n        print(e)\n        print(url)\n        return \"\"","7e1e761b":"df['url_title'] = df['link'].parallel_apply(get_url_title)","558cf2fd":"df[['category', 'url_title']].head()","210d4305":"#df['url_title'].value_counts().head()","b902de35":"df['category'].value_counts()","864ceed1":"df['category_len'] = df['category'].astype(str).apply(len)","1bd04a59":"df['category_len'].hist();","292a77bc":"df['publish_day'].value_counts()","d8cda4a1":"#2nd row, third column \n\ndf.iloc[1,3]","3489b066":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"category\"].value_counts()[:10].plot.barh(color='red', title='Categories');","e607512c":"#Code by Lucas Abrah\u00e3o https:\/\/www.kaggle.com\/lucasabrahao\/trabalho-manufatura-an-lise-de-dados-no-brasil\n\ndf[\"publish_day\"].value_counts().plot.barh(color='green', title='Publish Day');","261442f5":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","2f48f66c":"#https:\/\/stackoverflow.com\/questions\/55557004\/getting-attributeerror-float-object-has-no-attribute-replace-error-while\n#To avoid with tqdm AttributeError: 'float' object has no attribute\n\ndf[\"title\"] = df[\"title\"].astype(str)\ndf[\"title\"] = [x.replace(':',' ') for x in df[\"title\"]]","041f9f55":"df['clean_title'] = pd.Series([clean_text(i) for i in tqdm(df['title'])])","77b487ed":"words = df[\"clean_title\"].values","fcf47016":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nls = []\n\nfor i in words:\n    ls.append(str(i))","5d7348af":"ls[:5]","00166404":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# The wordcloud \nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","0d71e312":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk","8b1b9e1c":"stemmer = SnowballStemmer('arabic')","ff7be7b4":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# adapted from https:\/\/github.com\/bakrianoo\/aravec\n# function to clean and normalize text \ndef clean_text(text):\n    search = [\"\u0623\",\"\u0625\",\"\u0622\",\"\u0629\",\"_\",\"-\",\"\/\",\".\",\"\u060c\",\" \u0648 \",\" \u064a\u0627 \",'\"',\"\u0640\",\"'\",\"\u0649\",\"\\\\\",'\\n', '\\t','&quot;','?','\u061f','!']\n    replace = [\"\u0627\",\"\u0627\",\"\u0627\",\"\u0647\",\" \",\" \",\"\",\"\",\"\",\" \u0648\",\" \u064a\u0627\",\"\",\"\",\"\",\"\u064a\",\"\",' ', ' ',' ',' ? ',' \u061f ',' ! ']  \n    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n    text = re.sub(p_tashkeel,\"\", text)\n    p_longation = re.compile(r'(.)\\1+')\n    subst = r\"\\1\\1\"\n    text = re.sub(p_longation, subst, text)\n    text = text.replace('\u0648\u0648', '\u0648')\n    text = text.replace('\u064a\u064a', '\u064a')\n    text = text.replace('\u0627\u0627', '\u0627')\n    \n    for i in range(0, len(search)):\n        text = text.replace(search[i], replace[i])\n        \n    text = text.strip()\n    \n    return text","95194640":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# adapted from https:\/\/github.com\/bakrianoo\/aravec\nimport gensim\nimport spacy \nimport nltk \n\n# Load AraVec model from gensim    \nmodel = gensim.models.Word2Vec.load(\"full_grams_cbow_100_twitter.mdl\")\n\n# export to word2vec format for spacy model\nmodel.wv.save_word2vec_format(\".\/spacyModel\/aravec.txt\")\n# run the following command gzip .\/spacyModel\/aravec.txt   \n# run the following command python -m spacy  init-model ar spacy.aravec.model --vectors-loc .\/spacyModel\/aravec.txt.gz\n \n# load spacy model \nnlp = spacy.load(\".\/spacy.aravec.model\/\")","97253e35":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# adapted from https:\/\/github.com\/bakrianoo\/aravec\n# Define preprocessing class\nclass Preprocessor:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, text):\n        preprocessed = clean_text(text)\n        return self.tokenizer(preprocessed)\n        \n# apply preprocessing class to tokenizer \nnlp.tokenizer = Preprocessor(nlp.tokenizer)","5b59f68d":"!pip install farasa","edbc304a":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# to preprocess Arabic text \nfrom farasa.segmenter import FarasaSegmenter\nfrom arabert.preprocess_arabert import preprocess, never_split_tokens\n\n# to prepare dataset and calculate metrics \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n\n# to use Hugging Face Trainer https:\/\/huggingface.co\/transformers\/main_classes\/trainer.html\nfrom transformers import AutoConfig, BertForSequenceClassification, AutoTokenizer\nfrom transformers.data.processors import SingleSentenceClassificationProcessor\nfrom transformers import Trainer , TrainingArguments","dafc1bec":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# instantiate Farasa segmenter and preprocess text\nfarasa_segmenter = FarasaSegmenter(interactive=True)\ndf_tweets['text'] = df_tweets['text'].apply(lambda x: preprocess(x, \n                                                                 do_farasa_tokenization=True, \n                                                                 farasa=farasa_segmenter, \n                                                                 use_farasapy = True))\n# split dataset into train and test sets \ndf_train, df_test = train_test_split(df_tweets, test_size=0.2)","feba2e7b":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# adapted from https:\/\/github.com\/aub-mind\/arabert\nconfig = AutoConfig.from_pretrained('aubmindlab\/bert-base-arabert',num_labels=2, output_attentions=True) \ntokenizer = AutoTokenizer.from_pretrained('aubmindlab\/bert-base-arabert', \n                                          do_lower_case=False, \n                                          do_basic_tokenize=True, \n                                          never_split=never_split_tokens)\nmodel = BertForSequenceClassification.from_pretrained('aubmindlab\/bert-base-arabert', config=config)\n\ntrain_dataset = SingleSentenceClassificationProcessor(mode='classification')\ntest_dataset = SingleSentenceClassificationProcessor(mode='classification')\n\ntrain_features = train_dataset.get_features(tokenizer = tokenizer, max_length =128)\ntest_features = test_dataset.get_features(tokenizer = tokenizer, max_length =128) ","08ef1f07":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\ntraining_args = TrainingArguments(\".\/train\")\ntraining_args.do_train = True\ntraining_args.evaluate_during_training = True\ntraining_args.adam_epsilon = 1e-8\ntraining_args.learning_rate = 2e-5\ntraining_args.warmup_steps = 0\ntraining_args.per_gpu_train_batch_size = 16\ntraining_args.per_gpu_eval_batch_size = 16\ntraining_args.num_train_epochs = 5","0373c955":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n\n# adapted from https:\/\/github.com\/aub-mind\/arabert \n\ndef compute_metrics(p): #p should be of type EvalPrediction\n  preds = np.argmax(p.predictions, axis=1)\n  assert len(preds) == len(p.label_ids)\n  print(classification_report(p.label_ids,preds))\n  print(confusion_matrix(p.label_ids,preds))\n\n  f1_Positive = f1_score(p.label_ids,preds,pos_label=1,average='binary')\n  f1_Negative = f1_score(p.label_ids,preds,pos_label=0,average='binary')\n  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n  macro_precision = precision_score(p.label_ids,preds,average='macro')\n  macro_recall = recall_score(p.label_ids,preds,average='macro')\n  acc = accuracy_score(p.label_ids,preds)\n  return {\n      'f1_pos': f1_Positive,\n      'f1_neg': f1_Negative,\n      'macro_f1' : macro_f1, \n      'macro_precision': macro_precision,\n      'macro_recall': macro_recall,\n      'accuracy': acc\n  }","85b336da":"#Code by Haaya Naushan https:\/\/towardsdatascience.com\/machine-learning-advancements-in-arabic-nlp-c6982b2f602b\n# instantiate trainer\ntrainer = Trainer(model=model,\n                  args = training_args,\n                  train_dataset = train_features,\n                  eval_dataset = test_features,\n                  compute_metrics = compute_metrics)\n# start training \ntrainer.train()","b31352c4":"\"Snowball Stemmer\n\nThe following languages are supported: Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish and Swedish.\n\nhttps:\/\/www.kite.com\/python\/docs\/nltk.SnowballStemmer","260470bf":"#What have I done? Evrything is empty! Only ticks and commas.","8464034b":"#That's all. I'll try to learn AraVec and AraBERT. And see if they are compatible in Kaggle environment or if it's just some errors of mine again.\n\n#Required Skills:\n\nPython\n\nCode management\/version control\n\nUnderstanding Arabic is a plus   \n\n\nSeems that I don't have any at all!","0388f023":"#Analyse url domains","4d9c8518":"#Saving for a next work since I got an error (language?) that I'm not able to solve now.","1620f3e8":"#Natural Language Processing\n\nTopic Modelling","c88ca50c":"#Farasa: \"A fast and furious Segmenter for Arabic\"","5d6034a1":"![](http:\/\/slideplayer.com\/slide\/13795147\/85\/images\/17\/Why+AraVec+There+are+currently+no+publicly+available+sizable+pretrained+Arabic+word+embeddings..jpg)slidplayer.com","f40db0bb":"#And I failed","dce94d34":"#Trying to extract the title for each article.","dc4274bf":"#Cleaning functions","1f6dbddb":"![](https:\/\/i.ytimg.com\/vi\/wBwmWq8VNqk\/hqdefault.jpg)youtube.com","73f11505":"#Since I couldn't define url_title, I got another error.","cd49dc87":"#I've to learn AraVec and AraBERT to make NLP in arabic language.\n\nI don't know BERT. Next step: learn AraBERT and AraVec.","ffca67a6":"#Spoiler Alert: I failed in my first attempt with both AraBERT and AraVec.","5d1a8d52":"![](https:\/\/data01.123doks.com\/thumbv2\/123dok_us\/000\/675\/675419\/cover.webp)1library.net"}}