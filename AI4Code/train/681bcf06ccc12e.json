{"cell_type":{"80f37153":"code","a7745118":"code","6b33d0b5":"code","631d583f":"code","d8305fc4":"code","d09d961b":"code","c8d1de8f":"code","47af524a":"code","c9112b01":"code","19e25a6b":"code","9164547b":"code","6a1f891c":"code","d8fcd3fb":"code","cfbf7132":"code","338d1539":"code","1d14b855":"code","f1aa1144":"code","3e564149":"code","456b20bc":"code","6d5f8579":"code","07de00a4":"code","5add6104":"code","715f13be":"code","6aa08c4c":"code","afefef14":"code","b8b0756a":"code","22e6bcb7":"code","fc0cde6e":"code","59122c6f":"code","f7791f07":"code","4126ed75":"code","b80fd855":"code","25f66b46":"code","d9830aca":"code","0e53abbb":"code","ab46be04":"code","056bc5a6":"code","937b1d4f":"code","07b5c1a1":"code","f9b8a8d5":"code","b74790b7":"code","b9c4f454":"code","73030010":"code","afd034a4":"code","b966ec40":"code","2ef9bfee":"code","5604c690":"code","dbcb54e3":"markdown","2deda32f":"markdown","7bf24a95":"markdown","293179ba":"markdown","2af7f44d":"markdown","47b5f253":"markdown","5a315cb1":"markdown","802c41ce":"markdown","07e83f13":"markdown","40d69611":"markdown","63387869":"markdown","08e6779b":"markdown","b5fa7832":"markdown","f31b5901":"markdown","f1399539":"markdown","2e463694":"markdown","b5f46fca":"markdown","ef45999d":"markdown","0b1f6f9f":"markdown","6cafb319":"markdown","1fa3fc28":"markdown","6776aced":"markdown","526f8342":"markdown","437cf6f9":"markdown","724ae632":"markdown","ed5c039b":"markdown","d9d67bd4":"markdown","ddd817f3":"markdown"},"source":{"80f37153":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7745118":"#Import data science libraries\nimport pandas as pd\nimport numpy as np\n\n#Import visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","6b33d0b5":"#Load dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","631d583f":"#Check dataset information\ntrain.info()","d8305fc4":"#Dataset description\ntrain.describe(include = 'all')","d09d961b":"#find out column features\ntrain.columns","c8d1de8f":"#data samples\ntrain.sample(5)","47af524a":"#Check for missing values\npd.isnull(train).sum()","c9112b01":"#Treating missing Values\n#Replacing the age value by its mean,Dropiing Cabin column as more than 50% value is missing and Replacing Embarked with its mode vale\ntrain['Age'].fillna((train['Age'].mean()), inplace=True)\ntrain=train.drop(['Cabin'],axis=1)\ntrain['Embarked']=train['Embarked'].fillna(train['Embarked'].mode()[0])\ntrain.isnull().sum()","19e25a6b":"#Numeric Value distribution\nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","9164547b":"#Numeric Value distribution\nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index, df_cat[i].value_counts()).set_title(i)\n    plt.show()","6a1f891c":"#Comparing survival of each category\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Ticket', aggfunc = 'count'))\nprint()\n\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Ticket', aggfunc = 'count'))\nprint()\n\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Ticket', aggfunc = 'count'))\nprint()\n\nprint(pd.pivot_table(train, index = 'Survived', columns = 'SibSp', values = 'Ticket', aggfunc = 'count'))\nprint()\n\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Parch', values = 'Ticket', aggfunc = 'count'))\nprint()","d8fcd3fb":"#Sex Feature using barplot\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)\n\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","cfbf7132":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people that survived by Pclass\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","338d1539":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n#print percentages of females vs. males that survive\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)","1d14b855":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train)","f1aa1144":"##Treating missing Values\n#Replacing the age value by its mean,Dropiing Cabin column as more than 50% value is missing and Replacing Fare with its mode vale\ntest['Age'].fillna(train['Age'].mean(),inplace=True)\ntest=test.drop(['Cabin'],axis=1)\ntest['Fare']=test['Fare'].fillna(test['Fare'].mode()[0])","3e564149":"#Sort ages into logical categories\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","456b20bc":"train[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n#calculate percentages of CabinBool vs. survived\nprint(\"Percentage of CabinBool = 1 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of CabinBool = 0 who survived:\", train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x=\"CabinBool\", y=\"Survived\", data=train)\nplt.show()\n","6d5f8579":"#Using our test data\n\ntest.describe(include=\"all\")","07de00a4":"#I am dropping Cabin because alot of information are missing\n\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","5add6104":"#Lets check if Cabin has been dropped\ntest.describe(include=\"all\")","715f13be":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\ntrain.head()","6aa08c4c":"#create a combined group of both datasets\ncombine = [train, test]\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(train['Title'], train['Sex'])","afefef14":"#replace various titles with more common names\nfor dataset in combine:\n dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n\n dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","b8b0756a":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n dataset['Title'] = dataset['Title'].map(title_mapping)\n dataset['Title'] = dataset['Title'].fillna(0)\ntrain.head()","22e6bcb7":"# fill missing age with mode age group for each title\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n#I tried to get this code to work with using .map(), but couldn't.\n#I've put down a less elegant, temporary solution for now.\n#train = train.fillna({\"Age\": train[\"Title\"].map(age_title_mapping)})\n#test = test.fillna({\"Age\": test[\"Title\"].map(age_title_mapping)})\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n\nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]","fc0cde6e":"#map each Age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult':5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\ntrain.head()\n\n#dropping the Age feature for now, might change\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","59122c6f":"#drop the name feature since it contains no more useful information.\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","f7791f07":"#now we need to fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","4126ed75":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\ntrain.head()","b80fd855":"#fill in missing Fare value in test set based on mean fare for that Pclass\n#Pclass = 3\nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x]\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n\n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)\n\n#check train data\ntrain.head()","25f66b46":"#check test data\ntest.head()","d9830aca":"#we can also drop the Ticket feature since it's unlikely to yield any useful information\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n","0e53abbb":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","ab46be04":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","056bc5a6":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","937b1d4f":"# Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","07b5c1a1":"# Linear SVC\nfrom sklearn.svm import LinearSVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","f9b8a8d5":"# Perceptron\nfrom sklearn.linear_model import Perceptron\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","b74790b7":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","b9c4f454":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","73030010":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","afd034a4":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","b966ec40":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","2ef9bfee":"models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression','Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', 'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],'Score': [acc_svc, acc_knn, acc_logreg, acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree, acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","5604c690":"#It's time to create a submission.csv file to upload to the Kaggle competition!\n#set ids as PassengerId and predict survival\nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","dbcb54e3":"#### Parch Feature","2deda32f":"### Age Feature","7bf24a95":"**Observation**\n* We have a total of 418 passengers.\n* 1 value is missing from the Fare feature.\n* Around 20.5% of the Age feature is missing, we will need to fill that in\n* Around 78.23% of Cabin is missing, we may not be able to fill that in.","293179ba":"Yes, its been dropped","2af7f44d":"We can see that Age is missing 177 values, Cabin is missing 687 values and Embarked is missing 2 values","47b5f253":"**Observation**\n* There, we see that the total no of passengers in the boat were 891\n* There are missing information under Age-19.8%, Cabin-77%, and Embarked-1%","5a315cb1":"### Fare Feature","802c41ce":"## 4) Data Visualization","07e83f13":"## 7) Chosing the Best Model\n\n### Split Train data","40d69611":"### Sex Feature","63387869":"## 1) Import Libraries","08e6779b":"### Embarked Feature","b5fa7832":"#### SibSp Feature","f31b5901":"Observation\n* Sex and Pclass are important factors to predict survival","f1399539":"### Ticket Feature","2e463694":"## Create Submission File","b5f46fca":"#### Age Feature","ef45999d":"### Compare accuracies of all model","0b1f6f9f":"## 3) Data Analysis\n### To explore how reliable the information provided are.","6cafb319":"#### Cabin Feature","1fa3fc28":"## 2) Data Exploration","6776aced":"## 5) Feature Engineering- turning your inputs into things the algorithm can understand","526f8342":"#### Sex Feature","437cf6f9":"#### Pclass Features","724ae632":"## 6) Cleaning Data","ed5c039b":"## Definition of passengers attributes are:\n\n* PassengerID: Unique no for each passenger\n* Survived: our target lable, stating whether a passenger survived or not\n            1 - yes\n            0 - no\n* Pclass: Passenger's travel class\n* Name: Passenger's name with titles such as Mr., Mrs. etc.\n* Sex: Male or female\n* SibSp: Number of passenger's siblings and spouses aboard\n* Parch: Number of passenger's parents and children aboard\n* Ticket: Ticket number\n* Fare: Ticket price (pounds)\n* Cabin: Cabin number\n* Embarked: a location, where a passenger got on board of Titanic","d9d67bd4":"### Test Models","ddd817f3":"### Name Feature"}}