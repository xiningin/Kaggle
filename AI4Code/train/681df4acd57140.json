{"cell_type":{"17c10f60":"code","e003f6cd":"code","d3c454dc":"code","0649ff22":"code","6a33377f":"code","ecc3488c":"code","b39001c1":"code","792351d8":"code","034e3a4f":"code","95f6edc6":"code","e81f046e":"code","a993d9bb":"code","907204ab":"code","886845fd":"code","e57e86c2":"code","4e910d42":"code","2a778822":"code","a752e6c3":"code","bd123e07":"code","d3dec4eb":"code","9da7beec":"code","b99b10b4":"code","ceec249d":"code","49fcb028":"code","3d0405ec":"code","7a7a103b":"code","f5428313":"code","72ffdb8f":"code","11985c98":"code","0a9e6578":"code","3f9fdee1":"code","571884fa":"code","60599828":"code","14ad12c1":"code","90de7d71":"code","cd63085d":"code","3abdb291":"code","a012ca50":"code","3df4baf8":"code","62527b1a":"code","e75f62a3":"code","089ebe07":"code","ea199949":"code","5ce28c87":"code","73af9aa6":"code","a3e11407":"code","88bb9950":"code","447e8d1f":"code","daa5ed42":"code","7ed575d4":"code","e5e7a650":"code","09a3440f":"markdown","1ac00140":"markdown","5492985e":"markdown","7b880e4a":"markdown","d401130c":"markdown","d97d809a":"markdown","b3cf12fa":"markdown","3cdd7d00":"markdown","657ff40a":"markdown","a3252f11":"markdown","c2edafbc":"markdown","035f8948":"markdown","d2539abd":"markdown","446f8bc4":"markdown","b2fbf1b6":"markdown","67f1ac51":"markdown","4404ffaf":"markdown","a925b559":"markdown","daa458cc":"markdown","434f02d1":"markdown","20e1e20d":"markdown","46689429":"markdown","2e36ad94":"markdown","f235b704":"markdown","8e1d7f65":"markdown","7564234b":"markdown","bbdae969":"markdown","6a9168a4":"markdown","8f69da5d":"markdown","b747e96f":"markdown","b16b6808":"markdown","28783ea2":"markdown","36d028e8":"markdown","86bd3fca":"markdown","2dc25f56":"markdown","39e530ea":"markdown","a642a27c":"markdown","5dd7c881":"markdown","3ec30cc2":"markdown","3e52c6cc":"markdown"},"source":{"17c10f60":"# system\nimport os\nimport warnings\n\n# data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\n# setting warning to ignored\nwarnings.filterwarnings(\"ignore\")\nprint(os.listdir(\"..\/input\"))\n\n# importing classifiers\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\n# preprocessing\/ cross-validation\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# evaluation metrics\nfrom sklearn.metrics import make_scorer, roc_auc_score, auc, precision_score, recall_score, classification_report, roc_curve, accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve\n\n# pipeline builder\nfrom sklearn.pipeline import Pipeline\n\n# decompostion \nfrom sklearn.decomposition import PCA\n\n# ensemble models\nimport lightgbm as lgb\n\n# over sampling model\nfrom imblearn.over_sampling import SMOTE","e003f6cd":"# loading the data files\ntrain = pd.read_csv('..\/input\/train.csv', sep=',')\ntest = pd.read_csv('..\/input\/test.csv', sep=',')","d3c454dc":"\n# taking sneak peak to datasets\nprint(f'Dimension of our Train data {train.shape} \\n Data feature informations')\nprint(train.info())\nprint(f'Dimension of our Test data {test.shape} \\n Data feature informations')\nprint(test.info())\n\nprint(train.head(), test.head())\nprint(f'Train columns: {train.columns}\\nTest columns: {test.columns}')","0649ff22":"### EDA for understanding datasets and getting clues for feature selections.\n\n\n# Datatypes in dataset\nprint('Train target column datatype:',train.target.dtype)\nprint('Train var_0 column datatype:',train.var_0.dtype)\n\n\nprint('Train Describe:\\n',train.describe(),'\\nTest Describe:\\n', test.describe())\nprint('Different values in target:\\n',train.target.unique())\n","6a33377f":"# Looking Variance\nprint('Train Variance:\\n',train.var(),'\\nTest Variance:\\n', test.var())\n","ecc3488c":"# Looking Skewness\nprint('Train skewness:\\n',train.skew(),'\\nTest skewness:\\n', test.skew())\n","b39001c1":"# Missing value analysis\nprint('Train missing values:',train.isnull().sum().sum())\nprint('Test missing values:',test.isnull().sum().sum())\n","792351d8":"# Digging target variable\ntarget = train['target']\nprint('Different values in target:\\n',target.value_counts())\nprint('')\nprint(\"There are {}% target values with 1\".format(100 *target.value_counts()[1]\/(target.value_counts()[1] + target.value_counts()[0])))\n\nsns.countplot(train['target'], palette='Set1')\n\nplt.figure(figsize=(10,6))\ntrain['target'].value_counts().plot.pie(autopct='%1.1f%%', explode=([0,0.1]))\nplt.show()\n","034e3a4f":"# Boxplot Analysis\n# Plot  features.\ntrain.iloc[:, 2:50].plot(kind='box', figsize=[16,8])\ntrain.iloc[:, 50:101].plot(kind='box', figsize=[16,8])\ntrain.iloc[:, 101:151].plot(kind='box', figsize=[16,8])\ntrain.iloc[:, 151:].plot(kind='box', figsize=[16,8])\n","95f6edc6":"# Distribution plot Analysis\n\n# Function for quick plot of distribution\ndef plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show() \n    \nt0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\n\n","e81f046e":"\n# First 100 features dustribution\nfeatures = train.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)\n\n# Rest 100 features dustribution\nfeatures = train.columns.values[102:]\nplot_feature_distribution(t0, t1, '0', '1', features)\n\n","a993d9bb":"\n# 1. STD distribution of target variable\n\nt0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train set\")\nsns.distplot(t0[features].std(axis=1),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].std(axis=1),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train set\")\nsns.distplot(t0[features].std(axis=0),color=\"green\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].std(axis=0),color=\"red\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\n","907204ab":"\n# 2. Mean distribution of target variable\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(t0[features].mean(axis=1),color=\"green\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].mean(axis=1),color=\"red\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].mean(axis=0),color=\"red\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n","886845fd":"\n# 3. Min distribution\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n","e57e86c2":"\n# 4. Max distribution\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"gold\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].max(axis=1),color=\"darkblue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n","4e910d42":"\n# 5. Skew distribution\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\n","2a778822":"\n# 6. Kurtosis distribution\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n","a752e6c3":"\n# 7. Median distribution\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of median values per row in the train set\")\nsns.distplot(t0[features].median(axis=1),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].median(axis=1),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of median values per column in the train set\")\nsns.distplot(t0[features].median(axis=0),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].median(axis=0),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n","bd123e07":"\n# 8. Sum distribution\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of sum values per row in the train set\")\nsns.distplot(t0[features].sum(axis=1),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].sum(axis=1),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of sum values per column in the train set\")\nsns.distplot(t0[features].sum(axis=0),color=\"red\", kde=True,bins=150, label='target  0')\nsns.distplot(t1[features].sum(axis=0),color=\"blue\", kde=True,bins=150, label='target  1')\nplt.legend()\nplt.show()\n","d3dec4eb":"\n# Correlation Analysis\ndata_corr=train.drop(['target','ID_code'], axis=1).corr()\nprint('Maximum corr within all variables correlations :', np.sort(train.drop(['target','ID_code'], axis=1).corr())[:,-2:-1].max())\n","9da7beec":"\n# Correlation Heatmap\nplt.figure(figsize=(20,20))\nsns.heatmap(data_corr, square=True)\nplt.title('Feature Correlation')\nplt.show()\n","b99b10b4":"# Data Preprocessiing\n\n# Remove outliers\ntrain_x = train.iloc[:, 1:]\nIQR = train_x.quantile(.75) - train_x.quantile(.25)\nprint(\"Train.shape:\",train.shape)\ndf_in = train[~((train_x < (train_x.quantile(.25) - 1.5 * IQR)) |(train_x > (train_x.quantile(.75) + 1.5 * IQR))).any(axis=1)]\ndf_out = train[((train_x < (train_x.quantile(.25) - 1.5 * IQR)) |(train_x > (train_x.quantile(.75) + 1.5 * IQR))).any(axis=1)]\nprint(\"df_in.shape:\",df_in.shape)\nprint(\"df_out.shape:\",df_out.shape)","ceec249d":"print(\"df_in.target:\\n\", df_in['target'].value_counts())\nprint(\"df_out.target:\\n\", df_out['target'].value_counts())","49fcb028":"# PCA Analysis\n\n\n# feature extraction\npca = PCA().fit(train.drop(['target','ID_code'], axis=1))\n\nplt.figure(figsize=(10,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Eigenvalue')\nleg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3,shadow=False,markerscale=0.4)\nplt.grid(True)\nplt.show()","3d0405ec":"# Using Stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['target', 'ID_code'], axis=1), train['target'], test_size=0.3, random_state=147, stratify=train.target)\nprint('Shape:',X_train.shape, X_test.shape, y_train.shape, y_test.shape)","7a7a103b":"\n\nparameters = {'min_samples_leaf': [10,25]}\nforest = RandomForestClassifier(max_depth=15, n_estimators=15)\ngrid_rfc = GridSearchCV(forest, parameters, cv=3, n_jobs=1, verbose=3, scoring=make_scorer(roc_auc_score))\ngrid_rfc.fit(X_train, y_train)\nimp = grid_rfc.best_estimator_.feature_importances_\nidx = np.argsort(imp)[::-1][-26:]\n\nremove_features_RFC = train.columns[2:]\n#train.drop(remove_features_RFC[idx],axis=1, inplace=True)\n#test.drop(remove_features_RFC[idx],axis=1, inplace=True)\nremove_col = remove_features_RFC[idx]\nprint('Removing features:', remove_col)\nprint('Train shape:',train.shape)","f5428313":"remove_col = ['var_187', 'var_113', 'var_7', 'var_126', 'var_189', 'var_62',\n       'var_117', 'var_45', 'var_182', 'var_96', 'var_199', 'var_19', 'var_68',\n       'var_77', 'var_3', 'var_25', 'var_14', 'var_41', 'var_73', 'var_30',\n       'var_64', 'var_185', 'var_29', 'var_129', 'var_171', 'var_140']\n\ntrainFE = train.drop(remove_col,axis=1)\ntestFE = test.drop(remove_col,axis=1)\nprint('Removing features:', remove_col)\nprint('Columns left in Train :',trainFE.shape)\nprint('Columns left in Test :',testFE.shape)","72ffdb8f":"\nprint('Featuring Engineering raw data: Adding aggregates :')\nidx = features = train.columns[2:]\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)\n\nprint('Train:', train.shape)\nprint('Test:' , test.shape)","11985c98":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(16,6))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();\n\nt0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nfeatures = train.columns.values[202:]\nplot_new_feature_distribution(t0, t1, 'target: 0', 'target: 1', features)","0a9e6578":"\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['target','ID_code'], axis=1), train.target, test_size=0.3, random_state=147, stratify=train.target)\n\nprint('Shape:',X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n","3f9fdee1":"# Spot-Check Algorithms\nmodels = []\nmodels.append(( 'LR' , LogisticRegression(solver='liblinear')))\nmodels.append(( 'CART' , DecisionTreeClassifier()))\nmodels.append(( 'NB' , GaussianNB()))\nmodels.append(('RFC', RandomForestClassifier()))\n","571884fa":"def cv_auc_score(models,scoring, num_folds=3):\n    seed = 147 \n    results = []\n    names = []\n    \n    print('-> 3-Fold cross-validation ',scoring.__name__,'score for the training data for 4 classifiers.')\n    for name, model in models:\n        kfold = KFold( n_splits=num_folds, random_state=seed)\n        cv_results = cross_val_score(model, X_train, y_train, cv=kfold,verbose=3 ,scoring=make_scorer(scoring))\n        results.append(cv_results)\n        names.append(name)\n        print(\"Algo: \", name,'::',np.mean(cv_results))\n    \n    # Compare Algorithms\n    fig = plt.figure()\n\n    fig.suptitle( 'Algorithm Comparison: {}'.format(scoring.__name__ ))\n    ax = fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(names)\n    plt.show()\n\n\n","60599828":"# AUC score\nnum_folds = 3\nscoring=roc_auc_score\nprint(\"Scores without StandardScale\")\ncv_auc_score(models, scoring=scoring, num_folds=num_folds)","14ad12c1":"# Accuracy score\nscoring =  accuracy_score\nprint(\"Scores without StandardScale\")\ncv_auc_score(models, scoring=scoring, num_folds=num_folds)","90de7d71":"def aur_prob_value_precision_recall_curve(models, X_train,  X_test,y_train, y_test):\n    for name, model in models:\n        model.fit(X_train, y_train)\n        \n        y_pred = model.predict_proba(X_test)\n        y_pred2 = model.predict(X_test)\n        \n        print(name,' AUC prob: ',roc_auc_score(y_test, y_pred[:,1]))\n        print(name,' AUC value: ',roc_auc_score(y_test, y_pred2))\n        print(name,' f1 score: ',f1_score(y_test, y_pred2))\n        \n        precision, recall, thresholds = precision_recall_curve(y_test, y_pred[:,1])\n        fpr, tpr, thresholds = roc_curve(y_test, y_pred[:,1])\n        fpr2, tpr2, thresholds2 = roc_curve(y_test, y_pred2)\n        \n        fig, ax = plt.subplots(1,1, figsize=(6,6))  \n        ax.plot(precision, recall)\n        ax.plot(fpr, tpr, color='red')\n        ax.plot(fpr2, tpr2, color='green')\n        ax.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6)) \n        ax.legend([f'Precision-recall: {auc(recall, precision)}',f'AUC Prob: {auc(fpr, tpr)}',f'AUC Value: {auc(fpr2, tpr2)}'])\n       # ax.legned()\n        ax.set_xlabel('False Positive Rate')\n        ax.set_ylabel('True Positive Rate')\n        ax.set_title('Receiver operating characteristic {}'.format(name))\n","cd63085d":"def classification_report_models(models,X_train, X_test, y_train, y_test ):\n    for name, model in models:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        print(name, ':\\n', confusion_matrix(y_test, y_pred))\n        print(name,':\\n',classification_report(y_test, y_pred))","3abdb291":"print(\"AUC curve (Prob and Value) without StandardScale\")\naur_prob_value_precision_recall_curve(models=models, X_train = X_train, X_test= X_test, y_train=y_train, y_test=y_test)\nclassification_report_models(models=models, X_train = X_train, X_test= X_test, y_train=y_train, y_test=y_test)","a012ca50":"\ntr_X = train.drop([ 'ID_code'], axis=1)\ntest_X = test.drop(['ID_code'], axis=1)\nfor col in tr_X.drop(['target'], axis=1).columns:\n    tr_X[col] = ((tr_X[col] - tr_X[col].mean()) \/ tr_X[col].std()).astype('float32')\nfor col in test_X.columns:\n    test_X[col] = ((test_X[col] - test_X[col].mean()) \/ test_X[col].std()).astype('float32')\n  \n\n#Training data\nX=tr_X.drop(['target'],axis=1)\nY=train['target']\n#StratifiedKFold cross validator\ncv=StratifiedKFold(n_splits=5,random_state=147,shuffle=True)\nfor train_index,valid_index in cv.split(X,Y):\n    X_train1, X_valid=X.iloc[train_index], X.iloc[valid_index]\n    y_train1, y_valid=Y.iloc[train_index], Y.iloc[valid_index]\n\nprint('Shape of X_train :',X_train1.shape)\nprint('Shape of X_valid :',X_valid.shape)\nprint('Shape of y_train :',y_train1.shape)\nprint('Shape of y_valid :',y_valid.shape)","3df4baf8":"from imblearn.over_sampling import SMOTE\n#Synthetic Minority Oversampling Technique\nsm = SMOTE(random_state=147, ratio=1.0)\n#Generating synthetic data points\nX_smote,y_smote=sm.fit_sample(X_train1,y_train1)\nX_smote_v,y_smote_v=sm.fit_sample(X_valid,y_valid)","62527b1a":"print(\"AUC curve (Prob and Value) with Standardization and SMOTE oversampling\")\naur_prob_value_precision_recall_curve(models=models, X_train = X_smote, X_test= X_smote_v,  y_train=y_smote, y_test=y_smote_v)\nclassification_report_models(models=models, X_train = X_smote, X_test= X_smote_v, y_train=y_smote, y_test=y_smote_v)","e75f62a3":"#Training the model with simple train_test_split stratified data\n#training data\nlgb_train=lgb.Dataset(X_train,label=y_train)\n#validation data\nlgb_valid=lgb.Dataset(X_test,label=y_test)","089ebe07":"params={'boosting_type': 'gbdt', \n          'max_depth' : -1, #no limit for max_depth if <0\n          'objective': 'binary',\n          'boost_from_average':False, \n          'nthread': 20,\n          'metric':'auc',\n          'num_leaves': 50,\n          'learning_rate': 0.01,\n          'max_bin': 100,      #default 255\n          'subsample_for_bin': 100,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'bagging_fraction':0.5,\n          'bagging_freq':5,\n          'feature_fraction':0.08,\n          'min_split_gain': 0.45, #>0\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'is_unbalance':True,\n          }","ea199949":"# f1_score calculator function\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True\n\nevals_result = {}\n","5ce28c87":"\nnum_rounds=1000\nlgbm1= lgb.train(params,lgb_train,num_rounds,valid_sets=[lgb_train,lgb_valid],feval=lgb_f1_score,verbose_eval=100,early_stopping_rounds = 500)\n","73af9aa6":"# confusion matrix\nprint('Confusion matrix: Simple Lightgbm')\nconfusion_matrix(y_test, lgbm1.predict(X_test).round())\ndef plot_roc(y_test, y_pred, name):\n    fig, ax = plt.subplots(1,1, figsize=(6,6))  \n    fpr2, tpr2, thresholds2 = roc_curve(y_test,  y_pred)\n    ax.legend([f' {auc(fpr2, tpr2)}'])\n           # ax.legned()\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic {}'.format(name))\n    ax.plot(fpr2, tpr2)\nplot_roc(y_test,  lgbm1.predict(X_test).round(), name='Simple LightGBM')","a3e11407":"#Training the model with StratifiedKFold()+SMOTE() data\n#training data\nlgb_train2=lgb.Dataset(X_smote,label=y_smote)\n#validation data\nlgb_valid2=lgb.Dataset(X_smote_v,label=y_smote_v)","88bb9950":"num_rounds=10000\nlgbm3= lgb.train(params,lgb_train2,num_rounds,valid_sets=[lgb_train2,lgb_valid2],feval=lgb_f1_score,verbose_eval=1000,early_stopping_rounds = 5000)\n","447e8d1f":"# confusion matrix\nprint('Confusion matrix: SMOTE Lightgbm')\nconfusion_matrix(y_test, lgbm3.predict(X_test).round())\nplot_roc(y_test,  lgbm3.predict(X_test).round(), name='SMOTE LightGBM')\n","daa5ed42":"#final submission\n\nX_test=test.drop(['ID_code'],axis=1)\n#predict the model, probability predictions\nlightgbm_predict_prob3=lgbm3.predict(X_test,random_state=42,num_iteration=lgbm3.best_iteration)\nlightgbm_predict_prob1=lgbm1.predict(X_test,random_state=42,num_iteration=lgbm1.best_iteration)\n\n#Convert to binary output 1 or 0\nlightgbm_predict3=lightgbm_predict_prob3.round()\nlightgbm_predict1=lightgbm_predict_prob1.round()\n","7ed575d4":"submit=pd.DataFrame({'ID_code':test['ID_code'].values})\nsubmit1=pd.DataFrame({'ID_code':test['ID_code'].values})\n\n#submit['lightgbm_predict_prob']=lightgbm_predict_prob3\nsubmit['target']=lightgbm_predict3.astype(int)\nsubmit1['target']=lightgbm_predict1.astype(int)\n\nsubmit.to_csv('submission.csv',index=False)\nsubmit1.to_csv('submission1.csv',index=False)\nsubmit1.head()\nsubmit.head()","e5e7a650":"submit.shape","09a3440f":"### <a id='2.2.2'>2.2.2 Principal component analysis (PCA)<\/a>\n\n\n#### Feature decomposition using Principal Component Analysis ( PCA )  || Feature Extraction\n\nPrincipal Component Analysis (PCA) is a linear dimensionality reduction technique that can be utilized for extracting information from a high-dimensional space by projecting it into a lower-dimensional sub-space. It tries to preserve the essential parts that have more variation of the data and remove the non-essential parts with fewer variation.\n\nFrom the pair plot in Visualization, lot of feature pairs divide nicely the data to a similar extent, therefore, it makes sense to use one of the dimensionality reduction methods to try to use as many features as possible and maintian as much information as possible when working with only 2 dimensions. \n\nPCA is very sensitive to unstandardize data therefore we should use scaled version of our data for feature extraction. \n\n##### Deciding How Many Principal Components to Retain\n\nIn order to decide how many principal components should be retained, it is common to summarise the results of a principal components analysis by making a scree plot.\n\nUsing Elbow Method to determine the right number of components to be retain.","1ac00140":"#### Observation:\n1. Here, we have second column <b>\"target\"<\/b>, which is our objective to assert. \n2. As can bee seen above, except for the target all other features are of type float64 . \n3. Our target is int64 type with onlt two values or we can say two classes i.e. catogorical variable, so, later we'will apply encoding method upon it and convert it to numeric.\n* Standard deviation is relatively large for both train and test variable data;\n* min, max, mean, sdt values for train and test data looks quite close;\n* mean values are distributed over a large range.","5492985e":"### <a id='2.1.2.3'>2.1.2.3 Attributes Distributions and trends <\/a>","7b880e4a":"### <a id='2.2.4'>2.2.4 Feature Engineering<\/a>\n<B> Adding our aggregate features to dataset","d401130c":"<b>#Training the model with simple train_test_split stratified data","d97d809a":"<b>Observation:\n    \n > Only about 10% of total target is belong to class 1, therefore , this train dataset is imbalanced, hence need different sampling methods than random sampling.\n \n1. We have to solve an imbalanced class problem. The number of customers that will not make a transaction is much higher than those that will.\n2. It seem that there is no relationship of the target with the index of the train dataframe. This is more empressend by the zero targets than for the ones.","b3cf12fa":"### <a id='2.1.2.1'>2.1.2.1 Target Variable: Count plot \/ Pie Chart<\/a>\n","3cdd7d00":"# <a id='2'>2. Methodology<\/a>\n# <a id='2.1'>2.1 Exploratory Data Analysis(EDA)<\/a>\nIn this section, we'll explore the attributes and data values. Familiarity with data will provide more insight knowledge for data pre-processing,  analysize how to use graphical and numerical techniques to begin uncovering the structure of your data.\n\n## Objectives of Data Exploration\nExploratory data analysis (EDA) is a very important step which takes place after feature engineering and acquiring data and it should be done before any modeling. This is because it is very important for a data scientist to be able to understand the nature of the data without making assumptions. The results of data exploration can be extremely useful in grasping the structure of the data, the distribution of the values, and the presence of extreme values and interrelationships within the data set\n\n> Purpose of EDA:\n1. Summarize the statistics and visualization of data for better understanding.Crubing indication for tendencies of the data, its quality and to formulate assumptions and the hypothesis of our analysis.\n2. To create overall picture of the data with basic statistical description and aspects, and identify \n\nProspectives used to examine the data:\n1. Descriptive statistics\n2. Visualization","657ff40a":"<b>Observations:\n    \n > Maximum corr within all variables correlations is 0.009713 which is inferior, and hence, all the variables are almost independent i.e. no correlation between them. Therefore, we should not ignore direclty any variable or drop it in feature selection preocess.","a3252f11":"### Model run with 4 classifier without applying Standardization on data.","c2edafbc":"<b>Observation:<\/b>\n    \n* Here, the non scaled version of our data is working much better than  scaled version data. Although, PCA strongly avert non scaled version input but for now we will consider it during our modeling\n\n* Only 100 components can explain our 50+ % of features.","035f8948":"### <a id='2.3.3'>2.3.3 LightGBM<\/a>\n\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.\n\nSince it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019.\n\n\n#### Model building and training:\nWe need to convert our training data into LightGBM dataset format(this is mandatory for LightGBM training).\nAfter creating a converting dataset, I created a python dictionary with parameters and their values. Accuracy of your model totally depends on the values you provide to parameters. I will use AUC and F1_score as performance metric of our algorithm.\n\nWe will perform three run for following different flavours\n1. Training the model with simple train_test_split stratified data\n2. Training the model with StratifiedKFold()+SMOTE() data","d2539abd":"# <a id='3'>3. Conclusion <\/a>\n\nWe tried model with logistic regression, NB ,smote and lightgbm. But lightgbm model is performing well on imbalanced data compared to other models based on scores of roc_auc_score. Our model with StratifiedKFold()+SMOTE() seems to be overfitted beacuse of very high scores. So Will choose lgbm1  model with simple train_test_split stratified data for submission.","446f8bc4":"<b>Obsrevation:<\/b>\n> No missing values in dataset, niether in train nor in test dataset.","b2fbf1b6":"<b>Getting Started: Load libraries and dataset","67f1ac51":"### <a id='2.2.3'>2.2.3 Feature Selection <\/a>\n### Finding top features using ML Algos\nWe will use ML algorithms to find the top features from data variables. They can serve as a starting point to discover their nature and for trying to understand the data. In addition they may yield some ideas on how to generate new features. I am going to use GridSearchCV strategy for. Since, our target is imbalanced.\n\n1. Imbalanced dataset is relevant primarily in the context of supervised machine learning involving two or more classes.\n\n2. Imbalance means that the number of data points available for different the classes is different\n\nWe will look into using following algorithm:\n* Random Forest Classfier\n\nHere, after we will remove 25 less significant features from our dataset use RFC.","4404ffaf":"## <a id='2.1.1'>2.1.1 Descriptive statistics<\/a>\n\n## <a id='2.1.1.1'>2.1.1.1 Feature Analysis<\/a>\nIt is a summary statistic that quantitatively describes or summarizes features of a collection of information, process of condensing key characteristics of the data set into simple numeric metrics. Some of the common metrics used are mean, standard deviation, and correlation.","a925b559":"### <a id='2.1.2.2'>2.1.2.2 Outlier Analysis: Boxplot<\/a>","daa458cc":"<b>#Training the model with StratifiedKFold()+SMOTE() data","434f02d1":"#### Observation:\n1. Here, we have second column <b>\"target\"<\/b>, which is our objective to assert. Also, we will drop the column <b>ID_code<\/b> since, its no use for us.\n2. As can bee seen above, except for the target all other features are of type float64 . \n3. Our target is int64 type with onlt two values or we can say two classes i.e. catogorical variable, so, later we'will apply encoding method upon it and convert it to numeric.","20e1e20d":"<b>Observation <\/b>\n* Variance:\nMost of the component have variance are quite up and down, hence, variable informations are to be visualized for understanding of the data.\n* Skewness:\nSkewness is the extent to which the data are not symmetrical. Whether the skewness value is 0, positive, or negative reveals information about the shape of the data.\nFrom the data, we can see that most of the components are normal, close to 0 only.","46689429":"https:\/\/stackoverflow.com\/questions\/44172162\/f1-score-vs-roc-auc\n\nAs a rule of thumb, if the cost of having False negative is high, we want to increase the model sensitivity and recall!.\n\nFor instance, in fraud detection or sick patient detection, we don't want to label\/predict a fraudulent transaction (True Positive) as non-fraudulent (False Negative). Also, we don't want to label\/predict a contagious sick patient (True Positive) as not sick (False Negative).\n\nThis is because the consequences will be worse than a False Positive (incorrectly labelling a a harmless transaction as fraudulent or a non-contagious patient as contagious).\n\nOn the other hand, if the cost of having False Positive is high, then we want to increase the model specificity and precision!.\n\nFor instance, in email spam detection, we don't want to label\/predict a non-spam email (True Negative) as spam (False Positive). On the other hand, failing to label a spam email as spam (False Negative) is less costly.","2e36ad94":"## <a id='2.3'>2.3 Modeling<\/a>\n\n### <a id='2.3.1'>2.3.1 Classifiation Models( KFold without scaling)<\/a>\n\nUsing Logistic Regression, DecisionTreeClassifier, GaussianNB, and RandomForestClassifier as a classifer and applying KFold for cross validation.","f235b704":"### <a id='2.1.2.4'>2.1.2.4 Correlation Analysis \/ Heatmap<\/a>\n### Multi-variate\n1. Scatter plots\n2. Correlation matrix\n\n\nFirst, we will use the method corr() on a DataFrame that calculates the correlation between each pair of features. Then, we pass the resulting correlation matrix to heatmap() from seaborn, which renders a color-coded matrix for the provided values:","8e1d7f65":"### <a id='2.3.2'>2.3.2 KFold with scaled and SMOTE Oversampling sampling data<\/a>\n### Model run with 4 classifier with applying Standardization, StratifiedKFlod and SMOTE oversampling on data.","7564234b":"# <a id='1'>1. Introduction<\/a>\n\nAt Santander, mission is to help people and businesses prosper. We are always looking\nfor ways to help our customers understand their financial health and identify which\nproducts and services might help them achieve their monetary goals.\nOur data science team is continually challenging our machine learning algorithms,\nworking with the global data science community to make sure we can more accurately\nidentify new ways to solve our most common challenge, binary classification problems\nsuch as: is a customer satisfied? Will a customer buy this product? Can a customer pay\nthis loan?\n\n\n\nThe aim of this notebook is to understand the process of EDA and Data-preparation, selection of features, implementing machine learning tools. Later, will comparing and improving the best models.\n\n### <a id='1.1'>1.1 Problem Statement<\/a>\nIn this challenge, we need to identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted\n\n### Objective\nSince the labels in the data are discrete, the predication falls only into two categories. In machine learning, this is called classification problem.\n\n> Hence our goal is to solve a binary classification problem. In the data description you can see that the features given are numeric and anonymized.\n\n### Evaluation\nFor Classification the Accuracy metrics need to be considered are AUC, Precision & Recall. Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\n### <a id='1.2'>1.2 About Dataset<\/a>\nWe are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\n\nThe task is to predict the value of target column in the test set.\n\nDataset:\ntrain.csv - the training set.\ntest.csv - the test set. The test set contains some rows which are not included in scoring.\n\nThe Train dataset contains 200000 observations of 202 columns.\n\n1. The first two columns in the dataset store the unique ID_code numbers of the obseravtions and the corresponding \"target\" transaction prediction ,respectively.\n2. The columns 2-202 contain 200 real-value features that have been captured which can be used to build a model to predict weather a transaction done by customer.\n\nThe Test dataset contains 200000 observations of 201 columns.\n\n1. The first column in the dataset store the unique ID_code numbers of the obseravtions and the corresponding variables columns.\n2. The columns 1-201 contain 200 real-value features that have been captured which will be used to test our model prediction on weather a transaction done by customer.\n\n","bbdae969":"<b>Inspecting Dataset<\/b>\n\nThe \u201cinfo()\u201d method provides a concise summary of the data; from the output, it provides the type of data in each column, the number of non-null values in each column, and how much memory the data frame is using.","6a9168a4":"<b> Observation: <\/b>\n    LR and GaussianNB are better as compare to others.","8f69da5d":"CSV Download for Submit1:\n<a  href=\"submission1.csv\" target=\"_blank\">download submit1<\/a>\n\nCSV Download for Submit:\n<a  href=\"submission.csv\" target=\"_blank\">download submit<\/a>","b747e96f":"1. <b>Observation: <\/b>\nAll the outliers include our class1 target, so we cannot remove them.","b16b6808":"<B>Observation:<\/b>\nOur model with StratifiedKFold()+SMOTE() data gives best result:\n* training's auc: 0.997312\ttraining's f1: 0.97558\tvalid_1's auc: 0.952837\tvalid_1's f1: 0.876571\n\nOur model with simple train_test_split stratified data yield:\n* training's auc: 0.990451\ttraining's f1: 0.761308\tvalid_1's auc: 0.891113\tvalid_1's f1: 0.534388","28783ea2":"## <a id='2.1.2'>2.1.2 Visualization<\/a>\nIt is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. With a little domain knowledge, data visualizations can be used to express and demonstrate key relationships in plots and charts that are more visceral to yourself and stakeholders than measures of association or significance. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results.\n\nOne of our main goals for visualizing the data here, is to observe which features are most intiutive in predicting target. The other, is to draw general trend, may aid us in model selection and hyper parameter selection.\n\n### 1. Univariate\nFollowing are techniques which can be used to understand each attribute of your dataset independently:\n1. Histograms.\n2. Density Plots.\n3. Box Plots.\n4. Scatter Plots.\n5. Heatmap Plots.","36d028e8":"## <a id='2.1.1.2'>2.1.1.2 Missing Value Analysis<\/a>","86bd3fca":"<b> Observation: <\/b>\nThere are some significant differences in aggregate values. ","2dc25f56":"<b>Observations:\n    \nThere is a considerable number of features with significant different distribution for the two target values.\nFor example, var_0, var_1, var_2, var_5, var_9, var_13, var_106, var_109, var_139 and many others.\n","39e530ea":"<b>Observations:\n    \n   1. Most of the data have outlier and range of the data variables is high.\n   2. Data need to be free from ouliers and need to be scaled before applying any outlier senstive model algorithms.","a642a27c":"Observation: We are not removing feature because we might loose some performance.","5dd7c881":"## **Index**\n-     <a href='#1'>1 Introduction<\/a>\n    - <a href='#1.1'>      1.1 Problem Statement<\/a>\n    - <a href='#1.2'>      1.2 Data <\/a>\n-     <a href='#2'>2 Methodology<\/a>\n    - <a href='#2.1'>  2.1 Exploratory Data Analysis <\/a>\n        - <a href='#2.1.1'>          2.1.1 Descriptive Analysis <\/a>\n           - <a href='#2.1.1.1'>            2.1.1.1 Features Analysis <\/a>\n           - <a href='#2.1.1.2'>            2.1.1.2 Missing Value Analysis <\/a>\n        - <a href='#2.1.2'>          2.1.2 Visualization <\/a>\n           - <a href='#2.1.2.1'>            2.1.2.1 Target Variable: Count plot \/ Pie Chart <\/a>\n           - <a href='#2.1.2.2'>            2.1.2.2 Outlier Analysis <\/a>\n           - <a href='#2.1.2.3'>            2.1.2.3 Attributes Distributions and trends <\/a>\n           - <a href='#2.1.2.4'>            2.1.2.4 Correlation Analysis \/ Heatmap <\/a>\n    - <a href='#2.2'>      2.2 Data Preprocessing and Analysis <\/a>\n        - <a href='#2.2.1'>          2.2.1 Outlier Handling <\/a>\n        - <a href='#2.2.2'>          2.2.2 Principal component analysis (PCA) <\/a>\n        - <a href='#2.2.3'>          2.2.3 Feature Selection <\/a>\n        - <a href='#2.2.4'>          2.2.4 Feature Engineering <\/a>\n    - <a href='#2.3'>      2.3 Modeling <\/a>\n        - <a href='#2.3.1'>          2.3.1 Classifiation Models( KFold without scaling)<\/a>\n        - <a href='#2.3.2'>          2.3.2 SMOTE models <\/a>\n        - <a href='#2.3.3'>          2.3.3 LightGBM <\/a>\n- <a href='#3'>3 Conclusion<\/a>","3ec30cc2":"# <a id='2.2'>2.2 Data Preprocessing<\/a\n\nData preprocessing is a crucial step for any data analysis problem. Data Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis. It is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n\nThis involves a number of activities such as:\n* Assigning numerical values to categorical data;\n* Handling missing values; and\n* Normalizing the features (so that features on small scales do not dominate when fitting a model to the data).\n\nMethods can be used:\n* Handling Null Values\n* Standardization\n* Handling Categorical Variables\n* One-Hot Encoding\n* Multicollinearity","3e52c6cc":"### <a id='2.2.1'>2.2.1 Outlier Handling<\/a>"}}