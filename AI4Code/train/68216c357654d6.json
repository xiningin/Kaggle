{"cell_type":{"dada0579":"code","10355e48":"code","a5c0db9c":"code","8416141f":"code","ba7a8b86":"code","17f9285e":"code","2b89fc7f":"code","c7a123d7":"code","ff679eee":"code","0f2a5fac":"code","05a9764c":"code","4bf048f4":"code","df81bfda":"code","0bc7bfd6":"code","3a16ecdb":"code","9af276bf":"code","104bd724":"code","4f0d2a90":"code","8ce6c206":"code","d1ded754":"code","00cd826d":"code","3fbff268":"code","b7950993":"code","3c625d20":"code","041cf5f0":"code","1427b553":"code","7b0e19fd":"code","c3a5ed99":"code","f572492c":"code","27181784":"code","9feb5c9b":"code","0b75731d":"markdown","84e98bf5":"markdown","70e2befa":"markdown","477bf691":"markdown","534c1ef0":"markdown","87593334":"markdown","0483f87e":"markdown","9fdc24ba":"markdown","3dfd7399":"markdown","54c8abf1":"markdown","c652f184":"markdown","06583d9c":"markdown","76be66b6":"markdown","f3b5ea78":"markdown","a0aec06e":"markdown","bf926b76":"markdown","e92e41da":"markdown","5a81f5ca":"markdown","22ebb6ad":"markdown","f9c5f4d4":"markdown","84269afc":"markdown","ca94bd98":"markdown","1c7cc956":"markdown","c2fa0878":"markdown"},"source":{"dada0579":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","10355e48":"# Download the dataset\nprint('Downloading the dataset...')\n!wget -qq https:\/\/www.dropbox.com\/s\/fm2x6fq6xoxk5qi\/YDSP_CatsDogs.zip\nprint('Done!')\n  \n# Unzip to retrieve the images\nprint('Unzipping the file...')\n!unzip -qq YDSP_CatsDogs.zip\n!rm YDSP_CatsDogs.zip\nprint('Done!')","a5c0db9c":"!ls","8416141f":"# The folder structure \n!ls catsdogs\/ ","ba7a8b86":"!ls catsdogs\/cat  # Cats Images","17f9285e":"# General Libraries\n%matplotlib inline\nimport os\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\n\n# Image Processing and CNN Libraries\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Activation, Dropout, Flatten, Dense\n\n# Visualisation Libraries\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import confusion_matrix\nfrom IPython.display import Image","2b89fc7f":"# Specify the directory of the class folders\npath = \".\/catsdogs\"","c7a123d7":"# Input Size\nIMAGE_SIZE = 200\nIMAGE_WIDTH, IMAGE_HEIGHT = IMAGE_SIZE, IMAGE_SIZE\n\n# Hyperparams\nEPOCHS = 30\nBATCH_SIZE = 32","ff679eee":"# Train-Valid Data Split\ndef trainValidSplit(path, split_percent):\n  \"\"\"\n  Function to perform the train-validation split by a specified validation split percentage.\n  @args:\n    path: The directory to the class folders.\n    split_percent: Percentage of the data to split to form the validation dataset.\n  \"\"\"\n  file_names = glob(path + '\/**\/*')\n  for i in file_names:\n    path_dir = os.path.dirname(i)\n    os.rename(i, os.path.join(path_dir, os.path.splitext(os.path.basename(i))[0] + \".jpg\"))\n    \n  all_classes = [file.split('\/')[-2] for file in file_names]\n  x_train, x_valid, y_train, y_valid = train_test_split(file_names, all_classes, test_size=split_percent, random_state=42)\n  train_df = pd.DataFrame({'filename': x_train, 'class': y_train})\n  validation_df = pd.DataFrame({'filename': x_valid, 'class': y_valid})\n  return (train_df, validation_df)\n\n# Validation Split\nvalidation_split = 0.2\ntrain_df, validation_df = trainValidSplit(path, validation_split)","0f2a5fac":"# Training dataset\nprint(train_df.shape)\ntrain_df.head()","05a9764c":"# Validation Dataset\nprint(validation_df.shape)\nvalidation_df.head()","4bf048f4":"# Training Data Augmentation\ntraining_data_generator = ImageDataGenerator(\n    rescale=1.\/255,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True)","df81bfda":"validation_data_generator = ImageDataGenerator(\n    rescale=1.\/255)","0bc7bfd6":"training_generator = training_data_generator.flow_from_dataframe(train_df,\n                                             target_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n                                             batch_size = BATCH_SIZE,\n                                             shuffle = False,\n                                             class_mode = \"categorical\")\nvalidation_generator = validation_data_generator.flow_from_dataframe(validation_df,\n                                             target_size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n                                             batch_size = BATCH_SIZE,\n                                             shuffle = False,\n                                             class_mode = \"categorical\")","3a16ecdb":"# Number of classes\nnum_classes = len(training_generator.class_indices)\nclasses = sorted(training_generator.class_indices, key=training_generator.class_indices.get)\nprint('Number of classes: ', num_classes)\nprint('Class Labels: ', classes)","9af276bf":"def plotSample(datagen, image_num = 20, predict_model = None, image_per_col = 10):\n    \"\"\"\n    Helper function to plot images along with is class labels and predictions. The class labels & predictions are placed on top\n    of the individual image and its corresponding prediction in brackets.\n    @args:\n        datagen: Image Generator variable that contains the images to visualise.\n        image_num: Number of images to visualise. Defaults to 20 images.\n        predict_model: The trained model to make predictions if any. Defaults to None (No prediction to be made).\n        image_per_col: Number of images to plot for each column.\n    \"\"\"\n    from math import ceil\n    # Get first batch\n    images, labels = datagen.next()\n    batch_img_num = images.shape[0]\n  \n    # Get more batch if no. of images to plot > batch_size\n    while batch_img_num < image_num:\n        images_a, labels_a = datagen.next()\n        images, labels = np.concatenate([images, images_a]), np.concatenate([labels, labels_a])\n        batch_img_num += images_a.shape[0]\n    \n    # Decode one-hot encoding (if any)\n    if len(labels.shape) == 2:\n        labels = labels.argmax(axis = 1)\n  \n    # Swap the class name and its numeric representation\n    classes = dict((v,k) for k,v in datagen.class_indices.items())\n  \n    # Make prediction if this is to visualise the model predictive power\n    if predict_model:\n        preds_prob = predict_model.predict(images)\n        preds = preds_prob.argmax(axis = 1)\n        main_prob = preds_prob.max(axis=1)\n    \n    # Plot the images in the batch, along with the corresponding (predictions and) labels\n    fig = plt.figure(figsize=(25, ceil(image_num\/image_per_col)*2))\n    for idx in np.arange(image_num):\n        if predict_model and len(preds) <= idx:\n            break\n        ax = fig.add_subplot(ceil(image_num\/image_per_col), image_per_col, idx+1, xticks=[], yticks=[])\n        plt.imshow(images[idx])\n        if not predict_model:\n            ax.set_title(classes[labels[idx]])\n        else:\n            ax.set_title(\"{} ({:.2f}% {})\".format(classes[labels[idx]], main_prob[idx]*100, classes[preds[idx]]),\n                         color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))","104bd724":"plotSample(training_generator, 40)","4f0d2a90":"# Input Layer\ninput_shape = (IMAGE_WIDTH, IMAGE_HEIGHT, 3)\nInp = Input(shape=input_shape, name = 'Input')\n\n# Convolution Layer 1 with MaxPool\nx = Conv2D(32, (3, 3), padding='same', input_shape=input_shape, activation='relu', name = 'Conv_01')(Inp)\nx = MaxPooling2D(pool_size=(2, 2), name = 'MaxPool_01')(x)\n\n# Convolution Layer 2 with MaxPool\nx = Conv2D(64, (3, 3), padding='same', activation='relu', name = 'Conv_02')(x)\nx =  MaxPooling2D(pool_size=(2, 2), name = 'MaxPool_02')(x)\n\n# Convolution Layer 3 with MaxPool\nx = Conv2D(128, (3, 3), padding='same', activation='relu', name = 'Conv_03')(x)\nx =  MaxPooling2D(pool_size=(2, 2), name = 'MaxPool_03')(x)\n\n# Convolution Layer 4 with MaxPool\nx = Conv2D(256, (3, 3), padding='same', activation='relu', name = 'Conv_04')(x)\nx =  MaxPooling2D(pool_size=(2, 2), name = 'MaxPool_04')(x)\n\n# Convolution Layer 5 with MaxPool\nx = Conv2D(128, (3, 3), padding='same', activation='relu', name = 'Conv_05')(x)\nx =  MaxPooling2D(pool_size=(2, 2), name = 'MaxPool_05')(x)\n\n# Convolution Layer 6 with MaxPool\nx = Conv2D(256, (3, 3), padding='same', activation='relu', name = 'Conv_06')(x)\nx =  MaxPooling2D(pool_size=(2, 2), name = 'MaxPool_06')(x)\n\n# Flatten \nx = Flatten(name = 'Flatten')(x)\n\n# Fully Connected Layer\nx = Dense(256, activation = 'relu', name = 'Dense_01')(x)\n\n# Fully Connected Layer for classification\noutput = Dense(num_classes, activation='softmax', name = 'Output')(x)\n    \nmodel = Model(Inp, output)","8ce6c206":"# Print out the model structure\nmodel.summary()","d1ded754":"# Define the optimizer, loss and metrics\nopt = Adam(lr = 0.0001)\nmodel.compile(loss = tf.keras.losses.categorical_crossentropy,\n             optimizer = opt,\n             metrics = ['accuracy'])","00cd826d":"# Specify the steps to be taken for the training and validation\ntraining_steps = max(training_generator.samples \/\/ BATCH_SIZE, 1)\nvalidation_steps = max(validation_generator.samples \/\/ BATCH_SIZE, 1)\n\ntraining_generator.reset()  # Reset the training generator to start from the beginning\n\n# Train the model\nhistory = model.fit_generator(\n    training_generator,\n    steps_per_epoch = training_steps,\n    epochs = EPOCHS,\n    validation_data = validation_generator,\n    validation_steps = validation_steps,\n    verbose=1)","3fbff268":"def plot_train(hist, choice = 'accuracy'):\n    \"\"\"\n    Function to plot accuracy or loss over the training iterations.\n    @args:\n        hist: The history saved during the model training phase\n        choice: The type of function to plot. Should only be either 'accuracy' or 'loss'. Defaults to 'accuracy'.\n    \"\"\"\n    h = hist.history\n    if choice == 'accuracy':\n        meas='accuracy'\n        loc='lower right'\n    else:\n        meas='loss'\n        loc='upper right'\n    plt.plot(hist.history[meas])\n    plt.plot(hist.history['val_'+meas])\n    plt.title('model '+meas)\n    plt.ylabel(meas)\n    plt.xlabel('epoch')\n    plt.ylim(ymin=0)\n    plt.legend(['train', 'validation'], loc=loc)\n    \ndef view_classify(datagen, pred_model):\n    \"\"\"\n    Function for viewing an image and it's predicted class probabilities.\n    @args:\n        datagen: The image generator of the test data.\n        pred_model: The trained model to use to make predictions.\n    \"\"\"\n    # Get first batch\n    images, labels = datagen.next()\n    \n    img = images[0]\n    preds = model.predict(np.expand_dims(img, axis = 0))\n    ps = preds[0]\n    \n    # Swap the class name and its numeric representation\n    classes = sorted(datagen.class_indices, key=datagen.class_indices.get)\n    \n    print('Probabilities:')\n    print(pd.DataFrame({'Class Label': classes, 'Probabilties': ps}))\n    \n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.squeeze())\n    ax1.axis('off')\n    ax2.set_yticks(np.arange(len(classes)))\n    ax2.barh(np.arange(len(classes)), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticklabels(classes, size='small')\n    ax2.set_title('Class Probability')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()","b7950993":"# Plot the Model accuracy on train and validation set\nplot_train(history)","3c625d20":"# Plot the Model loss on train and validation set\nplot_train(history, 'loss')","041cf5f0":"# Try on one image\nimage_check = validation_df.filename[0] # Image Path\nImage(image_check)","1427b553":"# Visualise how the image is seen\ndef display_activation(model, img_path, col_size, row_size, act_index): \n    \"\"\"\n    Function to plot the feature maps of the convolution networks\n    @args:\n      model: The trained model which contains the convolutional layers to visualise.\n      img_path: The image path to the input image to the network.\n      col_size: The number of columns of feature map to fill.\n      row_size: The number of rows of feature map to fill.\n      act_index: The desired convolution layer to visualise. The range of this index starts from the 1st convolution (0) to the last layer before Flatten\/Dense\n    \"\"\"\n    # Load and rescale the image\n    img = load_img(img_path, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n    x = img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x \/= 255. \n    \n    # Make sure that the layer index to visualise is present in the network\n    assert len(model.layers) > act_index\n    \n    layer_outputs = [layer.output for layer in model.layers[1:]]\n    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n    activations = activation_model.predict(x)\n    activation = activations[act_index]\n    print('Layer in Visualisation:', model.layers[act_index+1].name)\n    activation_index=0\n    \n    # Plot the filters\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size): \n      for col in range(0,col_size):\n        ax[row][col].imshow(activation[0, :, :, activation_index], cmap='magma')\n        activation_index += 1\ndisplay_activation(model, image_check, 6, 5, 0)","7b0e19fd":"view_classify(validation_generator, model)","c3a5ed99":"plotSample(validation_generator, 40, model)","f572492c":"validation_generator.reset()  # Reset the validation generator to start from the beginning\n\n# Compute the overall accuracy of the model with validation dataset\nmetrics = model.evaluate_generator(validation_generator)\nprint(\"model accuracy:\",metrics[1])","27181784":"def plotConfusionMatrix(datagen, model):\n    \"\"\"\n    Function to compute and plot a confusion matrix of the class predictions.\n    @args:\n      datagen: The image generator of the test data to use to make predictions.\n      model: The trained model that is used to make predictions.\n    \"\"\"\n    # Get the class probabilities of each image\n    datagen.reset()\n    preds = model.predict_generator(datagen)\n    \n    # Compute the confusion matrix\n    datagen.reset()\n    true_labels = datagen.labels\n    classes = sorted(datagen.class_indices, key=datagen.class_indices.get)\n    cm = confusion_matrix(true_labels, preds.argmax(axis=1))\n    cm_df = pd.DataFrame(cm, index = classes, columns = classes)\n    \n    # Compute the overall accuracy\n    overall_acc = np.sum(true_labels == preds.argmax(axis = 1)) \/ len(true_labels)\n    \n    # Plot the confusion matrix\n    plt.figure(figsize = (5.5, 4))\n    sn.heatmap(cm_df, annot = True, cmap = 'magma_r', fmt='g')\n    plt.title('Overall Accuracy: {0:.2f}%'.format(overall_acc*100))\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","9feb5c9b":"plotConfusionMatrix(validation_generator, model)","0b75731d":"The number of steps for the training or validation is defined by:\n$$\\text{Number of steps} = \\frac{\\text{Number of Images}}{\\text{Batch Size}}$$\nThis will ensure that each image in the dataset is used by the model once per epoch.\n\nThe number of epoch then defines the number of times the model will iterate through the entire dataset.","84e98bf5":"More predictions....","70e2befa":"## Data Load\nDownload the dataset and unzip the data file.","477bf691":"## #3: Network Model Evaluation\nAfter the training, we evaluate the model to evaluate the model performance. In this section, we detail some ways to evaluate a model performance.","534c1ef0":"## Specify the path to the images\nThere is a specified structure of the directory folder to load. It is a good practice to organise the images into its corresponding class type.\n\nFor example, for the class 'cats', we place all the 'cats' images in the folder named as 'cats'.","87593334":"We plot the accuracy & loss during the training on a line chart to see how well the model generalizes.","0483f87e":"# Hands-On Exercise: CNN on Cats & Dogs Dataset","9fdc24ba":"### Data Train-Valid Split","3dfd7399":"Individual Class Accuracy Table (Confusion Matrix):","54c8abf1":"## #1: Data Pipeline\n","c652f184":"### Model Training","06583d9c":"### Quantitative Analysis\nHere, we will state some statistical methods to evaluate the model performance.\nModel Accuracy:","76be66b6":"### Model Definition","f3b5ea78":"### Data Augmentation","a0aec06e":"## #2: CNN Model","bf926b76":"## GPU Check\nTo Test if you have GPU set up\n\nRun the Cell below\n\nif no GPU is found press Runtime (in the menu at the top) and choose \"Change Runtime Type\" to GPU","e92e41da":"Make a prediction for observation","5a81f5ca":"### Visualising the images\nBefore we start any form of modelling, lets visualise how the images looks like. \n\nHere, we provide a helper function to do this.","22ebb6ad":"## Load Standard Python Libraries","f9c5f4d4":"We do **not** perform any augmentation to the validation dataset. \n\nWe should keep the validation dataset intact to ensure that the  model evaluation is consistent and accurate. This is because the validation set represents the actual images that your model is expected to predict on (which are not augmented).\n\n\n\n","84269afc":"The above output show us the loss and accuracy after each training epoch.","ca94bd98":"## Set Input Size and Model Hyperparameters","1c7cc956":"### Visual Analysis\nHere, we will state a few ways which make use of graphical illustration to visualise the performance","c2fa0878":"### Create an image generator"}}