{"cell_type":{"a83dc33a":"code","27f13b92":"code","0ab2806b":"code","f673868e":"code","7f8718c8":"code","a6db0429":"code","9d993832":"code","cfb0d534":"code","43aeef62":"code","58f2d3ab":"code","560a0a8b":"code","5becc010":"code","851b8ff1":"code","56b0dc01":"code","22097165":"code","fb39d7ea":"code","2acc00de":"code","222ce7b8":"code","0c9de324":"code","7972694e":"code","33399547":"code","e785b19d":"code","fe792e6f":"code","f8389622":"code","b3f0a754":"code","4b6c66a1":"code","8bab02ad":"code","19d9b46b":"code","bada9f58":"code","e0b22d77":"code","afc151c9":"code","1f95ab3e":"code","7037d381":"code","9b3c6980":"code","842050d6":"code","e82a46d6":"code","ddd6eba5":"code","2b4091ae":"code","63f57e77":"code","013881ac":"code","e48973bc":"code","1d7487eb":"code","790ce094":"code","e3283b47":"code","a70edc18":"code","e73e8343":"code","5c2a8872":"code","f302d547":"code","378b4bd6":"code","1f628b0a":"code","6c8d42b6":"code","a8bee53e":"code","05341c64":"code","5f7dd434":"code","b72bdc36":"code","f09fb472":"code","7c086beb":"code","124858ba":"code","eaf69689":"code","1066b523":"code","f45e11e9":"code","cb08cbed":"code","c98aeaea":"code","089c334b":"code","011849a2":"code","7f5a4615":"code","d829b281":"code","2608f2c1":"code","5bdbad28":"code","c0dd92d0":"code","3feea32f":"code","5ed89035":"code","b87bd849":"code","036e6244":"code","57d41dfa":"code","dd899c5b":"code","7a7ac8e0":"code","f22a5c22":"code","15a8fb9c":"code","2a947ba8":"code","e9e9f456":"code","d86ad2b9":"code","3cc883e9":"code","1b4dbcdf":"code","1957322c":"code","c1c1d487":"code","b7c80068":"code","a0ab1592":"code","50a577eb":"markdown","f008a92d":"markdown","7243ce61":"markdown","c72070d7":"markdown","8ff34f9e":"markdown","583e0e25":"markdown","11a5916f":"markdown","146ed9a1":"markdown","4f13c23c":"markdown","228b8fe8":"markdown","2060306a":"markdown","dcc45b82":"markdown","cd17dc13":"markdown"},"source":{"a83dc33a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","27f13b92":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nimport sklearn.metrics as metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sb\npd.set_option('display.max_columns', None)","0ab2806b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","f673868e":"#final_dateframe = pd.read_csv('\/kaggle\/input\/final-dataframe\/final_dataframe.csv',sep=',', thousands=',' )\ndata_version1 = pd.read_csv('\/kaggle\/input\/final-dataframe-death-may4\/final_dataframe_death_may4.csv' ).drop(columns='Unnamed: 0')","7f8718c8":"data_version1.iloc[:, 0:21].hist(figsize = (24,20))\nplt.show()","a6db0429":"df = data_version1\nfinal_dateframe = data_version1","9d993832":"## Create a function that append different models performance, parameter, and their data version to a dataframe\nmaster_performance_table = pd.DataFrame(columns = ['Model', 'dataset version','MAE Train',\n                                                   'MAE Test','MSE Train','MSE Test','R^2 test','cv_mae','cv_mse','Parameters'])\ndef master_performance(master_performance_table, Model_name,dataset_version, pred_test,pred_train,para,cv_mae_score,cv_mse_score):\n    MAE_train = metrics.mean_absolute_error(y_train, pred_train)\n    MAE_test = metrics.mean_absolute_error(y_test, pred_test)\n    MSE_train = metrics.mean_squared_error(y_train, pred_train)\n    MSE_test = metrics.mean_squared_error(y_test, pred_test)\n    R2_test = metrics.r2_score(y_test, pred_test)\n    master_performance_table = master_performance_table.append([{'Model':Model_name,'dataset version':dataset_version,'MAE Train':MAE_train,\n                                                                 'MAE Test':MAE_test,'MSE Train':MSE_train, 'MSE Test': MSE_test,\n                                                                 'R^2 test':R2_test,'cv_mae':cv_mae_score,'cv_mse':cv_mse_score,'Parameters':para}], ignore_index=True)\n    return master_performance_table\n    ","cfb0d534":"\nC_mat = data_version1.corr().iloc[0:20, 0:20]\nfig = plt.figure(figsize = (10,10))\n\nsb.heatmap(C_mat, vmax = .8, square = True)\nplt.show()","43aeef62":"data_version1 = data_version1.drop(columns='deaths')\n### data version 1 train\/test split\nX = data_version1.drop(['cases'],axis=1)\ny = data_version1['cases']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","58f2d3ab":"RF = RandomForestRegressor(random_state=43)\nRF.fit(X_train, y_train)\npredictions = RF.predict(X_test)\npred_train = RF.predict(X_train)","560a0a8b":"cv_mae = cross_val_score(RF,X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\ncv_mse = cross_val_score(RF,X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score = -np.mean(cv_mae)\ncv_mse_score = -np.mean(cv_mse)","5becc010":"cv_mse_score","851b8ff1":"master_performance_table = master_performance(master_performance_table,'Base Line Random Forest','data version 1',\n                                              predictions,pred_train,'NA',cv_mae_score,cv_mse_score)","56b0dc01":"master_performance_table","22097165":"#final_dateframe = pd.read_csv('\/kaggle\/input\/final-dataframe\/final_dataframe.csv',sep=',', thousands=',' )\ndata_version1 = pd.read_csv('\/kaggle\/input\/final-dataframe-death-may4\/final_dataframe_death_may4.csv' ).drop(columns='Unnamed: 0')\ndata_version1 = data_version1.drop(columns='cases')\n","fb39d7ea":"data_version1[data_version1['state_New York']==1]","2acc00de":"### data version 1 train\/test split\nX = data_version1.drop(['deaths'],axis=1)\ny = data_version1['deaths']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","222ce7b8":"RF = RandomForestRegressor(random_state=43)\nRF.fit(X_train, y_train)\npredictions = RF.predict(X_test)\npred_train = RF.predict(X_train)","0c9de324":"cv_mae = cross_val_score(RF,X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\ncv_mse = cross_val_score(RF,X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score = -np.mean(cv_mae)\ncv_mse_score = -np.mean(cv_mse)","7972694e":"master_performance_table = master_performance(master_performance_table,'Base Line Random Forest','data version 1',\n                                              predictions,pred_train,'NA',cv_mae_score,cv_mse_score)\nmaster_performance_table","33399547":"feature_list = data_version1.columns[1:]\n# Get numerical feature importances\nimportances = list(RF.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","e785b19d":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 300, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","fe792e6f":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nprint(tf.__version__)\n","f8389622":"X_train","b3f0a754":"train_stats = X_train.describe()\ntrain_stats = train_stats.transpose()\ntrain_stats","4b6c66a1":"def norm(x):\n    return (x - train_stats['mean']) \/ train_stats['std']\nX_train_norm = norm(X_train)\nX_test_norm = norm(X_test)\n","8bab02ad":"def build_model():\n    model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(X_train_norm.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n  ])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n    model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n    return model","19d9b46b":"model = build_model()\n\nmodel.summary()\n","bada9f58":"#!pip install git+https:\/\/github.com\/tensorflow\/docs","e0b22d77":"#import tensorflow_docs as tfdocs","afc151c9":"EPOCHS = 1000\n\nhistory = model.fit(\n  X_train_norm, y_train,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0)\n","1f95ab3e":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist\n","7037d381":"# plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n\n# plotter.plot({'Basic': history}, metric = \"mae\")\n# plt.ylim([0, 10])\n# plt.ylabel('MAE [MPG]')\n","9b3c6980":"test_predictions = model.predict(X_test_norm).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(y_test, test_predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nlims = [0, 2000]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)\n","842050d6":"error = test_predictions - y_test\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")\n","e82a46d6":"#final_dateframe = pd.read_csv('\/kaggle\/input\/final-dataframe\/final_dataframe.csv',sep=',', thousands=',' )\ndf_future= pd.read_csv('\/kaggle\/input\/future\/future_afterMay4.csv' ).iloc[:, 1:]","ddd6eba5":"df_future","2b4091ae":"X_test","63f57e77":"df_future","013881ac":"future_norm = norm(df_future)","e48973bc":"df_future['death_predicted'] = model.predict(future_norm).flatten()\ndf_future","1d7487eb":"plt.scatter(df_future.date_num, df_future.death_predicted)","790ce094":"from sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential","e3283b47":"scaler = MinMaxScaler()","a70edc18":"X_train = scaler.fit_transform(X_train)","e73e8343":"from keras.layers import Dense, Activation, Flatten\nfrom keras.callbacks import ModelCheckpoint","5c2a8872":"checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]","f302d547":"NN_model = Sequential()\n\n# The Input Layer :\nNN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n# The Hidden Layers :\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer :\nNN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network :\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","378b4bd6":"NN_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","1f628b0a":"rf_random = RandomizedSearchCV(estimator = RF, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)","6c8d42b6":"str(rf_random.best_params_)","a8bee53e":"RF_para = RandomForestRegressor(n_estimators = 144,min_samples_split=2, \n                                min_samples_leaf=1,max_features='auto',max_depth=110,\n                                bootstrap=True, random_state = 42)\nRF_para.fit(X_train, y_train)\npredictions_para = RF_para.predict(X_test)\npred_para_train = RF_para.predict(X_train)","05341c64":"cv_mae_pa = cross_val_score(RF_para,X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\ncv_mse_pa = cross_val_score(RF_para,X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score_pa = -np.mean(cv_mae_pa)\ncv_mse_score_pa = -np.mean(cv_mse_pa)","5f7dd434":"master_performance_table = master_performance(master_performance_table,'Tuning Random Forest','data version 1',\n                                              predictions_para,pred_para_train,str(rf_random.best_params_),cv_mae_score_pa,\n                                             cv_mse_score_pa)","b72bdc36":"master_performance_table","f09fb472":"X2 = final_dateframe.drop('cases',axis=1)\ny2 = final_dateframe['cases']\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size = 0.2, random_state = 42)","7c086beb":"RF2 = RandomForestRegressor(random_state=45)\nRF2.fit(X_train2, y_train2)\npredictions3 = RF2.predict(X_test2)","124858ba":"pred_train3 = RF2.predict(X_train2)","eaf69689":"cv_mae3 = cross_val_score(RF2,X_train2, y_train2, cv=5, scoring='neg_mean_absolute_error')\ncv_mse3 = cross_val_score(RF2,X_train2, y_train2, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score3 = -np.mean(cv_mae3)\ncv_mse_score3 = -np.mean(cv_mse3)","1066b523":"master_performance_table = master_performance(master_performance_table,'Base Line Random Forest','data version 2',\n                                              predictions3,pred_train3,'NA',cv_mae_score2,cv_mse_score2)","f45e11e9":"master_performance_table","cb08cbed":"feature_list = final_dateframe.columns[1:]\n# Get numerical feature importances\nimportances = list(RF2.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","c98aeaea":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 300, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","089c334b":"rf_random2 = RandomizedSearchCV(estimator = RF2, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=45, n_jobs = -1)\n\nrf_random2.fit(X_train2, y_train2)","011849a2":"rf_random.best_params_","7f5a4615":"RF_para2 = RandomForestRegressor(n_estimators = 144,min_samples_split=2, \n                                min_samples_leaf=1,max_features='auto',max_depth=110,\n                                bootstrap=True, random_state = 45)\nRF_para2.fit(X_train2, y_train2)\npredictions_para2 = RF_para2.predict(X_test2)\npred_para_train2 = RF_para2.predict(X_train2)","d829b281":"cv_mae4 = cross_val_score(RF_para2,X_train2, y_train2, cv=5, scoring='neg_mean_absolute_error')\ncv_mse4 = cross_val_score(RF_para2,X_train2, y_train2, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score4 = -np.mean(cv_mae4)\ncv_mse_score4 = -np.mean(cv_mse4)","2608f2c1":"master_performance_table = master_performance(master_performance_table,'Tuning Random Forest','data version 2',\n                                              predictions_para2,pred_para_train2,str(rf_random.best_params_),cv_mae_score4,cv_mse_score4)","5bdbad28":"master_performance_table","c0dd92d0":"feature_list = final_dateframe.columns[1:]\n# Get numerical feature importances\nimportances = list(RF_para.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","3feea32f":"from sklearn import datasets\nimport xgboost as xgb","5ed89035":"XGB = xgb.XGBRegressor()\nXGB.fit(X_train,y_train, eval_metric='mae')\npreds_test = XGB.predict(X_test)\npreds_train = XGB.predict(X_train)\n# print('\\nMAE train:', metrics.mean_absolute_error(y_train,XGB.predict(X_train)))\n# print('\\nMAE test:', metrics.mean_absolute_error(y_test,preds))\n# print('Mean Squared Error:', metrics.mean_squared_error(y_test,preds))","b87bd849":"cv_mae_xg = cross_val_score(XGB,X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\ncv_mse4_xg = cross_val_score(XGB,X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score4_xg = -np.mean(cv_mae_xg)\ncv_mse_score4_xg = -np.mean(cv_mse4_xg)","036e6244":"master_performance_table = master_performance(master_performance_table,'Simple Xgboost','data version 1',\n                                              preds_test,preds_train,'NA',cv_mae_score4_xg,cv_mse_score4_xg)","57d41dfa":"master_performance_table","dd899c5b":"XGB1 = xgb.XGBRegressor(max_depth = 4)\nXGB1.fit(X_train,y_train, eval_metric='mae')\npreds_test2 = XGB1.predict(X_test)\npreds_train2 = XGB1.predict(X_train)","7a7ac8e0":"cv_mae_xg2 = cross_val_score(XGB1,X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\ncv_mse4_xg2 = cross_val_score(XGB1,X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score4_xg2 = -np.mean(cv_mae_xg2)\ncv_mse_score4_xg2 = -np.mean(cv_mse4_xg2)","f22a5c22":"master_performance_table = master_performance(master_performance_table,'Simple Tuning Xgboost','data version 1',\n                                              preds_test2,preds_train2,'max_depth = 4',cv_mae_score4_xg2,cv_mse_score4_xg2)","15a8fb9c":"master_performance_table","2a947ba8":"## use random search to do large scale parameter tuning\nparam_grid = {\n        'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [100]}\nxgb_para = RandomizedSearchCV(XGB, param_grid, n_iter=20,\n                              n_jobs=1, verbose=2, cv=3,\n                              scoring='neg_mean_absolute_error', random_state=42)","e9e9f456":"xgb_para.fit(X_train, y_train)","d86ad2b9":"xgb_para.best_params_","3cc883e9":"XGB_para = xgb.XGBRegressor(subsample=0.6,silent = False,\n                           reg_lambda = 1, n_estimators=100,\n                           min_child_weight = 1, max_depth = 20,\n                           learning_rate = 0.1, gamma = 0.25,\n                           colsample_bytree = 0.8, colsample_bylevel = 0.8 )\nXGB_para.fit(X_train, y_train)\npredictions_para3 = XGB_para.predict(X_test)\npred_para_train3 = XGB_para.predict(X_train)","1b4dbcdf":"cv_mae_xg3 = cross_val_score(XGB_para,X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\ncv_mse4_xg3 = cross_val_score(XGB_para,X_train, y_train, cv=5, scoring='neg_mean_squared_error')\ncv_mae_score4_xg3 = -np.mean(cv_mae_xg3)\ncv_mse_score4_xg3 = -np.mean(cv_mse4_xg3)","1957322c":"master_performance_table = master_performance(master_performance_table,'Tuning Xgboost','data version 1',\n                                              predictions_para3,pred_para_train3,str(xgb_para.best_params_) ,cv_mae_score4_xg3,cv_mse_score4_xg3)","c1c1d487":"master_performance_table","b7c80068":"master_performance_table.to_csv('performance_table.csv')","a0ab1592":"fig, ax = plt.subplots(1, 1, figsize=(8, 13))\nxgb.plot_importance(XGB_para, max_num_features=20, height=0.5, ax=ax)","50a577eb":"## Normalized the dataset","f008a92d":"## Load dataset","7243ce61":"# The End","c72070d7":"## Predicting the future","8ff34f9e":"#### Hyper-Parameter Tuning","583e0e25":"### Update of this version\n- Used May 4 dataset\n- Generated data exploration\n- Analyzed death: RF\n- Normalized data, implemented neural network and generated performance comparison: deaths\n- Predicted into the future","11a5916f":"## Data distribution","146ed9a1":"#### Hyper Parameter for random forest data version 1","4f13c23c":"### hyper - parameter XGBoost ","228b8fe8":"## data version 2 models","2060306a":"## XGBoost","dcc45b82":"## Below: Zhengyang's XGBoost archived","cd17dc13":"## Deaths analyses"}}