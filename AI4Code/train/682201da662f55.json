{"cell_type":{"e6808c3c":"code","7a903feb":"code","c09da5ad":"code","7927dd51":"code","02a629bb":"code","b0c2b91b":"code","ffc1c031":"code","80c4e56f":"code","dd63ec46":"code","4f2a916c":"code","aaa0837e":"code","5a22ca2e":"code","18712da6":"code","7f380f91":"code","1b37f0e4":"code","21f4fe96":"code","04aa70d8":"code","1b75d30b":"code","1d0dad1a":"code","10912e71":"code","0df84ce4":"code","4d1c8379":"code","ef4db17b":"code","1a2d3afa":"markdown","07caea81":"markdown","3167e358":"markdown","5c6a9db2":"markdown","6ebf53af":"markdown","5a5e5b43":"markdown","87e3a333":"markdown","ff7553b3":"markdown","69a13755":"markdown","c1027785":"markdown","fe67e151":"markdown","4e22daf4":"markdown","456df075":"markdown","7cc8a2b7":"markdown","7da37b5d":"markdown","38fd5e79":"markdown","c3f67a62":"markdown","9b6571b3":"markdown","259fc41e":"markdown","ad5ab526":"markdown","5dd8334f":"markdown","912bdc8a":"markdown","eca59369":"markdown","cb6ebeb0":"markdown","e37339e6":"markdown","d50a10f1":"markdown"},"source":{"e6808c3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a903feb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nsns.set()\nnp.set_printoptions(threshold=sys.maxsize)","c09da5ad":"data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","7927dd51":"from sklearn.model_selection import train_test_split\nX = data.drop('SalePrice', axis=1)\ny = data['SalePrice']\nX_train, X_test, y_train, y_test =train_test_split(X,y, test_size=0.2)","02a629bb":"num = X_train.select_dtypes(exclude='object')\ncat = X_train.select_dtypes(include='object')","b0c2b91b":"num.drop('Id', axis=1, inplace=True)","ffc1c031":"from sklearn.impute import SimpleImputer\n\nImputer = SimpleImputer(missing_values=np.nan, strategy='mean').fit(num)\n\nnum_imputed = Imputer.transform(num)\n\nprint(np.isnan(num_imputed).sum()\/len(num))","80c4e56f":"num_imputed = pd.DataFrame(num_imputed, columns=num.columns)\nnum_imputed.head(3)","dd63ec46":"cm = sns.light_palette(\"green\", as_cmap=True)\n\nnum_var = pd.DataFrame((num\/num.mean()).var(), columns=['norm_variance']).apply(lambda x:np.round(x,4))\nnum_var = num_var.style.background_gradient(cmap=cm)\nnum_var","4f2a916c":"from sklearn.feature_selection import VarianceThreshold\n\nsel = VarianceThreshold(threshold=0.05).fit(num\/num.mean())\n\nsel_col = sel.get_support()\n\nnum_imputed_highVar = num_imputed.loc[:,sel_col]","aaa0837e":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(num_imputed_highVar)\n\nnum_imputed_highVar_scaled = scaler.transform(num_imputed_highVar)\nnum_imputed_highVar_scaled = pd.DataFrame(num_imputed_highVar_scaled, columns=num_imputed_highVar.columns)","5a22ca2e":"correlation = np.abs(num_imputed_highVar_scaled.corr())\ncorrelation.style.background_gradient(cmap=cm)","18712da6":"highly_correlated_name = dict()\nhighly_correlated_index = list()\nvisited = set()\n\n\nfor i in range(len(correlation.columns)):\n    for j in range(len(correlation.columns)):\n        if correlation.columns[i] != correlation.columns[j] and correlation.columns[i] not in visited and np.abs(correlation.loc[correlation.columns[i],correlation.columns[j]]) >=0.85:\n            highly_correlated_name[correlation.columns[i]]=correlation.columns[j]\n            highly_correlated_index.append(i)\n            visited.update([correlation.columns[i],correlation.columns[j]])\n\nnum_imputed_highVar_scaled_noCorr = num_imputed_highVar_scaled.drop(highly_correlated_name.keys(), axis=1)\nnumeric_preprocessed = num_imputed_highVar_scaled_noCorr.copy()","7f380f91":"missing = pd.DataFrame(cat.isnull().sum()\/len(cat), columns=['NaN'])\nmissing.style.background_gradient(cmap=cm)","1b37f0e4":"to_drop_cat = missing[missing['NaN']>=0.47]\ncat_dropTooMiss = cat.drop(to_drop_cat.index, axis=1)","21f4fe96":"cat_dropTooMiss_replaceNo = cat_dropTooMiss.fillna('No')","04aa70d8":"No = pd.DataFrame(['No' for i in range(len(cat_dropTooMiss_replaceNo.columns))],cat_dropTooMiss_replaceNo.columns)\ncat_dropTooMiss_replaceNo_forOHE = cat_dropTooMiss_replaceNo.append(No.T, ignore_index=True)\n\ncat_dropTooMiss_replaceNo_forOHE.tail(3)","1b75d30b":"from sklearn.preprocessing import OneHotEncoder\n\ncat_all = data.select_dtypes(include='object')\ncat_all.drop(to_drop_cat.index, axis=1, inplace=True)\ncat_all.fillna('No', inplace=True)\ncat_all = cat_all.append(No.T, ignore_index=True)\n\nohe = OneHotEncoder(sparse=False).fit(cat_all)\nohe.categories_","1d0dad1a":"cat_dropTooMiss_replaceNo_ohe = ohe.transform(cat_dropTooMiss_replaceNo)\ncategorical_preprocessed = cat_dropTooMiss_replaceNo_ohe.copy()","10912e71":"X_train_preprocessed = np.concatenate([categorical_preprocessed, numeric_preprocessed.values], axis=1)","0df84ce4":"y_scaler = StandardScaler().fit(y_train.values.reshape(-1,1))\ny_train_scaled = y_scaler.transform(y_train.values.reshape(-1,1))\ny_test_scaled = y_scaler.transform(y_test.values.reshape(-1,1))","4d1c8379":"def PREPROCESS(DATA: pd.DataFrame):\n    num = DATA.select_dtypes(exclude='object')\n    cat = DATA.select_dtypes(include='object')\n    \n    # NUMERIC\n        # 0) Drop 'Id'\n    num.drop('Id', axis=1, inplace=True)\n        # 1) Impute\n    num_imputed = Imputer.transform(num)\n        # 2) Filter out very low variance\n    num_imputed_highVar = num_imputed[:,sel_col]\n        # 3) Standard scale\n    num_imputed_highVar_scaled = scaler.transform(num_imputed_highVar)\n        # 4) Drop highly correlated columns\n    num_imputed_highVar_scaled_noCorr = np.delete(num_imputed_highVar_scaled, highly_correlated_index, axis=1)\n    # Preprocessed\n    numeric_preprocessed = num_imputed_highVar_scaled_noCorr.copy()\n    \n    \n    # CATEGORICAL\n        # 0) Drop too many missing\n    cat_dropTooMiss = cat.drop(to_drop_cat.index, axis=1)\n        # 1) Fill remaining missing value with 'No'\n    cat_dropTooMiss_replaceNo = cat_dropTooMiss.fillna('No')\n        # 2) One hot encoding\n    cat_dropTooMiss_replaceNo_ohe = ohe.transform(cat_dropTooMiss_replaceNo)\n    # Preprocessed\n    categorical_preprocessed = cat_dropTooMiss_replaceNo_ohe.copy()\n    \n    return np.concatenate([categorical_preprocessed, numeric_preprocessed], axis=1)","ef4db17b":"X_test_preprocessed = PREPROCESS(X_test)","1a2d3afa":"## 2) Discard the very low variance","07caea81":"We then scale the numerical features.","3167e358":"Now we have done\n- Drop 'Id'\n- Imputer  ->  **Imputer Obj.**\n- Filter out very low variance  ->  **sel_col Mask**\n- Standard scale  ->  **scaler Obj.**\n- Drop highly correlated columns  ->  **highly_correlated Dict**","5c6a9db2":"# Categorical features","6ebf53af":"## 3) Standard scale","5a5e5b43":"Finally, let's preprocess the test set as well.","87e3a333":"We'll drop the relatively low variance columns (<0.05)","ff7553b3":"## 4) Inspect correlation","69a13755":"Now we have done\n- Drop to many missing value  ->  **to_drop_cat**\n- Fill missing value with 'No' \n- One how encoding  ->  **ohe Obj.**","c1027785":"## 2) Fill 'No'\nAfter dropping those columns, remaining columns still have few missing value. <br>\nWe'll create a new class for them ('No')","fe67e151":"The result from SimpleImputer is Numpy array. Let's bring it back to interpletable Pandas DataFrame","4e22daf4":"We see that few features are correlated to each other. So, we have to drop them.","456df075":"# Separate numerical features(num) and categorical features(cat)","7cc8a2b7":"## 1) Missing values","7da37b5d":"Quickly visualize the variance of numerical features","38fd5e79":"Then we need to add 'No' to reference dataset for OneHotEncoder to have OneHotEncoder familiar with new added class.","c3f67a62":"## 3) One Hot encoding","9b6571b3":"# Declare independent variables(X) and target(y)","259fc41e":"## 1) Missing values","ad5ab526":"We saw that few features are missing too much. So, we'll drop them.","5dd8334f":"Create PREPROCESS() function that summarize what we've done so far.","912bdc8a":"# Numerical features","eca59369":"Scale the target feature (y)","cb6ebeb0":"# Categorical + Numerical","e37339e6":"## 0) Drop 'Id' column","d50a10f1":"# In this notebook, <br>\nI will show my way to preprocess the House-Price dataset. <br>\nI'm very beginner to this. <br>\n**Please feel free to discuss or leave an advice in the comment section :)** <br>\nThanks in advance"}}