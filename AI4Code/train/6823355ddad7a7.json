{"cell_type":{"defcac76":"code","1cefb93a":"code","51c58f26":"code","3027edaf":"code","ade3af05":"code","95b57364":"code","19aeb42e":"code","cb871020":"code","c0ac1e4d":"code","b66722e5":"code","45a21259":"code","1b23fa68":"code","1f0ff35a":"code","53c23541":"code","b7834120":"markdown"},"source":{"defcac76":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq","1cefb93a":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport collections\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom functools import partial\nfrom sklearn.model_selection import KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom datasets import Dataset\nfrom transformers import (AutoTokenizer, AutoModelForQuestionAnswering,AutoModel,\n                          AutoConfig,AdamW, get_cosine_schedule_with_warmup)","51c58f26":"train_data = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntest_data = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nsample = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv')\n\nconfig = {'max_length':384,\n          'doc_stride':128,\n          'max_answer_length':30,\n          \n          'lr':1e-5,\n          'wd':0.01,\n          'epochs':1,\n          'nfolds':5,\n          'batch_size':32,\n          'num_workers':4,\n          'seed':1000}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","3027edaf":"def prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    # ref: https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","ade3af05":"def postprocess_qa_predictions(\n    examples, tokenizer, features, raw_predictions, n_best_size=20, max_answer_length=30, squad_v2=False\n):\n    # ref: https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(examples):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None  # Only used if squad_v2 is True.\n        valid_answers = []\n\n        context = example[\"context\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char:end_char],\n                        }\n                    )\n\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n\n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n    return predictions","95b57364":"class ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):            \n        return {\"input_ids\": torch.tensor(self.data[idx][\"input_ids\"], dtype=torch.long),\n                \"attention_mask\": torch.tensor(self.data[idx][\"attention_mask\"], dtype=torch.long)}","19aeb42e":"class Model(nn.Module):\n    def __init__(self,model_name):\n        super(Model,self).__init__()\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.roberta = AutoModel.from_pretrained(model_name,config=config)\n        self.roberta.pooler = nn.Identity()\n        self.linear = nn.Linear(self.config.hidden_size,2)\n        \n    def loss_fn(self,start_logits,end_logits,start_positions,end_positions):\n        if len(start_positions.size()) > 1:\n            start_positions = start_positions.squeeze(-1)\n        if len(end_positions.size()) > 1:\n            end_positions = end_positions.squeeze(-1)\n\n        ignored_index = start_logits.size(1)\n        start_positions = start_positions.clamp(0, ignored_index)\n        end_positions = end_positions.clamp(0, ignored_index)\n        loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n        start_loss = loss_fct(start_logits, start_positions)\n        end_loss = loss_fct(end_logits, end_positions)\n        total_loss = 0.75 * start_loss + end_loss * 0.25\n        return total_loss\n    \n    def forward(self,**xb):\n        x = self.roberta(input_ids=xb['input_ids'],attention_mask=xb['attention_mask'])[0]\n        x = self.linear(x)\n        \n        start_logits,end_logits = x.split(1,dim=-1)\n        start_logits,end_logits = start_logits.squeeze(-1).contiguous(),end_logits.squeeze(-1).contiguous()\n            \n        return (start_logits,end_logits)","cb871020":"def get_prediction(df,model_paths,device='cuda'):\n    start_logits = list()\n    end_logits = list()\n    \n    for path,model_name in model_paths:\n#         model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n        model = Model(model_name)\n        model.eval()\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        pad_on_right = tokenizer.padding_side == 'right'\n\n        for f in range(config['nfolds']):\n            model.load_state_dict(torch.load(path.format(f),map_location=device))\n            model.to(device)\n            model.eval()\n\n            test_dataset = Dataset.from_pandas(df)\n            test_features = test_dataset.map(\n                            partial(\n                                prepare_validation_features, \n                                tokenizer=tokenizer,\n                                pad_on_right=pad_on_right, \n                                max_length=config['max_length'],\n                                doc_stride=config['doc_stride']\n                            ),\n                            batched=True,\n                            remove_columns=test_dataset.column_names)\n\n            test_feats_small = test_features.map(lambda example: example, remove_columns=['example_id', 'offset_mapping'])\n\n            test_ds = ChaiiDataset(test_feats_small)\n            test_dl = DataLoader(test_ds,\n                                batch_size = config[\"batch_size\"],\n                                num_workers = config['num_workers'],\n                                shuffle=False,\n                                pin_memory=True,\n                                drop_last=False)\n\n            with torch.no_grad():\n                pred = list()\n                start_logit = list()\n                end_logit = list()\n                for i, inputs in enumerate(test_dl):\n                    inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n                    outputs = model(**inputs)\n                    start = outputs[0].detach().cpu().numpy().tolist()\n                    end = outputs[1].detach().cpu().numpy().tolist()\n                    start_logit.extend(start)\n                    end_logit.extend(end)\n\n            start_logits.append(start_logit)\n            end_logits.append(end_logit)\n\n    torch.cuda.empty_cache()\n    start_logits, end_logits = np.mean(start_logits,axis=0), np.mean(end_logits,axis=0)\n\n    fin_preds = postprocess_qa_predictions(test_dataset, tokenizer, test_features, (start_logits, end_logits))\n    return fin_preds","c0ac1e4d":"model_paths = [\n    ('..\/input\/chaii-pytorch-xlmroberta-large\/model{0}\/model{0}.bin','..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2'),\n]","b66722e5":"predictions = get_prediction(test_data,model_paths)","45a21259":"test_data['PredictionString'] = test_data['id'].map(predictions)","1b23fa68":"bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\nbad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n\ntamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\ntamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\ntamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\nhindi_ad = \"\u0908\"\nhindi_bc = \"\u0908.\u092a\u0942\"\n\ncleaned_preds = []\nfor pred, context in test_data[[\"PredictionString\", \"context\"]].to_numpy():\n    if pred == \"\":\n        cleaned_preds.append(pred)\n        continue\n    while any([pred.startswith(y) for y in bad_starts]):\n        pred = pred[1:]\n    while any([pred.endswith(y) for y in bad_endings]):\n        if pred.endswith(\"...\"):\n            pred = pred[:-3]\n        else:\n            pred = pred[:-1]\n    \n    if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n        pred = pred+\".\"\n\n    cleaned_preds.append(pred)\n\ntest_data[\"PredictionString\"] = cleaned_preds","1f0ff35a":"test_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)","53c23541":"test_data","b7834120":"Training Notebook: \nhttps:\/\/www.kaggle.com\/maunish\/chaii-pytorch-train-xlmroberta-large"}}