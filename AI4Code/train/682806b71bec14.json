{"cell_type":{"2be268a7":"code","896a1041":"code","6c6d82c1":"code","35194f4c":"code","929a2243":"code","067feef2":"code","c3be0a2c":"code","23c1facf":"code","4f4d12a0":"code","a1e920b3":"code","7fb689bf":"code","4a7c42d6":"code","2934a138":"code","7580ec59":"code","21a0f1d6":"code","494b6a3e":"code","7ec33659":"code","60bcb469":"code","c23916f2":"code","49a92150":"code","510d0b75":"code","3a604990":"code","d9b1a733":"code","e9c0165d":"code","18a48089":"code","4f8d9851":"code","095ae41d":"code","f6632832":"code","451d70ed":"code","9db7a963":"code","a627ae86":"code","bd976488":"code","88a04505":"code","63cbcd7a":"code","92b85e17":"code","02c6a243":"code","2696dece":"code","f04387bd":"code","01ef9206":"code","50940120":"code","c9094621":"code","ddb178e3":"code","ff95dbdc":"code","9196863d":"code","0423777e":"code","66df10e6":"code","bea826f4":"code","bb1120b8":"code","10168b48":"code","39bae16d":"code","5113e93a":"code","ff5a58a4":"code","ba31a838":"code","4dceb3e8":"code","c6888d16":"code","8da45d05":"code","e6d7d810":"code","88195d21":"code","9e4e3bad":"code","1739a06b":"code","27cb15b1":"code","06053299":"code","9961c9ed":"code","3980cd7d":"code","9629da49":"code","46b17510":"code","99d4e132":"code","fd2916c1":"code","c97a6dbd":"code","af7b4cf6":"code","a35536ff":"code","b153fe5f":"code","8f9025a1":"code","ac17a56d":"code","ba5d1fb5":"code","18085537":"code","352474ea":"code","431a9107":"code","3ecd2a7f":"code","1c9b9672":"code","5d7d2176":"code","1ca76aac":"code","8c405352":"code","ce800a35":"code","58ed7be9":"code","78a9cc79":"code","2f252cca":"code","2743229d":"code","c68257c9":"code","5558928a":"code","9d1a72f9":"code","cc0e0e39":"code","6b1819ae":"code","e81d5489":"code","2c40a006":"code","79d56760":"code","7c489d14":"code","be00114a":"code","0800b48d":"code","0cc2ecb6":"code","5c0c2eb8":"code","16fa0c02":"code","052bdb52":"code","655bb039":"code","c886a2ad":"code","8aa7272a":"code","962e5534":"code","737b83be":"code","d2626965":"code","d71e2997":"code","52533d24":"code","efd0233d":"code","08ce0f63":"code","0b332820":"code","12378d86":"code","696b454d":"code","0bff8619":"code","13cf02c2":"code","94070706":"code","b17394b0":"code","4c9aa288":"code","e82cc0a8":"code","0ae9e648":"code","5ea21eac":"code","e4c3b55b":"code","f3db849d":"code","74aef0c4":"code","991f453f":"code","f2b485a6":"code","6de52b9d":"code","49374c6b":"code","84ed8152":"code","82243f37":"code","bcd72158":"code","5c80bd62":"code","03fd7a3f":"code","5f149f4c":"code","1b552896":"code","73328c1e":"code","2fa0de81":"code","dfc37b9c":"code","7eacd4e5":"code","07bfc9a4":"markdown","fdac619e":"markdown","9825d582":"markdown","4b53e8ba":"markdown","94684ded":"markdown","6382d579":"markdown","b0670818":"markdown","12b425d8":"markdown","db4cf60d":"markdown","ad4b5110":"markdown","d498a075":"markdown","ba62c086":"markdown","38f670de":"markdown","9561033c":"markdown","287d9c01":"markdown","90fb22ec":"markdown","74e0ceea":"markdown","2c2f6da3":"markdown","ded20c06":"markdown","75e6acf2":"markdown","37846e6d":"markdown","f976f659":"markdown","b7543b7f":"markdown","48f01ad2":"markdown","972adc5d":"markdown","402b133f":"markdown","9a41177a":"markdown","32783a96":"markdown","8495f381":"markdown","92cc4b19":"markdown","64d1276d":"markdown","7d3f2701":"markdown","ccb9dfa3":"markdown","f892cc2d":"markdown","1f851876":"markdown","5608703d":"markdown","6efcf170":"markdown","b231ea9d":"markdown","719b29dc":"markdown","6232f61b":"markdown","e4d500d8":"markdown","1e380d25":"markdown","4bcd12d9":"markdown","fccf291d":"markdown","a9d54fe5":"markdown","3aae0a0e":"markdown","19694ea2":"markdown","8e0c9415":"markdown","f52b85ec":"markdown","6858f5f2":"markdown","9010a8b8":"markdown","ce387f7d":"markdown","c2119773":"markdown","b84de55a":"markdown","4d2a4766":"markdown","b2b70d28":"markdown","e0d24a9a":"markdown","d1b97b03":"markdown","d08a78e3":"markdown","81e11d23":"markdown","9b13cb5d":"markdown","e74a42ba":"markdown","72e08aa1":"markdown","4a369257":"markdown","7f885933":"markdown","9b40a727":"markdown","087879e0":"markdown","aa75e035":"markdown","8f444729":"markdown","61250367":"markdown","1d8ccb27":"markdown","56cfbbcc":"markdown","f7133eb0":"markdown","fee05947":"markdown","851f3612":"markdown","c126b629":"markdown","5ec0e9be":"markdown","9ba0795a":"markdown","4571bd78":"markdown","5efc6ab1":"markdown","1f845b85":"markdown","6a1256a6":"markdown","578b98d6":"markdown","9fea4d87":"markdown","bc8a5afd":"markdown","d366aa56":"markdown"},"source":{"2be268a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport time\nimport math\nimport datetime\n\n\n# Import widgets\nfrom ipywidgets import widgets, interactive, interact\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport scipy\nimport statsmodels\nfrom scipy import signal\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.stattools import adfuller\n\nimport itertools\nfrom itertools import cycle\nplt.style.use('seaborn')\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","896a1041":"sell_prices_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ntrain_sales_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsubmission_file = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n","6c6d82c1":"sell_prices_df.info()","35194f4c":"train_sales_df.info()","929a2243":"calendar_df.info()","067feef2":"train_sales_df.head(3)","c3be0a2c":"d_cols = [c for c in train_sales_df.columns if 'd_' in c]\ntrain_sales_df['total_sales_all_days'] = train_sales_df[d_cols].sum(axis = 1)\ntrain_sales_df['avg_sales_all_days'] = train_sales_df[d_cols].mean(axis = 1)\ntrain_sales_df['median_sales_all_days'] = train_sales_df[d_cols].median(axis = 1)\n#train_sales_df.groupby(['id'])['total_sales_all_days'].sum().sort_values(ascending=False)","23c1facf":"df = train_sales_df.groupby(['cat_id'])['id'].count().reset_index(name='total_entries')\nfig = px.pie(df, values='total_entries', names='cat_id', \n            color_discrete_sequence=px.colors.sequential.RdBu,\n            width = 750, height=450, title = 'Distribution of Product_IDs Across Categories')\nfig.show()","4f4d12a0":"df = train_sales_df.groupby(['state_id'])['total_sales_all_days'].sum().reset_index()\nfig = px.pie(df, values='total_sales_all_days', names='state_id', \n            color_discrete_sequence=px.colors.sequential.Aggrnyl,\n            width = 750, height=450, title = 'Distribution of Total_Sales Across States')\nfig.show()","a1e920b3":"df1 = train_sales_df.groupby(['cat_id'])['id'].count().reset_index(name='total_entries')\ndf2 = train_sales_df.groupby(['cat_id', 'state_id'])['total_sales_all_days'].sum().reset_index()","7fb689bf":"sns.set_style('whitegrid')\nsns.axes_style(style='ticks')\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n\nsns.barplot(x = 'cat_id', y='total_entries', data=df1, \n            palette='mako', ax=ax1)\nsns.barplot(x = 'cat_id', y='total_sales_all_days', hue='state_id', data=df2, \n            palette='magma', ax=ax2)\n\nplt.xticks(rotation=90)\nplt.show()","4a7c42d6":"#df3 = train_sales_df.groupby(['cat_id', 'store_id'])['total_sales_all_days'].sum().reset_index()","2934a138":"#sns.set_style('whitegrid')\n#sns.axes_style(style='ticks')\n#fig, ax1 = plt.subplots(figsize=(14,5))\n\n#sns.barplot(x = 'store_id', y='total_sales_all_days', hue='cat_id', data=df3, \n#            palette='afmhot', ax=ax1)\n\n#plt.xticks(rotation=90)\n#plt.show()","7580ec59":"df = train_sales_df.groupby(['state_id', 'cat_id'])['id'].count().reset_index(name='num_sales_by_category')\nfig = px.bar(df, x=\"state_id\", y=\"num_sales_by_category\", \n             color=\"cat_id\", title=\"Distribution of Product_ids Count Across Categories & Each Locale\")\nfig.show()","21a0f1d6":"df = train_sales_df.groupby(['dept_id', 'store_id', 'state_id', 'cat_id'])[train_sales_df.columns[6:]].sum().reset_index()\ndf = df.sort_values('total_sales_all_days', ascending=False)","494b6a3e":"x_dept = df['dept_id']\nx_store = df['store_id']\n\ndef items_sold_per_days(x_spec,title_text, title):\n    \n    '''\n    returns plotly plots with drop down menus for specified parameter made in dataframe earlier\n    \n    inputs: x_spec (categorical feature on the x_axis), title_text(title on dropdown), \n            title (title of the plot)\n            \n    returns: plotly plots of categorical feature (x_axis) with dropdowns on specific \n    number of days        \n    '''\n    \n    cols = ['d_1', 'd_50', 'd_300', 'd_500', 'd_700', 'd_900', 'd_1100', 'd_1500', 'd_1700',\n        'total_sales_all_days', 'median_sales_all_days']\n\n    buttons1 = [dict(method = \"restyle\",\n                 args = [{'x': [x_spec, 'undefined'],\n                          'y': [df[cols[k]], 'undefined'],\n                          'visible':[True, False]}], \n                 label = cols[k])   for k in range(0, len(cols))]\n\n    fig = go.Figure()\n    fig.add_trace(go.Bar(x=x_spec, y = df['d_1'], name='Dept.Sales on day2',\n                     marker_color='Crimson'))\n\n    fig.update_layout(title_text= title_text,\n                  title_x= 0.4, width=750, height=450, \n                  margin=dict(t=100, b=20, l=0, r=0),\n                  autosize = False,\n                  updatemenus=[dict(active=0,\n                                    buttons=buttons1,\n                                    x=0.08,\n                                    y=1.13,\n                                    xanchor='left',\n                                    yanchor='top')\n                              ]); \n\n    fig.add_annotation( x=0.00,  y=1.13, showarrow=False, xref='paper', yref='paper', xanchor='left',\n                   text=\"With<br>\"+str(title));\n    fig.show()\n\nitems_sold_per_days(df['store_id'], 'Distribution of Sales Made on Each Store', 'Stores')\nitems_sold_per_days(df['state_id'], \"Distribution of Sales Made In Each State\", 'States')","7ec33659":"train_sales_df.groupby(['id'])['total_sales_all_days'].sum().sort_values(ascending=False)","60bcb469":"sns.set_style('whitegrid')\n\ndef plot_dailysales(spec_id):\n    \"\"\"\n    plots the behavior of dailysales of specific ids i.e. spec_id\n    \n    input: spec_id\n    returns : number of sales plotted across number of days \n    \"\"\"\n    train_sales_df.loc[train_sales_df['id'] == spec_id ].set_index('id')[d_cols]\\\n                .T\\\n                .plot(figsize = (13,2.5),\n                      title =  str(spec_id)+\"_item daily sales\", \n                      color = next(color_cycle) )\n    plt.legend()\n    plt.show()\n\nplot_dailysales('FOODS_3_090_CA_3_validation') \nplot_dailysales('FOODS_3_586_TX_2_validation')","c23916f2":"df_agg = pd.DataFrame(train_sales_df.groupby(['id', 'cat_id', 'store_id'])['total_sales_all_days'].sum().sort_values(ascending=False))\ndf_agg = df_agg.reset_index()\ndf_agg.head(3)","49a92150":"print(\"The 3 item_ids outselling most in FOODS category are: {}\".format(list(df_agg.loc[df_agg['cat_id'] == 'FOODS']['id'][:3])))\nprint(\"The 3 item_ids outselling most in HOUSEHOLDS category are: {}\".format(list(df_agg.loc[df_agg['cat_id'] == 'HOUSEHOLD']['id'][:3])))\nprint(\"The 3 item_ids outselling most in HOBBIES category are: {}\".format(list(df_agg.loc[df_agg['cat_id'] == 'HOBBIES']['id'][:3])))","510d0b75":"#df = pd.DataFrame({\"days\": list(train_sales_df[train_sales_df['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].columns),\n#                   \"sales_data\": list(train_sales_df[train_sales_df['id'] == 'FOODS_3_090_CA_3_validation'][d_cols].values.flatten())})","3a604990":"#fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 7))\n\ndef plot_sample_sales(spec_id, sm_start, sm_end, samples_pick=50):\n    \"\"\"\n    plots sample sales data with selection point and ending point specified, along with\n    samples_pick point that specifies the samples picked after specified intervals\n    \n    input: spec_id (item_id or id), sm_start (sample_start), sm_end(sample_end),\n    samples_pick (samples picked after how many intervals)\n    \n    returns: outputs a graph of sample points plotted against daily sales data d_1 to d_1913\n    \"\"\"\n    fig, ax1 = plt.subplots(figsize=(13, 2.5))\n    \n    x1 = list(train_sales_df[train_sales_df['id'] == spec_id][d_cols]\\\n              .columns)[sm_start:sm_end]\n    y1 = list(train_sales_df[train_sales_df['id'] == spec_id][d_cols]\\\n              .values.flatten())[sm_start:sm_end]\n    \n    #this conversion for regplot only\n    x1 = [x.replace(\"d_\", \"\") for x in x1]\n    x1 = [int(x) for x in x1]\n    \n    #sns.lineplot(x=x1, y=y1, ax=ax1)\n    sns.regplot(x=x1, y=y1, order=10, ax=ax1)\n    ax1.set_ylabel(\"Number of Sales\")\n    ax1.set_xlabel(\"Days\")\n\n    ax1.set_xticks(x1[::samples_pick])\n    ax1.set_xticklabels(x1[::samples_pick], rotation=0)\n\n    fig.tight_layout()\n    plt.show()","d9b1a733":"plot_sample_sales('FOODS_3_090_CA_3_validation', 500, 1300)\nplot_sample_sales('FOODS_3_586_TX_2_validation', 500, 1300)\nplot_sample_sales('FOODS_3_090_CA_1_validation', 500, 1300)\n","e9c0165d":"sell_prices_df.head(3)","18a48089":"# making a new column category out of the item_id \nsell_prices_df['category'] = sell_prices_df['item_id'].str.split(\"_\", expand=True)[0]","4f8d9851":"sns.set_style('whitegrid')\n#plt.figure(figsize=(15,5))\n\ndef kde_plotting(df, category, bin_size, color, label):\n    \n    '''\n    plots the kde density plot of the continuous features of df specified\n    \n    inputs: df, category(whether, foods, household or hobbies), bin_size(bin size for histogram)\n            color (color of the plot), label (label to the plot)\n    returns: kde plots with logarithmic scale taken on x_axis\n            \n    '''\n    fig, ax1 = plt.subplots(figsize=(13, 2.5) )\n\n    sns.distplot(df[df['category'] == category]['sell_price'], \n               axlabel = label ,bins=bin_size, color = color, ax=ax1) \n\n    fig.tight_layout()\n    ax1.set_xscale('log')\n    plt.legend()\n    plt.show()\n    \nkde_plotting(sell_prices_df, 'HOBBIES', 150, 'b', 'hobbies')   \nkde_plotting(sell_prices_df, 'FOODS', 250, 'g', 'foods') \nkde_plotting(sell_prices_df, 'HOUSEHOLD', 150, 'r', 'household') ","095ae41d":"def remove_outliers(df):\n    \n    '''\n    removes the outliers in continous features using quartile ranges\n    \n    inputs: df(df specified with continous features along side categorical features)\n    returns: df with removed outliers\n    '''\n    Q1=df.quantile(0.25)\n    Q3=df.quantile(0.75)\n    IQR=Q3-Q1\n    df_final=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))]\n    \n    return df_final\n\ndf = sell_prices_df[['category', 'sell_price']]\ndf = remove_outliers(df)","f6632832":"sns.set_style('whitegrid')\nsns.axes_style(style='ticks')\nplt.figure(figsize=(13,3))\nsns.boxplot(y=df['category'], x=df['sell_price'])\nplt.show()","451d70ed":"calendar_df.head(3)","9db7a963":"calendar_df.groupby(['event_name_1', 'event_type_1'])['wday'].count()","a627ae86":"calendar_df.groupby(['event_name_2', 'event_type_2'])['wday'].count()","bd976488":"#Making in and including a new entry of days as well as merging the events_1 and event_2 into\n# a single new events_names and types category\n\ncalendar_df['days'] = [d.split('-')[2] for d in calendar_df['date']]\ncalendar_df['events_names'] = pd.concat([calendar_df['event_name_1'], calendar_df['event_name_2']], \n                                        ignore_index=True)\ncalendar_df['events_types'] = pd.concat([calendar_df['event_type_1'], calendar_df['event_type_2']], \n                                        ignore_index=True)\n#calendar_df.drop(['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'], axis=1, inplace=True)","88a04505":"df = calendar_df.groupby(['events_types'])['snap_CA'].value_counts().reset_index(name='counts')","63cbcd7a":"sns.set_style('whitegrid')\nsns.axes_style(style='ticks')\nplt.figure(figsize=(13,3))\nsns.barplot(x = 'events_types', y='counts', hue='snap_CA', data=df, palette='bwr')\nplt.show()","92b85e17":"df = calendar_df.groupby(['events_names'])['snap_CA'].value_counts().reset_index(name='counts')","02c6a243":"sns.set_style('whitegrid')\nsns.axes_style(style='ticks')\nplt.figure(figsize=(13,3))\nsns.barplot(x = 'events_names', y='counts', hue='snap_CA', data=df, \n            order = df.sort_values(['counts'], ascending=False).events_names, \n            palette='OrRd')\nplt.xticks(rotation=90)\nplt.show()","2696dece":"#product id df (train_sales_df) vs revenue_df(sell_prices_df)\ntrain_sales_prices_df = train_sales_df.merge(sell_prices_df, how='inner', on='item_id', \n                                            left_index=True, right_index=True, \n                                            validate=\"1:1\")\n","f04387bd":"df = train_sales_prices_df.groupby(['cat_id', 'state_id', 'store_id_x'])['sell_price'].sum().reset_index(name='total_revenue')\ndf = df.sort_values(by='total_revenue', ascending=False)","01ef9206":"sns.set_style('whitegrid')\nsns.axes_style(style='ticks')\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n\nsns.barplot(x = 'state_id', y='total_revenue', data=df, \n            palette='coolwarm', ax=ax1)\nsns.barplot(x = 'cat_id', y='total_revenue', hue='state_id', data=df, \n            palette='plasma', ax=ax2)\n\nplt.xticks(rotation=90)\nplt.show()","50940120":"#train_sales_cal_df = train_sales_df.set_index('id')[d_cols].T.merge(calendar_df.set_index('d'), \n #                                                                   left_index=True, right_index=True,\n #                                                                   validate=\"1:1\").set_index('date')\n","c9094621":"##train_sales_cal_df.T","ddb178e3":"#Changing the date type to datetime so that it is depicted as so:\n\n#calendar_df.date = pd.to_datetime(calendar_df.date)\n#type(calendar_df.date)","ff95dbdc":"train_sales_cal_df = train_sales_df.set_index('id')[d_cols].T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\").set_index('date')\ntrain_sales_cal_df['total_sales'] = train_sales_cal_df.sum(axis=1)\n","9196863d":"train_sales_cal_df = train_sales_cal_df[['total_sales']]\ntrain_sales_cal_df = train_sales_cal_df.reset_index()","0423777e":"fig = px.line(train_sales_cal_df, x='date', y=\"total_sales\", \n             template = 'plotly_white', title = 'Total Sales by Product id', \n             width=800, height=450)\n\nfig.update_xaxes(rangeslider_visible=True)\nfig.update_layout(\n    xaxis=dict(\n    rangeselector=dict( buttons=list([ dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n    dict(count=6,label=\"6m\", step=\"month\", stepmode=\"backward\"),\n    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n    dict(step=\"all\")\n            ]) \n                      ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\"\n    )\n)\n\nfig.show()","66df10e6":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndef time_series_decompose(df, model_specified, period):\n\n    '''\n    returns decomposed time series components (with seasonality, trend and residuals ) of time_series df\n    \n    inputs: df(time series df), model_specified (whether additive or multiplicative ), period(number of days)\n    returns: trend values, seasonality values and residual values of time_series df\n    \n    '''\n    df_decomposition = seasonal_decompose(\n                               x=df.set_index(['date']), \n                               model= model_specified, \n                               period=period)\n    \n    est_trend = df_decomposition.trend\n    est_trend = est_trend.reset_index(name='trend').set_index('date')\n    est_seasonal = df_decomposition.seasonal\n    est_seasonal = est_seasonal.reset_index(name='seasonal').set_index('date')\n    est_residual = df_decomposition.resid\n    est_residual = est_residual.reset_index(name='resid').set_index('date')\n    \n    return est_trend, est_seasonal, est_residual\n\nsample_time_s1 = train_sales_cal_df[400:700]\nest_trend, est_seasonal, est_residual = time_series_decompose(sample_time_s1, \n                                                              'multiplicative', 30)","bea826f4":"fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 7))\n\nest_trend.plot(figsize=(15, 5) ,\n                 alpha=0.9, \n                 lw=3, ax = ax1)\nest_seasonal.plot(figsize=(15, 5) ,\n                 alpha=0.9, \n                 lw=3, ax = ax2)\nest_residual.plot(figsize=(15, 5) ,\n                 alpha=0.9, \n                 lw=3, ax = ax3)\n\nfig.tight_layout()\nplt.show()","bb1120b8":"train_sales_cal_df = train_sales_cal_df.set_index('date')","10168b48":"def time_and_rolling_plot(df, title_text, rolling_days=30, height=2.2):\n\n    '''\n    plots the time_series of dailysales made along with rolling mean plot\n    \n    inputs: df, title_text(plot title), rolling_days (rolling mean to be taken across), \n            height(height of the plot to be specified)\n            \n    returns: time series plot and rolling mean plot of the variable specified \n    '''\n    sns.set_style('whitegrid')\n    fig, ax1 = plt.subplots(1, 1, figsize=(13, height))\n    df.plot(figsize=(13, height),\n                        alpha=0.8,\n                        title=\"Time Plot for the \" + str(title_text), \n                        lw=2, color=next(color_cycle), ax=ax1)\n    df.rolling(rolling_days).mean().plot(figsize=(13, height),\n                                          alpha=1, \n                                          legend=\"Rolling Mean on 30 days\", \n                                          lw=2.5, color=next(color_cycle), ax=ax1)\n    ax1.legend(['Total Sales Per Category', 'total_sales_rolling_mean'])\n    plt.show()","39bae16d":"time_and_rolling_plot(train_sales_cal_df, 'Total_Sales_Data')","5113e93a":"#finding the product_ids of 25 most sold units\ntrain_sales_most_sold_25 = train_sales_df.groupby(['id'])['total_sales_all_days'].sum(axis=1).sort_values(ascending=False)[:25]\ntrain_sales_most_sold_25 = train_sales_most_sold_25.reset_index(name='total_sales')\ntrain_sales_most_sold_25.iloc[0:5]","ff5a58a4":"train_sales_most_sold_25['state'] = train_sales_most_sold_25['id'].str.split(\"_\", expand=True)[3]\ntrain_sales_most_sold_25.groupby(['state'])['total_sales'].sum(axis=1).sort_values(ascending=False)","ba31a838":"train_sales_cal_df = train_sales_df.set_index('id')[d_cols].T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\").set_index('date')","4dceb3e8":"time_and_rolling_plot(train_sales_cal_df[['FOODS_3_586_WI_3_validation']], \"Product_id_FOODS_3_586_WI_3_validation\")\ntime_and_rolling_plot(train_sales_cal_df[['FOODS_3_090_CA_1_validation']], \"Product_id_FOODS_3_090_CA_1_validation\")\ntime_and_rolling_plot(train_sales_cal_df[['FOODS_3_586_CA_3_validation']], \"Product_id_FOODS_3_586_CA_3_validation\")\ntime_and_rolling_plot(train_sales_cal_df[['FOODS_3_090_TX_2_validation']], \"Product_id_FOODS_3_090_TX_2_validation\")\ntime_and_rolling_plot(train_sales_cal_df[['FOODS_3_120_CA_3_validation']], \"Product_id_FOODS_3_120_CA_3_validation\")\ntime_and_rolling_plot(train_sales_cal_df[['FOODS_3_252_TX_3_validation']], \"Product_id_FOODS_3_252_TX_3_validation\")","c6888d16":"train_sales_cal_df = train_sales_df.groupby(['cat_id'])[d_cols].sum(axis=1).\\\n                                                T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\").set_index('date')","8da45d05":"train_sales_cal_df.head(2)","e6d7d810":"def plotly_time_plot_markers(df, text, samples_onwards=1000):\n\n    '''\n    plots time_series plotly scatter plot of the format specified in df \n    \n    inputs: df(dataframe), text(title text ), samples_onwards (time_series_sample and the end)\n    returns : plotly time series scatter plot\n    \n    '''\n    fig = go.Figure()\n    for c in df.columns[:4]:\n        fig.add_traces(go.Scatter(x=train_sales_cal_df[samples_onwards:].index, \n                              y=train_sales_cal_df[samples_onwards:][c],\n                              mode='markers',\n                              name = c))\n\n    fig.update_layout( title = 'Sales per Day by ' + str(text), template = 'plotly_white',\n        width=750, height=550,\n        xaxis_tickformatstops = [\n            dict(dtickrange=[\"M1\", \"M12\"], value=\"%b '%y\"),\n        ]\n    )\n    \n    fig.update_layout(\n    xaxis=dict(\n    rangeselector=dict( buttons=list([ dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n    dict(count=6,label=\"6m\", step=\"month\", stepmode=\"backward\"),\n    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n    dict(step=\"all\") ])  ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\" ) )\n\n    fig.update_xaxes(rangeslider_visible=True)    \n    fig.show()    \n\nplotly_time_plot_markers(train_sales_cal_df, 'Category')  ","88195d21":"train_sales_cal_df = train_sales_df.groupby(['state_id'])[d_cols].sum(axis=1).T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\").set_index('date')","9e4e3bad":"#train_sales_cal_df = train_sales_cal_df.reset_index()\ntrain_sales_cal_df","1739a06b":"from plotly.subplots import make_subplots\n\ndef plotly_time_plot_rolling(df, text, start_col=0, end_col=3, rolling_days=30):\n\n    '''\n    time series rolling plots of the categories specified in df\n    inputs: df (dataframe with ts as index, and columns as categories), text (title of the plot)\n            rolling_days (number of rolling days)\n    \n    returns: plotly time series plot of sales along the categories specified as columns in df\n    '''\n    fig = go.Figure()\n    for c in df.columns[start_col:end_col]:\n        fig.add_traces(go.Scatter(x=df.rolling(rolling_days).mean().index, \n                              y=df.rolling(rolling_days).mean()[c],\n                              mode='lines+markers',\n                              name = c))\n\n    fig.update_layout( title = 'Sales per Day by ' + str(text), template = 'plotly_white',\n        width=750, height=550,\n        xaxis_tickformatstops = [\n            dict(dtickrange=[\"M1\", \"M12\"], value=\"%b '%y\"),\n        ]\n    )\n    \n    fig.update_layout(\n    xaxis=dict(\n    rangeselector=dict( buttons=list([ dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n    dict(count=6,label=\"6m\", step=\"month\", stepmode=\"backward\"),\n    dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n    dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n    dict(step=\"all\") ])  ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\" ) )\n\n    fig.update_xaxes(rangeslider_visible=True)    \n    fig.show()   \n\nplotly_time_plot_rolling(train_sales_cal_df, 'State')","27cb15b1":"#referring to the solution provided at https:\/\/stackoverflow.com\/questions\/43223615\/join-dataframes-one-with-multiindex-columns-and-the-other-without\n\ntrain_sales_cal_df = train_sales_df.groupby(['state_id', 'cat_id'])[d_cols].sum(axis=1).T\\\n              .join(pd.concat([calendar_df.set_index('d')['date']], axis=1, keys=['dates']))\ntrain_sales_cal_df.head(2)","06053299":"#train_sales_cal_df = train_sales_cal_df.set_index('dates')","9961c9ed":"select_cols_ca = [c for c in train_sales_cal_df.columns if 'CA' in c]\nselect_cols_tx = [c for c in train_sales_cal_df.columns if 'TX' in c]\nselect_cols_wi = [c for c in train_sales_cal_df.columns if 'WI' in c]\n#train_sales_df['total_sales_all_days'] = train_sales_df[d_cols].sum(axis = 1)\n#train_sales_cal_df = \ndf_CA = train_sales_cal_df.set_index('dates')[select_cols_ca]\ndf_TX = train_sales_cal_df.set_index('dates')[select_cols_tx]\ndf_WI = train_sales_cal_df.set_index('dates')[select_cols_wi]\n\n#df_CA.columns = [col[1] for col in df_CA.columns]\n#df_TX.columns = [col[1] for col in df_TX.columns]\n#df_WI.columns = [col[1] for col in df_WI.columns]","3980cd7d":"time_and_rolling_plot(df_CA, 'CA(California) FOODS, HOBBIES and HOUSEHOLD', height=2.7)\ntime_and_rolling_plot(df_TX, 'TX(Texas) FOODS, HOBBIES and HOUSEHOLD', height=2.7)\ntime_and_rolling_plot(df_WI, 'WI(Wisconsin) FOODS, HOBBIES and HOUSEHOLD', height=2.7)\n#time_and_rolling_plot(df_CA.iloc[:,1].reset_index(), \"CA HOBBIES\")\n#time_and_rolling_plot(df_CA.iloc[:,2].reset_index(), \"CA State\")","9629da49":"train_sales_cal_df = train_sales_df.set_index('id')[d_cols].T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\").set_index('date')\ntrain_sales_cal_df['total_sales'] = train_sales_cal_df.sum(axis=1)\n","46b17510":"item_ids = [c for c in train_sales_cal_df.columns if 'validation' in c]","99d4e132":"\ntrain_sales_cal_df = train_sales_cal_df.reset_index()\ntrain_sales_cal_df['date'] = pd.to_datetime(train_sales_cal_df['date'])\ntrain_sales_cal_df['month'] = train_sales_cal_df.date.dt.month\ntrain_sales_cal_df['year'] = train_sales_cal_df.date.dt.year","fd2916c1":"df = train_sales_cal_df.groupby(['year', 'month'])[item_ids].sum()\ndf = df.reset_index()","c97a6dbd":"def find_items_per_specific_date(df, year, month, category, number):\n    \n    '''\n    finds the five most sold product_ids in user specified category, year and month\n    \n    inputs: df(item_ids along with year and month), year, month, category (household, \n    foods or hobbies), number (how many results to be displayed)\n    \n    returns : table of product ids with specfied number and date and number of sales made\n    '''\n    \n    df = df.loc[(df['year'] == year) & (df['month'] == month)][item_ids].T.reset_index()\n    df.columns = ['item_id', 'sales_count']\n    df = df.set_index('item_id')\n    item_type = [c for c in df.index if category in c]\n    df = df[df.index.isin(item_type)]\n    df = df.sort_values(by = ['sales_count'], ascending=False)[:number]\n    \n    return df\n\nprint(\"Most sold Foods during requested date\\n\"\n      ,find_items_per_specific_date(df, 2012, 7, 'FOODS', 5) )\nprint(\"Most sold Household items during requested date\\n\" \n      ,find_items_per_specific_date(df, 2013, 3, 'HOUSEHOLD', 5) )","af7b4cf6":"train_sales_cal_df = train_sales_df.groupby(['store_id'])[d_cols].sum(axis=1)\\\n                                                .T.merge(calendar_df.set_index('d')['date'], \n                                                        left_index=True, right_index=True,\n                                                        validate=\"1:1\").set_index('date')","a35536ff":"train_sales_cal_df.head(2)","b153fe5f":"#PLOT one by one to avoid memory overload\n\n#plotly_time_plot_rolling(train_sales_cal_df, 'Store')\nplotly_time_plot_rolling(train_sales_cal_df, 'Store', start_col=4, end_col=7)\n#plotly_time_plot_rolling(train_sales_cal_df, 'Store', start_col=7, end_col=10)","8f9025a1":"train_sales_cal_df = train_sales_df.groupby(['cat_id'])[d_cols].sum(axis=1).\\\n                                                T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\").set_index('date')","ac17a56d":"train_sales_cal_df = train_sales_cal_df.reset_index()\ntrain_sales_cal_df['date'] = pd.to_datetime(train_sales_cal_df['date'])\ntrain_sales_cal_df['month'] = train_sales_cal_df.date.dt.month\ntrain_sales_cal_df['day'] = train_sales_cal_df.date.dt.day\ntrain_sales_cal_df['year'] = train_sales_cal_df.date.dt.year\ntrain_sales_cal_df['weekday'] = train_sales_cal_df.date.dt.weekday\ntrain_sales_cal_df['weekday'] = train_sales_cal_df.date.dt.day_name()\ntrain_sales_cal_df.head(2)","ba5d1fb5":"df_food = train_sales_cal_df.groupby(['weekday', 'day'])['FOODS'].sum().reset_index(name='sales')\ndf_house = train_sales_cal_df.groupby(['weekday', 'day'])['HOUSEHOLD'].sum().reset_index(name='sales')\ndf_hob = train_sales_cal_df.groupby(['weekday', 'day'])['HOBBIES'].sum().reset_index(name='sales')\n","18085537":"def heatmap_plot_dates(df, title_text):\n\n    '''\n    returns the heatmap based plot for the df with time_series data in it\n    \n    inputs : df (dataframe with time based elements, such as weekdays, months etc.)\n             title_text (text of the plot to be shown)\n    returns : Heatmaps of Weekdays along number of days of a months\n    '''\n    fig = make_subplots(rows=1, cols=1, vertical_spacing = 0.15, \n                    subplot_titles=title_text,\n                    shared_yaxes = True\n                   )\n    fig.add_trace( go.Heatmap(\n                   z=[[df.loc[(df['day'] == day) & (df['weekday'] == weekday)]['sales'].sum()\n                     for day in range(1, 31+1)] for weekday in df.weekday.unique()],\n                   x=list(df.day.unique()),\n                   y=list(df.weekday.unique()),\n                   hoverongaps = False, \n                   colorbar = dict(title= title_text + '_Sales', \n                                   thickness=15)), row=1, col=1)\n    fig.update_layout(title = title_text + '_Sale Distribution', height=2 * 20 + 300, width=1 * 700, showlegend=False)\n    fig.show()","352474ea":"heatmap_plot_dates(df_food, 'FOODS')\nheatmap_plot_dates(df_hob, 'HOBBIES')\nheatmap_plot_dates(df_house, 'HOUSEHOLD')","431a9107":"train_sales_cal_df = train_sales_df.set_index('id')[d_cols].T.merge(calendar_df.set_index('d')['date'], \n                                                                    left_index=True, right_index=True,\n                                                                    validate=\"1:1\")\ntrain_sales_cal_df['avg_sales'] = train_sales_cal_df.mean(axis=1)\n","3ecd2a7f":"##train_sales_cal_df = train_sales_cal_df[['total_sales']]\n##train_sales_cal_df = train_sales_cal_df.reset_index()\n##train_sales_cal_df = train_sales_cal_df.set_index('date')\n\ntrain_sales_cal_df = train_sales_cal_df.reset_index()\ntrain_sales_cal_df.rename(columns={'index': 'daysnum'}, inplace=True)\ntrain_sales_cal_df = train_sales_cal_df.set_index('date').asfreq('d')\ntrain_sales_cal_df.head(2)","1c9b9672":"train_sales_cal_df.index = pd.to_datetime(train_sales_cal_df.index)\ntype(train_sales_cal_df.index)","5d7d2176":"def time_series_stationary_test(df):\n    \n    '''\n    returns time_series plots along with rolling mean plot, standard_dev calculated plot\n    and results of the Dickey_Fuller Test performed on time_series df\n    \n    inputs: df (time_series based df)\n    \n    returns : time_series plot, rolling_mean plot, std_plot and Dickey_Fuller Test Statistics\n              performed on time_series\n    '''\n    #df_filter = df.loc[df['id'] == series_ids][d_cols]\n    #df_filter = df.loc[df['id'] == series_ids]['sold']\n    #df_filter = pd.Series(df_filter.values.flatten())\n    \n    sns.set_style(\"whitegrid\")\n    fig, ax1 = plt.subplots(1, 1, figsize=(15, 5))\n    \n    rolling_mean = df.rolling(window=7).mean()\n    rolling_std =  df.rolling(window=7).std()\n    \n    #plotting the points\n    original_series = df.plot(figsize=(15, 5), alpha=1, \n                        lw=1.2, color=next(color_cycle), label='original_series', ax=ax1)\n    mean_series = rolling_mean.plot(figsize=(15, 5), alpha=1, \n                        lw=1.2, marker = 'o', color=next(color_cycle), label='rolling_mean', ax=ax1)\n    std_series = rolling_std.plot(figsize=(15, 5), alpha=1, \n                        lw=1.2, marker = '*' , color=next(color_cycle), label='rolling_std', ax=ax1)\n    \n    ax1.legend(['Original_Series', 'Mean_Series', \"Std_Series\"])\n    plt.show()\n    \n    #Performing a Dickey-Fuller Test\n    print(\"\\n============++++==========\\n\")\n    print(\"Results of the Dickey_Fuller Test\")\n    result = adfuller(df, autolag='AIC')\n    print('ADF Statistic: %f' % result[0])\n    print('No of lags: %f' %result[2])\n    print('No of Obs used for ADF Calculation & Critical Values: %f' %result[3])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    #output = pd.Series(result[0:4], index=['ADF Statistic','p-value','#Lags Used','Number of Observations Used'])\n    #for key,value in result[4].items():\n    #    output['Critical Value (%s)'%key] = value\n    #print (output)","1ca76aac":"time_series_stationary_test(train_sales_cal_df[['avg_sales']])","8c405352":"df_log_diff = np.log(train_sales_cal_df[['avg_sales']]) - np.log(train_sales_cal_df[['avg_sales']]).shift()\n#df['sold'] = df_log_diff\n#df['sold'] = df['sold'].dropna()\ndf_log_diff = df_log_diff.dropna()","ce800a35":"time_series_stationary_test(df_log_diff)","58ed7be9":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndef time_series_decompose(df, model_specified, period):\n    \n    '''\n    returns the trend, seasonality, and residual components decomposition of time_Series and\n    plots the results \n    \n    inputs: df(time_series df along with sales specified), model_specified (whether to use\n            additive or multiplicative), period (number of days where seasonality \n            is to be observed)\n    returns: decomposition plots along with decomposition components, in trend, seasonality\n    and residuals\n    '''\n\n    df_decomposition = seasonal_decompose(\n                               x=df, \n                               model= model_specified, \n                               period=period)\n    \n    est_trend = df_decomposition.trend\n    est_trend = est_trend.reset_index(name='trend').set_index('date')\n    est_seasonal = df_decomposition.seasonal\n    est_seasonal = est_seasonal.reset_index(name='seasonal').set_index('date')\n    est_residual = df_decomposition.resid\n    est_residual = est_residual.reset_index(name='resid').set_index('date')\n    \n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 24))\n\n    df.plot(figsize=(14, 8) ,\n                 alpha=0.8, \n                 lw=1.5, ax = ax1)\n    est_trend.plot(figsize=(14, 8) ,\n                 alpha=0.8, \n                 lw=1.5, ax = ax2)\n    est_seasonal.plot(figsize=(14, 8) ,\n                 alpha=0.8, \n                 lw=1.5, ax = ax3)\n    est_residual.plot(figsize=(14, 8) ,\n                 alpha=0.6, \n                 lw=3, style = 'o', ax = ax4)\n\n    fig.tight_layout()\n    plt.show()\n    \n    return est_trend, est_seasonal, est_residual\n\n#sample_time_s1 = train_sales_cal_df[400:700]\n#est_trend, est_seasonal, est_residual = time_series_decompose(sample_time_s1, 'multiplicative', 15)","78a9cc79":"trend_comp, seas_comp, residual_comp = time_series_decompose(train_sales_cal_df[['avg_sales']], 'multiplicative', 30)","2f252cca":"residual_comp = residual_comp.dropna()\ntime_series_stationary_test(residual_comp)","2743229d":"df_log_diff = np.log(train_sales_cal_df['avg_sales']) - np.log(train_sales_cal_df['avg_sales']).shift()\ndf_log_diff = df_log_diff.dropna()","c68257c9":"from statsmodels.tsa.stattools import acf, pacf\n\ndef auto_par_corr(ts_stat, nlags=25): \n\n    '''\n    returns the ACF and PACF plots of the time series\n    \n    inputs: ts_stat (time series along with data), nlags (optimal number of lags)\n    \n    returns: plot of ACF and PACF\n    '''\n    lag_acf = acf(ts_stat, nlags)\n    lag_pacf = pacf(ts_stat, nlags, method='ols')\n    \n    sns.set_style(\"whitegrid\")\n\n    \n    plt.figure(figsize=(15, 8))\n    plt.subplot(121)\n    #plt.plot(lag_acf)\n    plt.stem(lag_acf)\n    plt.axhline(y=0,linestyle='--',color='g')\n    plt.axhline(y=-1.96\/np.sqrt(len(ts_stat)), linestyle='--',color='r')\n    plt.axhline(y=1.96\/np.sqrt(len(ts_stat)), linestyle='--',color='r')\n    plt.title('Auto-Correlation')\n    \n    #lag_pacf.plot(figsize=(15, 5), alpha=1, \n    #                    lw=3, color=next(color_cycle), label='par_corr', ax=ax2)\n    plt.subplot(122)\n    #plt.plot(lag_pacf)\n    plt.stem(lag_pacf)\n    plt.axhline(y=0,linestyle='--',color='g')\n    plt.axhline(y=-1.96\/np.sqrt(len(ts_stat)), linestyle='--',color='r')\n    plt.axhline(y=1.96\/np.sqrt(len(ts_stat)), linestyle='--',color='r')\n    plt.title('Partial Auto-Correlation')\n    \n    plt.legend()\n    plt.show()\n    ","5558928a":"auto_par_corr(df_log_diff, nlags=25)","9d1a72f9":"#train_sales_cal_df.index = pd.DatetimeIndex(train_sales_cal_df.index.values,\n#                               freq=train_sales_cal_df.index.inferred_freq)","cc0e0e39":"#Let us split the time series into training and testing sets\n#perc_training = 0.7\n#split_p = round(len(train_sales_cal_df['avg_sales']) * perc_training)\n#training_p, testing_p = train_sales_cal_df['avg_sales'][0:split_p], train_sales_cal_df['avg_sales'][split_p:]\n\ntraining_p = train_sales_cal_df.loc[:'2015-10-14', ['avg_sales']]\ntesting_p = train_sales_cal_df.loc['2015-10-15':, ['avg_sales']]\ntraining_p.head(2)","6b1819ae":"#for prophet model:\ndf_event_1 = pd.DataFrame({'holiday': 'Event 1', 'ds': calendar_df[~calendar_df['event_name_1'].isna()]['date']})\ndf_event_2 = pd.DataFrame({'holiday': 'Event 2', 'ds': calendar_df[~calendar_df['event_name_2'].isna()]['date']})\ndf_snapca = pd.DataFrame({'holiday': 'snap_CA', 'ds': calendar_df[calendar_df['snap_CA'] == 1]['date']})\ndf_snaptx = pd.DataFrame({'holiday': 'snap_TX', 'ds': calendar_df[calendar_df['snap_TX'] == 1]['date']})\ndf_snapwi = pd.DataFrame({'holiday': 'snap_WI', 'ds': calendar_df[calendar_df['snap_WI'] == 1]['date']})\nholidays = pd.concat((df_event_1, df_event_2, df_snapca, df_snaptx, df_snapwi))\nholidays.head(3)","e81d5489":"#ever since spliting exo, it is to be added additionally\nholidays['ds'] = pd.to_datetime(holidays['ds'])","2c40a006":"# for SARIMAX Model:\n\ndata = {'date':train_sales_cal_df.index,\n        'holidays': 0 }\n\nexo = pd.DataFrame(data)\n\nfor idx, date1 in enumerate(exo['date']):\n    for date2 in holidays['ds']:\n        if date2 == date1:\n            exo['holidays'].iloc[idx] = 1\n        else:\n            continue       ","79d56760":"exo = exo.set_index(['date']).asfreq('D')\nexo.head(4)","7c489d14":"exo_train = exo.loc[:'2015-10-14']\nexo_test = exo.loc['2015-10-15':]","be00114a":"#Conversion into Series according to SARiMAX req:\n#conversion into timeseries if required, for now there is neither benefit nor requirement\n#exo_train = pd.Series(exo_train.iloc[:, 0])\n#exo_test = pd.Series(exo_test.iloc[:, 0])","0800b48d":"def SARIMAX_tune(ts, total_params):\n\n    '''\n    returns the optimal tuned results using AIC as a prediction accuracy metric\n    \n    inputs : ts (time series), total_params (SARIMAX models parameters to be specified)\n    \n    returns: results of SARIMAX tuned model\n    '''\n    results = []\n\n    for param in total_params:\n        try:\n            model = SARIMAX(ts, exog = exo, order=param[0], seasonal_order=param[1], \n                           initialization='approximate_diffuse')\n            res = model.fit(method='powell')\n            results.append((res, res.aic, res.params))\n            print(\"The results of SARIMAX{}x{} in AIC are: {}\".format(param[0], param[1], res.aic))\n        except Exception as e:\n            print(e)\n            continue\n    \n    return results","0cc2ecb6":"###########New Code#############\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\nfrom sklearn.model_selection import GridSearchCV\n\ndef SARIMAX_tune_with_RMSE(ts, total_params):\n    \n    '''\n    returns the optimal tuned results using RMSE as a prediction error metric\n    \n    inputs : ts (time series), total_params (SARIMAX models parameters to be specified)\n    \n    returns: results of SARIMAX tuned model with RMSE as the error metric\n    '''\n    tscv = TimeSeriesSplit(n_splits = 5)\n    rmse_array = []\n    timeseries = ts #train_sales_cal_df[['avg_sales']]\n    results = []\n    \n    for train_index, test_index in tscv.split(timeseries):\n        cv_train, cv_test = timeseries.iloc[train_index], timeseries.iloc[test_index]\n        \n        for param in total_params:\n            try:\n                model = SARIMAX(cv_train, order=param[0], seasonal_order=param[1], \n                               initialization='approximate_diffuse')\n                res = model.fit(method=\"powell\")\n                results.append((res, res.params))\n                #use this statement for further troubleshooting on mle #print(\"MLE retvals for this iter are: {}\".format(res.mle_retvals))\n                #print(\"The results of SARIMAX{}x{} in are: {}\".format(param[0], param[1]))\n            except Exception as e:\n                print(e)\n                continue\n\n            sarimax = model.fit(disp=0)\n            predictions = sarimax.predict(cv_test.index.values[0], cv_test.index.values[-1])\n            true_values = cv_test.values\n            #true_values = cv_test\n            #wrmse_array.append(wrmse(true_values, predictions))\n            #rmse_array.append(np.sqrt(mean_squared_error(true_values, predictions)))\n            print(\"Results of SARIMAX in {} x {} in RMSE: {}\".format(param[0], param[1], \n                                                                 np.sqrt(mean_squared_error(true_values, predictions))))\n        \n        return results","5c0c2eb8":"p, d, q =range(1,3), [1], range(1,3)\nP, D, Q, S =range(0,3), [1], range(1,3), [7]\npdq = list(itertools.product(p,d,q))\nseasonal_pdq = list(itertools.product(P,D,Q,S))\ntotal_params = list(itertools.product(pdq, seasonal_pdq))\n\n#NOTE: memory intensive operation if range set at 0,3, readings have already been taken by running it once:\n#all_results = SARIMAX_tune(train_sales_cal_df['avg_sales'], total_params)\n#all_results = SARIMAX_tune_with_RMSE(train_sales_cal_df[['avg_sales']], total_params)\n\n#More information regarding fixng the error nle_retvals can be found at:\n#https:\/\/stats.stackexchange.com\/questions\/313426\/mle-convergence-errors-with-statespace-sarimax","16fa0c02":"#all_results = SARIMAX_tune_with_RMSE(train_sales_cal_df[['avg_sales']], total_params)","052bdb52":"#######NOTE: THIS WAS THE MISTAKE THAT WAS STOPPING TIMESERIES GRAPHS FROM RENDERGIN#####\n#train_sales_cal_df.index = pd.to_datetime(train_sales_cal_df.index)\n#type(train_sales_cal_df.index)","655bb039":"#Fitting in the SARIMAX Model\n#include the exo_train in here , exo_test in prediction (model_fit_SARIMAX) to check results: for now, abandoning their use because\n# of the error in package, since even converting exo_train, exo_test into time series\/using df results\n# in errors. \n#model values earlier 1,1,1, 0,1,1,7 \n#values with RMSE - order=(2,1,1), seasonal_order=(2,1,1,7), \nmodel = SARIMAX(training_p,\n                order=(2,1,1), seasonal_order=(2,1,1,7), \n                enforce_stationarity=False, enforce_invertibility=False)\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\nmodel_fit.plot_diagnostics(figsize=(15,7))\nplt.show()\n#information on whether particular model is a goodfit\n#https:\/\/analyticsindiamag.com\/complete-guide-to-sarimax-in-python-for-time-series-modeling\/","c886a2ad":"def model_fit_SARIMAX(model_set, model_fit_srt_tm='2015-10-15', ts_st_tm='2013'):\n    \n    '''\n    makes predictions using SARIMAX model tuned earlier and returns plots along with \n    predictions made\n    \n    inputs: model_set (model tuned), model_fit_srt_tm (model prediction starting time), \n            ts_st_tm (time series plot to be plotted beginning point)\n    returns: forecast plot along with time_series, and pred (predictions made series)\n    \n    '''\n    \n    #1.# pred = model_fit.forecast(len(testing_p), alpha=0.05, dynamic=False)  # 95% conf\n    pred = model_set.get_prediction(start=pd.to_datetime(model_fit_srt_tm), end='2016-06-01', dynamic=False)\n    pred_conf = pred.conf_int()\n    #for dynamic prediction of time_series:\n    #pred_dy = model_set.get_prediction(dynamic=pred_srt_tm)\n    #pred_dy_conf = pred_dy.conf_int()\n\n    # Make as pandas series\n    #pred_series = pd.Series(pred, index=testing_p.index)\n\n    sns.set_style('whitegrid')\n    fig, ax1 = plt.subplots(1, 1, figsize=(14, 4))\n\n    train_sales_cal_df['avg_sales'][ts_st_tm:].plot(label='original',\n                                                  alpha=0.7, \n                                                  lw=1.5 , ax=ax1)\n    pred.predicted_mean.plot(label='predicted_validation',\n                             alpha=0.9, color='red',\n                             lw=2, ax=ax1)\n    ax1.fill_between(pred_conf.index, \n                     pred_conf.iloc[:,0], \n                     pred_conf.iloc[:, 1], color='purple', alpha=0.2)\n    fig.tight_layout()\n    ax1.set_xlabel('Date')\n    ax1.set_ylabel('Mean Number of Sales')\n    plt.legend(loc='upper left')\n    plt.show()\n    \n    return pred\n\npred_made = model_fit_SARIMAX(model_fit, ts_st_tm='2015')    ","8aa7272a":"#def forecast_error(prediction, original, date, col):\n#    \n#    '''\n#    returns the Mean Absolute Error (MAE) and root mean square error (RMSE)\n#    \n#    inputs: prediction (forecasted series, with dates as predicted above), original (orginal time series df), \n#            date (where the model was started for forecast), col(column with time_series_values)\n#    \n#    returns : MAE, and RMSE \n#    '''\n#    forecasted = prediction.predicted_mean\n#    original = original.loc[date:, col]\n#\n#    MAE = np.abs(forecasted - original).mean()\n#    RMSE = np.sqrt(((forecasted - original)**2).mean())\n#\n#    print(\"The mean absolute error is : {}\".format(MAE))\n#    print(\"The root mean square error is : {}\".format(RMSE))\n#\n#\n#forecast_error(pred_made, testing_p, '2015-10-15', 'avg_sales')    ","962e5534":"#fitting the model once again now to all the timeseries data (train_sales_cal_df['avg_sales']) available:\n#old_values - 2,1,1 & 0,1,2,7\nmodel = SARIMAX(train_sales_cal_df['avg_sales'],\n                order=(2,1,1), seasonal_order=(2,1,1,7), \n                enforce_stationarity=False, enforce_invertibility=False)\nmodel_fit = model.fit(disp=0)","737b83be":"#Making Future Prediction:\n#note: exo_sub = exo.loc['2016-01-25':], if it is needed to provide the holidays , equate exo_sub to exog in get forecast\n\n#pred_future = model_fit.get_forecast(steps=90) #Making Prediction for next 3 months\n\npred_future = model_fit.get_prediction(start='2016-04-24', end='2016-06-24', dynamic=False)\npred_f_conf = pred_future.conf_int()\n\nfig, ax2 = plt.subplots(figsize=(16, 4))\n\n\n#now using the entire original series, instead of training or testing:\n\ntrain_sales_cal_df['avg_sales']['2015-06-15':].plot(label='original',\n                 alpha=0.7, \n                 lw=1.5 , color=next(color_cycle), ax=ax2)\n\npred_future.predicted_mean.plot(label='one_step_ahead',\n                 alpha=0.9, \n                 lw=2, color=next(color_cycle), ax=ax2)\n\nci = pred_f_conf.loc['2016-04-24':]\n\nax2.fill_between(ci.index, \n                ci.iloc[:,0], \n                ci.iloc[:, 1], color=next(color_cycle), alpha=0.3)\n\nplt.legend(loc='upper left')\nplt.show()","d2626965":"# Memory reduction helper function:\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns: #columns\n        col_type = df[col].dtypes\n        if col_type in numerics: #numerics\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ncalendar_df = reduce_mem_usage(calendar_df)\n\nsell_prices_df = reduce_mem_usage(sell_prices_df)","d71e2997":"from scipy.sparse import csr_matrix\nimport gc\n\nNUM_ITEMS = train_sales_df.shape[0]  # 30490\nDAYS_PRED = submission_file.shape[1] - 1  # 28\n\n# Dataframe with only last 28 days:\nsales = train_sales_df\ncols = [\"d_{}\".format(i) for i in range(1914-28, 1914)]\ndata = sales[[\"id\", 'store_id', 'item_id'] + cols]\n\n# To long form:\ndata = data.melt(id_vars=[\"id\", 'store_id', 'item_id'], \n                 var_name=\"d\", value_name=\"sale\")\n\n# Add week of year column from 'calendar':\ndata = pd.merge(data, calendar_df, how = 'left', \n                left_on = ['d'], right_on = ['d'])\n\ndata = data[[\"id\", 'store_id', 'item_id', \"sale\", \"d\", \"wm_yr_wk\"]]\n\n# Add weekly price from 'sell_prices':\ndata = data.merge(sell_prices_df, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\ndata.drop(columns = ['wm_yr_wk'], inplace=True)\n\n# Calculate daily sales in USD:\ndata['sale_usd'] = data['sale'] * data['sell_price']\ndata.head()\n\n#this part is correct","52533d24":"# List of categories combinations for aggregations as defined in docs:\ndummies_list = [sales.state_id, sales.store_id, \n                sales.cat_id, sales.dept_id, \n                sales.state_id +'_'+ sales.cat_id, sales.state_id +'_'+ sales.dept_id,\n                sales.store_id +'_'+ sales.cat_id, sales.store_id +'_'+ sales.dept_id, \n                sales.item_id, sales.state_id +'_'+ sales.item_id, sales.id]\n\n\n## First element Level_0 aggregation 'all_sales':\ndummies_df_list =[pd.DataFrame(np.ones(sales.shape[0]).astype(np.int8), \n                               index=sales.index, columns=['all']).T]\n\n# List of dummy dataframes:\nfor i, cats in enumerate(dummies_list):\n    dummies_df_list +=[pd.get_dummies(cats, drop_first=False, dtype=np.int8).T]\n    \n# Concat dummy dataframes in one go:\n## Level is constructed for free.\nroll_mat_df = pd.concat(dummies_df_list, keys=list(range(12)), \n                        names=['level','id'])#.astype(np.int8, copy=False)\n\n# Save values as sparse matrix & save index for future reference:\nroll_index = roll_mat_df.index\nroll_mat_csr = csr_matrix(roll_mat_df.values)\nroll_mat_csr.shape","efd0233d":"roll_mat_df.to_pickle('roll_mat_df.pkl')\ndel dummies_df_list, roll_mat_df\ngc.collect()","08ce0f63":"# Fucntion to calculate S weights:\ndef get_s(drop_days=0):\n    \n    \"\"\"\n    drop_days: int, equals 0 by default, so S is calculated on all data.\n               If equals 28, last 28 days won't be used in calculating S.\n    \"\"\"\n    # Rollup sales:\n    d_name = ['d_' + str(i+1) for i in range(1913-drop_days)]\n    sales_train_val = roll_mat_csr * sales[d_name].values\n\n    no_sales = np.cumsum(sales_train_val, axis=1) == 0\n    sales_train_val = np.where(no_sales, np.nan, sales_train_val)\n\n    # Denominator of RMSSE \/ RMSSE\n    weight1 = np.nanmean(np.diff(sales_train_val,axis=1)**2,axis=1)\n    \n    return weight1","0b332820":"S = get_s(drop_days=0)\nS.shape","12378d86":"# Functinon to calculate weights:\ndef get_w(sale_usd):\n    \"\"\"\n    \"\"\"\n    # Calculate the total sales in USD for each item id:\n    total_sales_usd = sale_usd.groupby(\n        ['id'], sort=False)['sale_usd'].apply(np.sum).values\n    \n    # Roll up total sales by ids to higher levels:\n    weight2 = roll_mat_csr * total_sales_usd\n    \n    return 12*weight2\/np.sum(weight2)","696b454d":"W = get_w(data[['id','sale_usd']])\nW.shape","0bff8619":"# Predicted weights\n##W_df = pd.DataFrame(W,index = roll_index,columns=['w'])\n\n# Load the original weights:\n##data_pass = '\/kaggle\/input\/original-weights\/'\n##W_original_df = pd.read_csv(data_pass+'weights_validation.csv')\n\n# Set new index, calculate difference between original and predicted:\n##W_original_df = W_original_df.set_index(W_df.index)\n##W_original_df['Predicted'] = W_df.w\n##W_original_df['diff'] = W_original_df.Weight - W_original_df.Predicted\n\n# See where we are off by more than e-6\n##m = W_original_df.Weight.values - W_df.w.values > 0.000001\n##W_original_df[m]","13cf02c2":"SW = W\/np.sqrt(S)\nsw_df = pd.DataFrame(np.stack((S, W, SW), axis=-1),index = roll_index,columns=['s','w','sw'])\nsw_df.to_pickle('sw_df.pkl')","94070706":"# Function to do quick rollups:\ndef rollup(v):\n    '''\n    v - np.array of size (30490 rows, n day columns)\n    v_rolledup - array of size (n, 42840)\n    '''\n    return roll_mat_csr*v #(v.T*roll_mat_csr.T).T\n\n\n# Function to calculate WRMSSE:\ndef wrmsse(preds, y_true, score_only=False, s = S, w = W, sw=SW):\n    '''\n    preds - Predictions: pd.DataFrame of size (30490 rows, N day columns)\n    y_true - True values: pd.DataFrame of size (30490 rows, N day columns)\n    sequence_length - np.array of size (42840,)\n    sales_weight - sales weights based on last 28 days: np.array (42840,)\n    '''\n    \n    if score_only:\n        return np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(rollup(preds.values-y_true.values))\n                            ,axis=1)) * sw)\/12 #<-used to be mistake here\n    else: \n        score_matrix = (np.square(rollup(preds.values-y_true.values)) * np.square(w)[:, None])\/ s[:, None]\n        score = np.sum(np.sqrt(np.mean(score_matrix,axis=1)))\/12 #<-used to be mistake here\n        return score, score_matrix","b17394b0":"#dates_s = [pd.to_datetime(calendar_df.loc[calendar_df['d'] == str_date,'date'].values[0]) for str_date in d_cols]\n","4c9aa288":"#df_sale_group_item = train_sales_df[np.hstack([['dept_id','store_id'],d_cols])].groupby(['dept_id','store_id']).sum()\n#df_sale_group_item = df_sale_group_item.reset_index()\n\n#item_series =  df_sale_group_item[(df_sale_group_item.dept_id == 'FOODS_1') & (df_sale_group_item.store_id == 'TX_1')]\n#dates = pd.DataFrame({'ds': dates_s}, index=range(len(dates_s)))\n#dates['y'] = item_series[d_cols].values[0].transpose() ","e82cc0a8":"#prophet forecast from here\nfrom fbprophet import Prophet\nfrom tqdm.notebook import tqdm as tqdm\n\n####Observing behavior of Avg Number of Sales using Prophet#######\n####Converting the data into required ds and y format for Prophet Model#####\n\nts = train_sales_cal_df[['avg_sales']].reset_index()\nts = ts.set_axis(['ds', 'y'], axis=1, inplace=False)\n\n#####Specifying data for only one year to better visulize future predictions#######\nthreshold_date = pd.to_datetime('2015-04-22') \nselect_d = ts ['ds'] > threshold_date\n\nts = ts[select_d][['ds', 'y']]\n\n#####prophet_plot(ts)######\n\nmod_params = { 'weekly_seasonality': True, 'seasonality_mode': 'multiplicative'} \nsales_model = Prophet(**mod_params, interval_width=0.95)\n#data['cap'] = data['y'].max() + data['y'].std() * 0.05 #incase logistic model growth used\n\n######fitting the prophet model########\n\nsales_model.fit(ts)\nfuture = sales_model.make_future_dataframe(28, 'D')\n#future['cap'] = data['cap'].max() #incase logistic model growth used in params\nsales_forecast = sales_model.predict(future)\n\n########Train\/Test split of Data#######################\n\nthreshold_date = pd.to_datetime('2016-03-01') #for the test\/validation split\nforecast_date = pd.to_datetime('2016-04-24') #for the actual forecast date to begin\nselect_d = ts ['ds'] < threshold_date\n\nts_train = ts[select_d][['ds', 'y']]\nts_test = ts[~ select_d][['ds', 'y']]\n\nselect_d2 = sales_forecast['ds'] < forecast_date\n\n\nforecast_train = sales_forecast[select_d2]\nforecast_test = sales_forecast[~ select_d2]\n","0ae9e648":"#Seaborn based plot for the fb_Prophet Model\nsns.set_style('whitegrid')\nsns_c = sns.color_palette(palette='deep')\n\nfig, ax = plt.subplots(figsize=(15,5))\n\nax.fill_between(\n    x=sales_forecast['ds'], y1=sales_forecast['yhat_lower'], y2=sales_forecast['yhat_upper'],\n    color=sns_c[2],  alpha=0.25, label=r'95% confidence_interval'\n)\n\nsns.lineplot(x='ds', y='y', label='y_train', data=ts_train, ax=ax)\nsns.lineplot(x='ds', y='y', label='y_test', data=ts_test, ax=ax)\n#sns.lineplot(x='ds', y='yhat', label='y_hat', data=sales_forecast, ax=ax)\nsns.lineplot(x='ds', y='yhat', label='y_hat', data=forecast_test, ax=ax)\nax.axvline(forecast_date, color='g', linestyle='--', label='train test split')\n\nax.legend(loc='lower left')\nax.set(title='Avg Sales of Units', ylabel='');","5ea21eac":"#selecting the product ids for which we require prophet prediction\n\nid1 = 'FOODS_3_443_CA_1_validation'\nid2 =  'HOUSEHOLD_1_005_CA_3_validation'\nid3 =  'FOODS_3_823_WI_3_validation'\nid4 = 'FOODS_3_090_CA_3_validation'\nid5 = 'FOODS_3_586_TX_2_validation'\nid6 = 'FOODS_3_586_TX_3_validation'\n\n#train_sales_cal_df[id2]\n","e4c3b55b":"def make_id_timeseries(idx, date='2016-02-22'):\n    \n    '''\n    selects and makes the timeseries adjusted for fb prophet model by adjusting names\n    of columns\n    \n    inputs: product_id\n    \n    returns: time series with data specified in format for prophet\n    \n    '''\n    \n    dates_series = [date for date in train_sales_cal_df.index]\n    id_series =  train_sales_df[(train_sales_df.id == idx )]\n    item_ts = pd.DataFrame({'ds': dates_series}, index=range(len(dates_series)))\n    item_ts['y'] = id_series[d_cols].values[0].transpose()\n    \n    ######Following steps would shorten the behavior of historic timeline data########\n    ######Choosing the threshold date near to the date where prediction needs to be made \n    ######because of close correlation in time series\n    \n    threshold_date = pd.to_datetime(date) \n    mask = item_ts['ds'] > threshold_date\n\n    item_ts = item_ts[mask][['ds', 'y']]\n    \n    return item_ts\n\ndata1 = make_id_timeseries(id6)\ndata4 = make_id_timeseries(id4)","f3db849d":"#weighted_coeff = (df_cv['yhat'] - df_cv['y'])\/(df_cv['yhat'] - df_cv['y']).mean()\n#e = ((df_cv['yhat'] - df_cv['y']) * weighted_coeff )\/ (weighted_coeff*df_cv['y'])\n#resulting_wmape = pd.DataFrame(df_cv['yhat'], df_cv['y'], weighted_coeff, e)\n##print(\"yhat:{}, actual:{}, coeff:{}, error:{}\".format(df_cv['yhat'], df_cv['y'], weighted_coeff, e))\n#data = {'yhat':list(df_cv['yhat']),\n#        'actual':list(df_cv['y']),\n#        'diff' : list(df_cv['yhat'] - df_cv['y']),\n#       'weighted_coef': list(weighted_coeff), \n#       'error': list(e)}\n#  \n# Create DataFrame\n#pd.DataFrame(data)","74aef0c4":"##########New Code Working For Custom Error###########\nimport itertools\nfrom fbprophet.diagnostics import cross_validation\nfrom fbprophet.diagnostics import performance_metrics\n\n#Note : the code was generated using the explanation at the following link\n#https:\/\/www.baeldung.com\/cs\/mape-vs-wape-vs-wmape\n\ndef fbprophet_tuning_per_single_id(id_data, changepoint_prior_list=[0.5, 0.6, 0.7], \n                                   changepoint_range_list=[0.6, 0.7, 0.8], \n                                   n_changepoints_list=[50, 60]):\n    param_grid = {  \n        'changepoint_prior_scale': changepoint_prior_list,\n        'changepoint_range': changepoint_range_list,\n        'n_changepoints' : n_changepoints_list\n    }\n\n    # Generate all combinations of parameters\n    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n    #wapes = []  # Store the RMSEs for each params here\n    wmapes = []\n\n    # Use cross validation to evaluate all parameters\n    for params in all_params:\n        mod = Prophet(**params).fit(id_data)  # Fit model with given params\n        df_cv = cross_validation(mod, initial = '32 days', period = '3 days', horizon='28 days')\n        #df_p = performance_metrics(df_cv, rolling_window=1)\n        #rmses.append(df_p['rmse'].values[0])\n        #wapes.append(np.sum(df_cv['yhat'] - df_cv['y']) \/ (np.sum(df_cv['y'])))\n        weighted_coeff = (df_cv['yhat'] - df_cv['y'])\/(df_cv['yhat'] - df_cv['y']).mean()\n        wmapes.append(np.sum((df_cv['yhat'] - df_cv['y']) * weighted_coeff )\/ np.sum(weighted_coeff*df_cv['y']) )\n\n    # Find the best parameters\n    tuning_results = pd.DataFrame(all_params)\n    tuning_results['wmape'] = wmapes\n    print(tuning_results)\n    \nfbprophet_tuning_per_single_id(data1)    ","991f453f":"def prophet_plot(data, periods=28, freq='D'):\n    \n    '''\n    plots the fb prophet model for the data of id specified\n    \n    inputs: data (product id along with data for previous days), periods(predictions for \n    the next days to be made)\n    \n    returns: plots the immediate historic data along with forecast for the future made\n    '''\n    \n    mod_params = { 'weekly_seasonality': True, 'seasonality_mode': 'multiplicative'} \n\n    sales_model = Prophet(**mod_params, holidays=holidays, interval_width=0.95)\n    #data['cap'] = data['y'].max() + data['y'].std() * 0.05 #incase logistic model growth used\n    #fitting the model\n    sales_model.fit(data)\n    future = sales_model.make_future_dataframe(periods, freq)\n    #future['cap'] = data['cap'].max() #incase logistic model growth used in params\n\n    sales_forecast = sales_model.predict(future)\n    \n    plt.figure(figsize=(15,3))\n    #fig, ax = plt.subplots(figsize=(14,5))\n    sales_model.plot(sales_forecast, xlabel='Date', ylabel='Unit Sales Behavior')\n    #plt.axvline(x=sales_forecast[sales_forecast.ds == '2016-04-22']['ds'], color='r')\n    #ax.axvline(threshold_date, color='g', linestyle='--', label='Prediction Onwards')\n    #ax.legend(loc='upper left')\n    #ax.set(title='Avg Sales of Units', ylabel='');\n    plt.title(\"Unit Sales Trend on Each ID\")\n    plt.show()\n\n\nprophet_plot(data1)\nprophet_plot(data4)    ","f2b485a6":"all_ids = train_sales_cal_df.columns.drop(['daysnum', 'avg_sales'])\nsales_forecast_list = []\n\ndef prophet_pred_df(start=15500, end=15650):\n    '''\n    makes a dataframe with prophet predictions made for each product_id \n    \n    input: start = product_id start, end = product_id to be the last one for prediction\n    \n    returns: dataframe with predictions made for the next 28 days using prophet of the product\n             ids specified in input\n    '''\n\n    for idc in tqdm(range(start, end)):\n        train_sub = train_sales_cal_df.iloc[-28:, idc].reset_index()\n        idx = train_sub.columns[1]\n        train_sub.set_axis(['ds', 'y'], axis=1, inplace=True)\n        mod_params = { 'changepoint_prior_scale': 0.5 , 'changepoint_range': 0.6,\n                      'n_changepoints': 50, \n                      'weekly_seasonality': True, 'seasonality_mode': 'multiplicative'} \n        sales_model = Prophet(**mod_params, interval_width=0.95)\n        #data['cap'] = data['y'].max() + data['y'].std() * 0.05 #incase logistic model growth used\n\n        ######fitting the prophet model########\n        \n        sales_model.fit(train_sub)\n        future = sales_model.make_future_dataframe(28, 'D')\n        #future['cap'] = data['cap'].max() #incase logistic model growth used in params\n        sales_forecast = sales_model.predict(future)[-28:]\n        sales_forecast_list.append( (np.append(np.array([idx]), sales_forecast['yhat'].values.transpose())) )\n    \n    return sales_forecast_list\n\n#list_table would collect all entries\n\nlist_table = prophet_pred_df()","6de52b9d":"#changing column names to those as specified in submission file\ndf_prophet_forecast = pd.DataFrame(list_table)\ndf_prophet_forecast.columns = submission_file.columns\n#df_prophet_forecast = df_prophet_forecast.round(decimals = 2)\ndf_prophet_forecast.head(6)","49374c6b":"########Remove Unwanted Columns###################\ntrain_sales_df = train_sales_df.drop(['total_sales_all_days', 'avg_sales_all_days', 'median_sales_all_days'], axis=1)\n\n###########Making the dataframe of predictions for all ids############\nreq_sales_df = train_sales_df.iloc[:, np.r_[0,-28:0]].melt('id', var_name='d', value_name='sale')\nreq_sales_df = req_sales_df.merge(calendar_df.loc[:,['d','date','wday']])\n#req_sales_df\nlastest_date = int(req_sales_df.d.max()[2:])\nreq_sales_df = req_sales_df.groupby(['id','wday'])['sale'].mean()\nreq_sales_df.head(7)","84ed8152":"###########Generating Submission Dataframe of ids for 28 next days with 0 values############\n\nmv_avg_forecast = submission_file.copy()\nmv_avg_forecast.columns = ['id'] + ['d_' + str(lastest_date + x) for x in range(1, 29)]\nmv_avg_forecast = mv_avg_forecast.loc[mv_avg_forecast.id.str.contains('validation')]\n#mv_avg_forecast","82243f37":"###########Generating Dataframe for Moving Average Values###############\n\nmv_avg_forecast = mv_avg_forecast.melt('id', var_name='d', value_name='sale')\nmv_avg_forecast = mv_avg_forecast.drop('sale',axis = 1)\n#mv_avg_forecast\n\nmv_avg_forecast = mv_avg_forecast.merge(calendar_df.loc[:,['d','date','wday']])\nmv_avg_forecast = mv_avg_forecast.join(req_sales_df, on=['id', 'wday'])\n#mv_avg_forecast\n\nmv_avg_forecast = mv_avg_forecast.pivot(index='id', columns='d', values='sale')\nmv_avg_forecast = mv_avg_forecast.reset_index()\nmv_avg_forecast","bcd72158":"#del mv_avg_forecast.index.name\nmv_avg_forecast.rename_axis(None).index.name\nmv_avg_forecast.head(4)","5c80bd62":"#df_sub_valid = pd.concat([mv_avg_forecast, df_prophet_forecast_1, \n#                        df_prophet_forecast_2,df_prophet_forecast_3, \n#                        df_prophet_forecast_4, df_prophet_forecast_5, \n#                        df_prophet_forecast_6], sort=False)\n\n#re-adjusting the old columns against new-ones\n\nmv_avg_forecast.columns = submission_file.columns\n\ndf_valid = pd.concat([mv_avg_forecast[~mv_avg_forecast.id.isin(df_prophet_forecast.id)], df_prophet_forecast], sort=False)\n\ndf_eval = df_valid.copy()\ndf_eval['id'] = df_eval['id'].str.replace(\"validation\", \"evaluation\")\n\nsubmission_df = pd.concat([df_valid, df_eval], sort=False)\nsubmission_df = submission_df.sort_values('id')\n","03fd7a3f":"submission_df.columns","5f149f4c":"#submission_df.columns = ['id'] + ['F' + str(x) for x in range(1, 29)]\n#submission_df","1b552896":"#DAYS_PRED = 28\ndayCols = [\"d_{}\".format(i) for i in range(1914-DAYS_PRED, 1914)]\ny_true = train_sales_df[dayCols]","73328c1e":"df_valid_w = df_valid.drop('id', 1)\nfor col in df_valid_w.columns:\n    df_valid_w[col] = pd.to_numeric(df_valid_w[col], errors='coerce')#.convert_dtypes() cp95 solution recommends using convertdtypes if nan present","2fa0de81":"wrmsse(y_true, df_valid_w, score_only=True)","dfc37b9c":"# Fix negative forecast\nnum = submission_df._get_numeric_data()\nnum[num < 0] = 0\n# Prepare the submission file\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(f'Submission shape: {submission_df.shape}')","7eacd4e5":"#kaggle competitions submit -c m5-forecasting-accuracy -f submission.csv -m \"from 450 to 600\"","07bfc9a4":"### Final Submission File Preparation","fdac619e":"Looking at the above graphs, our regression model does a fairly good job of fitting the line on the sales trend observed between the days 500th to 900th, for the item ids 'FOODS_3_090_CA_3_validation' and 'FOODS_3_586_TX_2_validation'. The graph also points out to the similar trends of troughs and crests between the specific days pointing out towards the occurence of special occasions and events that are driving sales. (To do _explain on order of polynomial and take more cases of ids across foods, household categories across different stores)","9825d582":"### Prophet Forecasting On Average Sales Using Seaborn","4b53e8ba":"#### Removing Outliers to Observe Price Distribution? \n#### **Quartile Method**","94684ded":"#### Stationary Time Series\n\nBefore we could make the time series stationary, we are going to need some form of a method where we could confirm that the time series extracted is indeed stationary. There are a few statistics test as well as we can plot the time series mean and variance as a function of time to observe that the series is indeed stationary. \n\nOne of the time series stationary test is called the Dickey-Fuller Test. This test provides us with a test statistic as well as confidence intervals for the critical values. The objective is to determine whether the test statistic is less than the critical value, if it is we can **reject** the null hypothesis. The assumption of null and alternative hypotheses in this test is as follows:\n\n* Null Hypothesis: The time series has some time component, and has a unit root suggesting a highly dependent time orientation indicating the presence of non-stationarity.\n* Alternative Hypothesis: The time series does not have a unit root, indicating that it does NOT have a time dependent structure and consequently the time series is stationary.","6382d579":"### Questions on Distribution of Product & Behavior Across Timeline\n\nStarting with the dataframe denoted by train_sales_df that has the item specific ('id'), locale specific ('store_id' , 'state_id') and sales days specific (d_1 to d_1913) information; let us first make necessary adjustments to separate the sales days so that analysis along item_id, store_id and dept_id can be more easily observed across sales days only. ","b0670818":"#### **Decomposition**\nDecomposition is another way to make the time series stationary. The decomposition of dataset yields three properties for the time series dataset\n\n* Trend - That is whether there is an increase or decrease in total sales over long term\n* Seasonality - Whether there are repeating patterns within the dataset\n* Residual - What is the contribution of behavior in the time series dataset (total sales ) without trend and seasonality added","12b425d8":"### Validation SARIMAX","db4cf60d":"The above plots confirm the inferences of the following information:\n- The first left plot shows that residual errors fluctuate around a mean of zero and have uniform variance\n- The first right plot indicates that density plot also has a mean of almost zero\n- The second left plot indicate an almost a perfect fit on red line pointing to the fact that distribution is not skewed\n- The low auto-correlation of residuals is indicated in the second right plot. ","ad4b5110":"#### Dataset Description:\n\nThe description of the data files that are imported into this notebook is as follows:\n\n* `calendar_df.csv` - Contains information about the dates on which the products are sold and also contains information about the holidays and special occasions.\n* `train_sales_df.csv` - Contains the historical daily unit sales data per product and store and department id with almost a data of sales for 1900 days [d_1 - d_1913] \n* `submission_file.csv` - The correct format for submission file, containing the product ids and the column ids for the next 28 days sales data forecast. \n* `sell_prices_df.csv` - Contains information about the price of the products sold per store and date.\n* `sales_train_evaluation.csv` - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n","d498a075":"### Distribution of Total Sales Against Each Category?","ba62c086":"### Understanding the DataSet:","38f670de":"A couple of things of note here are :\n\n* In the first plot of autocorreation, the first significant value occurs at between 1 and 2, so therefore we are going to select q=1, while in the second plot of PACF, the significant value occurs at 2 so p=2\n* In the ACF plot, we have regular peaks at interval of 7 so we are going to select the value of parameter m in seasonal order as 7, and since this lag is positive therefore the value for P=1, and for Q=0.\n* As for value of d (differencing), we are going to select d as being 1 since we have taken a first order differencing, and since our seasonal pattern in ACF plot is stable we are going to choose value of D=1 (as opposed to D=0 when seasonal pattern is unstable) ","9561033c":"### SARIMAX Graphical Validation & Forecasting","287d9c01":"By looking at the above graphs, some of the trends regarding in the food distribution category along separate states have become evident. \n\n* Along the FOODS distribution, both CA and TX see large bumps in sales around june'12 and september'12-October'12 with the next peak coming around in Oct'13 that is larger than the sales in June'12 with the same trend continuing to repeat itself along other dates in 2014, and 2015\n* However, the sales along WI stores have a slightly different pattern with peaks of sales occuring in July'12 and next largest being observed in Feb-Mar'13. Similarly, the sales pick up around September in 2013, and then in Mar'13\n\n* In Household items distribution, the peaks of sales across all stores in CA, TX and WI occur around July, March while relatively huge dips in sales occuring in all three locations around the month of Jan-Feb. \n\n* In Hobbies items sales, there is surprising peak in sales (albiet different in actual number of units sold) across similar dates in all store locations (for example Feb'13 and Feb'15) with a slight dip in sale in Aug-Sep'13.","90fb22ec":"The test statistic is less than 1 % of the critical value so we reject the null hypothesis with 99 % confidence and therefore, the time series is stationary","74e0ceea":"#### What are **SNAP_CA, SNAP_TX, SNAP_WI**?\n\nSNAP stands for \"Supplementary Nutrition Assistance Program\" that is a federal level program aimed at providing food essentials to low-income households. This program is geared towards providing the food essentials and within the current dataset, the catagories of household items and hobbies items do not fall within the requirements of this program.\n\nThis program is only geared towards fighting the food hunger in america and only food related items can be purchased under this program","2c2f6da3":"## References and Further Readings:\n\nQuite a few resources proved helpful while others that offered more opportunity for learning are mentioned below\n\n- https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_sarimax_stata.html\n- https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/\n- https:\/\/towardsdatascience.com\/time-series-forecasting-a-getting-started-guide-c435f9fa2216\n- https:\/\/towardsdatascience.com\/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\n- https:\/\/facebook.github.io\/prophet\/docs\/diagnostics.html\n- https:\/\/www.baeldung.com\/cs\/mape-vs-wape-vs-wmape\n- https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/144067\n- Special thanks to the kernel of TSURU https:\/\/www.kaggle.com\/girmdshinsei\/for-japanese-beginner-with-wrmsse-in-lgbm#WRMSSE-calculation\n- Specal thanks to the kernel of sibmike for making WRMSSE calculation faster https:\/\/www.kaggle.com\/mubashir1\/fast-clear-wrmsse-18ms\/edit\n- https:\/\/www.kaggle.com\/binhlc\/forecasting-multiple-time-series-using-prophet\n- https:\/\/www.kaggle.com\/tnmasui\/m5-wrmsse-evaluation-dashboard","ded20c06":"### Distribution of Sales On Weekdays & Special Occasions?\n\nOur third dataset named, calendar_df, provides valuable information along the timeseries for the dataset of product_id. This dataset also contains information about Special occasions, SNAP (Supplementary Nutrition Assistance Programme) in the USA and coupled with the product_id dataset i.e. train_sales_df would be helpful in observing sales along weekdays, specific dates and special occasions","75e6acf2":"#### Specific Item OutSelling the Most?","37846e6d":"### ACF & PACF: Finding Pdq Visually?","f976f659":"We can observe that the test statistic value is less than even 1 % of the critical value, which indicates that we can reject the null hypothesis (that series is non-stationary) with 99 % confidence and p_value of 0 indicates that the test result is significant. ","b7543b7f":"With respect to the total number of sales, it is evident once again that the number of items sold on total have the greatest contributing share in CA, followed by Texas and Wisconsin. Now is it the case with the total revenue generated as well? We'd find that out using the revenue dataframe ","48f01ad2":"A look at both of these plots indicates the special occasions when the SNAP programme in CA were availed.  ","972adc5d":"### Custom Loss Function For Fb Prophet Tuning","402b133f":"#### Importing the Datasets:","9a41177a":"### Distribution of Total Sales Vs Store_ID?","32783a96":"#### Distribution of Total Sales Against Product_id on TimeSeries?","8495f381":"### **SARIMAX**\nSARIMAX (Seasonal Auto-Regressive Integrated Moving Averages With Exogenous Factors) is an extension of the ARIMA model but one that is able to handle the seasonality component along with exogenous factors. i.e. holidays. \n\nARIMA forecasting for a stationary time series is nothing but a regression equation. The prediction on the arima model is determined by its 'Trend' parameters p,d and q. They represent:\n\n* Number of AR (auto-regressive) terms (p) : AR terms are just lags of the dependent variable. e.g. if p is 2, the predictors for x(t) will be x(t-1), x(t-2) OR AR terms view the value at one time as a weighted sum of past values.\n* Number of MA (moving averages) terms (q) : MA terms are lagged forecast errors e(i) in prediction equation where e(i) is the difference between the moving avg at ith term vs the actual value. For q = 2, the predictors for x(t) will be e(t-1), e(t-2).\n* Number of differences (d) : These are the number of non-seasonal differences. \n\nThe SARIMAX adds other seasonal components to handle the seasonality with P, D, Q and m represpresenting the seasonal 'Auto-regressive' 'Differencing' and 'Moving-Average' Orders with m representing the number of time steps that are observing the seasonal behaviour. \n\nThe equation for SARIMAX is given as SARIMAX (p, d, q) = (P, D, Q, S)\n\nTo find out the appropriate values for p and q, we generally employ ACF (auto-correlation function) and PCF (Partial-correlation function). \n\n* **ACF** - AutoCorrelationFunction: It is the measure of the correlation of a time series against lagged version of itself. e.g. with a lag of 3, ACF would compare the time instances t1, t2 with t1-3, t2-3\n* **PCF** - PartialCorrelationFunction: It is a measure of correlation of a time series against a lagged version of itself but after removing the already calculated variations. e.g. with a lag of 4, PCF would check the correlation of against the lagged version but after removing the effects explained by lag1 to lag3 ","92cc4b19":"A few keypoints from the graph and the Dickey-Fuller tests indicate that:\n* Mean series is increasing with time and is not constant so series is not stationary\n* Standard deviation series also has quite a few deivations which indicates that the time series is not stationary\n* ADF statistic is larger than the critical value which indicates that the series still has a unit root","64d1276d":"%%%%^^^^^^^^^!!!^^^^^^^^^%%%%%","7d3f2701":"### Fb Prophet: Training and Validation Data Split","ccb9dfa3":"Since the train_sales_df contains the information about each specific item and the number of sales made, we can make a few observations regarding the most frequently purchased item too.\n\nWe could plot its behavior across the number of days to get a general gist of its sales pattern across given days. i.e d_1 to d_1913","f892cc2d":"#### Distribution of Product_ids Across Categories?","1f851876":"The results of the plot indicate that:\n\n* There is defintely an increase in the number of items being sold with time\n* The items sold in the Food and Household category have seen a more gradual increase than those of hobbies related items.","5608703d":"#### Distribution of Total Sales Sold Per State?","6efcf170":"Now that the dataset has been arranged in descending order of total sales, it would be a lot easier to estimate the item_id ('id') outselling others in each category","b231ea9d":"#### Distribution of Price Among Categories?","719b29dc":"Food items are the most sold out item that are followed by household items and then hobbies items.","6232f61b":"### Evluation Metrics:\n\nThis project uses quite a few error metrics that have been used to either optimise and fine tune the model or to find the prediction accuracy for the forecast made using time series. Given the situation of the time series forecast that we are gong to make, there are following error metrics used. \n\n1. **RMSE (Root Mean Square Error)** - In order to evaluate the sales forecasts made using SARIMAX, RMSE error metric was used. The Time series used in this case, took the average number of sales made, along the course of almost 1900 days, dsregarding the original heirarchical data arragement of product ids alongside several other levels. The utitlity of the error metrc RMSE is not invalidated since, we are considerng the average number of sales made, thereby nullyfying the effect of lots of zeroes for a lot of product ids, alongside only the number of days. \n\n2. **Custom Loss (WMAPE)**: The error metric Weighted Mean Absolute Percentage Error was used to evaluate and find the optimal parameters for the Facebook Designed Prophet Model. The WMAPE error is used for those instances where the priority of each product id is considered alongside the number of sales made. The weighted coefficients were calculated by dividing the difference between the forecast and the actual sales values by their mean value. \n\n3. **WRMSSE (Weighted Root Mean Square Scaled Error)** : In order to evaluate the efficiency of the model designed, the WRMSSE score was calculated once the prediction ready dataframe (forecast of almost 30,000 product ids) was ready. This was necesary because the WRMSSE score calculated required that the total entries of all product ids (almost around 30,000 products) be used and then a difference between the values of the validation dataset i.e. sales_train_validation.csv and the forecasted dataframe was taken. This WRMSSE error metric is the one that is provided by the competition and has been optimised to be used for the current competition. I order to evaluate it, we would have to use the whole dataset of sales_train_validation.csv that has been provided in single csv file. Further explanation for this error can be found at https:\/\/mofc.unic.ac.cy\/m5-competition\/\n\n","e4d500d8":"So, out of the most popular categories of 25 most sold items, more items had been sold in Texas than in either California or Wisconsin","1e380d25":"Looking at the graph, it is obvious that the unit sales across timeline have been on a steady increase. This would imply that \n\n* There are items remaining in inventory must be tracked\n* We could plot a sales of a few items to see how the behavior of each one fares across timeline. ","4bcd12d9":"### WRMSSE Error Calculation on Prediction","fccf291d":"## *Time Series Forecasting*\n\nIn time series forecasting, it is imperitive that the time series is stationary. A series is said to be stationary if its mean and variance remain constant over time. A time series forecasting requires that series be stationary so that any particular forecast will be more accurate if the current particular behavior over time is known. Should it be non-constant, there is high probability that the predictions made at one time would NOT be represented in series over some other time interval. \n\nTime series forecasting requires very specific criterion i.e.\n\n1. Constant Mean\n2. Constant Variance\n3. an auto-correlation that does not depend on time","a9d54fe5":"The above plot indicates the prophet model forecasting using average number of sales as a dependent variable. The plot indicatest that:\n- y_train(blue line plot) represents the training set data\n- y_test (green line ) is representative of validation data response\n- y_hat (red line) is a modeled response obtained using the Prophet Prediction Model\n- green filled area represents the 95 % confidence interval for the plot ","3aae0a0e":"The above results indicate that the presence of an increasing trend, as well as high seasonality in the time series being observed. The **residuals** are infact the component of the time series *without the seasonality and trend.*\n\nLet's see what the results of the dickey-fuller test in decomposition are","19694ea2":"## **Problem Statement?:**\n\nA large dateset makes it difficult to find all the underlying patterns present in the dataset. Fortunately, devising a few questions of SMART nature (SMART standing for specific, measureable, attainable, relevant and time-bound) are helpful in understanding patterns that could otherwise, might have been invisible with an all-in headfirst diving appraoch into the dataset. \n\nIn the specific probelm of m5-forecasting, we'd concern ourselves in finding out:\n\n- What is the general distribution of the sales_items(ids) across categories?\n- What is the behavior of categories across different stores?\n- What specific item id is outselling the most? \n- What is the specific id of item in each category that has most sales?\n- What is the most sales revenue collected? Is the item id that is outselling the most brings in the most revenue or is there variation along those trends?\n- What is the behavior of total sales (or mean sales) across time\n- What is the behavior of item_ids selling across different weekdays? Are there any specfic dates of the month when there are most sales?How does this trend change across different stores?\n\nAnd finally, we would be using SARIMAX and facebook developed Prophet Model to forecast the sales made in the next 28 days. ","8e0c9415":"These prophet plots of the individual product_ids indicate that the prophet model does quite a significantly better job at predicting the future response of the number of sales for each product_id. \n\nWe are going to use prophet model at predicting number of sales for individual ids because it would provide us with a better suited data for participation into the competition than say, using an average of sales for each product, as we did in SARIMAX.","f52b85ec":"#### **Methods For Making Time Series Stationary**\nSince the time series has both seasonality and trend components, we are going to adopt methods that would remove both trend and seasonality. Methods that we are going to use include:\n\n* **Differencing**\n* **Decomposition**","6858f5f2":"While there has been an increase in the number of sales across all three states i.e. CA, TX, and WI interesting to note is the fact that:\n\n* There is an overall increase in the sales of items across all states\n* WI sales cross TX sales at around Oct'14 and Sept'15 indicating that that could be boost in the sales of category of items that are seasonal. We'll try and investigate the sales of categories across separate states to further shed a light on this answer","9010a8b8":"## Further Recommendations on Improvement\n\nUsingh the prophet model along with a general moving average prediction for rest of the ids not predicted using prophet model were used to make forecast for the next 28 days. However, the accuracy score from kaggle private leaderboard has come out to be around 0.73. There are a few other kernels that have described ways to improve this further\n\nPlease Feel Free to Improve Upon this notebook and give your feedback\n\n- Run the model of prophet forecast for atleast 1\/3rd of the all product ids to explore its potential of improving the forecast , using a fast machine.\n- Use another model implementation such as LightGBM, Convolution NN to improve the score\n- Make new features with rolling mean as well as a lag feature that have been observed to improve score when using models such as lightgbm","ce387f7d":"It seems a few entries that have not been made to the 'event_name_1' attribute have been made available in a different category","c2119773":"Looking at the above plot with respect to specific stores, it is indicative that:\n\n* CA_3 store location has the most items sold\n* CA_4 store location has the least items old across the timeline\n* TX_2 has the most items sold across timeline with most items sales peaking around Sep'13\n* All Texas store locations (TX_1, TX_2, TX_3) experience a boost in sales around Feb_Mar'15 , with a peak around August'15 and then bottom out around Dec'15\n* Wisconsin Store locations experience a rather unpredictable pattern\n* Store WI_2 accelerates past the sold of WI_3 in Dec'12 and then continues to maintain the lead\n* Stores WI_2, WI_1 were underperformers in the beginning but experienced a boost in sales July'12 and November'12 respectively\n* There appears to be a continuous decline in the performance of sold items for the store location WI_3 that are only relatively improved towards the end","b84de55a":"#### Item ID Outselling Most in Each Category?","4d2a4766":"### Questions on Sales Revenue?\n\nFirst of all, we'd be interested in finding out what the specific revenue with respect to each product is? But since we have not been provided the dataset with ids (product_ids) in sell_prices_df(revenue dataframe) and there is a mismatch in dataset entries between the both datasets (train_sales_df with almost 30000 rows and sell_prices_df with almost 6M entries), therefore, atbest, a rough estimate could be made by merging both datasets.\n\nFrotunately enouhg, Revenue Dataframe (sell_prices_df) has the categorical level data available, so we could make an estimation regarding the items sold in each category to see what is the specific price where most items are getting sold. \n\nWe'll first try fitting the non-parametric method of probability distribution since we suspect the data distribution might not be exact replica of a distribution we have seen before, and may include peaks or outliers. (https:\/\/machinelearningmastery.com\/probability-density-estimation\/)\n\nIf the behavior of data distribution does not resemble a known distribution, further investigation would be made to remove the specific data outliers","b2b70d28":"### Prophet Forecast Using Individual Product IDs","e0d24a9a":"### WRMSSE Calculation\n\nIn order to measure the performance of the model, the competition metric has been provided already. This competiton metric can be found at the guidlines provided in the following link. \n\nWRMSSE is a sales metric that derives itself from RMSSE metric which is a variation on the metric of MASE (mean absolute scaled error). RMSSE specifically evaluate the sales forecast for datasets where heirarchical data level is made available. Since the first dataset of train_sales_val has heirarchical sales data available (i.e.sales divided along product_ids, item_ids, dept_ids, and store_ids level) and one that is also intermittent (i.e. has greater incidence of zero sales in some days while significant sales in other days), RMSSE's utility makes sense. The RMSSE metric also penalizes too large or too small values, therefore it is symmetric. \n\n\\begin{equation}\nWRMSSE = \\sum_{i=1}^{42840} \\left(\\frac{W_i}{\\sqrt{S_i}} \\times \\sqrt{\\sum{(D)^2}}\\right)\n\\end{equation}\n\nNote: Difference in rolled up vectors is equal to their rolled up difference:\n\\begin{equation}\nY\u00d7M\u2212Y^\u00d7M=(Y\u2212Y^)\u00d7M=D\n\\end{equation}\n\nwhere Y_t is the actual future value of the examined time series at point t, (Y_t ) is the generated forecast, n the length of the training sample (number of historical observations), and h the forecasting horizon. \n\n","d1b97b03":"The resullts will be compiled here :\n- First file with 50 ids prediction from 500 to 550 had an accuracy of 0.752\n- Second file with 260 (from 20 to 280) ids and holidays included had accuracy of 0.758\n- Third file with holidays removed, and 310 ids yields a public score of 0.754\n- Fourth file with simple moving average chosen yields a public leaderboard score of 1.082\n- Fifth file with product ids ranging from 350th to 1000th yielded a public leaderboard score of 0.7576\n- Sixth submission with product ids from 450th to 600th resulted in public leaderboard score of 0.7531\n- Seventh submission with product ids ranging from 15500th to 16000th gave a public leaderboard score 0.752\n- Eighth submission file with product ids ranging from 15500th to 17000th (almost 1500 product ids) using prophet forecast, resulted in public leaderboard score improvement for 0.738","d08a78e3":"A couple of points that could be drawn from this observation are:\n- most items have been sold in california\n- Texas and Wisconsin stores have almost total sales i.e. during the same timeframe of 1913 days, same number of items had been sold in both Texas and Wisconsin. Would this observation hold true in the sell_prices_df (revenue dataset) ? Does the observation remain same across different store locations in both Texas and Wisonconsin?","81e11d23":"The data_analysis on these points helps make it clear that:\n- Regarding the distribution of sales across department ids, most sales have been made across \"FOODS_3\" category followed by most sales made across household_1 category\n- Stores locale identity along with embedded state_ids helpe make it clear that the distribution of sales across Texas & Wisconsin stores are NOT the same , though the total number represented across categories (foods, houshold and hobbies) might have come the same.\n- The outperformers in Each state of CA, TX and Wisonsin are the stores with ids CA_3, TX_2 and WI_3 respectively","9b13cb5d":"#### Making Exogeneous Factors","e74a42ba":"### Distribution of Sales Items vs Sales Revenue? \nNow that we have product_id df as well as revenue_df, we'll merge the dataset now to start exploring trends of item_specific_data and sale_price_specific data ","72e08aa1":"##### **Differencing**\n\nIn order to perform this let us first take log , perform a log difference between the time series and its lag and look at the results","4a369257":"Looking at the above graphical representation, we can see that: \n- our model (indicated by green line) provides quite a good enough approximation for modeling the time_series behavior with the maroon background indicating a confidence interval for the new predicted time series. \n- We have applied the model on the test_dataset (data already present in our time series). In the next step, we would try to predict this time series response in the future","7f885933":"# A Comprehensive Tutorial To Understand EDA + Time Series Analysis + TimeSeries Forecasting SARIMAX & Prophet","9b40a727":"Same results across all stores","087879e0":"These are the five item ids that have been selling most in California and Texas locations.\n* While the first one \"FOODS_3_090_CA_3_validation\" is the same as the one that was most sold during our earlier analysis, the second observation is quite different to earlier assessment and the item most sold in second place has id \"FOODS_3_281_CA_3_validation\"\n* The items that were contributing to the sales bump of household items are also mentioned above. The household item sold most during the bump period is HOUSEHOLD_1_334_CA_3_validation ","aa75e035":"Now that we have taken a look at the data provided and have a general understanding of the dataset, we can see various relationships between the stores, the specific products sold, the categories that the consumers are most interested in as well as the sale of items per specific locale. Sell_prices_df contains information along the store level of some 6 million entries while the train_sales_df has informaiton about some 30,000 different products. \n\nFurthermore, the dataset in the calendar_df makes it possible to do a time series analysis of the products sold. ","8f444729":"### Importing the Libraries Required","61250367":"**=============++++++================**","1d8ccb27":"Once the total number of sales have been grouped against specific item id i.e. 'id' parameter, it is clear that the item \"**FOODS_3_090_CA_3_validation**\" has clearly sold most units than any other item in the category followed by **\"FOODS_3_586_TX_2_validation\"**. i.e. the first item belongs to food_3 category and sold in the CA_3 store location. Similarly, the second one belongs to TX_2 store location (i.e. second store in Texas) also belonging to the same category of FOODS_3 which is also consistent with observations made before","56cfbbcc":"### The Accuracy Metric AIC: \n\nSince the **AIC (Akaike-Information Criterion)** is given by the difference of number of paramters vs log-likelihood of the model, it is therefore desirable to select the values of AIC that are as small (or in the current case) as negative as possible. \nThe formula for the AIC is :\n\n*  **AIC = 2K - ln(k)**\n\nwhere k is the number of parameters for the selection of the model\n\nSince the combination for pdq in our case tends to come out as 2,1,1 and for seasonal_PDQ as 0,1,2,7 Therefore, we are going to select those values ","f7133eb0":"### **Prophet Time Series Modeling**\nProphet is a time-series forecasting tool developed by facebook team that has advanced capabilities to predict the time series data taking into consideration the weekly, monthly or yearly data seasonality. Prophet also has advanced capability to implement custom changepoints observed in time series data as well as specify holiday points where trends are continuously changing.","fee05947":"### Introduction:\n\nIn this competition named the M5-Forecasting, competition participants are expected to use hierarchical sales data extracted for analysis from Walmart, the world\u2019s largest company by revenue, and use it to forecast daily sales for the next 28 days. The sales dataset contains the information across some 30,000 different items along almost 1900 days. The distriubtion of the data has been divided around and covers stores in three US States (California, Texas, and Wisconsin) including item level, department, product categories, and store level details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset wouldn't only be helpful in improving forecasting accuracy but could also help expose the underlying patterns of sales observed through different locales as well as against a range of different product categories providing valuable business insights.\n\nOwing to the fact that a large dataset has been made available. We'd start off with a business formulation devised around a few problems. Whether the dataset would be enough to answer those questions would be determined by the dataset available. Should the answers not be found by the dataset, atleast necessary information regarding collection of a few datapoints would certainly help improve the future data design. ","851f3612":"From this graph it is difficult to tell the trend and seasonality patterns within our dataset. Althogh there is an increasing trend, the number of values for the entire dataset represented in the graph make it difficult to ascertain the true picture.\n\nFortunately, using the seasonal_decompose module in statsmodel, we could find out the seasonality and trend patterns in the timeseries. We are going to choose the \"mulitplicative seasonality\" owing to the fact of variability in the magnitude of the number of sales along the timeseries\n","c126b629":"After removing most of the outliers, it is apparent that for \n- FOODS related items, 75 % of the items sold are those that are less than 4 dollars\n- HOBBIES related items, 75 % of the items sold were less than 6 dollars with an mean price centered around 3.25-3.5 dollars\n- HOUSEHOLD items, 75 % of the items sold were less than 6.5 dollars. \n\nIt also represents that there are quite a few outliers in our price data. Since we had observed before using the kdeplots, that the distributions of the dataset were mostly skewed, we used the quartile method of removing the outliers. \n\n(https:\/\/www.analyticsvidhya.com\/blog\/2021\/05\/feature-engineering-how-to-detect-and-remove-outliers-with-python-code\/)","5ec0e9be":"### *Behaviour on TimeSeries & Analysis?*\n\nTimeseries questions would involve merging the datasets of product_ids and data information to enable a thorough analysis through time. We could find out information on the sales of items alongside the timeseries and also find out what specific effects, the weekends and specials occasions might have had on number of units sold\n\n**Rolling Averages** : Rolling averages are a useful tool to employ to understand the long term effects in a timeseries represented data. The rationale behind using rolling averages is the fact that resulting reading coming out of rolling averages smoothes out the data against daily fluctuations, and could help data professionals understand the long term impact of the sales\/inventory across time. Therefore, we are going to use both rolling average as a necassary tool to help aid ou","9ba0795a":"Looking at the distribution of data in 'event_name_1' and 'event_type_1' it is clear the data here relates to holidays which could reveal important trends when coupled with the information of sales made on the specific event.","4571bd78":"#### Distribution of Items Across Department & Store_ids?","5efc6ab1":"### Distribution of Sales Per Specific Days\/Weekends?","1f845b85":"The above two plots indicate:\n- Most items sold belong to the FOODS category, followed by HOUSEHOLD and HOBBIES\n- CA leads in the number of \"Total items\" sold in either category (FOODS, HOBBIES AND HOUSEHOLD) , while WISCONSIN lags behind TX in each category except FOODS. We would see whether the same difference translates in terms of revenue extracted off these states or not","6a1256a6":"Despite the fact that salesprices data contains almost 6M entries, in the present case, we are only considering common entries between revenue df and product_ids df. A few significant insights have come forward, i.e. \n\n- Although we saw that california consistently was the one state where the unique product_id most sales were made, the most revenue collected came from the Wisconsin State stores\n- Similarly, within the distribution of Categories, WI and TX contrinute more sales revenue than do the CA stores. \n- Wisconsin leads the revenue in FOODS and HOBBIES, while Texas leads the revenue in HOUSEHOLD\n- CA tends to contribute the smallest revenue out of all three states, despite having the most sales of items in its stores locations","578b98d6":"## *EDA (Exploratory Data Analysis):*","9fea4d87":"The 'original' label represents the timeseries data for which we have the data of average sales available. Whereas the 'one_ahead_label' indicates the forecast made for the time series of average sales, AND the filled background indicates the confidence interval. \n\nThe Forecast is a fairly good response of average number of sales that would be made into the future.","bc8a5afd":"fter looking at the trend, the seasonality and residuals patterns, we have the following information gathered:\n\n* There is definitely seasonality in our dataset, and in order to observe its pattern, we have taken a subset of dataset\n* Since the seasonal and residual components are changing the trend line by an amount that is 'Dependent\" of the changes in seasonal and residual components, we have a multiplicative behaviour","d366aa56":"We could extract following useful information by looking at these kde plots. \n\n- The probability distribution plot of the **household** items follows an almost normal distribution with a mean centered around a price of 5 Dollars and most items being sold within the 1 to 10 dollars range. This would indicate that most household items that are getting sold lie within the price bracket of 25 cents to 10 dollars\n- **Foods** items prices is a multimodal distribution indicating frequent variation in interest among food items purchased. The values occur both towards the relative higher price bracket as well as lower price bracket indicating that the degree of interest of consumers in food items is not only varied but that the Walmart stores have a catalogue of food items that are peaking consumer's interest across different categories. The price bracket in this case also happens to lie within 2 cents to 10 dollars with only very few items getting sold past that range\n- The probability distribution of **hobbies** related items prices indicates a mix of bimodal and multimodal distributions. This indicates that while a few items in specific category were sold more than others (first peak that lies in the area between 0.01 dollar to 1 dollar) there are also items towards a relative higher price bracket that have been also sold quite frequently enought to give it a multimodal distribution with small decreasing peaks indicative of decreasing interest in hobbies related items that are relatively expensive but sill significant enough to generate consumer interest. "}}