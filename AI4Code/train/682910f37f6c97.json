{"cell_type":{"75bb4ecb":"code","ecc3d30e":"code","61fe3ab7":"code","206906f3":"code","60503c9c":"code","352d7cb7":"code","a5cac164":"code","cd62e8e7":"code","b815f07c":"code","d5a61fea":"code","f87f0c86":"code","f68595e9":"code","e9dd2387":"code","4bffb160":"code","6833f6ae":"code","f8d1e125":"code","9cde9baf":"code","3100eb07":"code","74bc65f2":"code","a0f2a1f2":"code","ac12eb19":"code","5680bb07":"code","55a49347":"code","03dbcf7d":"code","659ffa5b":"code","88803c05":"code","4ee8cbc8":"code","0f056968":"code","aa5e98f5":"code","70311785":"code","4ae0fd84":"code","8ce927aa":"code","39578994":"markdown","d5a9c002":"markdown","d68eeeed":"markdown","e62984a3":"markdown","9e839373":"markdown","466a4a69":"markdown","89b43d9e":"markdown","6a419272":"markdown","9e060e92":"markdown","f3d6b579":"markdown","5e5cfda1":"markdown","e66862f5":"markdown","c4fb8ab1":"markdown","93fe067e":"markdown","ababf4e7":"markdown","bbf84ea5":"markdown","0acbb824":"markdown","2c052041":"markdown","104bea47":"markdown","5e20f4dd":"markdown","eea49078":"markdown","62c4f7a7":"markdown","a3324517":"markdown","af0a12e1":"markdown","66505c64":"markdown","de6812b6":"markdown","dc580d16":"markdown","7b57a343":"markdown","d1ba8bb5":"markdown","8318175e":"markdown"},"source":{"75bb4ecb":"# This is a simple demonstration of how interactions are made.\n# EX:\n\nfeat1=[1,2,3,4]\nfeat2=[2,3,4,5]\ninter_feat=[]\nfor i  in range(0,4):\n    inter_feat.append(feat1[i]*feat2[i])\ninter_feat","ecc3d30e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","61fe3ab7":"from sklearn.datasets import load_boston\n\n# This line of code would import our dataset.","206906f3":"l=load_boston()\n\n# This is a bunch format datatype which looks almost similar to a dictionary.","60503c9c":"l.keys()\n\n# This keys would help us to get the info of all that we want to know about the dataset.","352d7cb7":"l.data\n# This loads the data.","a5cac164":"l.feature_names\n\n# This is a list of all the feature's names that we gonna use.","cd62e8e7":"boston_data1=pd.DataFrame(data=l.data,columns=l.feature_names)\nboston_data2=pd.DataFrame(data=l.data,columns=l.feature_names)\n\n# Here we have created two copies of the datasets.","b815f07c":"# Lets just check the head of our dataset.\n# We would load first five rows.\n\nboston_data1.head(n=5)","d5a61fea":"boston_data1['Target']=l.target\nboston_data2['Target']=l.target\n\n# We add target variable in both the datasets. ","f87f0c86":"# Lets now again check the head of the datset.\n\nboston_data1.head(n=5)","f68595e9":"# Lets just quickly plot a pairplot to have an overview of whats happening.\n\nsns.pairplot(boston_data1)","e9dd2387":"corr_dataset=boston_data1.corr()\ncorr_dataset","4bffb160":"sns.heatmap(corr_dataset)","6833f6ae":"X=boston_data1.drop('Target',axis=1)\ny=boston_data1['Target']","f8d1e125":"from sklearn.model_selection import train_test_split","9cde9baf":"# Here a test size of 20% is used with a random state of 40.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=40)","3100eb07":"from sklearn.linear_model import LinearRegression","74bc65f2":"lr_bef_inter=LinearRegression()\n\n# Linear model before using interactions.","a0f2a1f2":"lr_bef_inter.fit(X_train,y_train)","ac12eb19":"lr_bef_inter.score(X_test,y_test)","5680bb07":"# Lets again load the correlation dataset.\n\ncorr_dataset","55a49347":"sns.heatmap(corr_dataset)","03dbcf7d":"# This is the function that we made to get interacting features.\n\ndef int_feat(cols):\n    col1=cols[0]\n    col2=cols[1]\n    return col1*col2","659ffa5b":"boston_data2['int_CRIM_RAD']=boston_data2[['CRIM','RAD']].apply(int_feat,axis=1)\nboston_data2['int_DIS_ZN']=boston_data2[['DIS','ZN']].apply(int_feat,axis=1)\nboston_data2['int_NOX_INDUS']=boston_data2[['NOX','INDUS']].apply(int_feat,axis=1)\nboston_data2['int_AGE_INDUS']=boston_data2[['AGE','INDUS']].apply(int_feat,axis=1)\nboston_data2['int_DIS_INDUS']=boston_data2[['DIS','INDUS']].apply(int_feat,axis=1)\nboston_data2['int_RAD_INDUS']=boston_data2[['RAD','INDUS']].apply(int_feat,axis=1)\nboston_data2['int_TAX_INDUS']=boston_data2[['TAX','INDUS']].apply(int_feat,axis=1)\nboston_data2['int_LSTAT_INDUS']=boston_data2[['LSTAT','INDUS']].apply(int_feat,axis=1)\nboston_data2['int_NOX_AGE']=boston_data2[['NOX','AGE']].apply(int_feat,axis=1)\nboston_data2['int_NOX_DIS']=boston_data2[['NOX','DIS']].apply(int_feat,axis=1)\nboston_data2['int_NOX_RAD']=boston_data2[['NOX','RAD']].apply(int_feat,axis=1)\nboston_data2['int_NOX_TAX']=boston_data2[['NOX','TAX']].apply(int_feat,axis=1)\nboston_data2['int_LSTAT_RM']=boston_data2[['LSTAT','RM']].apply(int_feat,axis=1)\nboston_data2['int_DIS_AGE']=boston_data2[['DIS','AGE']].apply(int_feat,axis=1)\nboston_data2['int_LSTAT_AGE']=boston_data2[['LSTAT','AGE']].apply(int_feat,axis=1)\n\n\n# This lines of code will add new interacted feature of our selected features to our dataset.","88803c05":"# We can see some new features been added to our dataset.\n\nboston_data2.head()","4ee8cbc8":"X_new=boston_data2.drop('Target',axis=1)\ny_new=boston_data2['Target']","0f056968":"X_New_train, X_New_test, y_New_train, y_New_test = train_test_split(X_new, y_new, test_size=0.20, random_state=40)","aa5e98f5":"# This is the new model that we created.\n\nlr_aft_int=LinearRegression()","70311785":"lr_aft_int.fit(X_New_train,y_New_train)","4ae0fd84":"lr_aft_int.score(X_New_test,y_New_test)","8ce927aa":"print('Accuracy of the model before adding interacting features     : {} %'.format(lr_bef_inter.score(X_test,y_test)*100))\nprint('Accuracy of the model after adding interacting features      : {} %'.format(lr_aft_int.score(X_New_test,y_New_test)*100))","39578994":"- It means sometimes adding complexity in the model actually helps in the improvisation of the model.","d5a9c002":"### INTERACTIONS","d68eeeed":"I would be writing comments in each of the cell. This would help me to give you some info about whats happening.","e62984a3":"Let us make two copies of datasets over here and if you ask me why i would just say bear with me :).","9e839373":"- Independent variables should barely correlate with each other.","466a4a69":"We would be using boston house dataset availabe in sci-kit learn library. These are ready to use datasets which are free from the null values and meant to be used for practice purposes. ","89b43d9e":"- As there were alot of correlation between the independent features, it was just ruining one of the Linear Regression assumption which got compensated after adding INTERACTIONS features.","6a419272":"Interaction between the two features is simply adding another feature, whose values are formed by performing product of observations from each of the two features.","9e060e92":"It looks like there is alot of correlation between Independent Variables. Lets check the correlation between correlations.","f3d6b579":"This Notebook will help us to understand how interactions helps us to increase model performance. Before getting into the code lets just understand what INTERACTIONS are, and how can we create one.","5e5cfda1":"Let's apply this concept on a stimulated dataset and see how it improves the accuracy.","e66862f5":"Lets try to use LINEAR REGRESSION model for this problem, but lets just divide our dataset to training and testing sets.","c4fb8ab1":"Distributions of each of the variables can be explored and can be transformed inorder to get much better results. But as we are concerned with the use of term INTERACTIONS lets just focus more on it.","93fe067e":"Looks like a good score but it should have performed well as it mostly had continous features in it.\nBut what was the problem. Did correlation between the independent variables had really affected our model ?","ababf4e7":"Lets just find it by applying interactions to our model.","bbf84ea5":"This can be done using some libraries from sci-kit learn. But i actually wanted to make You understand how it is actually done in the background. Although there are many other features that had correlation in between them as i wanted to make this as early as possible, so i had to skip some of them. You can use many of them to improve the model.","0acbb824":"### I hope this one actually actually helped you in gaining some knowledge THANK YOU and please give me an upvote if you really liked this.","2c052041":"#####  This seems to be a very good improvement to our model as adding interactive features actually helped our model to improve.","104bea47":"- Alot more can be done to improve the model performance but this was one of the great technique to improve the model performance.","5e20f4dd":"### MODEL COMPLEXITY","eea49078":"We can see there are more darker shades and lighter ones which represent strong relationship between them. Which means this dataset has highly correlated variables. Which affects one the Regression assumption.","62c4f7a7":"# How INTERACTIONS help in increasing model performance","a3324517":"Let's firstly import all of the libraries that we require.","af0a12e1":"## Interactions in Boston house dataset","66505c64":"We often create models that end up being an underfitting or overfitting. Both of them has alot to deal with the Bais-variance trade off as both of them affect our model in a bad way. There are several ways to reduce these problems inorder to get a Best fit model, and out of them one way is to add interactions between the features. This may not work always but would definitely add some value to the model. This is one of the way adding noise in the data inorder to make it more complex, but wait doesn't it tend the model to be overfitting ? and the answer is yes and no. Some times models do require extra dimensions to get more grip over the pattern. But some times when there are already enough dimensions available it is not good to use interactions for improving model performance.","de6812b6":"Looks good but lets just add target variable to this dataset and then we would start exploring the dataset","dc580d16":"Alot of them have strong correlations in between them(values above 0.60 i.e.,corr>0.60).\nlets understand more using visualisation.","7b57a343":"## How did this happend...?","d1ba8bb5":"**Note**: This result can be increased by using more interactions and using better models but we don't do this for now as the aim of this notebook is to show how interactions can be effective to boost our results.","8318175e":"Previously we had about 11 columns but we just increased them to some more and whatever that we have just added are actually the INTERACTING FEATURES. Now lets see by applying them to our model."}}