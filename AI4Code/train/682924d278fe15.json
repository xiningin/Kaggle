{"cell_type":{"d65cc886":"code","d04b6c00":"code","dd6e17f0":"code","7d461b8d":"code","bdd5774d":"code","60eb3dc5":"code","9d5abd6c":"code","f5285fc9":"code","9a644bf5":"code","4c2b64a8":"code","b296ce3a":"code","81906de0":"code","5c92497d":"code","51da32eb":"code","444ca29f":"code","9e7c0adf":"code","2257598d":"code","e38dd489":"code","e781684e":"code","380cbf03":"code","1b91f307":"code","926f2af5":"code","87735212":"code","99ca7167":"code","4abf8e1c":"code","f79cbf25":"code","2a374e16":"code","e0d79f54":"code","80134ecc":"code","70a545c0":"code","82b29c43":"code","8074490b":"code","7eb44b66":"code","7684517b":"code","29460852":"code","5e4f4cb6":"code","2522c43a":"code","19a3fb81":"markdown","c7d576c6":"markdown","cc9be647":"markdown","6fd648b1":"markdown","bdf15563":"markdown","30cb56d5":"markdown","55257879":"markdown"},"source":{"d65cc886":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\nsns.set_context(\"poster\")\nsns.set_palette('colorblind')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torch import nn\nimport torch.nn.functional as F\nimport torchvision\nimport time\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold","d04b6c00":"plt.rcParams['figure.figsize'] = (20.0, 10.0)","dd6e17f0":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')","7d461b8d":"train_mask = train_features['cp_type'] != 'ctl_vehicle'\ntrain_sig_ids = train_features.loc[train_mask]['sig_id']\ntrain = train_features.loc[train_mask]","bdd5774d":"test_mask = test_features['cp_type'] != 'ctl_vehicle'\ntest_sig_ids = test_features.loc[test_mask]['sig_id']\ntest = test_features.loc[test_mask]","60eb3dc5":"g_features = [cols for cols in train.columns if cols.startswith('g-')]\nc_features = [cols for cols in train.columns if cols.startswith('c-')]","9d5abd6c":"train_targets = train_targets[train_targets['sig_id'].isin(train_sig_ids)]","f5285fc9":"def make_pca_features(n_comp, train, test, feature_list, name, normalize=False, scaler=None):\n    \n    pca = PCA(n_comp)\n    train_pca = pca.fit_transform(train[feature_list])\n    test_pca = pca.transform(test[feature_list])\n    \n    if normalize and scaler is not None:\n        train_pca = scaler.fit_transform(train_pca)\n        test_pca = scaler.transform(test_pca)\n    \n    for i in range(n_comp):\n        train['{0}_{1}'.format(name, i)] = train_pca[:, i]\n        test['{0}_{1}'.format(name, i)] = test_pca[:, i]\n        \n    return train, test","9a644bf5":"def preprocess(data):\n    data['cp_time'] = data['cp_time'].map({24:0, 48:1, 72:2})\n    data['cp_dose'] = data['cp_dose'].map({'D1':0, 'D2':1})\n    return data","4c2b64a8":"train_w_pca, test_w_pca = make_pca_features(3, train, test, g_features, 'g_pca', normalize=True, scaler=StandardScaler())","b296ce3a":"train_w_pca, test_w_pca = make_pca_features(2, train_w_pca, test_w_pca, c_features, 'c_pca', normalize=True, scaler=StandardScaler())","81906de0":"X_train, X_val, y_train, y_val = train_test_split(preprocess(train_w_pca.drop(columns = ['sig_id', 'cp_type'])), train_targets.drop(columns = ['sig_id']), test_size=0.2)","5c92497d":"class TabDataset:\n    \n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.from_numpy(self.X.iloc[i, :].values.astype(np.float32))\n        y_i = torch.from_numpy(self.y.iloc[i, :].values.astype(np.float32))\n        \n        return X_i, y_i","51da32eb":"class TabDatasetTest:\n    \n    def __init__(self, X):\n        self.X = X\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.from_numpy(self.X.iloc[i, :].values.astype(np.float32))        \n        return X_i","444ca29f":"train_ds = TabDataset(X_train, y_train)\nvalid_ds = TabDataset(X_val, y_val)","9e7c0adf":"test_ds = TabDatasetTest(preprocess(test_w_pca.drop(columns = ['sig_id', 'cp_type'])))","2257598d":"test_ds[0].shape","e38dd489":"train_ds[0][1].dtype","e781684e":"train_dl = DataLoader(train_ds, batch_size=16, num_workers=8)\nvalid_dl = DataLoader(valid_ds, batch_size=16, num_workers=8)\ntest_dl = DataLoader(test_ds)","380cbf03":"def lin_block(in_size, out_size):\n    return nn.Sequential(\n        nn.BatchNorm1d(in_size), \n        nn.Dropout(0.2),\n        nn.utils.weight_norm(nn.Linear(in_size, out_size))\n    )","1b91f307":"class Model(nn.Module):\n    def __init__(self, in_size, hidden_size, out_size, num_blocks):\n        super().__init__()\n        \n        self.num_blocks = num_blocks\n        self.dense0 = nn.Sequential(\n        nn.BatchNorm1d(in_size), \n        nn.utils.weight_norm(nn.Linear(in_size, hidden_size))\n    )\n        \n        self.dense_blocks = nn.ModuleList()    \n        for i in range(self.num_blocks):\n            self.dense_blocks.append(lin_block(hidden_size, hidden_size))\n           \n        self.final = nn.Linear(hidden_size, out_size)\n                \n    def forward(self, x): \n\n        x = F.relu(self.dense0(x))\n\n        for i, block in enumerate(self.dense_blocks):                \n            x = F.relu(block(x))\n\n        x = self.final(x)\n        return x            ","926f2af5":"def fit(epochs, train_dl, valid_dl, model, loss_func, score_func, optimizer, scheduler):\n    losses, val_losses, scores = [], [], []\n    \n    for epoch in range(epochs):\n        t0 = time.time()\n        train_loss = 0.0\n        valid_loss = 0.0\n#         score = 0\n        \n        model.train()\n        for inputs, labels in train_dl:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            \n            loss = loss_func(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item()\n\n        model.eval();\n        with torch.no_grad():\n            for inputs, labels in valid_dl:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n\n                valid_loss += loss_func(outputs, labels).item()\n#                 score += score_func(outputs, labels).item()\n        \n        train_loss \/= len(train_dl)\n        valid_loss \/= len(valid_dl)\n#         score      \/= len(valid_dl)\n        \n        scheduler.step(valid_loss)\n        \n        print(f'[{epoch + 1}, {time.time() - t0:.1f}] train loss: {train_loss}, val loss: {valid_loss}') # , score: {score:.3f}')\n        losses.append(train_loss)\n        val_losses.append(valid_loss)\n#         scores.append(score)\n                \n    return losses, val_losses #, scores","87735212":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","99ca7167":"model = Model(879, 1024, 206, 2)\nmodel.to(device)","4abf8e1c":"weights = y_train.sum(axis=0)","f79cbf25":"weights","2a374e16":"w = torch.from_numpy(y_train.shape[0] \/ weights.values)","e0d79f54":"w.shape","80134ecc":"loss = nn.BCEWithLogitsLoss(pos_weight=w.to(device)) # pos_weight=torch.from_numpy(w).to(device)\noptimizer = torch.optim.Adam(model.parameters(), 1e-2)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)","70a545c0":"losses_train, losses_val = fit(20, train_dl, valid_dl, model, loss, None, optimizer, scheduler=scheduler)","82b29c43":"plt.plot(range(len(losses_train)), losses_train, label='train');\nplt.plot(range(len(losses_val)), losses_val, label='val');\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend();","8074490b":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","7eb44b66":"preds = inference_fn(model, test_dl, device)","7684517b":"results = pd.DataFrame(preds, columns=train_targets.columns[1:])\n\ntest_sig_ids.reset_index(drop=True, inplace=True)\nresults['sig_id'] = test_sig_ids","29460852":"sample_subs = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","5e4f4cb6":"submission = sample_subs[['sig_id']].merge(results, on='sig_id', how='left').fillna(0)","2522c43a":"submission.to_csv('submission.csv', index=False)","19a3fb81":"## 1. Data preprocessing","c7d576c6":"## 0. Data Exploration","cc9be647":"## 2. Model definition","6fd648b1":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","bdf15563":"We overfit without the dropout layers","30cb56d5":"### 1. Multilabel cross validation","55257879":"### 1. No cross validation"}}