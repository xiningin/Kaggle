{"cell_type":{"ef4a4bd2":"code","ac238fff":"code","ca940533":"code","172d2b69":"code","cabbdb84":"code","99aabc22":"code","6c99be08":"code","6ab04ec9":"code","ef67a596":"code","806c6eb6":"code","bfd70b26":"code","e1d049a0":"code","41007d3a":"code","e1b2f6af":"code","056054d5":"code","20abaa1f":"code","a76357b6":"code","08b6cc50":"code","bcef0755":"code","56850462":"code","19d008fa":"code","a03a8284":"code","cf3bf95b":"markdown","4c42239d":"markdown","f35b389b":"markdown","41caf96a":"markdown","9a05580b":"markdown","371efd95":"markdown","a7a7da62":"markdown","c43a1329":"markdown","40fc5dde":"markdown","b30e3cd1":"markdown","62dd9688":"markdown","9d0ea21e":"markdown","028eb5a5":"markdown","27162bf3":"markdown","f09ad616":"markdown","8f74ed6f":"markdown","11f5f1f9":"markdown","cc31cdad":"markdown","b0b12100":"markdown","353fb5d0":"markdown","0b6aff8d":"markdown","f160f7c2":"markdown","4d47c73b":"markdown","207bbf0b":"markdown","070db9f3":"markdown","2167052f":"markdown","696801ce":"markdown","5ac44b5d":"markdown","7188b5ca":"markdown"},"source":{"ef4a4bd2":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\nboston = load_boston()","ac238fff":"print(boston.keys())","ca940533":"print(boston.data.shape)","172d2b69":"print(boston.feature_names)","cabbdb84":"print(boston.DESCR)","99aabc22":"data = pd.DataFrame(boston.data)\ndata.columns = boston.feature_names","6c99be08":"data.head()","6ab04ec9":"data['PRICE'] = boston.target\ndata.head()","ef67a596":"data.info()","806c6eb6":"data.describe()","bfd70b26":"X, y = data.iloc[:,:-1],data.iloc[:,-1]","e1d049a0":"data_dmatrix = xgb.DMatrix(data=X,label=y)","41007d3a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","e1b2f6af":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)","056054d5":"xg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)","20abaa1f":"rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","a76357b6":"params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)","08b6cc50":"cv_results.head()","bcef0755":"print((cv_results[\"test-rmse-mean\"]).tail(1))","56850462":"xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)","19d008fa":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_reg,num_trees=1)\nplt.rcParams['figure.figsize'] = [30, 30]\nplt.show()","a03a8284":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()","cf3bf95b":"In this kernal, I will be using XGBoost to solve a regression problem. The dataset is taken from the UCI Machine Learning Repository and is also present in sklearn's datasets module. It has 14 explanatory variables describing various aspects of residential homes in Boston, the challenge is to predict the median value of owner-occupied homes per $1000s. ","4c42239d":"### Visualize Boosting Trees and Feature Importance","f35b389b":"XGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n\n* **gamma:** controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n* **alpha**: L1 regularization on leaf weights. A large value leads to more regularization.\n* **lambda**: L2 regularization on leaf weights and is smoother than L1 regularization.\n","41caf96a":"Create the train and test set for cross-validation of the results using the **train_test_split** function from sklearn's **model_selection** module with test_size size equal to 20% of the data.","9a05580b":"**Now let\u2019s convert it into a pandas DataFrame**","371efd95":"Importance of each feature column in the original dataset within the model.","a7a7da62":"Check for its shape","c43a1329":"Fit the regressor to the training set and make predictions on the test set using the familiar **.fit()** and **.predict()** methods.","40fc5dde":"Plotting the second tree with the matplotlib library:","b30e3cd1":"The boston variable itself is a dictionary, so check  its keys using the **.keys()** method.","62dd9688":"### What is Boosting\n\nBoosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. ","9d0ea21e":"Compute the rmse by invoking the **mean_sqaured_error** function from sklearn's** metrics** module.","028eb5a5":"Separate the target variable and rest of the variables using **.iloc** to subset the data.","27162bf3":"Explore the top 5 rows of the dataset","f09ad616":"**describe()** only gives summary statistics of columns which are continuous in nature ","8f74ed6f":"### XGBoost's hyperparameters\n\nTthe most common parameters are:\n\n* **learning_rate:** step size shrinkage used to prevent overfitting. Range is [0,1]\n* **max_depth**: determines how deeply each tree is allowed to grow during any boosting round.\n* **subsample:** percentage of samples used per tree. Low value can lead to underfitting.\n* **colsample_bytree:** percentage of features used per tree. High value can lead to overfitting.\n* **n_estimators:** number of trees you want to build.\n* **objective:** determines the loss function to be used like reg:linear for regression problems, reg:logistic for classification problems with only decision,\n* **binary:logistic** for classification problems with probability.\n","11f5f1f9":"**Content:**\n* What Boosting is and how XGBoost operates.\n* How to apply XGBoost on a dataset and validate the results.\n* About various hyper-parameters that can be tuned in XGBoost to improve model's performance.\n* How to visualize the Boosted Trees and Feature Importance","cc31cdad":"Convert the dataset into an optimized data structure called **Dmatrix** that XGBoost supports and gives it acclaimed performance and efficiency gains. ","b0b12100":"Get useful information about the data.","353fb5d0":"### Conclusion\n\nI hope this might have or will help you in some way or the other. You started off with understanding how Boosting works in general and then narrowed down to XGBoost specifically. ","0b6aff8d":"Print the feature names","f160f7c2":"Instantiate the XGBoost regressor object by calling the **XGBRegressor()** class from the XGBoost library with the hyper-parameters passed as arguments.","4d47c73b":"### XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification.\nXGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art\u201d machine learning algorithm to deal with structured data.","207bbf0b":"The description of the dataset is available in the dataset itself.","070db9f3":"### k-fold Cross Validation using XGBoost","2167052f":"### What makes XGBoost so popular?\n* **Speed and performance :** Originally written in C++, it is comparatively faster than other ensemble classifiers.\n* **Core algorithm is parallelizable :** Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPU\u2019s and across networks of computers making it feasible to train on very large datasets as well.\n* **Consistently outperforms other algorithm methods :** It has shown better performance on a variety of machine learning benchmark datasets.\n* **Wide variety of tuning parameters :** XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n\nXGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core. It is an optimized distributed gradient boosting library. ","696801ce":" Append **boston.target** to pandas DataFrame.","5ac44b5d":"Extract and print the final boosting round metric.","7188b5ca":"**Import the Boston Housing dataset  from scikit-learn  and other important libraries **"}}