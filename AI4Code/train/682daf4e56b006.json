{"cell_type":{"bdb4717c":"code","40093565":"code","01226f52":"code","9db35509":"code","746c64aa":"code","a5487186":"code","be06124e":"code","b727a3d3":"code","cf15289a":"code","5ac930b3":"code","f8e1d95b":"code","b5e9e952":"code","a01ee41f":"code","177071b1":"code","f10f183f":"code","85438851":"code","b6d8fbbb":"code","684de252":"code","f57b19a4":"code","e4d80760":"code","cc81f2d6":"code","3eab0a9a":"code","01eb0021":"markdown","5b11d7ef":"markdown","e9a4a95d":"markdown","454798b7":"markdown","748813a7":"markdown","aa4e520f":"markdown"},"source":{"bdb4717c":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndata_x = np.linspace(1.0, 10.0, 100).reshape(-1, 1)\n# a function transformation on input data, which needs to be fitted\ndata_y = 0.1*np.power(data_x, 2) + 0.5*np.random.randn(100,1)\n# normalize\ndata_x \/= np.max(data_x)\nplt.scatter(data_x, data_y)\nplt.show()","40093565":"order = np.random.permutation(len(data_x))\nportion = 20\ntest_x = data_x[order[:portion]]\ntest_y = data_y[order[:portion]]\ntrain_x = data_x[order[portion:]]\ntrain_y = data_y[order[portion:]]\ntrain_x.shape, train_y.shape, test_x.shape, test_y.shape","01226f52":"plt.figure()\n# plt.scatter(train_x[:, 1], train_y)\n# plt.scatter(test_x[:, 1], test_y)\n\nplt.scatter(train_x, train_y)\nplt.scatter(test_x, test_y)","9db35509":"w = [0, 0]\nLR = 0.1\nN = train_x.shape[0]\nfor epoch in range(300):\n    y_estimate = w[0] * train_x + w[1]\n    fun = train_y - y_estimate\n    w[0] = w[0] - LR * (-2 * train_x.T.dot(fun).sum() \/ N)\n    w[1] = w[1] - LR * (-2 * fun.sum() \/ N)\n    error = sum((y_estimate - train_y) ** 2)\n    if epoch % 50 == 0:\n        plt.figure()\n        plt.plot(test_x, w[0] * test_x + w[1])\n        plt.scatter(test_x, test_y)\n        plt.show()\n        print('error:', error)","746c64aa":"# train inference\nplt.plot(train_x, w[0] * train_x + w[1])\nplt.scatter(train_x, train_y)\nplt.show()","a5487186":"# test inference\nplt.plot(test_x, w[0] * test_x + w[1])\nplt.scatter(test_x, test_y)\nplt.show()","be06124e":"import numpy as np\nimport matplotlib.pyplot as plt","b727a3d3":"num_observations = 100\nx1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], size=num_observations)\nx2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], size=num_observations)\n\nsimulated_separableish_features = np.vstack((x1, x2))#.astype(np.float32)\nsimulated_labels = np.hstack((np.zeros(num_observations),\n                              np.ones(num_observations)))\nsimulated_separableish_features.shape, simulated_labels.shape","cf15289a":"plt.scatter(simulated_separableish_features[:, 0], \n            simulated_separableish_features[:, 1],\n            c = simulated_labels)","5ac930b3":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","f8e1d95b":"def log_likelihood(features, target, weights):\n    scores = np.dot(features, weights)\n    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n    return ll","b5e9e952":"def logistic_regression(features, target, num_steps, learning_rate):\n    # adding intercept\n#     intercept = np.ones((features.shape[0], 1))\n#     features = np.hstack((intercept, features))\n        \n    weights = np.zeros(features.shape[1])\n    \n    for step in range(num_steps):\n        scores = np.dot(features, weights)\n        predictions = sigmoid(scores)\n\n        # Update weights with gradient\n        output_error_signal = target - predictions\n        gradient = np.dot(features.T, output_error_signal)\n        weights += learning_rate * gradient\n        \n        # Print log-likelihood every so often\n        if step % 1000 == 0:\n            print(log_likelihood(features, target, weights))\n        \n    return weights","a01ee41f":"weights = logistic_regression(simulated_separableish_features, simulated_labels,\n                     num_steps = 5000, learning_rate = 0.01)","177071b1":"weights","f10f183f":"# adding intercept\n# data_with_intercept = np.hstack((np.ones((simulated_separableish_features.shape[0], 1)),\n#                                  simulated_separableish_features))\nfinal_scores = np.dot(simulated_separableish_features, weights)\npreds = np.round(sigmoid(final_scores))\n\nprint('Accuracy from scratch: {0}'.format((preds == simulated_labels).sum().astype(float) \/ len(preds)))","85438851":"# correct and in-correct samples\nplt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n            c = preds == simulated_labels)","b6d8fbbb":"import matplotlib.pyplot as plt\nimport numpy as np","684de252":"k = 3\npoints = np.vstack((\n                  (np.random.randn(100, 2)) - 4,\n                  (np.random.randn(100, 2)) + 4,\n                  (np.random.randn(100, 2))\n                  ))\nprint('shape:', points.shape)\nplt.scatter(points[:, 0], points[:, 1])","f57b19a4":"centroids = points.copy()\nnp.random.shuffle(centroids)\ncentroids = centroids[:k]\ncentroids","e4d80760":"def closest_centroid(points, centroids):\n    distances = np.sqrt(((points - centroids[:, np.newaxis])**2).sum(axis=2))\n    return np.argmin(distances, axis=0), sum(distances[0])","cc81f2d6":"def move_centroids(points, closest, centroids):\n    return np.array([points[closest==k].mean(axis=0) for k in range(centroids.shape[0])])","3eab0a9a":"for step in range(10):\n    plt.figure()\n    plt.scatter(points[:, 0], points[:, 1])\n    plt.scatter(centroids[:, 0], centroids[:, 1], c='r')\n    plt.show()\n    \n    closest, distances = closest_centroid(points, centroids)\n    print('distances:', distances)\n    \n    centroids = move_centroids(points, closest, centroids)","01eb0021":"# Goal of this kernel is to implement basic ML algorithms in plain NumPy","5b11d7ef":"# Linear Regression","e9a4a95d":"# K-means","454798b7":"Derivative of (y - (w[0] * x + w[1])) ** 2 will be, <br\/>\nfun = y - (w[0] * x + w[1]) # i.e. y_hat = m * x + b <br\/>\nRespective of w[0] -> 2 * (-x * fun) <br\/>\nRespective of w[1] -> 2 * (-1 * fun) <br\/>\n<br\/>\n\nDerivative is of loss function. <br\/>\nWhich is MSE here. So, if we use this same loss in Logistic Regression, it'll be the same gradient update code.","748813a7":"# Logistic Regression","aa4e520f":"## maybe 2 variable are not enough to fit this.. that's why only 75% performance"}}