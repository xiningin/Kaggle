{"cell_type":{"cff9faa6":"code","9a37ff7e":"code","1290c571":"code","2353267b":"code","3c451013":"code","4f4c88f7":"code","7b5131d5":"code","72833f8b":"markdown","a913cb76":"markdown","4c75f997":"markdown","864be4f9":"markdown","705236be":"markdown","3378997e":"markdown","95407233":"markdown","301ecc2d":"markdown","39e7c140":"markdown"},"source":{"cff9faa6":"# basic\nimport os, gc\nimport warnings\nimport numpy as np\nimport pandas as pd\n\n# visualize\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# reading tiff images\nimport tifffile as tiff \n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n# directory\nROOT = '..\/input\/hubmap-kidney-segmentation\/'","9a37ff7e":"train = pd.read_csv(f'{ROOT}train.csv')\ntrain","1290c571":"metadata = pd.read_csv(f'{ROOT}HuBMAP-20-dataset_information.csv')\nmetadata.head()","2353267b":"example_image = tiff.imread(os.path.join(ROOT, 'train\/2f6ecfcdf.tiff'))\nplt.figure(figsize=(16, 16))\nplt.imshow(example_image)\nplt.axis(\"off\")\nprint(f'Image Shape: {example_image.shape}')","3c451013":"TRAIN = '..\/input\/256256-hubmap\/train\/'\nMASKS = '..\/input\/256256-hubmap\/masks\/'\ntrain_images = os.listdir(TRAIN)\n\nfrom PIL import Image\nimport numpy as np\n\nplt.figure(figsize=(15,15))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    img = Image.open(TRAIN + train_images[i])\n    img = np.array(img.getdata())\n    img = img.reshape((256,256,3))\n    plt.imshow(img)\n    plt.title(train_images[i])\n    plt.grid(False)\n    plt.axis(False)\nplt.show()","4f4c88f7":"train_masks = os.listdir(MASKS)\nplt.figure(figsize=(15,15))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    img = Image.open(MASKS + train_masks[i])\n    img = np.array(img.getdata())\n    img = img.reshape((256,256))\n    plt.imshow(img)\n    plt.title(train_images[i])\n    plt.grid(False)\n    plt.axis(False)\nplt.show()","7b5131d5":"plt.figure(figsize=(15,15))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    Image2_mask = Image.open(MASKS + train_masks[i])\n    Image2_mask = np.array(Image2_mask.getdata())\n    Image2_mask = Image2_mask.reshape((256,256))\n    img = Image.open(TRAIN + train_images[i])\n    img = np.array(img.getdata())\n    img = img.reshape((256,256,3))\n    plt.imshow(img)\n    plt.imshow(Image2_mask, alpha=0.5)\n    plt.title(train_images[i])\n    plt.grid(False)\n    plt.axis(False)\nplt.show()\n","72833f8b":"1. [Competition purpose](#1)\n2. [Data Overview](#2)\n3. [Tiles](#3)\n\nThis notebook is using this notebook for the tiles making part : https:\/\/www.kaggle.com\/iafoss\/256x256-images \n\n\n\n\n<a id='1'><\/a>\n<h2 style='background:green; border:0; color:white;font-size:1.5em'><center> Competition purpose <\/center><\/h2>\n\n> Your challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a \u201cthree-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block\u201d (de Bono, 2013). The goal of this competition is the implementation of a successful and robust glomeruli FTU detector.\n\nThe dataset is comprised of 8 very large (>500MB - 5GB) TIFF files is huge reason why we are gonna use tiles. 8 big images are not fit for deep neural networks ! The training set includes annotations in both RLE-encoded and unencoded (JSON) forms. The annotations denote segmentations of glomeruli.\n\n\n## What we are prediciting?\n\nDevelop segmentation algorithms that identify glomeruli in the PAS stained microscopy data. Detect functional tissue units (FTUs) across different tissue preparation pipelines\n\n\n## Evaluation Metric: Dice Coefficient\n\nDice Coefficient is common in case our task involve **segmentation**. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. the Dice similarity coefficient for two sets X and Y is defined as:\n\n$$\\text{DC}(X, Y) = \\frac{2 \\times |X \\cap Y|}{|X| + |Y|}.$$\n\nwhere X is the predicted set of pixels and Y is the ground truth.\n\n<a id='2'><\/a>\n<h2 style='background:green; border:0; color:white;font-size:1.5em'><center> Overview <\/center><\/h2>\n","a913cb76":"We have some additional metadata in the `HuBMAP-20-dataset_information.csv` file such as sex, age, weight, height, laterality, percentage of cortex and medulla within the kidney. Note that this file provides also metadata for the test dataset.","4c75f997":"Finally, the images :","864be4f9":"Then the corresponding masks :","705236be":"# TO BE CONTINUED...","3378997e":"<a id='3'><\/a>\n<h2 style='background:green; border:0; color:white;font-size:1.5em'><center> Tiles <\/center><\/h2>\n\nWe use the dataset used in this brilliant notebook : https:\/\/www.kaggle.com\/iafoss\/256x256-images\n\nWe first load the image tiles :","95407233":"<h2 style='background:purple; border:0; color:white;font-size:2em'><center> HuBMAP Hacking the Kidney <\/center><\/h2>\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22990\/logos\/header.png\" alt=\"HuBMAP\">","301ecc2d":"We have a **train and a test folders with .tiff images and annotations in JSON**. We have **train.csv** and **HuBMAP-20-dataset_information.csv** containing image, masks information and metadata respectively. \n\nWe have 8 images in the train dataset, with 2 caracteristics in the `train.csv`: id and encoding (RLE-encoded representation of the mask).","39e7c140":"And then we superpose the two :"}}