{"cell_type":{"d48886b2":"code","998e9595":"code","829a8b5e":"code","f009c3ac":"code","09e53b34":"code","09d41aa9":"code","b61dae6f":"code","35c146cf":"code","f7b4a9ed":"code","2398c674":"code","dd72a299":"code","b53a122a":"code","572733a4":"code","9b997d78":"code","c64e4255":"code","6a362f1f":"code","3de9284a":"code","f0406de7":"code","f7a1e401":"code","d968f7fc":"code","284236db":"code","3e935b68":"code","ee90f04b":"code","866317e6":"code","ba9a55fc":"code","a5194e02":"code","046a0f1b":"code","0f179a38":"code","843ee172":"code","d9fff17f":"code","9c86ee28":"code","9252bcaa":"code","862df746":"code","248d7223":"code","51a1d10d":"code","bc579378":"code","8766b1b4":"code","09123b90":"code","c8a6775e":"code","bc245718":"code","345c4884":"code","62cb50b1":"code","0eb96cce":"code","4754ad5e":"code","376e1e5b":"code","132ee994":"code","edb7c48a":"code","a149572b":"code","58c33b6c":"code","c134bfe2":"code","693d254f":"code","d5f2f85c":"code","d38176c0":"code","97e238df":"code","7c8f0426":"code","31c39429":"code","b0bfce6a":"code","9a9301d8":"code","92afd8f6":"code","d946305c":"code","cc8906d5":"code","bafc0cf3":"code","a5efc26a":"code","03534e86":"code","1bf81f38":"code","d2bad05a":"code","0d1ce878":"code","04b887a4":"code","438c7433":"code","8b17f0f4":"code","d3ca34b2":"code","f3756bdf":"code","85edca0d":"code","1e7bbf29":"code","4e811c9e":"code","b9ba3187":"code","301399bb":"code","57949b0a":"code","52abea1f":"code","cf472d05":"code","5842a2d2":"code","8a3d7ac6":"code","34692ba8":"code","c053892a":"code","4635f804":"code","ce85dcfb":"code","c8cd3b9c":"code","45d70c17":"code","c7f82852":"code","d871c81a":"code","8d88694f":"code","a7ff2375":"code","cdaf1711":"code","6c8ad7bf":"code","31fdc639":"code","e7ea1fbf":"code","1960b398":"code","4179ecc1":"code","8f6e0730":"code","f85ea4e6":"code","90b5d403":"code","0e5fbdab":"code","0dbf76fd":"code","173e0504":"code","6c7b2a88":"code","7a1845bb":"code","70b3b5cf":"code","1db0529e":"code","e46dc7a9":"code","a8db2674":"code","cf741587":"code","1e65bf73":"code","826a8354":"code","873d47e5":"code","4ca37a94":"code","f6ec81f3":"code","f40119e5":"code","d360e0e3":"code","930cb274":"code","2a148117":"code","89f6263e":"code","c3ec3d53":"code","33870dc9":"code","73f66148":"code","cbce1432":"code","91b7642f":"code","68331bcf":"code","48d60d42":"code","88b5fe22":"code","e7da2316":"code","bbbd59c3":"code","bb48f19a":"code","2641cf84":"code","91666ccf":"code","bd2eb9ab":"code","22dda9de":"code","aab9390f":"code","46a07932":"code","424ee9f3":"code","b5990144":"code","8e37b2e6":"code","afaca539":"code","7dd3fabc":"code","964709e6":"code","b3fe482e":"code","0bc774d2":"code","adc57ae6":"code","a67995eb":"code","da80c19c":"code","4cd2635f":"code","9a2eee62":"code","a999b1fb":"code","e702d7db":"code","8fe4f755":"code","8e643ee1":"code","8da08027":"code","c6de1901":"code","9931fa44":"code","92902190":"code","a9552c47":"code","85602187":"code","41d281f7":"code","dd151a9c":"code","8e6b24df":"code","315f4715":"code","bbc6e39c":"code","71682582":"code","a16e7d06":"code","c62c6cbb":"code","8f1507ca":"code","719cf9f2":"code","c5f12c47":"code","7c56e0f1":"code","7597ce02":"code","08b48c73":"code","c882d89c":"code","5b62865b":"code","0159f4bd":"code","59f8ab5f":"markdown","819cb0ab":"markdown","a7fc44e1":"markdown","63729198":"markdown","44f0fde0":"markdown","5c5fd967":"markdown","320f734a":"markdown","edbe3305":"markdown","9eb8a77e":"markdown","3e6ae001":"markdown","1420569c":"markdown","b2c50f1c":"markdown","95572f0f":"markdown","3aef1750":"markdown","384223f0":"markdown","673769f4":"markdown","ab6bc9dc":"markdown","a9367969":"markdown","a4314114":"markdown","0f3fff2a":"markdown","d01ba3db":"markdown","7c54dc75":"markdown","e40075f6":"markdown","8e43ef47":"markdown","f3a2b518":"markdown","1196d7d3":"markdown","264129f7":"markdown","2cc22a76":"markdown","3c2c5117":"markdown","886cf7f9":"markdown","fcec7eca":"markdown","8281717f":"markdown","d8214a97":"markdown","133dcb45":"markdown","cba24e8b":"markdown","7b2d54a4":"markdown","450656f9":"markdown","5dcfc2ce":"markdown","3fbd6a1d":"markdown","e8c97e53":"markdown","16db270e":"markdown","9550b1eb":"markdown","5872a3b3":"markdown","2944f108":"markdown","fd0029ad":"markdown","a452cfe7":"markdown","debf7342":"markdown","30e29d6f":"markdown","921d62d8":"markdown","e92e7518":"markdown","d1a9290c":"markdown","1fc6340f":"markdown","109a8dbb":"markdown","612aeb82":"markdown","0232f5ca":"markdown","c80622b7":"markdown","99598b75":"markdown","25cc8140":"markdown","290dcdd3":"markdown","ab2e0b81":"markdown","8372cea7":"markdown","37c1d6f3":"markdown","bec7c26f":"markdown","14742b82":"markdown","77a6b06f":"markdown","d6158575":"markdown","ff6af20a":"markdown","d613d564":"markdown","63d17837":"markdown","be47d233":"markdown","aa368a81":"markdown","0eb0e229":"markdown","3b675692":"markdown","142b873a":"markdown","7f925a8a":"markdown","06301269":"markdown","c0fcb2ff":"markdown","a6d629a6":"markdown","17f61c8e":"markdown","1db1870e":"markdown","53aa4fdf":"markdown","7799007e":"markdown","bf14d589":"markdown","d3239944":"markdown","735ec8e1":"markdown","0c37a221":"markdown","509f6b7a":"markdown","cce165f3":"markdown","8ea1e79c":"markdown","c4de1851":"markdown","c26d90e8":"markdown","da80108f":"markdown","f6d25f99":"markdown","9769f85d":"markdown","83850508":"markdown","67a67a92":"markdown","f637c3a2":"markdown","b5287216":"markdown","8b2c771d":"markdown","413994c7":"markdown","463dfcbe":"markdown","b71dcd9e":"markdown","13502743":"markdown","a3bf8933":"markdown","a91dfa08":"markdown","0abdd097":"markdown","87ea9cff":"markdown","52857c3a":"markdown"},"source":{"d48886b2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","998e9595":"import numpy as np   \nimport pandas as pd    \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score,roc_curve,confusion_matrix,plot_confusion_matrix\n\n#Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nfrom sklearn import svm","829a8b5e":"ps_train = pd.read_csv(\"\/kaggle\/input\/predicting-pulsar-starintermediate\/pulsar_data_train.csv\")\n\nps_test = pd.read_csv(\"\/kaggle\/input\/predicting-pulsar-starintermediate\/pulsar_data_test.csv\")","f009c3ac":"ps_train1 = ps_train.copy()","09e53b34":"ps_train.head(10) #train set","09d41aa9":"ps_test.head(10) #test set","b61dae6f":"print(\"Train dataset shape: \",ps_train.shape)\nprint(\"Test dataset shape: \",ps_test.shape)","35c146cf":"print(\"Proportion of training data: %.2f\" % (ps_train.shape[0]\/(ps_train.shape[0] + ps_test.shape[0])*100), '%')\nprint(\"Proportion of test data: %.2f\" % (ps_test.shape[0]\/(ps_train.shape[0] + ps_test.shape[0])*100), '%')","f7b4a9ed":"ps_train.info() #train set","2398c674":"ps_test.info() #test set","dd72a299":"ps_train.describe() #train set","b53a122a":"ps_test.describe() #test set","572733a4":"print('For Train set')\nfor feature in ps_train.columns:\n    print('Missing values in feature ' + str(feature) + ' : ' + str(len(ps_train[ps_train[feature].isnull() == True])))\n\nprint('\\n')\nprint('For Test set')\nfor feature in ps_test.columns:\n    print('Missing values in feature ' + str(feature) + ' : ' + str(len(ps_test[ps_test[feature].isnull() == True])))","9b997d78":"print('There are total 12528 observations\\n')\nprint(\"Total missing values for train set:\")\nprint('Excess kurtosis of the integrated profile: ', 1735)\nprint('Standard deviation of the DM-SNR curve: ', 1178)\nprint('Skewness of the DM-SNR curve: ', 625 ,\"\\n\")\n\nprint(\"Total % of missing values for train set:\")\nprint('Excess kurtosis of the integrated profile: ', round(((1735\/12528)*100),2),'%')\nprint('Standard deviation of the DM-SNR curve: ', round(((1178\/12528)*100),2),'%')\nprint('Skewness of the DM-SNR curve:: ', round(((625\/12528)*100),2),'%')","c64e4255":"print('There are total 5370 observations\\n')\nprint(\"Total missing values for test set:\")\nprint('Excess kurtosis of the integrated profile: ', 767)\nprint('Standard deviation of the DM-SNR curve: ', 524)\nprint('Skewness of the DM-SNR curve: ', 244 ,\"\\n\")\n\nprint(\"Total % of missing values for test set:\")\nprint('Excess kurtosis of the integrated profile: ', round(((767\/5370)*100),2),'%')\nprint('Standard deviation of the DM-SNR curve: ', round(((524\/5370)*100),2),'%')\nprint('Skewness of the DM-SNR curve:: ', round(((244\/5370)*100),2),'%')","6a362f1f":"print(\"Duplicate rows in training data: \", ps_train.duplicated().sum())\nprint(\"Duplicate rows in test data: \", ps_test.duplicated().sum())","3de9284a":"ps_train.target_class.value_counts(1)","f0406de7":"ps_train['target_class'].value_counts()","f7a1e401":"sns.countplot(ps_train.target_class)","d968f7fc":"plt.figure(figsize=(15,10))\nplt.title('With Outliers',fontsize=16)\nps_train.boxplot(vert=0)","284236db":"# Number of outliers\nQ1 = ps_train.quantile(0.25)\nQ3 = ps_train.quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(1.5 * IQR)\nupper_range= Q3+(1.5 * IQR)\nprint('Number of Outliers:')\n((ps_train < (lower_range)) | (ps_train > (upper_range))).sum()","3e935b68":"print('Percentage of Outliers:')\n((((ps_train < (lower_range)) | (ps_train > (upper_range))).sum())\/17898)*100","ee90f04b":"fig, axes = plt.subplots(nrows=8,ncols=2,  figsize=(15, 40))\nfig.subplots_adjust(hspace = .4, wspace=.2)\nfor i in range(0,len(ps_train.columns)-1):\n  sns.distplot(ps_train[ps_train.columns[i]], ax=axes[i][0]).set_title(\"Hisotogram of\" + ps_train.columns[i])\n  sns.boxplot(ps_train[ps_train.columns[i]], ax=axes[i][1]).set_title(\"Boxplot of\" + ps_train.columns[i])","866317e6":"ps_train_pairplot = ps_train.copy() #creating deep copy\nps_train_pairplot = ps_train_pairplot.rename(columns={' Mean of the integrated profile': 'Mean IP', ' Standard deviation of the integrated profile': 'SD IP',\n                                  ' Excess kurtosis of the integrated profile': 'EK IP',' Skewness of the integrated profile': 'Skewness IP',\n                                  ' Mean of the DM-SNR curve':'Mean DM-SNR',' Standard deviation of the DM-SNR curve': 'SD DM-SNR',\n                                  ' Excess kurtosis of the DM-SNR curve': 'EK DM-SNR', ' Skewness of the DM-SNR curve': 'Skewness DM-SNR'})\n# Changing column names for better pairplot visualization","ba9a55fc":"sns.pairplot(data = ps_train_pairplot,hue = 'target_class',corner = True) #,height = 3,aspect = 1.2","a5194e02":"plt.figure(figsize=(12,7))\nsns.heatmap(ps_train.corr(), annot=True, fmt='.2f', cmap='Blues',mask=np.triu(ps_train.corr(),+1))","046a0f1b":"ps_train_1 = ps_train[ps_train.target_class == 1] #creating a dataset for only true pulsars for EDA\nps_train_0 = ps_train[ps_train.target_class == 0] #creating a dataset for only non pulsars for EDA\nps_train_1.shape","0f179a38":"ps_train_1.info()","843ee172":"ps_train_1.isnull().sum()","d9fff17f":"print(\"Total % of missing values:\")\nprint('Excess kurtosis of the integrated profile: ', round(((158\/1153)*100),2),'%')\nprint('Standard deviation of the DM-SNR curve: ', round(((105\/1153)*100),2),'%')\nprint('Skewness of the DM-SNR curve:: ', round(((62\/1153)*100),2),'%')","9c86ee28":"ps_train_1.describe()","9252bcaa":"fig, axes = plt.subplots(nrows=8,ncols=2,  figsize=(15, 40))\nfig.subplots_adjust(hspace = .4, wspace=.2)\nfor i in range(0,len(ps_train_1.columns)-1):\n  sns.distplot(ps_train_1[ps_train_1.columns[i]], ax=axes[i][0]).set_title(\"Hisotogram of\" + ps_train_1.columns[i])\n  sns.boxplot(ps_train_1[ps_train_1.columns[i]], ax=axes[i][1]).set_title(\"Boxplot of\" + ps_train_1.columns[i])","862df746":"ps_train_0.info()","248d7223":"ps_train_0.isnull().sum()","51a1d10d":"print(\"Total % of missing values:\")\nprint('Excess kurtosis of the integrated profile: ', round(((1577\/11375)*100),2),'%')\nprint('Standard deviation of the DM-SNR curve: ', round(((1073\/11375)*100),2),'%')\nprint('Skewness of the DM-SNR curve:: ', round(((563\/11375)*100),2),'%')","bc579378":"ps_train_0.describe()","8766b1b4":"fig, axes = plt.subplots(nrows=8,ncols=2,  figsize=(15, 40))\nfig.subplots_adjust(hspace = .4, wspace=.2)\nfor i in range(0,len(ps_train_0.columns)-1):\n  sns.distplot(ps_train_0[ps_train_0.columns[i]], ax=axes[i][0]).set_title(\"Hisotogram of\" + ps_train_0.columns[i])\n  sns.boxplot(ps_train_0[ps_train_0.columns[i]], ax=axes[i][1]).set_title(\"Boxplot of\" + ps_train_0.columns[i])","09123b90":"fig=plt.figure(figsize=(24,12))\nfor i in range(0,len(ps_train.columns)-1):\n    ax=fig.add_subplot(2,4,i+1).set_title(ps_train.columns[i])\n    sns.boxplot(x = 'target_class', y = ps_train.columns[i], data = ps_train)\n    plt.grid()","c8a6775e":"print('There are total 12528 observations\\n')\nprint(\"Total missing values for train set:\")\nprint('Excess kurtosis of the integrated profile: ', 1735)\nprint('Standard deviation of the DM-SNR curve: ', 1178)\nprint('Skewness of the DM-SNR curve: ', 625 ,\"\\n\")\n\nprint(\"Total % of missing values for train set:\")\nprint('Excess kurtosis of the integrated profile: ', round(((1735\/12528)*100),2),'%')\nprint('Standard deviation of the DM-SNR curve: ', round(((1178\/12528)*100),2),'%')\nprint('Skewness of the DM-SNR curve:: ', round(((625\/12528)*100),2),'%')","bc245718":"ps_train_median = ps_train.fillna(value=ps_train[[' Excess kurtosis of the integrated profile',' Standard deviation of the DM-SNR curve',' Skewness of the DM-SNR curve']].median())","345c4884":"ps_train_median.isnull().sum() #null values successfully imputed","62cb50b1":"ps_train_median.describe()","0eb96cce":"ps_train.describe() #of original train set","4754ad5e":"ps_train_LR = ps_train.copy() #new dataframe for linear regression method","376e1e5b":"# Preparing data for modelling\n\n#dataframe with only sets of required columns with no missing values\n\n# For Excess kurtosis of the integrated profile\ndf1 = ps_train.dropna(axis = 0, subset = [' Excess kurtosis of the integrated profile',' Skewness of the integrated profile']) \ndf1 = df1.loc[:,[' Excess kurtosis of the integrated profile',' Skewness of the integrated profile']] \ndf1_miss = pd.DataFrame(ps_train[' Skewness of the integrated profile'][ps_train[' Excess kurtosis of the integrated profile'].isnull()])\n\n\n# For Standard deviation of the DM-SNR curve\ndf2 = ps_train.dropna(axis = 0, subset = [' Standard deviation of the DM-SNR curve',' Mean of the DM-SNR curve'])\ndf2 = df2.loc[:,[' Standard deviation of the DM-SNR curve',' Mean of the DM-SNR curve']]\ndf2_miss = pd.DataFrame(ps_train[' Mean of the DM-SNR curve'][ps_train[' Standard deviation of the DM-SNR curve'].isnull()])\n\n# For Skewness of the DM-SNR curve\ndf3 = ps_train.dropna(axis = 0, subset = [' Skewness of the DM-SNR curve',' Excess kurtosis of the DM-SNR curve'])\ndf3 = df3.loc[:,[' Skewness of the DM-SNR curve',' Excess kurtosis of the DM-SNR curve']]\ndf3_miss = pd.DataFrame(ps_train[' Excess kurtosis of the DM-SNR curve'][ps_train[' Skewness of the DM-SNR curve'].isnull()])","132ee994":"#Creating X and Y variables for each of them\n\n# For Excess kurtosis of the integrated profile\nX1 = df1[[' Skewness of the integrated profile']]\ny1 = df1[' Excess kurtosis of the integrated profile']   # to be predicted\n\n# For Standard deviation of the DM-SNR curve\nX2 = df2[[' Mean of the DM-SNR curve']]\ny2 = df2[' Standard deviation of the DM-SNR curve']\n\n# For Skewness of the DM-SNR curve\nX3 = df3[[' Excess kurtosis of the DM-SNR curve']]\ny3 = df3[' Skewness of the DM-SNR curve']","edb7c48a":"# Importing required libraries for model building\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import train_test_split\n\n#Creating train test split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.30 , random_state=1)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.30 , random_state=1)\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.30 , random_state=1)\n\n# Fitting in the model\nlm1,lm2,lm3 = LinearRegression().fit(X1_train, y1_train),LinearRegression().fit(X2_train, y2_train),LinearRegression().fit(X3_train, y3_train)","a149572b":"# Finding the coefficient and intercepts in each case m = coefficient, c = intercept\nm1,m2,m3 = lm1.coef_[0],lm2.coef_[0],lm3.coef_[0]\nc1,c2,c3 = lm1.intercept_,lm2.intercept_,lm3.intercept_","58c33b6c":"# Creating dummy columns in dataset where if there is a null value, value is 0 otherwise 1\nps_train_LR['value1'] = ps_train[' Excess kurtosis of the integrated profile'].map(lambda x : 0 if np.isnan(x) else 1)\nps_train_LR['value2'] = ps_train[' Standard deviation of the DM-SNR curve'].map(lambda x : 0 if np.isnan(x) else 1)\nps_train_LR['value3'] = ps_train[' Skewness of the DM-SNR curve'].map(lambda x : 0 if np.isnan(x) else 1)","c134bfe2":"# Using y = mx + c method to predict missing values and imputing at that location\nfor i in range(0,len(ps_train_LR)-1):\n    if(ps_train_LR.value1[i] == 0):\n        ps_train_LR[' Excess kurtosis of the integrated profile'][i] = c1 + m1 * ps_train_LR[' Skewness of the integrated profile'][i] \n    if(ps_train_LR.value2[i] == 0):\n        ps_train_LR[' Standard deviation of the DM-SNR curve'][i] = c2 + m2 * ps_train_LR[' Mean of the DM-SNR curve'][i] \n    if(ps_train_LR.value3[i] == 0):\n        ps_train_LR[' Skewness of the DM-SNR curve'][i] = c3 + m3 * ps_train_LR[' Excess kurtosis of the DM-SNR curve'][i] ","693d254f":"ps_train_LR.drop(['value1', 'value2','value3'], axis = 1,inplace = True) # dropping dummy variables as not required anymore","d5f2f85c":"ps_train_LR.isnull().sum()","d38176c0":"ps_train_LR.describe()","97e238df":"ps_train.describe() # original dataset","7c8f0426":"from sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=3)\n\nX = ps_train.drop('target_class', axis=1)\ny = ps_train[['target_class']]\n\nps_train_KNN = imputer.fit_transform(X)\n\nps_train_KNN =   pd.DataFrame(data=ps_train_KNN,columns=X.columns)","31c39429":"ps_train_KNN['target_class'] = ps_train['target_class']\nps_train_KNN.isnull().sum()","b0bfce6a":"ps_train_KNN.describe()","9a9301d8":"ps_train.describe()","92afd8f6":"print('Percentage of Outliers:')\n((((ps_train < (lower_range)) | (ps_train > (upper_range))).sum())\/17898)*100","d946305c":"ps_train_out = ps_train_KNN.copy() #creating dataframe without outliers using missing value treated dataframe\n\ndef remove_outlier(col):\n    sorted(col)\n    Q1,Q3=np.percentile(col,[25,75])\n    IQR=Q3-Q1\n    lower_range= Q1-(1.5 * IQR)\n    upper_range= Q3+(1.5 * IQR)\n    return lower_range, upper_range","cc8906d5":"for column in ps_train_out.loc[:,:' Skewness of the DM-SNR curve'].columns: #I didn't took the target class here\n    if ps_train_out[column].dtype != 'object':\n        lr,ur=remove_outlier(ps_train_out[column])\n        ps_train_out[column]=np.where(ps_train_out[column]>ur,ur,ps_train_out[column])\n        ps_train_out[column]=np.where(ps_train_out[column]<lr,lr,ps_train_out[column])","bafc0cf3":"plt.figure(figsize=(10,8))\nplt.title('Without Outliers',fontsize=16)\nps_train_out.boxplot(vert=0)","a5efc26a":"# Correlation plot after outlier treatment\nplt.figure(figsize=(12,8))\nsns.heatmap(ps_train_out.corr(),annot=True, cmap='Blues',mask=np.triu(ps_train_out.corr(),+1))","03534e86":"plt.figure(figsize=(12,8))\nsns.heatmap(ps_train_out.drop(' Standard deviation of the DM-SNR curve',axis=1).corr(),annot=True, cmap='Blues',mask=np.triu(ps_train_out.drop([' Standard deviation of the DM-SNR curve'],axis=1).corr(),+1))","1bf81f38":"plt.figure(figsize=(12,8))\nsns.heatmap(ps_train_out.drop([' Standard deviation of the DM-SNR curve',' Excess kurtosis of the DM-SNR curve'],axis=1).corr(),annot=True, cmap='Blues',mask=np.triu(ps_train_out.drop([' Standard deviation of the DM-SNR curve',' Excess kurtosis of the DM-SNR curve'],axis=1).corr(),+1))","d2bad05a":"# VIF of all variables\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nX = ps_train_out.drop('target_class', axis=1)\nvif = [variance_inflation_factor(X.values, ix) for ix in range(X.shape[1])] \ni=0\nfor column in X.columns:\n  print (column ,\"--->\",  vif[i])\n  i = i+1","0d1ce878":"# VIF after removing Excess kurtosis of the DM-SNR curve and Standard deviation of the DM-SNR curve\nX1 = ps_train_out.drop(['target_class',' Excess kurtosis of the DM-SNR curve',' Standard deviation of the DM-SNR curve'], axis=1)\n\ni=0\nfor column in X1.columns:\n  print (column ,\"--->\",  vif[i])\n  i = i+1","04b887a4":"X = ps_train_out.drop('target_class',axis=1)\ny = ps_train_out[['target_class']]","438c7433":"from sklearn.model_selection import train_test_split\n\n# Keeping test size as 30% of dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1)","8b17f0f4":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler() \n# Scaling the data\nX_trains = ss.fit_transform(X_train)\nX_tests = ss.transform(X_test)","d3ca34b2":"# Building base Model\nDT = DecisionTreeClassifier(random_state=1) #random state given for consistency in results\nDT.fit(X_trains, y_train)","f3756bdf":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, DT.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, DT.predict(X_tests)),'\\n')","85edca0d":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(DT, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(DT, X_test, y_test, cv=10)))","1e7bbf29":"DT_param_random = [    \n    {'splitter' : ['best', 'random'],\n     'max_features' : list(range(1,X_train.shape[1])),\n     'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n     'min_samples_leaf' : randint(1, 1000), # 1-3% of length of dataset\n     'min_samples_split' : randint(300, 5000), # approx 3 times the min_samples_leaf\n     \"criterion\": [\"gini\", \"entropy\"]\n    }\n]\n\nDT_random1 = RandomizedSearchCV(DT, param_distributions = DT_param_random, cv = 5, verbose=True, n_jobs=-1)","4e811c9e":"DT_random1.fit(X_trains, y_train)","b9ba3187":"# Checking the best estimator values\nDT_random1.best_estimator_","301399bb":"# Best Random Model\nDT_random = DecisionTreeClassifier(max_depth=5.0,max_features=4,min_samples_leaf=525, min_samples_split=2593,criterion='entropy',random_state=1)\nDT_random.fit(X_trains, y_train)","57949b0a":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, DT_random.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, DT_random.predict(X_tests)),'\\n')","52abea1f":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(DT_random, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(DT_random, X_test, y_test, cv=10)))","cf472d05":"DT_param_grid = [    \n    {'splitter' : ['best', 'random'],\n     'max_features' : [4,5,6],\n     'max_depth' : [20,30,40],\n     'min_samples_leaf' : [100,200,300], # 1-3% of total dataset\n     'min_samples_split' : [300, 400, 500], # approx 3 times the min_samples_leaf\n     \"criterion\": [\"gini\", \"entropy\"]\n    }\n]\n\nDT_grid1 = GridSearchCV(DT, param_grid = DT_param_grid, cv = 5, verbose=False, n_jobs=-1)","5842a2d2":"DT_grid1.fit(X_trains, y_train) ","8a3d7ac6":"DT_grid1.best_estimator_","34692ba8":"# Best Grid Model\nDT_grid = DecisionTreeClassifier(max_depth=20,max_features=4,min_samples_leaf=100, min_samples_split=300,criterion='entropy',random_state=1)\nDT_grid.fit(X_trains, y_train)","c053892a":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, DT_grid.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, DT_grid.predict(X_tests)),'\\n')","4635f804":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(DT_grid, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(DT_grid, X_test, y_test, cv=10)))","ce85dcfb":"# Prediction for final model\nDT_train_predict = DT_grid.predict(X_trains)\nDT_test_predict = DT_grid.predict(X_tests)\n\n# Probability Prediction for final model\nDT_prob_train = DT_grid.predict_proba(X_trains)\nDT_prob_test = DT_grid.predict_proba(X_tests)\n\n# AUC score for final model\nDT_train_auc = metrics.roc_auc_score(y_train,DT_prob_train[:,1])\nDT_test_auc = metrics.roc_auc_score(y_test,DT_prob_test[:,1])","c8cd3b9c":"# Classification report in a dataframe of final model\nDT_df_train=pd.DataFrame(classification_report(y_train, DT_train_predict,output_dict=True)).transpose()\nDT_df_test=pd.DataFrame(classification_report(y_test, DT_test_predict,output_dict=True)).transpose()","45d70c17":"# Building base Model\nRF = RandomForestClassifier(random_state=1) \nRF.fit(X_trains, y_train)","c7f82852":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, RF.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, RF.predict(X_tests)),'\\n')","d871c81a":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(RF, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(RF, X_test, y_test, cv=10)))","8d88694f":"RF_param_random = [    \n    {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n     'max_features' : ['auto', 'sqrt'],\n     'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n     'min_samples_leaf' : randint(1, 300), # 1-3% of length of dataset\n     'min_samples_split' : randint(300, 3000), # approx 3 times the min_samples_leaf\n     'bootstrap': [True, False]\n    }\n]\n\nRF_random1 = RandomizedSearchCV(RF, param_distributions = RF_param_random, cv = 5, n_jobs=-1)","a7ff2375":"RF_random1.fit(X_trains, y_train)","cdaf1711":"RF_random1.best_estimator_","6c8ad7bf":"# Best Random Model\nRF_random = RandomForestClassifier(max_depth=20.0,max_features='sqrt',min_samples_leaf=66, min_samples_split=511,criterion='gini',n_estimators=1400,random_state=1)\nRF_random.fit(X_trains, y_train)","31fdc639":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, RF_random.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, RF_random.predict(X_tests)),'\\n')","e7ea1fbf":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(RF_random, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(RF_random, X_test, y_test, cv=10)))","1960b398":"RF_param_grid = [    \n    {'n_estimators': [300,400],\n     'max_features' : [3,4],\n     'max_depth' : [15,20],\n     'min_samples_leaf' : [100,200], # 1-3% of length of dataset\n     'min_samples_split' : [300,400] # approx 3 times the min_samples_leaf\n    }\n]\n\nRF_grid1 = GridSearchCV(RF, param_grid = RF_param_grid, cv = 3, n_jobs=-1)","4179ecc1":"RF_grid1.fit(X_trains, y_train) ","8f6e0730":"best_estimator_","f85ea4e6":"# Best Grid Model\nRF_grid = RandomForestClassifier(max_depth=15,max_features=3,min_samples_leaf=100, min_samples_split=300,criterion='gini',n_estimators=300,random_state=1)\nRF_grid.fit(X_trains, y_train)","90b5d403":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, RF_grid.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, RF_grid.predict(X_tests)),'\\n')","0e5fbdab":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(RF_grid, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(RF_grid, X_test, y_test, cv=10)))","0dbf76fd":"# Prediction for final model\nRF_train_predict = RF_random.predict(X_trains)\nRF_test_predict = RF_random.predict(X_tests)\n\n# Probability Prediction for final model\nRF_prob_train = RF_random.predict_proba(X_trains)\nRF_prob_test = RF_random.predict_proba(X_tests)\n\n# AUC score for final model\nRF_train_auc = metrics.roc_auc_score(y_train,RF_prob_train[:,1])\nRF_test_auc = metrics.roc_auc_score(y_test,RF_prob_test[:,1])","173e0504":"# Classification report in a dataframe of final model\nRF_df_train=pd.DataFrame(classification_report(y_train, RF_train_predict,output_dict=True)).transpose()\nRF_df_test=pd.DataFrame(classification_report(y_test, RF_test_predict,output_dict=True)).transpose()","6c7b2a88":"# Building base model\nLR = LogisticRegression(random_state = 1)\nLR.fit(X_trains, y_train)","7a1845bb":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, LR.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, LR.predict(X_tests)),'\\n')","70b3b5cf":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(LR, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(LR, X_test, y_test, cv=10)))","1db0529e":"LR_param_random = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\n\nLR_random1 = RandomizedSearchCV(LR, param_distributions = LR_param_random, cv = 5, n_jobs=-1)","e46dc7a9":"LR_random1.fit(X_trains,y_train)","a8db2674":"LR_random1.best_estimator_","cf741587":"# Best Random model\nLR_random = LogisticRegression(C=1.623776739188721,penalty='none', solver='newton-cg',max_iter=100, random_state = 1)\nLR_random.fit(X_trains,y_train)","1e65bf73":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, LR_random.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, LR_random.predict(X_tests)),'\\n')","826a8354":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(LR_random, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(LR_random, X_test, y_test, cv=10)))","873d47e5":"LR_param_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : [0.00001],\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]\n\nLR_grid1 = GridSearchCV(LR, param_grid = LR_param_grid, cv = 5, n_jobs=-1)","4ca37a94":"LR_grid1.fit(X_trains, y_train) ","f6ec81f3":"LR_grid1.best_estimator_","f40119e5":"# Best Grid model\nLR_grid = LogisticRegression(C=1e-05,penalty='none', solver='lbfgs',max_iter=100, random_state = 1)\nLR_grid.fit(X_trains,y_train)","d360e0e3":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, LR_grid.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, LR_grid.predict(X_tests)),'\\n')","930cb274":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(LR_grid, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(LR_grid, X_test, y_test, cv=10)))","2a148117":"# Prediction for final model\nLR_train_predict = LR_random.predict(X_trains)\nLR_test_predict = LR_random.predict(X_tests)\n\n# Probability Prediction for final model\nLR_prob_train = LR_random.predict_proba(X_trains)\nLR_prob_test = LR_random.predict_proba(X_tests)\n\n# AUC score for final model\nLR_train_auc = metrics.roc_auc_score(y_train,LR_prob_train[:,1])\nLR_test_auc = metrics.roc_auc_score(y_test,LR_prob_test[:,1])","89f6263e":"# Classification report in a dataframe of final model\nLR_df_train=pd.DataFrame(classification_report(y_train, LR_train_predict,output_dict=True)).transpose()\nLR_df_test=pd.DataFrame(classification_report(y_test, LR_test_predict,output_dict=True)).transpose()","c3ec3d53":"# Building base Model\nNB = GaussianNB()\nNB.fit(X_trains, y_train)","33870dc9":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, NB.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, NB.predict(X_tests)),'\\n')","73f66148":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(NB, X_trains, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(NB, X_tests, y_test, cv=10)))","cbce1432":"# Prediction for final model\nNB_train_predict = NB.predict(X_trains)\nNB_test_predict = NB.predict(X_tests)\n\n# Probability Prediction for final model\nNB_prob_train = NB.predict_proba(X_trains)\nNB_prob_test = NB.predict_proba(X_tests)\n\n# AUC score for final model\nNB_train_auc = metrics.roc_auc_score(y_train,NB_prob_train[:,1])\nNB_test_auc = metrics.roc_auc_score(y_test,NB_prob_test[:,1])","91b7642f":"# Classification report in a dataframe of final model\nNB_df_train=pd.DataFrame(classification_report(y_train, NB_train_predict,output_dict=True)).transpose()\nNB_df_test=pd.DataFrame(classification_report(y_test, NB_test_predict,output_dict=True)).transpose()","68331bcf":"# Building base Model\nXGB=xgb.XGBClassifier(random_state=1)\nXGB.fit(X_trains, y_train)","48d60d42":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, XGB.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, XGB.predict(X_tests)),'\\n')","88b5fe22":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(XGB, X_trains, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(XGB, X_tests, y_test, cv=10)))","e7da2316":"XGB_param_random = [    \n    {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n     'max_depth' : [int(x) for x in np.linspace(1, 32, 32, endpoint=True)],\n     'subsample': np.linspace(start = 0.00001, stop = 0.9, num = 20),\n      'gamma': np.linspace(start = 0.00001, stop = 0.9, num = 20),\n      'colsample_bytree': [0.2,0.4,0.6,0.8,1.0],\n     'learning_rate': np.linspace(start = 0.00001, stop = 0.1, num = 20)\n    }\n]\n\nXGB_random1 = RandomizedSearchCV(XGB, param_distributions = XGB_param_random, cv = 5, n_jobs=-1)","bbbd59c3":"XGB_random1.fit(X_trains, y_train)","bb48f19a":"XGB_random1.best_estimator_","2641cf84":"# Best Random model\nXGB_random = xgb.XGBClassifier(colsample_bytree=1.0,gamma=0.09474578947368421,n_estimators=1400,max_depth=15,subsample=0.14211368421052634, learning_rate=0.005272631578947368, random_state=1)\nXGB_random.fit(X_trains, y_train)","91666ccf":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, XGB_random.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, XGB_random.predict(X_tests)),'\\n')","bd2eb9ab":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(XGB_random, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(XGB_random, X_test, y_test, cv=10)))","22dda9de":"# Prediction for final model\nXGB_train_predict = XGB.predict(X_trains)\nXGB_test_predict = XGB.predict(X_tests)\n\n# Probability Prediction for final model\nXGB_prob_train = XGB.predict_proba(X_trains)\nXGB_prob_test = XGB.predict_proba(X_tests)\n\n# AUC score for final model\nXGB_train_auc = metrics.roc_auc_score(y_train,XGB_prob_train[:,1])\nXGB_test_auc = metrics.roc_auc_score(y_test,XGB_prob_test[:,1])","aab9390f":"# Classification report in a dataframe of final model\nXGB_df_train=pd.DataFrame(classification_report(y_train, XGB_train_predict,output_dict=True)).transpose()\nXGB_df_test=pd.DataFrame(classification_report(y_test, XGB_test_predict,output_dict=True)).transpose()","46a07932":"# Building base Model\nSVM = svm.SVC(random_state=1).fit(X_trains, y_train)","424ee9f3":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, SVM.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, SVM.predict(X_tests)),'\\n')","b5990144":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(SVM, X_trains, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(SVM, X_tests, y_test, cv=10)))","8e37b2e6":"SVM_param_random = {'C': [0.01,0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf'],\n              'tol':[0.01,0.001,0.0001],\n              'degree': [2,3,4,5]}  #,'linear','poly', 'sigmoid', 'precomputed'\n\nSVM_random1 = RandomizedSearchCV(SVM, param_distributions = SVM_param_random, cv = 5, n_jobs=-1)","afaca539":"SVM_random1.fit(X_trains, y_train) ","7dd3fabc":"SVM_random1.best_estimator_","964709e6":"# Best Random model\nSVM_random = svm.SVC(C=1,degree=2, gamma=0.1, kernel='rbf',tol=0.001, random_state=1)\nSVM_random.fit(X_trains, y_train)","b3fe482e":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, SVM_random.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, SVM_random.predict(X_tests)),'\\n')","0bc774d2":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(SVM_random, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(SVM_random, X_test, y_test, cv=10)))","adc57ae6":"SVM_param_grid = {'C': [0.01,0.1, 1, 10],  \n              'gamma': [0.09, 0.1, 0.2, 0.001], \n              'kernel': ['rbf'],\n              'tol':[0.001,0.0001],\n              'degree':[2,3]}\n\nSVM_grid1 = GridSearchCV(SVM, param_grid = SVM_param_grid, cv = 5, n_jobs=-1)","a67995eb":"SVM_grid1.fit(X_trains, y_train) ","da80c19c":"SVM_grid1.best_estimator_","4cd2635f":"# Best Random model\nSVM_grid = svm.SVC(C=10,degree=2, gamma=0.09, kernel='rbf',tol=0.001, random_state=1)\nSVM_grid.fit(X_trains, y_train)","9a2eee62":"# Classification Report\nprint('Classification Report of the training data:\\n\\n',metrics.classification_report(y_train, SVM_grid.predict(X_trains)),'\\n')\nprint('Classification Report of the test data:\\n\\n',metrics.classification_report(y_test, SVM_grid.predict(X_tests)),'\\n')","a999b1fb":"# Mean 10 fold cross validation scores for train and test set\nprint('Train set CV scores: %0.4f'%np.mean(cross_val_score(SVM_grid, X_train, y_train, cv=10)),'\\n')\nprint('Test set CV scores: %0.4f'%np.mean(cross_val_score(SVM_grid, X_test, y_test, cv=10)))","e702d7db":"# Prediction for final model\nSVM_train_predict = SVM.predict(X_trains)\nSVM_test_predict = SVM.predict(X_tests)","8fe4f755":"# Classification report in a dataframe of final model\nSVM_df_train=pd.DataFrame(classification_report(y_train, SVM_train_predict,output_dict=True)).transpose()\nSVM_df_test=pd.DataFrame(classification_report(y_test, SVM_test_predict,output_dict=True)).transpose()","8e643ee1":"models = {\"DT_grid\":DT_grid, \"RF_random\":RF_random, \"LR_random\":LR_random, \"NB\":NB, \"XGB\":XGB, \"SVM\":SVM}","8da08027":"model = [DT_grid, RF_random, LR_random, NB, XGB, SVM]\nmodel_name = ['Decision Tree', 'Random Forest', 'Logistic Regression', 'Naive Bayes', 'XGBoost', 'SVM']","c6de1901":"fig, axes = plt.subplots(nrows=2,ncols=6,  figsize=(25, 20))\nfig.subplots_adjust(wspace=0.8)\nidx = 0\nfor m,n in zip(model,model_name):\n  plot_confusion_matrix(m,X_trains,y_train,cmap='Greys',display_labels=['Non Pulsars','Pulsars'],values_format = '.0f', ax=axes[0][idx]);\n  axes[0][idx].set_title(\"Train of \" + n)\n  plot_confusion_matrix(m,X_tests,y_test,cmap='Greys',display_labels=['Non Pulsars','Pulsars'],values_format = '.0f', ax=axes[1][idx]);\n  axes[1][idx].set_title(\"Test of \" + n)\n  idx=idx+1","9931fa44":"# Creating a dataframe with 'Accuracy', 'AUC', 'Recall','Precision','F1 Score' values for all models\n\nindex=['Accuracy', 'AUC', 'Recall','Precision','F1 Score']\ndata = pd.DataFrame({'DT Train':[DT_df_train.loc[\"accuracy\"][0],DT_train_auc,DT_df_train.loc[\"1.0\"][1],DT_df_train.loc[\"1.0\"][0],DT_df_train.loc[\"1.0\"][2]],\n                     'DT Test':[DT_df_test.loc[\"accuracy\"][0],DT_test_auc,DT_df_test.loc[\"1.0\"][1],DT_df_test.loc[\"1.0\"][0],DT_df_test.loc[\"1.0\"][2]],\n                     'RF Train':[RF_df_train.loc[\"accuracy\"][0],RF_train_auc,RF_df_train.loc[\"1.0\"][1],RF_df_train.loc[\"1.0\"][0],RF_df_train.loc[\"1.0\"][2]],\n                     'RF Test':[RF_df_test.loc[\"accuracy\"][0],RF_test_auc,RF_df_test.loc[\"1.0\"][1],RF_df_test.loc[\"1.0\"][0],RF_df_test.loc[\"1.0\"][2]],\n                     'LR Train':[LR_df_train.loc[\"accuracy\"][0],LR_train_auc,LR_df_train.loc[\"1.0\"][1],LR_df_train.loc[\"1.0\"][0],LR_df_train.loc[\"1.0\"][2]],\n                     'LR Test':[LR_df_test.loc[\"accuracy\"][0],LR_test_auc,LR_df_test.loc[\"1.0\"][1],LR_df_test.loc[\"1.0\"][0],LR_df_test.loc[\"1.0\"][2]],\n                     'NB Train':[NB_df_train.loc[\"accuracy\"][0],NB_train_auc,NB_df_train.loc[\"1.0\"][1],NB_df_train.loc[\"1.0\"][0],NB_df_train.loc[\"1.0\"][2]],\n                     'NB Test':[NB_df_test.loc[\"accuracy\"][0],NB_test_auc,NB_df_test.loc[\"1.0\"][1],NB_df_test.loc[\"1.0\"][0],NB_df_test.loc[\"1.0\"][2]],\n                     'XGB Train':[XGB_df_train.loc[\"accuracy\"][0],XGB_train_auc,XGB_df_train.loc[\"1.0\"][1],XGB_df_train.loc[\"1.0\"][0],XGB_df_train.loc[\"1.0\"][2]],\n                     'XGB Test':[XGB_df_test.loc[\"accuracy\"][0],XGB_test_auc,XGB_df_test.loc[\"1.0\"][1],XGB_df_test.loc[\"1.0\"][0],XGB_df_test.loc[\"1.0\"][2]],\n                     'SVM Train':[SVM_df_train.loc[\"accuracy\"][0],0,SVM_df_train.loc[\"1.0\"][1],SVM_df_train.loc[\"1.0\"][0],SVM_df_train.loc[\"1.0\"][2]],\n                     'SVM Test':[SVM_df_test.loc[\"accuracy\"][0],0,SVM_df_test.loc[\"1.0\"][1],SVM_df_test.loc[\"1.0\"][0],SVM_df_test.loc[\"1.0\"][2]]\n                     },index=index)\ndata = round(data,3)","92902190":"data","a9552c47":"data1 = data.T\ndata1['Model'] = data1.index \ndata1 = data1.reset_index()","85602187":"# Accuracy of Train\nplt.figure(figsize = (15,10))\ngraph = sns.barplot(x = 'Model', y = 'Accuracy',data = data1.iloc[[0,2,4,6,8,10]])\n\nfor p in graph.patches:\n        graph.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","41d281f7":"# Accuracy of Test\nplt.figure(figsize = (15,10))\ngraph = sns.barplot(x = 'Model', y = 'Accuracy',data = data1.iloc[[1,3,5,7,9,11]])\n\nfor p in graph.patches:\n        graph.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","dd151a9c":"# F1-score of Train\nplt.figure(figsize = (15,10))\ngraph = sns.barplot(x = 'Model', y = 'F1 Score',data = data1.iloc[[0,2,4,6,8,10]])\n\nfor p in graph.patches:\n        graph.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","8e6b24df":"# F1-score of Test\nplt.figure(figsize = (15,10))\ngraph = sns.barplot(x = 'Model', y = 'F1 Score',data = data1.iloc[[1,3,5,7,9,11]])\n\nfor p in graph.patches:\n        graph.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","315f4715":"# AUC of Train\nplt.figure(figsize = (15,10))\ngraph = sns.barplot(x = 'Model', y = 'AUC',data = data1.iloc[[0,2,4,6,8]])\n\nfor p in graph.patches:\n        graph.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","bbc6e39c":"# AUC of Test\nplt.figure(figsize = (15,10))\ngraph = sns.barplot(x = 'Model', y = 'AUC',data = data1.iloc[[1,3,5,7,9]])\n\nfor p in graph.patches:\n        graph.annotate('{:.3f}'.format(p.get_height()), (p.get_x()+0.4, p.get_height()),\n                    ha='center', va='bottom',\n                    color= 'black')","71682582":"DT_train_fpr, DT_train_tpr, thresholds = metrics.roc_curve(y_train,DT_prob_train[:,1])\nDT_test_fpr, DT_test_tpr, thresholds = metrics.roc_curve(y_test,DT_prob_test[:,1])\nRF_train_fpr, RF_train_tpr, thresholds = metrics.roc_curve(y_train,RF_prob_train[:,1])\nRF_test_fpr, RF_test_tpr, thresholds = metrics.roc_curve(y_test,RF_prob_test[:,1])\nLR_train_fpr, LR_train_tpr, thresholds = metrics.roc_curve(y_train,LR_prob_train[:,1])\nLR_test_fpr, LR_test_tpr, thresholds = metrics.roc_curve(y_test,LR_prob_test[:,1])\nNB_train_fpr, NB_train_tpr, thresholds = metrics.roc_curve(y_train,NB_prob_train[:,1])\nNB_test_fpr, NB_test_tpr, thresholds = metrics.roc_curve(y_test,NB_prob_test[:,1])\nXGB_train_fpr, XGB_train_tpr, thresholds = metrics.roc_curve(y_train,XGB_prob_train[:,1])\nXGB_test_fpr, XGB_test_tpr, thresholds = metrics.roc_curve(y_test,XGB_prob_test[:,1])","a16e7d06":"plt.figure(figsize=(10,5))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(DT_train_fpr,DT_train_tpr,color='crimson',label=\"DT\")\nplt.plot(RF_train_fpr,RF_train_tpr,color='black',label=\"RF\")\nplt.plot(LR_train_fpr, LR_train_tpr,color='r',label=\"LR\")\nplt.plot(NB_train_fpr,NB_train_tpr,color='c',label=\"NB\")\nplt.plot(XGB_train_fpr,XGB_train_tpr,color='gray',label=\"XGB\")\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Training Data')\nplt.legend(loc=0)","c62c6cbb":"plt.figure(figsize=(10,5))\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.plot(DT_test_fpr,DT_test_tpr,color='crimson',label=\"DT\")\nplt.plot(RF_test_fpr,RF_test_tpr,color='black',label=\"RF\")\nplt.plot(LR_test_fpr, LR_test_tpr,color='r',label=\"LR\")\nplt.plot(NB_test_fpr,NB_test_tpr,color='c',label=\"NB\")\nplt.plot(XGB_test_fpr,XGB_test_tpr,color='gray',label=\"XGB\")\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Testing Data')\nplt.legend(loc=0)","8f1507ca":"ps_test1 = ps_test.copy()","719cf9f2":"X = ps_train1.drop('target_class', axis=1)\ny = ps_train1[['target_class']]\n\nps_test1 = imputer.fit_transform(X)\n\nps_test1 =   pd.DataFrame(data=ps_test1,columns=X.columns)","c5f12c47":"ps_test1.isnull().sum()","7c56e0f1":"for column in ps_test1.columns:\n    lr,ur=remove_outlier(ps_test1[column])\n    ps_test1[column]=np.where(ps_test1[column]>ur,ur,ps_test1[column])\n    ps_test1[column]=np.where(ps_test1[column]<lr,lr,ps_test1[column])","7597ce02":"plt.figure(figsize=(10,8))\nplt.title('Without Outliers',fontsize=16)\nps_test1.boxplot(vert=0)","08b48c73":"ps_test1_scaled = ss.transform(ps_test1)","c882d89c":"predictions = XGB.predict(ps_test1_scaled)","5b62865b":"predictions","0159f4bd":"ps_test1['Result'] = predictions","59f8ab5f":"# **Data Preprocessing**","819cb0ab":"## ROC Curve","a7fc44e1":"The model has performed well in terms of performance and the cross validation scores are also very consistent.\n\nThe performances of train and test set are exactly same as each other which also implies the model is not overfitting. We will try to tune the model and see if the recall can be improved or not.\n","63729198":"Tried few iterations with different values in grid search I was getting the same results. I guess this is the maximum performance that can be achieved for Decision Tree model. \n\n**Since the CV values and performance is best for Grid search model, it will be considered as final Decision Tree Model.**","44f0fde0":"Also, **Excess kurtosis of the DM-SNR curve** and **Skewness of the DM-SNR curve** are also highly correlated **(0.95)**, and its correlation values with other variables and even the target are almost similar. We remove the Excess kurtosis of the DM-SNR curve.","5c5fd967":"### Randomized Search CV","320f734a":"\n## Distribution of Pulsars","edbe3305":"There is no much improvement in performance for the test set.\nThough the model is not overfitting after tuning. \n\n**Since the CV values and performance time is best for Random search model, it will be considered as final Random Forest Model.**","9eb8a77e":"There is multicolliearity among the variables, we can either remove some variables or leave them as it is. Feature extraction is not desired since there are only 8 independent variables. Trial and error would be required to drop variables.\n\nWe can observe that **Mean of the DM-SNR curve** and **standard deviation of the DM-SNR curve** are highly correlated **(0.97)** and its correlation values with other variables and even the target are almost similar. Hence we can remove any 1 of them to reduce redundancy.","3e6ae001":"Out of all columns, only 3 columns had missing values and in given percentages. As confirmed from the distribution of these columns at **line 22**, they are **highly skewed and contain outliers**. So, if we are replacing missing values using statistical methods, it will be best to impute missing values using **median**.\n\nWe create a new dataframe below in which this method is implemented","1420569c":"## Shape of Datasets","b2c50f1c":"## Missing Values of Pulsars","95572f0f":"## Distribution of Data","3aef1750":"## Importing important libraries","384223f0":"**Our Approach in this case for feature selection:**\n\nIf we are interested in the effects of the predictor variables to the target and the interpretability of the model, then treating collinearity is necessary. But If we are interested about the predictive abilities of a model, then we can skip it and follow regular machine learning flows.\n\nIn this problem interpretibility is not that necessary but model performance and predictablility should be high. Keeping this in mind we can select ML models which can give high performance even though interpretibility is less, so will be continuing with all features and ignoring the correlation among variables to start with.\n\nBut to check if performance of models can be further enhanced we can remove very the above highly correlated features and check again.","673769f4":"### Base Model","ab6bc9dc":"## Five point Summary of datasets (Desription)","a9367969":"Dataset is **highly imbalanced**. There are 11375 non pulsars and 1153 pulsars with us. Dataset Might need sampling before model building process. \n\nThe most important metrics to be considered to evaluate the classification models should be F1-score and Recall since the target class is extremely less.","a4314114":"# Predict results","0f3fff2a":"## Feature Selection\n\nWe will check for multicolliearity using 2 ways:\n1. Correlation Plot\n2. VIF (Variance Inflation factor)","d01ba3db":"We can see the accuracy of train set is 100% and test set is 98%. Although difference in accuarices is less still it is an overfit model and this is not desirable. Also, the precision, recall and f1-score values are way different of train and test set. So we will try to tune the models in an attempt to converge their performances and increase the performance in terms of precision, recall and f1-score.","7c54dc75":"# **EDA of True Pulsar stars**\nAn attempt to study the attribute values of real pulsars and see if there is a trend in them","e40075f6":"### Base Model","8e43ef47":"**For Testing Data**","f3a2b518":"The model has performed well in terms of performance and the cross validation scores are also very consistent.\n\nThe performances of train and test set are consistent with each other which also implies the model is not overfitting. We will try to tune the model and see if the recall can be improved or not.\n","1196d7d3":"## Description of Non Pulsars","264129f7":"\nAlthough the missing values are less, we'll have to see how can it be treated, if nothing works then they could be dropped.","2cc22a76":"### Base Model","3c2c5117":"### Randomized Search CV","886cf7f9":"### Target Class Distribution","fcec7eca":"# **EDA of Non Pulsars**\nAn attempt to study the attribute values of non pulsars","8281717f":"#### Randomized Search CV","d8214a97":"After comparing all the above metrics, the best performing models are as follows:\n\n1. XG Boost\n2. Logistic Regression\n3. SVM\n4. Random Forest\n5. Decision Trees\n6. Naive Bayes\n\nSo, in this problem we will choose XGBoost as the final algorithm for model building.","133dcb45":"\n## Missing Value Treatment","cba24e8b":"## Description of Pulsars\n\n","7b2d54a4":"We can clearly see by comparing the F1-score values of train and test set(as it takes into account both recall and precision) that the model is not overfit anymore. \n\nThe recall values for test set have improved from 83% to 86% from base model with accuracy of 98% for both train and test sets. The test set has performed slightly better than train set.","450656f9":"## Missing Values of Non Pulsars","5dcfc2ce":"## **Logistic Regression**","3fbd6a1d":"## **Support Vector Machine**","e8c97e53":"## **Naive Bayes**","16db270e":"**Integrated Profile**\n* **Mean:** It has *no outliers*, *slightly right skewed* with mean = **56.52** and SD = **29.81**.\n* **SD:** It has *some outliers*, *right skewed* with mean = **38.7** and SD = **7.87**.\n* **Excess Kurtosis:** It has *no outliers*, *slighly right skewed* with mean = **3.12** and SD = **1.87**.\n* **Skewness:** It has *some outliers*, *right skewed* with mean = **15.56** and SD = **14.14**.\n---\n\n> Considering the population to be *normally distributed*, we can expect an intergrated profile of a pulsar star to have a mean around 56; Have high SD of around 38; be highly skewed towards right as mean skewness is expected to be around 15; and have highly pointed curve and heavy tails as mean excess Kurtosis is 3.12\n\n---\n\n**DM-SNR Curve**\n* **Mean:** It has *very few outliers*, *right skewed* with mean = **44.91** and SD = **45.13**.\n* **SD:** It has *no outliers*, *slightly right skewed* with mean = **56.92** and SD = **19.73**.\n* **Excess Kurtosis:** It has *some outliers*, *extreme right skewed* with mean = **2.78** and SD = **3.20**.\n* **Skewness:** It has *some outliers*, *extreme right skewed* with mean = **17.93** and SD = **46.92**.\n---\n\n> Considering the population to be *normally distributed*, we can expect an DM-SNR curve of a pulsar star to have a mean around 45; Have high SD of around 57; be highly skewed towards right as mean skewness is expected to be around 18; and have highly pointed curve and heavy tails as mean excess Kurtosis is 2.78\n\n---\n","9550b1eb":"## Correlation among variables","5872a3b3":"**Table of Values**\n\n|        | Pulsar     | Non Pulsar  |   Pulsar     | Non Pulsar     |\n| :------------- | :----------: | -----------: |:----------: | -----------: |\n|        | **Integrated Profile**    |     | **DM SNR Curve**     |      |\n|  Mean | 56.52   |  116.56   |44.91   | 8.90  |  \n| Standard Deviation   |38.7 |  56.92 |  116.56   |23.24   |\n| Excess Kurtosis   |3.12 |  0.2 |  2.78   |8.89  |\n| Skewness   |15.56 |  0.38 |  17.93   |114.36   |","2944f108":"### **3. Using KNN Imputer**\n\nOne of the good way to impute the NULL data is through KNN imputer which helps preserving the variablility in the data. Instead of imputing the NULL values using Mean, Median or Mode which affects the variability of data, we can use KNN Imputer to impute the data","fd0029ad":"## Distribution of Non Pulsars","a452cfe7":"# **Building Models**","debf7342":"KDE plot shows different densities for both target classes, could be useful for classifying the pulsars from non pulsars\n\n**Positive Correlation:**\n\n* Skewness and excess Kurtosis of IP\n* Skewness and excess Kurtosis of DM-SNR curve\n\n**Negative Correlation:**\n\n* Mean and SD of IP with excess Kurtosis and skewness of IP \n* Mean and SD of DM-SNR curve with excess Kurtosis and skewness of DM-SNR curve\n* Excess Kurtosis and skewness of IP with excess Kurtosis and skewness of DM-SNR curve","30e29d6f":"We can see that the **mean** and **standard deviation** values in the treated dataframe is close to the values in original dataframe. So we can consider this method for missing value treatment.","921d62d8":"\n\nThe magnitude of pulsars and non pulsars are at different scales as evident from above boxplots, indepth distribution of pulsars was covered above, the differences observed from non pulsars are as follows:\n\n**Integrated Profile**\n* **Mean:** The overall distribution of **non pulsars has** **higher magnitude around 117** as compared with **pulsars having mean around 56**\n* **SD:** The magnitude of SD is **higher for non pulsars with mean around 47** as compared with **pulsars having mean around 38**\n* **Excess Kurtosis:** It is **pretty low for non-pulsars with mean around 0.2** as compared with **pulsars having high mean kurtosis of 3.12**. It means pulsars have way pointier IP than non pulsars\n* **Skewness:** It is pretty low for **non pulsars mean around 0.38 (like normal distribution)** as compared with **pulsars having high skewness of 15**.\n\n**DM-SNR Curve**\n* **Mean:** The overall distribution of non pulsars has **very low magnitude around 8** as compared with **pulsars having mean around 56** and also has lot of outliers\n* **SD:** The magnitude of SD is also **lower for non pulsars with mean around 23** as compared with **pulsars having mean around 38**\n* **Excess Kurtosis:** It is **higher for non-pulsars with mean around 9** as compared with **pulsars having mean kurtosis of 3.12.** It means non pulsars have way pointier DM-SNR curve than pulsars\n* **Skewness:** It is also **pretty high mean around 114 for non pulsars** as compared with **pulsars having skewness of 15**. The DM-SNR curve seems to be extremely highly right skewed for non pulsars","e92e7518":"XGBoost seems to have the best ROC curve among all the models.","d1a9290c":"## **Random Forest**","1fc6340f":"# **Comparison of Pulsars and Non Pulsars**","109a8dbb":"## Missing Values","612aeb82":"## Data profile","0232f5ca":"## Confusion Matrix","c80622b7":"**Three** variables have missing values.\n\n**Train Set**","99598b75":"### Grid Search CV","25cc8140":"## Treating missing values","290dcdd3":"## Important Metrics Comparison","ab2e0b81":"**Integrated Profile**\n* **Mean:** It has *some outliers*, *normally distributed* with mean = **116.56** and SD = **17.43**.\n* **SD:** It has *some outliers*, *almost normally distributed,having right tail* with mean = **47.31** and SD = **6.15**.\n* **Excess Kurtosis:** It has *some outliers*, *almost normally distributed, having tail on both sides* with mean = **0.20** and SD = **0.33**.\n* **Skewness:** It has *some outliers*, *right skewed* with mean = **0.38** and SD = **0.98**.\n---\n\n> Considering the population to be *normally distributed*, we can expect an intergrated profile of a non pulsar star to have a mean around 117; Have high SD of around 47; not skewed as mean skewness is expected to be around 0.38; and have slightly pointed curve as mean excess Kurtosis is 0.38\n\n---\n\n**DM-SNR Curve**\n* **Mean:** It has *few outliers*, *extreme right skewed* with mean = **8.90** and SD = **24.59**.\n* **SD:** It has *few outliers*, *right skewed* with mean = **23.24** and SD = **16.71**.\n* **Excess Kurtosis:** It has *some outliers*, *slightly right skewed* with mean = **8.89** and SD = **4.26**.\n* **Skewness:** It has *some outliers*, *right skewed* with mean = **114.36** and SD = **107.81**.\n---\n\n> Considering the population to be *normally distributed*, we can expect an DM-SNR curve of a non pulsar star to have a mean around 9; Have high SD of around 23; be highly skewed towards right as mean skewness is expected to be around 114; and have highly pointed curve and heavy tails as mean excess Kurtosis is 8.89\n\n---\n","8372cea7":"### Duplicate Rows","37c1d6f3":"No duplicates present in both datasets","bec7c26f":"### **1. Using Statistical methods**","14742b82":"### Randomized Search CV","77a6b06f":"## **XGBoost**","d6158575":"**Variance Inflation Factor** \u2014 Variance inflation factor (VIF) is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. VIF value can be interpreted as\n\n1 (Non-collinear)\n\n1\u20135 (Medium collinear)\n\nMore than 5 (Highly collinear)","ff6af20a":"The dataset is not normally distributed. Apart from Mean and SD of integrated profile, all other variables are highly right skewed\n\nThe SD of integrated profile appears normal but has a tail in right side and only the mean of IP has heavy tail on left side and is thus left skewed","d613d564":"The model has performeed very well in terms of performance and the cross validation scores are also very consistent.\n\nThe performances of train and test set are consistent with each other which also implies the model is not overfitting.\n","63d17837":"All columns are of float datatype, and some columns have null values","be47d233":"**For Training Data**","aa368a81":"## Outlier Treatment ","0eb0e229":"## Reading the csv files\nSaving the train and test data as two dataframes","3b675692":"### Grid Search CV","142b873a":"## **Decision Tree**","7f925a8a":"### Base Model","06301269":"### Base Model","c0fcb2ff":"### **2. Using Linear Regression**\n\nIf we check out the correlation plot in **line 25** and pairlot in **line 24**, we see that the missing valued columns are highly and linearly positively correlated with the non missing valued columns in the following combination: (in bracket coefficient values)\n\n1.   **Excess kurtosis of the integrated profile** : Skewness of integrated profile (0.94)\n2.   **Standard deviation of the DM-SNR curve** : Mean of the DM-SNR curve (0.8)\n3.   **Skewness of the DM-SNR curve** : Excess kurtosis of DM-SNR curve(0.92)\n\nIf we know there is a correlation between the missing valued variables and other variables, we can often get better guesses by regressing the missing variable on other variables. So we will be using simple linear regression where \n\nX = column with complete data (predictor)\n\ny = column with missing data (regressor)","a6d629a6":"# Predicting a Pulsar Star\n\nThe journey of a star after its death is as interesting as when it's alive. When a star dies, it leads to a supernova explosion, leaving the core behind which goes to live on for several years, one of such cores is the Pulsar Star. \n\nPulsars are spherical, compact objects that are about the size of a large city but contain more mass than the sun. From Earth, pulsars often look like flickering stars. On and off, on and off, they seem to blink with a regular rhythm. But the light from pulsars does not actually flicker or pulse, as these objects are not actually stars! It is actually the radioactive waves emitted from these highly magnetic objects which get captured by our telescopes when it falls on it. \n\n![](https:\/\/www.ligo.org\/science\/Publication-S6VSR24KnownPulsar\/Images\/lightnew.gif)\n\nOne way to think of a pulsar is like a lighthouse. At night, a lighthouse emits a beam of light that sweeps across the sky. Even though the light is constantly shining, you only see the beam when it is pointing directly in your direction.\n\nDiscovered in 1967, Pulsars are fantastic cosmic tools for scientists to study a wide range of phenomena. Scientists watch for changes in a pulsar's blinking that could indicate something happening in the space nearby which helps in studying extreme states of matter and search for planets beyond Earth's solar system. Thanks to the exquisite timing of the pulsars pulses, scientists have made some of the most accurate distance measurements of cosmic objects. Pulsars have also been used to test aspects of Albert Einstein's theory of general relativity, such as the universal force of gravity. The possibilities are endless.\n\nBut till date only over 2,000 pulsars have been detected. With millions of random celestial readings being captured, it becomes extremely challenging to correctly identify true pulsars. This is where Data Science and Machine learning comes into picture. With the data available to us, it is our job to build various classification models and see which can most correctly differentiate pulsars from non pulsars. This will be extremely useful for scientists as well as astronomers for their celestial research.","17f61c8e":"## Information about datasets","1db1870e":"# **Comparison of Models**","53aa4fdf":"**1**: Pulsars             **0**: Non Pulsars\n\n**TP**: The predicted pulsar is actually a pulsar\n\n**TN**: The predicted non pulsar is actually a non pulsar\n\n**FP**: A true non pulsar is predicted as pulsar\n\n**FN**: A true pulsar is predicted as non pulsar\n\nThe true positive should be high, and errors should be minimum\n\n**The highest is for XGBoost, followed by Random Forest and SVM for both train and test sets**","7799007e":"The performance of tuned model is slighly better than base model. The test set has performed slightly better than train set and is not overfit anymore.\n\n**Since the CV values and performance time is best for Random search model, it will be considered as final Logistic Regression Model.**","bf14d589":"### Randomized Search CV","d3239944":"**Test Set**","735ec8e1":"# **EDA**","0c37a221":"# Splitting in Train and test set","509f6b7a":"**Correlations between 2 dependent variables**\n\nHighly positively correlated:\n\n*   **Skewness of the integrated profile** and **Excess kurtosis of the DM-SNR curve** \n*   **Skewness of the DM-SNR curve** and **Excess kurtosis of the DM-SNR curve**\n* **Mean the DM-SNR curve** and **Standard Deviation of the DM-SNR curve**\n\nHighly negatively correlated:\n\n*   **Mean of the integrated profile** and **Excess kurtosis of the integrated profile**\n*   **Mean of the integrated profile** and **Skewness of the integrated profile**\n* **Excess kurtosis the DM-SNR curve** and **Standard Deviation of the DM-SNR curve**\n\n**Correlations between independent and dependent variable**\n \n**Excess kurtosis of the integrated profile** and **Skewness of the integrated profile** have a high positive correlation with **Target_class**. ,**Mean of the integrated profile** has high negative correlation with **Target_class**.\n\nCorrelation indicates multicollinearity in data, which is undesirable during model building, needs to be treated during data preprocessing process.","cce165f3":"**Integrated Profile:**\n\n* **Mean:** Mean is around 111 which is almost close to the median value, there is a vast difference between the minimum and maximum values though and thus has large SD value of 25.6\n* **Standard deviation:** The mean SD is 46.6 close to the median and looking at min and max values it seems to be normally distributed throughout dataset\n* **Excess kurtosis:** More than 75% of data is below 0.5, low kurtosis tend to have light tails, or lack of outliers\n* **Skewness:** More than 75% of data is below 1.0, which implies most of the integrated profile might be only slightly skewed\n\n**DM-SNR curve:**\n* **Mean:** SD is pretty high, 29, with more than 75% values being below 5.6 and the max value extremely high. We can expect the mean of most curves to be on the lower side\n* **Standard deviation:** This also appears pretty skewed as maximum value is pretty high and 75% of all values are below 28, the median is 19, mean is 26, mean's higher magnitude could be a result of the extreme high values but if they were not included then we could expect the standard deviation to be around the median value\n* **Excess kurtosis:** The values are pretty high, mean being 8.23 and the max being 34, the values seem to be normally distributed\n* **Skewness:** The values are pretty high, mean being 102 and the max being 1191 which is extremely high as compared to 75th quartile","8ea1e79c":"# Preparing the test set","c4de1851":"## Pairplot","c26d90e8":"Since the amount of outliers is less, we could either remove it or cap it, but removing outliers is not good since data gets lost hence we will be **capping the outliers using IQR**\n\n*The target class is not included for outlier treatment here*","da80108f":"## Detection of Outliers","f6d25f99":"Since the data is not scaled, will will apply fit_transform on training data and transform the test data. Not applying scaling on y_train and y_test since their values are already 0 and 1.","9769f85d":"We can see the accuracy of train set is 100% and test set is 96%. Although difference in accuarices is less still it is an overfit model and this is not desirable. Also, the precision, recall and f1-score values are way different of train and test set. So we will try to tune the models in an attempt to converge their performances and increase the performance in terms of precision, recall and f1-score.","83850508":"We can clearly see by comparing the F1-score values of train and test set(as it takes into account both recall and precision) that the model is not overfit anymore. \n\nThe recall values for test set is constant at 83% as compared to base model with accuracy of 98% for both train and test sets. The test set has performed slightly better than train set.","67a67a92":"**Tuning hasn't improved the performance of models and the cross validation scores have reduced too. So to the best performing model for SVM is the base model.**","f637c3a2":"# **EDA on Training data**\nTraining sets are used for model development. Goal is to explore this data to get ideas for feature engineering and the general structure of the machine learning model as this would be used to train the model.","b5287216":"### Grid Search CV","8b2c771d":"## Outlier Treatment\n\nFrom boxplot at **line 71** we found that data is at different scales,and also has outliers in following percentages:","413994c7":"Even for true pulsar readings some missing values are present","463dfcbe":"## Scaling the data","b71dcd9e":"Mean of the integrated profile, Standard deviation of the integrated profile, Excess kurtosis of the DM-SNR curve and Skewness of the DM-SNR curve have outliers below **5%**\n\nExcess kurtosis of the integrated profile, Skewness of the integrated profile and Standard deviation of the DM-SNR curve  have outliers between **5%** and **10%**\n\nMean of the DM-SNR curve has **11.43%** of outliers\n\nAlthough numerical, data appears to be on different scales. Requires scaling before building model.","13502743":"The model performance has not improved as compared to base model. Also it was taking a lot of execution time too.\n\n**So to save time, resources and get better performance the base model will be considered as final model for xgboost.**","a3bf8933":"### Base Model","a91dfa08":"* Target is NaN in test set as it is to be predicted.\n* Each column is numerical in nature\n* Null and negative values visible in data\n\n","0abdd097":"We took the range of values as specified above. Used CV k value as 5.","87ea9cff":"### Grid Search CV","52857c3a":"We can see that the **mean** and **standard deviation** values in the treated dataframe are almost same with the values in original dataframe for desired columns.It has performed exceptionally well than median imputation method."}}