{"cell_type":{"7656dcba":"code","53677aa6":"code","0ed97482":"code","2026545e":"code","75b7798b":"code","fa3c6e76":"code","01707078":"code","278d7313":"code","ed178a21":"code","ab590dbd":"code","4477f9a6":"code","6d1973f5":"code","a4fcfa63":"code","27ec1c96":"code","600e7ebe":"code","0cfeb6a6":"code","bac186cc":"code","b31672c7":"code","98d571e9":"code","b3e4a076":"markdown","cf4b42ac":"markdown","298a9708":"markdown","3760548a":"markdown","6290b332":"markdown","7d755304":"markdown","4fb5e8c2":"markdown","7e27c7d2":"markdown","3f9a9bc8":"markdown","b223e373":"markdown","67c6ac52":"markdown","36281972":"markdown","11ea2a49":"markdown","d6a38576":"markdown","bdd44a49":"markdown","3c6adecc":"markdown"},"source":{"7656dcba":"import pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom collections import Counter, namedtuple\n\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.optimizers import Nadam,adam\n\nnp.random.seed(1)","53677aa6":"data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","0ed97482":"data.head()","2026545e":"data.drop(columns = ['id','keyword','location'], inplace=True)\nneg, pos = np.bincount(data.target)\nprint(f'Total: {len(data)} \\nPositive: {pos} \\nNegative: {neg}')","75b7798b":"data.isnull().sum()","fa3c6e76":"def clean_text(text):\n    \n    #remove urls\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = url_pattern.sub(r'', text)\n    \n    #remove html\n    html_pattern = re.compile(r'<.*?>')\n    text = html_pattern.sub(r'', text)\n    \n    #remove emojis\n    emoji_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'',text)\n    \n    #remove punctuations\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    text = text.translate(table)\n    \n    #remove stopwords\n    stop = set(stopwords.words('english'))\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n    return ' '.join(text)","01707078":"data['text'] = data['text'].apply(lambda x: clean_text(x))","278d7313":"data.head()","ed178a21":"def word_counter(text):  \n    \n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count    \n\ntext = data['text']\ncounter = word_counter(text)\n\nvocab_size = len(counter)","ab590dbd":"max_len = 20","4477f9a6":"t = Tokenizer(num_words = vocab_size)\nt.fit_on_texts(data['text'])\n\nword_index = t.word_index\n\ndict(list(word_index.items())[:10])","6d1973f5":"df = data[:7500]","a4fcfa63":"model = Sequential()\nmodel.add(Embedding(vocab_size, 200, input_length = max_len))\nmodel.add(LSTM(80))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nnadam = Nadam(learning_rate=0.0001)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer=nadam, metrics=['accuracy'])","27ec1c96":"model.summary()","600e7ebe":"skf = StratifiedKFold(n_splits=5)\nX = df['text']\ny = df['target']","0cfeb6a6":"accuracy = []\n# train model on 5 folds\nfor train_index, test_index in skf.split(X, y):\n    \n    train_x, test_x = X[train_index], X[test_index]\n    train_y, test_y = y[train_index], y[test_index]\n    print(\"Tweet before tokenization: \", train_x.iloc[0])\n    \n    #Tokenize the tweets using tokenizer.\n    train_tweets = t.texts_to_sequences(train_x)\n    test_tweets = t.texts_to_sequences(test_x)\n    print(\"Tweet after tokenization: \", train_tweets[0])\n    \n    #pad the tokenized tweet data\n    train_tweets_padded = pad_sequences(train_tweets, maxlen=max_len, padding='post', truncating='post')\n    test_tweets_padded = pad_sequences(test_tweets, maxlen=max_len, padding='post', truncating='post')\n    print('Tweet after padding: ', train_tweets_padded[0])\n    \n    #train model on processed tweets\n    history = model.fit(train_tweets_padded, train_y, epochs=5, validation_data = (test_tweets_padded,test_y))\n    \n    #make predictions\n    pred_y = model.predict_classes(test_tweets_padded)\n    print(\"Validation accuracy : \",accuracy_score(pred_y, test_y))\n    \n    #store validation accuracy\n    accuracy.append(accuracy_score(pred_y, test_y))","bac186cc":"print(\"Validation accuracy of the model :\", np.mean(accuracy))","b31672c7":"test_df = data[7501:]\n\ntokenized_tweets = t.texts_to_sequences(test_df['text'])\npadded_tweets = pad_sequences(tokenized_tweets, maxlen=max_len, padding='post', truncating='post')\ntest_y = test_df['target']\npred_y = model.predict_classes(padded_tweets)","98d571e9":"accuracy_score(pred_y, test_y)","b3e4a076":"We acheived 92% test accuracy!!\ud83c\udf89","cf4b42ac":"This kernel demonstrates binary classification of tweets using Sequence model.\n\n\nLet's start with importing all the necessary packages.","298a9708":"let's see how data looks like","3760548a":"References:\n\nhttps:\/\/machinelearningmastery.com\/sequence-classification-lstm-recurrent-neural-networks-python-keras\/\n\nhttps:\/\/www.youtube.com\/watch?v=j7EB7yeySDw","6290b332":"### Let's work with tweets\n\nClean the text by removing urls, html tags, emojis and stopwords.","7d755304":"Let's take initial 7500 examples for training and validation, remaining for testing.","4fb5e8c2":"We cannot directly use textual data as input to our sequence model. We need to map each word in the tweet to an integer. We can then use Embedding layer of keras to vector encode the words.\n\nLet's find the vocabulary size first.","7e27c7d2":"To map the words to unique integer values, we will be using keras Tokenizer.\n\nKeras Tokenizer can be used to get the sequence for each tweet. It maps each word to an integer, representing an index of that word in word_index list.","3f9a9bc8":"We need to have a fixed sized input for the model, here I am using maximum length as 20. Try with different values to find the best one. Usually a smaller value is recommended since it makes the input less sparse when padded with zeros.","b223e373":"Check for null values in data.","67c6ac52":"There is no class imbalance problem.","36281972":"### Let's build a sequential model using keras.\n","11ea2a49":"We will use this tokenizer later on train and test tweets.","d6a38576":"Our model is trained with validation accuracy of 91%, let's see how it performs on unseen tweets from test data.","bdd44a49":"#### Load data","3c6adecc":"Check for class imbalance"}}