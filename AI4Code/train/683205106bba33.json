{"cell_type":{"499c383b":"code","2e5e693b":"code","6fd123fc":"code","4493ed24":"code","078d6477":"code","a0a2d01f":"code","36477729":"code","06a0116c":"code","7acc7e56":"code","5793e1a2":"code","c3bd872a":"code","3e54dc33":"code","5aad855e":"code","501ff02e":"code","3d1c3a1d":"code","d52fe932":"code","0463129f":"code","2715d2e9":"code","9ff25850":"code","8bc35fca":"code","e3622e74":"code","c27705d1":"code","f743a3f6":"code","da55f4c4":"code","45e1ba97":"code","6c2fee06":"code","9abc556b":"code","46de60d8":"code","2d64086e":"code","c663c8f0":"code","11571ae3":"code","aff0922c":"code","ab6ce797":"code","68f05601":"code","04035924":"code","5230bd03":"code","94a19d0f":"code","2db308f8":"code","40a9f63f":"code","697c9d0d":"code","fc79c084":"code","7287edf9":"code","168b531d":"code","7490b3a5":"code","cd371dca":"code","e4dc238d":"code","f1828cdc":"code","4685e2ef":"code","6e987b4f":"code","6310d0b3":"code","1af679f5":"code","5436da8b":"code","7a4ae709":"code","7312f738":"code","98918b51":"code","19ecf61f":"code","500869f9":"code","863cbfea":"code","271db9c0":"code","cdaa1dab":"code","63faaa66":"code","73586a04":"code","820fa969":"code","caa33111":"code","8de08e1a":"code","6aa402ef":"code","d44da67a":"markdown","f0ff4cc2":"markdown","2ac6a130":"markdown","20c97b4f":"markdown","14e7b39e":"markdown","c442d13d":"markdown","decfd6ae":"markdown","da8bc56d":"markdown","1650aba3":"markdown"},"source":{"499c383b":"import os, gc, pickle, copy, datetime, warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\nwarnings.filterwarnings('ignore')","2e5e693b":"df_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\nprint(df_train.shape)\ndf_train.head()","6fd123fc":"df_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\nprint(df_test.shape)\ndf_test.head()","4493ed24":"# concat train and test\ndf_traintest = pd.concat([df_train, df_test])\nprint(df_train.shape, df_test.shape, df_traintest.shape)","078d6477":"# process date\ndf_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\ndf_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\ndf_traintest.head()","a0a2d01f":"day_before_valid = 7+85-7 # 3-18 day  before of validation\nday_before_public = 7+85 # 3-25, the day before public LB period\nday_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # last day of train\nprint(df_traintest['Date'][df_traintest['day']==day_before_valid].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_public].values[0])\nprint(df_traintest['Date'][df_traintest['day']==day_before_private].values[0])","36477729":"# concat Country\/Region and Province\/State\ndef func(x):\n    try:\n        x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n    except:\n        x_new = x['Country_Region']\n    return x_new\n        \ndf_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\ndf_traintest.head()","06a0116c":"df_traintest[(df_traintest['day']>=day_before_public-3) & (df_traintest['place_id']=='China\/Hubei')].head()","7acc7e56":"# concat lat and long\ndf_latlong = pd.read_csv(\"..\/input\/smokingstats\/df_Latlong.csv\")\ndf_latlong.head()","5793e1a2":"# concat Country\/Region and Province\/State\ndef func(x):\n    try:\n        x_new = x['Country\/Region'] + \"\/\" + x['Province\/State']\n    except:\n        x_new = x['Country\/Region']\n    return x_new\n        \ndf_latlong['place_id'] = df_latlong.apply(lambda x: func(x), axis=1)\ndf_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\ndf_latlong.head()","c3bd872a":"df_traintest = pd.merge(df_traintest, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\ndf_traintest.head()","3e54dc33":"# count the places with no Lat and Long.\ntmp = np.sort(df_traintest['place_id'][pd.isna(df_traintest['Lat'])].unique())\nprint(len(tmp)) # count Nan\ntmp","5aad855e":"# get place list\nplaces = np.sort(df_traintest['place_id'].unique())\nprint(len(places))","501ff02e":"# calc cases, fatalities per day\ndf_traintest2 = copy.deepcopy(df_traintest)\ndf_traintest2['cases\/day'] = 0\ndf_traintest2['fatal\/day'] = 0\ntmp_list = np.zeros(len(df_traintest2))\nfor place in places:\n    tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['cases\/day'][df_traintest2['place_id']==place] = tmp\n    tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n    tmp[1:] -= tmp[:-1]\n    df_traintest2['fatal\/day'][df_traintest2['place_id']==place] = tmp\nprint(df_traintest2.shape)\ndf_traintest2[df_traintest2['place_id']=='China\/Hubei'].head()","3d1c3a1d":"# aggregate cases and fatalities\ndef do_aggregation(df, col, mean_range):\n    df_new = copy.deepcopy(df)\n    col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n    df_new[col_new] = 0\n    tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n    df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n    df_new[col_new][pd.isna(df_new[col_new])] = 0\n    return df_new[[col_new]].reset_index(drop=True)\n\ndef do_aggregations(df):\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'cases\/day', [15,21]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14]).reset_index(drop=True)], axis=1)\n    df = pd.concat([df, do_aggregation(df, 'fatal\/day', [15,21]).reset_index(drop=True)], axis=1)\n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}cases'.format(threshold)] = tmp\n            \n    for threshold in [1, 10, 100]:\n        days_under_threshold = (df['Fatalities']<threshold).sum()\n        tmp = df['day'].values - 22 - days_under_threshold\n        tmp[tmp<=0] = 0\n        df['days_since_{}fatal'.format(threshold)] = tmp\n    \n    # process China\/Hubei\n    if df['place_id'][0]=='China\/Hubei':\n        df['days_since_1cases'] += 35 # 2019\/12\/8\n        df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n        df['days_since_100cases'] += 4 # 2020\/1\/18\n        df['days_since_1fatal'] += 13 # 2020\/1\/9\n    return df","d52fe932":"df_traintest3 = []\nfor place in places[:]:\n    df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n    df_tmp = do_aggregations(df_tmp)\n    df_traintest3.append(df_tmp)\ndf_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\ndf_traintest3[df_traintest3['place_id']=='China\/Hubei'].head()","0463129f":"# add Smoking rate per country\n# data of smoking rate is obtained from https:\/\/ourworldindata.org\/smoking\ndf_smoking = pd.read_csv(\"..\/input\/smokingstats\/share-of-adults-who-smoke.csv\")\nprint(np.sort(df_smoking['Entity'].unique())[:10])\ndf_smoking.head()","2715d2e9":"# extract newest data\ndf_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\ndf_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\ndf_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\ndf_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\ndf_smoking_recent.head()","9ff25850":"# merge\ndf_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\nprint(df_traintest4.shape)\ndf_traintest4.head()","8bc35fca":"# fill na with world smoking rate\nSmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\nprint(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\ndf_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\ndf_traintest4.head()","e3622e74":"# add data from World Economic Outlook Database\n# https:\/\/www.imf.org\/external\/pubs\/ft\/weo\/2017\/01\/weodata\/index.aspx\ndf_weo = pd.read_csv(\"..\/input\/smokingstats\/WEO.csv\")\ndf_weo.head()","c27705d1":"print(df_weo['Subject Descriptor'].unique())","f743a3f6":"subs  = df_weo['Subject Descriptor'].unique()[:-1]\ndf_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\nfor sub in subs[:]:\n    df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n    df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n    df_tmp.columns = ['Country', sub]\n    df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\ndf_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\ndf_weo_agg.columns\ndf_weo_agg['Country_Region'] = df_weo_agg['Country']\ndf_weo_agg.head()","da55f4c4":"# merge\ndf_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\nprint(df_traintest5.shape)\ndf_traintest5.head()","45e1ba97":"# add Life expectancy\n# Life expectancy at birth obtained from http:\/\/hdr.undp.org\/en\/data\ndf_life = pd.read_csv(\"..\/input\/smokingstats\/Life expectancy at birth.csv\")\ntmp = df_life.iloc[:,1].values.tolist()\ndf_life = df_life[['Country', '2018']]\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\n    \ndf_life['2018'] = df_life['2018'].apply(lambda x: func(x))\ndf_life.head()","6c2fee06":"df_life = df_life[['Country', '2018']]\ndf_life.columns = ['Country_Region', 'LifeExpectancy']","9abc556b":"# merge\ndf_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\nprint(len(df_traintest6))\ndf_traintest6.head()","46de60d8":"# add additional info from countryinfo dataset\ndf_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\ndf_country.head()","2d64086e":"df_country['Country_Region'] = df_country['country']\ndf_country = df_country[df_country['country'].duplicated()==False]","c663c8f0":"print(df_country[df_country['country'].duplicated()].shape)","11571ae3":"df_country[df_country['country'].duplicated()]","aff0922c":"df_traintest7 = pd.merge(df_traintest6, \n                         df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                         on=['Country_Region',], how='left')\nprint(df_traintest7.shape)\ndf_traintest7.head()","ab6ce797":"def encode_label(df, col, freq_limit=0):\n    df[col][pd.isna(df[col])] = 'nan'\n    tmp = df[col].value_counts()\n    cols = tmp.index.values\n    freq = tmp.values\n    num_cols = (freq>=freq_limit).sum()\n    print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n    col_new = '{}_le'.format(col)\n    df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n    for i, item in enumerate(cols[:num_cols]):\n        df_new[col_new][df[col]==item] = i\n\n    return df_new\n\ndef get_df_le(df, col_index, col_cat):\n    df_new = df[[col_index]]\n    for col in col_cat:\n        df_tmp = encode_label(df, col)\n        df_new = pd.concat([df_new, df_tmp], axis=1)\n    return df_new\n\ndf_traintest7['id'] = np.arange(len(df_traintest7))\ndf_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\ndf_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')","68f05601":"df_traintest8['cases\/day'] = df_traintest8['cases\/day'].astype(np.float)\ndf_traintest8['fatal\/day'] = df_traintest8['fatal\/day'].astype(np.float)","04035924":"# covert object type to float\ndef func(x):\n    x_new = 0\n    try:\n        x_new = float(x.replace(\",\", \"\"))\n    except:\n#         print(x)\n        x_new = np.nan\n    return x_new\ncols = [\n    'Gross_domestic_product__constant_prices', \n    'Gross_domestic_product__current_prices', \n    'Gross_domestic_product__deflator', \n    'Gross_domestic_product_per_capita__constant_prices', \n    'Gross_domestic_product_per_capita__current_prices', \n    'Output_gap_in_percent_of_potential_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n    'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n    'Implied_PPP_conversion_rate', 'Total_investment', \n    'Gross_national_savings', 'Inflation__average_consumer_prices', \n    'Inflation__end_of_period_consumer_prices', \n    'Six_month_London_interbank_offered_rate__LIBOR_', \n    'Volume_of_imports_of_goods_and_services', \n    'Volume_of_Imports_of_goods', \n    'Volume_of_exports_of_goods_and_services', \n    'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n    'General_government_revenue', 'General_government_total_expenditure', \n    'General_government_net_lending_borrowing', 'General_government_structural_balance', \n    'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n    'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n    'Current_account_balance', 'pop'\n]\nfor col in cols:\n    df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \nprint(df_traintest8['pop'].dtype)","5230bd03":"df_traintest8[df_traintest8['place_id']=='China\/Hubei'].head()","94a19d0f":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","2db308f8":"# train model to predict fatalities\/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }\n","40a9f63f":"# train model to predict fatalities\/day\n# features are selected manually based on valid score\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n#     'cases\/day_(8-14)',  \n#     'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n    'fatal\/day_(1-7)', \n    'fatal\/day_(8-14)', \n    'fatal\/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","697c9d0d":"y_true = df_valid['fatal\/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))","fc79c084":"# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp","7287edf9":"# train with all data before public\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","168b531d":"# train model to predict fatalities\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n    'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n    'cases\/day_(8-14)',  \n    'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n#     'fatal\/day_(1-7)', \n#     'fatal\/day_(8-14)', \n#     'fatal\/day_(15-21)', \n#     'recov\/day_(1-1)', 'recov\/day_(1-7)', \n#     'recov\/day_(8-14)',  'recov\/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","7490b3a5":"y_true = df_valid['cases\/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n","cd371dca":"# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var2\ntmp[\"importance\"] = model2.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\ntmp","e4dc238d":"df_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","f1828cdc":"# train model to predict fatalities\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration","4685e2ef":"# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","6e987b4f":"# train model to predict cases\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration","6310d0b3":"# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)","1af679f5":"# remove overlap for public LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_public)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_public<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\ndf_traintest9[df_traintest9['day']>day_before_public-2].head()","5436da8b":"# remove overlap for private LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_private)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_private<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\ndf_traintest10[df_traintest10['day']>day_before_private-2].head()","7a4ae709":"# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model.predict(X_valid)\n        pred_c = model2.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds)","7312f738":"# predict test data in public\ndf_preds_pri = []\nfor i, place in enumerate(places[:]):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri)","98918b51":"places_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['day']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]","19ecf61f":"print(\"Fatalities \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","500869f9":"print(\"Confirmed Cases \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","863cbfea":"print(\"Fatalities \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","271db9c0":"print(\"ConfirmedCases \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()","cdaa1dab":"# merge 2 preds\ndf_preds[df_preds['day']>day_before_private] = df_preds_pri[df_preds['day']>day_before_private]","63faaa66":"df_preds.to_csv(\"df_preds.csv\", index=None)","73586a04":"# load sample submission\ndf_sub = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","820fa969":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)","caa33111":"# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n#df_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head(10)","8de08e1a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\n%matplotlib inline\ndpi = 96\nplt.rcParams['figure.figsize'] = (1600\/dpi, 600\/dpi)\nplt.style.use('ggplot')\n\n# grabbing prepared dataset from https:\/\/www.kaggle.com\/jorijnsmit\/population-and-sub-continent-for-every-entity\ncovid = pd.read_csv('..\/input\/covid19\/covid.csv', parse_dates=['date'])\n\n# perform same manipulations from the prepared dataset to the test set\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv', parse_dates=['Date'])\ntest.columns = ['id', 'province_state', 'country_region', 'date']\ntest['country_region'].update(test['country_region'].str.replace('Georgia', 'Sakartvelo'))\ntest['entity'] = test['province_state'].where(~test['province_state'].isna(), test['country_region'])\ntest = test.set_index('id')[['date', 'entity']]\n\ndef logistic(t, k, r, a):\n    \"\"\"k > 0: final epidemic size\n    r > 0: infection rate\n    a = (k - c_0) \/ c_0\n    \"\"\"\n    \n    return k \/ (1 + a * np.exp(-r * t))\n\ndef solve(c):\n    \"\"\"port from https:\/\/mathworks.com\/matlabcentral\/fileexchange\/74411-fitvirus\"\"\"\n    \n    n = len(c)\n    nmax = max(1, n \/\/ 2)\n\n    for i in np.arange(1, nmax+1):\n        k1 = i\n        k3 = n - 1\n        if (n - i) % 2 == 0:\n            k3 -= 1\n\n        k2 = (k1 + k3) \/\/ 2\n        m = k2 - k1 - 1\n\n        if k1 < 1 or k2 < 1 or k3 < 1 or m < 1:\n            return None\n\n        k1 -= 1\n        k2 -= 1\n        k3 -= 1\n\n        # calculate k\n        v = c[k1] * c[k2] - 2 * c[k1] * c[k3] + c[k2] * c[k3]\n        if v <= 0:\n            continue\n        w = c[k2]**2 - c[k3] * c[k1]\n        if w <= 0:\n            continue\n        k = c[k2] * v \/ w\n        if k <= 0:\n            continue\n\n        # calculate r\n        x = c[k3] * (c[k2] - c[k1])\n        if x <= 0:\n            continue\n        y = c[k1] * (c[k3] - c[k2])\n        if y <= 0:\n            continue\n        r = (1 \/ m) * np.log(x \/ y)\n        if r <= 0:\n            continue\n\n        # calculate a\n        z = ((c[k3] - c[k2]) * (c[k2] - c[k1])) \/ w\n        if z <= 0:\n            continue\n        a = z * (x \/ y) ** ((k3 + 1 - m) \/ m)\n        if a <= 0:\n            continue\n        \n        return k, r, a\n\ndef plot_fit(x_train, y_train, x_predict, y_predict, r2):\n    fig, ax = plt.subplots()\n    ax.set_title(f'{subject} {r2}')\n    color = 'green' if r2 > 0.99 else 'red'\n    pd.Series(y_train, x_train).plot(subplots=True, style='.', color='black', legend=True, label='train')\n    pd.Series(y_predict, x_predict).plot(subplots=True, style=':', color=color, legend=True, label='predict')\n    plt.show()\n\nherd_immunity = 0.7\ntest_ratio = 0.2\n\nfor target in ['confirmed', 'fatal']:\n    for subject in tqdm(covid['entity'].unique()):\n        population = covid[covid['entity'] == subject]['population'].max()\n\n        x_train = covid[covid['entity'] == subject]['date'].dt.dayofyear.values\n        y_train = covid[covid['entity'] == subject][target].values\n\n        mask = y_train > 0\n        x_train_m = x_train[mask]\n        y_train_m = y_train[mask]\n        \n        # no point in modelling a single point or no ints at all\n        if x_train_m.size < 2 or x_train_m.sum() == 0:\n            continue\n\n        x_predict = test[test['entity'] == subject]['date'].dt.dayofyear.values\n        submission_size = x_predict.size\n        # start calculating sigmoid at same point x_train_m starts\n        x_predict = np.arange(start=x_train_m[0], stop=x_predict[-1]+1)\n\n        params = solve(y_train_m)\n\n        if params != None:\n        #try:\n            params = (max(params[0], max(y_train_m)), params[1], params[2])\n            lower_bounds = (max(y_train_m), 0, 0)\n            upper_bounds = (max(population * herd_immunity * test_ratio, params[0]), np.inf, np.inf)\n\n            params, _ = curve_fit(\n                logistic,\n                np.arange(x_train_m.size),\n                y_train_m,\n                p0=params,\n                bounds=(lower_bounds, upper_bounds),\n                maxfev=100000\n            )\n\n            y_eval = logistic(np.arange(x_train_m.size), params[0], params[1], params[2])\n            y_predict = logistic(np.arange(x_predict.size), params[0], params[1], params[2])\n\n            r2 = r2_score(y_train_m, y_eval)\n            covid.loc[covid['entity'] == subject, f'log_{target}'] = r2\n\n        else:\n            # we fit a polynomial instead\n            # while forcing cumulative behaviour, i.e. never lower numbers\n            # it's ugly\n            # i know\n\n            model = linear_model.LinearRegression()\n#             model = Pipeline([\n#                 (\"polynomial_features\", PolynomialFeatures(degree=2)), \n#                 (\"linear_regression\", linear_model.Ridge())\n#             ])\n            if target == 'fatal':\n                # pass more features; including confirmed!\n                pass\n            model.fit(x_train_m.reshape(-1, 1), y_train_m)\n\n            y_eval = model.predict(x_train_m.reshape(-1, 1))\n            y_predict = model.predict(x_predict.reshape(-1, 1))\n            y_predict = np.maximum.accumulate(y_predict)\n\n            r2 = r2_score(y_train_m, y_eval)\n            covid.loc[covid['entity'] == subject, f'poly_{target}'] = r2\n\n        if target == 'confirmed' and subject in ['Hubei', 'Italy', 'New York']:\n            plot_fit(x_train, y_train, x_predict, y_predict, r2)\n\n        # assign the prediction to the test dataframe\n        delta = submission_size - y_predict.size\n        if delta > 0:\n            filler = [100] * delta if target == 'confirmed' else [1] * delta\n            y_predict = filler + y_predict.tolist()\n        test.loc[test['entity'] == subject, target] = y_predict[-submission_size:]\n\n# resulting R2 scores for logistic approach\nfor target in ['confirmed', 'fatal']:\n    r2s = covid.groupby('entity')[f'log_{target}'].max()\n    print(r2s.describe())\n    print(r2s[r2s.isna()].index)\n\n# any doubtful maxima due to regression?\nfor target in ['confirmed', 'fatal']:\n    df = []\n    for subject in covid.loc[covid[f'poly_{target}'].isna()]['entity'].unique():\n        df.append(test[test['entity'] == subject][['entity', target]].max().to_dict())\n    df = pd.DataFrame(df).set_index('entity')\n    print(df[target].sort_values(ascending=False).fillna(0).astype('int').head(10))\n    \n# @TODO\n# some are way too high; this is a problem!\n# what are the parameters they are fitted on?\n\n# sanity check before submitting\nsubmission = test[['entity', 'date']].copy()\nsubmission[['confirmed', 'fatal']] = test[['confirmed', 'fatal']].fillna(0).astype('int')\nsubmission[submission['entity'] == 'Netherlands']\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\n%matplotlib inline\ndpi = 96\nplt.rcParams['figure.figsize'] = (1600\/dpi, 600\/dpi)\nplt.style.use('ggplot')\n\n# grabbing prepared dataset from https:\/\/www.kaggle.com\/jorijnsmit\/population-and-sub-continent-for-every-entity\ncovid = pd.read_csv('..\/input\/covid19\/covid.csv', parse_dates=['date'])\n\n# perform same manipulations from the prepared dataset to the test set\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv', parse_dates=['Date'])\ntest.columns = ['id', 'province_state', 'country_region', 'date']\ntest['country_region'].update(test['country_region'].str.replace('Georgia', 'Sakartvelo'))\ntest['entity'] = test['province_state'].where(~test['province_state'].isna(), test['country_region'])\ntest = test.set_index('id')[['date', 'entity']]\n\ndef logistic(t, k, r, a):\n    \"\"\"k > 0: final epidemic size\n    r > 0: infection rate\n    a = (k - c_0) \/ c_0\n    \"\"\"\n    \n    return k \/ (1 + a * np.exp(-r * t))\n\ndef solve(c):\n    \"\"\"port from https:\/\/mathworks.com\/matlabcentral\/fileexchange\/74411-fitvirus\"\"\"\n    \n    n = len(c)\n    nmax = max(1, n \/\/ 2)\n\n    for i in np.arange(1, nmax+1):\n        k1 = i\n        k3 = n - 1\n        if (n - i) % 2 == 0:\n            k3 -= 1\n\n        k2 = (k1 + k3) \/\/ 2\n        m = k2 - k1 - 1\n\n        if k1 < 1 or k2 < 1 or k3 < 1 or m < 1:\n            return None\n\n        k1 -= 1\n        k2 -= 1\n        k3 -= 1\n\n        # calculate k\n        v = c[k1] * c[k2] - 2 * c[k1] * c[k3] + c[k2] * c[k3]\n        if v <= 0:\n            continue\n        w = c[k2]**2 - c[k3] * c[k1]\n        if w <= 0:\n            continue\n        k = c[k2] * v \/ w\n        if k <= 0:\n            continue\n\n        # calculate r\n        x = c[k3] * (c[k2] - c[k1])\n        if x <= 0:\n            continue\n        y = c[k1] * (c[k3] - c[k2])\n        if y <= 0:\n            continue\n        r = (1 \/ m) * np.log(x \/ y)\n        if r <= 0:\n            continue\n\n        # calculate a\n        z = ((c[k3] - c[k2]) * (c[k2] - c[k1])) \/ w\n        if z <= 0:\n            continue\n        a = z * (x \/ y) ** ((k3 + 1 - m) \/ m)\n        if a <= 0:\n            continue\n        \n        return k, r, a\n\ndef plot_fit(x_train, y_train, x_predict, y_predict, r2):\n    fig, ax = plt.subplots()\n    ax.set_title(f'{subject} {r2}')\n    color = 'green' if r2 > 0.99 else 'red'\n    pd.Series(y_train, x_train).plot(subplots=True, style='.', color='black', legend=True, label='train')\n    pd.Series(y_predict, x_predict).plot(subplots=True, style=':', color=color, legend=True, label='predict')\n    plt.show()\n\nherd_immunity = 0.7\ntest_ratio = 0.2\n\nfor target in ['confirmed', 'fatal']:\n    for subject in tqdm(covid['entity'].unique()):\n        population = covid[covid['entity'] == subject]['population'].max()\n\n        x_train = covid[covid['entity'] == subject]['date'].dt.dayofyear.values\n        y_train = covid[covid['entity'] == subject][target].values\n\n        mask = y_train > 0\n        x_train_m = x_train[mask]\n        y_train_m = y_train[mask]\n        \n        # no point in modelling a single point or no ints at all\n        if x_train_m.size < 2 or x_train_m.sum() == 0:\n            continue\n\n        x_predict = test[test['entity'] == subject]['date'].dt.dayofyear.values\n        submission_size = x_predict.size\n        # start calculating sigmoid at same point x_train_m starts\n        x_predict = np.arange(start=x_train_m[0], stop=x_predict[-1]+1)\n\n        params = solve(y_train_m)\n\n        if params != None:\n        #try:\n            params = (max(params[0], max(y_train_m)), params[1], params[2])\n            lower_bounds = (max(y_train_m), 0, 0)\n            upper_bounds = (max(population * herd_immunity * test_ratio, params[0]), np.inf, np.inf)\n\n            params, _ = curve_fit(\n                logistic,\n                np.arange(x_train_m.size),\n                y_train_m,\n                p0=params,\n                bounds=(lower_bounds, upper_bounds),\n                maxfev=100000\n            )\n\n            y_eval = logistic(np.arange(x_train_m.size), params[0], params[1], params[2])\n            y_predict = logistic(np.arange(x_predict.size), params[0], params[1], params[2])\n\n            r2 = r2_score(y_train_m, y_eval)\n            covid.loc[covid['entity'] == subject, f'log_{target}'] = r2\n\n        else:\n            # we fit a polynomial instead\n            # while forcing cumulative behaviour, i.e. never lower numbers\n            # it's ugly\n            # i know\n\n            model = linear_model.LinearRegression()\n#             model = Pipeline([\n#                 (\"polynomial_features\", PolynomialFeatures(degree=2)), \n#                 (\"linear_regression\", linear_model.Ridge())\n#             ])\n            if target == 'fatal':\n                # pass more features; including confirmed!\n                pass\n            model.fit(x_train_m.reshape(-1, 1), y_train_m)\n\n            y_eval = model.predict(x_train_m.reshape(-1, 1))\n            y_predict = model.predict(x_predict.reshape(-1, 1))\n            y_predict = np.maximum.accumulate(y_predict)\n\n            r2 = r2_score(y_train_m, y_eval)\n            covid.loc[covid['entity'] == subject, f'poly_{target}'] = r2\n\n        if target == 'confirmed' and subject in ['Hubei', 'Italy', 'New York']:\n            plot_fit(x_train, y_train, x_predict, y_predict, r2)\n\n        # assign the prediction to the test dataframe\n        delta = submission_size - y_predict.size\n        if delta > 0:\n            filler = [100] * delta if target == 'confirmed' else [1] * delta\n            y_predict = filler + y_predict.tolist()\n        test.loc[test['entity'] == subject, target] = y_predict[-submission_size:]\n\n# resulting R2 scores for logistic approach\nfor target in ['confirmed', 'fatal']:\n    r2s = covid.groupby('entity')[f'log_{target}'].max()\n    print(r2s.describe())\n    print(r2s[r2s.isna()].index)\n\n# any doubtful maxima due to regression?\nfor target in ['confirmed', 'fatal']:\n    df = []\n    for subject in covid.loc[covid[f'poly_{target}'].isna()]['entity'].unique():\n        df.append(test[test['entity'] == subject][['entity', target]].max().to_dict())\n    df = pd.DataFrame(df).set_index('entity')\n    print(df[target].sort_values(ascending=False).fillna(0).astype('int').head(10))\n    \n# @TODO\n# some are way too high; this is a problem!\n# what are the parameters they are fitted on?\n\n# sanity check before submitting\nsubmission = test[['entity', 'date']].copy()\nsubmission[['confirmed', 'fatal']] = test[['confirmed', 'fatal']].fillna(0).astype('int')\nsubmission[submission['entity'] == 'Netherlands']","6aa402ef":"submission = submission[['confirmed', 'fatal']]\nsubmission.index.name = 'ForecastId'\nsubmission.columns = ['ConfirmedCases', 'Fatalities']\n\nsubmission['ConfirmedCases'] = ((df_sub['ConfirmedCases'].values + submission['ConfirmedCases'].values)\/2).astype(int)\nsubmission['Fatalities'] = ((df_sub['Fatalities'].values + submission['Fatalities'].values)\/2).astype(int)\n\nsubmission.to_csv('submission.csv')","d44da67a":"# Prediction","f0ff4cc2":"# Model training\nhere I train an LGBM model. The target is cases or fatalities per day because LGBM is failed to learn accumulated cases\/fatalities.\n### train a model for public LB","2ac6a130":"\n### train a model for private LB","20c97b4f":"# Data loading","14e7b39e":"- [week-1 version (2nd place as of 2020-4-4 update)](https:\/\/www.kaggle.com\/osciiart\/covid19-lightgbm?scriptVersionId=30830623)\n- [week-2 version](https:\/\/www.kaggle.com\/osciiart\/covid-19-lightgbm-no-leak?scriptVersionId=31248128)","c442d13d":"# Make submission","decfd6ae":"# Visualize prediction","da8bc56d":"Sigmoid Solution","1650aba3":"### Preprocessing"}}