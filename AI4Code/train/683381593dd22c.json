{"cell_type":{"50d7e07e":"code","90391945":"code","a346785d":"code","529d2bdb":"code","070c5816":"code","5d5e0239":"code","f001c5db":"code","c1fc9856":"code","e4765c59":"code","8166bb08":"code","ee7d6a0e":"code","b543325d":"code","08da60d8":"code","bd8a3006":"code","e78a1b46":"code","5f3ada39":"code","adfa78fa":"code","39529c83":"code","23c96ce3":"code","18b568ce":"code","48d0e0a9":"code","359d57b9":"code","198667e1":"code","43364c6a":"code","e8db7ca0":"code","9b84785e":"code","e54bd6aa":"code","0bf67e03":"code","d78472fa":"code","f2755adb":"code","68e3bb16":"code","4324cdb3":"code","3c6f578d":"code","e2fae14a":"code","2f40ba66":"code","b90604b5":"code","c17be623":"code","db6fbc79":"code","684346bb":"code","93d8b45e":"code","62a08c95":"code","c3bc1662":"code","28216d20":"code","67a350d9":"code","e0062862":"code","3fa1df71":"code","7b066093":"code","947d097f":"code","a0ddbc4f":"code","c6e1f858":"code","15e4a4f1":"code","8fa39fc8":"code","91a70932":"code","80065963":"code","1a13e429":"code","add6b32c":"code","d637ae16":"code","65b27ff8":"code","d5375418":"code","a0f8a55f":"code","61b6ba10":"code","9744746f":"markdown","3edec899":"markdown","9222ebf8":"markdown","793bc329":"markdown","d08850bb":"markdown","30550e09":"markdown","98501a72":"markdown","224229d6":"markdown","ac7fbbe0":"markdown","1f8a9c73":"markdown","9de9ad42":"markdown","e4f366b2":"markdown","672e3465":"markdown","a88e1ae6":"markdown","7f5f45d0":"markdown","a6159455":"markdown","f947d320":"markdown","d8ccaa0b":"markdown","73ba8596":"markdown","81e93181":"markdown","631d0d12":"markdown","37153532":"markdown","34130cd8":"markdown","541c4116":"markdown","1d2b731a":"markdown","6e8252b2":"markdown","0cc54797":"markdown","4346131e":"markdown","c714d37f":"markdown","d9378bad":"markdown","800d3551":"markdown","31fbdd06":"markdown","85f16eb6":"markdown","5bfe72de":"markdown","39b01d12":"markdown","b04c1bd7":"markdown","b3957dc7":"markdown","ff092257":"markdown","8899f8de":"markdown","3f2f69f3":"markdown","040fcb08":"markdown","eb4a9039":"markdown","4a990afa":"markdown","37509f7c":"markdown","8979a612":"markdown","bd3991db":"markdown","da449f2d":"markdown","da421bdc":"markdown","a78dc332":"markdown","1116cdd2":"markdown","81491f43":"markdown","58784340":"markdown","1a390f33":"markdown","2da022ed":"markdown","082d4b7e":"markdown","4038cf4c":"markdown","8173d6d8":"markdown","a5d8a611":"markdown"},"source":{"50d7e07e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","90391945":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","a346785d":"print(\"Missing data points within the Training dataset:\")\nprint(train_df.isnull().sum())\nprint(\"-\"*25)\nprint(\"Missing data points within the Test dataset:\")\nprint(test_df.isnull().sum())","529d2bdb":"train_df['Embarked'].value_counts()","070c5816":"train_df['Embarked'] = train_df['Embarked'].fillna(train_df['Embarked'].mode()[0])","5d5e0239":"test_df[test_df['Fare'].isnull() == True]","f001c5db":"test_df[test_df['Pclass'] == 3]['Fare'].median()","c1fc9856":"test_df['Fare'] = test_df['Fare'].fillna(test_df[test_df['Pclass'] == 3]['Fare'].median())","e4765c59":"test_df[test_df['PassengerId'] == 1044]","8166bb08":"print(\"Unique 'Cabin' values in the Training dataset\")\nprint(train_df['Cabin'].unique())\nprint(\"-\" * 50)\nprint(\"Unique 'Cabin' values in the Test dataset\")\nprint(test_df['Cabin'].unique())","ee7d6a0e":"train_df['Cabin'] = train_df['Cabin'].fillna('Missing')\ntest_df['Cabin'] = test_df['Cabin'].fillna('Missing')","b543325d":"train_df['Cabin'] = train_df['Cabin'].str[0]\ntest_df['Cabin'] = test_df['Cabin'].str[0]\n\nct = pd.crosstab(index=train_df['Survived'],columns=train_df[\"Cabin\"],normalize=\"columns\")\nct.T.plot(kind=\"bar\",figsize=(15,10),stacked=True)","08da60d8":"print(\"Missing data points within the Training dataset:\")\nprint(train_df.isnull().sum())\nprint(\"-\"*25)\nprint(\"Missing data points within the Test dataset:\")\nprint(test_df.isnull().sum())\n","bd8a3006":"train_df['Name']","e78a1b46":"train_df['Title'] = train_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].replace(' ',''))\ntest_df['Title'] = test_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].replace(' ',''))","5f3ada39":"train_df['Title'].value_counts()","adfa78fa":"test_df['Title'].value_counts()","39529c83":"combine = [train_df,test_df]\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'theCountess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Misc')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","23c96ce3":"train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","18b568ce":"test_ids = list(test_df['PassengerId'])\ny = train_df['Survived']\ntrain_df = train_df.drop(['PassengerId','Name','Ticket','Survived'],axis=1)\ntest_df = test_df.drop(['PassengerId','Name','Ticket'],axis=1)","48d0e0a9":"train_df.head()","359d57b9":"test_df.head()","198667e1":"train_df.head()","43364c6a":"train_df1 = train_df.copy(deep=True)\ntrain_df2 = train_df.copy(deep=True)\ntrain_df3 = train_df.copy(deep=True)\ntrain_df4 = train_df.copy(deep=True)","e8db7ca0":"train_df.groupby('Pclass').mean()['Age']","9b84785e":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass == 1:\n            return 38\n        elif Pclass == 2:\n            return 30\n        else:\n            return 25\n        \n    else:\n        return Age","e54bd6aa":"train_df1['Age'] = train_df1[['Age','Pclass']].apply(impute_age,axis=1)","0bf67e03":"train_df1.isnull().sum()","d78472fa":"train_df1.head()","f2755adb":"from sklearn.preprocessing import OneHotEncoder\nonehot_encoder = OneHotEncoder()\nfrom sklearn.compose import ColumnTransformer\ncolumnTransformer1 = ColumnTransformer([('encoder', onehot_encoder, [1,6,7,8])], remainder='passthrough')\ntrain_df1 = columnTransformer1.fit_transform(train_df1)","68e3bb16":"train_df2 = train_df2.drop('Cabin',axis=1)","4324cdb3":"train_df2['Age'] = train_df2[['Age','Pclass']].apply(impute_age,axis=1)","3c6f578d":"columnTransformer2 = ColumnTransformer([('encoder', onehot_encoder, [1,6,7])],remainder='passthrough')\ntrain_df2 = columnTransformer2.fit_transform(train_df2)","e2fae14a":"columnTransformer3 = ColumnTransformer([('encoder', onehot_encoder, [1,6,7,8])],remainder='passthrough')\ntrain_df3 = columnTransformer3.fit_transform(train_df3)","2f40ba66":"from sklearn.impute import KNNImputer\n\nimputer1 = KNNImputer()\ntrain_df3 = imputer1.fit_transform(train_df3)","b90604b5":"train_df4 = train_df4.drop(\"Cabin\", axis=1)","c17be623":"columnTransformer4 = ColumnTransformer([('encoder', onehot_encoder, [1,6,7])],remainder='passthrough')\ntrain_df4 = columnTransformer4.fit_transform(train_df4)","db6fbc79":"imputer2 = KNNImputer()\ntrain_df4 = imputer2.fit_transform(train_df4)","684346bb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df1,y,test_size=0.25,random_state=101)","93d8b45e":"from sklearn.ensemble import RandomForestClassifier\nscen1_rf_model = RandomForestClassifier()\nscen1_rf_model.fit(X_train,y_train)","62a08c95":"scen1_rf_model_preds = scen1_rf_model.predict(X_test)","c3bc1662":"from xgboost import XGBClassifier\nscen1_xgb_model = XGBClassifier()\nscen1_xgb_model.fit(X_train, y_train)\nscen1_xgb_model_preds = scen1_xgb_model.predict(X_test)","28216d20":"from sklearn.metrics import confusion_matrix,classification_report\nprint(\"RANDOM FOREST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen1_rf_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen1_rf_model_preds))\nprint(\"-\"*50)\nprint(\"XGBOOST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen1_xgb_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen1_xgb_model_preds))","67a350d9":"X_train, X_test, y_train, y_test = train_test_split(train_df2,y,test_size=0.25,random_state=101)","e0062862":"scen2_rf_model = RandomForestClassifier()\nscen2_rf_model.fit(X_train, y_train)\nscen2_rf_model_preds = scen2_rf_model.predict(X_test)","3fa1df71":"scen2_xgb_model = XGBClassifier()\nscen2_xgb_model.fit(X_train, y_train)\nscen2_xgb_model_preds = scen2_xgb_model.predict(X_test)","7b066093":"print(\"RANDOM FOREST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen2_rf_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen2_rf_model_preds))\nprint(\"-\"*50)\nprint(\"XGBOOST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen2_xgb_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen2_xgb_model_preds))","947d097f":"X_train, X_test, y_train, y_test = train_test_split(train_df3, y, test_size=0.25, random_state=101)","a0ddbc4f":"scen3_rf_model = RandomForestClassifier()\nscen3_rf_model.fit(X_train, y_train)\nscen3_rf_model_preds = scen3_rf_model.predict(X_test)","c6e1f858":"scen3_xgb_model = XGBClassifier()\nscen3_xgb_model.fit(X_train, y_train)\nscen3_xgb_model_preds = scen3_xgb_model.predict(X_test)","15e4a4f1":"print(\"RANDOM FOREST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen3_rf_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen3_rf_model_preds))\nprint(\"-\"*50)\nprint(\"XGBOOST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen3_xgb_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen3_xgb_model_preds))","8fa39fc8":"X_train, X_test, y_train, y_test = train_test_split(train_df4,y,test_size=0.25,random_state=101)","91a70932":"scen4_rf_model = RandomForestClassifier()\nscen4_rf_model.fit(X_train, y_train)\nscen4_rf_model_preds = scen4_rf_model.predict(X_test)","80065963":"scen4_xgb_model = XGBClassifier()\nscen4_xgb_model.fit(X_train,y_train)\nscen4_xgb_model_preds = scen4_xgb_model.predict(X_test)","1a13e429":"print(\"RANDOM FOREST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen4_rf_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen4_rf_model_preds))\nprint(\"-\"*50)\nprint(\"XGBOOST CLASSIFIER\")\nprint()\nprint(\"Confusion Matrix\")\nprint()\nprint(confusion_matrix(y_test,scen4_xgb_model_preds))\nprint()\nprint(\"Classification Report\")\nprint()\nprint(classification_report(y_test,scen4_xgb_model_preds))","add6b32c":"test_df = test_df.drop(\"Cabin\",axis=1)","d637ae16":"test_df = columnTransformer4.transform(test_df)","65b27ff8":"test_df = imputer2.transform(test_df)","d5375418":"from sklearn.model_selection import GridSearchCV\n\nfinal_xgb = XGBClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_xgb,param_grid={\n   \"n_estimators\":[100,500,1000,5000],\n   \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"accuracy\")\n\nmodel = gscv.fit(train_df4,y)\nfinal_xgb_model = model.best_estimator_","a0f8a55f":"final_predictions = final_xgb_model.predict(test_df)\n\nsubmission = pd.DataFrame({\"PassengerId\":test_ids,\"Survived\":final_predictions})","61b6ba10":"submission.to_csv(\"Submission_Attempt_2.csv\",index=False)","9744746f":"Now fill in the missing values using the same KNN imputer.","3edec899":"Since there are a wide range of different values possible, we shall simply fill in the null values with the text \"Missing\".","9222ebf8":"Let us investigate whether this feature has an impact on survival rate.","793bc329":"#### 5.4: Scenario 4\n\nFinally, within this dataset we shall impute the missing \"Age\" values using the KNN imputer whilst removing the \"Cabin\" column from the dataset. Let us remove the \"Cabin\" column first.","d08850bb":"We shall create a function called \"impute_age\" that will fill in these values if there is no \"Age\" value present.","30550e09":"Let us now produce a csv file to submit our predictions.","98501a72":"Let us investigate the titles that we have been able to extract.","224229d6":"From the classification reports and confusion matrices shown above, we can see that the both models performed similarly. An accuracy score of approximately 80% was achieved, with a similar F1-score achieved. \n\n#### 6.2: Scenario 2\n\nIn this section we shall repeat the process completed above using the second training dataframe. First, we split the dataset using the train_test_split function.","ac7fbbe0":"Let us check that there are now no missing values within the dataframe.","1f8a9c73":"##### 6.3.3: Model Analysis","9de9ad42":"Let us now fill in the missing \"Age\" values.","e4f366b2":"We now transform the columns using the same \"ColumnTransformer\".","672e3465":"We can see that there are no missing values within the \"train_df1\" dataframe. Let us now convert the categorical columns using the \"ColumnTransformer\" function from Sci-Kit Learn.","a88e1ae6":"We see a clear relationship between the cabin initial and survival rate. ","7f5f45d0":"#### 1.3: Missing values within the \"Cabin\" column\n\nLet's determine the possible values for this column within the training and test sets.","a6159455":"We have now successfully transformed each of the 4 dataframes ready for use within our machine learning models. \n\n### 6: Model Building\n\nIn this section we shall begin building models using each of the datasets created in section 5 above. Since it is known that RandomForest and XGBoost algorithms perform best in the titanic dataset, we shall be using those models. \n\nWe shall split each of the training sets using Sci-kit Learn's train_test_split function. Once we have trained models, we shall create predictions and analyse performance based on the F1-score and accuracy achieved by the models. \n\n#### 6.1: Scenario 1\n\nLet us split this dataset using the train_test_split function from Sci-kit Learn.","f947d320":"The \"Age\" column is the main analysis point of this project. As a result, the missing values within this column shall be imputed within step number 5. Let us now begin dealing with the other missing points.\n\n#### 1.1: Missing \"Embarked\" value within the training dataset\n\nWe shall now determine which value is the most likely to be missing from within this column. Since this column is categorical, we will first determine the range of values possible for this column.","d8ccaa0b":"#### 5.2: Scenario 2\n\nWithin this dataframe, we shall impute the missing \"Age\" values in the same way as above. However, for training purposes within this set, we shall remove the \"Cabin\" column. Let us do this now.","73ba8596":"##### 6.3.1: Random Forest Classifier","81e93181":"### 2: Extract New Features\n\nWe shall now extract a new feature from the \"Name\" column. Let us first look at some entries within this column.","631d0d12":"##### 6.2.2: XGBoost Classifier","37153532":"# Titanic Project - Attempt 2\n\nThis notebook comprises of my second attempt at this project. Within this project, we shall investigate how the imputation of the missing \"Age\" values affects results. \n\nWe shall undertake the following steps:\n\n0. Package and Data Imports\n1. Deal with Missing Data\n2. Extract New Features\n3. Drop Redundant Columns\n4. Create Multiple Copies of the Training Dataset\n5. Impute Missing 'Age' Values using 2 Different Methods\n6. Build Models\n7. Create Final Model and Generate Submission File\n\nLet us begin with the first step.\n\n### 0: Package and Data Imports\n\nIn this section we shall import the necessary packages as well as our training and test datasets. First we shall import the packages.","34130cd8":"##### 6.4.3: Model Analysis","541c4116":"### 5: Final Preparation of Dataframes\n\nIn this section we shall prepare each of the 4 dataframes created above based on the following specifications:\n\n1. Mean Imputed \"Age\" column using the \"Cabin\" column.\n2. Mean Imputed \"Age\" column without using the \"Cabin\" column.\n3. KNN Imputed \"Age\" column using the \"Cabin\" column.\n4. KNN Imputed \"Age\" column without using the \"Cabin\" column.\n\nLet us begin by investigating the average age of a passenger based on their \"Pclass\" value.","1d2b731a":"We see that this passenger belongs to the poorest class of passenger and as a result his ticket price will be cheaper. Let us fill in the \"Fare\" value using the median fare of his class.","6e8252b2":"##### 6.3.2: XGBoost Classifier","0cc54797":"#### 1.2: Missing \"Fare\" value within the test dataset\n\nLet us investigate this passenger.","4346131e":"#### 5.1: Scenario 1\n\nFor this dataframe, we shall impute the missing \"Age\" values using the function defined above. We shall also include the use of the \"Cabin\" column for training purposes. Let us first fill in the missing values.","c714d37f":"We observe that the title of each passenger is located within the same place for each entry. Let us create a new column called \"Title\" to store this information.","d9378bad":"We shall now import our data.","800d3551":"#### 5.3: Scenario 3\n\nFor this dataset, we shall be using the KNN Imputer to fill in the missing \"Age\" values. We shall also include the use of the cabin column for training purposes. Due to the use of the KNN imputer, we must perform the column transformation first.","31fbdd06":"We can see that the only column which now has missing values is the \"Age\" column. As explained above, we shall be imputing these values within step 5.","85f16eb6":"Let us take a look at the heads of our 2 dataframes.","5bfe72de":"We can now generate predictions.","39b01d12":"Let us extract the first character of each \"Cabon\" value and investigate the effcect that this has on survival.","b04c1bd7":"### 4: Creating multiple copies of the training dataframe\n\nWe require 4 different copies of the training dataframe in order to investigate the effect that the 2 different methods of \"Age\" imputation and the inclusion of the \"Cabin\" column has on the accuracy of predictions.","b3957dc7":"##### 6.2.1: Random Forest Classifier","ff092257":"It seems like the removal of the \"Cabin\" column slightly increases the accuracy score achieved by the models. The XGB Classifier manages to accurately predict 82% of the testing set. \n\n#### 6.3: Scenario 3","8899f8de":"Let us now generate predictions and create a submission dataframe.","3f2f69f3":"Let us now apply the Column Transformer.","040fcb08":"##### 6.4.2: XGBoost Classifier","eb4a9039":"#### 1.4: Check Missing Data Points\n\nLet us now check that each of the missing values have been filled in as expected.","4a990afa":"#### 6.4: Scenario 4","37509f7c":"Finally, we shall apply the KNN imputer to fill in the missing values.","8979a612":"Let us replace some titles with a more common name or classify them as \"Misc\".","bd3991db":"We observe 3 different possibilities, each corresponding to a unique boarding location. Since Southampton, \"S\", is by far the most popular embarkation location for passengers onboard the Titanic, we can reasonably assume that the 2 passengers where this value is missing also boarded at this location. As a result, we shall fill the missing values using the mode of this column.","da449f2d":"### 7: Final Model Building\n\nIn this section we shall build the final model and use it to generate predictions on the test set. The model we shall build involves the KNN imputed \"Age\" values, without the use of the \"Cabin\" column, which equates to scenario 4. First, we shall transform the \"test\" set to match the training set. The first step is to remove the \"Cabin\" column.","da421bdc":"Let us now transform the categorical features in the same way as above.","a78dc332":"##### 6.1.1: Random Forest Classifier\n\nLet us now train a Random Forest Classifier on the \"X_train\" dataset and create predictions using the \"X_test\" set. ","1116cdd2":"##### 6.1.2: XGBoost Classifier\n\nThe process for training models and generating predictions is the same as the steps completed above. As a result, we shall complete the entire model building process within one block of code.","81491f43":"##### 6.1.3: Model Analysis\n\nLet us produce classification reports and confusion matrices for both of the models created above.","58784340":"Let us now fit an XGBoost model making use of the GridSearchCV function from Sci-kit Learn.","1a390f33":"Let us now apply the KNN imputer.","2da022ed":"### 1: Dealing with Missing Data\n\nLet us investigate any missing data points within our training and test sets.","082d4b7e":"##### 6.4.1: Random Forest Classifier","4038cf4c":"##### 6.2.3: Model Analysis","8173d6d8":"Let us check that this has correctly filled in the missing fare.","a5d8a611":"We can clearly see a significant impact on the chances of survival based on a persons Title. We shall use this feature for prediction within our models.\n\n### 3: Drop Redundant Columns\n\nLet us remove the columns that we will be unable to use within our predicitve models. The features to be removed are \"PassengerId\", \"Name\" and \"Ticket\". We shall store the test set passenger Ids for use within the creation of the submission file."}}