{"cell_type":{"98e98c05":"code","3a577eae":"code","e0df073c":"code","c2052d7b":"code","784693bd":"code","1844fe5f":"code","0e8c26a2":"code","4fcb8488":"code","ce86cea5":"code","73eff84e":"code","46382a9d":"code","6cb0011e":"code","fc3b92e8":"code","883e41eb":"code","393d9981":"code","3a382a08":"code","2b589779":"code","02b47a4c":"code","b13dfca7":"code","a341cc97":"code","6d22dfe1":"code","c377e183":"code","ab74c6d0":"code","6fa34a81":"code","0182dd43":"code","cfa532a3":"code","6238693f":"code","c59b3205":"code","d82725e9":"code","ed84e22e":"code","159a61c0":"code","be38aa56":"code","e065e77f":"code","242dcd92":"code","ccc4ff85":"code","451a4f92":"code","c83f30cd":"code","12cc9e2a":"code","ebe54db8":"code","c24aff78":"code","18888791":"code","84cd8313":"code","b5d870da":"code","d764f9ba":"code","acd88878":"code","ba38bdd4":"code","fa4377f4":"code","e559ca8e":"markdown","d4ffb92f":"markdown","2831c71b":"markdown","1d2f7f47":"markdown","6c968508":"markdown","7ba7787a":"markdown","d66e1180":"markdown","6c2e6870":"markdown","93011444":"markdown","08da28f6":"markdown","206116d7":"markdown","e8a7c2d1":"markdown","55e98d3a":"markdown","ff82fe00":"markdown","83b3e95f":"markdown","15edc972":"markdown","0727a19d":"markdown","926c87cf":"markdown","96da05fd":"markdown","1ce4f5d5":"markdown","49523b00":"markdown","e8b62241":"markdown","ed0e755f":"markdown"},"source":{"98e98c05":"PLEASE ASSIST IN DEBUGGING THIS CODE. THANK YOU","3a577eae":"from keras.preprocessing import image\nfrom keras.models import Model, Sequential\nfrom keras.layers import Activation, Dense, GlobalAveragePooling2D, BatchNormalization, Flatten, Dropout, Conv2D, Conv2DTranspose, AveragePooling2D, MaxPooling2D, UpSampling2D, Input, Reshape\nfrom keras import backend as K\nfrom keras.optimizers import Nadam, Adam, SGD\nfrom keras.metrics import categorical_accuracy, binary_accuracy\n#from keras_contrib.losses import jaccard\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions\n\nfrom keras.applications.nasnet import NASNetLarge\nfrom keras.applications.nasnet import preprocess_input, decode_predictions\n\nimport tensorflow as tf\nimport platform\nimport tensorflow.python.client\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport sys, os \nsys.path.append(os.getcwd())\nimport cv2\nimport os.path\n\ndata_folder = '..\/input'\nmodel_folder = '..\/input\/models'\nhistory_folder = '..\/input\/history'\npred_result_folder_val = '..\/input\/val_predict_results'\nout_dist_pred_result_folder = '..\/input\/out_dist_predict_results'\nworkers = os.cpu_count()\nplt.rcParams['svg.fonttype'] = 'none'","e0df073c":"import pandas as pd\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\ndef load_isic_training_data(image_folder, ground_truth_file):\n    df_ground_truth = pd.read_csv(ground_truth_file)\n    # Category names\n    known_category_names = list(df_ground_truth.columns.values[1:9])\n    unknown_category_name = df_ground_truth.columns.values[9]\n    \n    # Add path and category columns\n    df_ground_truth['path'] = df_ground_truth.apply(lambda row : os.path.join(image_folder, row['image']+'.jpg'), axis=1)\n    df_ground_truth['category'] = np.argmax(np.array(df_ground_truth.iloc[:,1:10]), axis=1)\n    return df_ground_truth, known_category_names, unknown_category_name\n\ndef load_isic_training_and_out_dist_data(isic_image_folder, ground_truth_file, out_dist_image_folder):\n    \"\"\"ISIC training data and Out-of-distribution data are combined\"\"\"\n    df_ground_truth = pd.read_csv(ground_truth_file)\n    # Category names\n    known_category_names = list(df_ground_truth.columns.values[1:9])\n    unknown_category_name = df_ground_truth.columns.values[9]\n    \n    # Add path and category columns\n    df_ground_truth['path'] = df_ground_truth.apply(lambda row : os.path.join(isic_image_folder, row['image']+'.jpg'), axis=1)\n    \n    df_out_dist = get_dataframe_from_img_folder(out_dist_image_folder, has_path_col=True)\n    for name in known_category_names:\n        df_out_dist[name] = 0.0\n    df_out_dist[unknown_category_name] = 1.0\n    # Change the order of columns\n    df_out_dist = df_out_dist[df_ground_truth.columns.values]\n\n    df_combined = pd.concat([df_ground_truth, df_out_dist])\n    df_combined['category'] = np.argmax(np.array(df_combined.iloc[:,1:10]), axis=1)\n\n    category_names = known_category_names + [unknown_category_name]\n    return df_combined, category_names\n\ndef train_validation_split(df):\n    df_train, df_val = train_test_split(df, stratify=df['category'], test_size=0.2, random_state=1)\n    return df_train, df_val\n\ndef compute_class_weight_dict(df):\n    \"\"\"Compute class weights for weighting the loss function on imbalanced data.\"\"\"\n    class_weights = class_weight.compute_class_weight('balanced', np.unique(df['category']), df['category'])\n    class_weight_dict = dict(enumerate(class_weights))\n    return class_weight_dict, class_weights\n\ndef get_dataframe_from_img_folder(img_folder, has_path_col=True):\n    if has_path_col:\n        return pd.DataFrame([[Path(x).stem, x] for x in sorted(Path(img_folder).glob('**\/*.jpg'))], columns=['image', 'path'], dtype=np.str)\n    else:\n        return pd.DataFrame([Path(x).stem for x in sorted(Path(img_folder).glob('**\/*.jpg'))], columns=['image'], dtype=np.str)","c2052d7b":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import StrMethodFormatter\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n\ndef plot_complexity_graph(csv_file, title=None, figsize=(14, 10), feature_extract_epochs=None,\n                          loss_min=0, loss_max=2, epoch_min=None, epoch_max=90, accuracy_min=0, accuracy_max=1):\n    df = pd.read_csv(csv_file)\n\n    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=figsize)\n    fig.patch.set_facecolor('white')\n    fig.suptitle(title, fontsize=14)\n\n    ax1.plot(df['loss'], label='Training Loss')\n    ax1.plot(df['val_loss'], label='Validation Loss')\n    ax1.set(title='Training and Validation Loss', xlabel='', ylabel='Loss')\n    ax1.set_xlim([epoch_min, epoch_max])\n    ax1.set_ylim([loss_min, loss_max])\n    ax1.legend()\n\n    ax2.plot(df['balanced_accuracy'], label='Training Accuracy')\n    ax2.plot(df['val_balanced_accuracy'], label='Validation Accuracy')\n    ax2.set(title='Training and Validation Accuracy', xlabel='Epoch', ylabel='Balanced Accuracy')\n    ax2.set_xlim([epoch_min, epoch_max])\n    ax2.set_ylim([accuracy_min, accuracy_max])\n    ax2.legend()\n\n    if feature_extract_epochs is not None:\n        ax1.axvline(feature_extract_epochs-1, color='green', label='Start Fine Tuning')\n        ax2.axvline(feature_extract_epochs-1, color='green', label='Start Fine Tuning')\n        ax1.legend()\n        ax2.legend()\n    \n    # tight_layout() only considers ticklabels, axis labels, and titles. Thus, other artists may be clipped and also may overlap.\n    # [left, bottom, right, top]\n    fig.tight_layout(rect=[0, 0.02, 1, 0.96])\n    return fig\n\ndef plot_grouped_2bars(scalars, scalarlabels, xticklabels, title=None, xlabel=None, ylabel=None):\n    x = np.arange(len(xticklabels))  # the label locations\n    width = 0.35  # the width of the bars\n\n    # Create grouped bar chart\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.set_title(title)\n    fig.patch.set_facecolor('white')\n    rects1 = ax.bar(x - width\/2, scalars[0], width, label=scalarlabels[0])\n    rects2 = ax.bar(x + width\/2, scalars[1], width, label=scalarlabels[1])\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_xticks(x)\n    ax.set_xticklabels(xticklabels)\n    ax.set(xlabel=xlabel, ylabel=ylabel)\n    ax.legend()\n    autolabel(ax, rects1)\n    autolabel(ax, rects2)\n    fig.tight_layout()\n\ndef autolabel(ax, rects):\n    \"\"\"\n    Attach a text label above each bar in *rects*, displaying its height.\n    # References\n        https:\/\/matplotlib.org\/3.1.1\/gallery\/lines_bars_and_markers\/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n    \"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, figsize=(8, 6)):\n    \"\"\"\n    This function plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    # References\n        https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \"\"\"\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots(figsize=figsize)\n    fig.patch.set_facecolor('white')\n    ax.set(title=title,\n           ylabel='True Label',\n           xlabel='Predicted Label')\n    im, cbar = heatmap(cm, classes, classes, ax=ax, cmap=plt.cm.Blues, cbarlabel='', grid=False)\n    texts = annotate_heatmap(im, valfmt=\"{x:.2f}\")\n\n    fig.tight_layout()\n    return fig\n\n\ndef heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel=\"\", grid=True, **kwargs):\n    \"\"\"\n    Create a heatmap from a numpy array and two lists of labels.\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (N, M).\n    row_labels\n        A list or array of length N with the labels for the rows.\n    col_labels\n        A list or array of length M with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    # References\n        https:\/\/matplotlib.org\/3.1.0\/gallery\/images_contours_and_fields\/image_annotated_heatmap.html\n    \"\"\"\n\n    if not ax:\n        ax = plt.gca()\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticks(np.arange(data.shape[0]))\n    # ... and label them with the respective list entries.\n    ax.set_xticklabels(col_labels)\n    ax.set_yticklabels(row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False,\n                   labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Turn spines off and create white grid.\n    for edge, spine in ax.spines.items():\n        spine.set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n    if grid:\n        ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    return im, cbar\n\n\ndef annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\", textcolors=[\"black\", \"white\"], threshold=None, **textkw):\n    \"\"\"\n    A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. \"$ {x:.2f}\", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A list or array of two color specifications.  The first is used for\n        values below a threshold, the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    \"\"\"\n\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    if threshold is not None:\n        threshold = im.norm(threshold)\n    else:\n        threshold = im.norm(data.max())\/2.\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each \"pixel\".\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n            texts.append(text)\n\n    return texts\n\n\ndef plot_prob_bars(img_title, img_path, labels, probs, topk=5, title=None, figsize=(10, 4)):\n    fig, (ax1, ax2) = plt.subplots(figsize=figsize, ncols=2)\n    fig.patch.set_facecolor('white')\n\n    if title is not None:\n        fig.suptitle(title)\n\n    ax1.set_title(img_title)\n    ax1.imshow(plt.imread(img_path))\n\n    # Plot probabilities bar chart\n    ax2.set_title(\"Top {0} probabilities\".format(topk))\n    ax2.barh(np.arange(topk), probs)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(topk))\n    ax2.set_yticklabels(labels, size='medium')\n    ax2.yaxis.tick_right()\n    ax2.set_xlim(0, 1.0)\n    ax2.invert_yaxis()\n    fig.tight_layout(rect=[0, 0.02, 1, 0.96])\n    return fig","784693bd":"from collections import Counter\n#from data import load_isic_training_data\n#from visuals import autolabel\n\ntraining_image_folder = os.path.join(data_folder, 'ISIC_2019_Training_Input')\nground_truth_file = os.path.join(data_folder, '..\/input\/isic-2019-training-groundtruth\/ISIC_2019_Training_GroundTruth.csv')\n\ndf_ground_truth, known_category_names, unknown_category_name = load_isic_training_data(training_image_folder, ground_truth_file)\nknown_category_num = len(known_category_names)\nprint(\"Number of known categories: {}\".format(known_category_num))\nprint(known_category_names, '\\n')\nunknown_category_num = 1\nprint(\"Number of unknown categories: {}\".format(unknown_category_num))\nprint(unknown_category_name, '\\n')\nall_category_names = known_category_names + [unknown_category_name]\nall_category_num = known_category_num + unknown_category_num\n\n# mapping from category to index\nprint('Category to Index:')\ncategory_to_index = dict((c, i) for i, c in enumerate(all_category_names))\nprint(category_to_index, '\\n')\n\ncount_per_category = Counter(df_ground_truth['category'])\ntotal_sample_count = sum(count_per_category.values())\nprint(\"Original training data has {} samples.\".format(total_sample_count))\nfor i, c in enumerate(all_category_names):\n    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category[i], count_per_category[i]*100\/total_sample_count))\n    \n# Create a bar chart\nfig, ax = plt.subplots(figsize=(8, 5))\nfig.patch.set_facecolor('white')\nax.set(xlabel='Category', ylabel='Number of Images')\n# plt.bar(count_per_category.keys(), count_per_category.values())\nrects = plt.bar(all_category_names, [count_per_category[i] for i in range(all_category_num)])\nautolabel(ax, rects)\nfig.tight_layout()\n\ndf_ground_truth.head()","1844fe5f":"#from data import train_validation_split\n#from visuals import plot_grouped_2bars\n\ndf_train, df_val = train_validation_split(df_ground_truth)\n\n# Training Set\nsample_count_train = df_train.shape[0]\nprint(\"Training set has {} samples.\".format(sample_count_train))\ncount_per_category_train = Counter(df_train['category'])\nfor i, c in enumerate(all_category_names):\n    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_train[i], count_per_category_train[i]*100\/sample_count_train))\n\n# Validation Set\nsample_count_val = df_val.shape[0]\nprint(\"\\nValidation set has {} samples.\".format(sample_count_val))\ncount_per_category_val = Counter(df_val['category'])\nfor i, c in enumerate(all_category_names):\n    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_per_category_val[i], count_per_category_val[i]*100\/sample_count_val))\n\nplot_grouped_2bars(\n    scalars=[[count_per_category_train[i] for i in range(all_category_num)],\n             [count_per_category_val[i] for i in range(all_category_num)]],\n    scalarlabels=['Training', 'Validation'],\n    xticklabels=all_category_names,\n    xlabel='Category',\n    ylabel='Number of Images',\n    title='Distribution of Training and Validation Sets'\n)","0e8c26a2":"#from data import compute_class_weight_dict\n\nclass_weight_dict, class_weights = compute_class_weight_dict(df_train)\nprint('Class Weights Dictionary (without UNK):')\nprint(class_weight_dict)\n\n# Create a bar chart\nfig, ax = plt.subplots(figsize=(7, 5))\nfig.patch.set_facecolor('white')\nax.set_title('Class Weights')\nax.set(xlabel='Category', ylabel='Weight')\nplt.bar(known_category_names, [class_weight_dict[i] for i in range(known_category_num)]);","4fcb8488":"from __future__ import division\nfrom keras.preprocessing import image\nfrom keras import backend as K\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\n\ndef path_to_tensor(img_path, size=(224, 224)):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=size)\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths, size=(224, 224)):\n    list_of_tensors = [path_to_tensor(img_path, size) for img_path in img_paths]\n    return np.vstack(list_of_tensors)\n\ndef calculate_mean_std(img_paths):\n    \"\"\"\n    Calculate the image per channel mean and standard deviation.\n\n    # References\n        https:\/\/gist.github.com\/jdhao\/9a86d4b9e4f79c5330d54de991461fd6\n    \"\"\"\n    \n    # Number of channels of the dataset image, 3 for color jpg, 1 for grayscale img\n    channel_num = 3\n    pixel_num = 0 # store all pixel number in the dataset\n    channel_sum = np.zeros(channel_num)\n    channel_sum_squared = np.zeros(channel_num)\n\n    for path in img_paths:\n        im = cv2.imread(path) # image in M*N*CHANNEL_NUM shape, channel in BGR order\n        im = im\/255.\n        pixel_num += (im.size\/channel_num)\n        channel_sum += np.sum(im, axis=(0, 1))\n        channel_sum_squared += np.sum(np.square(im), axis=(0, 1))\n\n    bgr_mean = channel_sum \/ pixel_num\n    bgr_std = np.sqrt(channel_sum_squared \/ pixel_num - np.square(bgr_mean))\n    \n    # change the format from bgr to rgb\n    rgb_mean = list(bgr_mean)[::-1]\n    rgb_std = list(bgr_std)[::-1]\n    \n    return rgb_mean, rgb_std\n\ndef preprocess_input(x, data_format=None, **kwargs):\n    \"\"\"Preprocesses a numpy array encoding a batch of images. Each image is normalized by subtracting the mean and dividing by the standard deviation channel-wise.\n    This function only implements the 'torch' mode which scale pixels between 0 and 1 and then will normalize each channel with respect to the training dataset of approach 1 (not include validation set).\n\n    # Arguments\n        x: a 3D or 4D numpy array consists of RGB values within [0, 255].\n        data_format: data format of the image tensor.\n    # Returns\n        Preprocessed array.\n    # References\n        https:\/\/github.com\/keras-team\/keras-applications\/blob\/master\/keras_applications\/imagenet_utils.py\n    \"\"\"\n    if not issubclass(x.dtype.type, np.floating):\n        x = x.astype(K.floatx(), copy=False)\n\n    # Mean and STD from ImageNet\n    # mean = [0.485, 0.456, 0.406]\n    # std = [0.229, 0.224, 0.225]\n\n    # Mean and STD calculated over the Training Set\n    # Mean:[0.6236094091893962, 0.5198354883713194, 0.5038435406338101]\n    # STD:[0.2421814437693499, 0.22354427793687906, 0.2314805420919389]\n    x \/= 255.\n    mean = [0.6236, 0.5198, 0.5038]\n    std = [0.2422, 0.2235, 0.2315]\n\n    if data_format is None:\n        data_format = K.image_data_format()\n\n    # Zero-center by mean pixel\n    if data_format == 'channels_first':\n        if x.ndim == 3:\n            x[0, :, :] -= mean[0]\n            x[1, :, :] -= mean[1]\n            x[2, :, :] -= mean[2]\n            if std is not None:\n                x[0, :, :] \/= std[0]\n                x[1, :, :] \/= std[1]\n                x[2, :, :] \/= std[2]\n        else:\n            x[:, 0, :, :] -= mean[0]\n            x[:, 1, :, :] -= mean[1]\n            x[:, 2, :, :] -= mean[2]\n            if std is not None:\n                x[:, 0, :, :] \/= std[0]\n                x[:, 1, :, :] \/= std[1]\n                x[:, 2, :, :] \/= std[2]\n    else:\n        x[..., 0] -= mean[0]\n        x[..., 1] -= mean[1]\n        x[..., 2] -= mean[2]\n        if std is not None:\n            x[..., 0] \/= std[0]\n            x[..., 1] \/= std[1]\n            x[..., 2] \/= std[2]\n    return x\n\ndef preprocess_input_2(x, data_format=None, **kwargs):\n    \"\"\"Preprocesses a numpy array encoding a batch of images. Each image is normalized by subtracting the mean and dividing by the standard deviation channel-wise.\n    This function only implements the 'torch' mode which scale pixels between 0 and 1 and then will normalize each channel with respect to the training dataset of approach 2 (not include validation set).\n\n    # Arguments\n        x: a 3D or 4D numpy array consists of RGB values within [0, 255].\n        data_format: data format of the image tensor.\n    # Returns\n        Preprocessed array.\n    # References\n        https:\/\/github.com\/keras-team\/keras-applications\/blob\/master\/keras_applications\/imagenet_utils.py\n    \"\"\"\n    if not issubclass(x.dtype.type, np.floating):\n        x = x.astype(K.floatx(), copy=False)\n\n    # Mean and STD calculated over the training set of approach 2\n    # Mean:[0.6296238064420809, 0.5202302775509949, 0.5032952297664738]\n    # STD:[0.24130893564897463, 0.22150225707876617, 0.2297057828857888]\n    x \/= 255.\n    mean = [0.6296, 0.5202, 0.5033]\n    std = [0.2413, 0.2215, 0.2297]\n\n    if data_format is None:\n        data_format = K.image_data_format()\n\n    # Zero-center by mean pixel\n    if data_format == 'channels_first':\n        if x.ndim == 3:\n            x[0, :, :] -= mean[0]\n            x[1, :, :] -= mean[1]\n            x[2, :, :] -= mean[2]\n            if std is not None:\n                x[0, :, :] \/= std[0]\n                x[1, :, :] \/= std[1]\n                x[2, :, :] \/= std[2]\n        else:\n            x[:, 0, :, :] -= mean[0]\n            x[:, 1, :, :] -= mean[1]\n            x[:, 2, :, :] -= mean[2]\n            if std is not None:\n                x[:, 0, :, :] \/= std[0]\n                x[:, 1, :, :] \/= std[1]\n                x[:, 2, :, :] \/= std[2]\n    else:\n        x[..., 0] -= mean[0]\n        x[..., 1] -= mean[1]\n        x[..., 2] -= mean[2]\n        if std is not None:\n            x[..., 0] \/= std[0]\n            x[..., 1] \/= std[1]\n            x[..., 2] \/= std[2]\n    return x\n\ndef ensemble_predictions(result_folder, category_names, save_file=True,\n                         model_names=['inception_resnet' 'nasnet'],\n                         postfixes=['best_balanced_acc', 'best_loss', 'latest']):\n    \"\"\" Ensemble predictions of different models. \"\"\"\n    for postfix in postfixes:\n        # Load models' predictions\n        df_dict = {model_name : pd.read_csv(os.path.join(result_folder, \"{}_{}.csv\".format(model_name, postfix))) for model_name in model_names}\n\n        # Check row number\n        for i in range(1, len(model_names)):\n            if len(df_dict[model_names[0]]) != len(df_dict[model_names[i]]):\n                raise ValueError(\"Row numbers are inconsistent between {} and {}\".format(model_names[0], model_names[i]))\n\n        # Check whether values of image column are consistent\n        for i in range(1, len(model_names)):\n            inconsistent_idx = np.where(df_dict[model_names[0]].image != df_dict[model_names[i]].image)[0]\n            if len(inconsistent_idx) > 0:\n                raise ValueError(\"{} values of image column are inconsistent between {} and {}\"\n                                .format(len(inconsistent_idx), model_names[0], model_names[i]))\n\n        # Copy the first model's predictions\n        df_ensemble = df_dict[model_names[0]].drop(columns=['pred_category'])\n\n        # Add up predictions\n        for category_name in category_names:\n            for i in range(1, len(model_names)):\n                df_ensemble[category_name] = df_ensemble[category_name] + df_dict[model_names[i]][category_name]\n\n        # Take average of predictions\n        for category_name in category_names:\n            df_ensemble[category_name] = df_ensemble[category_name] \/ len(model_names)\n\n        # Ensemble Predictions\n        df_ensemble['pred_category'] = np.argmax(np.array(df_ensemble.iloc[:,1:(1+len(category_names))]), axis=1)\n\n        # Save Ensemble Predictions\n        if save_file:\n            ensemble_file = os.path.join(result_folder, \"Ensemble_{}.csv\".format(postfix))\n            df_ensemble.to_csv(path_or_buf=ensemble_file, index=False)\n            print('Save \"{}\"'.format(ensemble_file))\n    return df_ensemble\n\ndef logistic(x, x0=0, L=1, k=1):\n    \"\"\" Calculate the value of a logistic function.\n\n    # Arguments\n        x0: The x-value of the sigmoid's midpoint.\n        L: The curve's maximum value.\n        k: The logistic growth rate or steepness of the curve.\n    # References https:\/\/en.wikipedia.org\/wiki\/Logistic_function\n    \"\"\"\n\n    return L \/ (1 + np.exp(-k*(x-x0)))","ce86cea5":"#from utils import calculate_mean_std\n\n### Uncomment below codes to calculate per-channel mean and standard deviation over the training set\n#rgb_mean, rgb_std = calculate_mean_std(df_train['path'])\n#print(\"Mean:{}\\nSTD:{}\".format(rgb_mean, rgb_std))\n\n# Output was:\n# Mean:[0.6236094091893962, 0.5198354883713194, 0.5038435406338101]\n# STD:[0.2421814437693499, 0.22354427793687906, 0.2314805420919389]\n\n#or\n# Mean:[0.6296238064420809, 0.5202302775509949, 0.5032952297664738]\n# STD:[0.24130893564897463, 0.22150225707876617, 0.2297057828857888]","73eff84e":"from IPython.display import Image\n\ncategory_groups = df_train.groupby('category')\n\n# Number of samples for each category\nnum_per_category = 5\n\nfig, axes = plt.subplots(nrows=known_category_num, ncols=num_per_category, figsize=(12, 24))\nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\nfig.patch.set_facecolor('white')\n\nfor idx, val in enumerate(known_category_names):\n    i = 0\n    for index, row in category_groups.get_group(idx).head(num_per_category).iterrows():\n        ax = axes[idx, i]\n        ax.imshow(plt.imread(row['path']))\n        ax.set_xlabel(row['image'])\n        if ax.is_first_col():\n            ax.set_ylabel(val, fontsize=20)\n            ax.yaxis.label.set_color('blue')\n        i += 1\n    \nfig.tight_layout()\nfig.savefig('Samples of training data.jpg', format='jpg', dpi=300, bbox_inches='tight', pad_inches=0)","46382a9d":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_files\nimport keras \nfrom keras.utils import np_utils\nfrom glob import glob","6cb0011e":"def load_dataset(path):\n    data = load_files(path, shuffle=True)\n    img_files = np.array(data['filenames'])\n    img_targets = np_utils.to_categorical(np.array(data['target']), 3)\n    return img_files, img_targets","fc3b92e8":"train_files, train_labels = load_dataset(df_train)\nvalid_files, valid_labels = load_dataset(df_valid)","883e41eb":"from keras.preprocessing import image\nfrom tqdm import tqdm\n\ndef path_to_tensor(path):\n    \n    img = image.load_img(path, target_size = (224,224))\n    x= image.img_to_array(img)\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(paths):\n    list_of_tensors = [path_to_tensor(path) for path in tqdm(paths)]\n    return np.vstack(list_of_tensors)","393d9981":"train_tensors = paths_to_tensor(train_files).astype('float32')\/255\nvalid_tensors = paths_to_tensor(df_val).astype('float32')\/255\ntest_tensors = paths_to_tensor(df_test).astype('float32')\/255","3a382a08":"from keras.applications.inception_resnet_v2 import InceptionResNetV2\n\nmodel_inception_resnet = InceptionResNetV2(weights = 'imagenet', include_top = False)","2b589779":"train_features_inception_resnet = model_inception_resnet.predict(train_tensors, verbose=1)\nvalid_features_inception_resnet = model_inception_resnet.predict(valid_tensors, verbose=1)\ntest_features_inception_resnet = model_inception_resnet.predict(test_tensors, verbose=1)","02b47a4c":"model_inception_resnet = Sequential()\n\nmodel_inception_resnet.add(GlobalAveragePooling2D(input_shape = train_features_inception_resnet.shape[1:]))\nmodel_inception_resnet.add(Dropout(0.2))\nmodel_inception_resnet.add(Dense(1024, activation = 'relu'))\nmodel_inception_resnet.add(Dropout(0.2))\nmodel_inception_resnet.add(Dense(512, activation = 'relu'))\nmodel_inception_resnet.add(Dropout(0.2))\nmodel_inception_resnet.add(Dense(128, activation = 'relu'))\nmodel_inception_resnet.add(Dropout(0.2))\nmodel_inception_resnet.add(Dense(3, activation ='softmax'))\n\nmodel.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=input_shape))\nmodel.add(Conv2D(16, (3, 3), padding=\"same\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\nmodel.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\nmodel.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(96, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\nmodel.add(Conv2D(96, (3, 3), padding=\"valid\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(128, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\nmodel.add(Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Flatten())\n  \nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n    \nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n    \nmodel.add(Dense(num_class , activation='softmax'))\n\n\nmodel_inception_resnet.summary()","b13dfca7":"opt = keras.optimizers.Adam(lr=0.0001, decay=1e-6)\nmodel_inception_resnet.compile(optimizer= opt, metrics = ['accuracy'], loss='categorical_crossentropy')","a341cc97":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint_inception = ModelCheckpoint(\n    save_best_only = True, \n    verbose = 1, \n    filepath = 'models\/weights.best.from_inception_resnet_v2.hdf5')\n\nmodel_inception_resnet.fit(train_features_inception_resnet, \n          train_labels, \n          epochs=35, \n          batch_size= 64, \n          validation_data=(valid_features_inception, valid_labels), callbacks=[checkpoint_inception], verbose=1\n         )","6d22dfe1":"model_inception_resnet.load_weights('models\/weights.best.from_inception_resnet_v2.hdf5')","c377e183":"submission_inception_resnet = pd.DataFrame({'Id':test_files, 'task_1':test_predictions_task1,'task_2':test_predictions_task2})\npd.DataFrame.to_csv(submission_inception_resnet, 'submission.csv', index=False)","ab74c6d0":"from keras.applications.nasnet import NASNetLarge\nmodel_nasnet = NASNetLarge(weights = 'imagenet', include_top = False)","6fa34a81":"train_features_nasnet = model_nasnet.predict(train_tensors, verbose=1)\nvalid_features_nasnet = model_nasnet.predict(valid_tensors, verbose=1)\ntest_features_nasnet = model_nasnet.predict(test_tensors, verbose=1)","0182dd43":"\nmodel_nasnet = Sequential()\n\nmodel_nasnet.add(GlobalAveragePooling2D(input_shape = train_features_nasnet.shape[1:]))\nmodel_nasnet.add(Dropout(0.2))\nmodel_nasnet.add(Dense(1024, activation = 'relu'))\nmodel_nasnet.add(Dropout(0.2))\nmodel_nasnet.add(Dense(512, activation = 'relu'))\nmodel_nasnet.add(Dropout(0.2))\nmodel_nasnet.add(Dense(128, activation = 'relu'))\nmodel_nasnet.add(Dropout(0.2))\nmodel_nasnet.add(Dense(3, activation ='softmax'))\n\n    model.add(Conv2D(16, (3, 3), activation='relu', padding=\"same\", input_shape=input_shape))\n    model.add(Conv2D(16, (3, 3), padding=\"same\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n    model.add(Conv2D(32, (3, 3), padding=\"same\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n    model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(96, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n    model.add(Conv2D(96, (3, 3), padding=\"valid\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(128, (3, 3), dilation_rate=(2, 2), activation='relu', padding=\"same\"))\n    model.add(Conv2D(128, (3, 3), padding=\"valid\", activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n\n    model.add(Flatten())\n    \n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    \n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(BatchNormalization())\n    \n    model.add(Dense(num_class , activation='softmax'))\n\nmodel_nasnet.summary()","cfa532a3":"opt = keras.optimizers.Adam(lr=0.0001, decay=1e-6)\nmodel.compile(optimizer= opt, metrics = ['accuracy'], loss='categorical_crossentropy')","6238693f":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint(save_best_only = True, verbose =1, \n                             filepath = 'models\/weights.best.from_nasnet.hdf5')\n\nmodel.fit(train_features_nasnet, \n          train_labels, \n          epochs=25, \n          batch_size= 64, \n          validation_data=(valid_features_nasnet, valid_labels), callbacks=[checkpoint], verbose=1\n         )","c59b3205":"model.load_weights('models\/weights.best.from_nasnet.hdf5')","d82725e9":"test_predictions = np.argmax(model.predict(test_features_nasnet), axis=1)\naccuracy = 100 * np.sum(np.array(test_predictions) == np.argmax(test_labels, axis=1))\/len(test_predictions)\nprint ('Accuracy of NasNet model on test set = %.4f%%' % accuracy)","ed84e22e":"print(np.argmax(test_labels[25]))\nprint(test_predictions[25])","159a61c0":"import cv2\nimg = cv2.imread(test_files[25])\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)\nplt.show()","be38aa56":"test_predictions = model.predict(test_features_nasnet)\ntest_predictions_task1 = test_predictions[:,0]\ntest_predictions_task2 = test_predictions[:,2]","e065e77f":"submission_nasnet = pd.DataFrame({'Id':test_files, 'task_1':test_predictions_task1,'task_2':test_predictions_task2})\npd.DataFrame.to_csv(submission_nasnet, 'submission.csv', index=False)","242dcd92":"!python3 main.py \/home --approach 2 --modelfolder models --training --epoch 100 --batchsize 32 --maxqueuesize 10 --model weights.best.from_inception_resnet_v2 weights.best.from_nasnet ","ccc4ff85":"print(\"Starting...\\n\")\n\nstart_time = time.time()\nprint(date_time(1))\n\n# batch_size = 32\n# train_generator, validation_generator, test_generator, class_weights, steps_per_epoch, validation_steps = get_data(batch_size=batch_size)\n\nprint(\"\\n\\nCompliling Model ...\\n\")\nlearning_rate = 0.0001\noptimizer = Adam(learning_rate)\n# optimizer = Adam()\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n# steps_per_epoch = 180\n# validation_steps = 40\n\nverbose = 1\nepochs = 10\n\nprint(\"Trainning Model ...\\n\")\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=steps_per_epoch,\n    epochs=epochs,\n    verbose=verbose,\n    callbacks=callbacks,\n    validation_data=validation_generator,\n    validation_steps=validation_steps, \n    class_weight=class_weights)\n\nelapsed_time = time.time() - start_time\nelapsed_time = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n\nprint(\"\\nElapsed Time: \" + elapsed_time)\n# print(\"Elapsed Time\/Epoch: \" + elapsed_time\/epochs)\nprint(\"Completed Model Trainning\", date_time(1))","451a4f92":"def plot_performance(history=None, figure_directory=None):\n    xlabel = 'Epoch'\n    legends = ['Training', 'Validation']\n\n    ylim_pad = [0.01, 0.1]\n\n\n    plt.figure(figsize=(20, 5))\n\n    # Plot training & validation Accuracy values\n\n    y1 = history.history['acc']\n    y2 = history.history['val_acc']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[0]\n    max_y = max(max(y1), max(y2))+ylim_pad[0]\n\n\n    plt.subplot(121)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Accuracy\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Accuracy', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n\n\n    # Plot training & validation loss values\n\n    y1 = history.history['loss']\n    y2 = history.history['val_loss']\n\n    min_y = min(min(y1), min(y2))-ylim_pad[1]\n    max_y = max(max(y1), max(y2))+ylim_pad[1]\n\n\n    plt.subplot(122)\n\n    plt.plot(y1)\n    plt.plot(y2)\n\n    plt.title('Model Loss\\n'+date_time(1), fontsize=17)\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel('Loss', fontsize=15)\n    plt.ylim(min_y, max_y)\n    plt.legend(legends, loc='upper left')\n    plt.grid()\n    if figure_directory:\n        plt.savefig(figure_directory+\"\/history\")\n\n    plt.show()","c83f30cd":"plot_performance(history=history)","12cc9e2a":"ypreds = model.predict_generator(generator=test_generator, steps = len(test_generator),  verbose=1)","ebe54db8":"ypred = ypreds[:,1]","c24aff78":"sample_df = pd.read_csv(input_directory+\"sample_submission.csv\")\nsample_list = list(sample_df.id)\n\npred_dict = dict((key, value) for (key, value) in zip(test_generator.filenames, ypred))\n\npred_list_new = [pred_dict[f+'.tif'] for f in sample_list]\n\ntest_df = pd.DataFrame({'id':sample_list,'label':pred_list_new})\n\ntest_df.to_csv('submission.csv', header=True, index=False)","18888791":"test_df.head()","84cd8313":"from visuals import *\n\nmodel_names = ['weights.best.from_inception_resnet_v2', 'weights.best.from_nasnet']\nfeature_extract_epochs = 3\n\nfor model_name in model_names:\n    file_path = os.path.join(history_folder, \"{}.training.csv\".format(model_name))\n    if os.path.exists(file_path):\n        fig = plot_complexity_graph(csv_file=file_path,\n                              title=\"Complexity Graph of {}\".format(model_name),\n                              figsize=(14, 10),\n                              feature_extract_epochs=feature_extract_epochs)\n        fig.savefig(os.path.join(history_folder, \"{}.training.svg\".format(model_name)), format='svg',\n                    bbox_inches='tight', pad_inches=0)","b5d870da":"# !python3 main.py \/home --approach 2 --modelfolder models_2 --predval --predvalresultfolder predict_results_2 --model model_inception_resnet, model_nasnet\n!python main.py C:\\ISIC_2019 --approach 2 --modelfolder models_2 --predval --predvalresultfolder predict_results_2 --model weights.best.from_inception_resnet_v2 weights.best.from_nasnet ","d764f9ba":"from utils import ensemble_predictions\n\nensemble_predictions(pred_result_folder_val, category_names)","acd88878":"import pandas as pd\nfrom sklearn.metrics import balanced_accuracy_score, recall_score\nfrom visuals import plot_confusion_matrix\nfrom keras.utils import np_utils\nfrom keras_numpy_backend import categorical_crossentropy\n\nmodel_names = ['weights.best.from_inception_resnet_v2', 'weights.best.from_nasnet']\npostfix = 'best_balanced_acc'\nprint('Model selection criteria: ', postfix)\n\nfor model_name in model_names:\n    # Load predicted results\n    file_path = os.path.join(pred_result_folder_val, \"{}_{}.csv\".format(model_name, postfix))\n    if not os.path.exists(file_path):\n        continue\n\n    print(\"========== {} ==========\".format(model_name))\n    df = pd.read_csv(file_path)\n    y_true = df['category']\n    y_pred = df['pred_category']\n\n    # Compute Balanced Accuracy\n    print('balanced_accuracy_score: ', balanced_accuracy_score(y_true, y_pred))\n    print('macro recall_score: ', recall_score(y_true, y_pred, average='macro'))\n\n    # Compute categorical_crossentropy\n    y_true_onehot = np_utils.to_categorical(df['category'], num_classes=category_num)\n    y_pred_onehot = np.array(df.iloc[:,1:1+category_num])\n    print('categorical_crossentropy: ',\n          np.average(categorical_crossentropy(y_true_onehot, y_pred_onehot)))\n\n    # Compute weighted categorical_crossentropy\n    print('weighted categorical_crossentropy: ',\n          np.average(categorical_crossentropy(y_true_onehot, y_pred_onehot, class_weights=class_weights)))\n\n    # Confusion Matrix\n    fig = plot_confusion_matrix(y_true, y_pred, category_names, normalize=True,\n                                title=\"Confusion Matrix of {}\".format(model_name),\n                                figsize=(8, 6))\n    fig.savefig(os.path.join(pred_result_folder_val, \"{}_{}.svg\".format(model_name, postfix)), format='svg',\n                bbox_inches='tight', pad_inches=0)\n    print('')","ba38bdd4":"from visuals import plot_grouped_2bars\n\nsample_count_val = y_true.shape[0]\nprint(\"Validation set has {} samples.\\n\".format(sample_count_val))\n\nprint('========== Ground Truth ==========')\ncount_true = Counter(y_true)\nfor i, c in enumerate(category_names):\n    print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_true[i], count_true[i]*100\/sample_count_val))\n\nfor model_name in model_names:\n    # Load predicted results\n    file_path = os.path.join(pred_result_folder_val, \"{}_{}.csv\".format(model_name, postfix))\n    if not os.path.exists(file_path):\n        continue\n\n    print(\"\\n========== {} Prediction ==========\".format(model_name))\n    df = pd.read_csv(file_path)\n    y_pred = df['pred_category']\n    \n    count_pred = Counter(y_pred)\n    for i, c in enumerate(category_names):\n        print(\"'%s':\\t%d\\t(%.2f%%)\" % (c, count_pred[i], count_pred[i]*100\/sample_count_val))\n\n    # Plot Prediction Distribution\n    plot_grouped_2bars(\n        scalars=[[count_true[i] for i in range(category_num)],\n                 [count_pred[i] for i in range(category_num)]],\n        scalarlabels=['Ground Truth', 'Prediction'],\n        xticklabels=category_names,\n        xlabel='Category',\n        ylabel='Number of Images',\n        title=\"Prediction Distribution of {}\".format(model_name)\n    )","fa4377f4":"!python3 main.py \/home --approach 2 --modelfolder models_2 --predtest --predtestresultfolder test_predict_results_2 --model weights.best.from_inception_resnet_v2 weights.best.from_nasnet","e559ca8e":"**end of Inception_Resnet**","d4ffb92f":"**Shuffle and Split Training Data into Training and Validation Sets**","2831c71b":"**Class Weights based on the Training Set**","1d2f7f47":"**Predict Validation Set**","6c968508":"**Load Prediction Results on Validation Set**","7ba7787a":"**Deep Learning in Skin Lesion Analysis Towards Melanoma Detection** - Skin Lesion Classification","d66e1180":"Per - Channel Mean and Standard Deviation over the Training Set","6c2e6870":"**Transfer Learning**","93011444":"2. NasNetLarge","08da28f6":"**Importing Training Data**","206116d7":"1. Inception_Resnet","e8a7c2d1":"**Test DATA**","55e98d3a":"**Ensemble Models' Predictions on Validation Set**","ff82fe00":"Model Performance Visualization over the Epochs","83b3e95f":"**Model Performance**","15edc972":"**Predict Validation Set by Different Models**","0727a19d":"**COMMON PARAMETERS**","926c87cf":"**Pre-trained Models**","96da05fd":"**Predict Test Data by Different Models**","1ce4f5d5":"**Samples of each Known Category**","49523b00":"**Train Models by Transfer Learning**","e8b62241":"**Complexity Graph of Transfer Learning Models**","ed0e755f":"**Training**"}}