{"cell_type":{"6b37f6a2":"code","0ee32225":"code","166ba8f9":"code","5115aaa0":"code","e1795f71":"code","8d65b9e0":"code","98a8ae86":"code","77610a45":"code","1b3c3bce":"code","723f0707":"code","f86c5e16":"code","2ba53094":"code","83a2c4ae":"code","299ba021":"code","76698a53":"code","f9ee71a2":"code","301c2158":"code","8137c681":"code","016aac10":"code","14618106":"code","5c4cd1f7":"code","e758a752":"code","f399f587":"code","eb1907b1":"code","49497ab0":"code","b4d6e303":"code","da8f3367":"code","39c46a96":"code","18fcf33c":"code","e7ab4e3a":"code","a3b2f93a":"code","2240ca68":"code","76791eae":"code","a734060c":"code","b1a4b170":"code","c61a771d":"code","f469478c":"code","82afe3d1":"code","228a1c05":"code","c9541fe8":"code","6a521749":"code","c83f7d2e":"code","fa0ca22a":"code","e0b623bf":"code","0cc1d874":"code","13a4c827":"code","1d6c2e6d":"code","f59cda43":"code","3637d9b2":"code","a34a06ab":"markdown"},"source":{"6b37f6a2":"import pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport matplotlib.image as img","0ee32225":"train_data = pd.read_json(\"\/kaggle\/input\/facebook-hateful-meme-dataset\/data\/train.jsonl\",lines=True)\ntest_data = pd.read_json(\"\/kaggle\/input\/facebook-hateful-meme-dataset\/data\/dev.jsonl\",lines=True)","166ba8f9":"train_data.head(10)","5115aaa0":"test_data.head(10)","e1795f71":"train_data.info()","8d65b9e0":"test_data.info()","98a8ae86":"train_data.dtypes","77610a45":"test_data.dtypes","1b3c3bce":"train_data.isna().sum()","723f0707":"test_data.isna().sum()","f86c5e16":"train_data.label.value_counts()","2ba53094":"test_data.label.value_counts()","83a2c4ae":"plt.hist(train_data.label)\nplt.title('Train Data Labels',fontsize=20)\nplt.show()","299ba021":"plt.figure(figsize=(20,5))\nplt.title('Train data Label distribution', fontsize=20)\ntrain_data.label.value_counts().plot(kind='pie', labels=['Hate', 'No Hate'],\n                              wedgeprops=dict(width=.75), autopct='%1.0f%%', \n                              textprops={'fontsize': 12})\nplt.show()\nplt.title('Test data Label distribution', fontsize=20)\ntest_data.label.value_counts().plot(kind='pie', labels=['Hate', 'No Hate'],\n                              wedgeprops=dict(width=.75), autopct='%1.0f%%', \n                              textprops={'fontsize': 12})\nplt.show()","76698a53":"test_data.label.value_counts()","f9ee71a2":"import re\ndef text_data_preprocess(sentences):\n    chunk_words = {\n      \"i'm\": \"i am\",\n      \"don't\": \"do not\",\n      \"you're\": \"you are\",\n      \"it's\": \"it is\",\n      \"can't\": \"can not\",\n      \"that's\": \"that is\",\n      \"doesn't\": \"does not\",\n      \"i'll\": \"i will\",\n      \"didn't\": \"did not\",\n      \"he's\":\"he is\",\n      \"what's\": \"what is\",\n      \"there's\": \"there is\",\n      \"isn't\": \"is not\",\n      \"she's\": \"she is\",\n      \"let's\": \"let us\",\n      \"i've\": \"i have\",\n      \"they're\": \"they are\",\n      \"we're\": \"we are\",\n      \"ain't\": \"am not\",\n      \"you've\": \"you have\",\n      \"aren't\": \"are not\",\n      \"you'll\": \"you will\",\n      \"here's\": \"here is\",\n      \"haven't\": \"have not\",\n      \"i'd\": \"i had\",\n      \"they'll\": \"they will\",\n      \"won't\": \"will not\",\n      \"who's\": \"who is\",\n      \"where's\": \"where is\",\n      \"couldn't\": \"could not\",\n      \"shouldn't\": \"should not\",\n      \"wasn't\": \"was not\",\n      \"we'll\": \"we will\",\n      \"idk\": \"i do not know\",\n      \"y'all\": \"you all\",\n      \"wife's\": \"wife is\",\n      \"hasn't\": \"has not\",\n      \"she'll\": \"she will\",\n      \"we've\": \"we have\",\n      \"they've\":\"they have\",\n      \"wouldn't\": \"would not\",\n      \"name's\": \"name is\",\n      \"why's\": \"why is\",\n      \"that'd\": \"that would\",\n      \"lyin'\": \"lying\",\n      \"weren't\": \"were not\"\n  }\n    final_sentences = []\n    for sentence in sentences:\n        for key in chunk_words.keys():\n            if key in sentence:\n                sentence = sentence.replace(key,chunk_words[key])\n        sentence = re.sub(r\"'[a-z] \", ' ', sentence)\n        sentence = re.sub(r\"'\", ' ', sentence)\n        sentence = re.sub(r'[^\\w\\s]','',sentence)\n        sentence = re.sub(' +', ' ', sentence)\n        sentence = re.sub(\"\\d+\", \"\", sentence)\n        final_sentences.append(sentence)\n    return final_sentences","301c2158":"text_data_preprocess(train_data.text[0:100])","8137c681":"train_data['text_preprocess']=text_data_preprocess(train_data['text'])","016aac10":"train_data.head()","14618106":"train_data.to_csv('.\/preprocessed_data.csv')","5c4cd1f7":"path = \"..\/input\/facebook-hateful-meme-dataset\/data\/img\"\npngs = [os.path.join(path, file)\n        for file in os.listdir(path)\n        if file.endswith('.png')]\n","e758a752":"len(pngs)","f399f587":"pngs[0:10]","eb1907b1":"shapes=[]\nfor i in pngs[0:100]:\n    temp=img.imread(i)\n    shapes.append(temp.shape)\nprint(shapes[0:20])","49497ab0":"from transformers import *\nfrom transformers import TFDistilBertModel,DistilBertTokenizer,DistilBertConfig","b4d6e303":"dbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","da8f3367":"dbert_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')","39c46a96":"max_len=32\nsentences=train_data['text_preprocess']\nlabels=train_data['label']\nlen(sentences),len(labels)","18fcf33c":"dbert_tokenizer.tokenize(sentences[0])","e7ab4e3a":"dbert_tokenizer.tokenize(sentences[1])","a3b2f93a":"dbert_tokenizer.tokenize(sentences[2])","2240ca68":"dbert_tokenizer.tokenize(sentences[3])","76791eae":"dbert_inp=dbert_tokenizer.encode_plus(sentences[0],add_special_tokens = True,max_length =32,pad_to_max_length = True,truncation=True)\nprint(dbert_inp)","a734060c":"dbert_inp['input_ids']","b1a4b170":"dbert_tokenizer.decode(dbert_inp['input_ids'])","c61a771d":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport unicodedata\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Dropout, Input\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.metrics import confusion_matrix,f1_score,classification_report\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras import regularizers\nfrom transformers import *\nfrom transformers import BertTokenizer, TFBertModel, BertConfig,TFDistilBertModel,DistilBertTokenizer,DistilBertConfig","f469478c":"input_ids=[]\nattention_masks=[]\n\nfor sent in train_data['text_preprocess']:\n    dbert_inps=dbert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =max_len,pad_to_max_length = True,return_attention_mask = True,truncation=True)\n    input_ids.append(dbert_inps['input_ids'])\n    attention_masks.append(dbert_inps['attention_mask'])\n\ninput_ids=np.asarray(input_ids)\nattention_masks=np.array(attention_masks)\nlabels=np.array(labels)","82afe3d1":"def create_model():\n    inps = Input(shape = (max_len,), dtype='int64')\n    masks= Input(shape = (max_len,), dtype='int64')\n    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:,0,:]\n    dense = Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n    dropout= Dropout(0.5)(dense)\n    pred = Dense(2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n    model = tf.keras.Model(inputs=[inps,masks], outputs=pred)\n    print(model.summary())\n    return model","228a1c05":"model=create_model()","c9541fe8":"len(input_ids),len(attention_masks),len(labels)","6a521749":"print('Preparing the pickle file.....')\n\npickle_inp_path='dbert_inp.pkl'\npickle_mask_path='dbert_mask.pkl'\npickle_label_path='dbert_label.pkl'\n\npickle.dump((input_ids),open(pickle_inp_path,'wb'))\npickle.dump((attention_masks),open(pickle_mask_path,'wb'))\npickle.dump((labels),open(pickle_label_path,'wb'))\n\n\nprint('Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_label_path)","c83f7d2e":"print('Loading the saved pickle files..')\n\ninput_ids=pickle.load(open(pickle_inp_path, 'rb'))\nattention_masks=pickle.load(open(pickle_mask_path, 'rb'))\nlabels=pickle.load(open(pickle_label_path, 'rb'))\n\nprint('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))","fa0ca22a":"label_class_dict={0:'ham',1:'spam'}\ntarget_names=label_class_dict.values()","e0b623bf":"train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.2)\n\nprint('Train inp shape {} Val input shape {}\\nTrain label shape {} Val label shape {}\\nTrain attention mask shape {} Val attention mask shape {}'.format(train_inp.shape,val_inp.shape,train_label.shape,val_label.shape,train_mask.shape,val_mask.shape))\n\n\nlog_dir='dbert_model'\nmodel_save_path='.\/dbert_model.h5'\n\ncallbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n\nmodel.compile(loss=loss,optimizer=optimizer, metrics=[metric])","0cc1d874":"callbacks= [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\nmodel.compile(loss=loss,optimizer=optimizer, metrics=[metric])","13a4c827":"history=model.fit([train_inp,train_mask],train_label,batch_size=16,epochs=5,validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)","1d6c2e6d":"trained_model = create_model()\ntrained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\ntrained_model.load_weights(model_save_path)","f59cda43":"preds = trained_model.predict([val_inp,val_mask],batch_size=16)\npred_labels = preds.argmax(axis=1)\nf1 = f1_score(val_label,pred_labels)\nprint(f1)","3637d9b2":"target_names=['non hate','hate']\nprint('F1 score',f1)\nprint('Classification Report')\nprint(classification_report(val_label,pred_labels,target_names=target_names))\n\nprint('Training and saving built model.....')","a34a06ab":"## **Import Libraries**"}}