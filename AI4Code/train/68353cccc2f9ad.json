{"cell_type":{"ca032b21":"code","f6f4d7df":"code","81cec2bc":"code","ff638d1b":"code","59a8534e":"code","e45e6e72":"code","00fab303":"code","e988a302":"code","31d84100":"code","0a12abfc":"code","e439fe6e":"code","913bfbaa":"code","5beadc49":"code","3b128ab7":"code","18e08842":"code","4b091bf1":"code","01a85801":"code","16737a29":"code","f43937dc":"code","d5627d94":"code","123a6368":"code","575b18a1":"code","753e4188":"code","427c1053":"code","65bfeeaf":"code","7deb0a1a":"code","0482eaa7":"code","bcf062db":"code","dc8f27f4":"code","0f29e059":"code","22b458f7":"code","08cd2ff4":"markdown","c7988c28":"markdown","104a3810":"markdown","1aa74085":"markdown","f46872a2":"markdown","646a86db":"markdown","0154a8c2":"markdown","ccd74285":"markdown","78d37abf":"markdown","abe35535":"markdown","376a6439":"markdown","0ed829e4":"markdown","3c4c3e58":"markdown","b5e175d7":"markdown","aaa1ed72":"markdown","74e1b237":"markdown","03f431c8":"markdown","4b3770e6":"markdown","6755d870":"markdown","f240a69a":"markdown","c7a54fbf":"markdown","3a84f6ef":"markdown","5e9e2d68":"markdown","59f35e4b":"markdown","9d819f19":"markdown","565e1f25":"markdown","14b6f88e":"markdown","27de1abf":"markdown","38856b25":"markdown","a627f47e":"markdown","72b18906":"markdown","49b4630a":"markdown","5824f0ab":"markdown","479832c5":"markdown","7879a14e":"markdown","80a4fae1":"markdown","32b64160":"markdown","4df64497":"markdown","916bf1f4":"markdown","88a43e40":"markdown","bea8bdf1":"markdown","448e3b63":"markdown","d13fd360":"markdown"},"source":{"ca032b21":"# Install spacy quietly\n!pip install spacy -q","f6f4d7df":"# Download english language model\n!python -m spacy download en -q","81cec2bc":"# Install wordcloud\n!pip install wordcloud","ff638d1b":"import pandas as pd\nimport numpy as np\n\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import TruncatedSVD\n\nimport matplotlib.pyplot as plt\nimport wordcloud","59a8534e":"corpus = pd.read_csv('..\/input\/product-item-data\/sample-data.csv')","e45e6e72":"print(corpus.shape)\ncorpus.head()","00fab303":"print('Description of the first product : ',corpus.loc[0,'description'])","e988a302":"# Remove HTML elements\ncorpus['clean_description'] = corpus['description'].str.replace(r\"<[a-z\/]+>\", \" \") \n# Remove special characters and numbers\ncorpus['clean_description'] = corpus['clean_description'].str.replace(r\"[^A-Za-z]+\", \" \") \nprint('Description cleaned of the first product : ',corpus.loc[0,'clean_description'])","31d84100":"# Lowercase\ncorpus['clean_description'] = corpus['clean_description'].str.lower()\nprint('Description in lower case of the first product : ',corpus.loc[0,'clean_description'])","0a12abfc":"## Tokenize the cleaned description\ncorpus['clean_tokens'] = corpus['clean_description'].apply(lambda x: nlp(x))\ncorpus.head()","e439fe6e":"# Remove stop words\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\ncorpus['clean_tokens'] = corpus['clean_tokens'].apply(lambda x: [token.lemma_ for token in x if token.text not in STOP_WORDS])\ncorpus.head()","913bfbaa":"# Put back tokens into one single string\ncorpus[\"clean_document\"] = [\" \".join(x) for x in corpus['clean_tokens']]\ncorpus.head()","5beadc49":"# TF-IDF vector\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(corpus[\"clean_document\"])\n\n# X is a generator. We can transform that as an array\nX = X.toarray()\nprint(X.shape)","3b128ab7":"# Print the 50 first words into our vocabulary\nprint(sorted(vectorizer.vocabulary_.items())[:50])","18e08842":"# Create a dataframe with tf-idf\nX_df = pd.DataFrame(X, \n             columns=vectorizer.get_feature_names(), \n             index=[\"item_{}\".format(x) for x in range(corpus.shape[0])] )\n\nX_df.head()","4b091bf1":"# Clustering on documents with DBSCAN\nclustering = DBSCAN(eps=0.7, min_samples=3, metric=\"cosine\", algorithm=\"brute\")\n\n# Fit on data \n#No need to normalize data, it already is due to TF-IDF\nclustering.fit(X)\n\n# Write cluster ids into corpus and X_df\ncorpus['cluster_id'] = clustering.labels_\ndisplay(corpus.head())\nX_df['cluster_id'] = clustering.labels_\ndisplay(X_df.head())","01a85801":"# Number of documents in each cluster\ncorpus['cluster_id'].value_counts()","16737a29":"# Print sample of 3 documents for the 5 first cluster\nfor c in corpus['cluster_id'].value_counts().index[:5] :\n    print(\"CLUSTER \", c , ' :')\n    print('----')\n    for d in corpus.loc[corpus['cluster_id']==c,:].sample(3)['clean_description']:\n        print(d)\n        print()\n    print('-----------')","f43937dc":"# 5 Most frequent words in each cluster\ncols = [c for c in X_df.columns if c!='cluster_id']\n\nfor c in corpus['cluster_id'].value_counts().index[:5] :\n    print(\"CLUSTER \", c)\n    print(X_df.loc[X_df['cluster_id']==c,cols].mean(axis=0).sort_values(ascending=False)[0:5])\n    print('-----------')","d5627d94":"# Word cloud for the 5 first clusters\nwd = wordcloud.WordCloud()\nfor c in corpus['cluster_id'].value_counts().index[:5] :\n    print(\"CLUSTER \", c)\n    texts = \" \".join(corpus.loc[corpus['cluster_id']==c,'clean_description'])\n    cloud = wd.generate(texts)\n    plt.imshow(cloud)\n    plt.show()\n    print('-----------')","123a6368":"corpus.head()","575b18a1":"def find_similar_items(item_id):\n    \"\"\"\n    Return 5 product ids belonging to the same cluster as item_id\n    \"\"\"\n    cluster_id = corpus.loc[corpus['id']==item_id, 'cluster_id'].values[0]\n    similar_items = corpus.loc[corpus['cluster_id']==cluster_id,:].sample(5)\n    similar_item_ids = similar_items['id'].unique()\n    return similar_item_ids","753e4188":"# For printing in colors\nclass bcolors:\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'","427c1053":"product_id = int(input(\"What product would you like to buy ? \"))\nprint()\ntry:\n    item_desc = corpus.loc[corpus['id']==product_id, 'clean_description'].values[0]\nexcept:\n    print('Product not found in database. Please enter a valid product id.')\nelse:\n    print(f\"{bcolors.OKBLUE}Product found in database, description below :\")\n    print(item_desc)\n    print()\n    \n    print(\"Based on the analysis of the products' descriptions, you might also be interested by the following products : \")\n    print()\n\n    for i in find_similar_items(product_id):\n        print(f\"{bcolors.OKGREEN}Item #\", i)\n        print(corpus.loc[corpus['id']==i, 'clean_description'].values[0])\n        print('--------------------')","65bfeeaf":"# Train SVD model\nsvd_model = TruncatedSVD(n_components=12) # We test on 12 topics\nlsa = svd_model.fit_transform(X)\ntopic_encoded_df = pd.DataFrame(lsa, columns = [\"topic_\" + str(i) for i in range(lsa.shape[1])])\ntopic_encoded_df[\"documents\"] = corpus['clean_description']\ntopic_encoded_df.head()","7deb0a1a":"def extract_main_topics(x):\n    \"\"\"\n    Return the main topic for each document. The main topic is that have the maximum value for each line\n    \"\"\"\n    topics = np.abs(x)\n    main_topic = topics.sort_values(ascending=False).index[0]\n    return main_topic\n\n# Initialize column main_topics with 0\ntopic_encoded_df.loc[:, 'main_topic'] = 0\n\nfor i, row in topic_encoded_df.iloc[:,:-2].iterrows():\n    topic_encoded_df.loc[i, 'main_topic'] = extract_main_topics(row)\n\ntopic_encoded_df.head()","0482eaa7":"topic_encoded_df['main_topic'].value_counts()","bcf062db":"# Create DataFrame containing the description of each topic in terms of the words in the vocabulary\ntopics_description = pd.DataFrame(svd_model.components_, columns = vectorizer.get_feature_names(), \n                                  index = ['topic_' + str(i) for i in range(svd_model.components_.shape[0])])\n\n# Compute absolute values of coefficients\ntopics_description = topics_description.apply(np.abs, axis = 1)\n\n# Each word is map with a score of relevance for each topic\ntopics_description.head()","dc8f27f4":"# Loop over each topic and print the 5 most important words\nfor i,row in topics_description.iterrows():\n    print('TOPIC :', i)\n    print(row.sort_values(ascending=False)[0:5].index.tolist())\n    print()\n    print('-------------------------')\n    print()","0f29e059":"topic_encoded_df.head()","22b458f7":"# Loop over each topic and create wordcloud from documents that are related to this main topic\nwd = wordcloud.WordCloud()\n\ncols = [c for c in topic_encoded_df.columns if 'topic_' in c]\n\nfor t in cols:\n    print('-------------------------')\n    print()\n    print('TOPIC ', t)\n    \n    # Handle topics that are not main topics for any document in the corpus\n    if (topic_encoded_df['main_topic']==t).any() == False :\n        print('cannot create wordcloud for this topic')\n        continue\n    \n    texts = \" \".join(topic_encoded_df.loc[topic_encoded_df['main_topic']==t,'documents'])\n    cloud = wd.generate(texts)\n    plt.imshow(cloud)\n    plt.show()\n    \n    print()","08cd2ff4":"`Description is dirty due to HTML elements, punctuation and the not standardization of the words.`","c7988c28":"2. Creation of a new column named `main_topic` in `topic_encoded_df` where we store the main topics related to each document","104a3810":"2. Create a DataFrame containing the result from TF-IDF","1aa74085":"1. We Create a function named `find_similar_items` that return the 5 items ids belonging to the same cluster as the product `item_id` which is passed in arguments","f46872a2":"3. Using `str` methods to clean the texts. I save the clean texts into a column named `clean_description`","646a86db":"5. Using spacy to tokenize the documents and put the result in a new column named `clean_tokens`.","0154a8c2":"Contrary to clustering, LSA allows to map each document to a mixing of several topics. For this reason, it's a bit more difficult to interpret the topics as categories : one document can actually be related to several topics at a time. To make things easier, we can extract the main topic of each document.","ccd74285":"## Conclusion\n\n- It's difficult to compare the results from the clustering and LSA, in particular because we didn't get exactly the same number of \"topics\" for both algorithms. However, the wordclouds are not fundamentally different (for example, both algorithms identify a group of documents or topic related to sun protection, organic cotton or merino wood).\n\n- The major difference between these two approaches is that clustering maps a given document to a single group, whereas LSA links a document to several topics. For this reason, text clustering is usually more suitable for applications related to similarity measurements (for example, building a recommender system),  whereas LSA is widely used for topic modelling.","78d37abf":"- `Stop words are a set of commonly used words in a language. Examples of stop words in English are \u201ca\u201d, \u201cthe\u201d, \u201cis\u201d, \u201care\u201d and etc.`\n- `Lemmatization is a linguistic term that means grouping together words with the same root or lemma.`","abe35535":"- Latent Semantic Analysis, or LSA, is one of the foundational techniques in topic modeling. The core idea is to take a matrix of what we have \u2014 documents and terms \u2014 and decompose it into a separate document-topic matrix and a topic-term matrix.\n\n- The main goal of this part is to find relevant topics for each documents. Contrary to clustering, a unique documents can have multiple topics. For example Sport and Politics.","376a6439":"`Tokenization is a way of separating a piece of text into smaller units called tokens. Tokens can be either words, characters, or subwords.`","0ed829e4":"### Part 1 conclusion :\nNow we have the `clean_document` feature which is cleaned. We can train some NLP model on it.","3c4c3e58":"7. Writing all the cleaned tokens into one single string and put it in a new column named `clean_document`.","b5e175d7":"4. Using the attribute `components_` of the SVD model to print the 5 most important words in each topic","aaa1ed72":"3. Couting each main topic in the corpus ","74e1b237":"## Part 1 : Preprocessing <a class=\"anchor\" id=\"chapter1\"><\/a>","03f431c8":"4. Tranforming every character into lowercase.","4b3770e6":"### Part 1.2 : Text preprocessings <a class=\"anchor\" id=\"section_1_2\"><\/a>","6755d870":"Install libraries & load data","f240a69a":"# E-commerce : boosting online sales with NLP !\n\n## Introduction :\n\n- In this project, I work with a corpus of item descriptions from an outdoor apparel brand's product catalog.\n\n- The main goal is to use some NLP techniques to analyze text description of the product catalog in order to identify similar product, build a recommender system and create new topics with more meaning.","c7a54fbf":"3. Using DBSCAN to make some clustering on the TF-IDF matrix. ","3a84f6ef":"#### If you like this botebook, please up-vote \u261d\ufe0f\n#### It will help \ud83d\ude42","5e9e2d68":"1. Using TruncatedSVD to make some topic extraction from the TF-IDF matrix.","59f35e4b":"- When dealing with texts, the distance metric to be used is `cosine` instead of \"euclidean\". \n- eps = 0.7 and min samples = 3 enable to have almost 15 clusters without too much outliers","9d819f19":"1. Import pandas, numpy, spacy, sklearn (tfidf vectorizer, DBSCAN and TruncatedSVD), matplotlib and wordcloud","565e1f25":"2. Using python's `input()` function to allow the user to choose a product and submit some suggestions of similar items","14b6f88e":"Pre-requisite : if necessary, install the required libraries for NLP (spacy, wordclouds) and download the english language model. Then, import all the libraries.","27de1abf":"## Part 4 - LSA for topic extraction <a class=\"anchor\" id=\"chapter4\"><\/a>","38856b25":"### Part 1.1 : Libraries & data loading <a class=\"anchor\" id=\"section_1_1\"><\/a>","a627f47e":"`TF-IDF (term frequency-inverse document frequency)` is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n\nThis is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.","72b18906":"2. Reading the corpus and put it in a DataFrame named `corpus`. I print the full description in the first line","49b4630a":"5. Print a sample of 3 documents that belong to 5 clusters","5824f0ab":"- Each line of X correspond to a product description.\n- Each column of X correspond to a word into the vocabulary.\n- So each cell of X correspond to the score TF-IDF for a word into a product description.","479832c5":"1. TF-IDF transformation from the column `clean_document`","7879a14e":"4. Display number of documents in each cluster","80a4fae1":"- topic_0 is the most represented topic, as it's the main topic for more than `50%` of the documents of the corpus.","32b64160":"6. Print the 5 most frequent words in the 5 first clusters","4df64497":"### Table of Contents\n\n* [Part 1 : Preprocessing](#chapter1)\n    * [Part 1.1 : Libraries & data loading](#section_1_1)\n    * [Part 1.2 : Text preprocessings](#section_1_2)\n    \n\n* [Part 2 : Clustering model --> Identify similar products](#chapter2)\n\n* [Part 3 : Recommender system](#chapter3)\n\n* [Part 4 : LSA model --> topics extraction](#chapter4)\n","916bf1f4":"## Part 3 - Recommender system <a class=\"anchor\" id=\"chapter3\"><\/a>\n\nNow, we use the clusters created from part 1 to build a recommender system. \nThe aim is to be able to suggest to a user some products that are similar to the ones he is interested in. To do this, we consider that products belonging to the same cluster are similar.","88a43e40":"6. Removing the stop words and lemmatize `clean_tokens`","bea8bdf1":"7. Wordcloud for the 5 first clusters","448e3b63":"## Part 2 - Clustering model --> Identify similar products <a class=\"anchor\" id=\"chapter2\"><\/a>","d13fd360":"5. Make a wordcloud describing each topic and compare to the ones we obtain with clustering"}}