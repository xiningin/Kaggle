{"cell_type":{"696ba364":"code","cbbece61":"code","1902636d":"code","aff09394":"code","3d642734":"code","d483f6ca":"code","671f58e3":"code","1c6fc3fc":"code","85d70e03":"code","d83423e1":"code","f237db3f":"code","6d9b610a":"code","1deaff23":"code","2377ad7d":"code","f55610c1":"code","768a53f0":"code","9db3ace1":"code","2ff1043d":"code","9eb0f0d2":"code","02adfe20":"code","3c416a06":"code","46fe54f9":"code","4630224c":"code","8f76f57d":"code","b8403b53":"code","53006d9d":"code","083c8a3e":"code","1dc2aabf":"code","f6787bf6":"code","db9d4adb":"code","f75913b4":"code","d59fde24":"code","95da9481":"code","69f3af53":"code","311e0a51":"code","c8a4c22e":"code","c48ea09a":"code","93c7ff92":"code","04dc22cd":"code","4361906d":"code","59aefd24":"code","d462232d":"code","103c7d80":"code","51b5b460":"code","deef8bab":"markdown","7634eedb":"markdown","98335af1":"markdown","4da45627":"markdown","447663de":"markdown","46a960de":"markdown","ffed9ced":"markdown","21a4811e":"markdown","44f5f8e4":"markdown","4e17f640":"markdown","e2297e01":"markdown","8f7588ad":"markdown","e6aebebb":"markdown","d60e996f":"markdown","9e876a59":"markdown","1320b054":"markdown","1bdd6605":"markdown","faf2505a":"markdown","2c0b65fa":"markdown","5f676d76":"markdown","f0a37ab3":"markdown","92336916":"markdown","685f0883":"markdown","21ec4ecf":"markdown","7114615d":"markdown","b122a8d6":"markdown","e5b6a4c5":"markdown","87f33147":"markdown","41074802":"markdown","4134d0af":"markdown","326726b0":"markdown","b4b880d1":"markdown","f6a70a41":"markdown","f315f8fb":"markdown","1775b555":"markdown","fc51d815":"markdown","11ed1f99":"markdown","677a2f4c":"markdown","01af5c82":"markdown","c03b1a05":"markdown","978ae123":"markdown","2a5820cf":"markdown","a0ac53a1":"markdown","57d2d722":"markdown","cf718a25":"markdown"},"source":{"696ba364":"# display dataframes\nfrom IPython.display import display\n\n# logging\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n# default ditct\nfrom collections import defaultdict\n\n# regular expressions\nimport re\n\n# data analysis\nimport pandas as pd\n\n# display all columns\npd.set_option('max.columns', None)\n\n# linear algebra\nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# model evaluation\nfrom sklearn.model_selection import (train_test_split, \n                                     cross_val_score,\n                                     learning_curve,\n                                     GridSearchCV, \n                                     KFold)\n\n# metrics\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# persist final model pipeline\nfrom sklearn.externals import joblib\n\n# data preprocessing\nfrom sklearn.preprocessing import (StandardScaler,\n                                   LabelEncoder, \n                                   OneHotEncoder)\n\n# scikit-learn pipelines\nfrom sklearn.pipeline import make_pipeline, FeatureUnion\n\n# base class\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# models\nfrom sklearn.linear_model import (Lasso, LassoCV, Ridge, LinearRegression)\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import (RandomForestRegressor, BaggingRegressor)\nfrom xgboost import XGBRegressor\n\n# Feature selection\nfrom sklearn.feature_selection import SelectFromModel\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cbbece61":"# load data\ndata = pd.read_csv('..\/input\/train.csv')","1902636d":"# inspect head\ndata.head()","aff09394":"# inspect types\ndata.info(memory_usage='deep')","3d642734":"# data shape\nrows, cols = data.shape\nprint(f\"The dataset is composed of {rows} rows and {cols} columns.\")","d483f6ca":"# set seed\nSEED = 123\n\n# separate data into train and test\nX_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['SalePrice', 'Id']),\n                                                    data['SalePrice'], \n                                                    test_size=0.15, random_state=SEED)","671f58e3":"print(f\"Shape of training features {X_train.shape}\")\nprint(f\"Shape of test features {X_test.shape}\")","1c6fc3fc":"# Visualizing the target variable\nplt.figure(figsize=(12,8))\nsns.distplot(y_train, label=f'train target, skew: {y_train.skew():.2f}')\nsns.distplot(y_test, label=f'test target, skew: {y_test.skew():.2f}')\nplt.legend(loc='best')\nplt.show()","85d70e03":"# log transform target\ny_train = np.log(y_train)\ny_test = np.log(y_test)\n\n# Visualizing the target variable\nplt.figure(figsize=(12,8))\nsns.distplot(y_train, label=f'train target, skew: {y_train.skew():.2f}')\nsns.distplot(y_test, label=f'test target, skew: {y_test.skew():.2f}')\nplt.legend(loc='best')\nplt.show()","d83423e1":"cat_vars = data.select_dtypes(include=['object']).columns.tolist()\n\nfor cat_var in cat_vars:\n    fig, ax = plt.subplots(1, 2, figsize=(12,5))\n    try:\n        sns.countplot(y=cat_var, data=X_train, label='train', ax=ax[0])\n        sns.countplot(y=cat_var, data=X_test, label='test', ax=ax[1])\n        ax[0].set_title(cat_var + \" Train\")\n        ax[1].set_title(cat_var + \" Test\")\n        plt.legend(loc='best')\n        plt.show()\n    except Exception as e:\n        print(e)","f237db3f":"# year variables\nyear_cols = [col for col in data.columns if 'Yr' in col or 'Year' in col]\n\nfor col in year_cols:\n    if col == 'YrSold':\n        continue\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(12,5))\n        sns.distplot((X_train['YrSold'] - X_train[col]).dropna(), ax=ax[0])\n        ax[0].set_title(f\"{col} train\")\n        sns.distplot((X_test['YrSold'] - X_test[col]).dropna(), ax=ax[1])\n        ax[1].set_title(f\"{col} test\")        \n        plt.show()\n    except Exception as e:\n        print(e)","6d9b610a":"# explore continuous variables\ncont_vars = data.select_dtypes(include=['float', 'int']).columns.tolist()\n\n# assume that columns having more than 20 unique integer variables are continuous\ncont_vars = [col for col in cont_vars if (data[col].nunique() > 20)\n             and (col not in ['Id','SalePrice'] + year_cols)\n            ]\n\nfor cont_var in cont_vars:\n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(12,5))\n        sns.distplot(X_train[cont_var], ax=ax[0])\n        ax[0].set_title(f\"{cont_var} train\")\n        sns.distplot(X_test[cont_var], ax=ax[1])\n        ax[0].set_title(f\"{cont_var} test\")        \n        plt.show()\n    except Exception as e:\n        print(e)","1deaff23":"# categorical integer variables\ncat_int_vars = data.select_dtypes(include=['int']).columns\ncat_int_vars = [col for col in cat_int_vars if (col not in cont_vars + year_cols + ['Id', 'SalePrice'])]\n\nfor cat_int_var in cat_int_vars:\n    try:\n        fig, ax = plt.subplots(1,2, figsize=(12,5))\n        sns.countplot(y=X_train[cat_int_var], ax=ax[0])\n        ax[0].set_title(f\"{cat_int_var} train\")\n        sns.countplot(y=X_test[cat_int_var], ax=ax[1])\n        ax[1].set_title(f\"{cat_int_var} test\")\n        plt.show()\n    except Exception as e:\n        print(e)","2377ad7d":"def summarize_missingness(df):\n    '''\n    Utility function to summarize missing values\n    '''\n    nulls = df.isnull()\n    counts = nulls.sum()\n    percs = nulls.mean().mul(100.)\n    \n    nulls_df = pd.DataFrame({'Count of missing values': counts, 'Percentage of missing values': percs}, \n                            index=counts.index)\n    \n    display(nulls_df)","f55610c1":"# flag variables with missing values\nvars_with_na = [col for col in X_train.columns if X_train[col].isnull().sum() > 0]\n\nfor dataframe in [X_train, X_test]:\n    summarize_missingness(dataframe[vars_with_na])","768a53f0":"to_drop = []\n\nfor var in vars_with_na:\n    if X_train[var].isnull().mean() > 0.9:\n        to_drop.append(var)\n        \nX_train.drop(columns=to_drop, inplace=True)\nX_test.drop(columns=to_drop, inplace=True)\n\nvars_with_na = [var_with_na for var_with_na in vars_with_na if var_with_na not in to_drop]\ncat_vars = [col for col in cat_vars + cat_int_vars if col not in to_drop]\nnum_vars = [col for col in cont_vars + year_cols if col not in to_drop]\n\nsummarize_missingness(X_train[vars_with_na])\nsummarize_missingness(X_test[vars_with_na])","9db3ace1":"# deal with missing values in categorical columns\ncat_vars_with_na = [col for col in vars_with_na if col in cat_vars]\n\n# fill these with 'Missing'\ndef fill_cat_na(X, var_list):\n    df = X.copy()\n    for var in var_list:\n        if df[var].dtypes == \"object\":\n            # handle objects\n            df[var] = X[var].fillna('Missing')\n        else:\n            # handle integers\n            df[var] =  X[var].fillna(-999)\n    return df\n\n# Perform imputation and assert there are no missing vals left\nX_train = fill_cat_na(X_train, cat_vars_with_na)\nassert X_train[cat_vars_with_na].isnull().sum().sum() == 0\nsummarize_missingness(X_train[cat_vars_with_na])\n\nX_test = fill_cat_na(X_test, cat_vars_with_na)\nassert X_test[cat_vars_with_na].isnull().sum().sum() == 0\nsummarize_missingness(X_test[cat_vars_with_na])","2ff1043d":"# deal with missing values in numerical columns\nnum_vars_with_na = [col for col in vars_with_na if col in num_vars]\n\n# impute by replacing with mode and flagging the missing value\ndef fill_num_na(X_train, X_test, var_list):\n    for var in var_list:\n        # determine mode\n        mode_val = X_train[var].mode()[0]\n        \n        # impute training and flag\n        X_train[var + '_na'] = np.where(X_train[var].isnull(), 1,0)\n        X_train[var].fillna(mode_val, inplace=True)\n        \n        # impute test and flag\n        X_test[var + '_na'] = np.where(X_test[var].isnull(), 1,0)\n        X_test[var].fillna(mode_val, inplace=True)\n    \n    # make sure there are no missing value left\n    for frame in [X_train, X_test]:\n        assert frame[var_list].isnull().sum().sum() == 0\n        \n    return X_train, X_test\n\nX_train, X_test = fill_num_na(X_train, X_test, num_vars_with_na)\n\nsummarize_missingness(X_train[vars_with_na])\nsummarize_missingness(X_test[vars_with_na])","9eb0f0d2":"# assert that all missing values were handled in train and test\nassert X_train.isnull().sum().sum() == 0\nassert X_test.isnull().sum().sum() == 0","02adfe20":"num_vars_corr = list(set(num_vars).difference(year_cols))\n\nfor num_var in num_vars_corr:\n    try:\n        corr_coef_train = np.corrcoef(X_train[num_var], np.exp(y_train))[0,1]\n        corr_coef_test = np.corrcoef(X_test[num_var], np.exp(y_test))[0,1]\n        fig, ax = plt.subplots(1,2, figsize=(15,6))\n        ax[0].set_title(f\"Dependence of SalePrice on {num_var} train\")\n        ax[0].scatter(X_train[num_var], np.exp(y_train), label=f\"Correlation: {np.round(corr_coef_train, 2)}\")\n        ax[0].set_ylabel(\"SalePrice\")\n        ax[0].set_xlabel(f\"{num_var}\")\n        ax[0].legend()\n        ax[1].set_title(f\"Dependence of SalePrice on {num_var} test\")\n        ax[1].scatter(X_test[num_var], np.exp(y_test), c='r', label=f\"Correlation: {np.round(corr_coef_test, 2)}\")\n        ax[1].set_ylabel(\"SalePrice\")\n        ax[1].set_xlabel(f\"{num_var}\")\n        ax[1].legend()\n        plt.show()\n    except Exception as e:\n        print(e)","3c416a06":"df_num = pd.concat([X_train[num_vars_corr], pd.DataFrame(y_train)], axis=1)\ncorrelations = df_num.corr()\n\nfig, ax = plt.subplots(figsize=(16,12))\nmask = np.zeros_like(correlations)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(correlations, cmap='coolwarm', mask=mask,\n            annot=True, fmt='.2f', square=False, ax=ax)\nplt.show()","46fe54f9":"for year_col in year_cols:\n    if year_col == 'YrSold':\n        continue    \n    try:\n        fig, ax = plt.subplots(1, 2, figsize=(12,5))\n        sns.scatterplot(x=(X_train['YrSold'] - X_train[year_col]), y=np.exp(y_train), ax=ax[0])\n        ax[0].set_title(year_col + \" train\")\n        sns.scatterplot(x=(X_test['YrSold'] - X_test[year_col]), y=np.exp(y_test), \n                        ax=ax[1])\n        ax[1].set_title(year_col + \" test\")        \n        plt.show()\n    except Exception as e:\n        print(e)","4630224c":"for cat_col in cat_vars:\n    try:\n        fig, ax = plt.subplots(1,2, figsize=(12,5))\n        sns.barplot(x=X_train[cat_col], y=np.exp(y_train), estimator=np.median, ax=ax[0])\n        ax[0].set_title(f\"SalePrice median verus {col} train\")\n        sns.barplot(x=X_test[cat_col], y=np.exp(y_test), estimator=np.median, ax=ax[1])\n        ax[1].set_title(f\"SalePrice median verus {col} test\")\n        plt.tight_layout()\n        plt.show()\n    except Exception as e:\n        print(e)","8f76f57d":"def engineer_time_features(X_train, X_test, year_cols, ref_year='YrSold'):\n    '''\n    Utility function to engineer time features\n    '''\n    for col in year_cols:\n        if col != ref_year:\n            X_train[col] = X_train[ref_year] - X_train[col]\n            X_test[col] = X_test[ref_year] - X_test[col]\n            \n    X_train.drop(columns=ref_year, inplace=True)\n    X_test.drop(columns=ref_year, inplace=True)\n    \n    return X_train, X_test\n\nX_train, X_test = engineer_time_features(X_train, X_test, year_cols, ref_year='YrSold')\nnum_vars = [num_var for num_var in num_vars if num_var != 'YrSold']","b8403b53":"def engineer_rare_cat_vars(X_train, X_test, cat_vars, min_perc=0.03):\n    '''\n    Utility function to engineer rare categories.\n    \n    Removes categorical features with 1 category.\n    \n    Returns list of categorical variables also after removing features with 1 category.\n    '''\n    to_drop = []\n    \n    for col in cat_vars:\n        if X_train[col].dtypes == 'object':\n            # Find percentage of categories\n            percs = X_train[col].value_counts(normalize=True)\n            \n            # rare categories are the ones having a % smaller than min_perc\n            rare_categories = percs[percs < min_perc].index\n            \n            # Replace these with \"Rare\"\n            X_train.loc[X_train[col].isin(rare_categories), col] = \"Rare\"\n            X_test.loc[X_test[col].isin(rare_categories), col] = \"Rare\"\n            \n            logging.info(f\"Engineered categories for {col}\")\n        \n        # Remove features with one category only\n        if X_train[col].nunique() < 2:\n            to_drop.append(col)\n    \n    # drop columns containing one category if they exist\n    if to_drop:\n        X_train.drop(columns=to_drop, inplace=True)\n        X_test.drop(columns=to_drop, inplace=True)\n        cat_vars = list(set(cat_vars).difference(to_drop))\n    \n\n    return X_train, X_test, cat_vars, cat_vars","53006d9d":"X_train, X_test, cat_vars, cat_vars = engineer_rare_cat_vars( X_train, X_test, cat_vars)","083c8a3e":"class ColumnSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, cols):\n        if not isinstance(cols, list):\n            cols = [cols]\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X[self.cols] ","1dc2aabf":"class Scaler(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, cols):\n        if not isinstance(cols, list):\n            cols = [cols]\n        self.cols = cols\n    \n    def fit(self, X, y=None):\n        self.sc = {}\n        for col in self.cols:\n            sc = StandardScaler()\n            self.sc[col] = sc.fit(X[col].values.reshape(-1,1))\n        return self\n    \n    def transform(self, X, y=None):\n        X_final = X[self.cols].copy(deep=True)\n        for col in self.cols:\n            X_final[col] = self.sc[col].transform(X_final[col].values.reshape(-1,1))\n        X_final.columns = [f\"{col}_standardized\" for col in X_final.columns]\n        self.X = X_final\n        return X_final","f6787bf6":"import random \n\nclass OHE(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, cols):\n        if not isinstance(cols, list):\n            cols = [cols]\n        self.cols = cols\n            \n    def fit(self, X, y=None):\n        self.classes = {}\n        self.le_dict = {}\n        self.ohe = {}\n        for col in self.cols:\n            if X[col].dtypes != 'object': # Skip integer classes\n                continue\n            le = LabelEncoder()\n            ohe = OneHotEncoder(sparse=False)\n            X1 =  le.fit_transform(X[col]).reshape(-1,1)\n            self.le_dict[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n            self.classes[col] = [f\"{col}_{class_}\" for class_ in le.classes_]\n            self.ohe[col] =  ohe.fit(X1)\n\n        return self\n\n    \n    def transform(self, X, y=None):\n        X_final = X[self.cols].copy(deep=True)\n        for col in self.cols:\n            if X[col].dtypes != 'object':\n                continue\n            class_choices = list(self.le_dict[col].values())\n            X1 = X[col].apply(lambda x: \n                              self.le_dict[col].get(x, random.choice(class_choices))\n                              ).values.reshape(-1,1) \n            X_trans = self.ohe[col].transform(X1)\n            X_final = X_final.drop(columns=col)\n            cur_cols = X_final.columns.tolist()\n            X_final = np.concatenate([X_final.values, X_trans], axis=1)\n            X_final = pd.DataFrame(X_final, columns = cur_cols + self.classes[col],\n                                   index=X.index.tolist())\n        return X_final","db9d4adb":"ohe = OHE(cat_vars)\nnum_selector = ColumnSelector(num_vars)\n\nX_train_num = num_selector.fit_transform(X_train)\nX_test_num = num_selector.transform(X_test)\nnum_vars = X_train_num.columns.tolist()\n\nX_train_cat = ohe.fit_transform(X_train)\nX_test_cat = ohe.transform(X_test)\ncat_vars = X_train_cat.columns.tolist()","f75913b4":"X_train = pd.concat([X_train_cat, X_train_num], axis=1, ignore_index=False)\nX_test = pd.concat([X_test_cat, X_test_num], axis=1, ignore_index=False)","d59fde24":"feature_selection_pipeline = make_pipeline(\n                                        FeatureUnion([\n                                            (\"Numerical\", make_pipeline(ColumnSelector(num_vars),\n                                                                        StandardScaler())),\n                                            (\"Categorical\", ColumnSelector(cat_vars))                         \n                                             ]),\n                                        LassoCV(alphas=[5*10**(-4), 10**(-3), 10**(-2)], n_jobs=-1)\n                                        )","95da9481":"feature_selection_pipeline.fit(X_train, y_train)\n\nselector = SelectFromModel(feature_selection_pipeline.named_steps['lassocv'],\n                           threshold=10**(-3),\n                           prefit=True)\n\nselected_cols = X_train.columns[selector.get_support()]\nX_train_transf = X_train[selected_cols]\nX_test_transf = X_test[selected_cols]\n\ncat_vars_trans = [col for col in cat_vars if col in selected_cols]\nnum_vars_trans = [col for col in num_vars if col in selected_cols]","69f3af53":"print(f\"Dimensionality of the dataset before feature selection: {X_train.shape[1]}\")\nprint(f\"Dimensionality of the dataset after feature selection: {X_train_transf.shape[1]}\")","311e0a51":"def build_and_evaluate_models(X, y, models_list, preprocessing_pipeline, scoring, cv):\n    '''\n    Helper function that evaluates a list of sklearn models using k-fold CV.\n    \n    returns a pandas dataframe of cv negative RMSEs.\n    '''\n    \n    scores = dict()\n    best_score, best_model = -float('inf'), None\n    \n    for model in models_list:\n        model_name = model.__class__.__name__\n        logging.info(f\"Evaluating model {model_name}\")\n        \n        pipe = make_pipeline(preprocessing_pipeline, model)\n        cv_scores = cross_val_score(pipe, X, y,                                  \n                                     scoring=scoring,\n                                     cv=cv,\n                                     n_jobs=-1)\n        scores[model_name] = cv_scores\n        \n        cv_mean = cv_scores.mean()\n        if cv_mean > best_score:\n            best_score = cv_mean\n            best_model = pipe\n        \n        logging.info(f\"Done from evaluating model {model_name}\")\n     \n    scores = pd.DataFrame(scores)\n    \n    # multiply result by -1 and take root to obtain rmse\n    scores = np.sqrt(-scores)\n    \n    return -scores, best_model","c8a4c22e":"# Select all columns\nall_columns = num_vars_trans + cat_vars_trans\n\n# Build preprocessing pipeline\npreprocessing_pipeline = make_pipeline(\n                                        ColumnSelector(all_columns),\n                                        FeatureUnion([\n                                        (\"Numerical\", make_pipeline(ColumnSelector(num_vars_trans),\n                                                                    StandardScaler())),\n                                        (\"Categorical\", make_pipeline(ColumnSelector(cat_vars_trans)))\n                                         ])\n                                      )\nmodels = [\n            LinearRegression(), \n            SVR(),\n            XGBRegressor(random_state=SEED, n_jobs=-1),\n            Lasso(random_state=SEED),\n            RandomForestRegressor(random_state=SEED, n_jobs=-1)\n         ]\n\nN_SPLITS = 5\ncv = KFold(n_splits=N_SPLITS, random_state=SEED)\n\nresults, best_model = build_and_evaluate_models(X_train, y_train, models_list=models, \n                                                preprocessing_pipeline=preprocessing_pipeline,\n                                                scoring='neg_mean_squared_error',\n                                                cv=cv)","c48ea09a":"with plt.style.context('fivethirtyeight'):\n    fig, ax = plt.subplots(figsize=(12,8))\n    sns.boxplot(orient='h', data=results, ax=ax)\n    plt.title(f\"{N_SPLITS}-fold negative RMSE CV Scores of the different models.\")\n    plt.show()","93c7ff92":"# Set hyperparams grid\nhyperparams_grid = {\n                    'xgbregressor__colsample_bylevel': [0.6, 0.7, 0.8],\n                    'xgbregressor__max_depth': [2, 3, 4, 5],\n                    'xgbregressor__subsample': [0.6, 0.8, 0.9],\n                    'xgbregressor__learning_rate': [0.06]\n                    }\n\n# instantiate grid search\ngs = GridSearchCV(best_model, param_grid=hyperparams_grid, cv=cv,\n                  scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n\n# search\ngs.fit(X_train, y_train)","04dc22cd":"# best score\nbest_rmse = np.sqrt(-gs.best_score_)\n\nprint(f\"Best CV RMSE {best_rmse:.5f}\")","4361906d":"# best fitted model\nbest_model = gs.best_estimator_\n\n# Evaluate best model on test set\ny_test_pred = best_model.predict(X_test)\n\nrmse_test = np.sqrt(MSE(y_test, y_test_pred))\n\nprint(f\"Test set RMSE: {rmse_test:.5f}\")","59aefd24":"def plot_learning_curves(model, X, y, cv, scoring='neg_mean_squared_error'):\n    train_sizes, train_scores, test_scores = learning_curve(model, X, y, \n                                                            train_sizes= np.arange(0.1, 1.1, 0.1),\n                                                            scoring=scoring,\n                                                            cv=cv, \n                                                            n_jobs=-1)\n    train_scores_mean = np.sqrt(-train_scores).mean(axis=1)\n    train_scores_std =  np.sqrt(-train_scores).std(axis=1)\n    test_scores_mean = np.sqrt(-test_scores).mean(axis=1)\n    test_scores_std = np.sqrt(-test_scores).std(axis=1)\n    \n    \n    with plt.style.context('fivethirtyeight'):\n        plt.figure(figsize=(12,8))\n        plt.title(f\"Learning Curve for {best_model.named_steps['xgbregressor'].__class__.__name__}\")\n        plt.plot(train_sizes, train_scores_mean, label='train')\n        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n                         train_scores_mean + train_scores_std, alpha=0.3)\n        plt.plot(train_sizes, test_scores_mean, label='test')\n        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n                 test_scores_mean + test_scores_std, alpha=0.3)\n        plt.xlabel('Training set size')\n        plt.ylabel('RMSE')\n        plt.legend(loc='best')\n        plt.show()","d462232d":"plot_learning_curves(best_model, X_train, y_train, cv=cv)","103c7d80":"resid_df = pd.DataFrame({'y_test': np.exp(y_test), \n                         'Residuals (y_pred - y_test)': np.exp(y_test) - np.exp(y_test_pred)})\n\nwith plt.style.context('fivethirtyeight'):\n    sns.lmplot(x='y_test', y='Residuals (y_pred - y_test)', data=resid_df, fit_reg=False, aspect=1.8)\n    plt.title(\"Residual plot\")\n    plt.show()","51b5b460":"with plt.style.context('fivethirtyeight'):\n    fig, ax =  plt.subplots(figsize=(12,8))\n    sns.distplot(resid_df['Residuals (y_pred - y_test)'], ax=ax)\n    plt.title(\"Residual Density Plot\")\n    plt.show()","deef8bab":"## 5. Feature Engineering","7634eedb":"Finally, let's check whether the resuduals are normally distributed around 0. ","98335af1":"How many missing values?","4da45627":"## 7. Model Building & Evaluation","447663de":"### 4. More EDA","46a960de":"We are slightly overfitting as the test RMSE is a bit higher than the train set RMSE! We should experiment more with regularization and tree pruning to reduce overfitting. This would be a good exercise!\n\n### Residual Plot","ffed9ced":"We can see how the distribution is skewed to the right. Again this means that there's a room for improvement!","21a4811e":"## Anatomy of the Analysis","44f5f8e4":"Lots of outliers out there! It's good to find ways to handle these!","4e17f640":"We have a few outliers that are pulling the distribution towards higher prices. This means that there's a lot of room for improvement! We can now plot a histogram to see wether our residuals are normally distributed.","e2297e01":"Apparently the best model is XGBoost, let's plot a boxplot to visualize the performances of the different models.","8f7588ad":"Your analysis should have the following anatomy:\n\n1. Loading and inspecting the dataset\n2. Exploratory Data Analysis & Visualization\n3. Dealing with missing values\n4. More EDA\n5. Feature Engineering\n6. Feature Selection\n7. Model Building & Evaluation\n8. Model Validation, wrap-up & next steps","e6aebebb":"### Engineering time features","d60e996f":"## Scenario, Problem & Dataset Description","9e876a59":"## 8. Model Validation\n\n### Learning Curves","1320b054":"A lot of the categorical variables have rare labels that appear so little. This hints that we can think of performing feature engineering by grouping rare categories in a single category named \"Rare\".","1bdd6605":"### 1. Loading & Inspecting the data ","faf2505a":"### Tuning the best model","2c0b65fa":"Let's check how many features have we gotten rid of.","5f676d76":"Let's first start by reducing the number of the dataset by applying a feature selection pipeline. This pipeline contains a preprocessing step that standardizes numerical features. The LASSO CV model is then used to shrink the coefficients of features that contribute very little to the predictive performance of the model. These features are removed and only relevant features are kept.","f0a37ab3":"#### Correlation of SalePrice with Numerical Features","92336916":"We can now plot the learning curves to check whether we are overfitting the training set!","685f0883":"## 6. Feature Selection","21ec4ecf":"### Engineering rare categories","7114615d":"Let's drop columns where the percentage of missingness is greater than 90%.","b122a8d6":"In this project-based course you will learn how to strcuture a supervised machine learning project using data analysis and machine learning tools. You will learn how to build and validate a statistical\/machine learning model. Along the way, you will perform exploratory data analysis, you will run machine learning experiments and you will learn how to structure your codebase properly using OOP and scikit-learn pipelines.","e5b6a4c5":"# Structuring a Supervised Machine Leanring Project\n\n**Author**: Elie Kawerk","87f33147":"### Preparing the right format for modeling\n\nPrior to doing any modeling, we should prepare the data in the right format. For categrorical variables, we will do one-hot-encoding and for numerical variables, we'll perform Standrardization. All these operations can be encapsulated in a scikit-learn pipeline later on.\n\nLet's first start by creating a scikit-learn estimator that select particular columns.","41074802":"#### Visualizing Categorical Variables","4134d0af":"### 2. EDA and Visualization","326726b0":"It works! the target variable is mich more like a normal distribution after a log transformation.","b4b880d1":"#### Scatterplots to Explore the Dependence of SalePrice on Numerical Features","f6a70a41":"You are a data scientist at a real-estate company. Your company has a dataset showing the prices of houses along with 79 features decribing these houses. Your boss wants you to analyze this dataset, come up with a set of insights and to use these data to build a machine learning model that can be used to predict the price of a house based on these 79 features. The target variable here is `SalePrice`.\n\nThe dataset can be downloaded from Kaggle using the following link: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data. You will perform the analyis using the `train.csv` file.\n\nHere's a description of the different fields you can find in the file:\n\n- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n- MSSubClass: The building class\n- MSZoning: The general zoning classification\n- LotFrontage: Linear feet of street connected to property\n- LotArea: Lot size in square feet\n- Street: Type of road access\n- Alley: Type of alley access\n- LotShape: General shape of property\n- LandContour: Flatness of the property\n- Utilities: Type of utilities available\n- LotConfig: Lot configuration\n- LandSlope: Slope of property\n- Neighborhood: Physical locations within Ames city limits\n- Condition1: Proximity to main road or railroad\n- Condition2: Proximity to main road or railroad (if a second is present)\n- BldgType: Type of dwelling\n- HouseStyle: Style of dwelling\n- OverallQual: Overall material and finish quality\n- OverallCond: Overall condition rating\n- YearBuilt: Original construction date\n- YearRemodAdd: Remodel date\n- RoofStyle: Type of roof\n- RoofMatl: Roof material\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n- MasVnrType: Masonry veneer type\n- MasVnrArea: Masonry veneer area in square feet\n- ExterQual: Exterior material quality\n- ExterCond: Present condition of the material on the exterior\n- Foundation: Type of foundation\n- BsmtQual: Height of the basement\n- BsmtCond: General condition of the basement\n- BsmtExposure: Walkout or garden level basement walls\n- BsmtFinType1: Quality of basement finished area\n- BsmtFinSF1: Type 1 finished square feet\n- BsmtFinType2: Quality of second finished area (if present)\n- BsmtFinSF2: Type 2 finished square feet\n- BsmtUnfSF: Unfinished square feet of basement area\n- TotalBsmtSF: Total square feet of basement area\n- Heating: Type of heating\n- HeatingQC: Heating quality and condition\n- CentralAir: Central air conditioning\n- Electrical: Electrical system\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n- LowQualFinSF: Low quality finished square feet (all floors)\n- GrLivArea: Above grade (ground) living area square feet\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms\n- FullBath: Full bathrooms above grade\n- HalfBath: Half baths above grade\n- Bedroom: Number of bedrooms above basement level\n- Kitchen: Number of kitchens\n- KitchenQual: Kitchen quality\n- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n- Functional: Home functionality rating\n- Fireplaces: Number of fireplaces\n- FireplaceQu: Fireplace quality\n- GarageType: Garage location\n- GarageYrBlt: Year garage was built\n- GarageFinish: Interior finish of the garage\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet\n- GarageQual: Garage quality\n- GarageCond: Garage condition\n- PavedDrive: Paved driveway\n- WoodDeckSF: Wood deck area in square feet\n- OpenPorchSF: Open porch area in square feet\n- EnclosedPorch: Enclosed porch area in square feet\n- 3SsnPorch: Three season porch area in square feet\n- ScreenPorch: Screen porch area in square feet\n- PoolArea: Pool area in square feet\n- PoolQC: Pool quality\n- Fence: Fence quality\n- MiscFeature: Miscellaneous feature not covered in other categories\n- MiscVal: Value of miscellaneous feature\n- MoSold: Month Sold\n- YrSold: Year Sold\n- SaleType: Type of sale\n- SaleCondition: Condition of sale","f315f8fb":"### Wrap-up","1775b555":"The Sale Price drops with time relatively to the year sold.","fc51d815":"In this project, you went through a complete case study that showed you how to use your data analysis and machine learning skills to build a supervised machine learning model.\n\n**Improvements:**\n\n- Perform more EDA to engineer further features that could expose the model to more relevant signal.\n- Perform outlier detection and understand how these can be handled.\n- Experiment with more models and with model ensembling.\n- I could go on forever ...\n\n\nBest of luck in your data science and machine learning journey!\n\n*Elie Kawerk, PhD* ","11ed1f99":"#### Effect of year on SalePrice","677a2f4c":"Let's now use the reduced dataset to train a bunch of models, cross-validate and evaluate them and pick the best performing model. The function below helps us do this.","01af5c82":"### Influence of Categorical Features on SalePrice","c03b1a05":"As a first step, we will start by importing all the libraries that we shall use in our subsequent analysis.","978ae123":"Great! We went from 199 features to 77, that's 122 features less!","2a5820cf":"### 3. Dealing with Missing Values","a0ac53a1":"Let's now tune the best model using grid search Cross validation!","57d2d722":"The target variable is positively skewed, we can perform a log transformation to render the target distribution more gaussian like.","cf718a25":"Before starting our analysis, it is important to separate our data intro training and testing set. This is done to avoid over-fitting. There is an element of randomness in dividing the dataset, so remember to set the seed."}}